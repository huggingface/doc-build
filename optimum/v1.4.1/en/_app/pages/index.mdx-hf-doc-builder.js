import{S as it,i as dt,s as ct,e as o,k as d,w as ht,t as l,M as mt,c as s,d as t,m as c,a as n,x as pt,h as i,b as r,G as e,g as u,y as ft,L as ut,q as vt,o as gt,B as wt,v as yt}from"../chunks/vendor-hf-doc-builder.js";import{I as xt}from"../chunks/IconCopyLink-hf-doc-builder.js";function _t(Ue){let v,J,g,E,L,A,le,B,ie,j,T,de,K,z,ce,Q,$,he,W,I,h,w,G,me,pe,k,fe,V,ue,ve,ge,y,X,we,ye,O,xe,q,_e,be,Ee,x,D,ke,Oe,p,Ae,C,Ie,Pe,R,Ne,Te,ze,_,S,$e,Ge,P,Ve,H,Xe,qe,b,M,De,Ce,N,Re,U,Se,Y;return A=new xt({}),{c(){v=o("meta"),J=d(),g=o("h1"),E=o("a"),L=o("span"),ht(A.$$.fragment),le=d(),B=o("span"),ie=l("\u{1F917} Optimum"),j=d(),T=o("p"),de=l("\u{1F917} Optimum is an extension of \u{1F917} Transformers that provides a set of performance optimization tools to train and run models on targeted hardware with maximum efficiency."),K=d(),z=o("p"),ce=l(`The AI ecosystem evolves quickly, and more and more specialized hardware along with their own optimizations are emerging every day.
As such, \u{1F917} Optimum enables developers to efficiently use any of these platforms with the same ease inherent to \u{1F917} Transformers.`),Q=d(),$=o("p"),he=l("\u{1F917} Optimum is distributed as a collection of packages - check out the links below for an in-depth look at each one."),W=d(),I=o("div"),h=o("div"),w=o("a"),G=o("div"),me=l("Optimum Graphcore"),pe=d(),k=o("p"),fe=l("Train transformers on "),V=o("a"),ue=l("Graphcore IPUs"),ve=l(", a completely new kind of massively parallel processor to accelerate machine intelligence."),ge=d(),y=o("a"),X=o("div"),we=l("Optimum Habana"),ye=d(),O=o("p"),xe=l("Maximize training throughput and efficiency with "),q=o("a"),_e=l("Habana's Gaudi processor"),be=l("."),Ee=d(),x=o("a"),D=o("div"),ke=l("Optimum Intel"),Oe=d(),p=o("p"),Ae=l("Use Intel's "),C=o("a"),Ie=l("Neural Compressor"),Pe=l(" and "),R=o("a"),Ne=l("OpenVINO"),Te=l(" frameworks to accelerate transformer inference."),ze=d(),_=o("a"),S=o("div"),$e=l("ONNX Runtime"),Ge=d(),P=o("p"),Ve=l("Apply quantization and graph optimization to accelerate transformer training and inference with "),H=o("a"),Xe=l("ONNX Runtime"),qe=d(),b=o("a"),M=o("div"),De=l("Torch FX"),Ce=d(),N=o("p"),Re=l("Create and compose custom graph transformations to optimize PyTorch transformer models with "),U=o("a"),Se=l("Torch FX"),this.h()},l(a){const m=mt('[data-svelte="svelte-1phssyn"]',document.head);v=s(m,"META",{name:!0,content:!0}),m.forEach(t),J=c(a),g=s(a,"H1",{class:!0});var Z=n(g);E=s(Z,"A",{id:!0,class:!0,href:!0});var Fe=n(E);L=s(Fe,"SPAN",{});var Le=n(L);pt(A.$$.fragment,Le),Le.forEach(t),Fe.forEach(t),le=c(Z),B=s(Z,"SPAN",{});var Be=n(B);ie=i(Be,"\u{1F917} Optimum"),Be.forEach(t),Z.forEach(t),j=c(a),T=s(a,"P",{});var Je=n(T);de=i(Je,"\u{1F917} Optimum is an extension of \u{1F917} Transformers that provides a set of performance optimization tools to train and run models on targeted hardware with maximum efficiency."),Je.forEach(t),K=c(a),z=s(a,"P",{});var je=n(z);ce=i(je,`The AI ecosystem evolves quickly, and more and more specialized hardware along with their own optimizations are emerging every day.
As such, \u{1F917} Optimum enables developers to efficiently use any of these platforms with the same ease inherent to \u{1F917} Transformers.`),je.forEach(t),Q=c(a),$=s(a,"P",{});var Ke=n($);he=i(Ke,"\u{1F917} Optimum is distributed as a collection of packages - check out the links below for an in-depth look at each one."),Ke.forEach(t),W=c(a),I=s(a,"DIV",{class:!0});var Qe=n(I);h=s(Qe,"DIV",{class:!0});var f=n(h);w=s(f,"A",{class:!0,href:!0});var ee=n(w);G=s(ee,"DIV",{class:!0});var We=n(G);me=i(We,"Optimum Graphcore"),We.forEach(t),pe=c(ee),k=s(ee,"P",{class:!0});var te=n(k);fe=i(te,"Train transformers on "),V=s(te,"A",{href:!0});var Ye=n(V);ue=i(Ye,"Graphcore IPUs"),Ye.forEach(t),ve=i(te,", a completely new kind of massively parallel processor to accelerate machine intelligence."),te.forEach(t),ee.forEach(t),ge=c(f),y=s(f,"A",{class:!0,href:!0});var ae=n(y);X=s(ae,"DIV",{class:!0});var Ze=n(X);we=i(Ze,"Optimum Habana"),Ze.forEach(t),ye=c(ae),O=s(ae,"P",{class:!0});var re=n(O);xe=i(re,"Maximize training throughput and efficiency with "),q=s(re,"A",{href:!0});var et=n(q);_e=i(et,"Habana's Gaudi processor"),et.forEach(t),be=i(re,"."),re.forEach(t),ae.forEach(t),Ee=c(f),x=s(f,"A",{class:!0,href:!0});var oe=n(x);D=s(oe,"DIV",{class:!0});var tt=n(D);ke=i(tt,"Optimum Intel"),tt.forEach(t),Oe=c(oe),p=s(oe,"P",{class:!0});var F=n(p);Ae=i(F,"Use Intel's "),C=s(F,"A",{href:!0});var at=n(C);Ie=i(at,"Neural Compressor"),at.forEach(t),Pe=i(F," and "),R=s(F,"A",{href:!0});var rt=n(R);Ne=i(rt,"OpenVINO"),rt.forEach(t),Te=i(F," frameworks to accelerate transformer inference."),F.forEach(t),oe.forEach(t),ze=c(f),_=s(f,"A",{class:!0,href:!0});var se=n(_);S=s(se,"DIV",{class:!0});var ot=n(S);$e=i(ot,"ONNX Runtime"),ot.forEach(t),Ge=c(se),P=s(se,"P",{class:!0});var He=n(P);Ve=i(He,"Apply quantization and graph optimization to accelerate transformer training and inference with "),H=s(He,"A",{href:!0});var st=n(H);Xe=i(st,"ONNX Runtime"),st.forEach(t),He.forEach(t),se.forEach(t),qe=c(f),b=s(f,"A",{class:!0,href:!0});var ne=n(b);M=s(ne,"DIV",{class:!0});var nt=n(M);De=i(nt,"Torch FX"),nt.forEach(t),Ce=c(ne),N=s(ne,"P",{class:!0});var Me=n(N);Re=i(Me,"Create and compose custom graph transformations to optimize PyTorch transformer models with "),U=s(Me,"A",{href:!0});var lt=n(U);Se=i(lt,"Torch FX"),lt.forEach(t),Me.forEach(t),ne.forEach(t),f.forEach(t),Qe.forEach(t),this.h()},h(){r(v,"name","hf:doc:metadata"),r(v,"content",JSON.stringify(bt)),r(E,"id","optimum"),r(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(E,"href","#optimum"),r(g,"class","relative group"),r(G,"class","w-full text-center bg-gradient-to-br from-blue-400 to-blue-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"),r(V,"href","https://www.graphcore.ai/products/ipu"),r(k,"class","text-gray-700"),r(w,"class","!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg"),r(w,"href","./graphcore/index"),r(X,"class","w-full text-center bg-gradient-to-br from-indigo-400 to-indigo-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"),r(q,"href","https://docs.habana.ai/en/latest/Gaudi_Overview/Gaudi_Architecture.html"),r(O,"class","text-gray-700"),r(y,"class","!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg"),r(y,"href","./habana/index"),r(D,"class","w-full text-center bg-gradient-to-br from-pink-400 to-pink-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"),r(C,"href","https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html"),r(R,"href","https://docs.openvino.ai/latest/index.html"),r(p,"class","text-gray-700"),r(x,"class","!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg"),r(x,"href","./intel/index"),r(S,"class","w-full text-center bg-gradient-to-br from-yellow-400 to-yellow-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"),r(H,"href","https://onnxruntime.ai/"),r(P,"class","text-gray-700"),r(_,"class","!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg"),r(_,"href","./onnxruntime/overview"),r(M,"class","w-full text-center bg-gradient-to-br from-green-400 to-green-500 rounded-lg py-1.5 font-semibold mb-5 text-white text-lg leading-relaxed"),r(U,"href","https://pytorch.org/docs/stable/fx.html#"),r(N,"class","text-gray-700"),r(b,"class","!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg"),r(b,"href","./fx/overview"),r(h,"class","w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-3 md:gap-y-4 md:gap-x-5"),r(I,"class","mt-10")},m(a,m){e(document.head,v),u(a,J,m),u(a,g,m),e(g,E),e(E,L),ft(A,L,null),e(g,le),e(g,B),e(B,ie),u(a,j,m),u(a,T,m),e(T,de),u(a,K,m),u(a,z,m),e(z,ce),u(a,Q,m),u(a,$,m),e($,he),u(a,W,m),u(a,I,m),e(I,h),e(h,w),e(w,G),e(G,me),e(w,pe),e(w,k),e(k,fe),e(k,V),e(V,ue),e(k,ve),e(h,ge),e(h,y),e(y,X),e(X,we),e(y,ye),e(y,O),e(O,xe),e(O,q),e(q,_e),e(O,be),e(h,Ee),e(h,x),e(x,D),e(D,ke),e(x,Oe),e(x,p),e(p,Ae),e(p,C),e(C,Ie),e(p,Pe),e(p,R),e(R,Ne),e(p,Te),e(h,ze),e(h,_),e(_,S),e(S,$e),e(_,Ge),e(_,P),e(P,Ve),e(P,H),e(H,Xe),e(h,qe),e(h,b),e(b,M),e(M,De),e(b,Ce),e(b,N),e(N,Re),e(N,U),e(U,Se),Y=!0},p:ut,i(a){Y||(vt(A.$$.fragment,a),Y=!0)},o(a){gt(A.$$.fragment,a),Y=!1},d(a){t(v),a&&t(J),a&&t(g),wt(A),a&&t(j),a&&t(T),a&&t(K),a&&t(z),a&&t(Q),a&&t($),a&&t(W),a&&t(I)}}}const bt={local:"optimum",title:"\u{1F917} Optimum"};function Et(Ue){return yt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class At extends it{constructor(v){super();dt(this,v,Et,_t,ct,{})}}export{At as default,bt as metadata};
