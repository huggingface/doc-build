import{S as V,i as Z,s as ee,e as p,k as P,w as D,t as m,M as te,c as h,d as t,m as R,a as f,x as G,h as c,b as w,G as o,g as N,y as J,q as j,o as K,B as Q,v as ae}from"../../../chunks/vendor-hf-doc-builder.js";import{T as oe}from"../../../chunks/Tip-hf-doc-builder.js";import{I as ne}from"../../../chunks/IconCopyLink-hf-doc-builder.js";function re(F){let a,_,n,d,v,i,$,y,u,O;return{c(){a=p("p"),_=m("You can use "),n=p("a"),d=m("Netron"),v=m(" to visualize any ONNX file on the Hugging Face Hub. Simply append append the file\u2019s URL to "),i=p("code"),$=m("http://netron.app?url="),y=m(" as in "),u=p("a"),O=m("this example"),this.h()},l(s){a=h(s,"P",{});var l=f(a);_=c(l,"You can use "),n=h(l,"A",{href:!0,rel:!0});var x=f(n);d=c(x,"Netron"),x.forEach(t),v=c(l," to visualize any ONNX file on the Hugging Face Hub. Simply append append the file\u2019s URL to "),i=h(l,"CODE",{});var k=f(i);$=c(k,"http://netron.app?url="),k.forEach(t),y=c(l," as in "),u=h(l,"A",{href:!0,rel:!0});var E=f(u);O=c(E,"this example"),E.forEach(t),l.forEach(t),this.h()},h(){w(n,"href","https://netron.app/"),w(n,"rel","nofollow"),w(u,"href","https://netron.app/?url=https://huggingface.co/cmarkea/distilcamembert-base-ner/blob/main/model.onnx"),w(u,"rel","nofollow")},m(s,l){N(s,a,l),o(a,_),o(a,n),o(n,d),o(a,v),o(a,i),o(i,$),o(a,y),o(a,u),o(u,O)},d(s){s&&t(a)}}}function se(F){let a,_,n,d,v,i,$,y,u,O,s,l,x,k,E,S,g,z,T,L,A,b,M,q;return i=new ne({}),g=new oe({props:{$$slots:{default:[re]},$$scope:{ctx:F}}}),{c(){a=p("meta"),_=P(),n=p("h1"),d=p("a"),v=p("span"),D(i.$$.fragment),$=P(),y=p("span"),u=m("ONNX \u{1F91D} ONNX Runtime"),O=P(),s=p("p"),l=m("ONNX is an open standard that defines a common set of operators and a common file format to represent deep learning models in a wide variety of frameworks, including PyTorch and TensorFlow. When a model is exported to the ONNX format, these operators are used to construct a computational graph (often called an "),x=p("em"),k=m("intermediate representation"),E=m(") that represents the flow of data through the neural network."),S=P(),D(g.$$.fragment),z=P(),T=p("p"),L=m("By exposing a graph with standardized operators and data types, ONNX makes it easy to switch between frameworks. For example, a model trained in PyTorch can be exported to ONNX format and then imported in TensorFlow (and vice versa)."),A=P(),b=p("p"),M=m("Where ONNX really shines is when it is coupled with a dedicated accelerator like ONNX Runtime, or ORT for short. ORT provides tools to optimize the ONNX graph through techniques like operator fusion and constant folding, and defines an interface to execution providers that allow you to run the model on different types of hardware."),this.h()},l(e){const r=te('[data-svelte="svelte-1phssyn"]',document.head);a=h(r,"META",{name:!0,content:!0}),r.forEach(t),_=R(e),n=h(e,"H1",{class:!0});var X=f(n);d=h(X,"A",{id:!0,class:!0,href:!0});var W=f(d);v=h(W,"SPAN",{});var B=f(v);G(i.$$.fragment,B),B.forEach(t),W.forEach(t),$=R(X),y=h(X,"SPAN",{});var C=f(y);u=c(C,"ONNX \u{1F91D} ONNX Runtime"),C.forEach(t),X.forEach(t),O=R(e),s=h(e,"P",{});var H=f(s);l=c(H,"ONNX is an open standard that defines a common set of operators and a common file format to represent deep learning models in a wide variety of frameworks, including PyTorch and TensorFlow. When a model is exported to the ONNX format, these operators are used to construct a computational graph (often called an "),x=h(H,"EM",{});var U=f(x);k=c(U,"intermediate representation"),U.forEach(t),E=c(H,") that represents the flow of data through the neural network."),H.forEach(t),S=R(e),G(g.$$.fragment,e),z=R(e),T=h(e,"P",{});var I=f(T);L=c(I,"By exposing a graph with standardized operators and data types, ONNX makes it easy to switch between frameworks. For example, a model trained in PyTorch can be exported to ONNX format and then imported in TensorFlow (and vice versa)."),I.forEach(t),A=R(e),b=h(e,"P",{});var Y=f(b);M=c(Y,"Where ONNX really shines is when it is coupled with a dedicated accelerator like ONNX Runtime, or ORT for short. ORT provides tools to optimize the ONNX graph through techniques like operator fusion and constant folding, and defines an interface to execution providers that allow you to run the model on different types of hardware."),Y.forEach(t),this.h()},h(){w(a,"name","hf:doc:metadata"),w(a,"content",JSON.stringify(ie)),w(d,"id","onnx-onnx-runtime"),w(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(d,"href","#onnx-onnx-runtime"),w(n,"class","relative group")},m(e,r){o(document.head,a),N(e,_,r),N(e,n,r),o(n,d),o(d,v),J(i,v,null),o(n,$),o(n,y),o(y,u),N(e,O,r),N(e,s,r),o(s,l),o(s,x),o(x,k),o(s,E),N(e,S,r),J(g,e,r),N(e,z,r),N(e,T,r),o(T,L),N(e,A,r),N(e,b,r),o(b,M),q=!0},p(e,[r]){const X={};r&2&&(X.$$scope={dirty:r,ctx:e}),g.$set(X)},i(e){q||(j(i.$$.fragment,e),j(g.$$.fragment,e),q=!0)},o(e){K(i.$$.fragment,e),K(g.$$.fragment,e),q=!1},d(e){t(a),e&&t(_),e&&t(n),Q(i),e&&t(O),e&&t(s),e&&t(S),Q(g,e),e&&t(z),e&&t(T),e&&t(A),e&&t(b)}}}const ie={local:"onnx-onnx-runtime",title:"ONNX \u{1F91D} ONNX Runtime"};function le(F){return ae(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class fe extends V{constructor(a){super();Z(this,a,le,se,ee,{})}}export{fe as default,ie as metadata};
