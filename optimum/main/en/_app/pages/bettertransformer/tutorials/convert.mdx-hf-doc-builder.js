import{S as hr,i as vr,s as yr,e as s,k as m,w as u,t as n,M as $r,c as a,d as t,m as f,a as l,x as c,h as i,b as d,G as o,g as p,y as h,q as v,o as y,B as $,v as gr}from"../../../chunks/vendor-hf-doc-builder.js";import{T as wr}from"../../../chunks/Tip-hf-doc-builder.js";import{I as ve}from"../../../chunks/IconCopyLink-hf-doc-builder.js";import{C as I}from"../../../chunks/CodeBlock-hf-doc-builder.js";function _r(ze){let g;return{c(){g=n("Sometimes you can directly load your model on your GPU devices using `accelerate` library, therefore you can optionally try out the following command:")},l(b){g=i(b,"Sometimes you can directly load your model on your GPU devices using `accelerate` library, therefore you can optionally try out the following command:")},m(b,_){p(b,g,_)},d(b){b&&t(g)}}}function br(ze){let g,b,_,O,ye,R,Bt,P,St,$e,qt,It,ge,Ot,Ct,Fe,j,C,we,Q,Nt,_e,Mt,Ke,N,Dt,be,Ht,xt,Je,V,Re,w,Gt,W,Ut,Lt,Ee,Yt,zt,ke,Ft,Kt,Qe,T,M,Pe,X,Jt,je,Rt,Ve,D,Qt,Te,Vt,Wt,We,Z,Xe,H,Ze,ee,et,A,x,Ae,te,Xt,Be,Zt,tt,E,eo,Se,to,oo,qe,ro,so,ot,oe,rt,B,G,Ie,re,ao,Oe,lo,st,U,no,Ce,io,po,at,se,lt,k,mo,Ne,fo,uo,Me,co,ho,nt,ae,it,L,vo,De,yo,$o,pt,S,Y,He,le,go,xe,wo,mt,q,ne,_o,bo,Ge,Eo,ko,ft,ie,dt,de,Po,ut,pe,ct,z,jo,Ue,To,Ao,ht,me,vt,F,Bo,K,So,Le,qo,Io,yt;return R=new ve({}),Q=new ve({}),V=new I({props:{code:"pip install transformers accelerate optimum",highlighted:"pip install transformers accelerate optimum"}}),X=new ve({}),Z=new I({props:{code:`from transformers import AutoModel

model_id = "roberta-base"
model = AutoModel.from_pretrained(model_id)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model_id = <span class="hljs-string">&quot;roberta-base&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(model_id)`}}),H=new wr({props:{$$slots:{default:[_r]},$$scope:{ctx:ze}}}),ee=new I({props:{code:`from transformers import AutoModel

model_id = "roberta-base"
model = AutoModel.from_pretrained(model_id, device_map="auto")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model_id = <span class="hljs-string">&quot;roberta-base&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(model_id, device_map=<span class="hljs-string">&quot;auto&quot;</span>)`}}),te=new ve({}),oe=new I({props:{code:'model = model.to(0) # or model.to("cuda:0")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model = model.to(<span class="hljs-number">0</span>) <span class="hljs-comment"># or model.to(&quot;cuda:0&quot;)</span>'}}),re=new ve({}),se=new I({props:{code:`from optimum.bettertransformer import BetterTransformer

model = BetterTransformer.transform(model)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.bettertransformer <span class="hljs-keyword">import</span> BetterTransformer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = BetterTransformer.transform(model)`}}),ae=new I({props:{code:`from optimum.bettertransformer import BetterTransformer

model_bt = BetterTransformer.transform(model, keep_original_model=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.bettertransformer <span class="hljs-keyword">import</span> BetterTransformer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_bt = BetterTransformer.transform(model, keep_original_model=<span class="hljs-literal">True</span>)`}}),le=new ve({}),ie=new I({props:{code:`from optimum.pipelines import pipeline

pipe = pipeline("fill-mask", "distilbert-base-uncased", accelerator="bettertransformer")
pipe("I am a student at [MASK] University.")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.pipelines <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>pipe = pipeline(<span class="hljs-string">&quot;fill-mask&quot;</span>, <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, accelerator=<span class="hljs-string">&quot;bettertransformer&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pipe(<span class="hljs-string">&quot;I am a student at [MASK] University.&quot;</span>)`}}),pe=new I({props:{code:`from optimum.pipelines import pipeline

pipe = pipeline("fill-mask", "distilbert-base-uncased", accelerator="bettertransformer", device=0)
...`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.pipelines <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>pipe = pipeline(<span class="hljs-string">&quot;fill-mask&quot;</span>, <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, accelerator=<span class="hljs-string">&quot;bettertransformer&quot;</span>, device=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>...`}}),me=new I({props:{code:`from transformers import pipeline

pipe = pipeline("fill-mask", model=model_bt, tokenizer=tokenizer, device=0)
...`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>pipe = pipeline(<span class="hljs-string">&quot;fill-mask&quot;</span>, model=model_bt, tokenizer=tokenizer, device=<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>...`}}),{c(){g=s("meta"),b=m(),_=s("h1"),O=s("a"),ye=s("span"),u(R.$$.fragment),Bt=m(),P=s("span"),St=n("How to use "),$e=s("code"),qt=n("optimum"),It=n(" and "),ge=s("code"),Ot=n("BetterTransformer"),Ct=n("?"),Fe=m(),j=s("h2"),C=s("a"),we=s("span"),u(Q.$$.fragment),Nt=m(),_e=s("span"),Mt=n("Install dependencies"),Ke=m(),N=s("p"),Dt=n("You can easily use the "),be=s("code"),Ht=n("BetterTransformer"),xt=n(" integration with \u{1F917} Optimum, first install the dependencies as follows:"),Je=m(),u(V.$$.fragment),Re=m(),w=s("p"),Gt=n("Also, make sure to install the latest version of PyTorch by following the guidelines on the "),W=s("a"),Ut=n("PyTorch official website"),Lt=n(". Note that "),Ee=s("code"),Yt=n("BetterTransformer"),zt=n(" API is only compatible with "),ke=s("code"),Ft=n("torch>=1.13"),Kt=n(", so make sure to have this version installed on your environement before starting."),Qe=m(),T=s("h2"),M=s("a"),Pe=s("span"),u(X.$$.fragment),Jt=m(),je=s("span"),Rt=n("Step 1: Load your model"),Ve=m(),D=s("p"),Qt=n("First, load your Hugging Face model using \u{1F917} Transformers. Make sure to download one of the models that is supported by the "),Te=s("code"),Vt=n("BetterTransformer"),Wt=n(" API:"),We=m(),u(Z.$$.fragment),Xe=m(),u(H.$$.fragment),Ze=m(),u(ee.$$.fragment),et=m(),A=s("h2"),x=s("a"),Ae=s("span"),u(te.$$.fragment),Xt=m(),Be=s("span"),Zt=n("Step 2: Set your model on your preferred device"),tt=m(),E=s("p"),eo=n("If you did not used "),Se=s("code"),to=n('device_map="auto"'),oo=n(" to load your model (or if your model does not support "),qe=s("code"),ro=n('device_map="auto"'),so=n("), you can manually set your model to a GPU:"),ot=m(),u(oe.$$.fragment),rt=m(),B=s("h2"),G=s("a"),Ie=s("span"),u(re.$$.fragment),ao=m(),Oe=s("span"),lo=n("Step 3: Convert your model to BetterTransformer!"),st=m(),U=s("p"),no=n("Now time to convert your model using "),Ce=s("code"),io=n("BetterTransformer"),po=n(" API! You can run the commands below:"),at=m(),u(se.$$.fragment),lt=m(),k=s("p"),mo=n("By default, "),Ne=s("code"),fo=n("BetterTransformer.transform"),uo=n(" will overwrite your model, which means that your previous native model cannot be used anymore. If you want to keep it for some reasons, just add the flag "),Me=s("code"),co=n("keep_original_model=True"),ho=n("!"),nt=m(),u(ae.$$.fragment),it=m(),L=s("p"),vo=n("If your model does not support "),De=s("code"),yo=n("BetterTransformer"),$o=n(" API, this will displayed on an error trace. Note also that decoder-based models (OPT, BLOOM, etc.) are not supported yet but this is in the roadmap of PyTorch for the future."),pt=m(),S=s("h2"),Y=s("a"),He=s("span"),u(le.$$.fragment),go=m(),xe=s("span"),wo=n("Pipeline compatibility"),mt=m(),q=s("p"),ne=s("a"),_o=n("Transformer\u2019s pipeline"),bo=n(" is also compatible with this integration and you can use "),Ge=s("code"),Eo=n("BetterTransformer"),ko=n(" as an accelerator for your pipelines. The code snippet below shows how:"),ft=m(),u(ie.$$.fragment),dt=m(),de=s("p"),Po=n("If you want to run a pipeline on a GPU device, run:"),ut=m(),u(pe.$$.fragment),ct=m(),z=s("p"),jo=n("You can also use "),Ue=s("code"),To=n("transformers.pipeline"),Ao=n(" as usual and pass the convertede model directly:"),ht=m(),u(me.$$.fragment),vt=m(),F=s("p"),Bo=n("Please refer to the "),K=s("a"),So=n("official documentation of "),Le=s("code"),qo=n("pipeline"),Io=n(" for further usage. If you face into any issue, do not hesitate to open an isse on GitHub!"),this.h()},l(e){const r=$r('[data-svelte="svelte-1phssyn"]',document.head);g=a(r,"META",{name:!0,content:!0}),r.forEach(t),b=f(e),_=a(e,"H1",{class:!0});var fe=l(_);O=a(fe,"A",{id:!0,class:!0,href:!0});var Co=l(O);ye=a(Co,"SPAN",{});var No=l(ye);c(R.$$.fragment,No),No.forEach(t),Co.forEach(t),Bt=f(fe),P=a(fe,"SPAN",{});var ue=l(P);St=i(ue,"How to use "),$e=a(ue,"CODE",{});var Mo=l($e);qt=i(Mo,"optimum"),Mo.forEach(t),It=i(ue," and "),ge=a(ue,"CODE",{});var Do=l(ge);Ot=i(Do,"BetterTransformer"),Do.forEach(t),Ct=i(ue,"?"),ue.forEach(t),fe.forEach(t),Fe=f(e),j=a(e,"H2",{class:!0});var $t=l(j);C=a($t,"A",{id:!0,class:!0,href:!0});var Ho=l(C);we=a(Ho,"SPAN",{});var xo=l(we);c(Q.$$.fragment,xo),xo.forEach(t),Ho.forEach(t),Nt=f($t),_e=a($t,"SPAN",{});var Go=l(_e);Mt=i(Go,"Install dependencies"),Go.forEach(t),$t.forEach(t),Ke=f(e),N=a(e,"P",{});var gt=l(N);Dt=i(gt,"You can easily use the "),be=a(gt,"CODE",{});var Uo=l(be);Ht=i(Uo,"BetterTransformer"),Uo.forEach(t),xt=i(gt," integration with \u{1F917} Optimum, first install the dependencies as follows:"),gt.forEach(t),Je=f(e),c(V.$$.fragment,e),Re=f(e),w=a(e,"P",{});var J=l(w);Gt=i(J,"Also, make sure to install the latest version of PyTorch by following the guidelines on the "),W=a(J,"A",{href:!0,rel:!0});var Lo=l(W);Ut=i(Lo,"PyTorch official website"),Lo.forEach(t),Lt=i(J,". Note that "),Ee=a(J,"CODE",{});var Yo=l(Ee);Yt=i(Yo,"BetterTransformer"),Yo.forEach(t),zt=i(J," API is only compatible with "),ke=a(J,"CODE",{});var zo=l(ke);Ft=i(zo,"torch>=1.13"),zo.forEach(t),Kt=i(J,", so make sure to have this version installed on your environement before starting."),J.forEach(t),Qe=f(e),T=a(e,"H2",{class:!0});var wt=l(T);M=a(wt,"A",{id:!0,class:!0,href:!0});var Fo=l(M);Pe=a(Fo,"SPAN",{});var Ko=l(Pe);c(X.$$.fragment,Ko),Ko.forEach(t),Fo.forEach(t),Jt=f(wt),je=a(wt,"SPAN",{});var Jo=l(je);Rt=i(Jo,"Step 1: Load your model"),Jo.forEach(t),wt.forEach(t),Ve=f(e),D=a(e,"P",{});var _t=l(D);Qt=i(_t,"First, load your Hugging Face model using \u{1F917} Transformers. Make sure to download one of the models that is supported by the "),Te=a(_t,"CODE",{});var Ro=l(Te);Vt=i(Ro,"BetterTransformer"),Ro.forEach(t),Wt=i(_t," API:"),_t.forEach(t),We=f(e),c(Z.$$.fragment,e),Xe=f(e),c(H.$$.fragment,e),Ze=f(e),c(ee.$$.fragment,e),et=f(e),A=a(e,"H2",{class:!0});var bt=l(A);x=a(bt,"A",{id:!0,class:!0,href:!0});var Qo=l(x);Ae=a(Qo,"SPAN",{});var Vo=l(Ae);c(te.$$.fragment,Vo),Vo.forEach(t),Qo.forEach(t),Xt=f(bt),Be=a(bt,"SPAN",{});var Wo=l(Be);Zt=i(Wo,"Step 2: Set your model on your preferred device"),Wo.forEach(t),bt.forEach(t),tt=f(e),E=a(e,"P",{});var ce=l(E);eo=i(ce,"If you did not used "),Se=a(ce,"CODE",{});var Xo=l(Se);to=i(Xo,'device_map="auto"'),Xo.forEach(t),oo=i(ce," to load your model (or if your model does not support "),qe=a(ce,"CODE",{});var Zo=l(qe);ro=i(Zo,'device_map="auto"'),Zo.forEach(t),so=i(ce,"), you can manually set your model to a GPU:"),ce.forEach(t),ot=f(e),c(oe.$$.fragment,e),rt=f(e),B=a(e,"H2",{class:!0});var Et=l(B);G=a(Et,"A",{id:!0,class:!0,href:!0});var er=l(G);Ie=a(er,"SPAN",{});var tr=l(Ie);c(re.$$.fragment,tr),tr.forEach(t),er.forEach(t),ao=f(Et),Oe=a(Et,"SPAN",{});var or=l(Oe);lo=i(or,"Step 3: Convert your model to BetterTransformer!"),or.forEach(t),Et.forEach(t),st=f(e),U=a(e,"P",{});var kt=l(U);no=i(kt,"Now time to convert your model using "),Ce=a(kt,"CODE",{});var rr=l(Ce);io=i(rr,"BetterTransformer"),rr.forEach(t),po=i(kt," API! You can run the commands below:"),kt.forEach(t),at=f(e),c(se.$$.fragment,e),lt=f(e),k=a(e,"P",{});var he=l(k);mo=i(he,"By default, "),Ne=a(he,"CODE",{});var sr=l(Ne);fo=i(sr,"BetterTransformer.transform"),sr.forEach(t),uo=i(he," will overwrite your model, which means that your previous native model cannot be used anymore. If you want to keep it for some reasons, just add the flag "),Me=a(he,"CODE",{});var ar=l(Me);co=i(ar,"keep_original_model=True"),ar.forEach(t),ho=i(he,"!"),he.forEach(t),nt=f(e),c(ae.$$.fragment,e),it=f(e),L=a(e,"P",{});var Pt=l(L);vo=i(Pt,"If your model does not support "),De=a(Pt,"CODE",{});var lr=l(De);yo=i(lr,"BetterTransformer"),lr.forEach(t),$o=i(Pt," API, this will displayed on an error trace. Note also that decoder-based models (OPT, BLOOM, etc.) are not supported yet but this is in the roadmap of PyTorch for the future."),Pt.forEach(t),pt=f(e),S=a(e,"H2",{class:!0});var jt=l(S);Y=a(jt,"A",{id:!0,class:!0,href:!0});var nr=l(Y);He=a(nr,"SPAN",{});var ir=l(He);c(le.$$.fragment,ir),ir.forEach(t),nr.forEach(t),go=f(jt),xe=a(jt,"SPAN",{});var pr=l(xe);wo=i(pr,"Pipeline compatibility"),pr.forEach(t),jt.forEach(t),mt=f(e),q=a(e,"P",{});var Ye=l(q);ne=a(Ye,"A",{href:!0,rel:!0});var mr=l(ne);_o=i(mr,"Transformer\u2019s pipeline"),mr.forEach(t),bo=i(Ye," is also compatible with this integration and you can use "),Ge=a(Ye,"CODE",{});var fr=l(Ge);Eo=i(fr,"BetterTransformer"),fr.forEach(t),ko=i(Ye," as an accelerator for your pipelines. The code snippet below shows how:"),Ye.forEach(t),ft=f(e),c(ie.$$.fragment,e),dt=f(e),de=a(e,"P",{});var dr=l(de);Po=i(dr,"If you want to run a pipeline on a GPU device, run:"),dr.forEach(t),ut=f(e),c(pe.$$.fragment,e),ct=f(e),z=a(e,"P",{});var Tt=l(z);jo=i(Tt,"You can also use "),Ue=a(Tt,"CODE",{});var ur=l(Ue);To=i(ur,"transformers.pipeline"),ur.forEach(t),Ao=i(Tt," as usual and pass the convertede model directly:"),Tt.forEach(t),ht=f(e),c(me.$$.fragment,e),vt=f(e),F=a(e,"P",{});var At=l(F);Bo=i(At,"Please refer to the "),K=a(At,"A",{href:!0,rel:!0});var Oo=l(K);So=i(Oo,"official documentation of "),Le=a(Oo,"CODE",{});var cr=l(Le);qo=i(cr,"pipeline"),cr.forEach(t),Oo.forEach(t),Io=i(At," for further usage. If you face into any issue, do not hesitate to open an isse on GitHub!"),At.forEach(t),this.h()},h(){d(g,"name","hf:doc:metadata"),d(g,"content",JSON.stringify(Er)),d(O,"id","how-to-use-optimum-and-bettertransformer"),d(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(O,"href","#how-to-use-optimum-and-bettertransformer"),d(_,"class","relative group"),d(C,"id","install-dependencies"),d(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(C,"href","#install-dependencies"),d(j,"class","relative group"),d(W,"href","https://pytorch.org/get-started/locally/"),d(W,"rel","nofollow"),d(M,"id","step-1-load-your-model"),d(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(M,"href","#step-1-load-your-model"),d(T,"class","relative group"),d(x,"id","step-2-set-your-model-on-your-preferred-device"),d(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(x,"href","#step-2-set-your-model-on-your-preferred-device"),d(A,"class","relative group"),d(G,"id","step-3-convert-your-model-to-bettertransformer"),d(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(G,"href","#step-3-convert-your-model-to-bettertransformer"),d(B,"class","relative group"),d(Y,"id","pipeline-compatibility"),d(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Y,"href","#pipeline-compatibility"),d(S,"class","relative group"),d(ne,"href","https://huggingface.co/docs/transformers/main_classes/pipelines"),d(ne,"rel","nofollow"),d(K,"href","https://huggingface.co/docs/transformers/main_classes/pipelines"),d(K,"rel","nofollow")},m(e,r){o(document.head,g),p(e,b,r),p(e,_,r),o(_,O),o(O,ye),h(R,ye,null),o(_,Bt),o(_,P),o(P,St),o(P,$e),o($e,qt),o(P,It),o(P,ge),o(ge,Ot),o(P,Ct),p(e,Fe,r),p(e,j,r),o(j,C),o(C,we),h(Q,we,null),o(j,Nt),o(j,_e),o(_e,Mt),p(e,Ke,r),p(e,N,r),o(N,Dt),o(N,be),o(be,Ht),o(N,xt),p(e,Je,r),h(V,e,r),p(e,Re,r),p(e,w,r),o(w,Gt),o(w,W),o(W,Ut),o(w,Lt),o(w,Ee),o(Ee,Yt),o(w,zt),o(w,ke),o(ke,Ft),o(w,Kt),p(e,Qe,r),p(e,T,r),o(T,M),o(M,Pe),h(X,Pe,null),o(T,Jt),o(T,je),o(je,Rt),p(e,Ve,r),p(e,D,r),o(D,Qt),o(D,Te),o(Te,Vt),o(D,Wt),p(e,We,r),h(Z,e,r),p(e,Xe,r),h(H,e,r),p(e,Ze,r),h(ee,e,r),p(e,et,r),p(e,A,r),o(A,x),o(x,Ae),h(te,Ae,null),o(A,Xt),o(A,Be),o(Be,Zt),p(e,tt,r),p(e,E,r),o(E,eo),o(E,Se),o(Se,to),o(E,oo),o(E,qe),o(qe,ro),o(E,so),p(e,ot,r),h(oe,e,r),p(e,rt,r),p(e,B,r),o(B,G),o(G,Ie),h(re,Ie,null),o(B,ao),o(B,Oe),o(Oe,lo),p(e,st,r),p(e,U,r),o(U,no),o(U,Ce),o(Ce,io),o(U,po),p(e,at,r),h(se,e,r),p(e,lt,r),p(e,k,r),o(k,mo),o(k,Ne),o(Ne,fo),o(k,uo),o(k,Me),o(Me,co),o(k,ho),p(e,nt,r),h(ae,e,r),p(e,it,r),p(e,L,r),o(L,vo),o(L,De),o(De,yo),o(L,$o),p(e,pt,r),p(e,S,r),o(S,Y),o(Y,He),h(le,He,null),o(S,go),o(S,xe),o(xe,wo),p(e,mt,r),p(e,q,r),o(q,ne),o(ne,_o),o(q,bo),o(q,Ge),o(Ge,Eo),o(q,ko),p(e,ft,r),h(ie,e,r),p(e,dt,r),p(e,de,r),o(de,Po),p(e,ut,r),h(pe,e,r),p(e,ct,r),p(e,z,r),o(z,jo),o(z,Ue),o(Ue,To),o(z,Ao),p(e,ht,r),h(me,e,r),p(e,vt,r),p(e,F,r),o(F,Bo),o(F,K),o(K,So),o(K,Le),o(Le,qo),o(F,Io),yt=!0},p(e,[r]){const fe={};r&2&&(fe.$$scope={dirty:r,ctx:e}),H.$set(fe)},i(e){yt||(v(R.$$.fragment,e),v(Q.$$.fragment,e),v(V.$$.fragment,e),v(X.$$.fragment,e),v(Z.$$.fragment,e),v(H.$$.fragment,e),v(ee.$$.fragment,e),v(te.$$.fragment,e),v(oe.$$.fragment,e),v(re.$$.fragment,e),v(se.$$.fragment,e),v(ae.$$.fragment,e),v(le.$$.fragment,e),v(ie.$$.fragment,e),v(pe.$$.fragment,e),v(me.$$.fragment,e),yt=!0)},o(e){y(R.$$.fragment,e),y(Q.$$.fragment,e),y(V.$$.fragment,e),y(X.$$.fragment,e),y(Z.$$.fragment,e),y(H.$$.fragment,e),y(ee.$$.fragment,e),y(te.$$.fragment,e),y(oe.$$.fragment,e),y(re.$$.fragment,e),y(se.$$.fragment,e),y(ae.$$.fragment,e),y(le.$$.fragment,e),y(ie.$$.fragment,e),y(pe.$$.fragment,e),y(me.$$.fragment,e),yt=!1},d(e){t(g),e&&t(b),e&&t(_),$(R),e&&t(Fe),e&&t(j),$(Q),e&&t(Ke),e&&t(N),e&&t(Je),$(V,e),e&&t(Re),e&&t(w),e&&t(Qe),e&&t(T),$(X),e&&t(Ve),e&&t(D),e&&t(We),$(Z,e),e&&t(Xe),$(H,e),e&&t(Ze),$(ee,e),e&&t(et),e&&t(A),$(te),e&&t(tt),e&&t(E),e&&t(ot),$(oe,e),e&&t(rt),e&&t(B),$(re),e&&t(st),e&&t(U),e&&t(at),$(se,e),e&&t(lt),e&&t(k),e&&t(nt),$(ae,e),e&&t(it),e&&t(L),e&&t(pt),e&&t(S),$(le),e&&t(mt),e&&t(q),e&&t(ft),$(ie,e),e&&t(dt),e&&t(de),e&&t(ut),$(pe,e),e&&t(ct),e&&t(z),e&&t(ht),$(me,e),e&&t(vt),e&&t(F)}}}const Er={local:"how-to-use-optimum-and-bettertransformer",sections:[{local:"install-dependencies",title:"Install dependencies"},{local:"step-1-load-your-model",title:"Step 1: Load your model"},{local:"step-2-set-your-model-on-your-preferred-device",title:"Step 2: Set your model on your preferred device"},{local:"step-3-convert-your-model-to-bettertransformer",title:"Step 3: Convert your model to BetterTransformer!"},{local:"pipeline-compatibility",title:"Pipeline compatibility"}],title:"How to use `optimum` and `BetterTransformer`?"};function kr(ze){return gr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Br extends hr{constructor(g){super();vr(this,g,kr,br,yr,{})}}export{Br as default,Er as metadata};
