import{S as wn,i as En,s as kn,e as a,k as h,w as E,t as n,M as jn,c as r,d as s,m as d,a as o,x as k,h as l,b as p,G as e,g as c,y as j,L as gn,q as g,o as $,B as A,v as $n}from"../../../chunks/vendor-hf-doc-builder.js";import{I as Ae}from"../../../chunks/IconCopyLink-hf-doc-builder.js";import{C as qt}from"../../../chunks/CodeBlock-hf-doc-builder.js";function An(Yr){let B,Ht,N,q,Te,W,As,Ce,Ts,Ft,H,Cs,Le,Ls,Os,Yt,I,F,Oe,X,Ds,De,Bs,Ut,T,Ns,Z,Is,Ps,Be,xs,Ss,Gt,_,ee,Ms,te,Rs,qs,Hs,P,Fs,Ne,Ys,Us,Ie,Gs,zs,Js,Pe,Ks,Qs,se,Vs,xe,Ws,Xs,Zs,be,Se,ea,ta,zt,x,Y,Me,ae,sa,re,aa,Re,ra,oa,Jt,S,U,qe,oe,na,He,la,Kt,b,ia,Fe,ha,da,Ye,ca,pa,Ue,fa,ua,Ge,ma,_a,Qt,ne,Vt,G,ba,ze,ya,va,Wt,M,z,Je,le,wa,ie,Ea,Ke,ka,ja,Xt,v,ga,he,Qe,$a,Aa,Ve,Ta,Ca,We,La,Oa,Zt,de,es,ye,Da,ts,f,Xe,Ze,Ba,Na,et,tt,Ia,Pa,st,at,xa,Sa,rt,ot,Ma,Ra,nt,lt,qa,Ha,it,ht,Fa,Ya,dt,ct,Ua,Ga,pt,ft,za,Ja,ut,mt,Ka,Qa,_t,bt,Va,Wa,yt,vt,Xa,Za,wt,Et,er,tr,kt,jt,sr,ar,gt,$t,rr,or,m,At,nr,lr,ce,ir,hr,Tt,dr,cr,Ct,pr,fr,Lt,ur,mr,pe,Ot,_r,br,ss,ve,yr,as,fe,rs,C,vr,Dt,wr,Er,Bt,kr,jr,os,ue,ns,we,gr,ls,me,is,L,$r,Nt,Ar,Tr,It,Cr,Lr,hs,J,Or,Pt,Dr,Br,ds,R,K,xt,_e,Nr,St,Ir,cs,w,Pr,Mt,xr,Sr,Rt,Mr,Rr,Ee,qr,Hr,ps;return W=new Ae({}),X=new Ae({}),ae=new Ae({}),oe=new Ae({}),ne=new qt({props:{code:`from transformers import AutoModel

model = AutoModel.from_pretrained("bert-base-uncased")
print(model)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model)
...
          (LayerNorm): LayerNorm((<span class="hljs-number">768</span>,), eps=<span class="hljs-number">1e-12</span>, elementwise_affine=<span class="hljs-literal">True</span>)
          (dropout): Dropout(p=<span class="hljs-number">0.1</span>, inplace=<span class="hljs-literal">False</span>)
        )
      )
      (<span class="hljs-number">11</span>): BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=<span class="hljs-number">768</span>, out_features=<span class="hljs-number">768</span>, bias=<span class="hljs-literal">True</span>)
            (key): Linear(in_features=<span class="hljs-number">768</span>, out_features=<span class="hljs-number">768</span>, bias=<span class="hljs-literal">True</span>)
            (value): Linear(in_features=<span class="hljs-number">768</span>, out_features=<span class="hljs-number">768</span>, bias=<span class="hljs-literal">True</span>)
            (dropout): Dropout(p=<span class="hljs-number">0.1</span>, inplace=<span class="hljs-literal">False</span>)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=<span class="hljs-number">768</span>, out_features=<span class="hljs-number">768</span>, bias=<span class="hljs-literal">True</span>)
            (LayerNorm): LayerNorm((<span class="hljs-number">768</span>,), eps=<span class="hljs-number">1e-12</span>, elementwise_affine=<span class="hljs-literal">True</span>)
            (dropout): Dropout(p=<span class="hljs-number">0.1</span>, inplace=<span class="hljs-literal">False</span>)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=<span class="hljs-number">768</span>, out_features=<span class="hljs-number">3072</span>, bias=<span class="hljs-literal">True</span>)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=<span class="hljs-number">3072</span>, out_features=<span class="hljs-number">768</span>, bias=<span class="hljs-literal">True</span>)
          (LayerNorm): LayerNorm((<span class="hljs-number">768</span>,), eps=<span class="hljs-number">1e-12</span>, elementwise_affine=<span class="hljs-literal">True</span>)
          (dropout): Dropout(p=<span class="hljs-number">0.1</span>, inplace=<span class="hljs-literal">False</span>)
        )
      )
    )
  )
  (pooler): BertPooler(
    (dense): Linear(in_features=<span class="hljs-number">768</span>, out_features=<span class="hljs-number">768</span>, bias=<span class="hljs-literal">True</span>)
    (activation): Tanh()
  )
)`}}),le=new Ae({}),de=new qt({props:{code:`import torch
import torch.nn as nn

from ..base import BetterTransformerBaseLayer


class BertLayerBetterTransformer(BetterTransformerBaseLayer):
    def __init__(self, bert_layer, config):
...`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn

<span class="hljs-keyword">from</span> ..base <span class="hljs-keyword">import</span> BetterTransformerBaseLayer


<span class="hljs-keyword">class</span> <span class="hljs-title class_">BertLayerBetterTransformer</span>(<span class="hljs-title class_ inherited__">BetterTransformerBaseLayer</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, bert_layer, config</span>):
...`}}),fe=new qt({props:{code:`self.is_last_layer = False
self.validate_bettertransformer()

### Step 3: Building the forward pass

First of all, start with the line \`super().forward_checker()\`, this is needed so that the parent class can run all the safety checkers before.

After the first forward pass, the hidden states needs to be *nested* using the attention mask. Once they are nested, the attention mask is not needed anymore, therefore can be set to \`None\`. This is how the forward pass is built for \`Bert\`, these lines should remain pretty much similar accross models, but sometimes the shapes of the attention masks are different across models. 
\`\`\`python
super().forward_checker()

if hidden_states.is_nested:
    attention_mask = None

if attention_mask is not None:
    # attention mask comes in with values 0 and -inf. we convert to torch.nn.TransformerEncoder style bool mask
    # 0->false->keep this token -inf->true->mask this token
    attention_mask = attention_mask.bool()
    attention_mask = torch.reshape(attention_mask, (attention_mask.shape[0], attention_mask.shape[-1]))
    seqlen = attention_mask.shape[1]
    lengths = torch.sum(~attention_mask, 1)
    if not all([l == seqlen for l in lengths]):
        hidden_states = torch._nested_tensor_from_mask(hidden_states, ~attention_mask)
    attention_mask = None`,highlighted:`self.is_last_layer = <span class="hljs-literal">False</span>
self.validate_bettertransformer()

<span class="hljs-comment">### Step 3: Building the forward pass</span>

First of <span class="hljs-built_in">all</span>, start <span class="hljs-keyword">with</span> the line \`<span class="hljs-built_in">super</span>().forward_checker()\`, this <span class="hljs-keyword">is</span> needed so that the parent <span class="hljs-keyword">class</span> <span class="hljs-title class_">can</span> run <span class="hljs-built_in">all</span> the safety checkers before.

After the first forward <span class="hljs-keyword">pass</span>, the hidden states needs to be *nested* using the attention mask. Once they are nested, the attention mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> needed anymore, therefore can be <span class="hljs-built_in">set</span> to \`<span class="hljs-literal">None</span>\`. This <span class="hljs-keyword">is</span> how the forward <span class="hljs-keyword">pass</span> <span class="hljs-keyword">is</span> built <span class="hljs-keyword">for</span> \`Bert\`, these lines should remain pretty much similar accross models, but sometimes the shapes of the attention masks are different across models. 
\`\`\`python
<span class="hljs-built_in">super</span>().forward_checker()

<span class="hljs-keyword">if</span> hidden_states.is_nested:
    attention_mask = <span class="hljs-literal">None</span>

<span class="hljs-keyword">if</span> attention_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
    <span class="hljs-comment"># attention mask comes in with values 0 and -inf. we convert to torch.nn.TransformerEncoder style bool mask</span>
    <span class="hljs-comment"># 0-&gt;false-&gt;keep this token -inf-&gt;true-&gt;mask this token</span>
    attention_mask = attention_mask.<span class="hljs-built_in">bool</span>()
    attention_mask = torch.reshape(attention_mask, (attention_mask.shape[<span class="hljs-number">0</span>], attention_mask.shape[-<span class="hljs-number">1</span>]))
    seqlen = attention_mask.shape[<span class="hljs-number">1</span>]
    lengths = torch.<span class="hljs-built_in">sum</span>(~attention_mask, <span class="hljs-number">1</span>)
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">all</span>([l == seqlen <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> lengths]):
        hidden_states = torch._nested_tensor_from_mask(hidden_states, ~attention_mask)
    attention_mask = <span class="hljs-literal">None</span>`}}),ue=new qt({props:{code:`hidden_states = torch._transformer_encoder_layer_fwd(
    hidden_states,
    self.embed_dim,
    self.num_heads,
    self.in_proj_weight,
    self.in_proj_bias,
    self.out_proj_weight,
    self.out_proj_bias,
    self.use_gelu,
    self.norm_first,
    self.norm1_eps,
    self.norm1_weight,
    self.norm1_bias,
    self.norm2_weight,
    self.norm2_bias,
    self.linear1_weight,
    self.linear1_bias,
    self.linear2_weight,
    self.linear2_bias,
    attention_mask,
)`,highlighted:`hidden_states = torch._transformer_encoder_layer_fwd(
    hidden_states,
    self.embed_dim,
    self.num_heads,
    self.in_proj_weight,
    self.in_proj_bias,
    self.out_proj_weight,
    self.out_proj_bias,
    self.use_gelu,
    self.norm_first,
    self.norm1_eps,
    self.norm1_weight,
    self.norm1_bias,
    self.norm2_weight,
    self.norm2_bias,
    self.linear1_weight,
    self.linear1_bias,
    self.linear2_weight,
    self.linear2_bias,
    attention_mask,
)`}}),me=new qt({props:{code:`if hidden_states.is_nested and self.is_last_layer:
    hidden_states = hidden_states.to_padded_tensor(0.0)
return (hidden_states,)`,highlighted:`<span class="hljs-keyword">if</span> hidden_states.is_nested <span class="hljs-keyword">and</span> self.is_last_layer:
    hidden_states = hidden_states.to_padded_tensor(<span class="hljs-number">0.0</span>)
<span class="hljs-keyword">return</span> (hidden_states,)`}}),_e=new Ae({}),{c(){B=a("meta"),Ht=h(),N=a("h1"),q=a("a"),Te=a("span"),E(W.$$.fragment),As=h(),Ce=a("span"),Ts=n("Adding BetterTransformer support for new architectures"),Ft=h(),H=a("p"),Cs=n("You want to add a new model for "),Le=a("code"),Ls=n("BetterTransformer"),Os=n(" API? Check this guideline!"),Yt=h(),I=a("h2"),F=a("a"),Oe=a("span"),E(X.$$.fragment),Ds=h(),De=a("span"),Bs=n("Models that should be supported"),Ut=h(),T=a("p"),Ns=n("In theory, any model that has a transformer encoder layer, similar to the classic encoder described in the "),Z=a("a"),Is=n("\u201CAttention Is All You Need\u201D"),Ps=n(` paper should be supported.
More specifically, a model that has an encoder block with a MultiHead-Attention module (with pre or post-attention layer norm) should be convertible to its `),Be=a("code"),xs=n("BetterTransformer"),Ss=n(" equivalent. The conditions can be summarized as follows:"),Gt=h(),_=a("ul"),ee=a("li"),Ms=n("Use classic Multi Head attention module (for example, "),te=a("a"),Rs=n("DeBERTa"),qs=n(" cannot be supported)"),Hs=h(),P=a("li"),Fs=n("Use either "),Ne=a("code"),Ys=n("gelu"),Us=n(" or "),Ie=a("code"),Gs=n("relu"),zs=n(" activation function"),Js=h(),Pe=a("li"),Ks=n("Have an even number of attention heads"),Qs=h(),se=a("li"),Vs=n("Do not use any attention bias (for eg "),xe=a("code"),Ws=n("T5"),Xs=n(" uses attention bias, therefore cannot be supported)"),Zs=h(),be=a("li"),Se=a("code"),ea=n("eps"),ta=n(" must be equal between the first and second layer norms for each layer"),zt=h(),x=a("h2"),Y=a("a"),Me=a("span"),E(ae.$$.fragment),sa=h(),re=a("span"),aa=n("How to convert a model into its "),Re=a("code"),ra=n("BetterTransformer"),oa=n(" format?"),Jt=h(),S=a("h3"),U=a("a"),qe=a("span"),E(oe.$$.fragment),na=h(),He=a("span"),la=n("Step 1: Identifying the source layer to change"),Kt=h(),b=a("p"),ia=n("First, go to "),Fe=a("code"),ha=n("optimum/bettertransformer/__init__.py"),da=n(" and you\u2019ll see the dictionary "),Ye=a("code"),ca=n("BETTER_TRANFORMER_LAYERS_MAPPING_DICT"),pa=n(". This should contain the mapping between the Module that can be converted to its "),Ue=a("code"),fa=n("BetterTransformer"),ua=n(` equivalent.
Let us try to do it step by step for `),Ge=a("code"),ma=n("Bert"),_a=n(", first we need to identify the layers that needs to be replaced:"),Qt=h(),E(ne.$$.fragment),Vt=h(),G=a("p"),ba=n("You can clearly see that the layers that needs to be replaced are the "),ze=a("code"),ya=n("BertLayer"),va=n(" modules since it contains the whole encoder layer module."),Wt=h(),M=a("h3"),z=a("a"),Je=a("span"),E(le.$$.fragment),wa=h(),ie=a("span"),Ea=n("Step 2: Building the "),Ke=a("code"),ka=n("xxxLayerBetterTransformer"),ja=n(" module"),Xt=h(),v=a("p"),ga=n("Check that the identified module is not already copied from another module (by inspecting the source code in "),he=a("a"),Qe=a("code"),$a=n("transformers"),Aa=n(" and checking that the class definition does not start with "),Ve=a("code"),Ta=n("# Copied from ..."),Ca=n(") - and if not, create a class in "),We=a("code"),La=n("bettertransformer/models/encoder_model.py"),Oa=n(`.
Start with those lines:`),Zt=h(),E(de.$$.fragment),es=h(),ye=a("p"),Da=n("Now, make sure to fill all the necessary attributes, the list of attributes are:"),ts=h(),f=a("ul"),Xe=a("li"),Ze=a("code"),Ba=n("in_proj_weight"),Na=h(),et=a("li"),tt=a("code"),Ia=n("in_proj_bias"),Pa=h(),st=a("li"),at=a("code"),xa=n("out_proj_weight"),Sa=h(),rt=a("li"),ot=a("code"),Ma=n("out_proj_bias"),Ra=h(),nt=a("li"),lt=a("code"),qa=n("linear1_weight"),Ha=h(),it=a("li"),ht=a("code"),Fa=n("linear1_bias"),Ya=h(),dt=a("li"),ct=a("code"),Ua=n("linear2_weight"),Ga=h(),pt=a("li"),ft=a("code"),za=n("linear2_bias"),Ja=h(),ut=a("li"),mt=a("code"),Ka=n("norm1_eps"),Qa=h(),_t=a("li"),bt=a("code"),Va=n("norm1_weight"),Wa=h(),yt=a("li"),vt=a("code"),Xa=n("norm1_bias"),Za=h(),wt=a("li"),Et=a("code"),er=n("norm2_weight"),tr=h(),kt=a("li"),jt=a("code"),sr=n("norm2_bias"),ar=h(),gt=a("li"),$t=a("code"),rr=n("num_heads"),or=h(),m=a("li"),At=a("code"),nr=n("embed_dim"),lr=n(`
Note that these attributes correspond to all the components that are necessary to run a Transformer Encoder module, check the figure 1 on the `),ce=a("a"),ir=n("\u201CAttention Is All You Need\u201D"),hr=n(` paper.
Once you filled all these attributes (sometimes the `),Tt=a("code"),dr=n("query"),cr=n(", "),Ct=a("code"),pr=n("key"),fr=n(" and "),Lt=a("code"),ur=n("value"),mr=n(" layers needs to be \u201Ccontigufied\u201D, check the "),pe=a("a"),Ot=a("code"),_r=n("modeling_encoder.py"),br=n(" file to understand more.)"),ss=h(),ve=a("p"),yr=n("Make sure also to add the lines:"),as=h(),E(fe.$$.fragment),rs=h(),C=a("p"),vr=n("Once the "),Dt=a("code"),wr=n("hidden_states"),Er=n(" are nested, call "),Bt=a("code"),kr=n("torch._transformer_encoder_layer_fwd"),jr=n(" using the right arguments as follows:"),os=h(),E(ue.$$.fragment),ns=h(),we=a("p"),gr=n("At the last layer, it is important to \u201Cun-nest\u201D the hidden_states so that it can be processed by the next modules, this is done in these lines:"),ls=h(),E(me.$$.fragment),is=h(),L=a("p"),$r=n("Also make sure to return a "),Nt=a("code"),Ar=n("tuple"),Tr=n(" to follow the convention of "),It=a("code"),Cr=n("transformers"),Lr=n("."),hs=h(),J=a("p"),Or=n("The best way to reproduce this experiment on your own model is to try it by get some inspiration from the provided modeling scripts. Of course, we will be happy to help you converting your model if you open an issue or a Pull Request on "),Pt=a("code"),Dr=n("optimum"),Br=n("!"),ds=h(),R=a("h3"),K=a("a"),xt=a("span"),E(_e.$$.fragment),Nr=h(),St=a("span"),Ir=n("Step 4: Sanity check!"),cs=h(),w=a("p"),Pr=n("As a last step, make sure to update the "),Mt=a("code"),xr=n("BETTER_TRANFORMER_LAYERS_MAPPING_DICT"),Sr=n(" dictionary in  "),Rt=a("code"),Mr=n("optimum/bettertransformer/__init__.py"),Rr=n(" with the correct names, and you should be ready to convert your model. Try it out with the conversion method that is presented in the "),Ee=a("a"),qr=n("tutorials sections"),Hr=n("!"),this.h()},l(t){const i=jn('[data-svelte="svelte-1phssyn"]',document.head);B=r(i,"META",{name:!0,content:!0}),i.forEach(s),Ht=d(t),N=r(t,"H1",{class:!0});var fs=o(N);q=r(fs,"A",{id:!0,class:!0,href:!0});var Ur=o(q);Te=r(Ur,"SPAN",{});var Gr=o(Te);k(W.$$.fragment,Gr),Gr.forEach(s),Ur.forEach(s),As=d(fs),Ce=r(fs,"SPAN",{});var zr=o(Ce);Ts=l(zr,"Adding BetterTransformer support for new architectures"),zr.forEach(s),fs.forEach(s),Ft=d(t),H=r(t,"P",{});var us=o(H);Cs=l(us,"You want to add a new model for "),Le=r(us,"CODE",{});var Jr=o(Le);Ls=l(Jr,"BetterTransformer"),Jr.forEach(s),Os=l(us," API? Check this guideline!"),us.forEach(s),Yt=d(t),I=r(t,"H2",{class:!0});var ms=o(I);F=r(ms,"A",{id:!0,class:!0,href:!0});var Kr=o(F);Oe=r(Kr,"SPAN",{});var Qr=o(Oe);k(X.$$.fragment,Qr),Qr.forEach(s),Kr.forEach(s),Ds=d(ms),De=r(ms,"SPAN",{});var Vr=o(De);Bs=l(Vr,"Models that should be supported"),Vr.forEach(s),ms.forEach(s),Ut=d(t),T=r(t,"P",{});var ke=o(T);Ns=l(ke,"In theory, any model that has a transformer encoder layer, similar to the classic encoder described in the "),Z=r(ke,"A",{href:!0,rel:!0});var Wr=o(Z);Is=l(Wr,"\u201CAttention Is All You Need\u201D"),Wr.forEach(s),Ps=l(ke,` paper should be supported.
More specifically, a model that has an encoder block with a MultiHead-Attention module (with pre or post-attention layer norm) should be convertible to its `),Be=r(ke,"CODE",{});var Xr=o(Be);xs=l(Xr,"BetterTransformer"),Xr.forEach(s),Ss=l(ke," equivalent. The conditions can be summarized as follows:"),ke.forEach(s),Gt=d(t),_=r(t,"UL",{});var O=o(_);ee=r(O,"LI",{});var _s=o(ee);Ms=l(_s,"Use classic Multi Head attention module (for example, "),te=r(_s,"A",{href:!0,rel:!0});var Zr=o(te);Rs=l(Zr,"DeBERTa"),Zr.forEach(s),qs=l(_s," cannot be supported)"),_s.forEach(s),Hs=d(O),P=r(O,"LI",{});var je=o(P);Fs=l(je,"Use either "),Ne=r(je,"CODE",{});var eo=o(Ne);Ys=l(eo,"gelu"),eo.forEach(s),Us=l(je," or "),Ie=r(je,"CODE",{});var to=o(Ie);Gs=l(to,"relu"),to.forEach(s),zs=l(je," activation function"),je.forEach(s),Js=d(O),Pe=r(O,"LI",{});var so=o(Pe);Ks=l(so,"Have an even number of attention heads"),so.forEach(s),Qs=d(O),se=r(O,"LI",{});var bs=o(se);Vs=l(bs,"Do not use any attention bias (for eg "),xe=r(bs,"CODE",{});var ao=o(xe);Ws=l(ao,"T5"),ao.forEach(s),Xs=l(bs," uses attention bias, therefore cannot be supported)"),bs.forEach(s),Zs=d(O),be=r(O,"LI",{});var Fr=o(be);Se=r(Fr,"CODE",{});var ro=o(Se);ea=l(ro,"eps"),ro.forEach(s),ta=l(Fr," must be equal between the first and second layer norms for each layer"),Fr.forEach(s),O.forEach(s),zt=d(t),x=r(t,"H2",{class:!0});var ys=o(x);Y=r(ys,"A",{id:!0,class:!0,href:!0});var oo=o(Y);Me=r(oo,"SPAN",{});var no=o(Me);k(ae.$$.fragment,no),no.forEach(s),oo.forEach(s),sa=d(ys),re=r(ys,"SPAN",{});var vs=o(re);aa=l(vs,"How to convert a model into its "),Re=r(vs,"CODE",{});var lo=o(Re);ra=l(lo,"BetterTransformer"),lo.forEach(s),oa=l(vs," format?"),vs.forEach(s),ys.forEach(s),Jt=d(t),S=r(t,"H3",{class:!0});var ws=o(S);U=r(ws,"A",{id:!0,class:!0,href:!0});var io=o(U);qe=r(io,"SPAN",{});var ho=o(qe);k(oe.$$.fragment,ho),ho.forEach(s),io.forEach(s),na=d(ws),He=r(ws,"SPAN",{});var co=o(He);la=l(co,"Step 1: Identifying the source layer to change"),co.forEach(s),ws.forEach(s),Kt=d(t),b=r(t,"P",{});var D=o(b);ia=l(D,"First, go to "),Fe=r(D,"CODE",{});var po=o(Fe);ha=l(po,"optimum/bettertransformer/__init__.py"),po.forEach(s),da=l(D," and you\u2019ll see the dictionary "),Ye=r(D,"CODE",{});var fo=o(Ye);ca=l(fo,"BETTER_TRANFORMER_LAYERS_MAPPING_DICT"),fo.forEach(s),pa=l(D,". This should contain the mapping between the Module that can be converted to its "),Ue=r(D,"CODE",{});var uo=o(Ue);fa=l(uo,"BetterTransformer"),uo.forEach(s),ua=l(D,` equivalent.
Let us try to do it step by step for `),Ge=r(D,"CODE",{});var mo=o(Ge);ma=l(mo,"Bert"),mo.forEach(s),_a=l(D,", first we need to identify the layers that needs to be replaced:"),D.forEach(s),Qt=d(t),k(ne.$$.fragment,t),Vt=d(t),G=r(t,"P",{});var Es=o(G);ba=l(Es,"You can clearly see that the layers that needs to be replaced are the "),ze=r(Es,"CODE",{});var _o=o(ze);ya=l(_o,"BertLayer"),_o.forEach(s),va=l(Es," modules since it contains the whole encoder layer module."),Es.forEach(s),Wt=d(t),M=r(t,"H3",{class:!0});var ks=o(M);z=r(ks,"A",{id:!0,class:!0,href:!0});var bo=o(z);Je=r(bo,"SPAN",{});var yo=o(Je);k(le.$$.fragment,yo),yo.forEach(s),bo.forEach(s),wa=d(ks),ie=r(ks,"SPAN",{});var js=o(ie);Ea=l(js,"Step 2: Building the "),Ke=r(js,"CODE",{});var vo=o(Ke);ka=l(vo,"xxxLayerBetterTransformer"),vo.forEach(s),ja=l(js," module"),js.forEach(s),ks.forEach(s),Xt=d(t),v=r(t,"P",{});var Q=o(v);ga=l(Q,"Check that the identified module is not already copied from another module (by inspecting the source code in "),he=r(Q,"A",{href:!0,rel:!0});var wo=o(he);Qe=r(wo,"CODE",{});var Eo=o(Qe);$a=l(Eo,"transformers"),Eo.forEach(s),wo.forEach(s),Aa=l(Q," and checking that the class definition does not start with "),Ve=r(Q,"CODE",{});var ko=o(Ve);Ta=l(ko,"# Copied from ..."),ko.forEach(s),Ca=l(Q,") - and if not, create a class in "),We=r(Q,"CODE",{});var jo=o(We);La=l(jo,"bettertransformer/models/encoder_model.py"),jo.forEach(s),Oa=l(Q,`.
Start with those lines:`),Q.forEach(s),Zt=d(t),k(de.$$.fragment,t),es=d(t),ye=r(t,"P",{});var go=o(ye);Da=l(go,"Now, make sure to fill all the necessary attributes, the list of attributes are:"),go.forEach(s),ts=d(t),f=r(t,"UL",{});var u=o(f);Xe=r(u,"LI",{});var $o=o(Xe);Ze=r($o,"CODE",{});var Ao=o(Ze);Ba=l(Ao,"in_proj_weight"),Ao.forEach(s),$o.forEach(s),Na=d(u),et=r(u,"LI",{});var To=o(et);tt=r(To,"CODE",{});var Co=o(tt);Ia=l(Co,"in_proj_bias"),Co.forEach(s),To.forEach(s),Pa=d(u),st=r(u,"LI",{});var Lo=o(st);at=r(Lo,"CODE",{});var Oo=o(at);xa=l(Oo,"out_proj_weight"),Oo.forEach(s),Lo.forEach(s),Sa=d(u),rt=r(u,"LI",{});var Do=o(rt);ot=r(Do,"CODE",{});var Bo=o(ot);Ma=l(Bo,"out_proj_bias"),Bo.forEach(s),Do.forEach(s),Ra=d(u),nt=r(u,"LI",{});var No=o(nt);lt=r(No,"CODE",{});var Io=o(lt);qa=l(Io,"linear1_weight"),Io.forEach(s),No.forEach(s),Ha=d(u),it=r(u,"LI",{});var Po=o(it);ht=r(Po,"CODE",{});var xo=o(ht);Fa=l(xo,"linear1_bias"),xo.forEach(s),Po.forEach(s),Ya=d(u),dt=r(u,"LI",{});var So=o(dt);ct=r(So,"CODE",{});var Mo=o(ct);Ua=l(Mo,"linear2_weight"),Mo.forEach(s),So.forEach(s),Ga=d(u),pt=r(u,"LI",{});var Ro=o(pt);ft=r(Ro,"CODE",{});var qo=o(ft);za=l(qo,"linear2_bias"),qo.forEach(s),Ro.forEach(s),Ja=d(u),ut=r(u,"LI",{});var Ho=o(ut);mt=r(Ho,"CODE",{});var Fo=o(mt);Ka=l(Fo,"norm1_eps"),Fo.forEach(s),Ho.forEach(s),Qa=d(u),_t=r(u,"LI",{});var Yo=o(_t);bt=r(Yo,"CODE",{});var Uo=o(bt);Va=l(Uo,"norm1_weight"),Uo.forEach(s),Yo.forEach(s),Wa=d(u),yt=r(u,"LI",{});var Go=o(yt);vt=r(Go,"CODE",{});var zo=o(vt);Xa=l(zo,"norm1_bias"),zo.forEach(s),Go.forEach(s),Za=d(u),wt=r(u,"LI",{});var Jo=o(wt);Et=r(Jo,"CODE",{});var Ko=o(Et);er=l(Ko,"norm2_weight"),Ko.forEach(s),Jo.forEach(s),tr=d(u),kt=r(u,"LI",{});var Qo=o(kt);jt=r(Qo,"CODE",{});var Vo=o(jt);sr=l(Vo,"norm2_bias"),Vo.forEach(s),Qo.forEach(s),ar=d(u),gt=r(u,"LI",{});var Wo=o(gt);$t=r(Wo,"CODE",{});var Xo=o($t);rr=l(Xo,"num_heads"),Xo.forEach(s),Wo.forEach(s),or=d(u),m=r(u,"LI",{});var y=o(m);At=r(y,"CODE",{});var Zo=o(At);nr=l(Zo,"embed_dim"),Zo.forEach(s),lr=l(y,`
Note that these attributes correspond to all the components that are necessary to run a Transformer Encoder module, check the figure 1 on the `),ce=r(y,"A",{href:!0,rel:!0});var en=o(ce);ir=l(en,"\u201CAttention Is All You Need\u201D"),en.forEach(s),hr=l(y,` paper.
Once you filled all these attributes (sometimes the `),Tt=r(y,"CODE",{});var tn=o(Tt);dr=l(tn,"query"),tn.forEach(s),cr=l(y,", "),Ct=r(y,"CODE",{});var sn=o(Ct);pr=l(sn,"key"),sn.forEach(s),fr=l(y," and "),Lt=r(y,"CODE",{});var an=o(Lt);ur=l(an,"value"),an.forEach(s),mr=l(y," layers needs to be \u201Ccontigufied\u201D, check the "),pe=r(y,"A",{href:!0,rel:!0});var rn=o(pe);Ot=r(rn,"CODE",{});var on=o(Ot);_r=l(on,"modeling_encoder.py"),on.forEach(s),rn.forEach(s),br=l(y," file to understand more.)"),y.forEach(s),u.forEach(s),ss=d(t),ve=r(t,"P",{});var nn=o(ve);yr=l(nn,"Make sure also to add the lines:"),nn.forEach(s),as=d(t),k(fe.$$.fragment,t),rs=d(t),C=r(t,"P",{});var ge=o(C);vr=l(ge,"Once the "),Dt=r(ge,"CODE",{});var ln=o(Dt);wr=l(ln,"hidden_states"),ln.forEach(s),Er=l(ge," are nested, call "),Bt=r(ge,"CODE",{});var hn=o(Bt);kr=l(hn,"torch._transformer_encoder_layer_fwd"),hn.forEach(s),jr=l(ge," using the right arguments as follows:"),ge.forEach(s),os=d(t),k(ue.$$.fragment,t),ns=d(t),we=r(t,"P",{});var dn=o(we);gr=l(dn,"At the last layer, it is important to \u201Cun-nest\u201D the hidden_states so that it can be processed by the next modules, this is done in these lines:"),dn.forEach(s),ls=d(t),k(me.$$.fragment,t),is=d(t),L=r(t,"P",{});var $e=o(L);$r=l($e,"Also make sure to return a "),Nt=r($e,"CODE",{});var cn=o(Nt);Ar=l(cn,"tuple"),cn.forEach(s),Tr=l($e," to follow the convention of "),It=r($e,"CODE",{});var pn=o(It);Cr=l(pn,"transformers"),pn.forEach(s),Lr=l($e,"."),$e.forEach(s),hs=d(t),J=r(t,"P",{});var gs=o(J);Or=l(gs,"The best way to reproduce this experiment on your own model is to try it by get some inspiration from the provided modeling scripts. Of course, we will be happy to help you converting your model if you open an issue or a Pull Request on "),Pt=r(gs,"CODE",{});var fn=o(Pt);Dr=l(fn,"optimum"),fn.forEach(s),Br=l(gs,"!"),gs.forEach(s),ds=d(t),R=r(t,"H3",{class:!0});var $s=o(R);K=r($s,"A",{id:!0,class:!0,href:!0});var un=o(K);xt=r(un,"SPAN",{});var mn=o(xt);k(_e.$$.fragment,mn),mn.forEach(s),un.forEach(s),Nr=d($s),St=r($s,"SPAN",{});var _n=o(St);Ir=l(_n,"Step 4: Sanity check!"),_n.forEach(s),$s.forEach(s),cs=d(t),w=r(t,"P",{});var V=o(w);Pr=l(V,"As a last step, make sure to update the "),Mt=r(V,"CODE",{});var bn=o(Mt);xr=l(bn,"BETTER_TRANFORMER_LAYERS_MAPPING_DICT"),bn.forEach(s),Sr=l(V," dictionary in  "),Rt=r(V,"CODE",{});var yn=o(Rt);Mr=l(yn,"optimum/bettertransformer/__init__.py"),yn.forEach(s),Rr=l(V," with the correct names, and you should be ready to convert your model. Try it out with the conversion method that is presented in the "),Ee=r(V,"A",{href:!0});var vn=o(Ee);qr=l(vn,"tutorials sections"),vn.forEach(s),Hr=l(V,"!"),V.forEach(s),this.h()},h(){p(B,"name","hf:doc:metadata"),p(B,"content",JSON.stringify(Tn)),p(q,"id","adding-bettertransformer-support-for-new-architectures"),p(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(q,"href","#adding-bettertransformer-support-for-new-architectures"),p(N,"class","relative group"),p(F,"id","models-that-should-be-supported"),p(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(F,"href","#models-that-should-be-supported"),p(I,"class","relative group"),p(Z,"href","https://arxiv.org/abs/1706.03762"),p(Z,"rel","nofollow"),p(te,"href","https://arxiv.org/abs/2006.03654"),p(te,"rel","nofollow"),p(Y,"id","how-to-convert-a-model-into-its-bettertransformer-format"),p(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Y,"href","#how-to-convert-a-model-into-its-bettertransformer-format"),p(x,"class","relative group"),p(U,"id","step-1-identifying-the-source-layer-to-change"),p(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(U,"href","#step-1-identifying-the-source-layer-to-change"),p(S,"class","relative group"),p(z,"id","step-2-building-the-xxxlayerbettertransformer-module"),p(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(z,"href","#step-2-building-the-xxxlayerbettertransformer-module"),p(M,"class","relative group"),p(he,"href","https://github.com/huggingface/transformers"),p(he,"rel","nofollow"),p(ce,"href","https://arxiv.org/pdf/1706.03762.pdf"),p(ce,"rel","nofollow"),p(pe,"href","https://github.com/huggingface/optimum/tree/main/optimum/bettertransformer/models/encoder_model.py"),p(pe,"rel","nofollow"),p(K,"id","step-4:-sanity-check!"),p(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(K,"href","#step-4:-sanity-check!"),p(R,"class","relative group"),p(Ee,"href","./tutorials/content")},m(t,i){e(document.head,B),c(t,Ht,i),c(t,N,i),e(N,q),e(q,Te),j(W,Te,null),e(N,As),e(N,Ce),e(Ce,Ts),c(t,Ft,i),c(t,H,i),e(H,Cs),e(H,Le),e(Le,Ls),e(H,Os),c(t,Yt,i),c(t,I,i),e(I,F),e(F,Oe),j(X,Oe,null),e(I,Ds),e(I,De),e(De,Bs),c(t,Ut,i),c(t,T,i),e(T,Ns),e(T,Z),e(Z,Is),e(T,Ps),e(T,Be),e(Be,xs),e(T,Ss),c(t,Gt,i),c(t,_,i),e(_,ee),e(ee,Ms),e(ee,te),e(te,Rs),e(ee,qs),e(_,Hs),e(_,P),e(P,Fs),e(P,Ne),e(Ne,Ys),e(P,Us),e(P,Ie),e(Ie,Gs),e(P,zs),e(_,Js),e(_,Pe),e(Pe,Ks),e(_,Qs),e(_,se),e(se,Vs),e(se,xe),e(xe,Ws),e(se,Xs),e(_,Zs),e(_,be),e(be,Se),e(Se,ea),e(be,ta),c(t,zt,i),c(t,x,i),e(x,Y),e(Y,Me),j(ae,Me,null),e(x,sa),e(x,re),e(re,aa),e(re,Re),e(Re,ra),e(re,oa),c(t,Jt,i),c(t,S,i),e(S,U),e(U,qe),j(oe,qe,null),e(S,na),e(S,He),e(He,la),c(t,Kt,i),c(t,b,i),e(b,ia),e(b,Fe),e(Fe,ha),e(b,da),e(b,Ye),e(Ye,ca),e(b,pa),e(b,Ue),e(Ue,fa),e(b,ua),e(b,Ge),e(Ge,ma),e(b,_a),c(t,Qt,i),j(ne,t,i),c(t,Vt,i),c(t,G,i),e(G,ba),e(G,ze),e(ze,ya),e(G,va),c(t,Wt,i),c(t,M,i),e(M,z),e(z,Je),j(le,Je,null),e(M,wa),e(M,ie),e(ie,Ea),e(ie,Ke),e(Ke,ka),e(ie,ja),c(t,Xt,i),c(t,v,i),e(v,ga),e(v,he),e(he,Qe),e(Qe,$a),e(v,Aa),e(v,Ve),e(Ve,Ta),e(v,Ca),e(v,We),e(We,La),e(v,Oa),c(t,Zt,i),j(de,t,i),c(t,es,i),c(t,ye,i),e(ye,Da),c(t,ts,i),c(t,f,i),e(f,Xe),e(Xe,Ze),e(Ze,Ba),e(f,Na),e(f,et),e(et,tt),e(tt,Ia),e(f,Pa),e(f,st),e(st,at),e(at,xa),e(f,Sa),e(f,rt),e(rt,ot),e(ot,Ma),e(f,Ra),e(f,nt),e(nt,lt),e(lt,qa),e(f,Ha),e(f,it),e(it,ht),e(ht,Fa),e(f,Ya),e(f,dt),e(dt,ct),e(ct,Ua),e(f,Ga),e(f,pt),e(pt,ft),e(ft,za),e(f,Ja),e(f,ut),e(ut,mt),e(mt,Ka),e(f,Qa),e(f,_t),e(_t,bt),e(bt,Va),e(f,Wa),e(f,yt),e(yt,vt),e(vt,Xa),e(f,Za),e(f,wt),e(wt,Et),e(Et,er),e(f,tr),e(f,kt),e(kt,jt),e(jt,sr),e(f,ar),e(f,gt),e(gt,$t),e($t,rr),e(f,or),e(f,m),e(m,At),e(At,nr),e(m,lr),e(m,ce),e(ce,ir),e(m,hr),e(m,Tt),e(Tt,dr),e(m,cr),e(m,Ct),e(Ct,pr),e(m,fr),e(m,Lt),e(Lt,ur),e(m,mr),e(m,pe),e(pe,Ot),e(Ot,_r),e(m,br),c(t,ss,i),c(t,ve,i),e(ve,yr),c(t,as,i),j(fe,t,i),c(t,rs,i),c(t,C,i),e(C,vr),e(C,Dt),e(Dt,wr),e(C,Er),e(C,Bt),e(Bt,kr),e(C,jr),c(t,os,i),j(ue,t,i),c(t,ns,i),c(t,we,i),e(we,gr),c(t,ls,i),j(me,t,i),c(t,is,i),c(t,L,i),e(L,$r),e(L,Nt),e(Nt,Ar),e(L,Tr),e(L,It),e(It,Cr),e(L,Lr),c(t,hs,i),c(t,J,i),e(J,Or),e(J,Pt),e(Pt,Dr),e(J,Br),c(t,ds,i),c(t,R,i),e(R,K),e(K,xt),j(_e,xt,null),e(R,Nr),e(R,St),e(St,Ir),c(t,cs,i),c(t,w,i),e(w,Pr),e(w,Mt),e(Mt,xr),e(w,Sr),e(w,Rt),e(Rt,Mr),e(w,Rr),e(w,Ee),e(Ee,qr),e(w,Hr),ps=!0},p:gn,i(t){ps||(g(W.$$.fragment,t),g(X.$$.fragment,t),g(ae.$$.fragment,t),g(oe.$$.fragment,t),g(ne.$$.fragment,t),g(le.$$.fragment,t),g(de.$$.fragment,t),g(fe.$$.fragment,t),g(ue.$$.fragment,t),g(me.$$.fragment,t),g(_e.$$.fragment,t),ps=!0)},o(t){$(W.$$.fragment,t),$(X.$$.fragment,t),$(ae.$$.fragment,t),$(oe.$$.fragment,t),$(ne.$$.fragment,t),$(le.$$.fragment,t),$(de.$$.fragment,t),$(fe.$$.fragment,t),$(ue.$$.fragment,t),$(me.$$.fragment,t),$(_e.$$.fragment,t),ps=!1},d(t){s(B),t&&s(Ht),t&&s(N),A(W),t&&s(Ft),t&&s(H),t&&s(Yt),t&&s(I),A(X),t&&s(Ut),t&&s(T),t&&s(Gt),t&&s(_),t&&s(zt),t&&s(x),A(ae),t&&s(Jt),t&&s(S),A(oe),t&&s(Kt),t&&s(b),t&&s(Qt),A(ne,t),t&&s(Vt),t&&s(G),t&&s(Wt),t&&s(M),A(le),t&&s(Xt),t&&s(v),t&&s(Zt),A(de,t),t&&s(es),t&&s(ye),t&&s(ts),t&&s(f),t&&s(ss),t&&s(ve),t&&s(as),A(fe,t),t&&s(rs),t&&s(C),t&&s(os),A(ue,t),t&&s(ns),t&&s(we),t&&s(ls),A(me,t),t&&s(is),t&&s(L),t&&s(hs),t&&s(J),t&&s(ds),t&&s(R),A(_e),t&&s(cs),t&&s(w)}}}const Tn={local:"adding-bettertransformer-support-for-new-architectures",sections:[{local:"models-that-should-be-supported",title:"Models that should be supported"},{local:"how-to-convert-a-model-into-its-bettertransformer-format",sections:[{local:"step-1-identifying-the-source-layer-to-change",title:"Step 1: Identifying the source layer to change"},{local:"step-2-building-the-xxxlayerbettertransformer-module",title:"Step 2: Building the `xxxLayerBetterTransformer` module"}],title:"How to convert a model into its `BetterTransformer` format?"}],title:"Adding BetterTransformer support for new architectures"};function Cn(Yr){return $n(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Bn extends wn{constructor(B){super();En(this,B,Cn,An,kn,{})}}export{Bn as default,Tn as metadata};
