import{S as cl,i as ul,s as hl,e as o,k as s,w as _,t as a,M as _l,c as r,d as t,m,a as i,x as g,h as n,b as d,G as e,g as O,y as f,q as v,o as b,B as y,v as gl}from"../../../chunks/vendor-hf-doc-builder.js";import{T as pl}from"../../../chunks/Tip-hf-doc-builder.js";import{D as N}from"../../../chunks/Docstring-hf-doc-builder.js";import{I as ba}from"../../../chunks/IconCopyLink-hf-doc-builder.js";function fl(Ge){let h,D;return{c(){h=o("p"),D=a(`If your predictions or labels have different sequence length (for instance because you\u2019re doing dynamic padding
in a token classification task) the predictions will be padded (on the right) to allow for concatenation into
one array. The padding index is -100.`)},l(T){h=r(T,"P",{});var x=i(h);D=n(x,`If your predictions or labels have different sequence length (for instance because you\u2019re doing dynamic padding
in a token classification task) the predictions will be padded (on the right) to allow for concatenation into
one array. The padding index is -100.`),x.forEach(t)},m(T,x){O(T,h,x),e(h,D)},d(T){T&&t(h)}}}function vl(Ge){let h,D;return{c(){h=o("p"),D=a(`If your predictions or labels have different sequence length (for instance because you\u2019re doing dynamic padding
in a token classification task) the predictions will be padded (on the right) to allow for concatenation into
one array. The padding index is -100.`)},l(T){h=r(T,"P",{});var x=i(h);D=n(x,`If your predictions or labels have different sequence length (for instance because you\u2019re doing dynamic padding
in a token classification task) the predictions will be padded (on the right) to allow for concatenation into
one array. The padding index is -100.`),x.forEach(t)},m(T,x){O(T,h,x),e(h,D)},d(T){T&&t(h)}}}function bl(Ge){let h,D,T,x,mt,ge,Ha,dt,Ga,ya,M,ne,pt,fe,Ba,ct,ja,Ta,p,ve,Ja,ut,Ya,Ka,ht,Qa,Za,R,oe,_t,en,tn,be,an,nn,on,w,gt,rn,ln,ft,sn,mn,vt,dn,pn,bt,cn,un,yt,hn,_n,Tt,gn,fn,Ot,vn,bn,yn,Be,xt,Tn,On,xn,S,wt,wn,$n,$t,kn,En,kt,Nn,Rn,Et,Dn,Sn,qn,q,Nt,An,Pn,Rt,Fn,zn,Dt,In,Cn,St,Ln,Mn,Un,re,ye,Wn,qt,Vn,Xn,z,Te,Hn,At,Gn,Bn,Oe,jn,Pt,Jn,Yn,Kn,ie,xe,Qn,Ft,Zn,eo,I,we,to,U,ao,zt,no,oo,It,ro,io,lo,Ct,so,mo,le,$e,po,ke,co,Lt,uo,ho,_o,$,Ee,go,Mt,fo,vo,Ne,bo,Ut,yo,To,Oo,se,xo,Re,wo,Wt,$o,ko,Eo,W,V,No,Vt,Ro,Do,Xt,So,qo,Ao,X,Po,Ht,Fo,zo,Gt,Io,Co,Lo,H,Mo,Bt,Uo,Wo,jt,Vo,Xo,Ho,C,De,Go,G,Bo,Jt,jo,Jo,Yt,Yo,Ko,Qo,Kt,Zo,er,me,Se,tr,B,ar,Qt,nr,or,Zt,rr,ir,lr,de,qe,sr,ea,mr,Oa,j,pe,ta,Ae,dr,aa,pr,xa,F,Pe,cr,ce,Fe,ur,na,hr,_r,k,ze,gr,oa,fr,vr,Ie,br,ra,yr,Tr,Or,ue,xr,Ce,wr,ia,$r,kr,Er,J,Y,Nr,la,Rr,Dr,sa,Sr,qr,Ar,K,Pr,ma,Fr,zr,da,Ir,Cr,Lr,Q,Mr,pa,Ur,Wr,ca,Vr,Xr,wa,Z,he,ua,Le,Hr,ha,Gr,$a,Me,Ue,ka,ee,_e,_a,We,Br,ga,jr,Ea,Ve,Xe,Na;return ge=new ba({}),fe=new ba({}),ve=new N({props:{name:"class optimum.onnxruntime.ORTTrainer",anchor:"optimum.onnxruntime.ORTTrainer",parameters:[{name:"model",val:": typing.Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module] = None"},{name:"tokenizer",val:": typing.Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase] = None"},{name:"feature",val:": str = 'default'"},{name:"args",val:": ORTTrainingArguments = None"},{name:"data_collator",val:": typing.Optional[DataCollator] = None"},{name:"train_dataset",val:": typing.Optional[torch.utils.data.dataset.Dataset] = None"},{name:"eval_dataset",val:": typing.Optional[torch.utils.data.dataset.Dataset] = None"},{name:"model_init",val:": typing.Callable[[], transformers.modeling_utils.PreTrainedModel] = None"},{name:"compute_metrics",val:": typing.Union[typing.Callable[[transformers.trainer_utils.EvalPrediction], typing.Dict], NoneType] = None"},{name:"callbacks",val:": typing.Optional[typing.List[transformers.trainer_callback.TrainerCallback]] = None"},{name:"optimizers",val:": typing.Tuple[torch.optim.optimizer.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None)"},{name:"preprocess_logits_for_metrics",val:": typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = None"},{name:"onnx_model_path",val:": typing.Union[str, os.PathLike] = None"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTTrainer.model",description:`<strong>model</strong> (<a href="https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel" rel="nofollow">PreTrainedModel</a> or <code>torch.nn.Module</code>, <em>optional</em>) &#x2014;
The model to train, evaluate or use for predictions. If not provided, a <code>model_init</code> must be passed.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p><code>ORTTrainer</code> is optimized to work with the <a href="https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel" rel="nofollow">PreTrainedModel</a> provided by the transformers library.
You can still use your own models defined as <code>torch.nn.Module</code> for training with ONNX Runtime backend
and inference with PyTorch backend as long as they work the same way as the &#x1F917; Transformers models.</p>

					</div>`,name:"model"},{anchor:"optimum.onnxruntime.ORTTrainer.args",description:`<strong>args</strong> (<code>ORTTrainingArguments</code>, <em>optional</em>) &#x2014;
The arguments to tweak for training. Will default to a basic instance of <code>ORTTrainingArguments</code> with the
<code>output_dir</code> set to a directory named <em>tmp_trainer</em> in the current directory if not provided.`,name:"args"},{anchor:"optimum.onnxruntime.ORTTrainer.data_collator",description:`<strong>data_collator</strong> (<code>DataCollator</code>, <em>optional</em>) &#x2014;
The function to use to form a batch from a list of elements of <code>train_dataset</code> or <code>eval_dataset</code>. Will
default to <a href="https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.default_data_collator" rel="nofollow">default_data_collator</a> if no <code>tokenizer</code> is provided, an instance of
<a href="https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithPadding" rel="nofollow">DataCollatorWithPadding</a> otherwise.`,name:"data_collator"},{anchor:"optimum.onnxruntime.ORTTrainer.train_dataset",description:`<strong>train_dataset</strong> (<code>torch.utils.data.Dataset</code> or <code>torch.utils.data.IterableDataset</code>, <em>optional</em>) &#x2014;
The dataset to use for training. If it is a <a href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset" rel="nofollow">Dataset</a>, columns not accepted by the
<code>model.forward()</code> method are automatically removed.
Note that if it&#x2019;s a <code>torch.utils.data.IterableDataset</code> with some randomization and you are training in a
distributed fashion, your iterable dataset should either use a internal attribute <code>generator</code> that is a
<code>torch.Generator</code> for the randomization that must be identical on all processes (and the ORTTrainer will
manually set the seed of this <code>generator</code> at each epoch) or have a <code>set_epoch()</code> method that internally
sets the seed of the RNGs used.`,name:"train_dataset"},{anchor:"optimum.onnxruntime.ORTTrainer.eval_dataset",description:`<strong>eval_dataset</strong> (Union[<code>torch.utils.data.Dataset</code>, Dict[str, <code>torch.utils.data.Dataset</code>]), <em>optional</em>) &#x2014;
The dataset to use for evaluation. If it is a <a href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset" rel="nofollow">Dataset</a>, columns not accepted by the
<code>model.forward()</code> method are automatically removed. If it is a dictionary, it will evaluate on each
dataset prepending the dictionary key to the metric name.`,name:"eval_dataset"},{anchor:"optimum.onnxruntime.ORTTrainer.tokenizer",description:`<strong>tokenizer</strong> (<a href="https://huggingface.co/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase" rel="nofollow">PreTrainedTokenizerBase</a>, <em>optional</em>) &#x2014;
The tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs the
maximum length when batching inputs, and it will be saved along the model to make it easier to rerun an
interrupted training or reuse the fine-tuned model.`,name:"tokenizer"},{anchor:"optimum.onnxruntime.ORTTrainer.model_init",description:`<strong>model_init</strong> (<code>Callable[[], PreTrainedModel]</code>, <em>optional</em>) &#x2014;
A function that instantiates the model to be used. If provided, each call to <code>ORTTrainer.train</code> will start
from a new instance of the model as given by this function.
The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to
be able to choose different architectures according to hyper parameters (such as layer count, sizes of
inner layers, dropout probabilities etc).`,name:"model_init"},{anchor:"optimum.onnxruntime.ORTTrainer.compute_metrics",description:`<strong>compute_metrics</strong> (<code>Callable[[EvalPrediction], Dict]</code>, <em>optional</em>) &#x2014;
The function that will be used to compute metrics at evaluation. Must take a <code>EvalPrediction</code> and return
a dictionary string to metric values.`,name:"compute_metrics"},{anchor:"optimum.onnxruntime.ORTTrainer.callbacks",description:`<strong>callbacks</strong> (List of <code>TrainerCallback</code>, <em>optional</em>) &#x2014;
A list of callbacks to customize the training loop. Will add those to the list of default callbacks
detailed in <a href="callback">here</a>.
If you want to remove one of the default callbacks used, use the <code>ORTTrainer.remove_callback</code> method.`,name:"callbacks"},{anchor:"optimum.onnxruntime.ORTTrainer.optimizers",description:`<strong>optimizers</strong> (<code>Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]</code>, <em>optional</em>) &#x2014; A tuple
containing the optimizer and the scheduler to use. Will default to an instance of <code>AdamW</code> on your model
and a scheduler given by <code>get_linear_schedule_with_warmup</code> controlled by <code>args</code>.`,name:"optimizers"},{anchor:"optimum.onnxruntime.ORTTrainer.preprocess_logits_for_metrics",description:`<strong>preprocess_logits_for_metrics</strong> (<code>Callable[[torch.Tensor, torch.Tensor], torch.Tensor]</code>, <em>optional</em>) &#x2014;
A function that preprocess the logits right before caching them at each evaluation step. Must take two
tensors, the logits and the labels, and return the logits once processed as desired. The modifications made
by this function will be reflected in the predictions received by <code>compute_metrics</code>.
Note that the labels (second parameter) will be <code>None</code> if the dataset does not have them.`,name:"preprocess_logits_for_metrics"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer.py#L154"}}),ye=new N({props:{name:"compute_loss_ort",anchor:"optimum.onnxruntime.ORTTrainer.compute_loss_ort",parameters:[{name:"model",val:""},{name:"inputs",val:""},{name:"return_outputs",val:" = False"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer.py#L1370"}}),Te=new N({props:{name:"create_optimizer",anchor:"optimum.onnxruntime.ORTTrainer.create_optimizer",parameters:[],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer.py#L1574"}}),xe=new N({props:{name:"evaluate",anchor:"optimum.onnxruntime.ORTTrainer.evaluate",parameters:[{name:"eval_dataset",val:": typing.Optional[torch.utils.data.dataset.Dataset] = None"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"},{name:"metric_key_prefix",val:": str = 'eval'"},{name:"inference_with_ort",val:": bool = False"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTTrainer.evaluate.eval_dataset",description:`<strong>eval_dataset</strong> (<code>Dataset</code>, <em>optional</em>) &#x2014;
Pass a dataset if you wish to override <code>self.eval_dataset</code>. If it is a <a href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset" rel="nofollow">Dataset</a>, columns
not accepted by the <code>model.forward()</code> method are automatically removed. It must implement the <code>__len__</code>
method.`,name:"eval_dataset"},{anchor:"optimum.onnxruntime.ORTTrainer.evaluate.ignore_keys",description:`<strong>ignore_keys</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.`,name:"ignore_keys"},{anchor:"optimum.onnxruntime.ORTTrainer.evaluate.metric_key_prefix",description:`<strong>metric_key_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;eval&quot;</code>) &#x2014;
An optional prefix to be used as the metrics key prefix. For example the metrics &#x201C;bleu&#x201D; will be named
&#x201C;eval_bleu&#x201D; if the prefix is &#x201C;eval&#x201D; (default)`,name:"metric_key_prefix"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer.py#L737",returnDescription:`
<p>A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The
dictionary also contains the epoch number which comes from the training state.</p>
`}}),we=new N({props:{name:"evaluation_loop_ort",anchor:"optimum.onnxruntime.ORTTrainer.evaluation_loop_ort",parameters:[{name:"dataloader",val:": DataLoader"},{name:"description",val:": str"},{name:"prediction_loss_only",val:": typing.Optional[bool] = None"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"},{name:"metric_key_prefix",val:": str = 'eval'"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer.py#L897"}}),$e=new N({props:{name:"get_ort_optimizer_cls_and_kwargs",anchor:"optimum.onnxruntime.ORTTrainer.get_ort_optimizer_cls_and_kwargs",parameters:[{name:"args",val:": ORTTrainingArguments"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTTrainer.get_ort_optimizer_cls_and_kwargs.args",description:`<strong>args</strong> (<code>ORTTrainingArguments</code>) &#x2014;
The training arguments for the training session.`,name:"args"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer.py#L1627"}}),Ee=new N({props:{name:"predict",anchor:"optimum.onnxruntime.ORTTrainer.predict",parameters:[{name:"test_dataset",val:": Dataset"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"},{name:"metric_key_prefix",val:": str = 'test'"},{name:"inference_with_ort",val:": bool = False"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTTrainer.predict.test_dataset",description:`<strong>test_dataset</strong> (<code>Dataset</code>) &#x2014;
Dataset to run the predictions on. If it is an <code>datasets.Dataset</code>, columns not accepted by the
<code>model.forward()</code> method are automatically removed. Has to implement the method <code>__len__</code>`,name:"test_dataset"},{anchor:"optimum.onnxruntime.ORTTrainer.predict.ignore_keys",description:`<strong>ignore_keys</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.`,name:"ignore_keys"},{anchor:"optimum.onnxruntime.ORTTrainer.predict.metric_key_prefix",description:`<strong>metric_key_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;test&quot;</code>) &#x2014;
An optional prefix to be used as the metrics key prefix. For example the metrics &#x201C;bleu&#x201D; will be named
&#x201C;test_bleu&#x201D; if the prefix is &#x201C;test&#x201D; (default)`,name:"metric_key_prefix"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer.py#L814"}}),se=new pl({props:{$$slots:{default:[fl]},$$scope:{ctx:Ge}}}),De=new N({props:{name:"prediction_loop_ort",anchor:"optimum.onnxruntime.ORTTrainer.prediction_loop_ort",parameters:[{name:"dataloader",val:": DataLoader"},{name:"description",val:": str"},{name:"prediction_loss_only",val:": typing.Optional[bool] = None"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"},{name:"metric_key_prefix",val:": str = 'eval'"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer.py#L1121"}}),Se=new N({props:{name:"prediction_step_ort",anchor:"optimum.onnxruntime.ORTTrainer.prediction_step_ort",parameters:[{name:"model",val:": ORTModel"},{name:"inputs",val:": typing.Dict[str, typing.Union[torch.Tensor, typing.Any]]"},{name:"prediction_loss_only",val:": bool"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTTrainer.prediction_step_ort.model",description:`<strong>model</strong> (<code>ORTModel</code>) &#x2014;
The model to evaluate.`,name:"model"},{anchor:"optimum.onnxruntime.ORTTrainer.prediction_step_ort.inputs",description:`<strong>inputs</strong> (<code>Dict[str, Union[torch.Tensor, Any]]</code>) &#x2014;
The inputs and targets of the model.</p>
<p>The dictionary will be unpacked before being fed to the model. Most models expect the targets under the
argument <code>labels</code>. Check your model&#x2019;s documentation for all accepted arguments.`,name:"inputs"},{anchor:"optimum.onnxruntime.ORTTrainer.prediction_step_ort.prediction_loss_only",description:`<strong>prediction_loss_only</strong> (<code>bool</code>) &#x2014;
Whether or not to return the loss only.`,name:"prediction_loss_only"},{anchor:"optimum.onnxruntime.ORTTrainer.prediction_step_ort.ignore_keys",description:`<strong>ignore_keys</strong> (<code>Lst[str]</code>, <em>optional</em>) &#x2014;
A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.`,name:"ignore_keys"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer.py#L1284",returnDescription:`
<p>A tuple with the loss,
logits and labels (each being optional).</p>
`,returnType:`
<p>Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]</p>
`}}),qe=new N({props:{name:"train",anchor:"optimum.onnxruntime.ORTTrainer.train",parameters:[{name:"resume_from_checkpoint",val:": typing.Union[str, bool, NoneType] = None"},{name:"trial",val:": typing.Union[ForwardRef('optuna.Trial'), typing.Dict[str, typing.Any]] = None"},{name:"ignore_keys_for_eval",val:": typing.Optional[typing.List[str]] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTTrainer.train.resume_from_checkpoint",description:`<strong>resume_from_checkpoint</strong> (<code>str</code> or <code>bool</code>, <em>optional</em>) &#x2014;
If a <code>str</code>, local path to a saved checkpoint as saved by a previous instance of <code>ORTTrainer</code>. If a
<code>bool</code> and equals <code>True</code>, load the last checkpoint in <em>args.output_dir</em> as saved by a previous instance
of <code>ORTTrainer</code>. If present, training will resume from the model/optimizer/scheduler states loaded here.`,name:"resume_from_checkpoint"},{anchor:"optimum.onnxruntime.ORTTrainer.train.trial",description:`<strong>trial</strong> (<code>optuna.Trial</code> or <code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The trial run or the hyperparameter dictionary for hyperparameter search.`,name:"trial"},{anchor:"optimum.onnxruntime.ORTTrainer.train.ignore_keys_for_eval",description:`<strong>ignore_keys_for_eval</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions for evaluation during the training.
kwargs &#x2014;
Additional keyword arguments used to hide deprecated arguments`,name:"ignore_keys_for_eval"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer.py#L268"}}),Ae=new ba({}),Pe=new N({props:{name:"class optimum.onnxruntime.ORTSeq2SeqTrainer",anchor:"optimum.onnxruntime.ORTSeq2SeqTrainer",parameters:[{name:"model",val:": typing.Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module] = None"},{name:"tokenizer",val:": typing.Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase] = None"},{name:"feature",val:": str = 'default'"},{name:"args",val:": ORTTrainingArguments = None"},{name:"data_collator",val:": typing.Optional[DataCollator] = None"},{name:"train_dataset",val:": typing.Optional[torch.utils.data.dataset.Dataset] = None"},{name:"eval_dataset",val:": typing.Optional[torch.utils.data.dataset.Dataset] = None"},{name:"model_init",val:": typing.Callable[[], transformers.modeling_utils.PreTrainedModel] = None"},{name:"compute_metrics",val:": typing.Union[typing.Callable[[transformers.trainer_utils.EvalPrediction], typing.Dict], NoneType] = None"},{name:"callbacks",val:": typing.Optional[typing.List[transformers.trainer_callback.TrainerCallback]] = None"},{name:"optimizers",val:": typing.Tuple[torch.optim.optimizer.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None)"},{name:"preprocess_logits_for_metrics",val:": typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = None"},{name:"onnx_model_path",val:": typing.Union[str, os.PathLike] = None"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer_seq2seq.py#L72"}}),Fe=new N({props:{name:"evaluate",anchor:"optimum.onnxruntime.ORTSeq2SeqTrainer.evaluate",parameters:[{name:"eval_dataset",val:": typing.Optional[torch.utils.data.dataset.Dataset] = None"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"},{name:"metric_key_prefix",val:": str = 'eval'"},{name:"inference_with_ort",val:": bool = False"},{name:"**gen_kwargs",val:""}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTSeq2SeqTrainer.evaluate.eval_dataset",description:`<strong>eval_dataset</strong> (<code>Dataset</code>, <em>optional</em>) &#x2014;
Pass a dataset if you wish to override <code>self.eval_dataset</code>. If it is a <a href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset" rel="nofollow">Dataset</a>, columns
not accepted by the <code>model.forward()</code> method are automatically removed. It must implement the <code>__len__</code>
method.`,name:"eval_dataset"},{anchor:"optimum.onnxruntime.ORTSeq2SeqTrainer.evaluate.ignore_keys",description:`<strong>ignore_keys</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.`,name:"ignore_keys"},{anchor:"optimum.onnxruntime.ORTSeq2SeqTrainer.evaluate.metric_key_prefix",description:`<strong>metric_key_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;eval&quot;</code>) &#x2014;
An optional prefix to be used as the metrics key prefix. For example the metrics &#x201C;bleu&#x201D; will be named
&#x201C;eval_bleu&#x201D; if the prefix is &#x201C;eval&#x201D; (default)`,name:"metric_key_prefix"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer_seq2seq.py#L73",returnDescription:`
<p>A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The
dictionary also contains the epoch number which comes from the training state.</p>
`}}),ze=new N({props:{name:"predict",anchor:"optimum.onnxruntime.ORTSeq2SeqTrainer.predict",parameters:[{name:"test_dataset",val:": Dataset"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"},{name:"metric_key_prefix",val:": str = 'eval'"},{name:"inference_with_ort",val:": bool = False"},{name:"**gen_kwargs",val:""}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTSeq2SeqTrainer.predict.test_dataset",description:`<strong>test_dataset</strong> (<code>Dataset</code>) &#x2014;
Dataset to run the predictions on. If it is an <code>datasets.Dataset</code>, columns not accepted by the
<code>model.forward()</code> method are automatically removed. Has to implement the method <code>__len__</code>`,name:"test_dataset"},{anchor:"optimum.onnxruntime.ORTSeq2SeqTrainer.predict.ignore_keys",description:`<strong>ignore_keys</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.`,name:"ignore_keys"},{anchor:"optimum.onnxruntime.ORTSeq2SeqTrainer.predict.metric_key_prefix",description:`<strong>metric_key_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;test&quot;</code>) &#x2014;
An optional prefix to be used as the metrics key prefix. For example the metrics &#x201C;bleu&#x201D; will be named
&#x201C;test_bleu&#x201D; if the prefix is &#x201C;test&#x201D; (default)`,name:"metric_key_prefix"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer_seq2seq.py#L116"}}),ue=new pl({props:{$$slots:{default:[vl]},$$scope:{ctx:Ge}}}),Le=new ba({}),Ue=new N({props:{name:"class optimum.onnxruntime.ORTTrainingArguments",anchor:"optimum.onnxruntime.ORTTrainingArguments",parameters:[{name:"output_dir",val:": str"},{name:"overwrite_output_dir",val:": bool = False"},{name:"do_train",val:": bool = False"},{name:"do_eval",val:": bool = False"},{name:"do_predict",val:": bool = False"},{name:"evaluation_strategy",val:": typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'no'"},{name:"prediction_loss_only",val:": bool = False"},{name:"per_device_train_batch_size",val:": int = 8"},{name:"per_device_eval_batch_size",val:": int = 8"},{name:"per_gpu_train_batch_size",val:": typing.Optional[int] = None"},{name:"per_gpu_eval_batch_size",val:": typing.Optional[int] = None"},{name:"gradient_accumulation_steps",val:": int = 1"},{name:"eval_accumulation_steps",val:": typing.Optional[int] = None"},{name:"eval_delay",val:": typing.Optional[float] = 0"},{name:"learning_rate",val:": float = 5e-05"},{name:"weight_decay",val:": float = 0.0"},{name:"adam_beta1",val:": float = 0.9"},{name:"adam_beta2",val:": float = 0.999"},{name:"adam_epsilon",val:": float = 1e-08"},{name:"max_grad_norm",val:": float = 1.0"},{name:"num_train_epochs",val:": float = 3.0"},{name:"max_steps",val:": int = -1"},{name:"lr_scheduler_type",val:": typing.Union[transformers.trainer_utils.SchedulerType, str] = 'linear'"},{name:"warmup_ratio",val:": float = 0.0"},{name:"warmup_steps",val:": int = 0"},{name:"log_level",val:": typing.Optional[str] = 'passive'"},{name:"log_level_replica",val:": typing.Optional[str] = 'passive'"},{name:"log_on_each_node",val:": bool = True"},{name:"logging_dir",val:": typing.Optional[str] = None"},{name:"logging_strategy",val:": typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps'"},{name:"logging_first_step",val:": bool = False"},{name:"logging_steps",val:": int = 500"},{name:"logging_nan_inf_filter",val:": bool = True"},{name:"save_strategy",val:": typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps'"},{name:"save_steps",val:": int = 500"},{name:"save_total_limit",val:": typing.Optional[int] = None"},{name:"save_on_each_node",val:": bool = False"},{name:"no_cuda",val:": bool = False"},{name:"use_mps_device",val:": bool = False"},{name:"seed",val:": int = 42"},{name:"data_seed",val:": typing.Optional[int] = None"},{name:"jit_mode_eval",val:": bool = False"},{name:"use_ipex",val:": bool = False"},{name:"bf16",val:": bool = False"},{name:"fp16",val:": bool = False"},{name:"fp16_opt_level",val:": str = 'O1'"},{name:"half_precision_backend",val:": str = 'auto'"},{name:"bf16_full_eval",val:": bool = False"},{name:"fp16_full_eval",val:": bool = False"},{name:"tf32",val:": typing.Optional[bool] = None"},{name:"local_rank",val:": int = -1"},{name:"xpu_backend",val:": typing.Optional[str] = None"},{name:"tpu_num_cores",val:": typing.Optional[int] = None"},{name:"tpu_metrics_debug",val:": bool = False"},{name:"debug",val:": str = ''"},{name:"dataloader_drop_last",val:": bool = False"},{name:"eval_steps",val:": typing.Optional[int] = None"},{name:"dataloader_num_workers",val:": int = 0"},{name:"past_index",val:": int = -1"},{name:"run_name",val:": typing.Optional[str] = None"},{name:"disable_tqdm",val:": typing.Optional[bool] = None"},{name:"remove_unused_columns",val:": typing.Optional[bool] = True"},{name:"label_names",val:": typing.Optional[typing.List[str]] = None"},{name:"load_best_model_at_end",val:": typing.Optional[bool] = False"},{name:"metric_for_best_model",val:": typing.Optional[str] = None"},{name:"greater_is_better",val:": typing.Optional[bool] = None"},{name:"ignore_data_skip",val:": bool = False"},{name:"sharded_ddp",val:": str = ''"},{name:"fsdp",val:": str = ''"},{name:"fsdp_min_num_params",val:": int = 0"},{name:"fsdp_transformer_layer_cls_to_wrap",val:": typing.Optional[str] = None"},{name:"deepspeed",val:": typing.Optional[str] = None"},{name:"label_smoothing_factor",val:": float = 0.0"},{name:"optim",val:": typing.Optional[str] = 'adamw_hf'"},{name:"adafactor",val:": bool = False"},{name:"group_by_length",val:": bool = False"},{name:"length_column_name",val:": typing.Optional[str] = 'length'"},{name:"report_to",val:": typing.Optional[typing.List[str]] = None"},{name:"ddp_find_unused_parameters",val:": typing.Optional[bool] = None"},{name:"ddp_bucket_cap_mb",val:": typing.Optional[int] = None"},{name:"dataloader_pin_memory",val:": bool = True"},{name:"skip_memory_metrics",val:": bool = True"},{name:"use_legacy_prediction_loop",val:": bool = False"},{name:"push_to_hub",val:": bool = False"},{name:"resume_from_checkpoint",val:": typing.Optional[str] = None"},{name:"hub_model_id",val:": typing.Optional[str] = None"},{name:"hub_strategy",val:": typing.Union[transformers.trainer_utils.HubStrategy, str] = 'every_save'"},{name:"hub_token",val:": typing.Optional[str] = None"},{name:"hub_private_repo",val:": bool = False"},{name:"gradient_checkpointing",val:": bool = False"},{name:"include_inputs_for_metrics",val:": bool = False"},{name:"fp16_backend",val:": str = 'auto'"},{name:"push_to_hub_model_id",val:": typing.Optional[str] = None"},{name:"push_to_hub_organization",val:": typing.Optional[str] = None"},{name:"push_to_hub_token",val:": typing.Optional[str] = None"},{name:"mp_parameters",val:": str = ''"},{name:"auto_find_batch_size",val:": bool = False"},{name:"full_determinism",val:": bool = False"},{name:"torchdynamo",val:": typing.Optional[str] = None"},{name:"ray_scope",val:": typing.Optional[str] = 'last'"},{name:"ddp_timeout",val:": typing.Optional[int] = 1800"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTTrainingArguments.optim",description:`<strong>optim</strong> (<code>str</code> or <code>training_args.ORTOptimizerNames</code> or <code>transformers.training_args.OptimizerNames</code>, <em>optional</em>, defaults to <code>&quot;adamw_hf&quot;</code>) &#x2014;
The optimizer to use, including optimizers in Transformers: adamw_hf, adamw_torch, adamw_apex_fused, or adafactor. And optimizers implemented by ONNX Runtime: adamw_ort_fused.`,name:"optim"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/training_args.py#L56"}}),We=new ba({}),Xe=new N({props:{name:"class optimum.onnxruntime.ORTSeq2SeqTrainingArguments",anchor:"optimum.onnxruntime.ORTSeq2SeqTrainingArguments",parameters:[{name:"output_dir",val:": str"},{name:"overwrite_output_dir",val:": bool = False"},{name:"do_train",val:": bool = False"},{name:"do_eval",val:": bool = False"},{name:"do_predict",val:": bool = False"},{name:"evaluation_strategy",val:": typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'no'"},{name:"prediction_loss_only",val:": bool = False"},{name:"per_device_train_batch_size",val:": int = 8"},{name:"per_device_eval_batch_size",val:": int = 8"},{name:"per_gpu_train_batch_size",val:": typing.Optional[int] = None"},{name:"per_gpu_eval_batch_size",val:": typing.Optional[int] = None"},{name:"gradient_accumulation_steps",val:": int = 1"},{name:"eval_accumulation_steps",val:": typing.Optional[int] = None"},{name:"eval_delay",val:": typing.Optional[float] = 0"},{name:"learning_rate",val:": float = 5e-05"},{name:"weight_decay",val:": float = 0.0"},{name:"adam_beta1",val:": float = 0.9"},{name:"adam_beta2",val:": float = 0.999"},{name:"adam_epsilon",val:": float = 1e-08"},{name:"max_grad_norm",val:": float = 1.0"},{name:"num_train_epochs",val:": float = 3.0"},{name:"max_steps",val:": int = -1"},{name:"lr_scheduler_type",val:": typing.Union[transformers.trainer_utils.SchedulerType, str] = 'linear'"},{name:"warmup_ratio",val:": float = 0.0"},{name:"warmup_steps",val:": int = 0"},{name:"log_level",val:": typing.Optional[str] = 'passive'"},{name:"log_level_replica",val:": typing.Optional[str] = 'passive'"},{name:"log_on_each_node",val:": bool = True"},{name:"logging_dir",val:": typing.Optional[str] = None"},{name:"logging_strategy",val:": typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps'"},{name:"logging_first_step",val:": bool = False"},{name:"logging_steps",val:": int = 500"},{name:"logging_nan_inf_filter",val:": bool = True"},{name:"save_strategy",val:": typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps'"},{name:"save_steps",val:": int = 500"},{name:"save_total_limit",val:": typing.Optional[int] = None"},{name:"save_on_each_node",val:": bool = False"},{name:"no_cuda",val:": bool = False"},{name:"use_mps_device",val:": bool = False"},{name:"seed",val:": int = 42"},{name:"data_seed",val:": typing.Optional[int] = None"},{name:"jit_mode_eval",val:": bool = False"},{name:"use_ipex",val:": bool = False"},{name:"bf16",val:": bool = False"},{name:"fp16",val:": bool = False"},{name:"fp16_opt_level",val:": str = 'O1'"},{name:"half_precision_backend",val:": str = 'auto'"},{name:"bf16_full_eval",val:": bool = False"},{name:"fp16_full_eval",val:": bool = False"},{name:"tf32",val:": typing.Optional[bool] = None"},{name:"local_rank",val:": int = -1"},{name:"xpu_backend",val:": typing.Optional[str] = None"},{name:"tpu_num_cores",val:": typing.Optional[int] = None"},{name:"tpu_metrics_debug",val:": bool = False"},{name:"debug",val:": str = ''"},{name:"dataloader_drop_last",val:": bool = False"},{name:"eval_steps",val:": typing.Optional[int] = None"},{name:"dataloader_num_workers",val:": int = 0"},{name:"past_index",val:": int = -1"},{name:"run_name",val:": typing.Optional[str] = None"},{name:"disable_tqdm",val:": typing.Optional[bool] = None"},{name:"remove_unused_columns",val:": typing.Optional[bool] = True"},{name:"label_names",val:": typing.Optional[typing.List[str]] = None"},{name:"load_best_model_at_end",val:": typing.Optional[bool] = False"},{name:"metric_for_best_model",val:": typing.Optional[str] = None"},{name:"greater_is_better",val:": typing.Optional[bool] = None"},{name:"ignore_data_skip",val:": bool = False"},{name:"sharded_ddp",val:": str = ''"},{name:"fsdp",val:": str = ''"},{name:"fsdp_min_num_params",val:": int = 0"},{name:"fsdp_transformer_layer_cls_to_wrap",val:": typing.Optional[str] = None"},{name:"deepspeed",val:": typing.Optional[str] = None"},{name:"label_smoothing_factor",val:": float = 0.0"},{name:"optim",val:": typing.Optional[str] = 'adamw_hf'"},{name:"adafactor",val:": bool = False"},{name:"group_by_length",val:": bool = False"},{name:"length_column_name",val:": typing.Optional[str] = 'length'"},{name:"report_to",val:": typing.Optional[typing.List[str]] = None"},{name:"ddp_find_unused_parameters",val:": typing.Optional[bool] = None"},{name:"ddp_bucket_cap_mb",val:": typing.Optional[int] = None"},{name:"dataloader_pin_memory",val:": bool = True"},{name:"skip_memory_metrics",val:": bool = True"},{name:"use_legacy_prediction_loop",val:": bool = False"},{name:"push_to_hub",val:": bool = False"},{name:"resume_from_checkpoint",val:": typing.Optional[str] = None"},{name:"hub_model_id",val:": typing.Optional[str] = None"},{name:"hub_strategy",val:": typing.Union[transformers.trainer_utils.HubStrategy, str] = 'every_save'"},{name:"hub_token",val:": typing.Optional[str] = None"},{name:"hub_private_repo",val:": bool = False"},{name:"gradient_checkpointing",val:": bool = False"},{name:"include_inputs_for_metrics",val:": bool = False"},{name:"fp16_backend",val:": str = 'auto'"},{name:"push_to_hub_model_id",val:": typing.Optional[str] = None"},{name:"push_to_hub_organization",val:": typing.Optional[str] = None"},{name:"push_to_hub_token",val:": typing.Optional[str] = None"},{name:"mp_parameters",val:": str = ''"},{name:"auto_find_batch_size",val:": bool = False"},{name:"full_determinism",val:": bool = False"},{name:"torchdynamo",val:": typing.Optional[str] = None"},{name:"ray_scope",val:": typing.Optional[str] = 'last'"},{name:"ddp_timeout",val:": typing.Optional[int] = 1800"},{name:"sortish_sampler",val:": bool = False"},{name:"predict_with_generate",val:": bool = False"},{name:"generation_max_length",val:": typing.Optional[int] = None"},{name:"generation_num_beams",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTSeq2SeqTrainingArguments.optim",description:`<strong>optim</strong> (<code>str</code> or <code>training_args.ORTOptimizerNames</code> or <code>transformers.training_args.OptimizerNames</code>, <em>optional</em>, defaults to <code>&quot;adamw_hf&quot;</code>) &#x2014;
The optimizer to use, including optimizers in Transformers: adamw_hf, adamw_torch, adamw_apex_fused, or adafactor. And optimizers implemented by ONNX Runtime: adamw_ort_fused.`,name:"optim"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/training_args_seq2seq.py#L25"}}),{c(){h=o("meta"),D=s(),T=o("h1"),x=o("a"),mt=o("span"),_(ge.$$.fragment),Ha=s(),dt=o("span"),Ga=a("Trainer"),ya=s(),M=o("h2"),ne=o("a"),pt=o("span"),_(fe.$$.fragment),Ba=s(),ct=o("span"),ja=a("ORTTrainer"),Ta=s(),p=o("div"),_(ve.$$.fragment),Ja=s(),ut=o("p"),Ya=a("ORTTrainer is a simple but feature-complete training and eval loop for ONNX Runtime, optimized for \u{1F917} Transformers."),Ka=s(),ht=o("p"),Qa=a("Important attributes:"),Za=s(),R=o("ul"),oe=o("li"),_t=o("strong"),en=a("model"),tn=a(" \u2014 Always points to the core model. If using a transformers model, it will be a "),be=o("a"),an=a("PreTrainedModel"),nn=a(`
subclass.`),on=s(),w=o("li"),gt=o("strong"),rn=a("model_wrapped"),ln=a(` \u2014 Always points to the most external model in case one or more other modules wrap the
original model. This is the model that should be used for the forward pass. For example, under `),ft=o("code"),sn=a("DeepSpeed"),mn=a(`,
the inner model is first wrapped in `),vt=o("code"),dn=a("ORTModule"),pn=a(" and then in "),bt=o("code"),cn=a("DeepSpeed"),un=a(` and then again in
`),yt=o("code"),hn=a("torch.nn.DistributedDataParallel"),_n=a(". If the inner model hasn\u2019t been wrapped, then "),Tt=o("code"),gn=a("self.model_wrapped"),fn=a(` is the
same as `),Ot=o("code"),vn=a("self.model"),bn=a("."),yn=s(),Be=o("li"),xt=o("strong"),Tn=a("is_model_parallel"),On=a(` \u2014 Whether or not a model has been switched to a model parallel mode (different from
data parallelism, this means some of the model layers are split on different GPUs).`),xn=s(),S=o("li"),wt=o("strong"),wn=a("place_model_on_device"),$n=a(` \u2014 Whether or not to automatically place the model on the device - it will be set
to `),$t=o("code"),kn=a("False"),En=a(` if model parallel or deepspeed is used, or if the default
`),kt=o("code"),Nn=a("ORTTrainingArguments.place_model_on_device"),Rn=a(" is overridden to return "),Et=o("code"),Dn=a("False"),Sn=a(" ."),qn=s(),q=o("li"),Nt=o("strong"),An=a("is_in_train"),Pn=a(" \u2014 Whether or not a model is currently running "),Rt=o("code"),Fn=a("train"),zn=a(" (e.g. when "),Dt=o("code"),In=a("evaluate"),Cn=a(` is called while
in `),St=o("code"),Ln=a("train"),Mn=a(")"),Un=s(),re=o("div"),_(ye.$$.fragment),Wn=s(),qt=o("p"),Vn=a(`How the loss is computed by ORTTrainer. By default, all models return the loss in the first element.
Subclass and override for custom behavior.`),Xn=s(),z=o("div"),_(Te.$$.fragment),Hn=s(),At=o("p"),Gn=a("Setup the optimizer."),Bn=s(),Oe=o("p"),jn=a(`We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the
ORTTrainer\u2019s init through `),Pt=o("code"),Jn=a("optimizers"),Yn=a(", or subclass and override this method in a subclass."),Kn=s(),ie=o("div"),_(xe.$$.fragment),Qn=s(),Ft=o("p"),Zn=a("Run evaluation with ONNX Runtime or PyTorch backend and returns metrics."),eo=s(),I=o("div"),_(we.$$.fragment),to=s(),U=o("p"),ao=a("Prediction/evaluation loop, shared by "),zt=o("code"),no=a("ORTTrainer.evaluate()"),oo=a(" and "),It=o("code"),ro=a("ORTTrainer.predict()"),io=a("."),lo=s(),Ct=o("p"),so=a("Works both with or without labels."),mo=s(),le=o("div"),_($e.$$.fragment),po=s(),ke=o("p"),co=a("Returns the optimizer class and optimizer parameters implemented in ONNX Runtime based on "),Lt=o("code"),uo=a("ORTTrainingArguments"),ho=a("."),_o=s(),$=o("div"),_(Ee.$$.fragment),go=s(),Mt=o("p"),fo=a("Run prediction and returns predictions and potential metrics."),vo=s(),Ne=o("p"),bo=a(`Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method
will also return metrics, like in `),Ut=o("code"),yo=a("evaluate()"),To=a("."),Oo=s(),_(se.$$.fragment),xo=s(),Re=o("p"),wo=a("Returns: "),Wt=o("em"),$o=a("NamedTuple"),ko=a(" A namedtuple with the following keys:"),Eo=s(),W=o("ul"),V=o("li"),No=a("predictions ("),Vt=o("code"),Ro=a("np.ndarray"),Do=a("): The predictions on "),Xt=o("code"),So=a("test_dataset"),qo=a("."),Ao=s(),X=o("li"),Po=a("label_ids ("),Ht=o("code"),Fo=a("np.ndarray"),zo=a(", "),Gt=o("em"),Io=a("optional"),Co=a("): The labels (if the dataset contained some)."),Lo=s(),H=o("li"),Mo=a("metrics ("),Bt=o("code"),Uo=a("Dict[str, float]"),Wo=a(", "),jt=o("em"),Vo=a("optional"),Xo=a(`): The potential dictionary of metrics (if the dataset contained
labels).`),Ho=s(),C=o("div"),_(De.$$.fragment),Go=s(),G=o("p"),Bo=a("Prediction/evaluation loop, shared by "),Jt=o("code"),jo=a("ORTTrainer.evaluate()"),Jo=a(" and "),Yt=o("code"),Yo=a("ORTTrainer.predict()"),Ko=a("."),Qo=s(),Kt=o("p"),Zo=a("Works both with or without labels."),er=s(),me=o("div"),_(Se.$$.fragment),tr=s(),B=o("p"),ar=a("Perform an evaluation step on "),Qt=o("code"),nr=a("model"),or=a(" using "),Zt=o("code"),rr=a("inputs"),ir=a("."),lr=s(),de=o("div"),_(qe.$$.fragment),sr=s(),ea=o("p"),mr=a("Main entry point for training with ONNX Runtime accelerator."),Oa=s(),j=o("h2"),pe=o("a"),ta=o("span"),_(Ae.$$.fragment),dr=s(),aa=o("span"),pr=a("ORTSeq2SeqTrainer"),xa=s(),F=o("div"),_(Pe.$$.fragment),cr=s(),ce=o("div"),_(Fe.$$.fragment),ur=s(),na=o("p"),hr=a("Run evaluation with ONNX Runtime or PyTorch backend and returns metrics."),_r=s(),k=o("div"),_(ze.$$.fragment),gr=s(),oa=o("p"),fr=a("Run prediction and returns predictions and potential metrics."),vr=s(),Ie=o("p"),br=a(`Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method
will also return metrics, like in `),ra=o("code"),yr=a("evaluate()"),Tr=a("."),Or=s(),_(ue.$$.fragment),xr=s(),Ce=o("p"),wr=a("Returns: "),ia=o("em"),$r=a("NamedTuple"),kr=a(" A namedtuple with the following keys:"),Er=s(),J=o("ul"),Y=o("li"),Nr=a("predictions ("),la=o("code"),Rr=a("np.ndarray"),Dr=a("): The predictions on "),sa=o("code"),Sr=a("test_dataset"),qr=a("."),Ar=s(),K=o("li"),Pr=a("label_ids ("),ma=o("code"),Fr=a("np.ndarray"),zr=a(", "),da=o("em"),Ir=a("optional"),Cr=a("): The labels (if the dataset contained some)."),Lr=s(),Q=o("li"),Mr=a("metrics ("),pa=o("code"),Ur=a("Dict[str, float]"),Wr=a(", "),ca=o("em"),Vr=a("optional"),Xr=a(`): The potential dictionary of metrics (if the dataset contained
labels).`),wa=s(),Z=o("h2"),he=o("a"),ua=o("span"),_(Le.$$.fragment),Hr=s(),ha=o("span"),Gr=a("ORTTrainingArguments"),$a=s(),Me=o("div"),_(Ue.$$.fragment),ka=s(),ee=o("h2"),_e=o("a"),_a=o("span"),_(We.$$.fragment),Br=s(),ga=o("span"),jr=a("ORTSeq2SeqTrainingArguments"),Ea=s(),Ve=o("div"),_(Xe.$$.fragment),this.h()},l(l){const c=_l('[data-svelte="svelte-1phssyn"]',document.head);h=r(c,"META",{name:!0,content:!0}),c.forEach(t),D=m(l),T=r(l,"H1",{class:!0});var He=i(T);x=r(He,"A",{id:!0,class:!0,href:!0});var fa=i(x);mt=r(fa,"SPAN",{});var Yr=i(mt);g(ge.$$.fragment,Yr),Yr.forEach(t),fa.forEach(t),Ha=m(He),dt=r(He,"SPAN",{});var Kr=i(dt);Ga=n(Kr,"Trainer"),Kr.forEach(t),He.forEach(t),ya=m(l),M=r(l,"H2",{class:!0});var Ra=i(M);ne=r(Ra,"A",{id:!0,class:!0,href:!0});var Qr=i(ne);pt=r(Qr,"SPAN",{});var Zr=i(pt);g(fe.$$.fragment,Zr),Zr.forEach(t),Qr.forEach(t),Ba=m(Ra),ct=r(Ra,"SPAN",{});var ei=i(ct);ja=n(ei,"ORTTrainer"),ei.forEach(t),Ra.forEach(t),Ta=m(l),p=r(l,"DIV",{class:!0});var u=i(p);g(ve.$$.fragment,u),Ja=m(u),ut=r(u,"P",{});var ti=i(ut);Ya=n(ti,"ORTTrainer is a simple but feature-complete training and eval loop for ONNX Runtime, optimized for \u{1F917} Transformers."),ti.forEach(t),Ka=m(u),ht=r(u,"P",{});var ai=i(ht);Qa=n(ai,"Important attributes:"),ai.forEach(t),Za=m(u),R=r(u,"UL",{});var L=i(R);oe=r(L,"LI",{});var va=i(oe);_t=r(va,"STRONG",{});var ni=i(_t);en=n(ni,"model"),ni.forEach(t),tn=n(va," \u2014 Always points to the core model. If using a transformers model, it will be a "),be=r(va,"A",{href:!0,rel:!0});var oi=i(be);an=n(oi,"PreTrainedModel"),oi.forEach(t),nn=n(va,`
subclass.`),va.forEach(t),on=m(L),w=r(L,"LI",{});var E=i(w);gt=r(E,"STRONG",{});var ri=i(gt);rn=n(ri,"model_wrapped"),ri.forEach(t),ln=n(E,` \u2014 Always points to the most external model in case one or more other modules wrap the
original model. This is the model that should be used for the forward pass. For example, under `),ft=r(E,"CODE",{});var ii=i(ft);sn=n(ii,"DeepSpeed"),ii.forEach(t),mn=n(E,`,
the inner model is first wrapped in `),vt=r(E,"CODE",{});var li=i(vt);dn=n(li,"ORTModule"),li.forEach(t),pn=n(E," and then in "),bt=r(E,"CODE",{});var si=i(bt);cn=n(si,"DeepSpeed"),si.forEach(t),un=n(E,` and then again in
`),yt=r(E,"CODE",{});var mi=i(yt);hn=n(mi,"torch.nn.DistributedDataParallel"),mi.forEach(t),_n=n(E,". If the inner model hasn\u2019t been wrapped, then "),Tt=r(E,"CODE",{});var di=i(Tt);gn=n(di,"self.model_wrapped"),di.forEach(t),fn=n(E,` is the
same as `),Ot=r(E,"CODE",{});var pi=i(Ot);vn=n(pi,"self.model"),pi.forEach(t),bn=n(E,"."),E.forEach(t),yn=m(L),Be=r(L,"LI",{});var Jr=i(Be);xt=r(Jr,"STRONG",{});var ci=i(xt);Tn=n(ci,"is_model_parallel"),ci.forEach(t),On=n(Jr,` \u2014 Whether or not a model has been switched to a model parallel mode (different from
data parallelism, this means some of the model layers are split on different GPUs).`),Jr.forEach(t),xn=m(L),S=r(L,"LI",{});var te=i(S);wt=r(te,"STRONG",{});var ui=i(wt);wn=n(ui,"place_model_on_device"),ui.forEach(t),$n=n(te,` \u2014 Whether or not to automatically place the model on the device - it will be set
to `),$t=r(te,"CODE",{});var hi=i($t);kn=n(hi,"False"),hi.forEach(t),En=n(te,` if model parallel or deepspeed is used, or if the default
`),kt=r(te,"CODE",{});var _i=i(kt);Nn=n(_i,"ORTTrainingArguments.place_model_on_device"),_i.forEach(t),Rn=n(te," is overridden to return "),Et=r(te,"CODE",{});var gi=i(Et);Dn=n(gi,"False"),gi.forEach(t),Sn=n(te," ."),te.forEach(t),qn=m(L),q=r(L,"LI",{});var ae=i(q);Nt=r(ae,"STRONG",{});var fi=i(Nt);An=n(fi,"is_in_train"),fi.forEach(t),Pn=n(ae," \u2014 Whether or not a model is currently running "),Rt=r(ae,"CODE",{});var vi=i(Rt);Fn=n(vi,"train"),vi.forEach(t),zn=n(ae," (e.g. when "),Dt=r(ae,"CODE",{});var bi=i(Dt);In=n(bi,"evaluate"),bi.forEach(t),Cn=n(ae,` is called while
in `),St=r(ae,"CODE",{});var yi=i(St);Ln=n(yi,"train"),yi.forEach(t),Mn=n(ae,")"),ae.forEach(t),L.forEach(t),Un=m(u),re=r(u,"DIV",{class:!0});var Da=i(re);g(ye.$$.fragment,Da),Wn=m(Da),qt=r(Da,"P",{});var Ti=i(qt);Vn=n(Ti,`How the loss is computed by ORTTrainer. By default, all models return the loss in the first element.
Subclass and override for custom behavior.`),Ti.forEach(t),Da.forEach(t),Xn=m(u),z=r(u,"DIV",{class:!0});var je=i(z);g(Te.$$.fragment,je),Hn=m(je),At=r(je,"P",{});var Oi=i(At);Gn=n(Oi,"Setup the optimizer."),Oi.forEach(t),Bn=m(je),Oe=r(je,"P",{});var Sa=i(Oe);jn=n(Sa,`We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the
ORTTrainer\u2019s init through `),Pt=r(Sa,"CODE",{});var xi=i(Pt);Jn=n(xi,"optimizers"),xi.forEach(t),Yn=n(Sa,", or subclass and override this method in a subclass."),Sa.forEach(t),je.forEach(t),Kn=m(u),ie=r(u,"DIV",{class:!0});var qa=i(ie);g(xe.$$.fragment,qa),Qn=m(qa),Ft=r(qa,"P",{});var wi=i(Ft);Zn=n(wi,"Run evaluation with ONNX Runtime or PyTorch backend and returns metrics."),wi.forEach(t),qa.forEach(t),eo=m(u),I=r(u,"DIV",{class:!0});var Je=i(I);g(we.$$.fragment,Je),to=m(Je),U=r(Je,"P",{});var Ye=i(U);ao=n(Ye,"Prediction/evaluation loop, shared by "),zt=r(Ye,"CODE",{});var $i=i(zt);no=n($i,"ORTTrainer.evaluate()"),$i.forEach(t),oo=n(Ye," and "),It=r(Ye,"CODE",{});var ki=i(It);ro=n(ki,"ORTTrainer.predict()"),ki.forEach(t),io=n(Ye,"."),Ye.forEach(t),lo=m(Je),Ct=r(Je,"P",{});var Ei=i(Ct);so=n(Ei,"Works both with or without labels."),Ei.forEach(t),Je.forEach(t),mo=m(u),le=r(u,"DIV",{class:!0});var Aa=i(le);g($e.$$.fragment,Aa),po=m(Aa),ke=r(Aa,"P",{});var Pa=i(ke);co=n(Pa,"Returns the optimizer class and optimizer parameters implemented in ONNX Runtime based on "),Lt=r(Pa,"CODE",{});var Ni=i(Lt);uo=n(Ni,"ORTTrainingArguments"),Ni.forEach(t),ho=n(Pa,"."),Pa.forEach(t),Aa.forEach(t),_o=m(u),$=r(u,"DIV",{class:!0});var A=i($);g(Ee.$$.fragment,A),go=m(A),Mt=r(A,"P",{});var Ri=i(Mt);fo=n(Ri,"Run prediction and returns predictions and potential metrics."),Ri.forEach(t),vo=m(A),Ne=r(A,"P",{});var Fa=i(Ne);bo=n(Fa,`Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method
will also return metrics, like in `),Ut=r(Fa,"CODE",{});var Di=i(Ut);yo=n(Di,"evaluate()"),Di.forEach(t),To=n(Fa,"."),Fa.forEach(t),Oo=m(A),g(se.$$.fragment,A),xo=m(A),Re=r(A,"P",{});var za=i(Re);wo=n(za,"Returns: "),Wt=r(za,"EM",{});var Si=i(Wt);$o=n(Si,"NamedTuple"),Si.forEach(t),ko=n(za," A namedtuple with the following keys:"),za.forEach(t),Eo=m(A),W=r(A,"UL",{});var Ke=i(W);V=r(Ke,"LI",{});var Qe=i(V);No=n(Qe,"predictions ("),Vt=r(Qe,"CODE",{});var qi=i(Vt);Ro=n(qi,"np.ndarray"),qi.forEach(t),Do=n(Qe,"): The predictions on "),Xt=r(Qe,"CODE",{});var Ai=i(Xt);So=n(Ai,"test_dataset"),Ai.forEach(t),qo=n(Qe,"."),Qe.forEach(t),Ao=m(Ke),X=r(Ke,"LI",{});var Ze=i(X);Po=n(Ze,"label_ids ("),Ht=r(Ze,"CODE",{});var Pi=i(Ht);Fo=n(Pi,"np.ndarray"),Pi.forEach(t),zo=n(Ze,", "),Gt=r(Ze,"EM",{});var Fi=i(Gt);Io=n(Fi,"optional"),Fi.forEach(t),Co=n(Ze,"): The labels (if the dataset contained some)."),Ze.forEach(t),Lo=m(Ke),H=r(Ke,"LI",{});var et=i(H);Mo=n(et,"metrics ("),Bt=r(et,"CODE",{});var zi=i(Bt);Uo=n(zi,"Dict[str, float]"),zi.forEach(t),Wo=n(et,", "),jt=r(et,"EM",{});var Ii=i(jt);Vo=n(Ii,"optional"),Ii.forEach(t),Xo=n(et,`): The potential dictionary of metrics (if the dataset contained
labels).`),et.forEach(t),Ke.forEach(t),A.forEach(t),Ho=m(u),C=r(u,"DIV",{class:!0});var tt=i(C);g(De.$$.fragment,tt),Go=m(tt),G=r(tt,"P",{});var at=i(G);Bo=n(at,"Prediction/evaluation loop, shared by "),Jt=r(at,"CODE",{});var Ci=i(Jt);jo=n(Ci,"ORTTrainer.evaluate()"),Ci.forEach(t),Jo=n(at," and "),Yt=r(at,"CODE",{});var Li=i(Yt);Yo=n(Li,"ORTTrainer.predict()"),Li.forEach(t),Ko=n(at,"."),at.forEach(t),Qo=m(tt),Kt=r(tt,"P",{});var Mi=i(Kt);Zo=n(Mi,"Works both with or without labels."),Mi.forEach(t),tt.forEach(t),er=m(u),me=r(u,"DIV",{class:!0});var Ia=i(me);g(Se.$$.fragment,Ia),tr=m(Ia),B=r(Ia,"P",{});var nt=i(B);ar=n(nt,"Perform an evaluation step on "),Qt=r(nt,"CODE",{});var Ui=i(Qt);nr=n(Ui,"model"),Ui.forEach(t),or=n(nt," using "),Zt=r(nt,"CODE",{});var Wi=i(Zt);rr=n(Wi,"inputs"),Wi.forEach(t),ir=n(nt,"."),nt.forEach(t),Ia.forEach(t),lr=m(u),de=r(u,"DIV",{class:!0});var Ca=i(de);g(qe.$$.fragment,Ca),sr=m(Ca),ea=r(Ca,"P",{});var Vi=i(ea);mr=n(Vi,"Main entry point for training with ONNX Runtime accelerator."),Vi.forEach(t),Ca.forEach(t),u.forEach(t),Oa=m(l),j=r(l,"H2",{class:!0});var La=i(j);pe=r(La,"A",{id:!0,class:!0,href:!0});var Xi=i(pe);ta=r(Xi,"SPAN",{});var Hi=i(ta);g(Ae.$$.fragment,Hi),Hi.forEach(t),Xi.forEach(t),dr=m(La),aa=r(La,"SPAN",{});var Gi=i(aa);pr=n(Gi,"ORTSeq2SeqTrainer"),Gi.forEach(t),La.forEach(t),xa=m(l),F=r(l,"DIV",{class:!0});var ot=i(F);g(Pe.$$.fragment,ot),cr=m(ot),ce=r(ot,"DIV",{class:!0});var Ma=i(ce);g(Fe.$$.fragment,Ma),ur=m(Ma),na=r(Ma,"P",{});var Bi=i(na);hr=n(Bi,"Run evaluation with ONNX Runtime or PyTorch backend and returns metrics."),Bi.forEach(t),Ma.forEach(t),_r=m(ot),k=r(ot,"DIV",{class:!0});var P=i(k);g(ze.$$.fragment,P),gr=m(P),oa=r(P,"P",{});var ji=i(oa);fr=n(ji,"Run prediction and returns predictions and potential metrics."),ji.forEach(t),vr=m(P),Ie=r(P,"P",{});var Ua=i(Ie);br=n(Ua,`Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method
will also return metrics, like in `),ra=r(Ua,"CODE",{});var Ji=i(ra);yr=n(Ji,"evaluate()"),Ji.forEach(t),Tr=n(Ua,"."),Ua.forEach(t),Or=m(P),g(ue.$$.fragment,P),xr=m(P),Ce=r(P,"P",{});var Wa=i(Ce);wr=n(Wa,"Returns: "),ia=r(Wa,"EM",{});var Yi=i(ia);$r=n(Yi,"NamedTuple"),Yi.forEach(t),kr=n(Wa," A namedtuple with the following keys:"),Wa.forEach(t),Er=m(P),J=r(P,"UL",{});var rt=i(J);Y=r(rt,"LI",{});var it=i(Y);Nr=n(it,"predictions ("),la=r(it,"CODE",{});var Ki=i(la);Rr=n(Ki,"np.ndarray"),Ki.forEach(t),Dr=n(it,"): The predictions on "),sa=r(it,"CODE",{});var Qi=i(sa);Sr=n(Qi,"test_dataset"),Qi.forEach(t),qr=n(it,"."),it.forEach(t),Ar=m(rt),K=r(rt,"LI",{});var lt=i(K);Pr=n(lt,"label_ids ("),ma=r(lt,"CODE",{});var Zi=i(ma);Fr=n(Zi,"np.ndarray"),Zi.forEach(t),zr=n(lt,", "),da=r(lt,"EM",{});var el=i(da);Ir=n(el,"optional"),el.forEach(t),Cr=n(lt,"): The labels (if the dataset contained some)."),lt.forEach(t),Lr=m(rt),Q=r(rt,"LI",{});var st=i(Q);Mr=n(st,"metrics ("),pa=r(st,"CODE",{});var tl=i(pa);Ur=n(tl,"Dict[str, float]"),tl.forEach(t),Wr=n(st,", "),ca=r(st,"EM",{});var al=i(ca);Vr=n(al,"optional"),al.forEach(t),Xr=n(st,`): The potential dictionary of metrics (if the dataset contained
labels).`),st.forEach(t),rt.forEach(t),P.forEach(t),ot.forEach(t),wa=m(l),Z=r(l,"H2",{class:!0});var Va=i(Z);he=r(Va,"A",{id:!0,class:!0,href:!0});var nl=i(he);ua=r(nl,"SPAN",{});var ol=i(ua);g(Le.$$.fragment,ol),ol.forEach(t),nl.forEach(t),Hr=m(Va),ha=r(Va,"SPAN",{});var rl=i(ha);Gr=n(rl,"ORTTrainingArguments"),rl.forEach(t),Va.forEach(t),$a=m(l),Me=r(l,"DIV",{class:!0});var il=i(Me);g(Ue.$$.fragment,il),il.forEach(t),ka=m(l),ee=r(l,"H2",{class:!0});var Xa=i(ee);_e=r(Xa,"A",{id:!0,class:!0,href:!0});var ll=i(_e);_a=r(ll,"SPAN",{});var sl=i(_a);g(We.$$.fragment,sl),sl.forEach(t),ll.forEach(t),Br=m(Xa),ga=r(Xa,"SPAN",{});var ml=i(ga);jr=n(ml,"ORTSeq2SeqTrainingArguments"),ml.forEach(t),Xa.forEach(t),Ea=m(l),Ve=r(l,"DIV",{class:!0});var dl=i(Ve);g(Xe.$$.fragment,dl),dl.forEach(t),this.h()},h(){d(h,"name","hf:doc:metadata"),d(h,"content",JSON.stringify(yl)),d(x,"id","trainer"),d(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(x,"href","#trainer"),d(T,"class","relative group"),d(ne,"id","optimum.onnxruntime.ORTTrainer"),d(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ne,"href","#optimum.onnxruntime.ORTTrainer"),d(M,"class","relative group"),d(be,"href","https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),d(be,"rel","nofollow"),d(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(p,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(pe,"id","optimum.onnxruntime.ORTSeq2SeqTrainer"),d(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(pe,"href","#optimum.onnxruntime.ORTSeq2SeqTrainer"),d(j,"class","relative group"),d(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(he,"id","optimum.onnxruntime.ORTTrainingArguments"),d(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(he,"href","#optimum.onnxruntime.ORTTrainingArguments"),d(Z,"class","relative group"),d(Me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(_e,"id","optimum.onnxruntime.ORTSeq2SeqTrainingArguments"),d(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_e,"href","#optimum.onnxruntime.ORTSeq2SeqTrainingArguments"),d(ee,"class","relative group"),d(Ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(l,c){e(document.head,h),O(l,D,c),O(l,T,c),e(T,x),e(x,mt),f(ge,mt,null),e(T,Ha),e(T,dt),e(dt,Ga),O(l,ya,c),O(l,M,c),e(M,ne),e(ne,pt),f(fe,pt,null),e(M,Ba),e(M,ct),e(ct,ja),O(l,Ta,c),O(l,p,c),f(ve,p,null),e(p,Ja),e(p,ut),e(ut,Ya),e(p,Ka),e(p,ht),e(ht,Qa),e(p,Za),e(p,R),e(R,oe),e(oe,_t),e(_t,en),e(oe,tn),e(oe,be),e(be,an),e(oe,nn),e(R,on),e(R,w),e(w,gt),e(gt,rn),e(w,ln),e(w,ft),e(ft,sn),e(w,mn),e(w,vt),e(vt,dn),e(w,pn),e(w,bt),e(bt,cn),e(w,un),e(w,yt),e(yt,hn),e(w,_n),e(w,Tt),e(Tt,gn),e(w,fn),e(w,Ot),e(Ot,vn),e(w,bn),e(R,yn),e(R,Be),e(Be,xt),e(xt,Tn),e(Be,On),e(R,xn),e(R,S),e(S,wt),e(wt,wn),e(S,$n),e(S,$t),e($t,kn),e(S,En),e(S,kt),e(kt,Nn),e(S,Rn),e(S,Et),e(Et,Dn),e(S,Sn),e(R,qn),e(R,q),e(q,Nt),e(Nt,An),e(q,Pn),e(q,Rt),e(Rt,Fn),e(q,zn),e(q,Dt),e(Dt,In),e(q,Cn),e(q,St),e(St,Ln),e(q,Mn),e(p,Un),e(p,re),f(ye,re,null),e(re,Wn),e(re,qt),e(qt,Vn),e(p,Xn),e(p,z),f(Te,z,null),e(z,Hn),e(z,At),e(At,Gn),e(z,Bn),e(z,Oe),e(Oe,jn),e(Oe,Pt),e(Pt,Jn),e(Oe,Yn),e(p,Kn),e(p,ie),f(xe,ie,null),e(ie,Qn),e(ie,Ft),e(Ft,Zn),e(p,eo),e(p,I),f(we,I,null),e(I,to),e(I,U),e(U,ao),e(U,zt),e(zt,no),e(U,oo),e(U,It),e(It,ro),e(U,io),e(I,lo),e(I,Ct),e(Ct,so),e(p,mo),e(p,le),f($e,le,null),e(le,po),e(le,ke),e(ke,co),e(ke,Lt),e(Lt,uo),e(ke,ho),e(p,_o),e(p,$),f(Ee,$,null),e($,go),e($,Mt),e(Mt,fo),e($,vo),e($,Ne),e(Ne,bo),e(Ne,Ut),e(Ut,yo),e(Ne,To),e($,Oo),f(se,$,null),e($,xo),e($,Re),e(Re,wo),e(Re,Wt),e(Wt,$o),e(Re,ko),e($,Eo),e($,W),e(W,V),e(V,No),e(V,Vt),e(Vt,Ro),e(V,Do),e(V,Xt),e(Xt,So),e(V,qo),e(W,Ao),e(W,X),e(X,Po),e(X,Ht),e(Ht,Fo),e(X,zo),e(X,Gt),e(Gt,Io),e(X,Co),e(W,Lo),e(W,H),e(H,Mo),e(H,Bt),e(Bt,Uo),e(H,Wo),e(H,jt),e(jt,Vo),e(H,Xo),e(p,Ho),e(p,C),f(De,C,null),e(C,Go),e(C,G),e(G,Bo),e(G,Jt),e(Jt,jo),e(G,Jo),e(G,Yt),e(Yt,Yo),e(G,Ko),e(C,Qo),e(C,Kt),e(Kt,Zo),e(p,er),e(p,me),f(Se,me,null),e(me,tr),e(me,B),e(B,ar),e(B,Qt),e(Qt,nr),e(B,or),e(B,Zt),e(Zt,rr),e(B,ir),e(p,lr),e(p,de),f(qe,de,null),e(de,sr),e(de,ea),e(ea,mr),O(l,Oa,c),O(l,j,c),e(j,pe),e(pe,ta),f(Ae,ta,null),e(j,dr),e(j,aa),e(aa,pr),O(l,xa,c),O(l,F,c),f(Pe,F,null),e(F,cr),e(F,ce),f(Fe,ce,null),e(ce,ur),e(ce,na),e(na,hr),e(F,_r),e(F,k),f(ze,k,null),e(k,gr),e(k,oa),e(oa,fr),e(k,vr),e(k,Ie),e(Ie,br),e(Ie,ra),e(ra,yr),e(Ie,Tr),e(k,Or),f(ue,k,null),e(k,xr),e(k,Ce),e(Ce,wr),e(Ce,ia),e(ia,$r),e(Ce,kr),e(k,Er),e(k,J),e(J,Y),e(Y,Nr),e(Y,la),e(la,Rr),e(Y,Dr),e(Y,sa),e(sa,Sr),e(Y,qr),e(J,Ar),e(J,K),e(K,Pr),e(K,ma),e(ma,Fr),e(K,zr),e(K,da),e(da,Ir),e(K,Cr),e(J,Lr),e(J,Q),e(Q,Mr),e(Q,pa),e(pa,Ur),e(Q,Wr),e(Q,ca),e(ca,Vr),e(Q,Xr),O(l,wa,c),O(l,Z,c),e(Z,he),e(he,ua),f(Le,ua,null),e(Z,Hr),e(Z,ha),e(ha,Gr),O(l,$a,c),O(l,Me,c),f(Ue,Me,null),O(l,ka,c),O(l,ee,c),e(ee,_e),e(_e,_a),f(We,_a,null),e(ee,Br),e(ee,ga),e(ga,jr),O(l,Ea,c),O(l,Ve,c),f(Xe,Ve,null),Na=!0},p(l,[c]){const He={};c&2&&(He.$$scope={dirty:c,ctx:l}),se.$set(He);const fa={};c&2&&(fa.$$scope={dirty:c,ctx:l}),ue.$set(fa)},i(l){Na||(v(ge.$$.fragment,l),v(fe.$$.fragment,l),v(ve.$$.fragment,l),v(ye.$$.fragment,l),v(Te.$$.fragment,l),v(xe.$$.fragment,l),v(we.$$.fragment,l),v($e.$$.fragment,l),v(Ee.$$.fragment,l),v(se.$$.fragment,l),v(De.$$.fragment,l),v(Se.$$.fragment,l),v(qe.$$.fragment,l),v(Ae.$$.fragment,l),v(Pe.$$.fragment,l),v(Fe.$$.fragment,l),v(ze.$$.fragment,l),v(ue.$$.fragment,l),v(Le.$$.fragment,l),v(Ue.$$.fragment,l),v(We.$$.fragment,l),v(Xe.$$.fragment,l),Na=!0)},o(l){b(ge.$$.fragment,l),b(fe.$$.fragment,l),b(ve.$$.fragment,l),b(ye.$$.fragment,l),b(Te.$$.fragment,l),b(xe.$$.fragment,l),b(we.$$.fragment,l),b($e.$$.fragment,l),b(Ee.$$.fragment,l),b(se.$$.fragment,l),b(De.$$.fragment,l),b(Se.$$.fragment,l),b(qe.$$.fragment,l),b(Ae.$$.fragment,l),b(Pe.$$.fragment,l),b(Fe.$$.fragment,l),b(ze.$$.fragment,l),b(ue.$$.fragment,l),b(Le.$$.fragment,l),b(Ue.$$.fragment,l),b(We.$$.fragment,l),b(Xe.$$.fragment,l),Na=!1},d(l){t(h),l&&t(D),l&&t(T),y(ge),l&&t(ya),l&&t(M),y(fe),l&&t(Ta),l&&t(p),y(ve),y(ye),y(Te),y(xe),y(we),y($e),y(Ee),y(se),y(De),y(Se),y(qe),l&&t(Oa),l&&t(j),y(Ae),l&&t(xa),l&&t(F),y(Pe),y(Fe),y(ze),y(ue),l&&t(wa),l&&t(Z),y(Le),l&&t($a),l&&t(Me),y(Ue),l&&t(ka),l&&t(ee),y(We),l&&t(Ea),l&&t(Ve),y(Xe)}}}const yl={local:"trainer",sections:[{local:"optimum.onnxruntime.ORTTrainer",title:"ORTTrainer"},{local:"optimum.onnxruntime.ORTSeq2SeqTrainer",title:"ORTSeq2SeqTrainer"},{local:"optimum.onnxruntime.ORTTrainingArguments",title:"ORTTrainingArguments"},{local:"optimum.onnxruntime.ORTSeq2SeqTrainingArguments",title:"ORTSeq2SeqTrainingArguments"}],title:"Trainer"};function Tl(Ge){return gl(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class kl extends cl{constructor(h){super();ul(this,h,Tl,bl,hl,{})}}export{kl as default,yl as metadata};
