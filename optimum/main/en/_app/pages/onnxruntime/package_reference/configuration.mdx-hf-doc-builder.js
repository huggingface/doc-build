import{S as On,i as Cn,s as yn,e as n,k as l,w as g,t as r,M as wn,c as i,d as o,m as p,a,x as f,h as s,b as m,G as t,g as u,y as h,L as $n,q as _,o as b,B as v,v as Nn}from"../../../chunks/vendor-hf-doc-builder.js";import{D as C}from"../../../chunks/Docstring-hf-doc-builder.js";import{I as Vt}from"../../../chunks/IconCopyLink-hf-doc-builder.js";function kn(Wo){let y,Bt,w,F,Ot,V,Ne,Ct,ke,Ht,gt,qe,Jt,D,yt,$,Te,ft,Qe,Ee,ht,Ae,Fe,De,wt,x,Re,_t,Pe,Le,bt,We,Xe,vt,Ie,Se,jt,N,R,$t,B,Me,Nt,Ge,Kt,O,H,Ue,kt,Ve,Be,J,qt,He,Je,Tt,je,Yt,c,j,Ke,K,Ye,Qt,Ze,to,eo,P,Y,oo,Z,no,Et,io,ao,ro,L,tt,so,et,mo,At,lo,po,uo,W,ot,co,nt,go,Ft,fo,ho,_o,X,it,bo,at,vo,Dt,zo,xo,Oo,I,rt,Co,st,yo,Rt,wo,$o,Zt,k,S,Pt,mt,No,Lt,ko,te,q,lt,qo,Wt,To,ee,T,M,Xt,pt,Qo,It,Eo,oe,Q,dt,Ao,St,Fo,ne,E,G,Mt,ut,Do,Gt,Ro,ie,A,ct,Po,Ut,Lo,ae;return V=new Vt({}),B=new Vt({}),H=new C({props:{name:"class optimum.onnxruntime.OptimizationConfig",anchor:"optimum.onnxruntime.OptimizationConfig",parameters:[{name:"optimization_level",val:": int = 1"},{name:"optimize_for_gpu",val:": bool = False"},{name:"fp16",val:": bool = False"},{name:"optimize_with_onnxruntime_only",val:": typing.Optional[bool] = None"},{name:"enable_transformers_specific_optimizations",val:": bool = True"},{name:"disable_gelu",val:": typing.Optional[bool] = None"},{name:"disable_gelu_fusion",val:": bool = False"},{name:"disable_layer_norm",val:": typing.Optional[bool] = None"},{name:"disable_layer_norm_fusion",val:": bool = False"},{name:"disable_attention",val:": typing.Optional[bool] = None"},{name:"disable_attention_fusion",val:": bool = False"},{name:"disable_skip_layer_norm",val:": typing.Optional[bool] = None"},{name:"disable_skip_layer_norm_fusion",val:": bool = False"},{name:"disable_bias_skip_layer_norm",val:": typing.Optional[bool] = None"},{name:"disable_bias_skip_layer_norm_fusion",val:": bool = False"},{name:"disable_bias_gelu",val:": typing.Optional[bool] = None"},{name:"disable_bias_gelu_fusion",val:": bool = False"},{name:"disable_embed_layer_norm",val:": typing.Optional[bool] = None"},{name:"disable_embed_layer_norm_fusion",val:": bool = True"},{name:"enable_gelu_approximation",val:": bool = False"},{name:"use_mask_index",val:": bool = False"},{name:"no_attention_mask",val:": bool = False"},{name:"disable_shape_inference",val:": bool = False"}],parametersDescription:[{anchor:"optimum.onnxruntime.OptimizationConfig.optimization_level",description:`<strong>optimization_level</strong> (<code>int</code>, defaults to 1) &#x2014;
Optimization level performed by ONNX Runtime of the loaded graph.
Supported optimization level are 0, 1, 2 and 99.<ul>
<li>0: will disable all optimizations</li>
<li>1: will enable basic optimizations</li>
<li>2: will enable basic and extended optimizations, including complex node fusions applied to the nodes
assigned to the CPU or CUDA execution provider, making the resulting optimized graph hardware dependent</li>
<li>99: will enable all available optimizations including layout optimizations</li>
</ul>`,name:"optimization_level"},{anchor:"optimum.onnxruntime.OptimizationConfig.optimize_for_gpu",description:`<strong>optimize_for_gpu</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to optimize the model for GPU inference.
The optimized graph might contain operators for GPU or CPU only when <code>optimization_level</code> &gt; 1.`,name:"optimize_for_gpu"},{anchor:"optimum.onnxruntime.OptimizationConfig.fp16",description:`<strong>fp16</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether all weights and nodes should be converted from float32 to float16.`,name:"fp16"},{anchor:"optimum.onnxruntime.OptimizationConfig.enable_transformers_specific_optimizations",description:`<strong>enable_transformers_specific_optimizations</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to only use <code>transformers</code> specific optimizations on top of ONNX Runtime general optimizations.`,name:"enable_transformers_specific_optimizations"},{anchor:"optimum.onnxruntime.OptimizationConfig.disable_gelu_fusion",description:`<strong>disable_gelu_fusion</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to disable the Gelu fusion.`,name:"disable_gelu_fusion"},{anchor:"optimum.onnxruntime.OptimizationConfig.disable_layer_norm_fusion",description:`<strong>disable_layer_norm_fusion</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to disable Layer Normalization fusion.`,name:"disable_layer_norm_fusion"},{anchor:"optimum.onnxruntime.OptimizationConfig.disable_attention_fusion",description:`<strong>disable_attention_fusion</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to disable Attention fusion.`,name:"disable_attention_fusion"},{anchor:"optimum.onnxruntime.OptimizationConfig.disable_skip_layer_norm_fusion",description:`<strong>disable_skip_layer_norm_fusion</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to disable SkipLayerNormalization fusion.`,name:"disable_skip_layer_norm_fusion"},{anchor:"optimum.onnxruntime.OptimizationConfig.disable_bias_skip_layer_norm_fusion",description:`<strong>disable_bias_skip_layer_norm_fusion</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to disable Add Bias and SkipLayerNormalization fusion.`,name:"disable_bias_skip_layer_norm_fusion"},{anchor:"optimum.onnxruntime.OptimizationConfig.disable_bias_gelu_fusion",description:`<strong>disable_bias_gelu_fusion</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to disable Add Bias and Gelu / FastGelu fusion.`,name:"disable_bias_gelu_fusion"},{anchor:"optimum.onnxruntime.OptimizationConfig.disable_embed_layer_norm_fusion",description:`<strong>disable_embed_layer_norm_fusion</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to disable EmbedLayerNormalization fusion.
The default value is set to <code>True</code> since this fusion is incompatible with ONNX Runtime quantization.`,name:"disable_embed_layer_norm_fusion"},{anchor:"optimum.onnxruntime.OptimizationConfig.enable_gelu_approximation",description:`<strong>enable_gelu_approximation</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to enable Gelu / BiasGelu to FastGelu conversion.
The default value is set to <code>False</code> since this approximation might slightly impact the model&#x2019;s accuracy.`,name:"enable_gelu_approximation"},{anchor:"optimum.onnxruntime.OptimizationConfig.use_mask_index",description:`<strong>use_mask_index</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use mask index instead of raw attention mask in the attention operator.`,name:"use_mask_index"},{anchor:"optimum.onnxruntime.OptimizationConfig.no_attention_mask",description:`<strong>no_attention_mask</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to not use attention masks. Only works for bert model type.`,name:"no_attention_mask"},{anchor:"optimum.onnxruntime.OptimizationConfig.disable_shape_inference",description:`<strong>disable_shape_inference</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to disable symbolic shape inference.
The default value is set to <code>False</code> but symbolic shape inference might cause issues sometimes.`,name:"disable_shape_inference"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/configuration.py#L608"}}),j=new C({props:{name:"class optimum.onnxruntime.AutoOptimizationConfig",anchor:"optimum.onnxruntime.AutoOptimizationConfig",parameters:[],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/configuration.py#L748"}}),Y=new C({props:{name:"O1",anchor:"optimum.onnxruntime.AutoOptimizationConfig.O1",parameters:[{name:"for_gpu",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.onnxruntime.AutoOptimizationConfig.O1.for_gpu",description:`<strong>for_gpu</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the model to optimize will run on GPU, some optimizations depends on the hardware the model
will run on. Only needed for optimization_level &gt; 1.`,name:"for_gpu"},{anchor:"optimum.onnxruntime.AutoOptimizationConfig.O1.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
Arguments to provide to the <code>~OptimizationConfig</code> constructor.`,name:"kwargs"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/configuration.py#L802",returnDescription:`
<p>The <code>OptimizationConfig</code> corresponding to the O1 optimization level.</p>
`,returnType:`
<p><code>OptimizationConfig</code></p>
`}}),tt=new C({props:{name:"O2",anchor:"optimum.onnxruntime.AutoOptimizationConfig.O2",parameters:[{name:"for_gpu",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.onnxruntime.AutoOptimizationConfig.O2.for_gpu",description:`<strong>for_gpu</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the model to optimize will run on GPU, some optimizations depends on the hardware the model
will run on. Only needed for optimization_level &gt; 1.`,name:"for_gpu"},{anchor:"optimum.onnxruntime.AutoOptimizationConfig.O2.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
Arguments to provide to the <code>~OptimizationConfig</code> constructor.`,name:"kwargs"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/configuration.py#L819",returnDescription:`
<p>The <code>OptimizationConfig</code> corresponding to the O2 optimization level.</p>
`,returnType:`
<p><code>OptimizationConfig</code></p>
`}}),ot=new C({props:{name:"O3",anchor:"optimum.onnxruntime.AutoOptimizationConfig.O3",parameters:[{name:"for_gpu",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.onnxruntime.AutoOptimizationConfig.O3.for_gpu",description:`<strong>for_gpu</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the model to optimize will run on GPU, some optimizations depends on the hardware the model
will run on. Only needed for optimization_level &gt; 1.`,name:"for_gpu"},{anchor:"optimum.onnxruntime.AutoOptimizationConfig.O3.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
Arguments to provide to the <code>~OptimizationConfig</code> constructor.`,name:"kwargs"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/configuration.py#L836",returnDescription:`
<p>The <code>OptimizationConfig</code> corresponding to the O3 optimization level.</p>
`,returnType:`
<p><code>OptimizationConfig</code></p>
`}}),it=new C({props:{name:"O4",anchor:"optimum.onnxruntime.AutoOptimizationConfig.O4",parameters:[{name:"for_gpu",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.onnxruntime.AutoOptimizationConfig.O4.for_gpu",description:`<strong>for_gpu</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the model to optimize will run on GPU, some optimizations depends on the hardware the model
will run on. Only needed for optimization_level &gt; 1.`,name:"for_gpu"},{anchor:"optimum.onnxruntime.AutoOptimizationConfig.O4.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
Arguments to provide to the <code>~OptimizationConfig</code> constructor.`,name:"kwargs"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/configuration.py#L853",returnDescription:`
<p>The <code>OptimizationConfig</code> corresponding to the O4 optimization level.</p>
`,returnType:`
<p><code>OptimizationConfig</code></p>
`}}),rt=new C({props:{name:"with_optimization_level",anchor:"optimum.onnxruntime.AutoOptimizationConfig.with_optimization_level",parameters:[{name:"optimization_level",val:": str"},{name:"for_gpu",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.onnxruntime.AutoOptimizationConfig.with_optimization_level.optimization_level",description:`<strong>optimization_level</strong> (<code>str</code>) &#x2014;
The optimization level, the following values are allowed:<ul>
<li>O1: Basic general optimizations</li>
<li>O2: Basic and extended general optimizations, transformers-specific fusions.</li>
<li>O3: Same as O2 with Fast Gelu approximation.</li>
<li>O4: Same as O3 with mixed precision.</li>
</ul>`,name:"optimization_level"},{anchor:"optimum.onnxruntime.AutoOptimizationConfig.with_optimization_level.for_gpu",description:`<strong>for_gpu</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the model to optimize will run on GPU, some optimizations depends on the hardware the model
will run on. Only needed for optimization_level &gt; 1.`,name:"for_gpu"},{anchor:"optimum.onnxruntime.AutoOptimizationConfig.with_optimization_level.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
Arguments to provide to the <code>~OptimizationConfig</code> constructor.`,name:"kwargs"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/configuration.py#L775",returnDescription:`
<p>The <code>OptimizationConfig</code> corresponding to the requested optimization level.</p>
`,returnType:`
<p><code>OptimizationConfig</code></p>
`}}),mt=new Vt({}),lt=new C({props:{name:"class optimum.onnxruntime.configuration.QuantizationConfig",anchor:"optimum.onnxruntime.configuration.QuantizationConfig",parameters:[{name:"is_static",val:": bool"},{name:"format",val:": QuantFormat"},{name:"mode",val:": QuantizationMode = <QuantizationMode.QLinearOps: 1>"},{name:"activations_dtype",val:": QuantType = <QuantType.QUInt8: 1>"},{name:"activations_symmetric",val:": bool = False"},{name:"weights_dtype",val:": QuantType = <QuantType.QInt8: 0>"},{name:"weights_symmetric",val:": bool = True"},{name:"per_channel",val:": bool = False"},{name:"reduce_range",val:": bool = False"},{name:"nodes_to_quantize",val:": typing.List[str] = <factory>"},{name:"nodes_to_exclude",val:": typing.List[str] = <factory>"},{name:"operators_to_quantize",val:": typing.List[str] = <factory>"},{name:"qdq_add_pair_to_weight",val:": bool = False"},{name:"qdq_dedicated_pair",val:": bool = False"},{name:"qdq_op_type_per_channel_support_to_axis",val:": typing.Dict[str, int] = <factory>"}],parametersDescription:[{anchor:"optimum.onnxruntime.configuration.QuantizationConfig.is_static",description:`<strong>is_static</strong> (<code>bool</code>) &#x2014;
Whether to apply static quantization or dynamic quantization.`,name:"is_static"},{anchor:"optimum.onnxruntime.configuration.QuantizationConfig.format",description:`<strong>format</strong> (<code>QuantFormat</code>) &#x2014;
Targeted ONNX Runtime quantization representation format.
For the Operator Oriented (QOperator) format, all the quantized operators have their own ONNX definitions.
For the Tensor Oriented (QDQ) format, the model is quantized by inserting QuantizeLinear / DeQuantizeLinear
operators.`,name:"format"},{anchor:"optimum.onnxruntime.configuration.QuantizationConfig.mode",description:`<strong>mode</strong> (<code>QuantizationMode</code>, defaults to <code>QuantizationMode.QLinearOps</code>) &#x2014;
Targeted ONNX Runtime quantization mode, default is QLinearOps to match QDQ format.
When targeting dynamic quantization mode, the default value is <code>QuantizationMode.IntegerOps</code> whereas the
default value for static quantization mode is <code>QuantizationMode.QLinearOps</code>.`,name:"mode"},{anchor:"optimum.onnxruntime.configuration.QuantizationConfig.activations_dtype",description:`<strong>activations_dtype</strong> (<code>QuantType</code>, defaults to <code>QuantType.QUInt8</code>) &#x2014;
The quantization data types to use for the activations.`,name:"activations_dtype"},{anchor:"optimum.onnxruntime.configuration.QuantizationConfig.activations_symmetric",description:`<strong>activations_symmetric</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to apply symmetric quantization on the activations.`,name:"activations_symmetric"},{anchor:"optimum.onnxruntime.configuration.QuantizationConfig.weights_dtype",description:`<strong>weights_dtype</strong> (<code>QuantType</code>, defaults to <code>QuantType.QInt8</code>) &#x2014;
The quantization data types to use for the weights.`,name:"weights_dtype"},{anchor:"optimum.onnxruntime.configuration.QuantizationConfig.weights_symmetric",description:`<strong>weights_symmetric</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether to apply symmetric quantization on the weights.`,name:"weights_symmetric"},{anchor:"optimum.onnxruntime.configuration.QuantizationConfig.per_channel",description:`<strong>per_channel</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether we should quantize per-channel (also known as &#x201C;per-row&#x201D;). Enabling this can increase overall
accuracy while making the quantized model heavier.`,name:"per_channel"},{anchor:"optimum.onnxruntime.configuration.QuantizationConfig.reduce_range",description:`<strong>reduce_range</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use reduce-range 7-bits integers instead of 8-bits integers.`,name:"reduce_range"},{anchor:"optimum.onnxruntime.configuration.QuantizationConfig.nodes_to_quantize",description:`<strong>nodes_to_quantize</strong> (<code>list</code>) &#x2014;
List of the nodes names to quantize.`,name:"nodes_to_quantize"},{anchor:"optimum.onnxruntime.configuration.QuantizationConfig.nodes_to_exclude",description:`<strong>nodes_to_exclude</strong> (<code>list</code>) &#x2014;
List of the nodes names to exclude when applying quantization.`,name:"nodes_to_exclude"},{anchor:"optimum.onnxruntime.configuration.QuantizationConfig.operators_to_quantize",description:`<strong>operators_to_quantize</strong> (<code>list</code>, defaults to <code>[&quot;MatMul&quot;, &quot;Add&quot;]</code>) &#x2014;
List of the operators types to quantize.`,name:"operators_to_quantize"},{anchor:"optimum.onnxruntime.configuration.QuantizationConfig.qdq_add_pair_to_weight",description:`<strong>qdq_add_pair_to_weight</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
By default, floating-point weights are quantized and feed to solely inserted DeQuantizeLinear node.
If set to True, the floating-point weights will remain and both QuantizeLinear / DeQuantizeLinear nodes
will be inserted.`,name:"qdq_add_pair_to_weight"},{anchor:"optimum.onnxruntime.configuration.QuantizationConfig.qdq_dedicated_pair",description:`<strong>qdq_dedicated_pair</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
When inserting QDQ pair, multiple nodes can share a single QDQ pair as their inputs. If True, it will
create an identical and dedicated QDQ pair for each node.`,name:"qdq_dedicated_pair"},{anchor:"optimum.onnxruntime.configuration.QuantizationConfig.qdq_op_type_per_channel_support_to_axis",description:`<strong>qdq_op_type_per_channel_support_to_axis</strong> (<code>Dict[str, int]</code>) &#x2014;
Set the channel axis for a specific operator type. Effective only when per channel quantization is
supported and <code>per_channel</code> is set to True.`,name:"qdq_op_type_per_channel_support_to_axis"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/configuration.py#L220"}}),pt=new Vt({}),dt=new C({props:{name:"class optimum.onnxruntime.CalibrationConfig",anchor:"optimum.onnxruntime.CalibrationConfig",parameters:[{name:"dataset_name",val:": str"},{name:"dataset_config_name",val:": str"},{name:"dataset_split",val:": str"},{name:"dataset_num_samples",val:": int"},{name:"method",val:": CalibrationMethod"},{name:"num_bins",val:": typing.Optional[int] = None"},{name:"num_quantized_bins",val:": typing.Optional[int] = None"},{name:"percentile",val:": typing.Optional[float] = None"},{name:"moving_average",val:": typing.Optional[bool] = None"},{name:"averaging_constant",val:": typing.Optional[float] = None"}],parametersDescription:[{anchor:"optimum.onnxruntime.CalibrationConfig.dataset_name",description:`<strong>dataset_name</strong> (<code>str</code>) &#x2014;
The name of the calibration dataset.`,name:"dataset_name"},{anchor:"optimum.onnxruntime.CalibrationConfig.dataset_config_name",description:`<strong>dataset_config_name</strong> (<code>str</code>) &#x2014;
The name of the calibration dataset configuration.`,name:"dataset_config_name"},{anchor:"optimum.onnxruntime.CalibrationConfig.dataset_split",description:`<strong>dataset_split</strong> (<code>str</code>) &#x2014;
Which split of the dataset is used to perform the calibration step.`,name:"dataset_split"},{anchor:"optimum.onnxruntime.CalibrationConfig.dataset_num_samples",description:`<strong>dataset_num_samples</strong> (<code>int</code>) &#x2014;
The number of samples composing the calibration dataset.`,name:"dataset_num_samples"},{anchor:"optimum.onnxruntime.CalibrationConfig.method",description:`<strong>method</strong> (<code>CalibrationMethod</code>) &#x2014;
The method chosen to calculate the activations quantization parameters using the calibration dataset.`,name:"method"},{anchor:"optimum.onnxruntime.CalibrationConfig.num_bins",description:`<strong>num_bins</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of bins to use when creating the histogram when performing the calibration step using the
Percentile or Entropy method.`,name:"num_bins"},{anchor:"optimum.onnxruntime.CalibrationConfig.num_quantized_bins",description:`<strong>num_quantized_bins</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of quantized bins to use when performing the calibration step using the Entropy method.`,name:"num_quantized_bins"},{anchor:"optimum.onnxruntime.CalibrationConfig.percentile",description:`<strong>percentile</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The percentile to use when computing the activations quantization ranges when performing the calibration
step using the Percentile method.`,name:"percentile"},{anchor:"optimum.onnxruntime.CalibrationConfig.moving_average",description:`<strong>moving_average</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to compute the moving average of the minimum and maximum values when performing the calibration step
using the MinMax method.`,name:"moving_average"},{anchor:"optimum.onnxruntime.CalibrationConfig.averaging_constant",description:`<strong>averaging_constant</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The constant smoothing factor to use when computing the moving average of the minimum and maximum values.
Effective only when the MinMax calibration method is selected and <code>moving_average</code> is set to True.`,name:"averaging_constant"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/configuration.py#L42"}}),ut=new Vt({}),ct=new C({props:{name:"class optimum.onnxruntime.ORTConfig",anchor:"optimum.onnxruntime.ORTConfig",parameters:[{name:"opset",val:": typing.Optional[int] = None"},{name:"use_external_data_format",val:": bool = False"},{name:"optimization",val:": typing.Optional[optimum.onnxruntime.configuration.OptimizationConfig] = None"},{name:"quantization",val:": typing.Optional[optimum.onnxruntime.configuration.QuantizationConfig] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTConfig.opset",description:`<strong>opset</strong> (<code>int</code>, <em>optional</em>) &#x2014;
ONNX opset version to export the model with.`,name:"opset"},{anchor:"optimum.onnxruntime.ORTConfig.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Allow exporting model &gt;= than 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTConfig.optimization",description:`<strong>optimization</strong> (<code>OptimizationConfig</code>, <em>optional</em>, defaults to None) &#x2014;
Specify a configuration to optimize ONNX Runtime model`,name:"optimization"},{anchor:"optimum.onnxruntime.ORTConfig.quantization",description:`<strong>quantization</strong> (<code>QuantizationConfig</code>, <em>optional</em>, defaults to None) &#x2014;
Specify a configuration to quantize ONNX Runtime model`,name:"quantization"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/configuration.py#L871"}}),{c(){y=n("meta"),Bt=l(),w=n("h1"),F=n("a"),Ot=n("span"),g(V.$$.fragment),Ne=l(),Ct=n("span"),ke=r("Configuration"),Ht=l(),gt=n("p"),qe=r("The configuration classes are the way to specify how a task should be done. There are two tasks supported with the ONNX Runtime package:"),Jt=l(),D=n("ol"),yt=n("li"),$=n("p"),Te=r("Optimization: Performed by the "),ft=n("a"),Qe=r("ORTOptimizer"),Ee=r(", this task can be tweaked using an "),ht=n("a"),Ae=r("OptimizationConfig"),Fe=r("."),De=l(),wt=n("li"),x=n("p"),Re=r("Quantization: Performed by the "),_t=n("a"),Pe=r("ORTQuantizer"),Le=r(", quantization can be set using a "),bt=n("a"),We=r("QuantizationConfig"),Xe=r(". A calibration step is required in some cases (post training static quantization), which can be specified using a "),vt=n("a"),Ie=r("CalibrationConfig"),Se=r("."),jt=l(),N=n("h2"),R=n("a"),$t=n("span"),g(B.$$.fragment),Me=l(),Nt=n("span"),Ge=r("OptimizationConfig"),Kt=l(),O=n("div"),g(H.$$.fragment),Ue=l(),kt=n("p"),Ve=r(`OptimizationConfig is the configuration class handling all the ONNX Runtime optimization parameters.
There are two stacks of optimizations:`),Be=l(),J=n("ol"),qt=n("li"),He=r("The ONNX Runtime general-purpose optimization tool: it can work on any ONNX model."),Je=l(),Tt=n("li"),je=r("The ONNX Runtime transformers optimization tool: it can only work on a subset of transformers models."),Yt=l(),c=n("div"),g(j.$$.fragment),Ke=l(),K=n("p"),Ye=r("Factory to create common "),Qt=n("code"),Ze=r("OptimizationConfig"),to=r("."),eo=l(),P=n("div"),g(Y.$$.fragment),oo=l(),Z=n("p"),no=r("Creates an O1 "),Et=n("code"),io=r("~OptimizationConfig"),ao=r("."),ro=l(),L=n("div"),g(tt.$$.fragment),so=l(),et=n("p"),mo=r("Creates an O2 "),At=n("code"),lo=r("~OptimizationConfig"),po=r("."),uo=l(),W=n("div"),g(ot.$$.fragment),co=l(),nt=n("p"),go=r("Creates an O3 "),Ft=n("code"),fo=r("~OptimizationConfig"),ho=r("."),_o=l(),X=n("div"),g(it.$$.fragment),bo=l(),at=n("p"),vo=r("Creates an O4 "),Dt=n("code"),zo=r("~OptimizationConfig"),xo=r("."),Oo=l(),I=n("div"),g(rt.$$.fragment),Co=l(),st=n("p"),yo=r("Creates an "),Rt=n("code"),wo=r("~OptimizationConfig"),$o=r(" with pre-defined arguments according to an optimization level."),Zt=l(),k=n("h2"),S=n("a"),Pt=n("span"),g(mt.$$.fragment),No=l(),Lt=n("span"),ko=r("QuantizationConfig"),te=l(),q=n("div"),g(lt.$$.fragment),qo=l(),Wt=n("p"),To=r("QuantizationConfig is the configuration class handling all the ONNX Runtime quantization parameters."),ee=l(),T=n("h3"),M=n("a"),Xt=n("span"),g(pt.$$.fragment),Qo=l(),It=n("span"),Eo=r("CalibrationConfig"),oe=l(),Q=n("div"),g(dt.$$.fragment),Ao=l(),St=n("p"),Fo=r(`CalibrationConfig is the configuration class handling all the ONNX Runtime parameters related to the calibration
step of static quantization.`),ne=l(),E=n("h2"),G=n("a"),Mt=n("span"),g(ut.$$.fragment),Do=l(),Gt=n("span"),Ro=r("ORTConfig"),ie=l(),A=n("div"),g(ct.$$.fragment),Po=l(),Ut=n("p"),Lo=r(`ORTConfig is the configuration class handling all the ONNX Runtime parameters related to the ONNX IR model export,
optimization and quantization parameters.`),this.h()},l(e){const d=wn('[data-svelte="svelte-1phssyn"]',document.head);y=i(d,"META",{name:!0,content:!0}),d.forEach(o),Bt=p(e),w=i(e,"H1",{class:!0});var re=a(w);F=i(re,"A",{id:!0,class:!0,href:!0});var Xo=a(F);Ot=i(Xo,"SPAN",{});var Io=a(Ot);f(V.$$.fragment,Io),Io.forEach(o),Xo.forEach(o),Ne=p(re),Ct=i(re,"SPAN",{});var So=a(Ct);ke=s(So,"Configuration"),So.forEach(o),re.forEach(o),Ht=p(e),gt=i(e,"P",{});var Mo=a(gt);qe=s(Mo,"The configuration classes are the way to specify how a task should be done. There are two tasks supported with the ONNX Runtime package:"),Mo.forEach(o),Jt=p(e),D=i(e,"OL",{});var se=a(D);yt=i(se,"LI",{});var Go=a(yt);$=i(Go,"P",{});var zt=a($);Te=s(zt,"Optimization: Performed by the "),ft=i(zt,"A",{href:!0});var Uo=a(ft);Qe=s(Uo,"ORTOptimizer"),Uo.forEach(o),Ee=s(zt,", this task can be tweaked using an "),ht=i(zt,"A",{href:!0});var Vo=a(ht);Ae=s(Vo,"OptimizationConfig"),Vo.forEach(o),Fe=s(zt,"."),zt.forEach(o),Go.forEach(o),De=p(se),wt=i(se,"LI",{});var Bo=a(wt);x=i(Bo,"P",{});var U=a(x);Re=s(U,"Quantization: Performed by the "),_t=i(U,"A",{href:!0});var Ho=a(_t);Pe=s(Ho,"ORTQuantizer"),Ho.forEach(o),Le=s(U,", quantization can be set using a "),bt=i(U,"A",{href:!0});var Jo=a(bt);We=s(Jo,"QuantizationConfig"),Jo.forEach(o),Xe=s(U,". A calibration step is required in some cases (post training static quantization), which can be specified using a "),vt=i(U,"A",{href:!0});var jo=a(vt);Ie=s(jo,"CalibrationConfig"),jo.forEach(o),Se=s(U,"."),U.forEach(o),Bo.forEach(o),se.forEach(o),jt=p(e),N=i(e,"H2",{class:!0});var me=a(N);R=i(me,"A",{id:!0,class:!0,href:!0});var Ko=a(R);$t=i(Ko,"SPAN",{});var Yo=a($t);f(B.$$.fragment,Yo),Yo.forEach(o),Ko.forEach(o),Me=p(me),Nt=i(me,"SPAN",{});var Zo=a(Nt);Ge=s(Zo,"OptimizationConfig"),Zo.forEach(o),me.forEach(o),Kt=p(e),O=i(e,"DIV",{class:!0});var xt=a(O);f(H.$$.fragment,xt),Ue=p(xt),kt=i(xt,"P",{});var tn=a(kt);Ve=s(tn,`OptimizationConfig is the configuration class handling all the ONNX Runtime optimization parameters.
There are two stacks of optimizations:`),tn.forEach(o),Be=p(xt),J=i(xt,"OL",{});var le=a(J);qt=i(le,"LI",{});var en=a(qt);He=s(en,"The ONNX Runtime general-purpose optimization tool: it can work on any ONNX model."),en.forEach(o),Je=p(le),Tt=i(le,"LI",{});var on=a(Tt);je=s(on,"The ONNX Runtime transformers optimization tool: it can only work on a subset of transformers models."),on.forEach(o),le.forEach(o),xt.forEach(o),Yt=p(e),c=i(e,"DIV",{class:!0});var z=a(c);f(j.$$.fragment,z),Ke=p(z),K=i(z,"P",{});var pe=a(K);Ye=s(pe,"Factory to create common "),Qt=i(pe,"CODE",{});var nn=a(Qt);Ze=s(nn,"OptimizationConfig"),nn.forEach(o),to=s(pe,"."),pe.forEach(o),eo=p(z),P=i(z,"DIV",{class:!0});var de=a(P);f(Y.$$.fragment,de),oo=p(de),Z=i(de,"P",{});var ue=a(Z);no=s(ue,"Creates an O1 "),Et=i(ue,"CODE",{});var an=a(Et);io=s(an,"~OptimizationConfig"),an.forEach(o),ao=s(ue,"."),ue.forEach(o),de.forEach(o),ro=p(z),L=i(z,"DIV",{class:!0});var ce=a(L);f(tt.$$.fragment,ce),so=p(ce),et=i(ce,"P",{});var ge=a(et);mo=s(ge,"Creates an O2 "),At=i(ge,"CODE",{});var rn=a(At);lo=s(rn,"~OptimizationConfig"),rn.forEach(o),po=s(ge,"."),ge.forEach(o),ce.forEach(o),uo=p(z),W=i(z,"DIV",{class:!0});var fe=a(W);f(ot.$$.fragment,fe),co=p(fe),nt=i(fe,"P",{});var he=a(nt);go=s(he,"Creates an O3 "),Ft=i(he,"CODE",{});var sn=a(Ft);fo=s(sn,"~OptimizationConfig"),sn.forEach(o),ho=s(he,"."),he.forEach(o),fe.forEach(o),_o=p(z),X=i(z,"DIV",{class:!0});var _e=a(X);f(it.$$.fragment,_e),bo=p(_e),at=i(_e,"P",{});var be=a(at);vo=s(be,"Creates an O4 "),Dt=i(be,"CODE",{});var mn=a(Dt);zo=s(mn,"~OptimizationConfig"),mn.forEach(o),xo=s(be,"."),be.forEach(o),_e.forEach(o),Oo=p(z),I=i(z,"DIV",{class:!0});var ve=a(I);f(rt.$$.fragment,ve),Co=p(ve),st=i(ve,"P",{});var ze=a(st);yo=s(ze,"Creates an "),Rt=i(ze,"CODE",{});var ln=a(Rt);wo=s(ln,"~OptimizationConfig"),ln.forEach(o),$o=s(ze," with pre-defined arguments according to an optimization level."),ze.forEach(o),ve.forEach(o),z.forEach(o),Zt=p(e),k=i(e,"H2",{class:!0});var xe=a(k);S=i(xe,"A",{id:!0,class:!0,href:!0});var pn=a(S);Pt=i(pn,"SPAN",{});var dn=a(Pt);f(mt.$$.fragment,dn),dn.forEach(o),pn.forEach(o),No=p(xe),Lt=i(xe,"SPAN",{});var un=a(Lt);ko=s(un,"QuantizationConfig"),un.forEach(o),xe.forEach(o),te=p(e),q=i(e,"DIV",{class:!0});var Oe=a(q);f(lt.$$.fragment,Oe),qo=p(Oe),Wt=i(Oe,"P",{});var cn=a(Wt);To=s(cn,"QuantizationConfig is the configuration class handling all the ONNX Runtime quantization parameters."),cn.forEach(o),Oe.forEach(o),ee=p(e),T=i(e,"H3",{class:!0});var Ce=a(T);M=i(Ce,"A",{id:!0,class:!0,href:!0});var gn=a(M);Xt=i(gn,"SPAN",{});var fn=a(Xt);f(pt.$$.fragment,fn),fn.forEach(o),gn.forEach(o),Qo=p(Ce),It=i(Ce,"SPAN",{});var hn=a(It);Eo=s(hn,"CalibrationConfig"),hn.forEach(o),Ce.forEach(o),oe=p(e),Q=i(e,"DIV",{class:!0});var ye=a(Q);f(dt.$$.fragment,ye),Ao=p(ye),St=i(ye,"P",{});var _n=a(St);Fo=s(_n,`CalibrationConfig is the configuration class handling all the ONNX Runtime parameters related to the calibration
step of static quantization.`),_n.forEach(o),ye.forEach(o),ne=p(e),E=i(e,"H2",{class:!0});var we=a(E);G=i(we,"A",{id:!0,class:!0,href:!0});var bn=a(G);Mt=i(bn,"SPAN",{});var vn=a(Mt);f(ut.$$.fragment,vn),vn.forEach(o),bn.forEach(o),Do=p(we),Gt=i(we,"SPAN",{});var zn=a(Gt);Ro=s(zn,"ORTConfig"),zn.forEach(o),we.forEach(o),ie=p(e),A=i(e,"DIV",{class:!0});var $e=a(A);f(ct.$$.fragment,$e),Po=p($e),Ut=i($e,"P",{});var xn=a(Ut);Lo=s(xn,`ORTConfig is the configuration class handling all the ONNX Runtime parameters related to the ONNX IR model export,
optimization and quantization parameters.`),xn.forEach(o),$e.forEach(o),this.h()},h(){m(y,"name","hf:doc:metadata"),m(y,"content",JSON.stringify(qn)),m(F,"id","configuration"),m(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(F,"href","#configuration"),m(w,"class","relative group"),m(ft,"href","/docs/optimum/main/en/onnxruntime/package_reference/optimization#optimum.onnxruntime.ORTOptimizer"),m(ht,"href","/docs/optimum/main/en/onnxruntime/package_reference/configuration#optimum.onnxruntime.OptimizationConfig"),m(_t,"href","/docs/optimum/main/en/onnxruntime/package_reference/quantization#optimum.onnxruntime.ORTQuantizer"),m(bt,"href","/docs/optimum/main/en/onnxruntime/package_reference/configuration#optimum.onnxruntime.configuration.QuantizationConfig"),m(vt,"href","/docs/optimum/main/en/onnxruntime/package_reference/configuration#optimum.onnxruntime.CalibrationConfig"),m(R,"id","optimum.onnxruntime.OptimizationConfig"),m(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(R,"href","#optimum.onnxruntime.OptimizationConfig"),m(N,"class","relative group"),m(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(c,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(S,"id","optimum.onnxruntime.configuration.QuantizationConfig"),m(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(S,"href","#optimum.onnxruntime.configuration.QuantizationConfig"),m(k,"class","relative group"),m(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(M,"id","optimum.onnxruntime.CalibrationConfig"),m(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(M,"href","#optimum.onnxruntime.CalibrationConfig"),m(T,"class","relative group"),m(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(G,"id","optimum.onnxruntime.ORTConfig"),m(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(G,"href","#optimum.onnxruntime.ORTConfig"),m(E,"class","relative group"),m(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,d){t(document.head,y),u(e,Bt,d),u(e,w,d),t(w,F),t(F,Ot),h(V,Ot,null),t(w,Ne),t(w,Ct),t(Ct,ke),u(e,Ht,d),u(e,gt,d),t(gt,qe),u(e,Jt,d),u(e,D,d),t(D,yt),t(yt,$),t($,Te),t($,ft),t(ft,Qe),t($,Ee),t($,ht),t(ht,Ae),t($,Fe),t(D,De),t(D,wt),t(wt,x),t(x,Re),t(x,_t),t(_t,Pe),t(x,Le),t(x,bt),t(bt,We),t(x,Xe),t(x,vt),t(vt,Ie),t(x,Se),u(e,jt,d),u(e,N,d),t(N,R),t(R,$t),h(B,$t,null),t(N,Me),t(N,Nt),t(Nt,Ge),u(e,Kt,d),u(e,O,d),h(H,O,null),t(O,Ue),t(O,kt),t(kt,Ve),t(O,Be),t(O,J),t(J,qt),t(qt,He),t(J,Je),t(J,Tt),t(Tt,je),u(e,Yt,d),u(e,c,d),h(j,c,null),t(c,Ke),t(c,K),t(K,Ye),t(K,Qt),t(Qt,Ze),t(K,to),t(c,eo),t(c,P),h(Y,P,null),t(P,oo),t(P,Z),t(Z,no),t(Z,Et),t(Et,io),t(Z,ao),t(c,ro),t(c,L),h(tt,L,null),t(L,so),t(L,et),t(et,mo),t(et,At),t(At,lo),t(et,po),t(c,uo),t(c,W),h(ot,W,null),t(W,co),t(W,nt),t(nt,go),t(nt,Ft),t(Ft,fo),t(nt,ho),t(c,_o),t(c,X),h(it,X,null),t(X,bo),t(X,at),t(at,vo),t(at,Dt),t(Dt,zo),t(at,xo),t(c,Oo),t(c,I),h(rt,I,null),t(I,Co),t(I,st),t(st,yo),t(st,Rt),t(Rt,wo),t(st,$o),u(e,Zt,d),u(e,k,d),t(k,S),t(S,Pt),h(mt,Pt,null),t(k,No),t(k,Lt),t(Lt,ko),u(e,te,d),u(e,q,d),h(lt,q,null),t(q,qo),t(q,Wt),t(Wt,To),u(e,ee,d),u(e,T,d),t(T,M),t(M,Xt),h(pt,Xt,null),t(T,Qo),t(T,It),t(It,Eo),u(e,oe,d),u(e,Q,d),h(dt,Q,null),t(Q,Ao),t(Q,St),t(St,Fo),u(e,ne,d),u(e,E,d),t(E,G),t(G,Mt),h(ut,Mt,null),t(E,Do),t(E,Gt),t(Gt,Ro),u(e,ie,d),u(e,A,d),h(ct,A,null),t(A,Po),t(A,Ut),t(Ut,Lo),ae=!0},p:$n,i(e){ae||(_(V.$$.fragment,e),_(B.$$.fragment,e),_(H.$$.fragment,e),_(j.$$.fragment,e),_(Y.$$.fragment,e),_(tt.$$.fragment,e),_(ot.$$.fragment,e),_(it.$$.fragment,e),_(rt.$$.fragment,e),_(mt.$$.fragment,e),_(lt.$$.fragment,e),_(pt.$$.fragment,e),_(dt.$$.fragment,e),_(ut.$$.fragment,e),_(ct.$$.fragment,e),ae=!0)},o(e){b(V.$$.fragment,e),b(B.$$.fragment,e),b(H.$$.fragment,e),b(j.$$.fragment,e),b(Y.$$.fragment,e),b(tt.$$.fragment,e),b(ot.$$.fragment,e),b(it.$$.fragment,e),b(rt.$$.fragment,e),b(mt.$$.fragment,e),b(lt.$$.fragment,e),b(pt.$$.fragment,e),b(dt.$$.fragment,e),b(ut.$$.fragment,e),b(ct.$$.fragment,e),ae=!1},d(e){o(y),e&&o(Bt),e&&o(w),v(V),e&&o(Ht),e&&o(gt),e&&o(Jt),e&&o(D),e&&o(jt),e&&o(N),v(B),e&&o(Kt),e&&o(O),v(H),e&&o(Yt),e&&o(c),v(j),v(Y),v(tt),v(ot),v(it),v(rt),e&&o(Zt),e&&o(k),v(mt),e&&o(te),e&&o(q),v(lt),e&&o(ee),e&&o(T),v(pt),e&&o(oe),e&&o(Q),v(dt),e&&o(ne),e&&o(E),v(ut),e&&o(ie),e&&o(A),v(ct)}}}const qn={local:"configuration",sections:[{local:"optimum.onnxruntime.OptimizationConfig",title:"OptimizationConfig"},{local:"optimum.onnxruntime.configuration.QuantizationConfig",sections:[{local:"optimum.onnxruntime.CalibrationConfig",title:"CalibrationConfig"}],title:"QuantizationConfig"},{local:"optimum.onnxruntime.ORTConfig",title:"ORTConfig"}],title:"Configuration"};function Tn(Wo){return Nn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Fn extends On{constructor(y){super();Cn(this,y,Tn,kn,yn,{})}}export{Fn as default,qn as metadata};
