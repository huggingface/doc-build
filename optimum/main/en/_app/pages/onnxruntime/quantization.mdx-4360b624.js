import{S as ve,i as be,s as Te,e as a,k as m,w as h,t as p,M as $e,c as o,d as n,m as c,a as i,x as g,h as l,b as r,F as e,g as v,y as _,L as Re,q as f,o as x,B as z,v as Oe}from"../../chunks/vendor-19e06bd2.js";import{D as N}from"../../chunks/Docstring-395e5a9c.js";import{I as ee}from"../../chunks/IconCopyLink-3c713d38.js";function ye(ne){let b,mt,T,O,j,k,Ot,K,yt,ct,$,y,Y,C,qt,Z,Qt,ut,L,I,pt,R,q,tt,F,wt,et,Dt,lt,s,U,Pt,nt,Et,Nt,J,A,kt,Q,S,Ct,H,Lt,at,It,Ft,Ut,w,W,At,ot,St,Ht,D,V,Wt,X,Vt,it,Xt,Gt,Mt,P,G,Bt,M,Jt,rt,jt,Kt,Yt,E,B,Zt,st,te,dt;return k=new ee({}),C=new ee({}),I=new N({props:{name:"class optimum.onnxruntime.quantization.ORTCalibrationDataReader",anchor:"optimum.onnxruntime.quantization.ORTCalibrationDataReader",parameters:[{name:"dataset",val:": Dataset"},{name:"batch_size",val:": int = 1"}],source:"https://github.com/huggingface/optimum/blob/main/src/optimum/onnxruntime/quantization.py#L38"}}),F=new ee({}),U=new N({props:{name:"class optimum.onnxruntime.ORTQuantizer",anchor:"optimum.onnxruntime.ORTQuantizer",parameters:[{name:"tokenizer",val:": PreTrainedTokenizer"},{name:"model",val:": PreTrainedModel"},{name:"feature",val:": str = 'default'"},{name:"opset",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/optimum/blob/main/src/optimum/onnxruntime/quantization.py#L77"}}),A=new N({props:{name:"compute_ranges",anchor:"optimum.onnxruntime.ORTQuantizer.compute_ranges",parameters:[],source:"https://github.com/huggingface/optimum/blob/main/src/optimum/onnxruntime/quantization.py#L260",returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),S=new N({props:{name:"export",anchor:"optimum.onnxruntime.ORTQuantizer.export",parameters:[{name:"onnx_model_path",val:": typing.Union[str, os.PathLike]"},{name:"onnx_quantized_model_output_path",val:": typing.Union[str, os.PathLike]"},{name:"quantization_config",val:": QuantizationConfig"},{name:"calibration_tensors_range",val:": typing.Union[typing.Dict[str, typing.Tuple[float, float]], NoneType] = None"},{name:"use_external_data_format",val:": bool = False"},{name:"preprocessor",val:": typing.Optional[optimum.onnxruntime.preprocessors.quantization.QuantizationPreprocessor] = None"}],source:"https://github.com/huggingface/optimum/blob/main/src/optimum/onnxruntime/quantization.py#L273",parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.export.onnx_model_path",description:`<strong>onnx_model_path</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the model exported to an ONNX Intermediate Representation (IR).`,name:"onnx_model_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.export.onnx_quantized_model_output_path",description:`<strong>onnx_quantized_model_output_path</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the quantized model exported to an ONNX Intermediate Representation (IR).`,name:"onnx_quantized_model_output_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.export.quantization_config",description:`<strong>quantization_config</strong> (<code>QuantizationConfig</code>) &#x2014;
The configuration containing the parameters related to quantization.`,name:"quantization_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.export.calibration_tensors_range",description:`<strong>calibration_tensors_range</strong> (<code>Dict[NodeName, Tuple[float, float]]</code>, <em>optional</em>) &#x2014;
The dictionary mapping the nodes name to their quantization ranges, used and required only when applying
static quantization.`,name:"calibration_tensors_range"},{anchor:"optimum.onnxruntime.ORTQuantizer.export.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether uto se external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.export.preprocessor",description:`<strong>preprocessor</strong> (<code>QuantizationPreprocessor</code>, <em>optional</em>) &#x2014;
The preprocessor to use to collect the nodes to include or exclude from quantization.`,name:"preprocessor"}],returnDescription:`
<p>The path of the resulting quantized model.</p>
`}}),W=new N({props:{name:"fit",anchor:"optimum.onnxruntime.ORTQuantizer.fit",parameters:[{name:"dataset",val:": Dataset"},{name:"calibration_config",val:": CalibrationConfig"},{name:"onnx_model_path",val:": typing.Union[str, os.PathLike, pathlib.Path]"},{name:"onnx_augmented_model_name",val:": str = 'augmented_model.onnx'"},{name:"operators_to_quantize",val:": typing.Optional[typing.List[str]] = None"},{name:"batch_size",val:": int = 1"},{name:"use_external_data_format",val:": bool = False"},{name:"use_gpu",val:": bool = False"},{name:"force_symmetric_range",val:": bool = False"}],source:"https://github.com/huggingface/optimum/blob/main/src/optimum/onnxruntime/quantization.py#L137",parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.fit.dataset",description:`<strong>dataset</strong> (<code>Dataset</code>) &#x2014;
The dataset to use when performing the calibration step.`,name:"dataset"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.calibration_config",description:`<strong>calibration_config</strong> (<code>CalibrationConfig</code>) &#x2014;
The configuration containing the parameters related to the calibration step.`,name:"calibration_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.onnx_model_path",description:`<strong>onnx_model_path</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the model exported to an ONNX Intermediate Representation (IR).`,name:"onnx_model_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.onnx_augmented_model_name",description:`<strong>onnx_augmented_model_name</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the augmented model used to collect the quantization ranges.`,name:"onnx_augmented_model_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.operators_to_quantize",description:`<strong>operators_to_quantize</strong> (<code>list</code>, <em>optional</em>) &#x2014;
List of the operators types to quantize.`,name:"operators_to_quantize"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, defaults to 1) &#x2014;
The batch size to use when collecting the quantization ranges values.`,name:"batch_size"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether uto se external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.use_gpu",description:`<strong>use_gpu</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use the GPU when collecting the quantization ranges values.`,name:"use_gpu"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.force_symmetric_range",description:`<strong>force_symmetric_range</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to make the quantization ranges symmetric.`,name:"force_symmetric_range"}],returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),V=new N({props:{name:"from_pretrained",anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained",parameters:[{name:"model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"feature",val:": str"},{name:"opset",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/optimum/blob/main/src/optimum/onnxruntime/quantization.py#L82",parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.model_name_or_path",description:`<strong>model_name_or_path</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
Repository name in the Hugging Face Hub or path to a local directory hosting the model.`,name:"model_name_or_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.feature",description:`<strong>feature</strong> (<code>str</code>) &#x2014;
Feature to use when exporting the model.`,name:"feature"},{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.opset",description:`<strong>opset</strong> (<code>int</code>, <em>optional</em>) &#x2014;
ONNX opset version to export the model with.`,name:"opset"}],returnDescription:`
<p>An instance of <code>ORTQuantizer</code>.</p>
`}}),G=new N({props:{name:"get_calibration_dataset",anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset",parameters:[{name:"dataset_name",val:": str"},{name:"num_samples",val:": int = 100"},{name:"dataset_config_name",val:": typing.Optional[str] = None"},{name:"dataset_split",val:": typing.Optional[str] = None"},{name:"preprocess_function",val:": typing.Optional[typing.Callable] = None"},{name:"preprocess_batch",val:": bool = True"},{name:"seed",val:": int = 2016"}],source:"https://github.com/huggingface/optimum/blob/main/src/optimum/onnxruntime/quantization.py#L374",parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_name",description:`<strong>dataset_name</strong> (<code>str</code>) &#x2014;
The dataset repository name on the Hugging Face Hub or path to a local directory containing data files
to load to use for the calibration step.`,name:"dataset_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.num_samples",description:`<strong>num_samples</strong> (<code>int</code>, defaults to 100) &#x2014;
The maximum number of samples composing the calibration dataset.`,name:"num_samples"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_config_name",description:`<strong>dataset_config_name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The name of the dataset configuration.`,name:"dataset_config_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_split",description:`<strong>dataset_split</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Which split of the dataset to use to perform the calibration step.`,name:"dataset_split"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.preprocess_function",description:`<strong>preprocess_function</strong> (<code>Callable</code>, <em>optional</em>) &#x2014;
Processing function to apply to each example after loading dataset.`,name:"preprocess_function"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.preprocess_batch",description:`<strong>preprocess_batch</strong> (<code>int</code>, defaults to <code>True</code>) &#x2014;
Whether the <code>preprocess_function</code> should be batched.`,name:"preprocess_batch"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.seed",description:`<strong>seed</strong> (<code>int</code>, defaults to 2016) &#x2014;
The random seed to use when shuffling the calibration dataset.`,name:"seed"}],returnDescription:`
<p>The calibration <code>datasets.Dataset</code> to use for the post-training static quantization calibration
step.</p>
`}}),B=new N({props:{name:"partial_fit",anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit",parameters:[{name:"dataset",val:": Dataset"},{name:"calibration_config",val:": CalibrationConfig"},{name:"onnx_model_path",val:": typing.Union[str, os.PathLike]"},{name:"onnx_augmented_model_name",val:": str = 'augmented_model.onnx'"},{name:"operators_to_quantize",val:": typing.Optional[typing.List[str]] = None"},{name:"batch_size",val:": int = 1"},{name:"use_external_data_format",val:": bool = False"},{name:"use_gpu",val:": bool = False"},{name:"force_symmetric_range",val:": bool = False"}],source:"https://github.com/huggingface/optimum/blob/main/src/optimum/onnxruntime/quantization.py#L195",parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.dataset",description:`<strong>dataset</strong> (<code>Dataset</code>) &#x2014;
The dataset to use when performing the calibration step.`,name:"dataset"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.calibration_config",description:`<strong>calibration_config</strong> (<code>CalibrationConfig</code>) &#x2014;
The configuration containing the parameters related to the calibration step.`,name:"calibration_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.onnx_model_path",description:`<strong>onnx_model_path</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the model exported to an ONNX Intermediate Representation (IR).`,name:"onnx_model_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.onnx_augmented_model_name",description:`<strong>onnx_augmented_model_name</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the augmented model used to collect the quantization ranges.`,name:"onnx_augmented_model_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.operators_to_quantize",description:`<strong>operators_to_quantize</strong> (<code>list</code>, <em>optional</em>) &#x2014;
List of the operators types to quantize.`,name:"operators_to_quantize"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, defaults to 1) &#x2014;
The batch size to use when collecting the quantization ranges values.`,name:"batch_size"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether uto se external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.use_gpu",description:`<strong>use_gpu</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use the GPU when collecting the quantization ranges values.`,name:"use_gpu"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.force_symmetric_range",description:`<strong>force_symmetric_range</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to make the quantization ranges symmetric.`,name:"force_symmetric_range"}],returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),{c(){b=a("meta"),mt=m(),T=a("h1"),O=a("a"),j=a("span"),h(k.$$.fragment),Ot=m(),K=a("span"),yt=p("Quantization"),ct=m(),$=a("h2"),y=a("a"),Y=a("span"),h(C.$$.fragment),qt=m(),Z=a("span"),Qt=p("ORTCalibrationDataReader"),ut=m(),L=a("div"),h(I.$$.fragment),pt=m(),R=a("h2"),q=a("a"),tt=a("span"),h(F.$$.fragment),wt=m(),et=a("span"),Dt=p("ORTQuantizer"),lt=m(),s=a("div"),h(U.$$.fragment),Pt=m(),nt=a("p"),Et=p("Handles the ONNX Runtime quantization process for models shared on huggingface.co/models."),Nt=m(),J=a("div"),h(A.$$.fragment),kt=m(),Q=a("div"),h(S.$$.fragment),Ct=m(),H=a("p"),Lt=p("Quantize a model given the optimization specifications defined in "),at=a("code"),It=p("quantization_config"),Ft=p("."),Ut=m(),w=a("div"),h(W.$$.fragment),At=m(),ot=a("p"),St=p("Perform the calibration step and collect the quantization ranges."),Ht=m(),D=a("div"),h(V.$$.fragment),Wt=m(),X=a("p"),Vt=p("Instantiate a "),it=a("code"),Xt=p("ORTQuantizer"),Gt=p(" from a pretrained pytorch model and tokenizer."),Mt=m(),P=a("div"),h(G.$$.fragment),Bt=m(),M=a("p"),Jt=p("Create the calibration "),rt=a("code"),jt=p("datasets.Dataset"),Kt=p(" to use for the post-training static quantization calibration step"),Yt=m(),E=a("div"),h(B.$$.fragment),Zt=m(),st=a("p"),te=p("Perform the calibration step and collect the quantization ranges."),this.h()},l(t){const d=$e('[data-svelte="svelte-1phssyn"]',document.head);b=o(d,"META",{name:!0,content:!0}),d.forEach(n),mt=c(t),T=o(t,"H1",{class:!0});var ht=i(T);O=o(ht,"A",{id:!0,class:!0,href:!0});var ae=i(O);j=o(ae,"SPAN",{});var oe=i(j);g(k.$$.fragment,oe),oe.forEach(n),ae.forEach(n),Ot=c(ht),K=o(ht,"SPAN",{});var ie=i(K);yt=l(ie,"Quantization"),ie.forEach(n),ht.forEach(n),ct=c(t),$=o(t,"H2",{class:!0});var gt=i($);y=o(gt,"A",{id:!0,class:!0,href:!0});var re=i(y);Y=o(re,"SPAN",{});var se=i(Y);g(C.$$.fragment,se),se.forEach(n),re.forEach(n),qt=c(gt),Z=o(gt,"SPAN",{});var me=i(Z);Qt=l(me,"ORTCalibrationDataReader"),me.forEach(n),gt.forEach(n),ut=c(t),L=o(t,"DIV",{class:!0});var ce=i(L);g(I.$$.fragment,ce),ce.forEach(n),pt=c(t),R=o(t,"H2",{class:!0});var _t=i(R);q=o(_t,"A",{id:!0,class:!0,href:!0});var ue=i(q);tt=o(ue,"SPAN",{});var pe=i(tt);g(F.$$.fragment,pe),pe.forEach(n),ue.forEach(n),wt=c(_t),et=o(_t,"SPAN",{});var le=i(et);Dt=l(le,"ORTQuantizer"),le.forEach(n),_t.forEach(n),lt=c(t),s=o(t,"DIV",{class:!0});var u=i(s);g(U.$$.fragment,u),Pt=c(u),nt=o(u,"P",{});var de=i(nt);Et=l(de,"Handles the ONNX Runtime quantization process for models shared on huggingface.co/models."),de.forEach(n),Nt=c(u),J=o(u,"DIV",{class:!0});var he=i(J);g(A.$$.fragment,he),he.forEach(n),kt=c(u),Q=o(u,"DIV",{class:!0});var ft=i(Q);g(S.$$.fragment,ft),Ct=c(ft),H=o(ft,"P",{});var xt=i(H);Lt=l(xt,"Quantize a model given the optimization specifications defined in "),at=o(xt,"CODE",{});var ge=i(at);It=l(ge,"quantization_config"),ge.forEach(n),Ft=l(xt,"."),xt.forEach(n),ft.forEach(n),Ut=c(u),w=o(u,"DIV",{class:!0});var zt=i(w);g(W.$$.fragment,zt),At=c(zt),ot=o(zt,"P",{});var _e=i(ot);St=l(_e,"Perform the calibration step and collect the quantization ranges."),_e.forEach(n),zt.forEach(n),Ht=c(u),D=o(u,"DIV",{class:!0});var vt=i(D);g(V.$$.fragment,vt),Wt=c(vt),X=o(vt,"P",{});var bt=i(X);Vt=l(bt,"Instantiate a "),it=o(bt,"CODE",{});var fe=i(it);Xt=l(fe,"ORTQuantizer"),fe.forEach(n),Gt=l(bt," from a pretrained pytorch model and tokenizer."),bt.forEach(n),vt.forEach(n),Mt=c(u),P=o(u,"DIV",{class:!0});var Tt=i(P);g(G.$$.fragment,Tt),Bt=c(Tt),M=o(Tt,"P",{});var $t=i(M);Jt=l($t,"Create the calibration "),rt=o($t,"CODE",{});var xe=i(rt);jt=l(xe,"datasets.Dataset"),xe.forEach(n),Kt=l($t," to use for the post-training static quantization calibration step"),$t.forEach(n),Tt.forEach(n),Yt=c(u),E=o(u,"DIV",{class:!0});var Rt=i(E);g(B.$$.fragment,Rt),Zt=c(Rt),st=o(Rt,"P",{});var ze=i(st);te=l(ze,"Perform the calibration step and collect the quantization ranges."),ze.forEach(n),Rt.forEach(n),u.forEach(n),this.h()},h(){r(b,"name","hf:doc:metadata"),r(b,"content",JSON.stringify(qe)),r(O,"id","quantization"),r(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(O,"href","#quantization"),r(T,"class","relative group"),r(y,"id","optimum.onnxruntime.quantization.ORTCalibrationDataReader"),r(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(y,"href","#optimum.onnxruntime.quantization.ORTCalibrationDataReader"),r($,"class","relative group"),r(L,"class","docstring"),r(q,"id","optimum.onnxruntime.ORTQuantizer"),r(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(q,"href","#optimum.onnxruntime.ORTQuantizer"),r(R,"class","relative group"),r(J,"class","docstring"),r(Q,"class","docstring"),r(w,"class","docstring"),r(D,"class","docstring"),r(P,"class","docstring"),r(E,"class","docstring"),r(s,"class","docstring")},m(t,d){e(document.head,b),v(t,mt,d),v(t,T,d),e(T,O),e(O,j),_(k,j,null),e(T,Ot),e(T,K),e(K,yt),v(t,ct,d),v(t,$,d),e($,y),e(y,Y),_(C,Y,null),e($,qt),e($,Z),e(Z,Qt),v(t,ut,d),v(t,L,d),_(I,L,null),v(t,pt,d),v(t,R,d),e(R,q),e(q,tt),_(F,tt,null),e(R,wt),e(R,et),e(et,Dt),v(t,lt,d),v(t,s,d),_(U,s,null),e(s,Pt),e(s,nt),e(nt,Et),e(s,Nt),e(s,J),_(A,J,null),e(s,kt),e(s,Q),_(S,Q,null),e(Q,Ct),e(Q,H),e(H,Lt),e(H,at),e(at,It),e(H,Ft),e(s,Ut),e(s,w),_(W,w,null),e(w,At),e(w,ot),e(ot,St),e(s,Ht),e(s,D),_(V,D,null),e(D,Wt),e(D,X),e(X,Vt),e(X,it),e(it,Xt),e(X,Gt),e(s,Mt),e(s,P),_(G,P,null),e(P,Bt),e(P,M),e(M,Jt),e(M,rt),e(rt,jt),e(M,Kt),e(s,Yt),e(s,E),_(B,E,null),e(E,Zt),e(E,st),e(st,te),dt=!0},p:Re,i(t){dt||(f(k.$$.fragment,t),f(C.$$.fragment,t),f(I.$$.fragment,t),f(F.$$.fragment,t),f(U.$$.fragment,t),f(A.$$.fragment,t),f(S.$$.fragment,t),f(W.$$.fragment,t),f(V.$$.fragment,t),f(G.$$.fragment,t),f(B.$$.fragment,t),dt=!0)},o(t){x(k.$$.fragment,t),x(C.$$.fragment,t),x(I.$$.fragment,t),x(F.$$.fragment,t),x(U.$$.fragment,t),x(A.$$.fragment,t),x(S.$$.fragment,t),x(W.$$.fragment,t),x(V.$$.fragment,t),x(G.$$.fragment,t),x(B.$$.fragment,t),dt=!1},d(t){n(b),t&&n(mt),t&&n(T),z(k),t&&n(ct),t&&n($),z(C),t&&n(ut),t&&n(L),z(I),t&&n(pt),t&&n(R),z(F),t&&n(lt),t&&n(s),z(U),z(A),z(S),z(W),z(V),z(G),z(B)}}}const qe={local:"quantization",sections:[{local:"optimum.onnxruntime.quantization.ORTCalibrationDataReader",title:"ORTCalibrationDataReader"},{local:"optimum.onnxruntime.ORTQuantizer",title:"ORTQuantizer"}],title:"Quantization"};function Qe(ne){return Oe(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ee extends ve{constructor(b){super();be(this,b,Qe,ye,Te,{})}}export{Ee as default,qe as metadata};
