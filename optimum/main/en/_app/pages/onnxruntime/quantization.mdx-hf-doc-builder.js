import{S as uo,i as co,s as ho,e as n,k as m,w as h,t as s,M as fo,c as o,d as a,m as p,a as i,x as f,h as r,b as u,G as t,g as c,y as g,L as go,q as _,o as z,B as q,v as _o}from"../../chunks/vendor-hf-doc-builder.js";import{D as xe}from"../../chunks/Docstring-hf-doc-builder.js";import{C as ye}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as ke}from"../../chunks/IconCopyLink-hf-doc-builder.js";function zo(zn){let O,pt,Q,C,Ce,H,Kt,Se,Yt,ut,x,Zt,Ne,ea,ta,W,aa,na,ct,w,S,De,V,oa,Te,ia,Fe,sa,dt,y,ra,Ae,la,ma,Pe,pa,ua,ht,$e,B,ca,Le,da,ha,ft,G,gt,J,Me,fa,_t,K,zt,R,N,Xe,Y,ga,Ie,_a,qt,T,za,Ue,qa,ba,Z,va,xa,bt,ee,vt,j,D,He,te,ya,We,Ta,xt,$,$a,Ve,Oa,Qa,ae,wa,Ra,yt,ne,Tt,E,F,Be,oe,ja,Ge,Ea,$t,v,ka,Je,Ca,Sa,Ke,Na,Da,Ye,Fa,Aa,Ot,Oe,ie,Pa,Ze,La,Ma,Qt,se,wt,re,et,Xa,Rt,le,jt,me,tt,Ia,Et,pe,kt,k,A,at,ue,Ua,nt,Ha,Ct,d,ce,Wa,ot,Va,Ba,Qe,de,Ga,P,he,Ja,it,Ka,Ya,L,fe,Za,ge,en,st,tn,an,nn,M,_e,on,ze,sn,rt,rn,ln,mn,X,qe,pn,lt,un,cn,I,be,dn,ve,hn,mt,fn,gn,St;return H=new ke({}),V=new ke({}),G=new ye({props:{code:`from optimum.onnxruntime import ORTQuantizer, ORTModelForSequenceClassification

ort_model = ORTModelForSequenceClassification.from_pretrained("optimum/distilbert-base-uncased-finetuned-sst-2-english")

quantizer = ORTQuantizer.from_pretrained(ort_model)

...

quantizer.quantize(...)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer, ORTModelForSequenceClassification

<span class="hljs-comment"># Loading ONNX Model from the Hub</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ort_model = ORTModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;optimum/distilbert-base-uncased-finetuned-sst-2-english&quot;</span>)

<span class="hljs-comment"># Create a quantizer from a ORTModelForXXX</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(ort_model)

<span class="hljs-comment"># Configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>...

<span class="hljs-comment"># Quantize the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer.quantize(...)`}}),K=new ye({props:{code:`from optimum.onnxruntime import ORTQuantizer

quantizer = ORTQuantizer.from_pretrained("path/to/model")

...

quantizer.quantize(...)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer

<span class="hljs-comment"># This assumes a model.onnx exists in path/to/model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(<span class="hljs-string">&quot;path/to/model&quot;</span>)

<span class="hljs-comment"># Configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>...

<span class="hljs-comment"># Quantize the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer.quantize(...)`}}),Y=new ke({}),ee=new ye({props:{code:`from optimum.onnxruntime import ORTQuantizer, ORTModelForSequenceClassification
from optimum.onnxruntime.configuration import AutoQuantizationConfig

model_id = "distilbert-base-uncased-finetuned-sst-2-english"
onnx_model = ORTModelForSequenceClassification.from_pretrained(model_id, from_transformers=True)

quantizer = ORTQuantizer.from_pretrained(onnx_model)

dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)

model_quantized_path = quantizer.quantize(`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer, ORTModelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime.configuration <span class="hljs-keyword">import</span> AutoQuantizationConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>model_id = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="hljs-comment"># Load PyTorch model and convert to ONNX</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_model = ORTModelForSequenceClassification.from_pretrained(model_id, from_transformers=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># Create quantizer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(onnx_model)

<span class="hljs-comment"># Define the quantization strategy by creating the appropriate configuration </span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=<span class="hljs-literal">False</span>, per_channel=<span class="hljs-literal">False</span>)

<span class="hljs-comment"># Quantize the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_quantized_path = quantizer.quantize(
    save_dir=<span class="hljs-string">&quot;path/to/output/model&quot;</span>,
    quantization_config=dqconfig,
)`}}),te=new ke({}),ne=new ye({props:{code:`from functools import partial
from transformers import AutoTokenizer
from optimum.onnxruntime import ORTQuantizer, ORTModelForSequenceClassification
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoCalibrationConfig

model_id = "distilbert-base-uncased-finetuned-sst-2-english"

onnx_model = ORTModelForSequenceClassification.from_pretrained(model_id, from_transformers=True)
tokenizer = AutoTokenizer.from_pretrained(model_id)
quantizer = ORTQuantizer.from_pretrained(onnx_model)
qconfig = AutoQuantizationConfig.arm64(is_static=True, per_channel=False)

def preprocess_fn(ex, tokenizer):

calibration_dataset = quantizer.get_calibration_dataset(
calibration_config = AutoCalibrationConfig.minmax(calibration_dataset)

ranges = quantizer.fit(

model_quantized_path = quantizer.quantize(`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> partial
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer, ORTModelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime.configuration <span class="hljs-keyword">import</span> AutoQuantizationConfig, AutoCalibrationConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>model_id = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>

<span class="hljs-comment"># Load PyTorch model and convert to ONNX and create Quantizer and setup config</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_model = ORTModelForSequenceClassification.from_pretrained(model_id, from_transformers=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_id)
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = ORTQuantizer.from_pretrained(onnx_model)
<span class="hljs-meta">&gt;&gt;&gt; </span>qconfig = AutoQuantizationConfig.arm64(is_static=<span class="hljs-literal">True</span>, per_channel=<span class="hljs-literal">False</span>)

<span class="hljs-comment"># Create the calibration dataset</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_fn</span>(<span class="hljs-params">ex, tokenizer</span>):
    <span class="hljs-keyword">return</span> tokenizer(ex[<span class="hljs-string">&quot;sentence&quot;</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>calibration_dataset = quantizer.get_calibration_dataset(
    <span class="hljs-string">&quot;glue&quot;</span>,
    dataset_config_name=<span class="hljs-string">&quot;sst2&quot;</span>,
    preprocess_function=partial(preprocess_fn, tokenizer=tokenizer),
    num_samples=<span class="hljs-number">50</span>,
    dataset_split=<span class="hljs-string">&quot;train&quot;</span>,
)
<span class="hljs-comment"># Create the calibration configuration containing the parameters related to calibration.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>calibration_config = AutoCalibrationConfig.minmax(calibration_dataset)

<span class="hljs-comment"># Perform the calibration step: computes the activations quantization ranges</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ranges = quantizer.fit(
    dataset=calibration_dataset,
    calibration_config=calibration_config,
    operators_to_quantize=qconfig.operators_to_quantize,
)

<span class="hljs-comment"># Apply static quantization on the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_quantized_path = quantizer.quantize(
    save_dir=<span class="hljs-string">&quot;path/to/output/model&quot;</span>,
    calibration_tensors_range=ranges,
    quantization_config=qconfig,
)`}}),oe=new ke({}),se=new ye({props:{code:`from optimum.onnxruntime import ORTQuantizer, ORTModelForSeq2SeqLM
from optimum.onnxruntime.configuration import AutoQuantizationConfig

model_id = "optimum/t5-small"
onnx_model = ORTModelForSeq2SeqLM.from_pretrained(model_id)
model_dir = onnx_model.model_save_dir`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTQuantizer, ORTModelForSeq2SeqLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime.configuration <span class="hljs-keyword">import</span> AutoQuantizationConfig

<span class="hljs-comment"># load Seq2Seq model and set model file directory</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_id = <span class="hljs-string">&quot;optimum/t5-small&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_model = ORTModelForSeq2SeqLM.from_pretrained(model_id)
<span class="hljs-meta">&gt;&gt;&gt; </span>model_dir = onnx_model.model_save_dir`}}),le=new ye({props:{code:`encoder_quantizer = ORTQuantizer.from_pretrained(model_dir, file_name="encoder_model.onnx")

decoder_quantizer = ORTQuantizer.from_pretrained(model_dir, file_name="decoder_model.onnx")

decoder_wp_quantizer = ORTQuantizer.from_pretrained(model_dir, file_name="decoder_with_past_model.onnx")

quantizer = [encoder_quantizer, decoder_quantizer, decoder_wp_quantizer]`,highlighted:`<span class="hljs-comment"># Create encoder quantizer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_quantizer = ORTQuantizer.from_pretrained(model_dir, file_name=<span class="hljs-string">&quot;encoder_model.onnx&quot;</span>)

<span class="hljs-comment"># Create decoder quantizer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_quantizer = ORTQuantizer.from_pretrained(model_dir, file_name=<span class="hljs-string">&quot;decoder_model.onnx&quot;</span>)

<span class="hljs-comment"># Create decoder with past key values quantizer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_wp_quantizer = ORTQuantizer.from_pretrained(model_dir, file_name=<span class="hljs-string">&quot;decoder_with_past_model.onnx&quot;</span>)

<span class="hljs-comment"># Create Quantizer list</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>quantizer = [encoder_quantizer, decoder_quantizer, decoder_wp_quantizer]`}}),pe=new ye({props:{code:`dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)

[q.quantize(save_dir=".",quantization_config=dqconfig) for q in quantizer]`,highlighted:`<span class="hljs-comment"># Define the quantization strategy by creating the appropriate configuration </span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=<span class="hljs-literal">False</span>, per_channel=<span class="hljs-literal">False</span>)

<span class="hljs-comment"># Quantize the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>[q.quantize(save_dir=<span class="hljs-string">&quot;.&quot;</span>,quantization_config=dqconfig) <span class="hljs-keyword">for</span> q <span class="hljs-keyword">in</span> quantizer]`}}),ue=new ke({}),ce=new xe({props:{name:"class optimum.onnxruntime.ORTQuantizer",anchor:"optimum.onnxruntime.ORTQuantizer",parameters:[{name:"onnx_model_path",val:": typing.List[pathlib.Path]"}],source:"https://github.com/huggingface/optimum/blob/main/src/optimum/onnxruntime/quantization.py#L79"}}),de=new xe({props:{name:"compute_ranges",anchor:"optimum.onnxruntime.ORTQuantizer.compute_ranges",parameters:[],source:"https://github.com/huggingface/optimum/blob/main/src/optimum/onnxruntime/quantization.py#L245",returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),he=new xe({props:{name:"fit",anchor:"optimum.onnxruntime.ORTQuantizer.fit",parameters:[{name:"dataset",val:": Dataset"},{name:"calibration_config",val:": CalibrationConfig"},{name:"onnx_augmented_model_name",val:": str = 'augmented_model.onnx'"},{name:"operators_to_quantize",val:": typing.Optional[typing.List[str]] = None"},{name:"batch_size",val:": int = 1"},{name:"use_external_data_format",val:": bool = False"},{name:"use_gpu",val:": bool = False"},{name:"force_symmetric_range",val:": bool = False"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.fit.dataset",description:`<strong>dataset</strong> (<code>Dataset</code>) &#x2014;
The dataset to use when performing the calibration step.`,name:"dataset"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.calibration_config",description:`<strong>calibration_config</strong> (<code>CalibrationConfig</code>) &#x2014;
The configuration containing the parameters related to the calibration step.`,name:"calibration_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.onnx_augmented_model_name",description:`<strong>onnx_augmented_model_name</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the augmented model used to collect the quantization ranges.`,name:"onnx_augmented_model_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.operators_to_quantize",description:`<strong>operators_to_quantize</strong> (<code>list</code>, <em>optional</em>) &#x2014;
List of the operators types to quantize.`,name:"operators_to_quantize"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, defaults to 1) &#x2014;
The batch size to use when collecting the quantization ranges values.`,name:"batch_size"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether uto se external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.use_gpu",description:`<strong>use_gpu</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use the GPU when collecting the quantization ranges values.`,name:"use_gpu"},{anchor:"optimum.onnxruntime.ORTQuantizer.fit.force_symmetric_range",description:`<strong>force_symmetric_range</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to make the quantization ranges symmetric.`,name:"force_symmetric_range"}],source:"https://github.com/huggingface/optimum/blob/main/src/optimum/onnxruntime/quantization.py#L138",returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),fe=new xe({props:{name:"from_pretrained",anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained",parameters:[{name:"model_or_path",val:": typing.Union[str, pathlib.Path]"},{name:"file_name",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.model_or_path",description:`<strong>model_or_path</strong> (<code>Union[str, Path]</code>) &#x2014;
Can be either:<ul>
<li>A path to a saved exported ONNX Intermediate Representation (IR) model, e.g., \`./my_model_directory/.</li>
<li>Or a <code>ORTModelForXX</code> class, e.g., <code>ORTModelForQuestionAnswering</code>.</li>
</ul>`,name:"model_or_path"},{anchor:"optimum.onnxruntime.ORTQuantizer.from_pretrained.file_name(`Union[str,",description:"<strong>file_name(`Union[str,</strong> List[str]]<code>, *optional*) -- Overwrites the default model file name from </code>&#x201C;model.onnx&#x201D;<code>to</code>file_name`.\nThis allows you to load different model files from the same repository or directory.",name:"file_name(`Union[str,"}],source:"https://github.com/huggingface/optimum/blob/main/src/optimum/onnxruntime/quantization.py#L94",returnDescription:`
<p>An instance of <code>ORTQuantizer</code>.</p>
`}}),_e=new xe({props:{name:"get_calibration_dataset",anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset",parameters:[{name:"dataset_name",val:": str"},{name:"num_samples",val:": int = 100"},{name:"dataset_config_name",val:": typing.Optional[str] = None"},{name:"dataset_split",val:": typing.Optional[str] = None"},{name:"preprocess_function",val:": typing.Optional[typing.Callable] = None"},{name:"preprocess_batch",val:": bool = True"},{name:"seed",val:": int = 2016"},{name:"use_auth_token",val:": bool = False"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_name",description:`<strong>dataset_name</strong> (<code>str</code>) &#x2014;
The dataset repository name on the Hugging Face Hub or path to a local directory containing data files
to load to use for the calibration step.`,name:"dataset_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.num_samples",description:`<strong>num_samples</strong> (<code>int</code>, defaults to 100) &#x2014;
The maximum number of samples composing the calibration dataset.`,name:"num_samples"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_config_name",description:`<strong>dataset_config_name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The name of the dataset configuration.`,name:"dataset_config_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.dataset_split",description:`<strong>dataset_split</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Which split of the dataset to use to perform the calibration step.`,name:"dataset_split"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.preprocess_function",description:`<strong>preprocess_function</strong> (<code>Callable</code>, <em>optional</em>) &#x2014;
Processing function to apply to each example after loading dataset.`,name:"preprocess_function"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.preprocess_batch",description:`<strong>preprocess_batch</strong> (<code>bool</code>, defaults to <code>True</code>) &#x2014;
Whether the <code>preprocess_function</code> should be batched.`,name:"preprocess_batch"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.seed",description:`<strong>seed</strong> (<code>int</code>, defaults to 2016) &#x2014;
The random seed to use when shuffling the calibration dataset.`,name:"seed"},{anchor:"optimum.onnxruntime.ORTQuantizer.get_calibration_dataset.use_auth_token",description:`<strong>use_auth_token</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use the token generated when running <code>transformers-cli login</code> (necessary for some datasets
like ImageNet).`,name:"use_auth_token"}],source:"https://github.com/huggingface/optimum/blob/main/src/optimum/onnxruntime/quantization.py#L357",returnDescription:`
<p>The calibration <code>datasets.Dataset</code> to use for the post-training static quantization calibration
step.</p>
`}}),qe=new xe({props:{name:"partial_fit",anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit",parameters:[{name:"dataset",val:": Dataset"},{name:"calibration_config",val:": CalibrationConfig"},{name:"onnx_augmented_model_name",val:": str = 'augmented_model.onnx'"},{name:"operators_to_quantize",val:": typing.Optional[typing.List[str]] = None"},{name:"batch_size",val:": int = 1"},{name:"use_external_data_format",val:": bool = False"},{name:"use_gpu",val:": bool = False"},{name:"force_symmetric_range",val:": bool = False"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.dataset",description:`<strong>dataset</strong> (<code>Dataset</code>) &#x2014;
The dataset to use when performing the calibration step.`,name:"dataset"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.calibration_config",description:`<strong>calibration_config</strong> (<code>CalibrationConfig</code>) &#x2014;
The configuration containing the parameters related to the calibration step.`,name:"calibration_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.onnx_augmented_model_name",description:`<strong>onnx_augmented_model_name</strong> (<code>Union[str, os.PathLike]</code>) &#x2014;
The path used to save the augmented model used to collect the quantization ranges.`,name:"onnx_augmented_model_name"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.operators_to_quantize",description:`<strong>operators_to_quantize</strong> (<code>list</code>, <em>optional</em>) &#x2014;
List of the operators types to quantize.`,name:"operators_to_quantize"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, defaults to 1) &#x2014;
The batch size to use when collecting the quantization ranges values.`,name:"batch_size"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether uto se external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.use_gpu",description:`<strong>use_gpu</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use the GPU when collecting the quantization ranges values.`,name:"use_gpu"},{anchor:"optimum.onnxruntime.ORTQuantizer.partial_fit.force_symmetric_range",description:`<strong>force_symmetric_range</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to make the quantization ranges symmetric.`,name:"force_symmetric_range"}],source:"https://github.com/huggingface/optimum/blob/main/src/optimum/onnxruntime/quantization.py#L192",returnDescription:`
<p>The dictionary mapping the nodes name to their quantization ranges.</p>
`}}),be=new xe({props:{name:"quantize",anchor:"optimum.onnxruntime.ORTQuantizer.quantize",parameters:[{name:"quantization_config",val:": QuantizationConfig"},{name:"save_dir",val:": typing.Union[str, pathlib.Path]"},{name:"file_suffix",val:": typing.Optional[str] = 'quantized'"},{name:"calibration_tensors_range",val:": typing.Union[typing.Dict[str, typing.Tuple[float, float]], NoneType] = None"},{name:"use_external_data_format",val:": bool = False"},{name:"preprocessor",val:": typing.Optional[optimum.onnxruntime.preprocessors.quantization.QuantizationPreprocessor] = None"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTQuantizer.quantize.quantization_config",description:`<strong>quantization_config</strong> (<code>QuantizationConfig</code>) &#x2014;
The configuration containing the parameters related to quantization.`,name:"quantization_config"},{anchor:"optimum.onnxruntime.ORTQuantizer.quantize.save_dir",description:`<strong>save_dir</strong> (<code>Union[str, Path]</code>) &#x2014;
The directory where the quantized model should be saved.`,name:"save_dir"},{anchor:"optimum.onnxruntime.ORTQuantizer.quantize.file_suffix",description:`<strong>file_suffix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;quantized&quot;</code>) &#x2014;
The file_suffix used to save the quantized model.`,name:"file_suffix"},{anchor:"optimum.onnxruntime.ORTQuantizer.quantize.calibration_tensors_range",description:`<strong>calibration_tensors_range</strong> (<code>Dict[NodeName, Tuple[float, float]]</code>, <em>optional</em>) &#x2014;
The dictionary mapping the nodes name to their quantization ranges, used and required only when applying
static quantization.`,name:"calibration_tensors_range"},{anchor:"optimum.onnxruntime.ORTQuantizer.quantize.use_external_data_format",description:`<strong>use_external_data_format</strong> (<code>bool</code>, defaults to <code>False</code>) &#x2014;
Whether to use external data format to store model which size is &gt;= 2Gb.`,name:"use_external_data_format"},{anchor:"optimum.onnxruntime.ORTQuantizer.quantize.preprocessor",description:`<strong>preprocessor</strong> (<code>QuantizationPreprocessor</code>, <em>optional</em>) &#x2014;
The preprocessor to use to collect the nodes to include or exclude from quantization.`,name:"preprocessor"}],source:"https://github.com/huggingface/optimum/blob/main/src/optimum/onnxruntime/quantization.py#L258",returnDescription:`
<p>The path of the resulting quantized model.</p>
`}}),{c(){O=n("meta"),pt=m(),Q=n("h1"),C=n("a"),Ce=n("span"),h(H.$$.fragment),Kt=m(),Se=n("span"),Yt=s("Quantization"),ut=m(),x=n("p"),Zt=s("\u{1F917} Optimum provides an "),Ne=n("code"),ea=s("optimum.onnxruntime"),ta=s(" package that enables you to apply quantization on many model hosted on the \u{1F917} hub using the "),W=n("a"),aa=s("ONNX Runtime"),na=s(" quantization tool."),ct=m(),w=n("h2"),S=n("a"),De=n("span"),h(V.$$.fragment),oa=m(),Te=n("span"),ia=s("Creating an "),Fe=n("code"),sa=s("ORTQuantizer"),dt=m(),y=n("p"),ra=s("The "),Ae=n("code"),la=s("ORTQuantizer"),ma=s(" class is used to quantize your ONNX model. The class can be initialized using the "),Pe=n("code"),pa=s("from_pretrained()"),ua=s(" method, which supports different checkpoint formats."),ht=m(),$e=n("ol"),B=n("li"),ca=s("Using an already initialized "),Le=n("code"),da=s("ORTModelForXXX"),ha=s(" class."),ft=m(),h(G.$$.fragment),gt=m(),J=n("ol"),Me=n("li"),fa=s("Using a local ONNX model from a directory."),_t=m(),h(K.$$.fragment),zt=m(),R=n("h2"),N=n("a"),Xe=n("span"),h(Y.$$.fragment),ga=m(),Ie=n("span"),_a=s("Dynamic Quantization example"),qt=m(),T=n("p"),za=s("The "),Ue=n("code"),qa=s("ORTQuantizer"),ba=s(" class can be used to dynamically quantize your ONNX model. Below you will find an easy end-to-end example on how to dynamically quantize "),Z=n("a"),va=s("distilbert-base-uncased-finetuned-sst-2-english"),xa=s("."),bt=m(),h(ee.$$.fragment),vt=m(),j=n("h2"),D=n("a"),He=n("span"),h(te.$$.fragment),ya=m(),We=n("span"),Ta=s("Static Quantization example"),xt=m(),$=n("p"),$a=s("The "),Ve=n("code"),Oa=s("ORTQuantizer"),Qa=s(" class can be used to statically quantize your ONNX model. Below you will find an easy end-to-end example on how to statically quantize "),ae=n("a"),wa=s("distilbert-base-uncased-finetuned-sst-2-english"),Ra=s("."),yt=m(),h(ne.$$.fragment),Tt=m(),E=n("h2"),F=n("a"),Be=n("span"),h(oe.$$.fragment),ja=m(),Ge=n("span"),Ea=s("Quantize Seq2Seq models"),$t=m(),v=n("p"),ka=s("The "),Je=n("code"),Ca=s("ORTQuantizer"),Sa=s(" currently doesn\u2019t support multi-file models, like "),Ke=n("code"),Na=s("ORTModelForSeq2SeqLM"),Da=s(". If you want to quantize a Seq2Seq model, you have to quantize each model\u2019s component individually using the "),Ye=n("code"),Fa=s("ORTQuantizer"),Aa=s(" class. Currently, only dynamic quantization is supported for Seq2Seq model."),Ot=m(),Oe=n("ol"),ie=n("li"),Pa=s("Load seq2seq model as "),Ze=n("code"),La=s("ORTModelForSeq2SeqLM"),Ma=s("."),Qt=m(),h(se.$$.fragment),wt=m(),re=n("ol"),et=n("li"),Xa=s("Define Quantizer for encoder, decoder and decoder with past keys"),Rt=m(),h(le.$$.fragment),jt=m(),me=n("ol"),tt=n("li"),Ia=s("Quantize all models"),Et=m(),h(pe.$$.fragment),kt=m(),k=n("h2"),A=n("a"),at=n("span"),h(ue.$$.fragment),Ua=m(),nt=n("span"),Ha=s("ORTQuantizer"),Ct=m(),d=n("div"),h(ce.$$.fragment),Wa=m(),ot=n("p"),Va=s("Handles the ONNX Runtime quantization process for models shared on huggingface.co/models."),Ba=m(),Qe=n("div"),h(de.$$.fragment),Ga=m(),P=n("div"),h(he.$$.fragment),Ja=m(),it=n("p"),Ka=s("Perform the calibration step and collect the quantization ranges."),Ya=m(),L=n("div"),h(fe.$$.fragment),Za=m(),ge=n("p"),en=s("Instantiate a "),st=n("code"),tn=s("ORTQuantizer"),an=s(" from a pretrained pytorch model and preprocessor."),nn=m(),M=n("div"),h(_e.$$.fragment),on=m(),ze=n("p"),sn=s("Create the calibration "),rt=n("code"),rn=s("datasets.Dataset"),ln=s(" to use for the post-training static quantization calibration step"),mn=m(),X=n("div"),h(qe.$$.fragment),pn=m(),lt=n("p"),un=s("Perform the calibration step and collect the quantization ranges."),cn=m(),I=n("div"),h(be.$$.fragment),dn=m(),ve=n("p"),hn=s("Quantize a model given the optimization specifications defined in "),mt=n("code"),fn=s("quantization_config"),gn=s("."),this.h()},l(e){const l=fo('[data-svelte="svelte-1phssyn"]',document.head);O=o(l,"META",{name:!0,content:!0}),l.forEach(a),pt=p(e),Q=o(e,"H1",{class:!0});var Nt=i(Q);C=o(Nt,"A",{id:!0,class:!0,href:!0});var qn=i(C);Ce=o(qn,"SPAN",{});var bn=i(Ce);f(H.$$.fragment,bn),bn.forEach(a),qn.forEach(a),Kt=p(Nt),Se=o(Nt,"SPAN",{});var vn=i(Se);Yt=r(vn,"Quantization"),vn.forEach(a),Nt.forEach(a),ut=p(e),x=o(e,"P",{});var we=i(x);Zt=r(we,"\u{1F917} Optimum provides an "),Ne=o(we,"CODE",{});var xn=i(Ne);ea=r(xn,"optimum.onnxruntime"),xn.forEach(a),ta=r(we," package that enables you to apply quantization on many model hosted on the \u{1F917} hub using the "),W=o(we,"A",{href:!0,rel:!0});var yn=i(W);aa=r(yn,"ONNX Runtime"),yn.forEach(a),na=r(we," quantization tool."),we.forEach(a),ct=p(e),w=o(e,"H2",{class:!0});var Dt=i(w);S=o(Dt,"A",{id:!0,class:!0,href:!0});var Tn=i(S);De=o(Tn,"SPAN",{});var $n=i(De);f(V.$$.fragment,$n),$n.forEach(a),Tn.forEach(a),oa=p(Dt),Te=o(Dt,"SPAN",{});var _n=i(Te);ia=r(_n,"Creating an "),Fe=o(_n,"CODE",{});var On=i(Fe);sa=r(On,"ORTQuantizer"),On.forEach(a),_n.forEach(a),Dt.forEach(a),dt=p(e),y=o(e,"P",{});var Re=i(y);ra=r(Re,"The "),Ae=o(Re,"CODE",{});var Qn=i(Ae);la=r(Qn,"ORTQuantizer"),Qn.forEach(a),ma=r(Re," class is used to quantize your ONNX model. The class can be initialized using the "),Pe=o(Re,"CODE",{});var wn=i(Pe);pa=r(wn,"from_pretrained()"),wn.forEach(a),ua=r(Re," method, which supports different checkpoint formats."),Re.forEach(a),ht=p(e),$e=o(e,"OL",{});var Rn=i($e);B=o(Rn,"LI",{});var Ft=i(B);ca=r(Ft,"Using an already initialized "),Le=o(Ft,"CODE",{});var jn=i(Le);da=r(jn,"ORTModelForXXX"),jn.forEach(a),ha=r(Ft," class."),Ft.forEach(a),Rn.forEach(a),ft=p(e),f(G.$$.fragment,e),gt=p(e),J=o(e,"OL",{start:!0});var En=i(J);Me=o(En,"LI",{});var kn=i(Me);fa=r(kn,"Using a local ONNX model from a directory."),kn.forEach(a),En.forEach(a),_t=p(e),f(K.$$.fragment,e),zt=p(e),R=o(e,"H2",{class:!0});var At=i(R);N=o(At,"A",{id:!0,class:!0,href:!0});var Cn=i(N);Xe=o(Cn,"SPAN",{});var Sn=i(Xe);f(Y.$$.fragment,Sn),Sn.forEach(a),Cn.forEach(a),ga=p(At),Ie=o(At,"SPAN",{});var Nn=i(Ie);_a=r(Nn,"Dynamic Quantization example"),Nn.forEach(a),At.forEach(a),qt=p(e),T=o(e,"P",{});var je=i(T);za=r(je,"The "),Ue=o(je,"CODE",{});var Dn=i(Ue);qa=r(Dn,"ORTQuantizer"),Dn.forEach(a),ba=r(je," class can be used to dynamically quantize your ONNX model. Below you will find an easy end-to-end example on how to dynamically quantize "),Z=o(je,"A",{href:!0,rel:!0});var Fn=i(Z);va=r(Fn,"distilbert-base-uncased-finetuned-sst-2-english"),Fn.forEach(a),xa=r(je,"."),je.forEach(a),bt=p(e),f(ee.$$.fragment,e),vt=p(e),j=o(e,"H2",{class:!0});var Pt=i(j);D=o(Pt,"A",{id:!0,class:!0,href:!0});var An=i(D);He=o(An,"SPAN",{});var Pn=i(He);f(te.$$.fragment,Pn),Pn.forEach(a),An.forEach(a),ya=p(Pt),We=o(Pt,"SPAN",{});var Ln=i(We);Ta=r(Ln,"Static Quantization example"),Ln.forEach(a),Pt.forEach(a),xt=p(e),$=o(e,"P",{});var Ee=i($);$a=r(Ee,"The "),Ve=o(Ee,"CODE",{});var Mn=i(Ve);Oa=r(Mn,"ORTQuantizer"),Mn.forEach(a),Qa=r(Ee," class can be used to statically quantize your ONNX model. Below you will find an easy end-to-end example on how to statically quantize "),ae=o(Ee,"A",{href:!0,rel:!0});var Xn=i(ae);wa=r(Xn,"distilbert-base-uncased-finetuned-sst-2-english"),Xn.forEach(a),Ra=r(Ee,"."),Ee.forEach(a),yt=p(e),f(ne.$$.fragment,e),Tt=p(e),E=o(e,"H2",{class:!0});var Lt=i(E);F=o(Lt,"A",{id:!0,class:!0,href:!0});var In=i(F);Be=o(In,"SPAN",{});var Un=i(Be);f(oe.$$.fragment,Un),Un.forEach(a),In.forEach(a),ja=p(Lt),Ge=o(Lt,"SPAN",{});var Hn=i(Ge);Ea=r(Hn,"Quantize Seq2Seq models"),Hn.forEach(a),Lt.forEach(a),$t=p(e),v=o(e,"P",{});var U=i(v);ka=r(U,"The "),Je=o(U,"CODE",{});var Wn=i(Je);Ca=r(Wn,"ORTQuantizer"),Wn.forEach(a),Sa=r(U," currently doesn\u2019t support multi-file models, like "),Ke=o(U,"CODE",{});var Vn=i(Ke);Na=r(Vn,"ORTModelForSeq2SeqLM"),Vn.forEach(a),Da=r(U,". If you want to quantize a Seq2Seq model, you have to quantize each model\u2019s component individually using the "),Ye=o(U,"CODE",{});var Bn=i(Ye);Fa=r(Bn,"ORTQuantizer"),Bn.forEach(a),Aa=r(U," class. Currently, only dynamic quantization is supported for Seq2Seq model."),U.forEach(a),Ot=p(e),Oe=o(e,"OL",{});var Gn=i(Oe);ie=o(Gn,"LI",{});var Mt=i(ie);Pa=r(Mt,"Load seq2seq model as "),Ze=o(Mt,"CODE",{});var Jn=i(Ze);La=r(Jn,"ORTModelForSeq2SeqLM"),Jn.forEach(a),Ma=r(Mt,"."),Mt.forEach(a),Gn.forEach(a),Qt=p(e),f(se.$$.fragment,e),wt=p(e),re=o(e,"OL",{start:!0});var Kn=i(re);et=o(Kn,"LI",{});var Yn=i(et);Xa=r(Yn,"Define Quantizer for encoder, decoder and decoder with past keys"),Yn.forEach(a),Kn.forEach(a),Rt=p(e),f(le.$$.fragment,e),jt=p(e),me=o(e,"OL",{start:!0});var Zn=i(me);tt=o(Zn,"LI",{});var eo=i(tt);Ia=r(eo,"Quantize all models"),eo.forEach(a),Zn.forEach(a),Et=p(e),f(pe.$$.fragment,e),kt=p(e),k=o(e,"H2",{class:!0});var Xt=i(k);A=o(Xt,"A",{id:!0,class:!0,href:!0});var to=i(A);at=o(to,"SPAN",{});var ao=i(at);f(ue.$$.fragment,ao),ao.forEach(a),to.forEach(a),Ua=p(Xt),nt=o(Xt,"SPAN",{});var no=i(nt);Ha=r(no,"ORTQuantizer"),no.forEach(a),Xt.forEach(a),Ct=p(e),d=o(e,"DIV",{class:!0});var b=i(d);f(ce.$$.fragment,b),Wa=p(b),ot=o(b,"P",{});var oo=i(ot);Va=r(oo,"Handles the ONNX Runtime quantization process for models shared on huggingface.co/models."),oo.forEach(a),Ba=p(b),Qe=o(b,"DIV",{class:!0});var io=i(Qe);f(de.$$.fragment,io),io.forEach(a),Ga=p(b),P=o(b,"DIV",{class:!0});var It=i(P);f(he.$$.fragment,It),Ja=p(It),it=o(It,"P",{});var so=i(it);Ka=r(so,"Perform the calibration step and collect the quantization ranges."),so.forEach(a),It.forEach(a),Ya=p(b),L=o(b,"DIV",{class:!0});var Ut=i(L);f(fe.$$.fragment,Ut),Za=p(Ut),ge=o(Ut,"P",{});var Ht=i(ge);en=r(Ht,"Instantiate a "),st=o(Ht,"CODE",{});var ro=i(st);tn=r(ro,"ORTQuantizer"),ro.forEach(a),an=r(Ht," from a pretrained pytorch model and preprocessor."),Ht.forEach(a),Ut.forEach(a),nn=p(b),M=o(b,"DIV",{class:!0});var Wt=i(M);f(_e.$$.fragment,Wt),on=p(Wt),ze=o(Wt,"P",{});var Vt=i(ze);sn=r(Vt,"Create the calibration "),rt=o(Vt,"CODE",{});var lo=i(rt);rn=r(lo,"datasets.Dataset"),lo.forEach(a),ln=r(Vt," to use for the post-training static quantization calibration step"),Vt.forEach(a),Wt.forEach(a),mn=p(b),X=o(b,"DIV",{class:!0});var Bt=i(X);f(qe.$$.fragment,Bt),pn=p(Bt),lt=o(Bt,"P",{});var mo=i(lt);un=r(mo,"Perform the calibration step and collect the quantization ranges."),mo.forEach(a),Bt.forEach(a),cn=p(b),I=o(b,"DIV",{class:!0});var Gt=i(I);f(be.$$.fragment,Gt),dn=p(Gt),ve=o(Gt,"P",{});var Jt=i(ve);hn=r(Jt,"Quantize a model given the optimization specifications defined in "),mt=o(Jt,"CODE",{});var po=i(mt);fn=r(po,"quantization_config"),po.forEach(a),gn=r(Jt,"."),Jt.forEach(a),Gt.forEach(a),b.forEach(a),this.h()},h(){u(O,"name","hf:doc:metadata"),u(O,"content",JSON.stringify(qo)),u(C,"id","quantization"),u(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(C,"href","#quantization"),u(Q,"class","relative group"),u(W,"href","https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/quantization/README.md"),u(W,"rel","nofollow"),u(S,"id","creating-an-ortquantizer"),u(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(S,"href","#creating-an-ortquantizer"),u(w,"class","relative group"),u(J,"start","2"),u(N,"id","dynamic-quantization-example"),u(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(N,"href","#dynamic-quantization-example"),u(R,"class","relative group"),u(Z,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),u(Z,"rel","nofollow"),u(D,"id","static-quantization-example"),u(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(D,"href","#static-quantization-example"),u(j,"class","relative group"),u(ae,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),u(ae,"rel","nofollow"),u(F,"id","quantize-seq2seq-models"),u(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(F,"href","#quantize-seq2seq-models"),u(E,"class","relative group"),u(re,"start","2"),u(me,"start","3"),u(A,"id","optimum.onnxruntime.ORTQuantizer"),u(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(A,"href","#optimum.onnxruntime.ORTQuantizer"),u(k,"class","relative group"),u(Qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),u(d,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,l){t(document.head,O),c(e,pt,l),c(e,Q,l),t(Q,C),t(C,Ce),g(H,Ce,null),t(Q,Kt),t(Q,Se),t(Se,Yt),c(e,ut,l),c(e,x,l),t(x,Zt),t(x,Ne),t(Ne,ea),t(x,ta),t(x,W),t(W,aa),t(x,na),c(e,ct,l),c(e,w,l),t(w,S),t(S,De),g(V,De,null),t(w,oa),t(w,Te),t(Te,ia),t(Te,Fe),t(Fe,sa),c(e,dt,l),c(e,y,l),t(y,ra),t(y,Ae),t(Ae,la),t(y,ma),t(y,Pe),t(Pe,pa),t(y,ua),c(e,ht,l),c(e,$e,l),t($e,B),t(B,ca),t(B,Le),t(Le,da),t(B,ha),c(e,ft,l),g(G,e,l),c(e,gt,l),c(e,J,l),t(J,Me),t(Me,fa),c(e,_t,l),g(K,e,l),c(e,zt,l),c(e,R,l),t(R,N),t(N,Xe),g(Y,Xe,null),t(R,ga),t(R,Ie),t(Ie,_a),c(e,qt,l),c(e,T,l),t(T,za),t(T,Ue),t(Ue,qa),t(T,ba),t(T,Z),t(Z,va),t(T,xa),c(e,bt,l),g(ee,e,l),c(e,vt,l),c(e,j,l),t(j,D),t(D,He),g(te,He,null),t(j,ya),t(j,We),t(We,Ta),c(e,xt,l),c(e,$,l),t($,$a),t($,Ve),t(Ve,Oa),t($,Qa),t($,ae),t(ae,wa),t($,Ra),c(e,yt,l),g(ne,e,l),c(e,Tt,l),c(e,E,l),t(E,F),t(F,Be),g(oe,Be,null),t(E,ja),t(E,Ge),t(Ge,Ea),c(e,$t,l),c(e,v,l),t(v,ka),t(v,Je),t(Je,Ca),t(v,Sa),t(v,Ke),t(Ke,Na),t(v,Da),t(v,Ye),t(Ye,Fa),t(v,Aa),c(e,Ot,l),c(e,Oe,l),t(Oe,ie),t(ie,Pa),t(ie,Ze),t(Ze,La),t(ie,Ma),c(e,Qt,l),g(se,e,l),c(e,wt,l),c(e,re,l),t(re,et),t(et,Xa),c(e,Rt,l),g(le,e,l),c(e,jt,l),c(e,me,l),t(me,tt),t(tt,Ia),c(e,Et,l),g(pe,e,l),c(e,kt,l),c(e,k,l),t(k,A),t(A,at),g(ue,at,null),t(k,Ua),t(k,nt),t(nt,Ha),c(e,Ct,l),c(e,d,l),g(ce,d,null),t(d,Wa),t(d,ot),t(ot,Va),t(d,Ba),t(d,Qe),g(de,Qe,null),t(d,Ga),t(d,P),g(he,P,null),t(P,Ja),t(P,it),t(it,Ka),t(d,Ya),t(d,L),g(fe,L,null),t(L,Za),t(L,ge),t(ge,en),t(ge,st),t(st,tn),t(ge,an),t(d,nn),t(d,M),g(_e,M,null),t(M,on),t(M,ze),t(ze,sn),t(ze,rt),t(rt,rn),t(ze,ln),t(d,mn),t(d,X),g(qe,X,null),t(X,pn),t(X,lt),t(lt,un),t(d,cn),t(d,I),g(be,I,null),t(I,dn),t(I,ve),t(ve,hn),t(ve,mt),t(mt,fn),t(ve,gn),St=!0},p:go,i(e){St||(_(H.$$.fragment,e),_(V.$$.fragment,e),_(G.$$.fragment,e),_(K.$$.fragment,e),_(Y.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(ne.$$.fragment,e),_(oe.$$.fragment,e),_(se.$$.fragment,e),_(le.$$.fragment,e),_(pe.$$.fragment,e),_(ue.$$.fragment,e),_(ce.$$.fragment,e),_(de.$$.fragment,e),_(he.$$.fragment,e),_(fe.$$.fragment,e),_(_e.$$.fragment,e),_(qe.$$.fragment,e),_(be.$$.fragment,e),St=!0)},o(e){z(H.$$.fragment,e),z(V.$$.fragment,e),z(G.$$.fragment,e),z(K.$$.fragment,e),z(Y.$$.fragment,e),z(ee.$$.fragment,e),z(te.$$.fragment,e),z(ne.$$.fragment,e),z(oe.$$.fragment,e),z(se.$$.fragment,e),z(le.$$.fragment,e),z(pe.$$.fragment,e),z(ue.$$.fragment,e),z(ce.$$.fragment,e),z(de.$$.fragment,e),z(he.$$.fragment,e),z(fe.$$.fragment,e),z(_e.$$.fragment,e),z(qe.$$.fragment,e),z(be.$$.fragment,e),St=!1},d(e){a(O),e&&a(pt),e&&a(Q),q(H),e&&a(ut),e&&a(x),e&&a(ct),e&&a(w),q(V),e&&a(dt),e&&a(y),e&&a(ht),e&&a($e),e&&a(ft),q(G,e),e&&a(gt),e&&a(J),e&&a(_t),q(K,e),e&&a(zt),e&&a(R),q(Y),e&&a(qt),e&&a(T),e&&a(bt),q(ee,e),e&&a(vt),e&&a(j),q(te),e&&a(xt),e&&a($),e&&a(yt),q(ne,e),e&&a(Tt),e&&a(E),q(oe),e&&a($t),e&&a(v),e&&a(Ot),e&&a(Oe),e&&a(Qt),q(se,e),e&&a(wt),e&&a(re),e&&a(Rt),q(le,e),e&&a(jt),e&&a(me),e&&a(Et),q(pe,e),e&&a(kt),e&&a(k),q(ue),e&&a(Ct),e&&a(d),q(ce),q(de),q(he),q(fe),q(_e),q(qe),q(be)}}}const qo={local:"quantization",sections:[{local:"creating-an-ortquantizer",title:"Creating an `ORTQuantizer`"},{local:"dynamic-quantization-example",title:"Dynamic Quantization example "},{local:"static-quantization-example",title:"Static Quantization example "},{local:"quantize-seq2seq-models",title:"Quantize Seq2Seq models"},{local:"optimum.onnxruntime.ORTQuantizer",title:"ORTQuantizer"}],title:"Quantization"};function bo(zn){return _o(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class $o extends uo{constructor(O){super();co(this,O,bo,zo,ho,{})}}export{$o as default,qo as metadata};
