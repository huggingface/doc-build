import{S as an,i as on,s as rn,e as i,k as n,w as Qe,t as p,M as sn,c as a,d as t,m as f,a as r,x as Ve,h as c,b as u,N as nl,G as l,g as s,y as Xe,L as nn,q as Ye,o as Ze,B as et,v as fn}from"../../chunks/vendor-hf-doc-builder.js";import{I as tt}from"../../chunks/IconCopyLink-hf-doc-builder.js";function pn(Yo){let L,fl,y,C,lt,K,gi,it,Ni,pl,g,Ti,at,Ri,Mi,cl,ce,Oi,ul,v,ot,ki,zi,rt,Si,Ui,st,qi,Bi,nt,Gi,Hi,ft,Ki,vl,P,N,pt,W,Wi,ct,Di,ml,D,Zo,hl,J,Ji,F,ut,Fi,bl,ue,Qi,_l,ve,Vi,wl,me,Xi,El,he,vt,Yi,dl,be,Zi,Al,m,mt,ea,ta,ht,la,ia,bt,aa,oa,_t,ra,Ll,_e,sa,yl,E,wt,na,fa,Et,pa,ca,dt,ua,Pl,we,va,Il,I,T,At,Q,ma,Lt,ha,jl,V,er,xl,X,ba,Y,yt,_a,$l,Ee,wa,Cl,de,Ea,gl,Ae,da,Nl,Le,Pt,Aa,Tl,ye,La,Rl,h,It,ya,Pa,jt,Ia,ja,xt,xa,$a,$t,Ca,Ml,Pe,ga,Ol,R,Ct,Na,Ta,gt,Ra,kl,Ie,Ma,zl,j,M,Nt,Z,Oa,Tt,ka,Sl,ee,tr,Ul,te,za,le,Rt,Sa,ql,je,Ua,Bl,xe,qa,Gl,$e,Ba,Hl,Ce,Mt,Ga,Kl,ge,Ha,Wl,b,Ot,Ka,Wa,kt,Da,Ja,zt,Fa,Qa,St,Va,Dl,Ne,Xa,Jl,O,Ut,Ya,Za,qt,eo,Fl,Te,to,Ql,x,k,Bt,ie,lo,Gt,io,Vl,ae,lr,Xl,oe,ao,re,Ht,oo,Yl,Re,ro,Zl,Me,so,ei,Oe,no,ti,ke,Kt,fo,li,ze,po,ii,_,Wt,co,uo,Dt,vo,mo,Jt,ho,bo,Ft,_o,ai,Se,wo,oi,d,Qt,Eo,Ao,Vt,Lo,yo,Xt,Po,ri,Ue,Io,si,$,z,Yt,se,jo,Zt,xo,ni,ne,ir,fi,fe,$o,pe,el,Co,pi,qe,go,ci,Be,No,ui,Ge,To,vi,He,tl,Ro,mi,Ke,Mo,hi,w,ll,Oo,ko,il,zo,So,al,Uo,qo,ol,Bo,bi,We,Go,_i,S,rl,Ho,Ko,sl,Wo,wi,De,Do,Ei;return K=new tt({}),W=new tt({}),Q=new tt({}),Z=new tt({}),ie=new tt({}),se=new tt({}),{c(){L=i("meta"),fl=n(),y=i("h1"),C=i("a"),lt=i("span"),Qe(K.$$.fragment),gi=n(),it=i("span"),Ni=p("Using \u{1F917} Simulate to learn Agent behaviors with Stable-Baselines3"),pl=n(),g=i("p"),Ti=p("We provide several example RL integrations with the Stable-Baselines3 (LINK) library. To install this dependancy use "),at=i("code"),Ri=p("pip install simulate[sb3]"),Mi=p("."),cl=n(),ce=i("p"),Oi=p("Including:"),ul=n(),v=i("ul"),ot=i("li"),ki=p("Learning to navigate in a simple T-Maze"),zi=n(),rt=i("li"),Si=p("Collecting objects"),Ui=n(),st=i("li"),qi=p("Navigating in procedurally generated mazes"),Bi=n(),nt=i("li"),Gi=p("Physical interaction with movable objects"),Hi=n(),ft=i("li"),Ki=p("Reward functions based on line of sight observation of objects."),vl=n(),P=i("h2"),N=i("a"),pt=i("span"),Qe(W.$$.fragment),Wi=n(),ct=i("span"),Di=p("Learning to navigate in a simple T-Maze"),ml=n(),D=i("img"),hl=n(),J=i("p"),Ji=p("Example: "),F=i("a"),ut=i("code"),Fi=p("sb3_basic_maze.py"),bl=n(),ue=i("p"),Qi=p("Objective: Navigate to a spherical object in a simple T-Maze. Upon object collection, the environment resets."),_l=n(),ve=i("p"),Vi=p("Actors: An EgoCentric Camera Actor (LINK) equipped with a monocular camera."),wl=n(),me=i("p"),Xi=p("Observation space:"),El=n(),he=i("ul"),vt=i("li"),Yi=p("An RGB camera of shape (3, 40, 40)  (C, H, W) in uint8 format."),dl=n(),be=i("p"),Zi=p("Action space:"),Al=n(),m=i("ul"),mt=i("li"),ea=p("A discrete action space with 3 possible actions"),ta=n(),ht=i("li"),la=p("Turn left 10 degrees"),ia=n(),bt=i("li"),aa=p("Turn right 10 degrees"),oa=n(),_t=i("li"),ra=p("Move forward"),Ll=n(),_e=i("p"),sa=p("Reward function:"),yl=n(),E=i("ul"),wt=i("li"),na=p("A dense reward based on improvement in best euclidean distance to the object"),fa=n(),Et=i("li"),pa=p("A sparse reward of +1 when the object is collected"),ca=n(),dt=i("li"),ua=p("A timeout penaly of -1 if the agent does not reach the object in 200 time-steps"),Pl=n(),we=i("p"),va=p("Parallel: 4 independent instances of the same environment configuration."),Il=n(),I=i("h2"),T=i("a"),At=i("span"),Qe(Q.$$.fragment),ma=n(),Lt=i("span"),ha=p("Collecting objects"),jl=n(),V=i("img"),xl=n(),X=i("p"),ba=p("Example: "),Y=i("a"),yt=i("code"),_a=p("sb3_collectables.py"),$l=n(),Ee=i("p"),wa=p("Objective: Collect all 20 objects in a large square room."),Cl=n(),de=i("p"),Ea=p("Actors: An EgoCentric Camera Actor (LINK) equipped with a monocular camera."),gl=n(),Ae=i("p"),da=p("Observation space:"),Nl=n(),Le=i("ul"),Pt=i("li"),Aa=p("An RGB camera of shape (3, 40, 40)  (C, H, W) in uint8 format."),Tl=n(),ye=i("p"),La=p("Action space:"),Rl=n(),h=i("ul"),It=i("li"),ya=p("A discrete action space with 3 possible actions"),Pa=n(),jt=i("li"),Ia=p("Turn left 10 degrees"),ja=n(),xt=i("li"),xa=p("Turn right 10 degrees"),$a=n(),$t=i("li"),Ca=p("Move forward"),Ml=n(),Pe=i("p"),ga=p("Reward function:"),Ol=n(),R=i("ul"),Ct=i("li"),Na=p("A sparse reward of +1 when an object is collected"),Ta=n(),gt=i("li"),Ra=p("A timeout penaly of -1 if the agent does not reach the object in 500 time-steps"),kl=n(),Ie=i("p"),Ma=p("Parallel: 4 independent instances of the same environment configuration."),zl=n(),j=i("h2"),M=i("a"),Nt=i("span"),Qe(Z.$$.fragment),Oa=n(),Tt=i("span"),ka=p("Navigating in procedurally generated mazes"),Sl=n(),ee=i("img"),Ul=n(),te=i("p"),za=p("Example: "),le=i("a"),Rt=i("code"),Sa=p("sb3_procgen.py"),ql=n(),je=i("p"),Ua=p("Objective: Navigate to an object in a 3D maze, when the object is collected the environment resets."),Bl=n(),xe=i("p"),qa=p("Actors: An EgoCentric Camera Actor (LINK) equipped with a monocular camera"),Gl=n(),$e=i("p"),Ba=p("Observation space:"),Hl=n(),Ce=i("ul"),Mt=i("li"),Ga=p("An RGB camera of shape (3, 40, 40)  (C, H, W) in uint8 format."),Kl=n(),ge=i("p"),Ha=p("Action space:"),Wl=n(),b=i("ul"),Ot=i("li"),Ka=p("A discrete action space with 3 possible actions"),Wa=n(),kt=i("li"),Da=p("Turn left 10 degrees"),Ja=n(),zt=i("li"),Fa=p("Turn right 10 degrees"),Qa=n(),St=i("li"),Va=p("Move forward"),Dl=n(),Ne=i("p"),Xa=p("Reward function:"),Jl=n(),O=i("ul"),Ut=i("li"),Ya=p("A sparse reward of +1 when the object is reached"),Za=n(),qt=i("li"),eo=p("A timeout penaly of -1 if the agent does not reach the object in 500 time-steps"),Fl=n(),Te=i("p"),to=p("Parallel: 4 independent instances of randomly generated environment configurations."),Ql=n(),x=i("h2"),k=i("a"),Bt=i("span"),Qe(ie.$$.fragment),lo=n(),Gt=i("span"),io=p("Physical interaction with movable objects"),Vl=n(),ae=i("img"),Xl=n(),oe=i("p"),ao=p("Example: "),re=i("a"),Ht=i("code"),oo=p("sb3_move_boxes.py"),Yl=n(),Re=i("p"),ro=p("Objective: Push boxes in a room near to each other."),Zl=n(),Me=i("p"),so=p("Actors: An EgoCentric Camera Actor (LINK) equipped with a monocular camera"),ei=n(),Oe=i("p"),no=p("Observation space:"),ti=n(),ke=i("ul"),Kt=i("li"),fo=p("An RGB camera of shape (3, 40, 40)  (C, H, W) in uint8 format."),li=n(),ze=i("p"),po=p("Action space:"),ii=n(),_=i("ul"),Wt=i("li"),co=p("A discrete action space with 3 possible actions"),uo=n(),Dt=i("li"),vo=p("Turn left 10 degrees"),mo=n(),Jt=i("li"),ho=p("Turn right 10 degrees"),bo=n(),Ft=i("li"),_o=p("Move forward"),ai=n(),Se=i("p"),wo=p("Reward function:"),oi=n(),d=i("ul"),Qt=i("li"),Eo=p("A reward for moving the red and yellow boxes close to eachother"),Ao=n(),Vt=i("li"),Lo=p("A reward for moving the green and white boxes close to eachother"),yo=n(),Xt=i("li"),Po=p("A timeout penaly of -1 if the agent does not reach the object in 100 time-steps"),ri=n(),Ue=i("p"),Io=p("Parallel: 16 independent instances of the same environment configuration."),si=n(),$=i("h2"),z=i("a"),Yt=i("span"),Qe(se.$$.fragment),jo=n(),Zt=i("span"),xo=p("Reward functions based on line of sight observation of objects."),ni=n(),ne=i("img"),fi=n(),fe=i("p"),$o=p("Example: "),pe=i("a"),el=i("code"),Co=p("sb3_visual_reward.py"),pi=n(),qe=i("p"),go=p("Objective: Move the agent so the box is within the agents its field of view"),ci=n(),Be=i("p"),No=p("Actors: An EgoCentric Camera Actor (LINK) equipped with a monocular camera"),ui=n(),Ge=i("p"),To=p("Observation space:"),vi=n(),He=i("ul"),tl=i("li"),Ro=p("An RGB camera of shape (3, 40, 40)  (C, H, W) in uint8 format."),mi=n(),Ke=i("p"),Mo=p("Action space:"),hi=n(),w=i("ul"),ll=i("li"),Oo=p("A discrete action space with 3 possible actions"),ko=n(),il=i("li"),zo=p("Turn left 10 degrees"),So=n(),al=i("li"),Uo=p("Turn right 10 degrees"),qo=n(),ol=i("li"),Bo=p("Move forward"),bi=n(),We=i("p"),Go=p("Reward function:"),_i=n(),S=i("ul"),rl=i("li"),Ho=p("A sparse reward for moving the box within a 60 degree fov cone in front of the agent."),Ko=n(),sl=i("li"),Wo=p("A timeout penaly of -1 if the agent does not reach the object in 100 time-steps"),wi=n(),De=i("p"),Do=p("Parallel: 4 independent instances of the same environment configuration."),this.h()},l(e){const o=sn('[data-svelte="svelte-1phssyn"]',document.head);L=a(o,"META",{name:!0,content:!0}),o.forEach(t),fl=f(e),y=a(e,"H1",{class:!0});var di=r(y);C=a(di,"A",{id:!0,class:!0,href:!0});var ar=r(C);lt=a(ar,"SPAN",{});var or=r(lt);Ve(K.$$.fragment,or),or.forEach(t),ar.forEach(t),gi=f(di),it=a(di,"SPAN",{});var rr=r(it);Ni=c(rr,"Using \u{1F917} Simulate to learn Agent behaviors with Stable-Baselines3"),rr.forEach(t),di.forEach(t),pl=f(e),g=a(e,"P",{});var Ai=r(g);Ti=c(Ai,"We provide several example RL integrations with the Stable-Baselines3 (LINK) library. To install this dependancy use "),at=a(Ai,"CODE",{});var sr=r(at);Ri=c(sr,"pip install simulate[sb3]"),sr.forEach(t),Mi=c(Ai,"."),Ai.forEach(t),cl=f(e),ce=a(e,"P",{});var nr=r(ce);Oi=c(nr,"Including:"),nr.forEach(t),ul=f(e),v=a(e,"UL",{});var A=r(v);ot=a(A,"LI",{});var fr=r(ot);ki=c(fr,"Learning to navigate in a simple T-Maze"),fr.forEach(t),zi=f(A),rt=a(A,"LI",{});var pr=r(rt);Si=c(pr,"Collecting objects"),pr.forEach(t),Ui=f(A),st=a(A,"LI",{});var cr=r(st);qi=c(cr,"Navigating in procedurally generated mazes"),cr.forEach(t),Bi=f(A),nt=a(A,"LI",{});var ur=r(nt);Gi=c(ur,"Physical interaction with movable objects"),ur.forEach(t),Hi=f(A),ft=a(A,"LI",{});var vr=r(ft);Ki=c(vr,"Reward functions based on line of sight observation of objects."),vr.forEach(t),A.forEach(t),vl=f(e),P=a(e,"H2",{class:!0});var Li=r(P);N=a(Li,"A",{id:!0,class:!0,href:!0});var mr=r(N);pt=a(mr,"SPAN",{});var hr=r(pt);Ve(W.$$.fragment,hr),hr.forEach(t),mr.forEach(t),Wi=f(Li),ct=a(Li,"SPAN",{});var br=r(ct);Di=c(br,"Learning to navigate in a simple T-Maze"),br.forEach(t),Li.forEach(t),ml=f(e),D=a(e,"IMG",{class:!0,src:!0}),hl=f(e),J=a(e,"P",{});var Jo=r(J);Ji=c(Jo,"Example: "),F=a(Jo,"A",{href:!0,rel:!0});var _r=r(F);ut=a(_r,"CODE",{});var wr=r(ut);Fi=c(wr,"sb3_basic_maze.py"),wr.forEach(t),_r.forEach(t),Jo.forEach(t),bl=f(e),ue=a(e,"P",{});var Er=r(ue);Qi=c(Er,"Objective: Navigate to a spherical object in a simple T-Maze. Upon object collection, the environment resets."),Er.forEach(t),_l=f(e),ve=a(e,"P",{});var dr=r(ve);Vi=c(dr,"Actors: An EgoCentric Camera Actor (LINK) equipped with a monocular camera."),dr.forEach(t),wl=f(e),me=a(e,"P",{});var Ar=r(me);Xi=c(Ar,"Observation space:"),Ar.forEach(t),El=f(e),he=a(e,"UL",{});var Lr=r(he);vt=a(Lr,"LI",{});var yr=r(vt);Yi=c(yr,"An RGB camera of shape (3, 40, 40)  (C, H, W) in uint8 format."),yr.forEach(t),Lr.forEach(t),dl=f(e),be=a(e,"P",{});var Pr=r(be);Zi=c(Pr,"Action space:"),Pr.forEach(t),Al=f(e),m=a(e,"UL",{});var U=r(m);mt=a(U,"LI",{});var Ir=r(mt);ea=c(Ir,"A discrete action space with 3 possible actions"),Ir.forEach(t),ta=f(U),ht=a(U,"LI",{});var jr=r(ht);la=c(jr,"Turn left 10 degrees"),jr.forEach(t),ia=f(U),bt=a(U,"LI",{});var xr=r(bt);aa=c(xr,"Turn right 10 degrees"),xr.forEach(t),oa=f(U),_t=a(U,"LI",{});var $r=r(_t);ra=c($r,"Move forward"),$r.forEach(t),U.forEach(t),Ll=f(e),_e=a(e,"P",{});var Cr=r(_e);sa=c(Cr,"Reward function:"),Cr.forEach(t),yl=f(e),E=a(e,"UL",{});var Je=r(E);wt=a(Je,"LI",{});var gr=r(wt);na=c(gr,"A dense reward based on improvement in best euclidean distance to the object"),gr.forEach(t),fa=f(Je),Et=a(Je,"LI",{});var Nr=r(Et);pa=c(Nr,"A sparse reward of +1 when the object is collected"),Nr.forEach(t),ca=f(Je),dt=a(Je,"LI",{});var Tr=r(dt);ua=c(Tr,"A timeout penaly of -1 if the agent does not reach the object in 200 time-steps"),Tr.forEach(t),Je.forEach(t),Pl=f(e),we=a(e,"P",{});var Rr=r(we);va=c(Rr,"Parallel: 4 independent instances of the same environment configuration."),Rr.forEach(t),Il=f(e),I=a(e,"H2",{class:!0});var yi=r(I);T=a(yi,"A",{id:!0,class:!0,href:!0});var Mr=r(T);At=a(Mr,"SPAN",{});var Or=r(At);Ve(Q.$$.fragment,Or),Or.forEach(t),Mr.forEach(t),ma=f(yi),Lt=a(yi,"SPAN",{});var kr=r(Lt);ha=c(kr,"Collecting objects"),kr.forEach(t),yi.forEach(t),jl=f(e),V=a(e,"IMG",{class:!0,src:!0}),xl=f(e),X=a(e,"P",{});var Fo=r(X);ba=c(Fo,"Example: "),Y=a(Fo,"A",{href:!0,rel:!0});var zr=r(Y);yt=a(zr,"CODE",{});var Sr=r(yt);_a=c(Sr,"sb3_collectables.py"),Sr.forEach(t),zr.forEach(t),Fo.forEach(t),$l=f(e),Ee=a(e,"P",{});var Ur=r(Ee);wa=c(Ur,"Objective: Collect all 20 objects in a large square room."),Ur.forEach(t),Cl=f(e),de=a(e,"P",{});var qr=r(de);Ea=c(qr,"Actors: An EgoCentric Camera Actor (LINK) equipped with a monocular camera."),qr.forEach(t),gl=f(e),Ae=a(e,"P",{});var Br=r(Ae);da=c(Br,"Observation space:"),Br.forEach(t),Nl=f(e),Le=a(e,"UL",{});var Gr=r(Le);Pt=a(Gr,"LI",{});var Hr=r(Pt);Aa=c(Hr,"An RGB camera of shape (3, 40, 40)  (C, H, W) in uint8 format."),Hr.forEach(t),Gr.forEach(t),Tl=f(e),ye=a(e,"P",{});var Kr=r(ye);La=c(Kr,"Action space:"),Kr.forEach(t),Rl=f(e),h=a(e,"UL",{});var q=r(h);It=a(q,"LI",{});var Wr=r(It);ya=c(Wr,"A discrete action space with 3 possible actions"),Wr.forEach(t),Pa=f(q),jt=a(q,"LI",{});var Dr=r(jt);Ia=c(Dr,"Turn left 10 degrees"),Dr.forEach(t),ja=f(q),xt=a(q,"LI",{});var Jr=r(xt);xa=c(Jr,"Turn right 10 degrees"),Jr.forEach(t),$a=f(q),$t=a(q,"LI",{});var Fr=r($t);Ca=c(Fr,"Move forward"),Fr.forEach(t),q.forEach(t),Ml=f(e),Pe=a(e,"P",{});var Qr=r(Pe);ga=c(Qr,"Reward function:"),Qr.forEach(t),Ol=f(e),R=a(e,"UL",{});var Pi=r(R);Ct=a(Pi,"LI",{});var Vr=r(Ct);Na=c(Vr,"A sparse reward of +1 when an object is collected"),Vr.forEach(t),Ta=f(Pi),gt=a(Pi,"LI",{});var Xr=r(gt);Ra=c(Xr,"A timeout penaly of -1 if the agent does not reach the object in 500 time-steps"),Xr.forEach(t),Pi.forEach(t),kl=f(e),Ie=a(e,"P",{});var Yr=r(Ie);Ma=c(Yr,"Parallel: 4 independent instances of the same environment configuration."),Yr.forEach(t),zl=f(e),j=a(e,"H2",{class:!0});var Ii=r(j);M=a(Ii,"A",{id:!0,class:!0,href:!0});var Zr=r(M);Nt=a(Zr,"SPAN",{});var es=r(Nt);Ve(Z.$$.fragment,es),es.forEach(t),Zr.forEach(t),Oa=f(Ii),Tt=a(Ii,"SPAN",{});var ts=r(Tt);ka=c(ts,"Navigating in procedurally generated mazes"),ts.forEach(t),Ii.forEach(t),Sl=f(e),ee=a(e,"IMG",{class:!0,src:!0}),Ul=f(e),te=a(e,"P",{});var Qo=r(te);za=c(Qo,"Example: "),le=a(Qo,"A",{href:!0,rel:!0});var ls=r(le);Rt=a(ls,"CODE",{});var is=r(Rt);Sa=c(is,"sb3_procgen.py"),is.forEach(t),ls.forEach(t),Qo.forEach(t),ql=f(e),je=a(e,"P",{});var as=r(je);Ua=c(as,"Objective: Navigate to an object in a 3D maze, when the object is collected the environment resets."),as.forEach(t),Bl=f(e),xe=a(e,"P",{});var os=r(xe);qa=c(os,"Actors: An EgoCentric Camera Actor (LINK) equipped with a monocular camera"),os.forEach(t),Gl=f(e),$e=a(e,"P",{});var rs=r($e);Ba=c(rs,"Observation space:"),rs.forEach(t),Hl=f(e),Ce=a(e,"UL",{});var ss=r(Ce);Mt=a(ss,"LI",{});var ns=r(Mt);Ga=c(ns,"An RGB camera of shape (3, 40, 40)  (C, H, W) in uint8 format."),ns.forEach(t),ss.forEach(t),Kl=f(e),ge=a(e,"P",{});var fs=r(ge);Ha=c(fs,"Action space:"),fs.forEach(t),Wl=f(e),b=a(e,"UL",{});var B=r(b);Ot=a(B,"LI",{});var ps=r(Ot);Ka=c(ps,"A discrete action space with 3 possible actions"),ps.forEach(t),Wa=f(B),kt=a(B,"LI",{});var cs=r(kt);Da=c(cs,"Turn left 10 degrees"),cs.forEach(t),Ja=f(B),zt=a(B,"LI",{});var us=r(zt);Fa=c(us,"Turn right 10 degrees"),us.forEach(t),Qa=f(B),St=a(B,"LI",{});var vs=r(St);Va=c(vs,"Move forward"),vs.forEach(t),B.forEach(t),Dl=f(e),Ne=a(e,"P",{});var ms=r(Ne);Xa=c(ms,"Reward function:"),ms.forEach(t),Jl=f(e),O=a(e,"UL",{});var ji=r(O);Ut=a(ji,"LI",{});var hs=r(Ut);Ya=c(hs,"A sparse reward of +1 when the object is reached"),hs.forEach(t),Za=f(ji),qt=a(ji,"LI",{});var bs=r(qt);eo=c(bs,"A timeout penaly of -1 if the agent does not reach the object in 500 time-steps"),bs.forEach(t),ji.forEach(t),Fl=f(e),Te=a(e,"P",{});var _s=r(Te);to=c(_s,"Parallel: 4 independent instances of randomly generated environment configurations."),_s.forEach(t),Ql=f(e),x=a(e,"H2",{class:!0});var xi=r(x);k=a(xi,"A",{id:!0,class:!0,href:!0});var ws=r(k);Bt=a(ws,"SPAN",{});var Es=r(Bt);Ve(ie.$$.fragment,Es),Es.forEach(t),ws.forEach(t),lo=f(xi),Gt=a(xi,"SPAN",{});var ds=r(Gt);io=c(ds,"Physical interaction with movable objects"),ds.forEach(t),xi.forEach(t),Vl=f(e),ae=a(e,"IMG",{class:!0,src:!0}),Xl=f(e),oe=a(e,"P",{});var Vo=r(oe);ao=c(Vo,"Example: "),re=a(Vo,"A",{href:!0,rel:!0});var As=r(re);Ht=a(As,"CODE",{});var Ls=r(Ht);oo=c(Ls,"sb3_move_boxes.py"),Ls.forEach(t),As.forEach(t),Vo.forEach(t),Yl=f(e),Re=a(e,"P",{});var ys=r(Re);ro=c(ys,"Objective: Push boxes in a room near to each other."),ys.forEach(t),Zl=f(e),Me=a(e,"P",{});var Ps=r(Me);so=c(Ps,"Actors: An EgoCentric Camera Actor (LINK) equipped with a monocular camera"),Ps.forEach(t),ei=f(e),Oe=a(e,"P",{});var Is=r(Oe);no=c(Is,"Observation space:"),Is.forEach(t),ti=f(e),ke=a(e,"UL",{});var js=r(ke);Kt=a(js,"LI",{});var xs=r(Kt);fo=c(xs,"An RGB camera of shape (3, 40, 40)  (C, H, W) in uint8 format."),xs.forEach(t),js.forEach(t),li=f(e),ze=a(e,"P",{});var $s=r(ze);po=c($s,"Action space:"),$s.forEach(t),ii=f(e),_=a(e,"UL",{});var G=r(_);Wt=a(G,"LI",{});var Cs=r(Wt);co=c(Cs,"A discrete action space with 3 possible actions"),Cs.forEach(t),uo=f(G),Dt=a(G,"LI",{});var gs=r(Dt);vo=c(gs,"Turn left 10 degrees"),gs.forEach(t),mo=f(G),Jt=a(G,"LI",{});var Ns=r(Jt);ho=c(Ns,"Turn right 10 degrees"),Ns.forEach(t),bo=f(G),Ft=a(G,"LI",{});var Ts=r(Ft);_o=c(Ts,"Move forward"),Ts.forEach(t),G.forEach(t),ai=f(e),Se=a(e,"P",{});var Rs=r(Se);wo=c(Rs,"Reward function:"),Rs.forEach(t),oi=f(e),d=a(e,"UL",{});var Fe=r(d);Qt=a(Fe,"LI",{});var Ms=r(Qt);Eo=c(Ms,"A reward for moving the red and yellow boxes close to eachother"),Ms.forEach(t),Ao=f(Fe),Vt=a(Fe,"LI",{});var Os=r(Vt);Lo=c(Os,"A reward for moving the green and white boxes close to eachother"),Os.forEach(t),yo=f(Fe),Xt=a(Fe,"LI",{});var ks=r(Xt);Po=c(ks,"A timeout penaly of -1 if the agent does not reach the object in 100 time-steps"),ks.forEach(t),Fe.forEach(t),ri=f(e),Ue=a(e,"P",{});var zs=r(Ue);Io=c(zs,"Parallel: 16 independent instances of the same environment configuration."),zs.forEach(t),si=f(e),$=a(e,"H2",{class:!0});var $i=r($);z=a($i,"A",{id:!0,class:!0,href:!0});var Ss=r(z);Yt=a(Ss,"SPAN",{});var Us=r(Yt);Ve(se.$$.fragment,Us),Us.forEach(t),Ss.forEach(t),jo=f($i),Zt=a($i,"SPAN",{});var qs=r(Zt);xo=c(qs,"Reward functions based on line of sight observation of objects."),qs.forEach(t),$i.forEach(t),ni=f(e),ne=a(e,"IMG",{class:!0,src:!0}),fi=f(e),fe=a(e,"P",{});var Xo=r(fe);$o=c(Xo,"Example: "),pe=a(Xo,"A",{href:!0,rel:!0});var Bs=r(pe);el=a(Bs,"CODE",{});var Gs=r(el);Co=c(Gs,"sb3_visual_reward.py"),Gs.forEach(t),Bs.forEach(t),Xo.forEach(t),pi=f(e),qe=a(e,"P",{});var Hs=r(qe);go=c(Hs,"Objective: Move the agent so the box is within the agents its field of view"),Hs.forEach(t),ci=f(e),Be=a(e,"P",{});var Ks=r(Be);No=c(Ks,"Actors: An EgoCentric Camera Actor (LINK) equipped with a monocular camera"),Ks.forEach(t),ui=f(e),Ge=a(e,"P",{});var Ws=r(Ge);To=c(Ws,"Observation space:"),Ws.forEach(t),vi=f(e),He=a(e,"UL",{});var Ds=r(He);tl=a(Ds,"LI",{});var Js=r(tl);Ro=c(Js,"An RGB camera of shape (3, 40, 40)  (C, H, W) in uint8 format."),Js.forEach(t),Ds.forEach(t),mi=f(e),Ke=a(e,"P",{});var Fs=r(Ke);Mo=c(Fs,"Action space:"),Fs.forEach(t),hi=f(e),w=a(e,"UL",{});var H=r(w);ll=a(H,"LI",{});var Qs=r(ll);Oo=c(Qs,"A discrete action space with 3 possible actions"),Qs.forEach(t),ko=f(H),il=a(H,"LI",{});var Vs=r(il);zo=c(Vs,"Turn left 10 degrees"),Vs.forEach(t),So=f(H),al=a(H,"LI",{});var Xs=r(al);Uo=c(Xs,"Turn right 10 degrees"),Xs.forEach(t),qo=f(H),ol=a(H,"LI",{});var Ys=r(ol);Bo=c(Ys,"Move forward"),Ys.forEach(t),H.forEach(t),bi=f(e),We=a(e,"P",{});var Zs=r(We);Go=c(Zs,"Reward function:"),Zs.forEach(t),_i=f(e),S=a(e,"UL",{});var Ci=r(S);rl=a(Ci,"LI",{});var en=r(rl);Ho=c(en,"A sparse reward for moving the box within a 60 degree fov cone in front of the agent."),en.forEach(t),Ko=f(Ci),sl=a(Ci,"LI",{});var tn=r(sl);Wo=c(tn,"A timeout penaly of -1 if the agent does not reach the object in 100 time-steps"),tn.forEach(t),Ci.forEach(t),wi=f(e),De=a(e,"P",{});var ln=r(De);Do=c(ln,"Parallel: 4 independent instances of the same environment configuration."),ln.forEach(t),this.h()},h(){u(L,"name","hf:doc:metadata"),u(L,"content",JSON.stringify(cn)),u(C,"id","using-simulate-to-learn-agent-behaviors-with-stablebaselines3"),u(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(C,"href","#using-simulate-to-learn-agent-behaviors-with-stablebaselines3"),u(y,"class","relative group"),u(N,"id","learning-to-navigate-in-a-simple-tmaze"),u(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(N,"href","#learning-to-navigate-in-a-simple-tmaze"),u(P,"class","relative group"),u(D,"class","!m-0 !border-0 !dark:border-0 !shadow-none !max-w-lg w-[600px]"),nl(D.src,Zo="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/simulate/simulate_sb3_basic_maze.png")||u(D,"src",Zo),u(F,"href","https://github.com/huggingface/simulate/examples/rl/sb3_basic_maze.py"),u(F,"rel","nofollow"),u(T,"id","collecting-objects"),u(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(T,"href","#collecting-objects"),u(I,"class","relative group"),u(V,"class","!m-0 !border-0 !dark:border-0 !shadow-none !max-w-lg w-[600px]"),nl(V.src,er="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/simulate/simulate_sb3_collectables.png")||u(V,"src",er),u(Y,"href","https://github.com/huggingface/simulate/examples/rl/sb3_collectables.py"),u(Y,"rel","nofollow"),u(M,"id","navigating-in-procedurally-generated-mazes"),u(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(M,"href","#navigating-in-procedurally-generated-mazes"),u(j,"class","relative group"),u(ee,"class","!m-0 !border-0 !dark:border-0 !shadow-none !max-w-lg w-[600px]"),nl(ee.src,tr="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/simulate/simulate_sb3_procgen.png")||u(ee,"src",tr),u(le,"href","https://github.com/huggingface/simulate/examples/rl/sb3_procgen.py"),u(le,"rel","nofollow"),u(k,"id","physical-interaction-with-movable-objects"),u(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(k,"href","#physical-interaction-with-movable-objects"),u(x,"class","relative group"),u(ae,"class","!m-0 !border-0 !dark:border-0 !shadow-none !max-w-lg w-[600px]"),nl(ae.src,lr="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/simulate/simulate_sb3_move_boxes.png")||u(ae,"src",lr),u(re,"href","https://github.com/huggingface/simulate/examples/rl/sb3_move_boxes.py"),u(re,"rel","nofollow"),u(z,"id","reward-functions-based-on-line-of-sight-observation-of-objects"),u(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(z,"href","#reward-functions-based-on-line-of-sight-observation-of-objects"),u($,"class","relative group"),u(ne,"class","!m-0 !border-0 !dark:border-0 !shadow-none !max-w-lg w-[600px]"),nl(ne.src,ir="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/simulate/simulate_sb3_see_reward.png")||u(ne,"src",ir),u(pe,"href","https://github.com/huggingface/simulate/examples/rl/sb3_visual_reward.py"),u(pe,"rel","nofollow")},m(e,o){l(document.head,L),s(e,fl,o),s(e,y,o),l(y,C),l(C,lt),Xe(K,lt,null),l(y,gi),l(y,it),l(it,Ni),s(e,pl,o),s(e,g,o),l(g,Ti),l(g,at),l(at,Ri),l(g,Mi),s(e,cl,o),s(e,ce,o),l(ce,Oi),s(e,ul,o),s(e,v,o),l(v,ot),l(ot,ki),l(v,zi),l(v,rt),l(rt,Si),l(v,Ui),l(v,st),l(st,qi),l(v,Bi),l(v,nt),l(nt,Gi),l(v,Hi),l(v,ft),l(ft,Ki),s(e,vl,o),s(e,P,o),l(P,N),l(N,pt),Xe(W,pt,null),l(P,Wi),l(P,ct),l(ct,Di),s(e,ml,o),s(e,D,o),s(e,hl,o),s(e,J,o),l(J,Ji),l(J,F),l(F,ut),l(ut,Fi),s(e,bl,o),s(e,ue,o),l(ue,Qi),s(e,_l,o),s(e,ve,o),l(ve,Vi),s(e,wl,o),s(e,me,o),l(me,Xi),s(e,El,o),s(e,he,o),l(he,vt),l(vt,Yi),s(e,dl,o),s(e,be,o),l(be,Zi),s(e,Al,o),s(e,m,o),l(m,mt),l(mt,ea),l(m,ta),l(m,ht),l(ht,la),l(m,ia),l(m,bt),l(bt,aa),l(m,oa),l(m,_t),l(_t,ra),s(e,Ll,o),s(e,_e,o),l(_e,sa),s(e,yl,o),s(e,E,o),l(E,wt),l(wt,na),l(E,fa),l(E,Et),l(Et,pa),l(E,ca),l(E,dt),l(dt,ua),s(e,Pl,o),s(e,we,o),l(we,va),s(e,Il,o),s(e,I,o),l(I,T),l(T,At),Xe(Q,At,null),l(I,ma),l(I,Lt),l(Lt,ha),s(e,jl,o),s(e,V,o),s(e,xl,o),s(e,X,o),l(X,ba),l(X,Y),l(Y,yt),l(yt,_a),s(e,$l,o),s(e,Ee,o),l(Ee,wa),s(e,Cl,o),s(e,de,o),l(de,Ea),s(e,gl,o),s(e,Ae,o),l(Ae,da),s(e,Nl,o),s(e,Le,o),l(Le,Pt),l(Pt,Aa),s(e,Tl,o),s(e,ye,o),l(ye,La),s(e,Rl,o),s(e,h,o),l(h,It),l(It,ya),l(h,Pa),l(h,jt),l(jt,Ia),l(h,ja),l(h,xt),l(xt,xa),l(h,$a),l(h,$t),l($t,Ca),s(e,Ml,o),s(e,Pe,o),l(Pe,ga),s(e,Ol,o),s(e,R,o),l(R,Ct),l(Ct,Na),l(R,Ta),l(R,gt),l(gt,Ra),s(e,kl,o),s(e,Ie,o),l(Ie,Ma),s(e,zl,o),s(e,j,o),l(j,M),l(M,Nt),Xe(Z,Nt,null),l(j,Oa),l(j,Tt),l(Tt,ka),s(e,Sl,o),s(e,ee,o),s(e,Ul,o),s(e,te,o),l(te,za),l(te,le),l(le,Rt),l(Rt,Sa),s(e,ql,o),s(e,je,o),l(je,Ua),s(e,Bl,o),s(e,xe,o),l(xe,qa),s(e,Gl,o),s(e,$e,o),l($e,Ba),s(e,Hl,o),s(e,Ce,o),l(Ce,Mt),l(Mt,Ga),s(e,Kl,o),s(e,ge,o),l(ge,Ha),s(e,Wl,o),s(e,b,o),l(b,Ot),l(Ot,Ka),l(b,Wa),l(b,kt),l(kt,Da),l(b,Ja),l(b,zt),l(zt,Fa),l(b,Qa),l(b,St),l(St,Va),s(e,Dl,o),s(e,Ne,o),l(Ne,Xa),s(e,Jl,o),s(e,O,o),l(O,Ut),l(Ut,Ya),l(O,Za),l(O,qt),l(qt,eo),s(e,Fl,o),s(e,Te,o),l(Te,to),s(e,Ql,o),s(e,x,o),l(x,k),l(k,Bt),Xe(ie,Bt,null),l(x,lo),l(x,Gt),l(Gt,io),s(e,Vl,o),s(e,ae,o),s(e,Xl,o),s(e,oe,o),l(oe,ao),l(oe,re),l(re,Ht),l(Ht,oo),s(e,Yl,o),s(e,Re,o),l(Re,ro),s(e,Zl,o),s(e,Me,o),l(Me,so),s(e,ei,o),s(e,Oe,o),l(Oe,no),s(e,ti,o),s(e,ke,o),l(ke,Kt),l(Kt,fo),s(e,li,o),s(e,ze,o),l(ze,po),s(e,ii,o),s(e,_,o),l(_,Wt),l(Wt,co),l(_,uo),l(_,Dt),l(Dt,vo),l(_,mo),l(_,Jt),l(Jt,ho),l(_,bo),l(_,Ft),l(Ft,_o),s(e,ai,o),s(e,Se,o),l(Se,wo),s(e,oi,o),s(e,d,o),l(d,Qt),l(Qt,Eo),l(d,Ao),l(d,Vt),l(Vt,Lo),l(d,yo),l(d,Xt),l(Xt,Po),s(e,ri,o),s(e,Ue,o),l(Ue,Io),s(e,si,o),s(e,$,o),l($,z),l(z,Yt),Xe(se,Yt,null),l($,jo),l($,Zt),l(Zt,xo),s(e,ni,o),s(e,ne,o),s(e,fi,o),s(e,fe,o),l(fe,$o),l(fe,pe),l(pe,el),l(el,Co),s(e,pi,o),s(e,qe,o),l(qe,go),s(e,ci,o),s(e,Be,o),l(Be,No),s(e,ui,o),s(e,Ge,o),l(Ge,To),s(e,vi,o),s(e,He,o),l(He,tl),l(tl,Ro),s(e,mi,o),s(e,Ke,o),l(Ke,Mo),s(e,hi,o),s(e,w,o),l(w,ll),l(ll,Oo),l(w,ko),l(w,il),l(il,zo),l(w,So),l(w,al),l(al,Uo),l(w,qo),l(w,ol),l(ol,Bo),s(e,bi,o),s(e,We,o),l(We,Go),s(e,_i,o),s(e,S,o),l(S,rl),l(rl,Ho),l(S,Ko),l(S,sl),l(sl,Wo),s(e,wi,o),s(e,De,o),l(De,Do),Ei=!0},p:nn,i(e){Ei||(Ye(K.$$.fragment,e),Ye(W.$$.fragment,e),Ye(Q.$$.fragment,e),Ye(Z.$$.fragment,e),Ye(ie.$$.fragment,e),Ye(se.$$.fragment,e),Ei=!0)},o(e){Ze(K.$$.fragment,e),Ze(W.$$.fragment,e),Ze(Q.$$.fragment,e),Ze(Z.$$.fragment,e),Ze(ie.$$.fragment,e),Ze(se.$$.fragment,e),Ei=!1},d(e){t(L),e&&t(fl),e&&t(y),et(K),e&&t(pl),e&&t(g),e&&t(cl),e&&t(ce),e&&t(ul),e&&t(v),e&&t(vl),e&&t(P),et(W),e&&t(ml),e&&t(D),e&&t(hl),e&&t(J),e&&t(bl),e&&t(ue),e&&t(_l),e&&t(ve),e&&t(wl),e&&t(me),e&&t(El),e&&t(he),e&&t(dl),e&&t(be),e&&t(Al),e&&t(m),e&&t(Ll),e&&t(_e),e&&t(yl),e&&t(E),e&&t(Pl),e&&t(we),e&&t(Il),e&&t(I),et(Q),e&&t(jl),e&&t(V),e&&t(xl),e&&t(X),e&&t($l),e&&t(Ee),e&&t(Cl),e&&t(de),e&&t(gl),e&&t(Ae),e&&t(Nl),e&&t(Le),e&&t(Tl),e&&t(ye),e&&t(Rl),e&&t(h),e&&t(Ml),e&&t(Pe),e&&t(Ol),e&&t(R),e&&t(kl),e&&t(Ie),e&&t(zl),e&&t(j),et(Z),e&&t(Sl),e&&t(ee),e&&t(Ul),e&&t(te),e&&t(ql),e&&t(je),e&&t(Bl),e&&t(xe),e&&t(Gl),e&&t($e),e&&t(Hl),e&&t(Ce),e&&t(Kl),e&&t(ge),e&&t(Wl),e&&t(b),e&&t(Dl),e&&t(Ne),e&&t(Jl),e&&t(O),e&&t(Fl),e&&t(Te),e&&t(Ql),e&&t(x),et(ie),e&&t(Vl),e&&t(ae),e&&t(Xl),e&&t(oe),e&&t(Yl),e&&t(Re),e&&t(Zl),e&&t(Me),e&&t(ei),e&&t(Oe),e&&t(ti),e&&t(ke),e&&t(li),e&&t(ze),e&&t(ii),e&&t(_),e&&t(ai),e&&t(Se),e&&t(oi),e&&t(d),e&&t(ri),e&&t(Ue),e&&t(si),e&&t($),et(se),e&&t(ni),e&&t(ne),e&&t(fi),e&&t(fe),e&&t(pi),e&&t(qe),e&&t(ci),e&&t(Be),e&&t(ui),e&&t(Ge),e&&t(vi),e&&t(He),e&&t(mi),e&&t(Ke),e&&t(hi),e&&t(w),e&&t(bi),e&&t(We),e&&t(_i),e&&t(S),e&&t(wi),e&&t(De)}}}const cn={local:"using-simulate-to-learn-agent-behaviors-with-stablebaselines3",sections:[{local:"learning-to-navigate-in-a-simple-tmaze",title:"Learning to navigate in a simple T-Maze"},{local:"collecting-objects",title:"Collecting objects"},{local:"navigating-in-procedurally-generated-mazes",title:"Navigating in procedurally generated mazes"},{local:"physical-interaction-with-movable-objects",title:"Physical interaction with movable objects"},{local:"reward-functions-based-on-line-of-sight-observation-of-objects",title:"Reward functions based on line of sight observation of objects."}],title:"Using \u{1F917} Simulate to learn Agent behaviors with Stable-Baselines3"};function un(Yo){return fn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class hn extends an{constructor(L){super();on(this,L,un,pn,rn,{})}}export{hn as default,cn as metadata};
