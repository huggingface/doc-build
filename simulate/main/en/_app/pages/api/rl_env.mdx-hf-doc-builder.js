import{S as Nn,i as Wn,s as Fn,e as r,k as s,w as h,t as p,M as qn,c as a,d as n,m as i,a as o,x as f,h as m,b as l,G as e,g as q,y as g,L as Bn,q as v,o as b,B as _,v as Hn}from"../../chunks/vendor-hf-doc-builder.js";import{D as y}from"../../chunks/Docstring-hf-doc-builder.js";import{I as Jn}from"../../chunks/IconCopyLink-hf-doc-builder.js";function jn(fn){let R,Ue,P,x,ve,B,at,be,ot,Ge,c,H,st,_e,it,lt,T,J,ct,ye,pt,mt,I,j,dt,$e,ut,ht,O,z,ft,Ee,gt,vt,A,K,bt,Le,_t,yt,w,Q,$t,we,Et,Lt,De,wt,Dt,S,X,Rt,Re,Pt,xt,V,Y,Tt,Pe,It,Ot,k,Z,At,xe,St,Ne,d,ee,Vt,de,kt,te,Ct,Mt,C,ne,Ut,Te,Gt,Nt,M,re,Wt,Ie,Ft,qt,U,ae,Bt,Oe,Ht,Jt,G,oe,jt,Ae,zt,Kt,D,se,Qt,Se,Xt,Yt,Ve,Zt,en,N,ie,tn,ke,nn,rn,W,le,an,Ce,on,We,E,ce,sn,ue,ln,pe,cn,pn,F,me,mn,Me,dn,Fe;return B=new Jn({}),H=new y({props:{name:"class simulate.RLEnv",anchor:"simulate.RLEnv",parameters:[{name:"scene",val:": Scene"},{name:"time_step",val:": typing.Optional[float] = 0.03333333333333333"},{name:"frame_skip",val:": typing.Optional[int] = 4"}],parametersDescription:[{anchor:"simulate.RLEnv.scene",description:`<strong>scene</strong> (<code>Scene</code>) &#x2014;
The Simulate scene to be wrapped.`,name:"scene"},{anchor:"simulate.RLEnv.time_step",description:`<strong>time_step</strong> (<code>float</code>, <em>optional</em>, defaults to <code>1/30.0</code>) &#x2014;
The physics timestep of the environment.`,name:"time_step"},{anchor:"simulate.RLEnv.frame_skip",description:`<strong>frame_skip</strong> (<code>int</code>, <em>optional</em>, defaults to <code>4</code>) &#x2014;
The number of times an action is repeated in the backend simulation before the next observation is returned.`,name:"frame_skip"}],source:"https://github.com/huggingface/simulate/blob/main/src/simulate/rl/rl_env.py#L23"}}),J=new y({props:{name:"close",anchor:"simulate.RLEnv.close",parameters:[],source:"https://github.com/huggingface/simulate/blob/main/src/simulate/rl/rl_env.py#L255"}}),j=new y({props:{name:"get_attr",anchor:"simulate.RLEnv.get_attr",parameters:[{name:"attr_name",val:": str"},{name:"indices",val:": typing.Any = None"}],source:"https://github.com/huggingface/simulate/blob/main/src/simulate/rl/rl_env.py#L284"}}),z=new y({props:{name:"reset",anchor:"simulate.RLEnv.reset",parameters:[],source:"https://github.com/huggingface/simulate/blob/main/src/simulate/rl/rl_env.py#L205",returnDescription:`
<p>the observation of the environment after reset.</p>
`,returnType:`
<p>obs (<code>Dict</code>)</p>
`}}),K=new y({props:{name:"sample_action",anchor:"simulate.RLEnv.sample_action",parameters:[],source:"https://github.com/huggingface/simulate/blob/main/src/simulate/rl/rl_env.py#L259",returnDescription:`
<p>Lists of the actions, dimensions are n-maps, n-actors, action-dim.</p>
`,returnType:`
<p>action (<code>list[list[list[float]]]</code>)</p>
`}}),Q=new y({props:{name:"step",anchor:"simulate.RLEnv.step",parameters:[{name:"action",val:": typing.Union[typing.Dict, typing.List, numpy.ndarray]"}],parametersDescription:[{anchor:"simulate.RLEnv.step.action",description:`<strong>action</strong> (<code>Dict</code> or <code>List</code>) &#x2014;
The action to be taken by the environment.`,name:"action"}],source:"https://github.com/huggingface/simulate/blob/main/src/simulate/rl/rl_env.py#L82",returnDescription:`
<p>A dictionary of observations from the environment.
reward (<code>float</code>):
The reward for the action.
done (<code>bool</code>):
Whether the episode has ended.
info (<code>Dict</code>):
A dictionary of additional information.</p>
`,returnType:`
<p>observation (<code>Dict</code>)</p>
`}}),X=new y({props:{name:"step_async",anchor:"simulate.RLEnv.step_async",parameters:[{name:"actions",val:": ndarray"}],source:"https://github.com/huggingface/simulate/blob/main/src/simulate/rl/rl_env.py#L280"}}),Y=new y({props:{name:"step_recv_async",anchor:"simulate.RLEnv.step_recv_async",parameters:[],source:"https://github.com/huggingface/simulate/blob/main/src/simulate/rl/rl_env.py#L160",returnDescription:`
<p>A dictionary containing the observation from the environment.
reward (<code>float</code>):
The reward for the action.
done (<code>bool</code>):
Whether the episode has ended.
info (<code>Dict</code>):
A dictionary of additional information.</p>
`,returnType:`
<p>observation (<code>Dict</code>)</p>
`}}),Z=new y({props:{name:"step_send_async",anchor:"simulate.RLEnv.step_send_async",parameters:[{name:"action",val:": typing.Union[typing.Dict, typing.List, numpy.ndarray]"}],parametersDescription:[{anchor:"simulate.RLEnv.step_send_async.action",description:"<strong>action</strong> (<code>Dict</code> or <code>List</code> or <code>ndarray</code>) &#x2014; The action to be executed in the environment.",name:"action"}],source:"https://github.com/huggingface/simulate/blob/main/src/simulate/rl/rl_env.py#L106"}}),ee=new y({props:{name:"class simulate.ParallelRLEnv",anchor:"simulate.ParallelRLEnv",parameters:[{name:"map_fn",val:": typing.Union[typing.Callable, simulate.scene.Scene]"},{name:"n_maps",val:": typing.Optional[int] = 1"},{name:"n_show",val:": typing.Optional[int] = 1"},{name:"time_step",val:": typing.Optional[float] = 0.03333333333333333"},{name:"frame_skip",val:": typing.Optional[int] = 4"},{name:"**engine_kwargs",val:""}],parametersDescription:[{anchor:"simulate.ParallelRLEnv.map_fn",description:`<strong>map_fn</strong> (<code>Callable</code>) &#x2014;
a generator function that returns a RLEnv for generating instances of the desired environment.`,name:"map_fn"},{anchor:"simulate.ParallelRLEnv.n_parallel",description:`<strong>n_parallel</strong> (<code>int</code>) &#x2014;
the number of executable instances to create.`,name:"n_parallel"},{anchor:"simulate.ParallelRLEnv.starting_port",description:`<strong>starting_port</strong> (<code>int</code>, <em>optional</em>, defaults to <code>55001</code>) &#x2014;
initial communication port for spawned executables.`,name:"starting_port"}],source:"https://github.com/huggingface/simulate/blob/main/src/simulate/rl/parallel_rl_env.py#L52"}}),ne=new y({props:{name:"close",anchor:"simulate.ParallelRLEnv.close",parameters:[],source:"https://github.com/huggingface/simulate/blob/main/src/simulate/rl/parallel_rl_env.py#L299"}}),re=new y({props:{name:"env_is_wrapped",anchor:"simulate.ParallelRLEnv.env_is_wrapped",parameters:[{name:"wrapper_class",val:": typing.Type[gym.core.Wrapper]"},{name:"indices",val:": typing.Union[NoneType, int, typing.Iterable[int]] = None"}],source:"https://github.com/huggingface/simulate/blob/main/src/simulate/rl/parallel_rl_env.py#L322"}}),ae=new y({props:{name:"reset",anchor:"simulate.ParallelRLEnv.reset",parameters:[],source:"https://github.com/huggingface/simulate/blob/main/src/simulate/rl/parallel_rl_env.py#L226",returnDescription:`
<p>the observation of the environment after reset.</p>
`,returnType:`
<p>obs (<code>Dict</code>)</p>
`}}),oe=new y({props:{name:"sample_action",anchor:"simulate.ParallelRLEnv.sample_action",parameters:[],source:"https://github.com/huggingface/simulate/blob/main/src/simulate/rl/parallel_rl_env.py#L303",returnDescription:`
<p>Lists of the actions, dimensions are n-maps, n-actors, action-dim.</p>
`,returnType:`
<p>action (<code>list[list[list[float]]]</code>)</p>
`}}),se=new y({props:{name:"step",anchor:"simulate.ParallelRLEnv.step",parameters:[{name:"action",val:": typing.Union[typing.Dict, typing.List, numpy.ndarray]"}],parametersDescription:[{anchor:"simulate.ParallelRLEnv.step.action",description:`<strong>action</strong> (<code>Dict</code> or <code>List</code>) &#x2014;
a dict or list of actions for each actuator.`,name:"action"}],source:"https://github.com/huggingface/simulate/blob/main/src/simulate/rl/parallel_rl_env.py#L125",returnDescription:`
<p>a list of dict of observations for each sensor.
all_reward (<code>List[float]</code>):
all the rewards for the current step.
all_done (<code>List[bool]</code>):
whether each episode is done.
all_info (<code>List[Dict]</code>):
a list of dict of additional information.</p>
`,returnType:`
<p>all_observation (<code>List[Dict]</code>)</p>
`}}),ie=new y({props:{name:"step_recv_async",anchor:"simulate.ParallelRLEnv.step_recv_async",parameters:[],source:"https://github.com/huggingface/simulate/blob/main/src/simulate/rl/parallel_rl_env.py#L195",returnDescription:`
<p>A dict of observations for each sensor.
reward (<code>float</code>):
The reward for the current step.
done (<code>bool</code>):
Whether the episode is done.
info (<code>Dict</code>):
A dict of additional information.</p>
`,returnType:`
<p>obs (<code>Dict</code>)</p>
`}}),le=new y({props:{name:"step_send_async",anchor:"simulate.ParallelRLEnv.step_send_async",parameters:[{name:"action",val:": typing.Union[typing.Dict, typing.List, numpy.ndarray]"}],parametersDescription:[{anchor:"simulate.ParallelRLEnv.step_send_async.action",description:`<strong>action</strong> (<code>Dict</code> or <code>List</code> or <code>np.ndarray</code>) &#x2014;
A dict or list of actions for each actuator.`,name:"action"}],source:"https://github.com/huggingface/simulate/blob/main/src/simulate/rl/parallel_rl_env.py#L148"}}),ce=new y({props:{name:"class simulate.MultiProcessRLEnv",anchor:"simulate.MultiProcessRLEnv",parameters:[{name:"env_fn",val:": typing.Callable"},{name:"n_parallel",val:": int"},{name:"starting_port",val:": int = 55001"}],parametersDescription:[{anchor:"simulate.MultiProcessRLEnv.env_fn",description:`<strong>env_fn</strong> (<code>Callable</code>) &#x2014; a generator function that returns a RLEnv / ParallelRLEnv for generating instances
of the desired environment.`,name:"env_fn"},{anchor:"simulate.MultiProcessRLEnv.n_parallel",description:"<strong>n_parallel</strong> (<code>int</code>) &#x2014; the number of executable instances to create.",name:"n_parallel"},{anchor:"simulate.MultiProcessRLEnv.starting_port",description:"<strong>starting_port</strong> (<code>int</code>) &#x2014; initial communication port for spawned executables.",name:"starting_port"}],source:"https://github.com/huggingface/simulate/blob/main/src/simulate/rl/multi_proc_rl_env.py#L41"}}),me=new y({props:{name:"step",anchor:"simulate.MultiProcessRLEnv.step",parameters:[{name:"actions",val:": typing.Union[list, <built-in function array>, NoneType] = None"}],parametersDescription:[{anchor:"simulate.MultiProcessRLEnv.step.actions",description:"<strong>actions</strong> (<code>Dict</code> or <code>List</code>) &#x2014; TODO verify, a dict with actuator tags as keys and as values a Tensor of shape (n_show, n_actors, n_actions)",name:"actions"}],source:"https://github.com/huggingface/simulate/blob/main/src/simulate/rl/multi_proc_rl_env.py#L72",returnDescription:`
<p>TODO
all_reward (<code>float</code>): TODO
all_done (<code>bool</code>): TODO
all_info: TODO</p>
`,returnType:`
<p>all_observation (<code>Dict</code>)</p>
`}}),{c(){R=r("meta"),Ue=s(),P=r("h1"),x=r("a"),ve=r("span"),h(B.$$.fragment),at=s(),be=r("span"),ot=p("RL Environment Wrappers"),Ge=s(),c=r("div"),h(H.$$.fragment),st=s(),_e=r("p"),it=p("The basic RL environment wrapper for Simulate scene following the Gym API."),lt=s(),T=r("div"),h(J.$$.fragment),ct=s(),ye=r("p"),pt=p("Close the scene."),mt=s(),I=r("div"),h(j.$$.fragment),dt=s(),$e=r("p"),ut=p("Return a class attribute by name."),ht=s(),O=r("div"),h(z.$$.fragment),ft=s(),Ee=r("p"),gt=p("Resets the actors and the scene of the environment."),vt=s(),A=r("div"),h(K.$$.fragment),bt=s(),Le=r("p"),_t=p(`Samples an action from the actors in the environment. This function loads the configuration of maps and actors
to return the correct shape across multiple configurations.`),yt=s(),w=r("div"),h(Q.$$.fragment),$t=s(),we=r("p"),Et=p("The step function for the environment, follows the API from OpenAI Gym."),Lt=s(),De=r("p"),wt=p("TODO verify, a dict with actuator tags as keys and as values a Tensor of shape (n_show, n_actors, n_actions)"),Dt=s(),S=r("div"),h(X.$$.fragment),Rt=s(),Re=r("p"),Pt=p("Step the environment asynchronously."),xt=s(),V=r("div"),h(Y.$$.fragment),Tt=s(),Pe=r("p"),It=p("Receive the response from the environment asynchronously."),Ot=s(),k=r("div"),h(Z.$$.fragment),At=s(),xe=r("p"),St=p("Send action for execution asynchronously."),Ne=s(),d=r("div"),h(ee.$$.fragment),Vt=s(),de=r("p"),kt=p(`RL environment wrapper for Simulate scene. Uses functionality from the VecEnv in stable baselines 3
For more information on VecEnv, see the source
`),te=r("a"),Ct=p("https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html"),Mt=s(),C=r("div"),h(ne.$$.fragment),Ut=s(),Te=r("p"),Gt=p("Close the environment."),Nt=s(),M=r("div"),h(re.$$.fragment),Wt=s(),Ie=r("p"),Ft=p("Check if the environment is wrapped."),qt=s(),U=r("div"),h(ae.$$.fragment),Bt=s(),Oe=r("p"),Ht=p("Resets the actors and the scene of the environment."),Jt=s(),G=r("div"),h(oe.$$.fragment),jt=s(),Ae=r("p"),zt=p("Samples an action from the actors in the environment. This function loads the configuration of maps and actors to return the correct shape across multiple configurations."),Kt=s(),D=r("div"),h(se.$$.fragment),Qt=s(),Se=r("p"),Xt=p("The step function for the environment, follows the API from OpenAI Gym."),Yt=s(),Ve=r("p"),Zt=p("TODO verify, a dict with actuator tags as keys and as values a Tensor of shape (n_show, n_actors, n_actions)"),en=s(),N=r("div"),h(ie.$$.fragment),tn=s(),ke=r("p"),nn=p("Receive the response of a step from the environment asynchronously."),rn=s(),W=r("div"),h(le.$$.fragment),an=s(),Ce=r("p"),on=p("Send a step to the environment asynchronously."),We=s(),E=r("div"),h(ce.$$.fragment),sn=s(),ue=r("p"),ln=p(`Multi-process RL environment wrapper for Simulate scene. Spawns multiple backend executables to run in parallel,
in addition to the optionality of multiple maps.
Uses functionality from the VecEnv in stable baselines 3. For more information on VecEnv, see the source
`),pe=r("a"),cn=p("https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html"),pn=s(),F=r("div"),h(me.$$.fragment),mn=s(),Me=r("p"),dn=p("The step function for the environment, follows the API from OpenAI Gym."),this.h()},l(t){const L=qn('[data-svelte="svelte-1phssyn"]',document.head);R=a(L,"META",{name:!0,content:!0}),L.forEach(n),Ue=i(t),P=a(t,"H1",{class:!0});var qe=o(P);x=a(qe,"A",{id:!0,class:!0,href:!0});var gn=o(x);ve=a(gn,"SPAN",{});var vn=o(ve);f(B.$$.fragment,vn),vn.forEach(n),gn.forEach(n),at=i(qe),be=a(qe,"SPAN",{});var bn=o(be);ot=m(bn,"RL Environment Wrappers"),bn.forEach(n),qe.forEach(n),Ge=i(t),c=a(t,"DIV",{class:!0});var u=o(c);f(H.$$.fragment,u),st=i(u),_e=a(u,"P",{});var _n=o(_e);it=m(_n,"The basic RL environment wrapper for Simulate scene following the Gym API."),_n.forEach(n),lt=i(u),T=a(u,"DIV",{class:!0});var Be=o(T);f(J.$$.fragment,Be),ct=i(Be),ye=a(Be,"P",{});var yn=o(ye);pt=m(yn,"Close the scene."),yn.forEach(n),Be.forEach(n),mt=i(u),I=a(u,"DIV",{class:!0});var He=o(I);f(j.$$.fragment,He),dt=i(He),$e=a(He,"P",{});var $n=o($e);ut=m($n,"Return a class attribute by name."),$n.forEach(n),He.forEach(n),ht=i(u),O=a(u,"DIV",{class:!0});var Je=o(O);f(z.$$.fragment,Je),ft=i(Je),Ee=a(Je,"P",{});var En=o(Ee);gt=m(En,"Resets the actors and the scene of the environment."),En.forEach(n),Je.forEach(n),vt=i(u),A=a(u,"DIV",{class:!0});var je=o(A);f(K.$$.fragment,je),bt=i(je),Le=a(je,"P",{});var Ln=o(Le);_t=m(Ln,`Samples an action from the actors in the environment. This function loads the configuration of maps and actors
to return the correct shape across multiple configurations.`),Ln.forEach(n),je.forEach(n),yt=i(u),w=a(u,"DIV",{class:!0});var he=o(w);f(Q.$$.fragment,he),$t=i(he),we=a(he,"P",{});var wn=o(we);Et=m(wn,"The step function for the environment, follows the API from OpenAI Gym."),wn.forEach(n),Lt=i(he),De=a(he,"P",{});var Dn=o(De);wt=m(Dn,"TODO verify, a dict with actuator tags as keys and as values a Tensor of shape (n_show, n_actors, n_actions)"),Dn.forEach(n),he.forEach(n),Dt=i(u),S=a(u,"DIV",{class:!0});var ze=o(S);f(X.$$.fragment,ze),Rt=i(ze),Re=a(ze,"P",{});var Rn=o(Re);Pt=m(Rn,"Step the environment asynchronously."),Rn.forEach(n),ze.forEach(n),xt=i(u),V=a(u,"DIV",{class:!0});var Ke=o(V);f(Y.$$.fragment,Ke),Tt=i(Ke),Pe=a(Ke,"P",{});var Pn=o(Pe);It=m(Pn,"Receive the response from the environment asynchronously."),Pn.forEach(n),Ke.forEach(n),Ot=i(u),k=a(u,"DIV",{class:!0});var Qe=o(k);f(Z.$$.fragment,Qe),At=i(Qe),xe=a(Qe,"P",{});var xn=o(xe);St=m(xn,"Send action for execution asynchronously."),xn.forEach(n),Qe.forEach(n),u.forEach(n),Ne=i(t),d=a(t,"DIV",{class:!0});var $=o(d);f(ee.$$.fragment,$),Vt=i($),de=a($,"P",{});var un=o(de);kt=m(un,`RL environment wrapper for Simulate scene. Uses functionality from the VecEnv in stable baselines 3
For more information on VecEnv, see the source
`),te=a(un,"A",{href:!0,rel:!0});var Tn=o(te);Ct=m(Tn,"https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html"),Tn.forEach(n),un.forEach(n),Mt=i($),C=a($,"DIV",{class:!0});var Xe=o(C);f(ne.$$.fragment,Xe),Ut=i(Xe),Te=a(Xe,"P",{});var In=o(Te);Gt=m(In,"Close the environment."),In.forEach(n),Xe.forEach(n),Nt=i($),M=a($,"DIV",{class:!0});var Ye=o(M);f(re.$$.fragment,Ye),Wt=i(Ye),Ie=a(Ye,"P",{});var On=o(Ie);Ft=m(On,"Check if the environment is wrapped."),On.forEach(n),Ye.forEach(n),qt=i($),U=a($,"DIV",{class:!0});var Ze=o(U);f(ae.$$.fragment,Ze),Bt=i(Ze),Oe=a(Ze,"P",{});var An=o(Oe);Ht=m(An,"Resets the actors and the scene of the environment."),An.forEach(n),Ze.forEach(n),Jt=i($),G=a($,"DIV",{class:!0});var et=o(G);f(oe.$$.fragment,et),jt=i(et),Ae=a(et,"P",{});var Sn=o(Ae);zt=m(Sn,"Samples an action from the actors in the environment. This function loads the configuration of maps and actors to return the correct shape across multiple configurations."),Sn.forEach(n),et.forEach(n),Kt=i($),D=a($,"DIV",{class:!0});var fe=o(D);f(se.$$.fragment,fe),Qt=i(fe),Se=a(fe,"P",{});var Vn=o(Se);Xt=m(Vn,"The step function for the environment, follows the API from OpenAI Gym."),Vn.forEach(n),Yt=i(fe),Ve=a(fe,"P",{});var kn=o(Ve);Zt=m(kn,"TODO verify, a dict with actuator tags as keys and as values a Tensor of shape (n_show, n_actors, n_actions)"),kn.forEach(n),fe.forEach(n),en=i($),N=a($,"DIV",{class:!0});var tt=o(N);f(ie.$$.fragment,tt),tn=i(tt),ke=a(tt,"P",{});var Cn=o(ke);nn=m(Cn,"Receive the response of a step from the environment asynchronously."),Cn.forEach(n),tt.forEach(n),rn=i($),W=a($,"DIV",{class:!0});var nt=o(W);f(le.$$.fragment,nt),an=i(nt),Ce=a(nt,"P",{});var Mn=o(Ce);on=m(Mn,"Send a step to the environment asynchronously."),Mn.forEach(n),nt.forEach(n),$.forEach(n),We=i(t),E=a(t,"DIV",{class:!0});var ge=o(E);f(ce.$$.fragment,ge),sn=i(ge),ue=a(ge,"P",{});var hn=o(ue);ln=m(hn,`Multi-process RL environment wrapper for Simulate scene. Spawns multiple backend executables to run in parallel,
in addition to the optionality of multiple maps.
Uses functionality from the VecEnv in stable baselines 3. For more information on VecEnv, see the source
`),pe=a(hn,"A",{href:!0,rel:!0});var Un=o(pe);cn=m(Un,"https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html"),Un.forEach(n),hn.forEach(n),pn=i(ge),F=a(ge,"DIV",{class:!0});var rt=o(F);f(me.$$.fragment,rt),mn=i(rt),Me=a(rt,"P",{});var Gn=o(Me);dn=m(Gn,"The step function for the environment, follows the API from OpenAI Gym."),Gn.forEach(n),rt.forEach(n),ge.forEach(n),this.h()},h(){l(R,"name","hf:doc:metadata"),l(R,"content",JSON.stringify(zn)),l(x,"id","simulate.RLEnv"),l(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(x,"href","#simulate.RLEnv"),l(P,"class","relative group"),l(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(c,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(te,"href","https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html"),l(te,"rel","nofollow"),l(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(d,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(pe,"href","https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html"),l(pe,"rel","nofollow"),l(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,L){e(document.head,R),q(t,Ue,L),q(t,P,L),e(P,x),e(x,ve),g(B,ve,null),e(P,at),e(P,be),e(be,ot),q(t,Ge,L),q(t,c,L),g(H,c,null),e(c,st),e(c,_e),e(_e,it),e(c,lt),e(c,T),g(J,T,null),e(T,ct),e(T,ye),e(ye,pt),e(c,mt),e(c,I),g(j,I,null),e(I,dt),e(I,$e),e($e,ut),e(c,ht),e(c,O),g(z,O,null),e(O,ft),e(O,Ee),e(Ee,gt),e(c,vt),e(c,A),g(K,A,null),e(A,bt),e(A,Le),e(Le,_t),e(c,yt),e(c,w),g(Q,w,null),e(w,$t),e(w,we),e(we,Et),e(w,Lt),e(w,De),e(De,wt),e(c,Dt),e(c,S),g(X,S,null),e(S,Rt),e(S,Re),e(Re,Pt),e(c,xt),e(c,V),g(Y,V,null),e(V,Tt),e(V,Pe),e(Pe,It),e(c,Ot),e(c,k),g(Z,k,null),e(k,At),e(k,xe),e(xe,St),q(t,Ne,L),q(t,d,L),g(ee,d,null),e(d,Vt),e(d,de),e(de,kt),e(de,te),e(te,Ct),e(d,Mt),e(d,C),g(ne,C,null),e(C,Ut),e(C,Te),e(Te,Gt),e(d,Nt),e(d,M),g(re,M,null),e(M,Wt),e(M,Ie),e(Ie,Ft),e(d,qt),e(d,U),g(ae,U,null),e(U,Bt),e(U,Oe),e(Oe,Ht),e(d,Jt),e(d,G),g(oe,G,null),e(G,jt),e(G,Ae),e(Ae,zt),e(d,Kt),e(d,D),g(se,D,null),e(D,Qt),e(D,Se),e(Se,Xt),e(D,Yt),e(D,Ve),e(Ve,Zt),e(d,en),e(d,N),g(ie,N,null),e(N,tn),e(N,ke),e(ke,nn),e(d,rn),e(d,W),g(le,W,null),e(W,an),e(W,Ce),e(Ce,on),q(t,We,L),q(t,E,L),g(ce,E,null),e(E,sn),e(E,ue),e(ue,ln),e(ue,pe),e(pe,cn),e(E,pn),e(E,F),g(me,F,null),e(F,mn),e(F,Me),e(Me,dn),Fe=!0},p:Bn,i(t){Fe||(v(B.$$.fragment,t),v(H.$$.fragment,t),v(J.$$.fragment,t),v(j.$$.fragment,t),v(z.$$.fragment,t),v(K.$$.fragment,t),v(Q.$$.fragment,t),v(X.$$.fragment,t),v(Y.$$.fragment,t),v(Z.$$.fragment,t),v(ee.$$.fragment,t),v(ne.$$.fragment,t),v(re.$$.fragment,t),v(ae.$$.fragment,t),v(oe.$$.fragment,t),v(se.$$.fragment,t),v(ie.$$.fragment,t),v(le.$$.fragment,t),v(ce.$$.fragment,t),v(me.$$.fragment,t),Fe=!0)},o(t){b(B.$$.fragment,t),b(H.$$.fragment,t),b(J.$$.fragment,t),b(j.$$.fragment,t),b(z.$$.fragment,t),b(K.$$.fragment,t),b(Q.$$.fragment,t),b(X.$$.fragment,t),b(Y.$$.fragment,t),b(Z.$$.fragment,t),b(ee.$$.fragment,t),b(ne.$$.fragment,t),b(re.$$.fragment,t),b(ae.$$.fragment,t),b(oe.$$.fragment,t),b(se.$$.fragment,t),b(ie.$$.fragment,t),b(le.$$.fragment,t),b(ce.$$.fragment,t),b(me.$$.fragment,t),Fe=!1},d(t){n(R),t&&n(Ue),t&&n(P),_(B),t&&n(Ge),t&&n(c),_(H),_(J),_(j),_(z),_(K),_(Q),_(X),_(Y),_(Z),t&&n(Ne),t&&n(d),_(ee),_(ne),_(re),_(ae),_(oe),_(se),_(ie),_(le),t&&n(We),t&&n(E),_(ce),_(me)}}}const zn={local:"simulate.RLEnv",title:"RL Environment Wrappers"};function Kn(fn){return Hn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Zn extends Nn{constructor(R){super();Wn(this,R,Kn,jn,Fn,{})}}export{Zn as default,zn as metadata};
