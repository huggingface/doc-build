import{S as Dm,i as Nm,s as Om,e as n,k as l,w as f,t as s,M as Wm,c as r,d as o,m as d,a,x as u,h as i,b as c,F as e,g as p,y as g,q as _,o as v,B as k}from"../../chunks/vendor-4833417e.js";import{T as vr}from"../../chunks/Tip-fffd6df1.js";import{D as z}from"../../chunks/Docstring-7b52c3d4.js";import{C as Ve}from"../../chunks/CodeBlock-6a3d1b46.js";import{I as J}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-dacfbfaf.js";function Km(N){let h,$,w,T,y;return{c(){h=n("p"),$=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),w=n("code"),T=s("Module"),y=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var b=a(h);$=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),w=r(b,"CODE",{});var q=a(w);T=i(q,"Module"),q.forEach(o),y=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(R,b){p(R,h,b),e(h,$),e(h,w),e(w,T),e(h,y)},d(R){R&&o(h)}}}function Bm(N){let h,$,w,T,y;return{c(){h=n("p"),$=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),w=n("code"),T=s("Module"),y=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var b=a(h);$=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),w=r(b,"CODE",{});var q=a(w);T=i(q,"Module"),q.forEach(o),y=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(R,b){p(R,h,b),e(h,$),e(h,w),e(w,T),e(h,y)},d(R){R&&o(h)}}}function Qm(N){let h,$,w,T,y;return{c(){h=n("p"),$=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),w=n("code"),T=s("Module"),y=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var b=a(h);$=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),w=r(b,"CODE",{});var q=a(w);T=i(q,"Module"),q.forEach(o),y=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(R,b){p(R,h,b),e(h,$),e(h,w),e(w,T),e(h,y)},d(R){R&&o(h)}}}function Hm(N){let h,$,w,T,y;return{c(){h=n("p"),$=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),w=n("code"),T=s("Module"),y=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var b=a(h);$=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),w=r(b,"CODE",{});var q=a(w);T=i(q,"Module"),q.forEach(o),y=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(R,b){p(R,h,b),e(h,$),e(h,w),e(w,T),e(h,y)},d(R){R&&o(h)}}}function Vm(N){let h,$,w,T,y;return{c(){h=n("p"),$=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),w=n("code"),T=s("Module"),y=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var b=a(h);$=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),w=r(b,"CODE",{});var q=a(w);T=i(q,"Module"),q.forEach(o),y=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(R,b){p(R,h,b),e(h,$),e(h,w),e(w,T),e(h,y)},d(R){R&&o(h)}}}function Um(N){let h,$,w,T,y,R,b,q,ka,kr,se,ye,tn,Ue,wa,on,ba,wr,Ee,Ra,Xe,Ta,$a,br,lo,ya,Rr,co,nn,Ea,Tr,Z,za,Ge,qa,xa,Je,Aa,Pa,$r,ie,ze,rn,Ze,ja,an,La,yr,x,Ye,Ma,sn,Sa,Fa,L,ln,mo,Ca,Ia,dn,ho,Da,Na,cn,po,Oa,Wa,mn,fo,Ka,Ba,hn,uo,Qa,Ha,pn,go,Va,Ua,et,Xa,tt,Ga,Ja,Za,le,Ya,_o,es,ts,vo,os,ns,rs,fn,as,ss,ot,Er,de,qe,un,nt,is,gn,ls,zr,E,rt,ds,_n,cs,ms,xe,ko,hs,ps,wo,fs,us,gs,at,_s,bo,vs,ks,ws,Y,st,bs,vn,Rs,Ts,it,Ro,$s,kn,ys,Es,To,zs,wn,qs,xs,Ae,lt,As,dt,Ps,bn,js,Ls,Ms,O,ct,Ss,Rn,Fs,Cs,mt,Is,ce,Ds,Tn,Ns,Os,$n,Ws,Ks,Bs,$o,ht,Qs,P,pt,Hs,ft,Vs,yn,Us,Xs,Gs,me,En,Js,Zs,ut,Ys,zn,ei,ti,oi,gt,ni,qn,ri,ai,si,_t,yo,ii,xn,li,di,Eo,ci,An,mi,hi,Pn,pi,fi,vt,qr,he,Pe,jn,kt,ui,Ln,gi,xr,M,wt,_i,bt,vi,Mn,ki,wi,bi,je,zo,Ri,Ti,qo,$i,yi,Ei,Rt,zi,xo,qi,xi,Ai,j,Tt,Pi,$t,ji,Sn,Li,Mi,Si,pe,Fn,Fi,Ci,yt,Ii,Cn,Di,Ni,Oi,Et,Wi,In,Ki,Bi,Qi,zt,Ao,Hi,Dn,Vi,Ui,Po,Xi,Nn,Gi,Ji,On,Zi,Yi,qt,Ar,fe,Le,Wn,xt,el,Kn,tl,Pr,Q,At,ol,Bn,nl,rl,Me,Pt,al,Qn,sl,jr,ue,Se,Hn,jt,il,Vn,ll,Lr,H,Lt,dl,Mt,cl,St,ml,hl,pl,S,Ft,fl,ge,ul,jo,gl,_l,Un,vl,kl,wl,Fe,bl,Xn,Rl,Tl,Ct,Mr,_e,Ce,Gn,It,$l,Jn,yl,Sr,V,Dt,El,Nt,zl,Ot,ql,xl,Al,F,Wt,Pl,ve,jl,Lo,Ll,Ml,Zn,Sl,Fl,Cl,Ie,Il,Yn,Dl,Nl,Kt,Fr,ke,De,er,Bt,Ol,tr,Wl,Cr,U,Qt,Kl,Ht,Bl,Vt,Ql,Hl,Vl,C,Ut,Ul,we,Xl,Mo,Gl,Jl,or,Zl,Yl,ed,Ne,td,nr,od,nd,Xt,Ir,be,Oe,rr,Gt,rd,ar,ad,Dr,X,Jt,sd,Zt,id,Yt,ld,dd,cd,ee,eo,md,Re,hd,So,pd,fd,sr,ud,gd,_d,We,Nr,Te,Ke,ir,to,vd,lr,kd,Or,G,oo,wd,Be,dr,bd,Rd,no,Td,$d,yd,I,ro,Ed,$e,zd,Fo,qd,xd,cr,Ad,Pd,jd,Qe,Ld,mr,Md,Sd,ao,Wr;return R=new J({}),Ue=new J({}),Ze=new J({}),Ye=new z({props:{name:"class transformers.RealmConfig",anchor:"transformers.RealmConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"retriever_proj_size",val:" = 128"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"num_candidates",val:" = 8"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu_new'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"span_hidden_size",val:" = 256"},{name:"max_span_width",val:" = 10"},{name:"reader_layer_norm_eps",val:" = 0.001"},{name:"reader_beam_size",val:" = 5"},{name:"reader_seq_len",val:" = 320"},{name:"num_block_records",val:" = 13353718"},{name:"searcher_beam_size",val:" = 5000"},{name:"searcher_seq_len",val:" = 64"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/configuration_realm.py#L36",parametersDescription:[{anchor:"transformers.RealmConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the REALM model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmEmbedder">RealmEmbedder</a>, <a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmScorer">RealmScorer</a>, <a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmKnowledgeAugEncoder">RealmKnowledgeAugEncoder</a>, or
<a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmReader">RealmReader</a>.`,name:"vocab_size"},{anchor:"transformers.RealmConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimension of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.RealmConfig.retriever_proj_size",description:`<strong>retriever_proj_size</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Dimension of the retriever(embedder) projection.`,name:"retriever_proj_size"},{anchor:"transformers.RealmConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.RealmConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.RealmConfig.num_candidates",description:`<strong>num_candidates</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of candidates inputted to the RealmScorer or RealmKnowledgeAugEncoder.`,name:"num_candidates"},{anchor:"transformers.RealmConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimension of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.RealmConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu_new&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.RealmConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.RealmConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.RealmConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.RealmConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmEmbedder">RealmEmbedder</a>, <a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmScorer">RealmScorer</a>,
<a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmKnowledgeAugEncoder">RealmKnowledgeAugEncoder</a>, or <a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmReader">RealmReader</a>.`,name:"type_vocab_size"},{anchor:"transformers.RealmConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.RealmConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.RealmConfig.span_hidden_size",description:`<strong>span_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimension of the reader&#x2019;s spans.`,name:"span_hidden_size"},{anchor:"transformers.RealmConfig.max_span_width",description:`<strong>max_span_width</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
Max span width of the reader.`,name:"max_span_width"},{anchor:"transformers.RealmConfig.reader_layer_norm_eps",description:`<strong>reader_layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-3) &#x2014;
The epsilon used by the reader&#x2019;s layer normalization layers.`,name:"reader_layer_norm_eps"},{anchor:"transformers.RealmConfig.reader_beam_size",description:`<strong>reader_beam_size</strong> (<code>int</code>, <em>optional</em>, defaults to 5) &#x2014;
Beam size of the reader.`,name:"reader_beam_size"},{anchor:"transformers.RealmConfig.reader_seq_len",description:`<strong>reader_seq_len</strong> (<code>int</code>, <em>optional</em>, defaults to 288+32) &#x2014;
Maximum sequence length of the reader.`,name:"reader_seq_len"},{anchor:"transformers.RealmConfig.num_block_records",description:`<strong>num_block_records</strong> (<code>int</code>, <em>optional</em>, defaults to 13353718) &#x2014;
Number of block records.`,name:"num_block_records"},{anchor:"transformers.RealmConfig.searcher_beam_size",description:`<strong>searcher_beam_size</strong> (<code>int</code>, <em>optional</em>, defaults to 5000) &#x2014;
Beam size of the searcher. Note that when eval mode is enabled, <em>searcher_beam_size</em> will be the same as
<em>reader_beam_size</em>.`,name:"searcher_beam_size"},{anchor:"transformers.RealmConfig.searcher_seq_len",description:`<strong>searcher_seq_len</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Maximum sequence length of the searcher.`,name:"searcher_seq_len"}]}}),ot=new Ve({props:{code:`from transformers import RealmEmbedder, RealmConfig

# Initializing a REALM realm-cc-news-pretrained-* style configuration
configuration = RealmConfig()

# Initializing a model from the google/realm-cc-news-pretrained-embedder style configuration
model = RealmEmbedder(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmEmbedder, RealmConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a REALM realm-cc-news-pretrained-* style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = RealmConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the google/realm-cc-news-pretrained-embedder style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmEmbedder(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),nt=new J({}),rt=new z({props:{name:"class transformers.RealmTokenizer",anchor:"transformers.RealmTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/tokenization_realm.py#L88",parametersDescription:[{anchor:"transformers.RealmTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.RealmTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.RealmTokenizer.do_basic_tokenize",description:`<strong>do_basic_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to do basic tokenization before WordPiece.`,name:"do_basic_tokenize"},{anchor:"transformers.RealmTokenizer.never_split",description:`<strong>never_split</strong> (<code>Iterable</code>, <em>optional</em>) &#x2014;
Collection of tokens which will never be split during tokenization. Only has an effect when
<code>do_basic_tokenize=True</code>`,name:"never_split"},{anchor:"transformers.RealmTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.RealmTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.RealmTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.RealmTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.RealmTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.RealmTokenizer.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters.</p>
<p>This should likely be deactivated for Japanese (see this
<a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">issue</a>).
strip_accents &#x2014; (<code>bool</code>, <em>optional</em>):
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"tokenize_chinese_chars"}]}}),st=new z({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.RealmTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/tokenization_realm.py#L295",parametersDescription:[{anchor:"transformers.RealmTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.RealmTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),lt=new z({props:{name:"get_special_tokens_mask",anchor:"transformers.RealmTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/tokenization_realm.py#L320",parametersDescription:[{anchor:"transformers.RealmTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.RealmTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.RealmTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ct=new z({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.RealmTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/tokenization_realm.py#L348",parametersDescription:[{anchor:"transformers.RealmTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.RealmTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),mt=new Ve({props:{code:`0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),ht=new z({props:{name:"save_vocabulary",anchor:"transformers.RealmTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/tokenization_realm.py#L377"}}),pt=new z({props:{name:"batch_encode_candidates",anchor:"transformers.RealmTokenizer.batch_encode_candidates",parameters:[{name:"text",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/tokenization_realm.py#L222",parametersDescription:[{anchor:"transformers.RealmTokenizer.batch_encode_candidates.text",description:`<strong>text</strong> (<code>List[List[str]]</code>) &#x2014;
The batch of sequences to be encoded. Each sequence must be in this format: (batch_size,
num_candidates, text).`,name:"text"},{anchor:"transformers.RealmTokenizer.batch_encode_candidates.text_pair",description:`<strong>text_pair</strong> (<code>List[List[str]]</code>, <em>optional</em>) &#x2014;
The batch of sequences to be encoded. Each sequence must be in this format: (batch_size,
num_candidates, text).
**kwargs &#x2014;
Keyword arguments of the <strong>call</strong> method.`,name:"text_pair"}],returnDescription:`
<p>Encoded text or text pair.</p>
`,returnType:`
<p><a
  href="/docs/transformers/v4.17.0/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a></p>
`}}),vt=new Ve({props:{code:`from transformers import RealmTokenizer

# batch_size = 2, num_candidates = 2
text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-encoder")
tokenized_text = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># batch_size = 2, num_candidates = 2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = [[<span class="hljs-string">&quot;Hello world!&quot;</span>, <span class="hljs-string">&quot;Nice to meet you!&quot;</span>], [<span class="hljs-string">&quot;The cute cat.&quot;</span>, <span class="hljs-string">&quot;The adorable dog.&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-encoder&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_text = tokenizer.batch_encode_candidates(text, max_length=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`}}),kt=new J({}),wt=new z({props:{name:"class transformers.RealmTokenizerFast",anchor:"transformers.RealmTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/tokenization_realm_fast.py#L79",parametersDescription:[{anchor:"transformers.RealmTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.RealmTokenizerFast.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.RealmTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.RealmTokenizerFast.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.RealmTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.RealmTokenizerFast.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.RealmTokenizerFast.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.RealmTokenizerFast.clean_text",description:`<strong>clean_text</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to clean the text before tokenization by removing any control characters and replacing all
whitespaces by the classic one.`,name:"clean_text"},{anchor:"transformers.RealmTokenizerFast.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see <a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">this
issue</a>).`,name:"tokenize_chinese_chars"},{anchor:"transformers.RealmTokenizerFast.strip_accents",description:`<strong>strip_accents</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"strip_accents"},{anchor:"transformers.RealmTokenizerFast.wordpieces_prefix",description:`<strong>wordpieces_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;##&quot;</code>) &#x2014;
The prefix for subwords.`,name:"wordpieces_prefix"}]}}),Tt=new z({props:{name:"batch_encode_candidates",anchor:"transformers.RealmTokenizerFast.batch_encode_candidates",parameters:[{name:"text",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/tokenization_realm_fast.py#L170",parametersDescription:[{anchor:"transformers.RealmTokenizerFast.batch_encode_candidates.text",description:`<strong>text</strong> (<code>List[List[str]]</code>) &#x2014;
The batch of sequences to be encoded. Each sequence must be in this format: (batch_size,
num_candidates, text).`,name:"text"},{anchor:"transformers.RealmTokenizerFast.batch_encode_candidates.text_pair",description:`<strong>text_pair</strong> (<code>List[List[str]]</code>, <em>optional</em>) &#x2014;
The batch of sequences to be encoded. Each sequence must be in this format: (batch_size,
num_candidates, text).
**kwargs &#x2014;
Keyword arguments of the <strong>call</strong> method.`,name:"text_pair"}],returnDescription:`
<p>Encoded text or text pair.</p>
`,returnType:`
<p><a
  href="/docs/transformers/v4.17.0/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a></p>
`}}),qt=new Ve({props:{code:`from transformers import RealmTokenizerFast

# batch_size = 2, num_candidates = 2
text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

tokenizer = RealmTokenizerFast.from_pretrained("google/realm-cc-news-pretrained-encoder")
tokenized_text = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># batch_size = 2, num_candidates = 2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = [[<span class="hljs-string">&quot;Hello world!&quot;</span>, <span class="hljs-string">&quot;Nice to meet you!&quot;</span>], [<span class="hljs-string">&quot;The cute cat.&quot;</span>, <span class="hljs-string">&quot;The adorable dog.&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizerFast.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-encoder&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_text = tokenizer.batch_encode_candidates(text, max_length=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`}}),xt=new J({}),At=new z({props:{name:"class transformers.RealmRetriever",anchor:"transformers.RealmRetriever",parameters:[{name:"block_records",val:""},{name:"tokenizer",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/retrieval_realm.py#L73",parametersDescription:[{anchor:"transformers.RealmRetriever.block_records",description:`<strong>block_records</strong> (<code>np.ndarray</code>) &#x2014;
A numpy array which cantains evidence texts.`,name:"block_records"},{anchor:"transformers.RealmRetriever.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>) &#x2014;
The tokenizer to encode retrieved texts.`,name:"tokenizer"}]}}),Pt=new z({props:{name:"block_has_answer",anchor:"transformers.RealmRetriever.block_has_answer",parameters:[{name:"concat_inputs",val:""},{name:"answer_ids",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/retrieval_realm.py#L128"}}),jt=new J({}),Lt=new z({props:{name:"class transformers.RealmEmbedder",anchor:"transformers.RealmEmbedder",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/modeling_realm.py#L1146",parametersDescription:[{anchor:"transformers.RealmEmbedder.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.17.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ft=new z({props:{name:"forward",anchor:"transformers.RealmEmbedder.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/modeling_realm.py#L1160",parametersDescription:[{anchor:"transformers.RealmEmbedder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/v4.17.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.17.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmEmbedder.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmEmbedder.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmEmbedder.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RealmEmbedder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RealmEmbedder.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RealmEmbedder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RealmEmbedder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RealmEmbedder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.17.0/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <code>transformers.models.realm.modeling_realm.RealmEmbedderOutput</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>projected_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.retriever_proj_size)</code>) \u2014 Projected score.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Fe=new vr({props:{$$slots:{default:[Km]},$$scope:{ctx:N}}}),Ct=new Ve({props:{code:`from transformers import RealmTokenizer, RealmEmbedder
import torch

tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-embedder")
model = RealmEmbedder.from_pretrained("google/realm-cc-news-pretrained-embedder")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

projected_score = outputs.projected_score`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizer, RealmEmbedder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-embedder&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmEmbedder.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-embedder&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>projected_score = outputs.projected_score`}}),It=new J({}),Dt=new z({props:{name:"class transformers.RealmScorer",anchor:"transformers.RealmScorer",parameters:[{name:"config",val:""},{name:"query_embedder",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/modeling_realm.py#L1226",parametersDescription:[{anchor:"transformers.RealmScorer.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.17.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.RealmScorer.query_embedder",description:`<strong>query_embedder</strong> (<a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmEmbedder">RealmEmbedder</a>) &#x2014;
Embedder for input sequences. If not specified, it will use the same embedder as candidate sequences.`,name:"query_embedder"}]}}),Wt=new z({props:{name:"forward",anchor:"transformers.RealmScorer.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"candidate_input_ids",val:" = None"},{name:"candidate_attention_mask",val:" = None"},{name:"candidate_token_type_ids",val:" = None"},{name:"candidate_inputs_embeds",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/modeling_realm.py#L1242",parametersDescription:[{anchor:"transformers.RealmScorer.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/v4.17.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.17.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmScorer.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmScorer.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmScorer.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RealmScorer.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RealmScorer.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RealmScorer.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RealmScorer.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RealmScorer.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.17.0/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RealmScorer.forward.candidate_input_ids",description:`<strong>candidate_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>) &#x2014;
Indices of candidate input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/v4.17.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.17.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"candidate_input_ids"},{anchor:"transformers.RealmScorer.forward.candidate_attention_mask",description:`<strong>candidate_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"candidate_attention_mask"},{anchor:"transformers.RealmScorer.forward.candidate_token_type_ids",description:`<strong>candidate_token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"candidate_token_type_ids"},{anchor:"transformers.RealmScorer.forward.candidate_inputs_embeds",description:`<strong>candidate_inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size * num_candidates, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>candidate_input_ids</code> you can choose to directly pass an embedded
representation. This is useful if you want more control over how to convert <em>candidate_input_ids</em> indices
into associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"candidate_inputs_embeds"}],returnDescription:`
<p>A <code>transformers.models.realm.modeling_realm.RealmScorerOutput</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li><strong>relevance_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_candidates)</code>) \u2014 The relevance score of document candidates (before softmax).</li>
<li><strong>query_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.retriever_proj_size)</code>) \u2014 Query score derived from the query embedder.</li>
<li><strong>candidate_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_candidates, config.retriever_proj_size)</code>) \u2014 Candidate score derived from the embedder.</li>
</ul>
`}}),Ie=new vr({props:{$$slots:{default:[Bm]},$$scope:{ctx:N}}}),Kt=new Ve({props:{code:`import torch
from transformers import RealmTokenizer, RealmScorer

tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-scorer")
model = RealmScorer.from_pretrained("google/realm-cc-news-pretrained-scorer", num_candidates=2)

# batch_size = 2, num_candidates = 2
input_texts = ["How are you?", "What is the item in the picture?"]
candidates_texts = [["Hello world!", "Nice to meet you!"], ["A cute cat.", "An adorable dog."]]

inputs = tokenizer(input_texts, return_tensors="pt")
candidates_inputs = tokenizer.batch_encode_candidates(candidates_texts, max_length=10, return_tensors="pt")

outputs = model(
    **inputs,
    candidate_input_ids=candidates_inputs.input_ids,
    candidate_attention_mask=candidates_inputs.attention_mask,
    candidate_token_type_ids=candidates_inputs.token_type_ids,
)
relevance_score = outputs.relevance_score`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizer, RealmScorer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-scorer&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmScorer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-scorer&quot;</span>, num_candidates=<span class="hljs-number">2</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># batch_size = 2, num_candidates = 2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_texts = [<span class="hljs-string">&quot;How are you?&quot;</span>, <span class="hljs-string">&quot;What is the item in the picture?&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>candidates_texts = [[<span class="hljs-string">&quot;Hello world!&quot;</span>, <span class="hljs-string">&quot;Nice to meet you!&quot;</span>], [<span class="hljs-string">&quot;A cute cat.&quot;</span>, <span class="hljs-string">&quot;An adorable dog.&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(input_texts, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>candidates_inputs = tokenizer.batch_encode_candidates(candidates_texts, max_length=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(
<span class="hljs-meta">... </span>    **inputs,
<span class="hljs-meta">... </span>    candidate_input_ids=candidates_inputs.input_ids,
<span class="hljs-meta">... </span>    candidate_attention_mask=candidates_inputs.attention_mask,
<span class="hljs-meta">... </span>    candidate_token_type_ids=candidates_inputs.token_type_ids,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_score = outputs.relevance_score`}}),Bt=new J({}),Qt=new z({props:{name:"class transformers.RealmKnowledgeAugEncoder",anchor:"transformers.RealmKnowledgeAugEncoder",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/modeling_realm.py#L1373",parametersDescription:[{anchor:"transformers.RealmKnowledgeAugEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.17.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ut=new z({props:{name:"forward",anchor:"transformers.RealmKnowledgeAugEncoder.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"relevance_score",val:" = None"},{name:"labels",val:" = None"},{name:"mlm_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/modeling_realm.py#L1392",parametersDescription:[{anchor:"transformers.RealmKnowledgeAugEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/v4.17.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.17.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_candidates, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.17.0/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.relevance_score",description:`<strong>relevance_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_candidates)</code>, <em>optional</em>) &#x2014;
Relevance score derived from RealmScorer, must be specified if you want to compute the masked language
modeling loss.`,name:"relevance_score"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.mlm_mask",description:`<strong>mlm_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid calculating joint loss on certain positions. If not specified, the loss will not be masked.
Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"mlm_mask"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.17.0/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Ne=new vr({props:{$$slots:{default:[Qm]},$$scope:{ctx:N}}}),Xt=new Ve({props:{code:`import torch
from transformers import RealmTokenizer, RealmKnowledgeAugEncoder

tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-encoder")
model = RealmKnowledgeAugEncoder.from_pretrained(
    "google/realm-cc-news-pretrained-encoder", num_candidates=2
)

# batch_size = 2, num_candidates = 2
text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

inputs = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizer, RealmKnowledgeAugEncoder

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-encoder&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmKnowledgeAugEncoder.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google/realm-cc-news-pretrained-encoder&quot;</span>, num_candidates=<span class="hljs-number">2</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># batch_size = 2, num_candidates = 2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = [[<span class="hljs-string">&quot;Hello world!&quot;</span>, <span class="hljs-string">&quot;Nice to meet you!&quot;</span>], [<span class="hljs-string">&quot;The cute cat.&quot;</span>, <span class="hljs-string">&quot;The adorable dog.&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer.batch_encode_candidates(text, max_length=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Gt=new J({}),Jt=new z({props:{name:"class transformers.RealmReader",anchor:"transformers.RealmReader",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/modeling_realm.py#L1521",parametersDescription:[{anchor:"transformers.RealmReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.17.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),eo=new z({props:{name:"forward",anchor:"transformers.RealmReader.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"relevance_score",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"has_answers",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/modeling_realm.py#L1535",parametersDescription:[{anchor:"transformers.RealmReader.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(reader_beam_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/v4.17.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.17.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmReader.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(reader_beam_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmReader.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(reader_beam_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmReader.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(reader_beam_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RealmReader.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RealmReader.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(reader_beam_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RealmReader.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RealmReader.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RealmReader.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.17.0/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RealmReader.forward.relevance_score",description:`<strong>relevance_score</strong> (<code>torch.FloatTensor</code> of shape <code>(searcher_beam_size,)</code>, <em>optional</em>) &#x2014;
Relevance score, which must be specified if you want to compute the marginal log loss.`,name:"relevance_score"},{anchor:"transformers.RealmReader.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.RealmReader.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"},{anchor:"transformers.RealmReader.forward.has_answers",description:`<strong>has_answers</strong> (<code>torch.BoolTensor</code> of shape <code>(searcher_beam_size,)</code>, <em>optional</em>) &#x2014;
Whether or not the evidence block has answer(s).`,name:"has_answers"}],returnDescription:`
<p>A <code>transformers.models.realm.modeling_realm.RealmReaderOutput</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>start_positions</code>, <code>end_positions</code>, <code>has_answers</code> are provided) \u2014 Total loss.</p>
</li>
<li>
<p><strong>retriever_loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>start_positions</code>, <code>end_positions</code>, <code>has_answers</code> are provided) \u2014 Retriever loss.</p>
</li>
<li>
<p><strong>reader_loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>start_positions</code>, <code>end_positions</code>, <code>has_answers</code> are provided) \u2014 Reader loss.</p>
</li>
<li>
<p><strong>retriever_correct</strong> (<code>torch.BoolTensor</code> of shape <code>(config.searcher_beam_size,)</code>, <em>optional</em>) \u2014 Whether or not an evidence block contains answer.</p>
</li>
<li>
<p><strong>reader_correct</strong> (<code>torch.BoolTensor</code> of shape <code>(config.reader_beam_size, num_candidates)</code>, <em>optional</em>) \u2014 Whether or not a span candidate contains answer.</p>
</li>
<li>
<p><strong>block_idx</strong> (<code>torch.LongTensor</code> of shape <code>()</code>) \u2014 The index of the retrieved evidence block in which the predicted answer is most likely.</p>
</li>
<li>
<p><strong>candidate</strong> (<code>torch.LongTensor</code> of shape <code>()</code>) \u2014 The index of the retrieved span candidates in which the predicted answer is most likely.</p>
</li>
<li>
<p><strong>start_pos</strong> (<code>torch.IntTensor</code> of shape <code>()</code>) \u2014 Predicted answer starting position in <em>RealmReader</em>\u2019s inputs.
*<em> (<code>torch.IntTensor</code> of shape <code>()</code>) \u2014 Predicted answer ending position in </em>RealmReader*\u2018s inputs.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),We=new vr({props:{$$slots:{default:[Hm]},$$scope:{ctx:N}}}),to=new J({}),oo=new z({props:{name:"class transformers.RealmForOpenQA",anchor:"transformers.RealmForOpenQA",parameters:[{name:"config",val:""},{name:"retriever",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/modeling_realm.py#L1722",parametersDescription:[{anchor:"transformers.RealmForOpenQA.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.17.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ro=new z({props:{name:"forward",anchor:"transformers.RealmForOpenQA.forward",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"answer_ids",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/realm/modeling_realm.py#L1745",parametersDescription:[{anchor:"transformers.RealmForOpenQA.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(1, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/v4.17.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.17.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmForOpenQA.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(1, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmForOpenQA.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(1, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token (should not be used in this model by design).</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmForOpenQA.forward.answer_ids",description:`<strong>answer_ids</strong> (<code>list</code> of shape <code>(num_answers, answer_length)</code>, <em>optional</em>) &#x2014;
Answer ids for computing the marginal log-likelihood loss. Indices should be in <code>[-1, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-1</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"answer_ids"},{anchor:"transformers.RealmForOpenQA.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.17.0/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <code>transformers.models.realm.modeling_realm.RealmForOpenQAOutput</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li><strong>reader_output</strong> (<code>dict</code>) \u2014 Reader output.</li>
<li><strong>predicted_answer_ids</strong> (<code>torch.LongTensor</code> of shape <code>(answer_sequence_length)</code>) \u2014 Predicted answer ids.</li>
</ul>
`}}),Qe=new vr({props:{$$slots:{default:[Vm]},$$scope:{ctx:N}}}),ao=new Ve({props:{code:`import torch
from transformers import RealmForOpenQA, RealmRetriever, RealmTokenizer

retriever = RealmRetriever.from_pretrained("google/realm-orqa-nq-openqa")
tokenizer = RealmTokenizer.from_pretrained("google/realm-orqa-nq-openqa")
model = RealmForOpenQA.from_pretrained("google/realm-orqa-nq-openqa", retriever=retriever)

question = "Who is the pioneer in modern computer science?"
question_ids = tokenizer([question], return_tensors="pt")
answer_ids = tokenizer(
    ["alan mathison turing"],
    add_special_tokens=False,
    return_token_type_ids=False,
    return_attention_mask=False,
).input_ids

reader_output, predicted_answer_ids = model(**question_ids, answer_ids=answer_ids, return_dict=False)
predicted_answer = tokenizer.decode(predicted_answer_ids)
loss = reader_output.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmForOpenQA, RealmRetriever, RealmTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>retriever = RealmRetriever.from_pretrained(<span class="hljs-string">&quot;google/realm-orqa-nq-openqa&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-orqa-nq-openqa&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmForOpenQA.from_pretrained(<span class="hljs-string">&quot;google/realm-orqa-nq-openqa&quot;</span>, retriever=retriever)

<span class="hljs-meta">&gt;&gt;&gt; </span>question = <span class="hljs-string">&quot;Who is the pioneer in modern computer science?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>question_ids = tokenizer([question], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>answer_ids = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;alan mathison turing&quot;</span>],
<span class="hljs-meta">... </span>    add_special_tokens=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    return_token_type_ids=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    return_attention_mask=<span class="hljs-literal">False</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>reader_output, predicted_answer_ids = model(**question_ids, answer_ids=answer_ids, return_dict=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_answer = tokenizer.decode(predicted_answer_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = reader_output.loss`}}),{c(){h=n("meta"),$=l(),w=n("h1"),T=n("a"),y=n("span"),f(R.$$.fragment),b=l(),q=n("span"),ka=s("REALM"),kr=l(),se=n("h2"),ye=n("a"),tn=n("span"),f(Ue.$$.fragment),wa=l(),on=n("span"),ba=s("Overview"),wr=l(),Ee=n("p"),Ra=s("The REALM model was proposed in "),Xe=n("a"),Ta=s("REALM: Retrieval-Augmented Language Model Pre-Training"),$a=s(` by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang. It\u2019s a
retrieval-augmented language model that firstly retrieves documents from a textual knowledge corpus and then
utilizes retrieved documents to process question answering tasks.`),br=l(),lo=n("p"),ya=s("The abstract from the paper is the following:"),Rr=l(),co=n("p"),nn=n("em"),Ea=s(`Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks
such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network,
requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we
augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend
over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the
first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language
modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We
demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the
challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both
explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous
methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as
interpretability and modularity.`),Tr=l(),Z=n("p"),za=s("This model was contributed by "),Ge=n("a"),qa=s("qqaatw"),xa=s(`. The original code can be found
`),Je=n("a"),Aa=s("here"),Pa=s("."),$r=l(),ie=n("h2"),ze=n("a"),rn=n("span"),f(Ze.$$.fragment),ja=l(),an=n("span"),La=s("RealmConfig"),yr=l(),x=n("div"),f(Ye.$$.fragment),Ma=l(),sn=n("p"),Sa=s("This is the configuration class to store the configuration of"),Fa=l(),L=n("ol"),ln=n("li"),mo=n("a"),Ca=s("RealmEmbedder"),Ia=l(),dn=n("li"),ho=n("a"),Da=s("RealmScorer"),Na=l(),cn=n("li"),po=n("a"),Oa=s("RealmKnowledgeAugEncoder"),Wa=l(),mn=n("li"),fo=n("a"),Ka=s("RealmRetriever"),Ba=l(),hn=n("li"),uo=n("a"),Qa=s("RealmReader"),Ha=l(),pn=n("li"),go=n("a"),Va=s("RealmForOpenQA"),Ua=l(),et=n("p"),Xa=s(`It is used to instantiate an REALM model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the REALM
`),tt=n("a"),Ga=s("realm-cc-news-pretrained"),Ja=s(" architecture."),Za=l(),le=n("p"),Ya=s("Configuration objects inherit from "),_o=n("a"),es=s("PretrainedConfig"),ts=s(` and can be used to control the model outputs. Read the
documentation from `),vo=n("a"),os=s("PretrainedConfig"),ns=s(" for more information."),rs=l(),fn=n("p"),as=s("Example:"),ss=l(),f(ot.$$.fragment),Er=l(),de=n("h2"),qe=n("a"),un=n("span"),f(nt.$$.fragment),is=l(),gn=n("span"),ls=s("RealmTokenizer"),zr=l(),E=n("div"),f(rt.$$.fragment),ds=l(),_n=n("p"),cs=s("Construct a REALM tokenizer."),ms=l(),xe=n("p"),ko=n("a"),hs=s("RealmTokenizer"),ps=s(" is identical to "),wo=n("a"),fs=s("BertTokenizer"),us=s(` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),gs=l(),at=n("p"),_s=s("This tokenizer inherits from "),bo=n("a"),vs=s("PreTrainedTokenizer"),ks=s(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),ws=l(),Y=n("div"),f(st.$$.fragment),bs=l(),vn=n("p"),Rs=s(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A REALM sequence has the following format:`),Ts=l(),it=n("ul"),Ro=n("li"),$s=s("single sequence: "),kn=n("code"),ys=s("[CLS] X [SEP]"),Es=l(),To=n("li"),zs=s("pair of sequences: "),wn=n("code"),qs=s("[CLS] A [SEP] B [SEP]"),xs=l(),Ae=n("div"),f(lt.$$.fragment),As=l(),dt=n("p"),Ps=s(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),bn=n("code"),js=s("prepare_for_model"),Ls=s(" method."),Ms=l(),O=n("div"),f(ct.$$.fragment),Ss=l(),Rn=n("p"),Fs=s(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A REALM sequence
pair mask has the following format:`),Cs=l(),f(mt.$$.fragment),Is=l(),ce=n("p"),Ds=s("If "),Tn=n("code"),Ns=s("token_ids_1"),Os=s(" is "),$n=n("code"),Ws=s("None"),Ks=s(", this method only returns the first portion of the mask (0s)."),Bs=l(),$o=n("div"),f(ht.$$.fragment),Qs=l(),P=n("div"),f(pt.$$.fragment),Hs=l(),ft=n("p"),Vs=s("Encode a batch of text or text pair. This method is similar to regular "),yn=n("strong"),Us=s("call"),Xs=s(` method but has the following
differences:`),Gs=l(),me=n("ol"),En=n("li"),Js=s("Handle additional num_candidate axis. (batch_size, num_candidates, text)"),Zs=l(),ut=n("li"),Ys=s("Always pad the sequences to "),zn=n("em"),ei=s("max_length"),ti=s("."),oi=l(),gt=n("li"),ni=s("Must specify "),qn=n("em"),ri=s("max_length"),ai=s(" in order to stack packs of candidates into a batch."),si=l(),_t=n("ul"),yo=n("li"),ii=s("single sequence: "),xn=n("code"),li=s("[CLS] X [SEP]"),di=l(),Eo=n("li"),ci=s("pair of sequences: "),An=n("code"),mi=s("[CLS] A [SEP] B [SEP]"),hi=l(),Pn=n("p"),pi=s("Example:"),fi=l(),f(vt.$$.fragment),qr=l(),he=n("h2"),Pe=n("a"),jn=n("span"),f(kt.$$.fragment),ui=l(),Ln=n("span"),gi=s("RealmTokenizerFast"),xr=l(),M=n("div"),f(wt.$$.fragment),_i=l(),bt=n("p"),vi=s("Construct a \u201Cfast\u201D REALM tokenizer (backed by HuggingFace\u2019s "),Mn=n("em"),ki=s("tokenizers"),wi=s(" library). Based on WordPiece."),bi=l(),je=n("p"),zo=n("a"),Ri=s("RealmTokenizerFast"),Ti=s(" is identical to "),qo=n("a"),$i=s("BertTokenizerFast"),yi=s(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Ei=l(),Rt=n("p"),zi=s("This tokenizer inherits from "),xo=n("a"),qi=s("PreTrainedTokenizerFast"),xi=s(` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),Ai=l(),j=n("div"),f(Tt.$$.fragment),Pi=l(),$t=n("p"),ji=s("Encode a batch of text or text pair. This method is similar to regular "),Sn=n("strong"),Li=s("call"),Mi=s(` method but has the following
differences:`),Si=l(),pe=n("ol"),Fn=n("li"),Fi=s("Handle additional num_candidate axis. (batch_size, num_candidates, text)"),Ci=l(),yt=n("li"),Ii=s("Always pad the sequences to "),Cn=n("em"),Di=s("max_length"),Ni=s("."),Oi=l(),Et=n("li"),Wi=s("Must specify "),In=n("em"),Ki=s("max_length"),Bi=s(" in order to stack packs of candidates into a batch."),Qi=l(),zt=n("ul"),Ao=n("li"),Hi=s("single sequence: "),Dn=n("code"),Vi=s("[CLS] X [SEP]"),Ui=l(),Po=n("li"),Xi=s("pair of sequences: "),Nn=n("code"),Gi=s("[CLS] A [SEP] B [SEP]"),Ji=l(),On=n("p"),Zi=s("Example:"),Yi=l(),f(qt.$$.fragment),Ar=l(),fe=n("h2"),Le=n("a"),Wn=n("span"),f(xt.$$.fragment),el=l(),Kn=n("span"),tl=s("RealmRetriever"),Pr=l(),Q=n("div"),f(At.$$.fragment),ol=l(),Bn=n("p"),nl=s(`The retriever of REALM outputting the retrieved evidence block and whether the block has answers as well as answer
positions.\u201D`),rl=l(),Me=n("div"),f(Pt.$$.fragment),al=l(),Qn=n("p"),sl=s("check if retrieved_blocks has answers."),jr=l(),ue=n("h2"),Se=n("a"),Hn=n("span"),f(jt.$$.fragment),il=l(),Vn=n("span"),ll=s("RealmEmbedder"),Lr=l(),H=n("div"),f(Lt.$$.fragment),dl=l(),Mt=n("p"),cl=s(`The embedder of REALM outputting projected score that will be used to calculate relevance score.
This model is a PyTorch `),St=n("a"),ml=s("torch.nn.Module"),hl=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),pl=l(),S=n("div"),f(Ft.$$.fragment),fl=l(),ge=n("p"),ul=s("The "),jo=n("a"),gl=s("RealmEmbedder"),_l=s(" forward method, overrides the "),Un=n("code"),vl=s("__call__"),kl=s(" special method."),wl=l(),f(Fe.$$.fragment),bl=l(),Xn=n("p"),Rl=s("Example:"),Tl=l(),f(Ct.$$.fragment),Mr=l(),_e=n("h2"),Ce=n("a"),Gn=n("span"),f(It.$$.fragment),$l=l(),Jn=n("span"),yl=s("RealmScorer"),Sr=l(),V=n("div"),f(Dt.$$.fragment),El=l(),Nt=n("p"),zl=s(`The scorer of REALM outputting relevance scores representing the score of document candidates (before softmax).
This model is a PyTorch `),Ot=n("a"),ql=s("torch.nn.Module"),xl=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Al=l(),F=n("div"),f(Wt.$$.fragment),Pl=l(),ve=n("p"),jl=s("The "),Lo=n("a"),Ll=s("RealmScorer"),Ml=s(" forward method, overrides the "),Zn=n("code"),Sl=s("__call__"),Fl=s(" special method."),Cl=l(),f(Ie.$$.fragment),Il=l(),Yn=n("p"),Dl=s("Example:"),Nl=l(),f(Kt.$$.fragment),Fr=l(),ke=n("h2"),De=n("a"),er=n("span"),f(Bt.$$.fragment),Ol=l(),tr=n("span"),Wl=s("RealmKnowledgeAugEncoder"),Cr=l(),U=n("div"),f(Qt.$$.fragment),Kl=l(),Ht=n("p"),Bl=s(`The knowledge-augmented encoder of REALM outputting masked language model logits and marginal log-likelihood loss.
This model is a PyTorch `),Vt=n("a"),Ql=s("torch.nn.Module"),Hl=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Vl=l(),C=n("div"),f(Ut.$$.fragment),Ul=l(),we=n("p"),Xl=s("The "),Mo=n("a"),Gl=s("RealmKnowledgeAugEncoder"),Jl=s(" forward method, overrides the "),or=n("code"),Zl=s("__call__"),Yl=s(" special method."),ed=l(),f(Ne.$$.fragment),td=l(),nr=n("p"),od=s("Example:"),nd=l(),f(Xt.$$.fragment),Ir=l(),be=n("h2"),Oe=n("a"),rr=n("span"),f(Gt.$$.fragment),rd=l(),ar=n("span"),ad=s("RealmReader"),Dr=l(),X=n("div"),f(Jt.$$.fragment),sd=l(),Zt=n("p"),id=s(`The reader of REALM.
This model is a PyTorch `),Yt=n("a"),ld=s("torch.nn.Module"),dd=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),cd=l(),ee=n("div"),f(eo.$$.fragment),md=l(),Re=n("p"),hd=s("The "),So=n("a"),pd=s("RealmReader"),fd=s(" forward method, overrides the "),sr=n("code"),ud=s("__call__"),gd=s(" special method."),_d=l(),f(We.$$.fragment),Nr=l(),Te=n("h2"),Ke=n("a"),ir=n("span"),f(to.$$.fragment),vd=l(),lr=n("span"),kd=s("RealmForOpenQA"),Or=l(),G=n("div"),f(oo.$$.fragment),wd=l(),Be=n("p"),dr=n("code"),bd=s("RealmForOpenQA"),Rd=s(` for end-to-end open domain question answering.
This model is a PyTorch `),no=n("a"),Td=s("torch.nn.Module"),$d=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),yd=l(),I=n("div"),f(ro.$$.fragment),Ed=l(),$e=n("p"),zd=s("The "),Fo=n("a"),qd=s("RealmForOpenQA"),xd=s(" forward method, overrides the "),cr=n("code"),Ad=s("__call__"),Pd=s(" special method."),jd=l(),f(Qe.$$.fragment),Ld=l(),mr=n("p"),Md=s("Example:"),Sd=l(),f(ao.$$.fragment),this.h()},l(t){const m=Wm('[data-svelte="svelte-1phssyn"]',document.head);h=r(m,"META",{name:!0,content:!0}),m.forEach(o),$=d(t),w=r(t,"H1",{class:!0});var so=a(w);T=r(so,"A",{id:!0,class:!0,href:!0});var hr=a(T);y=r(hr,"SPAN",{});var pr=a(y);u(R.$$.fragment,pr),pr.forEach(o),hr.forEach(o),b=d(so),q=r(so,"SPAN",{});var fr=a(q);ka=i(fr,"REALM"),fr.forEach(o),so.forEach(o),kr=d(t),se=r(t,"H2",{class:!0});var io=a(se);ye=r(io,"A",{id:!0,class:!0,href:!0});var Wd=a(ye);tn=r(Wd,"SPAN",{});var Kd=a(tn);u(Ue.$$.fragment,Kd),Kd.forEach(o),Wd.forEach(o),wa=d(io),on=r(io,"SPAN",{});var Bd=a(on);ba=i(Bd,"Overview"),Bd.forEach(o),io.forEach(o),wr=d(t),Ee=r(t,"P",{});var Kr=a(Ee);Ra=i(Kr,"The REALM model was proposed in "),Xe=r(Kr,"A",{href:!0,rel:!0});var Qd=a(Xe);Ta=i(Qd,"REALM: Retrieval-Augmented Language Model Pre-Training"),Qd.forEach(o),$a=i(Kr,` by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang. It\u2019s a
retrieval-augmented language model that firstly retrieves documents from a textual knowledge corpus and then
utilizes retrieved documents to process question answering tasks.`),Kr.forEach(o),br=d(t),lo=r(t,"P",{});var Hd=a(lo);ya=i(Hd,"The abstract from the paper is the following:"),Hd.forEach(o),Rr=d(t),co=r(t,"P",{});var Vd=a(co);nn=r(Vd,"EM",{});var Ud=a(nn);Ea=i(Ud,`Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks
such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network,
requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we
augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend
over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the
first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language
modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We
demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the
challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both
explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous
methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as
interpretability and modularity.`),Ud.forEach(o),Vd.forEach(o),Tr=d(t),Z=r(t,"P",{});var Co=a(Z);za=i(Co,"This model was contributed by "),Ge=r(Co,"A",{href:!0,rel:!0});var Xd=a(Ge);qa=i(Xd,"qqaatw"),Xd.forEach(o),xa=i(Co,`. The original code can be found
`),Je=r(Co,"A",{href:!0,rel:!0});var Gd=a(Je);Aa=i(Gd,"here"),Gd.forEach(o),Pa=i(Co,"."),Co.forEach(o),$r=d(t),ie=r(t,"H2",{class:!0});var Br=a(ie);ze=r(Br,"A",{id:!0,class:!0,href:!0});var Jd=a(ze);rn=r(Jd,"SPAN",{});var Zd=a(rn);u(Ze.$$.fragment,Zd),Zd.forEach(o),Jd.forEach(o),ja=d(Br),an=r(Br,"SPAN",{});var Yd=a(an);La=i(Yd,"RealmConfig"),Yd.forEach(o),Br.forEach(o),yr=d(t),x=r(t,"DIV",{class:!0});var D=a(x);u(Ye.$$.fragment,D),Ma=d(D),sn=r(D,"P",{});var ec=a(sn);Sa=i(ec,"This is the configuration class to store the configuration of"),ec.forEach(o),Fa=d(D),L=r(D,"OL",{});var W=a(L);ln=r(W,"LI",{});var tc=a(ln);mo=r(tc,"A",{href:!0});var oc=a(mo);Ca=i(oc,"RealmEmbedder"),oc.forEach(o),tc.forEach(o),Ia=d(W),dn=r(W,"LI",{});var nc=a(dn);ho=r(nc,"A",{href:!0});var rc=a(ho);Da=i(rc,"RealmScorer"),rc.forEach(o),nc.forEach(o),Na=d(W),cn=r(W,"LI",{});var ac=a(cn);po=r(ac,"A",{href:!0});var sc=a(po);Oa=i(sc,"RealmKnowledgeAugEncoder"),sc.forEach(o),ac.forEach(o),Wa=d(W),mn=r(W,"LI",{});var ic=a(mn);fo=r(ic,"A",{href:!0});var lc=a(fo);Ka=i(lc,"RealmRetriever"),lc.forEach(o),ic.forEach(o),Ba=d(W),hn=r(W,"LI",{});var dc=a(hn);uo=r(dc,"A",{href:!0});var cc=a(uo);Qa=i(cc,"RealmReader"),cc.forEach(o),dc.forEach(o),Ha=d(W),pn=r(W,"LI",{});var mc=a(pn);go=r(mc,"A",{href:!0});var hc=a(go);Va=i(hc,"RealmForOpenQA"),hc.forEach(o),mc.forEach(o),W.forEach(o),Ua=d(D),et=r(D,"P",{});var Qr=a(et);Xa=i(Qr,`It is used to instantiate an REALM model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the REALM
`),tt=r(Qr,"A",{href:!0,rel:!0});var pc=a(tt);Ga=i(pc,"realm-cc-news-pretrained"),pc.forEach(o),Ja=i(Qr," architecture."),Qr.forEach(o),Za=d(D),le=r(D,"P",{});var Io=a(le);Ya=i(Io,"Configuration objects inherit from "),_o=r(Io,"A",{href:!0});var fc=a(_o);es=i(fc,"PretrainedConfig"),fc.forEach(o),ts=i(Io,` and can be used to control the model outputs. Read the
documentation from `),vo=r(Io,"A",{href:!0});var uc=a(vo);os=i(uc,"PretrainedConfig"),uc.forEach(o),ns=i(Io," for more information."),Io.forEach(o),rs=d(D),fn=r(D,"P",{});var gc=a(fn);as=i(gc,"Example:"),gc.forEach(o),ss=d(D),u(ot.$$.fragment,D),D.forEach(o),Er=d(t),de=r(t,"H2",{class:!0});var Hr=a(de);qe=r(Hr,"A",{id:!0,class:!0,href:!0});var _c=a(qe);un=r(_c,"SPAN",{});var vc=a(un);u(nt.$$.fragment,vc),vc.forEach(o),_c.forEach(o),is=d(Hr),gn=r(Hr,"SPAN",{});var kc=a(gn);ls=i(kc,"RealmTokenizer"),kc.forEach(o),Hr.forEach(o),zr=d(t),E=r(t,"DIV",{class:!0});var A=a(E);u(rt.$$.fragment,A),ds=d(A),_n=r(A,"P",{});var wc=a(_n);cs=i(wc,"Construct a REALM tokenizer."),wc.forEach(o),ms=d(A),xe=r(A,"P",{});var ur=a(xe);ko=r(ur,"A",{href:!0});var bc=a(ko);hs=i(bc,"RealmTokenizer"),bc.forEach(o),ps=i(ur," is identical to "),wo=r(ur,"A",{href:!0});var Rc=a(wo);fs=i(Rc,"BertTokenizer"),Rc.forEach(o),us=i(ur,` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),ur.forEach(o),gs=d(A),at=r(A,"P",{});var Vr=a(at);_s=i(Vr,"This tokenizer inherits from "),bo=r(Vr,"A",{href:!0});var Tc=a(bo);vs=i(Tc,"PreTrainedTokenizer"),Tc.forEach(o),ks=i(Vr,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Vr.forEach(o),ws=d(A),Y=r(A,"DIV",{class:!0});var Do=a(Y);u(st.$$.fragment,Do),bs=d(Do),vn=r(Do,"P",{});var $c=a(vn);Rs=i($c,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A REALM sequence has the following format:`),$c.forEach(o),Ts=d(Do),it=r(Do,"UL",{});var Ur=a(it);Ro=r(Ur,"LI",{});var Fd=a(Ro);$s=i(Fd,"single sequence: "),kn=r(Fd,"CODE",{});var yc=a(kn);ys=i(yc,"[CLS] X [SEP]"),yc.forEach(o),Fd.forEach(o),Es=d(Ur),To=r(Ur,"LI",{});var Cd=a(To);zs=i(Cd,"pair of sequences: "),wn=r(Cd,"CODE",{});var Ec=a(wn);qs=i(Ec,"[CLS] A [SEP] B [SEP]"),Ec.forEach(o),Cd.forEach(o),Ur.forEach(o),Do.forEach(o),xs=d(A),Ae=r(A,"DIV",{class:!0});var Xr=a(Ae);u(lt.$$.fragment,Xr),As=d(Xr),dt=r(Xr,"P",{});var Gr=a(dt);Ps=i(Gr,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),bn=r(Gr,"CODE",{});var zc=a(bn);js=i(zc,"prepare_for_model"),zc.forEach(o),Ls=i(Gr," method."),Gr.forEach(o),Xr.forEach(o),Ms=d(A),O=r(A,"DIV",{class:!0});var He=a(O);u(ct.$$.fragment,He),Ss=d(He),Rn=r(He,"P",{});var qc=a(Rn);Fs=i(qc,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A REALM sequence
pair mask has the following format:`),qc.forEach(o),Cs=d(He),u(mt.$$.fragment,He),Is=d(He),ce=r(He,"P",{});var No=a(ce);Ds=i(No,"If "),Tn=r(No,"CODE",{});var xc=a(Tn);Ns=i(xc,"token_ids_1"),xc.forEach(o),Os=i(No," is "),$n=r(No,"CODE",{});var Ac=a($n);Ws=i(Ac,"None"),Ac.forEach(o),Ks=i(No,", this method only returns the first portion of the mask (0s)."),No.forEach(o),He.forEach(o),Bs=d(A),$o=r(A,"DIV",{class:!0});var Pc=a($o);u(ht.$$.fragment,Pc),Pc.forEach(o),Qs=d(A),P=r(A,"DIV",{class:!0});var K=a(P);u(pt.$$.fragment,K),Hs=d(K),ft=r(K,"P",{});var Jr=a(ft);Vs=i(Jr,"Encode a batch of text or text pair. This method is similar to regular "),yn=r(Jr,"STRONG",{});var jc=a(yn);Us=i(jc,"call"),jc.forEach(o),Xs=i(Jr,` method but has the following
differences:`),Jr.forEach(o),Gs=d(K),me=r(K,"OL",{});var Oo=a(me);En=r(Oo,"LI",{});var Lc=a(En);Js=i(Lc,"Handle additional num_candidate axis. (batch_size, num_candidates, text)"),Lc.forEach(o),Zs=d(Oo),ut=r(Oo,"LI",{});var Zr=a(ut);Ys=i(Zr,"Always pad the sequences to "),zn=r(Zr,"EM",{});var Mc=a(zn);ei=i(Mc,"max_length"),Mc.forEach(o),ti=i(Zr,"."),Zr.forEach(o),oi=d(Oo),gt=r(Oo,"LI",{});var Yr=a(gt);ni=i(Yr,"Must specify "),qn=r(Yr,"EM",{});var Sc=a(qn);ri=i(Sc,"max_length"),Sc.forEach(o),ai=i(Yr," in order to stack packs of candidates into a batch."),Yr.forEach(o),Oo.forEach(o),si=d(K),_t=r(K,"UL",{});var ea=a(_t);yo=r(ea,"LI",{});var Id=a(yo);ii=i(Id,"single sequence: "),xn=r(Id,"CODE",{});var Fc=a(xn);li=i(Fc,"[CLS] X [SEP]"),Fc.forEach(o),Id.forEach(o),di=d(ea),Eo=r(ea,"LI",{});var Dd=a(Eo);ci=i(Dd,"pair of sequences: "),An=r(Dd,"CODE",{});var Cc=a(An);mi=i(Cc,"[CLS] A [SEP] B [SEP]"),Cc.forEach(o),Dd.forEach(o),ea.forEach(o),hi=d(K),Pn=r(K,"P",{});var Ic=a(Pn);pi=i(Ic,"Example:"),Ic.forEach(o),fi=d(K),u(vt.$$.fragment,K),K.forEach(o),A.forEach(o),qr=d(t),he=r(t,"H2",{class:!0});var ta=a(he);Pe=r(ta,"A",{id:!0,class:!0,href:!0});var Dc=a(Pe);jn=r(Dc,"SPAN",{});var Nc=a(jn);u(kt.$$.fragment,Nc),Nc.forEach(o),Dc.forEach(o),ui=d(ta),Ln=r(ta,"SPAN",{});var Oc=a(Ln);gi=i(Oc,"RealmTokenizerFast"),Oc.forEach(o),ta.forEach(o),xr=d(t),M=r(t,"DIV",{class:!0});var te=a(M);u(wt.$$.fragment,te),_i=d(te),bt=r(te,"P",{});var oa=a(bt);vi=i(oa,"Construct a \u201Cfast\u201D REALM tokenizer (backed by HuggingFace\u2019s "),Mn=r(oa,"EM",{});var Wc=a(Mn);ki=i(Wc,"tokenizers"),Wc.forEach(o),wi=i(oa," library). Based on WordPiece."),oa.forEach(o),bi=d(te),je=r(te,"P",{});var gr=a(je);zo=r(gr,"A",{href:!0});var Kc=a(zo);Ri=i(Kc,"RealmTokenizerFast"),Kc.forEach(o),Ti=i(gr," is identical to "),qo=r(gr,"A",{href:!0});var Bc=a(qo);$i=i(Bc,"BertTokenizerFast"),Bc.forEach(o),yi=i(gr,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),gr.forEach(o),Ei=d(te),Rt=r(te,"P",{});var na=a(Rt);zi=i(na,"This tokenizer inherits from "),xo=r(na,"A",{href:!0});var Qc=a(xo);qi=i(Qc,"PreTrainedTokenizerFast"),Qc.forEach(o),xi=i(na,` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),na.forEach(o),Ai=d(te),j=r(te,"DIV",{class:!0});var B=a(j);u(Tt.$$.fragment,B),Pi=d(B),$t=r(B,"P",{});var ra=a($t);ji=i(ra,"Encode a batch of text or text pair. This method is similar to regular "),Sn=r(ra,"STRONG",{});var Hc=a(Sn);Li=i(Hc,"call"),Hc.forEach(o),Mi=i(ra,` method but has the following
differences:`),ra.forEach(o),Si=d(B),pe=r(B,"OL",{});var Wo=a(pe);Fn=r(Wo,"LI",{});var Vc=a(Fn);Fi=i(Vc,"Handle additional num_candidate axis. (batch_size, num_candidates, text)"),Vc.forEach(o),Ci=d(Wo),yt=r(Wo,"LI",{});var aa=a(yt);Ii=i(aa,"Always pad the sequences to "),Cn=r(aa,"EM",{});var Uc=a(Cn);Di=i(Uc,"max_length"),Uc.forEach(o),Ni=i(aa,"."),aa.forEach(o),Oi=d(Wo),Et=r(Wo,"LI",{});var sa=a(Et);Wi=i(sa,"Must specify "),In=r(sa,"EM",{});var Xc=a(In);Ki=i(Xc,"max_length"),Xc.forEach(o),Bi=i(sa," in order to stack packs of candidates into a batch."),sa.forEach(o),Wo.forEach(o),Qi=d(B),zt=r(B,"UL",{});var ia=a(zt);Ao=r(ia,"LI",{});var Nd=a(Ao);Hi=i(Nd,"single sequence: "),Dn=r(Nd,"CODE",{});var Gc=a(Dn);Vi=i(Gc,"[CLS] X [SEP]"),Gc.forEach(o),Nd.forEach(o),Ui=d(ia),Po=r(ia,"LI",{});var Od=a(Po);Xi=i(Od,"pair of sequences: "),Nn=r(Od,"CODE",{});var Jc=a(Nn);Gi=i(Jc,"[CLS] A [SEP] B [SEP]"),Jc.forEach(o),Od.forEach(o),ia.forEach(o),Ji=d(B),On=r(B,"P",{});var Zc=a(On);Zi=i(Zc,"Example:"),Zc.forEach(o),Yi=d(B),u(qt.$$.fragment,B),B.forEach(o),te.forEach(o),Ar=d(t),fe=r(t,"H2",{class:!0});var la=a(fe);Le=r(la,"A",{id:!0,class:!0,href:!0});var Yc=a(Le);Wn=r(Yc,"SPAN",{});var em=a(Wn);u(xt.$$.fragment,em),em.forEach(o),Yc.forEach(o),el=d(la),Kn=r(la,"SPAN",{});var tm=a(Kn);tl=i(tm,"RealmRetriever"),tm.forEach(o),la.forEach(o),Pr=d(t),Q=r(t,"DIV",{class:!0});var Ko=a(Q);u(At.$$.fragment,Ko),ol=d(Ko),Bn=r(Ko,"P",{});var om=a(Bn);nl=i(om,`The retriever of REALM outputting the retrieved evidence block and whether the block has answers as well as answer
positions.\u201D`),om.forEach(o),rl=d(Ko),Me=r(Ko,"DIV",{class:!0});var da=a(Me);u(Pt.$$.fragment,da),al=d(da),Qn=r(da,"P",{});var nm=a(Qn);sl=i(nm,"check if retrieved_blocks has answers."),nm.forEach(o),da.forEach(o),Ko.forEach(o),jr=d(t),ue=r(t,"H2",{class:!0});var ca=a(ue);Se=r(ca,"A",{id:!0,class:!0,href:!0});var rm=a(Se);Hn=r(rm,"SPAN",{});var am=a(Hn);u(jt.$$.fragment,am),am.forEach(o),rm.forEach(o),il=d(ca),Vn=r(ca,"SPAN",{});var sm=a(Vn);ll=i(sm,"RealmEmbedder"),sm.forEach(o),ca.forEach(o),Lr=d(t),H=r(t,"DIV",{class:!0});var Bo=a(H);u(Lt.$$.fragment,Bo),dl=d(Bo),Mt=r(Bo,"P",{});var ma=a(Mt);cl=i(ma,`The embedder of REALM outputting projected score that will be used to calculate relevance score.
This model is a PyTorch `),St=r(ma,"A",{href:!0,rel:!0});var im=a(St);ml=i(im,"torch.nn.Module"),im.forEach(o),hl=i(ma,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ma.forEach(o),pl=d(Bo),S=r(Bo,"DIV",{class:!0});var oe=a(S);u(Ft.$$.fragment,oe),fl=d(oe),ge=r(oe,"P",{});var Qo=a(ge);ul=i(Qo,"The "),jo=r(Qo,"A",{href:!0});var lm=a(jo);gl=i(lm,"RealmEmbedder"),lm.forEach(o),_l=i(Qo," forward method, overrides the "),Un=r(Qo,"CODE",{});var dm=a(Un);vl=i(dm,"__call__"),dm.forEach(o),kl=i(Qo," special method."),Qo.forEach(o),wl=d(oe),u(Fe.$$.fragment,oe),bl=d(oe),Xn=r(oe,"P",{});var cm=a(Xn);Rl=i(cm,"Example:"),cm.forEach(o),Tl=d(oe),u(Ct.$$.fragment,oe),oe.forEach(o),Bo.forEach(o),Mr=d(t),_e=r(t,"H2",{class:!0});var ha=a(_e);Ce=r(ha,"A",{id:!0,class:!0,href:!0});var mm=a(Ce);Gn=r(mm,"SPAN",{});var hm=a(Gn);u(It.$$.fragment,hm),hm.forEach(o),mm.forEach(o),$l=d(ha),Jn=r(ha,"SPAN",{});var pm=a(Jn);yl=i(pm,"RealmScorer"),pm.forEach(o),ha.forEach(o),Sr=d(t),V=r(t,"DIV",{class:!0});var Ho=a(V);u(Dt.$$.fragment,Ho),El=d(Ho),Nt=r(Ho,"P",{});var pa=a(Nt);zl=i(pa,`The scorer of REALM outputting relevance scores representing the score of document candidates (before softmax).
This model is a PyTorch `),Ot=r(pa,"A",{href:!0,rel:!0});var fm=a(Ot);ql=i(fm,"torch.nn.Module"),fm.forEach(o),xl=i(pa,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),pa.forEach(o),Al=d(Ho),F=r(Ho,"DIV",{class:!0});var ne=a(F);u(Wt.$$.fragment,ne),Pl=d(ne),ve=r(ne,"P",{});var Vo=a(ve);jl=i(Vo,"The "),Lo=r(Vo,"A",{href:!0});var um=a(Lo);Ll=i(um,"RealmScorer"),um.forEach(o),Ml=i(Vo," forward method, overrides the "),Zn=r(Vo,"CODE",{});var gm=a(Zn);Sl=i(gm,"__call__"),gm.forEach(o),Fl=i(Vo," special method."),Vo.forEach(o),Cl=d(ne),u(Ie.$$.fragment,ne),Il=d(ne),Yn=r(ne,"P",{});var _m=a(Yn);Dl=i(_m,"Example:"),_m.forEach(o),Nl=d(ne),u(Kt.$$.fragment,ne),ne.forEach(o),Ho.forEach(o),Fr=d(t),ke=r(t,"H2",{class:!0});var fa=a(ke);De=r(fa,"A",{id:!0,class:!0,href:!0});var vm=a(De);er=r(vm,"SPAN",{});var km=a(er);u(Bt.$$.fragment,km),km.forEach(o),vm.forEach(o),Ol=d(fa),tr=r(fa,"SPAN",{});var wm=a(tr);Wl=i(wm,"RealmKnowledgeAugEncoder"),wm.forEach(o),fa.forEach(o),Cr=d(t),U=r(t,"DIV",{class:!0});var Uo=a(U);u(Qt.$$.fragment,Uo),Kl=d(Uo),Ht=r(Uo,"P",{});var ua=a(Ht);Bl=i(ua,`The knowledge-augmented encoder of REALM outputting masked language model logits and marginal log-likelihood loss.
This model is a PyTorch `),Vt=r(ua,"A",{href:!0,rel:!0});var bm=a(Vt);Ql=i(bm,"torch.nn.Module"),bm.forEach(o),Hl=i(ua,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ua.forEach(o),Vl=d(Uo),C=r(Uo,"DIV",{class:!0});var re=a(C);u(Ut.$$.fragment,re),Ul=d(re),we=r(re,"P",{});var Xo=a(we);Xl=i(Xo,"The "),Mo=r(Xo,"A",{href:!0});var Rm=a(Mo);Gl=i(Rm,"RealmKnowledgeAugEncoder"),Rm.forEach(o),Jl=i(Xo," forward method, overrides the "),or=r(Xo,"CODE",{});var Tm=a(or);Zl=i(Tm,"__call__"),Tm.forEach(o),Yl=i(Xo," special method."),Xo.forEach(o),ed=d(re),u(Ne.$$.fragment,re),td=d(re),nr=r(re,"P",{});var $m=a(nr);od=i($m,"Example:"),$m.forEach(o),nd=d(re),u(Xt.$$.fragment,re),re.forEach(o),Uo.forEach(o),Ir=d(t),be=r(t,"H2",{class:!0});var ga=a(be);Oe=r(ga,"A",{id:!0,class:!0,href:!0});var ym=a(Oe);rr=r(ym,"SPAN",{});var Em=a(rr);u(Gt.$$.fragment,Em),Em.forEach(o),ym.forEach(o),rd=d(ga),ar=r(ga,"SPAN",{});var zm=a(ar);ad=i(zm,"RealmReader"),zm.forEach(o),ga.forEach(o),Dr=d(t),X=r(t,"DIV",{class:!0});var Go=a(X);u(Jt.$$.fragment,Go),sd=d(Go),Zt=r(Go,"P",{});var _a=a(Zt);id=i(_a,`The reader of REALM.
This model is a PyTorch `),Yt=r(_a,"A",{href:!0,rel:!0});var qm=a(Yt);ld=i(qm,"torch.nn.Module"),qm.forEach(o),dd=i(_a,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),_a.forEach(o),cd=d(Go),ee=r(Go,"DIV",{class:!0});var Jo=a(ee);u(eo.$$.fragment,Jo),md=d(Jo),Re=r(Jo,"P",{});var Zo=a(Re);hd=i(Zo,"The "),So=r(Zo,"A",{href:!0});var xm=a(So);pd=i(xm,"RealmReader"),xm.forEach(o),fd=i(Zo," forward method, overrides the "),sr=r(Zo,"CODE",{});var Am=a(sr);ud=i(Am,"__call__"),Am.forEach(o),gd=i(Zo," special method."),Zo.forEach(o),_d=d(Jo),u(We.$$.fragment,Jo),Jo.forEach(o),Go.forEach(o),Nr=d(t),Te=r(t,"H2",{class:!0});var va=a(Te);Ke=r(va,"A",{id:!0,class:!0,href:!0});var Pm=a(Ke);ir=r(Pm,"SPAN",{});var jm=a(ir);u(to.$$.fragment,jm),jm.forEach(o),Pm.forEach(o),vd=d(va),lr=r(va,"SPAN",{});var Lm=a(lr);kd=i(Lm,"RealmForOpenQA"),Lm.forEach(o),va.forEach(o),Or=d(t),G=r(t,"DIV",{class:!0});var Yo=a(G);u(oo.$$.fragment,Yo),wd=d(Yo),Be=r(Yo,"P",{});var _r=a(Be);dr=r(_r,"CODE",{});var Mm=a(dr);bd=i(Mm,"RealmForOpenQA"),Mm.forEach(o),Rd=i(_r,` for end-to-end open domain question answering.
This model is a PyTorch `),no=r(_r,"A",{href:!0,rel:!0});var Sm=a(no);Td=i(Sm,"torch.nn.Module"),Sm.forEach(o),$d=i(_r,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),_r.forEach(o),yd=d(Yo),I=r(Yo,"DIV",{class:!0});var ae=a(I);u(ro.$$.fragment,ae),Ed=d(ae),$e=r(ae,"P",{});var en=a($e);zd=i(en,"The "),Fo=r(en,"A",{href:!0});var Fm=a(Fo);qd=i(Fm,"RealmForOpenQA"),Fm.forEach(o),xd=i(en," forward method, overrides the "),cr=r(en,"CODE",{});var Cm=a(cr);Ad=i(Cm,"__call__"),Cm.forEach(o),Pd=i(en," special method."),en.forEach(o),jd=d(ae),u(Qe.$$.fragment,ae),Ld=d(ae),mr=r(ae,"P",{});var Im=a(mr);Md=i(Im,"Example:"),Im.forEach(o),Sd=d(ae),u(ao.$$.fragment,ae),ae.forEach(o),Yo.forEach(o),this.h()},h(){c(h,"name","hf:doc:metadata"),c(h,"content",JSON.stringify(Xm)),c(T,"id","realm"),c(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(T,"href","#realm"),c(w,"class","relative group"),c(ye,"id","overview"),c(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ye,"href","#overview"),c(se,"class","relative group"),c(Xe,"href","https://arxiv.org/abs/2002.08909"),c(Xe,"rel","nofollow"),c(Ge,"href","https://huggingface.co/qqaatw"),c(Ge,"rel","nofollow"),c(Je,"href","https://github.com/google-research/language/tree/master/language/realm"),c(Je,"rel","nofollow"),c(ze,"id","transformers.RealmConfig"),c(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ze,"href","#transformers.RealmConfig"),c(ie,"class","relative group"),c(mo,"href","/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmEmbedder"),c(ho,"href","/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmScorer"),c(po,"href","/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmKnowledgeAugEncoder"),c(fo,"href","/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmRetriever"),c(uo,"href","/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmReader"),c(go,"href","/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmForOpenQA"),c(tt,"href","https://huggingface.co/google/realm-cc-news-pretrained-embedder"),c(tt,"rel","nofollow"),c(_o,"href","/docs/transformers/v4.17.0/en/main_classes/configuration#transformers.PretrainedConfig"),c(vo,"href","/docs/transformers/v4.17.0/en/main_classes/configuration#transformers.PretrainedConfig"),c(x,"class","docstring"),c(qe,"id","transformers.RealmTokenizer"),c(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qe,"href","#transformers.RealmTokenizer"),c(de,"class","relative group"),c(ko,"href","/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmTokenizer"),c(wo,"href","/docs/transformers/v4.17.0/en/model_doc/bert#transformers.BertTokenizer"),c(bo,"href","/docs/transformers/v4.17.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(Y,"class","docstring"),c(Ae,"class","docstring"),c(O,"class","docstring"),c($o,"class","docstring"),c(P,"class","docstring"),c(E,"class","docstring"),c(Pe,"id","transformers.RealmTokenizerFast"),c(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Pe,"href","#transformers.RealmTokenizerFast"),c(he,"class","relative group"),c(zo,"href","/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmTokenizerFast"),c(qo,"href","/docs/transformers/v4.17.0/en/model_doc/bert#transformers.BertTokenizerFast"),c(xo,"href","/docs/transformers/v4.17.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),c(j,"class","docstring"),c(M,"class","docstring"),c(Le,"id","transformers.RealmRetriever"),c(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Le,"href","#transformers.RealmRetriever"),c(fe,"class","relative group"),c(Me,"class","docstring"),c(Q,"class","docstring"),c(Se,"id","transformers.RealmEmbedder"),c(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Se,"href","#transformers.RealmEmbedder"),c(ue,"class","relative group"),c(St,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(St,"rel","nofollow"),c(jo,"href","/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmEmbedder"),c(S,"class","docstring"),c(H,"class","docstring"),c(Ce,"id","transformers.RealmScorer"),c(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ce,"href","#transformers.RealmScorer"),c(_e,"class","relative group"),c(Ot,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ot,"rel","nofollow"),c(Lo,"href","/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmScorer"),c(F,"class","docstring"),c(V,"class","docstring"),c(De,"id","transformers.RealmKnowledgeAugEncoder"),c(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(De,"href","#transformers.RealmKnowledgeAugEncoder"),c(ke,"class","relative group"),c(Vt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Vt,"rel","nofollow"),c(Mo,"href","/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmKnowledgeAugEncoder"),c(C,"class","docstring"),c(U,"class","docstring"),c(Oe,"id","transformers.RealmReader"),c(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Oe,"href","#transformers.RealmReader"),c(be,"class","relative group"),c(Yt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Yt,"rel","nofollow"),c(So,"href","/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmReader"),c(ee,"class","docstring"),c(X,"class","docstring"),c(Ke,"id","transformers.RealmForOpenQA"),c(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ke,"href","#transformers.RealmForOpenQA"),c(Te,"class","relative group"),c(no,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(no,"rel","nofollow"),c(Fo,"href","/docs/transformers/v4.17.0/en/model_doc/realm#transformers.RealmForOpenQA"),c(I,"class","docstring"),c(G,"class","docstring")},m(t,m){e(document.head,h),p(t,$,m),p(t,w,m),e(w,T),e(T,y),g(R,y,null),e(w,b),e(w,q),e(q,ka),p(t,kr,m),p(t,se,m),e(se,ye),e(ye,tn),g(Ue,tn,null),e(se,wa),e(se,on),e(on,ba),p(t,wr,m),p(t,Ee,m),e(Ee,Ra),e(Ee,Xe),e(Xe,Ta),e(Ee,$a),p(t,br,m),p(t,lo,m),e(lo,ya),p(t,Rr,m),p(t,co,m),e(co,nn),e(nn,Ea),p(t,Tr,m),p(t,Z,m),e(Z,za),e(Z,Ge),e(Ge,qa),e(Z,xa),e(Z,Je),e(Je,Aa),e(Z,Pa),p(t,$r,m),p(t,ie,m),e(ie,ze),e(ze,rn),g(Ze,rn,null),e(ie,ja),e(ie,an),e(an,La),p(t,yr,m),p(t,x,m),g(Ye,x,null),e(x,Ma),e(x,sn),e(sn,Sa),e(x,Fa),e(x,L),e(L,ln),e(ln,mo),e(mo,Ca),e(L,Ia),e(L,dn),e(dn,ho),e(ho,Da),e(L,Na),e(L,cn),e(cn,po),e(po,Oa),e(L,Wa),e(L,mn),e(mn,fo),e(fo,Ka),e(L,Ba),e(L,hn),e(hn,uo),e(uo,Qa),e(L,Ha),e(L,pn),e(pn,go),e(go,Va),e(x,Ua),e(x,et),e(et,Xa),e(et,tt),e(tt,Ga),e(et,Ja),e(x,Za),e(x,le),e(le,Ya),e(le,_o),e(_o,es),e(le,ts),e(le,vo),e(vo,os),e(le,ns),e(x,rs),e(x,fn),e(fn,as),e(x,ss),g(ot,x,null),p(t,Er,m),p(t,de,m),e(de,qe),e(qe,un),g(nt,un,null),e(de,is),e(de,gn),e(gn,ls),p(t,zr,m),p(t,E,m),g(rt,E,null),e(E,ds),e(E,_n),e(_n,cs),e(E,ms),e(E,xe),e(xe,ko),e(ko,hs),e(xe,ps),e(xe,wo),e(wo,fs),e(xe,us),e(E,gs),e(E,at),e(at,_s),e(at,bo),e(bo,vs),e(at,ks),e(E,ws),e(E,Y),g(st,Y,null),e(Y,bs),e(Y,vn),e(vn,Rs),e(Y,Ts),e(Y,it),e(it,Ro),e(Ro,$s),e(Ro,kn),e(kn,ys),e(it,Es),e(it,To),e(To,zs),e(To,wn),e(wn,qs),e(E,xs),e(E,Ae),g(lt,Ae,null),e(Ae,As),e(Ae,dt),e(dt,Ps),e(dt,bn),e(bn,js),e(dt,Ls),e(E,Ms),e(E,O),g(ct,O,null),e(O,Ss),e(O,Rn),e(Rn,Fs),e(O,Cs),g(mt,O,null),e(O,Is),e(O,ce),e(ce,Ds),e(ce,Tn),e(Tn,Ns),e(ce,Os),e(ce,$n),e($n,Ws),e(ce,Ks),e(E,Bs),e(E,$o),g(ht,$o,null),e(E,Qs),e(E,P),g(pt,P,null),e(P,Hs),e(P,ft),e(ft,Vs),e(ft,yn),e(yn,Us),e(ft,Xs),e(P,Gs),e(P,me),e(me,En),e(En,Js),e(me,Zs),e(me,ut),e(ut,Ys),e(ut,zn),e(zn,ei),e(ut,ti),e(me,oi),e(me,gt),e(gt,ni),e(gt,qn),e(qn,ri),e(gt,ai),e(P,si),e(P,_t),e(_t,yo),e(yo,ii),e(yo,xn),e(xn,li),e(_t,di),e(_t,Eo),e(Eo,ci),e(Eo,An),e(An,mi),e(P,hi),e(P,Pn),e(Pn,pi),e(P,fi),g(vt,P,null),p(t,qr,m),p(t,he,m),e(he,Pe),e(Pe,jn),g(kt,jn,null),e(he,ui),e(he,Ln),e(Ln,gi),p(t,xr,m),p(t,M,m),g(wt,M,null),e(M,_i),e(M,bt),e(bt,vi),e(bt,Mn),e(Mn,ki),e(bt,wi),e(M,bi),e(M,je),e(je,zo),e(zo,Ri),e(je,Ti),e(je,qo),e(qo,$i),e(je,yi),e(M,Ei),e(M,Rt),e(Rt,zi),e(Rt,xo),e(xo,qi),e(Rt,xi),e(M,Ai),e(M,j),g(Tt,j,null),e(j,Pi),e(j,$t),e($t,ji),e($t,Sn),e(Sn,Li),e($t,Mi),e(j,Si),e(j,pe),e(pe,Fn),e(Fn,Fi),e(pe,Ci),e(pe,yt),e(yt,Ii),e(yt,Cn),e(Cn,Di),e(yt,Ni),e(pe,Oi),e(pe,Et),e(Et,Wi),e(Et,In),e(In,Ki),e(Et,Bi),e(j,Qi),e(j,zt),e(zt,Ao),e(Ao,Hi),e(Ao,Dn),e(Dn,Vi),e(zt,Ui),e(zt,Po),e(Po,Xi),e(Po,Nn),e(Nn,Gi),e(j,Ji),e(j,On),e(On,Zi),e(j,Yi),g(qt,j,null),p(t,Ar,m),p(t,fe,m),e(fe,Le),e(Le,Wn),g(xt,Wn,null),e(fe,el),e(fe,Kn),e(Kn,tl),p(t,Pr,m),p(t,Q,m),g(At,Q,null),e(Q,ol),e(Q,Bn),e(Bn,nl),e(Q,rl),e(Q,Me),g(Pt,Me,null),e(Me,al),e(Me,Qn),e(Qn,sl),p(t,jr,m),p(t,ue,m),e(ue,Se),e(Se,Hn),g(jt,Hn,null),e(ue,il),e(ue,Vn),e(Vn,ll),p(t,Lr,m),p(t,H,m),g(Lt,H,null),e(H,dl),e(H,Mt),e(Mt,cl),e(Mt,St),e(St,ml),e(Mt,hl),e(H,pl),e(H,S),g(Ft,S,null),e(S,fl),e(S,ge),e(ge,ul),e(ge,jo),e(jo,gl),e(ge,_l),e(ge,Un),e(Un,vl),e(ge,kl),e(S,wl),g(Fe,S,null),e(S,bl),e(S,Xn),e(Xn,Rl),e(S,Tl),g(Ct,S,null),p(t,Mr,m),p(t,_e,m),e(_e,Ce),e(Ce,Gn),g(It,Gn,null),e(_e,$l),e(_e,Jn),e(Jn,yl),p(t,Sr,m),p(t,V,m),g(Dt,V,null),e(V,El),e(V,Nt),e(Nt,zl),e(Nt,Ot),e(Ot,ql),e(Nt,xl),e(V,Al),e(V,F),g(Wt,F,null),e(F,Pl),e(F,ve),e(ve,jl),e(ve,Lo),e(Lo,Ll),e(ve,Ml),e(ve,Zn),e(Zn,Sl),e(ve,Fl),e(F,Cl),g(Ie,F,null),e(F,Il),e(F,Yn),e(Yn,Dl),e(F,Nl),g(Kt,F,null),p(t,Fr,m),p(t,ke,m),e(ke,De),e(De,er),g(Bt,er,null),e(ke,Ol),e(ke,tr),e(tr,Wl),p(t,Cr,m),p(t,U,m),g(Qt,U,null),e(U,Kl),e(U,Ht),e(Ht,Bl),e(Ht,Vt),e(Vt,Ql),e(Ht,Hl),e(U,Vl),e(U,C),g(Ut,C,null),e(C,Ul),e(C,we),e(we,Xl),e(we,Mo),e(Mo,Gl),e(we,Jl),e(we,or),e(or,Zl),e(we,Yl),e(C,ed),g(Ne,C,null),e(C,td),e(C,nr),e(nr,od),e(C,nd),g(Xt,C,null),p(t,Ir,m),p(t,be,m),e(be,Oe),e(Oe,rr),g(Gt,rr,null),e(be,rd),e(be,ar),e(ar,ad),p(t,Dr,m),p(t,X,m),g(Jt,X,null),e(X,sd),e(X,Zt),e(Zt,id),e(Zt,Yt),e(Yt,ld),e(Zt,dd),e(X,cd),e(X,ee),g(eo,ee,null),e(ee,md),e(ee,Re),e(Re,hd),e(Re,So),e(So,pd),e(Re,fd),e(Re,sr),e(sr,ud),e(Re,gd),e(ee,_d),g(We,ee,null),p(t,Nr,m),p(t,Te,m),e(Te,Ke),e(Ke,ir),g(to,ir,null),e(Te,vd),e(Te,lr),e(lr,kd),p(t,Or,m),p(t,G,m),g(oo,G,null),e(G,wd),e(G,Be),e(Be,dr),e(dr,bd),e(Be,Rd),e(Be,no),e(no,Td),e(Be,$d),e(G,yd),e(G,I),g(ro,I,null),e(I,Ed),e(I,$e),e($e,zd),e($e,Fo),e(Fo,qd),e($e,xd),e($e,cr),e(cr,Ad),e($e,Pd),e(I,jd),g(Qe,I,null),e(I,Ld),e(I,mr),e(mr,Md),e(I,Sd),g(ao,I,null),Wr=!0},p(t,[m]){const so={};m&2&&(so.$$scope={dirty:m,ctx:t}),Fe.$set(so);const hr={};m&2&&(hr.$$scope={dirty:m,ctx:t}),Ie.$set(hr);const pr={};m&2&&(pr.$$scope={dirty:m,ctx:t}),Ne.$set(pr);const fr={};m&2&&(fr.$$scope={dirty:m,ctx:t}),We.$set(fr);const io={};m&2&&(io.$$scope={dirty:m,ctx:t}),Qe.$set(io)},i(t){Wr||(_(R.$$.fragment,t),_(Ue.$$.fragment,t),_(Ze.$$.fragment,t),_(Ye.$$.fragment,t),_(ot.$$.fragment,t),_(nt.$$.fragment,t),_(rt.$$.fragment,t),_(st.$$.fragment,t),_(lt.$$.fragment,t),_(ct.$$.fragment,t),_(mt.$$.fragment,t),_(ht.$$.fragment,t),_(pt.$$.fragment,t),_(vt.$$.fragment,t),_(kt.$$.fragment,t),_(wt.$$.fragment,t),_(Tt.$$.fragment,t),_(qt.$$.fragment,t),_(xt.$$.fragment,t),_(At.$$.fragment,t),_(Pt.$$.fragment,t),_(jt.$$.fragment,t),_(Lt.$$.fragment,t),_(Ft.$$.fragment,t),_(Fe.$$.fragment,t),_(Ct.$$.fragment,t),_(It.$$.fragment,t),_(Dt.$$.fragment,t),_(Wt.$$.fragment,t),_(Ie.$$.fragment,t),_(Kt.$$.fragment,t),_(Bt.$$.fragment,t),_(Qt.$$.fragment,t),_(Ut.$$.fragment,t),_(Ne.$$.fragment,t),_(Xt.$$.fragment,t),_(Gt.$$.fragment,t),_(Jt.$$.fragment,t),_(eo.$$.fragment,t),_(We.$$.fragment,t),_(to.$$.fragment,t),_(oo.$$.fragment,t),_(ro.$$.fragment,t),_(Qe.$$.fragment,t),_(ao.$$.fragment,t),Wr=!0)},o(t){v(R.$$.fragment,t),v(Ue.$$.fragment,t),v(Ze.$$.fragment,t),v(Ye.$$.fragment,t),v(ot.$$.fragment,t),v(nt.$$.fragment,t),v(rt.$$.fragment,t),v(st.$$.fragment,t),v(lt.$$.fragment,t),v(ct.$$.fragment,t),v(mt.$$.fragment,t),v(ht.$$.fragment,t),v(pt.$$.fragment,t),v(vt.$$.fragment,t),v(kt.$$.fragment,t),v(wt.$$.fragment,t),v(Tt.$$.fragment,t),v(qt.$$.fragment,t),v(xt.$$.fragment,t),v(At.$$.fragment,t),v(Pt.$$.fragment,t),v(jt.$$.fragment,t),v(Lt.$$.fragment,t),v(Ft.$$.fragment,t),v(Fe.$$.fragment,t),v(Ct.$$.fragment,t),v(It.$$.fragment,t),v(Dt.$$.fragment,t),v(Wt.$$.fragment,t),v(Ie.$$.fragment,t),v(Kt.$$.fragment,t),v(Bt.$$.fragment,t),v(Qt.$$.fragment,t),v(Ut.$$.fragment,t),v(Ne.$$.fragment,t),v(Xt.$$.fragment,t),v(Gt.$$.fragment,t),v(Jt.$$.fragment,t),v(eo.$$.fragment,t),v(We.$$.fragment,t),v(to.$$.fragment,t),v(oo.$$.fragment,t),v(ro.$$.fragment,t),v(Qe.$$.fragment,t),v(ao.$$.fragment,t),Wr=!1},d(t){o(h),t&&o($),t&&o(w),k(R),t&&o(kr),t&&o(se),k(Ue),t&&o(wr),t&&o(Ee),t&&o(br),t&&o(lo),t&&o(Rr),t&&o(co),t&&o(Tr),t&&o(Z),t&&o($r),t&&o(ie),k(Ze),t&&o(yr),t&&o(x),k(Ye),k(ot),t&&o(Er),t&&o(de),k(nt),t&&o(zr),t&&o(E),k(rt),k(st),k(lt),k(ct),k(mt),k(ht),k(pt),k(vt),t&&o(qr),t&&o(he),k(kt),t&&o(xr),t&&o(M),k(wt),k(Tt),k(qt),t&&o(Ar),t&&o(fe),k(xt),t&&o(Pr),t&&o(Q),k(At),k(Pt),t&&o(jr),t&&o(ue),k(jt),t&&o(Lr),t&&o(H),k(Lt),k(Ft),k(Fe),k(Ct),t&&o(Mr),t&&o(_e),k(It),t&&o(Sr),t&&o(V),k(Dt),k(Wt),k(Ie),k(Kt),t&&o(Fr),t&&o(ke),k(Bt),t&&o(Cr),t&&o(U),k(Qt),k(Ut),k(Ne),k(Xt),t&&o(Ir),t&&o(be),k(Gt),t&&o(Dr),t&&o(X),k(Jt),k(eo),k(We),t&&o(Nr),t&&o(Te),k(to),t&&o(Or),t&&o(G),k(oo),k(ro),k(Qe),k(ao)}}}const Xm={local:"realm",sections:[{local:"overview",title:"Overview"},{local:"transformers.RealmConfig",title:"RealmConfig"},{local:"transformers.RealmTokenizer",title:"RealmTokenizer"},{local:"transformers.RealmTokenizerFast",title:"RealmTokenizerFast"},{local:"transformers.RealmRetriever",title:"RealmRetriever"},{local:"transformers.RealmEmbedder",title:"RealmEmbedder"},{local:"transformers.RealmScorer",title:"RealmScorer"},{local:"transformers.RealmKnowledgeAugEncoder",title:"RealmKnowledgeAugEncoder"},{local:"transformers.RealmReader",title:"RealmReader"},{local:"transformers.RealmForOpenQA",title:"RealmForOpenQA"}],title:"REALM"};function Gm(N,h,$){let{fw:w}=h;return N.$$set=T=>{"fw"in T&&$(0,w=T.fw)},[w]}class nh extends Dm{constructor(h){super();Nm(this,h,Gm,Um,Om,{fw:0})}}export{nh as default,Xm as metadata};
