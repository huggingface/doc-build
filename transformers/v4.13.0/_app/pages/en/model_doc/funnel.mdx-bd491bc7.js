import{S as F2,i as v2,s as k2,e as s,k as l,w as k,t,L as w2,c as r,d as n,m as d,a,x as w,h as o,b as c,J as e,g as h,y as b,q as y,o as $,B as E}from"../../../chunks/vendor-e859c359.js";import{T as ze}from"../../../chunks/Tip-edc75249.js";import{D as ee}from"../../../chunks/Docstring-ade913b3.js";import{C as Ne}from"../../../chunks/CodeBlock-ce4317c2.js";import{I as Pe}from"../../../chunks/IconCopyLink-5fae3b20.js";import"../../../chunks/CopyButton-77addb3d.js";function b2(W){let p,M,m,g,F;return{c(){p=s("p"),M=t(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s("code"),g=t("Module"),F=t(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function y2(W){let p,M,m,g,F;return{c(){p=s("p"),M=t(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s("code"),g=t("Module"),F=t(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function $2(W){let p,M,m,g,F;return{c(){p=s("p"),M=t(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s("code"),g=t("Module"),F=t(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function E2(W){let p,M,m,g,F;return{c(){p=s("p"),M=t(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s("code"),g=t("Module"),F=t(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function M2(W){let p,M,m,g,F;return{c(){p=s("p"),M=t(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s("code"),g=t("Module"),F=t(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function z2(W){let p,M,m,g,F;return{c(){p=s("p"),M=t(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s("code"),g=t("Module"),F=t(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function P2(W){let p,M,m,g,F;return{c(){p=s("p"),M=t(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s("code"),g=t("Module"),F=t(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function q2(W){let p,M,m,g,F;return{c(){p=s("p"),M=t(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s("code"),g=t("Module"),F=t(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function C2(W){let p,M,m,g,F,T,_,z,ce,G,P,X,I,ne,ue,S,pe,ie,U,L,te,Z,q,x,oe,Q,le,se,O,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),P=s("p"),X=t("This second option is useful when using "),I=s("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all
the tensors in the first argument of the model call function: `),S=s("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=s("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),Z=l(),q=s("ul"),x=s("li"),oe=t("a single Tensor with "),Q=s("code"),le=t("input_ids"),se=t(" only and nothing else: "),O=s("code"),he=t("model(inputs_ids)"),de=l(),C=s("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=t("model([input_ids, attention_mask])"),ae=t(" or "),R=s("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=r(K,"LI",{});var we=a(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),P=r(u,"P",{});var D=a(P);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all
the tensors in the first argument of the model call function: `),S=r(D,"CODE",{});var be=a(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=r(u,"P",{});var ye=a(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),ye.forEach(n),Z=d(u),q=r(u,"UL",{});var j=a(q);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),O=r(V,"CODE",{});var ve=a(O);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,P,v),e(P,X),e(P,I),e(I,ne),e(P,ue),e(P,S),e(S,pe),e(P,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,q,v),e(q,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,O),e(O,he),e(q,de),e(q,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(q,N),e(q,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(P),u&&n(U),u&&n(L),u&&n(Z),u&&n(q)}}}function x2(W){let p,M,m,g,F;return{c(){p=s("p"),M=t(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s("code"),g=t("Module"),F=t(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function j2(W){let p,M,m,g,F,T,_,z,ce,G,P,X,I,ne,ue,S,pe,ie,U,L,te,Z,q,x,oe,Q,le,se,O,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),P=s("p"),X=t("This second option is useful when using "),I=s("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all
the tensors in the first argument of the model call function: `),S=s("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=s("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),Z=l(),q=s("ul"),x=s("li"),oe=t("a single Tensor with "),Q=s("code"),le=t("input_ids"),se=t(" only and nothing else: "),O=s("code"),he=t("model(inputs_ids)"),de=l(),C=s("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=t("model([input_ids, attention_mask])"),ae=t(" or "),R=s("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=r(K,"LI",{});var we=a(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),P=r(u,"P",{});var D=a(P);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all
the tensors in the first argument of the model call function: `),S=r(D,"CODE",{});var be=a(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=r(u,"P",{});var ye=a(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),ye.forEach(n),Z=d(u),q=r(u,"UL",{});var j=a(q);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),O=r(V,"CODE",{});var ve=a(O);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,P,v),e(P,X),e(P,I),e(I,ne),e(P,ue),e(P,S),e(S,pe),e(P,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,q,v),e(q,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,O),e(O,he),e(q,de),e(q,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(q,N),e(q,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(P),u&&n(U),u&&n(L),u&&n(Z),u&&n(q)}}}function L2(W){let p,M,m,g,F;return{c(){p=s("p"),M=t(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s("code"),g=t("Module"),F=t(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function A2(W){let p,M,m,g,F,T,_,z,ce,G,P,X,I,ne,ue,S,pe,ie,U,L,te,Z,q,x,oe,Q,le,se,O,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),P=s("p"),X=t("This second option is useful when using "),I=s("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all
the tensors in the first argument of the model call function: `),S=s("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=s("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),Z=l(),q=s("ul"),x=s("li"),oe=t("a single Tensor with "),Q=s("code"),le=t("input_ids"),se=t(" only and nothing else: "),O=s("code"),he=t("model(inputs_ids)"),de=l(),C=s("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=t("model([input_ids, attention_mask])"),ae=t(" or "),R=s("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=r(K,"LI",{});var we=a(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),P=r(u,"P",{});var D=a(P);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all
the tensors in the first argument of the model call function: `),S=r(D,"CODE",{});var be=a(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=r(u,"P",{});var ye=a(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),ye.forEach(n),Z=d(u),q=r(u,"UL",{});var j=a(q);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),O=r(V,"CODE",{});var ve=a(O);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,P,v),e(P,X),e(P,I),e(I,ne),e(P,ue),e(P,S),e(S,pe),e(P,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,q,v),e(q,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,O),e(O,he),e(q,de),e(q,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(q,N),e(q,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(P),u&&n(U),u&&n(L),u&&n(Z),u&&n(q)}}}function D2(W){let p,M,m,g,F;return{c(){p=s("p"),M=t(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s("code"),g=t("Module"),F=t(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function I2(W){let p,M,m,g,F,T,_,z,ce,G,P,X,I,ne,ue,S,pe,ie,U,L,te,Z,q,x,oe,Q,le,se,O,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),P=s("p"),X=t("This second option is useful when using "),I=s("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all
the tensors in the first argument of the model call function: `),S=s("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=s("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),Z=l(),q=s("ul"),x=s("li"),oe=t("a single Tensor with "),Q=s("code"),le=t("input_ids"),se=t(" only and nothing else: "),O=s("code"),he=t("model(inputs_ids)"),de=l(),C=s("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=t("model([input_ids, attention_mask])"),ae=t(" or "),R=s("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=r(K,"LI",{});var we=a(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),P=r(u,"P",{});var D=a(P);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all
the tensors in the first argument of the model call function: `),S=r(D,"CODE",{});var be=a(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=r(u,"P",{});var ye=a(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),ye.forEach(n),Z=d(u),q=r(u,"UL",{});var j=a(q);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),O=r(V,"CODE",{});var ve=a(O);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,P,v),e(P,X),e(P,I),e(I,ne),e(P,ue),e(P,S),e(S,pe),e(P,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,q,v),e(q,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,O),e(O,he),e(q,de),e(q,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(q,N),e(q,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(P),u&&n(U),u&&n(L),u&&n(Z),u&&n(q)}}}function S2(W){let p,M,m,g,F;return{c(){p=s("p"),M=t(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s("code"),g=t("Module"),F=t(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function O2(W){let p,M,m,g,F,T,_,z,ce,G,P,X,I,ne,ue,S,pe,ie,U,L,te,Z,q,x,oe,Q,le,se,O,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),P=s("p"),X=t("This second option is useful when using "),I=s("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all
the tensors in the first argument of the model call function: `),S=s("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=s("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),Z=l(),q=s("ul"),x=s("li"),oe=t("a single Tensor with "),Q=s("code"),le=t("input_ids"),se=t(" only and nothing else: "),O=s("code"),he=t("model(inputs_ids)"),de=l(),C=s("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=t("model([input_ids, attention_mask])"),ae=t(" or "),R=s("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=r(K,"LI",{});var we=a(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),P=r(u,"P",{});var D=a(P);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all
the tensors in the first argument of the model call function: `),S=r(D,"CODE",{});var be=a(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=r(u,"P",{});var ye=a(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),ye.forEach(n),Z=d(u),q=r(u,"UL",{});var j=a(q);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),O=r(V,"CODE",{});var ve=a(O);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,P,v),e(P,X),e(P,I),e(I,ne),e(P,ue),e(P,S),e(S,pe),e(P,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,q,v),e(q,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,O),e(O,he),e(q,de),e(q,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(q,N),e(q,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(P),u&&n(U),u&&n(L),u&&n(Z),u&&n(q)}}}function N2(W){let p,M,m,g,F;return{c(){p=s("p"),M=t(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s("code"),g=t("Module"),F=t(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function B2(W){let p,M,m,g,F,T,_,z,ce,G,P,X,I,ne,ue,S,pe,ie,U,L,te,Z,q,x,oe,Q,le,se,O,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),P=s("p"),X=t("This second option is useful when using "),I=s("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all
the tensors in the first argument of the model call function: `),S=s("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=s("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),Z=l(),q=s("ul"),x=s("li"),oe=t("a single Tensor with "),Q=s("code"),le=t("input_ids"),se=t(" only and nothing else: "),O=s("code"),he=t("model(inputs_ids)"),de=l(),C=s("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=t("model([input_ids, attention_mask])"),ae=t(" or "),R=s("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=r(K,"LI",{});var we=a(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),P=r(u,"P",{});var D=a(P);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all
the tensors in the first argument of the model call function: `),S=r(D,"CODE",{});var be=a(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=r(u,"P",{});var ye=a(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),ye.forEach(n),Z=d(u),q=r(u,"UL",{});var j=a(q);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),O=r(V,"CODE",{});var ve=a(O);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,P,v),e(P,X),e(P,I),e(I,ne),e(P,ue),e(P,S),e(S,pe),e(P,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,q,v),e(q,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,O),e(O,he),e(q,de),e(q,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(q,N),e(q,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(P),u&&n(U),u&&n(L),u&&n(Z),u&&n(q)}}}function W2(W){let p,M,m,g,F;return{c(){p=s("p"),M=t(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s("code"),g=t("Module"),F=t(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function Q2(W){let p,M,m,g,F,T,_,z,ce,G,P,X,I,ne,ue,S,pe,ie,U,L,te,Z,q,x,oe,Q,le,se,O,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),P=s("p"),X=t("This second option is useful when using "),I=s("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all
the tensors in the first argument of the model call function: `),S=s("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=s("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),Z=l(),q=s("ul"),x=s("li"),oe=t("a single Tensor with "),Q=s("code"),le=t("input_ids"),se=t(" only and nothing else: "),O=s("code"),he=t("model(inputs_ids)"),de=l(),C=s("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=t("model([input_ids, attention_mask])"),ae=t(" or "),R=s("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=r(K,"LI",{});var we=a(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),P=r(u,"P",{});var D=a(P);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all
the tensors in the first argument of the model call function: `),S=r(D,"CODE",{});var be=a(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=r(u,"P",{});var ye=a(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),ye.forEach(n),Z=d(u),q=r(u,"UL",{});var j=a(q);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),O=r(V,"CODE",{});var ve=a(O);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,P,v),e(P,X),e(P,I),e(I,ne),e(P,ue),e(P,S),e(S,pe),e(P,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,q,v),e(q,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,O),e(O,he),e(q,de),e(q,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(q,N),e(q,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(P),u&&n(U),u&&n(L),u&&n(Z),u&&n(q)}}}function R2(W){let p,M,m,g,F;return{c(){p=s("p"),M=t(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s("code"),g=t("Module"),F=t(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function H2(W){let p,M,m,g,F,T,_,z,ce,G,P,X,I,ne,ue,S,pe,ie,U,L,te,Z,q,x,oe,Q,le,se,O,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),P=s("p"),X=t("This second option is useful when using "),I=s("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all
the tensors in the first argument of the model call function: `),S=s("code"),pe=t("model(inputs)"),ie=t("."),U=l(),L=s("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),Z=l(),q=s("ul"),x=s("li"),oe=t("a single Tensor with "),Q=s("code"),le=t("input_ids"),se=t(" only and nothing else: "),O=s("code"),he=t("model(inputs_ids)"),de=l(),C=s("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=t("model([input_ids, attention_mask])"),ae=t(" or "),R=s("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(n),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(K),z=r(K,"LI",{});var we=a(z);ce=o(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(n),K.forEach(n),G=d(u),P=r(u,"P",{});var D=a(P);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all
the tensors in the first argument of the model call function: `),S=r(D,"CODE",{});var be=a(S);pe=o(be,"model(inputs)"),be.forEach(n),ie=o(D,"."),D.forEach(n),U=d(u),L=r(u,"P",{});var ye=a(L);te=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),ye.forEach(n),Z=d(u),q=r(u,"UL",{});var j=a(q);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(V," only and nothing else: "),O=r(V,"CODE",{});var ve=a(O);he=o(ve,"model(inputs_ids)"),ve.forEach(n),V.forEach(n),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(n),Y.forEach(n),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),j.forEach(n)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,P,v),e(P,X),e(P,I),e(I,ne),e(P,ue),e(P,S),e(S,pe),e(P,ie),h(u,U,v),h(u,L,v),e(L,te),h(u,Z,v),h(u,q,v),e(q,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,O),e(O,he),e(q,de),e(q,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(q,N),e(q,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(G),u&&n(P),u&&n(U),u&&n(L),u&&n(Z),u&&n(q)}}}function V2(W){let p,M,m,g,F;return{c(){p=s("p"),M=t(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s("code"),g=t("Module"),F=t(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(n),F=o(_,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function Y2(W){let p,M,m,g,F,T,_,z,ce,G,P,X,I,ne,ue,S,pe,ie,U,L,te,Z,q,x,oe,Q,le,se,O,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge,u,v,K,Te,we,D,Fe,be,ye,j,V,$e,ve,Y,Ee,ke,_e,Me,Va,Dp,Ip,yc,jn,Sp,Io,Op,Np,So,Bp,Wp,$c,Kn,Nt,ll,Oo,Qp,dl,Rp,Ec,Cn,No,Hp,xn,Vp,Ya,Yp,Up,Ua,Gp,Zp,Bo,Kp,Xp,Jp,Xn,eh,Ga,nh,th,Za,oh,sh,Mc,Jn,Bt,cl,Wo,rh,ul,ah,zc,qe,Qo,ih,pl,lh,dh,Wt,Ka,ch,uh,Xa,ph,hh,fh,Ro,mh,Ja,gh,_h,Th,Ln,Ho,Fh,hl,vh,kh,Vo,ei,wh,fl,bh,yh,ni,$h,ml,Eh,Mh,Qt,Yo,zh,Uo,Ph,gl,qh,Ch,xh,wn,Go,jh,_l,Lh,Ah,Zo,Dh,et,Ih,Tl,Sh,Oh,Fl,Nh,Bh,Wh,vl,Pc,nt,Rt,kl,Ko,Qh,wl,Rh,qc,Ze,Xo,Hh,Jo,Vh,bl,Yh,Uh,Gh,Ht,ti,Zh,Kh,oi,Xh,Jh,ef,es,nf,si,tf,of,sf,bn,ns,rf,yl,af,lf,ts,df,tt,cf,$l,uf,pf,El,hf,ff,Cc,ot,Vt,Ml,os,mf,zl,gf,xc,st,ss,_f,rs,Tf,ri,Ff,vf,jc,rt,as,kf,is,wf,ai,bf,yf,Lc,at,Yt,Pl,ls,$f,ql,Ef,Ac,We,ds,Mf,Cl,zf,Pf,cs,qf,us,Cf,xf,jf,ps,Lf,ii,Af,Df,If,hs,Sf,fs,Of,Nf,Bf,Ke,ms,Wf,it,Qf,li,Rf,Hf,xl,Vf,Yf,Uf,Ut,Gf,jl,Zf,Kf,gs,Dc,lt,Gt,Ll,_s,Xf,Al,Jf,Ic,Qe,Ts,em,Dl,nm,tm,Fs,om,vs,sm,rm,am,ks,im,di,lm,dm,cm,ws,um,bs,pm,hm,fm,Xe,ys,mm,dt,gm,ci,_m,Tm,Il,Fm,vm,km,Zt,wm,Sl,bm,ym,$s,Sc,ct,Kt,Ol,Es,$m,Nl,Em,Oc,Ms,Je,zs,Mm,ut,zm,ui,Pm,qm,Bl,Cm,xm,jm,Xt,Lm,Wl,Am,Dm,Ps,Nc,pt,Jt,Ql,qs,Im,Rl,Sm,Bc,Re,Cs,Om,xs,Nm,Hl,Bm,Wm,Qm,js,Rm,Ls,Hm,Vm,Ym,As,Um,pi,Gm,Zm,Km,Ds,Xm,Is,Jm,eg,ng,en,Ss,tg,ht,og,hi,sg,rg,Vl,ag,ig,lg,eo,dg,Yl,cg,ug,Os,Wc,ft,no,Ul,Ns,pg,Gl,hg,Qc,He,Bs,fg,Zl,mg,gg,Ws,_g,Qs,Tg,Fg,vg,Rs,kg,fi,wg,bg,yg,Hs,$g,Vs,Eg,Mg,zg,Be,Ys,Pg,mt,qg,mi,Cg,xg,Kl,jg,Lg,Ag,to,Dg,Xl,Ig,Sg,Us,Og,Jl,Ng,Bg,Gs,Rc,gt,oo,ed,Zs,Wg,nd,Qg,Hc,Ve,Ks,Rg,td,Hg,Vg,Xs,Yg,Js,Ug,Gg,Zg,er,Kg,gi,Xg,Jg,e_,nr,n_,tr,t_,o_,s_,nn,or,r_,_t,a_,_i,i_,l_,od,d_,c_,u_,so,p_,sd,h_,f_,sr,Vc,Tt,ro,rd,rr,m_,ad,g_,Yc,Ye,ar,__,id,T_,F_,ir,v_,lr,k_,w_,b_,dr,y_,Ti,$_,E_,M_,cr,z_,ur,P_,q_,C_,tn,pr,x_,Ft,j_,Fi,L_,A_,ld,D_,I_,S_,ao,O_,dd,N_,B_,hr,Uc,vt,io,cd,fr,W_,ud,Q_,Gc,Ue,mr,R_,kt,H_,pd,V_,Y_,hd,U_,G_,Z_,gr,K_,_r,X_,J_,eT,Tr,nT,vi,tT,oT,sT,Fr,rT,vr,aT,iT,lT,on,kr,dT,wt,cT,ki,uT,pT,fd,hT,fT,mT,lo,gT,md,_T,TT,wr,Zc,bt,co,gd,br,FT,_d,vT,Kc,xe,yr,kT,Td,wT,bT,$r,yT,Er,$T,ET,MT,Mr,zT,wi,PT,qT,CT,zr,xT,Pr,jT,LT,AT,uo,DT,sn,qr,IT,yt,ST,bi,OT,NT,Fd,BT,WT,QT,po,RT,vd,HT,VT,Cr,Xc,$t,ho,kd,xr,YT,wd,UT,Jc,je,jr,GT,bd,ZT,KT,Lr,XT,Ar,JT,eF,nF,Dr,tF,yi,oF,sF,rF,Ir,aF,Sr,iF,lF,dF,fo,cF,rn,Or,uF,Et,pF,$i,hF,fF,yd,mF,gF,_F,mo,TF,$d,FF,vF,Nr,eu,Mt,go,Ed,Br,kF,Md,wF,nu,Le,Wr,bF,zd,yF,$F,Qr,EF,Rr,MF,zF,PF,Hr,qF,Ei,CF,xF,jF,Vr,LF,Yr,AF,DF,IF,_o,SF,an,Ur,OF,zt,NF,Mi,BF,WF,Pd,QF,RF,HF,To,VF,qd,YF,UF,Gr,tu,Pt,Fo,Cd,Zr,GF,xd,ZF,ou,Ae,Kr,KF,Xr,XF,jd,JF,ev,nv,Jr,tv,ea,ov,sv,rv,na,av,zi,iv,lv,dv,ta,cv,oa,uv,pv,hv,vo,fv,ln,sa,mv,qt,gv,Pi,_v,Tv,Ld,Fv,vv,kv,ko,wv,Ad,bv,yv,ra,su,Ct,wo,Dd,aa,$v,Id,Ev,ru,De,ia,Mv,Sd,zv,Pv,la,qv,da,Cv,xv,jv,ca,Lv,qi,Av,Dv,Iv,ua,Sv,pa,Ov,Nv,Bv,bo,Wv,dn,ha,Qv,xt,Rv,Ci,Hv,Vv,Od,Yv,Uv,Gv,yo,Zv,Nd,Kv,Xv,fa,au,jt,$o,Bd,ma,Jv,Wd,ek,iu,Ie,ga,nk,Qd,tk,ok,_a,sk,Ta,rk,ak,ik,Fa,lk,xi,dk,ck,uk,va,pk,ka,hk,fk,mk,Eo,gk,cn,wa,_k,Lt,Tk,ji,Fk,vk,Rd,kk,wk,bk,Mo,yk,Hd,$k,Ek,ba,lu,At,zo,Vd,ya,Mk,Yd,zk,du,Se,$a,Pk,Ud,qk,Ck,Ea,xk,Ma,jk,Lk,Ak,za,Dk,Li,Ik,Sk,Ok,Pa,Nk,qa,Bk,Wk,Qk,Po,Rk,un,Ca,Hk,Dt,Vk,Ai,Yk,Uk,Gd,Gk,Zk,Kk,qo,Xk,Zd,Jk,ew,xa,cu,It,Co,Kd,ja,nw,Xd,tw,uu,Oe,La,ow,St,sw,Jd,rw,aw,ec,iw,lw,dw,Aa,cw,Da,uw,pw,hw,Ia,fw,Di,mw,gw,_w,Sa,Tw,Oa,Fw,vw,kw,xo,ww,pn,Na,bw,Ot,yw,Ii,$w,Ew,nc,Mw,zw,Pw,jo,qw,tc,Cw,xw,Ba,pu;return T=new Pe({}),ne=new Pe({}),Oo=new Pe({}),No=new ee({props:{name:"class transformers.FunnelConfig",anchor:"transformers.FunnelConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"block_sizes",val:" = [4, 4, 4]"},{name:"block_repeats",val:" = None"},{name:"num_decoder_layers",val:" = 2"},{name:"d_model",val:" = 768"},{name:"n_head",val:" = 12"},{name:"d_head",val:" = 64"},{name:"d_inner",val:" = 3072"},{name:"hidden_act",val:" = 'gelu_new'"},{name:"hidden_dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.1"},{name:"activation_dropout",val:" = 0.0"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 3"},{name:"initializer_range",val:" = 0.1"},{name:"initializer_std",val:" = None"},{name:"layer_norm_eps",val:" = 1e-09"},{name:"pooling_type",val:" = 'mean'"},{name:"attention_type",val:" = 'relative_shift'"},{name:"separate_cls",val:" = True"},{name:"truncate_seq",val:" = True"},{name:"pool_q_only",val:" = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/configuration_funnel.py#L37",parametersDescription:[{anchor:"transformers.FunnelConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the Funnel transformer. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or
<a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a>.`,name:"vocab_size"},{anchor:"transformers.FunnelConfig.block_sizes",description:`<strong>block_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[4, 4, 4]</code>) &#x2014;
The sizes of the blocks used in the model.`,name:"block_sizes"},{anchor:"transformers.FunnelConfig.block_repeats",description:`<strong>block_repeats</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If passed along, each layer of each block is repeated the number of times indicated.`,name:"block_repeats"},{anchor:"transformers.FunnelConfig.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The number of layers in the decoder (when not using the base model).`,name:"num_decoder_layers"},{anchor:"transformers.FunnelConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the model&#x2019;s hidden states.`,name:"d_model"},{anchor:"transformers.FunnelConfig.n_head",description:`<strong>n_head</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"n_head"},{anchor:"transformers.FunnelConfig.d_head",description:`<strong>d_head</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality of the model&#x2019;s heads.`,name:"d_head"},{anchor:"transformers.FunnelConfig.d_inner",description:`<strong>d_inner</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Inner dimension in the feed-forward blocks.`,name:"d_inner"},{anchor:"transformers.FunnelConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>callable</code>, <em>optional</em>, defaults to <code>&quot;gelu_new&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string,
<code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FunnelConfig.hidden_dropout",description:`<strong>hidden_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout"},{anchor:"transformers.FunnelConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.FunnelConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability used between the two layers of the feed-forward blocks.`,name:"activation_dropout"},{anchor:"transformers.FunnelConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.FunnelConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or
<a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.FunnelConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The standard deviation of the <em>uniform initializer</em> for initializing all weight matrices in attention
layers.`,name:"initializer_range"},{anchor:"transformers.FunnelConfig.initializer_std",description:`<strong>initializer_std</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The standard deviation of the <em>normal initializer</em> for initializing the embedding matrix and the weight of
linear layers. Will default to 1 for the embedding matrix and the value given by Xavier initialization for
linear layers.`,name:"initializer_std"},{anchor:"transformers.FunnelConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-9) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FunnelConfig.pooling_type",description:`<strong>pooling_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;mean&quot;</code>) &#x2014;
Possible values are <code>&quot;mean&quot;</code> or <code>&quot;max&quot;</code>. The way pooling is performed at the beginning of each block.`,name:"pooling_type"},{anchor:"transformers.FunnelConfig.attention_type",description:`<strong>attention_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;relative_shift&quot;</code>) &#x2014;
Possible values are <code>&quot;relative_shift&quot;</code> or <code>&quot;factorized&quot;</code>. The former is faster on CPU/GPU while the
latter is faster on TPU.`,name:"attention_type"},{anchor:"transformers.FunnelConfig.separate_cls",description:`<strong>separate_cls</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to separate the cls token when applying pooling.`,name:"separate_cls"},{anchor:"transformers.FunnelConfig.truncate_seq",description:`<strong>truncate_seq</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
When using <code>separate_cls</code>, whether or not to truncate the last token when pooling, to avoid getting a
sequence length that is not a multiple of 2.`,name:"truncate_seq"},{anchor:"transformers.FunnelConfig.pool_q_only",description:`<strong>pool_q_only</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to apply the pooling only to the query or to query, key and values for the attention layers.`,name:"pool_q_only"}]}}),Wo=new Pe({}),Qo=new ee({props:{name:"class transformers.FunnelTokenizer",anchor:"transformers.FunnelTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/tokenization_funnel.py#L58"}}),Ho=new ee({props:{name:"build\\_inputs\\_with\\_special\\_tokens",anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/tokenization_bert.py#L248",parametersDescription:[{anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Yo=new ee({props:{name:"get\\_special\\_tokens\\_mask",anchor:"transformers.BertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/tokenization_bert.py#L273",parametersDescription:[{anchor:"transformers.BertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Go=new ee({props:{name:"create\\_token\\_type\\_ids\\_from\\_sequences",anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/tokenization_funnel.py#L109",parametersDescription:[{anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given
sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Zo=new Ne({props:{code:`2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |,`,highlighted:`2<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),Ko=new Pe({}),Xo=new ee({props:{name:"class transformers.FunnelTokenizerFast",anchor:"transformers.FunnelTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"clean_text",val:" = True"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"wordpieces_prefix",val:" = '##'"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/tokenization_funnel_fast.py#L71"}}),ns=new ee({props:{name:"create\\_token\\_type\\_ids\\_from\\_sequences",anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/tokenization_funnel_fast.py#L125",parametersDescription:[{anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given
sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ts=new Ne({props:{code:`2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |,`,highlighted:`2<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),os=new Pe({}),ss=new ee({props:{name:"class transformers.models.funnel.modeling\\_funnel.FunnelForPreTrainingOutput",anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_funnel.py#L801",parametersDescription:[{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.loss",description:`<strong>loss</strong> (<em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) &#x2014;
Total loss of the ELECTRA-style objective.`,name:"loss"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Prediction scores of the head (scores for each token before SoftMax).`,name:"logits"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),as=new ee({props:{name:"class transformers.models.funnel.modeling\\_tf\\_funnel.TFFunnelForPreTrainingOutput",anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput",parameters:[{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_tf_funnel.py#L1005",parametersDescription:[{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Prediction scores of the head (scores for each token before SoftMax).`,name:"logits"},{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),ls=new Pe({}),ds=new ee({props:{name:"class transformers.FunnelBaseModel",anchor:"transformers.FunnelBaseModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_funnel.py#L896",parametersDescription:[{anchor:"transformers.FunnelBaseModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),ms=new ee({props:{name:"forward",anchor:"transformers.FunnelBaseModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_funnel.py#L912",parametersDescription:[{anchor:"transformers.FunnelBaseModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelBaseModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelBaseModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelBaseModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelBaseModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelBaseModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelBaseModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ut=new ze({props:{$$slots:{default:[b2]},$$scope:{ctx:W}}}),gs=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelBaseModel
import torch

tokenizer = FunnelTokenizer.from_pretrained('funnel-transformer/small-base')
model = FunnelBaseModel.from_pretrained('funnel-transformer/small-base')

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelBaseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small-base&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelBaseModel.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small-base&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),_s=new Pe({}),Ts=new ee({props:{name:"class transformers.FunnelModel",anchor:"transformers.FunnelModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_funnel.py#L973",parametersDescription:[{anchor:"transformers.FunnelModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),ys=new ee({props:{name:"forward",anchor:"transformers.FunnelModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_funnel.py#L990",parametersDescription:[{anchor:"transformers.FunnelModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Zt=new ze({props:{$$slots:{default:[y2]},$$scope:{ctx:W}}}),$s=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelModel
import torch

tokenizer = FunnelTokenizer.from_pretrained('funnel-transformer/small')
model = FunnelModel.from_pretrained('funnel-transformer/small')

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelModel.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Es=new Pe({}),zs=new ee({props:{name:"forward",anchor:"transformers.FunnelForPreTraining.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_funnel.py#L1090",parametersDescription:[{anchor:"transformers.FunnelForPreTraining.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForPreTraining.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForPreTraining.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForPreTraining.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForPreTraining.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForPreTraining.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForPreTraining.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForPreTraining.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the ELECTRA-style loss. Input should be a sequence of tokens (see <code>input_ids</code>
docstring) Indices should be in <code>[0, 1]</code>:</p>
<ul>
<li>0 indicates the token is an original token,</li>
<li>1 indicates the token was replaced.</li>
</ul>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/model_doc/funnel#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"
>FunnelForPreTrainingOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) \u2014 Total loss of the ELECTRA-style objective.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Prediction scores of the head (scores for each token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/model_doc/funnel#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"
>FunnelForPreTrainingOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Xt=new ze({props:{$$slots:{default:[$2]},$$scope:{ctx:W}}}),Ps=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForPreTraining
import torch

tokenizer = FunnelTokenizer.from_pretrained('funnel-transformer/small')
model = FunnelForPreTraining.from_pretrained('funnel-transformer/small')

inputs = tokenizer("Hello, my dog is cute", return_tensors= "pt")
logits = model(**inputs).logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForPreTraining.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors= <span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits`}}),qs=new Pe({}),Cs=new ee({props:{name:"class transformers.FunnelForMaskedLM",anchor:"transformers.FunnelForMaskedLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_funnel.py#L1163",parametersDescription:[{anchor:"transformers.FunnelForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Ss=new ee({props:{name:"forward",anchor:"transformers.FunnelForMaskedLM.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_funnel.py#L1179",parametersDescription:[{anchor:"transformers.FunnelForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForMaskedLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForMaskedLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),eo=new ze({props:{$$slots:{default:[E2]},$$scope:{ctx:W}}}),Os=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForMaskedLM
import torch

tokenizer = FunnelTokenizer.from_pretrained('funnel-transformer/small')
model = FunnelForMaskedLM.from_pretrained('funnel-transformer/small')

inputs = tokenizer("The capital of France is <mask>.", return_tensors="pt")
labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is &lt;mask&gt;.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ns=new Pe({}),Bs=new ee({props:{name:"class transformers.FunnelForSequenceClassification",anchor:"transformers.FunnelForSequenceClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_funnel.py#L1243",parametersDescription:[{anchor:"transformers.FunnelForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Ys=new ee({props:{name:"forward",anchor:"transformers.FunnelForSequenceClassification.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_funnel.py#L1254",parametersDescription:[{anchor:"transformers.FunnelForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForSequenceClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss),
If <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),to=new ze({props:{$$slots:{default:[M2]},$$scope:{ctx:W}}}),Us=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForSequenceClassification
import torch

tokenizer = FunnelTokenizer.from_pretrained('funnel-transformer/small-base')
model = FunnelForSequenceClassification.from_pretrained('funnel-transformer/small-base')

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small-base&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small-base&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>]).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Gs=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForSequenceClassification
import torch

tokenizer = FunnelTokenizer.from_pretrained('funnel-transformer/small-base')
model = FunnelForSequenceClassification.from_pretrained('funnel-transformer/small-base', problem_type="multi_label_classification")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([[1, 1]], dtype=torch.float) # need dtype=float for BCEWithLogitsLoss
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small-base&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small-base&#x27;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], dtype=torch.<span class="hljs-built_in">float</span>) <span class="hljs-comment"># need dtype=float for BCEWithLogitsLoss</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Zs=new Pe({}),Ks=new ee({props:{name:"class transformers.FunnelForMultipleChoice",anchor:"transformers.FunnelForMultipleChoice",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_funnel.py#L1336",parametersDescription:[{anchor:"transformers.FunnelForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),or=new ee({props:{name:"forward",anchor:"transformers.FunnelForMultipleChoice.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_funnel.py#L1345",parametersDescription:[{anchor:"transformers.FunnelForMultipleChoice.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForMultipleChoice.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForMultipleChoice.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForMultipleChoice.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForMultipleChoice.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForMultipleChoice.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForMultipleChoice.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForMultipleChoice.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices-1]</code> where <code>num_choices</code> is the size of the second dimension of the input tensors. (See
<code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>MultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>MultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),so=new ze({props:{$$slots:{default:[z2]},$$scope:{ctx:W}}}),sr=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForMultipleChoice
import torch

tokenizer = FunnelTokenizer.from_pretrained('funnel-transformer/small-base')
model = FunnelForMultipleChoice.from_pretrained('funnel-transformer/small-base')

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."
labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors='pt', padding=True)
outputs = model(**{k: v.unsqueeze(0) for k,v in encoding.items()}, labels=labels)  # batch size is 1

# the linear classifier still needs to be trained
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small-base&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForMultipleChoice.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small-base&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># choice0 is correct (according to Wikipedia ;)), batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v.unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k,v <span class="hljs-keyword">in</span> encoding.items()}, labels=labels)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),rr=new Pe({}),ar=new ee({props:{name:"class transformers.FunnelForTokenClassification",anchor:"transformers.FunnelForTokenClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_funnel.py#L1420",parametersDescription:[{anchor:"transformers.FunnelForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),pr=new ee({props:{name:"forward",anchor:"transformers.FunnelForTokenClassification.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_funnel.py#L1432",parametersDescription:[{anchor:"transformers.FunnelForTokenClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForTokenClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForTokenClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForTokenClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForTokenClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForTokenClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForTokenClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForTokenClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>TokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>TokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ao=new ze({props:{$$slots:{default:[P2]},$$scope:{ctx:W}}}),hr=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForTokenClassification
import torch

tokenizer = FunnelTokenizer.from_pretrained('funnel-transformer/small')
model = FunnelForTokenClassification.from_pretrained('funnel-transformer/small')

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1] * inputs["input_ids"].size(1)).unsqueeze(0)  # Batch size 1

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForTokenClassification.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>] * inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].size(<span class="hljs-number">1</span>)).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),fr=new Pe({}),mr=new ee({props:{name:"class transformers.FunnelForQuestionAnswering",anchor:"transformers.FunnelForQuestionAnswering",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_funnel.py#L1504",parametersDescription:[{anchor:"transformers.FunnelForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),kr=new ee({props:{name:"forward",anchor:"transformers.FunnelForQuestionAnswering.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_funnel.py#L1515",parametersDescription:[{anchor:"transformers.FunnelForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForQuestionAnswering.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForQuestionAnswering.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForQuestionAnswering.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForQuestionAnswering.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForQuestionAnswering.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the
sequence are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.FunnelForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the
sequence are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>QuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>QuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),lo=new ze({props:{$$slots:{default:[q2]},$$scope:{ctx:W}}}),wr=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForQuestionAnswering
import torch

tokenizer = FunnelTokenizer.from_pretrained('funnel-transformer/small')
model = FunnelForQuestionAnswering.from_pretrained('funnel-transformer/small')

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
inputs = tokenizer(question, text, return_tensors='pt')
start_positions = torch.tensor([1])
end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
loss = outputs.loss
start_scores = outputs.start_logits
end_scores = outputs.end_logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_positions = torch.tensor([<span class="hljs-number">1</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>end_positions = torch.tensor([<span class="hljs-number">3</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>start_scores = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_scores = outputs.end_logits`}}),br=new Pe({}),yr=new ee({props:{name:"class transformers.TFFunnelBaseModel",anchor:"transformers.TFFunnelBaseModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_tf_funnel.py#L1122",parametersDescription:[{anchor:"transformers.TFFunnelBaseModel.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),uo=new ze({props:{$$slots:{default:[C2]},$$scope:{ctx:W}}}),qr=new ee({props:{name:"call",anchor:"transformers.TFFunnelBaseModel.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_tf_funnel.py#L1127",parametersDescription:[{anchor:"transformers.TFFunnelBaseModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelBaseModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelBaseModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelBaseModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelBaseModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelBaseModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelBaseModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This
argument can be used in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelBaseModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>TFBaseModelOutput</a> or a tuple of
<code>tf.Tensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),po=new ze({props:{$$slots:{default:[x2]},$$scope:{ctx:W}}}),Cr=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelBaseModel
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained('funnel-transformer/small-base')
model = TFFunnelBaseModel.from_pretrained('funnel-transformer/small-base')

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelBaseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small-base&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelBaseModel.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small-base&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),xr=new Pe({}),jr=new ee({props:{name:"class transformers.TFFunnelModel",anchor:"transformers.TFFunnelModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_tf_funnel.py#L1183",parametersDescription:[{anchor:"transformers.TFFunnelModel.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),fo=new ze({props:{$$slots:{default:[j2]},$$scope:{ctx:W}}}),Or=new ee({props:{name:"call",anchor:"transformers.TFFunnelModel.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_tf_funnel.py#L1188",parametersDescription:[{anchor:"transformers.TFFunnelModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This
argument can be used in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>TFBaseModelOutput</a> or a tuple of
<code>tf.Tensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),mo=new ze({props:{$$slots:{default:[L2]},$$scope:{ctx:W}}}),Nr=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelModel
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained('funnel-transformer/small')
model = TFFunnelModel.from_pretrained('funnel-transformer/small')

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelModel.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Br=new Pe({}),Wr=new ee({props:{name:"class transformers.TFFunnelForPreTraining",anchor:"transformers.TFFunnelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_tf_funnel.py#L1246",parametersDescription:[{anchor:"transformers.TFFunnelForPreTraining.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),_o=new ze({props:{$$slots:{default:[A2]},$$scope:{ctx:W}}}),Ur=new ee({props:{name:"call",anchor:"transformers.TFFunnelForPreTraining.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_tf_funnel.py#L1253",parametersDescription:[{anchor:"transformers.TFFunnelForPreTraining.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForPreTraining.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForPreTraining.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForPreTraining.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForPreTraining.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForPreTraining.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForPreTraining.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This
argument can be used in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForPreTraining.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/model_doc/funnel#transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput"
>TFFunnelForPreTrainingOutput</a> or a tuple of
<code>tf.Tensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Prediction scores of the head (scores for each token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/model_doc/funnel#transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput"
>TFFunnelForPreTrainingOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),To=new ze({props:{$$slots:{default:[D2]},$$scope:{ctx:W}}}),Gr=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForPreTraining
import torch

tokenizer = TFFunnelTokenizer.from_pretrained('funnel-transformer/small')
model = TFFunnelForPreTraining.from_pretrained('funnel-transformer/small')

inputs = tokenizer("Hello, my dog is cute", return_tensors= "tf")
logits = model(inputs).logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = TFFunnelTokenizer.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForPreTraining.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors= <span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(inputs).logits`}}),Zr=new Pe({}),Kr=new ee({props:{name:"class transformers.TFFunnelForMaskedLM",anchor:"transformers.TFFunnelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_tf_funnel.py#L1324",parametersDescription:[{anchor:"transformers.TFFunnelForMaskedLM.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),vo=new ze({props:{$$slots:{default:[I2]},$$scope:{ctx:W}}}),sa=new ee({props:{name:"call",anchor:"transformers.TFFunnelForMaskedLM.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_tf_funnel.py#L1338",parametersDescription:[{anchor:"transformers.TFFunnelForMaskedLM.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForMaskedLM.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForMaskedLM.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForMaskedLM.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForMaskedLM.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForMaskedLM.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForMaskedLM.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This
argument can be used in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForMaskedLM.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForMaskedLM.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>TFMaskedLMOutput</a> or a tuple of
<code>tf.Tensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>TFMaskedLMOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),ko=new ze({props:{$$slots:{default:[S2]},$$scope:{ctx:W}}}),ra=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForMaskedLM
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained('funnel-transformer/small')
model = TFFunnelForMaskedLM.from_pretrained('funnel-transformer/small')

inputs = tokenizer("The capital of France is [MASK].", return_tensors="tf")
inputs["labels"] = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),aa=new Pe({}),ia=new ee({props:{name:"class transformers.TFFunnelForSequenceClassification",anchor:"transformers.TFFunnelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_tf_funnel.py#L1419",parametersDescription:[{anchor:"transformers.TFFunnelForSequenceClassification.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),bo=new ze({props:{$$slots:{default:[O2]},$$scope:{ctx:W}}}),ha=new ee({props:{name:"call",anchor:"transformers.TFFunnelForSequenceClassification.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_tf_funnel.py#L1427",parametersDescription:[{anchor:"transformers.TFFunnelForSequenceClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForSequenceClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForSequenceClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForSequenceClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForSequenceClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForSequenceClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForSequenceClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This
argument can be used in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForSequenceClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForSequenceClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss),
If <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>TFSequenceClassifierOutput</a> or a tuple of
<code>tf.Tensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>TFSequenceClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),yo=new ze({props:{$$slots:{default:[N2]},$$scope:{ctx:W}}}),fa=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForSequenceClassification
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained('funnel-transformer/small-base')
model = TFFunnelForSequenceClassification.from_pretrained('funnel-transformer/small-base')

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
inputs["labels"] = tf.reshape(tf.constant(1), (-1, 1)) # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small-base&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small-base&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(tf.constant(<span class="hljs-number">1</span>), (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)) <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ma=new Pe({}),ga=new ee({props:{name:"class transformers.TFFunnelForMultipleChoice",anchor:"transformers.TFFunnelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_tf_funnel.py#L1509",parametersDescription:[{anchor:"transformers.TFFunnelForMultipleChoice.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Eo=new ze({props:{$$slots:{default:[B2]},$$scope:{ctx:W}}}),wa=new ee({props:{name:"call",anchor:"transformers.TFFunnelForMultipleChoice.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_tf_funnel.py#L1526",parametersDescription:[{anchor:"transformers.TFFunnelForMultipleChoice.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForMultipleChoice.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForMultipleChoice.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForMultipleChoice.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForMultipleChoice.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForMultipleChoice.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForMultipleChoice.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This
argument can be used in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForMultipleChoice.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForMultipleChoice.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices]</code> where <code>num_choices</code> is the size of the second dimension of the input tensors. (See
<code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>TFMultipleChoiceModelOutput</a> or a tuple of
<code>tf.Tensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <em>(batch_size, )</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>TFMultipleChoiceModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Mo=new ze({props:{$$slots:{default:[W2]},$$scope:{ctx:W}}}),ba=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForMultipleChoice
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained('funnel-transformer/small-base')
model = TFFunnelForMultipleChoice.from_pretrained('funnel-transformer/small-base')

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors='tf', padding=True)
inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}
outputs = model(inputs)  # batch size is 1

# the linear classifier still needs to be trained
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small-base&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForMultipleChoice.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small-base&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&#x27;tf&#x27;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {k: tf.expand_dims(v, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ya=new Pe({}),$a=new ee({props:{name:"class transformers.TFFunnelForTokenClassification",anchor:"transformers.TFFunnelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_tf_funnel.py#L1645",parametersDescription:[{anchor:"transformers.TFFunnelForTokenClassification.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Po=new ze({props:{$$slots:{default:[Q2]},$$scope:{ctx:W}}}),Ca=new ee({props:{name:"call",anchor:"transformers.TFFunnelForTokenClassification.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_tf_funnel.py#L1656",parametersDescription:[{anchor:"transformers.TFFunnelForTokenClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForTokenClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForTokenClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForTokenClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForTokenClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForTokenClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForTokenClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This
argument can be used in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForTokenClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForTokenClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>TFTokenClassifierOutput</a> or a tuple of
<code>tf.Tensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of unmasked labels, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>TFTokenClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),qo=new ze({props:{$$slots:{default:[R2]},$$scope:{ctx:W}}}),xa=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForTokenClassification
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained('funnel-transformer/small')
model = TFFunnelForTokenClassification.from_pretrained('funnel-transformer/small')

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
input_ids = inputs["input_ids"]
inputs["labels"] = tf.reshape(tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids))) # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForTokenClassification.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(tf.constant([<span class="hljs-number">1</span>] * tf.size(input_ids).numpy()), (-<span class="hljs-number">1</span>, tf.size(input_ids))) <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ja=new Pe({}),La=new ee({props:{name:"class transformers.TFFunnelForQuestionAnswering",anchor:"transformers.TFFunnelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_tf_funnel.py#L1738",parametersDescription:[{anchor:"transformers.TFFunnelForQuestionAnswering.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),xo=new ze({props:{$$slots:{default:[H2]},$$scope:{ctx:W}}}),Na=new ee({props:{name:"call",anchor:"transformers.TFFunnelForQuestionAnswering.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/funnel/modeling_tf_funnel.py#L1748",parametersDescription:[{anchor:"transformers.TFFunnelForQuestionAnswering.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This
argument can be used in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.start_positions",description:`<strong>start_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the
sequence are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.end_positions",description:`<strong>end_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the
sequence are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>TFQuestionAnsweringModelOutput</a> or a tuple of
<code>tf.Tensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>start_positions</code> and <code>end_positions</code> are provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>TFQuestionAnsweringModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),jo=new ze({props:{$$slots:{default:[V2]},$$scope:{ctx:W}}}),Ba=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForQuestionAnswering
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained('funnel-transformer/small')
model = TFFunnelForQuestionAnswering.from_pretrained('funnel-transformer/small')

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
input_dict = tokenizer(question, text, return_tensors='tf')
outputs = model(input_dict)
start_logits = outputs.start_logits
end_logits = outputs.end_logits

all_tokens = tokenizer.convert_ids_to_tokens(input_dict["input_ids"].numpy()[0])
answer = ' '.join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0]+1]),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;funnel-transformer/small&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_dict = tokenizer(question, text, return_tensors=<span class="hljs-string">&#x27;tf&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_dict)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits

<span class="hljs-meta">&gt;&gt;&gt; </span>all_tokens = tokenizer.convert_ids_to_tokens(input_dict[<span class="hljs-string">&quot;input_ids&quot;</span>].numpy()[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>answer = <span class="hljs-string">&#x27; &#x27;</span>.join(all_tokens[tf.math.argmax(start_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] : tf.math.argmax(end_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]+<span class="hljs-number">1</span>])`}}),{c(){p=s("meta"),M=l(),m=s("h1"),g=s("a"),F=s("span"),k(T.$$.fragment),_=l(),z=s("span"),ce=t("Funnel Transformer"),G=l(),P=s("h2"),X=s("a"),I=s("span"),k(ne.$$.fragment),ue=l(),S=s("span"),pe=t("Overview"),ie=l(),U=s("p"),L=t("The Funnel Transformer model was proposed in the paper "),te=s("a"),Z=t(`Funnel-Transformer: Filtering out Sequential Redundancy for
Efficient Language Processing`),q=t(`. It is a bidirectional transformer model, like
BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks
(CNN) in computer vision.`),x=l(),oe=s("p"),Q=t("The abstract from the paper is the following:"),le=l(),se=s("p"),O=s("em"),he=t(`With the success of language pretraining, it is highly desirable to develop more efficient architectures of good
scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the
much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only
require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which
gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More
importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further
improve the model capacity. In addition, to perform token-level predictions as required by common pretraining
objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence
via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on
a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading
comprehension.`),de=l(),C=s("p"),fe=t("Tips:"),B=l(),J=s("ul"),ae=s("li"),R=t(`Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers.
The base model therefore has a final sequence length that is a quarter of the original one. This model can be used
directly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other
tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same
sequence length as the input.`),me=l(),N=s("li"),A=t(`The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be
used for `),re=s("a"),H=t("FunnelModel"),ge=t(", "),u=s("a"),v=t("FunnelForPreTraining"),K=t(`,
`),Te=s("a"),we=t("FunnelForMaskedLM"),D=t(", "),Fe=s("a"),be=t("FunnelForTokenClassification"),ye=t(` and
class:`),j=s("em"),V=t("~transformers.FunnelForQuestionAnswering"),$e=t(`. The second ones should be used for
`),ve=s("a"),Y=t("FunnelBaseModel"),Ee=t(", "),ke=s("a"),_e=t("FunnelForSequenceClassification"),Me=t(` and
`),Va=s("a"),Dp=t("FunnelForMultipleChoice"),Ip=t("."),yc=l(),jn=s("p"),Sp=t("This model was contributed by "),Io=s("a"),Op=t("sgugger"),Np=t(". The original code can be found "),So=s("a"),Bp=t("here"),Wp=t("."),$c=l(),Kn=s("h2"),Nt=s("a"),ll=s("span"),k(Oo.$$.fragment),Qp=l(),dl=s("span"),Rp=t("FunnelConfig"),Ec=l(),Cn=s("div"),k(No.$$.fragment),Hp=l(),xn=s("p"),Vp=t("This is the configuration class to store the configuration of a "),Ya=s("a"),Yp=t("FunnelModel"),Up=t(` or a
`),Ua=s("a"),Gp=t("TFBertModel"),Zp=t(`. It is used to instantiate a Funnel Transformer model according to the specified
arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar
configuration to that of the Funnel Transformer `),Bo=s("a"),Kp=t("funnel-transformer/small"),Xp=t(" architecture."),Jp=l(),Xn=s("p"),eh=t("Configuration objects inherit from "),Ga=s("a"),nh=t("PretrainedConfig"),th=t(` and can be used to control the model
outputs. Read the documentation from `),Za=s("a"),oh=t("PretrainedConfig"),sh=t(" for more information."),Mc=l(),Jn=s("h2"),Bt=s("a"),cl=s("span"),k(Wo.$$.fragment),rh=l(),ul=s("span"),ah=t("FunnelTokenizer"),zc=l(),qe=s("div"),k(Qo.$$.fragment),ih=l(),pl=s("p"),lh=t("Construct a Funnel Transformer tokenizer."),dh=l(),Wt=s("p"),Ka=s("a"),ch=t("FunnelTokenizer"),uh=t(" is identical to "),Xa=s("a"),ph=t("BertTokenizer"),hh=t(` and runs end-to-end
tokenization: punctuation splitting and wordpiece.`),fh=l(),Ro=s("p"),mh=t("Refer to superclass "),Ja=s("a"),gh=t("BertTokenizer"),_h=t(` for usage examples and documentation concerning
parameters.`),Th=l(),Ln=s("div"),k(Ho.$$.fragment),Fh=l(),hl=s("p"),vh=t(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),kh=l(),Vo=s("ul"),ei=s("li"),wh=t("single sequence: "),fl=s("code"),bh=t("[CLS] X [SEP]"),yh=l(),ni=s("li"),$h=t("pair of sequences: "),ml=s("code"),Eh=t("[CLS] A [SEP] B [SEP]"),Mh=l(),Qt=s("div"),k(Yo.$$.fragment),zh=l(),Uo=s("p"),Ph=t(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),gl=s("code"),qh=t("prepare_for_model"),Ch=t(" method."),xh=l(),wn=s("div"),k(Go.$$.fragment),jh=l(),_l=s("p"),Lh=t(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),Ah=l(),k(Zo.$$.fragment),Dh=l(),et=s("p"),Ih=t("If "),Tl=s("code"),Sh=t("token_ids_1"),Oh=t(" is "),Fl=s("code"),Nh=t("None"),Bh=t(", this method only returns the first portion of the mask (0s)."),Wh=l(),vl=s("div"),Pc=l(),nt=s("h2"),Rt=s("a"),kl=s("span"),k(Ko.$$.fragment),Qh=l(),wl=s("span"),Rh=t("FunnelTokenizerFast"),qc=l(),Ze=s("div"),k(Xo.$$.fragment),Hh=l(),Jo=s("p"),Vh=t("Construct a \u201Cfast\u201D Funnel Transformer tokenizer (backed by HuggingFace\u2019s "),bl=s("em"),Yh=t("tokenizers"),Uh=t(" library)."),Gh=l(),Ht=s("p"),ti=s("a"),Zh=t("FunnelTokenizerFast"),Kh=t(" is identical to "),oi=s("a"),Xh=t("BertTokenizerFast"),Jh=t(` and runs
end-to-end tokenization: punctuation splitting and wordpiece.`),ef=l(),es=s("p"),nf=t("Refer to superclass "),si=s("a"),tf=t("BertTokenizerFast"),of=t(` for usage examples and documentation concerning
parameters.`),sf=l(),bn=s("div"),k(ns.$$.fragment),rf=l(),yl=s("p"),af=t(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),lf=l(),k(ts.$$.fragment),df=l(),tt=s("p"),cf=t("If "),$l=s("code"),uf=t("token_ids_1"),pf=t(" is "),El=s("code"),hf=t("None"),ff=t(", this method only returns the first portion of the mask (0s)."),Cc=l(),ot=s("h2"),Vt=s("a"),Ml=s("span"),k(os.$$.fragment),mf=l(),zl=s("span"),gf=t("Funnel specific outputs"),xc=l(),st=s("div"),k(ss.$$.fragment),_f=l(),rs=s("p"),Tf=t("Output type of "),ri=s("a"),Ff=t("FunnelForPreTraining"),vf=t("."),jc=l(),rt=s("div"),k(as.$$.fragment),kf=l(),is=s("p"),wf=t("Output type of "),ai=s("a"),bf=t("FunnelForPreTraining"),yf=t("."),Lc=l(),at=s("h2"),Yt=s("a"),Pl=s("span"),k(ls.$$.fragment),$f=l(),ql=s("span"),Ef=t("FunnelBaseModel"),Ac=l(),We=s("div"),k(ds.$$.fragment),Mf=l(),Cl=s("p"),zf=t(`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),Pf=l(),cs=s("p"),qf=t("The Funnel Transformer model was proposed in "),us=s("a"),Cf=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),xf=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),jf=l(),ps=s("p"),Lf=t("This model inherits from "),ii=s("a"),Af=t("PreTrainedModel"),Df=t(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),If=l(),hs=s("p"),Sf=t("This model is also a PyTorch "),fs=s("a"),Of=t("torch.nn.Module"),Nf=t(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),Bf=l(),Ke=s("div"),k(ms.$$.fragment),Wf=l(),it=s("p"),Qf=t("The "),li=s("a"),Rf=t("FunnelBaseModel"),Hf=t(" forward method, overrides the "),xl=s("code"),Vf=t("__call__"),Yf=t(" special method."),Uf=l(),k(Ut.$$.fragment),Gf=l(),jl=s("p"),Zf=t("Example:"),Kf=l(),k(gs.$$.fragment),Dc=l(),lt=s("h2"),Gt=s("a"),Ll=s("span"),k(_s.$$.fragment),Xf=l(),Al=s("span"),Jf=t("FunnelModel"),Ic=l(),Qe=s("div"),k(Ts.$$.fragment),em=l(),Dl=s("p"),nm=t("The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),tm=l(),Fs=s("p"),om=t("The Funnel Transformer model was proposed in "),vs=s("a"),sm=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),rm=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),am=l(),ks=s("p"),im=t("This model inherits from "),di=s("a"),lm=t("PreTrainedModel"),dm=t(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),cm=l(),ws=s("p"),um=t("This model is also a PyTorch "),bs=s("a"),pm=t("torch.nn.Module"),hm=t(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),fm=l(),Xe=s("div"),k(ys.$$.fragment),mm=l(),dt=s("p"),gm=t("The "),ci=s("a"),_m=t("FunnelModel"),Tm=t(" forward method, overrides the "),Il=s("code"),Fm=t("__call__"),vm=t(" special method."),km=l(),k(Zt.$$.fragment),wm=l(),Sl=s("p"),bm=t("Example:"),ym=l(),k($s.$$.fragment),Sc=l(),ct=s("h2"),Kt=s("a"),Ol=s("span"),k(Es.$$.fragment),$m=l(),Nl=s("span"),Em=t("FunnelModelForPreTraining"),Oc=l(),Ms=s("div"),Je=s("div"),k(zs.$$.fragment),Mm=l(),ut=s("p"),zm=t("The "),ui=s("a"),Pm=t("FunnelForPreTraining"),qm=t(" forward method, overrides the "),Bl=s("code"),Cm=t("__call__"),xm=t(" special method."),jm=l(),k(Xt.$$.fragment),Lm=l(),Wl=s("p"),Am=t("Examples:"),Dm=l(),k(Ps.$$.fragment),Nc=l(),pt=s("h2"),Jt=s("a"),Ql=s("span"),k(qs.$$.fragment),Im=l(),Rl=s("span"),Sm=t("FunnelForMaskedLM"),Bc=l(),Re=s("div"),k(Cs.$$.fragment),Om=l(),xs=s("p"),Nm=t("Funnel Transformer Model with a "),Hl=s("em"),Bm=t("language modeling"),Wm=t(" head on top."),Qm=l(),js=s("p"),Rm=t("The Funnel Transformer model was proposed in "),Ls=s("a"),Hm=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Vm=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Ym=l(),As=s("p"),Um=t("This model inherits from "),pi=s("a"),Gm=t("PreTrainedModel"),Zm=t(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Km=l(),Ds=s("p"),Xm=t("This model is also a PyTorch "),Is=s("a"),Jm=t("torch.nn.Module"),eg=t(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),ng=l(),en=s("div"),k(Ss.$$.fragment),tg=l(),ht=s("p"),og=t("The "),hi=s("a"),sg=t("FunnelForMaskedLM"),rg=t(" forward method, overrides the "),Vl=s("code"),ag=t("__call__"),ig=t(" special method."),lg=l(),k(eo.$$.fragment),dg=l(),Yl=s("p"),cg=t("Example:"),ug=l(),k(Os.$$.fragment),Wc=l(),ft=s("h2"),no=s("a"),Ul=s("span"),k(Ns.$$.fragment),pg=l(),Gl=s("span"),hg=t("FunnelForSequenceClassification"),Qc=l(),He=s("div"),k(Bs.$$.fragment),fg=l(),Zl=s("p"),mg=t(`Funnel Transformer Model with a sequence classification/regression head on top (two linear layer on top of the
first timestep of the last hidden state) e.g. for GLUE tasks.`),gg=l(),Ws=s("p"),_g=t("The Funnel Transformer model was proposed in "),Qs=s("a"),Tg=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Fg=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),vg=l(),Rs=s("p"),kg=t("This model inherits from "),fi=s("a"),wg=t("PreTrainedModel"),bg=t(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),yg=l(),Hs=s("p"),$g=t("This model is also a PyTorch "),Vs=s("a"),Eg=t("torch.nn.Module"),Mg=t(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),zg=l(),Be=s("div"),k(Ys.$$.fragment),Pg=l(),mt=s("p"),qg=t("The "),mi=s("a"),Cg=t("FunnelForSequenceClassification"),xg=t(" forward method, overrides the "),Kl=s("code"),jg=t("__call__"),Lg=t(" special method."),Ag=l(),k(to.$$.fragment),Dg=l(),Xl=s("p"),Ig=t("Example of single-label classification:"),Sg=l(),k(Us.$$.fragment),Og=l(),Jl=s("p"),Ng=t("Example of multi-label classification:"),Bg=l(),k(Gs.$$.fragment),Rc=l(),gt=s("h2"),oo=s("a"),ed=s("span"),k(Zs.$$.fragment),Wg=l(),nd=s("span"),Qg=t("FunnelForMultipleChoice"),Hc=l(),Ve=s("div"),k(Ks.$$.fragment),Rg=l(),td=s("p"),Hg=t(`Funnel Transformer Model with a multiple choice classification head on top (two linear layer on top of the first
timestep of the last hidden state, and a softmax) e.g. for RocStories/SWAG tasks.`),Vg=l(),Xs=s("p"),Yg=t("The Funnel Transformer model was proposed in "),Js=s("a"),Ug=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Gg=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Zg=l(),er=s("p"),Kg=t("This model inherits from "),gi=s("a"),Xg=t("PreTrainedModel"),Jg=t(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),e_=l(),nr=s("p"),n_=t("This model is also a PyTorch "),tr=s("a"),t_=t("torch.nn.Module"),o_=t(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),s_=l(),nn=s("div"),k(or.$$.fragment),r_=l(),_t=s("p"),a_=t("The "),_i=s("a"),i_=t("FunnelForMultipleChoice"),l_=t(" forward method, overrides the "),od=s("code"),d_=t("__call__"),c_=t(" special method."),u_=l(),k(so.$$.fragment),p_=l(),sd=s("p"),h_=t("Example:"),f_=l(),k(sr.$$.fragment),Vc=l(),Tt=s("h2"),ro=s("a"),rd=s("span"),k(rr.$$.fragment),m_=l(),ad=s("span"),g_=t("FunnelForTokenClassification"),Yc=l(),Ye=s("div"),k(ar.$$.fragment),__=l(),id=s("p"),T_=t(`Funnel Transformer Model with a token classification head on top (a linear layer on top of the hidden-states
output) e.g. for Named-Entity-Recognition (NER) tasks.`),F_=l(),ir=s("p"),v_=t("The Funnel Transformer model was proposed in "),lr=s("a"),k_=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),w_=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),b_=l(),dr=s("p"),y_=t("This model inherits from "),Ti=s("a"),$_=t("PreTrainedModel"),E_=t(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),M_=l(),cr=s("p"),z_=t("This model is also a PyTorch "),ur=s("a"),P_=t("torch.nn.Module"),q_=t(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),C_=l(),tn=s("div"),k(pr.$$.fragment),x_=l(),Ft=s("p"),j_=t("The "),Fi=s("a"),L_=t("FunnelForTokenClassification"),A_=t(" forward method, overrides the "),ld=s("code"),D_=t("__call__"),I_=t(" special method."),S_=l(),k(ao.$$.fragment),O_=l(),dd=s("p"),N_=t("Example:"),B_=l(),k(hr.$$.fragment),Uc=l(),vt=s("h2"),io=s("a"),cd=s("span"),k(fr.$$.fragment),W_=l(),ud=s("span"),Q_=t("FunnelForQuestionAnswering"),Gc=l(),Ue=s("div"),k(mr.$$.fragment),R_=l(),kt=s("p"),H_=t(`Funnel Transformer Model with a span classification head on top for extractive question-answering tasks like SQuAD
(a linear layer on top of the hidden-states output to compute `),pd=s("em"),V_=t("span start logits"),Y_=t(" and "),hd=s("em"),U_=t("span end logits"),G_=t(")."),Z_=l(),gr=s("p"),K_=t("The Funnel Transformer model was proposed in "),_r=s("a"),X_=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),J_=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),eT=l(),Tr=s("p"),nT=t("This model inherits from "),vi=s("a"),tT=t("PreTrainedModel"),oT=t(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),sT=l(),Fr=s("p"),rT=t("This model is also a PyTorch "),vr=s("a"),aT=t("torch.nn.Module"),iT=t(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),lT=l(),on=s("div"),k(kr.$$.fragment),dT=l(),wt=s("p"),cT=t("The "),ki=s("a"),uT=t("FunnelForQuestionAnswering"),pT=t(" forward method, overrides the "),fd=s("code"),hT=t("__call__"),fT=t(" special method."),mT=l(),k(lo.$$.fragment),gT=l(),md=s("p"),_T=t("Example:"),TT=l(),k(wr.$$.fragment),Zc=l(),bt=s("h2"),co=s("a"),gd=s("span"),k(br.$$.fragment),FT=l(),_d=s("span"),vT=t("TFFunnelBaseModel"),Kc=l(),xe=s("div"),k(yr.$$.fragment),kT=l(),Td=s("p"),wT=t(`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),bT=l(),$r=s("p"),yT=t("The Funnel Transformer model was proposed in "),Er=s("a"),$T=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),ET=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),MT=l(),Mr=s("p"),zT=t("This model inherits from "),wi=s("a"),PT=t("TFPreTrainedModel"),qT=t(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),CT=l(),zr=s("p"),xT=t("This model is also a "),Pr=s("a"),jT=t("tf.keras.Model"),LT=t(` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),AT=l(),k(uo.$$.fragment),DT=l(),sn=s("div"),k(qr.$$.fragment),IT=l(),yt=s("p"),ST=t("The "),bi=s("a"),OT=t("TFFunnelBaseModel"),NT=t(" forward method, overrides the "),Fd=s("code"),BT=t("__call__"),WT=t(" special method."),QT=l(),k(po.$$.fragment),RT=l(),vd=s("p"),HT=t("Example:"),VT=l(),k(Cr.$$.fragment),Xc=l(),$t=s("h2"),ho=s("a"),kd=s("span"),k(xr.$$.fragment),YT=l(),wd=s("span"),UT=t("TFFunnelModel"),Jc=l(),je=s("div"),k(jr.$$.fragment),GT=l(),bd=s("p"),ZT=t("The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),KT=l(),Lr=s("p"),XT=t("The Funnel Transformer model was proposed in "),Ar=s("a"),JT=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),eF=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),nF=l(),Dr=s("p"),tF=t("This model inherits from "),yi=s("a"),oF=t("TFPreTrainedModel"),sF=t(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),rF=l(),Ir=s("p"),aF=t("This model is also a "),Sr=s("a"),iF=t("tf.keras.Model"),lF=t(` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),dF=l(),k(fo.$$.fragment),cF=l(),rn=s("div"),k(Or.$$.fragment),uF=l(),Et=s("p"),pF=t("The "),$i=s("a"),hF=t("TFFunnelModel"),fF=t(" forward method, overrides the "),yd=s("code"),mF=t("__call__"),gF=t(" special method."),_F=l(),k(mo.$$.fragment),TF=l(),$d=s("p"),FF=t("Example:"),vF=l(),k(Nr.$$.fragment),eu=l(),Mt=s("h2"),go=s("a"),Ed=s("span"),k(Br.$$.fragment),kF=l(),Md=s("span"),wF=t("TFFunnelModelForPreTraining"),nu=l(),Le=s("div"),k(Wr.$$.fragment),bF=l(),zd=s("p"),yF=t("Funnel model with a binary classification head on top as used during pretraining for identifying generated tokens."),$F=l(),Qr=s("p"),EF=t("The Funnel Transformer model was proposed in "),Rr=s("a"),MF=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),zF=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),PF=l(),Hr=s("p"),qF=t("This model inherits from "),Ei=s("a"),CF=t("TFPreTrainedModel"),xF=t(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),jF=l(),Vr=s("p"),LF=t("This model is also a "),Yr=s("a"),AF=t("tf.keras.Model"),DF=t(` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),IF=l(),k(_o.$$.fragment),SF=l(),an=s("div"),k(Ur.$$.fragment),OF=l(),zt=s("p"),NF=t("The "),Mi=s("a"),BF=t("TFFunnelForPreTraining"),WF=t(" forward method, overrides the "),Pd=s("code"),QF=t("__call__"),RF=t(" special method."),HF=l(),k(To.$$.fragment),VF=l(),qd=s("p"),YF=t("Examples:"),UF=l(),k(Gr.$$.fragment),tu=l(),Pt=s("h2"),Fo=s("a"),Cd=s("span"),k(Zr.$$.fragment),GF=l(),xd=s("span"),ZF=t("TFFunnelForMaskedLM"),ou=l(),Ae=s("div"),k(Kr.$$.fragment),KF=l(),Xr=s("p"),XF=t("Funnel Model with a "),jd=s("em"),JF=t("language modeling"),ev=t(" head on top."),nv=l(),Jr=s("p"),tv=t("The Funnel Transformer model was proposed in "),ea=s("a"),ov=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),sv=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),rv=l(),na=s("p"),av=t("This model inherits from "),zi=s("a"),iv=t("TFPreTrainedModel"),lv=t(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),dv=l(),ta=s("p"),cv=t("This model is also a "),oa=s("a"),uv=t("tf.keras.Model"),pv=t(` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),hv=l(),k(vo.$$.fragment),fv=l(),ln=s("div"),k(sa.$$.fragment),mv=l(),qt=s("p"),gv=t("The "),Pi=s("a"),_v=t("TFFunnelForMaskedLM"),Tv=t(" forward method, overrides the "),Ld=s("code"),Fv=t("__call__"),vv=t(" special method."),kv=l(),k(ko.$$.fragment),wv=l(),Ad=s("p"),bv=t("Example:"),yv=l(),k(ra.$$.fragment),su=l(),Ct=s("h2"),wo=s("a"),Dd=s("span"),k(aa.$$.fragment),$v=l(),Id=s("span"),Ev=t("TFFunnelForSequenceClassification"),ru=l(),De=s("div"),k(ia.$$.fragment),Mv=l(),Sd=s("p"),zv=t(`Funnel Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
output) e.g. for GLUE tasks.`),Pv=l(),la=s("p"),qv=t("The Funnel Transformer model was proposed in "),da=s("a"),Cv=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),xv=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),jv=l(),ca=s("p"),Lv=t("This model inherits from "),qi=s("a"),Av=t("TFPreTrainedModel"),Dv=t(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),Iv=l(),ua=s("p"),Sv=t("This model is also a "),pa=s("a"),Ov=t("tf.keras.Model"),Nv=t(` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),Bv=l(),k(bo.$$.fragment),Wv=l(),dn=s("div"),k(ha.$$.fragment),Qv=l(),xt=s("p"),Rv=t("The "),Ci=s("a"),Hv=t("TFFunnelForSequenceClassification"),Vv=t(" forward method, overrides the "),Od=s("code"),Yv=t("__call__"),Uv=t(" special method."),Gv=l(),k(yo.$$.fragment),Zv=l(),Nd=s("p"),Kv=t("Example:"),Xv=l(),k(fa.$$.fragment),au=l(),jt=s("h2"),$o=s("a"),Bd=s("span"),k(ma.$$.fragment),Jv=l(),Wd=s("span"),ek=t("TFFunnelForMultipleChoice"),iu=l(),Ie=s("div"),k(ga.$$.fragment),nk=l(),Qd=s("p"),tk=t(`Funnel Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),ok=l(),_a=s("p"),sk=t("The Funnel Transformer model was proposed in "),Ta=s("a"),rk=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),ak=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),ik=l(),Fa=s("p"),lk=t("This model inherits from "),xi=s("a"),dk=t("TFPreTrainedModel"),ck=t(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),uk=l(),va=s("p"),pk=t("This model is also a "),ka=s("a"),hk=t("tf.keras.Model"),fk=t(` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),mk=l(),k(Eo.$$.fragment),gk=l(),cn=s("div"),k(wa.$$.fragment),_k=l(),Lt=s("p"),Tk=t("The "),ji=s("a"),Fk=t("TFFunnelForMultipleChoice"),vk=t(" forward method, overrides the "),Rd=s("code"),kk=t("__call__"),wk=t(" special method."),bk=l(),k(Mo.$$.fragment),yk=l(),Hd=s("p"),$k=t("Example:"),Ek=l(),k(ba.$$.fragment),lu=l(),At=s("h2"),zo=s("a"),Vd=s("span"),k(ya.$$.fragment),Mk=l(),Yd=s("span"),zk=t("TFFunnelForTokenClassification"),du=l(),Se=s("div"),k($a.$$.fragment),Pk=l(),Ud=s("p"),qk=t(`Funnel Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),Ck=l(),Ea=s("p"),xk=t("The Funnel Transformer model was proposed in "),Ma=s("a"),jk=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Lk=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Ak=l(),za=s("p"),Dk=t("This model inherits from "),Li=s("a"),Ik=t("TFPreTrainedModel"),Sk=t(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),Ok=l(),Pa=s("p"),Nk=t("This model is also a "),qa=s("a"),Bk=t("tf.keras.Model"),Wk=t(` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),Qk=l(),k(Po.$$.fragment),Rk=l(),un=s("div"),k(Ca.$$.fragment),Hk=l(),Dt=s("p"),Vk=t("The "),Ai=s("a"),Yk=t("TFFunnelForTokenClassification"),Uk=t(" forward method, overrides the "),Gd=s("code"),Gk=t("__call__"),Zk=t(" special method."),Kk=l(),k(qo.$$.fragment),Xk=l(),Zd=s("p"),Jk=t("Example:"),ew=l(),k(xa.$$.fragment),cu=l(),It=s("h2"),Co=s("a"),Kd=s("span"),k(ja.$$.fragment),nw=l(),Xd=s("span"),tw=t("TFFunnelForQuestionAnswering"),uu=l(),Oe=s("div"),k(La.$$.fragment),ow=l(),St=s("p"),sw=t(`Funnel Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),Jd=s("em"),rw=t("span start logits"),aw=t(" and "),ec=s("em"),iw=t("span end logits"),lw=t(")."),dw=l(),Aa=s("p"),cw=t("The Funnel Transformer model was proposed in "),Da=s("a"),uw=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),pw=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),hw=l(),Ia=s("p"),fw=t("This model inherits from "),Di=s("a"),mw=t("TFPreTrainedModel"),gw=t(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),_w=l(),Sa=s("p"),Tw=t("This model is also a "),Oa=s("a"),Fw=t("tf.keras.Model"),vw=t(` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),kw=l(),k(xo.$$.fragment),ww=l(),pn=s("div"),k(Na.$$.fragment),bw=l(),Ot=s("p"),yw=t("The "),Ii=s("a"),$w=t("TFFunnelForQuestionAnswering"),Ew=t(" forward method, overrides the "),nc=s("code"),Mw=t("__call__"),zw=t(" special method."),Pw=l(),k(jo.$$.fragment),qw=l(),tc=s("p"),Cw=t("Example:"),xw=l(),k(Ba.$$.fragment),this.h()},l(i){const f=w2('[data-svelte="svelte-1phssyn"]',document.head);p=r(f,"META",{name:!0,content:!0}),f.forEach(n),M=d(i),m=r(i,"H1",{class:!0});var Wa=a(m);g=r(Wa,"A",{id:!0,class:!0,href:!0});var oc=a(g);F=r(oc,"SPAN",{});var sc=a(F);w(T.$$.fragment,sc),sc.forEach(n),oc.forEach(n),_=d(Wa),z=r(Wa,"SPAN",{});var rc=a(z);ce=o(rc,"Funnel Transformer"),rc.forEach(n),Wa.forEach(n),G=d(i),P=r(i,"H2",{class:!0});var Qa=a(P);X=r(Qa,"A",{id:!0,class:!0,href:!0});var ac=a(X);I=r(ac,"SPAN",{});var ic=a(I);w(ne.$$.fragment,ic),ic.forEach(n),ac.forEach(n),ue=d(Qa),S=r(Qa,"SPAN",{});var lc=a(S);pe=o(lc,"Overview"),lc.forEach(n),Qa.forEach(n),ie=d(i),U=r(i,"P",{});var Ra=a(U);L=o(Ra,"The Funnel Transformer model was proposed in the paper "),te=r(Ra,"A",{href:!0,rel:!0});var dc=a(te);Z=o(dc,`Funnel-Transformer: Filtering out Sequential Redundancy for
Efficient Language Processing`),dc.forEach(n),q=o(Ra,`. It is a bidirectional transformer model, like
BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks
(CNN) in computer vision.`),Ra.forEach(n),x=d(i),oe=r(i,"P",{});var cc=a(oe);Q=o(cc,"The abstract from the paper is the following:"),cc.forEach(n),le=d(i),se=r(i,"P",{});var uc=a(se);O=r(uc,"EM",{});var pc=a(O);he=o(pc,`With the success of language pretraining, it is highly desirable to develop more efficient architectures of good
scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the
much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only
require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which
gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More
importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further
improve the model capacity. In addition, to perform token-level predictions as required by common pretraining
objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence
via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on
a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading
comprehension.`),pc.forEach(n),uc.forEach(n),de=d(i),C=r(i,"P",{});var hc=a(C);fe=o(hc,"Tips:"),hc.forEach(n),B=d(i),J=r(i,"UL",{});var Ha=a(J);ae=r(Ha,"LI",{});var fc=a(ae);R=o(fc,`Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers.
The base model therefore has a final sequence length that is a quarter of the original one. This model can be used
directly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other
tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same
sequence length as the input.`),fc.forEach(n),me=d(Ha),N=r(Ha,"LI",{});var Ce=a(N);A=o(Ce,`The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be
used for `),re=r(Ce,"A",{href:!0});var mc=a(re);H=o(mc,"FunnelModel"),mc.forEach(n),ge=o(Ce,", "),u=r(Ce,"A",{href:!0});var gc=a(u);v=o(gc,"FunnelForPreTraining"),gc.forEach(n),K=o(Ce,`,
`),Te=r(Ce,"A",{href:!0});var _c=a(Te);we=o(_c,"FunnelForMaskedLM"),_c.forEach(n),D=o(Ce,", "),Fe=r(Ce,"A",{href:!0});var Tc=a(Fe);be=o(Tc,"FunnelForTokenClassification"),Tc.forEach(n),ye=o(Ce,` and
class:`),j=r(Ce,"EM",{});var Fc=a(j);V=o(Fc,"~transformers.FunnelForQuestionAnswering"),Fc.forEach(n),$e=o(Ce,`. The second ones should be used for
`),ve=r(Ce,"A",{href:!0});var vc=a(ve);Y=o(vc,"FunnelBaseModel"),vc.forEach(n),Ee=o(Ce,", "),ke=r(Ce,"A",{href:!0});var kc=a(ke);_e=o(kc,"FunnelForSequenceClassification"),kc.forEach(n),Me=o(Ce,` and
`),Va=r(Ce,"A",{href:!0});var Aw=a(Va);Dp=o(Aw,"FunnelForMultipleChoice"),Aw.forEach(n),Ip=o(Ce,"."),Ce.forEach(n),Ha.forEach(n),yc=d(i),jn=r(i,"P",{});var Si=a(jn);Sp=o(Si,"This model was contributed by "),Io=r(Si,"A",{href:!0,rel:!0});var Dw=a(Io);Op=o(Dw,"sgugger"),Dw.forEach(n),Np=o(Si,". The original code can be found "),So=r(Si,"A",{href:!0,rel:!0});var Iw=a(So);Bp=o(Iw,"here"),Iw.forEach(n),Wp=o(Si,"."),Si.forEach(n),$c=d(i),Kn=r(i,"H2",{class:!0});var hu=a(Kn);Nt=r(hu,"A",{id:!0,class:!0,href:!0});var Sw=a(Nt);ll=r(Sw,"SPAN",{});var Ow=a(ll);w(Oo.$$.fragment,Ow),Ow.forEach(n),Sw.forEach(n),Qp=d(hu),dl=r(hu,"SPAN",{});var Nw=a(dl);Rp=o(Nw,"FunnelConfig"),Nw.forEach(n),hu.forEach(n),Ec=d(i),Cn=r(i,"DIV",{class:!0});var Oi=a(Cn);w(No.$$.fragment,Oi),Hp=d(Oi),xn=r(Oi,"P",{});var Lo=a(xn);Vp=o(Lo,"This is the configuration class to store the configuration of a "),Ya=r(Lo,"A",{href:!0});var Bw=a(Ya);Yp=o(Bw,"FunnelModel"),Bw.forEach(n),Up=o(Lo,` or a
`),Ua=r(Lo,"A",{href:!0});var Ww=a(Ua);Gp=o(Ww,"TFBertModel"),Ww.forEach(n),Zp=o(Lo,`. It is used to instantiate a Funnel Transformer model according to the specified
arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar
configuration to that of the Funnel Transformer `),Bo=r(Lo,"A",{href:!0,rel:!0});var Qw=a(Bo);Kp=o(Qw,"funnel-transformer/small"),Qw.forEach(n),Xp=o(Lo," architecture."),Lo.forEach(n),Jp=d(Oi),Xn=r(Oi,"P",{});var Ni=a(Xn);eh=o(Ni,"Configuration objects inherit from "),Ga=r(Ni,"A",{href:!0});var Rw=a(Ga);nh=o(Rw,"PretrainedConfig"),Rw.forEach(n),th=o(Ni,` and can be used to control the model
outputs. Read the documentation from `),Za=r(Ni,"A",{href:!0});var Hw=a(Za);oh=o(Hw,"PretrainedConfig"),Hw.forEach(n),sh=o(Ni," for more information."),Ni.forEach(n),Oi.forEach(n),Mc=d(i),Jn=r(i,"H2",{class:!0});var fu=a(Jn);Bt=r(fu,"A",{id:!0,class:!0,href:!0});var Vw=a(Bt);cl=r(Vw,"SPAN",{});var Yw=a(cl);w(Wo.$$.fragment,Yw),Yw.forEach(n),Vw.forEach(n),rh=d(fu),ul=r(fu,"SPAN",{});var Uw=a(ul);ah=o(Uw,"FunnelTokenizer"),Uw.forEach(n),fu.forEach(n),zc=d(i),qe=r(i,"DIV",{class:!0});var Ge=a(qe);w(Qo.$$.fragment,Ge),ih=d(Ge),pl=r(Ge,"P",{});var Gw=a(pl);lh=o(Gw,"Construct a Funnel Transformer tokenizer."),Gw.forEach(n),dh=d(Ge),Wt=r(Ge,"P",{});var wc=a(Wt);Ka=r(wc,"A",{href:!0});var Zw=a(Ka);ch=o(Zw,"FunnelTokenizer"),Zw.forEach(n),uh=o(wc," is identical to "),Xa=r(wc,"A",{href:!0});var Kw=a(Xa);ph=o(Kw,"BertTokenizer"),Kw.forEach(n),hh=o(wc,` and runs end-to-end
tokenization: punctuation splitting and wordpiece.`),wc.forEach(n),fh=d(Ge),Ro=r(Ge,"P",{});var mu=a(Ro);mh=o(mu,"Refer to superclass "),Ja=r(mu,"A",{href:!0});var Xw=a(Ja);gh=o(Xw,"BertTokenizer"),Xw.forEach(n),_h=o(mu,` for usage examples and documentation concerning
parameters.`),mu.forEach(n),Th=d(Ge),Ln=r(Ge,"DIV",{class:!0});var Bi=a(Ln);w(Ho.$$.fragment,Bi),Fh=d(Bi),hl=r(Bi,"P",{});var Jw=a(hl);vh=o(Jw,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),Jw.forEach(n),kh=d(Bi),Vo=r(Bi,"UL",{});var gu=a(Vo);ei=r(gu,"LI",{});var jw=a(ei);wh=o(jw,"single sequence: "),fl=r(jw,"CODE",{});var e1=a(fl);bh=o(e1,"[CLS] X [SEP]"),e1.forEach(n),jw.forEach(n),yh=d(gu),ni=r(gu,"LI",{});var Lw=a(ni);$h=o(Lw,"pair of sequences: "),ml=r(Lw,"CODE",{});var n1=a(ml);Eh=o(n1,"[CLS] A [SEP] B [SEP]"),n1.forEach(n),Lw.forEach(n),gu.forEach(n),Bi.forEach(n),Mh=d(Ge),Qt=r(Ge,"DIV",{class:!0});var _u=a(Qt);w(Yo.$$.fragment,_u),zh=d(_u),Uo=r(_u,"P",{});var Tu=a(Uo);Ph=o(Tu,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),gl=r(Tu,"CODE",{});var t1=a(gl);qh=o(t1,"prepare_for_model"),t1.forEach(n),Ch=o(Tu," method."),Tu.forEach(n),_u.forEach(n),xh=d(Ge),wn=r(Ge,"DIV",{class:!0});var Ao=a(wn);w(Go.$$.fragment,Ao),jh=d(Ao),_l=r(Ao,"P",{});var o1=a(_l);Lh=o(o1,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),o1.forEach(n),Ah=d(Ao),w(Zo.$$.fragment,Ao),Dh=d(Ao),et=r(Ao,"P",{});var Wi=a(et);Ih=o(Wi,"If "),Tl=r(Wi,"CODE",{});var s1=a(Tl);Sh=o(s1,"token_ids_1"),s1.forEach(n),Oh=o(Wi," is "),Fl=r(Wi,"CODE",{});var r1=a(Fl);Nh=o(r1,"None"),r1.forEach(n),Bh=o(Wi,", this method only returns the first portion of the mask (0s)."),Wi.forEach(n),Ao.forEach(n),Wh=d(Ge),vl=r(Ge,"DIV",{class:!0}),a(vl).forEach(n),Ge.forEach(n),Pc=d(i),nt=r(i,"H2",{class:!0});var Fu=a(nt);Rt=r(Fu,"A",{id:!0,class:!0,href:!0});var a1=a(Rt);kl=r(a1,"SPAN",{});var i1=a(kl);w(Ko.$$.fragment,i1),i1.forEach(n),a1.forEach(n),Qh=d(Fu),wl=r(Fu,"SPAN",{});var l1=a(wl);Rh=o(l1,"FunnelTokenizerFast"),l1.forEach(n),Fu.forEach(n),qc=d(i),Ze=r(i,"DIV",{class:!0});var An=a(Ze);w(Xo.$$.fragment,An),Hh=d(An),Jo=r(An,"P",{});var vu=a(Jo);Vh=o(vu,"Construct a \u201Cfast\u201D Funnel Transformer tokenizer (backed by HuggingFace\u2019s "),bl=r(vu,"EM",{});var d1=a(bl);Yh=o(d1,"tokenizers"),d1.forEach(n),Uh=o(vu," library)."),vu.forEach(n),Gh=d(An),Ht=r(An,"P",{});var bc=a(Ht);ti=r(bc,"A",{href:!0});var c1=a(ti);Zh=o(c1,"FunnelTokenizerFast"),c1.forEach(n),Kh=o(bc," is identical to "),oi=r(bc,"A",{href:!0});var u1=a(oi);Xh=o(u1,"BertTokenizerFast"),u1.forEach(n),Jh=o(bc,` and runs
end-to-end tokenization: punctuation splitting and wordpiece.`),bc.forEach(n),ef=d(An),es=r(An,"P",{});var ku=a(es);nf=o(ku,"Refer to superclass "),si=r(ku,"A",{href:!0});var p1=a(si);tf=o(p1,"BertTokenizerFast"),p1.forEach(n),of=o(ku,` for usage examples and documentation concerning
parameters.`),ku.forEach(n),sf=d(An),bn=r(An,"DIV",{class:!0});var Do=a(bn);w(ns.$$.fragment,Do),rf=d(Do),yl=r(Do,"P",{});var h1=a(yl);af=o(h1,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),h1.forEach(n),lf=d(Do),w(ts.$$.fragment,Do),df=d(Do),tt=r(Do,"P",{});var Qi=a(tt);cf=o(Qi,"If "),$l=r(Qi,"CODE",{});var f1=a($l);uf=o(f1,"token_ids_1"),f1.forEach(n),pf=o(Qi," is "),El=r(Qi,"CODE",{});var m1=a(El);hf=o(m1,"None"),m1.forEach(n),ff=o(Qi,", this method only returns the first portion of the mask (0s)."),Qi.forEach(n),Do.forEach(n),An.forEach(n),Cc=d(i),ot=r(i,"H2",{class:!0});var wu=a(ot);Vt=r(wu,"A",{id:!0,class:!0,href:!0});var g1=a(Vt);Ml=r(g1,"SPAN",{});var _1=a(Ml);w(os.$$.fragment,_1),_1.forEach(n),g1.forEach(n),mf=d(wu),zl=r(wu,"SPAN",{});var T1=a(zl);gf=o(T1,"Funnel specific outputs"),T1.forEach(n),wu.forEach(n),xc=d(i),st=r(i,"DIV",{class:!0});var bu=a(st);w(ss.$$.fragment,bu),_f=d(bu),rs=r(bu,"P",{});var yu=a(rs);Tf=o(yu,"Output type of "),ri=r(yu,"A",{href:!0});var F1=a(ri);Ff=o(F1,"FunnelForPreTraining"),F1.forEach(n),vf=o(yu,"."),yu.forEach(n),bu.forEach(n),jc=d(i),rt=r(i,"DIV",{class:!0});var $u=a(rt);w(as.$$.fragment,$u),kf=d($u),is=r($u,"P",{});var Eu=a(is);wf=o(Eu,"Output type of "),ai=r(Eu,"A",{href:!0});var v1=a(ai);bf=o(v1,"FunnelForPreTraining"),v1.forEach(n),yf=o(Eu,"."),Eu.forEach(n),$u.forEach(n),Lc=d(i),at=r(i,"H2",{class:!0});var Mu=a(at);Yt=r(Mu,"A",{id:!0,class:!0,href:!0});var k1=a(Yt);Pl=r(k1,"SPAN",{});var w1=a(Pl);w(ls.$$.fragment,w1),w1.forEach(n),k1.forEach(n),$f=d(Mu),ql=r(Mu,"SPAN",{});var b1=a(ql);Ef=o(b1,"FunnelBaseModel"),b1.forEach(n),Mu.forEach(n),Ac=d(i),We=r(i,"DIV",{class:!0});var yn=a(We);w(ds.$$.fragment,yn),Mf=d(yn),Cl=r(yn,"P",{});var y1=a(Cl);zf=o(y1,`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),y1.forEach(n),Pf=d(yn),cs=r(yn,"P",{});var zu=a(cs);qf=o(zu,"The Funnel Transformer model was proposed in "),us=r(zu,"A",{href:!0,rel:!0});var $1=a(us);Cf=o($1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),$1.forEach(n),xf=o(zu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),zu.forEach(n),jf=d(yn),ps=r(yn,"P",{});var Pu=a(ps);Lf=o(Pu,"This model inherits from "),ii=r(Pu,"A",{href:!0});var E1=a(ii);Af=o(E1,"PreTrainedModel"),E1.forEach(n),Df=o(Pu,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Pu.forEach(n),If=d(yn),hs=r(yn,"P",{});var qu=a(hs);Sf=o(qu,"This model is also a PyTorch "),fs=r(qu,"A",{href:!0,rel:!0});var M1=a(fs);Of=o(M1,"torch.nn.Module"),M1.forEach(n),Nf=o(qu,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),qu.forEach(n),Bf=d(yn),Ke=r(yn,"DIV",{class:!0});var Dn=a(Ke);w(ms.$$.fragment,Dn),Wf=d(Dn),it=r(Dn,"P",{});var Ri=a(it);Qf=o(Ri,"The "),li=r(Ri,"A",{href:!0});var z1=a(li);Rf=o(z1,"FunnelBaseModel"),z1.forEach(n),Hf=o(Ri," forward method, overrides the "),xl=r(Ri,"CODE",{});var P1=a(xl);Vf=o(P1,"__call__"),P1.forEach(n),Yf=o(Ri," special method."),Ri.forEach(n),Uf=d(Dn),w(Ut.$$.fragment,Dn),Gf=d(Dn),jl=r(Dn,"P",{});var q1=a(jl);Zf=o(q1,"Example:"),q1.forEach(n),Kf=d(Dn),w(gs.$$.fragment,Dn),Dn.forEach(n),yn.forEach(n),Dc=d(i),lt=r(i,"H2",{class:!0});var Cu=a(lt);Gt=r(Cu,"A",{id:!0,class:!0,href:!0});var C1=a(Gt);Ll=r(C1,"SPAN",{});var x1=a(Ll);w(_s.$$.fragment,x1),x1.forEach(n),C1.forEach(n),Xf=d(Cu),Al=r(Cu,"SPAN",{});var j1=a(Al);Jf=o(j1,"FunnelModel"),j1.forEach(n),Cu.forEach(n),Ic=d(i),Qe=r(i,"DIV",{class:!0});var $n=a(Qe);w(Ts.$$.fragment,$n),em=d($n),Dl=r($n,"P",{});var L1=a(Dl);nm=o(L1,"The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),L1.forEach(n),tm=d($n),Fs=r($n,"P",{});var xu=a(Fs);om=o(xu,"The Funnel Transformer model was proposed in "),vs=r(xu,"A",{href:!0,rel:!0});var A1=a(vs);sm=o(A1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),A1.forEach(n),rm=o(xu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),xu.forEach(n),am=d($n),ks=r($n,"P",{});var ju=a(ks);im=o(ju,"This model inherits from "),di=r(ju,"A",{href:!0});var D1=a(di);lm=o(D1,"PreTrainedModel"),D1.forEach(n),dm=o(ju,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),ju.forEach(n),cm=d($n),ws=r($n,"P",{});var Lu=a(ws);um=o(Lu,"This model is also a PyTorch "),bs=r(Lu,"A",{href:!0,rel:!0});var I1=a(bs);pm=o(I1,"torch.nn.Module"),I1.forEach(n),hm=o(Lu,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),Lu.forEach(n),fm=d($n),Xe=r($n,"DIV",{class:!0});var In=a(Xe);w(ys.$$.fragment,In),mm=d(In),dt=r(In,"P",{});var Hi=a(dt);gm=o(Hi,"The "),ci=r(Hi,"A",{href:!0});var S1=a(ci);_m=o(S1,"FunnelModel"),S1.forEach(n),Tm=o(Hi," forward method, overrides the "),Il=r(Hi,"CODE",{});var O1=a(Il);Fm=o(O1,"__call__"),O1.forEach(n),vm=o(Hi," special method."),Hi.forEach(n),km=d(In),w(Zt.$$.fragment,In),wm=d(In),Sl=r(In,"P",{});var N1=a(Sl);bm=o(N1,"Example:"),N1.forEach(n),ym=d(In),w($s.$$.fragment,In),In.forEach(n),$n.forEach(n),Sc=d(i),ct=r(i,"H2",{class:!0});var Au=a(ct);Kt=r(Au,"A",{id:!0,class:!0,href:!0});var B1=a(Kt);Ol=r(B1,"SPAN",{});var W1=a(Ol);w(Es.$$.fragment,W1),W1.forEach(n),B1.forEach(n),$m=d(Au),Nl=r(Au,"SPAN",{});var Q1=a(Nl);Em=o(Q1,"FunnelModelForPreTraining"),Q1.forEach(n),Au.forEach(n),Oc=d(i),Ms=r(i,"DIV",{class:!0});var R1=a(Ms);Je=r(R1,"DIV",{class:!0});var Sn=a(Je);w(zs.$$.fragment,Sn),Mm=d(Sn),ut=r(Sn,"P",{});var Vi=a(ut);zm=o(Vi,"The "),ui=r(Vi,"A",{href:!0});var H1=a(ui);Pm=o(H1,"FunnelForPreTraining"),H1.forEach(n),qm=o(Vi," forward method, overrides the "),Bl=r(Vi,"CODE",{});var V1=a(Bl);Cm=o(V1,"__call__"),V1.forEach(n),xm=o(Vi," special method."),Vi.forEach(n),jm=d(Sn),w(Xt.$$.fragment,Sn),Lm=d(Sn),Wl=r(Sn,"P",{});var Y1=a(Wl);Am=o(Y1,"Examples:"),Y1.forEach(n),Dm=d(Sn),w(Ps.$$.fragment,Sn),Sn.forEach(n),R1.forEach(n),Nc=d(i),pt=r(i,"H2",{class:!0});var Du=a(pt);Jt=r(Du,"A",{id:!0,class:!0,href:!0});var U1=a(Jt);Ql=r(U1,"SPAN",{});var G1=a(Ql);w(qs.$$.fragment,G1),G1.forEach(n),U1.forEach(n),Im=d(Du),Rl=r(Du,"SPAN",{});var Z1=a(Rl);Sm=o(Z1,"FunnelForMaskedLM"),Z1.forEach(n),Du.forEach(n),Bc=d(i),Re=r(i,"DIV",{class:!0});var En=a(Re);w(Cs.$$.fragment,En),Om=d(En),xs=r(En,"P",{});var Iu=a(xs);Nm=o(Iu,"Funnel Transformer Model with a "),Hl=r(Iu,"EM",{});var K1=a(Hl);Bm=o(K1,"language modeling"),K1.forEach(n),Wm=o(Iu," head on top."),Iu.forEach(n),Qm=d(En),js=r(En,"P",{});var Su=a(js);Rm=o(Su,"The Funnel Transformer model was proposed in "),Ls=r(Su,"A",{href:!0,rel:!0});var X1=a(Ls);Hm=o(X1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),X1.forEach(n),Vm=o(Su," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Su.forEach(n),Ym=d(En),As=r(En,"P",{});var Ou=a(As);Um=o(Ou,"This model inherits from "),pi=r(Ou,"A",{href:!0});var J1=a(pi);Gm=o(J1,"PreTrainedModel"),J1.forEach(n),Zm=o(Ou,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Ou.forEach(n),Km=d(En),Ds=r(En,"P",{});var Nu=a(Ds);Xm=o(Nu,"This model is also a PyTorch "),Is=r(Nu,"A",{href:!0,rel:!0});var eb=a(Is);Jm=o(eb,"torch.nn.Module"),eb.forEach(n),eg=o(Nu,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),Nu.forEach(n),ng=d(En),en=r(En,"DIV",{class:!0});var On=a(en);w(Ss.$$.fragment,On),tg=d(On),ht=r(On,"P",{});var Yi=a(ht);og=o(Yi,"The "),hi=r(Yi,"A",{href:!0});var nb=a(hi);sg=o(nb,"FunnelForMaskedLM"),nb.forEach(n),rg=o(Yi," forward method, overrides the "),Vl=r(Yi,"CODE",{});var tb=a(Vl);ag=o(tb,"__call__"),tb.forEach(n),ig=o(Yi," special method."),Yi.forEach(n),lg=d(On),w(eo.$$.fragment,On),dg=d(On),Yl=r(On,"P",{});var ob=a(Yl);cg=o(ob,"Example:"),ob.forEach(n),ug=d(On),w(Os.$$.fragment,On),On.forEach(n),En.forEach(n),Wc=d(i),ft=r(i,"H2",{class:!0});var Bu=a(ft);no=r(Bu,"A",{id:!0,class:!0,href:!0});var sb=a(no);Ul=r(sb,"SPAN",{});var rb=a(Ul);w(Ns.$$.fragment,rb),rb.forEach(n),sb.forEach(n),pg=d(Bu),Gl=r(Bu,"SPAN",{});var ab=a(Gl);hg=o(ab,"FunnelForSequenceClassification"),ab.forEach(n),Bu.forEach(n),Qc=d(i),He=r(i,"DIV",{class:!0});var Mn=a(He);w(Bs.$$.fragment,Mn),fg=d(Mn),Zl=r(Mn,"P",{});var ib=a(Zl);mg=o(ib,`Funnel Transformer Model with a sequence classification/regression head on top (two linear layer on top of the
first timestep of the last hidden state) e.g. for GLUE tasks.`),ib.forEach(n),gg=d(Mn),Ws=r(Mn,"P",{});var Wu=a(Ws);_g=o(Wu,"The Funnel Transformer model was proposed in "),Qs=r(Wu,"A",{href:!0,rel:!0});var lb=a(Qs);Tg=o(lb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),lb.forEach(n),Fg=o(Wu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Wu.forEach(n),vg=d(Mn),Rs=r(Mn,"P",{});var Qu=a(Rs);kg=o(Qu,"This model inherits from "),fi=r(Qu,"A",{href:!0});var db=a(fi);wg=o(db,"PreTrainedModel"),db.forEach(n),bg=o(Qu,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Qu.forEach(n),yg=d(Mn),Hs=r(Mn,"P",{});var Ru=a(Hs);$g=o(Ru,"This model is also a PyTorch "),Vs=r(Ru,"A",{href:!0,rel:!0});var cb=a(Vs);Eg=o(cb,"torch.nn.Module"),cb.forEach(n),Mg=o(Ru,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),Ru.forEach(n),zg=d(Mn),Be=r(Mn,"DIV",{class:!0});var hn=a(Be);w(Ys.$$.fragment,hn),Pg=d(hn),mt=r(hn,"P",{});var Ui=a(mt);qg=o(Ui,"The "),mi=r(Ui,"A",{href:!0});var ub=a(mi);Cg=o(ub,"FunnelForSequenceClassification"),ub.forEach(n),xg=o(Ui," forward method, overrides the "),Kl=r(Ui,"CODE",{});var pb=a(Kl);jg=o(pb,"__call__"),pb.forEach(n),Lg=o(Ui," special method."),Ui.forEach(n),Ag=d(hn),w(to.$$.fragment,hn),Dg=d(hn),Xl=r(hn,"P",{});var hb=a(Xl);Ig=o(hb,"Example of single-label classification:"),hb.forEach(n),Sg=d(hn),w(Us.$$.fragment,hn),Og=d(hn),Jl=r(hn,"P",{});var fb=a(Jl);Ng=o(fb,"Example of multi-label classification:"),fb.forEach(n),Bg=d(hn),w(Gs.$$.fragment,hn),hn.forEach(n),Mn.forEach(n),Rc=d(i),gt=r(i,"H2",{class:!0});var Hu=a(gt);oo=r(Hu,"A",{id:!0,class:!0,href:!0});var mb=a(oo);ed=r(mb,"SPAN",{});var gb=a(ed);w(Zs.$$.fragment,gb),gb.forEach(n),mb.forEach(n),Wg=d(Hu),nd=r(Hu,"SPAN",{});var _b=a(nd);Qg=o(_b,"FunnelForMultipleChoice"),_b.forEach(n),Hu.forEach(n),Hc=d(i),Ve=r(i,"DIV",{class:!0});var zn=a(Ve);w(Ks.$$.fragment,zn),Rg=d(zn),td=r(zn,"P",{});var Tb=a(td);Hg=o(Tb,`Funnel Transformer Model with a multiple choice classification head on top (two linear layer on top of the first
timestep of the last hidden state, and a softmax) e.g. for RocStories/SWAG tasks.`),Tb.forEach(n),Vg=d(zn),Xs=r(zn,"P",{});var Vu=a(Xs);Yg=o(Vu,"The Funnel Transformer model was proposed in "),Js=r(Vu,"A",{href:!0,rel:!0});var Fb=a(Js);Ug=o(Fb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Fb.forEach(n),Gg=o(Vu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Vu.forEach(n),Zg=d(zn),er=r(zn,"P",{});var Yu=a(er);Kg=o(Yu,"This model inherits from "),gi=r(Yu,"A",{href:!0});var vb=a(gi);Xg=o(vb,"PreTrainedModel"),vb.forEach(n),Jg=o(Yu,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Yu.forEach(n),e_=d(zn),nr=r(zn,"P",{});var Uu=a(nr);n_=o(Uu,"This model is also a PyTorch "),tr=r(Uu,"A",{href:!0,rel:!0});var kb=a(tr);t_=o(kb,"torch.nn.Module"),kb.forEach(n),o_=o(Uu,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),Uu.forEach(n),s_=d(zn),nn=r(zn,"DIV",{class:!0});var Nn=a(nn);w(or.$$.fragment,Nn),r_=d(Nn),_t=r(Nn,"P",{});var Gi=a(_t);a_=o(Gi,"The "),_i=r(Gi,"A",{href:!0});var wb=a(_i);i_=o(wb,"FunnelForMultipleChoice"),wb.forEach(n),l_=o(Gi," forward method, overrides the "),od=r(Gi,"CODE",{});var bb=a(od);d_=o(bb,"__call__"),bb.forEach(n),c_=o(Gi," special method."),Gi.forEach(n),u_=d(Nn),w(so.$$.fragment,Nn),p_=d(Nn),sd=r(Nn,"P",{});var yb=a(sd);h_=o(yb,"Example:"),yb.forEach(n),f_=d(Nn),w(sr.$$.fragment,Nn),Nn.forEach(n),zn.forEach(n),Vc=d(i),Tt=r(i,"H2",{class:!0});var Gu=a(Tt);ro=r(Gu,"A",{id:!0,class:!0,href:!0});var $b=a(ro);rd=r($b,"SPAN",{});var Eb=a(rd);w(rr.$$.fragment,Eb),Eb.forEach(n),$b.forEach(n),m_=d(Gu),ad=r(Gu,"SPAN",{});var Mb=a(ad);g_=o(Mb,"FunnelForTokenClassification"),Mb.forEach(n),Gu.forEach(n),Yc=d(i),Ye=r(i,"DIV",{class:!0});var Pn=a(Ye);w(ar.$$.fragment,Pn),__=d(Pn),id=r(Pn,"P",{});var zb=a(id);T_=o(zb,`Funnel Transformer Model with a token classification head on top (a linear layer on top of the hidden-states
output) e.g. for Named-Entity-Recognition (NER) tasks.`),zb.forEach(n),F_=d(Pn),ir=r(Pn,"P",{});var Zu=a(ir);v_=o(Zu,"The Funnel Transformer model was proposed in "),lr=r(Zu,"A",{href:!0,rel:!0});var Pb=a(lr);k_=o(Pb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Pb.forEach(n),w_=o(Zu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Zu.forEach(n),b_=d(Pn),dr=r(Pn,"P",{});var Ku=a(dr);y_=o(Ku,"This model inherits from "),Ti=r(Ku,"A",{href:!0});var qb=a(Ti);$_=o(qb,"PreTrainedModel"),qb.forEach(n),E_=o(Ku,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Ku.forEach(n),M_=d(Pn),cr=r(Pn,"P",{});var Xu=a(cr);z_=o(Xu,"This model is also a PyTorch "),ur=r(Xu,"A",{href:!0,rel:!0});var Cb=a(ur);P_=o(Cb,"torch.nn.Module"),Cb.forEach(n),q_=o(Xu,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),Xu.forEach(n),C_=d(Pn),tn=r(Pn,"DIV",{class:!0});var Bn=a(tn);w(pr.$$.fragment,Bn),x_=d(Bn),Ft=r(Bn,"P",{});var Zi=a(Ft);j_=o(Zi,"The "),Fi=r(Zi,"A",{href:!0});var xb=a(Fi);L_=o(xb,"FunnelForTokenClassification"),xb.forEach(n),A_=o(Zi," forward method, overrides the "),ld=r(Zi,"CODE",{});var jb=a(ld);D_=o(jb,"__call__"),jb.forEach(n),I_=o(Zi," special method."),Zi.forEach(n),S_=d(Bn),w(ao.$$.fragment,Bn),O_=d(Bn),dd=r(Bn,"P",{});var Lb=a(dd);N_=o(Lb,"Example:"),Lb.forEach(n),B_=d(Bn),w(hr.$$.fragment,Bn),Bn.forEach(n),Pn.forEach(n),Uc=d(i),vt=r(i,"H2",{class:!0});var Ju=a(vt);io=r(Ju,"A",{id:!0,class:!0,href:!0});var Ab=a(io);cd=r(Ab,"SPAN",{});var Db=a(cd);w(fr.$$.fragment,Db),Db.forEach(n),Ab.forEach(n),W_=d(Ju),ud=r(Ju,"SPAN",{});var Ib=a(ud);Q_=o(Ib,"FunnelForQuestionAnswering"),Ib.forEach(n),Ju.forEach(n),Gc=d(i),Ue=r(i,"DIV",{class:!0});var qn=a(Ue);w(mr.$$.fragment,qn),R_=d(qn),kt=r(qn,"P",{});var Ki=a(kt);H_=o(Ki,`Funnel Transformer Model with a span classification head on top for extractive question-answering tasks like SQuAD
(a linear layer on top of the hidden-states output to compute `),pd=r(Ki,"EM",{});var Sb=a(pd);V_=o(Sb,"span start logits"),Sb.forEach(n),Y_=o(Ki," and "),hd=r(Ki,"EM",{});var Ob=a(hd);U_=o(Ob,"span end logits"),Ob.forEach(n),G_=o(Ki,")."),Ki.forEach(n),Z_=d(qn),gr=r(qn,"P",{});var ep=a(gr);K_=o(ep,"The Funnel Transformer model was proposed in "),_r=r(ep,"A",{href:!0,rel:!0});var Nb=a(_r);X_=o(Nb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Nb.forEach(n),J_=o(ep," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),ep.forEach(n),eT=d(qn),Tr=r(qn,"P",{});var np=a(Tr);nT=o(np,"This model inherits from "),vi=r(np,"A",{href:!0});var Bb=a(vi);tT=o(Bb,"PreTrainedModel"),Bb.forEach(n),oT=o(np,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),np.forEach(n),sT=d(qn),Fr=r(qn,"P",{});var tp=a(Fr);rT=o(tp,"This model is also a PyTorch "),vr=r(tp,"A",{href:!0,rel:!0});var Wb=a(vr);aT=o(Wb,"torch.nn.Module"),Wb.forEach(n),iT=o(tp,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),tp.forEach(n),lT=d(qn),on=r(qn,"DIV",{class:!0});var Wn=a(on);w(kr.$$.fragment,Wn),dT=d(Wn),wt=r(Wn,"P",{});var Xi=a(wt);cT=o(Xi,"The "),ki=r(Xi,"A",{href:!0});var Qb=a(ki);uT=o(Qb,"FunnelForQuestionAnswering"),Qb.forEach(n),pT=o(Xi," forward method, overrides the "),fd=r(Xi,"CODE",{});var Rb=a(fd);hT=o(Rb,"__call__"),Rb.forEach(n),fT=o(Xi," special method."),Xi.forEach(n),mT=d(Wn),w(lo.$$.fragment,Wn),gT=d(Wn),md=r(Wn,"P",{});var Hb=a(md);_T=o(Hb,"Example:"),Hb.forEach(n),TT=d(Wn),w(wr.$$.fragment,Wn),Wn.forEach(n),qn.forEach(n),Zc=d(i),bt=r(i,"H2",{class:!0});var op=a(bt);co=r(op,"A",{id:!0,class:!0,href:!0});var Vb=a(co);gd=r(Vb,"SPAN",{});var Yb=a(gd);w(br.$$.fragment,Yb),Yb.forEach(n),Vb.forEach(n),FT=d(op),_d=r(op,"SPAN",{});var Ub=a(_d);vT=o(Ub,"TFFunnelBaseModel"),Ub.forEach(n),op.forEach(n),Kc=d(i),xe=r(i,"DIV",{class:!0});var fn=a(xe);w(yr.$$.fragment,fn),kT=d(fn),Td=r(fn,"P",{});var Gb=a(Td);wT=o(Gb,`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),Gb.forEach(n),bT=d(fn),$r=r(fn,"P",{});var sp=a($r);yT=o(sp,"The Funnel Transformer model was proposed in "),Er=r(sp,"A",{href:!0,rel:!0});var Zb=a(Er);$T=o(Zb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Zb.forEach(n),ET=o(sp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),sp.forEach(n),MT=d(fn),Mr=r(fn,"P",{});var rp=a(Mr);zT=o(rp,"This model inherits from "),wi=r(rp,"A",{href:!0});var Kb=a(wi);PT=o(Kb,"TFPreTrainedModel"),Kb.forEach(n),qT=o(rp,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),rp.forEach(n),CT=d(fn),zr=r(fn,"P",{});var ap=a(zr);xT=o(ap,"This model is also a "),Pr=r(ap,"A",{href:!0,rel:!0});var Xb=a(Pr);jT=o(Xb,"tf.keras.Model"),Xb.forEach(n),LT=o(ap,` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),ap.forEach(n),AT=d(fn),w(uo.$$.fragment,fn),DT=d(fn),sn=r(fn,"DIV",{class:!0});var Qn=a(sn);w(qr.$$.fragment,Qn),IT=d(Qn),yt=r(Qn,"P",{});var Ji=a(yt);ST=o(Ji,"The "),bi=r(Ji,"A",{href:!0});var Jb=a(bi);OT=o(Jb,"TFFunnelBaseModel"),Jb.forEach(n),NT=o(Ji," forward method, overrides the "),Fd=r(Ji,"CODE",{});var ey=a(Fd);BT=o(ey,"__call__"),ey.forEach(n),WT=o(Ji," special method."),Ji.forEach(n),QT=d(Qn),w(po.$$.fragment,Qn),RT=d(Qn),vd=r(Qn,"P",{});var ny=a(vd);HT=o(ny,"Example:"),ny.forEach(n),VT=d(Qn),w(Cr.$$.fragment,Qn),Qn.forEach(n),fn.forEach(n),Xc=d(i),$t=r(i,"H2",{class:!0});var ip=a($t);ho=r(ip,"A",{id:!0,class:!0,href:!0});var ty=a(ho);kd=r(ty,"SPAN",{});var oy=a(kd);w(xr.$$.fragment,oy),oy.forEach(n),ty.forEach(n),YT=d(ip),wd=r(ip,"SPAN",{});var sy=a(wd);UT=o(sy,"TFFunnelModel"),sy.forEach(n),ip.forEach(n),Jc=d(i),je=r(i,"DIV",{class:!0});var mn=a(je);w(jr.$$.fragment,mn),GT=d(mn),bd=r(mn,"P",{});var ry=a(bd);ZT=o(ry,"The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),ry.forEach(n),KT=d(mn),Lr=r(mn,"P",{});var lp=a(Lr);XT=o(lp,"The Funnel Transformer model was proposed in "),Ar=r(lp,"A",{href:!0,rel:!0});var ay=a(Ar);JT=o(ay,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),ay.forEach(n),eF=o(lp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),lp.forEach(n),nF=d(mn),Dr=r(mn,"P",{});var dp=a(Dr);tF=o(dp,"This model inherits from "),yi=r(dp,"A",{href:!0});var iy=a(yi);oF=o(iy,"TFPreTrainedModel"),iy.forEach(n),sF=o(dp,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),dp.forEach(n),rF=d(mn),Ir=r(mn,"P",{});var cp=a(Ir);aF=o(cp,"This model is also a "),Sr=r(cp,"A",{href:!0,rel:!0});var ly=a(Sr);iF=o(ly,"tf.keras.Model"),ly.forEach(n),lF=o(cp,` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),cp.forEach(n),dF=d(mn),w(fo.$$.fragment,mn),cF=d(mn),rn=r(mn,"DIV",{class:!0});var Rn=a(rn);w(Or.$$.fragment,Rn),uF=d(Rn),Et=r(Rn,"P",{});var el=a(Et);pF=o(el,"The "),$i=r(el,"A",{href:!0});var dy=a($i);hF=o(dy,"TFFunnelModel"),dy.forEach(n),fF=o(el," forward method, overrides the "),yd=r(el,"CODE",{});var cy=a(yd);mF=o(cy,"__call__"),cy.forEach(n),gF=o(el," special method."),el.forEach(n),_F=d(Rn),w(mo.$$.fragment,Rn),TF=d(Rn),$d=r(Rn,"P",{});var uy=a($d);FF=o(uy,"Example:"),uy.forEach(n),vF=d(Rn),w(Nr.$$.fragment,Rn),Rn.forEach(n),mn.forEach(n),eu=d(i),Mt=r(i,"H2",{class:!0});var up=a(Mt);go=r(up,"A",{id:!0,class:!0,href:!0});var py=a(go);Ed=r(py,"SPAN",{});var hy=a(Ed);w(Br.$$.fragment,hy),hy.forEach(n),py.forEach(n),kF=d(up),Md=r(up,"SPAN",{});var fy=a(Md);wF=o(fy,"TFFunnelModelForPreTraining"),fy.forEach(n),up.forEach(n),nu=d(i),Le=r(i,"DIV",{class:!0});var gn=a(Le);w(Wr.$$.fragment,gn),bF=d(gn),zd=r(gn,"P",{});var my=a(zd);yF=o(my,"Funnel model with a binary classification head on top as used during pretraining for identifying generated tokens."),my.forEach(n),$F=d(gn),Qr=r(gn,"P",{});var pp=a(Qr);EF=o(pp,"The Funnel Transformer model was proposed in "),Rr=r(pp,"A",{href:!0,rel:!0});var gy=a(Rr);MF=o(gy,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),gy.forEach(n),zF=o(pp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),pp.forEach(n),PF=d(gn),Hr=r(gn,"P",{});var hp=a(Hr);qF=o(hp,"This model inherits from "),Ei=r(hp,"A",{href:!0});var _y=a(Ei);CF=o(_y,"TFPreTrainedModel"),_y.forEach(n),xF=o(hp,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),hp.forEach(n),jF=d(gn),Vr=r(gn,"P",{});var fp=a(Vr);LF=o(fp,"This model is also a "),Yr=r(fp,"A",{href:!0,rel:!0});var Ty=a(Yr);AF=o(Ty,"tf.keras.Model"),Ty.forEach(n),DF=o(fp,` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),fp.forEach(n),IF=d(gn),w(_o.$$.fragment,gn),SF=d(gn),an=r(gn,"DIV",{class:!0});var Hn=a(an);w(Ur.$$.fragment,Hn),OF=d(Hn),zt=r(Hn,"P",{});var nl=a(zt);NF=o(nl,"The "),Mi=r(nl,"A",{href:!0});var Fy=a(Mi);BF=o(Fy,"TFFunnelForPreTraining"),Fy.forEach(n),WF=o(nl," forward method, overrides the "),Pd=r(nl,"CODE",{});var vy=a(Pd);QF=o(vy,"__call__"),vy.forEach(n),RF=o(nl," special method."),nl.forEach(n),HF=d(Hn),w(To.$$.fragment,Hn),VF=d(Hn),qd=r(Hn,"P",{});var ky=a(qd);YF=o(ky,"Examples:"),ky.forEach(n),UF=d(Hn),w(Gr.$$.fragment,Hn),Hn.forEach(n),gn.forEach(n),tu=d(i),Pt=r(i,"H2",{class:!0});var mp=a(Pt);Fo=r(mp,"A",{id:!0,class:!0,href:!0});var wy=a(Fo);Cd=r(wy,"SPAN",{});var by=a(Cd);w(Zr.$$.fragment,by),by.forEach(n),wy.forEach(n),GF=d(mp),xd=r(mp,"SPAN",{});var yy=a(xd);ZF=o(yy,"TFFunnelForMaskedLM"),yy.forEach(n),mp.forEach(n),ou=d(i),Ae=r(i,"DIV",{class:!0});var _n=a(Ae);w(Kr.$$.fragment,_n),KF=d(_n),Xr=r(_n,"P",{});var gp=a(Xr);XF=o(gp,"Funnel Model with a "),jd=r(gp,"EM",{});var $y=a(jd);JF=o($y,"language modeling"),$y.forEach(n),ev=o(gp," head on top."),gp.forEach(n),nv=d(_n),Jr=r(_n,"P",{});var _p=a(Jr);tv=o(_p,"The Funnel Transformer model was proposed in "),ea=r(_p,"A",{href:!0,rel:!0});var Ey=a(ea);ov=o(Ey,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Ey.forEach(n),sv=o(_p," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),_p.forEach(n),rv=d(_n),na=r(_n,"P",{});var Tp=a(na);av=o(Tp,"This model inherits from "),zi=r(Tp,"A",{href:!0});var My=a(zi);iv=o(My,"TFPreTrainedModel"),My.forEach(n),lv=o(Tp,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),Tp.forEach(n),dv=d(_n),ta=r(_n,"P",{});var Fp=a(ta);cv=o(Fp,"This model is also a "),oa=r(Fp,"A",{href:!0,rel:!0});var zy=a(oa);uv=o(zy,"tf.keras.Model"),zy.forEach(n),pv=o(Fp,` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),Fp.forEach(n),hv=d(_n),w(vo.$$.fragment,_n),fv=d(_n),ln=r(_n,"DIV",{class:!0});var Vn=a(ln);w(sa.$$.fragment,Vn),mv=d(Vn),qt=r(Vn,"P",{});var tl=a(qt);gv=o(tl,"The "),Pi=r(tl,"A",{href:!0});var Py=a(Pi);_v=o(Py,"TFFunnelForMaskedLM"),Py.forEach(n),Tv=o(tl," forward method, overrides the "),Ld=r(tl,"CODE",{});var qy=a(Ld);Fv=o(qy,"__call__"),qy.forEach(n),vv=o(tl," special method."),tl.forEach(n),kv=d(Vn),w(ko.$$.fragment,Vn),wv=d(Vn),Ad=r(Vn,"P",{});var Cy=a(Ad);bv=o(Cy,"Example:"),Cy.forEach(n),yv=d(Vn),w(ra.$$.fragment,Vn),Vn.forEach(n),_n.forEach(n),su=d(i),Ct=r(i,"H2",{class:!0});var vp=a(Ct);wo=r(vp,"A",{id:!0,class:!0,href:!0});var xy=a(wo);Dd=r(xy,"SPAN",{});var jy=a(Dd);w(aa.$$.fragment,jy),jy.forEach(n),xy.forEach(n),$v=d(vp),Id=r(vp,"SPAN",{});var Ly=a(Id);Ev=o(Ly,"TFFunnelForSequenceClassification"),Ly.forEach(n),vp.forEach(n),ru=d(i),De=r(i,"DIV",{class:!0});var Tn=a(De);w(ia.$$.fragment,Tn),Mv=d(Tn),Sd=r(Tn,"P",{});var Ay=a(Sd);zv=o(Ay,`Funnel Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
output) e.g. for GLUE tasks.`),Ay.forEach(n),Pv=d(Tn),la=r(Tn,"P",{});var kp=a(la);qv=o(kp,"The Funnel Transformer model was proposed in "),da=r(kp,"A",{href:!0,rel:!0});var Dy=a(da);Cv=o(Dy,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Dy.forEach(n),xv=o(kp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),kp.forEach(n),jv=d(Tn),ca=r(Tn,"P",{});var wp=a(ca);Lv=o(wp,"This model inherits from "),qi=r(wp,"A",{href:!0});var Iy=a(qi);Av=o(Iy,"TFPreTrainedModel"),Iy.forEach(n),Dv=o(wp,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),wp.forEach(n),Iv=d(Tn),ua=r(Tn,"P",{});var bp=a(ua);Sv=o(bp,"This model is also a "),pa=r(bp,"A",{href:!0,rel:!0});var Sy=a(pa);Ov=o(Sy,"tf.keras.Model"),Sy.forEach(n),Nv=o(bp,` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),bp.forEach(n),Bv=d(Tn),w(bo.$$.fragment,Tn),Wv=d(Tn),dn=r(Tn,"DIV",{class:!0});var Yn=a(dn);w(ha.$$.fragment,Yn),Qv=d(Yn),xt=r(Yn,"P",{});var ol=a(xt);Rv=o(ol,"The "),Ci=r(ol,"A",{href:!0});var Oy=a(Ci);Hv=o(Oy,"TFFunnelForSequenceClassification"),Oy.forEach(n),Vv=o(ol," forward method, overrides the "),Od=r(ol,"CODE",{});var Ny=a(Od);Yv=o(Ny,"__call__"),Ny.forEach(n),Uv=o(ol," special method."),ol.forEach(n),Gv=d(Yn),w(yo.$$.fragment,Yn),Zv=d(Yn),Nd=r(Yn,"P",{});var By=a(Nd);Kv=o(By,"Example:"),By.forEach(n),Xv=d(Yn),w(fa.$$.fragment,Yn),Yn.forEach(n),Tn.forEach(n),au=d(i),jt=r(i,"H2",{class:!0});var yp=a(jt);$o=r(yp,"A",{id:!0,class:!0,href:!0});var Wy=a($o);Bd=r(Wy,"SPAN",{});var Qy=a(Bd);w(ma.$$.fragment,Qy),Qy.forEach(n),Wy.forEach(n),Jv=d(yp),Wd=r(yp,"SPAN",{});var Ry=a(Wd);ek=o(Ry,"TFFunnelForMultipleChoice"),Ry.forEach(n),yp.forEach(n),iu=d(i),Ie=r(i,"DIV",{class:!0});var Fn=a(Ie);w(ga.$$.fragment,Fn),nk=d(Fn),Qd=r(Fn,"P",{});var Hy=a(Qd);tk=o(Hy,`Funnel Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),Hy.forEach(n),ok=d(Fn),_a=r(Fn,"P",{});var $p=a(_a);sk=o($p,"The Funnel Transformer model was proposed in "),Ta=r($p,"A",{href:!0,rel:!0});var Vy=a(Ta);rk=o(Vy,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Vy.forEach(n),ak=o($p," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),$p.forEach(n),ik=d(Fn),Fa=r(Fn,"P",{});var Ep=a(Fa);lk=o(Ep,"This model inherits from "),xi=r(Ep,"A",{href:!0});var Yy=a(xi);dk=o(Yy,"TFPreTrainedModel"),Yy.forEach(n),ck=o(Ep,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),Ep.forEach(n),uk=d(Fn),va=r(Fn,"P",{});var Mp=a(va);pk=o(Mp,"This model is also a "),ka=r(Mp,"A",{href:!0,rel:!0});var Uy=a(ka);hk=o(Uy,"tf.keras.Model"),Uy.forEach(n),fk=o(Mp,` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),Mp.forEach(n),mk=d(Fn),w(Eo.$$.fragment,Fn),gk=d(Fn),cn=r(Fn,"DIV",{class:!0});var Un=a(cn);w(wa.$$.fragment,Un),_k=d(Un),Lt=r(Un,"P",{});var sl=a(Lt);Tk=o(sl,"The "),ji=r(sl,"A",{href:!0});var Gy=a(ji);Fk=o(Gy,"TFFunnelForMultipleChoice"),Gy.forEach(n),vk=o(sl," forward method, overrides the "),Rd=r(sl,"CODE",{});var Zy=a(Rd);kk=o(Zy,"__call__"),Zy.forEach(n),wk=o(sl," special method."),sl.forEach(n),bk=d(Un),w(Mo.$$.fragment,Un),yk=d(Un),Hd=r(Un,"P",{});var Ky=a(Hd);$k=o(Ky,"Example:"),Ky.forEach(n),Ek=d(Un),w(ba.$$.fragment,Un),Un.forEach(n),Fn.forEach(n),lu=d(i),At=r(i,"H2",{class:!0});var zp=a(At);zo=r(zp,"A",{id:!0,class:!0,href:!0});var Xy=a(zo);Vd=r(Xy,"SPAN",{});var Jy=a(Vd);w(ya.$$.fragment,Jy),Jy.forEach(n),Xy.forEach(n),Mk=d(zp),Yd=r(zp,"SPAN",{});var e2=a(Yd);zk=o(e2,"TFFunnelForTokenClassification"),e2.forEach(n),zp.forEach(n),du=d(i),Se=r(i,"DIV",{class:!0});var vn=a(Se);w($a.$$.fragment,vn),Pk=d(vn),Ud=r(vn,"P",{});var n2=a(Ud);qk=o(n2,`Funnel Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),n2.forEach(n),Ck=d(vn),Ea=r(vn,"P",{});var Pp=a(Ea);xk=o(Pp,"The Funnel Transformer model was proposed in "),Ma=r(Pp,"A",{href:!0,rel:!0});var t2=a(Ma);jk=o(t2,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),t2.forEach(n),Lk=o(Pp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Pp.forEach(n),Ak=d(vn),za=r(vn,"P",{});var qp=a(za);Dk=o(qp,"This model inherits from "),Li=r(qp,"A",{href:!0});var o2=a(Li);Ik=o(o2,"TFPreTrainedModel"),o2.forEach(n),Sk=o(qp,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),qp.forEach(n),Ok=d(vn),Pa=r(vn,"P",{});var Cp=a(Pa);Nk=o(Cp,"This model is also a "),qa=r(Cp,"A",{href:!0,rel:!0});var s2=a(qa);Bk=o(s2,"tf.keras.Model"),s2.forEach(n),Wk=o(Cp,` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),Cp.forEach(n),Qk=d(vn),w(Po.$$.fragment,vn),Rk=d(vn),un=r(vn,"DIV",{class:!0});var Gn=a(un);w(Ca.$$.fragment,Gn),Hk=d(Gn),Dt=r(Gn,"P",{});var rl=a(Dt);Vk=o(rl,"The "),Ai=r(rl,"A",{href:!0});var r2=a(Ai);Yk=o(r2,"TFFunnelForTokenClassification"),r2.forEach(n),Uk=o(rl," forward method, overrides the "),Gd=r(rl,"CODE",{});var a2=a(Gd);Gk=o(a2,"__call__"),a2.forEach(n),Zk=o(rl," special method."),rl.forEach(n),Kk=d(Gn),w(qo.$$.fragment,Gn),Xk=d(Gn),Zd=r(Gn,"P",{});var i2=a(Zd);Jk=o(i2,"Example:"),i2.forEach(n),ew=d(Gn),w(xa.$$.fragment,Gn),Gn.forEach(n),vn.forEach(n),cu=d(i),It=r(i,"H2",{class:!0});var xp=a(It);Co=r(xp,"A",{id:!0,class:!0,href:!0});var l2=a(Co);Kd=r(l2,"SPAN",{});var d2=a(Kd);w(ja.$$.fragment,d2),d2.forEach(n),l2.forEach(n),nw=d(xp),Xd=r(xp,"SPAN",{});var c2=a(Xd);tw=o(c2,"TFFunnelForQuestionAnswering"),c2.forEach(n),xp.forEach(n),uu=d(i),Oe=r(i,"DIV",{class:!0});var kn=a(Oe);w(La.$$.fragment,kn),ow=d(kn),St=r(kn,"P",{});var al=a(St);sw=o(al,`Funnel Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),Jd=r(al,"EM",{});var u2=a(Jd);rw=o(u2,"span start logits"),u2.forEach(n),aw=o(al," and "),ec=r(al,"EM",{});var p2=a(ec);iw=o(p2,"span end logits"),p2.forEach(n),lw=o(al,")."),al.forEach(n),dw=d(kn),Aa=r(kn,"P",{});var jp=a(Aa);cw=o(jp,"The Funnel Transformer model was proposed in "),Da=r(jp,"A",{href:!0,rel:!0});var h2=a(Da);uw=o(h2,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),h2.forEach(n),pw=o(jp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),jp.forEach(n),hw=d(kn),Ia=r(kn,"P",{});var Lp=a(Ia);fw=o(Lp,"This model inherits from "),Di=r(Lp,"A",{href:!0});var f2=a(Di);mw=o(f2,"TFPreTrainedModel"),f2.forEach(n),gw=o(Lp,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),Lp.forEach(n),_w=d(kn),Sa=r(kn,"P",{});var Ap=a(Sa);Tw=o(Ap,"This model is also a "),Oa=r(Ap,"A",{href:!0,rel:!0});var m2=a(Oa);Fw=o(m2,"tf.keras.Model"),m2.forEach(n),vw=o(Ap,` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),Ap.forEach(n),kw=d(kn),w(xo.$$.fragment,kn),ww=d(kn),pn=r(kn,"DIV",{class:!0});var Zn=a(pn);w(Na.$$.fragment,Zn),bw=d(Zn),Ot=r(Zn,"P",{});var il=a(Ot);yw=o(il,"The "),Ii=r(il,"A",{href:!0});var g2=a(Ii);$w=o(g2,"TFFunnelForQuestionAnswering"),g2.forEach(n),Ew=o(il," forward method, overrides the "),nc=r(il,"CODE",{});var _2=a(nc);Mw=o(_2,"__call__"),_2.forEach(n),zw=o(il," special method."),il.forEach(n),Pw=d(Zn),w(jo.$$.fragment,Zn),qw=d(Zn),tc=r(Zn,"P",{});var T2=a(tc);Cw=o(T2,"Example:"),T2.forEach(n),xw=d(Zn),w(Ba.$$.fragment,Zn),Zn.forEach(n),kn.forEach(n),this.h()},h(){c(p,"name","hf:doc:metadata"),c(p,"content",JSON.stringify(U2)),c(g,"id","funnel-transformer"),c(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g,"href","#funnel-transformer"),c(m,"class","relative group"),c(X,"id","overview"),c(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(X,"href","#overview"),c(P,"class","relative group"),c(te,"href","https://arxiv.org/abs/2006.03236"),c(te,"rel","nofollow"),c(re,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelModel"),c(u,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(Te,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(Fe,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(ve,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelBaseModel"),c(ke,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(Va,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(Io,"href","https://huggingface.co/sgugger"),c(Io,"rel","nofollow"),c(So,"href","https://github.com/laiguokun/Funnel-Transformer"),c(So,"rel","nofollow"),c(Nt,"id","transformers.FunnelConfig"),c(Nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Nt,"href","#transformers.FunnelConfig"),c(Kn,"class","relative group"),c(Ya,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelModel"),c(Ua,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertModel"),c(Bo,"href","https://huggingface.co/funnel-transformer/small"),c(Bo,"rel","nofollow"),c(Ga,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),c(Za,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),c(Cn,"class","docstring"),c(Bt,"id","transformers.FunnelTokenizer"),c(Bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bt,"href","#transformers.FunnelTokenizer"),c(Jn,"class","relative group"),c(Ka,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizer"),c(Xa,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer"),c(Ja,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer"),c(Ln,"class","docstring"),c(Qt,"class","docstring"),c(wn,"class","docstring"),c(vl,"class","docstring"),c(qe,"class","docstring"),c(Rt,"id","transformers.FunnelTokenizerFast"),c(Rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Rt,"href","#transformers.FunnelTokenizerFast"),c(nt,"class","relative group"),c(ti,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(oi,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizerFast"),c(si,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizerFast"),c(bn,"class","docstring"),c(Ze,"class","docstring"),c(Vt,"id","transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"),c(Vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Vt,"href","#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"),c(ot,"class","relative group"),c(ri,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(st,"class","docstring"),c(ai,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(rt,"class","docstring"),c(Yt,"id","transformers.FunnelBaseModel"),c(Yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Yt,"href","#transformers.FunnelBaseModel"),c(at,"class","relative group"),c(us,"href","https://arxiv.org/abs/2006.03236"),c(us,"rel","nofollow"),c(ii,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),c(fs,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(fs,"rel","nofollow"),c(li,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelBaseModel"),c(Ke,"class","docstring"),c(We,"class","docstring"),c(Gt,"id","transformers.FunnelModel"),c(Gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Gt,"href","#transformers.FunnelModel"),c(lt,"class","relative group"),c(vs,"href","https://arxiv.org/abs/2006.03236"),c(vs,"rel","nofollow"),c(di,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),c(bs,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(bs,"rel","nofollow"),c(ci,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelModel"),c(Xe,"class","docstring"),c(Qe,"class","docstring"),c(Kt,"id","transformers.FunnelForPreTraining"),c(Kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Kt,"href","#transformers.FunnelForPreTraining"),c(ct,"class","relative group"),c(ui,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(Je,"class","docstring"),c(Ms,"class","docstring"),c(Jt,"id","transformers.FunnelForMaskedLM"),c(Jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Jt,"href","#transformers.FunnelForMaskedLM"),c(pt,"class","relative group"),c(Ls,"href","https://arxiv.org/abs/2006.03236"),c(Ls,"rel","nofollow"),c(pi,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),c(Is,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Is,"rel","nofollow"),c(hi,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(en,"class","docstring"),c(Re,"class","docstring"),c(no,"id","transformers.FunnelForSequenceClassification"),c(no,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(no,"href","#transformers.FunnelForSequenceClassification"),c(ft,"class","relative group"),c(Qs,"href","https://arxiv.org/abs/2006.03236"),c(Qs,"rel","nofollow"),c(fi,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),c(Vs,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Vs,"rel","nofollow"),c(mi,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(Be,"class","docstring"),c(He,"class","docstring"),c(oo,"id","transformers.FunnelForMultipleChoice"),c(oo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(oo,"href","#transformers.FunnelForMultipleChoice"),c(gt,"class","relative group"),c(Js,"href","https://arxiv.org/abs/2006.03236"),c(Js,"rel","nofollow"),c(gi,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),c(tr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(tr,"rel","nofollow"),c(_i,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(nn,"class","docstring"),c(Ve,"class","docstring"),c(ro,"id","transformers.FunnelForTokenClassification"),c(ro,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ro,"href","#transformers.FunnelForTokenClassification"),c(Tt,"class","relative group"),c(lr,"href","https://arxiv.org/abs/2006.03236"),c(lr,"rel","nofollow"),c(Ti,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),c(ur,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(ur,"rel","nofollow"),c(Fi,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(tn,"class","docstring"),c(Ye,"class","docstring"),c(io,"id","transformers.FunnelForQuestionAnswering"),c(io,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(io,"href","#transformers.FunnelForQuestionAnswering"),c(vt,"class","relative group"),c(_r,"href","https://arxiv.org/abs/2006.03236"),c(_r,"rel","nofollow"),c(vi,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),c(vr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(vr,"rel","nofollow"),c(ki,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(on,"class","docstring"),c(Ue,"class","docstring"),c(co,"id","transformers.TFFunnelBaseModel"),c(co,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(co,"href","#transformers.TFFunnelBaseModel"),c(bt,"class","relative group"),c(Er,"href","https://arxiv.org/abs/2006.03236"),c(Er,"rel","nofollow"),c(wi,"href","/docs/transformers/master/en/main_classes/model#transformers.TFPreTrainedModel"),c(Pr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Pr,"rel","nofollow"),c(bi,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(sn,"class","docstring"),c(xe,"class","docstring"),c(ho,"id","transformers.TFFunnelModel"),c(ho,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ho,"href","#transformers.TFFunnelModel"),c($t,"class","relative group"),c(Ar,"href","https://arxiv.org/abs/2006.03236"),c(Ar,"rel","nofollow"),c(yi,"href","/docs/transformers/master/en/main_classes/model#transformers.TFPreTrainedModel"),c(Sr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Sr,"rel","nofollow"),c($i,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelModel"),c(rn,"class","docstring"),c(je,"class","docstring"),c(go,"id","transformers.TFFunnelForPreTraining"),c(go,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(go,"href","#transformers.TFFunnelForPreTraining"),c(Mt,"class","relative group"),c(Rr,"href","https://arxiv.org/abs/2006.03236"),c(Rr,"rel","nofollow"),c(Ei,"href","/docs/transformers/master/en/main_classes/model#transformers.TFPreTrainedModel"),c(Yr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Yr,"rel","nofollow"),c(Mi,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(an,"class","docstring"),c(Le,"class","docstring"),c(Fo,"id","transformers.TFFunnelForMaskedLM"),c(Fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fo,"href","#transformers.TFFunnelForMaskedLM"),c(Pt,"class","relative group"),c(ea,"href","https://arxiv.org/abs/2006.03236"),c(ea,"rel","nofollow"),c(zi,"href","/docs/transformers/master/en/main_classes/model#transformers.TFPreTrainedModel"),c(oa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(oa,"rel","nofollow"),c(Pi,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(ln,"class","docstring"),c(Ae,"class","docstring"),c(wo,"id","transformers.TFFunnelForSequenceClassification"),c(wo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(wo,"href","#transformers.TFFunnelForSequenceClassification"),c(Ct,"class","relative group"),c(da,"href","https://arxiv.org/abs/2006.03236"),c(da,"rel","nofollow"),c(qi,"href","/docs/transformers/master/en/main_classes/model#transformers.TFPreTrainedModel"),c(pa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(pa,"rel","nofollow"),c(Ci,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(dn,"class","docstring"),c(De,"class","docstring"),c($o,"id","transformers.TFFunnelForMultipleChoice"),c($o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($o,"href","#transformers.TFFunnelForMultipleChoice"),c(jt,"class","relative group"),c(Ta,"href","https://arxiv.org/abs/2006.03236"),c(Ta,"rel","nofollow"),c(xi,"href","/docs/transformers/master/en/main_classes/model#transformers.TFPreTrainedModel"),c(ka,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ka,"rel","nofollow"),c(ji,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(cn,"class","docstring"),c(Ie,"class","docstring"),c(zo,"id","transformers.TFFunnelForTokenClassification"),c(zo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zo,"href","#transformers.TFFunnelForTokenClassification"),c(At,"class","relative group"),c(Ma,"href","https://arxiv.org/abs/2006.03236"),c(Ma,"rel","nofollow"),c(Li,"href","/docs/transformers/master/en/main_classes/model#transformers.TFPreTrainedModel"),c(qa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(qa,"rel","nofollow"),c(Ai,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(un,"class","docstring"),c(Se,"class","docstring"),c(Co,"id","transformers.TFFunnelForQuestionAnswering"),c(Co,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Co,"href","#transformers.TFFunnelForQuestionAnswering"),c(It,"class","relative group"),c(Da,"href","https://arxiv.org/abs/2006.03236"),c(Da,"rel","nofollow"),c(Di,"href","/docs/transformers/master/en/main_classes/model#transformers.TFPreTrainedModel"),c(Oa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Oa,"rel","nofollow"),c(Ii,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(pn,"class","docstring"),c(Oe,"class","docstring")},m(i,f){e(document.head,p),h(i,M,f),h(i,m,f),e(m,g),e(g,F),b(T,F,null),e(m,_),e(m,z),e(z,ce),h(i,G,f),h(i,P,f),e(P,X),e(X,I),b(ne,I,null),e(P,ue),e(P,S),e(S,pe),h(i,ie,f),h(i,U,f),e(U,L),e(U,te),e(te,Z),e(U,q),h(i,x,f),h(i,oe,f),e(oe,Q),h(i,le,f),h(i,se,f),e(se,O),e(O,he),h(i,de,f),h(i,C,f),e(C,fe),h(i,B,f),h(i,J,f),e(J,ae),e(ae,R),e(J,me),e(J,N),e(N,A),e(N,re),e(re,H),e(N,ge),e(N,u),e(u,v),e(N,K),e(N,Te),e(Te,we),e(N,D),e(N,Fe),e(Fe,be),e(N,ye),e(N,j),e(j,V),e(N,$e),e(N,ve),e(ve,Y),e(N,Ee),e(N,ke),e(ke,_e),e(N,Me),e(N,Va),e(Va,Dp),e(N,Ip),h(i,yc,f),h(i,jn,f),e(jn,Sp),e(jn,Io),e(Io,Op),e(jn,Np),e(jn,So),e(So,Bp),e(jn,Wp),h(i,$c,f),h(i,Kn,f),e(Kn,Nt),e(Nt,ll),b(Oo,ll,null),e(Kn,Qp),e(Kn,dl),e(dl,Rp),h(i,Ec,f),h(i,Cn,f),b(No,Cn,null),e(Cn,Hp),e(Cn,xn),e(xn,Vp),e(xn,Ya),e(Ya,Yp),e(xn,Up),e(xn,Ua),e(Ua,Gp),e(xn,Zp),e(xn,Bo),e(Bo,Kp),e(xn,Xp),e(Cn,Jp),e(Cn,Xn),e(Xn,eh),e(Xn,Ga),e(Ga,nh),e(Xn,th),e(Xn,Za),e(Za,oh),e(Xn,sh),h(i,Mc,f),h(i,Jn,f),e(Jn,Bt),e(Bt,cl),b(Wo,cl,null),e(Jn,rh),e(Jn,ul),e(ul,ah),h(i,zc,f),h(i,qe,f),b(Qo,qe,null),e(qe,ih),e(qe,pl),e(pl,lh),e(qe,dh),e(qe,Wt),e(Wt,Ka),e(Ka,ch),e(Wt,uh),e(Wt,Xa),e(Xa,ph),e(Wt,hh),e(qe,fh),e(qe,Ro),e(Ro,mh),e(Ro,Ja),e(Ja,gh),e(Ro,_h),e(qe,Th),e(qe,Ln),b(Ho,Ln,null),e(Ln,Fh),e(Ln,hl),e(hl,vh),e(Ln,kh),e(Ln,Vo),e(Vo,ei),e(ei,wh),e(ei,fl),e(fl,bh),e(Vo,yh),e(Vo,ni),e(ni,$h),e(ni,ml),e(ml,Eh),e(qe,Mh),e(qe,Qt),b(Yo,Qt,null),e(Qt,zh),e(Qt,Uo),e(Uo,Ph),e(Uo,gl),e(gl,qh),e(Uo,Ch),e(qe,xh),e(qe,wn),b(Go,wn,null),e(wn,jh),e(wn,_l),e(_l,Lh),e(wn,Ah),b(Zo,wn,null),e(wn,Dh),e(wn,et),e(et,Ih),e(et,Tl),e(Tl,Sh),e(et,Oh),e(et,Fl),e(Fl,Nh),e(et,Bh),e(qe,Wh),e(qe,vl),h(i,Pc,f),h(i,nt,f),e(nt,Rt),e(Rt,kl),b(Ko,kl,null),e(nt,Qh),e(nt,wl),e(wl,Rh),h(i,qc,f),h(i,Ze,f),b(Xo,Ze,null),e(Ze,Hh),e(Ze,Jo),e(Jo,Vh),e(Jo,bl),e(bl,Yh),e(Jo,Uh),e(Ze,Gh),e(Ze,Ht),e(Ht,ti),e(ti,Zh),e(Ht,Kh),e(Ht,oi),e(oi,Xh),e(Ht,Jh),e(Ze,ef),e(Ze,es),e(es,nf),e(es,si),e(si,tf),e(es,of),e(Ze,sf),e(Ze,bn),b(ns,bn,null),e(bn,rf),e(bn,yl),e(yl,af),e(bn,lf),b(ts,bn,null),e(bn,df),e(bn,tt),e(tt,cf),e(tt,$l),e($l,uf),e(tt,pf),e(tt,El),e(El,hf),e(tt,ff),h(i,Cc,f),h(i,ot,f),e(ot,Vt),e(Vt,Ml),b(os,Ml,null),e(ot,mf),e(ot,zl),e(zl,gf),h(i,xc,f),h(i,st,f),b(ss,st,null),e(st,_f),e(st,rs),e(rs,Tf),e(rs,ri),e(ri,Ff),e(rs,vf),h(i,jc,f),h(i,rt,f),b(as,rt,null),e(rt,kf),e(rt,is),e(is,wf),e(is,ai),e(ai,bf),e(is,yf),h(i,Lc,f),h(i,at,f),e(at,Yt),e(Yt,Pl),b(ls,Pl,null),e(at,$f),e(at,ql),e(ql,Ef),h(i,Ac,f),h(i,We,f),b(ds,We,null),e(We,Mf),e(We,Cl),e(Cl,zf),e(We,Pf),e(We,cs),e(cs,qf),e(cs,us),e(us,Cf),e(cs,xf),e(We,jf),e(We,ps),e(ps,Lf),e(ps,ii),e(ii,Af),e(ps,Df),e(We,If),e(We,hs),e(hs,Sf),e(hs,fs),e(fs,Of),e(hs,Nf),e(We,Bf),e(We,Ke),b(ms,Ke,null),e(Ke,Wf),e(Ke,it),e(it,Qf),e(it,li),e(li,Rf),e(it,Hf),e(it,xl),e(xl,Vf),e(it,Yf),e(Ke,Uf),b(Ut,Ke,null),e(Ke,Gf),e(Ke,jl),e(jl,Zf),e(Ke,Kf),b(gs,Ke,null),h(i,Dc,f),h(i,lt,f),e(lt,Gt),e(Gt,Ll),b(_s,Ll,null),e(lt,Xf),e(lt,Al),e(Al,Jf),h(i,Ic,f),h(i,Qe,f),b(Ts,Qe,null),e(Qe,em),e(Qe,Dl),e(Dl,nm),e(Qe,tm),e(Qe,Fs),e(Fs,om),e(Fs,vs),e(vs,sm),e(Fs,rm),e(Qe,am),e(Qe,ks),e(ks,im),e(ks,di),e(di,lm),e(ks,dm),e(Qe,cm),e(Qe,ws),e(ws,um),e(ws,bs),e(bs,pm),e(ws,hm),e(Qe,fm),e(Qe,Xe),b(ys,Xe,null),e(Xe,mm),e(Xe,dt),e(dt,gm),e(dt,ci),e(ci,_m),e(dt,Tm),e(dt,Il),e(Il,Fm),e(dt,vm),e(Xe,km),b(Zt,Xe,null),e(Xe,wm),e(Xe,Sl),e(Sl,bm),e(Xe,ym),b($s,Xe,null),h(i,Sc,f),h(i,ct,f),e(ct,Kt),e(Kt,Ol),b(Es,Ol,null),e(ct,$m),e(ct,Nl),e(Nl,Em),h(i,Oc,f),h(i,Ms,f),e(Ms,Je),b(zs,Je,null),e(Je,Mm),e(Je,ut),e(ut,zm),e(ut,ui),e(ui,Pm),e(ut,qm),e(ut,Bl),e(Bl,Cm),e(ut,xm),e(Je,jm),b(Xt,Je,null),e(Je,Lm),e(Je,Wl),e(Wl,Am),e(Je,Dm),b(Ps,Je,null),h(i,Nc,f),h(i,pt,f),e(pt,Jt),e(Jt,Ql),b(qs,Ql,null),e(pt,Im),e(pt,Rl),e(Rl,Sm),h(i,Bc,f),h(i,Re,f),b(Cs,Re,null),e(Re,Om),e(Re,xs),e(xs,Nm),e(xs,Hl),e(Hl,Bm),e(xs,Wm),e(Re,Qm),e(Re,js),e(js,Rm),e(js,Ls),e(Ls,Hm),e(js,Vm),e(Re,Ym),e(Re,As),e(As,Um),e(As,pi),e(pi,Gm),e(As,Zm),e(Re,Km),e(Re,Ds),e(Ds,Xm),e(Ds,Is),e(Is,Jm),e(Ds,eg),e(Re,ng),e(Re,en),b(Ss,en,null),e(en,tg),e(en,ht),e(ht,og),e(ht,hi),e(hi,sg),e(ht,rg),e(ht,Vl),e(Vl,ag),e(ht,ig),e(en,lg),b(eo,en,null),e(en,dg),e(en,Yl),e(Yl,cg),e(en,ug),b(Os,en,null),h(i,Wc,f),h(i,ft,f),e(ft,no),e(no,Ul),b(Ns,Ul,null),e(ft,pg),e(ft,Gl),e(Gl,hg),h(i,Qc,f),h(i,He,f),b(Bs,He,null),e(He,fg),e(He,Zl),e(Zl,mg),e(He,gg),e(He,Ws),e(Ws,_g),e(Ws,Qs),e(Qs,Tg),e(Ws,Fg),e(He,vg),e(He,Rs),e(Rs,kg),e(Rs,fi),e(fi,wg),e(Rs,bg),e(He,yg),e(He,Hs),e(Hs,$g),e(Hs,Vs),e(Vs,Eg),e(Hs,Mg),e(He,zg),e(He,Be),b(Ys,Be,null),e(Be,Pg),e(Be,mt),e(mt,qg),e(mt,mi),e(mi,Cg),e(mt,xg),e(mt,Kl),e(Kl,jg),e(mt,Lg),e(Be,Ag),b(to,Be,null),e(Be,Dg),e(Be,Xl),e(Xl,Ig),e(Be,Sg),b(Us,Be,null),e(Be,Og),e(Be,Jl),e(Jl,Ng),e(Be,Bg),b(Gs,Be,null),h(i,Rc,f),h(i,gt,f),e(gt,oo),e(oo,ed),b(Zs,ed,null),e(gt,Wg),e(gt,nd),e(nd,Qg),h(i,Hc,f),h(i,Ve,f),b(Ks,Ve,null),e(Ve,Rg),e(Ve,td),e(td,Hg),e(Ve,Vg),e(Ve,Xs),e(Xs,Yg),e(Xs,Js),e(Js,Ug),e(Xs,Gg),e(Ve,Zg),e(Ve,er),e(er,Kg),e(er,gi),e(gi,Xg),e(er,Jg),e(Ve,e_),e(Ve,nr),e(nr,n_),e(nr,tr),e(tr,t_),e(nr,o_),e(Ve,s_),e(Ve,nn),b(or,nn,null),e(nn,r_),e(nn,_t),e(_t,a_),e(_t,_i),e(_i,i_),e(_t,l_),e(_t,od),e(od,d_),e(_t,c_),e(nn,u_),b(so,nn,null),e(nn,p_),e(nn,sd),e(sd,h_),e(nn,f_),b(sr,nn,null),h(i,Vc,f),h(i,Tt,f),e(Tt,ro),e(ro,rd),b(rr,rd,null),e(Tt,m_),e(Tt,ad),e(ad,g_),h(i,Yc,f),h(i,Ye,f),b(ar,Ye,null),e(Ye,__),e(Ye,id),e(id,T_),e(Ye,F_),e(Ye,ir),e(ir,v_),e(ir,lr),e(lr,k_),e(ir,w_),e(Ye,b_),e(Ye,dr),e(dr,y_),e(dr,Ti),e(Ti,$_),e(dr,E_),e(Ye,M_),e(Ye,cr),e(cr,z_),e(cr,ur),e(ur,P_),e(cr,q_),e(Ye,C_),e(Ye,tn),b(pr,tn,null),e(tn,x_),e(tn,Ft),e(Ft,j_),e(Ft,Fi),e(Fi,L_),e(Ft,A_),e(Ft,ld),e(ld,D_),e(Ft,I_),e(tn,S_),b(ao,tn,null),e(tn,O_),e(tn,dd),e(dd,N_),e(tn,B_),b(hr,tn,null),h(i,Uc,f),h(i,vt,f),e(vt,io),e(io,cd),b(fr,cd,null),e(vt,W_),e(vt,ud),e(ud,Q_),h(i,Gc,f),h(i,Ue,f),b(mr,Ue,null),e(Ue,R_),e(Ue,kt),e(kt,H_),e(kt,pd),e(pd,V_),e(kt,Y_),e(kt,hd),e(hd,U_),e(kt,G_),e(Ue,Z_),e(Ue,gr),e(gr,K_),e(gr,_r),e(_r,X_),e(gr,J_),e(Ue,eT),e(Ue,Tr),e(Tr,nT),e(Tr,vi),e(vi,tT),e(Tr,oT),e(Ue,sT),e(Ue,Fr),e(Fr,rT),e(Fr,vr),e(vr,aT),e(Fr,iT),e(Ue,lT),e(Ue,on),b(kr,on,null),e(on,dT),e(on,wt),e(wt,cT),e(wt,ki),e(ki,uT),e(wt,pT),e(wt,fd),e(fd,hT),e(wt,fT),e(on,mT),b(lo,on,null),e(on,gT),e(on,md),e(md,_T),e(on,TT),b(wr,on,null),h(i,Zc,f),h(i,bt,f),e(bt,co),e(co,gd),b(br,gd,null),e(bt,FT),e(bt,_d),e(_d,vT),h(i,Kc,f),h(i,xe,f),b(yr,xe,null),e(xe,kT),e(xe,Td),e(Td,wT),e(xe,bT),e(xe,$r),e($r,yT),e($r,Er),e(Er,$T),e($r,ET),e(xe,MT),e(xe,Mr),e(Mr,zT),e(Mr,wi),e(wi,PT),e(Mr,qT),e(xe,CT),e(xe,zr),e(zr,xT),e(zr,Pr),e(Pr,jT),e(zr,LT),e(xe,AT),b(uo,xe,null),e(xe,DT),e(xe,sn),b(qr,sn,null),e(sn,IT),e(sn,yt),e(yt,ST),e(yt,bi),e(bi,OT),e(yt,NT),e(yt,Fd),e(Fd,BT),e(yt,WT),e(sn,QT),b(po,sn,null),e(sn,RT),e(sn,vd),e(vd,HT),e(sn,VT),b(Cr,sn,null),h(i,Xc,f),h(i,$t,f),e($t,ho),e(ho,kd),b(xr,kd,null),e($t,YT),e($t,wd),e(wd,UT),h(i,Jc,f),h(i,je,f),b(jr,je,null),e(je,GT),e(je,bd),e(bd,ZT),e(je,KT),e(je,Lr),e(Lr,XT),e(Lr,Ar),e(Ar,JT),e(Lr,eF),e(je,nF),e(je,Dr),e(Dr,tF),e(Dr,yi),e(yi,oF),e(Dr,sF),e(je,rF),e(je,Ir),e(Ir,aF),e(Ir,Sr),e(Sr,iF),e(Ir,lF),e(je,dF),b(fo,je,null),e(je,cF),e(je,rn),b(Or,rn,null),e(rn,uF),e(rn,Et),e(Et,pF),e(Et,$i),e($i,hF),e(Et,fF),e(Et,yd),e(yd,mF),e(Et,gF),e(rn,_F),b(mo,rn,null),e(rn,TF),e(rn,$d),e($d,FF),e(rn,vF),b(Nr,rn,null),h(i,eu,f),h(i,Mt,f),e(Mt,go),e(go,Ed),b(Br,Ed,null),e(Mt,kF),e(Mt,Md),e(Md,wF),h(i,nu,f),h(i,Le,f),b(Wr,Le,null),e(Le,bF),e(Le,zd),e(zd,yF),e(Le,$F),e(Le,Qr),e(Qr,EF),e(Qr,Rr),e(Rr,MF),e(Qr,zF),e(Le,PF),e(Le,Hr),e(Hr,qF),e(Hr,Ei),e(Ei,CF),e(Hr,xF),e(Le,jF),e(Le,Vr),e(Vr,LF),e(Vr,Yr),e(Yr,AF),e(Vr,DF),e(Le,IF),b(_o,Le,null),e(Le,SF),e(Le,an),b(Ur,an,null),e(an,OF),e(an,zt),e(zt,NF),e(zt,Mi),e(Mi,BF),e(zt,WF),e(zt,Pd),e(Pd,QF),e(zt,RF),e(an,HF),b(To,an,null),e(an,VF),e(an,qd),e(qd,YF),e(an,UF),b(Gr,an,null),h(i,tu,f),h(i,Pt,f),e(Pt,Fo),e(Fo,Cd),b(Zr,Cd,null),e(Pt,GF),e(Pt,xd),e(xd,ZF),h(i,ou,f),h(i,Ae,f),b(Kr,Ae,null),e(Ae,KF),e(Ae,Xr),e(Xr,XF),e(Xr,jd),e(jd,JF),e(Xr,ev),e(Ae,nv),e(Ae,Jr),e(Jr,tv),e(Jr,ea),e(ea,ov),e(Jr,sv),e(Ae,rv),e(Ae,na),e(na,av),e(na,zi),e(zi,iv),e(na,lv),e(Ae,dv),e(Ae,ta),e(ta,cv),e(ta,oa),e(oa,uv),e(ta,pv),e(Ae,hv),b(vo,Ae,null),e(Ae,fv),e(Ae,ln),b(sa,ln,null),e(ln,mv),e(ln,qt),e(qt,gv),e(qt,Pi),e(Pi,_v),e(qt,Tv),e(qt,Ld),e(Ld,Fv),e(qt,vv),e(ln,kv),b(ko,ln,null),e(ln,wv),e(ln,Ad),e(Ad,bv),e(ln,yv),b(ra,ln,null),h(i,su,f),h(i,Ct,f),e(Ct,wo),e(wo,Dd),b(aa,Dd,null),e(Ct,$v),e(Ct,Id),e(Id,Ev),h(i,ru,f),h(i,De,f),b(ia,De,null),e(De,Mv),e(De,Sd),e(Sd,zv),e(De,Pv),e(De,la),e(la,qv),e(la,da),e(da,Cv),e(la,xv),e(De,jv),e(De,ca),e(ca,Lv),e(ca,qi),e(qi,Av),e(ca,Dv),e(De,Iv),e(De,ua),e(ua,Sv),e(ua,pa),e(pa,Ov),e(ua,Nv),e(De,Bv),b(bo,De,null),e(De,Wv),e(De,dn),b(ha,dn,null),e(dn,Qv),e(dn,xt),e(xt,Rv),e(xt,Ci),e(Ci,Hv),e(xt,Vv),e(xt,Od),e(Od,Yv),e(xt,Uv),e(dn,Gv),b(yo,dn,null),e(dn,Zv),e(dn,Nd),e(Nd,Kv),e(dn,Xv),b(fa,dn,null),h(i,au,f),h(i,jt,f),e(jt,$o),e($o,Bd),b(ma,Bd,null),e(jt,Jv),e(jt,Wd),e(Wd,ek),h(i,iu,f),h(i,Ie,f),b(ga,Ie,null),e(Ie,nk),e(Ie,Qd),e(Qd,tk),e(Ie,ok),e(Ie,_a),e(_a,sk),e(_a,Ta),e(Ta,rk),e(_a,ak),e(Ie,ik),e(Ie,Fa),e(Fa,lk),e(Fa,xi),e(xi,dk),e(Fa,ck),e(Ie,uk),e(Ie,va),e(va,pk),e(va,ka),e(ka,hk),e(va,fk),e(Ie,mk),b(Eo,Ie,null),e(Ie,gk),e(Ie,cn),b(wa,cn,null),e(cn,_k),e(cn,Lt),e(Lt,Tk),e(Lt,ji),e(ji,Fk),e(Lt,vk),e(Lt,Rd),e(Rd,kk),e(Lt,wk),e(cn,bk),b(Mo,cn,null),e(cn,yk),e(cn,Hd),e(Hd,$k),e(cn,Ek),b(ba,cn,null),h(i,lu,f),h(i,At,f),e(At,zo),e(zo,Vd),b(ya,Vd,null),e(At,Mk),e(At,Yd),e(Yd,zk),h(i,du,f),h(i,Se,f),b($a,Se,null),e(Se,Pk),e(Se,Ud),e(Ud,qk),e(Se,Ck),e(Se,Ea),e(Ea,xk),e(Ea,Ma),e(Ma,jk),e(Ea,Lk),e(Se,Ak),e(Se,za),e(za,Dk),e(za,Li),e(Li,Ik),e(za,Sk),e(Se,Ok),e(Se,Pa),e(Pa,Nk),e(Pa,qa),e(qa,Bk),e(Pa,Wk),e(Se,Qk),b(Po,Se,null),e(Se,Rk),e(Se,un),b(Ca,un,null),e(un,Hk),e(un,Dt),e(Dt,Vk),e(Dt,Ai),e(Ai,Yk),e(Dt,Uk),e(Dt,Gd),e(Gd,Gk),e(Dt,Zk),e(un,Kk),b(qo,un,null),e(un,Xk),e(un,Zd),e(Zd,Jk),e(un,ew),b(xa,un,null),h(i,cu,f),h(i,It,f),e(It,Co),e(Co,Kd),b(ja,Kd,null),e(It,nw),e(It,Xd),e(Xd,tw),h(i,uu,f),h(i,Oe,f),b(La,Oe,null),e(Oe,ow),e(Oe,St),e(St,sw),e(St,Jd),e(Jd,rw),e(St,aw),e(St,ec),e(ec,iw),e(St,lw),e(Oe,dw),e(Oe,Aa),e(Aa,cw),e(Aa,Da),e(Da,uw),e(Aa,pw),e(Oe,hw),e(Oe,Ia),e(Ia,fw),e(Ia,Di),e(Di,mw),e(Ia,gw),e(Oe,_w),e(Oe,Sa),e(Sa,Tw),e(Sa,Oa),e(Oa,Fw),e(Sa,vw),e(Oe,kw),b(xo,Oe,null),e(Oe,ww),e(Oe,pn),b(Na,pn,null),e(pn,bw),e(pn,Ot),e(Ot,yw),e(Ot,Ii),e(Ii,$w),e(Ot,Ew),e(Ot,nc),e(nc,Mw),e(Ot,zw),e(pn,Pw),b(jo,pn,null),e(pn,qw),e(pn,tc),e(tc,Cw),e(pn,xw),b(Ba,pn,null),pu=!0},p(i,[f]){const Wa={};f&2&&(Wa.$$scope={dirty:f,ctx:i}),Ut.$set(Wa);const oc={};f&2&&(oc.$$scope={dirty:f,ctx:i}),Zt.$set(oc);const sc={};f&2&&(sc.$$scope={dirty:f,ctx:i}),Xt.$set(sc);const rc={};f&2&&(rc.$$scope={dirty:f,ctx:i}),eo.$set(rc);const Qa={};f&2&&(Qa.$$scope={dirty:f,ctx:i}),to.$set(Qa);const ac={};f&2&&(ac.$$scope={dirty:f,ctx:i}),so.$set(ac);const ic={};f&2&&(ic.$$scope={dirty:f,ctx:i}),ao.$set(ic);const lc={};f&2&&(lc.$$scope={dirty:f,ctx:i}),lo.$set(lc);const Ra={};f&2&&(Ra.$$scope={dirty:f,ctx:i}),uo.$set(Ra);const dc={};f&2&&(dc.$$scope={dirty:f,ctx:i}),po.$set(dc);const cc={};f&2&&(cc.$$scope={dirty:f,ctx:i}),fo.$set(cc);const uc={};f&2&&(uc.$$scope={dirty:f,ctx:i}),mo.$set(uc);const pc={};f&2&&(pc.$$scope={dirty:f,ctx:i}),_o.$set(pc);const hc={};f&2&&(hc.$$scope={dirty:f,ctx:i}),To.$set(hc);const Ha={};f&2&&(Ha.$$scope={dirty:f,ctx:i}),vo.$set(Ha);const fc={};f&2&&(fc.$$scope={dirty:f,ctx:i}),ko.$set(fc);const Ce={};f&2&&(Ce.$$scope={dirty:f,ctx:i}),bo.$set(Ce);const mc={};f&2&&(mc.$$scope={dirty:f,ctx:i}),yo.$set(mc);const gc={};f&2&&(gc.$$scope={dirty:f,ctx:i}),Eo.$set(gc);const _c={};f&2&&(_c.$$scope={dirty:f,ctx:i}),Mo.$set(_c);const Tc={};f&2&&(Tc.$$scope={dirty:f,ctx:i}),Po.$set(Tc);const Fc={};f&2&&(Fc.$$scope={dirty:f,ctx:i}),qo.$set(Fc);const vc={};f&2&&(vc.$$scope={dirty:f,ctx:i}),xo.$set(vc);const kc={};f&2&&(kc.$$scope={dirty:f,ctx:i}),jo.$set(kc)},i(i){pu||(y(T.$$.fragment,i),y(ne.$$.fragment,i),y(Oo.$$.fragment,i),y(No.$$.fragment,i),y(Wo.$$.fragment,i),y(Qo.$$.fragment,i),y(Ho.$$.fragment,i),y(Yo.$$.fragment,i),y(Go.$$.fragment,i),y(Zo.$$.fragment,i),y(Ko.$$.fragment,i),y(Xo.$$.fragment,i),y(ns.$$.fragment,i),y(ts.$$.fragment,i),y(os.$$.fragment,i),y(ss.$$.fragment,i),y(as.$$.fragment,i),y(ls.$$.fragment,i),y(ds.$$.fragment,i),y(ms.$$.fragment,i),y(Ut.$$.fragment,i),y(gs.$$.fragment,i),y(_s.$$.fragment,i),y(Ts.$$.fragment,i),y(ys.$$.fragment,i),y(Zt.$$.fragment,i),y($s.$$.fragment,i),y(Es.$$.fragment,i),y(zs.$$.fragment,i),y(Xt.$$.fragment,i),y(Ps.$$.fragment,i),y(qs.$$.fragment,i),y(Cs.$$.fragment,i),y(Ss.$$.fragment,i),y(eo.$$.fragment,i),y(Os.$$.fragment,i),y(Ns.$$.fragment,i),y(Bs.$$.fragment,i),y(Ys.$$.fragment,i),y(to.$$.fragment,i),y(Us.$$.fragment,i),y(Gs.$$.fragment,i),y(Zs.$$.fragment,i),y(Ks.$$.fragment,i),y(or.$$.fragment,i),y(so.$$.fragment,i),y(sr.$$.fragment,i),y(rr.$$.fragment,i),y(ar.$$.fragment,i),y(pr.$$.fragment,i),y(ao.$$.fragment,i),y(hr.$$.fragment,i),y(fr.$$.fragment,i),y(mr.$$.fragment,i),y(kr.$$.fragment,i),y(lo.$$.fragment,i),y(wr.$$.fragment,i),y(br.$$.fragment,i),y(yr.$$.fragment,i),y(uo.$$.fragment,i),y(qr.$$.fragment,i),y(po.$$.fragment,i),y(Cr.$$.fragment,i),y(xr.$$.fragment,i),y(jr.$$.fragment,i),y(fo.$$.fragment,i),y(Or.$$.fragment,i),y(mo.$$.fragment,i),y(Nr.$$.fragment,i),y(Br.$$.fragment,i),y(Wr.$$.fragment,i),y(_o.$$.fragment,i),y(Ur.$$.fragment,i),y(To.$$.fragment,i),y(Gr.$$.fragment,i),y(Zr.$$.fragment,i),y(Kr.$$.fragment,i),y(vo.$$.fragment,i),y(sa.$$.fragment,i),y(ko.$$.fragment,i),y(ra.$$.fragment,i),y(aa.$$.fragment,i),y(ia.$$.fragment,i),y(bo.$$.fragment,i),y(ha.$$.fragment,i),y(yo.$$.fragment,i),y(fa.$$.fragment,i),y(ma.$$.fragment,i),y(ga.$$.fragment,i),y(Eo.$$.fragment,i),y(wa.$$.fragment,i),y(Mo.$$.fragment,i),y(ba.$$.fragment,i),y(ya.$$.fragment,i),y($a.$$.fragment,i),y(Po.$$.fragment,i),y(Ca.$$.fragment,i),y(qo.$$.fragment,i),y(xa.$$.fragment,i),y(ja.$$.fragment,i),y(La.$$.fragment,i),y(xo.$$.fragment,i),y(Na.$$.fragment,i),y(jo.$$.fragment,i),y(Ba.$$.fragment,i),pu=!0)},o(i){$(T.$$.fragment,i),$(ne.$$.fragment,i),$(Oo.$$.fragment,i),$(No.$$.fragment,i),$(Wo.$$.fragment,i),$(Qo.$$.fragment,i),$(Ho.$$.fragment,i),$(Yo.$$.fragment,i),$(Go.$$.fragment,i),$(Zo.$$.fragment,i),$(Ko.$$.fragment,i),$(Xo.$$.fragment,i),$(ns.$$.fragment,i),$(ts.$$.fragment,i),$(os.$$.fragment,i),$(ss.$$.fragment,i),$(as.$$.fragment,i),$(ls.$$.fragment,i),$(ds.$$.fragment,i),$(ms.$$.fragment,i),$(Ut.$$.fragment,i),$(gs.$$.fragment,i),$(_s.$$.fragment,i),$(Ts.$$.fragment,i),$(ys.$$.fragment,i),$(Zt.$$.fragment,i),$($s.$$.fragment,i),$(Es.$$.fragment,i),$(zs.$$.fragment,i),$(Xt.$$.fragment,i),$(Ps.$$.fragment,i),$(qs.$$.fragment,i),$(Cs.$$.fragment,i),$(Ss.$$.fragment,i),$(eo.$$.fragment,i),$(Os.$$.fragment,i),$(Ns.$$.fragment,i),$(Bs.$$.fragment,i),$(Ys.$$.fragment,i),$(to.$$.fragment,i),$(Us.$$.fragment,i),$(Gs.$$.fragment,i),$(Zs.$$.fragment,i),$(Ks.$$.fragment,i),$(or.$$.fragment,i),$(so.$$.fragment,i),$(sr.$$.fragment,i),$(rr.$$.fragment,i),$(ar.$$.fragment,i),$(pr.$$.fragment,i),$(ao.$$.fragment,i),$(hr.$$.fragment,i),$(fr.$$.fragment,i),$(mr.$$.fragment,i),$(kr.$$.fragment,i),$(lo.$$.fragment,i),$(wr.$$.fragment,i),$(br.$$.fragment,i),$(yr.$$.fragment,i),$(uo.$$.fragment,i),$(qr.$$.fragment,i),$(po.$$.fragment,i),$(Cr.$$.fragment,i),$(xr.$$.fragment,i),$(jr.$$.fragment,i),$(fo.$$.fragment,i),$(Or.$$.fragment,i),$(mo.$$.fragment,i),$(Nr.$$.fragment,i),$(Br.$$.fragment,i),$(Wr.$$.fragment,i),$(_o.$$.fragment,i),$(Ur.$$.fragment,i),$(To.$$.fragment,i),$(Gr.$$.fragment,i),$(Zr.$$.fragment,i),$(Kr.$$.fragment,i),$(vo.$$.fragment,i),$(sa.$$.fragment,i),$(ko.$$.fragment,i),$(ra.$$.fragment,i),$(aa.$$.fragment,i),$(ia.$$.fragment,i),$(bo.$$.fragment,i),$(ha.$$.fragment,i),$(yo.$$.fragment,i),$(fa.$$.fragment,i),$(ma.$$.fragment,i),$(ga.$$.fragment,i),$(Eo.$$.fragment,i),$(wa.$$.fragment,i),$(Mo.$$.fragment,i),$(ba.$$.fragment,i),$(ya.$$.fragment,i),$($a.$$.fragment,i),$(Po.$$.fragment,i),$(Ca.$$.fragment,i),$(qo.$$.fragment,i),$(xa.$$.fragment,i),$(ja.$$.fragment,i),$(La.$$.fragment,i),$(xo.$$.fragment,i),$(Na.$$.fragment,i),$(jo.$$.fragment,i),$(Ba.$$.fragment,i),pu=!1},d(i){n(p),i&&n(M),i&&n(m),E(T),i&&n(G),i&&n(P),E(ne),i&&n(ie),i&&n(U),i&&n(x),i&&n(oe),i&&n(le),i&&n(se),i&&n(de),i&&n(C),i&&n(B),i&&n(J),i&&n(yc),i&&n(jn),i&&n($c),i&&n(Kn),E(Oo),i&&n(Ec),i&&n(Cn),E(No),i&&n(Mc),i&&n(Jn),E(Wo),i&&n(zc),i&&n(qe),E(Qo),E(Ho),E(Yo),E(Go),E(Zo),i&&n(Pc),i&&n(nt),E(Ko),i&&n(qc),i&&n(Ze),E(Xo),E(ns),E(ts),i&&n(Cc),i&&n(ot),E(os),i&&n(xc),i&&n(st),E(ss),i&&n(jc),i&&n(rt),E(as),i&&n(Lc),i&&n(at),E(ls),i&&n(Ac),i&&n(We),E(ds),E(ms),E(Ut),E(gs),i&&n(Dc),i&&n(lt),E(_s),i&&n(Ic),i&&n(Qe),E(Ts),E(ys),E(Zt),E($s),i&&n(Sc),i&&n(ct),E(Es),i&&n(Oc),i&&n(Ms),E(zs),E(Xt),E(Ps),i&&n(Nc),i&&n(pt),E(qs),i&&n(Bc),i&&n(Re),E(Cs),E(Ss),E(eo),E(Os),i&&n(Wc),i&&n(ft),E(Ns),i&&n(Qc),i&&n(He),E(Bs),E(Ys),E(to),E(Us),E(Gs),i&&n(Rc),i&&n(gt),E(Zs),i&&n(Hc),i&&n(Ve),E(Ks),E(or),E(so),E(sr),i&&n(Vc),i&&n(Tt),E(rr),i&&n(Yc),i&&n(Ye),E(ar),E(pr),E(ao),E(hr),i&&n(Uc),i&&n(vt),E(fr),i&&n(Gc),i&&n(Ue),E(mr),E(kr),E(lo),E(wr),i&&n(Zc),i&&n(bt),E(br),i&&n(Kc),i&&n(xe),E(yr),E(uo),E(qr),E(po),E(Cr),i&&n(Xc),i&&n($t),E(xr),i&&n(Jc),i&&n(je),E(jr),E(fo),E(Or),E(mo),E(Nr),i&&n(eu),i&&n(Mt),E(Br),i&&n(nu),i&&n(Le),E(Wr),E(_o),E(Ur),E(To),E(Gr),i&&n(tu),i&&n(Pt),E(Zr),i&&n(ou),i&&n(Ae),E(Kr),E(vo),E(sa),E(ko),E(ra),i&&n(su),i&&n(Ct),E(aa),i&&n(ru),i&&n(De),E(ia),E(bo),E(ha),E(yo),E(fa),i&&n(au),i&&n(jt),E(ma),i&&n(iu),i&&n(Ie),E(ga),E(Eo),E(wa),E(Mo),E(ba),i&&n(lu),i&&n(At),E(ya),i&&n(du),i&&n(Se),E($a),E(Po),E(Ca),E(qo),E(xa),i&&n(cu),i&&n(It),E(ja),i&&n(uu),i&&n(Oe),E(La),E(xo),E(Na),E(jo),E(Ba)}}}const U2={local:"funnel-transformer",sections:[{local:"overview",title:"Overview"},{local:"transformers.FunnelConfig",title:"FunnelConfig"},{local:"transformers.FunnelTokenizer",title:"FunnelTokenizer"},{local:"transformers.FunnelTokenizerFast",title:"FunnelTokenizerFast"},{local:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",title:"Funnel specific outputs"},{local:"transformers.FunnelBaseModel",title:"FunnelBaseModel"},{local:"transformers.FunnelModel",title:"FunnelModel"},{local:"transformers.FunnelForPreTraining",title:"FunnelModelForPreTraining"},{local:"transformers.FunnelForMaskedLM",title:"FunnelForMaskedLM"},{local:"transformers.FunnelForSequenceClassification",title:"FunnelForSequenceClassification"},{local:"transformers.FunnelForMultipleChoice",title:"FunnelForMultipleChoice"},{local:"transformers.FunnelForTokenClassification",title:"FunnelForTokenClassification"},{local:"transformers.FunnelForQuestionAnswering",title:"FunnelForQuestionAnswering"},{local:"transformers.TFFunnelBaseModel",title:"TFFunnelBaseModel"},{local:"transformers.TFFunnelModel",title:"TFFunnelModel"},{local:"transformers.TFFunnelForPreTraining",title:"TFFunnelModelForPreTraining"},{local:"transformers.TFFunnelForMaskedLM",title:"TFFunnelForMaskedLM"},{local:"transformers.TFFunnelForSequenceClassification",title:"TFFunnelForSequenceClassification"},{local:"transformers.TFFunnelForMultipleChoice",title:"TFFunnelForMultipleChoice"},{local:"transformers.TFFunnelForTokenClassification",title:"TFFunnelForTokenClassification"},{local:"transformers.TFFunnelForQuestionAnswering",title:"TFFunnelForQuestionAnswering"}],title:"Funnel Transformer"};function G2(W,p,M){let{fw:m}=p;return W.$$set=g=>{"fw"in g&&M(0,m=g.fw)},[m]}class t$ extends F2{constructor(p){super();v2(this,p,G2,Y2,k2,{fw:0})}}export{t$ as default,U2 as metadata};
