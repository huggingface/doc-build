import{S as Yf,i as Gf,s as Qf,e as n,k as f,w as d,t as a,M as Jf,c as l,d as s,m as u,a as i,x as _,h as r,b as m,F as t,g as p,y as g,q as v,o as y,B as $}from"../chunks/vendor-4833417e.js";import{T as ba}from"../chunks/Tip-fffd6df1.js";import{Y as Bf}from"../chunks/Youtube-27813aed.js";import{I as Ne}from"../chunks/IconCopyLink-4b81c553.js";import{C as D}from"../chunks/CodeBlock-6a3d1b46.js";import{C as pe}from"../chunks/CodeBlockFw-27a176a0.js";import{D as Kf}from"../chunks/DocNotebookDropdown-ecff2a90.js";import"../chunks/CopyButton-dacfbfaf.js";function Vf(N){let h,k;return{c(){h=n("p"),k=a(`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`)},l(c){h=l(c,"P",{});var w=i(h);k=r(w,`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`),w.forEach(s)},m(c,w){p(c,h,w),t(h,k)},d(c){c&&s(h)}}}function Zf(N){let h,k,c,w,A,b,j,x;return{c(){h=n("p"),k=a("For more details about the "),c=n("a"),w=a("pipeline()"),A=a(" and associated tasks, refer to the documentation "),b=n("a"),j=a("here"),x=a("."),this.h()},l(S){h=l(S,"P",{});var E=i(h);k=r(E,"For more details about the "),c=l(E,"A",{href:!0});var O=i(c);w=r(O,"pipeline()"),O.forEach(s),A=r(E," and associated tasks, refer to the documentation "),b=l(E,"A",{href:!0});var R=i(b);j=r(R,"here"),R.forEach(s),x=r(E,"."),E.forEach(s),this.h()},h(){m(c,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(b,"href","./main_classes/pipelines")},m(S,E){p(S,h,E),t(h,k),t(h,c),t(c,w),t(h,A),t(h,b),t(b,j),t(h,x)},d(S){S&&s(h)}}}function Xf(N){let h,k,c,w,A,b,j,x;return{c(){h=n("p"),k=a("See the "),c=n("a"),w=a("task summary"),A=a(" for which "),b=n("a"),j=a("AutoModel"),x=a(" class to use for which task."),this.h()},l(S){h=l(S,"P",{});var E=i(h);k=r(E,"See the "),c=l(E,"A",{href:!0});var O=i(c);w=r(O,"task summary"),O.forEach(s),A=r(E," for which "),b=l(E,"A",{href:!0});var R=i(b);j=r(R,"AutoModel"),R.forEach(s),x=r(E," class to use for which task."),E.forEach(s),this.h()},h(){m(c,"href","./task_summary"),m(b,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModel")},m(S,E){p(S,h,E),t(h,k),t(h,c),t(c,w),t(h,A),t(h,b),t(b,j),t(h,x)},d(S){S&&s(h)}}}function eu(N){let h,k,c,w,A;return{c(){h=n("p"),k=a("All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),c=n("em"),w=a("before"),A=a(` the final activation
function (like softmax) because the final activation function is often fused with the loss.`)},l(b){h=l(b,"P",{});var j=i(h);k=r(j,"All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),c=l(j,"EM",{});var x=i(c);w=r(x,"before"),x.forEach(s),A=r(j,` the final activation
function (like softmax) because the final activation function is often fused with the loss.`),j.forEach(s)},m(b,j){p(b,h,j),t(h,k),t(h,c),t(c,w),t(h,A)},d(b){b&&s(h)}}}function tu(N){let h,k,c,w,A;return{c(){h=n("p"),k=a(`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),c=n("code"),w=a("None"),A=a(" are ignored.")},l(b){h=l(b,"P",{});var j=i(h);k=r(j,`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),c=l(j,"CODE",{});var x=i(c);w=r(x,"None"),x.forEach(s),A=r(j," are ignored."),j.forEach(s)},m(b,j){p(b,h,j),t(h,k),t(h,c),t(c,w),t(h,A)},d(b){b&&s(h)}}}function su(N){let h,k,c,w,A,b,j,x,S,E,O,R,H,fo,Tt,uo,mo,xt,ho,co,wa,fe,ka,te,ue,Ts,Oe,_o,xs,go,ja,Le,qt,vo,yo,Ea,De,Aa,me,$o,zt,bo,wo,Ta,Re,qs,ko,jo,xa,T,zs,Eo,Ao,Fs,To,xo,Ps,qo,zo,Ss,Fo,Po,Ms,So,Mo,Cs,Co,Io,Is,No,Oo,Ns,Lo,qa,He,Os,Do,Ro,za,U,Ls,Ho,Uo,Ds,Wo,Bo,Rs,Yo,Fa,Ue,Hs,Go,Qo,Pa,he,Us,Jo,Ko,Ws,Vo,Sa,ce,Ma,se,de,Bs,We,Zo,Ys,Xo,Ca,_e,en,Ft,tn,sn,Ia,Pt,an,Na,Be,Oa,ge,rn,St,on,nn,La,Ye,Da,W,ln,Ge,pn,fn,Gs,un,mn,Ra,Qe,Ha,ve,hn,Mt,cn,dn,Ua,Je,Wa,B,_n,Ct,gn,vn,Ke,yn,$n,Ba,Ve,Ya,ye,bn,It,wn,kn,Ga,Ze,Qa,Y,jn,Xe,En,An,et,Tn,xn,Ja,tt,Ka,Nt,qn,Va,st,Za,$e,zn,Ot,Fn,Pn,Xa,ae,be,Qs,at,Sn,Js,Mn,er,M,Cn,Lt,In,Nn,rt,On,Ln,Dt,Dn,Rn,ot,Hn,Un,tr,nt,sr,G,Wn,Rt,Bn,Yn,Ks,Gn,Qn,ar,lt,rr,Q,Jn,Ht,Kn,Vn,Vs,Zn,Xn,or,it,nr,J,el,Ut,tl,sl,Wt,al,rl,lr,re,we,Zs,pt,ol,Xs,nl,ir,ft,pr,q,ll,Bt,il,pl,Yt,fl,ul,Gt,ml,hl,Qt,cl,dl,ea,_l,gl,Jt,vl,yl,fr,K,$l,ta,bl,wl,Kt,kl,jl,ur,oe,ke,sa,ut,El,aa,Al,mr,V,Tl,ra,xl,ql,Vt,zl,Fl,hr,je,Pl,Zt,Sl,Ml,cr,mt,dr,Ee,Cl,oa,Il,Nl,_r,Xt,Ol,gr,ht,vr,es,Ll,yr,Ae,ts,ss,Dl,Rl,Hl,as,rs,Ul,Wl,$r,Te,Bl,os,Yl,Gl,br,ct,wr,xe,Ql,ns,Jl,Kl,kr,ne,qe,na,dt,Vl,la,Zl,jr,F,Xl,ls,ei,ti,is,si,ai,ps,ri,oi,fs,ni,li,us,ii,pi,Er,_t,Ar,ze,Tr,Fe,fi,ia,ui,mi,xr,gt,qr,Z,hi,pa,ci,di,fa,_i,gi,zr,vt,Fr,Pe,Pr,z,vi,yt,ua,yi,$i,$t,ma,bi,wi,ms,ki,ji,ha,Ei,Ai,bt,Ti,xi,hs,qi,zi,Sr,Se,Mr,le,Me,ca,wt,Fi,da,Pi,Cr,Ce,Si,cs,Mi,Ci,Ir,kt,Nr,Ie,Ii,ds,Ni,Oi,Or,jt,Lr,X,Li,_a,Di,Ri,ga,Hi,Ui,Dr,Et,Rr;return b=new Ne({}),O=new Kf({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/tensorflow/quicktour.ipynb"}]}}),fe=new ba({props:{$$slots:{default:[Vf]},$$scope:{ctx:N}}}),Oe=new Ne({}),De=new Bf({props:{id:"tiZFewofSLM"}}),ce=new ba({props:{$$slots:{default:[Zf]},$$scope:{ctx:N}}}),We=new Ne({}),Be=new pe({props:{group1:{id:"pt",code:"pip install torch",highlighted:"pip install torch"},group2:{id:"tf",code:"pip install tensorflow",highlighted:"pip install tensorflow"}}}),Ye=new D({props:{code:`from transformers import pipeline

classifier = pipeline("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),Qe=new D({props:{code:'classifier("We are very happy to show you the \u{1F917} Transformers library.")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9998</span>}]`}}),Je=new D({props:{code:`results = classifier(["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">{result[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, with score: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>`}}),Ve=new D({props:{code:"pip install datasets ",highlighted:"pip install datasets "}}),Ze=new D({props:{code:`import torch
from transformers import pipeline

speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)`}}),tt=new D({props:{code:`import datasets

dataset = datasets.load_dataset("superb", name="asr", split="test")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> datasets

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = datasets.load_dataset(<span class="hljs-string">&quot;superb&quot;</span>, name=<span class="hljs-string">&quot;asr&quot;</span>, split=<span class="hljs-string">&quot;test&quot;</span>)`}}),st=new D({props:{code:`files = dataset["file"]
speech_recognizer(files[:4])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dataset[<span class="hljs-string">&quot;file&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer(files[:<span class="hljs-number">4</span>])
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOWER FAT AND SAUCE&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;STUFFERED INTO YOU HIS BELLY COUNSELLED HIM&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;HO BERTIE ANY GOOD IN YOUR MIND&#x27;</span>}]`}}),at=new Ne({}),nt=new D({props:{code:'model_name = "nlptown/bert-base-multilingual-uncased-sentiment"',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>'}}),lt=new pe({props:{group1:{id:"pt",code:`from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`},group2:{id:"tf",code:`from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}}),it=new D({props:{code:`classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;5 stars&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.7273</span>}]`}}),pt=new Ne({}),ft=new Bf({props:{id:"AhChOFRegn4"}}),ut=new Ne({}),mt=new D({props:{code:`from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),ht=new D({props:{code:`encoding = tokenizer("We are very happy to show you the \u{1F917} Transformers library.")
print(encoding)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">11312</span>, <span class="hljs-number">10320</span>, <span class="hljs-number">12495</span>, <span class="hljs-number">19308</span>, <span class="hljs-number">10114</span>, <span class="hljs-number">11391</span>, <span class="hljs-number">10855</span>, <span class="hljs-number">10103</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">13299</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),ct=new pe({props:{group1:{id:"pt",code:`pt_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`},group2:{id:"tf",code:`tf_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`}}}),dt=new Ne({}),_t=new pe({props:{group1:{id:"pt",code:`from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`},group2:{id:"tf",code:`from transformers import TFAutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`}}}),ze=new ba({props:{$$slots:{default:[Xf]},$$scope:{ctx:N}}}),gt=new pe({props:{group1:{id:"pt",code:"pt_outputs = pt_model(**pt_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)'},group2:{id:"tf",code:"tf_outputs = tf_model(tf_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)'}}}),vt=new pe({props:{group1:{id:"pt",code:`from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">0.0021</span>, <span class="hljs-number">0.0018</span>, <span class="hljs-number">0.0115</span>, <span class="hljs-number">0.2121</span>, <span class="hljs-number">0.7725</span>],
        [<span class="hljs-number">0.2084</span>, <span class="hljs-number">0.1826</span>, <span class="hljs-number">0.1969</span>, <span class="hljs-number">0.1755</span>, <span class="hljs-number">0.2365</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)`},group2:{id:"tf",code:`import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
print(tf.math.round(tf_predictions * 10**4) / 10**4)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tf.math.<span class="hljs-built_in">round</span>(tf_predictions * <span class="hljs-number">10</span>**<span class="hljs-number">4</span>) / <span class="hljs-number">10</span>**<span class="hljs-number">4</span>)
tf.Tensor(
[[<span class="hljs-number">0.0021</span> <span class="hljs-number">0.0018</span> <span class="hljs-number">0.0116</span> <span class="hljs-number">0.2121</span> <span class="hljs-number">0.7725</span>]
 [<span class="hljs-number">0.2084</span> <span class="hljs-number">0.1826</span> <span class="hljs-number">0.1969</span> <span class="hljs-number">0.1755</span>  <span class="hljs-number">0.2365</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>), dtype=float32)`}}}),Pe=new ba({props:{$$slots:{default:[eu]},$$scope:{ctx:N}}}),Se=new ba({props:{$$slots:{default:[tu]},$$scope:{ctx:N}}}),wt=new Ne({}),kt=new pe({props:{group1:{id:"pt",code:`pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)`},group2:{id:"tf",code:`tf_save_directory = "./tf_save_pretrained"
tokenizer.save_pretrained(tf_save_directory)
tf_model.save_pretrained(tf_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`}}}),jt=new pe({props:{group1:{id:"pt",code:'pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)'},group2:{id:"tf",code:'tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)'}}}),Et=new pe({props:{group1:{id:"pt",code:`from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`},group2:{id:"tf",code:`from transformers import TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`}}}),{c(){h=n("meta"),k=f(),c=n("h1"),w=n("a"),A=n("span"),d(b.$$.fragment),j=f(),x=n("span"),S=a("Quick tour"),E=f(),d(O.$$.fragment),R=f(),H=n("p"),fo=a("Get up and running with \u{1F917} Transformers! Start using the "),Tt=n("a"),uo=a("pipeline()"),mo=a(" for rapid inference, and quickly load a pretrained model and tokenizer with an "),xt=n("a"),ho=a("AutoClass"),co=a(" to solve your text, vision or audio task."),wa=f(),d(fe.$$.fragment),ka=f(),te=n("h2"),ue=n("a"),Ts=n("span"),d(Oe.$$.fragment),_o=f(),xs=n("span"),go=a("Pipeline"),ja=f(),Le=n("p"),qt=n("a"),vo=a("pipeline()"),yo=a(" is the easiest way to use a pretrained model for a given task."),Ea=f(),d(De.$$.fragment),Aa=f(),me=n("p"),$o=a("The "),zt=n("a"),bo=a("pipeline()"),wo=a(" supports many common tasks out-of-the-box:"),Ta=f(),Re=n("p"),qs=n("strong"),ko=a("Text"),jo=a(":"),xa=f(),T=n("ul"),zs=n("li"),Eo=a("Sentiment analysis: classify the polarity of a given text."),Ao=f(),Fs=n("li"),To=a("Text generation (in English): generate text from a given input."),xo=f(),Ps=n("li"),qo=a("Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),zo=f(),Ss=n("li"),Fo=a("Question answering: extract the answer from the context, given some context and a question."),Po=f(),Ms=n("li"),So=a("Fill-mask: fill in the blank given a text with masked words."),Mo=f(),Cs=n("li"),Co=a("Summarization: generate a summary of a long sequence of text or document."),Io=f(),Is=n("li"),No=a("Translation: translate text into another language."),Oo=f(),Ns=n("li"),Lo=a("Feature extraction: create a tensor representation of the text."),qa=f(),He=n("p"),Os=n("strong"),Do=a("Image"),Ro=a(":"),za=f(),U=n("ul"),Ls=n("li"),Ho=a("Image classification: classify an image."),Uo=f(),Ds=n("li"),Wo=a("Image segmentation: classify every pixel in an image."),Bo=f(),Rs=n("li"),Yo=a("Object detection: detect objects within an image."),Fa=f(),Ue=n("p"),Hs=n("strong"),Go=a("Audio"),Qo=a(":"),Pa=f(),he=n("ul"),Us=n("li"),Jo=a("Audio classification: assign a label to a given segment of audio."),Ko=f(),Ws=n("li"),Vo=a("Automatic speech recognition (ASR): transcribe audio data into text."),Sa=f(),d(ce.$$.fragment),Ma=f(),se=n("h3"),de=n("a"),Bs=n("span"),d(We.$$.fragment),Zo=f(),Ys=n("span"),Xo=a("Pipeline usage"),Ca=f(),_e=n("p"),en=a("In the following example, you will use the "),Ft=n("a"),tn=a("pipeline()"),sn=a(" for sentiment analysis."),Ia=f(),Pt=n("p"),an=a("Install the following dependencies if you haven\u2019t already:"),Na=f(),d(Be.$$.fragment),Oa=f(),ge=n("p"),rn=a("Import "),St=n("a"),on=a("pipeline()"),nn=a(" and specify the task you want to complete:"),La=f(),d(Ye.$$.fragment),Da=f(),W=n("p"),ln=a("The pipeline downloads and caches a default "),Ge=n("a"),pn=a("pretrained model"),fn=a(" and tokenizer for sentiment analysis. Now you can use the "),Gs=n("code"),un=a("classifier"),mn=a(" on your target text:"),Ra=f(),d(Qe.$$.fragment),Ha=f(),ve=n("p"),hn=a("For more than one sentence, pass a list of sentences to the "),Mt=n("a"),cn=a("pipeline()"),dn=a(" which returns a list of dictionaries:"),Ua=f(),d(Je.$$.fragment),Wa=f(),B=n("p"),_n=a("The "),Ct=n("a"),gn=a("pipeline()"),vn=a(" can also iterate over an entire dataset. Start by installing the "),Ke=n("a"),yn=a("\u{1F917} Datasets"),$n=a(" library:"),Ba=f(),d(Ve.$$.fragment),Ya=f(),ye=n("p"),bn=a("Create a "),It=n("a"),wn=a("pipeline()"),kn=a(" with the task you want to solve for and the model you want to use."),Ga=f(),d(Ze.$$.fragment),Qa=f(),Y=n("p"),jn=a("Next, load a dataset (see the \u{1F917} Datasets "),Xe=n("a"),En=a("Quick Start"),An=a(" for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),et=n("a"),Tn=a("SUPERB"),xn=a(" dataset:"),Ja=f(),d(tt.$$.fragment),Ka=f(),Nt=n("p"),qn=a("You can pass a whole dataset pipeline:"),Va=f(),d(st.$$.fragment),Za=f(),$e=n("p"),zn=a("For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the "),Ot=n("a"),Fn=a("pipeline documentation"),Pn=a(" for more information."),Xa=f(),ae=n("h3"),be=n("a"),Qs=n("span"),d(at.$$.fragment),Sn=f(),Js=n("span"),Mn=a("Use another model and tokenizer in the pipeline"),er=f(),M=n("p"),Cn=a("The "),Lt=n("a"),In=a("pipeline()"),Nn=a(" can accommodate any model from the "),rt=n("a"),On=a("Model Hub"),Ln=a(", making it easy to adapt the "),Dt=n("a"),Dn=a("pipeline()"),Rn=a(" for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),ot=n("a"),Hn=a("BERT model"),Un=a(" fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),tr=f(),d(nt.$$.fragment),sr=f(),G=n("p"),Wn=a("Use the "),Rt=n("a"),Bn=a("AutoModelForSequenceClassification"),Yn=a(" and [\u2018AutoTokenizer\u2019] to load the pretrained model and it\u2019s associated tokenizer (more on an "),Ks=n("code"),Gn=a("AutoClass"),Qn=a(" below):"),ar=f(),d(lt.$$.fragment),rr=f(),Q=n("p"),Jn=a("Then you can specify the model and tokenizer in the "),Ht=n("a"),Kn=a("pipeline()"),Vn=a(", and apply the "),Vs=n("code"),Zn=a("classifier"),Xn=a(" on your target text:"),or=f(),d(it.$$.fragment),nr=f(),J=n("p"),el=a("If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),Ut=n("a"),tl=a("fine-tuning tutorial"),sl=a(" to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),Wt=n("a"),al=a("here"),rl=a(") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),lr=f(),re=n("h2"),we=n("a"),Zs=n("span"),d(pt.$$.fragment),ol=f(),Xs=n("span"),nl=a("AutoClass"),ir=f(),d(ft.$$.fragment),pr=f(),q=n("p"),ll=a("Under the hood, the "),Bt=n("a"),il=a("AutoModelForSequenceClassification"),pl=a(" and "),Yt=n("a"),fl=a("AutoTokenizer"),ul=a(" classes work together to power the "),Gt=n("a"),ml=a("pipeline()"),hl=a(". An "),Qt=n("a"),cl=a("AutoClass"),dl=a(" is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),ea=n("code"),_l=a("AutoClass"),gl=a(" for your task and it\u2019s associated tokenizer with "),Jt=n("a"),vl=a("AutoTokenizer"),yl=a("."),fr=f(),K=n("p"),$l=a("Let\u2019s return to our example and see how you can use the "),ta=n("code"),bl=a("AutoClass"),wl=a(" to replicate the results of the "),Kt=n("a"),kl=a("pipeline()"),jl=a("."),ur=f(),oe=n("h3"),ke=n("a"),sa=n("span"),d(ut.$$.fragment),El=f(),aa=n("span"),Al=a("AutoTokenizer"),mr=f(),V=n("p"),Tl=a("A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),ra=n("em"),xl=a("tokens"),ql=a(". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),Vt=n("a"),zl=a("here"),Fl=a("). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),hr=f(),je=n("p"),Pl=a("Load a tokenizer with "),Zt=n("a"),Sl=a("AutoTokenizer"),Ml=a(":"),cr=f(),d(mt.$$.fragment),dr=f(),Ee=n("p"),Cl=a("Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),oa=n("em"),Il=a("vocabulary"),Nl=a("."),_r=f(),Xt=n("p"),Ol=a("Pass your text to the tokenizer:"),gr=f(),d(ht.$$.fragment),vr=f(),es=n("p"),Ll=a("The tokenizer will return a dictionary containing:"),yr=f(),Ae=n("ul"),ts=n("li"),ss=n("a"),Dl=a("input_ids"),Rl=a(": numerical representions of your tokens."),Hl=f(),as=n("li"),rs=n("a"),Ul=a("atttention_mask"),Wl=a(": indicates which tokens should be attended to."),$r=f(),Te=n("p"),Bl=a("Just like the "),os=n("a"),Yl=a("pipeline()"),Gl=a(", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),br=f(),d(ct.$$.fragment),wr=f(),xe=n("p"),Ql=a("Read the "),ns=n("a"),Jl=a("preprocessing"),Kl=a(" tutorial for more details about tokenization."),kr=f(),ne=n("h3"),qe=n("a"),na=n("span"),d(dt.$$.fragment),Vl=f(),la=n("span"),Zl=a("AutoModel"),jr=f(),F=n("p"),Xl=a("\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),ls=n("a"),ei=a("AutoModel"),ti=a(" like you would load an "),is=n("a"),si=a("AutoTokenizer"),ai=a(". The only difference is selecting the correct "),ps=n("a"),ri=a("AutoModel"),oi=a(" for the task. Since you are doing text - or sequence - classification, load "),fs=n("a"),ni=a("AutoModelForSequenceClassification"),li=a(". The TensorFlow equivalent is simply "),us=n("a"),ii=a("TFAutoModelForSequenceClassification"),pi=a(":"),Er=f(),d(_t.$$.fragment),Ar=f(),d(ze.$$.fragment),Tr=f(),Fe=n("p"),fi=a("Now you can pass your preprocessed batch of inputs directly to the model. If you are using a PyTorch model, unpack the dictionary by adding "),ia=n("code"),ui=a("**"),mi=a(". For TensorFlow models, pass the dictionary keys directly to the tensors:"),xr=f(),d(gt.$$.fragment),qr=f(),Z=n("p"),hi=a("The model outputs the final activations in the "),pa=n("code"),ci=a("logits"),di=a(" attribute. Apply the softmax function to the "),fa=n("code"),_i=a("logits"),gi=a(" to retrieve the probabilities:"),zr=f(),d(vt.$$.fragment),Fr=f(),d(Pe.$$.fragment),Pr=f(),z=n("p"),vi=a("Models are a standard "),yt=n("a"),ua=n("code"),yi=a("torch.nn.Module"),$i=a(" or a "),$t=n("a"),ma=n("code"),bi=a("tf.keras.Model"),wi=a(" so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),ms=n("a"),ki=a("Trainer"),ji=a(" class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),ha=n("code"),Ei=a("fit"),Ai=a(" method from "),bt=n("a"),Ti=a("Keras"),xi=a(". Refer to the "),hs=n("a"),qi=a("training tutorial"),zi=a(" for more details."),Sr=f(),d(Se.$$.fragment),Mr=f(),le=n("h3"),Me=n("a"),ca=n("span"),d(wt.$$.fragment),Fi=f(),da=n("span"),Pi=a("Save a model"),Cr=f(),Ce=n("p"),Si=a("Once your model is fine-tuned, you can save it with its tokenizer using "),cs=n("a"),Mi=a("PreTrainedModel.save_pretrained()"),Ci=a(":"),Ir=f(),d(kt.$$.fragment),Nr=f(),Ie=n("p"),Ii=a("When you are ready to use the model again, reload it with "),ds=n("a"),Ni=a("PreTrainedModel.from_pretrained()"),Oi=a(":"),Or=f(),d(jt.$$.fragment),Lr=f(),X=n("p"),Li=a("One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),_a=n("code"),Di=a("from_pt"),Ri=a(" or "),ga=n("code"),Hi=a("from_tf"),Ui=a(" parameter can convert the model from one framework to the other:"),Dr=f(),d(Et.$$.fragment),this.h()},l(e){const o=Jf('[data-svelte="svelte-1phssyn"]',document.head);h=l(o,"META",{name:!0,content:!0}),o.forEach(s),k=u(e),c=l(e,"H1",{class:!0});var At=i(c);w=l(At,"A",{id:!0,class:!0,href:!0});var va=i(w);A=l(va,"SPAN",{});var ya=i(A);_(b.$$.fragment,ya),ya.forEach(s),va.forEach(s),j=u(At),x=l(At,"SPAN",{});var $a=i(x);S=r($a,"Quick tour"),$a.forEach(s),At.forEach(s),E=u(e),_(O.$$.fragment,e),R=u(e),H=l(e,"P",{});var ie=i(H);fo=r(ie,"Get up and running with \u{1F917} Transformers! Start using the "),Tt=l(ie,"A",{href:!0});var Ki=i(Tt);uo=r(Ki,"pipeline()"),Ki.forEach(s),mo=r(ie," for rapid inference, and quickly load a pretrained model and tokenizer with an "),xt=l(ie,"A",{href:!0});var Vi=i(xt);ho=r(Vi,"AutoClass"),Vi.forEach(s),co=r(ie," to solve your text, vision or audio task."),ie.forEach(s),wa=u(e),_(fe.$$.fragment,e),ka=u(e),te=l(e,"H2",{class:!0});var Hr=i(te);ue=l(Hr,"A",{id:!0,class:!0,href:!0});var Zi=i(ue);Ts=l(Zi,"SPAN",{});var Xi=i(Ts);_(Oe.$$.fragment,Xi),Xi.forEach(s),Zi.forEach(s),_o=u(Hr),xs=l(Hr,"SPAN",{});var ep=i(xs);go=r(ep,"Pipeline"),ep.forEach(s),Hr.forEach(s),ja=u(e),Le=l(e,"P",{});var Wi=i(Le);qt=l(Wi,"A",{href:!0});var tp=i(qt);vo=r(tp,"pipeline()"),tp.forEach(s),yo=r(Wi," is the easiest way to use a pretrained model for a given task."),Wi.forEach(s),Ea=u(e),_(De.$$.fragment,e),Aa=u(e),me=l(e,"P",{});var Ur=i(me);$o=r(Ur,"The "),zt=l(Ur,"A",{href:!0});var sp=i(zt);bo=r(sp,"pipeline()"),sp.forEach(s),wo=r(Ur," supports many common tasks out-of-the-box:"),Ur.forEach(s),Ta=u(e),Re=l(e,"P",{});var Bi=i(Re);qs=l(Bi,"STRONG",{});var ap=i(qs);ko=r(ap,"Text"),ap.forEach(s),jo=r(Bi,":"),Bi.forEach(s),xa=u(e),T=l(e,"UL",{});var P=i(T);zs=l(P,"LI",{});var rp=i(zs);Eo=r(rp,"Sentiment analysis: classify the polarity of a given text."),rp.forEach(s),Ao=u(P),Fs=l(P,"LI",{});var op=i(Fs);To=r(op,"Text generation (in English): generate text from a given input."),op.forEach(s),xo=u(P),Ps=l(P,"LI",{});var np=i(Ps);qo=r(np,"Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),np.forEach(s),zo=u(P),Ss=l(P,"LI",{});var lp=i(Ss);Fo=r(lp,"Question answering: extract the answer from the context, given some context and a question."),lp.forEach(s),Po=u(P),Ms=l(P,"LI",{});var ip=i(Ms);So=r(ip,"Fill-mask: fill in the blank given a text with masked words."),ip.forEach(s),Mo=u(P),Cs=l(P,"LI",{});var pp=i(Cs);Co=r(pp,"Summarization: generate a summary of a long sequence of text or document."),pp.forEach(s),Io=u(P),Is=l(P,"LI",{});var fp=i(Is);No=r(fp,"Translation: translate text into another language."),fp.forEach(s),Oo=u(P),Ns=l(P,"LI",{});var up=i(Ns);Lo=r(up,"Feature extraction: create a tensor representation of the text."),up.forEach(s),P.forEach(s),qa=u(e),He=l(e,"P",{});var Yi=i(He);Os=l(Yi,"STRONG",{});var mp=i(Os);Do=r(mp,"Image"),mp.forEach(s),Ro=r(Yi,":"),Yi.forEach(s),za=u(e),U=l(e,"UL",{});var _s=i(U);Ls=l(_s,"LI",{});var hp=i(Ls);Ho=r(hp,"Image classification: classify an image."),hp.forEach(s),Uo=u(_s),Ds=l(_s,"LI",{});var cp=i(Ds);Wo=r(cp,"Image segmentation: classify every pixel in an image."),cp.forEach(s),Bo=u(_s),Rs=l(_s,"LI",{});var dp=i(Rs);Yo=r(dp,"Object detection: detect objects within an image."),dp.forEach(s),_s.forEach(s),Fa=u(e),Ue=l(e,"P",{});var Gi=i(Ue);Hs=l(Gi,"STRONG",{});var _p=i(Hs);Go=r(_p,"Audio"),_p.forEach(s),Qo=r(Gi,":"),Gi.forEach(s),Pa=u(e),he=l(e,"UL",{});var Wr=i(he);Us=l(Wr,"LI",{});var gp=i(Us);Jo=r(gp,"Audio classification: assign a label to a given segment of audio."),gp.forEach(s),Ko=u(Wr),Ws=l(Wr,"LI",{});var vp=i(Ws);Vo=r(vp,"Automatic speech recognition (ASR): transcribe audio data into text."),vp.forEach(s),Wr.forEach(s),Sa=u(e),_(ce.$$.fragment,e),Ma=u(e),se=l(e,"H3",{class:!0});var Br=i(se);de=l(Br,"A",{id:!0,class:!0,href:!0});var yp=i(de);Bs=l(yp,"SPAN",{});var $p=i(Bs);_(We.$$.fragment,$p),$p.forEach(s),yp.forEach(s),Zo=u(Br),Ys=l(Br,"SPAN",{});var bp=i(Ys);Xo=r(bp,"Pipeline usage"),bp.forEach(s),Br.forEach(s),Ca=u(e),_e=l(e,"P",{});var Yr=i(_e);en=r(Yr,"In the following example, you will use the "),Ft=l(Yr,"A",{href:!0});var wp=i(Ft);tn=r(wp,"pipeline()"),wp.forEach(s),sn=r(Yr," for sentiment analysis."),Yr.forEach(s),Ia=u(e),Pt=l(e,"P",{});var kp=i(Pt);an=r(kp,"Install the following dependencies if you haven\u2019t already:"),kp.forEach(s),Na=u(e),_(Be.$$.fragment,e),Oa=u(e),ge=l(e,"P",{});var Gr=i(ge);rn=r(Gr,"Import "),St=l(Gr,"A",{href:!0});var jp=i(St);on=r(jp,"pipeline()"),jp.forEach(s),nn=r(Gr," and specify the task you want to complete:"),Gr.forEach(s),La=u(e),_(Ye.$$.fragment,e),Da=u(e),W=l(e,"P",{});var gs=i(W);ln=r(gs,"The pipeline downloads and caches a default "),Ge=l(gs,"A",{href:!0,rel:!0});var Ep=i(Ge);pn=r(Ep,"pretrained model"),Ep.forEach(s),fn=r(gs," and tokenizer for sentiment analysis. Now you can use the "),Gs=l(gs,"CODE",{});var Ap=i(Gs);un=r(Ap,"classifier"),Ap.forEach(s),mn=r(gs," on your target text:"),gs.forEach(s),Ra=u(e),_(Qe.$$.fragment,e),Ha=u(e),ve=l(e,"P",{});var Qr=i(ve);hn=r(Qr,"For more than one sentence, pass a list of sentences to the "),Mt=l(Qr,"A",{href:!0});var Tp=i(Mt);cn=r(Tp,"pipeline()"),Tp.forEach(s),dn=r(Qr," which returns a list of dictionaries:"),Qr.forEach(s),Ua=u(e),_(Je.$$.fragment,e),Wa=u(e),B=l(e,"P",{});var vs=i(B);_n=r(vs,"The "),Ct=l(vs,"A",{href:!0});var xp=i(Ct);gn=r(xp,"pipeline()"),xp.forEach(s),vn=r(vs," can also iterate over an entire dataset. Start by installing the "),Ke=l(vs,"A",{href:!0,rel:!0});var qp=i(Ke);yn=r(qp,"\u{1F917} Datasets"),qp.forEach(s),$n=r(vs," library:"),vs.forEach(s),Ba=u(e),_(Ve.$$.fragment,e),Ya=u(e),ye=l(e,"P",{});var Jr=i(ye);bn=r(Jr,"Create a "),It=l(Jr,"A",{href:!0});var zp=i(It);wn=r(zp,"pipeline()"),zp.forEach(s),kn=r(Jr," with the task you want to solve for and the model you want to use."),Jr.forEach(s),Ga=u(e),_(Ze.$$.fragment,e),Qa=u(e),Y=l(e,"P",{});var ys=i(Y);jn=r(ys,"Next, load a dataset (see the \u{1F917} Datasets "),Xe=l(ys,"A",{href:!0,rel:!0});var Fp=i(Xe);En=r(Fp,"Quick Start"),Fp.forEach(s),An=r(ys," for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),et=l(ys,"A",{href:!0,rel:!0});var Pp=i(et);Tn=r(Pp,"SUPERB"),Pp.forEach(s),xn=r(ys," dataset:"),ys.forEach(s),Ja=u(e),_(tt.$$.fragment,e),Ka=u(e),Nt=l(e,"P",{});var Sp=i(Nt);qn=r(Sp,"You can pass a whole dataset pipeline:"),Sp.forEach(s),Va=u(e),_(st.$$.fragment,e),Za=u(e),$e=l(e,"P",{});var Kr=i($e);zn=r(Kr,"For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the "),Ot=l(Kr,"A",{href:!0});var Mp=i(Ot);Fn=r(Mp,"pipeline documentation"),Mp.forEach(s),Pn=r(Kr," for more information."),Kr.forEach(s),Xa=u(e),ae=l(e,"H3",{class:!0});var Vr=i(ae);be=l(Vr,"A",{id:!0,class:!0,href:!0});var Cp=i(be);Qs=l(Cp,"SPAN",{});var Ip=i(Qs);_(at.$$.fragment,Ip),Ip.forEach(s),Cp.forEach(s),Sn=u(Vr),Js=l(Vr,"SPAN",{});var Np=i(Js);Mn=r(Np,"Use another model and tokenizer in the pipeline"),Np.forEach(s),Vr.forEach(s),er=u(e),M=l(e,"P",{});var ee=i(M);Cn=r(ee,"The "),Lt=l(ee,"A",{href:!0});var Op=i(Lt);In=r(Op,"pipeline()"),Op.forEach(s),Nn=r(ee," can accommodate any model from the "),rt=l(ee,"A",{href:!0,rel:!0});var Lp=i(rt);On=r(Lp,"Model Hub"),Lp.forEach(s),Ln=r(ee,", making it easy to adapt the "),Dt=l(ee,"A",{href:!0});var Dp=i(Dt);Dn=r(Dp,"pipeline()"),Dp.forEach(s),Rn=r(ee," for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),ot=l(ee,"A",{href:!0,rel:!0});var Rp=i(ot);Hn=r(Rp,"BERT model"),Rp.forEach(s),Un=r(ee," fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),ee.forEach(s),tr=u(e),_(nt.$$.fragment,e),sr=u(e),G=l(e,"P",{});var $s=i(G);Wn=r($s,"Use the "),Rt=l($s,"A",{href:!0});var Hp=i(Rt);Bn=r(Hp,"AutoModelForSequenceClassification"),Hp.forEach(s),Yn=r($s," and [\u2018AutoTokenizer\u2019] to load the pretrained model and it\u2019s associated tokenizer (more on an "),Ks=l($s,"CODE",{});var Up=i(Ks);Gn=r(Up,"AutoClass"),Up.forEach(s),Qn=r($s," below):"),$s.forEach(s),ar=u(e),_(lt.$$.fragment,e),rr=u(e),Q=l(e,"P",{});var bs=i(Q);Jn=r(bs,"Then you can specify the model and tokenizer in the "),Ht=l(bs,"A",{href:!0});var Wp=i(Ht);Kn=r(Wp,"pipeline()"),Wp.forEach(s),Vn=r(bs,", and apply the "),Vs=l(bs,"CODE",{});var Bp=i(Vs);Zn=r(Bp,"classifier"),Bp.forEach(s),Xn=r(bs," on your target text:"),bs.forEach(s),or=u(e),_(it.$$.fragment,e),nr=u(e),J=l(e,"P",{});var ws=i(J);el=r(ws,"If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),Ut=l(ws,"A",{href:!0});var Yp=i(Ut);tl=r(Yp,"fine-tuning tutorial"),Yp.forEach(s),sl=r(ws," to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),Wt=l(ws,"A",{href:!0});var Gp=i(Wt);al=r(Gp,"here"),Gp.forEach(s),rl=r(ws,") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),ws.forEach(s),lr=u(e),re=l(e,"H2",{class:!0});var Zr=i(re);we=l(Zr,"A",{id:!0,class:!0,href:!0});var Qp=i(we);Zs=l(Qp,"SPAN",{});var Jp=i(Zs);_(pt.$$.fragment,Jp),Jp.forEach(s),Qp.forEach(s),ol=u(Zr),Xs=l(Zr,"SPAN",{});var Kp=i(Xs);nl=r(Kp,"AutoClass"),Kp.forEach(s),Zr.forEach(s),ir=u(e),_(ft.$$.fragment,e),pr=u(e),q=l(e,"P",{});var C=i(q);ll=r(C,"Under the hood, the "),Bt=l(C,"A",{href:!0});var Vp=i(Bt);il=r(Vp,"AutoModelForSequenceClassification"),Vp.forEach(s),pl=r(C," and "),Yt=l(C,"A",{href:!0});var Zp=i(Yt);fl=r(Zp,"AutoTokenizer"),Zp.forEach(s),ul=r(C," classes work together to power the "),Gt=l(C,"A",{href:!0});var Xp=i(Gt);ml=r(Xp,"pipeline()"),Xp.forEach(s),hl=r(C,". An "),Qt=l(C,"A",{href:!0});var ef=i(Qt);cl=r(ef,"AutoClass"),ef.forEach(s),dl=r(C," is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),ea=l(C,"CODE",{});var tf=i(ea);_l=r(tf,"AutoClass"),tf.forEach(s),gl=r(C," for your task and it\u2019s associated tokenizer with "),Jt=l(C,"A",{href:!0});var sf=i(Jt);vl=r(sf,"AutoTokenizer"),sf.forEach(s),yl=r(C,"."),C.forEach(s),fr=u(e),K=l(e,"P",{});var ks=i(K);$l=r(ks,"Let\u2019s return to our example and see how you can use the "),ta=l(ks,"CODE",{});var af=i(ta);bl=r(af,"AutoClass"),af.forEach(s),wl=r(ks," to replicate the results of the "),Kt=l(ks,"A",{href:!0});var rf=i(Kt);kl=r(rf,"pipeline()"),rf.forEach(s),jl=r(ks,"."),ks.forEach(s),ur=u(e),oe=l(e,"H3",{class:!0});var Xr=i(oe);ke=l(Xr,"A",{id:!0,class:!0,href:!0});var of=i(ke);sa=l(of,"SPAN",{});var nf=i(sa);_(ut.$$.fragment,nf),nf.forEach(s),of.forEach(s),El=u(Xr),aa=l(Xr,"SPAN",{});var lf=i(aa);Al=r(lf,"AutoTokenizer"),lf.forEach(s),Xr.forEach(s),mr=u(e),V=l(e,"P",{});var js=i(V);Tl=r(js,"A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),ra=l(js,"EM",{});var pf=i(ra);xl=r(pf,"tokens"),pf.forEach(s),ql=r(js,". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),Vt=l(js,"A",{href:!0});var ff=i(Vt);zl=r(ff,"here"),ff.forEach(s),Fl=r(js,"). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),js.forEach(s),hr=u(e),je=l(e,"P",{});var eo=i(je);Pl=r(eo,"Load a tokenizer with "),Zt=l(eo,"A",{href:!0});var uf=i(Zt);Sl=r(uf,"AutoTokenizer"),uf.forEach(s),Ml=r(eo,":"),eo.forEach(s),cr=u(e),_(mt.$$.fragment,e),dr=u(e),Ee=l(e,"P",{});var to=i(Ee);Cl=r(to,"Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),oa=l(to,"EM",{});var mf=i(oa);Il=r(mf,"vocabulary"),mf.forEach(s),Nl=r(to,"."),to.forEach(s),_r=u(e),Xt=l(e,"P",{});var hf=i(Xt);Ol=r(hf,"Pass your text to the tokenizer:"),hf.forEach(s),gr=u(e),_(ht.$$.fragment,e),vr=u(e),es=l(e,"P",{});var cf=i(es);Ll=r(cf,"The tokenizer will return a dictionary containing:"),cf.forEach(s),yr=u(e),Ae=l(e,"UL",{});var so=i(Ae);ts=l(so,"LI",{});var Qi=i(ts);ss=l(Qi,"A",{href:!0});var df=i(ss);Dl=r(df,"input_ids"),df.forEach(s),Rl=r(Qi,": numerical representions of your tokens."),Qi.forEach(s),Hl=u(so),as=l(so,"LI",{});var Ji=i(as);rs=l(Ji,"A",{href:!0});var _f=i(rs);Ul=r(_f,"atttention_mask"),_f.forEach(s),Wl=r(Ji,": indicates which tokens should be attended to."),Ji.forEach(s),so.forEach(s),$r=u(e),Te=l(e,"P",{});var ao=i(Te);Bl=r(ao,"Just like the "),os=l(ao,"A",{href:!0});var gf=i(os);Yl=r(gf,"pipeline()"),gf.forEach(s),Gl=r(ao,", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),ao.forEach(s),br=u(e),_(ct.$$.fragment,e),wr=u(e),xe=l(e,"P",{});var ro=i(xe);Ql=r(ro,"Read the "),ns=l(ro,"A",{href:!0});var vf=i(ns);Jl=r(vf,"preprocessing"),vf.forEach(s),Kl=r(ro," tutorial for more details about tokenization."),ro.forEach(s),kr=u(e),ne=l(e,"H3",{class:!0});var oo=i(ne);qe=l(oo,"A",{id:!0,class:!0,href:!0});var yf=i(qe);na=l(yf,"SPAN",{});var $f=i(na);_(dt.$$.fragment,$f),$f.forEach(s),yf.forEach(s),Vl=u(oo),la=l(oo,"SPAN",{});var bf=i(la);Zl=r(bf,"AutoModel"),bf.forEach(s),oo.forEach(s),jr=u(e),F=l(e,"P",{});var L=i(F);Xl=r(L,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),ls=l(L,"A",{href:!0});var wf=i(ls);ei=r(wf,"AutoModel"),wf.forEach(s),ti=r(L," like you would load an "),is=l(L,"A",{href:!0});var kf=i(is);si=r(kf,"AutoTokenizer"),kf.forEach(s),ai=r(L,". The only difference is selecting the correct "),ps=l(L,"A",{href:!0});var jf=i(ps);ri=r(jf,"AutoModel"),jf.forEach(s),oi=r(L," for the task. Since you are doing text - or sequence - classification, load "),fs=l(L,"A",{href:!0});var Ef=i(fs);ni=r(Ef,"AutoModelForSequenceClassification"),Ef.forEach(s),li=r(L,". The TensorFlow equivalent is simply "),us=l(L,"A",{href:!0});var Af=i(us);ii=r(Af,"TFAutoModelForSequenceClassification"),Af.forEach(s),pi=r(L,":"),L.forEach(s),Er=u(e),_(_t.$$.fragment,e),Ar=u(e),_(ze.$$.fragment,e),Tr=u(e),Fe=l(e,"P",{});var no=i(Fe);fi=r(no,"Now you can pass your preprocessed batch of inputs directly to the model. If you are using a PyTorch model, unpack the dictionary by adding "),ia=l(no,"CODE",{});var Tf=i(ia);ui=r(Tf,"**"),Tf.forEach(s),mi=r(no,". For TensorFlow models, pass the dictionary keys directly to the tensors:"),no.forEach(s),xr=u(e),_(gt.$$.fragment,e),qr=u(e),Z=l(e,"P",{});var Es=i(Z);hi=r(Es,"The model outputs the final activations in the "),pa=l(Es,"CODE",{});var xf=i(pa);ci=r(xf,"logits"),xf.forEach(s),di=r(Es," attribute. Apply the softmax function to the "),fa=l(Es,"CODE",{});var qf=i(fa);_i=r(qf,"logits"),qf.forEach(s),gi=r(Es," to retrieve the probabilities:"),Es.forEach(s),zr=u(e),_(vt.$$.fragment,e),Fr=u(e),_(Pe.$$.fragment,e),Pr=u(e),z=l(e,"P",{});var I=i(z);vi=r(I,"Models are a standard "),yt=l(I,"A",{href:!0,rel:!0});var zf=i(yt);ua=l(zf,"CODE",{});var Ff=i(ua);yi=r(Ff,"torch.nn.Module"),Ff.forEach(s),zf.forEach(s),$i=r(I," or a "),$t=l(I,"A",{href:!0,rel:!0});var Pf=i($t);ma=l(Pf,"CODE",{});var Sf=i(ma);bi=r(Sf,"tf.keras.Model"),Sf.forEach(s),Pf.forEach(s),wi=r(I," so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),ms=l(I,"A",{href:!0});var Mf=i(ms);ki=r(Mf,"Trainer"),Mf.forEach(s),ji=r(I," class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),ha=l(I,"CODE",{});var Cf=i(ha);Ei=r(Cf,"fit"),Cf.forEach(s),Ai=r(I," method from "),bt=l(I,"A",{href:!0,rel:!0});var If=i(bt);Ti=r(If,"Keras"),If.forEach(s),xi=r(I,". Refer to the "),hs=l(I,"A",{href:!0});var Nf=i(hs);qi=r(Nf,"training tutorial"),Nf.forEach(s),zi=r(I," for more details."),I.forEach(s),Sr=u(e),_(Se.$$.fragment,e),Mr=u(e),le=l(e,"H3",{class:!0});var lo=i(le);Me=l(lo,"A",{id:!0,class:!0,href:!0});var Of=i(Me);ca=l(Of,"SPAN",{});var Lf=i(ca);_(wt.$$.fragment,Lf),Lf.forEach(s),Of.forEach(s),Fi=u(lo),da=l(lo,"SPAN",{});var Df=i(da);Pi=r(Df,"Save a model"),Df.forEach(s),lo.forEach(s),Cr=u(e),Ce=l(e,"P",{});var io=i(Ce);Si=r(io,"Once your model is fine-tuned, you can save it with its tokenizer using "),cs=l(io,"A",{href:!0});var Rf=i(cs);Mi=r(Rf,"PreTrainedModel.save_pretrained()"),Rf.forEach(s),Ci=r(io,":"),io.forEach(s),Ir=u(e),_(kt.$$.fragment,e),Nr=u(e),Ie=l(e,"P",{});var po=i(Ie);Ii=r(po,"When you are ready to use the model again, reload it with "),ds=l(po,"A",{href:!0});var Hf=i(ds);Ni=r(Hf,"PreTrainedModel.from_pretrained()"),Hf.forEach(s),Oi=r(po,":"),po.forEach(s),Or=u(e),_(jt.$$.fragment,e),Lr=u(e),X=l(e,"P",{});var As=i(X);Li=r(As,"One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),_a=l(As,"CODE",{});var Uf=i(_a);Di=r(Uf,"from_pt"),Uf.forEach(s),Ri=r(As," or "),ga=l(As,"CODE",{});var Wf=i(ga);Hi=r(Wf,"from_tf"),Wf.forEach(s),Ui=r(As," parameter can convert the model from one framework to the other:"),As.forEach(s),Dr=u(e),_(Et.$$.fragment,e),this.h()},h(){m(h,"name","hf:doc:metadata"),m(h,"content",JSON.stringify(au)),m(w,"id","quick-tour"),m(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(w,"href","#quick-tour"),m(c,"class","relative group"),m(Tt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(xt,"href","./model_doc/auto"),m(ue,"id","pipeline"),m(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ue,"href","#pipeline"),m(te,"class","relative group"),m(qt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(zt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(de,"id","pipeline-usage"),m(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(de,"href","#pipeline-usage"),m(se,"class","relative group"),m(Ft,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(St,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(Ge,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),m(Ge,"rel","nofollow"),m(Mt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(Ct,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(Ke,"href","https://huggingface.co/docs/datasets/"),m(Ke,"rel","nofollow"),m(It,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(Xe,"href","https://huggingface.co/docs/datasets/quickstart.html"),m(Xe,"rel","nofollow"),m(et,"href","https://huggingface.co/datasets/superb"),m(et,"rel","nofollow"),m(Ot,"href","./main_classes/pipelines"),m(be,"id","use-another-model-and-tokenizer-in-the-pipeline"),m(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(be,"href","#use-another-model-and-tokenizer-in-the-pipeline"),m(ae,"class","relative group"),m(Lt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(rt,"href","https://huggingface.co/models"),m(rt,"rel","nofollow"),m(Dt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(ot,"href","https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"),m(ot,"rel","nofollow"),m(Rt,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),m(Ht,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(Ut,"href","./training"),m(Wt,"href","./model_sharing"),m(we,"id","autoclass"),m(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(we,"href","#autoclass"),m(re,"class","relative group"),m(Bt,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),m(Yt,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),m(Gt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(Qt,"href","./model_doc/auto"),m(Jt,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),m(Kt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(ke,"id","autotokenizer"),m(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ke,"href","#autotokenizer"),m(oe,"class","relative group"),m(Vt,"href","./tokenizer_summary"),m(Zt,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),m(ss,"href","./glossary#input-ids"),m(rs,"href",".glossary#attention-mask"),m(os,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(ns,"href","./preprocessing"),m(qe,"id","automodel"),m(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(qe,"href","#automodel"),m(ne,"class","relative group"),m(ls,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModel"),m(is,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),m(ps,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModel"),m(fs,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),m(us,"href","/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification"),m(yt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),m(yt,"rel","nofollow"),m($t,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),m($t,"rel","nofollow"),m(ms,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(bt,"href","https://keras.io/"),m(bt,"rel","nofollow"),m(hs,"href","./training"),m(Me,"id","save-a-model"),m(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Me,"href","#save-a-model"),m(le,"class","relative group"),m(cs,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),m(ds,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained")},m(e,o){t(document.head,h),p(e,k,o),p(e,c,o),t(c,w),t(w,A),g(b,A,null),t(c,j),t(c,x),t(x,S),p(e,E,o),g(O,e,o),p(e,R,o),p(e,H,o),t(H,fo),t(H,Tt),t(Tt,uo),t(H,mo),t(H,xt),t(xt,ho),t(H,co),p(e,wa,o),g(fe,e,o),p(e,ka,o),p(e,te,o),t(te,ue),t(ue,Ts),g(Oe,Ts,null),t(te,_o),t(te,xs),t(xs,go),p(e,ja,o),p(e,Le,o),t(Le,qt),t(qt,vo),t(Le,yo),p(e,Ea,o),g(De,e,o),p(e,Aa,o),p(e,me,o),t(me,$o),t(me,zt),t(zt,bo),t(me,wo),p(e,Ta,o),p(e,Re,o),t(Re,qs),t(qs,ko),t(Re,jo),p(e,xa,o),p(e,T,o),t(T,zs),t(zs,Eo),t(T,Ao),t(T,Fs),t(Fs,To),t(T,xo),t(T,Ps),t(Ps,qo),t(T,zo),t(T,Ss),t(Ss,Fo),t(T,Po),t(T,Ms),t(Ms,So),t(T,Mo),t(T,Cs),t(Cs,Co),t(T,Io),t(T,Is),t(Is,No),t(T,Oo),t(T,Ns),t(Ns,Lo),p(e,qa,o),p(e,He,o),t(He,Os),t(Os,Do),t(He,Ro),p(e,za,o),p(e,U,o),t(U,Ls),t(Ls,Ho),t(U,Uo),t(U,Ds),t(Ds,Wo),t(U,Bo),t(U,Rs),t(Rs,Yo),p(e,Fa,o),p(e,Ue,o),t(Ue,Hs),t(Hs,Go),t(Ue,Qo),p(e,Pa,o),p(e,he,o),t(he,Us),t(Us,Jo),t(he,Ko),t(he,Ws),t(Ws,Vo),p(e,Sa,o),g(ce,e,o),p(e,Ma,o),p(e,se,o),t(se,de),t(de,Bs),g(We,Bs,null),t(se,Zo),t(se,Ys),t(Ys,Xo),p(e,Ca,o),p(e,_e,o),t(_e,en),t(_e,Ft),t(Ft,tn),t(_e,sn),p(e,Ia,o),p(e,Pt,o),t(Pt,an),p(e,Na,o),g(Be,e,o),p(e,Oa,o),p(e,ge,o),t(ge,rn),t(ge,St),t(St,on),t(ge,nn),p(e,La,o),g(Ye,e,o),p(e,Da,o),p(e,W,o),t(W,ln),t(W,Ge),t(Ge,pn),t(W,fn),t(W,Gs),t(Gs,un),t(W,mn),p(e,Ra,o),g(Qe,e,o),p(e,Ha,o),p(e,ve,o),t(ve,hn),t(ve,Mt),t(Mt,cn),t(ve,dn),p(e,Ua,o),g(Je,e,o),p(e,Wa,o),p(e,B,o),t(B,_n),t(B,Ct),t(Ct,gn),t(B,vn),t(B,Ke),t(Ke,yn),t(B,$n),p(e,Ba,o),g(Ve,e,o),p(e,Ya,o),p(e,ye,o),t(ye,bn),t(ye,It),t(It,wn),t(ye,kn),p(e,Ga,o),g(Ze,e,o),p(e,Qa,o),p(e,Y,o),t(Y,jn),t(Y,Xe),t(Xe,En),t(Y,An),t(Y,et),t(et,Tn),t(Y,xn),p(e,Ja,o),g(tt,e,o),p(e,Ka,o),p(e,Nt,o),t(Nt,qn),p(e,Va,o),g(st,e,o),p(e,Za,o),p(e,$e,o),t($e,zn),t($e,Ot),t(Ot,Fn),t($e,Pn),p(e,Xa,o),p(e,ae,o),t(ae,be),t(be,Qs),g(at,Qs,null),t(ae,Sn),t(ae,Js),t(Js,Mn),p(e,er,o),p(e,M,o),t(M,Cn),t(M,Lt),t(Lt,In),t(M,Nn),t(M,rt),t(rt,On),t(M,Ln),t(M,Dt),t(Dt,Dn),t(M,Rn),t(M,ot),t(ot,Hn),t(M,Un),p(e,tr,o),g(nt,e,o),p(e,sr,o),p(e,G,o),t(G,Wn),t(G,Rt),t(Rt,Bn),t(G,Yn),t(G,Ks),t(Ks,Gn),t(G,Qn),p(e,ar,o),g(lt,e,o),p(e,rr,o),p(e,Q,o),t(Q,Jn),t(Q,Ht),t(Ht,Kn),t(Q,Vn),t(Q,Vs),t(Vs,Zn),t(Q,Xn),p(e,or,o),g(it,e,o),p(e,nr,o),p(e,J,o),t(J,el),t(J,Ut),t(Ut,tl),t(J,sl),t(J,Wt),t(Wt,al),t(J,rl),p(e,lr,o),p(e,re,o),t(re,we),t(we,Zs),g(pt,Zs,null),t(re,ol),t(re,Xs),t(Xs,nl),p(e,ir,o),g(ft,e,o),p(e,pr,o),p(e,q,o),t(q,ll),t(q,Bt),t(Bt,il),t(q,pl),t(q,Yt),t(Yt,fl),t(q,ul),t(q,Gt),t(Gt,ml),t(q,hl),t(q,Qt),t(Qt,cl),t(q,dl),t(q,ea),t(ea,_l),t(q,gl),t(q,Jt),t(Jt,vl),t(q,yl),p(e,fr,o),p(e,K,o),t(K,$l),t(K,ta),t(ta,bl),t(K,wl),t(K,Kt),t(Kt,kl),t(K,jl),p(e,ur,o),p(e,oe,o),t(oe,ke),t(ke,sa),g(ut,sa,null),t(oe,El),t(oe,aa),t(aa,Al),p(e,mr,o),p(e,V,o),t(V,Tl),t(V,ra),t(ra,xl),t(V,ql),t(V,Vt),t(Vt,zl),t(V,Fl),p(e,hr,o),p(e,je,o),t(je,Pl),t(je,Zt),t(Zt,Sl),t(je,Ml),p(e,cr,o),g(mt,e,o),p(e,dr,o),p(e,Ee,o),t(Ee,Cl),t(Ee,oa),t(oa,Il),t(Ee,Nl),p(e,_r,o),p(e,Xt,o),t(Xt,Ol),p(e,gr,o),g(ht,e,o),p(e,vr,o),p(e,es,o),t(es,Ll),p(e,yr,o),p(e,Ae,o),t(Ae,ts),t(ts,ss),t(ss,Dl),t(ts,Rl),t(Ae,Hl),t(Ae,as),t(as,rs),t(rs,Ul),t(as,Wl),p(e,$r,o),p(e,Te,o),t(Te,Bl),t(Te,os),t(os,Yl),t(Te,Gl),p(e,br,o),g(ct,e,o),p(e,wr,o),p(e,xe,o),t(xe,Ql),t(xe,ns),t(ns,Jl),t(xe,Kl),p(e,kr,o),p(e,ne,o),t(ne,qe),t(qe,na),g(dt,na,null),t(ne,Vl),t(ne,la),t(la,Zl),p(e,jr,o),p(e,F,o),t(F,Xl),t(F,ls),t(ls,ei),t(F,ti),t(F,is),t(is,si),t(F,ai),t(F,ps),t(ps,ri),t(F,oi),t(F,fs),t(fs,ni),t(F,li),t(F,us),t(us,ii),t(F,pi),p(e,Er,o),g(_t,e,o),p(e,Ar,o),g(ze,e,o),p(e,Tr,o),p(e,Fe,o),t(Fe,fi),t(Fe,ia),t(ia,ui),t(Fe,mi),p(e,xr,o),g(gt,e,o),p(e,qr,o),p(e,Z,o),t(Z,hi),t(Z,pa),t(pa,ci),t(Z,di),t(Z,fa),t(fa,_i),t(Z,gi),p(e,zr,o),g(vt,e,o),p(e,Fr,o),g(Pe,e,o),p(e,Pr,o),p(e,z,o),t(z,vi),t(z,yt),t(yt,ua),t(ua,yi),t(z,$i),t(z,$t),t($t,ma),t(ma,bi),t(z,wi),t(z,ms),t(ms,ki),t(z,ji),t(z,ha),t(ha,Ei),t(z,Ai),t(z,bt),t(bt,Ti),t(z,xi),t(z,hs),t(hs,qi),t(z,zi),p(e,Sr,o),g(Se,e,o),p(e,Mr,o),p(e,le,o),t(le,Me),t(Me,ca),g(wt,ca,null),t(le,Fi),t(le,da),t(da,Pi),p(e,Cr,o),p(e,Ce,o),t(Ce,Si),t(Ce,cs),t(cs,Mi),t(Ce,Ci),p(e,Ir,o),g(kt,e,o),p(e,Nr,o),p(e,Ie,o),t(Ie,Ii),t(Ie,ds),t(ds,Ni),t(Ie,Oi),p(e,Or,o),g(jt,e,o),p(e,Lr,o),p(e,X,o),t(X,Li),t(X,_a),t(_a,Di),t(X,Ri),t(X,ga),t(ga,Hi),t(X,Ui),p(e,Dr,o),g(Et,e,o),Rr=!0},p(e,[o]){const At={};o&2&&(At.$$scope={dirty:o,ctx:e}),fe.$set(At);const va={};o&2&&(va.$$scope={dirty:o,ctx:e}),ce.$set(va);const ya={};o&2&&(ya.$$scope={dirty:o,ctx:e}),ze.$set(ya);const $a={};o&2&&($a.$$scope={dirty:o,ctx:e}),Pe.$set($a);const ie={};o&2&&(ie.$$scope={dirty:o,ctx:e}),Se.$set(ie)},i(e){Rr||(v(b.$$.fragment,e),v(O.$$.fragment,e),v(fe.$$.fragment,e),v(Oe.$$.fragment,e),v(De.$$.fragment,e),v(ce.$$.fragment,e),v(We.$$.fragment,e),v(Be.$$.fragment,e),v(Ye.$$.fragment,e),v(Qe.$$.fragment,e),v(Je.$$.fragment,e),v(Ve.$$.fragment,e),v(Ze.$$.fragment,e),v(tt.$$.fragment,e),v(st.$$.fragment,e),v(at.$$.fragment,e),v(nt.$$.fragment,e),v(lt.$$.fragment,e),v(it.$$.fragment,e),v(pt.$$.fragment,e),v(ft.$$.fragment,e),v(ut.$$.fragment,e),v(mt.$$.fragment,e),v(ht.$$.fragment,e),v(ct.$$.fragment,e),v(dt.$$.fragment,e),v(_t.$$.fragment,e),v(ze.$$.fragment,e),v(gt.$$.fragment,e),v(vt.$$.fragment,e),v(Pe.$$.fragment,e),v(Se.$$.fragment,e),v(wt.$$.fragment,e),v(kt.$$.fragment,e),v(jt.$$.fragment,e),v(Et.$$.fragment,e),Rr=!0)},o(e){y(b.$$.fragment,e),y(O.$$.fragment,e),y(fe.$$.fragment,e),y(Oe.$$.fragment,e),y(De.$$.fragment,e),y(ce.$$.fragment,e),y(We.$$.fragment,e),y(Be.$$.fragment,e),y(Ye.$$.fragment,e),y(Qe.$$.fragment,e),y(Je.$$.fragment,e),y(Ve.$$.fragment,e),y(Ze.$$.fragment,e),y(tt.$$.fragment,e),y(st.$$.fragment,e),y(at.$$.fragment,e),y(nt.$$.fragment,e),y(lt.$$.fragment,e),y(it.$$.fragment,e),y(pt.$$.fragment,e),y(ft.$$.fragment,e),y(ut.$$.fragment,e),y(mt.$$.fragment,e),y(ht.$$.fragment,e),y(ct.$$.fragment,e),y(dt.$$.fragment,e),y(_t.$$.fragment,e),y(ze.$$.fragment,e),y(gt.$$.fragment,e),y(vt.$$.fragment,e),y(Pe.$$.fragment,e),y(Se.$$.fragment,e),y(wt.$$.fragment,e),y(kt.$$.fragment,e),y(jt.$$.fragment,e),y(Et.$$.fragment,e),Rr=!1},d(e){s(h),e&&s(k),e&&s(c),$(b),e&&s(E),$(O,e),e&&s(R),e&&s(H),e&&s(wa),$(fe,e),e&&s(ka),e&&s(te),$(Oe),e&&s(ja),e&&s(Le),e&&s(Ea),$(De,e),e&&s(Aa),e&&s(me),e&&s(Ta),e&&s(Re),e&&s(xa),e&&s(T),e&&s(qa),e&&s(He),e&&s(za),e&&s(U),e&&s(Fa),e&&s(Ue),e&&s(Pa),e&&s(he),e&&s(Sa),$(ce,e),e&&s(Ma),e&&s(se),$(We),e&&s(Ca),e&&s(_e),e&&s(Ia),e&&s(Pt),e&&s(Na),$(Be,e),e&&s(Oa),e&&s(ge),e&&s(La),$(Ye,e),e&&s(Da),e&&s(W),e&&s(Ra),$(Qe,e),e&&s(Ha),e&&s(ve),e&&s(Ua),$(Je,e),e&&s(Wa),e&&s(B),e&&s(Ba),$(Ve,e),e&&s(Ya),e&&s(ye),e&&s(Ga),$(Ze,e),e&&s(Qa),e&&s(Y),e&&s(Ja),$(tt,e),e&&s(Ka),e&&s(Nt),e&&s(Va),$(st,e),e&&s(Za),e&&s($e),e&&s(Xa),e&&s(ae),$(at),e&&s(er),e&&s(M),e&&s(tr),$(nt,e),e&&s(sr),e&&s(G),e&&s(ar),$(lt,e),e&&s(rr),e&&s(Q),e&&s(or),$(it,e),e&&s(nr),e&&s(J),e&&s(lr),e&&s(re),$(pt),e&&s(ir),$(ft,e),e&&s(pr),e&&s(q),e&&s(fr),e&&s(K),e&&s(ur),e&&s(oe),$(ut),e&&s(mr),e&&s(V),e&&s(hr),e&&s(je),e&&s(cr),$(mt,e),e&&s(dr),e&&s(Ee),e&&s(_r),e&&s(Xt),e&&s(gr),$(ht,e),e&&s(vr),e&&s(es),e&&s(yr),e&&s(Ae),e&&s($r),e&&s(Te),e&&s(br),$(ct,e),e&&s(wr),e&&s(xe),e&&s(kr),e&&s(ne),$(dt),e&&s(jr),e&&s(F),e&&s(Er),$(_t,e),e&&s(Ar),$(ze,e),e&&s(Tr),e&&s(Fe),e&&s(xr),$(gt,e),e&&s(qr),e&&s(Z),e&&s(zr),$(vt,e),e&&s(Fr),$(Pe,e),e&&s(Pr),e&&s(z),e&&s(Sr),$(Se,e),e&&s(Mr),e&&s(le),$(wt),e&&s(Cr),e&&s(Ce),e&&s(Ir),$(kt,e),e&&s(Nr),e&&s(Ie),e&&s(Or),$(jt,e),e&&s(Lr),e&&s(X),e&&s(Dr),$(Et,e)}}}const au={local:"quick-tour",sections:[{local:"pipeline",sections:[{local:"pipeline-usage",title:"Pipeline usage"},{local:"use-another-model-and-tokenizer-in-the-pipeline",title:"Use another model and tokenizer in the pipeline"}],title:"Pipeline"},{local:"autoclass",sections:[{local:"autotokenizer",title:"AutoTokenizer"},{local:"automodel",title:"AutoModel"},{local:"save-a-model",title:"Save a model"}],title:"AutoClass"}],title:"Quick tour"};function ru(N,h,k){let{fw:c}=h;return N.$$set=w=>{"fw"in w&&k(0,c=w.fw)},[c]}class hu extends Yf{constructor(h){super();Gf(this,h,ru,su,Qf,{fw:0})}}export{hu as default,au as metadata};
