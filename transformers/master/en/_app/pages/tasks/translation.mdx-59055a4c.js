import{S as Rn,i as Xn,s as Gn,e as l,k as f,w as $,t as n,M as Qn,c as r,d as t,m as h,a as i,x as k,h as o,b as u,F as s,g as p,y as w,q as v,o as b,B as q}from"../../chunks/vendor-4833417e.js";import{T as Ts}from"../../chunks/Tip-fffd6df1.js";import{Y as Kn}from"../../chunks/Youtube-27813aed.js";import{I as yt}from"../../chunks/IconCopyLink-4b81c553.js";import{C as A}from"../../chunks/CodeBlock-90ffda97.js";import{C as Vn}from"../../chunks/CodeBlockFw-03f30a28.js";import"../../chunks/CopyButton-04a16537.js";function eo(C){let m,j,c,_,E;return{c(){m=l("p"),j=n("See the translation "),c=l("a"),_=n("task page"),E=n(" for more information about its associated models, datasets, and metrics."),this.h()},l(d){m=r(d,"P",{});var g=i(m);j=o(g,"See the translation "),c=r(g,"A",{href:!0,rel:!0});var S=i(c);_=o(S,"task page"),S.forEach(t),E=o(g," for more information about its associated models, datasets, and metrics."),g.forEach(t),this.h()},h(){u(c,"href","https://huggingface.co/tasks/translation"),u(c,"rel","nofollow")},m(d,g){p(d,m,g),s(m,j),s(m,c),s(c,_),s(m,E)},d(d){d&&t(m)}}}function to(C){let m,j,c,_,E,d,g,S;return{c(){m=l("p"),j=n("If you aren\u2019t familiar with fine-tuning a model with the "),c=l("a"),_=n("Trainer"),E=n(", take a look at the basic tutorial "),d=l("a"),g=n("here"),S=n("!"),this.h()},l(T){m=r(T,"P",{});var y=i(m);j=o(y,"If you aren\u2019t familiar with fine-tuning a model with the "),c=r(y,"A",{href:!0});var z=i(c);_=o(z,"Trainer"),z.forEach(t),E=o(y,", take a look at the basic tutorial "),d=r(y,"A",{href:!0});var D=i(d);g=o(D,"here"),D.forEach(t),S=o(y,"!"),y.forEach(t),this.h()},h(){u(c,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),u(d,"href","training#finetune-with-trainer")},m(T,y){p(T,m,y),s(m,j),s(m,c),s(c,_),s(m,E),s(m,d),s(d,g),s(m,S)},d(T){T&&t(m)}}}function so(C){let m,j,c,_,E;return{c(){m=l("p"),j=n("If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),c=l("a"),_=n("here"),E=n("!"),this.h()},l(d){m=r(d,"P",{});var g=i(m);j=o(g,"If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),c=r(g,"A",{href:!0});var S=i(c);_=o(S,"here"),S.forEach(t),E=o(g,"!"),g.forEach(t),this.h()},h(){u(c,"href","training#finetune-with-keras")},m(d,g){p(d,m,g),s(m,j),s(m,c),s(c,_),s(m,E)},d(d){d&&t(m)}}}function ao(C){let m,j,c,_,E,d,g,S;return{c(){m=l("p"),j=n(`For a more in-depth example of how to fine-tune a model for translation, take a look at the corresponding
`),c=l("a"),_=n("PyTorch notebook"),E=n(`
or `),d=l("a"),g=n("TensorFlow notebook"),S=n("."),this.h()},l(T){m=r(T,"P",{});var y=i(m);j=o(y,`For a more in-depth example of how to fine-tune a model for translation, take a look at the corresponding
`),c=r(y,"A",{href:!0,rel:!0});var z=i(c);_=o(z,"PyTorch notebook"),z.forEach(t),E=o(y,`
or `),d=r(y,"A",{href:!0,rel:!0});var D=i(d);g=o(D,"TensorFlow notebook"),D.forEach(t),S=o(y,"."),y.forEach(t),this.h()},h(){u(c,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/translation.ipynb"),u(c,"rel","nofollow"),u(d,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/translation-tf.ipynb"),u(d,"rel","nofollow")},m(T,y){p(T,m,y),s(m,j),s(m,c),s(c,_),s(m,E),s(m,d),s(d,g),s(m,S)},d(T){T&&t(m)}}}function no(C){let m,j,c,_,E,d,g,S,T,y,z,D,De,xs,Et,L,zs,oe,As,Fs,le,Ps,Cs,St,Y,Tt,N,H,Ve,re,Ds,et,Ls,xt,Le,Ms,zt,ie,At,Me,Os,Ft,pe,Pt,Oe,Is,Ct,fe,Dt,Z,Ns,tt,Us,Bs,Lt,U,J,st,he,Ws,at,Ys,Mt,me,Ot,Ie,Hs,It,ue,Nt,Ne,Zs,Ut,M,nt,Js,Ks,ot,Rs,Xs,ce,Gs,lt,Qs,Vs,Bt,de,Wt,F,ea,_e,rt,ta,sa,it,aa,na,pt,oa,la,Yt,ge,Ht,x,ra,Ue,ia,pa,ft,fa,ha,ht,ma,ua,mt,ca,da,Zt,$e,Jt,B,K,ut,ke,_a,ct,ga,Kt,R,$a,Be,ka,wa,Rt,we,Xt,X,Gt,We,va,Qt,O,ve,ba,Ye,qa,ja,ya,be,Ea,He,Sa,Ta,xa,qe,za,Ze,Aa,Fa,Vt,je,es,W,G,dt,ye,Pa,_t,Ca,ts,Je,Da,ss,Q,as,P,La,gt,Ma,Oa,Ee,$t,Ia,Na,kt,Ua,Ba,ns,Se,os,Ke,Wa,ls,Te,rs,V,Ya,Re,Ha,Za,is,xe,ps,ee,Ja,ze,wt,Ka,Ra,fs,Ae,hs,te,Xa,Fe,vt,Ga,Qa,ms,Pe,us,se,cs;return d=new yt({}),z=new Kn({props:{id:"1JvfrvZgi6c"}}),Y=new Ts({props:{$$slots:{default:[eo]},$$scope:{ctx:C}}}),re=new yt({}),ie=new A({props:{code:`from datasets import load_dataset

books = load_dataset("opus_books", "en-fr"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>books = load_dataset(<span class="hljs-string">&quot;opus_books&quot;</span>, <span class="hljs-string">&quot;en-fr&quot;</span>)`}}),pe=new A({props:{code:'books = books["train"].train_test_split(test_size=0.2),',highlighted:'books = books[<span class="hljs-string">&quot;train&quot;</span>].train_test_split(test_size=<span class="hljs-number">0.2</span>)'}}),fe=new A({props:{code:'books["train"][0],',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>books[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-string">&#x27;90560&#x27;</span>,
 <span class="hljs-string">&#x27;translation&#x27;</span>: {<span class="hljs-string">&#x27;en&#x27;</span>: <span class="hljs-string">&#x27;But this lofty plateau measured only a few fathoms, and soon we reentered Our Element.&#x27;</span>,
  <span class="hljs-string">&#x27;fr&#x27;</span>: <span class="hljs-string">&#x27;Mais ce plateau \xE9lev\xE9 ne mesurait que quelques toises, et bient\xF4t nous f\xFBmes rentr\xE9s dans notre \xE9l\xE9ment.&#x27;</span>}}`}}),he=new yt({}),me=new Kn({props:{id:"XAR8jnZZuUs"}}),ue=new A({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("t5-small"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-small&quot;</span>)`}}),de=new A({props:{code:`source_lang = "en"
target_lang = "fr"
prefix = "translate English to French: "


def preprocess_function(examples):
    inputs = [prefix + example[source_lang] for example in examples["translation"]]
    targets = [example[target_lang] for example in examples["translation"]]
    model_inputs = tokenizer(inputs, max_length=128, truncation=True)

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=128, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>source_lang = <span class="hljs-string">&quot;en&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_lang = <span class="hljs-string">&quot;fr&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>prefix = <span class="hljs-string">&quot;translate English to French: &quot;</span>


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    inputs = [prefix + example[source_lang] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;translation&quot;</span>]]
<span class="hljs-meta">... </span>    targets = [example[target_lang] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;translation&quot;</span>]]
<span class="hljs-meta">... </span>    model_inputs = tokenizer(inputs, max_length=<span class="hljs-number">128</span>, truncation=<span class="hljs-literal">True</span>)

<span class="hljs-meta">... </span>    <span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>        labels = tokenizer(targets, max_length=<span class="hljs-number">128</span>, truncation=<span class="hljs-literal">True</span>)

<span class="hljs-meta">... </span>    model_inputs[<span class="hljs-string">&quot;labels&quot;</span>] = labels[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> model_inputs`}}),ge=new A({props:{code:"tokenized_books = books.map(preprocess_function, batched=True),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_books = books.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)'}}),$e=new Vn({props:{pt:{code:`from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)`},tf:{code:`from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors="tf")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}}),ke=new yt({}),we=new A({props:{code:`from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

model = AutoModelForSeq2SeqLM.from_pretrained("t5-small"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-small&quot;</span>)`}}),X=new Ts({props:{$$slots:{default:[to]},$$scope:{ctx:C}}}),je=new A({props:{code:`training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=1,
    fp16=True,
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_books["train"],
    eval_dataset=tokenized_books["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train(),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = Seq2SeqTrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>    save_total_limit=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    fp16=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Seq2SeqTrainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=tokenized_books[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=tokenized_books[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),ye=new yt({}),Q=new Ts({props:{$$slots:{default:[so]},$$scope:{ctx:C}}}),Se=new A({props:{code:`tf_train_set = tokenized_books["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_test_set = tokenized_books["test"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = tokenized_books[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_test_set = tokenized_books[<span class="hljs-string">&quot;test&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`}}),Te=new A({props:{code:`from transformers import create_optimizer, AdamWeightDecay

optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer, AdamWeightDecay

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = AdamWeightDecay(learning_rate=<span class="hljs-number">2e-5</span>, weight_decay_rate=<span class="hljs-number">0.01</span>)`}}),xe=new A({props:{code:`from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-small"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-small&quot;</span>)`}}),Ae=new A({props:{code:"model.compile(optimizer=optimizer),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)'}}),Pe=new A({props:{code:"model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=<span class="hljs-number">3</span>)'}}),se=new Ts({props:{$$slots:{default:[ao]},$$scope:{ctx:C}}}),{c(){m=l("meta"),j=f(),c=l("h1"),_=l("a"),E=l("span"),$(d.$$.fragment),g=f(),S=l("span"),T=n("Translation"),y=f(),$(z.$$.fragment),D=f(),De=l("p"),xs=n("Translation converts a sequence of text from one language to another. It is one of several tasks you can formulate as a sequence-to-sequence problem, a powerful framework that extends to vision and audio tasks."),Et=f(),L=l("p"),zs=n("This guide will show you how to fine-tune "),oe=l("a"),As=n("T5"),Fs=n(" on the English-French subset of the "),le=l("a"),Ps=n("OPUS Books"),Cs=n(" dataset to translate English text to French."),St=f(),$(Y.$$.fragment),Tt=f(),N=l("h2"),H=l("a"),Ve=l("span"),$(re.$$.fragment),Ds=f(),et=l("span"),Ls=n("Load OPUS Books dataset"),xt=f(),Le=l("p"),Ms=n("Load the OPUS Books dataset from the \u{1F917} Datasets library:"),zt=f(),$(ie.$$.fragment),At=f(),Me=l("p"),Os=n("Split this dataset into a train and test set:"),Ft=f(),$(pe.$$.fragment),Pt=f(),Oe=l("p"),Is=n("Then take a look at an example:"),Ct=f(),$(fe.$$.fragment),Dt=f(),Z=l("p"),Ns=n("The "),tt=l("code"),Us=n("translation"),Bs=n(" field is a dictionary containing the English and French translations of the text."),Lt=f(),U=l("h2"),J=l("a"),st=l("span"),$(he.$$.fragment),Ws=f(),at=l("span"),Ys=n("Preprocess"),Mt=f(),$(me.$$.fragment),Ot=f(),Ie=l("p"),Hs=n("Load the T5 tokenizer to process the language pairs:"),It=f(),$(ue.$$.fragment),Nt=f(),Ne=l("p"),Zs=n("The preprocessing function needs to:"),Ut=f(),M=l("ol"),nt=l("li"),Js=n("Prefix the input with a prompt so T5 knows this is a translation task. Some models capable of multiple NLP tasks require prompting for specific tasks."),Ks=f(),ot=l("li"),Rs=n("Tokenize the input (English) and target (French) separately. You can\u2019t tokenize French text with a tokenizer pretrained on an English vocabulary. A context manager will help set the tokenizer to French first before tokenizing it."),Xs=f(),ce=l("li"),Gs=n("Truncate sequences to be no longer than the maximum length set by the "),lt=l("code"),Qs=n("max_length"),Vs=n(" parameter."),Bt=f(),$(de.$$.fragment),Wt=f(),F=l("p"),ea=n("Use \u{1F917} Datasets "),_e=l("a"),rt=l("code"),ta=n("map"),sa=n(" function to apply the preprocessing function over the entire dataset. You can speed up the "),it=l("code"),aa=n("map"),na=n(" function by setting "),pt=l("code"),oa=n("batched=True"),la=n(" to process multiple elements of the dataset at once:"),Yt=f(),$(ge.$$.fragment),Ht=f(),x=l("p"),ra=n("Use "),Ue=l("a"),ia=n("DataCollatorForSeq2Seq"),pa=n(" to create a batch of examples. It will also "),ft=l("em"),fa=n("dynamically pad"),ha=n(" your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),ht=l("code"),ma=n("tokenizer"),ua=n(" function by setting "),mt=l("code"),ca=n("padding=True"),da=n(", dynamic padding is more efficient."),Zt=f(),$($e.$$.fragment),Jt=f(),B=l("h2"),K=l("a"),ut=l("span"),$(ke.$$.fragment),_a=f(),ct=l("span"),ga=n("Fine-tune with Trainer"),Kt=f(),R=l("p"),$a=n("Load T5 with "),Be=l("a"),ka=n("AutoModelForSeq2SeqLM"),wa=n(":"),Rt=f(),$(we.$$.fragment),Xt=f(),$(X.$$.fragment),Gt=f(),We=l("p"),va=n("At this point, only three steps remain:"),Qt=f(),O=l("ol"),ve=l("li"),ba=n("Define your training hyperparameters in "),Ye=l("a"),qa=n("Seq2SeqTrainingArguments"),ja=n("."),ya=f(),be=l("li"),Ea=n("Pass the training arguments to "),He=l("a"),Sa=n("Seq2SeqTrainer"),Ta=n(" along with the model, dataset, tokenizer, and data collator."),xa=f(),qe=l("li"),za=n("Call "),Ze=l("a"),Aa=n("train()"),Fa=n(" to fine-tune your model."),Vt=f(),$(je.$$.fragment),es=f(),W=l("h2"),G=l("a"),dt=l("span"),$(ye.$$.fragment),Pa=f(),_t=l("span"),Ca=n("Fine-tune with TensorFlow"),ts=f(),Je=l("p"),Da=n("To fine-tune a model in TensorFlow is just as easy, with only a few differences."),ss=f(),$(Q.$$.fragment),as=f(),P=l("p"),La=n("Convert your datasets to the "),gt=l("code"),Ma=n("tf.data.Dataset"),Oa=n(" format with "),Ee=l("a"),$t=l("code"),Ia=n("to_tf_dataset"),Na=n(". Specify inputs and labels in "),kt=l("code"),Ua=n("columns"),Ba=n(", whether to shuffle the dataset order, batch size, and the data collator:"),ns=f(),$(Se.$$.fragment),os=f(),Ke=l("p"),Wa=n("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),ls=f(),$(Te.$$.fragment),rs=f(),V=l("p"),Ya=n("Load T5 with "),Re=l("a"),Ha=n("TFAutoModelForSeq2SeqLM"),Za=n(":"),is=f(),$(xe.$$.fragment),ps=f(),ee=l("p"),Ja=n("Configure the model for training with "),ze=l("a"),wt=l("code"),Ka=n("compile"),Ra=n(":"),fs=f(),$(Ae.$$.fragment),hs=f(),te=l("p"),Xa=n("Call "),Fe=l("a"),vt=l("code"),Ga=n("fit"),Qa=n(" to fine-tune the model:"),ms=f(),$(Pe.$$.fragment),us=f(),$(se.$$.fragment),this.h()},l(e){const a=Qn('[data-svelte="svelte-1phssyn"]',document.head);m=r(a,"META",{name:!0,content:!0}),a.forEach(t),j=h(e),c=r(e,"H1",{class:!0});var Ce=i(c);_=r(Ce,"A",{id:!0,class:!0,href:!0});var bt=i(_);E=r(bt,"SPAN",{});var qt=i(E);k(d.$$.fragment,qt),qt.forEach(t),bt.forEach(t),g=h(Ce),S=r(Ce,"SPAN",{});var jt=i(S);T=o(jt,"Translation"),jt.forEach(t),Ce.forEach(t),y=h(e),k(z.$$.fragment,e),D=h(e),De=r(e,"P",{});var Va=i(De);xs=o(Va,"Translation converts a sequence of text from one language to another. It is one of several tasks you can formulate as a sequence-to-sequence problem, a powerful framework that extends to vision and audio tasks."),Va.forEach(t),Et=h(e),L=r(e,"P",{});var Xe=i(L);zs=o(Xe,"This guide will show you how to fine-tune "),oe=r(Xe,"A",{href:!0,rel:!0});var en=i(oe);As=o(en,"T5"),en.forEach(t),Fs=o(Xe," on the English-French subset of the "),le=r(Xe,"A",{href:!0,rel:!0});var tn=i(le);Ps=o(tn,"OPUS Books"),tn.forEach(t),Cs=o(Xe," dataset to translate English text to French."),Xe.forEach(t),St=h(e),k(Y.$$.fragment,e),Tt=h(e),N=r(e,"H2",{class:!0});var ds=i(N);H=r(ds,"A",{id:!0,class:!0,href:!0});var sn=i(H);Ve=r(sn,"SPAN",{});var an=i(Ve);k(re.$$.fragment,an),an.forEach(t),sn.forEach(t),Ds=h(ds),et=r(ds,"SPAN",{});var nn=i(et);Ls=o(nn,"Load OPUS Books dataset"),nn.forEach(t),ds.forEach(t),xt=h(e),Le=r(e,"P",{});var on=i(Le);Ms=o(on,"Load the OPUS Books dataset from the \u{1F917} Datasets library:"),on.forEach(t),zt=h(e),k(ie.$$.fragment,e),At=h(e),Me=r(e,"P",{});var ln=i(Me);Os=o(ln,"Split this dataset into a train and test set:"),ln.forEach(t),Ft=h(e),k(pe.$$.fragment,e),Pt=h(e),Oe=r(e,"P",{});var rn=i(Oe);Is=o(rn,"Then take a look at an example:"),rn.forEach(t),Ct=h(e),k(fe.$$.fragment,e),Dt=h(e),Z=r(e,"P",{});var _s=i(Z);Ns=o(_s,"The "),tt=r(_s,"CODE",{});var pn=i(tt);Us=o(pn,"translation"),pn.forEach(t),Bs=o(_s," field is a dictionary containing the English and French translations of the text."),_s.forEach(t),Lt=h(e),U=r(e,"H2",{class:!0});var gs=i(U);J=r(gs,"A",{id:!0,class:!0,href:!0});var fn=i(J);st=r(fn,"SPAN",{});var hn=i(st);k(he.$$.fragment,hn),hn.forEach(t),fn.forEach(t),Ws=h(gs),at=r(gs,"SPAN",{});var mn=i(at);Ys=o(mn,"Preprocess"),mn.forEach(t),gs.forEach(t),Mt=h(e),k(me.$$.fragment,e),Ot=h(e),Ie=r(e,"P",{});var un=i(Ie);Hs=o(un,"Load the T5 tokenizer to process the language pairs:"),un.forEach(t),It=h(e),k(ue.$$.fragment,e),Nt=h(e),Ne=r(e,"P",{});var cn=i(Ne);Zs=o(cn,"The preprocessing function needs to:"),cn.forEach(t),Ut=h(e),M=r(e,"OL",{});var Ge=i(M);nt=r(Ge,"LI",{});var dn=i(nt);Js=o(dn,"Prefix the input with a prompt so T5 knows this is a translation task. Some models capable of multiple NLP tasks require prompting for specific tasks."),dn.forEach(t),Ks=h(Ge),ot=r(Ge,"LI",{});var _n=i(ot);Rs=o(_n,"Tokenize the input (English) and target (French) separately. You can\u2019t tokenize French text with a tokenizer pretrained on an English vocabulary. A context manager will help set the tokenizer to French first before tokenizing it."),_n.forEach(t),Xs=h(Ge),ce=r(Ge,"LI",{});var $s=i(ce);Gs=o($s,"Truncate sequences to be no longer than the maximum length set by the "),lt=r($s,"CODE",{});var gn=i(lt);Qs=o(gn,"max_length"),gn.forEach(t),Vs=o($s," parameter."),$s.forEach(t),Ge.forEach(t),Bt=h(e),k(de.$$.fragment,e),Wt=h(e),F=r(e,"P",{});var ae=i(F);ea=o(ae,"Use \u{1F917} Datasets "),_e=r(ae,"A",{href:!0,rel:!0});var $n=i(_e);rt=r($n,"CODE",{});var kn=i(rt);ta=o(kn,"map"),kn.forEach(t),$n.forEach(t),sa=o(ae," function to apply the preprocessing function over the entire dataset. You can speed up the "),it=r(ae,"CODE",{});var wn=i(it);aa=o(wn,"map"),wn.forEach(t),na=o(ae," function by setting "),pt=r(ae,"CODE",{});var vn=i(pt);oa=o(vn,"batched=True"),vn.forEach(t),la=o(ae," to process multiple elements of the dataset at once:"),ae.forEach(t),Yt=h(e),k(ge.$$.fragment,e),Ht=h(e),x=r(e,"P",{});var I=i(x);ra=o(I,"Use "),Ue=r(I,"A",{href:!0});var bn=i(Ue);ia=o(bn,"DataCollatorForSeq2Seq"),bn.forEach(t),pa=o(I," to create a batch of examples. It will also "),ft=r(I,"EM",{});var qn=i(ft);fa=o(qn,"dynamically pad"),qn.forEach(t),ha=o(I," your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),ht=r(I,"CODE",{});var jn=i(ht);ma=o(jn,"tokenizer"),jn.forEach(t),ua=o(I," function by setting "),mt=r(I,"CODE",{});var yn=i(mt);ca=o(yn,"padding=True"),yn.forEach(t),da=o(I,", dynamic padding is more efficient."),I.forEach(t),Zt=h(e),k($e.$$.fragment,e),Jt=h(e),B=r(e,"H2",{class:!0});var ks=i(B);K=r(ks,"A",{id:!0,class:!0,href:!0});var En=i(K);ut=r(En,"SPAN",{});var Sn=i(ut);k(ke.$$.fragment,Sn),Sn.forEach(t),En.forEach(t),_a=h(ks),ct=r(ks,"SPAN",{});var Tn=i(ct);ga=o(Tn,"Fine-tune with Trainer"),Tn.forEach(t),ks.forEach(t),Kt=h(e),R=r(e,"P",{});var ws=i(R);$a=o(ws,"Load T5 with "),Be=r(ws,"A",{href:!0});var xn=i(Be);ka=o(xn,"AutoModelForSeq2SeqLM"),xn.forEach(t),wa=o(ws,":"),ws.forEach(t),Rt=h(e),k(we.$$.fragment,e),Xt=h(e),k(X.$$.fragment,e),Gt=h(e),We=r(e,"P",{});var zn=i(We);va=o(zn,"At this point, only three steps remain:"),zn.forEach(t),Qt=h(e),O=r(e,"OL",{});var Qe=i(O);ve=r(Qe,"LI",{});var vs=i(ve);ba=o(vs,"Define your training hyperparameters in "),Ye=r(vs,"A",{href:!0});var An=i(Ye);qa=o(An,"Seq2SeqTrainingArguments"),An.forEach(t),ja=o(vs,"."),vs.forEach(t),ya=h(Qe),be=r(Qe,"LI",{});var bs=i(be);Ea=o(bs,"Pass the training arguments to "),He=r(bs,"A",{href:!0});var Fn=i(He);Sa=o(Fn,"Seq2SeqTrainer"),Fn.forEach(t),Ta=o(bs," along with the model, dataset, tokenizer, and data collator."),bs.forEach(t),xa=h(Qe),qe=r(Qe,"LI",{});var qs=i(qe);za=o(qs,"Call "),Ze=r(qs,"A",{href:!0});var Pn=i(Ze);Aa=o(Pn,"train()"),Pn.forEach(t),Fa=o(qs," to fine-tune your model."),qs.forEach(t),Qe.forEach(t),Vt=h(e),k(je.$$.fragment,e),es=h(e),W=r(e,"H2",{class:!0});var js=i(W);G=r(js,"A",{id:!0,class:!0,href:!0});var Cn=i(G);dt=r(Cn,"SPAN",{});var Dn=i(dt);k(ye.$$.fragment,Dn),Dn.forEach(t),Cn.forEach(t),Pa=h(js),_t=r(js,"SPAN",{});var Ln=i(_t);Ca=o(Ln,"Fine-tune with TensorFlow"),Ln.forEach(t),js.forEach(t),ts=h(e),Je=r(e,"P",{});var Mn=i(Je);Da=o(Mn,"To fine-tune a model in TensorFlow is just as easy, with only a few differences."),Mn.forEach(t),ss=h(e),k(Q.$$.fragment,e),as=h(e),P=r(e,"P",{});var ne=i(P);La=o(ne,"Convert your datasets to the "),gt=r(ne,"CODE",{});var On=i(gt);Ma=o(On,"tf.data.Dataset"),On.forEach(t),Oa=o(ne," format with "),Ee=r(ne,"A",{href:!0,rel:!0});var In=i(Ee);$t=r(In,"CODE",{});var Nn=i($t);Ia=o(Nn,"to_tf_dataset"),Nn.forEach(t),In.forEach(t),Na=o(ne,". Specify inputs and labels in "),kt=r(ne,"CODE",{});var Un=i(kt);Ua=o(Un,"columns"),Un.forEach(t),Ba=o(ne,", whether to shuffle the dataset order, batch size, and the data collator:"),ne.forEach(t),ns=h(e),k(Se.$$.fragment,e),os=h(e),Ke=r(e,"P",{});var Bn=i(Ke);Wa=o(Bn,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Bn.forEach(t),ls=h(e),k(Te.$$.fragment,e),rs=h(e),V=r(e,"P",{});var ys=i(V);Ya=o(ys,"Load T5 with "),Re=r(ys,"A",{href:!0});var Wn=i(Re);Ha=o(Wn,"TFAutoModelForSeq2SeqLM"),Wn.forEach(t),Za=o(ys,":"),ys.forEach(t),is=h(e),k(xe.$$.fragment,e),ps=h(e),ee=r(e,"P",{});var Es=i(ee);Ja=o(Es,"Configure the model for training with "),ze=r(Es,"A",{href:!0,rel:!0});var Yn=i(ze);wt=r(Yn,"CODE",{});var Hn=i(wt);Ka=o(Hn,"compile"),Hn.forEach(t),Yn.forEach(t),Ra=o(Es,":"),Es.forEach(t),fs=h(e),k(Ae.$$.fragment,e),hs=h(e),te=r(e,"P",{});var Ss=i(te);Xa=o(Ss,"Call "),Fe=r(Ss,"A",{href:!0,rel:!0});var Zn=i(Fe);vt=r(Zn,"CODE",{});var Jn=i(vt);Ga=o(Jn,"fit"),Jn.forEach(t),Zn.forEach(t),Qa=o(Ss," to fine-tune the model:"),Ss.forEach(t),ms=h(e),k(Pe.$$.fragment,e),us=h(e),k(se.$$.fragment,e),this.h()},h(){u(m,"name","hf:doc:metadata"),u(m,"content",JSON.stringify(oo)),u(_,"id","translation"),u(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(_,"href","#translation"),u(c,"class","relative group"),u(oe,"href","https://huggingface.co/t5-small"),u(oe,"rel","nofollow"),u(le,"href","https://huggingface.co/datasets/opus_books"),u(le,"rel","nofollow"),u(H,"id","load-opus-books-dataset"),u(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(H,"href","#load-opus-books-dataset"),u(N,"class","relative group"),u(J,"id","preprocess"),u(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(J,"href","#preprocess"),u(U,"class","relative group"),u(_e,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map"),u(_e,"rel","nofollow"),u(Ue,"href","/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq"),u(K,"id","finetune-with-trainer"),u(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(K,"href","#finetune-with-trainer"),u(B,"class","relative group"),u(Be,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM"),u(Ye,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments"),u(He,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Seq2SeqTrainer"),u(Ze,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer.train"),u(G,"id","finetune-with-tensorflow"),u(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(G,"href","#finetune-with-tensorflow"),u(W,"class","relative group"),u(Ee,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.to_tf_dataset"),u(Ee,"rel","nofollow"),u(Re,"href","/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSeq2SeqLM"),u(ze,"href","https://keras.io/api/models/model_training_apis/#compile-method"),u(ze,"rel","nofollow"),u(Fe,"href","https://keras.io/api/models/model_training_apis/#fit-method"),u(Fe,"rel","nofollow")},m(e,a){s(document.head,m),p(e,j,a),p(e,c,a),s(c,_),s(_,E),w(d,E,null),s(c,g),s(c,S),s(S,T),p(e,y,a),w(z,e,a),p(e,D,a),p(e,De,a),s(De,xs),p(e,Et,a),p(e,L,a),s(L,zs),s(L,oe),s(oe,As),s(L,Fs),s(L,le),s(le,Ps),s(L,Cs),p(e,St,a),w(Y,e,a),p(e,Tt,a),p(e,N,a),s(N,H),s(H,Ve),w(re,Ve,null),s(N,Ds),s(N,et),s(et,Ls),p(e,xt,a),p(e,Le,a),s(Le,Ms),p(e,zt,a),w(ie,e,a),p(e,At,a),p(e,Me,a),s(Me,Os),p(e,Ft,a),w(pe,e,a),p(e,Pt,a),p(e,Oe,a),s(Oe,Is),p(e,Ct,a),w(fe,e,a),p(e,Dt,a),p(e,Z,a),s(Z,Ns),s(Z,tt),s(tt,Us),s(Z,Bs),p(e,Lt,a),p(e,U,a),s(U,J),s(J,st),w(he,st,null),s(U,Ws),s(U,at),s(at,Ys),p(e,Mt,a),w(me,e,a),p(e,Ot,a),p(e,Ie,a),s(Ie,Hs),p(e,It,a),w(ue,e,a),p(e,Nt,a),p(e,Ne,a),s(Ne,Zs),p(e,Ut,a),p(e,M,a),s(M,nt),s(nt,Js),s(M,Ks),s(M,ot),s(ot,Rs),s(M,Xs),s(M,ce),s(ce,Gs),s(ce,lt),s(lt,Qs),s(ce,Vs),p(e,Bt,a),w(de,e,a),p(e,Wt,a),p(e,F,a),s(F,ea),s(F,_e),s(_e,rt),s(rt,ta),s(F,sa),s(F,it),s(it,aa),s(F,na),s(F,pt),s(pt,oa),s(F,la),p(e,Yt,a),w(ge,e,a),p(e,Ht,a),p(e,x,a),s(x,ra),s(x,Ue),s(Ue,ia),s(x,pa),s(x,ft),s(ft,fa),s(x,ha),s(x,ht),s(ht,ma),s(x,ua),s(x,mt),s(mt,ca),s(x,da),p(e,Zt,a),w($e,e,a),p(e,Jt,a),p(e,B,a),s(B,K),s(K,ut),w(ke,ut,null),s(B,_a),s(B,ct),s(ct,ga),p(e,Kt,a),p(e,R,a),s(R,$a),s(R,Be),s(Be,ka),s(R,wa),p(e,Rt,a),w(we,e,a),p(e,Xt,a),w(X,e,a),p(e,Gt,a),p(e,We,a),s(We,va),p(e,Qt,a),p(e,O,a),s(O,ve),s(ve,ba),s(ve,Ye),s(Ye,qa),s(ve,ja),s(O,ya),s(O,be),s(be,Ea),s(be,He),s(He,Sa),s(be,Ta),s(O,xa),s(O,qe),s(qe,za),s(qe,Ze),s(Ze,Aa),s(qe,Fa),p(e,Vt,a),w(je,e,a),p(e,es,a),p(e,W,a),s(W,G),s(G,dt),w(ye,dt,null),s(W,Pa),s(W,_t),s(_t,Ca),p(e,ts,a),p(e,Je,a),s(Je,Da),p(e,ss,a),w(Q,e,a),p(e,as,a),p(e,P,a),s(P,La),s(P,gt),s(gt,Ma),s(P,Oa),s(P,Ee),s(Ee,$t),s($t,Ia),s(P,Na),s(P,kt),s(kt,Ua),s(P,Ba),p(e,ns,a),w(Se,e,a),p(e,os,a),p(e,Ke,a),s(Ke,Wa),p(e,ls,a),w(Te,e,a),p(e,rs,a),p(e,V,a),s(V,Ya),s(V,Re),s(Re,Ha),s(V,Za),p(e,is,a),w(xe,e,a),p(e,ps,a),p(e,ee,a),s(ee,Ja),s(ee,ze),s(ze,wt),s(wt,Ka),s(ee,Ra),p(e,fs,a),w(Ae,e,a),p(e,hs,a),p(e,te,a),s(te,Xa),s(te,Fe),s(Fe,vt),s(vt,Ga),s(te,Qa),p(e,ms,a),w(Pe,e,a),p(e,us,a),w(se,e,a),cs=!0},p(e,[a]){const Ce={};a&2&&(Ce.$$scope={dirty:a,ctx:e}),Y.$set(Ce);const bt={};a&2&&(bt.$$scope={dirty:a,ctx:e}),X.$set(bt);const qt={};a&2&&(qt.$$scope={dirty:a,ctx:e}),Q.$set(qt);const jt={};a&2&&(jt.$$scope={dirty:a,ctx:e}),se.$set(jt)},i(e){cs||(v(d.$$.fragment,e),v(z.$$.fragment,e),v(Y.$$.fragment,e),v(re.$$.fragment,e),v(ie.$$.fragment,e),v(pe.$$.fragment,e),v(fe.$$.fragment,e),v(he.$$.fragment,e),v(me.$$.fragment,e),v(ue.$$.fragment,e),v(de.$$.fragment,e),v(ge.$$.fragment,e),v($e.$$.fragment,e),v(ke.$$.fragment,e),v(we.$$.fragment,e),v(X.$$.fragment,e),v(je.$$.fragment,e),v(ye.$$.fragment,e),v(Q.$$.fragment,e),v(Se.$$.fragment,e),v(Te.$$.fragment,e),v(xe.$$.fragment,e),v(Ae.$$.fragment,e),v(Pe.$$.fragment,e),v(se.$$.fragment,e),cs=!0)},o(e){b(d.$$.fragment,e),b(z.$$.fragment,e),b(Y.$$.fragment,e),b(re.$$.fragment,e),b(ie.$$.fragment,e),b(pe.$$.fragment,e),b(fe.$$.fragment,e),b(he.$$.fragment,e),b(me.$$.fragment,e),b(ue.$$.fragment,e),b(de.$$.fragment,e),b(ge.$$.fragment,e),b($e.$$.fragment,e),b(ke.$$.fragment,e),b(we.$$.fragment,e),b(X.$$.fragment,e),b(je.$$.fragment,e),b(ye.$$.fragment,e),b(Q.$$.fragment,e),b(Se.$$.fragment,e),b(Te.$$.fragment,e),b(xe.$$.fragment,e),b(Ae.$$.fragment,e),b(Pe.$$.fragment,e),b(se.$$.fragment,e),cs=!1},d(e){t(m),e&&t(j),e&&t(c),q(d),e&&t(y),q(z,e),e&&t(D),e&&t(De),e&&t(Et),e&&t(L),e&&t(St),q(Y,e),e&&t(Tt),e&&t(N),q(re),e&&t(xt),e&&t(Le),e&&t(zt),q(ie,e),e&&t(At),e&&t(Me),e&&t(Ft),q(pe,e),e&&t(Pt),e&&t(Oe),e&&t(Ct),q(fe,e),e&&t(Dt),e&&t(Z),e&&t(Lt),e&&t(U),q(he),e&&t(Mt),q(me,e),e&&t(Ot),e&&t(Ie),e&&t(It),q(ue,e),e&&t(Nt),e&&t(Ne),e&&t(Ut),e&&t(M),e&&t(Bt),q(de,e),e&&t(Wt),e&&t(F),e&&t(Yt),q(ge,e),e&&t(Ht),e&&t(x),e&&t(Zt),q($e,e),e&&t(Jt),e&&t(B),q(ke),e&&t(Kt),e&&t(R),e&&t(Rt),q(we,e),e&&t(Xt),q(X,e),e&&t(Gt),e&&t(We),e&&t(Qt),e&&t(O),e&&t(Vt),q(je,e),e&&t(es),e&&t(W),q(ye),e&&t(ts),e&&t(Je),e&&t(ss),q(Q,e),e&&t(as),e&&t(P),e&&t(ns),q(Se,e),e&&t(os),e&&t(Ke),e&&t(ls),q(Te,e),e&&t(rs),e&&t(V),e&&t(is),q(xe,e),e&&t(ps),e&&t(ee),e&&t(fs),q(Ae,e),e&&t(hs),e&&t(te),e&&t(ms),q(Pe,e),e&&t(us),q(se,e)}}}const oo={local:"translation",sections:[{local:"load-opus-books-dataset",title:"Load OPUS Books dataset"},{local:"preprocess",title:"Preprocess"},{local:"finetune-with-trainer",title:"Fine-tune with Trainer"},{local:"finetune-with-tensorflow",title:"Fine-tune with TensorFlow"}],title:"Translation"};function lo(C,m,j){let{fw:c}=m;return C.$$set=_=>{"fw"in _&&j(0,c=_.fw)},[c]}class co extends Rn{constructor(m){super();Xn(this,m,lo,no,Gn,{fw:0})}}export{co as default,oo as metadata};
