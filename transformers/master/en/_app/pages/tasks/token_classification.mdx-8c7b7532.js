import{S as To,i as zo,s as qo,e as o,k as h,w as $,t as a,M as Co,c as r,d as e,m,a as i,x as w,h as n,b as u,F as t,g as f,y as j,q as k,o as b,B as v,L as Eo}from"../../chunks/vendor-6b77c823.js";import{T as ca}from"../../chunks/Tip-39098574.js";import{Y as xo}from"../../chunks/Youtube-5c6e11e6.js";import{I as se}from"../../chunks/IconCopyLink-7a11ce68.js";import{C as q}from"../../chunks/CodeBlock-3a8b25a8.js";import{F as Ao,M as yo}from"../../chunks/Markdown-4489c441.js";function Do(z){let c,d,p,_,x;return{c(){c=o("p"),d=a("See the token classification "),p=o("a"),_=a("task page"),x=a(" for more information about other forms of token classification and their associated models, datasets, and metrics."),this.h()},l(g){c=r(g,"P",{});var E=i(c);d=n(E,"See the token classification "),p=r(E,"A",{href:!0,rel:!0});var T=i(p);_=n(T,"task page"),T.forEach(e),x=n(E," for more information about other forms of token classification and their associated models, datasets, and metrics."),E.forEach(e),this.h()},h(){u(p,"href","https://huggingface.co/tasks/token-classification"),u(p,"rel","nofollow")},m(g,E){f(g,c,E),t(c,d),t(c,p),t(p,_),t(c,x)},d(g){g&&e(c)}}}function Fo(z){let c,d;return c=new q({props:{code:`from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)`}}),{c(){$(c.$$.fragment)},l(p){w(c.$$.fragment,p)},m(p,_){j(c,p,_),d=!0},p:Eo,i(p){d||(k(c.$$.fragment,p),d=!0)},o(p){b(c.$$.fragment,p),d=!1},d(p){v(c,p)}}}function Po(z){let c,d;return c=new yo({props:{$$slots:{default:[Fo]},$$scope:{ctx:z}}}),{c(){$(c.$$.fragment)},l(p){w(c.$$.fragment,p)},m(p,_){j(c,p,_),d=!0},p(p,_){const x={};_&2&&(x.$$scope={dirty:_,ctx:p}),c.$set(x)},i(p){d||(k(c.$$.fragment,p),d=!0)},o(p){b(c.$$.fragment,p),d=!1},d(p){v(c,p)}}}function So(z){let c,d;return c=new q({props:{code:`from transformers import DataCollatorForTokenClassification

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){$(c.$$.fragment)},l(p){w(c.$$.fragment,p)},m(p,_){j(c,p,_),d=!0},p:Eo,i(p){d||(k(c.$$.fragment,p),d=!0)},o(p){b(c.$$.fragment,p),d=!1},d(p){v(c,p)}}}function Oo(z){let c,d;return c=new yo({props:{$$slots:{default:[So]},$$scope:{ctx:z}}}),{c(){$(c.$$.fragment)},l(p){w(c.$$.fragment,p)},m(p,_){j(c,p,_),d=!0},p(p,_){const x={};_&2&&(x.$$scope={dirty:_,ctx:p}),c.$set(x)},i(p){d||(k(c.$$.fragment,p),d=!0)},o(p){b(c.$$.fragment,p),d=!1},d(p){v(c,p)}}}function Lo(z){let c,d,p,_,x,g,E,T;return{c(){c=o("p"),d=a("If you aren\u2019t familiar with fine-tuning a model with the "),p=o("a"),_=a("Trainer"),x=a(", take a look at the basic tutorial "),g=o("a"),E=a("here"),T=a("!"),this.h()},l(C){c=r(C,"P",{});var y=i(c);d=n(y,"If you aren\u2019t familiar with fine-tuning a model with the "),p=r(y,"A",{href:!0});var D=i(p);_=n(D,"Trainer"),D.forEach(e),x=n(y,", take a look at the basic tutorial "),g=r(y,"A",{href:!0});var O=i(g);E=n(O,"here"),O.forEach(e),T=n(y,"!"),y.forEach(e),this.h()},h(){u(p,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),u(g,"href","../training#finetune-with-trainer")},m(C,y){f(C,c,y),t(c,d),t(c,p),t(p,_),t(c,x),t(c,g),t(g,E),t(c,T)},d(C){C&&e(c)}}}function No(z){let c,d,p,_,x;return{c(){c=o("p"),d=a("If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),p=o("a"),_=a("here"),x=a("!"),this.h()},l(g){c=r(g,"P",{});var E=i(c);d=n(E,"If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),p=r(E,"A",{href:!0});var T=i(p);_=n(T,"here"),T.forEach(e),x=n(E,"!"),E.forEach(e),this.h()},h(){u(p,"href","../training#finetune-with-keras")},m(g,E){f(g,c,E),t(c,d),t(c,p),t(p,_),t(c,x)},d(g){g&&e(c)}}}function Io(z){let c,d,p,_,x,g,E,T;return{c(){c=o("p"),d=a(`For a more in-depth example of how to fine-tune a model for token classification, take a look at the corresponding
`),p=o("a"),_=a("PyTorch notebook"),x=a(`
or `),g=o("a"),E=a("TensorFlow notebook"),T=a("."),this.h()},l(C){c=r(C,"P",{});var y=i(c);d=n(y,`For a more in-depth example of how to fine-tune a model for token classification, take a look at the corresponding
`),p=r(y,"A",{href:!0,rel:!0});var D=i(p);_=n(D,"PyTorch notebook"),D.forEach(e),x=n(y,`
or `),g=r(y,"A",{href:!0,rel:!0});var O=i(g);E=n(O,"TensorFlow notebook"),O.forEach(e),T=n(y,"."),y.forEach(e),this.h()},h(){u(p,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification.ipynb"),u(p,"rel","nofollow"),u(g,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification-tf.ipynb"),u(g,"rel","nofollow")},m(C,y){f(C,c,y),t(c,d),t(c,p),t(p,_),t(c,x),t(c,g),t(g,E),t(c,T)},d(C){C&&e(c)}}}function Bo(z){let c,d,p,_,x,g,E,T,C,y,D,O,Ks,fa,te,L,ha,us,ma,ua,ds,da,_a,ee,Z,ae,H,J,ut,_s,ga,dt,$a,ne,Vs,wa,le,gs,oe,Zs,ja,re,$s,ie,G,ka,_t,ba,va,pe,ws,ce,N,xa,gt,Ea,ya,$t,Ta,za,fe,I,Js,wt,qa,Ca,Aa,B,jt,Da,Fa,kt,Pa,Sa,bt,Oa,La,Na,Gs,vt,Ia,Ba,he,Y,Q,xt,js,Ma,Et,Ra,me,ks,ue,X,Ua,yt,Wa,Ha,de,bs,_e,ss,Ya,Tt,Ka,Va,ge,vs,$e,M,Za,zt,Ja,Ga,qt,Qa,Xa,we,R,xs,sn,Es,Ct,tn,en,an,S,nn,At,ln,on,Dt,rn,pn,Ft,cn,fn,hn,ys,mn,Pt,un,dn,je,Qs,_n,ke,Ts,be,F,gn,zs,St,$n,wn,Ot,jn,kn,Lt,bn,vn,ve,qs,xe,A,xn,Xs,En,yn,Nt,Tn,zn,It,qn,Cn,Bt,An,Dn,Ee,ts,ye,K,es,Mt,Cs,Fn,Rt,Pn,Te,as,Sn,st,On,Ln,ze,As,qe,ns,Ce,tt,Nn,Ae,U,Ds,In,et,Bn,Mn,Rn,Fs,Un,at,Wn,Hn,Yn,Ps,Kn,nt,Vn,Zn,De,Ss,Fe,V,ls,Ut,Os,Jn,Wt,Gn,Pe,lt,Qn,Se,os,Oe,P,Xn,Ht,sl,tl,Ls,Yt,el,al,Kt,nl,ll,Le,Ns,Ne,ot,ol,Ie,Is,Be,rs,rl,rt,il,pl,Me,Bs,Re,is,cl,Ms,Vt,fl,hl,Ue,Rs,We,ps,ml,Us,Zt,ul,dl,He,Ws,Ye,cs,Ke;return g=new se({}),D=new xo({props:{id:"wVHdVlPScxA"}}),Z=new ca({props:{$$slots:{default:[Do]},$$scope:{ctx:z}}}),_s=new se({}),gs=new q({props:{code:`from datasets import load_dataset

wnut = load_dataset("wnut_17")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>wnut = load_dataset(<span class="hljs-string">&quot;wnut_17&quot;</span>)`}}),$s=new q({props:{code:'wnut["train"][0]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>wnut[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-string">&#x27;0&#x27;</span>,
 <span class="hljs-string">&#x27;ner_tags&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>, <span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;tokens&#x27;</span>: [<span class="hljs-string">&#x27;@paulwalk&#x27;</span>, <span class="hljs-string">&#x27;It&#x27;</span>, <span class="hljs-string">&quot;&#x27;s&quot;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;view&#x27;</span>, <span class="hljs-string">&#x27;from&#x27;</span>, <span class="hljs-string">&#x27;where&#x27;</span>, <span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&quot;&#x27;m&quot;</span>, <span class="hljs-string">&#x27;living&#x27;</span>, <span class="hljs-string">&#x27;for&#x27;</span>, <span class="hljs-string">&#x27;two&#x27;</span>, <span class="hljs-string">&#x27;weeks&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;Empire&#x27;</span>, <span class="hljs-string">&#x27;State&#x27;</span>, <span class="hljs-string">&#x27;Building&#x27;</span>, <span class="hljs-string">&#x27;=&#x27;</span>, <span class="hljs-string">&#x27;ESB&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;Pretty&#x27;</span>, <span class="hljs-string">&#x27;bad&#x27;</span>, <span class="hljs-string">&#x27;storm&#x27;</span>, <span class="hljs-string">&#x27;here&#x27;</span>, <span class="hljs-string">&#x27;last&#x27;</span>, <span class="hljs-string">&#x27;evening&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]
}`}}),ws=new q({props:{code:`label_list = wnut["train"].features[f"ner_tags"].feature.names
label_list`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>label_list = wnut[<span class="hljs-string">&quot;train&quot;</span>].features[<span class="hljs-string">f&quot;ner_tags&quot;</span>].feature.names
<span class="hljs-meta">&gt;&gt;&gt; </span>label_list
[
    <span class="hljs-string">&quot;O&quot;</span>,
    <span class="hljs-string">&quot;B-corporation&quot;</span>,
    <span class="hljs-string">&quot;I-corporation&quot;</span>,
    <span class="hljs-string">&quot;B-creative-work&quot;</span>,
    <span class="hljs-string">&quot;I-creative-work&quot;</span>,
    <span class="hljs-string">&quot;B-group&quot;</span>,
    <span class="hljs-string">&quot;I-group&quot;</span>,
    <span class="hljs-string">&quot;B-location&quot;</span>,
    <span class="hljs-string">&quot;I-location&quot;</span>,
    <span class="hljs-string">&quot;B-person&quot;</span>,
    <span class="hljs-string">&quot;I-person&quot;</span>,
    <span class="hljs-string">&quot;B-product&quot;</span>,
    <span class="hljs-string">&quot;I-product&quot;</span>,
]`}}),js=new se({}),ks=new xo({props:{id:"iY2AZYdZAr0"}}),bs=new q({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),vs=new q({props:{code:`tokenized_input = tokenizer(example["tokens"], is_split_into_words=True)
tokens = tokenizer.convert_ids_to_tokens(tokenized_input["input_ids"])
tokens`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_input = tokenizer(example[<span class="hljs-string">&quot;tokens&quot;</span>], is_split_into_words=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokens = tokenizer.convert_ids_to_tokens(tokenized_input[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>tokens
[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;@&#x27;</span>, <span class="hljs-string">&#x27;paul&#x27;</span>, <span class="hljs-string">&#x27;##walk&#x27;</span>, <span class="hljs-string">&#x27;it&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;view&#x27;</span>, <span class="hljs-string">&#x27;from&#x27;</span>, <span class="hljs-string">&#x27;where&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;m&#x27;</span>, <span class="hljs-string">&#x27;living&#x27;</span>, <span class="hljs-string">&#x27;for&#x27;</span>, <span class="hljs-string">&#x27;two&#x27;</span>, <span class="hljs-string">&#x27;weeks&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;empire&#x27;</span>, <span class="hljs-string">&#x27;state&#x27;</span>, <span class="hljs-string">&#x27;building&#x27;</span>, <span class="hljs-string">&#x27;=&#x27;</span>, <span class="hljs-string">&#x27;es&#x27;</span>, <span class="hljs-string">&#x27;##b&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;pretty&#x27;</span>, <span class="hljs-string">&#x27;bad&#x27;</span>, <span class="hljs-string">&#x27;storm&#x27;</span>, <span class="hljs-string">&#x27;here&#x27;</span>, <span class="hljs-string">&#x27;last&#x27;</span>, <span class="hljs-string">&#x27;evening&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]`}}),Ts=new q({props:{code:`def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

    labels = []
    for i, label in enumerate(examples[f"ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:  # Set the special tokens to -100.
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:  # Only label the first token of a given word.
                label_ids.append(label[word_idx])
            else:
                label_ids.append(-100)
            previous_word_idx = word_idx
        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_and_align_labels</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    tokenized_inputs = tokenizer(examples[<span class="hljs-string">&quot;tokens&quot;</span>], truncation=<span class="hljs-literal">True</span>, is_split_into_words=<span class="hljs-literal">True</span>)

<span class="hljs-meta">... </span>    labels = []
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(examples[<span class="hljs-string">f&quot;ner_tags&quot;</span>]):
<span class="hljs-meta">... </span>        word_ids = tokenized_inputs.word_ids(batch_index=i)  <span class="hljs-comment"># Map tokens to their respective word.</span>
<span class="hljs-meta">... </span>        previous_word_idx = <span class="hljs-literal">None</span>
<span class="hljs-meta">... </span>        label_ids = []
<span class="hljs-meta">... </span>        <span class="hljs-keyword">for</span> word_idx <span class="hljs-keyword">in</span> word_ids:  <span class="hljs-comment"># Set the special tokens to -100.</span>
<span class="hljs-meta">... </span>            <span class="hljs-keyword">if</span> word_idx <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
<span class="hljs-meta">... </span>                label_ids.append(-<span class="hljs-number">100</span>)
<span class="hljs-meta">... </span>            <span class="hljs-keyword">elif</span> word_idx != previous_word_idx:  <span class="hljs-comment"># Only label the first token of a given word.</span>
<span class="hljs-meta">... </span>                label_ids.append(label[word_idx])
<span class="hljs-meta">... </span>            <span class="hljs-keyword">else</span>:
<span class="hljs-meta">... </span>                label_ids.append(-<span class="hljs-number">100</span>)
<span class="hljs-meta">... </span>            previous_word_idx = word_idx
<span class="hljs-meta">... </span>        labels.append(label_ids)

<span class="hljs-meta">... </span>    tokenized_inputs[<span class="hljs-string">&quot;labels&quot;</span>] = labels
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenized_inputs`}}),qs=new q({props:{code:"tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_wnut = wnut.<span class="hljs-built_in">map</span>(tokenize_and_align_labels, batched=<span class="hljs-literal">True</span>)'}}),ts=new Ao({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Oo],pytorch:[Po]},$$scope:{ctx:z}}}),Cs=new se({}),As=new q({props:{code:`from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer

model = AutoModelForTokenClassification.from_pretrained("distilbert-base-uncased", num_labels=14)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForTokenClassification, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">14</span>)`}}),ns=new ca({props:{$$slots:{default:[Lo]},$$scope:{ctx:z}}}),Ss=new q({props:{code:`training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_wnut["train"],
    eval_dataset=tokenized_wnut["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=tokenized_wnut[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=tokenized_wnut[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),Os=new se({}),os=new ca({props:{$$slots:{default:[No]},$$scope:{ctx:z}}}),Ns=new q({props:{code:`tf_train_set = tokenized_wnut["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_set = tokenized_wnut["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = tokenized_wnut[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_validation_set = tokenized_wnut[<span class="hljs-string">&quot;validation&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`}}),Is=new q({props:{code:`from transformers import create_optimizer

batch_size = 16
num_train_epochs = 3
num_train_steps = (len(tokenized_wnut["train"]) // batch_size) * num_train_epochs
optimizer, lr_schedule = create_optimizer(
    init_lr=2e-5,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
    num_warmup_steps=0,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer

<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = <span class="hljs-number">16</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_train_epochs = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_train_steps = (<span class="hljs-built_in">len</span>(tokenized_wnut[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size) * num_train_epochs
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer, lr_schedule = create_optimizer(
<span class="hljs-meta">... </span>    init_lr=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    num_train_steps=num_train_steps,
<span class="hljs-meta">... </span>    weight_decay_rate=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>    num_warmup_steps=<span class="hljs-number">0</span>,
<span class="hljs-meta">... </span>)`}}),Bs=new q({props:{code:`from transformers import TFAutoModelForTokenClassification

model = TFAutoModelForTokenClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),Rs=new q({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),Ws=new q({props:{code:"model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=<span class="hljs-number">3</span>)'}}),cs=new ca({props:{$$slots:{default:[Io]},$$scope:{ctx:z}}}),{c(){c=o("meta"),d=h(),p=o("h1"),_=o("a"),x=o("span"),$(g.$$.fragment),E=h(),T=o("span"),C=a("Token classification"),y=h(),$(D.$$.fragment),O=h(),Ks=o("p"),fa=a("Token classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization."),te=h(),L=o("p"),ha=a("This guide will show you how to fine-tune "),us=o("a"),ma=a("DistilBERT"),ua=a(" on the "),ds=o("a"),da=a("WNUT 17"),_a=a(" dataset to detect new entities."),ee=h(),$(Z.$$.fragment),ae=h(),H=o("h2"),J=o("a"),ut=o("span"),$(_s.$$.fragment),ga=h(),dt=o("span"),$a=a("Load WNUT 17 dataset"),ne=h(),Vs=o("p"),wa=a("Load the WNUT 17 dataset from the \u{1F917} Datasets library:"),le=h(),$(gs.$$.fragment),oe=h(),Zs=o("p"),ja=a("Then take a look at an example:"),re=h(),$($s.$$.fragment),ie=h(),G=o("p"),ka=a("Each number in "),_t=o("code"),ba=a("ner_tags"),va=a(" represents an entity. Convert the number to a label name for more information:"),pe=h(),$(ws.$$.fragment),ce=h(),N=o("p"),xa=a("The "),gt=o("code"),Ea=a("ner_tag"),ya=a(" describes an entity, such as a corporation, location, or person. The letter that prefixes each "),$t=o("code"),Ta=a("ner_tag"),za=a(" indicates the token position of the entity:"),fe=h(),I=o("ul"),Js=o("li"),wt=o("code"),qa=a("B-"),Ca=a(" indicates the beginning of an entity."),Aa=h(),B=o("li"),jt=o("code"),Da=a("I-"),Fa=a(" indicates a token is contained inside the same entity (e.g., the "),kt=o("code"),Pa=a("State"),Sa=a(` token is a part of an entity like
`),bt=o("code"),Oa=a("Empire State Building"),La=a(")."),Na=h(),Gs=o("li"),vt=o("code"),Ia=a("0"),Ba=a(" indicates the token doesn\u2019t correspond to any entity."),he=h(),Y=o("h2"),Q=o("a"),xt=o("span"),$(js.$$.fragment),Ma=h(),Et=o("span"),Ra=a("Preprocess"),me=h(),$(ks.$$.fragment),ue=h(),X=o("p"),Ua=a("Load the DistilBERT tokenizer to process the "),yt=o("code"),Wa=a("tokens"),Ha=a(":"),de=h(),$(bs.$$.fragment),_e=h(),ss=o("p"),Ya=a("Since the input has already been split into words, set "),Tt=o("code"),Ka=a("is_split_into_words=True"),Va=a(" to tokenize the words into subwords:"),ge=h(),$(vs.$$.fragment),$e=h(),M=o("p"),Za=a("Adding the special tokens "),zt=o("code"),Ja=a("[CLS]"),Ga=a(" and "),qt=o("code"),Qa=a("[SEP]"),Xa=a(" and subword tokenization creates a mismatch between the input and labels. A single word corresponding to a single label may be split into two subwords. You will need to realign the tokens and labels by:"),we=h(),R=o("ol"),xs=o("li"),sn=a("Mapping all tokens to their corresponding word with the "),Es=o("a"),Ct=o("code"),tn=a("word_ids"),en=a(" method."),an=h(),S=o("li"),nn=a("Assigning the label "),At=o("code"),ln=a("-100"),on=a(" to the special tokens "),Dt=o("code"),rn=a("[CLS]"),pn=a(" and "),Ft=o("code"),cn=a("[SEP]"),fn=a(` so the PyTorch loss function ignores
them.`),hn=h(),ys=o("li"),mn=a("Only labeling the first token of a given word. Assign "),Pt=o("code"),un=a("-100"),dn=a(" to other subtokens from the same word."),je=h(),Qs=o("p"),_n=a("Here is how you can create a function to realign the tokens and labels, and truncate sequences to be no longer than DistilBERT\u2019s maximum input length::"),ke=h(),$(Ts.$$.fragment),be=h(),F=o("p"),gn=a("Use \u{1F917} Datasets "),zs=o("a"),St=o("code"),$n=a("map"),wn=a(" function to tokenize and align the labels over the entire dataset. You can speed up the "),Ot=o("code"),jn=a("map"),kn=a(" function by setting "),Lt=o("code"),bn=a("batched=True"),vn=a(" to process multiple elements of the dataset at once:"),ve=h(),$(qs.$$.fragment),xe=h(),A=o("p"),xn=a("Use "),Xs=o("a"),En=a("DataCollatorForTokenClassification"),yn=a(" to create a batch of examples. It will also "),Nt=o("em"),Tn=a("dynamically pad"),zn=a(" your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),It=o("code"),qn=a("tokenizer"),Cn=a(" function by setting "),Bt=o("code"),An=a("padding=True"),Dn=a(", dynamic padding is more efficient."),Ee=h(),$(ts.$$.fragment),ye=h(),K=o("h2"),es=o("a"),Mt=o("span"),$(Cs.$$.fragment),Fn=h(),Rt=o("span"),Pn=a("Fine-tune with Trainer"),Te=h(),as=o("p"),Sn=a("Load DistilBERT with "),st=o("a"),On=a("AutoModelForTokenClassification"),Ln=a(" along with the number of expected labels:"),ze=h(),$(As.$$.fragment),qe=h(),$(ns.$$.fragment),Ce=h(),tt=o("p"),Nn=a("At this point, only three steps remain:"),Ae=h(),U=o("ol"),Ds=o("li"),In=a("Define your training hyperparameters in "),et=o("a"),Bn=a("TrainingArguments"),Mn=a("."),Rn=h(),Fs=o("li"),Un=a("Pass the training arguments to "),at=o("a"),Wn=a("Trainer"),Hn=a(" along with the model, dataset, tokenizer, and data collator."),Yn=h(),Ps=o("li"),Kn=a("Call "),nt=o("a"),Vn=a("train()"),Zn=a(" to fine-tune your model."),De=h(),$(Ss.$$.fragment),Fe=h(),V=o("h2"),ls=o("a"),Ut=o("span"),$(Os.$$.fragment),Jn=h(),Wt=o("span"),Gn=a("Fine-tune with TensorFlow"),Pe=h(),lt=o("p"),Qn=a("To fine-tune a model in TensorFlow is just as easy, with only a few differences."),Se=h(),$(os.$$.fragment),Oe=h(),P=o("p"),Xn=a("Convert your datasets to the "),Ht=o("code"),sl=a("tf.data.Dataset"),tl=a(" format with "),Ls=o("a"),Yt=o("code"),el=a("to_tf_dataset"),al=a(". Specify inputs and labels in "),Kt=o("code"),nl=a("columns"),ll=a(", whether to shuffle the dataset order, batch size, and the data collator:"),Le=h(),$(Ns.$$.fragment),Ne=h(),ot=o("p"),ol=a("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Ie=h(),$(Is.$$.fragment),Be=h(),rs=o("p"),rl=a("Load DistilBERT with "),rt=o("a"),il=a("TFAutoModelForTokenClassification"),pl=a(" along with the number of expected labels:"),Me=h(),$(Bs.$$.fragment),Re=h(),is=o("p"),cl=a("Configure the model for training with "),Ms=o("a"),Vt=o("code"),fl=a("compile"),hl=a(":"),Ue=h(),$(Rs.$$.fragment),We=h(),ps=o("p"),ml=a("Call "),Us=o("a"),Zt=o("code"),ul=a("fit"),dl=a(" to fine-tune the model:"),He=h(),$(Ws.$$.fragment),Ye=h(),$(cs.$$.fragment),this.h()},l(s){const l=Co('[data-svelte="svelte-1phssyn"]',document.head);c=r(l,"META",{name:!0,content:!0}),l.forEach(e),d=m(s),p=r(s,"H1",{class:!0});var Hs=i(p);_=r(Hs,"A",{id:!0,class:!0,href:!0});var Jt=i(_);x=r(Jt,"SPAN",{});var Gt=i(x);w(g.$$.fragment,Gt),Gt.forEach(e),Jt.forEach(e),E=m(Hs),T=r(Hs,"SPAN",{});var Qt=i(T);C=n(Qt,"Token classification"),Qt.forEach(e),Hs.forEach(e),y=m(s),w(D.$$.fragment,s),O=m(s),Ks=r(s,"P",{});var Xt=i(Ks);fa=n(Xt,"Token classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization."),Xt.forEach(e),te=m(s),L=r(s,"P",{});var it=i(L);ha=n(it,"This guide will show you how to fine-tune "),us=r(it,"A",{href:!0,rel:!0});var $l=i(us);ma=n($l,"DistilBERT"),$l.forEach(e),ua=n(it," on the "),ds=r(it,"A",{href:!0,rel:!0});var wl=i(ds);da=n(wl,"WNUT 17"),wl.forEach(e),_a=n(it," dataset to detect new entities."),it.forEach(e),ee=m(s),w(Z.$$.fragment,s),ae=m(s),H=r(s,"H2",{class:!0});var Ve=i(H);J=r(Ve,"A",{id:!0,class:!0,href:!0});var jl=i(J);ut=r(jl,"SPAN",{});var kl=i(ut);w(_s.$$.fragment,kl),kl.forEach(e),jl.forEach(e),ga=m(Ve),dt=r(Ve,"SPAN",{});var bl=i(dt);$a=n(bl,"Load WNUT 17 dataset"),bl.forEach(e),Ve.forEach(e),ne=m(s),Vs=r(s,"P",{});var vl=i(Vs);wa=n(vl,"Load the WNUT 17 dataset from the \u{1F917} Datasets library:"),vl.forEach(e),le=m(s),w(gs.$$.fragment,s),oe=m(s),Zs=r(s,"P",{});var xl=i(Zs);ja=n(xl,"Then take a look at an example:"),xl.forEach(e),re=m(s),w($s.$$.fragment,s),ie=m(s),G=r(s,"P",{});var Ze=i(G);ka=n(Ze,"Each number in "),_t=r(Ze,"CODE",{});var El=i(_t);ba=n(El,"ner_tags"),El.forEach(e),va=n(Ze," represents an entity. Convert the number to a label name for more information:"),Ze.forEach(e),pe=m(s),w(ws.$$.fragment,s),ce=m(s),N=r(s,"P",{});var pt=i(N);xa=n(pt,"The "),gt=r(pt,"CODE",{});var yl=i(gt);Ea=n(yl,"ner_tag"),yl.forEach(e),ya=n(pt," describes an entity, such as a corporation, location, or person. The letter that prefixes each "),$t=r(pt,"CODE",{});var Tl=i($t);Ta=n(Tl,"ner_tag"),Tl.forEach(e),za=n(pt," indicates the token position of the entity:"),pt.forEach(e),fe=m(s),I=r(s,"UL",{});var ct=i(I);Js=r(ct,"LI",{});var _l=i(Js);wt=r(_l,"CODE",{});var zl=i(wt);qa=n(zl,"B-"),zl.forEach(e),Ca=n(_l," indicates the beginning of an entity."),_l.forEach(e),Aa=m(ct),B=r(ct,"LI",{});var Ys=i(B);jt=r(Ys,"CODE",{});var ql=i(jt);Da=n(ql,"I-"),ql.forEach(e),Fa=n(Ys," indicates a token is contained inside the same entity (e.g., the "),kt=r(Ys,"CODE",{});var Cl=i(kt);Pa=n(Cl,"State"),Cl.forEach(e),Sa=n(Ys,` token is a part of an entity like
`),bt=r(Ys,"CODE",{});var Al=i(bt);Oa=n(Al,"Empire State Building"),Al.forEach(e),La=n(Ys,")."),Ys.forEach(e),Na=m(ct),Gs=r(ct,"LI",{});var gl=i(Gs);vt=r(gl,"CODE",{});var Dl=i(vt);Ia=n(Dl,"0"),Dl.forEach(e),Ba=n(gl," indicates the token doesn\u2019t correspond to any entity."),gl.forEach(e),ct.forEach(e),he=m(s),Y=r(s,"H2",{class:!0});var Je=i(Y);Q=r(Je,"A",{id:!0,class:!0,href:!0});var Fl=i(Q);xt=r(Fl,"SPAN",{});var Pl=i(xt);w(js.$$.fragment,Pl),Pl.forEach(e),Fl.forEach(e),Ma=m(Je),Et=r(Je,"SPAN",{});var Sl=i(Et);Ra=n(Sl,"Preprocess"),Sl.forEach(e),Je.forEach(e),me=m(s),w(ks.$$.fragment,s),ue=m(s),X=r(s,"P",{});var Ge=i(X);Ua=n(Ge,"Load the DistilBERT tokenizer to process the "),yt=r(Ge,"CODE",{});var Ol=i(yt);Wa=n(Ol,"tokens"),Ol.forEach(e),Ha=n(Ge,":"),Ge.forEach(e),de=m(s),w(bs.$$.fragment,s),_e=m(s),ss=r(s,"P",{});var Qe=i(ss);Ya=n(Qe,"Since the input has already been split into words, set "),Tt=r(Qe,"CODE",{});var Ll=i(Tt);Ka=n(Ll,"is_split_into_words=True"),Ll.forEach(e),Va=n(Qe," to tokenize the words into subwords:"),Qe.forEach(e),ge=m(s),w(vs.$$.fragment,s),$e=m(s),M=r(s,"P",{});var ft=i(M);Za=n(ft,"Adding the special tokens "),zt=r(ft,"CODE",{});var Nl=i(zt);Ja=n(Nl,"[CLS]"),Nl.forEach(e),Ga=n(ft," and "),qt=r(ft,"CODE",{});var Il=i(qt);Qa=n(Il,"[SEP]"),Il.forEach(e),Xa=n(ft," and subword tokenization creates a mismatch between the input and labels. A single word corresponding to a single label may be split into two subwords. You will need to realign the tokens and labels by:"),ft.forEach(e),we=m(s),R=r(s,"OL",{});var ht=i(R);xs=r(ht,"LI",{});var Xe=i(xs);sn=n(Xe,"Mapping all tokens to their corresponding word with the "),Es=r(Xe,"A",{href:!0,rel:!0});var Bl=i(Es);Ct=r(Bl,"CODE",{});var Ml=i(Ct);tn=n(Ml,"word_ids"),Ml.forEach(e),Bl.forEach(e),en=n(Xe," method."),Xe.forEach(e),an=m(ht),S=r(ht,"LI",{});var fs=i(S);nn=n(fs,"Assigning the label "),At=r(fs,"CODE",{});var Rl=i(At);ln=n(Rl,"-100"),Rl.forEach(e),on=n(fs," to the special tokens "),Dt=r(fs,"CODE",{});var Ul=i(Dt);rn=n(Ul,"[CLS]"),Ul.forEach(e),pn=n(fs," and "),Ft=r(fs,"CODE",{});var Wl=i(Ft);cn=n(Wl,"[SEP]"),Wl.forEach(e),fn=n(fs,` so the PyTorch loss function ignores
them.`),fs.forEach(e),hn=m(ht),ys=r(ht,"LI",{});var sa=i(ys);mn=n(sa,"Only labeling the first token of a given word. Assign "),Pt=r(sa,"CODE",{});var Hl=i(Pt);un=n(Hl,"-100"),Hl.forEach(e),dn=n(sa," to other subtokens from the same word."),sa.forEach(e),ht.forEach(e),je=m(s),Qs=r(s,"P",{});var Yl=i(Qs);_n=n(Yl,"Here is how you can create a function to realign the tokens and labels, and truncate sequences to be no longer than DistilBERT\u2019s maximum input length::"),Yl.forEach(e),ke=m(s),w(Ts.$$.fragment,s),be=m(s),F=r(s,"P",{});var hs=i(F);gn=n(hs,"Use \u{1F917} Datasets "),zs=r(hs,"A",{href:!0,rel:!0});var Kl=i(zs);St=r(Kl,"CODE",{});var Vl=i(St);$n=n(Vl,"map"),Vl.forEach(e),Kl.forEach(e),wn=n(hs," function to tokenize and align the labels over the entire dataset. You can speed up the "),Ot=r(hs,"CODE",{});var Zl=i(Ot);jn=n(Zl,"map"),Zl.forEach(e),kn=n(hs," function by setting "),Lt=r(hs,"CODE",{});var Jl=i(Lt);bn=n(Jl,"batched=True"),Jl.forEach(e),vn=n(hs," to process multiple elements of the dataset at once:"),hs.forEach(e),ve=m(s),w(qs.$$.fragment,s),xe=m(s),A=r(s,"P",{});var W=i(A);xn=n(W,"Use "),Xs=r(W,"A",{href:!0});var Gl=i(Xs);En=n(Gl,"DataCollatorForTokenClassification"),Gl.forEach(e),yn=n(W," to create a batch of examples. It will also "),Nt=r(W,"EM",{});var Ql=i(Nt);Tn=n(Ql,"dynamically pad"),Ql.forEach(e),zn=n(W," your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),It=r(W,"CODE",{});var Xl=i(It);qn=n(Xl,"tokenizer"),Xl.forEach(e),Cn=n(W," function by setting "),Bt=r(W,"CODE",{});var so=i(Bt);An=n(so,"padding=True"),so.forEach(e),Dn=n(W,", dynamic padding is more efficient."),W.forEach(e),Ee=m(s),w(ts.$$.fragment,s),ye=m(s),K=r(s,"H2",{class:!0});var ta=i(K);es=r(ta,"A",{id:!0,class:!0,href:!0});var to=i(es);Mt=r(to,"SPAN",{});var eo=i(Mt);w(Cs.$$.fragment,eo),eo.forEach(e),to.forEach(e),Fn=m(ta),Rt=r(ta,"SPAN",{});var ao=i(Rt);Pn=n(ao,"Fine-tune with Trainer"),ao.forEach(e),ta.forEach(e),Te=m(s),as=r(s,"P",{});var ea=i(as);Sn=n(ea,"Load DistilBERT with "),st=r(ea,"A",{href:!0});var no=i(st);On=n(no,"AutoModelForTokenClassification"),no.forEach(e),Ln=n(ea," along with the number of expected labels:"),ea.forEach(e),ze=m(s),w(As.$$.fragment,s),qe=m(s),w(ns.$$.fragment,s),Ce=m(s),tt=r(s,"P",{});var lo=i(tt);Nn=n(lo,"At this point, only three steps remain:"),lo.forEach(e),Ae=m(s),U=r(s,"OL",{});var mt=i(U);Ds=r(mt,"LI",{});var aa=i(Ds);In=n(aa,"Define your training hyperparameters in "),et=r(aa,"A",{href:!0});var oo=i(et);Bn=n(oo,"TrainingArguments"),oo.forEach(e),Mn=n(aa,"."),aa.forEach(e),Rn=m(mt),Fs=r(mt,"LI",{});var na=i(Fs);Un=n(na,"Pass the training arguments to "),at=r(na,"A",{href:!0});var ro=i(at);Wn=n(ro,"Trainer"),ro.forEach(e),Hn=n(na," along with the model, dataset, tokenizer, and data collator."),na.forEach(e),Yn=m(mt),Ps=r(mt,"LI",{});var la=i(Ps);Kn=n(la,"Call "),nt=r(la,"A",{href:!0});var io=i(nt);Vn=n(io,"train()"),io.forEach(e),Zn=n(la," to fine-tune your model."),la.forEach(e),mt.forEach(e),De=m(s),w(Ss.$$.fragment,s),Fe=m(s),V=r(s,"H2",{class:!0});var oa=i(V);ls=r(oa,"A",{id:!0,class:!0,href:!0});var po=i(ls);Ut=r(po,"SPAN",{});var co=i(Ut);w(Os.$$.fragment,co),co.forEach(e),po.forEach(e),Jn=m(oa),Wt=r(oa,"SPAN",{});var fo=i(Wt);Gn=n(fo,"Fine-tune with TensorFlow"),fo.forEach(e),oa.forEach(e),Pe=m(s),lt=r(s,"P",{});var ho=i(lt);Qn=n(ho,"To fine-tune a model in TensorFlow is just as easy, with only a few differences."),ho.forEach(e),Se=m(s),w(os.$$.fragment,s),Oe=m(s),P=r(s,"P",{});var ms=i(P);Xn=n(ms,"Convert your datasets to the "),Ht=r(ms,"CODE",{});var mo=i(Ht);sl=n(mo,"tf.data.Dataset"),mo.forEach(e),tl=n(ms," format with "),Ls=r(ms,"A",{href:!0,rel:!0});var uo=i(Ls);Yt=r(uo,"CODE",{});var _o=i(Yt);el=n(_o,"to_tf_dataset"),_o.forEach(e),uo.forEach(e),al=n(ms,". Specify inputs and labels in "),Kt=r(ms,"CODE",{});var go=i(Kt);nl=n(go,"columns"),go.forEach(e),ll=n(ms,", whether to shuffle the dataset order, batch size, and the data collator:"),ms.forEach(e),Le=m(s),w(Ns.$$.fragment,s),Ne=m(s),ot=r(s,"P",{});var $o=i(ot);ol=n($o,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),$o.forEach(e),Ie=m(s),w(Is.$$.fragment,s),Be=m(s),rs=r(s,"P",{});var ra=i(rs);rl=n(ra,"Load DistilBERT with "),rt=r(ra,"A",{href:!0});var wo=i(rt);il=n(wo,"TFAutoModelForTokenClassification"),wo.forEach(e),pl=n(ra," along with the number of expected labels:"),ra.forEach(e),Me=m(s),w(Bs.$$.fragment,s),Re=m(s),is=r(s,"P",{});var ia=i(is);cl=n(ia,"Configure the model for training with "),Ms=r(ia,"A",{href:!0,rel:!0});var jo=i(Ms);Vt=r(jo,"CODE",{});var ko=i(Vt);fl=n(ko,"compile"),ko.forEach(e),jo.forEach(e),hl=n(ia,":"),ia.forEach(e),Ue=m(s),w(Rs.$$.fragment,s),We=m(s),ps=r(s,"P",{});var pa=i(ps);ml=n(pa,"Call "),Us=r(pa,"A",{href:!0,rel:!0});var bo=i(Us);Zt=r(bo,"CODE",{});var vo=i(Zt);ul=n(vo,"fit"),vo.forEach(e),bo.forEach(e),dl=n(pa," to fine-tune the model:"),pa.forEach(e),He=m(s),w(Ws.$$.fragment,s),Ye=m(s),w(cs.$$.fragment,s),this.h()},h(){u(c,"name","hf:doc:metadata"),u(c,"content",JSON.stringify(Mo)),u(_,"id","token-classification"),u(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(_,"href","#token-classification"),u(p,"class","relative group"),u(us,"href","https://huggingface.co/distilbert-base-uncased"),u(us,"rel","nofollow"),u(ds,"href","https://huggingface.co/datasets/wnut_17"),u(ds,"rel","nofollow"),u(J,"id","load-wnut-17-dataset"),u(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(J,"href","#load-wnut-17-dataset"),u(H,"class","relative group"),u(Q,"id","preprocess"),u(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Q,"href","#preprocess"),u(Y,"class","relative group"),u(Es,"href","https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#tokenizers.Encoding.word_ids"),u(Es,"rel","nofollow"),u(zs,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map"),u(zs,"rel","nofollow"),u(Xs,"href","/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorForTokenClassification"),u(es,"id","finetune-with-trainer"),u(es,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(es,"href","#finetune-with-trainer"),u(K,"class","relative group"),u(st,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForTokenClassification"),u(et,"href","/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments"),u(at,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),u(nt,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer.train"),u(ls,"id","finetune-with-tensorflow"),u(ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ls,"href","#finetune-with-tensorflow"),u(V,"class","relative group"),u(Ls,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.to_tf_dataset"),u(Ls,"rel","nofollow"),u(rt,"href","/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForTokenClassification"),u(Ms,"href","https://keras.io/api/models/model_training_apis/#compile-method"),u(Ms,"rel","nofollow"),u(Us,"href","https://keras.io/api/models/model_training_apis/#fit-method"),u(Us,"rel","nofollow")},m(s,l){t(document.head,c),f(s,d,l),f(s,p,l),t(p,_),t(_,x),j(g,x,null),t(p,E),t(p,T),t(T,C),f(s,y,l),j(D,s,l),f(s,O,l),f(s,Ks,l),t(Ks,fa),f(s,te,l),f(s,L,l),t(L,ha),t(L,us),t(us,ma),t(L,ua),t(L,ds),t(ds,da),t(L,_a),f(s,ee,l),j(Z,s,l),f(s,ae,l),f(s,H,l),t(H,J),t(J,ut),j(_s,ut,null),t(H,ga),t(H,dt),t(dt,$a),f(s,ne,l),f(s,Vs,l),t(Vs,wa),f(s,le,l),j(gs,s,l),f(s,oe,l),f(s,Zs,l),t(Zs,ja),f(s,re,l),j($s,s,l),f(s,ie,l),f(s,G,l),t(G,ka),t(G,_t),t(_t,ba),t(G,va),f(s,pe,l),j(ws,s,l),f(s,ce,l),f(s,N,l),t(N,xa),t(N,gt),t(gt,Ea),t(N,ya),t(N,$t),t($t,Ta),t(N,za),f(s,fe,l),f(s,I,l),t(I,Js),t(Js,wt),t(wt,qa),t(Js,Ca),t(I,Aa),t(I,B),t(B,jt),t(jt,Da),t(B,Fa),t(B,kt),t(kt,Pa),t(B,Sa),t(B,bt),t(bt,Oa),t(B,La),t(I,Na),t(I,Gs),t(Gs,vt),t(vt,Ia),t(Gs,Ba),f(s,he,l),f(s,Y,l),t(Y,Q),t(Q,xt),j(js,xt,null),t(Y,Ma),t(Y,Et),t(Et,Ra),f(s,me,l),j(ks,s,l),f(s,ue,l),f(s,X,l),t(X,Ua),t(X,yt),t(yt,Wa),t(X,Ha),f(s,de,l),j(bs,s,l),f(s,_e,l),f(s,ss,l),t(ss,Ya),t(ss,Tt),t(Tt,Ka),t(ss,Va),f(s,ge,l),j(vs,s,l),f(s,$e,l),f(s,M,l),t(M,Za),t(M,zt),t(zt,Ja),t(M,Ga),t(M,qt),t(qt,Qa),t(M,Xa),f(s,we,l),f(s,R,l),t(R,xs),t(xs,sn),t(xs,Es),t(Es,Ct),t(Ct,tn),t(xs,en),t(R,an),t(R,S),t(S,nn),t(S,At),t(At,ln),t(S,on),t(S,Dt),t(Dt,rn),t(S,pn),t(S,Ft),t(Ft,cn),t(S,fn),t(R,hn),t(R,ys),t(ys,mn),t(ys,Pt),t(Pt,un),t(ys,dn),f(s,je,l),f(s,Qs,l),t(Qs,_n),f(s,ke,l),j(Ts,s,l),f(s,be,l),f(s,F,l),t(F,gn),t(F,zs),t(zs,St),t(St,$n),t(F,wn),t(F,Ot),t(Ot,jn),t(F,kn),t(F,Lt),t(Lt,bn),t(F,vn),f(s,ve,l),j(qs,s,l),f(s,xe,l),f(s,A,l),t(A,xn),t(A,Xs),t(Xs,En),t(A,yn),t(A,Nt),t(Nt,Tn),t(A,zn),t(A,It),t(It,qn),t(A,Cn),t(A,Bt),t(Bt,An),t(A,Dn),f(s,Ee,l),j(ts,s,l),f(s,ye,l),f(s,K,l),t(K,es),t(es,Mt),j(Cs,Mt,null),t(K,Fn),t(K,Rt),t(Rt,Pn),f(s,Te,l),f(s,as,l),t(as,Sn),t(as,st),t(st,On),t(as,Ln),f(s,ze,l),j(As,s,l),f(s,qe,l),j(ns,s,l),f(s,Ce,l),f(s,tt,l),t(tt,Nn),f(s,Ae,l),f(s,U,l),t(U,Ds),t(Ds,In),t(Ds,et),t(et,Bn),t(Ds,Mn),t(U,Rn),t(U,Fs),t(Fs,Un),t(Fs,at),t(at,Wn),t(Fs,Hn),t(U,Yn),t(U,Ps),t(Ps,Kn),t(Ps,nt),t(nt,Vn),t(Ps,Zn),f(s,De,l),j(Ss,s,l),f(s,Fe,l),f(s,V,l),t(V,ls),t(ls,Ut),j(Os,Ut,null),t(V,Jn),t(V,Wt),t(Wt,Gn),f(s,Pe,l),f(s,lt,l),t(lt,Qn),f(s,Se,l),j(os,s,l),f(s,Oe,l),f(s,P,l),t(P,Xn),t(P,Ht),t(Ht,sl),t(P,tl),t(P,Ls),t(Ls,Yt),t(Yt,el),t(P,al),t(P,Kt),t(Kt,nl),t(P,ll),f(s,Le,l),j(Ns,s,l),f(s,Ne,l),f(s,ot,l),t(ot,ol),f(s,Ie,l),j(Is,s,l),f(s,Be,l),f(s,rs,l),t(rs,rl),t(rs,rt),t(rt,il),t(rs,pl),f(s,Me,l),j(Bs,s,l),f(s,Re,l),f(s,is,l),t(is,cl),t(is,Ms),t(Ms,Vt),t(Vt,fl),t(is,hl),f(s,Ue,l),j(Rs,s,l),f(s,We,l),f(s,ps,l),t(ps,ml),t(ps,Us),t(Us,Zt),t(Zt,ul),t(ps,dl),f(s,He,l),j(Ws,s,l),f(s,Ye,l),j(cs,s,l),Ke=!0},p(s,[l]){const Hs={};l&2&&(Hs.$$scope={dirty:l,ctx:s}),Z.$set(Hs);const Jt={};l&2&&(Jt.$$scope={dirty:l,ctx:s}),ts.$set(Jt);const Gt={};l&2&&(Gt.$$scope={dirty:l,ctx:s}),ns.$set(Gt);const Qt={};l&2&&(Qt.$$scope={dirty:l,ctx:s}),os.$set(Qt);const Xt={};l&2&&(Xt.$$scope={dirty:l,ctx:s}),cs.$set(Xt)},i(s){Ke||(k(g.$$.fragment,s),k(D.$$.fragment,s),k(Z.$$.fragment,s),k(_s.$$.fragment,s),k(gs.$$.fragment,s),k($s.$$.fragment,s),k(ws.$$.fragment,s),k(js.$$.fragment,s),k(ks.$$.fragment,s),k(bs.$$.fragment,s),k(vs.$$.fragment,s),k(Ts.$$.fragment,s),k(qs.$$.fragment,s),k(ts.$$.fragment,s),k(Cs.$$.fragment,s),k(As.$$.fragment,s),k(ns.$$.fragment,s),k(Ss.$$.fragment,s),k(Os.$$.fragment,s),k(os.$$.fragment,s),k(Ns.$$.fragment,s),k(Is.$$.fragment,s),k(Bs.$$.fragment,s),k(Rs.$$.fragment,s),k(Ws.$$.fragment,s),k(cs.$$.fragment,s),Ke=!0)},o(s){b(g.$$.fragment,s),b(D.$$.fragment,s),b(Z.$$.fragment,s),b(_s.$$.fragment,s),b(gs.$$.fragment,s),b($s.$$.fragment,s),b(ws.$$.fragment,s),b(js.$$.fragment,s),b(ks.$$.fragment,s),b(bs.$$.fragment,s),b(vs.$$.fragment,s),b(Ts.$$.fragment,s),b(qs.$$.fragment,s),b(ts.$$.fragment,s),b(Cs.$$.fragment,s),b(As.$$.fragment,s),b(ns.$$.fragment,s),b(Ss.$$.fragment,s),b(Os.$$.fragment,s),b(os.$$.fragment,s),b(Ns.$$.fragment,s),b(Is.$$.fragment,s),b(Bs.$$.fragment,s),b(Rs.$$.fragment,s),b(Ws.$$.fragment,s),b(cs.$$.fragment,s),Ke=!1},d(s){e(c),s&&e(d),s&&e(p),v(g),s&&e(y),v(D,s),s&&e(O),s&&e(Ks),s&&e(te),s&&e(L),s&&e(ee),v(Z,s),s&&e(ae),s&&e(H),v(_s),s&&e(ne),s&&e(Vs),s&&e(le),v(gs,s),s&&e(oe),s&&e(Zs),s&&e(re),v($s,s),s&&e(ie),s&&e(G),s&&e(pe),v(ws,s),s&&e(ce),s&&e(N),s&&e(fe),s&&e(I),s&&e(he),s&&e(Y),v(js),s&&e(me),v(ks,s),s&&e(ue),s&&e(X),s&&e(de),v(bs,s),s&&e(_e),s&&e(ss),s&&e(ge),v(vs,s),s&&e($e),s&&e(M),s&&e(we),s&&e(R),s&&e(je),s&&e(Qs),s&&e(ke),v(Ts,s),s&&e(be),s&&e(F),s&&e(ve),v(qs,s),s&&e(xe),s&&e(A),s&&e(Ee),v(ts,s),s&&e(ye),s&&e(K),v(Cs),s&&e(Te),s&&e(as),s&&e(ze),v(As,s),s&&e(qe),v(ns,s),s&&e(Ce),s&&e(tt),s&&e(Ae),s&&e(U),s&&e(De),v(Ss,s),s&&e(Fe),s&&e(V),v(Os),s&&e(Pe),s&&e(lt),s&&e(Se),v(os,s),s&&e(Oe),s&&e(P),s&&e(Le),v(Ns,s),s&&e(Ne),s&&e(ot),s&&e(Ie),v(Is,s),s&&e(Be),s&&e(rs),s&&e(Me),v(Bs,s),s&&e(Re),s&&e(is),s&&e(Ue),v(Rs,s),s&&e(We),s&&e(ps),s&&e(He),v(Ws,s),s&&e(Ye),v(cs,s)}}}const Mo={local:"token-classification",sections:[{local:"load-wnut-17-dataset",title:"Load WNUT 17 dataset"},{local:"preprocess",title:"Preprocess"},{local:"finetune-with-trainer",title:"Fine-tune with Trainer"},{local:"finetune-with-tensorflow",title:"Fine-tune with TensorFlow"}],title:"Token classification"};function Ro(z,c,d){let{fw:p}=c;return z.$$set=_=>{"fw"in _&&d(0,p=_.fw)},[p]}class Zo extends To{constructor(c){super();zo(this,c,Ro,Bo,qo,{fw:0})}}export{Zo as default,Mo as metadata};
