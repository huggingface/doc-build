import{S as rt,i as st,s as ht,e as n,k as p,w as et,t as r,L as ft,c as o,d as t,m as c,a as i,x as tt,h as s,b as f,J as a,g as h,y as at,K as pt,q as nt,o as ot,B as it}from"../../chunks/vendor-9e2b328e.js";import{I as lt}from"../../chunks/IconCopyLink-fd0e58fd.js";function ct(se){let m,$,u,d,N,b,he,W,fe,F,v,T,q,y,pe,j,ce,H,P,ue,x,de,me,Y,A,ge,z,M,J,ve,X,S,Te,Z,g,O,Pe,_e,R,we,Ge,D,be,E,ye,xe,K,C,De,U,_,Ee,B,ke,$e,Q,w,Ae,L,Me,Se,V,G,Ce,k,Le,Ie,ee;return b=new lt({}),y=new lt({}),{c(){m=n("meta"),$=p(),u=n("h1"),d=n("a"),N=n("span"),et(b.$$.fragment),he=p(),W=n("span"),fe=r("DialoGPT"),F=p(),v=n("h2"),T=n("a"),q=n("span"),et(y.$$.fragment),pe=p(),j=n("span"),ce=r("Overview"),H=p(),P=n("p"),ue=r("DialoGPT was proposed in "),x=n("a"),de=r("DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation"),me=r(` by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao,
Jianfeng Gao, Jingjing Liu, Bill Dolan. It\u2019s a GPT2 Model trained on 147M conversation-like exchanges extracted from
Reddit.`),Y=p(),A=n("p"),ge=r("The abstract from the paper is the following:"),z=p(),M=n("p"),J=n("em"),ve=r(`We present a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained
transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning
from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human
both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems
that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline
systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response
generation and the development of more intelligent open-domain dialogue systems.`),X=p(),S=n("p"),Te=r("Tips:"),Z=p(),g=n("ul"),O=n("li"),Pe=r(`DialoGPT is a model with absolute position embeddings so it\u2019s usually advised to pad the inputs on the right rather
than the left.`),_e=p(),R=n("li"),we=r(`DialoGPT was trained with a causal language modeling (CLM) objective on conversational data and is therefore powerful
at response generation in open-domain dialogue systems.`),Ge=p(),D=n("li"),be=r("DialoGPT enables the user to create a chat bot in just 10 lines of code as shown on "),E=n("a"),ye=r("DialoGPT\u2019s model card"),xe=r("."),K=p(),C=n("p"),De=r("Training:"),U=p(),_=n("p"),Ee=r("In order to train or fine-tune DialoGPT, one can use causal language modeling training. To cite the official paper: "),B=n("em"),ke=r(`We
follow the OpenAI GPT-2 to model a multiturn dialogue session as a long text and frame the generation task as language
modeling. We first concatenate all dialog turns within a dialogue session into a long text x_1,\u2026, x_N (N is the
sequence length), ended by the end-of-text token.`),$e=r(" For more information please confer to the original paper."),Q=p(),w=n("p"),Ae=r("DialoGPT\u2019s architecture is based on the GPT2 model, so one can refer to "),L=n("a"),Me=r("GPT2\u2019s documentation page"),Se=r("."),V=p(),G=n("p"),Ce=r("The original code can be found "),k=n("a"),Le=r("here"),Ie=r("."),this.h()},l(e){const l=ft('[data-svelte="svelte-1phssyn"]',document.head);m=o(l,"META",{name:!0,content:!0}),l.forEach(t),$=c(e),u=o(e,"H1",{class:!0});var te=i(u);d=o(te,"A",{id:!0,class:!0,href:!0});var Ne=i(d);N=o(Ne,"SPAN",{});var We=i(N);tt(b.$$.fragment,We),We.forEach(t),Ne.forEach(t),he=c(te),W=o(te,"SPAN",{});var qe=i(W);fe=s(qe,"DialoGPT"),qe.forEach(t),te.forEach(t),F=c(e),v=o(e,"H2",{class:!0});var ae=i(v);T=o(ae,"A",{id:!0,class:!0,href:!0});var je=i(T);q=o(je,"SPAN",{});var Je=i(q);tt(y.$$.fragment,Je),Je.forEach(t),je.forEach(t),pe=c(ae),j=o(ae,"SPAN",{});var Oe=i(j);ce=s(Oe,"Overview"),Oe.forEach(t),ae.forEach(t),H=c(e),P=o(e,"P",{});var ne=i(P);ue=s(ne,"DialoGPT was proposed in "),x=o(ne,"A",{href:!0,rel:!0});var Re=i(x);de=s(Re,"DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation"),Re.forEach(t),me=s(ne,` by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao,
Jianfeng Gao, Jingjing Liu, Bill Dolan. It\u2019s a GPT2 Model trained on 147M conversation-like exchanges extracted from
Reddit.`),ne.forEach(t),Y=c(e),A=o(e,"P",{});var Be=i(A);ge=s(Be,"The abstract from the paper is the following:"),Be.forEach(t),z=c(e),M=o(e,"P",{});var Fe=i(M);J=o(Fe,"EM",{});var He=i(J);ve=s(He,`We present a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained
transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning
from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human
both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems
that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline
systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response
generation and the development of more intelligent open-domain dialogue systems.`),He.forEach(t),Fe.forEach(t),X=c(e),S=o(e,"P",{});var Ye=i(S);Te=s(Ye,"Tips:"),Ye.forEach(t),Z=c(e),g=o(e,"UL",{});var I=i(g);O=o(I,"LI",{});var ze=i(O);Pe=s(ze,`DialoGPT is a model with absolute position embeddings so it\u2019s usually advised to pad the inputs on the right rather
than the left.`),ze.forEach(t),_e=c(I),R=o(I,"LI",{});var Xe=i(R);we=s(Xe,`DialoGPT was trained with a causal language modeling (CLM) objective on conversational data and is therefore powerful
at response generation in open-domain dialogue systems.`),Xe.forEach(t),Ge=c(I),D=o(I,"LI",{});var oe=i(D);be=s(oe,"DialoGPT enables the user to create a chat bot in just 10 lines of code as shown on "),E=o(oe,"A",{href:!0,rel:!0});var Ze=i(E);ye=s(Ze,"DialoGPT\u2019s model card"),Ze.forEach(t),xe=s(oe,"."),oe.forEach(t),I.forEach(t),K=c(e),C=o(e,"P",{});var Ke=i(C);De=s(Ke,"Training:"),Ke.forEach(t),U=c(e),_=o(e,"P",{});var ie=i(_);Ee=s(ie,"In order to train or fine-tune DialoGPT, one can use causal language modeling training. To cite the official paper: "),B=o(ie,"EM",{});var Ue=i(B);ke=s(Ue,`We
follow the OpenAI GPT-2 to model a multiturn dialogue session as a long text and frame the generation task as language
modeling. We first concatenate all dialog turns within a dialogue session into a long text x_1,\u2026, x_N (N is the
sequence length), ended by the end-of-text token.`),Ue.forEach(t),$e=s(ie," For more information please confer to the original paper."),ie.forEach(t),Q=c(e),w=o(e,"P",{});var le=i(w);Ae=s(le,"DialoGPT\u2019s architecture is based on the GPT2 model, so one can refer to "),L=o(le,"A",{href:!0});var Qe=i(L);Me=s(Qe,"GPT2\u2019s documentation page"),Qe.forEach(t),Se=s(le,"."),le.forEach(t),V=c(e),G=o(e,"P",{});var re=i(G);Ce=s(re,"The original code can be found "),k=o(re,"A",{href:!0,rel:!0});var Ve=i(k);Le=s(Ve,"here"),Ve.forEach(t),Ie=s(re,"."),re.forEach(t),this.h()},h(){f(m,"name","hf:doc:metadata"),f(m,"content",JSON.stringify(ut)),f(d,"id","dialogpt"),f(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(d,"href","#dialogpt"),f(u,"class","relative group"),f(T,"id","overview"),f(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(T,"href","#overview"),f(v,"class","relative group"),f(x,"href","https://arxiv.org/abs/1911.00536"),f(x,"rel","nofollow"),f(E,"href","https://huggingface.co/microsoft/DialoGPT-medium"),f(E,"rel","nofollow"),f(L,"href","gpt2"),f(k,"href","https://github.com/microsoft/DialoGPT"),f(k,"rel","nofollow")},m(e,l){a(document.head,m),h(e,$,l),h(e,u,l),a(u,d),a(d,N),at(b,N,null),a(u,he),a(u,W),a(W,fe),h(e,F,l),h(e,v,l),a(v,T),a(T,q),at(y,q,null),a(v,pe),a(v,j),a(j,ce),h(e,H,l),h(e,P,l),a(P,ue),a(P,x),a(x,de),a(P,me),h(e,Y,l),h(e,A,l),a(A,ge),h(e,z,l),h(e,M,l),a(M,J),a(J,ve),h(e,X,l),h(e,S,l),a(S,Te),h(e,Z,l),h(e,g,l),a(g,O),a(O,Pe),a(g,_e),a(g,R),a(R,we),a(g,Ge),a(g,D),a(D,be),a(D,E),a(E,ye),a(D,xe),h(e,K,l),h(e,C,l),a(C,De),h(e,U,l),h(e,_,l),a(_,Ee),a(_,B),a(B,ke),a(_,$e),h(e,Q,l),h(e,w,l),a(w,Ae),a(w,L),a(L,Me),a(w,Se),h(e,V,l),h(e,G,l),a(G,Ce),a(G,k),a(k,Le),a(G,Ie),ee=!0},p:pt,i(e){ee||(nt(b.$$.fragment,e),nt(y.$$.fragment,e),ee=!0)},o(e){ot(b.$$.fragment,e),ot(y.$$.fragment,e),ee=!1},d(e){t(m),e&&t($),e&&t(u),it(b),e&&t(F),e&&t(v),it(y),e&&t(H),e&&t(P),e&&t(Y),e&&t(A),e&&t(z),e&&t(M),e&&t(X),e&&t(S),e&&t(Z),e&&t(g),e&&t(K),e&&t(C),e&&t(U),e&&t(_),e&&t(Q),e&&t(w),e&&t(V),e&&t(G)}}}const ut={local:"dialogpt",sections:[{local:"overview",title:"Overview"}],title:"DialoGPT"};function dt(se,m,$){let{fw:u}=m;return se.$$set=d=>{"fw"in d&&$(0,u=d.fw)},[u]}class vt extends rt{constructor(m){super();st(this,m,dt,ct,ht,{fw:0})}}export{vt as default,ut as metadata};
