import{S as ac,i as nc,s as ic,e as a,k as l,w as u,t as r,L as cc,c as n,d as o,m as d,a as i,x as _,h as s,b as c,J as e,g as p,y as g,q as v,o as T,B as x}from"../../chunks/vendor-9e2b328e.js";import{T as sc}from"../../chunks/Tip-76f97a76.js";import{D as W}from"../../chunks/Docstring-50fd6873.js";import{C as Pr}from"../../chunks/CodeBlock-88e23343.js";import{I as _t}from"../../chunks/IconCopyLink-fd0e58fd.js";import"../../chunks/CopyButton-4ae140ab.js";function lc(ye){let f,j,m,w,L,S,B,D;return{c(){f=a("p"),j=r(`This class method is simply calling the feature extractor
`),m=a("a"),w=r("from_pretrained()"),L=r(` and the tokenizer
`),S=a("code"),B=r("from_pretrained"),D=r(` methods. Please refer to the docstrings of the
methods above for more information.`),this.h()},l(z){f=n(z,"P",{});var b=i(f);j=s(b,`This class method is simply calling the feature extractor
`),m=n(b,"A",{href:!0});var C=i(m);w=s(C,"from_pretrained()"),C.forEach(o),L=s(b,` and the tokenizer
`),S=n(b,"CODE",{});var q=i(S);B=s(q,"from_pretrained"),q.forEach(o),D=s(b,` methods. Please refer to the docstrings of the
methods above for more information.`),b.forEach(o),this.h()},h(){c(m,"href","/docs/transformers/master/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained")},m(z,b){p(z,f,b),e(f,j),e(f,m),e(m,w),e(f,L),e(f,S),e(S,B),e(f,D)},d(z){z&&o(f)}}}function dc(ye){let f,j,m,w,L,S,B,D;return{c(){f=a("p"),j=r("This class method is simply calling "),m=a("a"),w=r("save_pretrained()"),L=r(` and
`),S=a("code"),B=r("save_pretrained"),D=r(`. Please refer to the docstrings of the methods
above for more information.`),this.h()},l(z){f=n(z,"P",{});var b=i(f);j=s(b,"This class method is simply calling "),m=n(b,"A",{href:!0});var C=i(m);w=s(C,"save_pretrained()"),C.forEach(o),L=s(b,` and
`),S=n(b,"CODE",{});var q=i(S);B=s(q,"save_pretrained"),q.forEach(o),D=s(b,`. Please refer to the docstrings of the methods
above for more information.`),b.forEach(o),this.h()},h(){c(m,"href","/docs/transformers/master/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained")},m(z,b){p(z,f,b),e(f,j),e(f,m),e(m,w),e(f,L),e(f,S),e(S,B),e(f,D)},d(z){z&&o(f)}}}function hc(ye){let f,j,m,w,L,S,B,D,z,b,C,q,to,$e,zr,oo,Cr,Fo,R,Mr,gt,jr,qr,Ee,Ar,Fr,Lo,y,Lr,ro,Dr,Ir,so,Wr,Vr,vt,Nr,Br,Tt,Or,Ur,xt,Hr,Rr,ao,Jr,Gr,Do,ie,Kr,Pe,Qr,Xr,Io,ce,Yr,ze,Zr,es,Wo,kt,ts,Vo,J,Ce,os,Me,rs,ss,as,je,ns,bt,is,cs,ls,qe,ds,Ae,hs,ps,No,ee,le,no,Fe,fs,io,ms,Bo,G,us,wt,_s,gs,St,vs,Ts,Oo,P,xs,yt,ks,bs,$t,ws,Ss,Et,ys,$s,Pt,Es,Ps,zt,zs,Cs,Uo,Ct,co,Ms,Ho,Le,Ro,Mt,De,lo,js,qs,ho,As,Jo,Ie,Go,de,Fs,We,Ls,Ds,Ko,te,he,po,Ve,Is,fo,Ws,Qo,M,Ne,Vs,oe,Ns,jt,Bs,Os,Be,Us,Hs,Rs,re,Js,qt,Gs,Ks,At,Qs,Xs,Ys,mo,Zs,ea,Oe,Xo,se,pe,uo,Ue,ta,_o,oa,Yo,E,He,ra,go,sa,aa,Re,na,Ft,ia,ca,la,fe,Je,da,vo,ha,pa,K,Ge,fa,To,ma,ua,Ke,_a,xo,ga,va,Ta,ko,Zo,ae,me,bo,Qe,xa,wo,ka,er,k,Xe,ba,So,wa,Sa,A,Lt,ya,$a,Dt,Ea,Pa,It,za,Ca,Ye,yo,Ma,ja,qa,Wt,Aa,Fa,La,ue,Ze,Da,O,Ia,$o,Wa,Va,Vt,Na,Ba,et,Eo,Oa,Ua,Ha,Ra,Q,tt,Ja,Po,Ga,Ka,_e,Qa,X,ot,Xa,rt,Ya,Nt,Za,en,tn,ge,on,ve,st,rn,at,sn,Bt,an,nn,cn,Te,nt,ln,it,dn,Ot,hn,pn,fn,xe,ct,mn,zo,un,tr,ne,ke,Co,lt,_n,Mo,gn,or,I,dt,vn,U,Tn,Ut,xn,kn,jo,bn,wn,Ht,Sn,yn,$n,ht,En,pt,Pn,zn,Cn,Y,ft,Mn,qo,jn,qn,mt,rr;return S=new _t({}),$e=new _t({}),Fe=new _t({}),Le=new Pr({props:{code:`import torch
from transformers import Speech2Text2Processor, SpeechEncoderDecoderModel
from datasets import load_dataset
import soundfile as sf

model = SpeechEncoderDecoderModel.from_pretrained("facebook/s2t-wav2vec2-large-en-de")
processor = Speech2Text2Processor.from_pretrained("facebook/s2t-wav2vec2-large-en-de")


def map_to_array(batch):
    speech, _ = sf.read(batch["file"])
    batch["speech"] = speech
    return batch


ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
ds = ds.map(map_to_array)

inputs = processor(ds["speech"][0], sampling_rate=16_000, return_tensors="pt")
generated_ids = model.generate(inputs=inputs["input_values"], attention_mask=inputs["attention_mask"])

transcription = processor.batch_decode(generated_ids),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Speech2Text2Processor, SpeechEncoderDecoderModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> soundfile <span class="hljs-keyword">as</span> sf

<span class="hljs-meta">&gt;&gt;&gt; </span>model = SpeechEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;facebook/s2t-wav2vec2-large-en-de&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Speech2Text2Processor.from_pretrained(<span class="hljs-string">&quot;facebook/s2t-wav2vec2-large-en-de&quot;</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">map_to_array</span>(<span class="hljs-params">batch</span>):
<span class="hljs-meta">... </span>    speech, _ = sf.read(batch[<span class="hljs-string">&quot;file&quot;</span>])
<span class="hljs-meta">... </span>    batch[<span class="hljs-string">&quot;speech&quot;</span>] = speech
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> batch


<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.<span class="hljs-built_in">map</span>(map_to_array)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(ds[<span class="hljs-string">&quot;speech&quot;</span>][<span class="hljs-number">0</span>], sampling_rate=<span class="hljs-number">16_000</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(inputs=inputs[<span class="hljs-string">&quot;input_values&quot;</span>], attention_mask=inputs[<span class="hljs-string">&quot;attention_mask&quot;</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>transcription = processor.batch_decode(generated_ids)`}}),Ie=new Pr({props:{code:`from datasets import load_dataset
from transformers import pipeline

librispeech_en = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
asr = pipeline(
    "automatic-speech-recognition",
    model="facebook/s2t-wav2vec2-large-en-de",
    feature_extractor="facebook/s2t-wav2vec2-large-en-de",
)

translation_de = asr(librispeech_en[0]["file"]),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>librispeech_en = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>asr = pipeline(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>,
<span class="hljs-meta">... </span>    model=<span class="hljs-string">&quot;facebook/s2t-wav2vec2-large-en-de&quot;</span>,
<span class="hljs-meta">... </span>    feature_extractor=<span class="hljs-string">&quot;facebook/s2t-wav2vec2-large-en-de&quot;</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>translation_de = asr(librispeech_en[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;file&quot;</span>])`}}),Ve=new _t({}),Ne=new W({props:{name:"class transformers.Speech2Text2Config",anchor:"transformers.Speech2Text2Config",parameters:[{name:"vocab_size",val:" = 10000"},{name:"decoder_layers",val:" = 6"},{name:"decoder_ffn_dim",val:" = 2048"},{name:"decoder_attention_heads",val:" = 4"},{name:"decoder_layerdrop",val:" = 0.0"},{name:"use_cache",val:" = True"},{name:"activation_function",val:" = 'relu'"},{name:"d_model",val:" = 256"},{name:"dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.0"},{name:"activation_dropout",val:" = 0.0"},{name:"init_std",val:" = 0.02"},{name:"decoder_start_token_id",val:" = 2"},{name:"classifier_dropout",val:" = 0.0"},{name:"scale_embedding",val:" = True"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"max_source_positions",val:" = 6000"},{name:"max_target_positions",val:" = 1024"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text_2/configuration_speech_to_text_2.py#L29",parametersDescription:[{anchor:"transformers.Speech2Text2Config.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 50265) &#x2014;
Vocabulary size of the Speech2Text model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a>`,name:"vocab_size"},{anchor:"transformers.Speech2Text2Config.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of the layers and the pooler layer.`,name:"d_model"},{anchor:"transformers.Speech2Text2Config.decoder_layers",description:`<strong>decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of decoder layers.`,name:"decoder_layers"},{anchor:"transformers.Speech2Text2Config.decoder_attention_heads",description:`<strong>decoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer decoder.`,name:"decoder_attention_heads"},{anchor:"transformers.Speech2Text2Config.decoder_ffn_dim",description:`<strong>decoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"decoder_ffn_dim"},{anchor:"transformers.Speech2Text2Config.activation_function",description:`<strong>activation_function</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the pooler. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>,
<code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.Speech2Text2Config.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, and pooler.`,name:"dropout"},{anchor:"transformers.Speech2Text2Config.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.Speech2Text2Config.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for activations inside the fully connected layer.`,name:"activation_dropout"},{anchor:"transformers.Speech2Text2Config.classifier_dropout",description:`<strong>classifier_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for classifier.`,name:"classifier_dropout"},{anchor:"transformers.Speech2Text2Config.init_std",description:`<strong>init_std</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
<a href="https://arxiv.org/abs/1909.11556%3E%60" rel="nofollow">https://arxiv.org/abs/1909.11556&gt;\`</a>__ for more details. decoder_layerdrop: (<code>float</code>, <em>optional</em>, defaults to
0.0): The LayerDrop probability for the decoder. See the [LayerDrop paper](see
<a href="https://arxiv.org/abs/1909.11556" rel="nofollow">https://arxiv.org/abs/1909.11556</a>) for more details.`,name:"init_std"},{anchor:"transformers.Speech2Text2Config.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"},{anchor:"transformers.Speech2Text2Config.max_source_positions",description:`<strong>max_source_positions</strong> (<code>int</code>, <em>optional</em>, defaults to 6000) &#x2014;
The maximum sequence length of log-mel filter-bank features that this model might ever be used with.
max_target_positions &#x2014; (<code>int</code>, <em>optional</em>, defaults to 1024):
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_source_positions"}]}}),Oe=new Pr({props:{code:`from transformers import Speech2Text2ForCausalLM, Speech2Text2Config

# Initializing a Speech2Text2 s2t_transformer_s style configuration
configuration = Speech2Text2Config()

# Initializing a model from the s2t_transformer_s style configuration
model = Speech2Text2ForCausalLM(configuration)

# Accessing the model configuration
configuration = model.config,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Speech2Text2ForCausalLM, Speech2Text2Config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Speech2Text2 s2t_transformer_s style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Speech2Text2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the s2t_transformer_s style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Speech2Text2ForCausalLM(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),Ue=new _t({}),He=new W({props:{name:"class transformers.Speech2Text2Tokenizer",anchor:"transformers.Speech2Text2Tokenizer",parameters:[{name:"vocab_file",val:""},{name:"bos_token",val:" = '<s>'"},{name:"pad_token",val:" = '<pad>'"},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"do_lower_case",val:" = False"},{name:"merges_file",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text_2/tokenization_speech_to_text_2.py#L67",parametersDescription:[{anchor:"transformers.Speech2Text2Tokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.Speech2Text2Tokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sentence token.`,name:"bos_token"},{anchor:"transformers.Speech2Text2Tokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sentence token.`,name:"eos_token"},{anchor:"transformers.Speech2Text2Tokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.Speech2Text2Tokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.</p>
<p>**kwargs &#x2014;
Additional keyword arguments passed along to <a href="/docs/transformers/master/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>`,name:"pad_token"}]}}),Je=new W({props:{name:"batch_decode",anchor:"transformers.PreTrainedTokenizerBase.batch_decode",parameters:[{name:"sequences",val:": typing.Union[typing.List[int], typing.List[typing.List[int]], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')]"},{name:"skip_special_tokens",val:": bool = False"},{name:"clean_up_tokenization_spaces",val:": bool = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_utils_base.py#L3250",parametersDescription:[{anchor:"transformers.PreTrainedTokenizerBase.batch_decode.sequences",description:`<strong>sequences</strong> (<code>Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]</code>) &#x2014;
List of tokenized input ids. Can be obtained using the <code>__call__</code> method.`,name:"sequences"},{anchor:"transformers.PreTrainedTokenizerBase.batch_decode.skip_special_tokens",description:`<strong>skip_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to remove special tokens in the decoding.`,name:"skip_special_tokens"},{anchor:"transformers.PreTrainedTokenizerBase.batch_decode.clean_up_tokenization_spaces",description:`<strong>clean_up_tokenization_spaces</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to clean up the tokenization spaces.`,name:"clean_up_tokenization_spaces"},{anchor:"transformers.PreTrainedTokenizerBase.batch_decode.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the underlying model specific decode method.`,name:"kwargs"}],returnDescription:`
<p>The list of decoded sentences.</p>
`,returnType:`
<p><code>List[str]</code></p>
`}}),Ge=new W({props:{name:"decode",anchor:"transformers.PreTrainedTokenizerBase.decode",parameters:[{name:"token_ids",val:": typing.Union[int, typing.List[int], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')]"},{name:"skip_special_tokens",val:": bool = False"},{name:"clean_up_tokenization_spaces",val:": bool = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_utils_base.py#L3283",parametersDescription:[{anchor:"transformers.PreTrainedTokenizerBase.decode.token_ids",description:`<strong>token_ids</strong> (<code>Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]</code>) &#x2014;
List of tokenized input ids. Can be obtained using the <code>__call__</code> method.`,name:"token_ids"},{anchor:"transformers.PreTrainedTokenizerBase.decode.skip_special_tokens",description:`<strong>skip_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to remove special tokens in the decoding.`,name:"skip_special_tokens"},{anchor:"transformers.PreTrainedTokenizerBase.decode.clean_up_tokenization_spaces",description:`<strong>clean_up_tokenization_spaces</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to clean up the tokenization spaces.`,name:"clean_up_tokenization_spaces"},{anchor:"transformers.PreTrainedTokenizerBase.decode.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the underlying model specific decode method.`,name:"kwargs"}],returnDescription:`
<p>The decoded sentence.</p>
`,returnType:`
<p><code>str</code></p>
`}}),Qe=new _t({}),Xe=new W({props:{name:"class transformers.Speech2Text2Processor",anchor:"transformers.Speech2Text2Processor",parameters:[{name:"feature_extractor",val:""},{name:"tokenizer",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text_2/processing_speech_to_text_2.py#L23",parametersDescription:[{anchor:"transformers.Speech2Text2Processor.feature_extractor",description:`<strong>feature_extractor</strong> (<code>AutoFeatureExtractor</code>) &#x2014;
An instance of <a href="/docs/transformers/master/en/model_doc/auto#transformers.AutoFeatureExtractor">AutoFeatureExtractor</a>. The feature extractor is a required input.`,name:"feature_extractor"},{anchor:"transformers.Speech2Text2Processor.tokenizer",description:`<strong>tokenizer</strong> (<code>Speech2Text2Tokenizer</code>) &#x2014;
An instance of <a href="/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer">Speech2Text2Tokenizer</a>. The tokenizer is a required input.`,name:"tokenizer"}]}}),Ze=new W({props:{name:"__call__",anchor:"transformers.Speech2Text2Processor.__call__",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text_2/processing_speech_to_text_2.py#L44"}}),tt=new W({props:{name:"from_pretrained",anchor:"transformers.ProcessorMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/processing_utils.py#L157",parametersDescription:[{anchor:"transformers.ProcessorMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/master/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.
**kwargs &#x2014;
Additional keyword arguments passed along to both
<a href="/docs/transformers/master/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained">from_pretrained()</a> and
<code>from_pretrained</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"}]}}),_e=new sc({props:{$$slots:{default:[lc]},$$scope:{ctx:ye}}}),ot=new W({props:{name:"save_pretrained",anchor:"transformers.ProcessorMixin.save_pretrained",parameters:[{name:"save_directory",val:""},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/processing_utils.py#L95",parametersDescription:[{anchor:"transformers.ProcessorMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the feature extractor JSON file and the tokenizer files will be saved (directory will
be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.ProcessorMixin.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your processor to the Hugging Face model hub after saving it.</p>
<div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400">
						
<p>Using <code>push_to_hub=True</code> will synchronize the repository you are pushing to with <code>save_directory</code>,
which requires <code>save_directory</code> to be a local clone of the repo you are pushing to if it&#x2019;s an existing
folder. Pass along <code>temp_dir=True</code> to use a temporary directory instead.</p>

					</div>
<p>kwargs &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/master/en/main_classes/model#transformers.file_utils.PushToHubMixin.push_to_hub">push_to_hub()</a> method.`,name:"push_to_hub"}]}}),ge=new sc({props:{$$slots:{default:[dc]},$$scope:{ctx:ye}}}),st=new W({props:{name:"batch_decode",anchor:"transformers.Speech2Text2Processor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text_2/processing_speech_to_text_2.py#L54"}}),nt=new W({props:{name:"decode",anchor:"transformers.Speech2Text2Processor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text_2/processing_speech_to_text_2.py#L61"}}),ct=new W({props:{name:"as_target_processor",anchor:"transformers.Speech2Text2Processor.as_target_processor",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text_2/processing_speech_to_text_2.py#L68"}}),lt=new _t({}),dt=new W({props:{name:"class transformers.Speech2Text2ForCausalLM",anchor:"transformers.Speech2Text2ForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py#L743",parametersDescription:[{anchor:"transformers.Speech2Text2ForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ft=new W({props:{name:"forward",anchor:"transformers.Speech2Text2ForCausalLM.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"encoder_hidden_states",val:" = None"},{name:"encoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"past_key_values",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py#L774",parametersDescription:[{anchor:"transformers.Speech2Text2ForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
provide it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer">Speech2Text2Tokenizer</a>. See <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong>  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
if the model is configured as a decoder.`,name:"encoder_hidden_states"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.encoder_attention_mask",description:`<strong>encoder_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used
in the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:`,name:"encoder_attention_mask"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of
shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>. The two additional
tensors are only required when the model is used as a decoder in a Sequence to Sequence model.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the
cross-attention blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those
that don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of
all <code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding
(see <code>past_key_values</code>).</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"use_cache"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more detail.`,name:"output_hidden_states"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
>transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config"
>Speech2Text2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Cross attentions weights after the attention softmax, used to compute the weighted average in the
cross-attention heads.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> tuples of length <code>config.n_layers</code>, with each tuple containing the cached key,
value states of the self-attention and the cross-attention layers if model is used in encoder-decoder
setting. Only relevant if <code>config.is_decoder = True</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
>transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),mt=new Pr({props:{code:`from transformers import (
    SpeechEncoderDecoderModel,
    Speech2Text2ForCausalLM,
    Wav2Vec2Model,
    Speech2Text2Config,
    Wav2Vec2Config,
)

encoder = Wav2Vec2Model(Wav2Vec2Config())
decoder = Speech2Text2ForCausalLM(Speech2Text2Config())

model = SpeechEncoderDecoderModel(encoder=encoder, decoder=decoder),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    SpeechEncoderDecoderModel,
<span class="hljs-meta">... </span>    Speech2Text2ForCausalLM,
<span class="hljs-meta">... </span>    Wav2Vec2Model,
<span class="hljs-meta">... </span>    Speech2Text2Config,
<span class="hljs-meta">... </span>    Wav2Vec2Config,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder = Wav2Vec2Model(Wav2Vec2Config())
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder = Speech2Text2ForCausalLM(Speech2Text2Config())
<span class="hljs-comment"># init speech2text model</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>model = SpeechEncoderDecoderModel(encoder=encoder, decoder=decoder)`}}),{c(){f=a("meta"),j=l(),m=a("h1"),w=a("a"),L=a("span"),u(S.$$.fragment),B=l(),D=a("span"),z=r("Speech2Text2"),b=l(),C=a("h2"),q=a("a"),to=a("span"),u($e.$$.fragment),zr=l(),oo=a("span"),Cr=r("Overview"),Fo=l(),R=a("p"),Mr=r("The Speech2Text2 model is used together with "),gt=a("a"),jr=r("Wav2Vec2"),qr=r(` for Speech Translation models proposed in
`),Ee=a("a"),Ar=r("Large-Scale Self- and Semi-Supervised Learning for Speech Translation"),Fr=r(` by
Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.`),Lo=l(),y=a("p"),Lr=r("Speech2Text2 is a "),ro=a("em"),Dr=r("decoder-only"),Ir=r(" transformer model that can be used with any speech "),so=a("em"),Wr=r("encoder-only"),Vr=r(`, such as
`),vt=a("a"),Nr=r("Wav2Vec2"),Br=r(" or "),Tt=a("a"),Or=r("HuBERT"),Ur=r(` for Speech-to-Text tasks. Please refer to the
`),xt=a("a"),Hr=r("SpeechEncoderDecoder"),Rr=r(" class on how to combine Speech2Text2 with any speech "),ao=a("em"),Jr=r("encoder-only"),Gr=r(`
model.`),Do=l(),ie=a("p"),Kr=r("This model was contributed by "),Pe=a("a"),Qr=r("Patrick von Platen"),Xr=r("."),Io=l(),ce=a("p"),Yr=r("The original code can be found "),ze=a("a"),Zr=r("here"),es=r("."),Wo=l(),kt=a("p"),ts=r("Tips:"),Vo=l(),J=a("ul"),Ce=a("li"),os=r(`Speech2Text2 achieves state-of-the-art results on the CoVoST Speech Translation dataset. For more information, see
the `),Me=a("a"),rs=r("official models"),ss=r(" ."),as=l(),je=a("li"),ns=r("Speech2Text2 is always used within the "),bt=a("a"),is=r("SpeechEncoderDecoder"),cs=r(" framework."),ls=l(),qe=a("li"),ds=r("Speech2Text2\u2019s tokenizer is based on "),Ae=a("a"),hs=r("fastBPE"),ps=r("."),No=l(),ee=a("h2"),le=a("a"),no=a("span"),u(Fe.$$.fragment),fs=l(),io=a("span"),ms=r("Inference"),Bo=l(),G=a("p"),us=r("Speech2Text2\u2019s "),wt=a("a"),_s=r("SpeechEncoderDecoderModel"),gs=r(` model accepts raw waveform input values from speech and
makes use of `),St=a("a"),vs=r("generate()"),Ts=r(` to translate the input speech
autoregressively to the target language.`),Oo=l(),P=a("p"),xs=r("The "),yt=a("a"),ks=r("Wav2Vec2FeatureExtractor"),bs=r(` class is responsible for preprocessing the input speech and
`),$t=a("a"),ws=r("Speech2Text2Tokenizer"),Ss=r(` decodes the generated target tokens to the target string. The
`),Et=a("a"),ys=r("Speech2Text2Processor"),$s=r(" wraps "),Pt=a("a"),Es=r("Wav2Vec2FeatureExtractor"),Ps=r(` and
`),zt=a("a"),zs=r("Speech2Text2Tokenizer"),Cs=r(` into a single instance to both extract the input features and decode the
predicted token ids.`),Uo=l(),Ct=a("ul"),co=a("li"),Ms=r("Step-by-step Speech Translation"),Ho=l(),u(Le.$$.fragment),Ro=l(),Mt=a("ul"),De=a("li"),lo=a("p"),js=r("Speech Translation via Pipelines"),qs=l(),ho=a("p"),As=r("The automatic speech recognition pipeline can also be used to translate speech in just a couple lines of code"),Jo=l(),u(Ie.$$.fragment),Go=l(),de=a("p"),Fs=r("See "),We=a("a"),Ls=r("model hub"),Ds=r(" to look for Speech2Text2 checkpoints."),Ko=l(),te=a("h2"),he=a("a"),po=a("span"),u(Ve.$$.fragment),Is=l(),fo=a("span"),Ws=r("Speech2Text2Config"),Qo=l(),M=a("div"),u(Ne.$$.fragment),Vs=l(),oe=a("p"),Ns=r("This is the configuration class to store the configuration of a "),jt=a("a"),Bs=r("Speech2Text2ForCausalLM"),Os=r(`. It is used to
instantiate an Speech2Text2 model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Speech2Text2
`),Be=a("a"),Us=r("facebook/s2t-small-librispeech-asr"),Hs=r(" architecture."),Rs=l(),re=a("p"),Js=r("Configuration objects inherit from "),qt=a("a"),Gs=r("PretrainedConfig"),Ks=r(` and can be used to control the model outputs. Read the
documentation from `),At=a("a"),Qs=r("PretrainedConfig"),Xs=r(" for more information."),Ys=l(),mo=a("p"),Zs=r("Example:"),ea=l(),u(Oe.$$.fragment),Xo=l(),se=a("h2"),pe=a("a"),uo=a("span"),u(Ue.$$.fragment),ta=l(),_o=a("span"),oa=r("Speech2TextTokenizer"),Yo=l(),E=a("div"),u(He.$$.fragment),ra=l(),go=a("p"),sa=r("Constructs a Speech2Text2Tokenizer."),aa=l(),Re=a("p"),na=r("This tokenizer inherits from "),Ft=a("a"),ia=r("PreTrainedTokenizer"),ca=r(` which contains some of the main methods. Users should refer to
the superclass for more information regarding such methods.`),la=l(),fe=a("div"),u(Je.$$.fragment),da=l(),vo=a("p"),ha=r("Convert a list of lists of token ids into a list of strings by calling decode."),pa=l(),K=a("div"),u(Ge.$$.fragment),fa=l(),To=a("p"),ma=r(`Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special
tokens and clean up tokenization spaces.`),ua=l(),Ke=a("p"),_a=r("Similar to doing "),xo=a("code"),ga=r("self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))"),va=r("."),Ta=l(),ko=a("div"),Zo=l(),ae=a("h2"),me=a("a"),bo=a("span"),u(Qe.$$.fragment),xa=l(),wo=a("span"),ka=r("Speech2Text2Processor"),er=l(),k=a("div"),u(Xe.$$.fragment),ba=l(),So=a("p"),wa=r(`Constructs a Speech2Text2 processor which wraps a Speech2Text2 feature extractor and a Speech2Text2 tokenizer into
a single processor.`),Sa=l(),A=a("p"),Lt=a("a"),ya=r("Speech2Text2Processor"),$a=r(" offers all the functionalities of "),Dt=a("a"),Ea=r("AutoFeatureExtractor"),Pa=r(" and "),It=a("a"),za=r("Speech2Text2Tokenizer"),Ca=r(`.
See the `),Ye=a("a"),yo=a("strong"),Ma=r("call"),ja=r("()"),qa=r(" and "),Wt=a("a"),Aa=r("decode()"),Fa=r(" for more information."),La=l(),ue=a("div"),u(Ze.$$.fragment),Da=l(),O=a("p"),Ia=r(`When used in normal mode, this method forwards all its arguments to AutoFeatureExtractor\u2019s
`),$o=a("code"),Wa=r("__call__()"),Va=r(`and returns its output. If used in the context
`),Vt=a("a"),Na=r("as_target_processor()"),Ba=r(` this method forwards all its arguments to
Speech2Text2Tokenizer\u2019s `),et=a("a"),Eo=a("strong"),Oa=r("call"),Ua=r("()"),Ha=r(`. Please refer to the doctsring of the above two
methods for more information.`),Ra=l(),Q=a("div"),u(tt.$$.fragment),Ja=l(),Po=a("p"),Ga=r("Instantiate a processor associated with a pretrained model."),Ka=l(),u(_e.$$.fragment),Qa=l(),X=a("div"),u(ot.$$.fragment),Xa=l(),rt=a("p"),Ya=r(`Saves the attributes of this processor (feature extractor, tokenizer\u2026) in the specified directory so that it
can be reloaded using the `),Nt=a("a"),Za=r("from_pretrained()"),en=r(" method."),tn=l(),u(ge.$$.fragment),on=l(),ve=a("div"),u(st.$$.fragment),rn=l(),at=a("p"),sn=r("This method forwards all its arguments to Speech2Text2Tokenizer\u2019s "),Bt=a("a"),an=r("batch_decode()"),nn=r(`. Please
refer to the docstring of this method for more information.`),cn=l(),Te=a("div"),u(nt.$$.fragment),ln=l(),it=a("p"),dn=r("This method forwards all its arguments to Speech2Text2Tokenizer\u2019s "),Ot=a("a"),hn=r("decode()"),pn=r(`. Please refer
to the docstring of this method for more information.`),fn=l(),xe=a("div"),u(ct.$$.fragment),mn=l(),zo=a("p"),un=r(`Temporarily sets the tokenizer for processing the input. Useful for encoding the labels when fine-tuning
Speech2Text2.`),tr=l(),ne=a("h2"),ke=a("a"),Co=a("span"),u(lt.$$.fragment),_n=l(),Mo=a("span"),gn=r("Speech2Text2ForCausalLM"),or=l(),I=a("div"),u(dt.$$.fragment),vn=l(),U=a("p"),Tn=r("The Speech2Text2 Decoder with a language modeling head. Can be used as the decoder part of "),Ut=a("a"),xn=r("EncoderDecoderModel"),kn=r(" and "),jo=a("code"),bn=r("SpeechEncoderDecoder"),wn=r(`.
This model inherits from `),Ht=a("a"),Sn=r("PreTrainedModel"),yn=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),$n=l(),ht=a("p"),En=r("This model is also a PyTorch "),pt=a("a"),Pn=r("torch.nn.Module"),zn=r(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Cn=l(),Y=a("div"),u(ft.$$.fragment),Mn=l(),qo=a("p"),jn=r("Example:"),qn=l(),u(mt.$$.fragment),this.h()},l(t){const h=cc('[data-svelte="svelte-1phssyn"]',document.head);f=n(h,"META",{name:!0,content:!0}),h.forEach(o),j=d(t),m=n(t,"H1",{class:!0});var ut=i(m);w=n(ut,"A",{id:!0,class:!0,href:!0});var Ao=i(w);L=n(Ao,"SPAN",{});var Ln=i(L);_(S.$$.fragment,Ln),Ln.forEach(o),Ao.forEach(o),B=d(ut),D=n(ut,"SPAN",{});var Dn=i(D);z=s(Dn,"Speech2Text2"),Dn.forEach(o),ut.forEach(o),b=d(t),C=n(t,"H2",{class:!0});var sr=i(C);q=n(sr,"A",{id:!0,class:!0,href:!0});var In=i(q);to=n(In,"SPAN",{});var Wn=i(to);_($e.$$.fragment,Wn),Wn.forEach(o),In.forEach(o),zr=d(sr),oo=n(sr,"SPAN",{});var Vn=i(oo);Cr=s(Vn,"Overview"),Vn.forEach(o),sr.forEach(o),Fo=d(t),R=n(t,"P",{});var Rt=i(R);Mr=s(Rt,"The Speech2Text2 model is used together with "),gt=n(Rt,"A",{href:!0});var Nn=i(gt);jr=s(Nn,"Wav2Vec2"),Nn.forEach(o),qr=s(Rt,` for Speech Translation models proposed in
`),Ee=n(Rt,"A",{href:!0,rel:!0});var Bn=i(Ee);Ar=s(Bn,"Large-Scale Self- and Semi-Supervised Learning for Speech Translation"),Bn.forEach(o),Fr=s(Rt,` by
Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.`),Rt.forEach(o),Lo=d(t),y=n(t,"P",{});var F=i(y);Lr=s(F,"Speech2Text2 is a "),ro=n(F,"EM",{});var On=i(ro);Dr=s(On,"decoder-only"),On.forEach(o),Ir=s(F," transformer model that can be used with any speech "),so=n(F,"EM",{});var Un=i(so);Wr=s(Un,"encoder-only"),Un.forEach(o),Vr=s(F,`, such as
`),vt=n(F,"A",{href:!0});var Hn=i(vt);Nr=s(Hn,"Wav2Vec2"),Hn.forEach(o),Br=s(F," or "),Tt=n(F,"A",{href:!0});var Rn=i(Tt);Or=s(Rn,"HuBERT"),Rn.forEach(o),Ur=s(F,` for Speech-to-Text tasks. Please refer to the
`),xt=n(F,"A",{href:!0});var Jn=i(xt);Hr=s(Jn,"SpeechEncoderDecoder"),Jn.forEach(o),Rr=s(F," class on how to combine Speech2Text2 with any speech "),ao=n(F,"EM",{});var Gn=i(ao);Jr=s(Gn,"encoder-only"),Gn.forEach(o),Gr=s(F,`
model.`),F.forEach(o),Do=d(t),ie=n(t,"P",{});var ar=i(ie);Kr=s(ar,"This model was contributed by "),Pe=n(ar,"A",{href:!0,rel:!0});var Kn=i(Pe);Qr=s(Kn,"Patrick von Platen"),Kn.forEach(o),Xr=s(ar,"."),ar.forEach(o),Io=d(t),ce=n(t,"P",{});var nr=i(ce);Yr=s(nr,"The original code can be found "),ze=n(nr,"A",{href:!0,rel:!0});var Qn=i(ze);Zr=s(Qn,"here"),Qn.forEach(o),es=s(nr,"."),nr.forEach(o),Wo=d(t),kt=n(t,"P",{});var Xn=i(kt);ts=s(Xn,"Tips:"),Xn.forEach(o),Vo=d(t),J=n(t,"UL",{});var Jt=i(J);Ce=n(Jt,"LI",{});var ir=i(Ce);os=s(ir,`Speech2Text2 achieves state-of-the-art results on the CoVoST Speech Translation dataset. For more information, see
the `),Me=n(ir,"A",{href:!0,rel:!0});var Yn=i(Me);rs=s(Yn,"official models"),Yn.forEach(o),ss=s(ir," ."),ir.forEach(o),as=d(Jt),je=n(Jt,"LI",{});var cr=i(je);ns=s(cr,"Speech2Text2 is always used within the "),bt=n(cr,"A",{href:!0});var Zn=i(bt);is=s(Zn,"SpeechEncoderDecoder"),Zn.forEach(o),cs=s(cr," framework."),cr.forEach(o),ls=d(Jt),qe=n(Jt,"LI",{});var lr=i(qe);ds=s(lr,"Speech2Text2\u2019s tokenizer is based on "),Ae=n(lr,"A",{href:!0,rel:!0});var ei=i(Ae);hs=s(ei,"fastBPE"),ei.forEach(o),ps=s(lr,"."),lr.forEach(o),Jt.forEach(o),No=d(t),ee=n(t,"H2",{class:!0});var dr=i(ee);le=n(dr,"A",{id:!0,class:!0,href:!0});var ti=i(le);no=n(ti,"SPAN",{});var oi=i(no);_(Fe.$$.fragment,oi),oi.forEach(o),ti.forEach(o),fs=d(dr),io=n(dr,"SPAN",{});var ri=i(io);ms=s(ri,"Inference"),ri.forEach(o),dr.forEach(o),Bo=d(t),G=n(t,"P",{});var Gt=i(G);us=s(Gt,"Speech2Text2\u2019s "),wt=n(Gt,"A",{href:!0});var si=i(wt);_s=s(si,"SpeechEncoderDecoderModel"),si.forEach(o),gs=s(Gt,` model accepts raw waveform input values from speech and
makes use of `),St=n(Gt,"A",{href:!0});var ai=i(St);vs=s(ai,"generate()"),ai.forEach(o),Ts=s(Gt,` to translate the input speech
autoregressively to the target language.`),Gt.forEach(o),Oo=d(t),P=n(t,"P",{});var V=i(P);xs=s(V,"The "),yt=n(V,"A",{href:!0});var ni=i(yt);ks=s(ni,"Wav2Vec2FeatureExtractor"),ni.forEach(o),bs=s(V,` class is responsible for preprocessing the input speech and
`),$t=n(V,"A",{href:!0});var ii=i($t);ws=s(ii,"Speech2Text2Tokenizer"),ii.forEach(o),Ss=s(V,` decodes the generated target tokens to the target string. The
`),Et=n(V,"A",{href:!0});var ci=i(Et);ys=s(ci,"Speech2Text2Processor"),ci.forEach(o),$s=s(V," wraps "),Pt=n(V,"A",{href:!0});var li=i(Pt);Es=s(li,"Wav2Vec2FeatureExtractor"),li.forEach(o),Ps=s(V,` and
`),zt=n(V,"A",{href:!0});var di=i(zt);zs=s(di,"Speech2Text2Tokenizer"),di.forEach(o),Cs=s(V,` into a single instance to both extract the input features and decode the
predicted token ids.`),V.forEach(o),Uo=d(t),Ct=n(t,"UL",{});var hi=i(Ct);co=n(hi,"LI",{});var pi=i(co);Ms=s(pi,"Step-by-step Speech Translation"),pi.forEach(o),hi.forEach(o),Ho=d(t),_(Le.$$.fragment,t),Ro=d(t),Mt=n(t,"UL",{});var fi=i(Mt);De=n(fi,"LI",{});var hr=i(De);lo=n(hr,"P",{});var mi=i(lo);js=s(mi,"Speech Translation via Pipelines"),mi.forEach(o),qs=d(hr),ho=n(hr,"P",{});var ui=i(ho);As=s(ui,"The automatic speech recognition pipeline can also be used to translate speech in just a couple lines of code"),ui.forEach(o),hr.forEach(o),fi.forEach(o),Jo=d(t),_(Ie.$$.fragment,t),Go=d(t),de=n(t,"P",{});var pr=i(de);Fs=s(pr,"See "),We=n(pr,"A",{href:!0,rel:!0});var _i=i(We);Ls=s(_i,"model hub"),_i.forEach(o),Ds=s(pr," to look for Speech2Text2 checkpoints."),pr.forEach(o),Ko=d(t),te=n(t,"H2",{class:!0});var fr=i(te);he=n(fr,"A",{id:!0,class:!0,href:!0});var gi=i(he);po=n(gi,"SPAN",{});var vi=i(po);_(Ve.$$.fragment,vi),vi.forEach(o),gi.forEach(o),Is=d(fr),fo=n(fr,"SPAN",{});var Ti=i(fo);Ws=s(Ti,"Speech2Text2Config"),Ti.forEach(o),fr.forEach(o),Qo=d(t),M=n(t,"DIV",{class:!0});var Z=i(M);_(Ne.$$.fragment,Z),Vs=d(Z),oe=n(Z,"P",{});var Kt=i(oe);Ns=s(Kt,"This is the configuration class to store the configuration of a "),jt=n(Kt,"A",{href:!0});var xi=i(jt);Bs=s(xi,"Speech2Text2ForCausalLM"),xi.forEach(o),Os=s(Kt,`. It is used to
instantiate an Speech2Text2 model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Speech2Text2
`),Be=n(Kt,"A",{href:!0,rel:!0});var ki=i(Be);Us=s(ki,"facebook/s2t-small-librispeech-asr"),ki.forEach(o),Hs=s(Kt," architecture."),Kt.forEach(o),Rs=d(Z),re=n(Z,"P",{});var Qt=i(re);Js=s(Qt,"Configuration objects inherit from "),qt=n(Qt,"A",{href:!0});var bi=i(qt);Gs=s(bi,"PretrainedConfig"),bi.forEach(o),Ks=s(Qt,` and can be used to control the model outputs. Read the
documentation from `),At=n(Qt,"A",{href:!0});var wi=i(At);Qs=s(wi,"PretrainedConfig"),wi.forEach(o),Xs=s(Qt," for more information."),Qt.forEach(o),Ys=d(Z),mo=n(Z,"P",{});var Si=i(mo);Zs=s(Si,"Example:"),Si.forEach(o),ea=d(Z),_(Oe.$$.fragment,Z),Z.forEach(o),Xo=d(t),se=n(t,"H2",{class:!0});var mr=i(se);pe=n(mr,"A",{id:!0,class:!0,href:!0});var yi=i(pe);uo=n(yi,"SPAN",{});var $i=i(uo);_(Ue.$$.fragment,$i),$i.forEach(o),yi.forEach(o),ta=d(mr),_o=n(mr,"SPAN",{});var Ei=i(_o);oa=s(Ei,"Speech2TextTokenizer"),Ei.forEach(o),mr.forEach(o),Yo=d(t),E=n(t,"DIV",{class:!0});var N=i(E);_(He.$$.fragment,N),ra=d(N),go=n(N,"P",{});var Pi=i(go);sa=s(Pi,"Constructs a Speech2Text2Tokenizer."),Pi.forEach(o),aa=d(N),Re=n(N,"P",{});var ur=i(Re);na=s(ur,"This tokenizer inherits from "),Ft=n(ur,"A",{href:!0});var zi=i(Ft);ia=s(zi,"PreTrainedTokenizer"),zi.forEach(o),ca=s(ur,` which contains some of the main methods. Users should refer to
the superclass for more information regarding such methods.`),ur.forEach(o),la=d(N),fe=n(N,"DIV",{class:!0});var _r=i(fe);_(Je.$$.fragment,_r),da=d(_r),vo=n(_r,"P",{});var Ci=i(vo);ha=s(Ci,"Convert a list of lists of token ids into a list of strings by calling decode."),Ci.forEach(o),_r.forEach(o),pa=d(N),K=n(N,"DIV",{class:!0});var Xt=i(K);_(Ge.$$.fragment,Xt),fa=d(Xt),To=n(Xt,"P",{});var Mi=i(To);ma=s(Mi,`Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special
tokens and clean up tokenization spaces.`),Mi.forEach(o),ua=d(Xt),Ke=n(Xt,"P",{});var gr=i(Ke);_a=s(gr,"Similar to doing "),xo=n(gr,"CODE",{});var ji=i(xo);ga=s(ji,"self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))"),ji.forEach(o),va=s(gr,"."),gr.forEach(o),Xt.forEach(o),Ta=d(N),ko=n(N,"DIV",{class:!0}),i(ko).forEach(o),N.forEach(o),Zo=d(t),ae=n(t,"H2",{class:!0});var vr=i(ae);me=n(vr,"A",{id:!0,class:!0,href:!0});var qi=i(me);bo=n(qi,"SPAN",{});var Ai=i(bo);_(Qe.$$.fragment,Ai),Ai.forEach(o),qi.forEach(o),xa=d(vr),wo=n(vr,"SPAN",{});var Fi=i(wo);ka=s(Fi,"Speech2Text2Processor"),Fi.forEach(o),vr.forEach(o),er=d(t),k=n(t,"DIV",{class:!0});var $=i(k);_(Xe.$$.fragment,$),ba=d($),So=n($,"P",{});var Li=i(So);wa=s(Li,`Constructs a Speech2Text2 processor which wraps a Speech2Text2 feature extractor and a Speech2Text2 tokenizer into
a single processor.`),Li.forEach(o),Sa=d($),A=n($,"P",{});var H=i(A);Lt=n(H,"A",{href:!0});var Di=i(Lt);ya=s(Di,"Speech2Text2Processor"),Di.forEach(o),$a=s(H," offers all the functionalities of "),Dt=n(H,"A",{href:!0});var Ii=i(Dt);Ea=s(Ii,"AutoFeatureExtractor"),Ii.forEach(o),Pa=s(H," and "),It=n(H,"A",{href:!0});var Wi=i(It);za=s(Wi,"Speech2Text2Tokenizer"),Wi.forEach(o),Ca=s(H,`.
See the `),Ye=n(H,"A",{href:!0});var An=i(Ye);yo=n(An,"STRONG",{});var Vi=i(yo);Ma=s(Vi,"call"),Vi.forEach(o),ja=s(An,"()"),An.forEach(o),qa=s(H," and "),Wt=n(H,"A",{href:!0});var Ni=i(Wt);Aa=s(Ni,"decode()"),Ni.forEach(o),Fa=s(H," for more information."),H.forEach(o),La=d($),ue=n($,"DIV",{class:!0});var Tr=i(ue);_(Ze.$$.fragment,Tr),Da=d(Tr),O=n(Tr,"P",{});var be=i(O);Ia=s(be,`When used in normal mode, this method forwards all its arguments to AutoFeatureExtractor\u2019s
`),$o=n(be,"CODE",{});var Bi=i($o);Wa=s(Bi,"__call__()"),Bi.forEach(o),Va=s(be,`and returns its output. If used in the context
`),Vt=n(be,"A",{href:!0});var Oi=i(Vt);Na=s(Oi,"as_target_processor()"),Oi.forEach(o),Ba=s(be,` this method forwards all its arguments to
Speech2Text2Tokenizer\u2019s `),et=n(be,"A",{href:!0});var Fn=i(et);Eo=n(Fn,"STRONG",{});var Ui=i(Eo);Oa=s(Ui,"call"),Ui.forEach(o),Ua=s(Fn,"()"),Fn.forEach(o),Ha=s(be,`. Please refer to the doctsring of the above two
methods for more information.`),be.forEach(o),Tr.forEach(o),Ra=d($),Q=n($,"DIV",{class:!0});var Yt=i(Q);_(tt.$$.fragment,Yt),Ja=d(Yt),Po=n(Yt,"P",{});var Hi=i(Po);Ga=s(Hi,"Instantiate a processor associated with a pretrained model."),Hi.forEach(o),Ka=d(Yt),_(_e.$$.fragment,Yt),Yt.forEach(o),Qa=d($),X=n($,"DIV",{class:!0});var Zt=i(X);_(ot.$$.fragment,Zt),Xa=d(Zt),rt=n(Zt,"P",{});var xr=i(rt);Ya=s(xr,`Saves the attributes of this processor (feature extractor, tokenizer\u2026) in the specified directory so that it
can be reloaded using the `),Nt=n(xr,"A",{href:!0});var Ri=i(Nt);Za=s(Ri,"from_pretrained()"),Ri.forEach(o),en=s(xr," method."),xr.forEach(o),tn=d(Zt),_(ge.$$.fragment,Zt),Zt.forEach(o),on=d($),ve=n($,"DIV",{class:!0});var kr=i(ve);_(st.$$.fragment,kr),rn=d(kr),at=n(kr,"P",{});var br=i(at);sn=s(br,"This method forwards all its arguments to Speech2Text2Tokenizer\u2019s "),Bt=n(br,"A",{href:!0});var Ji=i(Bt);an=s(Ji,"batch_decode()"),Ji.forEach(o),nn=s(br,`. Please
refer to the docstring of this method for more information.`),br.forEach(o),kr.forEach(o),cn=d($),Te=n($,"DIV",{class:!0});var wr=i(Te);_(nt.$$.fragment,wr),ln=d(wr),it=n(wr,"P",{});var Sr=i(it);dn=s(Sr,"This method forwards all its arguments to Speech2Text2Tokenizer\u2019s "),Ot=n(Sr,"A",{href:!0});var Gi=i(Ot);hn=s(Gi,"decode()"),Gi.forEach(o),pn=s(Sr,`. Please refer
to the docstring of this method for more information.`),Sr.forEach(o),wr.forEach(o),fn=d($),xe=n($,"DIV",{class:!0});var yr=i(xe);_(ct.$$.fragment,yr),mn=d(yr),zo=n(yr,"P",{});var Ki=i(zo);un=s(Ki,`Temporarily sets the tokenizer for processing the input. Useful for encoding the labels when fine-tuning
Speech2Text2.`),Ki.forEach(o),yr.forEach(o),$.forEach(o),tr=d(t),ne=n(t,"H2",{class:!0});var $r=i(ne);ke=n($r,"A",{id:!0,class:!0,href:!0});var Qi=i(ke);Co=n(Qi,"SPAN",{});var Xi=i(Co);_(lt.$$.fragment,Xi),Xi.forEach(o),Qi.forEach(o),_n=d($r),Mo=n($r,"SPAN",{});var Yi=i(Mo);gn=s(Yi,"Speech2Text2ForCausalLM"),Yi.forEach(o),$r.forEach(o),or=d(t),I=n(t,"DIV",{class:!0});var we=i(I);_(dt.$$.fragment,we),vn=d(we),U=n(we,"P",{});var Se=i(U);Tn=s(Se,"The Speech2Text2 Decoder with a language modeling head. Can be used as the decoder part of "),Ut=n(Se,"A",{href:!0});var Zi=i(Ut);xn=s(Zi,"EncoderDecoderModel"),Zi.forEach(o),kn=s(Se," and "),jo=n(Se,"CODE",{});var ec=i(jo);bn=s(ec,"SpeechEncoderDecoder"),ec.forEach(o),wn=s(Se,`.
This model inherits from `),Ht=n(Se,"A",{href:!0});var tc=i(Ht);Sn=s(tc,"PreTrainedModel"),tc.forEach(o),yn=s(Se,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Se.forEach(o),$n=d(we),ht=n(we,"P",{});var Er=i(ht);En=s(Er,"This model is also a PyTorch "),pt=n(Er,"A",{href:!0,rel:!0});var oc=i(pt);Pn=s(oc,"torch.nn.Module"),oc.forEach(o),zn=s(Er,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Er.forEach(o),Cn=d(we),Y=n(we,"DIV",{class:!0});var eo=i(Y);_(ft.$$.fragment,eo),Mn=d(eo),qo=n(eo,"P",{});var rc=i(qo);jn=s(rc,"Example:"),rc.forEach(o),qn=d(eo),_(mt.$$.fragment,eo),eo.forEach(o),we.forEach(o),this.h()},h(){c(f,"name","hf:doc:metadata"),c(f,"content",JSON.stringify(pc)),c(w,"id","speech2text2"),c(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(w,"href","#speech2text2"),c(m,"class","relative group"),c(q,"id","overview"),c(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(q,"href","#overview"),c(C,"class","relative group"),c(gt,"href","wav2vec2"),c(Ee,"href","https://arxiv.org/abs/2104.06678"),c(Ee,"rel","nofollow"),c(vt,"href","wav2vec2"),c(Tt,"href","hubert"),c(xt,"href","speech-encoder-decoder"),c(Pe,"href","https://huggingface.co/patrickvonplaten"),c(Pe,"rel","nofollow"),c(ze,"href","https://github.com/pytorch/fairseq/blob/1f7ef9ed1e1061f8c7f88f8b94c7186834398690/fairseq/models/wav2vec/wav2vec2_asr.py#L266"),c(ze,"rel","nofollow"),c(Me,"href","https://huggingface.co/models?other=speech2text2"),c(Me,"rel","nofollow"),c(bt,"href","speech-encoder-decoder"),c(Ae,"href","https://github.com/glample/fastBPE"),c(Ae,"rel","nofollow"),c(le,"id","inference"),c(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(le,"href","#inference"),c(ee,"class","relative group"),c(wt,"href","/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel"),c(St,"href","/docs/transformers/master/en/main_classes/model#transformers.generation_utils.GenerationMixin.generate"),c(yt,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c($t,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),c(Et,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),c(Pt,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(zt,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),c(We,"href","https://huggingface.co/models?filter=speech2text2"),c(We,"rel","nofollow"),c(he,"id","transformers.Speech2Text2Config"),c(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(he,"href","#transformers.Speech2Text2Config"),c(te,"class","relative group"),c(jt,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM"),c(Be,"href","https://huggingface.co/facebook/s2t-small-librispeech-asr"),c(Be,"rel","nofollow"),c(qt,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),c(At,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),c(M,"class","docstring"),c(pe,"id","transformers.Speech2Text2Tokenizer"),c(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pe,"href","#transformers.Speech2Text2Tokenizer"),c(se,"class","relative group"),c(Ft,"href","/docs/transformers/master/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(fe,"class","docstring"),c(K,"class","docstring"),c(ko,"class","docstring"),c(E,"class","docstring"),c(me,"id","transformers.Speech2Text2Processor"),c(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(me,"href","#transformers.Speech2Text2Processor"),c(ae,"class","relative group"),c(Lt,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),c(Dt,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoFeatureExtractor"),c(It,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),c(Ye,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor.__call__"),c(Wt,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor.decode"),c(Vt,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor.as_target_processor"),c(et,"href","/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__"),c(ue,"class","docstring"),c(Q,"class","docstring"),c(Nt,"href","/docs/transformers/master/en/main_classes/processors#transformers.ProcessorMixin.from_pretrained"),c(X,"class","docstring"),c(Bt,"href","/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode"),c(ve,"class","docstring"),c(Ot,"href","/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode"),c(Te,"class","docstring"),c(xe,"class","docstring"),c(k,"class","docstring"),c(ke,"id","transformers.Speech2Text2ForCausalLM"),c(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ke,"href","#transformers.Speech2Text2ForCausalLM"),c(ne,"class","relative group"),c(Ut,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel"),c(Ht,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),c(pt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(pt,"rel","nofollow"),c(Y,"class","docstring"),c(I,"class","docstring")},m(t,h){e(document.head,f),p(t,j,h),p(t,m,h),e(m,w),e(w,L),g(S,L,null),e(m,B),e(m,D),e(D,z),p(t,b,h),p(t,C,h),e(C,q),e(q,to),g($e,to,null),e(C,zr),e(C,oo),e(oo,Cr),p(t,Fo,h),p(t,R,h),e(R,Mr),e(R,gt),e(gt,jr),e(R,qr),e(R,Ee),e(Ee,Ar),e(R,Fr),p(t,Lo,h),p(t,y,h),e(y,Lr),e(y,ro),e(ro,Dr),e(y,Ir),e(y,so),e(so,Wr),e(y,Vr),e(y,vt),e(vt,Nr),e(y,Br),e(y,Tt),e(Tt,Or),e(y,Ur),e(y,xt),e(xt,Hr),e(y,Rr),e(y,ao),e(ao,Jr),e(y,Gr),p(t,Do,h),p(t,ie,h),e(ie,Kr),e(ie,Pe),e(Pe,Qr),e(ie,Xr),p(t,Io,h),p(t,ce,h),e(ce,Yr),e(ce,ze),e(ze,Zr),e(ce,es),p(t,Wo,h),p(t,kt,h),e(kt,ts),p(t,Vo,h),p(t,J,h),e(J,Ce),e(Ce,os),e(Ce,Me),e(Me,rs),e(Ce,ss),e(J,as),e(J,je),e(je,ns),e(je,bt),e(bt,is),e(je,cs),e(J,ls),e(J,qe),e(qe,ds),e(qe,Ae),e(Ae,hs),e(qe,ps),p(t,No,h),p(t,ee,h),e(ee,le),e(le,no),g(Fe,no,null),e(ee,fs),e(ee,io),e(io,ms),p(t,Bo,h),p(t,G,h),e(G,us),e(G,wt),e(wt,_s),e(G,gs),e(G,St),e(St,vs),e(G,Ts),p(t,Oo,h),p(t,P,h),e(P,xs),e(P,yt),e(yt,ks),e(P,bs),e(P,$t),e($t,ws),e(P,Ss),e(P,Et),e(Et,ys),e(P,$s),e(P,Pt),e(Pt,Es),e(P,Ps),e(P,zt),e(zt,zs),e(P,Cs),p(t,Uo,h),p(t,Ct,h),e(Ct,co),e(co,Ms),p(t,Ho,h),g(Le,t,h),p(t,Ro,h),p(t,Mt,h),e(Mt,De),e(De,lo),e(lo,js),e(De,qs),e(De,ho),e(ho,As),p(t,Jo,h),g(Ie,t,h),p(t,Go,h),p(t,de,h),e(de,Fs),e(de,We),e(We,Ls),e(de,Ds),p(t,Ko,h),p(t,te,h),e(te,he),e(he,po),g(Ve,po,null),e(te,Is),e(te,fo),e(fo,Ws),p(t,Qo,h),p(t,M,h),g(Ne,M,null),e(M,Vs),e(M,oe),e(oe,Ns),e(oe,jt),e(jt,Bs),e(oe,Os),e(oe,Be),e(Be,Us),e(oe,Hs),e(M,Rs),e(M,re),e(re,Js),e(re,qt),e(qt,Gs),e(re,Ks),e(re,At),e(At,Qs),e(re,Xs),e(M,Ys),e(M,mo),e(mo,Zs),e(M,ea),g(Oe,M,null),p(t,Xo,h),p(t,se,h),e(se,pe),e(pe,uo),g(Ue,uo,null),e(se,ta),e(se,_o),e(_o,oa),p(t,Yo,h),p(t,E,h),g(He,E,null),e(E,ra),e(E,go),e(go,sa),e(E,aa),e(E,Re),e(Re,na),e(Re,Ft),e(Ft,ia),e(Re,ca),e(E,la),e(E,fe),g(Je,fe,null),e(fe,da),e(fe,vo),e(vo,ha),e(E,pa),e(E,K),g(Ge,K,null),e(K,fa),e(K,To),e(To,ma),e(K,ua),e(K,Ke),e(Ke,_a),e(Ke,xo),e(xo,ga),e(Ke,va),e(E,Ta),e(E,ko),p(t,Zo,h),p(t,ae,h),e(ae,me),e(me,bo),g(Qe,bo,null),e(ae,xa),e(ae,wo),e(wo,ka),p(t,er,h),p(t,k,h),g(Xe,k,null),e(k,ba),e(k,So),e(So,wa),e(k,Sa),e(k,A),e(A,Lt),e(Lt,ya),e(A,$a),e(A,Dt),e(Dt,Ea),e(A,Pa),e(A,It),e(It,za),e(A,Ca),e(A,Ye),e(Ye,yo),e(yo,Ma),e(Ye,ja),e(A,qa),e(A,Wt),e(Wt,Aa),e(A,Fa),e(k,La),e(k,ue),g(Ze,ue,null),e(ue,Da),e(ue,O),e(O,Ia),e(O,$o),e($o,Wa),e(O,Va),e(O,Vt),e(Vt,Na),e(O,Ba),e(O,et),e(et,Eo),e(Eo,Oa),e(et,Ua),e(O,Ha),e(k,Ra),e(k,Q),g(tt,Q,null),e(Q,Ja),e(Q,Po),e(Po,Ga),e(Q,Ka),g(_e,Q,null),e(k,Qa),e(k,X),g(ot,X,null),e(X,Xa),e(X,rt),e(rt,Ya),e(rt,Nt),e(Nt,Za),e(rt,en),e(X,tn),g(ge,X,null),e(k,on),e(k,ve),g(st,ve,null),e(ve,rn),e(ve,at),e(at,sn),e(at,Bt),e(Bt,an),e(at,nn),e(k,cn),e(k,Te),g(nt,Te,null),e(Te,ln),e(Te,it),e(it,dn),e(it,Ot),e(Ot,hn),e(it,pn),e(k,fn),e(k,xe),g(ct,xe,null),e(xe,mn),e(xe,zo),e(zo,un),p(t,tr,h),p(t,ne,h),e(ne,ke),e(ke,Co),g(lt,Co,null),e(ne,_n),e(ne,Mo),e(Mo,gn),p(t,or,h),p(t,I,h),g(dt,I,null),e(I,vn),e(I,U),e(U,Tn),e(U,Ut),e(Ut,xn),e(U,kn),e(U,jo),e(jo,bn),e(U,wn),e(U,Ht),e(Ht,Sn),e(U,yn),e(I,$n),e(I,ht),e(ht,En),e(ht,pt),e(pt,Pn),e(ht,zn),e(I,Cn),e(I,Y),g(ft,Y,null),e(Y,Mn),e(Y,qo),e(qo,jn),e(Y,qn),g(mt,Y,null),rr=!0},p(t,[h]){const ut={};h&2&&(ut.$$scope={dirty:h,ctx:t}),_e.$set(ut);const Ao={};h&2&&(Ao.$$scope={dirty:h,ctx:t}),ge.$set(Ao)},i(t){rr||(v(S.$$.fragment,t),v($e.$$.fragment,t),v(Fe.$$.fragment,t),v(Le.$$.fragment,t),v(Ie.$$.fragment,t),v(Ve.$$.fragment,t),v(Ne.$$.fragment,t),v(Oe.$$.fragment,t),v(Ue.$$.fragment,t),v(He.$$.fragment,t),v(Je.$$.fragment,t),v(Ge.$$.fragment,t),v(Qe.$$.fragment,t),v(Xe.$$.fragment,t),v(Ze.$$.fragment,t),v(tt.$$.fragment,t),v(_e.$$.fragment,t),v(ot.$$.fragment,t),v(ge.$$.fragment,t),v(st.$$.fragment,t),v(nt.$$.fragment,t),v(ct.$$.fragment,t),v(lt.$$.fragment,t),v(dt.$$.fragment,t),v(ft.$$.fragment,t),v(mt.$$.fragment,t),rr=!0)},o(t){T(S.$$.fragment,t),T($e.$$.fragment,t),T(Fe.$$.fragment,t),T(Le.$$.fragment,t),T(Ie.$$.fragment,t),T(Ve.$$.fragment,t),T(Ne.$$.fragment,t),T(Oe.$$.fragment,t),T(Ue.$$.fragment,t),T(He.$$.fragment,t),T(Je.$$.fragment,t),T(Ge.$$.fragment,t),T(Qe.$$.fragment,t),T(Xe.$$.fragment,t),T(Ze.$$.fragment,t),T(tt.$$.fragment,t),T(_e.$$.fragment,t),T(ot.$$.fragment,t),T(ge.$$.fragment,t),T(st.$$.fragment,t),T(nt.$$.fragment,t),T(ct.$$.fragment,t),T(lt.$$.fragment,t),T(dt.$$.fragment,t),T(ft.$$.fragment,t),T(mt.$$.fragment,t),rr=!1},d(t){o(f),t&&o(j),t&&o(m),x(S),t&&o(b),t&&o(C),x($e),t&&o(Fo),t&&o(R),t&&o(Lo),t&&o(y),t&&o(Do),t&&o(ie),t&&o(Io),t&&o(ce),t&&o(Wo),t&&o(kt),t&&o(Vo),t&&o(J),t&&o(No),t&&o(ee),x(Fe),t&&o(Bo),t&&o(G),t&&o(Oo),t&&o(P),t&&o(Uo),t&&o(Ct),t&&o(Ho),x(Le,t),t&&o(Ro),t&&o(Mt),t&&o(Jo),x(Ie,t),t&&o(Go),t&&o(de),t&&o(Ko),t&&o(te),x(Ve),t&&o(Qo),t&&o(M),x(Ne),x(Oe),t&&o(Xo),t&&o(se),x(Ue),t&&o(Yo),t&&o(E),x(He),x(Je),x(Ge),t&&o(Zo),t&&o(ae),x(Qe),t&&o(er),t&&o(k),x(Xe),x(Ze),x(tt),x(_e),x(ot),x(ge),x(st),x(nt),x(ct),t&&o(tr),t&&o(ne),x(lt),t&&o(or),t&&o(I),x(dt),x(ft),x(mt)}}}const pc={local:"speech2text2",sections:[{local:"overview",title:"Overview"},{local:"inference",title:"Inference"},{local:"transformers.Speech2Text2Config",title:"Speech2Text2Config"},{local:"transformers.Speech2Text2Tokenizer",title:"Speech2TextTokenizer"},{local:"transformers.Speech2Text2Processor",title:"Speech2Text2Processor"},{local:"transformers.Speech2Text2ForCausalLM",title:"Speech2Text2ForCausalLM"}],title:"Speech2Text2"};function fc(ye,f,j){let{fw:m}=f;return ye.$$set=w=>{"fw"in w&&j(0,m=w.fw)},[m]}class xc extends ac{constructor(f){super();nc(this,f,fc,hc,ic,{fw:0})}}export{xc as default,pc as metadata};
