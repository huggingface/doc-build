import{S as Zh,i as el,s as tl,e as n,k as c,w as f,t as s,M as ol,c as r,d as o,m as h,a as d,x as _,h as a,b as i,F as e,g as p,y as g,q as v,o as k,B as T}from"../../chunks/vendor-4833417e.js";import{T as Rn}from"../../chunks/Tip-fffd6df1.js";import{D as z}from"../../chunks/Docstring-4f315ed9.js";import{C as Go}from"../../chunks/CodeBlock-6a3d1b46.js";import{I as re}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-dacfbfaf.js";function nl(I){let u,P,m,y,N;return{c(){u=n("p"),P=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),y=s("Module"),N=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(w){u=r(w,"P",{});var b=d(u);P=a(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(b,"CODE",{});var q=d(m);y=a(q,"Module"),q.forEach(o),N=a(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(w,b){p(w,u,b),e(u,P),e(u,m),e(m,y),e(u,N)},d(w){w&&o(u)}}}function rl(I){let u,P,m,y,N;return{c(){u=n("p"),P=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),y=s("Module"),N=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(w){u=r(w,"P",{});var b=d(u);P=a(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(b,"CODE",{});var q=d(m);y=a(q,"Module"),q.forEach(o),N=a(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(w,b){p(w,u,b),e(u,P),e(u,m),e(m,y),e(u,N)},d(w){w&&o(u)}}}function sl(I){let u,P,m,y,N;return{c(){u=n("p"),P=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),y=s("Module"),N=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(w){u=r(w,"P",{});var b=d(u);P=a(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(b,"CODE",{});var q=d(m);y=a(q,"Module"),q.forEach(o),N=a(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(w,b){p(w,u,b),e(u,P),e(u,m),e(m,y),e(u,N)},d(w){w&&o(u)}}}function al(I){let u,P,m,y,N;return{c(){u=n("p"),P=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),y=s("Module"),N=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(w){u=r(w,"P",{});var b=d(u);P=a(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(b,"CODE",{});var q=d(m);y=a(q,"Module"),q.forEach(o),N=a(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(w,b){p(w,u,b),e(u,P),e(u,m),e(m,y),e(u,N)},d(w){w&&o(u)}}}function dl(I){let u,P,m,y,N;return{c(){u=n("p"),P=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),y=s("Module"),N=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(w){u=r(w,"P",{});var b=d(u);P=a(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(b,"CODE",{});var q=d(m);y=a(q,"Module"),q.forEach(o),N=a(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(w,b){p(w,u,b),e(u,P),e(u,m),e(m,y),e(u,N)},d(w){w&&o(u)}}}function il(I){let u,P,m,y,N,w,b,q,Xr,Jn,se,Wo,Zr,es,Qe,ts,os,Yn,ae,Ee,Ho,Xe,ns,Vo,rs,Kn,xe,ss,Ze,as,ds,Qn,ao,is,Xn,io,cs,Zn,co,Uo,hs,er,Ce,ls,et,ps,us,tr,de,Se,Ro,tt,ms,Jo,fs,or,U,ot,_s,nt,gs,ho,vs,ks,Ts,ie,bs,lo,ws,ys,po,Ps,Ns,nr,ce,De,Yo,rt,qs,Ko,zs,rr,$,st,$s,Qo,Fs,Ms,at,Es,uo,xs,Cs,Ss,R,dt,Ds,Xo,Os,Ls,it,mo,js,Zo,As,Is,fo,Bs,en,Gs,Ws,Oe,ct,Hs,tn,Vs,Us,W,ht,Rs,on,Js,Ys,lt,Ks,he,Qs,nn,Xs,Zs,rn,ea,ta,oa,Le,pt,na,ut,ra,sn,sa,aa,sr,le,je,an,mt,da,dn,ia,ar,pe,ft,ca,cn,ha,dr,ue,_t,la,hn,pa,ir,me,gt,ua,ln,ma,cr,fe,vt,fa,pn,_a,hr,_e,Ae,un,kt,ga,mn,va,lr,E,Tt,ka,bt,Ta,_o,ba,wa,ya,ge,Pa,wt,Na,qa,fn,za,$a,Fa,yt,Ma,Pt,Ea,xa,Ca,S,Nt,Sa,ve,Da,go,Oa,La,_n,ja,Aa,Ia,Ie,Ba,gn,Ga,Wa,qt,pr,ke,Be,vn,zt,Ha,kn,Va,ur,F,$t,Ua,Ft,Ra,vo,Ja,Ya,Ka,Te,Qa,Mt,Xa,Za,Tn,ed,td,od,Et,nd,xt,rd,sd,ad,B,dd,bn,id,cd,wn,hd,ld,yn,pd,ud,ko,md,fd,_d,D,Ct,gd,be,vd,To,kd,Td,Pn,bd,wd,yd,Ge,Pd,Nn,Nd,qd,St,mr,we,We,qn,Dt,zd,zn,$d,fr,M,Ot,Fd,Lt,Md,bo,Ed,xd,Cd,ye,Sd,jt,Dd,Od,$n,Ld,jd,Ad,At,Id,It,Bd,Gd,Wd,G,Hd,Fn,Vd,Ud,Mn,Rd,Jd,En,Yd,Kd,wo,Qd,Xd,Zd,O,Bt,ei,Pe,ti,yo,oi,ni,xn,ri,si,ai,He,di,Cn,ii,ci,Gt,_r,Ne,Ve,Sn,Wt,hi,Dn,li,gr,x,Ht,pi,Vt,ui,Po,mi,fi,_i,qe,gi,Ut,vi,ki,On,Ti,bi,wi,Rt,yi,Jt,Pi,Ni,qi,L,Yt,zi,ze,$i,No,Fi,Mi,Ln,Ei,xi,Ci,Ue,Si,jn,Di,Oi,Kt,vr,$e,Re,An,Qt,Li,In,ji,kr,C,Xt,Ai,Zt,Ii,qo,Bi,Gi,Wi,Fe,Hi,eo,Vi,Ui,Bn,Ri,Ji,Yi,to,Ki,oo,Qi,Xi,Zi,j,no,ec,Me,tc,zo,oc,nc,Gn,rc,sc,ac,Je,dc,Wn,ic,cc,ro,Tr;return w=new re({}),Xe=new re({}),tt=new re({}),ot=new z({props:{name:"class transformers.ProphetNetConfig",anchor:"transformers.ProphetNetConfig",parameters:[{name:"activation_dropout",val:" = 0.1"},{name:"activation_function",val:" = 'gelu'"},{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 1024"},{name:"encoder_ffn_dim",val:" = 4096"},{name:"num_encoder_layers",val:" = 12"},{name:"num_encoder_attention_heads",val:" = 16"},{name:"decoder_ffn_dim",val:" = 4096"},{name:"num_decoder_layers",val:" = 12"},{name:"num_decoder_attention_heads",val:" = 16"},{name:"attention_dropout",val:" = 0.1"},{name:"dropout",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"init_std",val:" = 0.02"},{name:"is_encoder_decoder",val:" = True"},{name:"add_cross_attention",val:" = True"},{name:"decoder_start_token_id",val:" = 0"},{name:"ngram",val:" = 2"},{name:"num_buckets",val:" = 32"},{name:"relative_max_distance",val:" = 128"},{name:"disable_ngram_loss",val:" = False"},{name:"eps",val:" = 0.0"},{name:"use_cache",val:" = True"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 1"},{name:"eos_token_id",val:" = 2"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/configuration_prophetnet.py#L29",parametersDescription:[{anchor:"transformers.ProphetNetConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for activations inside the fully connected layer.`,name:"activation_dropout"},{anchor:"transformers.ProphetNetConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.ProphetNetConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the ProphetNET model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a>.`,name:"vocab_size"},{anchor:"transformers.ProphetNetConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of the layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.ProphetNetConfig.encoder_ffn_dim",description:`<strong>encoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"encoder_ffn_dim"},{anchor:"transformers.ProphetNetConfig.num_encoder_layers",description:`<strong>num_encoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of encoder layers.`,name:"num_encoder_layers"},{anchor:"transformers.ProphetNetConfig.num_encoder_attention_heads",description:`<strong>num_encoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_encoder_attention_heads"},{anchor:"transformers.ProphetNetConfig.decoder_ffn_dim",description:`<strong>decoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the <code>intermediate</code> (often named feed-forward) layer in decoder.`,name:"decoder_ffn_dim"},{anchor:"transformers.ProphetNetConfig.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of decoder layers.`,name:"num_decoder_layers"},{anchor:"transformers.ProphetNetConfig.num_decoder_attention_heads",description:`<strong>num_decoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer decoder.`,name:"num_decoder_attention_heads"},{anchor:"transformers.ProphetNetConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.ProphetNetConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.ProphetNetConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.ProphetNetConfig.init_std",description:`<strong>init_std</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"init_std"},{anchor:"transformers.ProphetNetConfig.add_cross_attention",description:`<strong>add_cross_attention</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether cross-attention layers should be added to the model.`,name:"add_cross_attention"},{anchor:"transformers.ProphetNetConfig.is_encoder_decoder",description:`<strong>is_encoder_decoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether this is an encoder/decoder model.`,name:"is_encoder_decoder"},{anchor:"transformers.ProphetNetConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.ProphetNetConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Beginning of stream token id.`,name:"bos_token_id"},{anchor:"transformers.ProphetNetConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
End of stream token id.`,name:"eos_token_id"},{anchor:"transformers.ProphetNetConfig.ngram",description:`<strong>ngram</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of future tokens to predict. Set to 1 to be same as traditional Language model to predict next first
token.`,name:"ngram"},{anchor:"transformers.ProphetNetConfig.num_buckets",description:`<strong>num_buckets</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of buckets to use for each attention layer. This is for relative position calculation. See the
[T5 paper](see <a href="https://arxiv.org/abs/1910.10683" rel="nofollow">https://arxiv.org/abs/1910.10683</a>) for more details.`,name:"num_buckets"},{anchor:"transformers.ProphetNetConfig.relative_max_distance",description:`<strong>relative_max_distance</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Relative distances greater than this number will be put into the last same bucket. This is for relative
position calculation. See the [T5 paper](see <a href="https://arxiv.org/abs/1910.10683" rel="nofollow">https://arxiv.org/abs/1910.10683</a>) for more details.`,name:"relative_max_distance"},{anchor:"transformers.ProphetNetConfig.disable_ngram_loss",description:`<strong>disable_ngram_loss</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether be trained predicting only the next first token.`,name:"disable_ngram_loss"},{anchor:"transformers.ProphetNetConfig.eps",description:`<strong>eps</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Controls the <code>epsilon</code> parameter value for label smoothing in the loss calculation. If set to 0, no label
smoothing is performed.`,name:"eps"},{anchor:"transformers.ProphetNetConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}]}}),rt=new re({}),st=new z({props:{name:"class transformers.ProphetNetTokenizer",anchor:"transformers.ProphetNetTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"x_sep_token",val:" = '[X_SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/tokenization_prophetnet.py#L55",parametersDescription:[{anchor:"transformers.ProphetNetTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.ProphetNetTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.ProphetNetTokenizer.do_basic_tokenize",description:`<strong>do_basic_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to do basic tokenization before WordPiece.`,name:"do_basic_tokenize"},{anchor:"transformers.ProphetNetTokenizer.never_split",description:`<strong>never_split</strong> (<code>Iterable</code>, <em>optional</em>) &#x2014;
Collection of tokens which will never be split during tokenization. Only has an effect when
<code>do_basic_tokenize=True</code>`,name:"never_split"},{anchor:"transformers.ProphetNetTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.ProphetNetTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.ProphetNetTokenizer.x_sep_token",description:`<strong>x_sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[X_SEP]&quot;</code>) &#x2014;
Special second separator token, which can be generated by <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a>. It is
used to separate bullet-point like sentences in summarization, <em>e.g.</em>.`,name:"x_sep_token"},{anchor:"transformers.ProphetNetTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.ProphetNetTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.ProphetNetTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.ProphetNetTokenizer.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters.</p>
<p>This should likely be deactivated for Japanese (see this
<a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">issue</a>).
strip_accents &#x2014; (<code>bool</code>, <em>optional</em>):
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"tokenize_chinese_chars"}]}}),dt=new z({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.ProphetNetTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/tokenization_prophetnet.py#L266",parametersDescription:[{anchor:"transformers.ProphetNetTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.ProphetNetTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ct=new z({props:{name:"convert_tokens_to_string",anchor:"transformers.ProphetNetTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/tokenization_prophetnet.py#L186"}}),ht=new z({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.ProphetNetTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/tokenization_prophetnet.py#L218",parametersDescription:[{anchor:"transformers.ProphetNetTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.ProphetNetTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),lt=new Go({props:{code:`0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),pt=new z({props:{name:"get_special_tokens_mask",anchor:"transformers.ProphetNetTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/tokenization_prophetnet.py#L191",parametersDescription:[{anchor:"transformers.ProphetNetTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.ProphetNetTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.ProphetNetTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),mt=new re({}),ft=new z({props:{name:"class transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput",anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"logits_ngram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_ngram_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_ngram_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L252",parametersDescription:[{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss.`,name:"loss"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the main stream language modeling head (scores for each vocabulary token before
SoftMax).`,name:"logits"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.logits_ngram",description:`<strong>logits_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the predict stream language modeling head (scores for each vocabulary token before
SoftMax).`,name:"logits_ngram"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_ngram_hidden_states",description:`<strong>decoder_ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.`,name:"decoder_ngram_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_ngram_attentions",description:`<strong>decoder_ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the self-attention heads.`,name:"decoder_ngram_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the`,name:"cross_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)</code>. Attentions weights of the encoder, after the attention
softmax, used to compute the weighted average in the self-attention heads.`,name:"encoder_attentions"}]}}),_t=new z({props:{name:"class transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput",anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor"},{name:"last_hidden_state_ngram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_ngram_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_ngram_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L336",parametersDescription:[{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>) &#x2014;
Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.last_hidden_state_ngram",description:`<strong>last_hidden_state_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,ngram * decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.`,name:"last_hidden_state_ngram"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_ngram_hidden_states",description:`<strong>decoder_ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.`,name:"decoder_ngram_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_ngram_attentions",description:`<strong>decoder_ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the`,name:"decoder_ngram_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the`,name:"cross_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}]}}),gt=new z({props:{name:"class transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput",anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor"},{name:"last_hidden_state_ngram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"hidden_states_ngram",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"ngram_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L421",parametersDescription:[{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>) &#x2014;
Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.last_hidden_state_ngram",description:`<strong>last_hidden_state_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.`,name:"last_hidden_state_ngram"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.ngram_hidden_states",description:`<strong>ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.`,name:"ngram_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.ngram_attentions",description:`<strong>ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the`,name:"ngram_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the`,name:"cross_attentions"}]}}),vt=new z({props:{name:"class transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput",anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"logits_ngram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"hidden_states_ngram",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"ngram_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L481",parametersDescription:[{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss.`,name:"loss"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the main stream language modeling head (scores for each vocabulary token before
SoftMax).`,name:"logits"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.logits_ngram",description:`<strong>logits_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the predict stream language modeling head (scores for each vocabulary token before
SoftMax).`,name:"logits_ngram"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.ngram_hidden_states",description:`<strong>ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.`,name:"ngram_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.ngram_attentions",description:`<strong>ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the`,name:"ngram_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the`,name:"cross_attentions"}]}}),kt=new re({}),Tt=new z({props:{name:"class transformers.ProphetNetModel",anchor:"transformers.ProphetNetModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L1751",parametersDescription:[{anchor:"transformers.ProphetNetModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Nt=new z({props:{name:"forward",anchor:"transformers.ProphetNetModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"decoder_head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"encoder_outputs",val:": typing.Optional[typing.Tuple] = None"},{name:"past_key_values",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"decoder_inputs_embeds",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L1783",parametersDescription:[{anchor:"transformers.ProphetNetModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetModel.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>ProphetNet uses the <code>eos_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If
<code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).`,name:"decoder_input_ids"},{anchor:"transformers.ProphetNetModel.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.ProphetNetModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetModel.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.ProphetNetModel.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.ProphetNetModel.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) is a sequence of
hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.ProphetNetModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.ProphetNetModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.ProphetNetModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>) \u2014 Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>last_hidden_state_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,ngram * decoder_sequence_length, config.vocab_size)</code>) \u2014 Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>decoder_ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ie=new Rn({props:{$$slots:{default:[nl]},$$scope:{ctx:I}}}),qt=new Go({props:{code:`from transformers import ProphetNetTokenizer, ProphetNetModel

tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = ProphetNetModel.from_pretrained("microsoft/prophetnet-large-uncased")

input_ids = tokenizer(
    "Studies have been shown that owning a dog is good for you", return_tensors="pt"
).input_ids  # Batch size 1
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

last_hidden_states = outputs.last_hidden_state  # main stream hidden states
last_hidden_states_ngram = outputs.last_hidden_state_ngram  # predict hidden states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetModel.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Studies have been shown that owning a dog is good for you&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(<span class="hljs-string">&quot;Studies show that&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state  <span class="hljs-comment"># main stream hidden states</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states_ngram = outputs.last_hidden_state_ngram  <span class="hljs-comment"># predict hidden states</span>`}}),zt=new re({}),$t=new z({props:{name:"class transformers.ProphetNetEncoder",anchor:"transformers.ProphetNetEncoder",parameters:[{name:"config",val:": ProphetNetConfig"},{name:"word_embeddings",val:": Embedding = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L1244",parametersDescription:[{anchor:"transformers.ProphetNetEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ct=new z({props:{name:"forward",anchor:"transformers.ProphetNetEncoder.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L1274",parametersDescription:[{anchor:"transformers.ProphetNetEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetEncoder.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetEncoder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetEncoder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetEncoder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetEncoder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ge=new Rn({props:{$$slots:{default:[rl]},$$scope:{ctx:I}}}),St=new Go({props:{code:`from transformers import ProphetNetTokenizer, ProphetNetEncoder
import torch

tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = ProphetNetEncoder.from_pretrained("patrickvonplaten/prophetnet-large-uncased-standalone")
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetEncoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetEncoder.from_pretrained(<span class="hljs-string">&quot;patrickvonplaten/prophetnet-large-uncased-standalone&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Dt=new re({}),Ot=new z({props:{name:"class transformers.ProphetNetDecoder",anchor:"transformers.ProphetNetDecoder",parameters:[{name:"config",val:": ProphetNetConfig"},{name:"word_embeddings",val:": Embedding = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L1384",parametersDescription:[{anchor:"transformers.ProphetNetDecoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Bt=new z({props:{name:"forward",anchor:"transformers.ProphetNetDecoder.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"encoder_hidden_states",val:" = None"},{name:"encoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"past_key_values",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L1421",parametersDescription:[{anchor:"transformers.ProphetNetDecoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetDecoder.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetDecoder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetDecoder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetDecoder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetDecoder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ProphetNetDecoder.forward.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong>  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
the model is configured as a decoder.`,name:"encoder_hidden_states"},{anchor:"transformers.ProphetNetDecoder.forward.encoder_attention_mask",description:`<strong>encoder_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:`,name:"encoder_attention_mask"},{anchor:"transformers.ProphetNetDecoder.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.ProphetNetDecoder.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.ProphetNetDecoder.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"use_cache"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>) \u2014 Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>last_hidden_state_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) \u2014 Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),He=new Rn({props:{$$slots:{default:[sl]},$$scope:{ctx:I}}}),Gt=new Go({props:{code:`from transformers import ProphetNetTokenizer, ProphetNetDecoder
import torch

tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = ProphetNetDecoder.from_pretrained("microsoft/prophetnet-large-uncased", add_cross_attention=False)
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetDecoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetDecoder.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>, add_cross_attention=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Wt=new re({}),Ht=new z({props:{name:"class transformers.ProphetNetForConditionalGeneration",anchor:"transformers.ProphetNetForConditionalGeneration",parameters:[{name:"config",val:": ProphetNetConfig"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L1878",parametersDescription:[{anchor:"transformers.ProphetNetForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Yt=new z({props:{name:"forward",anchor:"transformers.ProphetNetForConditionalGeneration.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"decoder_head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"encoder_outputs",val:" = None"},{name:"past_key_values",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"decoder_inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L1899",parametersDescription:[{anchor:"transformers.ProphetNetForConditionalGeneration.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>ProphetNet uses the <code>eos_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If
<code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).`,name:"decoder_input_ids"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) is a sequence of
hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[-100, 0, ..., config.vocab_size - 1]</code>. All labels set to <code>-100</code> are ignored (masked), the loss is only computed for
labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the main stream language modeling head (scores for each vocabulary token before
SoftMax).</p>
</li>
<li>
<p><strong>logits_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the predict stream language modeling head (scores for each vocabulary token before
SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>decoder_ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)</code>. Attentions weights of the encoder, after the attention
softmax, used to compute the weighted average in the self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ue=new Rn({props:{$$slots:{default:[al]},$$scope:{ctx:I}}}),Kt=new Go({props:{code:`from transformers import ProphetNetTokenizer, ProphetNetForConditionalGeneration

tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = ProphetNetForConditionalGeneration.from_pretrained("microsoft/prophetnet-large-uncased")

input_ids = tokenizer(
    "Studies have been shown that owning a dog is good for you", return_tensors="pt"
).input_ids  # Batch size 1
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

logits_next_token = outputs.logits  # logits to predict next token as usual
logits_ngram_next_tokens = outputs.logits_ngram  # logits to predict 2nd, 3rd, ... next tokens`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Studies have been shown that owning a dog is good for you&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(<span class="hljs-string">&quot;Studies show that&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits_next_token = outputs.logits  <span class="hljs-comment"># logits to predict next token as usual</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_ngram_next_tokens = outputs.logits_ngram  <span class="hljs-comment"># logits to predict 2nd, 3rd, ... next tokens</span>`}}),Qt=new re({}),Xt=new z({props:{name:"class transformers.ProphetNetForCausalLM",anchor:"transformers.ProphetNetForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L2087",parametersDescription:[{anchor:"transformers.ProphetNetForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),no=new z({props:{name:"forward",anchor:"transformers.ProphetNetForCausalLM.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"encoder_hidden_states",val:" = None"},{name:"encoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"past_key_values",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L2122",parametersDescription:[{anchor:"transformers.ProphetNetForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetForCausalLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ProphetNetForCausalLM.forward.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
the model is configured as a decoder.`,name:"encoder_hidden_states"},{anchor:"transformers.ProphetNetForCausalLM.forward.encoder_attention_mask",description:`<strong>encoder_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:`,name:"encoder_attention_mask"},{anchor:"transformers.ProphetNetForCausalLM.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.ProphetNetForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.ProphetNetForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"use_cache"},{anchor:"transformers.ProphetNetForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in
<code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are
ignored (masked), the loss is only computed for the tokens with labels n <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the main stream language modeling head (scores for each vocabulary token before
SoftMax).</p>
</li>
<li>
<p><strong>logits_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the predict stream language modeling head (scores for each vocabulary token before
SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Je=new Rn({props:{$$slots:{default:[dl]},$$scope:{ctx:I}}}),ro=new Go({props:{code:`from transformers import ProphetNetTokenizer, ProphetNetForCausalLM
import torch

tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = ProphetNetForCausalLM.from_pretrained("microsoft/prophetnet-large-uncased")
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

logits = outputs.logits

# Model can also be used with EncoderDecoder framework
from transformers import BertTokenizer, EncoderDecoderModel, ProphetNetTokenizer
import torch

tokenizer_enc = BertTokenizer.from_pretrained("bert-large-uncased")
tokenizer_dec = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = EncoderDecoderModel.from_encoder_decoder_pretrained(
    "bert-large-uncased", "microsoft/prophetnet-large-uncased"
)

ARTICLE = (
    "the us state department said wednesday it had received no "
    "formal word from bolivia that it was expelling the us ambassador there "
    "but said the charges made against him are \`\` baseless ."
)
input_ids = tokenizer_enc(ARTICLE, return_tensors="pt").input_ids
labels = tokenizer_dec(
    "us rejects charges against its ambassador in bolivia", return_tensors="pt"
).input_ids
outputs = model(input_ids=input_ids, decoder_input_ids=labels[:, :-1], labels=labels[:, 1:])

loss = outputs.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetForCausalLM.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> model.config.is_decoder, <span class="hljs-string">f&quot;<span class="hljs-subst">{model.__class__}</span> has to be configured as a decoder.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Model can also be used with EncoderDecoder framework</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, EncoderDecoderModel, ProphetNetTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_enc = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_dec = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel.from_encoder_decoder_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;bert-large-uncased&quot;</span>, <span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>ARTICLE = (
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;the us state department said wednesday it had received no &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;formal word from bolivia that it was expelling the us ambassador there &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;but said the charges made against him are \`\` baseless .&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer_enc(ARTICLE, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer_dec(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;us rejects charges against its ambassador in bolivia&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=labels[:, :-<span class="hljs-number">1</span>], labels=labels[:, <span class="hljs-number">1</span>:])

<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),{c(){u=n("meta"),P=c(),m=n("h1"),y=n("a"),N=n("span"),f(w.$$.fragment),b=c(),q=n("span"),Xr=s("ProphetNet"),Jn=c(),se=n("p"),Wo=n("strong"),Zr=s("DISCLAIMER:"),es=s(" If you see something strange, file a "),Qe=n("a"),ts=s("Github Issue"),os=s(` and assign
@patrickvonplaten`),Yn=c(),ae=n("h2"),Ee=n("a"),Ho=n("span"),f(Xe.$$.fragment),ns=c(),Vo=n("span"),rs=s("Overview"),Kn=c(),xe=n("p"),ss=s("The ProphetNet model was proposed in "),Ze=n("a"),as=s("ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,"),ds=s(` by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei
Zhang, Ming Zhou on 13 Jan, 2020.`),Qn=c(),ao=n("p"),is=s(`ProphetNet is an encoder-decoder model and can predict n-future tokens for \u201Cngram\u201D language modeling instead of just
the next token.`),Xn=c(),io=n("p"),cs=s("The abstract from the paper is the following:"),Zn=c(),co=n("p"),Uo=n("em"),hs=s(`In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel
self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of
the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by
n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time
step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent
overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale
dataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for
abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new
state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.`),er=c(),Ce=n("p"),ls=s("The Authors\u2019 code can be found "),et=n("a"),ps=s("here"),us=s("."),tr=c(),de=n("h2"),Se=n("a"),Ro=n("span"),f(tt.$$.fragment),ms=c(),Jo=n("span"),fs=s("ProphetNetConfig"),or=c(),U=n("div"),f(ot.$$.fragment),_s=c(),nt=n("p"),gs=s("This is the configuration class to store the configuration of a "),ho=n("a"),vs=s("ProphetNetModel"),ks=s(`. It is used to instantiate a
ProphetNet model according to the specified arguments, defining the model architecture.`),Ts=c(),ie=n("p"),bs=s("Configuration objects inherit from "),lo=n("a"),ws=s("PretrainedConfig"),ys=s(` and can be used to control the model outputs. Read the
documentation from `),po=n("a"),Ps=s("PretrainedConfig"),Ns=s(" for more information."),nr=c(),ce=n("h2"),De=n("a"),Yo=n("span"),f(rt.$$.fragment),qs=c(),Ko=n("span"),zs=s("ProphetNetTokenizer"),rr=c(),$=n("div"),f(st.$$.fragment),$s=c(),Qo=n("p"),Fs=s("Construct a ProphetNetTokenizer. Based on WordPiece."),Ms=c(),at=n("p"),Es=s("This tokenizer inherits from "),uo=n("a"),xs=s("PreTrainedTokenizer"),Cs=s(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Ss=c(),R=n("div"),f(dt.$$.fragment),Ds=c(),Xo=n("p"),Os=s(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),Ls=c(),it=n("ul"),mo=n("li"),js=s("single sequence: "),Zo=n("code"),As=s("[CLS] X [SEP]"),Is=c(),fo=n("li"),Bs=s("pair of sequences: "),en=n("code"),Gs=s("[CLS] A [SEP] B [SEP]"),Ws=c(),Oe=n("div"),f(ct.$$.fragment),Hs=c(),tn=n("p"),Vs=s("Converts a sequence of tokens (string) in a single string."),Us=c(),W=n("div"),f(ht.$$.fragment),Rs=c(),on=n("p"),Js=s(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A ProphetNet
sequence pair mask has the following format:`),Ys=c(),f(lt.$$.fragment),Ks=c(),he=n("p"),Qs=s("If "),nn=n("code"),Xs=s("token_ids_1"),Zs=s(" is "),rn=n("code"),ea=s("None"),ta=s(", this method only returns the first portion of the mask (0s)."),oa=c(),Le=n("div"),f(pt.$$.fragment),na=c(),ut=n("p"),ra=s(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),sn=n("code"),sa=s("prepare_for_model"),aa=s(" method."),sr=c(),le=n("h2"),je=n("a"),an=n("span"),f(mt.$$.fragment),da=c(),dn=n("span"),ia=s("ProphetNet specific outputs"),ar=c(),pe=n("div"),f(ft.$$.fragment),ca=c(),cn=n("p"),ha=s("Base class for sequence-to-sequence language models outputs."),dr=c(),ue=n("div"),f(_t.$$.fragment),la=c(),hn=n("p"),pa=s(`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),ir=c(),me=n("div"),f(gt.$$.fragment),ua=c(),ln=n("p"),ma=s("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),cr=c(),fe=n("div"),f(vt.$$.fragment),fa=c(),pn=n("p"),_a=s("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),hr=c(),_e=n("h2"),Ae=n("a"),un=n("span"),f(kt.$$.fragment),ga=c(),mn=n("span"),va=s("ProphetNetModel"),lr=c(),E=n("div"),f(Tt.$$.fragment),ka=c(),bt=n("p"),Ta=s(`The bare ProphetNet Model outputting raw hidden-states without any specific head on top.
This model inherits from `),_o=n("a"),ba=s("PreTrainedModel"),wa=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ya=c(),ge=n("p"),Pa=s("Original ProphetNet code can be found "),wt=n("a"),Na=s("here"),qa=s(`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),fn=n("code"),za=s("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),$a=s("."),Fa=c(),yt=n("p"),Ma=s("This model is a PyTorch "),Pt=n("a"),Ea=s("torch.nn.Module"),xa=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Ca=c(),S=n("div"),f(Nt.$$.fragment),Sa=c(),ve=n("p"),Da=s("The "),go=n("a"),Oa=s("ProphetNetModel"),La=s(" forward method, overrides the "),_n=n("code"),ja=s("__call__"),Aa=s(" special method."),Ia=c(),f(Ie.$$.fragment),Ba=c(),gn=n("p"),Ga=s("Example:"),Wa=c(),f(qt.$$.fragment),pr=c(),ke=n("h2"),Be=n("a"),vn=n("span"),f(zt.$$.fragment),Ha=c(),kn=n("span"),Va=s("ProphetNetEncoder"),ur=c(),F=n("div"),f($t.$$.fragment),Ua=c(),Ft=n("p"),Ra=s(`The standalone encoder part of the ProphetNetModel.
This model inherits from `),vo=n("a"),Ja=s("PreTrainedModel"),Ya=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ka=c(),Te=n("p"),Qa=s("Original ProphetNet code can be found "),Mt=n("a"),Xa=s("here"),Za=s(`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Tn=n("code"),ed=s("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),td=s("."),od=c(),Et=n("p"),nd=s("This model is a PyTorch "),xt=n("a"),rd=s("torch.nn.Module"),sd=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),ad=c(),B=n("p"),dd=s("word_embeddings  ("),bn=n("code"),id=s("torch.nn.Embeddings"),cd=s(" of shape "),wn=n("code"),hd=s("(config.vocab_size, config.hidden_size)"),ld=s(", "),yn=n("em"),pd=s("optional"),ud=s(`):
The word embedding parameters. This can be used to initialize `),ko=n("a"),md=s("ProphetNetEncoder"),fd=s(` with pre-defined word
embeddings instead of randomly initialized word embeddings.`),_d=c(),D=n("div"),f(Ct.$$.fragment),gd=c(),be=n("p"),vd=s("The "),To=n("a"),kd=s("ProphetNetEncoder"),Td=s(" forward method, overrides the "),Pn=n("code"),bd=s("__call__"),wd=s(" special method."),yd=c(),f(Ge.$$.fragment),Pd=c(),Nn=n("p"),Nd=s("Example:"),qd=c(),f(St.$$.fragment),mr=c(),we=n("h2"),We=n("a"),qn=n("span"),f(Dt.$$.fragment),zd=c(),zn=n("span"),$d=s("ProphetNetDecoder"),fr=c(),M=n("div"),f(Ot.$$.fragment),Fd=c(),Lt=n("p"),Md=s(`The standalone decoder part of the ProphetNetModel.
This model inherits from `),bo=n("a"),Ed=s("PreTrainedModel"),xd=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Cd=c(),ye=n("p"),Sd=s("Original ProphetNet code can be found "),jt=n("a"),Dd=s("here"),Od=s(`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),$n=n("code"),Ld=s("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),jd=s("."),Ad=c(),At=n("p"),Id=s("This model is a PyTorch "),It=n("a"),Bd=s("torch.nn.Module"),Gd=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Wd=c(),G=n("p"),Hd=s("word_embeddings  ("),Fn=n("code"),Vd=s("torch.nn.Embeddings"),Ud=s(" of shape "),Mn=n("code"),Rd=s("(config.vocab_size, config.hidden_size)"),Jd=s(", "),En=n("em"),Yd=s("optional"),Kd=s(`):
The word embedding parameters. This can be used to initialize `),wo=n("a"),Qd=s("ProphetNetEncoder"),Xd=s(` with pre-defined word
embeddings instead of randomly initialized word embeddings.`),Zd=c(),O=n("div"),f(Bt.$$.fragment),ei=c(),Pe=n("p"),ti=s("The "),yo=n("a"),oi=s("ProphetNetDecoder"),ni=s(" forward method, overrides the "),xn=n("code"),ri=s("__call__"),si=s(" special method."),ai=c(),f(He.$$.fragment),di=c(),Cn=n("p"),ii=s("Example:"),ci=c(),f(Gt.$$.fragment),_r=c(),Ne=n("h2"),Ve=n("a"),Sn=n("span"),f(Wt.$$.fragment),hi=c(),Dn=n("span"),li=s("ProphetNetForConditionalGeneration"),gr=c(),x=n("div"),f(Ht.$$.fragment),pi=c(),Vt=n("p"),ui=s(`The ProphetNet Model with a language modeling head. Can be used for sequence generation tasks.
This model inherits from `),Po=n("a"),mi=s("PreTrainedModel"),fi=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),_i=c(),qe=n("p"),gi=s("Original ProphetNet code can be found "),Ut=n("a"),vi=s("here"),ki=s(`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),On=n("code"),Ti=s("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),bi=s("."),wi=c(),Rt=n("p"),yi=s("This model is a PyTorch "),Jt=n("a"),Pi=s("torch.nn.Module"),Ni=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),qi=c(),L=n("div"),f(Yt.$$.fragment),zi=c(),ze=n("p"),$i=s("The "),No=n("a"),Fi=s("ProphetNetForConditionalGeneration"),Mi=s(" forward method, overrides the "),Ln=n("code"),Ei=s("__call__"),xi=s(" special method."),Ci=c(),f(Ue.$$.fragment),Si=c(),jn=n("p"),Di=s("Example:"),Oi=c(),f(Kt.$$.fragment),vr=c(),$e=n("h2"),Re=n("a"),An=n("span"),f(Qt.$$.fragment),Li=c(),In=n("span"),ji=s("ProphetNetForCausalLM"),kr=c(),C=n("div"),f(Xt.$$.fragment),Ai=c(),Zt=n("p"),Ii=s(`The standalone decoder part of the ProphetNetModel with a lm head on top. The model can be used for causal language modeling.
This model inherits from `),qo=n("a"),Bi=s("PreTrainedModel"),Gi=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Wi=c(),Fe=n("p"),Hi=s("Original ProphetNet code can be found "),eo=n("a"),Vi=s("here"),Ui=s(`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Bn=n("code"),Ri=s("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),Ji=s("."),Yi=c(),to=n("p"),Ki=s("This model is a PyTorch "),oo=n("a"),Qi=s("torch.nn.Module"),Xi=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Zi=c(),j=n("div"),f(no.$$.fragment),ec=c(),Me=n("p"),tc=s("The "),zo=n("a"),oc=s("ProphetNetForCausalLM"),nc=s(" forward method, overrides the "),Gn=n("code"),rc=s("__call__"),sc=s(" special method."),ac=c(),f(Je.$$.fragment),dc=c(),Wn=n("p"),ic=s("Example:"),cc=c(),f(ro.$$.fragment),this.h()},l(t){const l=ol('[data-svelte="svelte-1phssyn"]',document.head);u=r(l,"META",{name:!0,content:!0}),l.forEach(o),P=h(t),m=r(t,"H1",{class:!0});var so=d(m);y=r(so,"A",{id:!0,class:!0,href:!0});var Hn=d(y);N=r(Hn,"SPAN",{});var Vn=d(N);_(w.$$.fragment,Vn),Vn.forEach(o),Hn.forEach(o),b=h(so),q=r(so,"SPAN",{});var Un=d(q);Xr=a(Un,"ProphetNet"),Un.forEach(o),so.forEach(o),Jn=h(t),se=r(t,"P",{});var Ye=d(se);Wo=r(Ye,"STRONG",{});var pc=d(Wo);Zr=a(pc,"DISCLAIMER:"),pc.forEach(o),es=a(Ye," If you see something strange, file a "),Qe=r(Ye,"A",{href:!0,rel:!0});var uc=d(Qe);ts=a(uc,"Github Issue"),uc.forEach(o),os=a(Ye,` and assign
@patrickvonplaten`),Ye.forEach(o),Yn=h(t),ae=r(t,"H2",{class:!0});var br=d(ae);Ee=r(br,"A",{id:!0,class:!0,href:!0});var mc=d(Ee);Ho=r(mc,"SPAN",{});var fc=d(Ho);_(Xe.$$.fragment,fc),fc.forEach(o),mc.forEach(o),ns=h(br),Vo=r(br,"SPAN",{});var _c=d(Vo);rs=a(_c,"Overview"),_c.forEach(o),br.forEach(o),Kn=h(t),xe=r(t,"P",{});var wr=d(xe);ss=a(wr,"The ProphetNet model was proposed in "),Ze=r(wr,"A",{href:!0,rel:!0});var gc=d(Ze);as=a(gc,"ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,"),gc.forEach(o),ds=a(wr,` by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei
Zhang, Ming Zhou on 13 Jan, 2020.`),wr.forEach(o),Qn=h(t),ao=r(t,"P",{});var vc=d(ao);is=a(vc,`ProphetNet is an encoder-decoder model and can predict n-future tokens for \u201Cngram\u201D language modeling instead of just
the next token.`),vc.forEach(o),Xn=h(t),io=r(t,"P",{});var kc=d(io);cs=a(kc,"The abstract from the paper is the following:"),kc.forEach(o),Zn=h(t),co=r(t,"P",{});var Tc=d(co);Uo=r(Tc,"EM",{});var bc=d(Uo);hs=a(bc,`In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel
self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of
the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by
n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time
step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent
overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale
dataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for
abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new
state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.`),bc.forEach(o),Tc.forEach(o),er=h(t),Ce=r(t,"P",{});var yr=d(Ce);ls=a(yr,"The Authors\u2019 code can be found "),et=r(yr,"A",{href:!0,rel:!0});var wc=d(et);ps=a(wc,"here"),wc.forEach(o),us=a(yr,"."),yr.forEach(o),tr=h(t),de=r(t,"H2",{class:!0});var Pr=d(de);Se=r(Pr,"A",{id:!0,class:!0,href:!0});var yc=d(Se);Ro=r(yc,"SPAN",{});var Pc=d(Ro);_(tt.$$.fragment,Pc),Pc.forEach(o),yc.forEach(o),ms=h(Pr),Jo=r(Pr,"SPAN",{});var Nc=d(Jo);fs=a(Nc,"ProphetNetConfig"),Nc.forEach(o),Pr.forEach(o),or=h(t),U=r(t,"DIV",{class:!0});var $o=d(U);_(ot.$$.fragment,$o),_s=h($o),nt=r($o,"P",{});var Nr=d(nt);gs=a(Nr,"This is the configuration class to store the configuration of a "),ho=r(Nr,"A",{href:!0});var qc=d(ho);vs=a(qc,"ProphetNetModel"),qc.forEach(o),ks=a(Nr,`. It is used to instantiate a
ProphetNet model according to the specified arguments, defining the model architecture.`),Nr.forEach(o),Ts=h($o),ie=r($o,"P",{});var Fo=d(ie);bs=a(Fo,"Configuration objects inherit from "),lo=r(Fo,"A",{href:!0});var zc=d(lo);ws=a(zc,"PretrainedConfig"),zc.forEach(o),ys=a(Fo,` and can be used to control the model outputs. Read the
documentation from `),po=r(Fo,"A",{href:!0});var $c=d(po);Ps=a($c,"PretrainedConfig"),$c.forEach(o),Ns=a(Fo," for more information."),Fo.forEach(o),$o.forEach(o),nr=h(t),ce=r(t,"H2",{class:!0});var qr=d(ce);De=r(qr,"A",{id:!0,class:!0,href:!0});var Fc=d(De);Yo=r(Fc,"SPAN",{});var Mc=d(Yo);_(rt.$$.fragment,Mc),Mc.forEach(o),Fc.forEach(o),qs=h(qr),Ko=r(qr,"SPAN",{});var Ec=d(Ko);zs=a(Ec,"ProphetNetTokenizer"),Ec.forEach(o),qr.forEach(o),rr=h(t),$=r(t,"DIV",{class:!0});var A=d($);_(st.$$.fragment,A),$s=h(A),Qo=r(A,"P",{});var xc=d(Qo);Fs=a(xc,"Construct a ProphetNetTokenizer. Based on WordPiece."),xc.forEach(o),Ms=h(A),at=r(A,"P",{});var zr=d(at);Es=a(zr,"This tokenizer inherits from "),uo=r(zr,"A",{href:!0});var Cc=d(uo);xs=a(Cc,"PreTrainedTokenizer"),Cc.forEach(o),Cs=a(zr,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),zr.forEach(o),Ss=h(A),R=r(A,"DIV",{class:!0});var Mo=d(R);_(dt.$$.fragment,Mo),Ds=h(Mo),Xo=r(Mo,"P",{});var Sc=d(Xo);Os=a(Sc,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),Sc.forEach(o),Ls=h(Mo),it=r(Mo,"UL",{});var $r=d(it);mo=r($r,"LI",{});var hc=d(mo);js=a(hc,"single sequence: "),Zo=r(hc,"CODE",{});var Dc=d(Zo);As=a(Dc,"[CLS] X [SEP]"),Dc.forEach(o),hc.forEach(o),Is=h($r),fo=r($r,"LI",{});var lc=d(fo);Bs=a(lc,"pair of sequences: "),en=r(lc,"CODE",{});var Oc=d(en);Gs=a(Oc,"[CLS] A [SEP] B [SEP]"),Oc.forEach(o),lc.forEach(o),$r.forEach(o),Mo.forEach(o),Ws=h(A),Oe=r(A,"DIV",{class:!0});var Fr=d(Oe);_(ct.$$.fragment,Fr),Hs=h(Fr),tn=r(Fr,"P",{});var Lc=d(tn);Vs=a(Lc,"Converts a sequence of tokens (string) in a single string."),Lc.forEach(o),Fr.forEach(o),Us=h(A),W=r(A,"DIV",{class:!0});var Ke=d(W);_(ht.$$.fragment,Ke),Rs=h(Ke),on=r(Ke,"P",{});var jc=d(on);Js=a(jc,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A ProphetNet
sequence pair mask has the following format:`),jc.forEach(o),Ys=h(Ke),_(lt.$$.fragment,Ke),Ks=h(Ke),he=r(Ke,"P",{});var Eo=d(he);Qs=a(Eo,"If "),nn=r(Eo,"CODE",{});var Ac=d(nn);Xs=a(Ac,"token_ids_1"),Ac.forEach(o),Zs=a(Eo," is "),rn=r(Eo,"CODE",{});var Ic=d(rn);ea=a(Ic,"None"),Ic.forEach(o),ta=a(Eo,", this method only returns the first portion of the mask (0s)."),Eo.forEach(o),Ke.forEach(o),oa=h(A),Le=r(A,"DIV",{class:!0});var Mr=d(Le);_(pt.$$.fragment,Mr),na=h(Mr),ut=r(Mr,"P",{});var Er=d(ut);ra=a(Er,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),sn=r(Er,"CODE",{});var Bc=d(sn);sa=a(Bc,"prepare_for_model"),Bc.forEach(o),aa=a(Er," method."),Er.forEach(o),Mr.forEach(o),A.forEach(o),sr=h(t),le=r(t,"H2",{class:!0});var xr=d(le);je=r(xr,"A",{id:!0,class:!0,href:!0});var Gc=d(je);an=r(Gc,"SPAN",{});var Wc=d(an);_(mt.$$.fragment,Wc),Wc.forEach(o),Gc.forEach(o),da=h(xr),dn=r(xr,"SPAN",{});var Hc=d(dn);ia=a(Hc,"ProphetNet specific outputs"),Hc.forEach(o),xr.forEach(o),ar=h(t),pe=r(t,"DIV",{class:!0});var Cr=d(pe);_(ft.$$.fragment,Cr),ca=h(Cr),cn=r(Cr,"P",{});var Vc=d(cn);ha=a(Vc,"Base class for sequence-to-sequence language models outputs."),Vc.forEach(o),Cr.forEach(o),dr=h(t),ue=r(t,"DIV",{class:!0});var Sr=d(ue);_(_t.$$.fragment,Sr),la=h(Sr),hn=r(Sr,"P",{});var Uc=d(hn);pa=a(Uc,`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),Uc.forEach(o),Sr.forEach(o),ir=h(t),me=r(t,"DIV",{class:!0});var Dr=d(me);_(gt.$$.fragment,Dr),ua=h(Dr),ln=r(Dr,"P",{});var Rc=d(ln);ma=a(Rc,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Rc.forEach(o),Dr.forEach(o),cr=h(t),fe=r(t,"DIV",{class:!0});var Or=d(fe);_(vt.$$.fragment,Or),fa=h(Or),pn=r(Or,"P",{});var Jc=d(pn);_a=a(Jc,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Jc.forEach(o),Or.forEach(o),hr=h(t),_e=r(t,"H2",{class:!0});var Lr=d(_e);Ae=r(Lr,"A",{id:!0,class:!0,href:!0});var Yc=d(Ae);un=r(Yc,"SPAN",{});var Kc=d(un);_(kt.$$.fragment,Kc),Kc.forEach(o),Yc.forEach(o),ga=h(Lr),mn=r(Lr,"SPAN",{});var Qc=d(mn);va=a(Qc,"ProphetNetModel"),Qc.forEach(o),Lr.forEach(o),lr=h(t),E=r(t,"DIV",{class:!0});var J=d(E);_(Tt.$$.fragment,J),ka=h(J),bt=r(J,"P",{});var jr=d(bt);Ta=a(jr,`The bare ProphetNet Model outputting raw hidden-states without any specific head on top.
This model inherits from `),_o=r(jr,"A",{href:!0});var Xc=d(_o);ba=a(Xc,"PreTrainedModel"),Xc.forEach(o),wa=a(jr,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),jr.forEach(o),ya=h(J),ge=r(J,"P",{});var xo=d(ge);Pa=a(xo,"Original ProphetNet code can be found "),wt=r(xo,"A",{href:!0,rel:!0});var Zc=d(wt);Na=a(Zc,"here"),Zc.forEach(o),qa=a(xo,`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),fn=r(xo,"CODE",{});var eh=d(fn);za=a(eh,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),eh.forEach(o),$a=a(xo,"."),xo.forEach(o),Fa=h(J),yt=r(J,"P",{});var Ar=d(yt);Ma=a(Ar,"This model is a PyTorch "),Pt=r(Ar,"A",{href:!0,rel:!0});var th=d(Pt);Ea=a(th,"torch.nn.Module"),th.forEach(o),xa=a(Ar,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Ar.forEach(o),Ca=h(J),S=r(J,"DIV",{class:!0});var Y=d(S);_(Nt.$$.fragment,Y),Sa=h(Y),ve=r(Y,"P",{});var Co=d(ve);Da=a(Co,"The "),go=r(Co,"A",{href:!0});var oh=d(go);Oa=a(oh,"ProphetNetModel"),oh.forEach(o),La=a(Co," forward method, overrides the "),_n=r(Co,"CODE",{});var nh=d(_n);ja=a(nh,"__call__"),nh.forEach(o),Aa=a(Co," special method."),Co.forEach(o),Ia=h(Y),_(Ie.$$.fragment,Y),Ba=h(Y),gn=r(Y,"P",{});var rh=d(gn);Ga=a(rh,"Example:"),rh.forEach(o),Wa=h(Y),_(qt.$$.fragment,Y),Y.forEach(o),J.forEach(o),pr=h(t),ke=r(t,"H2",{class:!0});var Ir=d(ke);Be=r(Ir,"A",{id:!0,class:!0,href:!0});var sh=d(Be);vn=r(sh,"SPAN",{});var ah=d(vn);_(zt.$$.fragment,ah),ah.forEach(o),sh.forEach(o),Ha=h(Ir),kn=r(Ir,"SPAN",{});var dh=d(kn);Va=a(dh,"ProphetNetEncoder"),dh.forEach(o),Ir.forEach(o),ur=h(t),F=r(t,"DIV",{class:!0});var H=d(F);_($t.$$.fragment,H),Ua=h(H),Ft=r(H,"P",{});var Br=d(Ft);Ra=a(Br,`The standalone encoder part of the ProphetNetModel.
This model inherits from `),vo=r(Br,"A",{href:!0});var ih=d(vo);Ja=a(ih,"PreTrainedModel"),ih.forEach(o),Ya=a(Br,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Br.forEach(o),Ka=h(H),Te=r(H,"P",{});var So=d(Te);Qa=a(So,"Original ProphetNet code can be found "),Mt=r(So,"A",{href:!0,rel:!0});var ch=d(Mt);Xa=a(ch,"here"),ch.forEach(o),Za=a(So,`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Tn=r(So,"CODE",{});var hh=d(Tn);ed=a(hh,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),hh.forEach(o),td=a(So,"."),So.forEach(o),od=h(H),Et=r(H,"P",{});var Gr=d(Et);nd=a(Gr,"This model is a PyTorch "),xt=r(Gr,"A",{href:!0,rel:!0});var lh=d(xt);rd=a(lh,"torch.nn.Module"),lh.forEach(o),sd=a(Gr,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Gr.forEach(o),ad=h(H),B=r(H,"P",{});var K=d(B);dd=a(K,"word_embeddings  ("),bn=r(K,"CODE",{});var ph=d(bn);id=a(ph,"torch.nn.Embeddings"),ph.forEach(o),cd=a(K," of shape "),wn=r(K,"CODE",{});var uh=d(wn);hd=a(uh,"(config.vocab_size, config.hidden_size)"),uh.forEach(o),ld=a(K,", "),yn=r(K,"EM",{});var mh=d(yn);pd=a(mh,"optional"),mh.forEach(o),ud=a(K,`):
The word embedding parameters. This can be used to initialize `),ko=r(K,"A",{href:!0});var fh=d(ko);md=a(fh,"ProphetNetEncoder"),fh.forEach(o),fd=a(K,` with pre-defined word
embeddings instead of randomly initialized word embeddings.`),K.forEach(o),_d=h(H),D=r(H,"DIV",{class:!0});var Q=d(D);_(Ct.$$.fragment,Q),gd=h(Q),be=r(Q,"P",{});var Do=d(be);vd=a(Do,"The "),To=r(Do,"A",{href:!0});var _h=d(To);kd=a(_h,"ProphetNetEncoder"),_h.forEach(o),Td=a(Do," forward method, overrides the "),Pn=r(Do,"CODE",{});var gh=d(Pn);bd=a(gh,"__call__"),gh.forEach(o),wd=a(Do," special method."),Do.forEach(o),yd=h(Q),_(Ge.$$.fragment,Q),Pd=h(Q),Nn=r(Q,"P",{});var vh=d(Nn);Nd=a(vh,"Example:"),vh.forEach(o),qd=h(Q),_(St.$$.fragment,Q),Q.forEach(o),H.forEach(o),mr=h(t),we=r(t,"H2",{class:!0});var Wr=d(we);We=r(Wr,"A",{id:!0,class:!0,href:!0});var kh=d(We);qn=r(kh,"SPAN",{});var Th=d(qn);_(Dt.$$.fragment,Th),Th.forEach(o),kh.forEach(o),zd=h(Wr),zn=r(Wr,"SPAN",{});var bh=d(zn);$d=a(bh,"ProphetNetDecoder"),bh.forEach(o),Wr.forEach(o),fr=h(t),M=r(t,"DIV",{class:!0});var V=d(M);_(Ot.$$.fragment,V),Fd=h(V),Lt=r(V,"P",{});var Hr=d(Lt);Md=a(Hr,`The standalone decoder part of the ProphetNetModel.
This model inherits from `),bo=r(Hr,"A",{href:!0});var wh=d(bo);Ed=a(wh,"PreTrainedModel"),wh.forEach(o),xd=a(Hr,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Hr.forEach(o),Cd=h(V),ye=r(V,"P",{});var Oo=d(ye);Sd=a(Oo,"Original ProphetNet code can be found "),jt=r(Oo,"A",{href:!0,rel:!0});var yh=d(jt);Dd=a(yh,"here"),yh.forEach(o),Od=a(Oo,`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),$n=r(Oo,"CODE",{});var Ph=d($n);Ld=a(Ph,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),Ph.forEach(o),jd=a(Oo,"."),Oo.forEach(o),Ad=h(V),At=r(V,"P",{});var Vr=d(At);Id=a(Vr,"This model is a PyTorch "),It=r(Vr,"A",{href:!0,rel:!0});var Nh=d(It);Bd=a(Nh,"torch.nn.Module"),Nh.forEach(o),Gd=a(Vr,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Vr.forEach(o),Wd=h(V),G=r(V,"P",{});var X=d(G);Hd=a(X,"word_embeddings  ("),Fn=r(X,"CODE",{});var qh=d(Fn);Vd=a(qh,"torch.nn.Embeddings"),qh.forEach(o),Ud=a(X," of shape "),Mn=r(X,"CODE",{});var zh=d(Mn);Rd=a(zh,"(config.vocab_size, config.hidden_size)"),zh.forEach(o),Jd=a(X,", "),En=r(X,"EM",{});var $h=d(En);Yd=a($h,"optional"),$h.forEach(o),Kd=a(X,`):
The word embedding parameters. This can be used to initialize `),wo=r(X,"A",{href:!0});var Fh=d(wo);Qd=a(Fh,"ProphetNetEncoder"),Fh.forEach(o),Xd=a(X,` with pre-defined word
embeddings instead of randomly initialized word embeddings.`),X.forEach(o),Zd=h(V),O=r(V,"DIV",{class:!0});var Z=d(O);_(Bt.$$.fragment,Z),ei=h(Z),Pe=r(Z,"P",{});var Lo=d(Pe);ti=a(Lo,"The "),yo=r(Lo,"A",{href:!0});var Mh=d(yo);oi=a(Mh,"ProphetNetDecoder"),Mh.forEach(o),ni=a(Lo," forward method, overrides the "),xn=r(Lo,"CODE",{});var Eh=d(xn);ri=a(Eh,"__call__"),Eh.forEach(o),si=a(Lo," special method."),Lo.forEach(o),ai=h(Z),_(He.$$.fragment,Z),di=h(Z),Cn=r(Z,"P",{});var xh=d(Cn);ii=a(xh,"Example:"),xh.forEach(o),ci=h(Z),_(Gt.$$.fragment,Z),Z.forEach(o),V.forEach(o),_r=h(t),Ne=r(t,"H2",{class:!0});var Ur=d(Ne);Ve=r(Ur,"A",{id:!0,class:!0,href:!0});var Ch=d(Ve);Sn=r(Ch,"SPAN",{});var Sh=d(Sn);_(Wt.$$.fragment,Sh),Sh.forEach(o),Ch.forEach(o),hi=h(Ur),Dn=r(Ur,"SPAN",{});var Dh=d(Dn);li=a(Dh,"ProphetNetForConditionalGeneration"),Dh.forEach(o),Ur.forEach(o),gr=h(t),x=r(t,"DIV",{class:!0});var ee=d(x);_(Ht.$$.fragment,ee),pi=h(ee),Vt=r(ee,"P",{});var Rr=d(Vt);ui=a(Rr,`The ProphetNet Model with a language modeling head. Can be used for sequence generation tasks.
This model inherits from `),Po=r(Rr,"A",{href:!0});var Oh=d(Po);mi=a(Oh,"PreTrainedModel"),Oh.forEach(o),fi=a(Rr,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Rr.forEach(o),_i=h(ee),qe=r(ee,"P",{});var jo=d(qe);gi=a(jo,"Original ProphetNet code can be found "),Ut=r(jo,"A",{href:!0,rel:!0});var Lh=d(Ut);vi=a(Lh,"here"),Lh.forEach(o),ki=a(jo,`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),On=r(jo,"CODE",{});var jh=d(On);Ti=a(jh,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),jh.forEach(o),bi=a(jo,"."),jo.forEach(o),wi=h(ee),Rt=r(ee,"P",{});var Jr=d(Rt);yi=a(Jr,"This model is a PyTorch "),Jt=r(Jr,"A",{href:!0,rel:!0});var Ah=d(Jt);Pi=a(Ah,"torch.nn.Module"),Ah.forEach(o),Ni=a(Jr,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Jr.forEach(o),qi=h(ee),L=r(ee,"DIV",{class:!0});var te=d(L);_(Yt.$$.fragment,te),zi=h(te),ze=r(te,"P",{});var Ao=d(ze);$i=a(Ao,"The "),No=r(Ao,"A",{href:!0});var Ih=d(No);Fi=a(Ih,"ProphetNetForConditionalGeneration"),Ih.forEach(o),Mi=a(Ao," forward method, overrides the "),Ln=r(Ao,"CODE",{});var Bh=d(Ln);Ei=a(Bh,"__call__"),Bh.forEach(o),xi=a(Ao," special method."),Ao.forEach(o),Ci=h(te),_(Ue.$$.fragment,te),Si=h(te),jn=r(te,"P",{});var Gh=d(jn);Di=a(Gh,"Example:"),Gh.forEach(o),Oi=h(te),_(Kt.$$.fragment,te),te.forEach(o),ee.forEach(o),vr=h(t),$e=r(t,"H2",{class:!0});var Yr=d($e);Re=r(Yr,"A",{id:!0,class:!0,href:!0});var Wh=d(Re);An=r(Wh,"SPAN",{});var Hh=d(An);_(Qt.$$.fragment,Hh),Hh.forEach(o),Wh.forEach(o),Li=h(Yr),In=r(Yr,"SPAN",{});var Vh=d(In);ji=a(Vh,"ProphetNetForCausalLM"),Vh.forEach(o),Yr.forEach(o),kr=h(t),C=r(t,"DIV",{class:!0});var oe=d(C);_(Xt.$$.fragment,oe),Ai=h(oe),Zt=r(oe,"P",{});var Kr=d(Zt);Ii=a(Kr,`The standalone decoder part of the ProphetNetModel with a lm head on top. The model can be used for causal language modeling.
This model inherits from `),qo=r(Kr,"A",{href:!0});var Uh=d(qo);Bi=a(Uh,"PreTrainedModel"),Uh.forEach(o),Gi=a(Kr,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Kr.forEach(o),Wi=h(oe),Fe=r(oe,"P",{});var Io=d(Fe);Hi=a(Io,"Original ProphetNet code can be found "),eo=r(Io,"A",{href:!0,rel:!0});var Rh=d(eo);Vi=a(Rh,"here"),Rh.forEach(o),Ui=a(Io,`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Bn=r(Io,"CODE",{});var Jh=d(Bn);Ri=a(Jh,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),Jh.forEach(o),Ji=a(Io,"."),Io.forEach(o),Yi=h(oe),to=r(oe,"P",{});var Qr=d(to);Ki=a(Qr,"This model is a PyTorch "),oo=r(Qr,"A",{href:!0,rel:!0});var Yh=d(oo);Qi=a(Yh,"torch.nn.Module"),Yh.forEach(o),Xi=a(Qr,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Qr.forEach(o),Zi=h(oe),j=r(oe,"DIV",{class:!0});var ne=d(j);_(no.$$.fragment,ne),ec=h(ne),Me=r(ne,"P",{});var Bo=d(Me);tc=a(Bo,"The "),zo=r(Bo,"A",{href:!0});var Kh=d(zo);oc=a(Kh,"ProphetNetForCausalLM"),Kh.forEach(o),nc=a(Bo," forward method, overrides the "),Gn=r(Bo,"CODE",{});var Qh=d(Gn);rc=a(Qh,"__call__"),Qh.forEach(o),sc=a(Bo," special method."),Bo.forEach(o),ac=h(ne),_(Je.$$.fragment,ne),dc=h(ne),Wn=r(ne,"P",{});var Xh=d(Wn);ic=a(Xh,"Example:"),Xh.forEach(o),cc=h(ne),_(ro.$$.fragment,ne),ne.forEach(o),oe.forEach(o),this.h()},h(){i(u,"name","hf:doc:metadata"),i(u,"content",JSON.stringify(cl)),i(y,"id","prophetnet"),i(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(y,"href","#prophetnet"),i(m,"class","relative group"),i(Qe,"href","https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title"),i(Qe,"rel","nofollow"),i(Ee,"id","overview"),i(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Ee,"href","#overview"),i(ae,"class","relative group"),i(Ze,"href","https://arxiv.org/abs/2001.04063"),i(Ze,"rel","nofollow"),i(et,"href","https://github.com/microsoft/ProphetNet"),i(et,"rel","nofollow"),i(Se,"id","transformers.ProphetNetConfig"),i(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Se,"href","#transformers.ProphetNetConfig"),i(de,"class","relative group"),i(ho,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetModel"),i(lo,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),i(po,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),i(U,"class","docstring"),i(De,"id","transformers.ProphetNetTokenizer"),i(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(De,"href","#transformers.ProphetNetTokenizer"),i(ce,"class","relative group"),i(uo,"href","/docs/transformers/master/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),i(R,"class","docstring"),i(Oe,"class","docstring"),i(W,"class","docstring"),i(Le,"class","docstring"),i($,"class","docstring"),i(je,"id","transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput"),i(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(je,"href","#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput"),i(le,"class","relative group"),i(pe,"class","docstring"),i(ue,"class","docstring"),i(me,"class","docstring"),i(fe,"class","docstring"),i(Ae,"id","transformers.ProphetNetModel"),i(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Ae,"href","#transformers.ProphetNetModel"),i(_e,"class","relative group"),i(_o,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),i(wt,"href","https://github.com/microsoft/ProphetNet"),i(wt,"rel","nofollow"),i(Pt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(Pt,"rel","nofollow"),i(go,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetModel"),i(S,"class","docstring"),i(E,"class","docstring"),i(Be,"id","transformers.ProphetNetEncoder"),i(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Be,"href","#transformers.ProphetNetEncoder"),i(ke,"class","relative group"),i(vo,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),i(Mt,"href","https://github.com/microsoft/ProphetNet"),i(Mt,"rel","nofollow"),i(xt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(xt,"rel","nofollow"),i(ko,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetEncoder"),i(To,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetEncoder"),i(D,"class","docstring"),i(F,"class","docstring"),i(We,"id","transformers.ProphetNetDecoder"),i(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(We,"href","#transformers.ProphetNetDecoder"),i(we,"class","relative group"),i(bo,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),i(jt,"href","https://github.com/microsoft/ProphetNet"),i(jt,"rel","nofollow"),i(It,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(It,"rel","nofollow"),i(wo,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetEncoder"),i(yo,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetDecoder"),i(O,"class","docstring"),i(M,"class","docstring"),i(Ve,"id","transformers.ProphetNetForConditionalGeneration"),i(Ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Ve,"href","#transformers.ProphetNetForConditionalGeneration"),i(Ne,"class","relative group"),i(Po,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),i(Ut,"href","https://github.com/microsoft/ProphetNet"),i(Ut,"rel","nofollow"),i(Jt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(Jt,"rel","nofollow"),i(No,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),i(L,"class","docstring"),i(x,"class","docstring"),i(Re,"id","transformers.ProphetNetForCausalLM"),i(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Re,"href","#transformers.ProphetNetForCausalLM"),i($e,"class","relative group"),i(qo,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),i(eo,"href","https://github.com/microsoft/ProphetNet"),i(eo,"rel","nofollow"),i(oo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(oo,"rel","nofollow"),i(zo,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),i(j,"class","docstring"),i(C,"class","docstring")},m(t,l){e(document.head,u),p(t,P,l),p(t,m,l),e(m,y),e(y,N),g(w,N,null),e(m,b),e(m,q),e(q,Xr),p(t,Jn,l),p(t,se,l),e(se,Wo),e(Wo,Zr),e(se,es),e(se,Qe),e(Qe,ts),e(se,os),p(t,Yn,l),p(t,ae,l),e(ae,Ee),e(Ee,Ho),g(Xe,Ho,null),e(ae,ns),e(ae,Vo),e(Vo,rs),p(t,Kn,l),p(t,xe,l),e(xe,ss),e(xe,Ze),e(Ze,as),e(xe,ds),p(t,Qn,l),p(t,ao,l),e(ao,is),p(t,Xn,l),p(t,io,l),e(io,cs),p(t,Zn,l),p(t,co,l),e(co,Uo),e(Uo,hs),p(t,er,l),p(t,Ce,l),e(Ce,ls),e(Ce,et),e(et,ps),e(Ce,us),p(t,tr,l),p(t,de,l),e(de,Se),e(Se,Ro),g(tt,Ro,null),e(de,ms),e(de,Jo),e(Jo,fs),p(t,or,l),p(t,U,l),g(ot,U,null),e(U,_s),e(U,nt),e(nt,gs),e(nt,ho),e(ho,vs),e(nt,ks),e(U,Ts),e(U,ie),e(ie,bs),e(ie,lo),e(lo,ws),e(ie,ys),e(ie,po),e(po,Ps),e(ie,Ns),p(t,nr,l),p(t,ce,l),e(ce,De),e(De,Yo),g(rt,Yo,null),e(ce,qs),e(ce,Ko),e(Ko,zs),p(t,rr,l),p(t,$,l),g(st,$,null),e($,$s),e($,Qo),e(Qo,Fs),e($,Ms),e($,at),e(at,Es),e(at,uo),e(uo,xs),e(at,Cs),e($,Ss),e($,R),g(dt,R,null),e(R,Ds),e(R,Xo),e(Xo,Os),e(R,Ls),e(R,it),e(it,mo),e(mo,js),e(mo,Zo),e(Zo,As),e(it,Is),e(it,fo),e(fo,Bs),e(fo,en),e(en,Gs),e($,Ws),e($,Oe),g(ct,Oe,null),e(Oe,Hs),e(Oe,tn),e(tn,Vs),e($,Us),e($,W),g(ht,W,null),e(W,Rs),e(W,on),e(on,Js),e(W,Ys),g(lt,W,null),e(W,Ks),e(W,he),e(he,Qs),e(he,nn),e(nn,Xs),e(he,Zs),e(he,rn),e(rn,ea),e(he,ta),e($,oa),e($,Le),g(pt,Le,null),e(Le,na),e(Le,ut),e(ut,ra),e(ut,sn),e(sn,sa),e(ut,aa),p(t,sr,l),p(t,le,l),e(le,je),e(je,an),g(mt,an,null),e(le,da),e(le,dn),e(dn,ia),p(t,ar,l),p(t,pe,l),g(ft,pe,null),e(pe,ca),e(pe,cn),e(cn,ha),p(t,dr,l),p(t,ue,l),g(_t,ue,null),e(ue,la),e(ue,hn),e(hn,pa),p(t,ir,l),p(t,me,l),g(gt,me,null),e(me,ua),e(me,ln),e(ln,ma),p(t,cr,l),p(t,fe,l),g(vt,fe,null),e(fe,fa),e(fe,pn),e(pn,_a),p(t,hr,l),p(t,_e,l),e(_e,Ae),e(Ae,un),g(kt,un,null),e(_e,ga),e(_e,mn),e(mn,va),p(t,lr,l),p(t,E,l),g(Tt,E,null),e(E,ka),e(E,bt),e(bt,Ta),e(bt,_o),e(_o,ba),e(bt,wa),e(E,ya),e(E,ge),e(ge,Pa),e(ge,wt),e(wt,Na),e(ge,qa),e(ge,fn),e(fn,za),e(ge,$a),e(E,Fa),e(E,yt),e(yt,Ma),e(yt,Pt),e(Pt,Ea),e(yt,xa),e(E,Ca),e(E,S),g(Nt,S,null),e(S,Sa),e(S,ve),e(ve,Da),e(ve,go),e(go,Oa),e(ve,La),e(ve,_n),e(_n,ja),e(ve,Aa),e(S,Ia),g(Ie,S,null),e(S,Ba),e(S,gn),e(gn,Ga),e(S,Wa),g(qt,S,null),p(t,pr,l),p(t,ke,l),e(ke,Be),e(Be,vn),g(zt,vn,null),e(ke,Ha),e(ke,kn),e(kn,Va),p(t,ur,l),p(t,F,l),g($t,F,null),e(F,Ua),e(F,Ft),e(Ft,Ra),e(Ft,vo),e(vo,Ja),e(Ft,Ya),e(F,Ka),e(F,Te),e(Te,Qa),e(Te,Mt),e(Mt,Xa),e(Te,Za),e(Te,Tn),e(Tn,ed),e(Te,td),e(F,od),e(F,Et),e(Et,nd),e(Et,xt),e(xt,rd),e(Et,sd),e(F,ad),e(F,B),e(B,dd),e(B,bn),e(bn,id),e(B,cd),e(B,wn),e(wn,hd),e(B,ld),e(B,yn),e(yn,pd),e(B,ud),e(B,ko),e(ko,md),e(B,fd),e(F,_d),e(F,D),g(Ct,D,null),e(D,gd),e(D,be),e(be,vd),e(be,To),e(To,kd),e(be,Td),e(be,Pn),e(Pn,bd),e(be,wd),e(D,yd),g(Ge,D,null),e(D,Pd),e(D,Nn),e(Nn,Nd),e(D,qd),g(St,D,null),p(t,mr,l),p(t,we,l),e(we,We),e(We,qn),g(Dt,qn,null),e(we,zd),e(we,zn),e(zn,$d),p(t,fr,l),p(t,M,l),g(Ot,M,null),e(M,Fd),e(M,Lt),e(Lt,Md),e(Lt,bo),e(bo,Ed),e(Lt,xd),e(M,Cd),e(M,ye),e(ye,Sd),e(ye,jt),e(jt,Dd),e(ye,Od),e(ye,$n),e($n,Ld),e(ye,jd),e(M,Ad),e(M,At),e(At,Id),e(At,It),e(It,Bd),e(At,Gd),e(M,Wd),e(M,G),e(G,Hd),e(G,Fn),e(Fn,Vd),e(G,Ud),e(G,Mn),e(Mn,Rd),e(G,Jd),e(G,En),e(En,Yd),e(G,Kd),e(G,wo),e(wo,Qd),e(G,Xd),e(M,Zd),e(M,O),g(Bt,O,null),e(O,ei),e(O,Pe),e(Pe,ti),e(Pe,yo),e(yo,oi),e(Pe,ni),e(Pe,xn),e(xn,ri),e(Pe,si),e(O,ai),g(He,O,null),e(O,di),e(O,Cn),e(Cn,ii),e(O,ci),g(Gt,O,null),p(t,_r,l),p(t,Ne,l),e(Ne,Ve),e(Ve,Sn),g(Wt,Sn,null),e(Ne,hi),e(Ne,Dn),e(Dn,li),p(t,gr,l),p(t,x,l),g(Ht,x,null),e(x,pi),e(x,Vt),e(Vt,ui),e(Vt,Po),e(Po,mi),e(Vt,fi),e(x,_i),e(x,qe),e(qe,gi),e(qe,Ut),e(Ut,vi),e(qe,ki),e(qe,On),e(On,Ti),e(qe,bi),e(x,wi),e(x,Rt),e(Rt,yi),e(Rt,Jt),e(Jt,Pi),e(Rt,Ni),e(x,qi),e(x,L),g(Yt,L,null),e(L,zi),e(L,ze),e(ze,$i),e(ze,No),e(No,Fi),e(ze,Mi),e(ze,Ln),e(Ln,Ei),e(ze,xi),e(L,Ci),g(Ue,L,null),e(L,Si),e(L,jn),e(jn,Di),e(L,Oi),g(Kt,L,null),p(t,vr,l),p(t,$e,l),e($e,Re),e(Re,An),g(Qt,An,null),e($e,Li),e($e,In),e(In,ji),p(t,kr,l),p(t,C,l),g(Xt,C,null),e(C,Ai),e(C,Zt),e(Zt,Ii),e(Zt,qo),e(qo,Bi),e(Zt,Gi),e(C,Wi),e(C,Fe),e(Fe,Hi),e(Fe,eo),e(eo,Vi),e(Fe,Ui),e(Fe,Bn),e(Bn,Ri),e(Fe,Ji),e(C,Yi),e(C,to),e(to,Ki),e(to,oo),e(oo,Qi),e(to,Xi),e(C,Zi),e(C,j),g(no,j,null),e(j,ec),e(j,Me),e(Me,tc),e(Me,zo),e(zo,oc),e(Me,nc),e(Me,Gn),e(Gn,rc),e(Me,sc),e(j,ac),g(Je,j,null),e(j,dc),e(j,Wn),e(Wn,ic),e(j,cc),g(ro,j,null),Tr=!0},p(t,[l]){const so={};l&2&&(so.$$scope={dirty:l,ctx:t}),Ie.$set(so);const Hn={};l&2&&(Hn.$$scope={dirty:l,ctx:t}),Ge.$set(Hn);const Vn={};l&2&&(Vn.$$scope={dirty:l,ctx:t}),He.$set(Vn);const Un={};l&2&&(Un.$$scope={dirty:l,ctx:t}),Ue.$set(Un);const Ye={};l&2&&(Ye.$$scope={dirty:l,ctx:t}),Je.$set(Ye)},i(t){Tr||(v(w.$$.fragment,t),v(Xe.$$.fragment,t),v(tt.$$.fragment,t),v(ot.$$.fragment,t),v(rt.$$.fragment,t),v(st.$$.fragment,t),v(dt.$$.fragment,t),v(ct.$$.fragment,t),v(ht.$$.fragment,t),v(lt.$$.fragment,t),v(pt.$$.fragment,t),v(mt.$$.fragment,t),v(ft.$$.fragment,t),v(_t.$$.fragment,t),v(gt.$$.fragment,t),v(vt.$$.fragment,t),v(kt.$$.fragment,t),v(Tt.$$.fragment,t),v(Nt.$$.fragment,t),v(Ie.$$.fragment,t),v(qt.$$.fragment,t),v(zt.$$.fragment,t),v($t.$$.fragment,t),v(Ct.$$.fragment,t),v(Ge.$$.fragment,t),v(St.$$.fragment,t),v(Dt.$$.fragment,t),v(Ot.$$.fragment,t),v(Bt.$$.fragment,t),v(He.$$.fragment,t),v(Gt.$$.fragment,t),v(Wt.$$.fragment,t),v(Ht.$$.fragment,t),v(Yt.$$.fragment,t),v(Ue.$$.fragment,t),v(Kt.$$.fragment,t),v(Qt.$$.fragment,t),v(Xt.$$.fragment,t),v(no.$$.fragment,t),v(Je.$$.fragment,t),v(ro.$$.fragment,t),Tr=!0)},o(t){k(w.$$.fragment,t),k(Xe.$$.fragment,t),k(tt.$$.fragment,t),k(ot.$$.fragment,t),k(rt.$$.fragment,t),k(st.$$.fragment,t),k(dt.$$.fragment,t),k(ct.$$.fragment,t),k(ht.$$.fragment,t),k(lt.$$.fragment,t),k(pt.$$.fragment,t),k(mt.$$.fragment,t),k(ft.$$.fragment,t),k(_t.$$.fragment,t),k(gt.$$.fragment,t),k(vt.$$.fragment,t),k(kt.$$.fragment,t),k(Tt.$$.fragment,t),k(Nt.$$.fragment,t),k(Ie.$$.fragment,t),k(qt.$$.fragment,t),k(zt.$$.fragment,t),k($t.$$.fragment,t),k(Ct.$$.fragment,t),k(Ge.$$.fragment,t),k(St.$$.fragment,t),k(Dt.$$.fragment,t),k(Ot.$$.fragment,t),k(Bt.$$.fragment,t),k(He.$$.fragment,t),k(Gt.$$.fragment,t),k(Wt.$$.fragment,t),k(Ht.$$.fragment,t),k(Yt.$$.fragment,t),k(Ue.$$.fragment,t),k(Kt.$$.fragment,t),k(Qt.$$.fragment,t),k(Xt.$$.fragment,t),k(no.$$.fragment,t),k(Je.$$.fragment,t),k(ro.$$.fragment,t),Tr=!1},d(t){o(u),t&&o(P),t&&o(m),T(w),t&&o(Jn),t&&o(se),t&&o(Yn),t&&o(ae),T(Xe),t&&o(Kn),t&&o(xe),t&&o(Qn),t&&o(ao),t&&o(Xn),t&&o(io),t&&o(Zn),t&&o(co),t&&o(er),t&&o(Ce),t&&o(tr),t&&o(de),T(tt),t&&o(or),t&&o(U),T(ot),t&&o(nr),t&&o(ce),T(rt),t&&o(rr),t&&o($),T(st),T(dt),T(ct),T(ht),T(lt),T(pt),t&&o(sr),t&&o(le),T(mt),t&&o(ar),t&&o(pe),T(ft),t&&o(dr),t&&o(ue),T(_t),t&&o(ir),t&&o(me),T(gt),t&&o(cr),t&&o(fe),T(vt),t&&o(hr),t&&o(_e),T(kt),t&&o(lr),t&&o(E),T(Tt),T(Nt),T(Ie),T(qt),t&&o(pr),t&&o(ke),T(zt),t&&o(ur),t&&o(F),T($t),T(Ct),T(Ge),T(St),t&&o(mr),t&&o(we),T(Dt),t&&o(fr),t&&o(M),T(Ot),T(Bt),T(He),T(Gt),t&&o(_r),t&&o(Ne),T(Wt),t&&o(gr),t&&o(x),T(Ht),T(Yt),T(Ue),T(Kt),t&&o(vr),t&&o($e),T(Qt),t&&o(kr),t&&o(C),T(Xt),T(no),T(Je),T(ro)}}}const cl={local:"prophetnet",sections:[{local:"overview",title:"Overview"},{local:"transformers.ProphetNetConfig",title:"ProphetNetConfig"},{local:"transformers.ProphetNetTokenizer",title:"ProphetNetTokenizer"},{local:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput",title:"ProphetNet specific outputs"},{local:"transformers.ProphetNetModel",title:"ProphetNetModel"},{local:"transformers.ProphetNetEncoder",title:"ProphetNetEncoder"},{local:"transformers.ProphetNetDecoder",title:"ProphetNetDecoder"},{local:"transformers.ProphetNetForConditionalGeneration",title:"ProphetNetForConditionalGeneration"},{local:"transformers.ProphetNetForCausalLM",title:"ProphetNetForCausalLM"}],title:"ProphetNet"};function hl(I,u,P){let{fw:m}=u;return I.$$set=y=>{"fw"in y&&P(0,m=y.fw)},[m]}class gl extends Zh{constructor(u){super();el(this,u,hl,il,tl,{fw:0})}}export{gl as default,cl as metadata};
