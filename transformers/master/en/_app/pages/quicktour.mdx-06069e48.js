import{S as Jf,i as Vf,s as Zf,e as n,k as f,w as d,t as a,M as Xf,c as l,d as s,m as u,a as i,x as _,h as o,b as m,F as t,g as p,y as g,q as v,o as y,B as $}from"../chunks/vendor-4833417e.js";import{T as ba}from"../chunks/Tip-fffd6df1.js";import{Y as Yf}from"../chunks/Youtube-27813aed.js";import{I as De}from"../chunks/IconCopyLink-4b81c553.js";import{C as R}from"../chunks/CodeBlock-90ffda97.js";import{C as fe}from"../chunks/CodeBlockFw-03f30a28.js";import{D as eu}from"../chunks/DocNotebookDropdown-ecff2a90.js";import"../chunks/CopyButton-04a16537.js";function tu(I){let h,b;return{c(){h=n("p"),b=a(`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`)},l(c){h=l(c,"P",{});var k=i(h);b=o(k,`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`),k.forEach(s)},m(c,k){p(c,h,k),t(h,b)},d(c){c&&s(h)}}}function su(I){let h,b,c,k,A,w,E,q;return{c(){h=n("p"),b=a("For more details about the "),c=n("a"),k=a("pipeline()"),A=a(" and associated tasks, refer to the documentation "),w=n("a"),E=a("here"),q=a("."),this.h()},l(S){h=l(S,"P",{});var j=i(h);b=o(j,"For more details about the "),c=l(j,"A",{href:!0});var D=i(c);k=o(D,"pipeline()"),D.forEach(s),A=o(j," and associated tasks, refer to the documentation "),w=l(j,"A",{href:!0});var U=i(w);E=o(U,"here"),U.forEach(s),q=o(j,"."),j.forEach(s),this.h()},h(){m(c,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(w,"href","./main_classes/pipelines")},m(S,j){p(S,h,j),t(h,b),t(h,c),t(c,k),t(h,A),t(h,w),t(w,E),t(h,q)},d(S){S&&s(h)}}}function au(I){let h,b,c,k,A,w,E,q;return{c(){h=n("p"),b=a("See the "),c=n("a"),k=a("task summary"),A=a(" for which "),w=n("a"),E=a("AutoModel"),q=a(" class to use for which task."),this.h()},l(S){h=l(S,"P",{});var j=i(h);b=o(j,"See the "),c=l(j,"A",{href:!0});var D=i(c);k=o(D,"task summary"),D.forEach(s),A=o(j," for which "),w=l(j,"A",{href:!0});var U=i(w);E=o(U,"AutoModel"),U.forEach(s),q=o(j," class to use for which task."),j.forEach(s),this.h()},h(){m(c,"href","./task_summary"),m(w,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModel")},m(S,j){p(S,h,j),t(h,b),t(h,c),t(c,k),t(h,A),t(h,w),t(w,E),t(h,q)},d(S){S&&s(h)}}}function ou(I){let h,b,c,k,A;return{c(){h=n("p"),b=a("All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),c=n("em"),k=a("before"),A=a(` the final activation
function (like softmax) because the final activation function is often fused with the loss.`)},l(w){h=l(w,"P",{});var E=i(h);b=o(E,"All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),c=l(E,"EM",{});var q=i(c);k=o(q,"before"),q.forEach(s),A=o(E,` the final activation
function (like softmax) because the final activation function is often fused with the loss.`),E.forEach(s)},m(w,E){p(w,h,E),t(h,b),t(h,c),t(c,k),t(h,A)},d(w){w&&s(h)}}}function ru(I){let h,b,c,k,A;return{c(){h=n("p"),b=a(`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),c=n("code"),k=a("None"),A=a(" are ignored.")},l(w){h=l(w,"P",{});var E=i(h);b=o(E,`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),c=l(E,"CODE",{});var q=i(c);k=o(q,"None"),q.forEach(s),A=o(E," are ignored."),E.forEach(s)},m(w,E){p(w,h,E),t(h,b),t(h,c),t(c,k),t(h,A)},d(w){w&&s(h)}}}function nu(I){let h,b,c,k,A,w,E,q,S,j,D,U,W,ur,qt,mr,hr,zt,cr,dr,Ea,ue,ja,se,me,As,Oe,_r,Ts,gr,Aa,Le,xt,vr,yr,Ta,Re,qa,he,$r,Ft,wr,kr,za,Ue,qs,br,Er,xa,T,zs,jr,Ar,xs,Tr,qr,Fs,zr,xr,Ps,Fr,Pr,Ss,Sr,Cr,Cs,Mr,Nr,Ms,Ir,Dr,Ns,Or,Fa,We,Is,Lr,Rr,Pa,H,Ds,Ur,Wr,Os,Hr,Br,Ls,Kr,Sa,He,Rs,Gr,Qr,Ca,ce,Us,Yr,Jr,Ws,Vr,Ma,de,Na,ae,_e,Hs,Be,Zr,Bs,Xr,Ia,ge,en,Pt,tn,sn,Da,St,an,Oa,Ke,La,ve,on,Ct,rn,nn,Ra,Ge,Ua,B,ln,Qe,pn,fn,Ks,un,mn,Wa,Ye,Ha,ye,hn,Mt,cn,dn,Ba,Je,Ka,K,_n,Nt,gn,vn,Ve,yn,$n,Ga,Ze,Qa,O,wn,It,kn,bn,Gs,En,jn,Qs,An,Tn,Ya,Xe,Ja,G,qn,et,zn,xn,tt,Fn,Pn,Va,st,Za,$e,Sn,Ys,Cn,Mn,Xa,at,eo,oe,we,Js,ot,Nn,Vs,In,to,C,Dn,Dt,On,Ln,rt,Rn,Un,Ot,Wn,Hn,nt,Bn,Kn,so,lt,ao,Q,Gn,Lt,Qn,Yn,Zs,Jn,Vn,oo,it,ro,Y,Zn,Rt,Xn,el,Xs,tl,sl,no,pt,lo,J,al,Ut,ol,rl,Wt,nl,ll,io,re,ke,ea,ft,il,ta,pl,po,ut,fo,z,fl,Ht,ul,ml,Bt,hl,cl,Kt,dl,_l,Gt,gl,vl,sa,yl,$l,Qt,wl,kl,uo,V,bl,aa,El,jl,Yt,Al,Tl,mo,ne,be,oa,mt,ql,ra,zl,ho,Z,xl,na,Fl,Pl,Jt,Sl,Cl,co,Ee,Ml,Vt,Nl,Il,_o,ht,go,je,Dl,la,Ol,Ll,vo,Zt,Rl,yo,ct,$o,Xt,Ul,wo,Ae,es,ts,Wl,Hl,Bl,ss,as,Kl,Gl,ko,Te,Ql,os,Yl,Jl,bo,dt,Eo,qe,Vl,rs,Zl,Xl,jo,le,ze,ia,_t,ei,pa,ti,Ao,F,si,ns,ai,oi,ls,ri,ni,is,li,ii,ps,pi,fi,fs,ui,mi,To,gt,qo,xe,zo,Fe,hi,fa,ci,di,xo,vt,Fo,X,_i,ua,gi,vi,ma,yi,$i,Po,yt,So,Pe,Co,x,wi,$t,ha,ki,bi,wt,ca,Ei,ji,us,Ai,Ti,da,qi,zi,kt,xi,Fi,ms,Pi,Si,Mo,Se,No,ie,Ce,_a,bt,Ci,ga,Mi,Io,Me,Ni,hs,Ii,Di,Do,Et,Oo,Ne,Oi,cs,Li,Ri,Lo,jt,Ro,ee,Ui,va,Wi,Hi,ya,Bi,Ki,Uo,At,Wo;return w=new De({}),D=new eu({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/tensorflow/quicktour.ipynb"}]}}),ue=new ba({props:{$$slots:{default:[tu]},$$scope:{ctx:I}}}),Oe=new De({}),Re=new Yf({props:{id:"tiZFewofSLM"}}),de=new ba({props:{$$slots:{default:[su]},$$scope:{ctx:I}}}),Be=new De({}),Ke=new fe({props:{pt:{code:"pip install torch",highlighted:"pip install torch"},tf:{code:"pip install tensorflow",highlighted:"pip install tensorflow"}}}),Ge=new R({props:{code:`from transformers import pipeline

classifier = pipeline("sentiment-analysis"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),Ye=new R({props:{code:'classifier("We are very happy to show you the \u{1F917} Transformers library."),',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
[{<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;POSITIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9998</span>}]`}}),Je=new R({props:{code:`results = classifier(["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">{result[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, with score: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>`}}),Ze=new R({props:{code:"pip install datasets ,",highlighted:"pip install datasets "}}),Xe=new R({props:{code:`from transformers import pipeline

speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h", device=0),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, device=<span class="hljs-number">0</span>)`}}),st=new R({props:{code:`import datasets

dataset = datasets.load_dataset("superb", name="asr", split="test"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> datasets

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = datasets.load_dataset(<span class="hljs-string">&quot;superb&quot;</span>, name=<span class="hljs-string">&quot;asr&quot;</span>, split=<span class="hljs-string">&quot;test&quot;</span>)`}}),at=new R({props:{code:`from transformers.pipelines.pt_utils import KeyDataset
from tqdm.auto import tqdm

for out in tqdm(speech_recognizer(KeyDataset(dataset, "file"))):
    print(out),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.pipelines.pt_utils <span class="hljs-keyword">import</span> KeyDataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> out <span class="hljs-keyword">in</span> tqdm(speech_recognizer(KeyDataset(dataset, <span class="hljs-string">&quot;file&quot;</span>))):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(out)
{<span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOWER FAT AND SAUCE&quot;</span>}`}}),ot=new De({}),lt=new R({props:{code:'model_name = "nlptown/bert-base-multilingual-uncased-sentiment",',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>'}}),it=new fe({props:{pt:{code:`from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`},tf:{code:`from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}}),pt=new R({props:{code:`classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers."),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.&quot;</span>)
[{<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;5 stars&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.7272651791572571</span>}]`}}),ft=new De({}),ut=new Yf({props:{id:"AhChOFRegn4"}}),mt=new De({}),ht=new R({props:{code:`from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),ct=new R({props:{code:`encoding = tokenizer("We are very happy to show you the \u{1F917} Transformers library.")
print(encoding),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&quot;input_ids&quot;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">2057</span>, <span class="hljs-number">2024</span>, <span class="hljs-number">2200</span>, <span class="hljs-number">3407</span>, <span class="hljs-number">2000</span>, <span class="hljs-number">2265</span>, <span class="hljs-number">2017</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">100</span>, <span class="hljs-number">19081</span>, <span class="hljs-number">3075</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&quot;attention_mask&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),dt=new fe({props:{pt:{code:`pt_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`},tf:{code:`tf_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`}}}),_t=new De({}),gt=new fe({props:{pt:{code:`from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`},tf:{code:`from transformers import TFAutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`}}}),xe=new ba({props:{$$slots:{default:[au]},$$scope:{ctx:I}}}),vt=new fe({props:{pt:{code:"pt_outputs = pt_model(**pt_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)'},tf:{code:"tf_outputs = tf_model(tf_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)'}}}),yt=new fe({props:{pt:{code:`from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">2.2043e-04</span>, <span class="hljs-number">9.9978e-01</span>],
        [<span class="hljs-number">5.3086e-01</span>, <span class="hljs-number">4.6914e-01</span>]], grad_fn=&lt;SoftmaxBackward&gt;)`},tf:{code:`import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
print(tf_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tf_predictions)
tf.Tensor(
[[<span class="hljs-number">2.2043e-04</span> <span class="hljs-number">9.9978e-01</span>]
 [<span class="hljs-number">5.3086e-01</span> <span class="hljs-number">4.6914e-01</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32)`}}}),Pe=new ba({props:{$$slots:{default:[ou]},$$scope:{ctx:I}}}),Se=new ba({props:{$$slots:{default:[ru]},$$scope:{ctx:I}}}),bt=new De({}),Et=new fe({props:{pt:{code:`pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)`},tf:{code:`tf_save_directory = "./tf_save_pretrained"
tokenizer.save_pretrained(tf_save_directory)
tf_model.save_pretrained(tf_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`}}}),jt=new fe({props:{pt:{code:'pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)'},tf:{code:'tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)'}}}),At=new fe({props:{pt:{code:`from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`},tf:{code:`from transformers import TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`}}}),{c(){h=n("meta"),b=f(),c=n("h1"),k=n("a"),A=n("span"),d(w.$$.fragment),E=f(),q=n("span"),S=a("Quick tour"),j=f(),d(D.$$.fragment),U=f(),W=n("p"),ur=a("Get up and running with \u{1F917} Transformers! Start using the "),qt=n("a"),mr=a("pipeline()"),hr=a(" for rapid inference, and quickly load a pretrained model and tokenizer with an "),zt=n("a"),cr=a("AutoClass"),dr=a(" to solve your text, vision or audio task."),Ea=f(),d(ue.$$.fragment),ja=f(),se=n("h2"),me=n("a"),As=n("span"),d(Oe.$$.fragment),_r=f(),Ts=n("span"),gr=a("Pipeline"),Aa=f(),Le=n("p"),xt=n("a"),vr=a("pipeline()"),yr=a(" is the easiest way to use a pretrained model for a given task."),Ta=f(),d(Re.$$.fragment),qa=f(),he=n("p"),$r=a("The "),Ft=n("a"),wr=a("pipeline()"),kr=a(" supports many common tasks out-of-the-box:"),za=f(),Ue=n("p"),qs=n("strong"),br=a("Text"),Er=a(":"),xa=f(),T=n("ul"),zs=n("li"),jr=a("Sentiment analysis: classify the polarity of a given text."),Ar=f(),xs=n("li"),Tr=a("Text generation (in English): generate text from a given input."),qr=f(),Fs=n("li"),zr=a("Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),xr=f(),Ps=n("li"),Fr=a("Question answering: extract the answer from the context, given some context and a question."),Pr=f(),Ss=n("li"),Sr=a("Fill-mask: fill in the blank given a text with masked words."),Cr=f(),Cs=n("li"),Mr=a("Summarization: generate a summary of a long sequence of text or document."),Nr=f(),Ms=n("li"),Ir=a("Translation: translate text into another language."),Dr=f(),Ns=n("li"),Or=a("Feature extraction: create a tensor representation of the text."),Fa=f(),We=n("p"),Is=n("strong"),Lr=a("Image"),Rr=a(":"),Pa=f(),H=n("ul"),Ds=n("li"),Ur=a("Image classification: classify an image."),Wr=f(),Os=n("li"),Hr=a("Image segmentation: classify every pixel in an image."),Br=f(),Ls=n("li"),Kr=a("Object detection: detect objects within an image."),Sa=f(),He=n("p"),Rs=n("strong"),Gr=a("Audio"),Qr=a(":"),Ca=f(),ce=n("ul"),Us=n("li"),Yr=a("Audio classification: assign a label to a given segment of audio."),Jr=f(),Ws=n("li"),Vr=a("Automatic speech recognition (ASR): transcribe audio data into text."),Ma=f(),d(de.$$.fragment),Na=f(),ae=n("h3"),_e=n("a"),Hs=n("span"),d(Be.$$.fragment),Zr=f(),Bs=n("span"),Xr=a("Pipeline usage"),Ia=f(),ge=n("p"),en=a("In the following example, you will use the "),Pt=n("a"),tn=a("pipeline()"),sn=a(" for sentiment analysis."),Da=f(),St=n("p"),an=a("Install the following dependencies if you haven\u2019t already:"),Oa=f(),d(Ke.$$.fragment),La=f(),ve=n("p"),on=a("Import "),Ct=n("a"),rn=a("pipeline()"),nn=a(" and specify the task you want to complete:"),Ra=f(),d(Ge.$$.fragment),Ua=f(),B=n("p"),ln=a("The pipeline downloads and caches a default "),Qe=n("a"),pn=a("pretrained model"),fn=a(" and tokenizer for sentiment analysis. Now you can use the "),Ks=n("code"),un=a("classifier"),mn=a(" on your target text:"),Wa=f(),d(Ye.$$.fragment),Ha=f(),ye=n("p"),hn=a("For more than one sentence, pass a list of sentences to the "),Mt=n("a"),cn=a("pipeline()"),dn=a(" which returns a list of dictionaries:"),Ba=f(),d(Je.$$.fragment),Ka=f(),K=n("p"),_n=a("The "),Nt=n("a"),gn=a("pipeline()"),vn=a(" can also iterate over an entire dataset. Start by installing the "),Ve=n("a"),yn=a("\u{1F917} Datasets"),$n=a(" library:"),Ga=f(),d(Ze.$$.fragment),Qa=f(),O=n("p"),wn=a("Create a "),It=n("a"),kn=a("pipeline()"),bn=a(" with the task you want to solve for and the model you want to use. Set the "),Gs=n("code"),En=a("device"),jn=a(" parameter to "),Qs=n("code"),An=a("0"),Tn=a(" to place the tensors on a CUDA device:"),Ya=f(),d(Xe.$$.fragment),Ja=f(),G=n("p"),qn=a("Next, load a dataset (see the \u{1F917} Datasets "),et=n("a"),zn=a("Quick Start"),xn=a(" for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),tt=n("a"),Fn=a("SUPERB"),Pn=a(" dataset:"),Va=f(),d(st.$$.fragment),Za=f(),$e=n("p"),Sn=a("Now you can iterate over the dataset with the pipeline. "),Ys=n("code"),Cn=a("KeyDataset"),Mn=a(" retrieves the item in the dictionary returned by the dataset:"),Xa=f(),d(at.$$.fragment),eo=f(),oe=n("h3"),we=n("a"),Js=n("span"),d(ot.$$.fragment),Nn=f(),Vs=n("span"),In=a("Use another model and tokenizer in the pipeline"),to=f(),C=n("p"),Dn=a("The "),Dt=n("a"),On=a("pipeline()"),Ln=a(" can accommodate any model from the "),rt=n("a"),Rn=a("Model Hub"),Un=a(", making it easy to adapt the "),Ot=n("a"),Wn=a("pipeline()"),Hn=a(" for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),nt=n("a"),Bn=a("BERT model"),Kn=a(" fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),so=f(),d(lt.$$.fragment),ao=f(),Q=n("p"),Gn=a("Use the "),Lt=n("a"),Qn=a("AutoModelForSequenceClassification"),Yn=a(" and [\u2018AutoTokenizer\u2019] to load the pretrained model and it\u2019s associated tokenizer (more on an "),Zs=n("code"),Jn=a("AutoClass"),Vn=a(" below):"),oo=f(),d(it.$$.fragment),ro=f(),Y=n("p"),Zn=a("Then you can specify the model and tokenizer in the "),Rt=n("a"),Xn=a("pipeline()"),el=a(", and apply the "),Xs=n("code"),tl=a("classifier"),sl=a(" on your target text:"),no=f(),d(pt.$$.fragment),lo=f(),J=n("p"),al=a("If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),Ut=n("a"),ol=a("fine-tuning tutorial"),rl=a(" to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),Wt=n("a"),nl=a("here"),ll=a(") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),io=f(),re=n("h2"),ke=n("a"),ea=n("span"),d(ft.$$.fragment),il=f(),ta=n("span"),pl=a("AutoClass"),po=f(),d(ut.$$.fragment),fo=f(),z=n("p"),fl=a("Under the hood, the "),Ht=n("a"),ul=a("AutoModelForSequenceClassification"),ml=a(" and "),Bt=n("a"),hl=a("AutoTokenizer"),cl=a(" classes work together to power the "),Kt=n("a"),dl=a("pipeline()"),_l=a(". An "),Gt=n("a"),gl=a("AutoClass"),vl=a(" is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),sa=n("code"),yl=a("AutoClass"),$l=a(" for your task and it\u2019s associated tokenizer with "),Qt=n("a"),wl=a("AutoTokenizer"),kl=a("."),uo=f(),V=n("p"),bl=a("Let\u2019s return to our example and see how you can use the "),aa=n("code"),El=a("AutoClass"),jl=a(" to replicate the results of the "),Yt=n("a"),Al=a("pipeline()"),Tl=a("."),mo=f(),ne=n("h3"),be=n("a"),oa=n("span"),d(mt.$$.fragment),ql=f(),ra=n("span"),zl=a("AutoTokenizer"),ho=f(),Z=n("p"),xl=a("A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),na=n("em"),Fl=a("tokens"),Pl=a(". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),Jt=n("a"),Sl=a("here"),Cl=a("). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),co=f(),Ee=n("p"),Ml=a("Load a tokenizer with "),Vt=n("a"),Nl=a("AutoTokenizer"),Il=a(":"),_o=f(),d(ht.$$.fragment),go=f(),je=n("p"),Dl=a("Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),la=n("em"),Ol=a("vocabulary"),Ll=a("."),vo=f(),Zt=n("p"),Rl=a("Pass your text to the tokenizer:"),yo=f(),d(ct.$$.fragment),$o=f(),Xt=n("p"),Ul=a("The tokenizer will return a dictionary containing:"),wo=f(),Ae=n("ul"),es=n("li"),ts=n("a"),Wl=a("input_ids"),Hl=a(": numerical representions of your tokens."),Bl=f(),ss=n("li"),as=n("a"),Kl=a("atttention_mask"),Gl=a(": indicates which tokens should be attended to."),ko=f(),Te=n("p"),Ql=a("Just like the "),os=n("a"),Yl=a("pipeline()"),Jl=a(", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),bo=f(),d(dt.$$.fragment),Eo=f(),qe=n("p"),Vl=a("Read the "),rs=n("a"),Zl=a("preprocessing"),Xl=a(" tutorial for more details about tokenization."),jo=f(),le=n("h3"),ze=n("a"),ia=n("span"),d(_t.$$.fragment),ei=f(),pa=n("span"),ti=a("AutoModel"),Ao=f(),F=n("p"),si=a("\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),ns=n("a"),ai=a("AutoModel"),oi=a(" like you would load an "),ls=n("a"),ri=a("AutoTokenizer"),ni=a(". The only difference is selecting the correct "),is=n("a"),li=a("AutoModel"),ii=a(" for the task. Since you are doing text - or sequence - classification, load "),ps=n("a"),pi=a("AutoModelForSequenceClassification"),fi=a(". The TensorFlow equivalent is simply "),fs=n("a"),ui=a("TFAutoModelForSequenceClassification"),mi=a(":"),To=f(),d(gt.$$.fragment),qo=f(),d(xe.$$.fragment),zo=f(),Fe=n("p"),hi=a("Now you can pass your preprocessed batch of inputs directly to the model. If you are using a PyTorch model, unpack the dictionary by adding "),fa=n("code"),ci=a("**"),di=a(". For TensorFlow models, pass the dictionary keys directly to the tensors:"),xo=f(),d(vt.$$.fragment),Fo=f(),X=n("p"),_i=a("The model outputs the final activations in the "),ua=n("code"),gi=a("logits"),vi=a(" attribute. Apply the softmax function to the "),ma=n("code"),yi=a("logits"),$i=a(" to retrieve the probabilities:"),Po=f(),d(yt.$$.fragment),So=f(),d(Pe.$$.fragment),Co=f(),x=n("p"),wi=a("Models are a standard "),$t=n("a"),ha=n("code"),ki=a("torch.nn.Module"),bi=a(" or a "),wt=n("a"),ca=n("code"),Ei=a("tf.keras.Model"),ji=a(" so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),us=n("a"),Ai=a("Trainer"),Ti=a(" class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),da=n("code"),qi=a("fit"),zi=a(" method from "),kt=n("a"),xi=a("Keras"),Fi=a(". Refer to the "),ms=n("a"),Pi=a("training tutorial"),Si=a(" for more details."),Mo=f(),d(Se.$$.fragment),No=f(),ie=n("h3"),Ce=n("a"),_a=n("span"),d(bt.$$.fragment),Ci=f(),ga=n("span"),Mi=a("Save a model"),Io=f(),Me=n("p"),Ni=a("Once your model is fine-tuned, you can save it with its tokenizer using "),hs=n("a"),Ii=a("PreTrainedModel.save_pretrained()"),Di=a(":"),Do=f(),d(Et.$$.fragment),Oo=f(),Ne=n("p"),Oi=a("When you are ready to use the model again, reload it with "),cs=n("a"),Li=a("PreTrainedModel.from_pretrained()"),Ri=a(":"),Lo=f(),d(jt.$$.fragment),Ro=f(),ee=n("p"),Ui=a("One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),va=n("code"),Wi=a("from_pt"),Hi=a(" or "),ya=n("code"),Bi=a("from_tf"),Ki=a(" parameter can convert the model from one framework to the other:"),Uo=f(),d(At.$$.fragment),this.h()},l(e){const r=Xf('[data-svelte="svelte-1phssyn"]',document.head);h=l(r,"META",{name:!0,content:!0}),r.forEach(s),b=u(e),c=l(e,"H1",{class:!0});var Tt=i(c);k=l(Tt,"A",{id:!0,class:!0,href:!0});var $a=i(k);A=l($a,"SPAN",{});var wa=i(A);_(w.$$.fragment,wa),wa.forEach(s),$a.forEach(s),E=u(Tt),q=l(Tt,"SPAN",{});var ka=i(q);S=o(ka,"Quick tour"),ka.forEach(s),Tt.forEach(s),j=u(e),_(D.$$.fragment,e),U=u(e),W=l(e,"P",{});var pe=i(W);ur=o(pe,"Get up and running with \u{1F917} Transformers! Start using the "),qt=l(pe,"A",{href:!0});var Xi=i(qt);mr=o(Xi,"pipeline()"),Xi.forEach(s),hr=o(pe," for rapid inference, and quickly load a pretrained model and tokenizer with an "),zt=l(pe,"A",{href:!0});var ep=i(zt);cr=o(ep,"AutoClass"),ep.forEach(s),dr=o(pe," to solve your text, vision or audio task."),pe.forEach(s),Ea=u(e),_(ue.$$.fragment,e),ja=u(e),se=l(e,"H2",{class:!0});var Ho=i(se);me=l(Ho,"A",{id:!0,class:!0,href:!0});var tp=i(me);As=l(tp,"SPAN",{});var sp=i(As);_(Oe.$$.fragment,sp),sp.forEach(s),tp.forEach(s),_r=u(Ho),Ts=l(Ho,"SPAN",{});var ap=i(Ts);gr=o(ap,"Pipeline"),ap.forEach(s),Ho.forEach(s),Aa=u(e),Le=l(e,"P",{});var Gi=i(Le);xt=l(Gi,"A",{href:!0});var op=i(xt);vr=o(op,"pipeline()"),op.forEach(s),yr=o(Gi," is the easiest way to use a pretrained model for a given task."),Gi.forEach(s),Ta=u(e),_(Re.$$.fragment,e),qa=u(e),he=l(e,"P",{});var Bo=i(he);$r=o(Bo,"The "),Ft=l(Bo,"A",{href:!0});var rp=i(Ft);wr=o(rp,"pipeline()"),rp.forEach(s),kr=o(Bo," supports many common tasks out-of-the-box:"),Bo.forEach(s),za=u(e),Ue=l(e,"P",{});var Qi=i(Ue);qs=l(Qi,"STRONG",{});var np=i(qs);br=o(np,"Text"),np.forEach(s),Er=o(Qi,":"),Qi.forEach(s),xa=u(e),T=l(e,"UL",{});var P=i(T);zs=l(P,"LI",{});var lp=i(zs);jr=o(lp,"Sentiment analysis: classify the polarity of a given text."),lp.forEach(s),Ar=u(P),xs=l(P,"LI",{});var ip=i(xs);Tr=o(ip,"Text generation (in English): generate text from a given input."),ip.forEach(s),qr=u(P),Fs=l(P,"LI",{});var pp=i(Fs);zr=o(pp,"Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),pp.forEach(s),xr=u(P),Ps=l(P,"LI",{});var fp=i(Ps);Fr=o(fp,"Question answering: extract the answer from the context, given some context and a question."),fp.forEach(s),Pr=u(P),Ss=l(P,"LI",{});var up=i(Ss);Sr=o(up,"Fill-mask: fill in the blank given a text with masked words."),up.forEach(s),Cr=u(P),Cs=l(P,"LI",{});var mp=i(Cs);Mr=o(mp,"Summarization: generate a summary of a long sequence of text or document."),mp.forEach(s),Nr=u(P),Ms=l(P,"LI",{});var hp=i(Ms);Ir=o(hp,"Translation: translate text into another language."),hp.forEach(s),Dr=u(P),Ns=l(P,"LI",{});var cp=i(Ns);Or=o(cp,"Feature extraction: create a tensor representation of the text."),cp.forEach(s),P.forEach(s),Fa=u(e),We=l(e,"P",{});var Yi=i(We);Is=l(Yi,"STRONG",{});var dp=i(Is);Lr=o(dp,"Image"),dp.forEach(s),Rr=o(Yi,":"),Yi.forEach(s),Pa=u(e),H=l(e,"UL",{});var ds=i(H);Ds=l(ds,"LI",{});var _p=i(Ds);Ur=o(_p,"Image classification: classify an image."),_p.forEach(s),Wr=u(ds),Os=l(ds,"LI",{});var gp=i(Os);Hr=o(gp,"Image segmentation: classify every pixel in an image."),gp.forEach(s),Br=u(ds),Ls=l(ds,"LI",{});var vp=i(Ls);Kr=o(vp,"Object detection: detect objects within an image."),vp.forEach(s),ds.forEach(s),Sa=u(e),He=l(e,"P",{});var Ji=i(He);Rs=l(Ji,"STRONG",{});var yp=i(Rs);Gr=o(yp,"Audio"),yp.forEach(s),Qr=o(Ji,":"),Ji.forEach(s),Ca=u(e),ce=l(e,"UL",{});var Ko=i(ce);Us=l(Ko,"LI",{});var $p=i(Us);Yr=o($p,"Audio classification: assign a label to a given segment of audio."),$p.forEach(s),Jr=u(Ko),Ws=l(Ko,"LI",{});var wp=i(Ws);Vr=o(wp,"Automatic speech recognition (ASR): transcribe audio data into text."),wp.forEach(s),Ko.forEach(s),Ma=u(e),_(de.$$.fragment,e),Na=u(e),ae=l(e,"H3",{class:!0});var Go=i(ae);_e=l(Go,"A",{id:!0,class:!0,href:!0});var kp=i(_e);Hs=l(kp,"SPAN",{});var bp=i(Hs);_(Be.$$.fragment,bp),bp.forEach(s),kp.forEach(s),Zr=u(Go),Bs=l(Go,"SPAN",{});var Ep=i(Bs);Xr=o(Ep,"Pipeline usage"),Ep.forEach(s),Go.forEach(s),Ia=u(e),ge=l(e,"P",{});var Qo=i(ge);en=o(Qo,"In the following example, you will use the "),Pt=l(Qo,"A",{href:!0});var jp=i(Pt);tn=o(jp,"pipeline()"),jp.forEach(s),sn=o(Qo," for sentiment analysis."),Qo.forEach(s),Da=u(e),St=l(e,"P",{});var Ap=i(St);an=o(Ap,"Install the following dependencies if you haven\u2019t already:"),Ap.forEach(s),Oa=u(e),_(Ke.$$.fragment,e),La=u(e),ve=l(e,"P",{});var Yo=i(ve);on=o(Yo,"Import "),Ct=l(Yo,"A",{href:!0});var Tp=i(Ct);rn=o(Tp,"pipeline()"),Tp.forEach(s),nn=o(Yo," and specify the task you want to complete:"),Yo.forEach(s),Ra=u(e),_(Ge.$$.fragment,e),Ua=u(e),B=l(e,"P",{});var _s=i(B);ln=o(_s,"The pipeline downloads and caches a default "),Qe=l(_s,"A",{href:!0,rel:!0});var qp=i(Qe);pn=o(qp,"pretrained model"),qp.forEach(s),fn=o(_s," and tokenizer for sentiment analysis. Now you can use the "),Ks=l(_s,"CODE",{});var zp=i(Ks);un=o(zp,"classifier"),zp.forEach(s),mn=o(_s," on your target text:"),_s.forEach(s),Wa=u(e),_(Ye.$$.fragment,e),Ha=u(e),ye=l(e,"P",{});var Jo=i(ye);hn=o(Jo,"For more than one sentence, pass a list of sentences to the "),Mt=l(Jo,"A",{href:!0});var xp=i(Mt);cn=o(xp,"pipeline()"),xp.forEach(s),dn=o(Jo," which returns a list of dictionaries:"),Jo.forEach(s),Ba=u(e),_(Je.$$.fragment,e),Ka=u(e),K=l(e,"P",{});var gs=i(K);_n=o(gs,"The "),Nt=l(gs,"A",{href:!0});var Fp=i(Nt);gn=o(Fp,"pipeline()"),Fp.forEach(s),vn=o(gs," can also iterate over an entire dataset. Start by installing the "),Ve=l(gs,"A",{href:!0,rel:!0});var Pp=i(Ve);yn=o(Pp,"\u{1F917} Datasets"),Pp.forEach(s),$n=o(gs," library:"),gs.forEach(s),Ga=u(e),_(Ze.$$.fragment,e),Qa=u(e),O=l(e,"P",{});var Ie=i(O);wn=o(Ie,"Create a "),It=l(Ie,"A",{href:!0});var Sp=i(It);kn=o(Sp,"pipeline()"),Sp.forEach(s),bn=o(Ie," with the task you want to solve for and the model you want to use. Set the "),Gs=l(Ie,"CODE",{});var Cp=i(Gs);En=o(Cp,"device"),Cp.forEach(s),jn=o(Ie," parameter to "),Qs=l(Ie,"CODE",{});var Mp=i(Qs);An=o(Mp,"0"),Mp.forEach(s),Tn=o(Ie," to place the tensors on a CUDA device:"),Ie.forEach(s),Ya=u(e),_(Xe.$$.fragment,e),Ja=u(e),G=l(e,"P",{});var vs=i(G);qn=o(vs,"Next, load a dataset (see the \u{1F917} Datasets "),et=l(vs,"A",{href:!0,rel:!0});var Np=i(et);zn=o(Np,"Quick Start"),Np.forEach(s),xn=o(vs," for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),tt=l(vs,"A",{href:!0,rel:!0});var Ip=i(tt);Fn=o(Ip,"SUPERB"),Ip.forEach(s),Pn=o(vs," dataset:"),vs.forEach(s),Va=u(e),_(st.$$.fragment,e),Za=u(e),$e=l(e,"P",{});var Vo=i($e);Sn=o(Vo,"Now you can iterate over the dataset with the pipeline. "),Ys=l(Vo,"CODE",{});var Dp=i(Ys);Cn=o(Dp,"KeyDataset"),Dp.forEach(s),Mn=o(Vo," retrieves the item in the dictionary returned by the dataset:"),Vo.forEach(s),Xa=u(e),_(at.$$.fragment,e),eo=u(e),oe=l(e,"H3",{class:!0});var Zo=i(oe);we=l(Zo,"A",{id:!0,class:!0,href:!0});var Op=i(we);Js=l(Op,"SPAN",{});var Lp=i(Js);_(ot.$$.fragment,Lp),Lp.forEach(s),Op.forEach(s),Nn=u(Zo),Vs=l(Zo,"SPAN",{});var Rp=i(Vs);In=o(Rp,"Use another model and tokenizer in the pipeline"),Rp.forEach(s),Zo.forEach(s),to=u(e),C=l(e,"P",{});var te=i(C);Dn=o(te,"The "),Dt=l(te,"A",{href:!0});var Up=i(Dt);On=o(Up,"pipeline()"),Up.forEach(s),Ln=o(te," can accommodate any model from the "),rt=l(te,"A",{href:!0,rel:!0});var Wp=i(rt);Rn=o(Wp,"Model Hub"),Wp.forEach(s),Un=o(te,", making it easy to adapt the "),Ot=l(te,"A",{href:!0});var Hp=i(Ot);Wn=o(Hp,"pipeline()"),Hp.forEach(s),Hn=o(te," for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),nt=l(te,"A",{href:!0,rel:!0});var Bp=i(nt);Bn=o(Bp,"BERT model"),Bp.forEach(s),Kn=o(te," fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),te.forEach(s),so=u(e),_(lt.$$.fragment,e),ao=u(e),Q=l(e,"P",{});var ys=i(Q);Gn=o(ys,"Use the "),Lt=l(ys,"A",{href:!0});var Kp=i(Lt);Qn=o(Kp,"AutoModelForSequenceClassification"),Kp.forEach(s),Yn=o(ys," and [\u2018AutoTokenizer\u2019] to load the pretrained model and it\u2019s associated tokenizer (more on an "),Zs=l(ys,"CODE",{});var Gp=i(Zs);Jn=o(Gp,"AutoClass"),Gp.forEach(s),Vn=o(ys," below):"),ys.forEach(s),oo=u(e),_(it.$$.fragment,e),ro=u(e),Y=l(e,"P",{});var $s=i(Y);Zn=o($s,"Then you can specify the model and tokenizer in the "),Rt=l($s,"A",{href:!0});var Qp=i(Rt);Xn=o(Qp,"pipeline()"),Qp.forEach(s),el=o($s,", and apply the "),Xs=l($s,"CODE",{});var Yp=i(Xs);tl=o(Yp,"classifier"),Yp.forEach(s),sl=o($s," on your target text:"),$s.forEach(s),no=u(e),_(pt.$$.fragment,e),lo=u(e),J=l(e,"P",{});var ws=i(J);al=o(ws,"If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),Ut=l(ws,"A",{href:!0});var Jp=i(Ut);ol=o(Jp,"fine-tuning tutorial"),Jp.forEach(s),rl=o(ws," to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),Wt=l(ws,"A",{href:!0});var Vp=i(Wt);nl=o(Vp,"here"),Vp.forEach(s),ll=o(ws,") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),ws.forEach(s),io=u(e),re=l(e,"H2",{class:!0});var Xo=i(re);ke=l(Xo,"A",{id:!0,class:!0,href:!0});var Zp=i(ke);ea=l(Zp,"SPAN",{});var Xp=i(ea);_(ft.$$.fragment,Xp),Xp.forEach(s),Zp.forEach(s),il=u(Xo),ta=l(Xo,"SPAN",{});var ef=i(ta);pl=o(ef,"AutoClass"),ef.forEach(s),Xo.forEach(s),po=u(e),_(ut.$$.fragment,e),fo=u(e),z=l(e,"P",{});var M=i(z);fl=o(M,"Under the hood, the "),Ht=l(M,"A",{href:!0});var tf=i(Ht);ul=o(tf,"AutoModelForSequenceClassification"),tf.forEach(s),ml=o(M," and "),Bt=l(M,"A",{href:!0});var sf=i(Bt);hl=o(sf,"AutoTokenizer"),sf.forEach(s),cl=o(M," classes work together to power the "),Kt=l(M,"A",{href:!0});var af=i(Kt);dl=o(af,"pipeline()"),af.forEach(s),_l=o(M,". An "),Gt=l(M,"A",{href:!0});var of=i(Gt);gl=o(of,"AutoClass"),of.forEach(s),vl=o(M," is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),sa=l(M,"CODE",{});var rf=i(sa);yl=o(rf,"AutoClass"),rf.forEach(s),$l=o(M," for your task and it\u2019s associated tokenizer with "),Qt=l(M,"A",{href:!0});var nf=i(Qt);wl=o(nf,"AutoTokenizer"),nf.forEach(s),kl=o(M,"."),M.forEach(s),uo=u(e),V=l(e,"P",{});var ks=i(V);bl=o(ks,"Let\u2019s return to our example and see how you can use the "),aa=l(ks,"CODE",{});var lf=i(aa);El=o(lf,"AutoClass"),lf.forEach(s),jl=o(ks," to replicate the results of the "),Yt=l(ks,"A",{href:!0});var pf=i(Yt);Al=o(pf,"pipeline()"),pf.forEach(s),Tl=o(ks,"."),ks.forEach(s),mo=u(e),ne=l(e,"H3",{class:!0});var er=i(ne);be=l(er,"A",{id:!0,class:!0,href:!0});var ff=i(be);oa=l(ff,"SPAN",{});var uf=i(oa);_(mt.$$.fragment,uf),uf.forEach(s),ff.forEach(s),ql=u(er),ra=l(er,"SPAN",{});var mf=i(ra);zl=o(mf,"AutoTokenizer"),mf.forEach(s),er.forEach(s),ho=u(e),Z=l(e,"P",{});var bs=i(Z);xl=o(bs,"A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),na=l(bs,"EM",{});var hf=i(na);Fl=o(hf,"tokens"),hf.forEach(s),Pl=o(bs,". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),Jt=l(bs,"A",{href:!0});var cf=i(Jt);Sl=o(cf,"here"),cf.forEach(s),Cl=o(bs,"). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),bs.forEach(s),co=u(e),Ee=l(e,"P",{});var tr=i(Ee);Ml=o(tr,"Load a tokenizer with "),Vt=l(tr,"A",{href:!0});var df=i(Vt);Nl=o(df,"AutoTokenizer"),df.forEach(s),Il=o(tr,":"),tr.forEach(s),_o=u(e),_(ht.$$.fragment,e),go=u(e),je=l(e,"P",{});var sr=i(je);Dl=o(sr,"Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),la=l(sr,"EM",{});var _f=i(la);Ol=o(_f,"vocabulary"),_f.forEach(s),Ll=o(sr,"."),sr.forEach(s),vo=u(e),Zt=l(e,"P",{});var gf=i(Zt);Rl=o(gf,"Pass your text to the tokenizer:"),gf.forEach(s),yo=u(e),_(ct.$$.fragment,e),$o=u(e),Xt=l(e,"P",{});var vf=i(Xt);Ul=o(vf,"The tokenizer will return a dictionary containing:"),vf.forEach(s),wo=u(e),Ae=l(e,"UL",{});var ar=i(Ae);es=l(ar,"LI",{});var Vi=i(es);ts=l(Vi,"A",{href:!0});var yf=i(ts);Wl=o(yf,"input_ids"),yf.forEach(s),Hl=o(Vi,": numerical representions of your tokens."),Vi.forEach(s),Bl=u(ar),ss=l(ar,"LI",{});var Zi=i(ss);as=l(Zi,"A",{href:!0});var $f=i(as);Kl=o($f,"atttention_mask"),$f.forEach(s),Gl=o(Zi,": indicates which tokens should be attended to."),Zi.forEach(s),ar.forEach(s),ko=u(e),Te=l(e,"P",{});var or=i(Te);Ql=o(or,"Just like the "),os=l(or,"A",{href:!0});var wf=i(os);Yl=o(wf,"pipeline()"),wf.forEach(s),Jl=o(or,", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),or.forEach(s),bo=u(e),_(dt.$$.fragment,e),Eo=u(e),qe=l(e,"P",{});var rr=i(qe);Vl=o(rr,"Read the "),rs=l(rr,"A",{href:!0});var kf=i(rs);Zl=o(kf,"preprocessing"),kf.forEach(s),Xl=o(rr," tutorial for more details about tokenization."),rr.forEach(s),jo=u(e),le=l(e,"H3",{class:!0});var nr=i(le);ze=l(nr,"A",{id:!0,class:!0,href:!0});var bf=i(ze);ia=l(bf,"SPAN",{});var Ef=i(ia);_(_t.$$.fragment,Ef),Ef.forEach(s),bf.forEach(s),ei=u(nr),pa=l(nr,"SPAN",{});var jf=i(pa);ti=o(jf,"AutoModel"),jf.forEach(s),nr.forEach(s),Ao=u(e),F=l(e,"P",{});var L=i(F);si=o(L,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),ns=l(L,"A",{href:!0});var Af=i(ns);ai=o(Af,"AutoModel"),Af.forEach(s),oi=o(L," like you would load an "),ls=l(L,"A",{href:!0});var Tf=i(ls);ri=o(Tf,"AutoTokenizer"),Tf.forEach(s),ni=o(L,". The only difference is selecting the correct "),is=l(L,"A",{href:!0});var qf=i(is);li=o(qf,"AutoModel"),qf.forEach(s),ii=o(L," for the task. Since you are doing text - or sequence - classification, load "),ps=l(L,"A",{href:!0});var zf=i(ps);pi=o(zf,"AutoModelForSequenceClassification"),zf.forEach(s),fi=o(L,". The TensorFlow equivalent is simply "),fs=l(L,"A",{href:!0});var xf=i(fs);ui=o(xf,"TFAutoModelForSequenceClassification"),xf.forEach(s),mi=o(L,":"),L.forEach(s),To=u(e),_(gt.$$.fragment,e),qo=u(e),_(xe.$$.fragment,e),zo=u(e),Fe=l(e,"P",{});var lr=i(Fe);hi=o(lr,"Now you can pass your preprocessed batch of inputs directly to the model. If you are using a PyTorch model, unpack the dictionary by adding "),fa=l(lr,"CODE",{});var Ff=i(fa);ci=o(Ff,"**"),Ff.forEach(s),di=o(lr,". For TensorFlow models, pass the dictionary keys directly to the tensors:"),lr.forEach(s),xo=u(e),_(vt.$$.fragment,e),Fo=u(e),X=l(e,"P",{});var Es=i(X);_i=o(Es,"The model outputs the final activations in the "),ua=l(Es,"CODE",{});var Pf=i(ua);gi=o(Pf,"logits"),Pf.forEach(s),vi=o(Es," attribute. Apply the softmax function to the "),ma=l(Es,"CODE",{});var Sf=i(ma);yi=o(Sf,"logits"),Sf.forEach(s),$i=o(Es," to retrieve the probabilities:"),Es.forEach(s),Po=u(e),_(yt.$$.fragment,e),So=u(e),_(Pe.$$.fragment,e),Co=u(e),x=l(e,"P",{});var N=i(x);wi=o(N,"Models are a standard "),$t=l(N,"A",{href:!0,rel:!0});var Cf=i($t);ha=l(Cf,"CODE",{});var Mf=i(ha);ki=o(Mf,"torch.nn.Module"),Mf.forEach(s),Cf.forEach(s),bi=o(N," or a "),wt=l(N,"A",{href:!0,rel:!0});var Nf=i(wt);ca=l(Nf,"CODE",{});var If=i(ca);Ei=o(If,"tf.keras.Model"),If.forEach(s),Nf.forEach(s),ji=o(N," so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),us=l(N,"A",{href:!0});var Df=i(us);Ai=o(Df,"Trainer"),Df.forEach(s),Ti=o(N," class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),da=l(N,"CODE",{});var Of=i(da);qi=o(Of,"fit"),Of.forEach(s),zi=o(N," method from "),kt=l(N,"A",{href:!0,rel:!0});var Lf=i(kt);xi=o(Lf,"Keras"),Lf.forEach(s),Fi=o(N,". Refer to the "),ms=l(N,"A",{href:!0});var Rf=i(ms);Pi=o(Rf,"training tutorial"),Rf.forEach(s),Si=o(N," for more details."),N.forEach(s),Mo=u(e),_(Se.$$.fragment,e),No=u(e),ie=l(e,"H3",{class:!0});var ir=i(ie);Ce=l(ir,"A",{id:!0,class:!0,href:!0});var Uf=i(Ce);_a=l(Uf,"SPAN",{});var Wf=i(_a);_(bt.$$.fragment,Wf),Wf.forEach(s),Uf.forEach(s),Ci=u(ir),ga=l(ir,"SPAN",{});var Hf=i(ga);Mi=o(Hf,"Save a model"),Hf.forEach(s),ir.forEach(s),Io=u(e),Me=l(e,"P",{});var pr=i(Me);Ni=o(pr,"Once your model is fine-tuned, you can save it with its tokenizer using "),hs=l(pr,"A",{href:!0});var Bf=i(hs);Ii=o(Bf,"PreTrainedModel.save_pretrained()"),Bf.forEach(s),Di=o(pr,":"),pr.forEach(s),Do=u(e),_(Et.$$.fragment,e),Oo=u(e),Ne=l(e,"P",{});var fr=i(Ne);Oi=o(fr,"When you are ready to use the model again, reload it with "),cs=l(fr,"A",{href:!0});var Kf=i(cs);Li=o(Kf,"PreTrainedModel.from_pretrained()"),Kf.forEach(s),Ri=o(fr,":"),fr.forEach(s),Lo=u(e),_(jt.$$.fragment,e),Ro=u(e),ee=l(e,"P",{});var js=i(ee);Ui=o(js,"One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),va=l(js,"CODE",{});var Gf=i(va);Wi=o(Gf,"from_pt"),Gf.forEach(s),Hi=o(js," or "),ya=l(js,"CODE",{});var Qf=i(ya);Bi=o(Qf,"from_tf"),Qf.forEach(s),Ki=o(js," parameter can convert the model from one framework to the other:"),js.forEach(s),Uo=u(e),_(At.$$.fragment,e),this.h()},h(){m(h,"name","hf:doc:metadata"),m(h,"content",JSON.stringify(lu)),m(k,"id","quick-tour"),m(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(k,"href","#quick-tour"),m(c,"class","relative group"),m(qt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(zt,"href","./model_doc/auto"),m(me,"id","pipeline"),m(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(me,"href","#pipeline"),m(se,"class","relative group"),m(xt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(Ft,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(_e,"id","pipeline-usage"),m(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(_e,"href","#pipeline-usage"),m(ae,"class","relative group"),m(Pt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(Ct,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(Qe,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),m(Qe,"rel","nofollow"),m(Mt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(Nt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(Ve,"href","https://huggingface.co/docs/datasets/"),m(Ve,"rel","nofollow"),m(It,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(et,"href","https://huggingface.co/docs/datasets/quickstart.html"),m(et,"rel","nofollow"),m(tt,"href","https://huggingface.co/datasets/superb"),m(tt,"rel","nofollow"),m(we,"id","use-another-model-and-tokenizer-in-the-pipeline"),m(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(we,"href","#use-another-model-and-tokenizer-in-the-pipeline"),m(oe,"class","relative group"),m(Dt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(rt,"href","https://huggingface.co/models"),m(rt,"rel","nofollow"),m(Ot,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(nt,"href","https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"),m(nt,"rel","nofollow"),m(Lt,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),m(Rt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(Ut,"href","./training"),m(Wt,"href","./model_sharing"),m(ke,"id","autoclass"),m(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ke,"href","#autoclass"),m(re,"class","relative group"),m(Ht,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),m(Bt,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),m(Kt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(Gt,"href","./model_doc/auto"),m(Qt,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),m(Yt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(be,"id","autotokenizer"),m(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(be,"href","#autotokenizer"),m(ne,"class","relative group"),m(Jt,"href","./tokenizer_summary"),m(Vt,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),m(ts,"href","./glossary#input-ids"),m(as,"href",".glossary#attention-mask"),m(os,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(rs,"href","./preprocessing"),m(ze,"id","automodel"),m(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ze,"href","#automodel"),m(le,"class","relative group"),m(ns,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModel"),m(ls,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),m(is,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModel"),m(ps,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),m(fs,"href","/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification"),m($t,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),m($t,"rel","nofollow"),m(wt,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),m(wt,"rel","nofollow"),m(us,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(kt,"href","https://keras.io/"),m(kt,"rel","nofollow"),m(ms,"href","./training"),m(Ce,"id","save-a-model"),m(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ce,"href","#save-a-model"),m(ie,"class","relative group"),m(hs,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),m(cs,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained")},m(e,r){t(document.head,h),p(e,b,r),p(e,c,r),t(c,k),t(k,A),g(w,A,null),t(c,E),t(c,q),t(q,S),p(e,j,r),g(D,e,r),p(e,U,r),p(e,W,r),t(W,ur),t(W,qt),t(qt,mr),t(W,hr),t(W,zt),t(zt,cr),t(W,dr),p(e,Ea,r),g(ue,e,r),p(e,ja,r),p(e,se,r),t(se,me),t(me,As),g(Oe,As,null),t(se,_r),t(se,Ts),t(Ts,gr),p(e,Aa,r),p(e,Le,r),t(Le,xt),t(xt,vr),t(Le,yr),p(e,Ta,r),g(Re,e,r),p(e,qa,r),p(e,he,r),t(he,$r),t(he,Ft),t(Ft,wr),t(he,kr),p(e,za,r),p(e,Ue,r),t(Ue,qs),t(qs,br),t(Ue,Er),p(e,xa,r),p(e,T,r),t(T,zs),t(zs,jr),t(T,Ar),t(T,xs),t(xs,Tr),t(T,qr),t(T,Fs),t(Fs,zr),t(T,xr),t(T,Ps),t(Ps,Fr),t(T,Pr),t(T,Ss),t(Ss,Sr),t(T,Cr),t(T,Cs),t(Cs,Mr),t(T,Nr),t(T,Ms),t(Ms,Ir),t(T,Dr),t(T,Ns),t(Ns,Or),p(e,Fa,r),p(e,We,r),t(We,Is),t(Is,Lr),t(We,Rr),p(e,Pa,r),p(e,H,r),t(H,Ds),t(Ds,Ur),t(H,Wr),t(H,Os),t(Os,Hr),t(H,Br),t(H,Ls),t(Ls,Kr),p(e,Sa,r),p(e,He,r),t(He,Rs),t(Rs,Gr),t(He,Qr),p(e,Ca,r),p(e,ce,r),t(ce,Us),t(Us,Yr),t(ce,Jr),t(ce,Ws),t(Ws,Vr),p(e,Ma,r),g(de,e,r),p(e,Na,r),p(e,ae,r),t(ae,_e),t(_e,Hs),g(Be,Hs,null),t(ae,Zr),t(ae,Bs),t(Bs,Xr),p(e,Ia,r),p(e,ge,r),t(ge,en),t(ge,Pt),t(Pt,tn),t(ge,sn),p(e,Da,r),p(e,St,r),t(St,an),p(e,Oa,r),g(Ke,e,r),p(e,La,r),p(e,ve,r),t(ve,on),t(ve,Ct),t(Ct,rn),t(ve,nn),p(e,Ra,r),g(Ge,e,r),p(e,Ua,r),p(e,B,r),t(B,ln),t(B,Qe),t(Qe,pn),t(B,fn),t(B,Ks),t(Ks,un),t(B,mn),p(e,Wa,r),g(Ye,e,r),p(e,Ha,r),p(e,ye,r),t(ye,hn),t(ye,Mt),t(Mt,cn),t(ye,dn),p(e,Ba,r),g(Je,e,r),p(e,Ka,r),p(e,K,r),t(K,_n),t(K,Nt),t(Nt,gn),t(K,vn),t(K,Ve),t(Ve,yn),t(K,$n),p(e,Ga,r),g(Ze,e,r),p(e,Qa,r),p(e,O,r),t(O,wn),t(O,It),t(It,kn),t(O,bn),t(O,Gs),t(Gs,En),t(O,jn),t(O,Qs),t(Qs,An),t(O,Tn),p(e,Ya,r),g(Xe,e,r),p(e,Ja,r),p(e,G,r),t(G,qn),t(G,et),t(et,zn),t(G,xn),t(G,tt),t(tt,Fn),t(G,Pn),p(e,Va,r),g(st,e,r),p(e,Za,r),p(e,$e,r),t($e,Sn),t($e,Ys),t(Ys,Cn),t($e,Mn),p(e,Xa,r),g(at,e,r),p(e,eo,r),p(e,oe,r),t(oe,we),t(we,Js),g(ot,Js,null),t(oe,Nn),t(oe,Vs),t(Vs,In),p(e,to,r),p(e,C,r),t(C,Dn),t(C,Dt),t(Dt,On),t(C,Ln),t(C,rt),t(rt,Rn),t(C,Un),t(C,Ot),t(Ot,Wn),t(C,Hn),t(C,nt),t(nt,Bn),t(C,Kn),p(e,so,r),g(lt,e,r),p(e,ao,r),p(e,Q,r),t(Q,Gn),t(Q,Lt),t(Lt,Qn),t(Q,Yn),t(Q,Zs),t(Zs,Jn),t(Q,Vn),p(e,oo,r),g(it,e,r),p(e,ro,r),p(e,Y,r),t(Y,Zn),t(Y,Rt),t(Rt,Xn),t(Y,el),t(Y,Xs),t(Xs,tl),t(Y,sl),p(e,no,r),g(pt,e,r),p(e,lo,r),p(e,J,r),t(J,al),t(J,Ut),t(Ut,ol),t(J,rl),t(J,Wt),t(Wt,nl),t(J,ll),p(e,io,r),p(e,re,r),t(re,ke),t(ke,ea),g(ft,ea,null),t(re,il),t(re,ta),t(ta,pl),p(e,po,r),g(ut,e,r),p(e,fo,r),p(e,z,r),t(z,fl),t(z,Ht),t(Ht,ul),t(z,ml),t(z,Bt),t(Bt,hl),t(z,cl),t(z,Kt),t(Kt,dl),t(z,_l),t(z,Gt),t(Gt,gl),t(z,vl),t(z,sa),t(sa,yl),t(z,$l),t(z,Qt),t(Qt,wl),t(z,kl),p(e,uo,r),p(e,V,r),t(V,bl),t(V,aa),t(aa,El),t(V,jl),t(V,Yt),t(Yt,Al),t(V,Tl),p(e,mo,r),p(e,ne,r),t(ne,be),t(be,oa),g(mt,oa,null),t(ne,ql),t(ne,ra),t(ra,zl),p(e,ho,r),p(e,Z,r),t(Z,xl),t(Z,na),t(na,Fl),t(Z,Pl),t(Z,Jt),t(Jt,Sl),t(Z,Cl),p(e,co,r),p(e,Ee,r),t(Ee,Ml),t(Ee,Vt),t(Vt,Nl),t(Ee,Il),p(e,_o,r),g(ht,e,r),p(e,go,r),p(e,je,r),t(je,Dl),t(je,la),t(la,Ol),t(je,Ll),p(e,vo,r),p(e,Zt,r),t(Zt,Rl),p(e,yo,r),g(ct,e,r),p(e,$o,r),p(e,Xt,r),t(Xt,Ul),p(e,wo,r),p(e,Ae,r),t(Ae,es),t(es,ts),t(ts,Wl),t(es,Hl),t(Ae,Bl),t(Ae,ss),t(ss,as),t(as,Kl),t(ss,Gl),p(e,ko,r),p(e,Te,r),t(Te,Ql),t(Te,os),t(os,Yl),t(Te,Jl),p(e,bo,r),g(dt,e,r),p(e,Eo,r),p(e,qe,r),t(qe,Vl),t(qe,rs),t(rs,Zl),t(qe,Xl),p(e,jo,r),p(e,le,r),t(le,ze),t(ze,ia),g(_t,ia,null),t(le,ei),t(le,pa),t(pa,ti),p(e,Ao,r),p(e,F,r),t(F,si),t(F,ns),t(ns,ai),t(F,oi),t(F,ls),t(ls,ri),t(F,ni),t(F,is),t(is,li),t(F,ii),t(F,ps),t(ps,pi),t(F,fi),t(F,fs),t(fs,ui),t(F,mi),p(e,To,r),g(gt,e,r),p(e,qo,r),g(xe,e,r),p(e,zo,r),p(e,Fe,r),t(Fe,hi),t(Fe,fa),t(fa,ci),t(Fe,di),p(e,xo,r),g(vt,e,r),p(e,Fo,r),p(e,X,r),t(X,_i),t(X,ua),t(ua,gi),t(X,vi),t(X,ma),t(ma,yi),t(X,$i),p(e,Po,r),g(yt,e,r),p(e,So,r),g(Pe,e,r),p(e,Co,r),p(e,x,r),t(x,wi),t(x,$t),t($t,ha),t(ha,ki),t(x,bi),t(x,wt),t(wt,ca),t(ca,Ei),t(x,ji),t(x,us),t(us,Ai),t(x,Ti),t(x,da),t(da,qi),t(x,zi),t(x,kt),t(kt,xi),t(x,Fi),t(x,ms),t(ms,Pi),t(x,Si),p(e,Mo,r),g(Se,e,r),p(e,No,r),p(e,ie,r),t(ie,Ce),t(Ce,_a),g(bt,_a,null),t(ie,Ci),t(ie,ga),t(ga,Mi),p(e,Io,r),p(e,Me,r),t(Me,Ni),t(Me,hs),t(hs,Ii),t(Me,Di),p(e,Do,r),g(Et,e,r),p(e,Oo,r),p(e,Ne,r),t(Ne,Oi),t(Ne,cs),t(cs,Li),t(Ne,Ri),p(e,Lo,r),g(jt,e,r),p(e,Ro,r),p(e,ee,r),t(ee,Ui),t(ee,va),t(va,Wi),t(ee,Hi),t(ee,ya),t(ya,Bi),t(ee,Ki),p(e,Uo,r),g(At,e,r),Wo=!0},p(e,[r]){const Tt={};r&2&&(Tt.$$scope={dirty:r,ctx:e}),ue.$set(Tt);const $a={};r&2&&($a.$$scope={dirty:r,ctx:e}),de.$set($a);const wa={};r&2&&(wa.$$scope={dirty:r,ctx:e}),xe.$set(wa);const ka={};r&2&&(ka.$$scope={dirty:r,ctx:e}),Pe.$set(ka);const pe={};r&2&&(pe.$$scope={dirty:r,ctx:e}),Se.$set(pe)},i(e){Wo||(v(w.$$.fragment,e),v(D.$$.fragment,e),v(ue.$$.fragment,e),v(Oe.$$.fragment,e),v(Re.$$.fragment,e),v(de.$$.fragment,e),v(Be.$$.fragment,e),v(Ke.$$.fragment,e),v(Ge.$$.fragment,e),v(Ye.$$.fragment,e),v(Je.$$.fragment,e),v(Ze.$$.fragment,e),v(Xe.$$.fragment,e),v(st.$$.fragment,e),v(at.$$.fragment,e),v(ot.$$.fragment,e),v(lt.$$.fragment,e),v(it.$$.fragment,e),v(pt.$$.fragment,e),v(ft.$$.fragment,e),v(ut.$$.fragment,e),v(mt.$$.fragment,e),v(ht.$$.fragment,e),v(ct.$$.fragment,e),v(dt.$$.fragment,e),v(_t.$$.fragment,e),v(gt.$$.fragment,e),v(xe.$$.fragment,e),v(vt.$$.fragment,e),v(yt.$$.fragment,e),v(Pe.$$.fragment,e),v(Se.$$.fragment,e),v(bt.$$.fragment,e),v(Et.$$.fragment,e),v(jt.$$.fragment,e),v(At.$$.fragment,e),Wo=!0)},o(e){y(w.$$.fragment,e),y(D.$$.fragment,e),y(ue.$$.fragment,e),y(Oe.$$.fragment,e),y(Re.$$.fragment,e),y(de.$$.fragment,e),y(Be.$$.fragment,e),y(Ke.$$.fragment,e),y(Ge.$$.fragment,e),y(Ye.$$.fragment,e),y(Je.$$.fragment,e),y(Ze.$$.fragment,e),y(Xe.$$.fragment,e),y(st.$$.fragment,e),y(at.$$.fragment,e),y(ot.$$.fragment,e),y(lt.$$.fragment,e),y(it.$$.fragment,e),y(pt.$$.fragment,e),y(ft.$$.fragment,e),y(ut.$$.fragment,e),y(mt.$$.fragment,e),y(ht.$$.fragment,e),y(ct.$$.fragment,e),y(dt.$$.fragment,e),y(_t.$$.fragment,e),y(gt.$$.fragment,e),y(xe.$$.fragment,e),y(vt.$$.fragment,e),y(yt.$$.fragment,e),y(Pe.$$.fragment,e),y(Se.$$.fragment,e),y(bt.$$.fragment,e),y(Et.$$.fragment,e),y(jt.$$.fragment,e),y(At.$$.fragment,e),Wo=!1},d(e){s(h),e&&s(b),e&&s(c),$(w),e&&s(j),$(D,e),e&&s(U),e&&s(W),e&&s(Ea),$(ue,e),e&&s(ja),e&&s(se),$(Oe),e&&s(Aa),e&&s(Le),e&&s(Ta),$(Re,e),e&&s(qa),e&&s(he),e&&s(za),e&&s(Ue),e&&s(xa),e&&s(T),e&&s(Fa),e&&s(We),e&&s(Pa),e&&s(H),e&&s(Sa),e&&s(He),e&&s(Ca),e&&s(ce),e&&s(Ma),$(de,e),e&&s(Na),e&&s(ae),$(Be),e&&s(Ia),e&&s(ge),e&&s(Da),e&&s(St),e&&s(Oa),$(Ke,e),e&&s(La),e&&s(ve),e&&s(Ra),$(Ge,e),e&&s(Ua),e&&s(B),e&&s(Wa),$(Ye,e),e&&s(Ha),e&&s(ye),e&&s(Ba),$(Je,e),e&&s(Ka),e&&s(K),e&&s(Ga),$(Ze,e),e&&s(Qa),e&&s(O),e&&s(Ya),$(Xe,e),e&&s(Ja),e&&s(G),e&&s(Va),$(st,e),e&&s(Za),e&&s($e),e&&s(Xa),$(at,e),e&&s(eo),e&&s(oe),$(ot),e&&s(to),e&&s(C),e&&s(so),$(lt,e),e&&s(ao),e&&s(Q),e&&s(oo),$(it,e),e&&s(ro),e&&s(Y),e&&s(no),$(pt,e),e&&s(lo),e&&s(J),e&&s(io),e&&s(re),$(ft),e&&s(po),$(ut,e),e&&s(fo),e&&s(z),e&&s(uo),e&&s(V),e&&s(mo),e&&s(ne),$(mt),e&&s(ho),e&&s(Z),e&&s(co),e&&s(Ee),e&&s(_o),$(ht,e),e&&s(go),e&&s(je),e&&s(vo),e&&s(Zt),e&&s(yo),$(ct,e),e&&s($o),e&&s(Xt),e&&s(wo),e&&s(Ae),e&&s(ko),e&&s(Te),e&&s(bo),$(dt,e),e&&s(Eo),e&&s(qe),e&&s(jo),e&&s(le),$(_t),e&&s(Ao),e&&s(F),e&&s(To),$(gt,e),e&&s(qo),$(xe,e),e&&s(zo),e&&s(Fe),e&&s(xo),$(vt,e),e&&s(Fo),e&&s(X),e&&s(Po),$(yt,e),e&&s(So),$(Pe,e),e&&s(Co),e&&s(x),e&&s(Mo),$(Se,e),e&&s(No),e&&s(ie),$(bt),e&&s(Io),e&&s(Me),e&&s(Do),$(Et,e),e&&s(Oo),e&&s(Ne),e&&s(Lo),$(jt,e),e&&s(Ro),e&&s(ee),e&&s(Uo),$(At,e)}}}const lu={local:"quick-tour",sections:[{local:"pipeline",sections:[{local:"pipeline-usage",title:"Pipeline usage"},{local:"use-another-model-and-tokenizer-in-the-pipeline",title:"Use another model and tokenizer in the pipeline"}],title:"Pipeline"},{local:"autoclass",sections:[{local:"autotokenizer",title:"AutoTokenizer"},{local:"automodel",title:"AutoModel"},{local:"save-a-model",title:"Save a model"}],title:"AutoClass"}],title:"Quick tour"};function iu(I,h,b){let{fw:c}=h;return I.$$set=k=>{"fw"in k&&b(0,c=k.fw)},[c]}class gu extends Jf{constructor(h){super();Vf(this,h,iu,nu,Zf,{fw:0})}}export{gu as default,lu as metadata};
