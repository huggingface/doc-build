import{S as eu,i as tu,s as su,e as n,k as f,w as d,t as a,M as au,c as l,d as s,m as u,a as i,x as _,h as r,b as m,F as t,g as p,y as g,q as v,o as $,B as y}from"../chunks/vendor-4833417e.js";import{T as Ea}from"../chunks/Tip-fffd6df1.js";import{Y as Xf}from"../chunks/Youtube-27813aed.js";import{I as Oe}from"../chunks/IconCopyLink-4b81c553.js";import{C as R}from"../chunks/CodeBlock-6a3d1b46.js";import{C as fe}from"../chunks/CodeBlockFw-27a176a0.js";import{D as ru}from"../chunks/DocNotebookDropdown-ecff2a90.js";import"../chunks/CopyButton-dacfbfaf.js";function ou(N){let h,k;return{c(){h=n("p"),k=a(`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`)},l(c){h=l(c,"P",{});var w=i(h);k=r(w,`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`),w.forEach(s)},m(c,w){p(c,h,w),t(h,k)},d(c){c&&s(h)}}}function nu(N){let h,k,c,w,A,b,E,x;return{c(){h=n("p"),k=a("For more details about the "),c=n("a"),w=a("pipeline()"),A=a(" and associated tasks, refer to the documentation "),b=n("a"),E=a("here"),x=a("."),this.h()},l(P){h=l(P,"P",{});var j=i(h);k=r(j,"For more details about the "),c=l(j,"A",{href:!0});var O=i(c);w=r(O,"pipeline()"),O.forEach(s),A=r(j," and associated tasks, refer to the documentation "),b=l(j,"A",{href:!0});var H=i(b);E=r(H,"here"),H.forEach(s),x=r(j,"."),j.forEach(s),this.h()},h(){m(c,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(b,"href","./main_classes/pipelines")},m(P,j){p(P,h,j),t(h,k),t(h,c),t(c,w),t(h,A),t(h,b),t(b,E),t(h,x)},d(P){P&&s(h)}}}function lu(N){let h,k,c,w,A,b,E,x;return{c(){h=n("p"),k=a("See the "),c=n("a"),w=a("task summary"),A=a(" for which "),b=n("a"),E=a("AutoModel"),x=a(" class to use for which task."),this.h()},l(P){h=l(P,"P",{});var j=i(h);k=r(j,"See the "),c=l(j,"A",{href:!0});var O=i(c);w=r(O,"task summary"),O.forEach(s),A=r(j," for which "),b=l(j,"A",{href:!0});var H=i(b);E=r(H,"AutoModel"),H.forEach(s),x=r(j," class to use for which task."),j.forEach(s),this.h()},h(){m(c,"href","./task_summary"),m(b,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModel")},m(P,j){p(P,h,j),t(h,k),t(h,c),t(c,w),t(h,A),t(h,b),t(b,E),t(h,x)},d(P){P&&s(h)}}}function iu(N){let h,k,c,w,A;return{c(){h=n("p"),k=a("All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),c=n("em"),w=a("before"),A=a(` the final activation
function (like softmax) because the final activation function is often fused with the loss.`)},l(b){h=l(b,"P",{});var E=i(h);k=r(E,"All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),c=l(E,"EM",{});var x=i(c);w=r(x,"before"),x.forEach(s),A=r(E,` the final activation
function (like softmax) because the final activation function is often fused with the loss.`),E.forEach(s)},m(b,E){p(b,h,E),t(h,k),t(h,c),t(c,w),t(h,A)},d(b){b&&s(h)}}}function pu(N){let h,k,c,w,A;return{c(){h=n("p"),k=a(`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),c=n("code"),w=a("None"),A=a(" are ignored.")},l(b){h=l(b,"P",{});var E=i(h);k=r(E,`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),c=l(E,"CODE",{});var x=i(c);w=r(x,"None"),x.forEach(s),A=r(E," are ignored."),E.forEach(s)},m(b,E){p(b,h,E),t(h,k),t(h,c),t(c,w),t(h,A)},d(b){b&&s(h)}}}function fu(N){let h,k,c,w,A,b,E,x,P,j,O,H,U,mo,xt,ho,co,qt,_o,go,ja,ue,Aa,se,me,xs,De,vo,qs,$o,Ta,Le,zt,yo,bo,xa,Re,qa,he,wo,Ft,ko,Eo,za,He,zs,jo,Ao,Fa,T,Fs,To,xo,Ss,qo,zo,Ps,Fo,So,Ms,Po,Mo,Cs,Co,Io,Is,No,Oo,Ns,Do,Lo,Os,Ro,Sa,Ue,Ds,Ho,Uo,Pa,W,Ls,Wo,Bo,Rs,Yo,Go,Hs,Qo,Ma,We,Us,Jo,Ko,Ca,ce,Ws,Vo,Zo,Bs,Xo,Ia,de,Na,ae,_e,Ys,Be,en,Gs,tn,Oa,ge,sn,St,an,rn,Da,Pt,on,La,Ye,Ra,ve,nn,Mt,ln,pn,Ha,Ge,Ua,B,fn,Qe,un,mn,Qs,hn,cn,Wa,Je,Ba,$e,dn,Ct,_n,gn,Ya,Ke,Ga,Y,vn,It,$n,yn,Ve,bn,wn,Qa,Ze,Ja,D,kn,Nt,En,jn,Js,An,Tn,Ks,xn,qn,Ka,Xe,Va,G,zn,et,Fn,Sn,tt,Pn,Mn,Za,st,Xa,Ot,Cn,er,at,tr,ye,In,Dt,Nn,On,sr,re,be,Vs,rt,Dn,Zs,Ln,ar,M,Rn,Lt,Hn,Un,ot,Wn,Bn,Rt,Yn,Gn,nt,Qn,Jn,rr,lt,or,Q,Kn,Ht,Vn,Zn,Xs,Xn,el,nr,it,lr,J,tl,Ut,sl,al,ea,rl,ol,ir,pt,pr,K,nl,Wt,ll,il,Bt,pl,fl,fr,oe,we,ta,ft,ul,sa,ml,ur,ut,mr,q,hl,Yt,cl,dl,Gt,_l,gl,Qt,vl,$l,Jt,yl,bl,aa,wl,kl,Kt,El,jl,hr,V,Al,ra,Tl,xl,Vt,ql,zl,cr,ne,ke,oa,mt,Fl,na,Sl,dr,Z,Pl,la,Ml,Cl,Zt,Il,Nl,_r,Ee,Ol,Xt,Dl,Ll,gr,ht,vr,je,Rl,ia,Hl,Ul,$r,es,Wl,yr,ct,br,ts,Bl,wr,Ae,ss,as,Yl,Gl,Ql,rs,os,Jl,Kl,kr,Te,Vl,ns,Zl,Xl,Er,dt,jr,xe,ei,ls,ti,si,Ar,le,qe,pa,_t,ai,fa,ri,Tr,F,oi,is,ni,li,ps,ii,pi,fs,fi,ui,us,mi,hi,ms,ci,di,xr,gt,qr,ze,zr,Fe,_i,ua,gi,vi,Fr,vt,Sr,X,$i,ma,yi,bi,ha,wi,ki,Pr,$t,Mr,Se,Cr,z,Ei,yt,ca,ji,Ai,bt,da,Ti,xi,hs,qi,zi,_a,Fi,Si,wt,Pi,Mi,cs,Ci,Ii,Ir,Pe,Nr,ie,Me,ga,kt,Ni,va,Oi,Or,Ce,Di,ds,Li,Ri,Dr,Et,Lr,Ie,Hi,_s,Ui,Wi,Rr,jt,Hr,ee,Bi,$a,Yi,Gi,ya,Qi,Ji,Ur,At,Wr;return b=new Oe({}),O=new ru({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/tensorflow/quicktour.ipynb"}]}}),ue=new Ea({props:{$$slots:{default:[ou]},$$scope:{ctx:N}}}),De=new Oe({}),Re=new Xf({props:{id:"tiZFewofSLM"}}),de=new Ea({props:{$$slots:{default:[nu]},$$scope:{ctx:N}}}),Be=new Oe({}),Ye=new fe({props:{group1:{id:"pt",code:"pip install torch",highlighted:"pip install torch"},group2:{id:"tf",code:"pip install tensorflow",highlighted:"pip install tensorflow"}}}),Ge=new R({props:{code:`from transformers import pipeline

classifier = pipeline("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),Je=new R({props:{code:'classifier("We are very happy to show you the \u{1F917} Transformers library.")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9998</span>}]`}}),Ke=new R({props:{code:`results = classifier(["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">{result[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, with score: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>`}}),Ze=new R({props:{code:"pip install datasets ",highlighted:"pip install datasets "}}),Xe=new R({props:{code:`from transformers import pipeline

speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h", device=0)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, device=<span class="hljs-number">0</span>)`}}),st=new R({props:{code:`import datasets

dataset = datasets.load_dataset("superb", name="asr", split="test")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> datasets

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = datasets.load_dataset(<span class="hljs-string">&quot;superb&quot;</span>, name=<span class="hljs-string">&quot;asr&quot;</span>, split=<span class="hljs-string">&quot;test&quot;</span>)`}}),at=new R({props:{code:`files = dataset["file"]
speech_recognizer(files[:4])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dataset[<span class="hljs-string">&quot;file&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer(files[:<span class="hljs-number">4</span>])
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOWER FAT AND SAUCE&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;STUFFERED INTO YOU HIS BELLY COUNSELLED HIM&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;HO BERTIE ANY GOOD IN YOUR MIND&#x27;</span>}]`}}),rt=new Oe({}),lt=new R({props:{code:'model_name = "nlptown/bert-base-multilingual-uncased-sentiment"',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>'}}),it=new fe({props:{group1:{id:"pt",code:`from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`},group2:{id:"tf",code:`from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}}),pt=new R({props:{code:`classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;5 stars&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.7273</span>}]`}}),ft=new Oe({}),ut=new Xf({props:{id:"AhChOFRegn4"}}),mt=new Oe({}),ht=new R({props:{code:`from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),ct=new R({props:{code:`encoding = tokenizer("We are very happy to show you the \u{1F917} Transformers library.")
print(encoding)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">11312</span>, <span class="hljs-number">10320</span>, <span class="hljs-number">12495</span>, <span class="hljs-number">19308</span>, <span class="hljs-number">10114</span>, <span class="hljs-number">11391</span>, <span class="hljs-number">10855</span>, <span class="hljs-number">10103</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">13299</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),dt=new fe({props:{group1:{id:"pt",code:`pt_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`},group2:{id:"tf",code:`tf_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`}}}),_t=new Oe({}),gt=new fe({props:{group1:{id:"pt",code:`from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`},group2:{id:"tf",code:`from transformers import TFAutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`}}}),ze=new Ea({props:{$$slots:{default:[lu]},$$scope:{ctx:N}}}),vt=new fe({props:{group1:{id:"pt",code:"pt_outputs = pt_model(**pt_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)'},group2:{id:"tf",code:"tf_outputs = tf_model(tf_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)'}}}),$t=new fe({props:{group1:{id:"pt",code:`from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">0.0021</span>, <span class="hljs-number">0.0018</span>, <span class="hljs-number">0.0115</span>, <span class="hljs-number">0.2121</span>, <span class="hljs-number">0.7725</span>],
        [<span class="hljs-number">0.2084</span>, <span class="hljs-number">0.1826</span>, <span class="hljs-number">0.1969</span>, <span class="hljs-number">0.1755</span>, <span class="hljs-number">0.2365</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)`},group2:{id:"tf",code:`import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
print(tf_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tf_predictions)
tf.Tensor(
[[<span class="hljs-number">0.00206</span> <span class="hljs-number">0.00177</span> <span class="hljs-number">0.01155</span> <span class="hljs-number">0.21209</span> <span class="hljs-number">0.77253</span>]
 [<span class="hljs-number">0.20842</span> <span class="hljs-number">0.18262</span> <span class="hljs-number">0.19693</span> <span class="hljs-number">0.1755</span>  <span class="hljs-number">0.23652</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>), dtype=float32)`}}}),Se=new Ea({props:{$$slots:{default:[iu]},$$scope:{ctx:N}}}),Pe=new Ea({props:{$$slots:{default:[pu]},$$scope:{ctx:N}}}),kt=new Oe({}),Et=new fe({props:{group1:{id:"pt",code:`pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)`},group2:{id:"tf",code:`tf_save_directory = "./tf_save_pretrained"
tokenizer.save_pretrained(tf_save_directory)
tf_model.save_pretrained(tf_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`}}}),jt=new fe({props:{group1:{id:"pt",code:'pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)'},group2:{id:"tf",code:'tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)'}}}),At=new fe({props:{group1:{id:"pt",code:`from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`},group2:{id:"tf",code:`from transformers import TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`}}}),{c(){h=n("meta"),k=f(),c=n("h1"),w=n("a"),A=n("span"),d(b.$$.fragment),E=f(),x=n("span"),P=a("Quick tour"),j=f(),d(O.$$.fragment),H=f(),U=n("p"),mo=a("Get up and running with \u{1F917} Transformers! Start using the "),xt=n("a"),ho=a("pipeline()"),co=a(" for rapid inference, and quickly load a pretrained model and tokenizer with an "),qt=n("a"),_o=a("AutoClass"),go=a(" to solve your text, vision or audio task."),ja=f(),d(ue.$$.fragment),Aa=f(),se=n("h2"),me=n("a"),xs=n("span"),d(De.$$.fragment),vo=f(),qs=n("span"),$o=a("Pipeline"),Ta=f(),Le=n("p"),zt=n("a"),yo=a("pipeline()"),bo=a(" is the easiest way to use a pretrained model for a given task."),xa=f(),d(Re.$$.fragment),qa=f(),he=n("p"),wo=a("The "),Ft=n("a"),ko=a("pipeline()"),Eo=a(" supports many common tasks out-of-the-box:"),za=f(),He=n("p"),zs=n("strong"),jo=a("Text"),Ao=a(":"),Fa=f(),T=n("ul"),Fs=n("li"),To=a("Sentiment analysis: classify the polarity of a given text."),xo=f(),Ss=n("li"),qo=a("Text generation (in English): generate text from a given input."),zo=f(),Ps=n("li"),Fo=a("Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),So=f(),Ms=n("li"),Po=a("Question answering: extract the answer from the context, given some context and a question."),Mo=f(),Cs=n("li"),Co=a("Fill-mask: fill in the blank given a text with masked words."),Io=f(),Is=n("li"),No=a("Summarization: generate a summary of a long sequence of text or document."),Oo=f(),Ns=n("li"),Do=a("Translation: translate text into another language."),Lo=f(),Os=n("li"),Ro=a("Feature extraction: create a tensor representation of the text."),Sa=f(),Ue=n("p"),Ds=n("strong"),Ho=a("Image"),Uo=a(":"),Pa=f(),W=n("ul"),Ls=n("li"),Wo=a("Image classification: classify an image."),Bo=f(),Rs=n("li"),Yo=a("Image segmentation: classify every pixel in an image."),Go=f(),Hs=n("li"),Qo=a("Object detection: detect objects within an image."),Ma=f(),We=n("p"),Us=n("strong"),Jo=a("Audio"),Ko=a(":"),Ca=f(),ce=n("ul"),Ws=n("li"),Vo=a("Audio classification: assign a label to a given segment of audio."),Zo=f(),Bs=n("li"),Xo=a("Automatic speech recognition (ASR): transcribe audio data into text."),Ia=f(),d(de.$$.fragment),Na=f(),ae=n("h3"),_e=n("a"),Ys=n("span"),d(Be.$$.fragment),en=f(),Gs=n("span"),tn=a("Pipeline usage"),Oa=f(),ge=n("p"),sn=a("In the following example, you will use the "),St=n("a"),an=a("pipeline()"),rn=a(" for sentiment analysis."),Da=f(),Pt=n("p"),on=a("Install the following dependencies if you haven\u2019t already:"),La=f(),d(Ye.$$.fragment),Ra=f(),ve=n("p"),nn=a("Import "),Mt=n("a"),ln=a("pipeline()"),pn=a(" and specify the task you want to complete:"),Ha=f(),d(Ge.$$.fragment),Ua=f(),B=n("p"),fn=a("The pipeline downloads and caches a default "),Qe=n("a"),un=a("pretrained model"),mn=a(" and tokenizer for sentiment analysis. Now you can use the "),Qs=n("code"),hn=a("classifier"),cn=a(" on your target text:"),Wa=f(),d(Je.$$.fragment),Ba=f(),$e=n("p"),dn=a("For more than one sentence, pass a list of sentences to the "),Ct=n("a"),_n=a("pipeline()"),gn=a(" which returns a list of dictionaries:"),Ya=f(),d(Ke.$$.fragment),Ga=f(),Y=n("p"),vn=a("The "),It=n("a"),$n=a("pipeline()"),yn=a(" can also iterate over an entire dataset. Start by installing the "),Ve=n("a"),bn=a("\u{1F917} Datasets"),wn=a(" library:"),Qa=f(),d(Ze.$$.fragment),Ja=f(),D=n("p"),kn=a("Create a "),Nt=n("a"),En=a("pipeline()"),jn=a(" with the task you want to solve for and the model you want to use. Set the "),Js=n("code"),An=a("device"),Tn=a(" parameter to "),Ks=n("code"),xn=a("0"),qn=a(" to place the tensors on a CUDA device:"),Ka=f(),d(Xe.$$.fragment),Va=f(),G=n("p"),zn=a("Next, load a dataset (see the \u{1F917} Datasets "),et=n("a"),Fn=a("Quick Start"),Sn=a(" for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),tt=n("a"),Pn=a("SUPERB"),Mn=a(" dataset:"),Za=f(),d(st.$$.fragment),Xa=f(),Ot=n("p"),Cn=a("You can pass a whole dataset pipeline:"),er=f(),d(at.$$.fragment),tr=f(),ye=n("p"),In=a("For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the "),Dt=n("a"),Nn=a("pipeline documentation"),On=a(" for more information."),sr=f(),re=n("h3"),be=n("a"),Vs=n("span"),d(rt.$$.fragment),Dn=f(),Zs=n("span"),Ln=a("Use another model and tokenizer in the pipeline"),ar=f(),M=n("p"),Rn=a("The "),Lt=n("a"),Hn=a("pipeline()"),Un=a(" can accommodate any model from the "),ot=n("a"),Wn=a("Model Hub"),Bn=a(", making it easy to adapt the "),Rt=n("a"),Yn=a("pipeline()"),Gn=a(" for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),nt=n("a"),Qn=a("BERT model"),Jn=a(" fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),rr=f(),d(lt.$$.fragment),or=f(),Q=n("p"),Kn=a("Use the "),Ht=n("a"),Vn=a("AutoModelForSequenceClassification"),Zn=a(" and [\u2018AutoTokenizer\u2019] to load the pretrained model and it\u2019s associated tokenizer (more on an "),Xs=n("code"),Xn=a("AutoClass"),el=a(" below):"),nr=f(),d(it.$$.fragment),lr=f(),J=n("p"),tl=a("Then you can specify the model and tokenizer in the "),Ut=n("a"),sl=a("pipeline()"),al=a(", and apply the "),ea=n("code"),rl=a("classifier"),ol=a(" on your target text:"),ir=f(),d(pt.$$.fragment),pr=f(),K=n("p"),nl=a("If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),Wt=n("a"),ll=a("fine-tuning tutorial"),il=a(" to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),Bt=n("a"),pl=a("here"),fl=a(") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),fr=f(),oe=n("h2"),we=n("a"),ta=n("span"),d(ft.$$.fragment),ul=f(),sa=n("span"),ml=a("AutoClass"),ur=f(),d(ut.$$.fragment),mr=f(),q=n("p"),hl=a("Under the hood, the "),Yt=n("a"),cl=a("AutoModelForSequenceClassification"),dl=a(" and "),Gt=n("a"),_l=a("AutoTokenizer"),gl=a(" classes work together to power the "),Qt=n("a"),vl=a("pipeline()"),$l=a(". An "),Jt=n("a"),yl=a("AutoClass"),bl=a(" is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),aa=n("code"),wl=a("AutoClass"),kl=a(" for your task and it\u2019s associated tokenizer with "),Kt=n("a"),El=a("AutoTokenizer"),jl=a("."),hr=f(),V=n("p"),Al=a("Let\u2019s return to our example and see how you can use the "),ra=n("code"),Tl=a("AutoClass"),xl=a(" to replicate the results of the "),Vt=n("a"),ql=a("pipeline()"),zl=a("."),cr=f(),ne=n("h3"),ke=n("a"),oa=n("span"),d(mt.$$.fragment),Fl=f(),na=n("span"),Sl=a("AutoTokenizer"),dr=f(),Z=n("p"),Pl=a("A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),la=n("em"),Ml=a("tokens"),Cl=a(". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),Zt=n("a"),Il=a("here"),Nl=a("). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),_r=f(),Ee=n("p"),Ol=a("Load a tokenizer with "),Xt=n("a"),Dl=a("AutoTokenizer"),Ll=a(":"),gr=f(),d(ht.$$.fragment),vr=f(),je=n("p"),Rl=a("Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),ia=n("em"),Hl=a("vocabulary"),Ul=a("."),$r=f(),es=n("p"),Wl=a("Pass your text to the tokenizer:"),yr=f(),d(ct.$$.fragment),br=f(),ts=n("p"),Bl=a("The tokenizer will return a dictionary containing:"),wr=f(),Ae=n("ul"),ss=n("li"),as=n("a"),Yl=a("input_ids"),Gl=a(": numerical representions of your tokens."),Ql=f(),rs=n("li"),os=n("a"),Jl=a("atttention_mask"),Kl=a(": indicates which tokens should be attended to."),kr=f(),Te=n("p"),Vl=a("Just like the "),ns=n("a"),Zl=a("pipeline()"),Xl=a(", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),Er=f(),d(dt.$$.fragment),jr=f(),xe=n("p"),ei=a("Read the "),ls=n("a"),ti=a("preprocessing"),si=a(" tutorial for more details about tokenization."),Ar=f(),le=n("h3"),qe=n("a"),pa=n("span"),d(_t.$$.fragment),ai=f(),fa=n("span"),ri=a("AutoModel"),Tr=f(),F=n("p"),oi=a("\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),is=n("a"),ni=a("AutoModel"),li=a(" like you would load an "),ps=n("a"),ii=a("AutoTokenizer"),pi=a(". The only difference is selecting the correct "),fs=n("a"),fi=a("AutoModel"),ui=a(" for the task. Since you are doing text - or sequence - classification, load "),us=n("a"),mi=a("AutoModelForSequenceClassification"),hi=a(". The TensorFlow equivalent is simply "),ms=n("a"),ci=a("TFAutoModelForSequenceClassification"),di=a(":"),xr=f(),d(gt.$$.fragment),qr=f(),d(ze.$$.fragment),zr=f(),Fe=n("p"),_i=a("Now you can pass your preprocessed batch of inputs directly to the model. If you are using a PyTorch model, unpack the dictionary by adding "),ua=n("code"),gi=a("**"),vi=a(". For TensorFlow models, pass the dictionary keys directly to the tensors:"),Fr=f(),d(vt.$$.fragment),Sr=f(),X=n("p"),$i=a("The model outputs the final activations in the "),ma=n("code"),yi=a("logits"),bi=a(" attribute. Apply the softmax function to the "),ha=n("code"),wi=a("logits"),ki=a(" to retrieve the probabilities:"),Pr=f(),d($t.$$.fragment),Mr=f(),d(Se.$$.fragment),Cr=f(),z=n("p"),Ei=a("Models are a standard "),yt=n("a"),ca=n("code"),ji=a("torch.nn.Module"),Ai=a(" or a "),bt=n("a"),da=n("code"),Ti=a("tf.keras.Model"),xi=a(" so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),hs=n("a"),qi=a("Trainer"),zi=a(" class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),_a=n("code"),Fi=a("fit"),Si=a(" method from "),wt=n("a"),Pi=a("Keras"),Mi=a(". Refer to the "),cs=n("a"),Ci=a("training tutorial"),Ii=a(" for more details."),Ir=f(),d(Pe.$$.fragment),Nr=f(),ie=n("h3"),Me=n("a"),ga=n("span"),d(kt.$$.fragment),Ni=f(),va=n("span"),Oi=a("Save a model"),Or=f(),Ce=n("p"),Di=a("Once your model is fine-tuned, you can save it with its tokenizer using "),ds=n("a"),Li=a("PreTrainedModel.save_pretrained()"),Ri=a(":"),Dr=f(),d(Et.$$.fragment),Lr=f(),Ie=n("p"),Hi=a("When you are ready to use the model again, reload it with "),_s=n("a"),Ui=a("PreTrainedModel.from_pretrained()"),Wi=a(":"),Rr=f(),d(jt.$$.fragment),Hr=f(),ee=n("p"),Bi=a("One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),$a=n("code"),Yi=a("from_pt"),Gi=a(" or "),ya=n("code"),Qi=a("from_tf"),Ji=a(" parameter can convert the model from one framework to the other:"),Ur=f(),d(At.$$.fragment),this.h()},l(e){const o=au('[data-svelte="svelte-1phssyn"]',document.head);h=l(o,"META",{name:!0,content:!0}),o.forEach(s),k=u(e),c=l(e,"H1",{class:!0});var Tt=i(c);w=l(Tt,"A",{id:!0,class:!0,href:!0});var ba=i(w);A=l(ba,"SPAN",{});var wa=i(A);_(b.$$.fragment,wa),wa.forEach(s),ba.forEach(s),E=u(Tt),x=l(Tt,"SPAN",{});var ka=i(x);P=r(ka,"Quick tour"),ka.forEach(s),Tt.forEach(s),j=u(e),_(O.$$.fragment,e),H=u(e),U=l(e,"P",{});var pe=i(U);mo=r(pe,"Get up and running with \u{1F917} Transformers! Start using the "),xt=l(pe,"A",{href:!0});var sp=i(xt);ho=r(sp,"pipeline()"),sp.forEach(s),co=r(pe," for rapid inference, and quickly load a pretrained model and tokenizer with an "),qt=l(pe,"A",{href:!0});var ap=i(qt);_o=r(ap,"AutoClass"),ap.forEach(s),go=r(pe," to solve your text, vision or audio task."),pe.forEach(s),ja=u(e),_(ue.$$.fragment,e),Aa=u(e),se=l(e,"H2",{class:!0});var Br=i(se);me=l(Br,"A",{id:!0,class:!0,href:!0});var rp=i(me);xs=l(rp,"SPAN",{});var op=i(xs);_(De.$$.fragment,op),op.forEach(s),rp.forEach(s),vo=u(Br),qs=l(Br,"SPAN",{});var np=i(qs);$o=r(np,"Pipeline"),np.forEach(s),Br.forEach(s),Ta=u(e),Le=l(e,"P",{});var Ki=i(Le);zt=l(Ki,"A",{href:!0});var lp=i(zt);yo=r(lp,"pipeline()"),lp.forEach(s),bo=r(Ki," is the easiest way to use a pretrained model for a given task."),Ki.forEach(s),xa=u(e),_(Re.$$.fragment,e),qa=u(e),he=l(e,"P",{});var Yr=i(he);wo=r(Yr,"The "),Ft=l(Yr,"A",{href:!0});var ip=i(Ft);ko=r(ip,"pipeline()"),ip.forEach(s),Eo=r(Yr," supports many common tasks out-of-the-box:"),Yr.forEach(s),za=u(e),He=l(e,"P",{});var Vi=i(He);zs=l(Vi,"STRONG",{});var pp=i(zs);jo=r(pp,"Text"),pp.forEach(s),Ao=r(Vi,":"),Vi.forEach(s),Fa=u(e),T=l(e,"UL",{});var S=i(T);Fs=l(S,"LI",{});var fp=i(Fs);To=r(fp,"Sentiment analysis: classify the polarity of a given text."),fp.forEach(s),xo=u(S),Ss=l(S,"LI",{});var up=i(Ss);qo=r(up,"Text generation (in English): generate text from a given input."),up.forEach(s),zo=u(S),Ps=l(S,"LI",{});var mp=i(Ps);Fo=r(mp,"Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),mp.forEach(s),So=u(S),Ms=l(S,"LI",{});var hp=i(Ms);Po=r(hp,"Question answering: extract the answer from the context, given some context and a question."),hp.forEach(s),Mo=u(S),Cs=l(S,"LI",{});var cp=i(Cs);Co=r(cp,"Fill-mask: fill in the blank given a text with masked words."),cp.forEach(s),Io=u(S),Is=l(S,"LI",{});var dp=i(Is);No=r(dp,"Summarization: generate a summary of a long sequence of text or document."),dp.forEach(s),Oo=u(S),Ns=l(S,"LI",{});var _p=i(Ns);Do=r(_p,"Translation: translate text into another language."),_p.forEach(s),Lo=u(S),Os=l(S,"LI",{});var gp=i(Os);Ro=r(gp,"Feature extraction: create a tensor representation of the text."),gp.forEach(s),S.forEach(s),Sa=u(e),Ue=l(e,"P",{});var Zi=i(Ue);Ds=l(Zi,"STRONG",{});var vp=i(Ds);Ho=r(vp,"Image"),vp.forEach(s),Uo=r(Zi,":"),Zi.forEach(s),Pa=u(e),W=l(e,"UL",{});var gs=i(W);Ls=l(gs,"LI",{});var $p=i(Ls);Wo=r($p,"Image classification: classify an image."),$p.forEach(s),Bo=u(gs),Rs=l(gs,"LI",{});var yp=i(Rs);Yo=r(yp,"Image segmentation: classify every pixel in an image."),yp.forEach(s),Go=u(gs),Hs=l(gs,"LI",{});var bp=i(Hs);Qo=r(bp,"Object detection: detect objects within an image."),bp.forEach(s),gs.forEach(s),Ma=u(e),We=l(e,"P",{});var Xi=i(We);Us=l(Xi,"STRONG",{});var wp=i(Us);Jo=r(wp,"Audio"),wp.forEach(s),Ko=r(Xi,":"),Xi.forEach(s),Ca=u(e),ce=l(e,"UL",{});var Gr=i(ce);Ws=l(Gr,"LI",{});var kp=i(Ws);Vo=r(kp,"Audio classification: assign a label to a given segment of audio."),kp.forEach(s),Zo=u(Gr),Bs=l(Gr,"LI",{});var Ep=i(Bs);Xo=r(Ep,"Automatic speech recognition (ASR): transcribe audio data into text."),Ep.forEach(s),Gr.forEach(s),Ia=u(e),_(de.$$.fragment,e),Na=u(e),ae=l(e,"H3",{class:!0});var Qr=i(ae);_e=l(Qr,"A",{id:!0,class:!0,href:!0});var jp=i(_e);Ys=l(jp,"SPAN",{});var Ap=i(Ys);_(Be.$$.fragment,Ap),Ap.forEach(s),jp.forEach(s),en=u(Qr),Gs=l(Qr,"SPAN",{});var Tp=i(Gs);tn=r(Tp,"Pipeline usage"),Tp.forEach(s),Qr.forEach(s),Oa=u(e),ge=l(e,"P",{});var Jr=i(ge);sn=r(Jr,"In the following example, you will use the "),St=l(Jr,"A",{href:!0});var xp=i(St);an=r(xp,"pipeline()"),xp.forEach(s),rn=r(Jr," for sentiment analysis."),Jr.forEach(s),Da=u(e),Pt=l(e,"P",{});var qp=i(Pt);on=r(qp,"Install the following dependencies if you haven\u2019t already:"),qp.forEach(s),La=u(e),_(Ye.$$.fragment,e),Ra=u(e),ve=l(e,"P",{});var Kr=i(ve);nn=r(Kr,"Import "),Mt=l(Kr,"A",{href:!0});var zp=i(Mt);ln=r(zp,"pipeline()"),zp.forEach(s),pn=r(Kr," and specify the task you want to complete:"),Kr.forEach(s),Ha=u(e),_(Ge.$$.fragment,e),Ua=u(e),B=l(e,"P",{});var vs=i(B);fn=r(vs,"The pipeline downloads and caches a default "),Qe=l(vs,"A",{href:!0,rel:!0});var Fp=i(Qe);un=r(Fp,"pretrained model"),Fp.forEach(s),mn=r(vs," and tokenizer for sentiment analysis. Now you can use the "),Qs=l(vs,"CODE",{});var Sp=i(Qs);hn=r(Sp,"classifier"),Sp.forEach(s),cn=r(vs," on your target text:"),vs.forEach(s),Wa=u(e),_(Je.$$.fragment,e),Ba=u(e),$e=l(e,"P",{});var Vr=i($e);dn=r(Vr,"For more than one sentence, pass a list of sentences to the "),Ct=l(Vr,"A",{href:!0});var Pp=i(Ct);_n=r(Pp,"pipeline()"),Pp.forEach(s),gn=r(Vr," which returns a list of dictionaries:"),Vr.forEach(s),Ya=u(e),_(Ke.$$.fragment,e),Ga=u(e),Y=l(e,"P",{});var $s=i(Y);vn=r($s,"The "),It=l($s,"A",{href:!0});var Mp=i(It);$n=r(Mp,"pipeline()"),Mp.forEach(s),yn=r($s," can also iterate over an entire dataset. Start by installing the "),Ve=l($s,"A",{href:!0,rel:!0});var Cp=i(Ve);bn=r(Cp,"\u{1F917} Datasets"),Cp.forEach(s),wn=r($s," library:"),$s.forEach(s),Qa=u(e),_(Ze.$$.fragment,e),Ja=u(e),D=l(e,"P",{});var Ne=i(D);kn=r(Ne,"Create a "),Nt=l(Ne,"A",{href:!0});var Ip=i(Nt);En=r(Ip,"pipeline()"),Ip.forEach(s),jn=r(Ne," with the task you want to solve for and the model you want to use. Set the "),Js=l(Ne,"CODE",{});var Np=i(Js);An=r(Np,"device"),Np.forEach(s),Tn=r(Ne," parameter to "),Ks=l(Ne,"CODE",{});var Op=i(Ks);xn=r(Op,"0"),Op.forEach(s),qn=r(Ne," to place the tensors on a CUDA device:"),Ne.forEach(s),Ka=u(e),_(Xe.$$.fragment,e),Va=u(e),G=l(e,"P",{});var ys=i(G);zn=r(ys,"Next, load a dataset (see the \u{1F917} Datasets "),et=l(ys,"A",{href:!0,rel:!0});var Dp=i(et);Fn=r(Dp,"Quick Start"),Dp.forEach(s),Sn=r(ys," for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),tt=l(ys,"A",{href:!0,rel:!0});var Lp=i(tt);Pn=r(Lp,"SUPERB"),Lp.forEach(s),Mn=r(ys," dataset:"),ys.forEach(s),Za=u(e),_(st.$$.fragment,e),Xa=u(e),Ot=l(e,"P",{});var Rp=i(Ot);Cn=r(Rp,"You can pass a whole dataset pipeline:"),Rp.forEach(s),er=u(e),_(at.$$.fragment,e),tr=u(e),ye=l(e,"P",{});var Zr=i(ye);In=r(Zr,"For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the "),Dt=l(Zr,"A",{href:!0});var Hp=i(Dt);Nn=r(Hp,"pipeline documentation"),Hp.forEach(s),On=r(Zr," for more information."),Zr.forEach(s),sr=u(e),re=l(e,"H3",{class:!0});var Xr=i(re);be=l(Xr,"A",{id:!0,class:!0,href:!0});var Up=i(be);Vs=l(Up,"SPAN",{});var Wp=i(Vs);_(rt.$$.fragment,Wp),Wp.forEach(s),Up.forEach(s),Dn=u(Xr),Zs=l(Xr,"SPAN",{});var Bp=i(Zs);Ln=r(Bp,"Use another model and tokenizer in the pipeline"),Bp.forEach(s),Xr.forEach(s),ar=u(e),M=l(e,"P",{});var te=i(M);Rn=r(te,"The "),Lt=l(te,"A",{href:!0});var Yp=i(Lt);Hn=r(Yp,"pipeline()"),Yp.forEach(s),Un=r(te," can accommodate any model from the "),ot=l(te,"A",{href:!0,rel:!0});var Gp=i(ot);Wn=r(Gp,"Model Hub"),Gp.forEach(s),Bn=r(te,", making it easy to adapt the "),Rt=l(te,"A",{href:!0});var Qp=i(Rt);Yn=r(Qp,"pipeline()"),Qp.forEach(s),Gn=r(te," for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),nt=l(te,"A",{href:!0,rel:!0});var Jp=i(nt);Qn=r(Jp,"BERT model"),Jp.forEach(s),Jn=r(te," fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),te.forEach(s),rr=u(e),_(lt.$$.fragment,e),or=u(e),Q=l(e,"P",{});var bs=i(Q);Kn=r(bs,"Use the "),Ht=l(bs,"A",{href:!0});var Kp=i(Ht);Vn=r(Kp,"AutoModelForSequenceClassification"),Kp.forEach(s),Zn=r(bs," and [\u2018AutoTokenizer\u2019] to load the pretrained model and it\u2019s associated tokenizer (more on an "),Xs=l(bs,"CODE",{});var Vp=i(Xs);Xn=r(Vp,"AutoClass"),Vp.forEach(s),el=r(bs," below):"),bs.forEach(s),nr=u(e),_(it.$$.fragment,e),lr=u(e),J=l(e,"P",{});var ws=i(J);tl=r(ws,"Then you can specify the model and tokenizer in the "),Ut=l(ws,"A",{href:!0});var Zp=i(Ut);sl=r(Zp,"pipeline()"),Zp.forEach(s),al=r(ws,", and apply the "),ea=l(ws,"CODE",{});var Xp=i(ea);rl=r(Xp,"classifier"),Xp.forEach(s),ol=r(ws," on your target text:"),ws.forEach(s),ir=u(e),_(pt.$$.fragment,e),pr=u(e),K=l(e,"P",{});var ks=i(K);nl=r(ks,"If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),Wt=l(ks,"A",{href:!0});var ef=i(Wt);ll=r(ef,"fine-tuning tutorial"),ef.forEach(s),il=r(ks," to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),Bt=l(ks,"A",{href:!0});var tf=i(Bt);pl=r(tf,"here"),tf.forEach(s),fl=r(ks,") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),ks.forEach(s),fr=u(e),oe=l(e,"H2",{class:!0});var eo=i(oe);we=l(eo,"A",{id:!0,class:!0,href:!0});var sf=i(we);ta=l(sf,"SPAN",{});var af=i(ta);_(ft.$$.fragment,af),af.forEach(s),sf.forEach(s),ul=u(eo),sa=l(eo,"SPAN",{});var rf=i(sa);ml=r(rf,"AutoClass"),rf.forEach(s),eo.forEach(s),ur=u(e),_(ut.$$.fragment,e),mr=u(e),q=l(e,"P",{});var C=i(q);hl=r(C,"Under the hood, the "),Yt=l(C,"A",{href:!0});var of=i(Yt);cl=r(of,"AutoModelForSequenceClassification"),of.forEach(s),dl=r(C," and "),Gt=l(C,"A",{href:!0});var nf=i(Gt);_l=r(nf,"AutoTokenizer"),nf.forEach(s),gl=r(C," classes work together to power the "),Qt=l(C,"A",{href:!0});var lf=i(Qt);vl=r(lf,"pipeline()"),lf.forEach(s),$l=r(C,". An "),Jt=l(C,"A",{href:!0});var pf=i(Jt);yl=r(pf,"AutoClass"),pf.forEach(s),bl=r(C," is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),aa=l(C,"CODE",{});var ff=i(aa);wl=r(ff,"AutoClass"),ff.forEach(s),kl=r(C," for your task and it\u2019s associated tokenizer with "),Kt=l(C,"A",{href:!0});var uf=i(Kt);El=r(uf,"AutoTokenizer"),uf.forEach(s),jl=r(C,"."),C.forEach(s),hr=u(e),V=l(e,"P",{});var Es=i(V);Al=r(Es,"Let\u2019s return to our example and see how you can use the "),ra=l(Es,"CODE",{});var mf=i(ra);Tl=r(mf,"AutoClass"),mf.forEach(s),xl=r(Es," to replicate the results of the "),Vt=l(Es,"A",{href:!0});var hf=i(Vt);ql=r(hf,"pipeline()"),hf.forEach(s),zl=r(Es,"."),Es.forEach(s),cr=u(e),ne=l(e,"H3",{class:!0});var to=i(ne);ke=l(to,"A",{id:!0,class:!0,href:!0});var cf=i(ke);oa=l(cf,"SPAN",{});var df=i(oa);_(mt.$$.fragment,df),df.forEach(s),cf.forEach(s),Fl=u(to),na=l(to,"SPAN",{});var _f=i(na);Sl=r(_f,"AutoTokenizer"),_f.forEach(s),to.forEach(s),dr=u(e),Z=l(e,"P",{});var js=i(Z);Pl=r(js,"A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),la=l(js,"EM",{});var gf=i(la);Ml=r(gf,"tokens"),gf.forEach(s),Cl=r(js,". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),Zt=l(js,"A",{href:!0});var vf=i(Zt);Il=r(vf,"here"),vf.forEach(s),Nl=r(js,"). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),js.forEach(s),_r=u(e),Ee=l(e,"P",{});var so=i(Ee);Ol=r(so,"Load a tokenizer with "),Xt=l(so,"A",{href:!0});var $f=i(Xt);Dl=r($f,"AutoTokenizer"),$f.forEach(s),Ll=r(so,":"),so.forEach(s),gr=u(e),_(ht.$$.fragment,e),vr=u(e),je=l(e,"P",{});var ao=i(je);Rl=r(ao,"Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),ia=l(ao,"EM",{});var yf=i(ia);Hl=r(yf,"vocabulary"),yf.forEach(s),Ul=r(ao,"."),ao.forEach(s),$r=u(e),es=l(e,"P",{});var bf=i(es);Wl=r(bf,"Pass your text to the tokenizer:"),bf.forEach(s),yr=u(e),_(ct.$$.fragment,e),br=u(e),ts=l(e,"P",{});var wf=i(ts);Bl=r(wf,"The tokenizer will return a dictionary containing:"),wf.forEach(s),wr=u(e),Ae=l(e,"UL",{});var ro=i(Ae);ss=l(ro,"LI",{});var ep=i(ss);as=l(ep,"A",{href:!0});var kf=i(as);Yl=r(kf,"input_ids"),kf.forEach(s),Gl=r(ep,": numerical representions of your tokens."),ep.forEach(s),Ql=u(ro),rs=l(ro,"LI",{});var tp=i(rs);os=l(tp,"A",{href:!0});var Ef=i(os);Jl=r(Ef,"atttention_mask"),Ef.forEach(s),Kl=r(tp,": indicates which tokens should be attended to."),tp.forEach(s),ro.forEach(s),kr=u(e),Te=l(e,"P",{});var oo=i(Te);Vl=r(oo,"Just like the "),ns=l(oo,"A",{href:!0});var jf=i(ns);Zl=r(jf,"pipeline()"),jf.forEach(s),Xl=r(oo,", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),oo.forEach(s),Er=u(e),_(dt.$$.fragment,e),jr=u(e),xe=l(e,"P",{});var no=i(xe);ei=r(no,"Read the "),ls=l(no,"A",{href:!0});var Af=i(ls);ti=r(Af,"preprocessing"),Af.forEach(s),si=r(no," tutorial for more details about tokenization."),no.forEach(s),Ar=u(e),le=l(e,"H3",{class:!0});var lo=i(le);qe=l(lo,"A",{id:!0,class:!0,href:!0});var Tf=i(qe);pa=l(Tf,"SPAN",{});var xf=i(pa);_(_t.$$.fragment,xf),xf.forEach(s),Tf.forEach(s),ai=u(lo),fa=l(lo,"SPAN",{});var qf=i(fa);ri=r(qf,"AutoModel"),qf.forEach(s),lo.forEach(s),Tr=u(e),F=l(e,"P",{});var L=i(F);oi=r(L,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),is=l(L,"A",{href:!0});var zf=i(is);ni=r(zf,"AutoModel"),zf.forEach(s),li=r(L," like you would load an "),ps=l(L,"A",{href:!0});var Ff=i(ps);ii=r(Ff,"AutoTokenizer"),Ff.forEach(s),pi=r(L,". The only difference is selecting the correct "),fs=l(L,"A",{href:!0});var Sf=i(fs);fi=r(Sf,"AutoModel"),Sf.forEach(s),ui=r(L," for the task. Since you are doing text - or sequence - classification, load "),us=l(L,"A",{href:!0});var Pf=i(us);mi=r(Pf,"AutoModelForSequenceClassification"),Pf.forEach(s),hi=r(L,". The TensorFlow equivalent is simply "),ms=l(L,"A",{href:!0});var Mf=i(ms);ci=r(Mf,"TFAutoModelForSequenceClassification"),Mf.forEach(s),di=r(L,":"),L.forEach(s),xr=u(e),_(gt.$$.fragment,e),qr=u(e),_(ze.$$.fragment,e),zr=u(e),Fe=l(e,"P",{});var io=i(Fe);_i=r(io,"Now you can pass your preprocessed batch of inputs directly to the model. If you are using a PyTorch model, unpack the dictionary by adding "),ua=l(io,"CODE",{});var Cf=i(ua);gi=r(Cf,"**"),Cf.forEach(s),vi=r(io,". For TensorFlow models, pass the dictionary keys directly to the tensors:"),io.forEach(s),Fr=u(e),_(vt.$$.fragment,e),Sr=u(e),X=l(e,"P",{});var As=i(X);$i=r(As,"The model outputs the final activations in the "),ma=l(As,"CODE",{});var If=i(ma);yi=r(If,"logits"),If.forEach(s),bi=r(As," attribute. Apply the softmax function to the "),ha=l(As,"CODE",{});var Nf=i(ha);wi=r(Nf,"logits"),Nf.forEach(s),ki=r(As," to retrieve the probabilities:"),As.forEach(s),Pr=u(e),_($t.$$.fragment,e),Mr=u(e),_(Se.$$.fragment,e),Cr=u(e),z=l(e,"P",{});var I=i(z);Ei=r(I,"Models are a standard "),yt=l(I,"A",{href:!0,rel:!0});var Of=i(yt);ca=l(Of,"CODE",{});var Df=i(ca);ji=r(Df,"torch.nn.Module"),Df.forEach(s),Of.forEach(s),Ai=r(I," or a "),bt=l(I,"A",{href:!0,rel:!0});var Lf=i(bt);da=l(Lf,"CODE",{});var Rf=i(da);Ti=r(Rf,"tf.keras.Model"),Rf.forEach(s),Lf.forEach(s),xi=r(I," so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),hs=l(I,"A",{href:!0});var Hf=i(hs);qi=r(Hf,"Trainer"),Hf.forEach(s),zi=r(I," class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),_a=l(I,"CODE",{});var Uf=i(_a);Fi=r(Uf,"fit"),Uf.forEach(s),Si=r(I," method from "),wt=l(I,"A",{href:!0,rel:!0});var Wf=i(wt);Pi=r(Wf,"Keras"),Wf.forEach(s),Mi=r(I,". Refer to the "),cs=l(I,"A",{href:!0});var Bf=i(cs);Ci=r(Bf,"training tutorial"),Bf.forEach(s),Ii=r(I," for more details."),I.forEach(s),Ir=u(e),_(Pe.$$.fragment,e),Nr=u(e),ie=l(e,"H3",{class:!0});var po=i(ie);Me=l(po,"A",{id:!0,class:!0,href:!0});var Yf=i(Me);ga=l(Yf,"SPAN",{});var Gf=i(ga);_(kt.$$.fragment,Gf),Gf.forEach(s),Yf.forEach(s),Ni=u(po),va=l(po,"SPAN",{});var Qf=i(va);Oi=r(Qf,"Save a model"),Qf.forEach(s),po.forEach(s),Or=u(e),Ce=l(e,"P",{});var fo=i(Ce);Di=r(fo,"Once your model is fine-tuned, you can save it with its tokenizer using "),ds=l(fo,"A",{href:!0});var Jf=i(ds);Li=r(Jf,"PreTrainedModel.save_pretrained()"),Jf.forEach(s),Ri=r(fo,":"),fo.forEach(s),Dr=u(e),_(Et.$$.fragment,e),Lr=u(e),Ie=l(e,"P",{});var uo=i(Ie);Hi=r(uo,"When you are ready to use the model again, reload it with "),_s=l(uo,"A",{href:!0});var Kf=i(_s);Ui=r(Kf,"PreTrainedModel.from_pretrained()"),Kf.forEach(s),Wi=r(uo,":"),uo.forEach(s),Rr=u(e),_(jt.$$.fragment,e),Hr=u(e),ee=l(e,"P",{});var Ts=i(ee);Bi=r(Ts,"One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),$a=l(Ts,"CODE",{});var Vf=i($a);Yi=r(Vf,"from_pt"),Vf.forEach(s),Gi=r(Ts," or "),ya=l(Ts,"CODE",{});var Zf=i(ya);Qi=r(Zf,"from_tf"),Zf.forEach(s),Ji=r(Ts," parameter can convert the model from one framework to the other:"),Ts.forEach(s),Ur=u(e),_(At.$$.fragment,e),this.h()},h(){m(h,"name","hf:doc:metadata"),m(h,"content",JSON.stringify(uu)),m(w,"id","quick-tour"),m(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(w,"href","#quick-tour"),m(c,"class","relative group"),m(xt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(qt,"href","./model_doc/auto"),m(me,"id","pipeline"),m(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(me,"href","#pipeline"),m(se,"class","relative group"),m(zt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(Ft,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(_e,"id","pipeline-usage"),m(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(_e,"href","#pipeline-usage"),m(ae,"class","relative group"),m(St,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(Mt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(Qe,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),m(Qe,"rel","nofollow"),m(Ct,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(It,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(Ve,"href","https://huggingface.co/docs/datasets/"),m(Ve,"rel","nofollow"),m(Nt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(et,"href","https://huggingface.co/docs/datasets/quickstart.html"),m(et,"rel","nofollow"),m(tt,"href","https://huggingface.co/datasets/superb"),m(tt,"rel","nofollow"),m(Dt,"href","./main_classes/pipelines"),m(be,"id","use-another-model-and-tokenizer-in-the-pipeline"),m(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(be,"href","#use-another-model-and-tokenizer-in-the-pipeline"),m(re,"class","relative group"),m(Lt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(ot,"href","https://huggingface.co/models"),m(ot,"rel","nofollow"),m(Rt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(nt,"href","https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"),m(nt,"rel","nofollow"),m(Ht,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),m(Ut,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(Wt,"href","./training"),m(Bt,"href","./model_sharing"),m(we,"id","autoclass"),m(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(we,"href","#autoclass"),m(oe,"class","relative group"),m(Yt,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),m(Gt,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),m(Qt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(Jt,"href","./model_doc/auto"),m(Kt,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),m(Vt,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(ke,"id","autotokenizer"),m(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ke,"href","#autotokenizer"),m(ne,"class","relative group"),m(Zt,"href","./tokenizer_summary"),m(Xt,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),m(as,"href","./glossary#input-ids"),m(os,"href",".glossary#attention-mask"),m(ns,"href","/docs/transformers/master/en/main_classes/pipelines#transformers.pipeline"),m(ls,"href","./preprocessing"),m(qe,"id","automodel"),m(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(qe,"href","#automodel"),m(le,"class","relative group"),m(is,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModel"),m(ps,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),m(fs,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModel"),m(us,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),m(ms,"href","/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification"),m(yt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),m(yt,"rel","nofollow"),m(bt,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),m(bt,"rel","nofollow"),m(hs,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(wt,"href","https://keras.io/"),m(wt,"rel","nofollow"),m(cs,"href","./training"),m(Me,"id","save-a-model"),m(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Me,"href","#save-a-model"),m(ie,"class","relative group"),m(ds,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),m(_s,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained")},m(e,o){t(document.head,h),p(e,k,o),p(e,c,o),t(c,w),t(w,A),g(b,A,null),t(c,E),t(c,x),t(x,P),p(e,j,o),g(O,e,o),p(e,H,o),p(e,U,o),t(U,mo),t(U,xt),t(xt,ho),t(U,co),t(U,qt),t(qt,_o),t(U,go),p(e,ja,o),g(ue,e,o),p(e,Aa,o),p(e,se,o),t(se,me),t(me,xs),g(De,xs,null),t(se,vo),t(se,qs),t(qs,$o),p(e,Ta,o),p(e,Le,o),t(Le,zt),t(zt,yo),t(Le,bo),p(e,xa,o),g(Re,e,o),p(e,qa,o),p(e,he,o),t(he,wo),t(he,Ft),t(Ft,ko),t(he,Eo),p(e,za,o),p(e,He,o),t(He,zs),t(zs,jo),t(He,Ao),p(e,Fa,o),p(e,T,o),t(T,Fs),t(Fs,To),t(T,xo),t(T,Ss),t(Ss,qo),t(T,zo),t(T,Ps),t(Ps,Fo),t(T,So),t(T,Ms),t(Ms,Po),t(T,Mo),t(T,Cs),t(Cs,Co),t(T,Io),t(T,Is),t(Is,No),t(T,Oo),t(T,Ns),t(Ns,Do),t(T,Lo),t(T,Os),t(Os,Ro),p(e,Sa,o),p(e,Ue,o),t(Ue,Ds),t(Ds,Ho),t(Ue,Uo),p(e,Pa,o),p(e,W,o),t(W,Ls),t(Ls,Wo),t(W,Bo),t(W,Rs),t(Rs,Yo),t(W,Go),t(W,Hs),t(Hs,Qo),p(e,Ma,o),p(e,We,o),t(We,Us),t(Us,Jo),t(We,Ko),p(e,Ca,o),p(e,ce,o),t(ce,Ws),t(Ws,Vo),t(ce,Zo),t(ce,Bs),t(Bs,Xo),p(e,Ia,o),g(de,e,o),p(e,Na,o),p(e,ae,o),t(ae,_e),t(_e,Ys),g(Be,Ys,null),t(ae,en),t(ae,Gs),t(Gs,tn),p(e,Oa,o),p(e,ge,o),t(ge,sn),t(ge,St),t(St,an),t(ge,rn),p(e,Da,o),p(e,Pt,o),t(Pt,on),p(e,La,o),g(Ye,e,o),p(e,Ra,o),p(e,ve,o),t(ve,nn),t(ve,Mt),t(Mt,ln),t(ve,pn),p(e,Ha,o),g(Ge,e,o),p(e,Ua,o),p(e,B,o),t(B,fn),t(B,Qe),t(Qe,un),t(B,mn),t(B,Qs),t(Qs,hn),t(B,cn),p(e,Wa,o),g(Je,e,o),p(e,Ba,o),p(e,$e,o),t($e,dn),t($e,Ct),t(Ct,_n),t($e,gn),p(e,Ya,o),g(Ke,e,o),p(e,Ga,o),p(e,Y,o),t(Y,vn),t(Y,It),t(It,$n),t(Y,yn),t(Y,Ve),t(Ve,bn),t(Y,wn),p(e,Qa,o),g(Ze,e,o),p(e,Ja,o),p(e,D,o),t(D,kn),t(D,Nt),t(Nt,En),t(D,jn),t(D,Js),t(Js,An),t(D,Tn),t(D,Ks),t(Ks,xn),t(D,qn),p(e,Ka,o),g(Xe,e,o),p(e,Va,o),p(e,G,o),t(G,zn),t(G,et),t(et,Fn),t(G,Sn),t(G,tt),t(tt,Pn),t(G,Mn),p(e,Za,o),g(st,e,o),p(e,Xa,o),p(e,Ot,o),t(Ot,Cn),p(e,er,o),g(at,e,o),p(e,tr,o),p(e,ye,o),t(ye,In),t(ye,Dt),t(Dt,Nn),t(ye,On),p(e,sr,o),p(e,re,o),t(re,be),t(be,Vs),g(rt,Vs,null),t(re,Dn),t(re,Zs),t(Zs,Ln),p(e,ar,o),p(e,M,o),t(M,Rn),t(M,Lt),t(Lt,Hn),t(M,Un),t(M,ot),t(ot,Wn),t(M,Bn),t(M,Rt),t(Rt,Yn),t(M,Gn),t(M,nt),t(nt,Qn),t(M,Jn),p(e,rr,o),g(lt,e,o),p(e,or,o),p(e,Q,o),t(Q,Kn),t(Q,Ht),t(Ht,Vn),t(Q,Zn),t(Q,Xs),t(Xs,Xn),t(Q,el),p(e,nr,o),g(it,e,o),p(e,lr,o),p(e,J,o),t(J,tl),t(J,Ut),t(Ut,sl),t(J,al),t(J,ea),t(ea,rl),t(J,ol),p(e,ir,o),g(pt,e,o),p(e,pr,o),p(e,K,o),t(K,nl),t(K,Wt),t(Wt,ll),t(K,il),t(K,Bt),t(Bt,pl),t(K,fl),p(e,fr,o),p(e,oe,o),t(oe,we),t(we,ta),g(ft,ta,null),t(oe,ul),t(oe,sa),t(sa,ml),p(e,ur,o),g(ut,e,o),p(e,mr,o),p(e,q,o),t(q,hl),t(q,Yt),t(Yt,cl),t(q,dl),t(q,Gt),t(Gt,_l),t(q,gl),t(q,Qt),t(Qt,vl),t(q,$l),t(q,Jt),t(Jt,yl),t(q,bl),t(q,aa),t(aa,wl),t(q,kl),t(q,Kt),t(Kt,El),t(q,jl),p(e,hr,o),p(e,V,o),t(V,Al),t(V,ra),t(ra,Tl),t(V,xl),t(V,Vt),t(Vt,ql),t(V,zl),p(e,cr,o),p(e,ne,o),t(ne,ke),t(ke,oa),g(mt,oa,null),t(ne,Fl),t(ne,na),t(na,Sl),p(e,dr,o),p(e,Z,o),t(Z,Pl),t(Z,la),t(la,Ml),t(Z,Cl),t(Z,Zt),t(Zt,Il),t(Z,Nl),p(e,_r,o),p(e,Ee,o),t(Ee,Ol),t(Ee,Xt),t(Xt,Dl),t(Ee,Ll),p(e,gr,o),g(ht,e,o),p(e,vr,o),p(e,je,o),t(je,Rl),t(je,ia),t(ia,Hl),t(je,Ul),p(e,$r,o),p(e,es,o),t(es,Wl),p(e,yr,o),g(ct,e,o),p(e,br,o),p(e,ts,o),t(ts,Bl),p(e,wr,o),p(e,Ae,o),t(Ae,ss),t(ss,as),t(as,Yl),t(ss,Gl),t(Ae,Ql),t(Ae,rs),t(rs,os),t(os,Jl),t(rs,Kl),p(e,kr,o),p(e,Te,o),t(Te,Vl),t(Te,ns),t(ns,Zl),t(Te,Xl),p(e,Er,o),g(dt,e,o),p(e,jr,o),p(e,xe,o),t(xe,ei),t(xe,ls),t(ls,ti),t(xe,si),p(e,Ar,o),p(e,le,o),t(le,qe),t(qe,pa),g(_t,pa,null),t(le,ai),t(le,fa),t(fa,ri),p(e,Tr,o),p(e,F,o),t(F,oi),t(F,is),t(is,ni),t(F,li),t(F,ps),t(ps,ii),t(F,pi),t(F,fs),t(fs,fi),t(F,ui),t(F,us),t(us,mi),t(F,hi),t(F,ms),t(ms,ci),t(F,di),p(e,xr,o),g(gt,e,o),p(e,qr,o),g(ze,e,o),p(e,zr,o),p(e,Fe,o),t(Fe,_i),t(Fe,ua),t(ua,gi),t(Fe,vi),p(e,Fr,o),g(vt,e,o),p(e,Sr,o),p(e,X,o),t(X,$i),t(X,ma),t(ma,yi),t(X,bi),t(X,ha),t(ha,wi),t(X,ki),p(e,Pr,o),g($t,e,o),p(e,Mr,o),g(Se,e,o),p(e,Cr,o),p(e,z,o),t(z,Ei),t(z,yt),t(yt,ca),t(ca,ji),t(z,Ai),t(z,bt),t(bt,da),t(da,Ti),t(z,xi),t(z,hs),t(hs,qi),t(z,zi),t(z,_a),t(_a,Fi),t(z,Si),t(z,wt),t(wt,Pi),t(z,Mi),t(z,cs),t(cs,Ci),t(z,Ii),p(e,Ir,o),g(Pe,e,o),p(e,Nr,o),p(e,ie,o),t(ie,Me),t(Me,ga),g(kt,ga,null),t(ie,Ni),t(ie,va),t(va,Oi),p(e,Or,o),p(e,Ce,o),t(Ce,Di),t(Ce,ds),t(ds,Li),t(Ce,Ri),p(e,Dr,o),g(Et,e,o),p(e,Lr,o),p(e,Ie,o),t(Ie,Hi),t(Ie,_s),t(_s,Ui),t(Ie,Wi),p(e,Rr,o),g(jt,e,o),p(e,Hr,o),p(e,ee,o),t(ee,Bi),t(ee,$a),t($a,Yi),t(ee,Gi),t(ee,ya),t(ya,Qi),t(ee,Ji),p(e,Ur,o),g(At,e,o),Wr=!0},p(e,[o]){const Tt={};o&2&&(Tt.$$scope={dirty:o,ctx:e}),ue.$set(Tt);const ba={};o&2&&(ba.$$scope={dirty:o,ctx:e}),de.$set(ba);const wa={};o&2&&(wa.$$scope={dirty:o,ctx:e}),ze.$set(wa);const ka={};o&2&&(ka.$$scope={dirty:o,ctx:e}),Se.$set(ka);const pe={};o&2&&(pe.$$scope={dirty:o,ctx:e}),Pe.$set(pe)},i(e){Wr||(v(b.$$.fragment,e),v(O.$$.fragment,e),v(ue.$$.fragment,e),v(De.$$.fragment,e),v(Re.$$.fragment,e),v(de.$$.fragment,e),v(Be.$$.fragment,e),v(Ye.$$.fragment,e),v(Ge.$$.fragment,e),v(Je.$$.fragment,e),v(Ke.$$.fragment,e),v(Ze.$$.fragment,e),v(Xe.$$.fragment,e),v(st.$$.fragment,e),v(at.$$.fragment,e),v(rt.$$.fragment,e),v(lt.$$.fragment,e),v(it.$$.fragment,e),v(pt.$$.fragment,e),v(ft.$$.fragment,e),v(ut.$$.fragment,e),v(mt.$$.fragment,e),v(ht.$$.fragment,e),v(ct.$$.fragment,e),v(dt.$$.fragment,e),v(_t.$$.fragment,e),v(gt.$$.fragment,e),v(ze.$$.fragment,e),v(vt.$$.fragment,e),v($t.$$.fragment,e),v(Se.$$.fragment,e),v(Pe.$$.fragment,e),v(kt.$$.fragment,e),v(Et.$$.fragment,e),v(jt.$$.fragment,e),v(At.$$.fragment,e),Wr=!0)},o(e){$(b.$$.fragment,e),$(O.$$.fragment,e),$(ue.$$.fragment,e),$(De.$$.fragment,e),$(Re.$$.fragment,e),$(de.$$.fragment,e),$(Be.$$.fragment,e),$(Ye.$$.fragment,e),$(Ge.$$.fragment,e),$(Je.$$.fragment,e),$(Ke.$$.fragment,e),$(Ze.$$.fragment,e),$(Xe.$$.fragment,e),$(st.$$.fragment,e),$(at.$$.fragment,e),$(rt.$$.fragment,e),$(lt.$$.fragment,e),$(it.$$.fragment,e),$(pt.$$.fragment,e),$(ft.$$.fragment,e),$(ut.$$.fragment,e),$(mt.$$.fragment,e),$(ht.$$.fragment,e),$(ct.$$.fragment,e),$(dt.$$.fragment,e),$(_t.$$.fragment,e),$(gt.$$.fragment,e),$(ze.$$.fragment,e),$(vt.$$.fragment,e),$($t.$$.fragment,e),$(Se.$$.fragment,e),$(Pe.$$.fragment,e),$(kt.$$.fragment,e),$(Et.$$.fragment,e),$(jt.$$.fragment,e),$(At.$$.fragment,e),Wr=!1},d(e){s(h),e&&s(k),e&&s(c),y(b),e&&s(j),y(O,e),e&&s(H),e&&s(U),e&&s(ja),y(ue,e),e&&s(Aa),e&&s(se),y(De),e&&s(Ta),e&&s(Le),e&&s(xa),y(Re,e),e&&s(qa),e&&s(he),e&&s(za),e&&s(He),e&&s(Fa),e&&s(T),e&&s(Sa),e&&s(Ue),e&&s(Pa),e&&s(W),e&&s(Ma),e&&s(We),e&&s(Ca),e&&s(ce),e&&s(Ia),y(de,e),e&&s(Na),e&&s(ae),y(Be),e&&s(Oa),e&&s(ge),e&&s(Da),e&&s(Pt),e&&s(La),y(Ye,e),e&&s(Ra),e&&s(ve),e&&s(Ha),y(Ge,e),e&&s(Ua),e&&s(B),e&&s(Wa),y(Je,e),e&&s(Ba),e&&s($e),e&&s(Ya),y(Ke,e),e&&s(Ga),e&&s(Y),e&&s(Qa),y(Ze,e),e&&s(Ja),e&&s(D),e&&s(Ka),y(Xe,e),e&&s(Va),e&&s(G),e&&s(Za),y(st,e),e&&s(Xa),e&&s(Ot),e&&s(er),y(at,e),e&&s(tr),e&&s(ye),e&&s(sr),e&&s(re),y(rt),e&&s(ar),e&&s(M),e&&s(rr),y(lt,e),e&&s(or),e&&s(Q),e&&s(nr),y(it,e),e&&s(lr),e&&s(J),e&&s(ir),y(pt,e),e&&s(pr),e&&s(K),e&&s(fr),e&&s(oe),y(ft),e&&s(ur),y(ut,e),e&&s(mr),e&&s(q),e&&s(hr),e&&s(V),e&&s(cr),e&&s(ne),y(mt),e&&s(dr),e&&s(Z),e&&s(_r),e&&s(Ee),e&&s(gr),y(ht,e),e&&s(vr),e&&s(je),e&&s($r),e&&s(es),e&&s(yr),y(ct,e),e&&s(br),e&&s(ts),e&&s(wr),e&&s(Ae),e&&s(kr),e&&s(Te),e&&s(Er),y(dt,e),e&&s(jr),e&&s(xe),e&&s(Ar),e&&s(le),y(_t),e&&s(Tr),e&&s(F),e&&s(xr),y(gt,e),e&&s(qr),y(ze,e),e&&s(zr),e&&s(Fe),e&&s(Fr),y(vt,e),e&&s(Sr),e&&s(X),e&&s(Pr),y($t,e),e&&s(Mr),y(Se,e),e&&s(Cr),e&&s(z),e&&s(Ir),y(Pe,e),e&&s(Nr),e&&s(ie),y(kt),e&&s(Or),e&&s(Ce),e&&s(Dr),y(Et,e),e&&s(Lr),e&&s(Ie),e&&s(Rr),y(jt,e),e&&s(Hr),e&&s(ee),e&&s(Ur),y(At,e)}}}const uu={local:"quick-tour",sections:[{local:"pipeline",sections:[{local:"pipeline-usage",title:"Pipeline usage"},{local:"use-another-model-and-tokenizer-in-the-pipeline",title:"Use another model and tokenizer in the pipeline"}],title:"Pipeline"},{local:"autoclass",sections:[{local:"autotokenizer",title:"AutoTokenizer"},{local:"automodel",title:"AutoModel"},{local:"save-a-model",title:"Save a model"}],title:"AutoClass"}],title:"Quick tour"};function mu(N,h,k){let{fw:c}=h;return N.$$set=w=>{"fw"in w&&k(0,c=w.fw)},[c]}class bu extends eu{constructor(h){super();tu(this,h,mu,fu,su,{fw:0})}}export{bu as default,uu as metadata};
