import{S as rc,i as ac,s as sc,e as r,k as c,w as u,t,M as dc,c as a,d as o,m as l,a as s,x as g,h as n,b as i,F as e,g as y,y as _,q as v,o as b,B as T,v as ic}from"../../chunks/vendor-6b77c823.js";import{T as Dd}from"../../chunks/Tip-39098574.js";import{D as B}from"../../chunks/Docstring-abef54e3.js";import{C as wo}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as Yt}from"../../chunks/IconCopyLink-7a11ce68.js";function cc(pe){let p,P,f,V,z;return{c(){p=r("p"),P=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),V=t("Module"),z=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(w){p=a(w,"P",{});var j=s(p);P=n(j,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(j,"CODE",{});var C=s(f);V=n(C,"Module"),C.forEach(o),z=n(j,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),j.forEach(o)},m(w,j){y(w,p,j),e(p,P),e(p,f),e(f,V),e(p,z)},d(w){w&&o(p)}}}function lc(pe){let p,P,f,V,z;return{c(){p=r("p"),P=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),V=t("Module"),z=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(w){p=a(w,"P",{});var j=s(p);P=n(j,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(j,"CODE",{});var C=s(f);V=n(C,"Module"),C.forEach(o),z=n(j,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),j.forEach(o)},m(w,j){y(w,p,j),e(p,P),e(p,f),e(f,V),e(p,z)},d(w){w&&o(p)}}}function pc(pe){let p,P,f,V,z;return{c(){p=r("p"),P=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),V=t("Module"),z=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(w){p=a(w,"P",{});var j=s(p);P=n(j,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(j,"CODE",{});var C=s(f);V=n(C,"Module"),C.forEach(o),z=n(j,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),j.forEach(o)},m(w,j){y(w,p,j),e(p,P),e(p,f),e(f,V),e(p,z)},d(w){w&&o(p)}}}function hc(pe){let p,P,f,V,z,w,j,C,Dn,Jt,m,qn,Eo,$n,Fn,ht,zn,Pn,ko,Cn,An,xo,Sn,In,jo,Ln,Nn,Mo,On,Rn,mt,Bn,Gn,Vo,Wn,Un,Do,Hn,Zn,qo,Yn,Jn,$o,Kn,Qn,Kt,he,Xn,xe,er,or,Qt,G,tr,Fo,nr,rr,zo,ar,sr,Xt,Q,me,ft,je,dr,ut,ir,en,F,Me,cr,fe,Po,lr,pr,Co,hr,mr,fr,X,ur,Ao,gr,_r,So,vr,br,Tr,gt,yr,wr,Ve,Er,ue,De,kr,qe,xr,Io,jr,Mr,Vr,ge,$e,Dr,ee,qr,_t,$r,Fr,vt,zr,Pr,on,oe,_e,bt,Fe,Cr,Tt,Ar,tn,E,ze,Sr,te,Ir,Lo,Lr,Nr,No,Or,Rr,Br,Pe,Gr,Ce,Wr,Ur,Hr,Ae,Zr,Se,Yr,Jr,Kr,yt,Qr,Xr,Ie,ea,Oo,oa,ta,na,Le,ra,Ne,aa,sa,da,W,Ro,ia,ca,wt,la,pa,Et,ha,ma,fa,A,Oe,ua,ne,ga,Bo,_a,va,kt,ba,Ta,ya,ve,wa,xt,Ea,ka,Re,xa,S,Be,ja,jt,Ma,Va,re,Da,Mt,qa,$a,Vt,Fa,za,Pa,Dt,Ca,Aa,Ge,nn,ae,be,qt,We,Sa,$t,Ia,rn,k,Ue,La,se,Na,Go,Oa,Ra,Wo,Ba,Ga,Wa,He,Ua,Ze,Ha,Za,Ya,Ye,Ja,Je,Ka,Qa,Xa,Ft,es,os,Ke,ts,Uo,ns,rs,as,Qe,ss,Xe,ds,is,cs,U,Ho,ls,ps,Zo,hs,ms,Yo,fs,us,gs,I,eo,_s,de,vs,Jo,bs,Ts,zt,ys,ws,Es,Te,ks,Pt,xs,js,oo,Ms,O,to,Vs,Ct,Ds,qs,At,$s,Fs,no,an,ie,ye,St,ro,zs,It,Ps,sn,x,ao,Cs,ce,As,Ko,Ss,Is,Qo,Ls,Ns,Os,so,Rs,io,Bs,Gs,Ws,co,Us,lo,Hs,Zs,Ys,Lt,Js,Ks,po,Qs,Xo,Xs,ed,od,ho,td,mo,nd,rd,ad,H,et,sd,dd,Nt,id,cd,Ot,ld,pd,hd,L,fo,md,le,fd,ot,ud,gd,Rt,_d,vd,bd,we,Td,Bt,yd,wd,uo,Ed,R,go,kd,Gt,xd,jd,Wt,Md,Vd,_o,dn;return w=new Yt({}),je=new Yt({}),Me=new B({props:{name:"class transformers.VisionEncoderDecoderConfig",anchor:"transformers.VisionEncoderDecoderConfig",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py#L27",parametersDescription:[{anchor:"transformers.VisionEncoderDecoderConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments. Notably:</p>
<ul>
<li><strong>encoder</strong> (<a href="/docs/transformers/v4.18.0/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014; An instance of a configuration object that defines
the encoder config.</li>
<li><strong>decoder</strong> (<a href="/docs/transformers/v4.18.0/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014; An instance of a configuration object that defines
the decoder config.</li>
</ul>`,name:"kwargs"}]}}),Ve=new wo({props:{code:`from transformers import BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel

# Initializing a ViT & BERT style configuration
config_encoder = ViTConfig()
config_decoder = BertConfig()

config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

# Initializing a ViTBert model from a ViT & bert-base-uncased style configurations
model = VisionEncoderDecoderModel(config=config)

# Accessing the model configuration
config_encoder = model.config.encoder
config_decoder = model.config.decoder
# set decoder config to causal lm
config_decoder.is_decoder = True
config_decoder.add_cross_attention = True

# Saving the model, including its configuration
model.save_pretrained("my-model")

# loading model and config from pretrained folder
encoder_decoder_config = VisionEncoderDecoderConfig.from_pretrained("my-model")
model = VisionEncoderDecoderModel.from_pretrained("my-model", config=encoder_decoder_config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a ViT &amp; BERT style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_encoder = ViTConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder = BertConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a ViTBert model from a ViT &amp; bert-base-uncased style configurations</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionEncoderDecoderModel(config=config)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_encoder = model.config.encoder
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder = model.config.decoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># set decoder config to causal lm</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder.is_decoder = <span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder.add_cross_attention = <span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Saving the model, including its configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;my-model&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># loading model and config from pretrained folder</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_decoder_config = VisionEncoderDecoderConfig.from_pretrained(<span class="hljs-string">&quot;my-model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;my-model&quot;</span>, config=encoder_decoder_config)`}}),De=new B({props:{name:"from_encoder_decoder_configs",anchor:"transformers.VisionEncoderDecoderConfig.from_encoder_decoder_configs",parameters:[{name:"encoder_config",val:": PretrainedConfig"},{name:"decoder_config",val:": PretrainedConfig"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py#L93",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/v4.18.0/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"
>VisionEncoderDecoderConfig</a></p>
`}}),$e=new B({props:{name:"to_dict",anchor:"transformers.VisionEncoderDecoderConfig.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py#L110",returnDescription:`
<p>Dictionary of all the attributes that make up this configuration instance,</p>
`,returnType:`
<p><code>Dict[str, any]</code></p>
`}}),Fe=new Yt({}),ze=new B({props:{name:"class transformers.VisionEncoderDecoderModel",anchor:"transformers.VisionEncoderDecoderModel",parameters:[{name:"config",val:": typing.Optional[transformers.configuration_utils.PretrainedConfig] = None"},{name:"encoder",val:": typing.Optional[transformers.modeling_utils.PreTrainedModel] = None"},{name:"decoder",val:": typing.Optional[transformers.modeling_utils.PreTrainedModel] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py#L148",parametersDescription:[{anchor:"transformers.VisionEncoderDecoderModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.18.0/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.18.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Oe=new B({props:{name:"forward",anchor:"transformers.VisionEncoderDecoderModel.forward",parameters:[{name:"pixel_values",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"encoder_outputs",val:" = None"},{name:"past_key_values",val:" = None"},{name:"decoder_inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py#L400",parametersDescription:[{anchor:"transformers.VisionEncoderDecoderModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using a feature extractor (e.g. if you use ViT as the encoder,
you should use <a href="/docs/transformers/v4.18.0/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a>). See <a href="/docs/transformers/v4.18.0/en/model_doc/vit#transformers.ViTFeatureExtractor.__call__">ViTFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.VisionEncoderDecoderModel.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.18.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See <a href="/docs/transformers/v4.18.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.18.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>For training, <code>decoder_input_ids</code> are automatically created by the model by shifting the <code>labels</code> to the
right, replacing -100 by the <code>pad_token_id</code> and prepending them with the <code>decoder_start_token_id</code>.`,name:"decoder_input_ids"},{anchor:"transformers.VisionEncoderDecoderModel.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.VisionEncoderDecoderModel.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
This tuple must consist of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) is a tensor
of hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the
decoder.`,name:"encoder_outputs"},{anchor:"transformers.VisionEncoderDecoderModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.VisionEncoderDecoderModel.forward.decoder_inputs_embeds",description:`<strong>decoder_inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, target_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>decoder_input_ids</code> you can choose to directly pass an embedded
representation. This is useful if you want more control over how to convert <code>decoder_input_ids</code> indices
into associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"decoder_inputs_embeds"},{anchor:"transformers.VisionEncoderDecoderModel.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss for the decoder. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.VisionEncoderDecoderModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.VisionEncoderDecoderModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.VisionEncoderDecoderModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.VisionEncoderDecoderModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the model will return a <code>Seq2SeqLMOutput</code> instead of a plain tuple.
kwargs &#x2014; (<em>optional</em>) Remaining dictionary of keyword arguments. Keyword arguments come in two flavors:</p>
<ul>
<li>Without a prefix which will be input as <code>**encoder_kwargs</code> for the encoder forward function.</li>
<li>With a <em>decoder_</em> prefix which will be input as <code>**decoder_kwargs</code> for the decoder forward function.</li>
</ul>`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.18.0/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>transformers.modeling_outputs.Seq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.18.0/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"
>VisionEncoderDecoderConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.18.0/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>transformers.modeling_outputs.Seq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ve=new Dd({props:{$$slots:{default:[cc]},$$scope:{ctx:pe}}}),Re=new wo({props:{code:`from transformers import TrOCRProcessor, VisionEncoderDecoderModel
import requests
from PIL import Image
import torch

processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten")

# load image from the IAM dataset
url = "https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg"
image = Image.open(requests.get(url, stream=True).raw).convert("RGB")

# training
model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
model.config.pad_token_id = processor.tokenizer.pad_token_id
model.config.vocab_size = model.config.decoder.vocab_size

pixel_values = processor(image, return_tensors="pt").pixel_values
text = "hello world"
labels = processor.tokenizer(text, return_tensors="pt").input_ids
outputs = model(pixel_values=pixel_values, labels=labels)
loss = outputs.loss

# inference (generation)
generated_ids = model.generate(pixel_values)
generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrOCRProcessor, VisionEncoderDecoderModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = TrOCRProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/trocr-base-handwritten&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;microsoft/trocr-base-handwritten&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load image from the IAM dataset</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw).convert(<span class="hljs-string">&quot;RGB&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># training</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = processor.tokenizer.pad_token_id
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.vocab_size = model.config.decoder.vocab_size

<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values
<span class="hljs-meta">&gt;&gt;&gt; </span>text = <span class="hljs-string">&quot;hello world&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = processor.tokenizer(text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(pixel_values=pixel_values, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># inference (generation)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(pixel_values)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]`}}),Be=new B({props:{name:"from_encoder_decoder_pretrained",anchor:"transformers.VisionEncoderDecoderModel.from_encoder_decoder_pretrained",parameters:[{name:"encoder_pretrained_model_name_or_path",val:": str = None"},{name:"decoder_pretrained_model_name_or_path",val:": str = None"},{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py#L245",parametersDescription:[{anchor:"transformers.VisionEncoderDecoderModel.from_encoder_decoder_pretrained.encoder_pretrained_model_name_or_path",description:`<strong>encoder_pretrained_model_name_or_path</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Information necessary to initiate the image encoder. Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co. An
example is <code>google/vit-base-patch16-224-in21k</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.18.0/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"encoder_pretrained_model_name_or_path"},{anchor:"transformers.VisionEncoderDecoderModel.from_encoder_decoder_pretrained.decoder_pretrained_model_name_or_path",description:`<strong>decoder_pretrained_model_name_or_path</strong> (<code>str</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Information necessary to initiate the text decoder. Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.18.0/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"decoder_pretrained_model_name_or_path"},{anchor:"transformers.VisionEncoderDecoderModel.from_encoder_decoder_pretrained.model_args",description:`<strong>model_args</strong> (remaining positional arguments, <em>optional</em>) &#x2014;
All remaning positional arguments will be passed to the underlying model&#x2019;s <code>__init__</code> method.`,name:"model_args"},{anchor:"transformers.VisionEncoderDecoderModel.from_encoder_decoder_pretrained.kwargs",description:`<strong>kwargs</strong> (remaining dictionary of keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>).</p>
<ul>
<li>To update the encoder configuration, use the prefix <em>encoder_</em> for each configuration parameter.</li>
<li>To update the decoder configuration, use the prefix <em>decoder_</em> for each configuration parameter.</li>
<li>To update the parent model configuration, do not use a prefix for each configuration parameter.</li>
</ul>
<p>Behaves differently depending on whether a <code>config</code> is provided or automatically loaded.`,name:"kwargs"}]}}),Ge=new wo({props:{code:`from transformers import VisionEncoderDecoderModel

# initialize a vit-bert from a pretrained ViT and a pretrained BERT model. Note that the cross-attention layers will be randomly initialized
model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(
    "google/vit-base-patch16-224-in21k", "bert-base-uncased"
)
# saving model after fine-tuning
model.save_pretrained("./vit-bert")
# load fine-tuned model
model = VisionEncoderDecoderModel.from_pretrained("./vit-bert")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> VisionEncoderDecoderModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a vit-bert from a pretrained ViT and a pretrained BERT model. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>, <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saving model after fine-tuning</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;./vit-bert&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load fine-tuned model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;./vit-bert&quot;</span>)`}}),We=new Yt({}),Ue=new B({props:{name:"class transformers.TFVisionEncoderDecoderModel",anchor:"transformers.TFVisionEncoderDecoderModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py#L176",parametersDescription:[{anchor:"transformers.TFVisionEncoderDecoderModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.18.0/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.18.0/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),eo=new B({props:{name:"call",anchor:"transformers.TFVisionEncoderDecoderModel.call",parameters:[{name:"pixel_values",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"encoder_outputs",val:" = None"},{name:"past_key_values",val:" = None"},{name:"decoder_inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py#L513",parametersDescription:[{anchor:"transformers.TFVisionEncoderDecoderModel.call.pixel_values",description:`<strong>pixel_values</strong> (<code>np.ndarray</code>, <code>tf.Tensor</code>, <code>List[tf.Tensor]</code> \`<code>Dict[str, tf.Tensor]</code> or <code>Dict[str, np.ndarray]</code> and each example must have the shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using the vision&#x2019;s model&#x2019;s feature extractor. For example, using
<a href="/docs/transformers/v4.18.0/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a>. See <a href="/docs/transformers/v4.18.0/en/model_doc/vit#transformers.ViTFeatureExtractor.__call__">ViTFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.18.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See <a href="/docs/transformers/v4.18.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.18.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>Provide for sequence to sequence training to the decoder. Indices can be obtained using
<a href="/docs/transformers/v4.18.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See <a href="/docs/transformers/v4.18.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.18.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for
details.`,name:"decoder_input_ids"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(tuple(tf.Tensor)</code>, <em>optional</em>) &#x2014;
This tuple must consist of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) is a tensor of hidden-states at the output
of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(tf.Tensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.decoder_inputs_embeds",description:`<strong>decoder_inputs_embeds</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, target_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>decoder_input_ids</code> you can choose to directly pass an embedded
representation. This is useful if you want more control over how to convert <code>decoder_input_ids</code> indices
into associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"decoder_inputs_embeds"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.labels",description:`<strong>labels</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss for the decoder. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the model will return a <code>Seq2SeqLMOutput</code> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).
kwargs &#x2014; (<em>optional</em>) Remaining dictionary of keyword arguments. Keyword arguments come in two flavors:</p>
<ul>
<li>Without a prefix which will be input as <code>**encoder_kwargs</code> for the encoder forward function.</li>
<li>With a <em>decoder_</em> prefix which will be input as <code>**decoder_kwargs</code> for the decoder forward function.</li>
</ul>`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.18.0/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput"
>transformers.modeling_tf_outputs.TFSeq2SeqLMOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/v4.18.0/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"
>VisionEncoderDecoderConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.18.0/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput"
>transformers.modeling_tf_outputs.TFSeq2SeqLMOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Te=new Dd({props:{$$slots:{default:[lc]},$$scope:{ctx:pe}}}),oo=new wo({props:{code:`from transformers import AutoFeatureExtractor, AutoTokenizer, TFVisionEncoderDecoderModel
from PIL import Image
import requests

feature_extractor = AutoFeatureExtractor.from_pretrained("google/vit-base-patch16-224-in21k")
decoder_tokenizer = AutoTokenizer.from_pretrained("gpt2")

# initialize a bert2gpt2 from a pretrained BERT and GPT2 models. Note that the cross-attention layers will be randomly initialized
model = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
    "google/vit-base-patch16-224-in21k", "gpt2"
)

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
img = Image.open(requests.get(url, stream=True).raw)

# forward
pixel_values = feature_extractor(images=img, return_tensors="tf").pixel_values  # Batch size 1
decoder_input_ids = decoder_tokenizer("Linda Davis", return_tensors="tf").input_ids  # Batch size 1
outputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)

# training
outputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids, labels=decoder_input_ids)
loss, logits = outputs.loss, outputs.logits

# save and load from pretrained
model.save_pretrained("vit-gpt2")
model = TFVisionEncoderDecoderModel.from_pretrained("vit-gpt2")

# generation
generated = model.generate(pixel_values, decoder_start_token_id=model.config.decoder.bos_token_id)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor, AutoTokenizer, TFVisionEncoderDecoderModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a bert2gpt2 from a pretrained BERT and GPT2 models. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>, <span class="hljs-string">&quot;gpt2&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># forward</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = feature_extractor(images=img, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>).pixel_values  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = decoder_tokenizer(<span class="hljs-string">&quot;Linda Davis&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># training</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids, labels=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss, logits = outputs.loss, outputs.logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># save and load from pretrained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;vit-gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFVisionEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;vit-gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># generation</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generated = model.generate(pixel_values, decoder_start_token_id=model.config.decoder.bos_token_id)`}}),to=new B({props:{name:"from_encoder_decoder_pretrained",anchor:"transformers.TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained",parameters:[{name:"encoder_pretrained_model_name_or_path",val:": str = None"},{name:"decoder_pretrained_model_name_or_path",val:": str = None"},{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py#L348",parametersDescription:[{anchor:"transformers.TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained.encoder_pretrained_model_name_or_path",description:`<strong>encoder_pretrained_model_name_or_path</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Information necessary to initiate the encoder. Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co. An
example is <code>google/vit-base-patch16-224-in21k</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.18.0/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>pytorch index checkpoint file</em> (e.g, <code>./pt_model/</code>). In this case,
<code>encoder_from_pt</code> should be set to <code>True</code>.</li>
</ul>`,name:"encoder_pretrained_model_name_or_path"},{anchor:"transformers.TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained.decoder_pretrained_model_name_or_path",description:`<strong>decoder_pretrained_model_name_or_path</strong> (<code>str</code>, <em>optional</em>, defaults to <em>None</em>) &#x2014;
Information necessary to initiate the decoder. Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.18.0/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>pytorch checkpoint file</em> (e.g, <code>./pt_model/</code>). In this case,
<code>decoder_from_pt</code> should be set to <code>True</code>.</li>
</ul>`,name:"decoder_pretrained_model_name_or_path"},{anchor:"transformers.TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained.model_args",description:`<strong>model_args</strong> (remaining positional arguments, <em>optional</em>) &#x2014;
All remaning positional arguments will be passed to the underlying model&#x2019;s <code>__init__</code> method.`,name:"model_args"},{anchor:"transformers.TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained.kwargs",description:`<strong>kwargs</strong> (remaining dictionary of keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>).</p>
<ul>
<li>To update the encoder configuration, use the prefix <em>encoder_</em> for each configuration parameter.</li>
<li>To update the decoder configuration, use the prefix <em>decoder_</em> for each configuration parameter.</li>
<li>To update the parent model configuration, do not use a prefix for each configuration parameter.</li>
</ul>
<p>Behaves differently depending on whether a <code>config</code> is provided or automatically loaded.`,name:"kwargs"}]}}),no=new wo({props:{code:`from transformers import TFVisionEncoderDecoderModel

# initialize a vit-bert from a pretrained ViT and a pretrained BERT model. Note that the cross-attention layers will be randomly initialized
model = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
    "google/vit-base-patch16-224-in21k", "bert-base-uncased"
)
# saving model after fine-tuning
model.save_pretrained("./vit-bert")
# load fine-tuned model
model = TFVisionEncoderDecoderModel.from_pretrained("./vit-bert")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFVisionEncoderDecoderModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a vit-bert from a pretrained ViT and a pretrained BERT model. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>, <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saving model after fine-tuning</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;./vit-bert&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load fine-tuned model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFVisionEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;./vit-bert&quot;</span>)`}}),ro=new Yt({}),ao=new B({props:{name:"class transformers.FlaxVisionEncoderDecoderModel",anchor:"transformers.FlaxVisionEncoderDecoderModel",parameters:[{name:"config",val:": VisionEncoderDecoderConfig"},{name:"input_shape",val:": typing.Optional[typing.Tuple] = None"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py#L267",parametersDescription:[{anchor:"transformers.FlaxVisionEncoderDecoderModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.18.0/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.18.0/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on GPUs) and
<code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see <a href="/docs/transformers/v4.18.0/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and
<a href="/docs/transformers/v4.18.0/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}]}}),fo=new B({props:{name:"__call__",anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__",parameters:[{name:"pixel_values",val:": ndarray"},{name:"decoder_input_ids",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"},{name:"decoder_attention_mask",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"},{name:"decoder_position_ids",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"train",val:": bool = False"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py#L582",parametersDescription:[{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.pixel_values",description:`<strong>pixel_values</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using the vision model&#x2019;s feature extractor. For example, using
<a href="/docs/transformers/v4.18.0/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a>. See <a href="/docs/transformers/v4.18.0/en/model_doc/vit#transformers.ViTFeatureExtractor.__call__">ViTFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.18.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See <a href="/docs/transformers/v4.18.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.18.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a>`,name:"decoder_input_ids"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.decoder_position_ids",description:`<strong>decoder_position_ids</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the
range <code>[0, config.decoder.max_position_embeddings - 1]</code>.`,name:"decoder_position_ids"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the model will return a <code>FlaxSeq2SeqLMOutput</code> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.18.0/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput"
>transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.18.0/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"
>VisionEncoderDecoderConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(jnp.ndarray)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.18.0/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput"
>transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),we=new Dd({props:{$$slots:{default:[pc]},$$scope:{ctx:pe}}}),uo=new wo({props:{code:`from transformers import FlaxVisionEncoderDecoderModel, ViTFeatureExtractor, GPT2Tokenizer
from PIL import Image
import requests

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

feature_extractor = ViTFeatureExtractor.from_pretrained("google/vit-base-patch16-224-in21k")

# load output tokenizer
tokenizer_output = GPT2Tokenizer.from_pretrained("gpt2")

# initialize a vit-gpt2 from pretrained ViT and GPT2 models. Note that the cross-attention layers will be randomly initialized
model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
    "google/vit-base-patch16-224-in21k", "gpt2"
)

pixel_values = feature_extractor(images=image, return_tensors="np").pixel_values

# use GPT2's eos_token as the pad as well as eos token
model.config.eos_token_id = model.config.decoder.eos_token_id
model.config.pad_token_id = model.config.eos_token_id

# generation
sequences = model.generate(pixel_values, num_beams=4, max_length=12).sequences

captions = tokenizer_output.batch_decode(sequences, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxVisionEncoderDecoderModel, ViTFeatureExtractor, GPT2Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = ViTFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load output tokenizer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_output = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a vit-gpt2 from pretrained ViT and GPT2 models. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>, <span class="hljs-string">&quot;gpt2&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = feature_extractor(images=image, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).pixel_values

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># use GPT2&#x27;s eos_token as the pad as well as eos token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.eos_token_id = model.config.decoder.eos_token_id
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = model.config.eos_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># generation</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>sequences = model.generate(pixel_values, num_beams=<span class="hljs-number">4</span>, max_length=<span class="hljs-number">12</span>).sequences

<span class="hljs-meta">&gt;&gt;&gt; </span>captions = tokenizer_output.batch_decode(sequences, skip_special_tokens=<span class="hljs-literal">True</span>)`}}),go=new B({props:{name:"from_encoder_decoder_pretrained",anchor:"transformers.FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained",parameters:[{name:"encoder_pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike, NoneType] = None"},{name:"decoder_pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike, NoneType] = None"},{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py#L707",parametersDescription:[{anchor:"transformers.FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained.encoder_pretrained_model_name_or_path",description:`<strong>encoder_pretrained_model_name_or_path</strong> (<code>Union[str, os.PathLike]</code>, <em>optional</em>) &#x2014;
Information necessary to initiate the encoder. Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co. An
example is <code>google/vit-base-patch16-224-in21k</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.18.0/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"encoder_pretrained_model_name_or_path"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained.decoder_pretrained_model_name_or_path",description:`<strong>decoder_pretrained_model_name_or_path</strong> (<code>Union[str, os.PathLike]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Information necessary to initiate the decoder. Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.18.0/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"decoder_pretrained_model_name_or_path"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained.model_args",description:`<strong>model_args</strong> (remaining positional arguments, <em>optional</em>) &#x2014;
All remaning positional arguments will be passed to the underlying model&#x2019;s <code>__init__</code> method.`,name:"model_args"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained.kwargs",description:`<strong>kwargs</strong> (remaining dictionary of keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>).</p>
<ul>
<li>To update the encoder configuration, use the prefix <em>encoder_</em> for each configuration parameter.</li>
<li>To update the decoder configuration, use the prefix <em>decoder_</em> for each configuration parameter.</li>
<li>To update the parent model configuration, do not use a prefix for each configuration parameter.</li>
</ul>
<p>Behaves differently depending on whether a <code>config</code> is provided or automatically loaded.`,name:"kwargs"}]}}),_o=new wo({props:{code:`from transformers import FlaxVisionEncoderDecoderModel

# initialize a vit-gpt2 from a pretrained ViT and a pretrained GPT2 model. Note that the cross-attention layers will be randomly initialized
model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
    "google/vit-base-patch16-224-in21k", "gpt2"
)
# saving model after fine-tuning
model.save_pretrained("./vit-gpt2")
# load fine-tuned model
model = FlaxVisionEncoderDecoderModel.from_pretrained("./vit-gpt2")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxVisionEncoderDecoderModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a vit-gpt2 from a pretrained ViT and a pretrained GPT2 model. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>, <span class="hljs-string">&quot;gpt2&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saving model after fine-tuning</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;./vit-gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load fine-tuned model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxVisionEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;./vit-gpt2&quot;</span>)`}}),{c(){p=r("meta"),P=c(),f=r("h1"),V=r("a"),z=r("span"),u(w.$$.fragment),j=c(),C=r("span"),Dn=t("Vision Encoder Decoder Models"),Jt=c(),m=r("p"),qn=t("The "),Eo=r("a"),$n=t("VisionEncoderDecoderModel"),Fn=t(` can be used to initialize an image-to-text-sequence model with any
pretrained Transformer-based vision autoencoding model as the encoder (`),ht=r("em"),zn=t("e.g."),Pn=c(),ko=r("a"),Cn=t("ViT"),An=t(", "),xo=r("a"),Sn=t("BEiT"),In=t(", "),jo=r("a"),Ln=t("DeiT"),Nn=t(", "),Mo=r("a"),On=t("Swin"),Rn=t(`)
and any pretrained language model as the decoder (`),mt=r("em"),Bn=t("e.g."),Gn=c(),Vo=r("a"),Wn=t("RoBERTa"),Un=t(", "),Do=r("a"),Hn=t("GPT2"),Zn=t(", "),qo=r("a"),Yn=t("BERT"),Jn=t(", "),$o=r("a"),Kn=t("DistilBERT"),Qn=t(")."),Kt=c(),he=r("p"),Xn=t(`The effectiveness of initializing image-to-text-sequence models with pretrained checkpoints has been shown in (for
example) `),xe=r("a"),er=t("TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models"),or=t(` by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang,
Zhoujun Li, Furu Wei.`),Qt=c(),G=r("p"),tr=t("An example of how to use a "),Fo=r("a"),nr=t("VisionEncoderDecoderModel"),rr=t(" for inference can be seen in "),zo=r("a"),ar=t("TrOCR"),sr=t("."),Xt=c(),Q=r("h2"),me=r("a"),ft=r("span"),u(je.$$.fragment),dr=c(),ut=r("span"),ir=t("VisionEncoderDecoderConfig"),en=c(),F=r("div"),u(Me.$$.fragment),cr=c(),fe=r("p"),Po=r("a"),lr=t("VisionEncoderDecoderConfig"),pr=t(` is the configuration class to store the configuration of a
`),Co=r("a"),hr=t("VisionEncoderDecoderModel"),mr=t(`. It is used to instantiate a Vision-Encoder-Text-Decoder model according to the
specified arguments, defining the encoder and decoder configs.`),fr=c(),X=r("p"),ur=t("Configuration objects inherit from "),Ao=r("a"),gr=t("PretrainedConfig"),_r=t(` and can be used to control the model outputs. Read the
documentation from `),So=r("a"),vr=t("PretrainedConfig"),br=t(" for more information."),Tr=c(),gt=r("p"),yr=t("Examples:"),wr=c(),u(Ve.$$.fragment),Er=c(),ue=r("div"),u(De.$$.fragment),kr=c(),qe=r("p"),xr=t("Instantiate a "),Io=r("a"),jr=t("VisionEncoderDecoderConfig"),Mr=t(` (or a derived class) from a pre-trained encoder model
configuration and decoder model configuration.`),Vr=c(),ge=r("div"),u($e.$$.fragment),Dr=c(),ee=r("p"),qr=t("Serializes this instance to a Python dictionary. Override the default "),_t=r("em"),$r=t("to_dict()"),Fr=t(" from "),vt=r("em"),zr=t("PretrainedConfig"),Pr=t("."),on=c(),oe=r("h2"),_e=r("a"),bt=r("span"),u(Fe.$$.fragment),Cr=c(),Tt=r("span"),Ar=t("VisionEncoderDecoderModel"),tn=c(),E=r("div"),u(ze.$$.fragment),Sr=c(),te=r("p"),Ir=t(`This class can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model
as the encoder and any pretrained text autoregressive model as the decoder. The encoder is loaded via
`),Lo=r("a"),Lr=t("from_pretrained()"),Nr=t(" function and the decoder is loaded via "),No=r("a"),Or=t("from_pretrained()"),Rr=t(`
function. Cross-attention layers are automatically added to the decoder and should be fine-tuned on a downstream
generative task, like image captioning.`),Br=c(),Pe=r("p"),Gr=t(`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),Ce=r("a"),Wr=t(`Leveraging Pre-trained Checkpoints for Sequence Generation
Tasks`),Ur=t(` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),Hr=c(),Ae=r("p"),Zr=t("Additionally, in "),Se=r("a"),Yr=t(`TrOCR: Transformer-based Optical Character Recognition with Pre-trained
Models`),Jr=t(` it is shown how leveraging large pretrained vision models for optical
character recognition (OCR) yields a significant performance improvement.`),Kr=c(),yt=r("p"),Qr=t(`After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned, it can be saved/loaded just like any
other models (see the examples for more information).`),Xr=c(),Ie=r("p"),ea=t("This model inherits from "),Oo=r("a"),oa=t("PreTrainedModel"),ta=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),na=c(),Le=r("p"),ra=t("This model is also a PyTorch "),Ne=r("a"),aa=t("torch.nn.Module"),sa=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),da=c(),W=r("p"),Ro=r("a"),ia=t("VisionEncoderDecoderModel"),ca=t(` is a generic model class that will be instantiated as a transformer architecture with
one of the base vision model classes of the library as encoder and another one as decoder when created with the
:meth`),wt=r("em"),la=t("~transformers.AutoModel.from_pretrained"),pa=t(` class method for the encoder and
:meth`),Et=r("em"),ha=t("~transformers.AutoModelForCausalLM.from_pretrained"),ma=t(" class method for the decoder."),fa=c(),A=r("div"),u(Oe.$$.fragment),ua=c(),ne=r("p"),ga=t("The "),Bo=r("a"),_a=t("VisionEncoderDecoderModel"),va=t(" forward method, overrides the "),kt=r("code"),ba=t("__call__"),Ta=t(" special method."),ya=c(),u(ve.$$.fragment),wa=c(),xt=r("p"),Ea=t("Examples:"),ka=c(),u(Re.$$.fragment),xa=c(),S=r("div"),u(Be.$$.fragment),ja=c(),jt=r("p"),Ma=t(`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),Va=c(),re=r("p"),Da=t("The model is set in evaluation mode by default using "),Mt=r("code"),qa=t("model.eval()"),$a=t(` (Dropout modules are deactivated). To train
the model, you need to first set it back in training mode with `),Vt=r("code"),Fa=t("model.train()"),za=t("."),Pa=c(),Dt=r("p"),Ca=t("Example:"),Aa=c(),u(Ge.$$.fragment),nn=c(),ae=r("h2"),be=r("a"),qt=r("span"),u(We.$$.fragment),Sa=c(),$t=r("span"),Ia=t("TFVisionEncoderDecoderModel"),rn=c(),k=r("div"),u(Ue.$$.fragment),La=c(),se=r("p"),Na=t(`This class can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model
as the encoder and any pretrained text autoregressive model as the decoder. The encoder is loaded via
`),Go=r("a"),Oa=t("from_pretrained()"),Ra=t(" function and the decoder is loaded via "),Wo=r("a"),Ba=t("from_pretrained()"),Ga=t(`
function. Cross-attention layers are automatically added to the decoder and should be fine-tuned on a downstream
generative task, like image captioning.`),Wa=c(),He=r("p"),Ua=t(`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),Ze=r("a"),Ha=t(`Leveraging Pre-trained Checkpoints for Sequence Generation
Tasks`),Za=t(` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),Ya=c(),Ye=r("p"),Ja=t("Additionally, in "),Je=r("a"),Ka=t(`TrOCR: Transformer-based Optical Character Recognition with Pre-trained
Models`),Qa=t(` it is shown how leveraging large pretrained vision models for optical
character recognition (OCR) yields a significant performance improvement.`),Xa=c(),Ft=r("p"),es=t(`After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned, it can be saved/loaded just like any
other models (see the examples for more information).`),os=c(),Ke=r("p"),ts=t("This model inherits from "),Uo=r("a"),ns=t("TFPreTrainedModel"),rs=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),as=c(),Qe=r("p"),ss=t("This model is also a "),Xe=r("a"),ds=t("tf.keras.Model"),is=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),cs=c(),U=r("p"),Ho=r("a"),ls=t("TFVisionEncoderDecoderModel"),ps=t(` is a generic model class that will be instantiated as a transformer architecture
with one of the base vision model classes of the library as encoder and another one of the base model classes as
decoder when created with the `),Zo=r("a"),hs=t("from_pretrained()"),ms=t(` class method for the encoder and
`),Yo=r("a"),fs=t("from_pretrained()"),us=t(" class method for the decoder."),gs=c(),I=r("div"),u(eo.$$.fragment),_s=c(),de=r("p"),vs=t("The "),Jo=r("a"),bs=t("TFVisionEncoderDecoderModel"),Ts=t(" forward method, overrides the "),zt=r("code"),ys=t("__call__"),ws=t(" special method."),Es=c(),u(Te.$$.fragment),ks=c(),Pt=r("p"),xs=t("Examples:"),js=c(),u(oo.$$.fragment),Ms=c(),O=r("div"),u(to.$$.fragment),Vs=c(),Ct=r("p"),Ds=t(`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),qs=c(),At=r("p"),$s=t("Example:"),Fs=c(),u(no.$$.fragment),an=c(),ie=r("h2"),ye=r("a"),St=r("span"),u(ro.$$.fragment),zs=c(),It=r("span"),Ps=t("FlaxVisionEncoderDecoderModel"),sn=c(),x=r("div"),u(ao.$$.fragment),Cs=c(),ce=r("p"),As=t(`This class can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model
as the encoder and any pretrained text autoregressive model as the decoder. The encoder is loaded via
`),Ko=r("a"),Ss=t("from_pretrained()"),Is=t(" function and the decoder is loaded via "),Qo=r("a"),Ls=t("from_pretrained()"),Ns=t(`
function. Cross-attention layers are automatically added to the decoder and should be fine-tuned on a downstream
generative task, like image captioning.`),Os=c(),so=r("p"),Rs=t(`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),io=r("a"),Bs=t(`Leveraging Pre-trained Checkpoints for Sequence Generation
Tasks`),Gs=t(` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),Ws=c(),co=r("p"),Us=t("Additionally, in "),lo=r("a"),Hs=t(`TrOCR: Transformer-based Optical Character Recognition with Pre-trained
Models`),Zs=t(` it is shown how leveraging large pretrained vision models for optical
character recognition (OCR) yields a significant performance improvement.`),Ys=c(),Lt=r("p"),Js=t(`After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned, it can be saved/loaded just like any
other models (see the examples for more information).`),Ks=c(),po=r("p"),Qs=t("This model inherits from "),Xo=r("a"),Xs=t("FlaxPreTrainedModel"),ed=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),od=c(),ho=r("p"),td=t(`This model is also a Flax Linen
`),mo=r("a"),nd=t("flax.nn.Module"),rd=t(` subclass. Use it as a
regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.`),ad=c(),H=r("p"),et=r("a"),sd=t("FlaxVisionEncoderDecoderModel"),dd=t(` is a generic model class that will be instantiated as a transformer architecture
with the module (flax.nn.Module) of one of the base vision model classes of the library as encoder module and
another one as decoder module when created with the :meth`),Nt=r("em"),id=t("~transformers.FlaxAutoModel.from_pretrained"),cd=t(` class method
for the encoder and :meth`),Ot=r("em"),ld=t("~transformers.FlaxAutoModelForCausalLM.from_pretrained"),pd=t(" class method for the decoder."),hd=c(),L=r("div"),u(fo.$$.fragment),md=c(),le=r("p"),fd=t("The "),ot=r("a"),ud=t("FlaxVisionEncoderDecoderModel"),gd=t(" forward method, overrides the "),Rt=r("code"),_d=t("__call__"),vd=t(" special method."),bd=c(),u(we.$$.fragment),Td=c(),Bt=r("p"),yd=t("Examples:"),wd=c(),u(uo.$$.fragment),Ed=c(),R=r("div"),u(go.$$.fragment),kd=c(),Gt=r("p"),xd=t(`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),jd=c(),Wt=r("p"),Md=t("Example:"),Vd=c(),u(_o.$$.fragment),this.h()},l(d){const h=dc('[data-svelte="svelte-1phssyn"]',document.head);p=a(h,"META",{name:!0,content:!0}),h.forEach(o),P=l(d),f=a(d,"H1",{class:!0});var vo=s(f);V=a(vo,"A",{id:!0,class:!0,href:!0});var Ut=s(V);z=a(Ut,"SPAN",{});var Ht=s(z);g(w.$$.fragment,Ht),Ht.forEach(o),Ut.forEach(o),j=l(vo),C=a(vo,"SPAN",{});var qd=s(C);Dn=n(qd,"Vision Encoder Decoder Models"),qd.forEach(o),vo.forEach(o),Jt=l(d),m=a(d,"P",{});var M=s(m);qn=n(M,"The "),Eo=a(M,"A",{href:!0});var $d=s(Eo);$n=n($d,"VisionEncoderDecoderModel"),$d.forEach(o),Fn=n(M,` can be used to initialize an image-to-text-sequence model with any
pretrained Transformer-based vision autoencoding model as the encoder (`),ht=a(M,"EM",{});var Fd=s(ht);zn=n(Fd,"e.g."),Fd.forEach(o),Pn=l(M),ko=a(M,"A",{href:!0});var zd=s(ko);Cn=n(zd,"ViT"),zd.forEach(o),An=n(M,", "),xo=a(M,"A",{href:!0});var Pd=s(xo);Sn=n(Pd,"BEiT"),Pd.forEach(o),In=n(M,", "),jo=a(M,"A",{href:!0});var Cd=s(jo);Ln=n(Cd,"DeiT"),Cd.forEach(o),Nn=n(M,", "),Mo=a(M,"A",{href:!0});var Ad=s(Mo);On=n(Ad,"Swin"),Ad.forEach(o),Rn=n(M,`)
and any pretrained language model as the decoder (`),mt=a(M,"EM",{});var Sd=s(mt);Bn=n(Sd,"e.g."),Sd.forEach(o),Gn=l(M),Vo=a(M,"A",{href:!0});var Id=s(Vo);Wn=n(Id,"RoBERTa"),Id.forEach(o),Un=n(M,", "),Do=a(M,"A",{href:!0});var Ld=s(Do);Hn=n(Ld,"GPT2"),Ld.forEach(o),Zn=n(M,", "),qo=a(M,"A",{href:!0});var Nd=s(qo);Yn=n(Nd,"BERT"),Nd.forEach(o),Jn=n(M,", "),$o=a(M,"A",{href:!0});var Od=s($o);Kn=n(Od,"DistilBERT"),Od.forEach(o),Qn=n(M,")."),M.forEach(o),Kt=l(d),he=a(d,"P",{});var cn=s(he);Xn=n(cn,`The effectiveness of initializing image-to-text-sequence models with pretrained checkpoints has been shown in (for
example) `),xe=a(cn,"A",{href:!0,rel:!0});var Rd=s(xe);er=n(Rd,"TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models"),Rd.forEach(o),or=n(cn,` by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang,
Zhoujun Li, Furu Wei.`),cn.forEach(o),Qt=l(d),G=a(d,"P",{});var tt=s(G);tr=n(tt,"An example of how to use a "),Fo=a(tt,"A",{href:!0});var Bd=s(Fo);nr=n(Bd,"VisionEncoderDecoderModel"),Bd.forEach(o),rr=n(tt," for inference can be seen in "),zo=a(tt,"A",{href:!0});var Gd=s(zo);ar=n(Gd,"TrOCR"),Gd.forEach(o),sr=n(tt,"."),tt.forEach(o),Xt=l(d),Q=a(d,"H2",{class:!0});var ln=s(Q);me=a(ln,"A",{id:!0,class:!0,href:!0});var Wd=s(me);ft=a(Wd,"SPAN",{});var Ud=s(ft);g(je.$$.fragment,Ud),Ud.forEach(o),Wd.forEach(o),dr=l(ln),ut=a(ln,"SPAN",{});var Hd=s(ut);ir=n(Hd,"VisionEncoderDecoderConfig"),Hd.forEach(o),ln.forEach(o),en=l(d),F=a(d,"DIV",{class:!0});var N=s(F);g(Me.$$.fragment,N),cr=l(N),fe=a(N,"P",{});var Zt=s(fe);Po=a(Zt,"A",{href:!0});var Zd=s(Po);lr=n(Zd,"VisionEncoderDecoderConfig"),Zd.forEach(o),pr=n(Zt,` is the configuration class to store the configuration of a
`),Co=a(Zt,"A",{href:!0});var Yd=s(Co);hr=n(Yd,"VisionEncoderDecoderModel"),Yd.forEach(o),mr=n(Zt,`. It is used to instantiate a Vision-Encoder-Text-Decoder model according to the
specified arguments, defining the encoder and decoder configs.`),Zt.forEach(o),fr=l(N),X=a(N,"P",{});var nt=s(X);ur=n(nt,"Configuration objects inherit from "),Ao=a(nt,"A",{href:!0});var Jd=s(Ao);gr=n(Jd,"PretrainedConfig"),Jd.forEach(o),_r=n(nt,` and can be used to control the model outputs. Read the
documentation from `),So=a(nt,"A",{href:!0});var Kd=s(So);vr=n(Kd,"PretrainedConfig"),Kd.forEach(o),br=n(nt," for more information."),nt.forEach(o),Tr=l(N),gt=a(N,"P",{});var Qd=s(gt);yr=n(Qd,"Examples:"),Qd.forEach(o),wr=l(N),g(Ve.$$.fragment,N),Er=l(N),ue=a(N,"DIV",{class:!0});var pn=s(ue);g(De.$$.fragment,pn),kr=l(pn),qe=a(pn,"P",{});var hn=s(qe);xr=n(hn,"Instantiate a "),Io=a(hn,"A",{href:!0});var Xd=s(Io);jr=n(Xd,"VisionEncoderDecoderConfig"),Xd.forEach(o),Mr=n(hn,` (or a derived class) from a pre-trained encoder model
configuration and decoder model configuration.`),hn.forEach(o),pn.forEach(o),Vr=l(N),ge=a(N,"DIV",{class:!0});var mn=s(ge);g($e.$$.fragment,mn),Dr=l(mn),ee=a(mn,"P",{});var rt=s(ee);qr=n(rt,"Serializes this instance to a Python dictionary. Override the default "),_t=a(rt,"EM",{});var ei=s(_t);$r=n(ei,"to_dict()"),ei.forEach(o),Fr=n(rt," from "),vt=a(rt,"EM",{});var oi=s(vt);zr=n(oi,"PretrainedConfig"),oi.forEach(o),Pr=n(rt,"."),rt.forEach(o),mn.forEach(o),N.forEach(o),on=l(d),oe=a(d,"H2",{class:!0});var fn=s(oe);_e=a(fn,"A",{id:!0,class:!0,href:!0});var ti=s(_e);bt=a(ti,"SPAN",{});var ni=s(bt);g(Fe.$$.fragment,ni),ni.forEach(o),ti.forEach(o),Cr=l(fn),Tt=a(fn,"SPAN",{});var ri=s(Tt);Ar=n(ri,"VisionEncoderDecoderModel"),ri.forEach(o),fn.forEach(o),tn=l(d),E=a(d,"DIV",{class:!0});var D=s(E);g(ze.$$.fragment,D),Sr=l(D),te=a(D,"P",{});var at=s(te);Ir=n(at,`This class can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model
as the encoder and any pretrained text autoregressive model as the decoder. The encoder is loaded via
`),Lo=a(at,"A",{href:!0});var ai=s(Lo);Lr=n(ai,"from_pretrained()"),ai.forEach(o),Nr=n(at," function and the decoder is loaded via "),No=a(at,"A",{href:!0});var si=s(No);Or=n(si,"from_pretrained()"),si.forEach(o),Rr=n(at,`
function. Cross-attention layers are automatically added to the decoder and should be fine-tuned on a downstream
generative task, like image captioning.`),at.forEach(o),Br=l(D),Pe=a(D,"P",{});var un=s(Pe);Gr=n(un,`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),Ce=a(un,"A",{href:!0,rel:!0});var di=s(Ce);Wr=n(di,`Leveraging Pre-trained Checkpoints for Sequence Generation
Tasks`),di.forEach(o),Ur=n(un,` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),un.forEach(o),Hr=l(D),Ae=a(D,"P",{});var gn=s(Ae);Zr=n(gn,"Additionally, in "),Se=a(gn,"A",{href:!0,rel:!0});var ii=s(Se);Yr=n(ii,`TrOCR: Transformer-based Optical Character Recognition with Pre-trained
Models`),ii.forEach(o),Jr=n(gn,` it is shown how leveraging large pretrained vision models for optical
character recognition (OCR) yields a significant performance improvement.`),gn.forEach(o),Kr=l(D),yt=a(D,"P",{});var ci=s(yt);Qr=n(ci,`After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned, it can be saved/loaded just like any
other models (see the examples for more information).`),ci.forEach(o),Xr=l(D),Ie=a(D,"P",{});var _n=s(Ie);ea=n(_n,"This model inherits from "),Oo=a(_n,"A",{href:!0});var li=s(Oo);oa=n(li,"PreTrainedModel"),li.forEach(o),ta=n(_n,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),_n.forEach(o),na=l(D),Le=a(D,"P",{});var vn=s(Le);ra=n(vn,"This model is also a PyTorch "),Ne=a(vn,"A",{href:!0,rel:!0});var pi=s(Ne);aa=n(pi,"torch.nn.Module"),pi.forEach(o),sa=n(vn,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),vn.forEach(o),da=l(D),W=a(D,"P",{});var bo=s(W);Ro=a(bo,"A",{href:!0});var hi=s(Ro);ia=n(hi,"VisionEncoderDecoderModel"),hi.forEach(o),ca=n(bo,` is a generic model class that will be instantiated as a transformer architecture with
one of the base vision model classes of the library as encoder and another one as decoder when created with the
:meth`),wt=a(bo,"EM",{});var mi=s(wt);la=n(mi,"~transformers.AutoModel.from_pretrained"),mi.forEach(o),pa=n(bo,` class method for the encoder and
:meth`),Et=a(bo,"EM",{});var fi=s(Et);ha=n(fi,"~transformers.AutoModelForCausalLM.from_pretrained"),fi.forEach(o),ma=n(bo," class method for the decoder."),bo.forEach(o),fa=l(D),A=a(D,"DIV",{class:!0});var Z=s(A);g(Oe.$$.fragment,Z),ua=l(Z),ne=a(Z,"P",{});var st=s(ne);ga=n(st,"The "),Bo=a(st,"A",{href:!0});var ui=s(Bo);_a=n(ui,"VisionEncoderDecoderModel"),ui.forEach(o),va=n(st," forward method, overrides the "),kt=a(st,"CODE",{});var gi=s(kt);ba=n(gi,"__call__"),gi.forEach(o),Ta=n(st," special method."),st.forEach(o),ya=l(Z),g(ve.$$.fragment,Z),wa=l(Z),xt=a(Z,"P",{});var _i=s(xt);Ea=n(_i,"Examples:"),_i.forEach(o),ka=l(Z),g(Re.$$.fragment,Z),Z.forEach(o),xa=l(D),S=a(D,"DIV",{class:!0});var Y=s(S);g(Be.$$.fragment,Y),ja=l(Y),jt=a(Y,"P",{});var vi=s(jt);Ma=n(vi,`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),vi.forEach(o),Va=l(Y),re=a(Y,"P",{});var dt=s(re);Da=n(dt,"The model is set in evaluation mode by default using "),Mt=a(dt,"CODE",{});var bi=s(Mt);qa=n(bi,"model.eval()"),bi.forEach(o),$a=n(dt,` (Dropout modules are deactivated). To train
the model, you need to first set it back in training mode with `),Vt=a(dt,"CODE",{});var Ti=s(Vt);Fa=n(Ti,"model.train()"),Ti.forEach(o),za=n(dt,"."),dt.forEach(o),Pa=l(Y),Dt=a(Y,"P",{});var yi=s(Dt);Ca=n(yi,"Example:"),yi.forEach(o),Aa=l(Y),g(Ge.$$.fragment,Y),Y.forEach(o),D.forEach(o),nn=l(d),ae=a(d,"H2",{class:!0});var bn=s(ae);be=a(bn,"A",{id:!0,class:!0,href:!0});var wi=s(be);qt=a(wi,"SPAN",{});var Ei=s(qt);g(We.$$.fragment,Ei),Ei.forEach(o),wi.forEach(o),Sa=l(bn),$t=a(bn,"SPAN",{});var ki=s($t);Ia=n(ki,"TFVisionEncoderDecoderModel"),ki.forEach(o),bn.forEach(o),rn=l(d),k=a(d,"DIV",{class:!0});var q=s(k);g(Ue.$$.fragment,q),La=l(q),se=a(q,"P",{});var it=s(se);Na=n(it,`This class can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model
as the encoder and any pretrained text autoregressive model as the decoder. The encoder is loaded via
`),Go=a(it,"A",{href:!0});var xi=s(Go);Oa=n(xi,"from_pretrained()"),xi.forEach(o),Ra=n(it," function and the decoder is loaded via "),Wo=a(it,"A",{href:!0});var ji=s(Wo);Ba=n(ji,"from_pretrained()"),ji.forEach(o),Ga=n(it,`
function. Cross-attention layers are automatically added to the decoder and should be fine-tuned on a downstream
generative task, like image captioning.`),it.forEach(o),Wa=l(q),He=a(q,"P",{});var Tn=s(He);Ua=n(Tn,`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),Ze=a(Tn,"A",{href:!0,rel:!0});var Mi=s(Ze);Ha=n(Mi,`Leveraging Pre-trained Checkpoints for Sequence Generation
Tasks`),Mi.forEach(o),Za=n(Tn,` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),Tn.forEach(o),Ya=l(q),Ye=a(q,"P",{});var yn=s(Ye);Ja=n(yn,"Additionally, in "),Je=a(yn,"A",{href:!0,rel:!0});var Vi=s(Je);Ka=n(Vi,`TrOCR: Transformer-based Optical Character Recognition with Pre-trained
Models`),Vi.forEach(o),Qa=n(yn,` it is shown how leveraging large pretrained vision models for optical
character recognition (OCR) yields a significant performance improvement.`),yn.forEach(o),Xa=l(q),Ft=a(q,"P",{});var Di=s(Ft);es=n(Di,`After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned, it can be saved/loaded just like any
other models (see the examples for more information).`),Di.forEach(o),os=l(q),Ke=a(q,"P",{});var wn=s(Ke);ts=n(wn,"This model inherits from "),Uo=a(wn,"A",{href:!0});var qi=s(Uo);ns=n(qi,"TFPreTrainedModel"),qi.forEach(o),rs=n(wn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),wn.forEach(o),as=l(q),Qe=a(q,"P",{});var En=s(Qe);ss=n(En,"This model is also a "),Xe=a(En,"A",{href:!0,rel:!0});var $i=s(Xe);ds=n($i,"tf.keras.Model"),$i.forEach(o),is=n(En,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),En.forEach(o),cs=l(q),U=a(q,"P",{});var To=s(U);Ho=a(To,"A",{href:!0});var Fi=s(Ho);ls=n(Fi,"TFVisionEncoderDecoderModel"),Fi.forEach(o),ps=n(To,` is a generic model class that will be instantiated as a transformer architecture
with one of the base vision model classes of the library as encoder and another one of the base model classes as
decoder when created with the `),Zo=a(To,"A",{href:!0});var zi=s(Zo);hs=n(zi,"from_pretrained()"),zi.forEach(o),ms=n(To,` class method for the encoder and
`),Yo=a(To,"A",{href:!0});var Pi=s(Yo);fs=n(Pi,"from_pretrained()"),Pi.forEach(o),us=n(To," class method for the decoder."),To.forEach(o),gs=l(q),I=a(q,"DIV",{class:!0});var J=s(I);g(eo.$$.fragment,J),_s=l(J),de=a(J,"P",{});var ct=s(de);vs=n(ct,"The "),Jo=a(ct,"A",{href:!0});var Ci=s(Jo);bs=n(Ci,"TFVisionEncoderDecoderModel"),Ci.forEach(o),Ts=n(ct," forward method, overrides the "),zt=a(ct,"CODE",{});var Ai=s(zt);ys=n(Ai,"__call__"),Ai.forEach(o),ws=n(ct," special method."),ct.forEach(o),Es=l(J),g(Te.$$.fragment,J),ks=l(J),Pt=a(J,"P",{});var Si=s(Pt);xs=n(Si,"Examples:"),Si.forEach(o),js=l(J),g(oo.$$.fragment,J),J.forEach(o),Ms=l(q),O=a(q,"DIV",{class:!0});var Ee=s(O);g(to.$$.fragment,Ee),Vs=l(Ee),Ct=a(Ee,"P",{});var Ii=s(Ct);Ds=n(Ii,`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),Ii.forEach(o),qs=l(Ee),At=a(Ee,"P",{});var Li=s(At);$s=n(Li,"Example:"),Li.forEach(o),Fs=l(Ee),g(no.$$.fragment,Ee),Ee.forEach(o),q.forEach(o),an=l(d),ie=a(d,"H2",{class:!0});var kn=s(ie);ye=a(kn,"A",{id:!0,class:!0,href:!0});var Ni=s(ye);St=a(Ni,"SPAN",{});var Oi=s(St);g(ro.$$.fragment,Oi),Oi.forEach(o),Ni.forEach(o),zs=l(kn),It=a(kn,"SPAN",{});var Ri=s(It);Ps=n(Ri,"FlaxVisionEncoderDecoderModel"),Ri.forEach(o),kn.forEach(o),sn=l(d),x=a(d,"DIV",{class:!0});var $=s(x);g(ao.$$.fragment,$),Cs=l($),ce=a($,"P",{});var lt=s(ce);As=n(lt,`This class can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model
as the encoder and any pretrained text autoregressive model as the decoder. The encoder is loaded via
`),Ko=a(lt,"A",{href:!0});var Bi=s(Ko);Ss=n(Bi,"from_pretrained()"),Bi.forEach(o),Is=n(lt," function and the decoder is loaded via "),Qo=a(lt,"A",{href:!0});var Gi=s(Qo);Ls=n(Gi,"from_pretrained()"),Gi.forEach(o),Ns=n(lt,`
function. Cross-attention layers are automatically added to the decoder and should be fine-tuned on a downstream
generative task, like image captioning.`),lt.forEach(o),Os=l($),so=a($,"P",{});var xn=s(so);Rs=n(xn,`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),io=a(xn,"A",{href:!0,rel:!0});var Wi=s(io);Bs=n(Wi,`Leveraging Pre-trained Checkpoints for Sequence Generation
Tasks`),Wi.forEach(o),Gs=n(xn,` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),xn.forEach(o),Ws=l($),co=a($,"P",{});var jn=s(co);Us=n(jn,"Additionally, in "),lo=a(jn,"A",{href:!0,rel:!0});var Ui=s(lo);Hs=n(Ui,`TrOCR: Transformer-based Optical Character Recognition with Pre-trained
Models`),Ui.forEach(o),Zs=n(jn,` it is shown how leveraging large pretrained vision models for optical
character recognition (OCR) yields a significant performance improvement.`),jn.forEach(o),Ys=l($),Lt=a($,"P",{});var Hi=s(Lt);Js=n(Hi,`After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned, it can be saved/loaded just like any
other models (see the examples for more information).`),Hi.forEach(o),Ks=l($),po=a($,"P",{});var Mn=s(po);Qs=n(Mn,"This model inherits from "),Xo=a(Mn,"A",{href:!0});var Zi=s(Xo);Xs=n(Zi,"FlaxPreTrainedModel"),Zi.forEach(o),ed=n(Mn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Mn.forEach(o),od=l($),ho=a($,"P",{});var Vn=s(ho);td=n(Vn,`This model is also a Flax Linen
`),mo=a(Vn,"A",{href:!0,rel:!0});var Yi=s(mo);nd=n(Yi,"flax.nn.Module"),Yi.forEach(o),rd=n(Vn,` subclass. Use it as a
regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.`),Vn.forEach(o),ad=l($),H=a($,"P",{});var yo=s(H);et=a(yo,"A",{href:!0});var Ji=s(et);sd=n(Ji,"FlaxVisionEncoderDecoderModel"),Ji.forEach(o),dd=n(yo,` is a generic model class that will be instantiated as a transformer architecture
with the module (flax.nn.Module) of one of the base vision model classes of the library as encoder module and
another one as decoder module when created with the :meth`),Nt=a(yo,"EM",{});var Ki=s(Nt);id=n(Ki,"~transformers.FlaxAutoModel.from_pretrained"),Ki.forEach(o),cd=n(yo,` class method
for the encoder and :meth`),Ot=a(yo,"EM",{});var Qi=s(Ot);ld=n(Qi,"~transformers.FlaxAutoModelForCausalLM.from_pretrained"),Qi.forEach(o),pd=n(yo," class method for the decoder."),yo.forEach(o),hd=l($),L=a($,"DIV",{class:!0});var K=s(L);g(fo.$$.fragment,K),md=l(K),le=a(K,"P",{});var pt=s(le);fd=n(pt,"The "),ot=a(pt,"A",{href:!0});var Xi=s(ot);ud=n(Xi,"FlaxVisionEncoderDecoderModel"),Xi.forEach(o),gd=n(pt," forward method, overrides the "),Rt=a(pt,"CODE",{});var ec=s(Rt);_d=n(ec,"__call__"),ec.forEach(o),vd=n(pt," special method."),pt.forEach(o),bd=l(K),g(we.$$.fragment,K),Td=l(K),Bt=a(K,"P",{});var oc=s(Bt);yd=n(oc,"Examples:"),oc.forEach(o),wd=l(K),g(uo.$$.fragment,K),K.forEach(o),Ed=l($),R=a($,"DIV",{class:!0});var ke=s(R);g(go.$$.fragment,ke),kd=l(ke),Gt=a(ke,"P",{});var tc=s(Gt);xd=n(tc,`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),tc.forEach(o),jd=l(ke),Wt=a(ke,"P",{});var nc=s(Wt);Md=n(nc,"Example:"),nc.forEach(o),Vd=l(ke),g(_o.$$.fragment,ke),ke.forEach(o),$.forEach(o),this.h()},h(){i(p,"name","hf:doc:metadata"),i(p,"content",JSON.stringify(mc)),i(V,"id","vision-encoder-decoder-models"),i(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(V,"href","#vision-encoder-decoder-models"),i(f,"class","relative group"),i(Eo,"href","/docs/transformers/v4.18.0/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),i(ko,"href","vit"),i(xo,"href","beit"),i(jo,"href","deit"),i(Mo,"href","swin"),i(Vo,"href","roberta"),i(Do,"href","gpt2"),i(qo,"href","bert"),i($o,"href","distilbert"),i(xe,"href","https://arxiv.org/abs/2109.10282"),i(xe,"rel","nofollow"),i(Fo,"href","/docs/transformers/v4.18.0/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),i(zo,"href","trocr"),i(me,"id","transformers.VisionEncoderDecoderConfig"),i(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(me,"href","#transformers.VisionEncoderDecoderConfig"),i(Q,"class","relative group"),i(Po,"href","/docs/transformers/v4.18.0/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"),i(Co,"href","/docs/transformers/v4.18.0/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),i(Ao,"href","/docs/transformers/v4.18.0/en/main_classes/configuration#transformers.PretrainedConfig"),i(So,"href","/docs/transformers/v4.18.0/en/main_classes/configuration#transformers.PretrainedConfig"),i(Io,"href","/docs/transformers/v4.18.0/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"),i(ue,"class","docstring"),i(ge,"class","docstring"),i(F,"class","docstring"),i(_e,"id","transformers.VisionEncoderDecoderModel"),i(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(_e,"href","#transformers.VisionEncoderDecoderModel"),i(oe,"class","relative group"),i(Lo,"href","/docs/transformers/v4.18.0/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained"),i(No,"href","/docs/transformers/v4.18.0/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained"),i(Ce,"href","https://arxiv.org/abs/1907.12461"),i(Ce,"rel","nofollow"),i(Se,"href","https://arxiv.org/abs/2109.10282"),i(Se,"rel","nofollow"),i(Oo,"href","/docs/transformers/v4.18.0/en/main_classes/model#transformers.PreTrainedModel"),i(Ne,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(Ne,"rel","nofollow"),i(Ro,"href","/docs/transformers/v4.18.0/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),i(Bo,"href","/docs/transformers/v4.18.0/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),i(A,"class","docstring"),i(S,"class","docstring"),i(E,"class","docstring"),i(be,"id","transformers.TFVisionEncoderDecoderModel"),i(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(be,"href","#transformers.TFVisionEncoderDecoderModel"),i(ae,"class","relative group"),i(Go,"href","/docs/transformers/v4.18.0/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained"),i(Wo,"href","/docs/transformers/v4.18.0/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained"),i(Ze,"href","https://arxiv.org/abs/1907.12461"),i(Ze,"rel","nofollow"),i(Je,"href","https://arxiv.org/abs/2109.10282"),i(Je,"rel","nofollow"),i(Uo,"href","/docs/transformers/v4.18.0/en/main_classes/model#transformers.TFPreTrainedModel"),i(Xe,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),i(Xe,"rel","nofollow"),i(Ho,"href","/docs/transformers/v4.18.0/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel"),i(Zo,"href","/docs/transformers/v4.18.0/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained"),i(Yo,"href","/docs/transformers/v4.18.0/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained"),i(Jo,"href","/docs/transformers/v4.18.0/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel"),i(I,"class","docstring"),i(O,"class","docstring"),i(k,"class","docstring"),i(ye,"id","transformers.FlaxVisionEncoderDecoderModel"),i(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(ye,"href","#transformers.FlaxVisionEncoderDecoderModel"),i(ie,"class","relative group"),i(Ko,"href","/docs/transformers/v4.18.0/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained"),i(Qo,"href","/docs/transformers/v4.18.0/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained"),i(io,"href","https://arxiv.org/abs/1907.12461"),i(io,"rel","nofollow"),i(lo,"href","https://arxiv.org/abs/2109.10282"),i(lo,"rel","nofollow"),i(Xo,"href","/docs/transformers/v4.18.0/en/main_classes/model#transformers.FlaxPreTrainedModel"),i(mo,"href","https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html"),i(mo,"rel","nofollow"),i(et,"href","/docs/transformers/v4.18.0/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel"),i(ot,"href","/docs/transformers/v4.18.0/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel"),i(L,"class","docstring"),i(R,"class","docstring"),i(x,"class","docstring")},m(d,h){e(document.head,p),y(d,P,h),y(d,f,h),e(f,V),e(V,z),_(w,z,null),e(f,j),e(f,C),e(C,Dn),y(d,Jt,h),y(d,m,h),e(m,qn),e(m,Eo),e(Eo,$n),e(m,Fn),e(m,ht),e(ht,zn),e(m,Pn),e(m,ko),e(ko,Cn),e(m,An),e(m,xo),e(xo,Sn),e(m,In),e(m,jo),e(jo,Ln),e(m,Nn),e(m,Mo),e(Mo,On),e(m,Rn),e(m,mt),e(mt,Bn),e(m,Gn),e(m,Vo),e(Vo,Wn),e(m,Un),e(m,Do),e(Do,Hn),e(m,Zn),e(m,qo),e(qo,Yn),e(m,Jn),e(m,$o),e($o,Kn),e(m,Qn),y(d,Kt,h),y(d,he,h),e(he,Xn),e(he,xe),e(xe,er),e(he,or),y(d,Qt,h),y(d,G,h),e(G,tr),e(G,Fo),e(Fo,nr),e(G,rr),e(G,zo),e(zo,ar),e(G,sr),y(d,Xt,h),y(d,Q,h),e(Q,me),e(me,ft),_(je,ft,null),e(Q,dr),e(Q,ut),e(ut,ir),y(d,en,h),y(d,F,h),_(Me,F,null),e(F,cr),e(F,fe),e(fe,Po),e(Po,lr),e(fe,pr),e(fe,Co),e(Co,hr),e(fe,mr),e(F,fr),e(F,X),e(X,ur),e(X,Ao),e(Ao,gr),e(X,_r),e(X,So),e(So,vr),e(X,br),e(F,Tr),e(F,gt),e(gt,yr),e(F,wr),_(Ve,F,null),e(F,Er),e(F,ue),_(De,ue,null),e(ue,kr),e(ue,qe),e(qe,xr),e(qe,Io),e(Io,jr),e(qe,Mr),e(F,Vr),e(F,ge),_($e,ge,null),e(ge,Dr),e(ge,ee),e(ee,qr),e(ee,_t),e(_t,$r),e(ee,Fr),e(ee,vt),e(vt,zr),e(ee,Pr),y(d,on,h),y(d,oe,h),e(oe,_e),e(_e,bt),_(Fe,bt,null),e(oe,Cr),e(oe,Tt),e(Tt,Ar),y(d,tn,h),y(d,E,h),_(ze,E,null),e(E,Sr),e(E,te),e(te,Ir),e(te,Lo),e(Lo,Lr),e(te,Nr),e(te,No),e(No,Or),e(te,Rr),e(E,Br),e(E,Pe),e(Pe,Gr),e(Pe,Ce),e(Ce,Wr),e(Pe,Ur),e(E,Hr),e(E,Ae),e(Ae,Zr),e(Ae,Se),e(Se,Yr),e(Ae,Jr),e(E,Kr),e(E,yt),e(yt,Qr),e(E,Xr),e(E,Ie),e(Ie,ea),e(Ie,Oo),e(Oo,oa),e(Ie,ta),e(E,na),e(E,Le),e(Le,ra),e(Le,Ne),e(Ne,aa),e(Le,sa),e(E,da),e(E,W),e(W,Ro),e(Ro,ia),e(W,ca),e(W,wt),e(wt,la),e(W,pa),e(W,Et),e(Et,ha),e(W,ma),e(E,fa),e(E,A),_(Oe,A,null),e(A,ua),e(A,ne),e(ne,ga),e(ne,Bo),e(Bo,_a),e(ne,va),e(ne,kt),e(kt,ba),e(ne,Ta),e(A,ya),_(ve,A,null),e(A,wa),e(A,xt),e(xt,Ea),e(A,ka),_(Re,A,null),e(E,xa),e(E,S),_(Be,S,null),e(S,ja),e(S,jt),e(jt,Ma),e(S,Va),e(S,re),e(re,Da),e(re,Mt),e(Mt,qa),e(re,$a),e(re,Vt),e(Vt,Fa),e(re,za),e(S,Pa),e(S,Dt),e(Dt,Ca),e(S,Aa),_(Ge,S,null),y(d,nn,h),y(d,ae,h),e(ae,be),e(be,qt),_(We,qt,null),e(ae,Sa),e(ae,$t),e($t,Ia),y(d,rn,h),y(d,k,h),_(Ue,k,null),e(k,La),e(k,se),e(se,Na),e(se,Go),e(Go,Oa),e(se,Ra),e(se,Wo),e(Wo,Ba),e(se,Ga),e(k,Wa),e(k,He),e(He,Ua),e(He,Ze),e(Ze,Ha),e(He,Za),e(k,Ya),e(k,Ye),e(Ye,Ja),e(Ye,Je),e(Je,Ka),e(Ye,Qa),e(k,Xa),e(k,Ft),e(Ft,es),e(k,os),e(k,Ke),e(Ke,ts),e(Ke,Uo),e(Uo,ns),e(Ke,rs),e(k,as),e(k,Qe),e(Qe,ss),e(Qe,Xe),e(Xe,ds),e(Qe,is),e(k,cs),e(k,U),e(U,Ho),e(Ho,ls),e(U,ps),e(U,Zo),e(Zo,hs),e(U,ms),e(U,Yo),e(Yo,fs),e(U,us),e(k,gs),e(k,I),_(eo,I,null),e(I,_s),e(I,de),e(de,vs),e(de,Jo),e(Jo,bs),e(de,Ts),e(de,zt),e(zt,ys),e(de,ws),e(I,Es),_(Te,I,null),e(I,ks),e(I,Pt),e(Pt,xs),e(I,js),_(oo,I,null),e(k,Ms),e(k,O),_(to,O,null),e(O,Vs),e(O,Ct),e(Ct,Ds),e(O,qs),e(O,At),e(At,$s),e(O,Fs),_(no,O,null),y(d,an,h),y(d,ie,h),e(ie,ye),e(ye,St),_(ro,St,null),e(ie,zs),e(ie,It),e(It,Ps),y(d,sn,h),y(d,x,h),_(ao,x,null),e(x,Cs),e(x,ce),e(ce,As),e(ce,Ko),e(Ko,Ss),e(ce,Is),e(ce,Qo),e(Qo,Ls),e(ce,Ns),e(x,Os),e(x,so),e(so,Rs),e(so,io),e(io,Bs),e(so,Gs),e(x,Ws),e(x,co),e(co,Us),e(co,lo),e(lo,Hs),e(co,Zs),e(x,Ys),e(x,Lt),e(Lt,Js),e(x,Ks),e(x,po),e(po,Qs),e(po,Xo),e(Xo,Xs),e(po,ed),e(x,od),e(x,ho),e(ho,td),e(ho,mo),e(mo,nd),e(ho,rd),e(x,ad),e(x,H),e(H,et),e(et,sd),e(H,dd),e(H,Nt),e(Nt,id),e(H,cd),e(H,Ot),e(Ot,ld),e(H,pd),e(x,hd),e(x,L),_(fo,L,null),e(L,md),e(L,le),e(le,fd),e(le,ot),e(ot,ud),e(le,gd),e(le,Rt),e(Rt,_d),e(le,vd),e(L,bd),_(we,L,null),e(L,Td),e(L,Bt),e(Bt,yd),e(L,wd),_(uo,L,null),e(x,Ed),e(x,R),_(go,R,null),e(R,kd),e(R,Gt),e(Gt,xd),e(R,jd),e(R,Wt),e(Wt,Md),e(R,Vd),_(_o,R,null),dn=!0},p(d,[h]){const vo={};h&2&&(vo.$$scope={dirty:h,ctx:d}),ve.$set(vo);const Ut={};h&2&&(Ut.$$scope={dirty:h,ctx:d}),Te.$set(Ut);const Ht={};h&2&&(Ht.$$scope={dirty:h,ctx:d}),we.$set(Ht)},i(d){dn||(v(w.$$.fragment,d),v(je.$$.fragment,d),v(Me.$$.fragment,d),v(Ve.$$.fragment,d),v(De.$$.fragment,d),v($e.$$.fragment,d),v(Fe.$$.fragment,d),v(ze.$$.fragment,d),v(Oe.$$.fragment,d),v(ve.$$.fragment,d),v(Re.$$.fragment,d),v(Be.$$.fragment,d),v(Ge.$$.fragment,d),v(We.$$.fragment,d),v(Ue.$$.fragment,d),v(eo.$$.fragment,d),v(Te.$$.fragment,d),v(oo.$$.fragment,d),v(to.$$.fragment,d),v(no.$$.fragment,d),v(ro.$$.fragment,d),v(ao.$$.fragment,d),v(fo.$$.fragment,d),v(we.$$.fragment,d),v(uo.$$.fragment,d),v(go.$$.fragment,d),v(_o.$$.fragment,d),dn=!0)},o(d){b(w.$$.fragment,d),b(je.$$.fragment,d),b(Me.$$.fragment,d),b(Ve.$$.fragment,d),b(De.$$.fragment,d),b($e.$$.fragment,d),b(Fe.$$.fragment,d),b(ze.$$.fragment,d),b(Oe.$$.fragment,d),b(ve.$$.fragment,d),b(Re.$$.fragment,d),b(Be.$$.fragment,d),b(Ge.$$.fragment,d),b(We.$$.fragment,d),b(Ue.$$.fragment,d),b(eo.$$.fragment,d),b(Te.$$.fragment,d),b(oo.$$.fragment,d),b(to.$$.fragment,d),b(no.$$.fragment,d),b(ro.$$.fragment,d),b(ao.$$.fragment,d),b(fo.$$.fragment,d),b(we.$$.fragment,d),b(uo.$$.fragment,d),b(go.$$.fragment,d),b(_o.$$.fragment,d),dn=!1},d(d){o(p),d&&o(P),d&&o(f),T(w),d&&o(Jt),d&&o(m),d&&o(Kt),d&&o(he),d&&o(Qt),d&&o(G),d&&o(Xt),d&&o(Q),T(je),d&&o(en),d&&o(F),T(Me),T(Ve),T(De),T($e),d&&o(on),d&&o(oe),T(Fe),d&&o(tn),d&&o(E),T(ze),T(Oe),T(ve),T(Re),T(Be),T(Ge),d&&o(nn),d&&o(ae),T(We),d&&o(rn),d&&o(k),T(Ue),T(eo),T(Te),T(oo),T(to),T(no),d&&o(an),d&&o(ie),T(ro),d&&o(sn),d&&o(x),T(ao),T(fo),T(we),T(uo),T(go),T(_o)}}}const mc={local:"vision-encoder-decoder-models",sections:[{local:"transformers.VisionEncoderDecoderConfig",title:"VisionEncoderDecoderConfig"},{local:"transformers.VisionEncoderDecoderModel",title:"VisionEncoderDecoderModel"},{local:"transformers.TFVisionEncoderDecoderModel",title:"TFVisionEncoderDecoderModel"},{local:"transformers.FlaxVisionEncoderDecoderModel",title:"FlaxVisionEncoderDecoderModel"}],title:"Vision Encoder Decoder Models"};function fc(pe){return ic(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Tc extends rc{constructor(p){super();ac(this,p,fc,hc,sc,{})}}export{Tc as default,mc as metadata};
