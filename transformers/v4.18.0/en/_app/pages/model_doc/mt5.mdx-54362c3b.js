import{S as Xd,i as Kd,s as Jd,e as r,k as d,w as f,t as a,M as Qd,c as n,d as s,m as p,a as o,x as h,h as i,b as l,F as e,g as c,y as u,L as Yd,q as g,o as _,B as v,v as Zd}from"../../chunks/vendor-6b77c823.js";import{D as T}from"../../chunks/Docstring-17b815d9.js";import{C as Ce}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as C}from"../../chunks/IconCopyLink-7a11ce68.js";function ep(sl){let G,Tr,O,se,ks,Pe,Ln,Ts,Nn,br,V,re,bs,Ae,Dn,ys,In,yr,ne,Gn,Se,On,Vn,$r,Ut,Un,wr,Wt,$s,Wn,zr,oe,Bn,Le,Hn,Rn,xr,Bt,Xn,Mr,y,ws,zs,Ne,Kn,Jn,xs,Ms,De,Qn,Yn,Es,qs,Ie,Zn,eo,Fs,js,Ge,to,so,Cs,Ht,Oe,ro,no,Er,S,oo,Ve,ao,io,Ue,lo,po,qr,U,ae,Ps,We,mo,As,co,Fr,P,Be,fo,A,ho,Rt,uo,go,Xt,_o,vo,He,ko,To,bo,W,yo,Kt,$o,wo,Jt,zo,xo,jr,B,ie,Ss,Re,Mo,Ls,Eo,Cr,k,Xe,qo,Ke,Fo,Je,jo,Co,Po,Qe,Ao,Qt,So,Lo,No,L,Ye,Do,Ns,Io,Go,Ze,Yt,Oo,Ds,Vo,Uo,Zt,Wo,Is,Bo,Ho,le,et,Ro,Gs,Xo,Ko,de,tt,Jo,Os,Qo,Yo,pe,st,Zo,rt,ea,Vs,ta,sa,Pr,me,ra,es,na,oa,Ar,H,ce,Us,nt,aa,Ws,ia,Sr,b,ot,la,R,da,Bs,pa,ma,at,ca,fa,ha,it,ua,ts,ga,_a,va,N,lt,ka,Hs,Ta,ba,dt,ss,ya,Rs,$a,wa,rs,za,Xs,xa,Ma,fe,pt,Ea,Ks,qa,Lr,he,Fa,ns,ja,Ca,Nr,X,ue,Js,mt,Pa,Qs,Aa,Dr,w,ct,Sa,ft,La,os,Na,Da,Ia,Ys,Ga,Oa,ht,Ir,K,ge,Zs,ut,Va,er,Ua,Gr,z,gt,Wa,_t,Ba,as,Ha,Ra,Xa,tr,Ka,Ja,vt,Or,J,_e,sr,kt,Qa,rr,Ya,Vr,x,Tt,Za,bt,ei,is,ti,si,ri,nr,ni,oi,yt,Ur,Q,ve,or,$t,ai,ar,ii,Wr,M,wt,li,zt,di,ls,pi,mi,ci,ir,fi,hi,xt,Br,Y,ke,lr,Mt,ui,dr,gi,Hr,E,Et,_i,qt,vi,ds,ki,Ti,bi,pr,yi,$i,Ft,Rr,Z,Te,mr,jt,wi,cr,zi,Xr,q,Ct,xi,Pt,Mi,ps,Ei,qi,Fi,fr,ji,Ci,At,Kr,ee,be,hr,St,Pi,ur,Ai,Jr,F,Lt,Si,Nt,Li,ms,Ni,Di,Ii,gr,Gi,Oi,Dt,Qr,te,ye,_r,It,Vi,vr,Ui,Yr,j,Gt,Wi,Ot,Bi,cs,Hi,Ri,Xi,kr,Ki,Ji,Vt,Zr;return Pe=new C({}),Ae=new C({}),We=new C({}),Be=new T({props:{name:"class transformers.MT5Config",anchor:"transformers.MT5Config",parameters:[{name:"vocab_size",val:" = 250112"},{name:"d_model",val:" = 512"},{name:"d_kv",val:" = 64"},{name:"d_ff",val:" = 1024"},{name:"num_layers",val:" = 8"},{name:"num_decoder_layers",val:" = None"},{name:"num_heads",val:" = 6"},{name:"relative_attention_num_buckets",val:" = 32"},{name:"relative_attention_max_distance",val:" = 128"},{name:"dropout_rate",val:" = 0.1"},{name:"layer_norm_epsilon",val:" = 1e-06"},{name:"initializer_factor",val:" = 1.0"},{name:"feed_forward_proj",val:" = 'gated-gelu'"},{name:"is_encoder_decoder",val:" = True"},{name:"use_cache",val:" = True"},{name:"tokenizer_class",val:" = 'T5Tokenizer'"},{name:"tie_word_embeddings",val:" = False"},{name:"pad_token_id",val:" = 0"},{name:"eos_token_id",val:" = 1"},{name:"decoder_start_token_id",val:" = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MT5Config.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 250112) &#x2014;
Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.18.0/en/model_doc/t5#transformers.T5Model">T5Model</a> or <a href="/docs/transformers/v4.18.0/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a>.`,name:"vocab_size"},{anchor:"transformers.MT5Config.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Size of the encoder layers and the pooler layer.`,name:"d_model"},{anchor:"transformers.MT5Config.d_kv",description:`<strong>d_kv</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Size of the key, query, value projections per attention head. <code>d_kv</code> has to be equal to <code>d_model // num_heads</code>.`,name:"d_kv"},{anchor:"transformers.MT5Config.d_ff",description:`<strong>d_ff</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Size of the intermediate feed forward layer in each <code>T5Block</code>.`,name:"d_ff"},{anchor:"transformers.MT5Config.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_layers"},{anchor:"transformers.MT5Config.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Number of hidden layers in the Transformer decoder. Will use the same value as <code>num_layers</code> if not set.`,name:"num_decoder_layers"},{anchor:"transformers.MT5Config.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_heads"},{anchor:"transformers.MT5Config.relative_attention_num_buckets",description:`<strong>relative_attention_num_buckets</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of buckets to use for each attention layer.`,name:"relative_attention_num_buckets"},{anchor:"transformers.MT5Config.relative_attention_max_distance",description:`<strong>relative_attention_max_distance</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
The maximum distance of the longer sequences for the bucket separation.`,name:"relative_attention_max_distance"},{anchor:"transformers.MT5Config.dropout_rate",description:`<strong>dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The ratio for all dropout layers.`,name:"dropout_rate"},{anchor:"transformers.MT5Config.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-6) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.MT5Config.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.MT5Config.feed_forward_proj",description:`<strong>feed_forward_proj</strong> (<code>string</code>, <em>optional</em>, defaults to <code>&quot;gated-gelu&quot;</code>) &#x2014;
Type of feed forward layer to be used. Should be one of <code>&quot;relu&quot;</code> or <code>&quot;gated-gelu&quot;</code>.`,name:"feed_forward_proj"},{anchor:"transformers.MT5Config.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/mt5/configuration_mt5.py#L24"}}),Re=new C({}),Xe=new T({props:{name:"class transformers.T5Tokenizer",anchor:"transformers.T5Tokenizer",parameters:[{name:"vocab_file",val:""},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 100"},{name:"additional_special_tokens",val:" = None"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.T5Tokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.T5Tokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.T5Tokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.T5Tokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.T5Tokenizer.extra_ids",description:`<strong>extra_ids</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
accessible as &#x201C;<extra<em>id{%d}&gt;&#x201D; where &#x201D;{%d}&#x201D; is a number between 0 and extra_ids-1. Extra tokens are
indexed from the end of the vocabulary up to beginning (&#x201C;<extra_id_0>&#x201D; is the last token in the vocabulary
like in T5 preprocessing see
<a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117" rel="nofollow">here</a>).</extra_id_0></extra<em>`,name:"extra_ids"},{anchor:"transformers.T5Tokenizer.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"},{anchor:"transformers.T5Tokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"},{anchor:"transformers.T5Tokenizer.sp_model",description:`<strong>sp_model</strong> (<code>SentencePieceProcessor</code>) &#x2014;
The <em>SentencePiece</em> processor that is used for every conversion (string, tokens and IDs).`,name:"sp_model"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/t5/tokenization_t5.py#L53"}}),Ye=new T({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/t5/tokenization_t5.py#L223",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),et=new T({props:{name:"convert_tokens_to_string",anchor:"transformers.T5Tokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/t5/tokenization_t5.py#L284"}}),tt=new T({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/t5/tokenization_t5.py#L201",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),st=new T({props:{name:"get_special_tokens_mask",anchor:"transformers.T5Tokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/t5/tokenization_t5.py#L163",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),nt=new C({}),ot=new T({props:{name:"class transformers.T5TokenizerFast",anchor:"transformers.T5TokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 100"},{name:"additional_special_tokens",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.T5TokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.T5TokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.T5TokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.T5TokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.T5TokenizerFast.extra_ids",description:`<strong>extra_ids</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
accessible as &#x201C;<extra<em>id{%d}&gt;&#x201D; where &#x201D;{%d}&#x201D; is a number between 0 and extra_ids-1. Extra tokens are
indexed from the end of the vocabulary up to beginning (&#x201C;<extra_id_0>&#x201D; is the last token in the vocabulary
like in T5 preprocessing see
<a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117" rel="nofollow">here</a>).</extra_id_0></extra<em>`,name:"extra_ids"},{anchor:"transformers.T5TokenizerFast.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/t5/tokenization_t5_fast.py#L62"}}),lt=new T({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/t5/tokenization_t5_fast.py#L165",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),pt=new T({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/t5/tokenization_t5_fast.py#L191",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),mt=new C({}),ct=new T({props:{name:"class transformers.MT5Model",anchor:"transformers.MT5Model",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/mt5/modeling_mt5.py#L28"}}),ht=new Ce({props:{code:`from transformers import MT5Model, T5Tokenizer

model = MT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="pt")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="pt")

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=labels["input_ids"])
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),ut=new C({}),gt=new T({props:{name:"class transformers.MT5ForConditionalGeneration",anchor:"transformers.MT5ForConditionalGeneration",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/mt5/modeling_mt5.py#L62"}}),vt=new Ce({props:{code:`from transformers import MT5ForConditionalGeneration, T5Tokenizer

model = MT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="pt")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="pt")

outputs = model(**inputs, labels=labels["input_ids"])
loss = outputs.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),kt=new C({}),Tt=new T({props:{name:"class transformers.MT5EncoderModel",anchor:"transformers.MT5EncoderModel",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/mt5/modeling_mt5.py#L94"}}),yt=new Ce({props:{code:`from transformers import MT5EncoderModel, T5Tokenizer

model = MT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="pt").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5EncoderModel, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_state = outputs.last_hidden_state`}}),$t=new C({}),wt=new T({props:{name:"class transformers.TFMT5Model",anchor:"transformers.TFMT5Model",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/mt5/modeling_tf_mt5.py#L28"}}),xt=new Ce({props:{code:`from transformers import TFMT5Model, T5Tokenizer

model = TFMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="tf")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="tf")

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=labels["input_ids"])
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),Mt=new C({}),Et=new T({props:{name:"class transformers.TFMT5ForConditionalGeneration",anchor:"transformers.TFMT5ForConditionalGeneration",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/mt5/modeling_tf_mt5.py#L53"}}),Ft=new Ce({props:{code:`from transformers import TFMT5ForConditionalGeneration, T5Tokenizer

model = TFMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="tf")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="tf")

outputs = model(**inputs, labels=labels["input_ids"])
loss = outputs.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),jt=new C({}),Ct=new T({props:{name:"class transformers.TFMT5EncoderModel",anchor:"transformers.TFMT5EncoderModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/mt5/modeling_tf_mt5.py#L79"}}),At=new Ce({props:{code:`from transformers import TFMT5EncoderModel, T5Tokenizer

model = TFMT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="tf").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5EncoderModel, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_state = outputs.last_hidden_state`}}),St=new C({}),Lt=new T({props:{name:"class transformers.FlaxMT5Model",anchor:"transformers.FlaxMT5Model",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/mt5/modeling_flax_mt5.py#L28"}}),Dt=new Ce({props:{code:`from transformers import FlaxMT5Model, T5Tokenizer

model = FlaxMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

with tokenizer.as_target_tokenizer():
    decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=decoder_input_ids)
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxMT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxMT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    decoder_input_ids = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),It=new C({}),Gt=new T({props:{name:"class transformers.FlaxMT5ForConditionalGeneration",anchor:"transformers.FlaxMT5ForConditionalGeneration",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/mt5/modeling_flax_mt5.py#L55"}}),Vt=new Ce({props:{code:`from transformers import FlaxMT5ForConditionalGeneration, T5Tokenizer

model = FlaxMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

with tokenizer.as_target_tokenizer():
    decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids

outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxMT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxMT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    decoder_input_ids = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){G=r("meta"),Tr=d(),O=r("h1"),se=r("a"),ks=r("span"),f(Pe.$$.fragment),Ln=d(),Ts=r("span"),Nn=a("mT5"),br=d(),V=r("h2"),re=r("a"),bs=r("span"),f(Ae.$$.fragment),Dn=d(),ys=r("span"),In=a("Overview"),yr=d(),ne=r("p"),Gn=a("The mT5 model was presented in "),Se=r("a"),On=a("mT5: A massively multilingual pre-trained text-to-text transformer"),Vn=a(` by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, Colin Raffel.`),$r=d(),Ut=r("p"),Un=a("The abstract from the paper is the following:"),wr=d(),Wt=r("p"),$s=r("em"),Wn=a(`The recent \u201CText-to-Text Transfer Transformer\u201D (T5) leveraged a unified text-to-text format and scale to attain
state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail
the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. We also describe a simple technique to prevent \u201Caccidental translation\u201D in the zero-shot setting, where a
generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model
checkpoints used in this work are publicly available.`),zr=d(),oe=r("p"),Bn=a("Note: mT5 was only pre-trained on "),Le=r("a"),Hn=a("mC4"),Rn=a(` excluding any supervised training.
Therefore, this model has to be fine-tuned before it is useable on a downstream task, unlike the original T5 model.
Since mT5 was pre-trained unsupervisedly, there\u2019s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`),xr=d(),Bt=r("p"),Xn=a("Google has released the following variants:"),Mr=d(),y=r("ul"),ws=r("li"),zs=r("p"),Ne=r("a"),Kn=a("google/mt5-small"),Jn=d(),xs=r("li"),Ms=r("p"),De=r("a"),Qn=a("google/mt5-base"),Yn=d(),Es=r("li"),qs=r("p"),Ie=r("a"),Zn=a("google/mt5-large"),eo=d(),Fs=r("li"),js=r("p"),Ge=r("a"),to=a("google/mt5-xl"),so=d(),Cs=r("li"),Ht=r("p"),Oe=r("a"),ro=a("google/mt5-xxl"),no=a("."),Er=d(),S=r("p"),oo=a("This model was contributed by "),Ve=r("a"),ao=a("patrickvonplaten"),io=a(`. The original code can be
found `),Ue=r("a"),lo=a("here"),po=a("."),qr=d(),U=r("h2"),ae=r("a"),Ps=r("span"),f(We.$$.fragment),mo=d(),As=r("span"),co=a("MT5Config"),Fr=d(),P=r("div"),f(Be.$$.fragment),fo=d(),A=r("p"),ho=a("This is the configuration class to store the configuration of a "),Rt=r("a"),uo=a("MT5Model"),go=a(" or a "),Xt=r("a"),_o=a("TFMT5Model"),vo=a(`. It is used to
instantiate a mT5 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the mT5
`),He=r("a"),ko=a("google/mt5-small"),To=a(" architecture."),bo=d(),W=r("p"),yo=a("Configuration objects inherit from "),Kt=r("a"),$o=a("PretrainedConfig"),wo=a(` and can be used to control the model outputs. Read the
documentation from `),Jt=r("a"),zo=a("PretrainedConfig"),xo=a(" for more information."),jr=d(),B=r("h2"),ie=r("a"),Ss=r("span"),f(Re.$$.fragment),Mo=d(),Ls=r("span"),Eo=a("MT5Tokenizer"),Cr=d(),k=r("div"),f(Xe.$$.fragment),qo=d(),Ke=r("p"),Fo=a("Construct a T5 tokenizer. Based on "),Je=r("a"),jo=a("SentencePiece"),Co=a("."),Po=d(),Qe=r("p"),Ao=a("This tokenizer inherits from "),Qt=r("a"),So=a("PreTrainedTokenizer"),Lo=a(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),No=d(),L=r("div"),f(Ye.$$.fragment),Do=d(),Ns=r("p"),Io=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),Go=d(),Ze=r("ul"),Yt=r("li"),Oo=a("single sequence: "),Ds=r("code"),Vo=a("X </s>"),Uo=d(),Zt=r("li"),Wo=a("pair of sequences: "),Is=r("code"),Bo=a("A </s> B </s>"),Ho=d(),le=r("div"),f(et.$$.fragment),Ro=d(),Gs=r("p"),Xo=a("Converts a sequence of tokens (string) in a single string."),Ko=d(),de=r("div"),f(tt.$$.fragment),Jo=d(),Os=r("p"),Qo=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),Yo=d(),pe=r("div"),f(st.$$.fragment),Zo=d(),rt=r("p"),ea=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Vs=r("code"),ta=a("prepare_for_model"),sa=a(" method."),Pr=d(),me=r("p"),ra=a("See "),es=r("a"),na=a("T5Tokenizer"),oa=a(" for all details."),Ar=d(),H=r("h2"),ce=r("a"),Us=r("span"),f(nt.$$.fragment),aa=d(),Ws=r("span"),ia=a("MT5TokenizerFast"),Sr=d(),b=r("div"),f(ot.$$.fragment),la=d(),R=r("p"),da=a("Construct a \u201Cfast\u201D T5 tokenizer (backed by HuggingFace\u2019s "),Bs=r("em"),pa=a("tokenizers"),ma=a(` library). Based on
`),at=r("a"),ca=a("Unigram"),fa=a("."),ha=d(),it=r("p"),ua=a("This tokenizer inherits from "),ts=r("a"),ga=a("PreTrainedTokenizerFast"),_a=a(` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),va=d(),N=r("div"),f(lt.$$.fragment),ka=d(),Hs=r("p"),Ta=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),ba=d(),dt=r("ul"),ss=r("li"),ya=a("single sequence: "),Rs=r("code"),$a=a("X </s>"),wa=d(),rs=r("li"),za=a("pair of sequences: "),Xs=r("code"),xa=a("A </s> B </s>"),Ma=d(),fe=r("div"),f(pt.$$.fragment),Ea=d(),Ks=r("p"),qa=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),Lr=d(),he=r("p"),Fa=a("See "),ns=r("a"),ja=a("T5TokenizerFast"),Ca=a(" for all details."),Nr=d(),X=r("h2"),ue=r("a"),Js=r("span"),f(mt.$$.fragment),Pa=d(),Qs=r("span"),Aa=a("MT5Model"),Dr=d(),w=r("div"),f(ct.$$.fragment),Sa=d(),ft=r("p"),La=a("This class overrides "),os=r("a"),Na=a("T5Model"),Da=a(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Ia=d(),Ys=r("p"),Ga=a("Examples:"),Oa=d(),f(ht.$$.fragment),Ir=d(),K=r("h2"),ge=r("a"),Zs=r("span"),f(ut.$$.fragment),Va=d(),er=r("span"),Ua=a("MT5ForConditionalGeneration"),Gr=d(),z=r("div"),f(gt.$$.fragment),Wa=d(),_t=r("p"),Ba=a("This class overrides "),as=r("a"),Ha=a("T5ForConditionalGeneration"),Ra=a(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Xa=d(),tr=r("p"),Ka=a("Examples:"),Ja=d(),f(vt.$$.fragment),Or=d(),J=r("h2"),_e=r("a"),sr=r("span"),f(kt.$$.fragment),Qa=d(),rr=r("span"),Ya=a("MT5EncoderModel"),Vr=d(),x=r("div"),f(Tt.$$.fragment),Za=d(),bt=r("p"),ei=a("This class overrides "),is=r("a"),ti=a("T5EncoderModel"),si=a(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),ri=d(),nr=r("p"),ni=a("Examples:"),oi=d(),f(yt.$$.fragment),Ur=d(),Q=r("h2"),ve=r("a"),or=r("span"),f($t.$$.fragment),ai=d(),ar=r("span"),ii=a("TFMT5Model"),Wr=d(),M=r("div"),f(wt.$$.fragment),li=d(),zt=r("p"),di=a("This class overrides "),ls=r("a"),pi=a("TFT5Model"),mi=a(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),ci=d(),ir=r("p"),fi=a("Examples:"),hi=d(),f(xt.$$.fragment),Br=d(),Y=r("h2"),ke=r("a"),lr=r("span"),f(Mt.$$.fragment),ui=d(),dr=r("span"),gi=a("TFMT5ForConditionalGeneration"),Hr=d(),E=r("div"),f(Et.$$.fragment),_i=d(),qt=r("p"),vi=a("This class overrides "),ds=r("a"),ki=a("TFT5ForConditionalGeneration"),Ti=a(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),bi=d(),pr=r("p"),yi=a("Examples:"),$i=d(),f(Ft.$$.fragment),Rr=d(),Z=r("h2"),Te=r("a"),mr=r("span"),f(jt.$$.fragment),wi=d(),cr=r("span"),zi=a("TFMT5EncoderModel"),Xr=d(),q=r("div"),f(Ct.$$.fragment),xi=d(),Pt=r("p"),Mi=a("This class overrides "),ps=r("a"),Ei=a("TFT5EncoderModel"),qi=a(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Fi=d(),fr=r("p"),ji=a("Examples:"),Ci=d(),f(At.$$.fragment),Kr=d(),ee=r("h2"),be=r("a"),hr=r("span"),f(St.$$.fragment),Pi=d(),ur=r("span"),Ai=a("FlaxMT5Model"),Jr=d(),F=r("div"),f(Lt.$$.fragment),Si=d(),Nt=r("p"),Li=a("This class overrides "),ms=r("a"),Ni=a("FlaxT5Model"),Di=a(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Ii=d(),gr=r("p"),Gi=a("Examples:"),Oi=d(),f(Dt.$$.fragment),Qr=d(),te=r("h2"),ye=r("a"),_r=r("span"),f(It.$$.fragment),Vi=d(),vr=r("span"),Ui=a("FlaxMT5ForConditionalGeneration"),Yr=d(),j=r("div"),f(Gt.$$.fragment),Wi=d(),Ot=r("p"),Bi=a("This class overrides "),cs=r("a"),Hi=a("FlaxT5ForConditionalGeneration"),Ri=a(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Xi=d(),kr=r("p"),Ki=a("Examples:"),Ji=d(),f(Vt.$$.fragment),this.h()},l(t){const m=Qd('[data-svelte="svelte-1phssyn"]',document.head);G=n(m,"META",{name:!0,content:!0}),m.forEach(s),Tr=p(t),O=n(t,"H1",{class:!0});var en=o(O);se=n(en,"A",{id:!0,class:!0,href:!0});var rl=o(se);ks=n(rl,"SPAN",{});var nl=o(ks);h(Pe.$$.fragment,nl),nl.forEach(s),rl.forEach(s),Ln=p(en),Ts=n(en,"SPAN",{});var ol=o(Ts);Nn=i(ol,"mT5"),ol.forEach(s),en.forEach(s),br=p(t),V=n(t,"H2",{class:!0});var tn=o(V);re=n(tn,"A",{id:!0,class:!0,href:!0});var al=o(re);bs=n(al,"SPAN",{});var il=o(bs);h(Ae.$$.fragment,il),il.forEach(s),al.forEach(s),Dn=p(tn),ys=n(tn,"SPAN",{});var ll=o(ys);In=i(ll,"Overview"),ll.forEach(s),tn.forEach(s),yr=p(t),ne=n(t,"P",{});var sn=o(ne);Gn=i(sn,"The mT5 model was presented in "),Se=n(sn,"A",{href:!0,rel:!0});var dl=o(Se);On=i(dl,"mT5: A massively multilingual pre-trained text-to-text transformer"),dl.forEach(s),Vn=i(sn,` by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, Colin Raffel.`),sn.forEach(s),$r=p(t),Ut=n(t,"P",{});var pl=o(Ut);Un=i(pl,"The abstract from the paper is the following:"),pl.forEach(s),wr=p(t),Wt=n(t,"P",{});var ml=o(Wt);$s=n(ml,"EM",{});var cl=o($s);Wn=i(cl,`The recent \u201CText-to-Text Transfer Transformer\u201D (T5) leveraged a unified text-to-text format and scale to attain
state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail
the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. We also describe a simple technique to prevent \u201Caccidental translation\u201D in the zero-shot setting, where a
generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model
checkpoints used in this work are publicly available.`),cl.forEach(s),ml.forEach(s),zr=p(t),oe=n(t,"P",{});var rn=o(oe);Bn=i(rn,"Note: mT5 was only pre-trained on "),Le=n(rn,"A",{href:!0,rel:!0});var fl=o(Le);Hn=i(fl,"mC4"),fl.forEach(s),Rn=i(rn,` excluding any supervised training.
Therefore, this model has to be fine-tuned before it is useable on a downstream task, unlike the original T5 model.
Since mT5 was pre-trained unsupervisedly, there\u2019s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`),rn.forEach(s),xr=p(t),Bt=n(t,"P",{});var hl=o(Bt);Xn=i(hl,"Google has released the following variants:"),hl.forEach(s),Mr=p(t),y=n(t,"UL",{});var D=o(y);ws=n(D,"LI",{});var ul=o(ws);zs=n(ul,"P",{});var gl=o(zs);Ne=n(gl,"A",{href:!0,rel:!0});var _l=o(Ne);Kn=i(_l,"google/mt5-small"),_l.forEach(s),gl.forEach(s),ul.forEach(s),Jn=p(D),xs=n(D,"LI",{});var vl=o(xs);Ms=n(vl,"P",{});var kl=o(Ms);De=n(kl,"A",{href:!0,rel:!0});var Tl=o(De);Qn=i(Tl,"google/mt5-base"),Tl.forEach(s),kl.forEach(s),vl.forEach(s),Yn=p(D),Es=n(D,"LI",{});var bl=o(Es);qs=n(bl,"P",{});var yl=o(qs);Ie=n(yl,"A",{href:!0,rel:!0});var $l=o(Ie);Zn=i($l,"google/mt5-large"),$l.forEach(s),yl.forEach(s),bl.forEach(s),eo=p(D),Fs=n(D,"LI",{});var wl=o(Fs);js=n(wl,"P",{});var zl=o(js);Ge=n(zl,"A",{href:!0,rel:!0});var xl=o(Ge);to=i(xl,"google/mt5-xl"),xl.forEach(s),zl.forEach(s),wl.forEach(s),so=p(D),Cs=n(D,"LI",{});var Ml=o(Cs);Ht=n(Ml,"P",{});var Qi=o(Ht);Oe=n(Qi,"A",{href:!0,rel:!0});var El=o(Oe);ro=i(El,"google/mt5-xxl"),El.forEach(s),no=i(Qi,"."),Qi.forEach(s),Ml.forEach(s),D.forEach(s),Er=p(t),S=n(t,"P",{});var fs=o(S);oo=i(fs,"This model was contributed by "),Ve=n(fs,"A",{href:!0,rel:!0});var ql=o(Ve);ao=i(ql,"patrickvonplaten"),ql.forEach(s),io=i(fs,`. The original code can be
found `),Ue=n(fs,"A",{href:!0,rel:!0});var Fl=o(Ue);lo=i(Fl,"here"),Fl.forEach(s),po=i(fs,"."),fs.forEach(s),qr=p(t),U=n(t,"H2",{class:!0});var nn=o(U);ae=n(nn,"A",{id:!0,class:!0,href:!0});var jl=o(ae);Ps=n(jl,"SPAN",{});var Cl=o(Ps);h(We.$$.fragment,Cl),Cl.forEach(s),jl.forEach(s),mo=p(nn),As=n(nn,"SPAN",{});var Pl=o(As);co=i(Pl,"MT5Config"),Pl.forEach(s),nn.forEach(s),Fr=p(t),P=n(t,"DIV",{class:!0});var hs=o(P);h(Be.$$.fragment,hs),fo=p(hs),A=n(hs,"P",{});var $e=o(A);ho=i($e,"This is the configuration class to store the configuration of a "),Rt=n($e,"A",{href:!0});var Al=o(Rt);uo=i(Al,"MT5Model"),Al.forEach(s),go=i($e," or a "),Xt=n($e,"A",{href:!0});var Sl=o(Xt);_o=i(Sl,"TFMT5Model"),Sl.forEach(s),vo=i($e,`. It is used to
instantiate a mT5 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the mT5
`),He=n($e,"A",{href:!0,rel:!0});var Ll=o(He);ko=i(Ll,"google/mt5-small"),Ll.forEach(s),To=i($e," architecture."),$e.forEach(s),bo=p(hs),W=n(hs,"P",{});var us=o(W);yo=i(us,"Configuration objects inherit from "),Kt=n(us,"A",{href:!0});var Nl=o(Kt);$o=i(Nl,"PretrainedConfig"),Nl.forEach(s),wo=i(us,` and can be used to control the model outputs. Read the
documentation from `),Jt=n(us,"A",{href:!0});var Dl=o(Jt);zo=i(Dl,"PretrainedConfig"),Dl.forEach(s),xo=i(us," for more information."),us.forEach(s),hs.forEach(s),jr=p(t),B=n(t,"H2",{class:!0});var on=o(B);ie=n(on,"A",{id:!0,class:!0,href:!0});var Il=o(ie);Ss=n(Il,"SPAN",{});var Gl=o(Ss);h(Re.$$.fragment,Gl),Gl.forEach(s),Il.forEach(s),Mo=p(on),Ls=n(on,"SPAN",{});var Ol=o(Ls);Eo=i(Ol,"MT5Tokenizer"),Ol.forEach(s),on.forEach(s),Cr=p(t),k=n(t,"DIV",{class:!0});var $=o(k);h(Xe.$$.fragment,$),qo=p($),Ke=n($,"P",{});var an=o(Ke);Fo=i(an,"Construct a T5 tokenizer. Based on "),Je=n(an,"A",{href:!0,rel:!0});var Vl=o(Je);jo=i(Vl,"SentencePiece"),Vl.forEach(s),Co=i(an,"."),an.forEach(s),Po=p($),Qe=n($,"P",{});var ln=o(Qe);Ao=i(ln,"This tokenizer inherits from "),Qt=n(ln,"A",{href:!0});var Ul=o(Qt);So=i(Ul,"PreTrainedTokenizer"),Ul.forEach(s),Lo=i(ln,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),ln.forEach(s),No=p($),L=n($,"DIV",{class:!0});var gs=o(L);h(Ye.$$.fragment,gs),Do=p(gs),Ns=n(gs,"P",{});var Wl=o(Ns);Io=i(Wl,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),Wl.forEach(s),Go=p(gs),Ze=n(gs,"UL",{});var dn=o(Ze);Yt=n(dn,"LI",{});var Yi=o(Yt);Oo=i(Yi,"single sequence: "),Ds=n(Yi,"CODE",{});var Bl=o(Ds);Vo=i(Bl,"X </s>"),Bl.forEach(s),Yi.forEach(s),Uo=p(dn),Zt=n(dn,"LI",{});var Zi=o(Zt);Wo=i(Zi,"pair of sequences: "),Is=n(Zi,"CODE",{});var Hl=o(Is);Bo=i(Hl,"A </s> B </s>"),Hl.forEach(s),Zi.forEach(s),dn.forEach(s),gs.forEach(s),Ho=p($),le=n($,"DIV",{class:!0});var pn=o(le);h(et.$$.fragment,pn),Ro=p(pn),Gs=n(pn,"P",{});var Rl=o(Gs);Xo=i(Rl,"Converts a sequence of tokens (string) in a single string."),Rl.forEach(s),pn.forEach(s),Ko=p($),de=n($,"DIV",{class:!0});var mn=o(de);h(tt.$$.fragment,mn),Jo=p(mn),Os=n(mn,"P",{});var Xl=o(Os);Qo=i(Xl,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),Xl.forEach(s),mn.forEach(s),Yo=p($),pe=n($,"DIV",{class:!0});var cn=o(pe);h(st.$$.fragment,cn),Zo=p(cn),rt=n(cn,"P",{});var fn=o(rt);ea=i(fn,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Vs=n(fn,"CODE",{});var Kl=o(Vs);ta=i(Kl,"prepare_for_model"),Kl.forEach(s),sa=i(fn," method."),fn.forEach(s),cn.forEach(s),$.forEach(s),Pr=p(t),me=n(t,"P",{});var hn=o(me);ra=i(hn,"See "),es=n(hn,"A",{href:!0});var Jl=o(es);na=i(Jl,"T5Tokenizer"),Jl.forEach(s),oa=i(hn," for all details."),hn.forEach(s),Ar=p(t),H=n(t,"H2",{class:!0});var un=o(H);ce=n(un,"A",{id:!0,class:!0,href:!0});var Ql=o(ce);Us=n(Ql,"SPAN",{});var Yl=o(Us);h(nt.$$.fragment,Yl),Yl.forEach(s),Ql.forEach(s),aa=p(un),Ws=n(un,"SPAN",{});var Zl=o(Ws);ia=i(Zl,"MT5TokenizerFast"),Zl.forEach(s),un.forEach(s),Sr=p(t),b=n(t,"DIV",{class:!0});var I=o(b);h(ot.$$.fragment,I),la=p(I),R=n(I,"P",{});var _s=o(R);da=i(_s,"Construct a \u201Cfast\u201D T5 tokenizer (backed by HuggingFace\u2019s "),Bs=n(_s,"EM",{});var ed=o(Bs);pa=i(ed,"tokenizers"),ed.forEach(s),ma=i(_s,` library). Based on
`),at=n(_s,"A",{href:!0,rel:!0});var td=o(at);ca=i(td,"Unigram"),td.forEach(s),fa=i(_s,"."),_s.forEach(s),ha=p(I),it=n(I,"P",{});var gn=o(it);ua=i(gn,"This tokenizer inherits from "),ts=n(gn,"A",{href:!0});var sd=o(ts);ga=i(sd,"PreTrainedTokenizerFast"),sd.forEach(s),_a=i(gn,` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),gn.forEach(s),va=p(I),N=n(I,"DIV",{class:!0});var vs=o(N);h(lt.$$.fragment,vs),ka=p(vs),Hs=n(vs,"P",{});var rd=o(Hs);Ta=i(rd,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),rd.forEach(s),ba=p(vs),dt=n(vs,"UL",{});var _n=o(dt);ss=n(_n,"LI",{});var el=o(ss);ya=i(el,"single sequence: "),Rs=n(el,"CODE",{});var nd=o(Rs);$a=i(nd,"X </s>"),nd.forEach(s),el.forEach(s),wa=p(_n),rs=n(_n,"LI",{});var tl=o(rs);za=i(tl,"pair of sequences: "),Xs=n(tl,"CODE",{});var od=o(Xs);xa=i(od,"A </s> B </s>"),od.forEach(s),tl.forEach(s),_n.forEach(s),vs.forEach(s),Ma=p(I),fe=n(I,"DIV",{class:!0});var vn=o(fe);h(pt.$$.fragment,vn),Ea=p(vn),Ks=n(vn,"P",{});var ad=o(Ks);qa=i(ad,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),ad.forEach(s),vn.forEach(s),I.forEach(s),Lr=p(t),he=n(t,"P",{});var kn=o(he);Fa=i(kn,"See "),ns=n(kn,"A",{href:!0});var id=o(ns);ja=i(id,"T5TokenizerFast"),id.forEach(s),Ca=i(kn," for all details."),kn.forEach(s),Nr=p(t),X=n(t,"H2",{class:!0});var Tn=o(X);ue=n(Tn,"A",{id:!0,class:!0,href:!0});var ld=o(ue);Js=n(ld,"SPAN",{});var dd=o(Js);h(mt.$$.fragment,dd),dd.forEach(s),ld.forEach(s),Pa=p(Tn),Qs=n(Tn,"SPAN",{});var pd=o(Qs);Aa=i(pd,"MT5Model"),pd.forEach(s),Tn.forEach(s),Dr=p(t),w=n(t,"DIV",{class:!0});var we=o(w);h(ct.$$.fragment,we),Sa=p(we),ft=n(we,"P",{});var bn=o(ft);La=i(bn,"This class overrides "),os=n(bn,"A",{href:!0});var md=o(os);Na=i(md,"T5Model"),md.forEach(s),Da=i(bn,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),bn.forEach(s),Ia=p(we),Ys=n(we,"P",{});var cd=o(Ys);Ga=i(cd,"Examples:"),cd.forEach(s),Oa=p(we),h(ht.$$.fragment,we),we.forEach(s),Ir=p(t),K=n(t,"H2",{class:!0});var yn=o(K);ge=n(yn,"A",{id:!0,class:!0,href:!0});var fd=o(ge);Zs=n(fd,"SPAN",{});var hd=o(Zs);h(ut.$$.fragment,hd),hd.forEach(s),fd.forEach(s),Va=p(yn),er=n(yn,"SPAN",{});var ud=o(er);Ua=i(ud,"MT5ForConditionalGeneration"),ud.forEach(s),yn.forEach(s),Gr=p(t),z=n(t,"DIV",{class:!0});var ze=o(z);h(gt.$$.fragment,ze),Wa=p(ze),_t=n(ze,"P",{});var $n=o(_t);Ba=i($n,"This class overrides "),as=n($n,"A",{href:!0});var gd=o(as);Ha=i(gd,"T5ForConditionalGeneration"),gd.forEach(s),Ra=i($n,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),$n.forEach(s),Xa=p(ze),tr=n(ze,"P",{});var _d=o(tr);Ka=i(_d,"Examples:"),_d.forEach(s),Ja=p(ze),h(vt.$$.fragment,ze),ze.forEach(s),Or=p(t),J=n(t,"H2",{class:!0});var wn=o(J);_e=n(wn,"A",{id:!0,class:!0,href:!0});var vd=o(_e);sr=n(vd,"SPAN",{});var kd=o(sr);h(kt.$$.fragment,kd),kd.forEach(s),vd.forEach(s),Qa=p(wn),rr=n(wn,"SPAN",{});var Td=o(rr);Ya=i(Td,"MT5EncoderModel"),Td.forEach(s),wn.forEach(s),Vr=p(t),x=n(t,"DIV",{class:!0});var xe=o(x);h(Tt.$$.fragment,xe),Za=p(xe),bt=n(xe,"P",{});var zn=o(bt);ei=i(zn,"This class overrides "),is=n(zn,"A",{href:!0});var bd=o(is);ti=i(bd,"T5EncoderModel"),bd.forEach(s),si=i(zn,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),zn.forEach(s),ri=p(xe),nr=n(xe,"P",{});var yd=o(nr);ni=i(yd,"Examples:"),yd.forEach(s),oi=p(xe),h(yt.$$.fragment,xe),xe.forEach(s),Ur=p(t),Q=n(t,"H2",{class:!0});var xn=o(Q);ve=n(xn,"A",{id:!0,class:!0,href:!0});var $d=o(ve);or=n($d,"SPAN",{});var wd=o(or);h($t.$$.fragment,wd),wd.forEach(s),$d.forEach(s),ai=p(xn),ar=n(xn,"SPAN",{});var zd=o(ar);ii=i(zd,"TFMT5Model"),zd.forEach(s),xn.forEach(s),Wr=p(t),M=n(t,"DIV",{class:!0});var Me=o(M);h(wt.$$.fragment,Me),li=p(Me),zt=n(Me,"P",{});var Mn=o(zt);di=i(Mn,"This class overrides "),ls=n(Mn,"A",{href:!0});var xd=o(ls);pi=i(xd,"TFT5Model"),xd.forEach(s),mi=i(Mn,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Mn.forEach(s),ci=p(Me),ir=n(Me,"P",{});var Md=o(ir);fi=i(Md,"Examples:"),Md.forEach(s),hi=p(Me),h(xt.$$.fragment,Me),Me.forEach(s),Br=p(t),Y=n(t,"H2",{class:!0});var En=o(Y);ke=n(En,"A",{id:!0,class:!0,href:!0});var Ed=o(ke);lr=n(Ed,"SPAN",{});var qd=o(lr);h(Mt.$$.fragment,qd),qd.forEach(s),Ed.forEach(s),ui=p(En),dr=n(En,"SPAN",{});var Fd=o(dr);gi=i(Fd,"TFMT5ForConditionalGeneration"),Fd.forEach(s),En.forEach(s),Hr=p(t),E=n(t,"DIV",{class:!0});var Ee=o(E);h(Et.$$.fragment,Ee),_i=p(Ee),qt=n(Ee,"P",{});var qn=o(qt);vi=i(qn,"This class overrides "),ds=n(qn,"A",{href:!0});var jd=o(ds);ki=i(jd,"TFT5ForConditionalGeneration"),jd.forEach(s),Ti=i(qn,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),qn.forEach(s),bi=p(Ee),pr=n(Ee,"P",{});var Cd=o(pr);yi=i(Cd,"Examples:"),Cd.forEach(s),$i=p(Ee),h(Ft.$$.fragment,Ee),Ee.forEach(s),Rr=p(t),Z=n(t,"H2",{class:!0});var Fn=o(Z);Te=n(Fn,"A",{id:!0,class:!0,href:!0});var Pd=o(Te);mr=n(Pd,"SPAN",{});var Ad=o(mr);h(jt.$$.fragment,Ad),Ad.forEach(s),Pd.forEach(s),wi=p(Fn),cr=n(Fn,"SPAN",{});var Sd=o(cr);zi=i(Sd,"TFMT5EncoderModel"),Sd.forEach(s),Fn.forEach(s),Xr=p(t),q=n(t,"DIV",{class:!0});var qe=o(q);h(Ct.$$.fragment,qe),xi=p(qe),Pt=n(qe,"P",{});var jn=o(Pt);Mi=i(jn,"This class overrides "),ps=n(jn,"A",{href:!0});var Ld=o(ps);Ei=i(Ld,"TFT5EncoderModel"),Ld.forEach(s),qi=i(jn,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),jn.forEach(s),Fi=p(qe),fr=n(qe,"P",{});var Nd=o(fr);ji=i(Nd,"Examples:"),Nd.forEach(s),Ci=p(qe),h(At.$$.fragment,qe),qe.forEach(s),Kr=p(t),ee=n(t,"H2",{class:!0});var Cn=o(ee);be=n(Cn,"A",{id:!0,class:!0,href:!0});var Dd=o(be);hr=n(Dd,"SPAN",{});var Id=o(hr);h(St.$$.fragment,Id),Id.forEach(s),Dd.forEach(s),Pi=p(Cn),ur=n(Cn,"SPAN",{});var Gd=o(ur);Ai=i(Gd,"FlaxMT5Model"),Gd.forEach(s),Cn.forEach(s),Jr=p(t),F=n(t,"DIV",{class:!0});var Fe=o(F);h(Lt.$$.fragment,Fe),Si=p(Fe),Nt=n(Fe,"P",{});var Pn=o(Nt);Li=i(Pn,"This class overrides "),ms=n(Pn,"A",{href:!0});var Od=o(ms);Ni=i(Od,"FlaxT5Model"),Od.forEach(s),Di=i(Pn,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Pn.forEach(s),Ii=p(Fe),gr=n(Fe,"P",{});var Vd=o(gr);Gi=i(Vd,"Examples:"),Vd.forEach(s),Oi=p(Fe),h(Dt.$$.fragment,Fe),Fe.forEach(s),Qr=p(t),te=n(t,"H2",{class:!0});var An=o(te);ye=n(An,"A",{id:!0,class:!0,href:!0});var Ud=o(ye);_r=n(Ud,"SPAN",{});var Wd=o(_r);h(It.$$.fragment,Wd),Wd.forEach(s),Ud.forEach(s),Vi=p(An),vr=n(An,"SPAN",{});var Bd=o(vr);Ui=i(Bd,"FlaxMT5ForConditionalGeneration"),Bd.forEach(s),An.forEach(s),Yr=p(t),j=n(t,"DIV",{class:!0});var je=o(j);h(Gt.$$.fragment,je),Wi=p(je),Ot=n(je,"P",{});var Sn=o(Ot);Bi=i(Sn,"This class overrides "),cs=n(Sn,"A",{href:!0});var Hd=o(cs);Hi=i(Hd,"FlaxT5ForConditionalGeneration"),Hd.forEach(s),Ri=i(Sn,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Sn.forEach(s),Xi=p(je),kr=n(je,"P",{});var Rd=o(kr);Ki=i(Rd,"Examples:"),Rd.forEach(s),Ji=p(je),h(Vt.$$.fragment,je),je.forEach(s),this.h()},h(){l(G,"name","hf:doc:metadata"),l(G,"content",JSON.stringify(tp)),l(se,"id","mt5"),l(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(se,"href","#mt5"),l(O,"class","relative group"),l(re,"id","overview"),l(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(re,"href","#overview"),l(V,"class","relative group"),l(Se,"href","https://arxiv.org/abs/2010.11934"),l(Se,"rel","nofollow"),l(Le,"href","https://huggingface.co/datasets/mc4"),l(Le,"rel","nofollow"),l(Ne,"href","https://huggingface.co/google/mt5-small"),l(Ne,"rel","nofollow"),l(De,"href","https://huggingface.co/google/mt5-base"),l(De,"rel","nofollow"),l(Ie,"href","https://huggingface.co/google/mt5-large"),l(Ie,"rel","nofollow"),l(Ge,"href","https://huggingface.co/google/mt5-xl"),l(Ge,"rel","nofollow"),l(Oe,"href","https://huggingface.co/google/mt5-xxl"),l(Oe,"rel","nofollow"),l(Ve,"href","https://huggingface.co/patrickvonplaten"),l(Ve,"rel","nofollow"),l(Ue,"href","https://github.com/google-research/multilingual-t5"),l(Ue,"rel","nofollow"),l(ae,"id","transformers.MT5Config"),l(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ae,"href","#transformers.MT5Config"),l(U,"class","relative group"),l(Rt,"href","/docs/transformers/v4.18.0/en/model_doc/mt5#transformers.MT5Model"),l(Xt,"href","/docs/transformers/v4.18.0/en/model_doc/mt5#transformers.TFMT5Model"),l(He,"href","https://huggingface.co/google/mt5-small"),l(He,"rel","nofollow"),l(Kt,"href","/docs/transformers/v4.18.0/en/main_classes/configuration#transformers.PretrainedConfig"),l(Jt,"href","/docs/transformers/v4.18.0/en/main_classes/configuration#transformers.PretrainedConfig"),l(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ie,"id","transformers.T5Tokenizer"),l(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ie,"href","#transformers.T5Tokenizer"),l(B,"class","relative group"),l(Je,"href","https://github.com/google/sentencepiece"),l(Je,"rel","nofollow"),l(Qt,"href","/docs/transformers/v4.18.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),l(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(es,"href","/docs/transformers/v4.18.0/en/model_doc/mt5#transformers.T5Tokenizer"),l(ce,"id","transformers.T5TokenizerFast"),l(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ce,"href","#transformers.T5TokenizerFast"),l(H,"class","relative group"),l(at,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models"),l(at,"rel","nofollow"),l(ts,"href","/docs/transformers/v4.18.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),l(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(b,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ns,"href","/docs/transformers/v4.18.0/en/model_doc/mt5#transformers.T5TokenizerFast"),l(ue,"id","transformers.MT5Model"),l(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ue,"href","#transformers.MT5Model"),l(X,"class","relative group"),l(os,"href","/docs/transformers/v4.18.0/en/model_doc/t5#transformers.T5Model"),l(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ge,"id","transformers.MT5ForConditionalGeneration"),l(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ge,"href","#transformers.MT5ForConditionalGeneration"),l(K,"class","relative group"),l(as,"href","/docs/transformers/v4.18.0/en/model_doc/t5#transformers.T5ForConditionalGeneration"),l(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(_e,"id","transformers.MT5EncoderModel"),l(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(_e,"href","#transformers.MT5EncoderModel"),l(J,"class","relative group"),l(is,"href","/docs/transformers/v4.18.0/en/model_doc/t5#transformers.T5EncoderModel"),l(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ve,"id","transformers.TFMT5Model"),l(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ve,"href","#transformers.TFMT5Model"),l(Q,"class","relative group"),l(ls,"href","/docs/transformers/v4.18.0/en/model_doc/t5#transformers.TFT5Model"),l(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ke,"id","transformers.TFMT5ForConditionalGeneration"),l(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ke,"href","#transformers.TFMT5ForConditionalGeneration"),l(Y,"class","relative group"),l(ds,"href","/docs/transformers/v4.18.0/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),l(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Te,"id","transformers.TFMT5EncoderModel"),l(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Te,"href","#transformers.TFMT5EncoderModel"),l(Z,"class","relative group"),l(ps,"href","/docs/transformers/v4.18.0/en/model_doc/t5#transformers.TFT5EncoderModel"),l(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(be,"id","transformers.FlaxMT5Model"),l(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(be,"href","#transformers.FlaxMT5Model"),l(ee,"class","relative group"),l(ms,"href","/docs/transformers/v4.18.0/en/model_doc/t5#transformers.FlaxT5Model"),l(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ye,"id","transformers.FlaxMT5ForConditionalGeneration"),l(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ye,"href","#transformers.FlaxMT5ForConditionalGeneration"),l(te,"class","relative group"),l(cs,"href","/docs/transformers/v4.18.0/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),l(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,m){e(document.head,G),c(t,Tr,m),c(t,O,m),e(O,se),e(se,ks),u(Pe,ks,null),e(O,Ln),e(O,Ts),e(Ts,Nn),c(t,br,m),c(t,V,m),e(V,re),e(re,bs),u(Ae,bs,null),e(V,Dn),e(V,ys),e(ys,In),c(t,yr,m),c(t,ne,m),e(ne,Gn),e(ne,Se),e(Se,On),e(ne,Vn),c(t,$r,m),c(t,Ut,m),e(Ut,Un),c(t,wr,m),c(t,Wt,m),e(Wt,$s),e($s,Wn),c(t,zr,m),c(t,oe,m),e(oe,Bn),e(oe,Le),e(Le,Hn),e(oe,Rn),c(t,xr,m),c(t,Bt,m),e(Bt,Xn),c(t,Mr,m),c(t,y,m),e(y,ws),e(ws,zs),e(zs,Ne),e(Ne,Kn),e(y,Jn),e(y,xs),e(xs,Ms),e(Ms,De),e(De,Qn),e(y,Yn),e(y,Es),e(Es,qs),e(qs,Ie),e(Ie,Zn),e(y,eo),e(y,Fs),e(Fs,js),e(js,Ge),e(Ge,to),e(y,so),e(y,Cs),e(Cs,Ht),e(Ht,Oe),e(Oe,ro),e(Ht,no),c(t,Er,m),c(t,S,m),e(S,oo),e(S,Ve),e(Ve,ao),e(S,io),e(S,Ue),e(Ue,lo),e(S,po),c(t,qr,m),c(t,U,m),e(U,ae),e(ae,Ps),u(We,Ps,null),e(U,mo),e(U,As),e(As,co),c(t,Fr,m),c(t,P,m),u(Be,P,null),e(P,fo),e(P,A),e(A,ho),e(A,Rt),e(Rt,uo),e(A,go),e(A,Xt),e(Xt,_o),e(A,vo),e(A,He),e(He,ko),e(A,To),e(P,bo),e(P,W),e(W,yo),e(W,Kt),e(Kt,$o),e(W,wo),e(W,Jt),e(Jt,zo),e(W,xo),c(t,jr,m),c(t,B,m),e(B,ie),e(ie,Ss),u(Re,Ss,null),e(B,Mo),e(B,Ls),e(Ls,Eo),c(t,Cr,m),c(t,k,m),u(Xe,k,null),e(k,qo),e(k,Ke),e(Ke,Fo),e(Ke,Je),e(Je,jo),e(Ke,Co),e(k,Po),e(k,Qe),e(Qe,Ao),e(Qe,Qt),e(Qt,So),e(Qe,Lo),e(k,No),e(k,L),u(Ye,L,null),e(L,Do),e(L,Ns),e(Ns,Io),e(L,Go),e(L,Ze),e(Ze,Yt),e(Yt,Oo),e(Yt,Ds),e(Ds,Vo),e(Ze,Uo),e(Ze,Zt),e(Zt,Wo),e(Zt,Is),e(Is,Bo),e(k,Ho),e(k,le),u(et,le,null),e(le,Ro),e(le,Gs),e(Gs,Xo),e(k,Ko),e(k,de),u(tt,de,null),e(de,Jo),e(de,Os),e(Os,Qo),e(k,Yo),e(k,pe),u(st,pe,null),e(pe,Zo),e(pe,rt),e(rt,ea),e(rt,Vs),e(Vs,ta),e(rt,sa),c(t,Pr,m),c(t,me,m),e(me,ra),e(me,es),e(es,na),e(me,oa),c(t,Ar,m),c(t,H,m),e(H,ce),e(ce,Us),u(nt,Us,null),e(H,aa),e(H,Ws),e(Ws,ia),c(t,Sr,m),c(t,b,m),u(ot,b,null),e(b,la),e(b,R),e(R,da),e(R,Bs),e(Bs,pa),e(R,ma),e(R,at),e(at,ca),e(R,fa),e(b,ha),e(b,it),e(it,ua),e(it,ts),e(ts,ga),e(it,_a),e(b,va),e(b,N),u(lt,N,null),e(N,ka),e(N,Hs),e(Hs,Ta),e(N,ba),e(N,dt),e(dt,ss),e(ss,ya),e(ss,Rs),e(Rs,$a),e(dt,wa),e(dt,rs),e(rs,za),e(rs,Xs),e(Xs,xa),e(b,Ma),e(b,fe),u(pt,fe,null),e(fe,Ea),e(fe,Ks),e(Ks,qa),c(t,Lr,m),c(t,he,m),e(he,Fa),e(he,ns),e(ns,ja),e(he,Ca),c(t,Nr,m),c(t,X,m),e(X,ue),e(ue,Js),u(mt,Js,null),e(X,Pa),e(X,Qs),e(Qs,Aa),c(t,Dr,m),c(t,w,m),u(ct,w,null),e(w,Sa),e(w,ft),e(ft,La),e(ft,os),e(os,Na),e(ft,Da),e(w,Ia),e(w,Ys),e(Ys,Ga),e(w,Oa),u(ht,w,null),c(t,Ir,m),c(t,K,m),e(K,ge),e(ge,Zs),u(ut,Zs,null),e(K,Va),e(K,er),e(er,Ua),c(t,Gr,m),c(t,z,m),u(gt,z,null),e(z,Wa),e(z,_t),e(_t,Ba),e(_t,as),e(as,Ha),e(_t,Ra),e(z,Xa),e(z,tr),e(tr,Ka),e(z,Ja),u(vt,z,null),c(t,Or,m),c(t,J,m),e(J,_e),e(_e,sr),u(kt,sr,null),e(J,Qa),e(J,rr),e(rr,Ya),c(t,Vr,m),c(t,x,m),u(Tt,x,null),e(x,Za),e(x,bt),e(bt,ei),e(bt,is),e(is,ti),e(bt,si),e(x,ri),e(x,nr),e(nr,ni),e(x,oi),u(yt,x,null),c(t,Ur,m),c(t,Q,m),e(Q,ve),e(ve,or),u($t,or,null),e(Q,ai),e(Q,ar),e(ar,ii),c(t,Wr,m),c(t,M,m),u(wt,M,null),e(M,li),e(M,zt),e(zt,di),e(zt,ls),e(ls,pi),e(zt,mi),e(M,ci),e(M,ir),e(ir,fi),e(M,hi),u(xt,M,null),c(t,Br,m),c(t,Y,m),e(Y,ke),e(ke,lr),u(Mt,lr,null),e(Y,ui),e(Y,dr),e(dr,gi),c(t,Hr,m),c(t,E,m),u(Et,E,null),e(E,_i),e(E,qt),e(qt,vi),e(qt,ds),e(ds,ki),e(qt,Ti),e(E,bi),e(E,pr),e(pr,yi),e(E,$i),u(Ft,E,null),c(t,Rr,m),c(t,Z,m),e(Z,Te),e(Te,mr),u(jt,mr,null),e(Z,wi),e(Z,cr),e(cr,zi),c(t,Xr,m),c(t,q,m),u(Ct,q,null),e(q,xi),e(q,Pt),e(Pt,Mi),e(Pt,ps),e(ps,Ei),e(Pt,qi),e(q,Fi),e(q,fr),e(fr,ji),e(q,Ci),u(At,q,null),c(t,Kr,m),c(t,ee,m),e(ee,be),e(be,hr),u(St,hr,null),e(ee,Pi),e(ee,ur),e(ur,Ai),c(t,Jr,m),c(t,F,m),u(Lt,F,null),e(F,Si),e(F,Nt),e(Nt,Li),e(Nt,ms),e(ms,Ni),e(Nt,Di),e(F,Ii),e(F,gr),e(gr,Gi),e(F,Oi),u(Dt,F,null),c(t,Qr,m),c(t,te,m),e(te,ye),e(ye,_r),u(It,_r,null),e(te,Vi),e(te,vr),e(vr,Ui),c(t,Yr,m),c(t,j,m),u(Gt,j,null),e(j,Wi),e(j,Ot),e(Ot,Bi),e(Ot,cs),e(cs,Hi),e(Ot,Ri),e(j,Xi),e(j,kr),e(kr,Ki),e(j,Ji),u(Vt,j,null),Zr=!0},p:Yd,i(t){Zr||(g(Pe.$$.fragment,t),g(Ae.$$.fragment,t),g(We.$$.fragment,t),g(Be.$$.fragment,t),g(Re.$$.fragment,t),g(Xe.$$.fragment,t),g(Ye.$$.fragment,t),g(et.$$.fragment,t),g(tt.$$.fragment,t),g(st.$$.fragment,t),g(nt.$$.fragment,t),g(ot.$$.fragment,t),g(lt.$$.fragment,t),g(pt.$$.fragment,t),g(mt.$$.fragment,t),g(ct.$$.fragment,t),g(ht.$$.fragment,t),g(ut.$$.fragment,t),g(gt.$$.fragment,t),g(vt.$$.fragment,t),g(kt.$$.fragment,t),g(Tt.$$.fragment,t),g(yt.$$.fragment,t),g($t.$$.fragment,t),g(wt.$$.fragment,t),g(xt.$$.fragment,t),g(Mt.$$.fragment,t),g(Et.$$.fragment,t),g(Ft.$$.fragment,t),g(jt.$$.fragment,t),g(Ct.$$.fragment,t),g(At.$$.fragment,t),g(St.$$.fragment,t),g(Lt.$$.fragment,t),g(Dt.$$.fragment,t),g(It.$$.fragment,t),g(Gt.$$.fragment,t),g(Vt.$$.fragment,t),Zr=!0)},o(t){_(Pe.$$.fragment,t),_(Ae.$$.fragment,t),_(We.$$.fragment,t),_(Be.$$.fragment,t),_(Re.$$.fragment,t),_(Xe.$$.fragment,t),_(Ye.$$.fragment,t),_(et.$$.fragment,t),_(tt.$$.fragment,t),_(st.$$.fragment,t),_(nt.$$.fragment,t),_(ot.$$.fragment,t),_(lt.$$.fragment,t),_(pt.$$.fragment,t),_(mt.$$.fragment,t),_(ct.$$.fragment,t),_(ht.$$.fragment,t),_(ut.$$.fragment,t),_(gt.$$.fragment,t),_(vt.$$.fragment,t),_(kt.$$.fragment,t),_(Tt.$$.fragment,t),_(yt.$$.fragment,t),_($t.$$.fragment,t),_(wt.$$.fragment,t),_(xt.$$.fragment,t),_(Mt.$$.fragment,t),_(Et.$$.fragment,t),_(Ft.$$.fragment,t),_(jt.$$.fragment,t),_(Ct.$$.fragment,t),_(At.$$.fragment,t),_(St.$$.fragment,t),_(Lt.$$.fragment,t),_(Dt.$$.fragment,t),_(It.$$.fragment,t),_(Gt.$$.fragment,t),_(Vt.$$.fragment,t),Zr=!1},d(t){s(G),t&&s(Tr),t&&s(O),v(Pe),t&&s(br),t&&s(V),v(Ae),t&&s(yr),t&&s(ne),t&&s($r),t&&s(Ut),t&&s(wr),t&&s(Wt),t&&s(zr),t&&s(oe),t&&s(xr),t&&s(Bt),t&&s(Mr),t&&s(y),t&&s(Er),t&&s(S),t&&s(qr),t&&s(U),v(We),t&&s(Fr),t&&s(P),v(Be),t&&s(jr),t&&s(B),v(Re),t&&s(Cr),t&&s(k),v(Xe),v(Ye),v(et),v(tt),v(st),t&&s(Pr),t&&s(me),t&&s(Ar),t&&s(H),v(nt),t&&s(Sr),t&&s(b),v(ot),v(lt),v(pt),t&&s(Lr),t&&s(he),t&&s(Nr),t&&s(X),v(mt),t&&s(Dr),t&&s(w),v(ct),v(ht),t&&s(Ir),t&&s(K),v(ut),t&&s(Gr),t&&s(z),v(gt),v(vt),t&&s(Or),t&&s(J),v(kt),t&&s(Vr),t&&s(x),v(Tt),v(yt),t&&s(Ur),t&&s(Q),v($t),t&&s(Wr),t&&s(M),v(wt),v(xt),t&&s(Br),t&&s(Y),v(Mt),t&&s(Hr),t&&s(E),v(Et),v(Ft),t&&s(Rr),t&&s(Z),v(jt),t&&s(Xr),t&&s(q),v(Ct),v(At),t&&s(Kr),t&&s(ee),v(St),t&&s(Jr),t&&s(F),v(Lt),v(Dt),t&&s(Qr),t&&s(te),v(It),t&&s(Yr),t&&s(j),v(Gt),v(Vt)}}}const tp={local:"mt5",sections:[{local:"overview",title:"Overview"},{local:"transformers.MT5Config",title:"MT5Config"},{local:"transformers.T5Tokenizer",title:"MT5Tokenizer"},{local:"transformers.T5TokenizerFast",title:"MT5TokenizerFast"},{local:"transformers.MT5Model",title:"MT5Model"},{local:"transformers.MT5ForConditionalGeneration",title:"MT5ForConditionalGeneration"},{local:"transformers.MT5EncoderModel",title:"MT5EncoderModel"},{local:"transformers.TFMT5Model",title:"TFMT5Model"},{local:"transformers.TFMT5ForConditionalGeneration",title:"TFMT5ForConditionalGeneration"},{local:"transformers.TFMT5EncoderModel",title:"TFMT5EncoderModel"},{local:"transformers.FlaxMT5Model",title:"FlaxMT5Model"},{local:"transformers.FlaxMT5ForConditionalGeneration",title:"FlaxMT5ForConditionalGeneration"}],title:"mT5"};function sp(sl){return Zd(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ip extends Xd{constructor(G){super();Kd(this,G,sp,ep,Jd,{})}}export{ip as default,tp as metadata};
