import{S as sf,i as lf,s as df,e as a,k as m,w as b,t as s,M as cf,c as n,d as t,m as f,a as r,x as F,h as i,b as d,G as e,g as _,y as k,q as $,o as T,B as w,v as mf,L as bo}from"../../chunks/vendor-hf-doc-builder.js";import{T as Ut}from"../../chunks/Tip-hf-doc-builder.js";import{D as M}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Fo}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as L}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as vo}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function ff(y){let c,x,u,p,v;return p=new Fo({props:{code:`from transformers import FlavaModel, FlavaForPreTraining, FlavaConfig

# Initializing a FlavaConfig with style configuration
configuration = FlavaConfig()

# Initializing a FlavaModel and FlavaForPreTraining model from the style configuration
model = FlavaModel(configuration)
model_pre = FlavaForPreTraining(configuration)

# Accessing the model configuration
configuration = model.config
configuration_pre = model_pre.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaModel, FlavaForPreTraining, FlavaConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaConfig with style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FlavaConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaModel and FlavaForPreTraining model from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaModel(configuration)
<span class="hljs-meta">&gt;&gt;&gt; </span>model_pre = FlavaForPreTraining(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration_pre = model_pre.config`}}),{c(){c=a("p"),x=s("Example:"),u=m(),b(p.$$.fragment)},l(l){c=n(l,"P",{});var h=r(c);x=i(h,"Example:"),h.forEach(t),u=f(l),F(p.$$.fragment,l)},m(l,h){_(l,c,h),e(c,x),_(l,u,h),k(p,l,h),v=!0},p:bo,i(l){v||($(p.$$.fragment,l),v=!0)},o(l){T(p.$$.fragment,l),v=!1},d(l){l&&t(c),l&&t(u),w(p,l)}}}function hf(y){let c,x,u,p,v;return p=new Fo({props:{code:`from transformers import FlavaTextModel, FlavaTextConfig

# Initializing a FlavaTextModel with  style configuration
configuration = FlavaTextConfig()

# Initializing a FlavaTextConfig from the style configuration
model = FlavaTextModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaTextModel, FlavaTextConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaTextModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FlavaTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaTextConfig from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){c=a("p"),x=s("Example:"),u=m(),b(p.$$.fragment)},l(l){c=n(l,"P",{});var h=r(c);x=i(h,"Example:"),h.forEach(t),u=f(l),F(p.$$.fragment,l)},m(l,h){_(l,c,h),e(c,x),_(l,u,h),k(p,l,h),v=!0},p:bo,i(l){v||($(p.$$.fragment,l),v=!0)},o(l){T(p.$$.fragment,l),v=!1},d(l){l&&t(c),l&&t(u),w(p,l)}}}function pf(y){let c,x,u,p,v;return p=new Fo({props:{code:`from transformers import FlavaImageModel, FlavaImageConfig

# Initializing a FlavaImageModel with  style configuration
configuration = FlavaImageConfig()

# Initializing a FlavaImageModel model from the style configuration
model = FlavaImageModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaImageModel, FlavaImageConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaImageModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FlavaImageConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaImageModel model from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaImageModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){c=a("p"),x=s("Example:"),u=m(),b(p.$$.fragment)},l(l){c=n(l,"P",{});var h=r(c);x=i(h,"Example:"),h.forEach(t),u=f(l),F(p.$$.fragment,l)},m(l,h){_(l,c,h),e(c,x),_(l,u,h),k(p,l,h),v=!0},p:bo,i(l){v||($(p.$$.fragment,l),v=!0)},o(l){T(p.$$.fragment,l),v=!1},d(l){l&&t(c),l&&t(u),w(p,l)}}}function gf(y){let c,x,u,p,v;return p=new Fo({props:{code:`from transformers import FlavaMultimodalModel, FlavaMultimodalConfig

# Initializing a FlavaMultimodalModel with  style configuration
configuration = FlavaMultimodalConfig()

# Initializing a FlavaMultimodalModel model from the style configuration
model = FlavaMultimodalModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaMultimodalModel, FlavaMultimodalConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaMultimodalModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FlavaMultimodalConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaMultimodalModel model from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaMultimodalModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){c=a("p"),x=s("Example:"),u=m(),b(p.$$.fragment)},l(l){c=n(l,"P",{});var h=r(c);x=i(h,"Example:"),h.forEach(t),u=f(l),F(p.$$.fragment,l)},m(l,h){_(l,c,h),e(c,x),_(l,u,h),k(p,l,h),v=!0},p:bo,i(l){v||($(p.$$.fragment,l),v=!0)},o(l){T(p.$$.fragment,l),v=!1},d(l){l&&t(c),l&&t(u),w(p,l)}}}function uf(y){let c,x,u,p,v;return{c(){c=a("p"),x=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a("code"),p=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){c=n(l,"P",{});var h=r(c);x=i(h,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(h,"CODE",{});var z=r(u);p=i(z,"Module"),z.forEach(t),v=i(h,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),h.forEach(t)},m(l,h){_(l,c,h),e(c,x),e(c,u),e(u,p),e(c,v)},d(l){l&&t(c)}}}function _f(y){let c,x,u,p,v;return{c(){c=a("p"),x=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a("code"),p=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){c=n(l,"P",{});var h=r(c);x=i(h,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(h,"CODE",{});var z=r(u);p=i(z,"Module"),z.forEach(t),v=i(h,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),h.forEach(t)},m(l,h){_(l,c,h),e(c,x),e(c,u),e(u,p),e(c,v)},d(l){l&&t(c)}}}function vf(y){let c,x,u,p,v;return p=new Fo({props:{code:`from PIL import Image
import requests
from transformers import FlavaProcessor, FlavaModel

model = FlavaModel.from_pretrained("facebook/flava-full")
processor = FlavaProcessor.from_pretrained("facebook/flava-full")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=["a photo of a cat"], images=image, return_tensors="pt", padding=True)

outputs = model(**inputs)
logits_per_image = outputs.contrastive_logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaProcessor, FlavaModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaModel.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = FlavaProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.contrastive_logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`}}),{c(){c=a("p"),x=s("Examples:"),u=m(),b(p.$$.fragment)},l(l){c=n(l,"P",{});var h=r(c);x=i(h,"Examples:"),h.forEach(t),u=f(l),F(p.$$.fragment,l)},m(l,h){_(l,c,h),e(c,x),_(l,u,h),k(p,l,h),v=!0},p:bo,i(l){v||($(p.$$.fragment,l),v=!0)},o(l){T(p.$$.fragment,l),v=!1},d(l){l&&t(c),l&&t(u),w(p,l)}}}function bf(y){let c,x,u,p,v;return{c(){c=a("p"),x=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a("code"),p=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){c=n(l,"P",{});var h=r(c);x=i(h,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(h,"CODE",{});var z=r(u);p=i(z,"Module"),z.forEach(t),v=i(h,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),h.forEach(t)},m(l,h){_(l,c,h),e(c,x),e(c,u),e(u,p),e(c,v)},d(l){l&&t(c)}}}function Ff(y){let c,x,u,p,v;return{c(){c=a("p"),x=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a("code"),p=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){c=n(l,"P",{});var h=r(c);x=i(h,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(h,"CODE",{});var z=r(u);p=i(z,"Module"),z.forEach(t),v=i(h,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),h.forEach(t)},m(l,h){_(l,c,h),e(c,x),e(c,u),e(u,p),e(c,v)},d(l){l&&t(c)}}}function kf(y){let c,x,u,p,v;return{c(){c=a("p"),x=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a("code"),p=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){c=n(l,"P",{});var h=r(c);x=i(h,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(h,"CODE",{});var z=r(u);p=i(z,"Module"),z.forEach(t),v=i(h,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),h.forEach(t)},m(l,h){_(l,c,h),e(c,x),e(c,u),e(u,p),e(c,v)},d(l){l&&t(c)}}}function $f(y){let c,x,u,p,v;return p=new Fo({props:{code:`from transformers import BertTokenizer, FlavaTextModel
import torch

tokenizer = BertTokenizer.from_pretrained("facebook/flava-full")
model = FlavaTextModel.from_pretrained("facebook/flava-full")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, FlavaTextModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaTextModel.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){c=a("p"),x=s("Example:"),u=m(),b(p.$$.fragment)},l(l){c=n(l,"P",{});var h=r(c);x=i(h,"Example:"),h.forEach(t),u=f(l),F(p.$$.fragment,l)},m(l,h){_(l,c,h),e(c,x),_(l,u,h),k(p,l,h),v=!0},p:bo,i(l){v||($(p.$$.fragment,l),v=!0)},o(l){T(p.$$.fragment,l),v=!1},d(l){l&&t(c),l&&t(u),w(p,l)}}}function Tf(y){let c,x,u,p,v;return{c(){c=a("p"),x=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a("code"),p=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){c=n(l,"P",{});var h=r(c);x=i(h,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(h,"CODE",{});var z=r(u);p=i(z,"Module"),z.forEach(t),v=i(h,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),h.forEach(t)},m(l,h){_(l,c,h),e(c,x),e(c,u),e(u,p),e(c,v)},d(l){l&&t(c)}}}function wf(y){let c,x,u,p,v;return p=new Fo({props:{code:`from transformers import FlavaFeatureExtractor, FlavaImageModel
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

feature_extractor = FlavaFeatureExtractor.from_pretrained("facebook/flava-full")
model = FlavaImageModel.from_pretrained("facebook/flava-full")

inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
list(last_hidden_states.shape)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaFeatureExtractor, FlavaImageModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = FlavaFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaImageModel.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">197</span>, <span class="hljs-number">768</span>]`}}),{c(){c=a("p"),x=s("Example:"),u=m(),b(p.$$.fragment)},l(l){c=n(l,"P",{});var h=r(c);x=i(h,"Example:"),h.forEach(t),u=f(l),F(p.$$.fragment,l)},m(l,h){_(l,c,h),e(c,x),_(l,u,h),k(p,l,h),v=!0},p:bo,i(l){v||($(p.$$.fragment,l),v=!0)},o(l){T(p.$$.fragment,l),v=!1},d(l){l&&t(c),l&&t(u),w(p,l)}}}function xf(y){let c,x,u,p,v;return{c(){c=a("p"),x=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a("code"),p=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){c=n(l,"P",{});var h=r(c);x=i(h,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(h,"CODE",{});var z=r(u);p=i(z,"Module"),z.forEach(t),v=i(h,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),h.forEach(t)},m(l,h){_(l,c,h),e(c,x),e(c,u),e(u,p),e(c,v)},d(l){l&&t(c)}}}function yf(y){let c,x,u,p,v;return p=new Fo({props:{code:`from transformers import BertTokenizer, FlavaMultimodalModel
import torch

tokenizer = BertTokenizer.from_pretrained("facebook/flava-full")
model = FlavaMultimodalModel.from_pretrained("facebook/flava-full")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, FlavaMultimodalModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaMultimodalModel.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){c=a("p"),x=s("Example:"),u=m(),b(p.$$.fragment)},l(l){c=n(l,"P",{});var h=r(c);x=i(h,"Example:"),h.forEach(t),u=f(l),F(p.$$.fragment,l)},m(l,h){_(l,c,h),e(c,x),_(l,u,h),k(p,l,h),v=!0},p:bo,i(l){v||($(p.$$.fragment,l),v=!0)},o(l){T(p.$$.fragment,l),v=!1},d(l){l&&t(c),l&&t(u),w(p,l)}}}function Mf(y){let c,x,u,p,v,l,h,z,as,Sn,se,Ie,Ua,ko,ns,Ra,rs,Un,Ae,ss,$o,is,ls,Rn,Rt,ds,Hn,Ht,cs,Gn,Gt,Ha,ms,Zn,X,fs,To,hs,ps,wo,gs,us,Kn,ie,je,Ga,xo,_s,Za,vs,Xn,E,yo,bs,J,Zt,Fs,ks,Kt,$s,Ts,Mo,ws,xs,ys,le,Ms,Xt,zs,Es,Jt,Cs,Ps,Is,qe,As,Le,zo,js,Eo,qs,Qt,Ls,Os,Ns,Oe,Co,Ds,Po,Vs,Yt,Ws,Bs,Jn,de,Ne,Ka,Io,Ss,Xa,Us,Qn,P,Ao,Rs,jo,Hs,ea,Gs,Zs,Ks,qo,Xs,Lo,Js,Qs,Ys,ce,ei,oa,oi,ti,ta,ai,ni,ri,De,Yn,me,Ve,Ja,Oo,si,Qa,ii,er,I,No,li,Do,di,aa,ci,mi,fi,Vo,hi,Wo,pi,gi,ui,fe,_i,na,vi,bi,ra,Fi,ki,$i,We,or,he,Be,Ya,Bo,Ti,en,wi,tr,A,So,xi,Uo,yi,sa,Mi,zi,Ei,Ro,Ci,Ho,Pi,Ii,Ai,pe,ji,ia,qi,Li,la,Oi,Ni,Di,Se,ar,ge,Ue,on,Go,Vi,tn,Wi,nr,Zo,Ko,rr,ue,Re,an,Xo,Bi,nn,Si,sr,j,Jo,Ui,rn,Ri,Hi,O,da,Gi,Zi,ca,Ki,Xi,ma,Ji,Qi,sn,Yi,el,fa,ol,tl,al,He,Qo,nl,Yo,rl,ha,sl,il,ll,Ge,et,dl,ot,cl,pa,ml,fl,ir,_e,Ze,ln,tt,hl,dn,pl,lr,R,at,gl,cn,ul,_l,nt,vl,ga,bl,Fl,dr,ve,Ke,mn,rt,kl,fn,$l,cr,N,st,Tl,hn,wl,xl,it,yl,lt,Ml,zl,El,Q,dt,Cl,be,Pl,ua,Il,Al,pn,jl,ql,Ll,Xe,mr,Fe,Je,gn,ct,Ol,un,Nl,fr,q,mt,Dl,ft,Vl,ht,Wl,Bl,Sl,D,pt,Ul,ke,Rl,_a,Hl,Gl,_n,Zl,Kl,Xl,Qe,Jl,Ye,Ql,Y,gt,Yl,$e,ed,va,od,td,vn,ad,nd,rd,eo,sd,ee,ut,id,Te,ld,ba,dd,cd,bn,md,fd,hd,oo,hr,we,to,Fn,_t,pd,kn,gd,pr,C,vt,ud,bt,_d,$n,vd,bd,Fd,Ft,kd,kt,$d,Td,wd,Fa,$t,xd,ka,Tt,yd,$a,wt,gr,xe,ao,Tn,xt,Md,wn,zd,ur,H,yt,Ed,Mt,Cd,zt,Pd,Id,Ad,V,Et,jd,ye,qd,Ta,Ld,Od,xn,Nd,Dd,Vd,no,Wd,ro,_r,Me,so,yn,Ct,Bd,Mn,Sd,vr,G,Pt,Ud,It,Rd,At,Hd,Gd,Zd,W,jt,Kd,ze,Xd,wa,Jd,Qd,zn,Yd,ec,oc,io,tc,lo,br,Ee,co,En,qt,ac,Cn,nc,Fr,Z,Lt,rc,Ot,sc,Nt,ic,lc,dc,B,Dt,cc,Ce,mc,xa,fc,hc,Pn,pc,gc,uc,mo,_c,fo,kr;return l=new L({}),ko=new L({}),xo=new L({}),yo=new M({props:{name:"class transformers.FlavaConfig",anchor:"transformers.FlavaConfig",parameters:[{name:"image_config_dict",val:": typing.Dict[str, typing.Any] = None"},{name:"text_config_dict",val:": typing.Dict[str, typing.Any] = None"},{name:"multimodal_config_dict",val:": typing.Dict[str, typing.Any] = None"},{name:"image_codebook_config_dict",val:": typing.Dict[str, typing.Any] = None"},{name:"hidden_size",val:": int = 768"},{name:"layer_norm_eps",val:": float = 1e-12"},{name:"projection_dim",val:": int = 768"},{name:"init_codebook",val:": bool = True"},{name:"logit_scale_init_value",val:": float = 2.6592"},{name:"initializer_range",val:": float = 0.02"},{name:"ce_ignore_index",val:": int = -100"},{name:"mim_weight",val:": float = 1.0"},{name:"mlm_weight",val:": float = 1.0"},{name:"global_contrastive_weight",val:": float = 1.0"},{name:"itm_weight",val:": float = 1.0"},{name:"mmm_image_weight",val:": float = 1.0"},{name:"mmm_text_weight",val:": float = 1.0"},{name:"global_backprop_contrastive",val:": bool = True"},{name:"skip_unmasked_multimodal_encoder",val:": bool = True"},{name:"return_loss",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaConfig.text_config_dict",description:`<strong>text_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaTextConfig">FlavaTextConfig</a>.`,name:"text_config_dict"},{anchor:"transformers.FlavaConfig.image_config_dict",description:`<strong>image_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaImageConfig">FlavaImageConfig</a>.`,name:"image_config_dict"},{anchor:"transformers.FlavaConfig.multimodal_config_dict",description:`<strong>multimodal_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaMultimodalConfig">FlavaMultimodalConfig</a>.`,name:"multimodal_config_dict"},{anchor:"transformers.FlavaConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FlavaConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FlavaConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimentionality of text and image projection layers.`,name:"projection_dim"},{anchor:"transformers.FlavaConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> paramter. Default is used as per the original FLAVA/CLIP
implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.FlavaConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FlavaConfig.ce_ignore_index",description:`<strong>ce_ignore_index</strong> (<code>int</code>, <em>optional</em>, defaults to -100) &#x2014;
Cross entropy index to ignore.`,name:"ce_ignore_index"},{anchor:"transformers.FlavaConfig.mim_weight",description:`<strong>mim_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MIM (Masked Image Modeling) unimodal loss`,name:"mim_weight"},{anchor:"transformers.FlavaConfig.mlm_weight",description:`<strong>mlm_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MLM (Masked Language Modeling) unimodal loss`,name:"mlm_weight"},{anchor:"transformers.FlavaConfig.global_contrastive_weight",description:`<strong>global_contrastive_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to global contrastive cross-alignment loss.`,name:"global_contrastive_weight"},{anchor:"transformers.FlavaConfig.itm_weight",description:`<strong>itm_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to image-text matching multimodal loss.`,name:"itm_weight"},{anchor:"transformers.FlavaConfig.mmm_image_weight",description:`<strong>mmm_image_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MMM loss&#x2019;s image part.`,name:"mmm_image_weight"},{anchor:"transformers.FlavaConfig.mmm_text_weight",description:`<strong>mmm_text_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MMM loss&#x2019;s text part.`,name:"mmm_text_weight"},{anchor:"transformers.FlavaConfig.global_backprop_contrastive",description:`<strong>global_backprop_contrastive</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use global backpropgation through all workers in contrastive loss.`,name:"global_backprop_contrastive"},{anchor:"transformers.FlavaConfig.skip_unmasked_multimodal_encoder",description:`<strong>skip_unmasked_multimodal_encoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to skip running unmasked multimodal encoder whose outputs are not used by FLAVA losses.`,name:"skip_unmasked_multimodal_encoder"},{anchor:"transformers.FlavaConfig.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to return loss or not`,name:"return_loss"},{anchor:"transformers.FlavaConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/configuration_flava.py#L463"}}),qe=new vo({props:{anchor:"transformers.FlavaConfig.example",$$slots:{default:[ff]},$$scope:{ctx:y}}}),zo=new M({props:{name:"from_configs",anchor:"transformers.FlavaConfig.from_configs",parameters:[{name:"image_config",val:": FlavaImageConfig"},{name:"text_config",val:": FlavaTextConfig"},{name:"multimodal_config",val:": FlavaMultimodalConfig"},{name:"image_codebook_config",val:": FlavaImageCodebookConfig"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/configuration_flava.py#L608",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaConfig"
>FlavaConfig</a></p>
`}}),Co=new M({props:{name:"to_dict",anchor:"transformers.FlavaConfig.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/configuration_flava.py#L633",returnDescription:`
<p>Dictionary of all the attributes that make up this configuration instance,</p>
`,returnType:`
<p><code>Dict[str, any]</code></p>
`}}),Io=new L({}),Ao=new M({props:{name:"class transformers.FlavaTextConfig",anchor:"transformers.FlavaTextConfig",parameters:[{name:"vocab_size",val:": int = 30522"},{name:"type_vocab_size",val:": int = 2"},{name:"max_position_embeddings",val:": int = 512"},{name:"position_embedding_type",val:": str = 'absolute'"},{name:"hidden_size",val:": int = 768"},{name:"num_hidden_layers",val:": int = 12"},{name:"num_attention_heads",val:": int = 12"},{name:"intermediate_size",val:": int = 3072"},{name:"hidden_act",val:": str = 'gelu'"},{name:"hidden_dropout_prob",val:": float = 0.0"},{name:"attention_probs_dropout_prob",val:": float = 0.0"},{name:"initializer_range",val:": float = 0.02"},{name:"layer_norm_eps",val:": float = 1e-12"},{name:"pad_token_id",val:": int = 0"},{name:"qkv_bias",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaTextModel">FlavaTextModel</a>.`,name:"vocab_size"},{anchor:"transformers.FlavaTextConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaTextModel">FlavaTextModel</a>. Note that even though
text encoder allows <code>token_type_ids</code>&#x2019;s value as 2, for text-only pretraining and fine-tuning, only 1 is
used similar to RoBERTa.`,name:"type_vocab_size"},{anchor:"transformers.FlavaTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048). For VL, max_length passed to model is 77.`,name:"max_position_embeddings"},{anchor:"transformers.FlavaTextConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.FlavaTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FlavaTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FlavaTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FlavaTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FlavaTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FlavaTextConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FlavaTextConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FlavaTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FlavaTextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FlavaTextConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.FlavaTextConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.FlavaTextConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.FlavaTextConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/configuration_flava.py#L150"}}),De=new vo({props:{anchor:"transformers.FlavaTextConfig.example",$$slots:{default:[hf]},$$scope:{ctx:y}}}),Oo=new L({}),No=new M({props:{name:"class transformers.FlavaImageConfig",anchor:"transformers.FlavaImageConfig",parameters:[{name:"hidden_size",val:": int = 768"},{name:"num_hidden_layers",val:": int = 12"},{name:"num_attention_heads",val:": int = 12"},{name:"intermediate_size",val:": int = 3072"},{name:"hidden_act",val:": int = 'gelu'"},{name:"hidden_dropout_prob",val:": float = 0.0"},{name:"attention_probs_dropout_prob",val:": float = 0.0"},{name:"initializer_range",val:": float = 0.02"},{name:"layer_norm_eps",val:": float = 1e-12"},{name:"image_size",val:": int = 224"},{name:"patch_size",val:": int = 16"},{name:"num_channels",val:": int = 3"},{name:"qkv_bias",val:": bool = True"},{name:"mask_token",val:": bool = True"},{name:"vocab_size",val:": int = 8192"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaImageConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FlavaImageConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FlavaImageConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FlavaImageConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FlavaImageConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FlavaImageConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FlavaImageConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FlavaImageConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FlavaImageConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FlavaImageConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.FlavaImageConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.FlavaImageConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.FlavaImageConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.FlavaImageConfig.mask_token",description:`<strong>mask_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use a mask token or not. Used in MIM (Masked Image Modeling) loss for FLAVA.`,name:"mask_token"},{anchor:"transformers.FlavaImageConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8192) &#x2014;
Vocabulary size of the <a href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaImageCodebook">FlavaImageCodebook</a> used in conjunction with <a href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaImageModel">FlavaImageModel</a> for MIM (Masked
Image Modeling) loss for FLAVA.`,name:"vocab_size"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/configuration_flava.py#L32"}}),We=new vo({props:{anchor:"transformers.FlavaImageConfig.example",$$slots:{default:[pf]},$$scope:{ctx:y}}}),Bo=new L({}),So=new M({props:{name:"class transformers.FlavaMultimodalConfig",anchor:"transformers.FlavaMultimodalConfig",parameters:[{name:"hidden_size",val:": int = 768"},{name:"num_hidden_layers",val:": int = 6"},{name:"num_attention_heads",val:": int = 12"},{name:"intermediate_size",val:": int = 3072"},{name:"hidden_act",val:": int = 'gelu'"},{name:"hidden_dropout_prob",val:": int = 0.0"},{name:"attention_probs_dropout_prob",val:": int = 0.0"},{name:"initializer_range",val:": float = 0.02"},{name:"layer_norm_eps",val:": float = 1e-12"},{name:"qkv_bias",val:": bool = True"},{name:"use_cls_token",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaMultimodalConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FlavaMultimodalConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FlavaMultimodalConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FlavaMultimodalConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FlavaMultimodalConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FlavaMultimodalConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FlavaMultimodalConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FlavaMultimodalConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FlavaMultimodalConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FlavaMultimodalConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.FlavaMultimodalConfig.use_cls_token",description:`<strong>use_cls_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use an extra CLS token for multimodal settings. Usually needed by the FLAVA model.`,name:"use_cls_token"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/configuration_flava.py#L278"}}),Se=new vo({props:{anchor:"transformers.FlavaMultimodalConfig.example",$$slots:{default:[gf]},$$scope:{ctx:y}}}),Go=new L({}),Ko=new M({props:{name:"class transformers.FlavaImageCodebookConfig",anchor:"transformers.FlavaImageCodebookConfig",parameters:[{name:"num_groups",val:": int = 4"},{name:"input_channels",val:": int = 3"},{name:"num_blocks_per_group",val:": int = 2"},{name:"hidden_size",val:": int = 256"},{name:"vocab_size",val:": int = 8192"},{name:"freeze",val:": int = True"},{name:"initializer_range",val:": float = 0.02"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/configuration_flava.py#L379"}}),Xo=new L({}),Jo=new M({props:{name:"class transformers.FlavaProcessor",anchor:"transformers.FlavaProcessor",parameters:[{name:"feature_extractor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.FlavaProcessor.feature_extractor",description:'<strong>feature_extractor</strong> (<a href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaFeatureExtractor">FlavaFeatureExtractor</a>) &#x2014; The feature extractor is a required input.',name:"feature_extractor"},{anchor:"transformers.FlavaProcessor.tokenizer",description:'<strong>tokenizer</strong> (<a href="/docs/transformers/v4.22.1/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a>) &#x2014; The tokenizer is a required input.',name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/processing_flava.py#L26"}}),Qo=new M({props:{name:"batch_decode",anchor:"transformers.FlavaProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/processing_flava.py#L112"}}),et=new M({props:{name:"decode",anchor:"transformers.FlavaProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/processing_flava.py#L119"}}),tt=new L({}),at=new M({props:{name:"class transformers.FlavaFeatureExtractor",anchor:"transformers.FlavaFeatureExtractor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": typing.Union[int, typing.Tuple[int, int]] = 224"},{name:"resample",val:": int = <Resampling.BICUBIC: 3>"},{name:"do_center_crop",val:": bool = True"},{name:"crop_size",val:": typing.Union[int, typing.Tuple[int, int]] = 224"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": typing.Tuple[float, float, float] = [0.48145466, 0.4578275, 0.40821073]"},{name:"image_std",val:": typing.Tuple[float, float, float] = [0.26862954, 0.26130258, 0.27577711]"},{name:"input_size_patches",val:": int = 14"},{name:"total_mask_patches",val:": int = 75"},{name:"mask_group_min_patches",val:": int = 16"},{name:"mask_group_max_patches",val:": typing.Optional[int] = None"},{name:"mask_group_min_aspect_ratio",val:": float = 0.3"},{name:"mask_group_max_aspect_ratio",val:": typing.Optional[float] = None"},{name:"codebook_do_resize",val:": bool = True"},{name:"codebook_size",val:": bool = 112"},{name:"codebook_resample",val:": int = <Resampling.LANCZOS: 1>"},{name:"codebook_do_center_crop",val:": bool = True"},{name:"codebook_crop_size",val:": int = 112"},{name:"codebook_do_map_pixels",val:": bool = True"},{name:"codebook_do_normalize",val:": bool = True"},{name:"codebook_image_mean",val:": typing.Tuple[float, float, float] = [0.0, 0.0, 0.0]"},{name:"codebook_image_std",val:": typing.Tuple[float, float, float] = [1.0, 1.0, 1.0]"},{name:"**kwargs",val:": typing.Any"}],parametersDescription:[{anchor:"transformers.FlavaFeatureExtractor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the input to a certain <code>size</code>.`,name:"do_resize"},{anchor:"transformers.FlavaFeatureExtractor.size",description:`<strong>size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
Resize the input to the given size. Only has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"size"},{anchor:"transformers.FlavaFeatureExtractor.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.BICUBIC</code>) &#x2014;
An optional resampling filter. This can be one of <code>PIL.Image.NEAREST</code>, <code>PIL.Image.BOX</code>,
<code>PIL.Image.BILINEAR</code>, <code>PIL.Image.HAMMING</code>, <code>PIL.Image.BICUBIC</code> or <code>PIL.Image.LANCZOS</code>. Only has an effect`,name:"resample"},{anchor:"transformers.FlavaFeatureExtractor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to crop the input at the center. If the input size is smaller than <code>crop_size</code> along any edge, the
image is padded with 0&#x2019;s and then center cropped.`,name:"do_center_crop"},{anchor:"transformers.FlavaFeatureExtractor.crop_size",description:`<strong>crop_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
Desired output size when applying center-cropping. Only has an effect if <code>do_center_crop</code> is set to <code>True</code>.`,name:"crop_size"},{anchor:"transformers.FlavaFeatureExtractor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input with <code>image_mean</code> and <code>image_std</code>.`,name:"do_normalize"},{anchor:"transformers.FlavaFeatureExtractor.image_mean",description:`<strong>image_mean</strong> (<code>Tuple[float, float, float]</code>, <em>optional</em>, defaults to <code>[0.485, 0.456, 0.406]</code>) &#x2014;
The sequence of means for each channel, to be used when normalizing images.`,name:"image_mean"},{anchor:"transformers.FlavaFeatureExtractor.image_std",description:`<strong>image_std</strong> (<code>Tuple[float, float, float]</code>, <em>optional</em>, defaults to <code>[0.229, 0.224, 0.225]</code>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images.`,name:"image_std"},{anchor:"transformers.FlavaFeatureExtractor.input_size_patches",description:`<strong>input_size_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 14) &#x2014;
Number of patches in the image in height and width direction. 14x14 = 196 total patches.`,name:"input_size_patches"},{anchor:"transformers.FlavaFeatureExtractor.total_mask_patches",description:`<strong>total_mask_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 75) &#x2014;
Total number of patches that should be masked.`,name:"total_mask_patches"},{anchor:"transformers.FlavaFeatureExtractor.mask_group_min_patches",description:`<strong>mask_group_min_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Minimum number of patches that should be masked.`,name:"mask_group_min_patches"},{anchor:"transformers.FlavaFeatureExtractor.mask_group_max_patches",description:`<strong>mask_group_max_patches</strong> (<code>int</code>, <em>optional</em>, defaults to None) &#x2014;
Maximum number of patches that should be masked.`,name:"mask_group_max_patches"},{anchor:"transformers.FlavaFeatureExtractor.mask_group_min_aspect_ratio",description:`<strong>mask_group_min_aspect_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 0.3) &#x2014;
Minimum aspect ratio of the mask window.`,name:"mask_group_min_aspect_ratio"},{anchor:"transformers.FlavaFeatureExtractor.mask_group_max_aspect_ratio",description:`<strong>mask_group_max_aspect_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to None) &#x2014;
Maximum aspect ratio of the mask window`,name:"mask_group_max_aspect_ratio"},{anchor:"transformers.FlavaFeatureExtractor.codebook_do_resize",description:`<strong>codebook_do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the input for codebook to a certain <code>codebook_size</code>.`,name:"codebook_do_resize"},{anchor:"transformers.FlavaFeatureExtractor.codebook_size",description:`<strong>codebook_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
Resize the input for codebook to the given size. Only has an effect if <code>codebook_do_resize</code> is set to
<code>True</code>.`,name:"codebook_size"},{anchor:"transformers.FlavaFeatureExtractor.codebook_resample",description:`<strong>codebook_resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.BICUBIC</code>) &#x2014;
An optional resampling filter. This can be one of <code>PIL.Image.NEAREST</code>, <code>PIL.Image.BOX</code>,
<code>PIL.Image.BILINEAR</code>, <code>PIL.Image.HAMMING</code>, <code>PIL.Image.BICUBIC</code> or <code>PIL.Image.LANCZOS</code>. Only has an effect`,name:"codebook_resample"},{anchor:"transformers.FlavaFeatureExtractor.codebook_do_center_crop",description:`<strong>codebook_do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to crop the input for codebook at the center. If the input size is smaller than
<code>codebook_crop_size</code> along any edge, the image is padded with 0&#x2019;s and then center cropped.`,name:"codebook_do_center_crop"},{anchor:"transformers.FlavaFeatureExtractor.codebook_crop_size",description:`<strong>codebook_crop_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
Desired output size for codebook input when applying center-cropping. Only has an effect if
<code>codebook_do_center_crop</code> is set to <code>True</code>.`,name:"codebook_crop_size"},{anchor:"transformers.FlavaFeatureExtractor.codebook_do_normalize",description:`<strong>codebook_do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input for codebook with <code>codebook_image_mean</code> and <code>codebook_image_std</code>.`,name:"codebook_do_normalize"},{anchor:"transformers.FlavaFeatureExtractor.codebook_image_mean",description:`<strong>codebook_image_mean</strong> (<code>Tuple[float, float, float]</code>, <em>optional</em>, defaults to <code>[0, 0, 0]</code>) &#x2014;
The sequence of means for each channel, to be used when normalizing images for codebook.`,name:"codebook_image_mean"},{anchor:"transformers.FlavaFeatureExtractor.codebook_image_std",description:`<strong>codebook_image_std</strong> (<code>Tuple[float, float, float]</code>, <em>optional</em>, defaults to <code>[0.5, 0.5, 0.5]</code>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images for codebook.`,name:"codebook_image_std"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/feature_extraction_flava.py#L120"}}),rt=new L({}),st=new M({props:{name:"class transformers.FlavaForPreTraining",anchor:"transformers.FlavaForPreTraining",parameters:[{name:"config",val:": FlavaConfig"},{name:"image_codebook",val:": typing.Optional[torch.nn.modules.module.Module] = None"}],parametersDescription:[{anchor:"transformers.FlavaForPreTraining.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaConfig">FlavaConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.22.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlavaForPreTraining.image_codebook",description:`<strong>image_codebook</strong> (<code>nn.Module</code>) &#x2014; If passed, the image codebook will be set to this. Otherwise. it will
be initialized using the image_codebook_config defined in the config first as the first parameter.`,name:"image_codebook"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/modeling_flava.py#L1735"}}),dt=new M({props:{name:"forward",anchor:"transformers.FlavaForPreTraining.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"input_ids_masked",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"codebook_pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"image_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"skip_unmasked_multimodal_encoder",val:": bool = None"},{name:"mlm_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"mim_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"itm_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": bool = True"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"return_loss",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaForPreTraining.forward.input_ids_masked",description:`<strong>input_ids_masked</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. These ones are the masked version of the original task
to be used with MLM. Indices can be obtained using <a href="/docs/transformers/v4.22.1/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> along with
<code>DataCollatorForMaskedLanguageModeling</code>. See <a href="/docs/transformers/v4.22.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.22.1/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids_masked"},{anchor:"transformers.FlavaForPreTraining.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/v4.22.1/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/v4.22.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.22.1/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlavaForPreTraining.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FlavaForPreTraining.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaFeatureExtractor">FlavaFeatureExtractor</a>. See
<code>FlavaFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.FlavaForPreTraining.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FlavaForPreTraining.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FlavaForPreTraining.forward.image_attention_mask",description:`<strong>image_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices specifically for images. Mask values selected
in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"image_attention_mask"},{anchor:"transformers.FlavaForPreTraining.forward.skip_unmasked_multimodal_encoder",description:`<strong>skip_unmasked_multimodal_encoder</strong> (<em>bool</em>, <em>optional</em>) &#x2014;
Skip any calculations for multimodal encoder for unmasked inputs. FLAVA pretraining doesn&#x2019;t need unmasked
multimodal embeddings or outputs as of now.`,name:"skip_unmasked_multimodal_encoder"},{anchor:"transformers.FlavaForPreTraining.forward.mlm_labels",description:`<strong>mlm_labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>, <em>optional</em>) &#x2014;
Labels for computing the left-to-right language and multimodal masked modeling loss (next word prediction).
Indices should be in <code>[-100, 0, ..., text_config.vocab_size - 1]</code> (see <code>input_ids</code> docstring). Tokens with
indices set to <code>-100</code> are ignored (masked), the loss is only computed for the tokens with labels in <code>[0, ..., text_config.vocab_size - 1]</code>.`,name:"mlm_labels"},{anchor:"transformers.FlavaForPreTraining.forward.mim_labels",description:`<strong>mim_labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Labels for computing the image and multimodal masked modeling loss. Indices should be in <code>[-100, 0, ..., image_config.vocab_size - 1]</code>. Tokens with indices set to <code>-100</code> are ignored (masked), the loss is only
computed for the tokens with labels in <code>[0, ..., image_config.vocab_size - 1]</code>. If not passed, they are
generated automatically using the image codebook assigned to the model. By default, it uses
<a href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaImageCodebook">FlavaImageCodebook</a>. See <a href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaImageCodebook">FlavaImageCodebook</a> to understand how to generate mim_labels.`,name:"mim_labels"},{anchor:"transformers.FlavaForPreTraining.forward.itm_labels",description:`<strong>itm_labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, 1)</code>, <em>optional</em>) &#x2014;
Labels for computing the image-text matching loss. 0 means the pairs don&#x2019;t match and 1 means they match.
The pairs with 0 will be skipped for calculation of MMM and global contrastive losses as well.`,name:"itm_labels"},{anchor:"transformers.FlavaForPreTraining.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>, default to None) &#x2014;
Whether to return calculated loss or not.`,name:"return_loss"},{anchor:"transformers.FlavaForPreTraining.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_len)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaForPreTraining.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaForPreTraining.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaForPreTraining.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaForPreTraining.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.22.1/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.</p>
<p>Examples &#x2014;`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/modeling_flava.py#L1771",returnDescription:`
<p>A <code>transformers.models.flava.modeling_flava.FlavaForPreTrainingOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.flava.configuration_flava.FlavaConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code>, <em>optional</em>, returned when <code>return_loss</code> is True) \u2014 Total loss calculated for this model.</p>
</li>
<li>
<p><strong>loss_info</strong> (<code>FlavaLosses</code>) \u2014 Detailed info for FLAVA Pretraining losses. Check <code>FlavaLosses</code> class description for the information on
the keys.</p>
</li>
<li>
<p><strong>image_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>.</p>
</li>
<li>
<p><strong>image_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>.</p>
</li>
<li>
<p><strong>text_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>text_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The output of the <a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_unmasked_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_output</strong> (<code>BaseModelOutputWithPooling</code>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_unmasked_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The output of the <a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaMultimodalModel"
>FlavaMultimodalModel</a>.</p>
</li>
<li>
<p><strong>image_masked_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>. Uses <code>bool_masked_pos</code>
to create masked images.</p>
</li>
<li>
<p><strong>image_masked_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>. Uses <code>bool_masked_pos</code> to create masked images.</p>
</li>
<li>
<p><strong>text_masked_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>text_masked_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>input_ids_masked</code> are present) \u2014 The output of the <a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_masked_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_masked_output</strong> (<code>BaseModelOutputWithPooling</code>, returned when <code>input_ids_masked</code> and <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaMultimodalModel"
>FlavaMultimodalModel</a>.</p>
</li>
<li>
<p><strong>mim_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_image_patches, image_vocab_size)</code> or of shape <code>(total_masked_patches, image_vocab_size)</code> , <em>optional</em>, returned when <code>pixel_values</code> are present and <code>input_ids_masked</code> are not) \u2014 The logits for MIM unimodal loss. Uses <code>book_masked_pos</code> to get masked patches. The flattened output is
returned when <code>bool_masked_pos</code> has some of the patches masked.</p>
</li>
<li>
<p><strong>mlm_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length, text_vocab_size)</code> or of shape <code>(total_masked_seq_length, text_vocab_size)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> are present and <code>pixel_values</code> are not) \u2014 The logits for MLM unimodal loss. The flattened output is returned when <code>input_ids_masked</code> has some of
the tokens masked.</p>
</li>
<li>
<p><strong>itm_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, 2)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> and <code>pixel_values</code> are present) \u2014 The logits for ITM loss. Note that ITM loss is calculated on masked pairs in FLAVA.</p>
</li>
<li>
<p><strong>mmm_image_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_image_patches, image_vocab_size)</code> or of shape<code>(total_masked_patches, image_vocab_size)</code>, <em>optional</em>, returned when <code>pixel_values</code> and <code>input_ids_masked</code> are present) \u2014 The logits for MMM image multimodal loss. Uses <code>book_masked_pos</code> to get masked patches. The flattened
output is returned when <code>bool_masked_pos</code> has some of the patches masked.</p>
</li>
<li>
<p><strong>mmm_text_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length, text_vocab_size)</code> or of shape <code>(</code>(total_masked_seq_length, text_vocab_size)<code>), *optional*, returned when </code>pixel_values<code>and</code>input_ids_masked<code>are present) -- The logits for MMM text multimodal loss. The flattened output is returned when</code>input_ids_masked\` has
some of the tokens masked.</p>
</li>
<li>
<p><strong>contrastive_logits_per_image</strong> (<code>torch.FloatTensor</code> of shape <code>(image_batch_size, text_batch_size)</code>) \u2014 The scaled dot product scores between <code>image_embeddings</code> and <code>text_embeddings</code> but passed through FLAVA\u2019s
<code>image_projection</code> and <code>text_projection</code> layers respectively. This represents the image-text similarity
scores. This is calculated on unmasked images and texts.</p>
</li>
<li>
<p><strong>contrastive_logits_per_text</strong> (<code>torch.FloatTensor</code> of shape <code>(text_batch_size, image_batch_size)</code>) \u2014 The scaled dot product scores between <code>text_embeddings</code> and <code>image_embeddings</code> but passed through FLAVA\u2019s
<code>text_projection</code> and <code>image_projection</code> layers respectively. This is calculated on unmasked images and
texts.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.flava.modeling_flava.FlavaForPreTrainingOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Xe=new Ut({props:{$$slots:{default:[uf]},$$scope:{ctx:y}}}),ct=new L({}),mt=new M({props:{name:"class transformers.FlavaModel",anchor:"transformers.FlavaModel",parameters:[{name:"config",val:": FlavaConfig"}],parametersDescription:[{anchor:"transformers.FlavaModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaConfig">FlavaConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.22.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/modeling_flava.py#L1203"}}),pt=new M({props:{name:"forward",anchor:"transformers.FlavaModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"image_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"skip_multimodal_encoder",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": bool = True"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaFeatureExtractor">FlavaFeatureExtractor</a>. See
<code>FlavaFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.FlavaModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FlavaModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FlavaModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/v4.22.1/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/v4.22.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.22.1/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlavaModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FlavaModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.22.1/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FlavaModel.forward.skip_multimodal_encoder",description:`<strong>skip_multimodal_encoder</strong> (<em>bool</em>, <em>optional</em>) &#x2014;
Skip any calculations for multimodal encoder. Useful if multimodal encoding is not going to be used.`,name:"skip_multimodal_encoder"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/modeling_flava.py#L1347",returnDescription:`
<p>A <code>transformers.models.flava.modeling_flava.FlavaModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.flava.configuration_flava.FlavaConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>image_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>.</li>
<li><strong>image_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>.</li>
<li><strong>text_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</li>
<li><strong>text_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The output of the <a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</li>
<li><strong>multimodal_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</li>
<li><strong>multimodal_output</strong> (<code>BaseModelOutputWithPooling</code>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The output of the <a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaMultimodalModel"
>FlavaMultimodalModel</a>.</li>
</ul>
`,returnType:`
<p><code>transformers.models.flava.modeling_flava.FlavaModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Qe=new Ut({props:{$$slots:{default:[_f]},$$scope:{ctx:y}}}),Ye=new vo({props:{anchor:"transformers.FlavaModel.forward.example",$$slots:{default:[vf]},$$scope:{ctx:y}}}),gt=new M({props:{name:"get_text_features",anchor:"transformers.FlavaModel.get_text_features",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/v4.22.1/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/v4.22.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.22.1/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlavaModel.get_text_features.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FlavaModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaModel.get_text_features.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.22.1/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/modeling_flava.py#L1249"}}),eo=new Ut({props:{$$slots:{default:[bf]},$$scope:{ctx:y}}}),ut=new M({props:{name:"get_image_features",anchor:"transformers.FlavaModel.get_image_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"interpolate_pos_encoding",val:": typing.Optional[bool] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaFeatureExtractor">FlavaFeatureExtractor</a>. See
<code>FlavaFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.FlavaModel.get_image_features.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FlavaModel.get_image_features.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FlavaModel.get_image_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaModel.get_image_features.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.22.1/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/modeling_flava.py#L1295"}}),oo=new Ut({props:{$$slots:{default:[Ff]},$$scope:{ctx:y}}}),_t=new L({}),vt=new M({props:{name:"class transformers.FlavaImageCodebook",anchor:"transformers.FlavaImageCodebook",parameters:[{name:"config",val:": FlavaImageCodebookConfig"},{name:"**kwargs",val:": typing.Any"}],parametersDescription:[{anchor:"transformers.FlavaImageCodebook.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaImageCodebookConfig">FlavaImageCodebookConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.22.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/modeling_flava.py#L1521"}}),$t=new M({props:{name:"forward",anchor:"transformers.FlavaImageCodebook.forward",parameters:[{name:"pixel_values",val:": FloatTensor"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/modeling_flava.py#L1605"}}),Tt=new M({props:{name:"get_codebook_indices",anchor:"transformers.FlavaImageCodebook.get_codebook_indices",parameters:[{name:"pixel_values",val:": Tensor"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/modeling_flava.py#L1571"}}),wt=new M({props:{name:"get_codebook_probs",anchor:"transformers.FlavaImageCodebook.get_codebook_probs",parameters:[{name:"pixel_values",val:": Tensor"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/modeling_flava.py#L1601"}}),xt=new L({}),yt=new M({props:{name:"class transformers.FlavaTextModel",anchor:"transformers.FlavaTextModel",parameters:[{name:"config",val:": FlavaTextConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.FlavaTextModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaTextConfig">FlavaTextConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.22.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/modeling_flava.py#L998"}}),Et=new M({props:{name:"forward",anchor:"transformers.FlavaTextModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/v4.22.1/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/v4.22.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.22.1/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlavaTextModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FlavaTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaTextModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.22.1/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/modeling_flava.py#L1029",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.22.1/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaTextConfig"
>FlavaTextConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.22.1/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),no=new Ut({props:{$$slots:{default:[kf]},$$scope:{ctx:y}}}),ro=new vo({props:{anchor:"transformers.FlavaTextModel.forward.example",$$slots:{default:[$f]},$$scope:{ctx:y}}}),Ct=new L({}),Pt=new M({props:{name:"class transformers.FlavaImageModel",anchor:"transformers.FlavaImageModel",parameters:[{name:"config",val:": FlavaImageConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.FlavaImageModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaImageConfig">FlavaImageConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.22.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/modeling_flava.py#L898"}}),jt=new M({props:{name:"forward",anchor:"transformers.FlavaImageModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"interpolate_pos_encoding",val:": typing.Optional[bool] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaImageModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaFeatureExtractor">FlavaFeatureExtractor</a>. See
<code>FlavaFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.FlavaImageModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FlavaImageModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FlavaImageModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaImageModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaImageModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaImageModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaImageModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.22.1/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/modeling_flava.py#L931",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.22.1/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaImageConfig"
>FlavaImageConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.22.1/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),io=new Ut({props:{$$slots:{default:[Tf]},$$scope:{ctx:y}}}),lo=new vo({props:{anchor:"transformers.FlavaImageModel.forward.example",$$slots:{default:[wf]},$$scope:{ctx:y}}}),qt=new L({}),Lt=new M({props:{name:"class transformers.FlavaMultimodalModel",anchor:"transformers.FlavaMultimodalModel",parameters:[{name:"config",val:": FlavaMultimodalConfig"},{name:"add_pooling_layer",val:" = True"}],parametersDescription:[{anchor:"transformers.FlavaMultimodalModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaMultimodalConfig">FlavaMultimodalConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.22.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/modeling_flava.py#L1104"}}),Dt=new M({props:{name:"forward",anchor:"transformers.FlavaMultimodalModel.forward",parameters:[{name:"hidden_states",val:": Tensor"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaMultimodalModel.forward.hidden_states",description:`<strong>hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len, hidden_size)</code>) &#x2014;
The concatenated hidden states of unimodal encoders.`,name:"hidden_states"},{anchor:"transformers.FlavaMultimodalModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaMultimodalModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaMultimodalModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaMultimodalModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaMultimodalModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.22.1/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.22.1/src/transformers/models/flava/modeling_flava.py#L1132",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.22.1/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaMultimodalConfig"
>FlavaMultimodalConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.22.1/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),mo=new Ut({props:{$$slots:{default:[xf]},$$scope:{ctx:y}}}),fo=new vo({props:{anchor:"transformers.FlavaMultimodalModel.forward.example",$$slots:{default:[yf]},$$scope:{ctx:y}}}),{c(){c=a("meta"),x=m(),u=a("h1"),p=a("a"),v=a("span"),b(l.$$.fragment),h=m(),z=a("span"),as=s("FLAVA"),Sn=m(),se=a("h2"),Ie=a("a"),Ua=a("span"),b(ko.$$.fragment),ns=m(),Ra=a("span"),rs=s("Overview"),Un=m(),Ae=a("p"),ss=s("The FLAVA model was proposed in "),$o=a("a"),is=s("FLAVA: A Foundational Language And Vision Alignment Model"),ls=s(" by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022."),Rn=m(),Rt=a("p"),ds=s(`The paper aims at creating a single unified foundation model which can work across vision, language
as well as vision-and-language multimodal tasks.`),Hn=m(),Ht=a("p"),cs=s("The abstract from the paper is the following:"),Gn=m(),Gt=a("p"),Ha=a("em"),ms=s(`State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety
of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal
(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising
direction would be to use a single holistic universal model, as a \u201Cfoundation\u201D, that targets all modalities
at once \u2014 a true vision and language foundation model should be good at vision tasks, language tasks, and
cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate
impressive performance on a wide range of 35 tasks spanning these target modalities.`),Zn=m(),X=a("p"),fs=s("This model was contributed by "),To=a("a"),hs=s("aps"),ps=s(". The original code can be found "),wo=a("a"),gs=s("here"),us=s("."),Kn=m(),ie=a("h2"),je=a("a"),Ga=a("span"),b(xo.$$.fragment),_s=m(),Za=a("span"),vs=s("FlavaConfig"),Xn=m(),E=a("div"),b(yo.$$.fragment),bs=m(),J=a("p"),Zt=a("a"),Fs=s("FlavaConfig"),ks=s(" is the configuration class to store the configuration of a "),Kt=a("a"),$s=s("FlavaModel"),Ts=s(`. It is used to
instantiate FLAVA model according to the specified arguments, defining the text model, image model, image codebook
and multimodal model configs. Instantiating a configuration with the defaults will yield a similar configuration to
that of the FLAVA `),Mo=a("a"),ws=s("facebook/flava-full"),xs=s(" architecture."),ys=m(),le=a("p"),Ms=s("Configuration objects inherit from "),Xt=a("a"),zs=s("PretrainedConfig"),Es=s(` and can be used to control the model outputs. Read the
documentation from `),Jt=a("a"),Cs=s("PretrainedConfig"),Ps=s(" for more information."),Is=m(),b(qe.$$.fragment),As=m(),Le=a("div"),b(zo.$$.fragment),js=m(),Eo=a("p"),qs=s("Instantiate a "),Qt=a("a"),Ls=s("FlavaConfig"),Os=s(` (or a derived class) from flava text model configuration, flava image model
configuration, flava multimodal model and flava codebook model configuration.`),Ns=m(),Oe=a("div"),b(Co.$$.fragment),Ds=m(),Po=a("p"),Vs=s("Serializes this instance to a Python dictionary. Override the default "),Yt=a("a"),Ws=s("to_dict()"),Bs=s("."),Jn=m(),de=a("h2"),Ne=a("a"),Ka=a("span"),b(Io.$$.fragment),Ss=m(),Xa=a("span"),Us=s("FlavaTextConfig"),Qn=m(),P=a("div"),b(Ao.$$.fragment),Rs=m(),jo=a("p"),Hs=s("This is the configuration class to store the configuration of a "),ea=a("a"),Gs=s("FlavaTextModel"),Zs=s(`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture.`),Ks=m(),qo=a("p"),Xs=s(`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
`),Lo=a("a"),Js=s("facebook/flava-full"),Qs=s(" architecture."),Ys=m(),ce=a("p"),ei=s("Configuration objects inherit from "),oa=a("a"),oi=s("PretrainedConfig"),ti=s(` and can be used to control the model outputs. Read the
documentation from `),ta=a("a"),ai=s("PretrainedConfig"),ni=s(" for more information."),ri=m(),b(De.$$.fragment),Yn=m(),me=a("h2"),Ve=a("a"),Ja=a("span"),b(Oo.$$.fragment),si=m(),Qa=a("span"),ii=s("FlavaImageConfig"),er=m(),I=a("div"),b(No.$$.fragment),li=m(),Do=a("p"),di=s("This is the configuration class to store the configuration of a "),aa=a("a"),ci=s("FlavaImageModel"),mi=s(`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture.`),fi=m(),Vo=a("p"),hi=s(`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
`),Wo=a("a"),pi=s("facebook/flava-full"),gi=s(" architecture."),ui=m(),fe=a("p"),_i=s("Configuration objects inherit from "),na=a("a"),vi=s("PretrainedConfig"),bi=s(` and can be used to control the model outputs. Read the
documentation from `),ra=a("a"),Fi=s("PretrainedConfig"),ki=s(" for more information."),$i=m(),b(We.$$.fragment),or=m(),he=a("h2"),Be=a("a"),Ya=a("span"),b(Bo.$$.fragment),Ti=m(),en=a("span"),wi=s("FlavaMultimodalConfig"),tr=m(),A=a("div"),b(So.$$.fragment),xi=m(),Uo=a("p"),yi=s("This is the configuration class to store the configuration of a "),sa=a("a"),Mi=s("FlavaMultimodalModel"),zi=s(`. It is used to instantiate
an FLAVA model according to the specified arguments, defining the model architecture.`),Ei=m(),Ro=a("p"),Ci=s(`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
`),Ho=a("a"),Pi=s("facebook/flava-full"),Ii=s(" architecture."),Ai=m(),pe=a("p"),ji=s("Configuration objects inherit from "),ia=a("a"),qi=s("PretrainedConfig"),Li=s(` and can be used to control the model outputs. Read the
documentation from `),la=a("a"),Oi=s("PretrainedConfig"),Ni=s(" for more information."),Di=m(),b(Se.$$.fragment),ar=m(),ge=a("h2"),Ue=a("a"),on=a("span"),b(Go.$$.fragment),Vi=m(),tn=a("span"),Wi=s("FlavaImageCodebookConfig"),nr=m(),Zo=a("div"),b(Ko.$$.fragment),rr=m(),ue=a("h2"),Re=a("a"),an=a("span"),b(Xo.$$.fragment),Bi=m(),nn=a("span"),Si=s("FlavaProcessor"),sr=m(),j=a("div"),b(Jo.$$.fragment),Ui=m(),rn=a("p"),Ri=s("Constructs a FLAVA processor which wraps a FLAVA feature extractor and a FLAVA tokenizer into a single processor."),Hi=m(),O=a("p"),da=a("a"),Gi=s("FlavaProcessor"),Zi=s(" offers all the functionalities of "),ca=a("a"),Ki=s("FlavaFeatureExtractor"),Xi=s(" and "),ma=a("a"),Ji=s("BertTokenizerFast"),Qi=s(`. See the
`),sn=a("code"),Yi=s("__call__()"),el=s(" and "),fa=a("a"),ol=s("decode()"),tl=s(" for more information."),al=m(),He=a("div"),b(Qo.$$.fragment),nl=m(),Yo=a("p"),rl=s("This method forwards all its arguments to BertTokenizerFast\u2019s "),ha=a("a"),sl=s("batch_decode()"),il=s(`. Please
refer to the docstring of this method for more information.`),ll=m(),Ge=a("div"),b(et.$$.fragment),dl=m(),ot=a("p"),cl=s("This method forwards all its arguments to BertTokenizerFast\u2019s "),pa=a("a"),ml=s("decode()"),fl=s(`. Please refer to
the docstring of this method for more information.`),ir=m(),_e=a("h2"),Ze=a("a"),ln=a("span"),b(tt.$$.fragment),hl=m(),dn=a("span"),pl=s("FlavaFeatureExtractor"),lr=m(),R=a("div"),b(at.$$.fragment),gl=m(),cn=a("p"),ul=s("Constructs a FLAVA feature extractor."),_l=m(),nt=a("p"),vl=s("This feature extractor inherits from "),ga=a("a"),bl=s("FeatureExtractionMixin"),Fl=s(` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),dr=m(),ve=a("h2"),Ke=a("a"),mn=a("span"),b(rt.$$.fragment),kl=m(),fn=a("span"),$l=s("FlavaForPreTraining"),cr=m(),N=a("div"),b(st.$$.fragment),Tl=m(),hn=a("p"),wl=s("The FLAVA model for pretraining which outputs losses, embeddings, logits and transformer outputs."),xl=m(),it=a("p"),yl=s("This model is a PyTorch "),lt=a("a"),Ml=s("torch.nn.Module"),zl=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),El=m(),Q=a("div"),b(dt.$$.fragment),Cl=m(),be=a("p"),Pl=s("The "),ua=a("a"),Il=s("FlavaForPreTraining"),Al=s(" forward method, overrides the "),pn=a("code"),jl=s("__call__"),ql=s(" special method."),Ll=m(),b(Xe.$$.fragment),mr=m(),Fe=a("h2"),Je=a("a"),gn=a("span"),b(ct.$$.fragment),Ol=m(),un=a("span"),Nl=s("FlavaModel"),fr=m(),q=a("div"),b(mt.$$.fragment),Dl=m(),ft=a("p"),Vl=s(`The bare FLAVA Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),ht=a("a"),Wl=s("torch.nn.Module"),Bl=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Sl=m(),D=a("div"),b(pt.$$.fragment),Ul=m(),ke=a("p"),Rl=s("The "),_a=a("a"),Hl=s("FlavaModel"),Gl=s(" forward method, overrides the "),_n=a("code"),Zl=s("__call__"),Kl=s(" special method."),Xl=m(),b(Qe.$$.fragment),Jl=m(),b(Ye.$$.fragment),Ql=m(),Y=a("div"),b(gt.$$.fragment),Yl=m(),$e=a("p"),ed=s("The "),va=a("a"),od=s("FlavaModel"),td=s(" forward method, overrides the "),vn=a("code"),ad=s("__call__"),nd=s(" special method."),rd=m(),b(eo.$$.fragment),sd=m(),ee=a("div"),b(ut.$$.fragment),id=m(),Te=a("p"),ld=s("The "),ba=a("a"),dd=s("FlavaModel"),cd=s(" forward method, overrides the "),bn=a("code"),md=s("__call__"),fd=s(" special method."),hd=m(),b(oo.$$.fragment),hr=m(),we=a("h2"),to=a("a"),Fn=a("span"),b(_t.$$.fragment),pd=m(),kn=a("span"),gd=s("FlavaImageCodebook"),pr=m(),C=a("div"),b(vt.$$.fragment),ud=m(),bt=a("p"),_d=s(`The FLAVA\u2019s image codebook model inspired from DALL-E\u2019s original encoder. Outputs raw hidden states and can be used
to generate image tokens for an image based on DALL-E\u2019s vocab. Used to generate labels for MIM. Use
`),$n=a("code"),vd=s("get_codebook_indices"),bd=s(" to get image tokens for an image."),Fd=m(),Ft=a("p"),kd=s("This model is a PyTorch "),kt=a("a"),$d=s("torch.nn.Module"),Td=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),wd=m(),Fa=a("div"),b($t.$$.fragment),xd=m(),ka=a("div"),b(Tt.$$.fragment),yd=m(),$a=a("div"),b(wt.$$.fragment),gr=m(),xe=a("h2"),ao=a("a"),Tn=a("span"),b(xt.$$.fragment),Md=m(),wn=a("span"),zd=s("FlavaTextModel"),ur=m(),H=a("div"),b(yt.$$.fragment),Ed=m(),Mt=a("p"),Cd=s(`The bare FLAVA Text Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),zt=a("a"),Pd=s("torch.nn.Module"),Id=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ad=m(),V=a("div"),b(Et.$$.fragment),jd=m(),ye=a("p"),qd=s("The "),Ta=a("a"),Ld=s("FlavaTextModel"),Od=s(" forward method, overrides the "),xn=a("code"),Nd=s("__call__"),Dd=s(" special method."),Vd=m(),b(no.$$.fragment),Wd=m(),b(ro.$$.fragment),_r=m(),Me=a("h2"),so=a("a"),yn=a("span"),b(Ct.$$.fragment),Bd=m(),Mn=a("span"),Sd=s("FlavaImageModel"),vr=m(),G=a("div"),b(Pt.$$.fragment),Ud=m(),It=a("p"),Rd=s(`The bare FLAVA Image Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),At=a("a"),Hd=s("torch.nn.Module"),Gd=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Zd=m(),W=a("div"),b(jt.$$.fragment),Kd=m(),ze=a("p"),Xd=s("The "),wa=a("a"),Jd=s("FlavaImageModel"),Qd=s(" forward method, overrides the "),zn=a("code"),Yd=s("__call__"),ec=s(" special method."),oc=m(),b(io.$$.fragment),tc=m(),b(lo.$$.fragment),br=m(),Ee=a("h2"),co=a("a"),En=a("span"),b(qt.$$.fragment),ac=m(),Cn=a("span"),nc=s("FlavaMultimodalModel"),Fr=m(),Z=a("div"),b(Lt.$$.fragment),rc=m(),Ot=a("p"),sc=s(`The bare FLAVA Multimodal Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Nt=a("a"),ic=s("torch.nn.Module"),lc=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),dc=m(),B=a("div"),b(Dt.$$.fragment),cc=m(),Ce=a("p"),mc=s("The "),xa=a("a"),fc=s("FlavaMultimodalModel"),hc=s(" forward method, overrides the "),Pn=a("code"),pc=s("__call__"),gc=s(" special method."),uc=m(),b(mo.$$.fragment),_c=m(),b(fo.$$.fragment),this.h()},l(o){const g=cf('[data-svelte="svelte-1phssyn"]',document.head);c=n(g,"META",{name:!0,content:!0}),g.forEach(t),x=f(o),u=n(o,"H1",{class:!0});var Vt=r(u);p=n(Vt,"A",{id:!0,class:!0,href:!0});var In=r(p);v=n(In,"SPAN",{});var An=r(v);F(l.$$.fragment,An),An.forEach(t),In.forEach(t),h=f(Vt),z=n(Vt,"SPAN",{});var jn=r(z);as=i(jn,"FLAVA"),jn.forEach(t),Vt.forEach(t),Sn=f(o),se=n(o,"H2",{class:!0});var Wt=r(se);Ie=n(Wt,"A",{id:!0,class:!0,href:!0});var qn=r(Ie);Ua=n(qn,"SPAN",{});var Ln=r(Ua);F(ko.$$.fragment,Ln),Ln.forEach(t),qn.forEach(t),ns=f(Wt),Ra=n(Wt,"SPAN",{});var On=r(Ra);rs=i(On,"Overview"),On.forEach(t),Wt.forEach(t),Un=f(o),Ae=n(o,"P",{});var Bt=r(Ae);ss=i(Bt,"The FLAVA model was proposed in "),$o=n(Bt,"A",{href:!0,rel:!0});var Nn=r($o);is=i(Nn,"FLAVA: A Foundational Language And Vision Alignment Model"),Nn.forEach(t),ls=i(Bt," by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022."),Bt.forEach(t),Rn=f(o),Rt=n(o,"P",{});var Dn=r(Rt);ds=i(Dn,`The paper aims at creating a single unified foundation model which can work across vision, language
as well as vision-and-language multimodal tasks.`),Dn.forEach(t),Hn=f(o),Ht=n(o,"P",{});var Vn=r(Ht);cs=i(Vn,"The abstract from the paper is the following:"),Vn.forEach(t),Gn=f(o),Gt=n(o,"P",{});var Wn=r(Gt);Ha=n(Wn,"EM",{});var Bn=r(Ha);ms=i(Bn,`State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety
of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal
(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising
direction would be to use a single holistic universal model, as a \u201Cfoundation\u201D, that targets all modalities
at once \u2014 a true vision and language foundation model should be good at vision tasks, language tasks, and
cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate
impressive performance on a wide range of 35 tasks spanning these target modalities.`),Bn.forEach(t),Wn.forEach(t),Zn=f(o),X=n(o,"P",{});var Pe=r(X);fs=i(Pe,"This model was contributed by "),To=n(Pe,"A",{href:!0,rel:!0});var vc=r(To);hs=i(vc,"aps"),vc.forEach(t),ps=i(Pe,". The original code can be found "),wo=n(Pe,"A",{href:!0,rel:!0});var bc=r(wo);gs=i(bc,"here"),bc.forEach(t),us=i(Pe,"."),Pe.forEach(t),Kn=f(o),ie=n(o,"H2",{class:!0});var $r=r(ie);je=n($r,"A",{id:!0,class:!0,href:!0});var Fc=r(je);Ga=n(Fc,"SPAN",{});var kc=r(Ga);F(xo.$$.fragment,kc),kc.forEach(t),Fc.forEach(t),_s=f($r),Za=n($r,"SPAN",{});var $c=r(Za);vs=i($c,"FlavaConfig"),$c.forEach(t),$r.forEach(t),Xn=f(o),E=n(o,"DIV",{class:!0});var S=r(E);F(yo.$$.fragment,S),bs=f(S),J=n(S,"P",{});var St=r(J);Zt=n(St,"A",{href:!0});var Tc=r(Zt);Fs=i(Tc,"FlavaConfig"),Tc.forEach(t),ks=i(St," is the configuration class to store the configuration of a "),Kt=n(St,"A",{href:!0});var wc=r(Kt);$s=i(wc,"FlavaModel"),wc.forEach(t),Ts=i(St,`. It is used to
instantiate FLAVA model according to the specified arguments, defining the text model, image model, image codebook
and multimodal model configs. Instantiating a configuration with the defaults will yield a similar configuration to
that of the FLAVA `),Mo=n(St,"A",{href:!0,rel:!0});var xc=r(Mo);ws=i(xc,"facebook/flava-full"),xc.forEach(t),xs=i(St," architecture."),St.forEach(t),ys=f(S),le=n(S,"P",{});var ya=r(le);Ms=i(ya,"Configuration objects inherit from "),Xt=n(ya,"A",{href:!0});var yc=r(Xt);zs=i(yc,"PretrainedConfig"),yc.forEach(t),Es=i(ya,` and can be used to control the model outputs. Read the
documentation from `),Jt=n(ya,"A",{href:!0});var Mc=r(Jt);Cs=i(Mc,"PretrainedConfig"),Mc.forEach(t),Ps=i(ya," for more information."),ya.forEach(t),Is=f(S),F(qe.$$.fragment,S),As=f(S),Le=n(S,"DIV",{class:!0});var Tr=r(Le);F(zo.$$.fragment,Tr),js=f(Tr),Eo=n(Tr,"P",{});var wr=r(Eo);qs=i(wr,"Instantiate a "),Qt=n(wr,"A",{href:!0});var zc=r(Qt);Ls=i(zc,"FlavaConfig"),zc.forEach(t),Os=i(wr,` (or a derived class) from flava text model configuration, flava image model
configuration, flava multimodal model and flava codebook model configuration.`),wr.forEach(t),Tr.forEach(t),Ns=f(S),Oe=n(S,"DIV",{class:!0});var xr=r(Oe);F(Co.$$.fragment,xr),Ds=f(xr),Po=n(xr,"P",{});var yr=r(Po);Vs=i(yr,"Serializes this instance to a Python dictionary. Override the default "),Yt=n(yr,"A",{href:!0});var Ec=r(Yt);Ws=i(Ec,"to_dict()"),Ec.forEach(t),Bs=i(yr,"."),yr.forEach(t),xr.forEach(t),S.forEach(t),Jn=f(o),de=n(o,"H2",{class:!0});var Mr=r(de);Ne=n(Mr,"A",{id:!0,class:!0,href:!0});var Cc=r(Ne);Ka=n(Cc,"SPAN",{});var Pc=r(Ka);F(Io.$$.fragment,Pc),Pc.forEach(t),Cc.forEach(t),Ss=f(Mr),Xa=n(Mr,"SPAN",{});var Ic=r(Xa);Us=i(Ic,"FlavaTextConfig"),Ic.forEach(t),Mr.forEach(t),Qn=f(o),P=n(o,"DIV",{class:!0});var oe=r(P);F(Ao.$$.fragment,oe),Rs=f(oe),jo=n(oe,"P",{});var zr=r(jo);Hs=i(zr,"This is the configuration class to store the configuration of a "),ea=n(zr,"A",{href:!0});var Ac=r(ea);Gs=i(Ac,"FlavaTextModel"),Ac.forEach(t),Zs=i(zr,`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture.`),zr.forEach(t),Ks=f(oe),qo=n(oe,"P",{});var Er=r(qo);Xs=i(Er,`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
`),Lo=n(Er,"A",{href:!0,rel:!0});var jc=r(Lo);Js=i(jc,"facebook/flava-full"),jc.forEach(t),Qs=i(Er," architecture."),Er.forEach(t),Ys=f(oe),ce=n(oe,"P",{});var Ma=r(ce);ei=i(Ma,"Configuration objects inherit from "),oa=n(Ma,"A",{href:!0});var qc=r(oa);oi=i(qc,"PretrainedConfig"),qc.forEach(t),ti=i(Ma,` and can be used to control the model outputs. Read the
documentation from `),ta=n(Ma,"A",{href:!0});var Lc=r(ta);ai=i(Lc,"PretrainedConfig"),Lc.forEach(t),ni=i(Ma," for more information."),Ma.forEach(t),ri=f(oe),F(De.$$.fragment,oe),oe.forEach(t),Yn=f(o),me=n(o,"H2",{class:!0});var Cr=r(me);Ve=n(Cr,"A",{id:!0,class:!0,href:!0});var Oc=r(Ve);Ja=n(Oc,"SPAN",{});var Nc=r(Ja);F(Oo.$$.fragment,Nc),Nc.forEach(t),Oc.forEach(t),si=f(Cr),Qa=n(Cr,"SPAN",{});var Dc=r(Qa);ii=i(Dc,"FlavaImageConfig"),Dc.forEach(t),Cr.forEach(t),er=f(o),I=n(o,"DIV",{class:!0});var te=r(I);F(No.$$.fragment,te),li=f(te),Do=n(te,"P",{});var Pr=r(Do);di=i(Pr,"This is the configuration class to store the configuration of a "),aa=n(Pr,"A",{href:!0});var Vc=r(aa);ci=i(Vc,"FlavaImageModel"),Vc.forEach(t),mi=i(Pr,`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture.`),Pr.forEach(t),fi=f(te),Vo=n(te,"P",{});var Ir=r(Vo);hi=i(Ir,`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
`),Wo=n(Ir,"A",{href:!0,rel:!0});var Wc=r(Wo);pi=i(Wc,"facebook/flava-full"),Wc.forEach(t),gi=i(Ir," architecture."),Ir.forEach(t),ui=f(te),fe=n(te,"P",{});var za=r(fe);_i=i(za,"Configuration objects inherit from "),na=n(za,"A",{href:!0});var Bc=r(na);vi=i(Bc,"PretrainedConfig"),Bc.forEach(t),bi=i(za,` and can be used to control the model outputs. Read the
documentation from `),ra=n(za,"A",{href:!0});var Sc=r(ra);Fi=i(Sc,"PretrainedConfig"),Sc.forEach(t),ki=i(za," for more information."),za.forEach(t),$i=f(te),F(We.$$.fragment,te),te.forEach(t),or=f(o),he=n(o,"H2",{class:!0});var Ar=r(he);Be=n(Ar,"A",{id:!0,class:!0,href:!0});var Uc=r(Be);Ya=n(Uc,"SPAN",{});var Rc=r(Ya);F(Bo.$$.fragment,Rc),Rc.forEach(t),Uc.forEach(t),Ti=f(Ar),en=n(Ar,"SPAN",{});var Hc=r(en);wi=i(Hc,"FlavaMultimodalConfig"),Hc.forEach(t),Ar.forEach(t),tr=f(o),A=n(o,"DIV",{class:!0});var ae=r(A);F(So.$$.fragment,ae),xi=f(ae),Uo=n(ae,"P",{});var jr=r(Uo);yi=i(jr,"This is the configuration class to store the configuration of a "),sa=n(jr,"A",{href:!0});var Gc=r(sa);Mi=i(Gc,"FlavaMultimodalModel"),Gc.forEach(t),zi=i(jr,`. It is used to instantiate
an FLAVA model according to the specified arguments, defining the model architecture.`),jr.forEach(t),Ei=f(ae),Ro=n(ae,"P",{});var qr=r(Ro);Ci=i(qr,`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
`),Ho=n(qr,"A",{href:!0,rel:!0});var Zc=r(Ho);Pi=i(Zc,"facebook/flava-full"),Zc.forEach(t),Ii=i(qr," architecture."),qr.forEach(t),Ai=f(ae),pe=n(ae,"P",{});var Ea=r(pe);ji=i(Ea,"Configuration objects inherit from "),ia=n(Ea,"A",{href:!0});var Kc=r(ia);qi=i(Kc,"PretrainedConfig"),Kc.forEach(t),Li=i(Ea,` and can be used to control the model outputs. Read the
documentation from `),la=n(Ea,"A",{href:!0});var Xc=r(la);Oi=i(Xc,"PretrainedConfig"),Xc.forEach(t),Ni=i(Ea," for more information."),Ea.forEach(t),Di=f(ae),F(Se.$$.fragment,ae),ae.forEach(t),ar=f(o),ge=n(o,"H2",{class:!0});var Lr=r(ge);Ue=n(Lr,"A",{id:!0,class:!0,href:!0});var Jc=r(Ue);on=n(Jc,"SPAN",{});var Qc=r(on);F(Go.$$.fragment,Qc),Qc.forEach(t),Jc.forEach(t),Vi=f(Lr),tn=n(Lr,"SPAN",{});var Yc=r(tn);Wi=i(Yc,"FlavaImageCodebookConfig"),Yc.forEach(t),Lr.forEach(t),nr=f(o),Zo=n(o,"DIV",{class:!0});var em=r(Zo);F(Ko.$$.fragment,em),em.forEach(t),rr=f(o),ue=n(o,"H2",{class:!0});var Or=r(ue);Re=n(Or,"A",{id:!0,class:!0,href:!0});var om=r(Re);an=n(om,"SPAN",{});var tm=r(an);F(Xo.$$.fragment,tm),tm.forEach(t),om.forEach(t),Bi=f(Or),nn=n(Or,"SPAN",{});var am=r(nn);Si=i(am,"FlavaProcessor"),am.forEach(t),Or.forEach(t),sr=f(o),j=n(o,"DIV",{class:!0});var ne=r(j);F(Jo.$$.fragment,ne),Ui=f(ne),rn=n(ne,"P",{});var nm=r(rn);Ri=i(nm,"Constructs a FLAVA processor which wraps a FLAVA feature extractor and a FLAVA tokenizer into a single processor."),nm.forEach(t),Hi=f(ne),O=n(ne,"P",{});var K=r(O);da=n(K,"A",{href:!0});var rm=r(da);Gi=i(rm,"FlavaProcessor"),rm.forEach(t),Zi=i(K," offers all the functionalities of "),ca=n(K,"A",{href:!0});var sm=r(ca);Ki=i(sm,"FlavaFeatureExtractor"),sm.forEach(t),Xi=i(K," and "),ma=n(K,"A",{href:!0});var im=r(ma);Ji=i(im,"BertTokenizerFast"),im.forEach(t),Qi=i(K,`. See the
`),sn=n(K,"CODE",{});var lm=r(sn);Yi=i(lm,"__call__()"),lm.forEach(t),el=i(K," and "),fa=n(K,"A",{href:!0});var dm=r(fa);ol=i(dm,"decode()"),dm.forEach(t),tl=i(K," for more information."),K.forEach(t),al=f(ne),He=n(ne,"DIV",{class:!0});var Nr=r(He);F(Qo.$$.fragment,Nr),nl=f(Nr),Yo=n(Nr,"P",{});var Dr=r(Yo);rl=i(Dr,"This method forwards all its arguments to BertTokenizerFast\u2019s "),ha=n(Dr,"A",{href:!0});var cm=r(ha);sl=i(cm,"batch_decode()"),cm.forEach(t),il=i(Dr,`. Please
refer to the docstring of this method for more information.`),Dr.forEach(t),Nr.forEach(t),ll=f(ne),Ge=n(ne,"DIV",{class:!0});var Vr=r(Ge);F(et.$$.fragment,Vr),dl=f(Vr),ot=n(Vr,"P",{});var Wr=r(ot);cl=i(Wr,"This method forwards all its arguments to BertTokenizerFast\u2019s "),pa=n(Wr,"A",{href:!0});var mm=r(pa);ml=i(mm,"decode()"),mm.forEach(t),fl=i(Wr,`. Please refer to
the docstring of this method for more information.`),Wr.forEach(t),Vr.forEach(t),ne.forEach(t),ir=f(o),_e=n(o,"H2",{class:!0});var Br=r(_e);Ze=n(Br,"A",{id:!0,class:!0,href:!0});var fm=r(Ze);ln=n(fm,"SPAN",{});var hm=r(ln);F(tt.$$.fragment,hm),hm.forEach(t),fm.forEach(t),hl=f(Br),dn=n(Br,"SPAN",{});var pm=r(dn);pl=i(pm,"FlavaFeatureExtractor"),pm.forEach(t),Br.forEach(t),lr=f(o),R=n(o,"DIV",{class:!0});var Ca=r(R);F(at.$$.fragment,Ca),gl=f(Ca),cn=n(Ca,"P",{});var gm=r(cn);ul=i(gm,"Constructs a FLAVA feature extractor."),gm.forEach(t),_l=f(Ca),nt=n(Ca,"P",{});var Sr=r(nt);vl=i(Sr,"This feature extractor inherits from "),ga=n(Sr,"A",{href:!0});var um=r(ga);bl=i(um,"FeatureExtractionMixin"),um.forEach(t),Fl=i(Sr,` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),Sr.forEach(t),Ca.forEach(t),dr=f(o),ve=n(o,"H2",{class:!0});var Ur=r(ve);Ke=n(Ur,"A",{id:!0,class:!0,href:!0});var _m=r(Ke);mn=n(_m,"SPAN",{});var vm=r(mn);F(rt.$$.fragment,vm),vm.forEach(t),_m.forEach(t),kl=f(Ur),fn=n(Ur,"SPAN",{});var bm=r(fn);$l=i(bm,"FlavaForPreTraining"),bm.forEach(t),Ur.forEach(t),cr=f(o),N=n(o,"DIV",{class:!0});var ho=r(N);F(st.$$.fragment,ho),Tl=f(ho),hn=n(ho,"P",{});var Fm=r(hn);wl=i(Fm,"The FLAVA model for pretraining which outputs losses, embeddings, logits and transformer outputs."),Fm.forEach(t),xl=f(ho),it=n(ho,"P",{});var Rr=r(it);yl=i(Rr,"This model is a PyTorch "),lt=n(Rr,"A",{href:!0,rel:!0});var km=r(lt);Ml=i(km,"torch.nn.Module"),km.forEach(t),zl=i(Rr,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Rr.forEach(t),El=f(ho),Q=n(ho,"DIV",{class:!0});var Pa=r(Q);F(dt.$$.fragment,Pa),Cl=f(Pa),be=n(Pa,"P",{});var Ia=r(be);Pl=i(Ia,"The "),ua=n(Ia,"A",{href:!0});var $m=r(ua);Il=i($m,"FlavaForPreTraining"),$m.forEach(t),Al=i(Ia," forward method, overrides the "),pn=n(Ia,"CODE",{});var Tm=r(pn);jl=i(Tm,"__call__"),Tm.forEach(t),ql=i(Ia," special method."),Ia.forEach(t),Ll=f(Pa),F(Xe.$$.fragment,Pa),Pa.forEach(t),ho.forEach(t),mr=f(o),Fe=n(o,"H2",{class:!0});var Hr=r(Fe);Je=n(Hr,"A",{id:!0,class:!0,href:!0});var wm=r(Je);gn=n(wm,"SPAN",{});var xm=r(gn);F(ct.$$.fragment,xm),xm.forEach(t),wm.forEach(t),Ol=f(Hr),un=n(Hr,"SPAN",{});var ym=r(un);Nl=i(ym,"FlavaModel"),ym.forEach(t),Hr.forEach(t),fr=f(o),q=n(o,"DIV",{class:!0});var re=r(q);F(mt.$$.fragment,re),Dl=f(re),ft=n(re,"P",{});var Gr=r(ft);Vl=i(Gr,`The bare FLAVA Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),ht=n(Gr,"A",{href:!0,rel:!0});var Mm=r(ht);Wl=i(Mm,"torch.nn.Module"),Mm.forEach(t),Bl=i(Gr,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Gr.forEach(t),Sl=f(re),D=n(re,"DIV",{class:!0});var po=r(D);F(pt.$$.fragment,po),Ul=f(po),ke=n(po,"P",{});var Aa=r(ke);Rl=i(Aa,"The "),_a=n(Aa,"A",{href:!0});var zm=r(_a);Hl=i(zm,"FlavaModel"),zm.forEach(t),Gl=i(Aa," forward method, overrides the "),_n=n(Aa,"CODE",{});var Em=r(_n);Zl=i(Em,"__call__"),Em.forEach(t),Kl=i(Aa," special method."),Aa.forEach(t),Xl=f(po),F(Qe.$$.fragment,po),Jl=f(po),F(Ye.$$.fragment,po),po.forEach(t),Ql=f(re),Y=n(re,"DIV",{class:!0});var ja=r(Y);F(gt.$$.fragment,ja),Yl=f(ja),$e=n(ja,"P",{});var qa=r($e);ed=i(qa,"The "),va=n(qa,"A",{href:!0});var Cm=r(va);od=i(Cm,"FlavaModel"),Cm.forEach(t),td=i(qa," forward method, overrides the "),vn=n(qa,"CODE",{});var Pm=r(vn);ad=i(Pm,"__call__"),Pm.forEach(t),nd=i(qa," special method."),qa.forEach(t),rd=f(ja),F(eo.$$.fragment,ja),ja.forEach(t),sd=f(re),ee=n(re,"DIV",{class:!0});var La=r(ee);F(ut.$$.fragment,La),id=f(La),Te=n(La,"P",{});var Oa=r(Te);ld=i(Oa,"The "),ba=n(Oa,"A",{href:!0});var Im=r(ba);dd=i(Im,"FlavaModel"),Im.forEach(t),cd=i(Oa," forward method, overrides the "),bn=n(Oa,"CODE",{});var Am=r(bn);md=i(Am,"__call__"),Am.forEach(t),fd=i(Oa," special method."),Oa.forEach(t),hd=f(La),F(oo.$$.fragment,La),La.forEach(t),re.forEach(t),hr=f(o),we=n(o,"H2",{class:!0});var Zr=r(we);to=n(Zr,"A",{id:!0,class:!0,href:!0});var jm=r(to);Fn=n(jm,"SPAN",{});var qm=r(Fn);F(_t.$$.fragment,qm),qm.forEach(t),jm.forEach(t),pd=f(Zr),kn=n(Zr,"SPAN",{});var Lm=r(kn);gd=i(Lm,"FlavaImageCodebook"),Lm.forEach(t),Zr.forEach(t),pr=f(o),C=n(o,"DIV",{class:!0});var U=r(C);F(vt.$$.fragment,U),ud=f(U),bt=n(U,"P",{});var Kr=r(bt);_d=i(Kr,`The FLAVA\u2019s image codebook model inspired from DALL-E\u2019s original encoder. Outputs raw hidden states and can be used
to generate image tokens for an image based on DALL-E\u2019s vocab. Used to generate labels for MIM. Use
`),$n=n(Kr,"CODE",{});var Om=r($n);vd=i(Om,"get_codebook_indices"),Om.forEach(t),bd=i(Kr," to get image tokens for an image."),Kr.forEach(t),Fd=f(U),Ft=n(U,"P",{});var Xr=r(Ft);kd=i(Xr,"This model is a PyTorch "),kt=n(Xr,"A",{href:!0,rel:!0});var Nm=r(kt);$d=i(Nm,"torch.nn.Module"),Nm.forEach(t),Td=i(Xr,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Xr.forEach(t),wd=f(U),Fa=n(U,"DIV",{class:!0});var Dm=r(Fa);F($t.$$.fragment,Dm),Dm.forEach(t),xd=f(U),ka=n(U,"DIV",{class:!0});var Vm=r(ka);F(Tt.$$.fragment,Vm),Vm.forEach(t),yd=f(U),$a=n(U,"DIV",{class:!0});var Wm=r($a);F(wt.$$.fragment,Wm),Wm.forEach(t),U.forEach(t),gr=f(o),xe=n(o,"H2",{class:!0});var Jr=r(xe);ao=n(Jr,"A",{id:!0,class:!0,href:!0});var Bm=r(ao);Tn=n(Bm,"SPAN",{});var Sm=r(Tn);F(xt.$$.fragment,Sm),Sm.forEach(t),Bm.forEach(t),Md=f(Jr),wn=n(Jr,"SPAN",{});var Um=r(wn);zd=i(Um,"FlavaTextModel"),Um.forEach(t),Jr.forEach(t),ur=f(o),H=n(o,"DIV",{class:!0});var Na=r(H);F(yt.$$.fragment,Na),Ed=f(Na),Mt=n(Na,"P",{});var Qr=r(Mt);Cd=i(Qr,`The bare FLAVA Text Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),zt=n(Qr,"A",{href:!0,rel:!0});var Rm=r(zt);Pd=i(Rm,"torch.nn.Module"),Rm.forEach(t),Id=i(Qr,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Qr.forEach(t),Ad=f(Na),V=n(Na,"DIV",{class:!0});var go=r(V);F(Et.$$.fragment,go),jd=f(go),ye=n(go,"P",{});var Da=r(ye);qd=i(Da,"The "),Ta=n(Da,"A",{href:!0});var Hm=r(Ta);Ld=i(Hm,"FlavaTextModel"),Hm.forEach(t),Od=i(Da," forward method, overrides the "),xn=n(Da,"CODE",{});var Gm=r(xn);Nd=i(Gm,"__call__"),Gm.forEach(t),Dd=i(Da," special method."),Da.forEach(t),Vd=f(go),F(no.$$.fragment,go),Wd=f(go),F(ro.$$.fragment,go),go.forEach(t),Na.forEach(t),_r=f(o),Me=n(o,"H2",{class:!0});var Yr=r(Me);so=n(Yr,"A",{id:!0,class:!0,href:!0});var Zm=r(so);yn=n(Zm,"SPAN",{});var Km=r(yn);F(Ct.$$.fragment,Km),Km.forEach(t),Zm.forEach(t),Bd=f(Yr),Mn=n(Yr,"SPAN",{});var Xm=r(Mn);Sd=i(Xm,"FlavaImageModel"),Xm.forEach(t),Yr.forEach(t),vr=f(o),G=n(o,"DIV",{class:!0});var Va=r(G);F(Pt.$$.fragment,Va),Ud=f(Va),It=n(Va,"P",{});var es=r(It);Rd=i(es,`The bare FLAVA Image Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),At=n(es,"A",{href:!0,rel:!0});var Jm=r(At);Hd=i(Jm,"torch.nn.Module"),Jm.forEach(t),Gd=i(es,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),es.forEach(t),Zd=f(Va),W=n(Va,"DIV",{class:!0});var uo=r(W);F(jt.$$.fragment,uo),Kd=f(uo),ze=n(uo,"P",{});var Wa=r(ze);Xd=i(Wa,"The "),wa=n(Wa,"A",{href:!0});var Qm=r(wa);Jd=i(Qm,"FlavaImageModel"),Qm.forEach(t),Qd=i(Wa," forward method, overrides the "),zn=n(Wa,"CODE",{});var Ym=r(zn);Yd=i(Ym,"__call__"),Ym.forEach(t),ec=i(Wa," special method."),Wa.forEach(t),oc=f(uo),F(io.$$.fragment,uo),tc=f(uo),F(lo.$$.fragment,uo),uo.forEach(t),Va.forEach(t),br=f(o),Ee=n(o,"H2",{class:!0});var os=r(Ee);co=n(os,"A",{id:!0,class:!0,href:!0});var ef=r(co);En=n(ef,"SPAN",{});var of=r(En);F(qt.$$.fragment,of),of.forEach(t),ef.forEach(t),ac=f(os),Cn=n(os,"SPAN",{});var tf=r(Cn);nc=i(tf,"FlavaMultimodalModel"),tf.forEach(t),os.forEach(t),Fr=f(o),Z=n(o,"DIV",{class:!0});var Ba=r(Z);F(Lt.$$.fragment,Ba),rc=f(Ba),Ot=n(Ba,"P",{});var ts=r(Ot);sc=i(ts,`The bare FLAVA Multimodal Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Nt=n(ts,"A",{href:!0,rel:!0});var af=r(Nt);ic=i(af,"torch.nn.Module"),af.forEach(t),lc=i(ts,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ts.forEach(t),dc=f(Ba),B=n(Ba,"DIV",{class:!0});var _o=r(B);F(Dt.$$.fragment,_o),cc=f(_o),Ce=n(_o,"P",{});var Sa=r(Ce);mc=i(Sa,"The "),xa=n(Sa,"A",{href:!0});var nf=r(xa);fc=i(nf,"FlavaMultimodalModel"),nf.forEach(t),hc=i(Sa," forward method, overrides the "),Pn=n(Sa,"CODE",{});var rf=r(Pn);pc=i(rf,"__call__"),rf.forEach(t),gc=i(Sa," special method."),Sa.forEach(t),uc=f(_o),F(mo.$$.fragment,_o),_c=f(_o),F(fo.$$.fragment,_o),_o.forEach(t),Ba.forEach(t),this.h()},h(){d(c,"name","hf:doc:metadata"),d(c,"content",JSON.stringify(zf)),d(p,"id","flava"),d(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(p,"href","#flava"),d(u,"class","relative group"),d(Ie,"id","overview"),d(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ie,"href","#overview"),d(se,"class","relative group"),d($o,"href","https://arxiv.org/abs/2112.04482"),d($o,"rel","nofollow"),d(To,"href","https://huggingface.co/aps"),d(To,"rel","nofollow"),d(wo,"href","https://github.com/facebookresearch/multimodal/tree/main/examples/flava"),d(wo,"rel","nofollow"),d(je,"id","transformers.FlavaConfig"),d(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(je,"href","#transformers.FlavaConfig"),d(ie,"class","relative group"),d(Zt,"href","/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaConfig"),d(Kt,"href","/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaModel"),d(Mo,"href","https://huggingface.co/facebook/flava-full"),d(Mo,"rel","nofollow"),d(Xt,"href","/docs/transformers/v4.22.1/en/main_classes/configuration#transformers.PretrainedConfig"),d(Jt,"href","/docs/transformers/v4.22.1/en/main_classes/configuration#transformers.PretrainedConfig"),d(Qt,"href","/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaConfig"),d(Le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Yt,"href","/docs/transformers/v4.22.1/en/main_classes/configuration#transformers.PretrainedConfig.to_dict"),d(Oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ne,"id","transformers.FlavaTextConfig"),d(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ne,"href","#transformers.FlavaTextConfig"),d(de,"class","relative group"),d(ea,"href","/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaTextModel"),d(Lo,"href","https://huggingface.co/facebook/flava-full"),d(Lo,"rel","nofollow"),d(oa,"href","/docs/transformers/v4.22.1/en/main_classes/configuration#transformers.PretrainedConfig"),d(ta,"href","/docs/transformers/v4.22.1/en/main_classes/configuration#transformers.PretrainedConfig"),d(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ve,"id","transformers.FlavaImageConfig"),d(Ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ve,"href","#transformers.FlavaImageConfig"),d(me,"class","relative group"),d(aa,"href","/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaImageModel"),d(Wo,"href","https://huggingface.co/facebook/flava-full"),d(Wo,"rel","nofollow"),d(na,"href","/docs/transformers/v4.22.1/en/main_classes/configuration#transformers.PretrainedConfig"),d(ra,"href","/docs/transformers/v4.22.1/en/main_classes/configuration#transformers.PretrainedConfig"),d(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Be,"id","transformers.FlavaMultimodalConfig"),d(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Be,"href","#transformers.FlavaMultimodalConfig"),d(he,"class","relative group"),d(sa,"href","/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaMultimodalModel"),d(Ho,"href","https://huggingface.co/facebook/flava-full"),d(Ho,"rel","nofollow"),d(ia,"href","/docs/transformers/v4.22.1/en/main_classes/configuration#transformers.PretrainedConfig"),d(la,"href","/docs/transformers/v4.22.1/en/main_classes/configuration#transformers.PretrainedConfig"),d(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ue,"id","transformers.FlavaImageCodebookConfig"),d(Ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ue,"href","#transformers.FlavaImageCodebookConfig"),d(ge,"class","relative group"),d(Zo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Re,"id","transformers.FlavaProcessor"),d(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Re,"href","#transformers.FlavaProcessor"),d(ue,"class","relative group"),d(da,"href","/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaProcessor"),d(ca,"href","/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaFeatureExtractor"),d(ma,"href","/docs/transformers/v4.22.1/en/model_doc/bert#transformers.BertTokenizerFast"),d(fa,"href","/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaProcessor.decode"),d(ha,"href","/docs/transformers/v4.22.1/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer.batch_decode"),d(He,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(pa,"href","/docs/transformers/v4.22.1/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer.decode"),d(Ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ze,"id","transformers.FlavaFeatureExtractor"),d(Ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ze,"href","#transformers.FlavaFeatureExtractor"),d(_e,"class","relative group"),d(ga,"href","/docs/transformers/v4.22.1/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),d(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ke,"id","transformers.FlavaForPreTraining"),d(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ke,"href","#transformers.FlavaForPreTraining"),d(ve,"class","relative group"),d(lt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(lt,"rel","nofollow"),d(ua,"href","/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaForPreTraining"),d(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Je,"id","transformers.FlavaModel"),d(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Je,"href","#transformers.FlavaModel"),d(Fe,"class","relative group"),d(ht,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(ht,"rel","nofollow"),d(_a,"href","/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaModel"),d(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(va,"href","/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaModel"),d(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ba,"href","/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaModel"),d(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(to,"id","transformers.FlavaImageCodebook"),d(to,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(to,"href","#transformers.FlavaImageCodebook"),d(we,"class","relative group"),d(kt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(kt,"rel","nofollow"),d(Fa,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ka,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d($a,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ao,"id","transformers.FlavaTextModel"),d(ao,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ao,"href","#transformers.FlavaTextModel"),d(xe,"class","relative group"),d(zt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(zt,"rel","nofollow"),d(Ta,"href","/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaTextModel"),d(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(so,"id","transformers.FlavaImageModel"),d(so,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(so,"href","#transformers.FlavaImageModel"),d(Me,"class","relative group"),d(At,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(At,"rel","nofollow"),d(wa,"href","/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaImageModel"),d(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(co,"id","transformers.FlavaMultimodalModel"),d(co,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(co,"href","#transformers.FlavaMultimodalModel"),d(Ee,"class","relative group"),d(Nt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(Nt,"rel","nofollow"),d(xa,"href","/docs/transformers/v4.22.1/en/model_doc/flava#transformers.FlavaMultimodalModel"),d(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,g){e(document.head,c),_(o,x,g),_(o,u,g),e(u,p),e(p,v),k(l,v,null),e(u,h),e(u,z),e(z,as),_(o,Sn,g),_(o,se,g),e(se,Ie),e(Ie,Ua),k(ko,Ua,null),e(se,ns),e(se,Ra),e(Ra,rs),_(o,Un,g),_(o,Ae,g),e(Ae,ss),e(Ae,$o),e($o,is),e(Ae,ls),_(o,Rn,g),_(o,Rt,g),e(Rt,ds),_(o,Hn,g),_(o,Ht,g),e(Ht,cs),_(o,Gn,g),_(o,Gt,g),e(Gt,Ha),e(Ha,ms),_(o,Zn,g),_(o,X,g),e(X,fs),e(X,To),e(To,hs),e(X,ps),e(X,wo),e(wo,gs),e(X,us),_(o,Kn,g),_(o,ie,g),e(ie,je),e(je,Ga),k(xo,Ga,null),e(ie,_s),e(ie,Za),e(Za,vs),_(o,Xn,g),_(o,E,g),k(yo,E,null),e(E,bs),e(E,J),e(J,Zt),e(Zt,Fs),e(J,ks),e(J,Kt),e(Kt,$s),e(J,Ts),e(J,Mo),e(Mo,ws),e(J,xs),e(E,ys),e(E,le),e(le,Ms),e(le,Xt),e(Xt,zs),e(le,Es),e(le,Jt),e(Jt,Cs),e(le,Ps),e(E,Is),k(qe,E,null),e(E,As),e(E,Le),k(zo,Le,null),e(Le,js),e(Le,Eo),e(Eo,qs),e(Eo,Qt),e(Qt,Ls),e(Eo,Os),e(E,Ns),e(E,Oe),k(Co,Oe,null),e(Oe,Ds),e(Oe,Po),e(Po,Vs),e(Po,Yt),e(Yt,Ws),e(Po,Bs),_(o,Jn,g),_(o,de,g),e(de,Ne),e(Ne,Ka),k(Io,Ka,null),e(de,Ss),e(de,Xa),e(Xa,Us),_(o,Qn,g),_(o,P,g),k(Ao,P,null),e(P,Rs),e(P,jo),e(jo,Hs),e(jo,ea),e(ea,Gs),e(jo,Zs),e(P,Ks),e(P,qo),e(qo,Xs),e(qo,Lo),e(Lo,Js),e(qo,Qs),e(P,Ys),e(P,ce),e(ce,ei),e(ce,oa),e(oa,oi),e(ce,ti),e(ce,ta),e(ta,ai),e(ce,ni),e(P,ri),k(De,P,null),_(o,Yn,g),_(o,me,g),e(me,Ve),e(Ve,Ja),k(Oo,Ja,null),e(me,si),e(me,Qa),e(Qa,ii),_(o,er,g),_(o,I,g),k(No,I,null),e(I,li),e(I,Do),e(Do,di),e(Do,aa),e(aa,ci),e(Do,mi),e(I,fi),e(I,Vo),e(Vo,hi),e(Vo,Wo),e(Wo,pi),e(Vo,gi),e(I,ui),e(I,fe),e(fe,_i),e(fe,na),e(na,vi),e(fe,bi),e(fe,ra),e(ra,Fi),e(fe,ki),e(I,$i),k(We,I,null),_(o,or,g),_(o,he,g),e(he,Be),e(Be,Ya),k(Bo,Ya,null),e(he,Ti),e(he,en),e(en,wi),_(o,tr,g),_(o,A,g),k(So,A,null),e(A,xi),e(A,Uo),e(Uo,yi),e(Uo,sa),e(sa,Mi),e(Uo,zi),e(A,Ei),e(A,Ro),e(Ro,Ci),e(Ro,Ho),e(Ho,Pi),e(Ro,Ii),e(A,Ai),e(A,pe),e(pe,ji),e(pe,ia),e(ia,qi),e(pe,Li),e(pe,la),e(la,Oi),e(pe,Ni),e(A,Di),k(Se,A,null),_(o,ar,g),_(o,ge,g),e(ge,Ue),e(Ue,on),k(Go,on,null),e(ge,Vi),e(ge,tn),e(tn,Wi),_(o,nr,g),_(o,Zo,g),k(Ko,Zo,null),_(o,rr,g),_(o,ue,g),e(ue,Re),e(Re,an),k(Xo,an,null),e(ue,Bi),e(ue,nn),e(nn,Si),_(o,sr,g),_(o,j,g),k(Jo,j,null),e(j,Ui),e(j,rn),e(rn,Ri),e(j,Hi),e(j,O),e(O,da),e(da,Gi),e(O,Zi),e(O,ca),e(ca,Ki),e(O,Xi),e(O,ma),e(ma,Ji),e(O,Qi),e(O,sn),e(sn,Yi),e(O,el),e(O,fa),e(fa,ol),e(O,tl),e(j,al),e(j,He),k(Qo,He,null),e(He,nl),e(He,Yo),e(Yo,rl),e(Yo,ha),e(ha,sl),e(Yo,il),e(j,ll),e(j,Ge),k(et,Ge,null),e(Ge,dl),e(Ge,ot),e(ot,cl),e(ot,pa),e(pa,ml),e(ot,fl),_(o,ir,g),_(o,_e,g),e(_e,Ze),e(Ze,ln),k(tt,ln,null),e(_e,hl),e(_e,dn),e(dn,pl),_(o,lr,g),_(o,R,g),k(at,R,null),e(R,gl),e(R,cn),e(cn,ul),e(R,_l),e(R,nt),e(nt,vl),e(nt,ga),e(ga,bl),e(nt,Fl),_(o,dr,g),_(o,ve,g),e(ve,Ke),e(Ke,mn),k(rt,mn,null),e(ve,kl),e(ve,fn),e(fn,$l),_(o,cr,g),_(o,N,g),k(st,N,null),e(N,Tl),e(N,hn),e(hn,wl),e(N,xl),e(N,it),e(it,yl),e(it,lt),e(lt,Ml),e(it,zl),e(N,El),e(N,Q),k(dt,Q,null),e(Q,Cl),e(Q,be),e(be,Pl),e(be,ua),e(ua,Il),e(be,Al),e(be,pn),e(pn,jl),e(be,ql),e(Q,Ll),k(Xe,Q,null),_(o,mr,g),_(o,Fe,g),e(Fe,Je),e(Je,gn),k(ct,gn,null),e(Fe,Ol),e(Fe,un),e(un,Nl),_(o,fr,g),_(o,q,g),k(mt,q,null),e(q,Dl),e(q,ft),e(ft,Vl),e(ft,ht),e(ht,Wl),e(ft,Bl),e(q,Sl),e(q,D),k(pt,D,null),e(D,Ul),e(D,ke),e(ke,Rl),e(ke,_a),e(_a,Hl),e(ke,Gl),e(ke,_n),e(_n,Zl),e(ke,Kl),e(D,Xl),k(Qe,D,null),e(D,Jl),k(Ye,D,null),e(q,Ql),e(q,Y),k(gt,Y,null),e(Y,Yl),e(Y,$e),e($e,ed),e($e,va),e(va,od),e($e,td),e($e,vn),e(vn,ad),e($e,nd),e(Y,rd),k(eo,Y,null),e(q,sd),e(q,ee),k(ut,ee,null),e(ee,id),e(ee,Te),e(Te,ld),e(Te,ba),e(ba,dd),e(Te,cd),e(Te,bn),e(bn,md),e(Te,fd),e(ee,hd),k(oo,ee,null),_(o,hr,g),_(o,we,g),e(we,to),e(to,Fn),k(_t,Fn,null),e(we,pd),e(we,kn),e(kn,gd),_(o,pr,g),_(o,C,g),k(vt,C,null),e(C,ud),e(C,bt),e(bt,_d),e(bt,$n),e($n,vd),e(bt,bd),e(C,Fd),e(C,Ft),e(Ft,kd),e(Ft,kt),e(kt,$d),e(Ft,Td),e(C,wd),e(C,Fa),k($t,Fa,null),e(C,xd),e(C,ka),k(Tt,ka,null),e(C,yd),e(C,$a),k(wt,$a,null),_(o,gr,g),_(o,xe,g),e(xe,ao),e(ao,Tn),k(xt,Tn,null),e(xe,Md),e(xe,wn),e(wn,zd),_(o,ur,g),_(o,H,g),k(yt,H,null),e(H,Ed),e(H,Mt),e(Mt,Cd),e(Mt,zt),e(zt,Pd),e(Mt,Id),e(H,Ad),e(H,V),k(Et,V,null),e(V,jd),e(V,ye),e(ye,qd),e(ye,Ta),e(Ta,Ld),e(ye,Od),e(ye,xn),e(xn,Nd),e(ye,Dd),e(V,Vd),k(no,V,null),e(V,Wd),k(ro,V,null),_(o,_r,g),_(o,Me,g),e(Me,so),e(so,yn),k(Ct,yn,null),e(Me,Bd),e(Me,Mn),e(Mn,Sd),_(o,vr,g),_(o,G,g),k(Pt,G,null),e(G,Ud),e(G,It),e(It,Rd),e(It,At),e(At,Hd),e(It,Gd),e(G,Zd),e(G,W),k(jt,W,null),e(W,Kd),e(W,ze),e(ze,Xd),e(ze,wa),e(wa,Jd),e(ze,Qd),e(ze,zn),e(zn,Yd),e(ze,ec),e(W,oc),k(io,W,null),e(W,tc),k(lo,W,null),_(o,br,g),_(o,Ee,g),e(Ee,co),e(co,En),k(qt,En,null),e(Ee,ac),e(Ee,Cn),e(Cn,nc),_(o,Fr,g),_(o,Z,g),k(Lt,Z,null),e(Z,rc),e(Z,Ot),e(Ot,sc),e(Ot,Nt),e(Nt,ic),e(Ot,lc),e(Z,dc),e(Z,B),k(Dt,B,null),e(B,cc),e(B,Ce),e(Ce,mc),e(Ce,xa),e(xa,fc),e(Ce,hc),e(Ce,Pn),e(Pn,pc),e(Ce,gc),e(B,uc),k(mo,B,null),e(B,_c),k(fo,B,null),kr=!0},p(o,[g]){const Vt={};g&2&&(Vt.$$scope={dirty:g,ctx:o}),qe.$set(Vt);const In={};g&2&&(In.$$scope={dirty:g,ctx:o}),De.$set(In);const An={};g&2&&(An.$$scope={dirty:g,ctx:o}),We.$set(An);const jn={};g&2&&(jn.$$scope={dirty:g,ctx:o}),Se.$set(jn);const Wt={};g&2&&(Wt.$$scope={dirty:g,ctx:o}),Xe.$set(Wt);const qn={};g&2&&(qn.$$scope={dirty:g,ctx:o}),Qe.$set(qn);const Ln={};g&2&&(Ln.$$scope={dirty:g,ctx:o}),Ye.$set(Ln);const On={};g&2&&(On.$$scope={dirty:g,ctx:o}),eo.$set(On);const Bt={};g&2&&(Bt.$$scope={dirty:g,ctx:o}),oo.$set(Bt);const Nn={};g&2&&(Nn.$$scope={dirty:g,ctx:o}),no.$set(Nn);const Dn={};g&2&&(Dn.$$scope={dirty:g,ctx:o}),ro.$set(Dn);const Vn={};g&2&&(Vn.$$scope={dirty:g,ctx:o}),io.$set(Vn);const Wn={};g&2&&(Wn.$$scope={dirty:g,ctx:o}),lo.$set(Wn);const Bn={};g&2&&(Bn.$$scope={dirty:g,ctx:o}),mo.$set(Bn);const Pe={};g&2&&(Pe.$$scope={dirty:g,ctx:o}),fo.$set(Pe)},i(o){kr||($(l.$$.fragment,o),$(ko.$$.fragment,o),$(xo.$$.fragment,o),$(yo.$$.fragment,o),$(qe.$$.fragment,o),$(zo.$$.fragment,o),$(Co.$$.fragment,o),$(Io.$$.fragment,o),$(Ao.$$.fragment,o),$(De.$$.fragment,o),$(Oo.$$.fragment,o),$(No.$$.fragment,o),$(We.$$.fragment,o),$(Bo.$$.fragment,o),$(So.$$.fragment,o),$(Se.$$.fragment,o),$(Go.$$.fragment,o),$(Ko.$$.fragment,o),$(Xo.$$.fragment,o),$(Jo.$$.fragment,o),$(Qo.$$.fragment,o),$(et.$$.fragment,o),$(tt.$$.fragment,o),$(at.$$.fragment,o),$(rt.$$.fragment,o),$(st.$$.fragment,o),$(dt.$$.fragment,o),$(Xe.$$.fragment,o),$(ct.$$.fragment,o),$(mt.$$.fragment,o),$(pt.$$.fragment,o),$(Qe.$$.fragment,o),$(Ye.$$.fragment,o),$(gt.$$.fragment,o),$(eo.$$.fragment,o),$(ut.$$.fragment,o),$(oo.$$.fragment,o),$(_t.$$.fragment,o),$(vt.$$.fragment,o),$($t.$$.fragment,o),$(Tt.$$.fragment,o),$(wt.$$.fragment,o),$(xt.$$.fragment,o),$(yt.$$.fragment,o),$(Et.$$.fragment,o),$(no.$$.fragment,o),$(ro.$$.fragment,o),$(Ct.$$.fragment,o),$(Pt.$$.fragment,o),$(jt.$$.fragment,o),$(io.$$.fragment,o),$(lo.$$.fragment,o),$(qt.$$.fragment,o),$(Lt.$$.fragment,o),$(Dt.$$.fragment,o),$(mo.$$.fragment,o),$(fo.$$.fragment,o),kr=!0)},o(o){T(l.$$.fragment,o),T(ko.$$.fragment,o),T(xo.$$.fragment,o),T(yo.$$.fragment,o),T(qe.$$.fragment,o),T(zo.$$.fragment,o),T(Co.$$.fragment,o),T(Io.$$.fragment,o),T(Ao.$$.fragment,o),T(De.$$.fragment,o),T(Oo.$$.fragment,o),T(No.$$.fragment,o),T(We.$$.fragment,o),T(Bo.$$.fragment,o),T(So.$$.fragment,o),T(Se.$$.fragment,o),T(Go.$$.fragment,o),T(Ko.$$.fragment,o),T(Xo.$$.fragment,o),T(Jo.$$.fragment,o),T(Qo.$$.fragment,o),T(et.$$.fragment,o),T(tt.$$.fragment,o),T(at.$$.fragment,o),T(rt.$$.fragment,o),T(st.$$.fragment,o),T(dt.$$.fragment,o),T(Xe.$$.fragment,o),T(ct.$$.fragment,o),T(mt.$$.fragment,o),T(pt.$$.fragment,o),T(Qe.$$.fragment,o),T(Ye.$$.fragment,o),T(gt.$$.fragment,o),T(eo.$$.fragment,o),T(ut.$$.fragment,o),T(oo.$$.fragment,o),T(_t.$$.fragment,o),T(vt.$$.fragment,o),T($t.$$.fragment,o),T(Tt.$$.fragment,o),T(wt.$$.fragment,o),T(xt.$$.fragment,o),T(yt.$$.fragment,o),T(Et.$$.fragment,o),T(no.$$.fragment,o),T(ro.$$.fragment,o),T(Ct.$$.fragment,o),T(Pt.$$.fragment,o),T(jt.$$.fragment,o),T(io.$$.fragment,o),T(lo.$$.fragment,o),T(qt.$$.fragment,o),T(Lt.$$.fragment,o),T(Dt.$$.fragment,o),T(mo.$$.fragment,o),T(fo.$$.fragment,o),kr=!1},d(o){t(c),o&&t(x),o&&t(u),w(l),o&&t(Sn),o&&t(se),w(ko),o&&t(Un),o&&t(Ae),o&&t(Rn),o&&t(Rt),o&&t(Hn),o&&t(Ht),o&&t(Gn),o&&t(Gt),o&&t(Zn),o&&t(X),o&&t(Kn),o&&t(ie),w(xo),o&&t(Xn),o&&t(E),w(yo),w(qe),w(zo),w(Co),o&&t(Jn),o&&t(de),w(Io),o&&t(Qn),o&&t(P),w(Ao),w(De),o&&t(Yn),o&&t(me),w(Oo),o&&t(er),o&&t(I),w(No),w(We),o&&t(or),o&&t(he),w(Bo),o&&t(tr),o&&t(A),w(So),w(Se),o&&t(ar),o&&t(ge),w(Go),o&&t(nr),o&&t(Zo),w(Ko),o&&t(rr),o&&t(ue),w(Xo),o&&t(sr),o&&t(j),w(Jo),w(Qo),w(et),o&&t(ir),o&&t(_e),w(tt),o&&t(lr),o&&t(R),w(at),o&&t(dr),o&&t(ve),w(rt),o&&t(cr),o&&t(N),w(st),w(dt),w(Xe),o&&t(mr),o&&t(Fe),w(ct),o&&t(fr),o&&t(q),w(mt),w(pt),w(Qe),w(Ye),w(gt),w(eo),w(ut),w(oo),o&&t(hr),o&&t(we),w(_t),o&&t(pr),o&&t(C),w(vt),w($t),w(Tt),w(wt),o&&t(gr),o&&t(xe),w(xt),o&&t(ur),o&&t(H),w(yt),w(Et),w(no),w(ro),o&&t(_r),o&&t(Me),w(Ct),o&&t(vr),o&&t(G),w(Pt),w(jt),w(io),w(lo),o&&t(br),o&&t(Ee),w(qt),o&&t(Fr),o&&t(Z),w(Lt),w(Dt),w(mo),w(fo)}}}const zf={local:"flava",sections:[{local:"overview",title:"Overview"},{local:"transformers.FlavaConfig",title:"FlavaConfig"},{local:"transformers.FlavaTextConfig",title:"FlavaTextConfig"},{local:"transformers.FlavaImageConfig",title:"FlavaImageConfig"},{local:"transformers.FlavaMultimodalConfig",title:"FlavaMultimodalConfig"},{local:"transformers.FlavaImageCodebookConfig",title:"FlavaImageCodebookConfig"},{local:"transformers.FlavaProcessor",title:"FlavaProcessor"},{local:"transformers.FlavaFeatureExtractor",title:"FlavaFeatureExtractor"},{local:"transformers.FlavaForPreTraining",title:"FlavaForPreTraining"},{local:"transformers.FlavaModel",title:"FlavaModel"},{local:"transformers.FlavaImageCodebook",title:"FlavaImageCodebook"},{local:"transformers.FlavaTextModel",title:"FlavaTextModel"},{local:"transformers.FlavaImageModel",title:"FlavaImageModel"},{local:"transformers.FlavaMultimodalModel",title:"FlavaMultimodalModel"}],title:"FLAVA"};function Ef(y){return mf(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Lf extends sf{constructor(c){super();lf(this,c,Ef,Mf,df,{})}}export{Lf as default,zf as metadata};
