import{S as Ue,i as Fe,s as Ye,e as o,k as u,w,t as e,M as Ve,c as r,d as t,m as d,a as i,x as y,h as n,b as m,G as a,g as p,y as k,q as x,o as E,B as A,v as Ne}from"../../chunks/vendor-hf-doc-builder.js";import{T as ee}from"../../chunks/Tip-hf-doc-builder.js";import{Y as He}from"../../chunks/Youtube-hf-doc-builder.js";import{I as et}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as U}from"../../chunks/CodeBlock-hf-doc-builder.js";import{F as Be,M as Ge}from"../../chunks/Markdown-hf-doc-builder.js";function Je(F){let c,j,h,$,v;return{c(){c=o("p"),j=e("See the automatic speech recognition "),h=o("a"),$=e("task page"),v=e(" for more information about its associated models, datasets, and metrics."),this.h()},l(g){c=r(g,"P",{});var q=i(c);j=n(q,"See the automatic speech recognition "),h=r(q,"A",{href:!0,rel:!0});var T=i(h);$=n(T,"task page"),T.forEach(t),v=n(q," for more information about its associated models, datasets, and metrics."),q.forEach(t),this.h()},h(){m(h,"href","https://huggingface.co/tasks/automatic-speech-recognition"),m(h,"rel","nofollow")},m(g,q){p(g,c,q),a(c,j),a(c,h),a(h,$),a(c,v)},d(g){g&&t(c)}}}function Ke(F){let c,j,h,$,v,g,q,T;return{c(){c=o("p"),j=e("If you aren\u2019t familiar with fine-tuning a model with the "),h=o("a"),$=e("Trainer"),v=e(", take a look at the basic tutorial "),g=o("a"),q=e("here"),T=e("!"),this.h()},l(P){c=r(P,"P",{});var _=i(c);j=n(_,"If you aren\u2019t familiar with fine-tuning a model with the "),h=r(_,"A",{href:!0});var D=i(h);$=n(D,"Trainer"),D.forEach(t),v=n(_,", take a look at the basic tutorial "),g=r(_,"A",{href:!0});var C=i(g);q=n(C,"here"),C.forEach(t),T=n(_,"!"),_.forEach(t),this.h()},h(){m(h,"href","/docs/transformers/v4.22.1/en/main_classes/trainer#transformers.Trainer"),m(g,"href","../training#finetune-with-trainer")},m(P,_){p(P,c,_),a(c,j),a(c,h),a(h,$),a(c,v),a(c,g),a(g,q),a(c,T)},d(P){P&&t(c)}}}function Qe(F){let c,j,h,$,v,g,q,T,P,_,D,C,K,as,us,S,R,I,ys,ts,Q,ks,xs,Y,V,X,M,N,cs,L,Es,H,As,ds,W,Z,B;return _=new U({props:{code:`from transformers import AutoModelForCTC, TrainingArguments, Trainer

model = AutoModelForCTC.from_pretrained(
    "facebook/wav2vec2-base",
    ctc_loss_reduction="mean",
    pad_token_id=processor.tokenizer.pad_token_id,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCTC, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>,
<span class="hljs-meta">... </span>    ctc_loss_reduction=<span class="hljs-string">&quot;mean&quot;</span>,
<span class="hljs-meta">... </span>    pad_token_id=processor.tokenizer.pad_token_id,
<span class="hljs-meta">... </span>)`}}),C=new ee({props:{$$slots:{default:[Ke]},$$scope:{ctx:F}}}),Z=new U({props:{code:`training_args = TrainingArguments(
    output_dir="./results",
    group_by_length=True,
    per_device_train_batch_size=16,
    evaluation_strategy="steps",
    num_train_epochs=3,
    fp16=True,
    gradient_checkpointing=True,
    learning_rate=1e-4,
    weight_decay=0.005,
    save_total_limit=2,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_minds["train"],
    eval_dataset=encoded_minds["test"],
    tokenizer=processor.feature_extractor,
    data_collator=data_collator,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    group_by_length=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;steps&quot;</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    fp16=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    gradient_checkpointing=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">1e-4</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.005</span>,
<span class="hljs-meta">... </span>    save_total_limit=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=encoded_minds[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=encoded_minds[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=processor.feature_extractor,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),{c(){c=o("p"),j=e("Load Wav2Vec2 with "),h=o("a"),$=e("AutoModelForCTC"),v=e(". For "),g=o("code"),q=e("ctc_loss_reduction"),T=e(", it is often better to use the average instead of the default summation:"),P=u(),w(_.$$.fragment),D=u(),w(C.$$.fragment),K=u(),as=o("p"),us=e("At this point, only three steps remain:"),S=u(),R=o("ol"),I=o("li"),ys=e("Define your training hyperparameters in "),ts=o("a"),Q=e("TrainingArguments"),ks=e("."),xs=u(),Y=o("li"),V=e("Pass the training arguments to "),X=o("a"),M=e("Trainer"),N=e(" along with the model, datasets, tokenizer, and data collator."),cs=u(),L=o("li"),Es=e("Call "),H=o("a"),As=e("train()"),ds=e(" to fine-tune your model."),W=u(),w(Z.$$.fragment),this.h()},l(f){c=r(f,"P",{});var b=i(c);j=n(b,"Load Wav2Vec2 with "),h=r(b,"A",{href:!0});var es=i(h);$=n(es,"AutoModelForCTC"),es.forEach(t),v=n(b,". For "),g=r(b,"CODE",{});var ns=i(g);q=n(ns,"ctc_loss_reduction"),ns.forEach(t),T=n(b,", it is often better to use the average instead of the default summation:"),b.forEach(t),P=d(f),y(_.$$.fragment,f),D=d(f),y(C.$$.fragment,f),K=d(f),as=r(f,"P",{});var qs=i(as);us=n(qs,"At this point, only three steps remain:"),qs.forEach(t),S=d(f),R=r(f,"OL",{});var z=i(R);I=r(z,"LI",{});var ms=i(I);ys=n(ms,"Define your training hyperparameters in "),ts=r(ms,"A",{href:!0});var Ts=i(ts);Q=n(Ts,"TrainingArguments"),Ts.forEach(t),ks=n(ms,"."),ms.forEach(t),xs=d(z),Y=r(z,"LI",{});var G=i(Y);V=n(G,"Pass the training arguments to "),X=r(G,"A",{href:!0});var Ps=i(X);M=n(Ps,"Trainer"),Ps.forEach(t),N=n(G," along with the model, datasets, tokenizer, and data collator."),G.forEach(t),cs=d(z),L=r(z,"LI",{});var ss=i(L);Es=n(ss,"Call "),H=r(ss,"A",{href:!0});var Bs=i(H);As=n(Bs,"train()"),Bs.forEach(t),ds=n(ss," to fine-tune your model."),ss.forEach(t),z.forEach(t),W=d(f),y(Z.$$.fragment,f),this.h()},h(){m(h,"href","/docs/transformers/v4.22.1/en/model_doc/auto#transformers.AutoModelForCTC"),m(ts,"href","/docs/transformers/v4.22.1/en/main_classes/trainer#transformers.TrainingArguments"),m(X,"href","/docs/transformers/v4.22.1/en/main_classes/trainer#transformers.Trainer"),m(H,"href","/docs/transformers/v4.22.1/en/main_classes/trainer#transformers.Trainer.train")},m(f,b){p(f,c,b),a(c,j),a(c,h),a(h,$),a(c,v),a(c,g),a(g,q),a(c,T),p(f,P,b),k(_,f,b),p(f,D,b),k(C,f,b),p(f,K,b),p(f,as,b),a(as,us),p(f,S,b),p(f,R,b),a(R,I),a(I,ys),a(I,ts),a(ts,Q),a(I,ks),a(R,xs),a(R,Y),a(Y,V),a(Y,X),a(X,M),a(Y,N),a(R,cs),a(R,L),a(L,Es),a(L,H),a(H,As),a(L,ds),p(f,W,b),k(Z,f,b),B=!0},p(f,b){const es={};b&2&&(es.$$scope={dirty:b,ctx:f}),C.$set(es)},i(f){B||(x(_.$$.fragment,f),x(C.$$.fragment,f),x(Z.$$.fragment,f),B=!0)},o(f){E(_.$$.fragment,f),E(C.$$.fragment,f),E(Z.$$.fragment,f),B=!1},d(f){f&&t(c),f&&t(P),A(_,f),f&&t(D),A(C,f),f&&t(K),f&&t(as),f&&t(S),f&&t(R),f&&t(W),A(Z,f)}}}function Xe(F){let c,j;return c=new Ge({props:{$$slots:{default:[Qe]},$$scope:{ctx:F}}}),{c(){w(c.$$.fragment)},l(h){y(c.$$.fragment,h)},m(h,$){k(c,h,$),j=!0},p(h,$){const v={};$&2&&(v.$$scope={dirty:$,ctx:h}),c.$set(v)},i(h){j||(x(c.$$.fragment,h),j=!0)},o(h){E(c.$$.fragment,h),j=!1},d(h){A(c,h)}}}function Ze(F){let c,j,h,$,v,g,q,T;return{c(){c=o("p"),j=e("For a more in-depth example of how to fine-tune a model for automatic speech recognition, take a look at this blog "),h=o("a"),$=e("post"),v=e(" for English ASR and this "),g=o("a"),q=e("post"),T=e(" for multilingual ASR."),this.h()},l(P){c=r(P,"P",{});var _=i(c);j=n(_,"For a more in-depth example of how to fine-tune a model for automatic speech recognition, take a look at this blog "),h=r(_,"A",{href:!0,rel:!0});var D=i(h);$=n(D,"post"),D.forEach(t),v=n(_," for English ASR and this "),g=r(_,"A",{href:!0,rel:!0});var C=i(g);q=n(C,"post"),C.forEach(t),T=n(_," for multilingual ASR."),_.forEach(t),this.h()},h(){m(h,"href","https://huggingface.co/blog/fine-tune-wav2vec2-english"),m(h,"rel","nofollow"),m(g,"href","https://huggingface.co/blog/fine-tune-xlsr-wav2vec2"),m(g,"rel","nofollow")},m(P,_){p(P,c,_),a(c,j),a(c,h),a(h,$),a(c,v),a(c,g),a(g,q),a(c,T)},d(P){P&&t(c)}}}function sn(F){let c,j,h,$,v,g,q,T,P,_,D,C,K,as,us,S,R,I,ys,ts,Q,ks,xs,Y,V,X,M,N,cs,L,Es,H,As,ds,W,Z,B,f,b,es,ns,qs,z,ms,Ts,G,Ps,ss,Bs,ka,Cs,xa,O,nt,ea,lt,ot,na,rt,it,la,pt,ct,oa,ht,ft,Ea,Ds,Aa,Gs,ut,qa,Ss,Ta,ls,dt,ra,mt,gt,ia,_t,$t,Pa,hs,gs,pa,Is,jt,ca,bt,Ca,Js,vt,Da,Ls,Sa,_s,wt,Rs,yt,kt,Ia,Os,La,Ks,xt,Ra,os,Ms,Et,ha,At,qt,Tt,Ws,Pt,fa,Ct,Dt,St,ua,It,Oa,zs,Ma,rs,Lt,Us,Rt,Ot,da,Mt,Wt,Wa,Fs,za,J,zt,Qs,Ut,Ft,ma,Yt,Vt,ga,Nt,Ht,Ua,is,Bt,_a,Gt,Jt,$a,Kt,Qt,Fa,Ys,Ya,$s,Xt,ja,Zt,se,Va,Vs,Na,fs,js,ba,Ns,ae,va,te,Ha,bs,Ba,vs,Ga;return g=new et({}),D=new He({props:{id:"TksaY_FDgnk"}}),V=new ee({props:{$$slots:{default:[Je]},$$scope:{ctx:F}}}),L=new et({}),ns=new U({props:{code:`from datasets import load_dataset, Audio

minds = load_dataset("PolyAI/minds14", name="en-US", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>minds = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),G=new U({props:{code:"minds = minds.train_test_split(test_size=0.2)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>minds = minds.train_test_split(test_size=<span class="hljs-number">0.2</span>)'}}),Cs=new U({props:{code:"minds",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>minds
DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;path&#x27;</span>, <span class="hljs-string">&#x27;audio&#x27;</span>, <span class="hljs-string">&#x27;transcription&#x27;</span>, <span class="hljs-string">&#x27;english_transcription&#x27;</span>, <span class="hljs-string">&#x27;intent_class&#x27;</span>, <span class="hljs-string">&#x27;lang_id&#x27;</span>],
        num_rows: <span class="hljs-number">450</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;path&#x27;</span>, <span class="hljs-string">&#x27;audio&#x27;</span>, <span class="hljs-string">&#x27;transcription&#x27;</span>, <span class="hljs-string">&#x27;english_transcription&#x27;</span>, <span class="hljs-string">&#x27;intent_class&#x27;</span>, <span class="hljs-string">&#x27;lang_id&#x27;</span>],
        num_rows: <span class="hljs-number">113</span>
    })
})`}}),Ds=new U({props:{code:'minds = minds.remove_columns(["english_transcription", "intent_class", "lang_id"])',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>minds = minds.remove_columns([<span class="hljs-string">&quot;english_transcription&quot;</span>, <span class="hljs-string">&quot;intent_class&quot;</span>, <span class="hljs-string">&quot;lang_id&quot;</span>])'}}),Ss=new U({props:{code:'minds["train"][0]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>minds[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;audio&#x27;</span>: {<span class="hljs-string">&#x27;array&#x27;</span>: array([-<span class="hljs-number">0.00024414</span>,  <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        , ...,  <span class="hljs-number">0.00024414</span>,
          <span class="hljs-number">0.00024414</span>,  <span class="hljs-number">0.00024414</span>], dtype=float32),
  <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav&#x27;</span>,
  <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">8000</span>},
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav&#x27;</span>,
 <span class="hljs-string">&#x27;transcription&#x27;</span>: <span class="hljs-string">&quot;hi I&#x27;m trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing&quot;</span>}`}}),Is=new et({}),Ls=new U({props:{code:`from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>)`}}),Os=new U({props:{code:`minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
minds["train"][0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>minds = minds.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16_000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>minds[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;audio&#x27;</span>: {<span class="hljs-string">&#x27;array&#x27;</span>: array([-<span class="hljs-number">2.38064706e-04</span>, -<span class="hljs-number">1.58618059e-04</span>, -<span class="hljs-number">5.43987835e-06</span>, ...,
          <span class="hljs-number">2.78103951e-04</span>,  <span class="hljs-number">2.38446111e-04</span>,  <span class="hljs-number">1.18740834e-04</span>], dtype=float32),
  <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav&#x27;</span>,
  <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">16000</span>},
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav&#x27;</span>,
 <span class="hljs-string">&#x27;transcription&#x27;</span>: <span class="hljs-string">&quot;hi I&#x27;m trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing&quot;</span>}`}}),zs=new U({props:{code:`def prepare_dataset(batch):
    audio = batch["audio"]

    batch = processor(audio=audio["array"], sampling_rate=audio["sampling_rate"]).input_values[0]
    batch["input_length"] = len(batch["input_values"])

    batch["labels"] = processor(text=batch["transcription"]).input_ids
    return batch`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_dataset</span>(<span class="hljs-params">batch</span>):
<span class="hljs-meta">... </span>    audio = batch[<span class="hljs-string">&quot;audio&quot;</span>]

<span class="hljs-meta">... </span>    batch = processor(audio=audio[<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=audio[<span class="hljs-string">&quot;sampling_rate&quot;</span>]).input_values[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>    batch[<span class="hljs-string">&quot;input_length&quot;</span>] = <span class="hljs-built_in">len</span>(batch[<span class="hljs-string">&quot;input_values&quot;</span>])

<span class="hljs-meta">... </span>    batch[<span class="hljs-string">&quot;labels&quot;</span>] = processor(text=batch[<span class="hljs-string">&quot;transcription&quot;</span>]).input_ids
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> batch`}}),Fs=new U({props:{code:'encoded_minds = minds.map(prepare_dataset, remove_columns=minds.column_names["train"], num_proc=4)',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_minds = minds.<span class="hljs-built_in">map</span>(prepare_dataset, remove_columns=minds.column_names[<span class="hljs-string">&quot;train&quot;</span>], num_proc=<span class="hljs-number">4</span>)'}}),Ys=new U({props:{code:`import torch

from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union


@dataclass
class DataCollatorCTCWithPadding:

    processor: AutoProcessor
    padding: Union[bool, str] = True

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # split inputs and labels since they have to be of different lengths and need
        # different padding methods
        input_features = [{"input_values": feature["input_values"]} for feature in features]
        label_features = [{"input_ids": feature["labels"]} for feature in features]

        batch = self.processor.pad(input_features, padding=self.padding, return_tensors="pt")

        labels_batch = self.processor.pad(labels=label_features, padding=self.padding, return_tensors="pt")

        # replace padding with -100 to ignore loss correctly
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        batch["labels"] = labels

        return batch`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> dataclasses <span class="hljs-keyword">import</span> dataclass, field
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Any</span>, <span class="hljs-type">Dict</span>, <span class="hljs-type">List</span>, <span class="hljs-type">Optional</span>, <span class="hljs-type">Union</span>


<span class="hljs-meta">&gt;&gt;&gt; </span>@dataclass
<span class="hljs-meta">... </span><span class="hljs-keyword">class</span> <span class="hljs-title class_">DataCollatorCTCWithPadding</span>:

<span class="hljs-meta">... </span>    processor: AutoProcessor
<span class="hljs-meta">... </span>    padding: <span class="hljs-type">Union</span>[<span class="hljs-built_in">bool</span>, <span class="hljs-built_in">str</span>] = <span class="hljs-literal">True</span>

<span class="hljs-meta">... </span>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, features: <span class="hljs-type">List</span>[<span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Union</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], torch.Tensor]]]</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, torch.Tensor]:
<span class="hljs-meta">... </span>        <span class="hljs-comment"># split inputs and labels since they have to be of different lengths and need</span>
<span class="hljs-meta">... </span>        <span class="hljs-comment"># different padding methods</span>
<span class="hljs-meta">... </span>        input_features = [{<span class="hljs-string">&quot;input_values&quot;</span>: feature[<span class="hljs-string">&quot;input_values&quot;</span>]} <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features]
<span class="hljs-meta">... </span>        label_features = [{<span class="hljs-string">&quot;input_ids&quot;</span>: feature[<span class="hljs-string">&quot;labels&quot;</span>]} <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features]

<span class="hljs-meta">... </span>        batch = self.processor.pad(input_features, padding=self.padding, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">... </span>        labels_batch = self.processor.pad(labels=label_features, padding=self.padding, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">... </span>        <span class="hljs-comment"># replace padding with -100 to ignore loss correctly</span>
<span class="hljs-meta">... </span>        labels = labels_batch[<span class="hljs-string">&quot;input_ids&quot;</span>].masked_fill(labels_batch.attention_mask.ne(<span class="hljs-number">1</span>), -<span class="hljs-number">100</span>)

<span class="hljs-meta">... </span>        batch[<span class="hljs-string">&quot;labels&quot;</span>] = labels

<span class="hljs-meta">... </span>        <span class="hljs-keyword">return</span> batch`}}),Vs=new U({props:{code:"data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorCTCWithPadding(processor=processor, padding=<span class="hljs-literal">True</span>)'}}),Ns=new et({}),bs=new Be({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[Xe]},$$scope:{ctx:F}}}),vs=new ee({props:{$$slots:{default:[Ze]},$$scope:{ctx:F}}}),{c(){c=o("meta"),j=u(),h=o("h1"),$=o("a"),v=o("span"),w(g.$$.fragment),q=u(),T=o("span"),P=e("Automatic speech recognition"),_=u(),w(D.$$.fragment),C=u(),K=o("p"),as=e("Automatic speech recognition (ASR) converts a speech signal to text. It is an example of a sequence-to-sequence task, going from a sequence of audio inputs to textual outputs. Voice assistants like Siri and Alexa utilize ASR models to assist users."),us=u(),S=o("p"),R=e("This guide will show you how to fine-tune "),I=o("a"),ys=e("Wav2Vec2"),ts=e(" on the "),Q=o("a"),ks=e("MInDS-14"),xs=e(" dataset to transcribe audio to text."),Y=u(),w(V.$$.fragment),X=u(),M=o("h2"),N=o("a"),cs=o("span"),w(L.$$.fragment),Es=u(),H=o("span"),As=e("Load MInDS-14 dataset"),ds=u(),W=o("p"),Z=e("Load the "),B=o("a"),f=e("MInDS-14"),b=e(" from the \u{1F917} Datasets library:"),es=u(),w(ns.$$.fragment),qs=u(),z=o("p"),ms=e("Split this dataset into a train and test set:"),Ts=u(),w(G.$$.fragment),Ps=u(),ss=o("p"),Bs=e("Then take a look at the dataset:"),ka=u(),w(Cs.$$.fragment),xa=u(),O=o("p"),nt=e("While the dataset contains a lot of helpful information, like "),ea=o("code"),lt=e("lang_id"),ot=e(" and "),na=o("code"),rt=e("intent_class"),it=e(", you will focus on the "),la=o("code"),pt=e("audio"),ct=e(" and "),oa=o("code"),ht=e("transcription"),ft=e(" columns in this guide. Remove the other columns:"),Ea=u(),w(Ds.$$.fragment),Aa=u(),Gs=o("p"),ut=e("Take a look at the example again:"),qa=u(),w(Ss.$$.fragment),Ta=u(),ls=o("p"),dt=e("The "),ra=o("code"),mt=e("audio"),gt=e(" column contains a 1-dimensional "),ia=o("code"),_t=e("array"),$t=e(" of the speech signal that must be called to load and resample the audio file."),Pa=u(),hs=o("h2"),gs=o("a"),pa=o("span"),w(Is.$$.fragment),jt=u(),ca=o("span"),bt=e("Preprocess"),Ca=u(),Js=o("p"),vt=e("Load the Wav2Vec2 processor to process the audio signal and transcribed text:"),Da=u(),w(Ls.$$.fragment),Sa=u(),_s=o("p"),wt=e("The "),Rs=o("a"),yt=e("MInDS-14"),kt=e(" dataset has a sampling rate of 8000khz. You will need to resample the dataset to use the pretrained Wav2Vec2 model:"),Ia=u(),w(Os.$$.fragment),La=u(),Ks=o("p"),xt=e("The preprocessing function needs to:"),Ra=u(),os=o("ol"),Ms=o("li"),Et=e("Call the "),ha=o("code"),At=e("audio"),qt=e(" column to load and resample the audio file."),Tt=u(),Ws=o("li"),Pt=e("Extract the "),fa=o("code"),Ct=e("input_values"),Dt=e(" from the audio file."),St=u(),ua=o("li"),It=e("Typically, when you call the processor, you call the feature extractor. Since you also want to tokenize text, instruct the processor to call the tokenizer instead with a context manager."),Oa=u(),w(zs.$$.fragment),Ma=u(),rs=o("p"),Lt=e("Use \u{1F917} Datasets "),Us=o("a"),Rt=e("map"),Ot=e(" function to apply the preprocessing function over the entire dataset. You can speed up the map function by increasing the number of processes with "),da=o("code"),Mt=e("num_proc"),Wt=e(". Remove the columns you don\u2019t need:"),Wa=u(),w(Fs.$$.fragment),za=u(),J=o("p"),zt=e("\u{1F917} Transformers doesn\u2019t have a data collator for automatic speech recognition, so you will need to create one. You can adapt the "),Qs=o("a"),Ut=e("DataCollatorWithPadding"),Ft=e(" to create a batch of examples for automatic speech recognition. It will also dynamically pad your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),ma=o("code"),Yt=e("tokenizer"),Vt=e(" function by setting "),ga=o("code"),Nt=e("padding=True"),Ht=e(", dynamic padding is more efficient."),Ua=u(),is=o("p"),Bt=e("Unlike other data collators, this specific data collator needs to apply a different padding method to "),_a=o("code"),Gt=e("input_values"),Jt=e(" and "),$a=o("code"),Kt=e("labels"),Qt=e(". You can apply a different padding method with a context manager:"),Fa=u(),w(Ys.$$.fragment),Ya=u(),$s=o("p"),Xt=e("Create a batch of examples and dynamically pad them with "),ja=o("code"),Zt=e("DataCollatorForCTCWithPadding"),se=e(":"),Va=u(),w(Vs.$$.fragment),Na=u(),fs=o("h2"),js=o("a"),ba=o("span"),w(Ns.$$.fragment),ae=u(),va=o("span"),te=e("Train"),Ha=u(),w(bs.$$.fragment),Ba=u(),w(vs.$$.fragment),this.h()},l(s){const l=Ve('[data-svelte="svelte-1phssyn"]',document.head);c=r(l,"META",{name:!0,content:!0}),l.forEach(t),j=d(s),h=r(s,"H1",{class:!0});var Hs=i(h);$=r(Hs,"A",{id:!0,class:!0,href:!0});var wa=i($);v=r(wa,"SPAN",{});var ya=i(v);y(g.$$.fragment,ya),ya.forEach(t),wa.forEach(t),q=d(Hs),T=r(Hs,"SPAN",{});var ne=i(T);P=n(ne,"Automatic speech recognition"),ne.forEach(t),Hs.forEach(t),_=d(s),y(D.$$.fragment,s),C=d(s),K=r(s,"P",{});var le=i(K);as=n(le,"Automatic speech recognition (ASR) converts a speech signal to text. It is an example of a sequence-to-sequence task, going from a sequence of audio inputs to textual outputs. Voice assistants like Siri and Alexa utilize ASR models to assist users."),le.forEach(t),us=d(s),S=r(s,"P",{});var Xs=i(S);R=n(Xs,"This guide will show you how to fine-tune "),I=r(Xs,"A",{href:!0,rel:!0});var oe=i(I);ys=n(oe,"Wav2Vec2"),oe.forEach(t),ts=n(Xs," on the "),Q=r(Xs,"A",{href:!0,rel:!0});var re=i(Q);ks=n(re,"MInDS-14"),re.forEach(t),xs=n(Xs," dataset to transcribe audio to text."),Xs.forEach(t),Y=d(s),y(V.$$.fragment,s),X=d(s),M=r(s,"H2",{class:!0});var Ja=i(M);N=r(Ja,"A",{id:!0,class:!0,href:!0});var ie=i(N);cs=r(ie,"SPAN",{});var pe=i(cs);y(L.$$.fragment,pe),pe.forEach(t),ie.forEach(t),Es=d(Ja),H=r(Ja,"SPAN",{});var ce=i(H);As=n(ce,"Load MInDS-14 dataset"),ce.forEach(t),Ja.forEach(t),ds=d(s),W=r(s,"P",{});var Ka=i(W);Z=n(Ka,"Load the "),B=r(Ka,"A",{href:!0,rel:!0});var he=i(B);f=n(he,"MInDS-14"),he.forEach(t),b=n(Ka," from the \u{1F917} Datasets library:"),Ka.forEach(t),es=d(s),y(ns.$$.fragment,s),qs=d(s),z=r(s,"P",{});var fe=i(z);ms=n(fe,"Split this dataset into a train and test set:"),fe.forEach(t),Ts=d(s),y(G.$$.fragment,s),Ps=d(s),ss=r(s,"P",{});var ue=i(ss);Bs=n(ue,"Then take a look at the dataset:"),ue.forEach(t),ka=d(s),y(Cs.$$.fragment,s),xa=d(s),O=r(s,"P",{});var ps=i(O);nt=n(ps,"While the dataset contains a lot of helpful information, like "),ea=r(ps,"CODE",{});var de=i(ea);lt=n(de,"lang_id"),de.forEach(t),ot=n(ps," and "),na=r(ps,"CODE",{});var me=i(na);rt=n(me,"intent_class"),me.forEach(t),it=n(ps,", you will focus on the "),la=r(ps,"CODE",{});var ge=i(la);pt=n(ge,"audio"),ge.forEach(t),ct=n(ps," and "),oa=r(ps,"CODE",{});var _e=i(oa);ht=n(_e,"transcription"),_e.forEach(t),ft=n(ps," columns in this guide. Remove the other columns:"),ps.forEach(t),Ea=d(s),y(Ds.$$.fragment,s),Aa=d(s),Gs=r(s,"P",{});var $e=i(Gs);ut=n($e,"Take a look at the example again:"),$e.forEach(t),qa=d(s),y(Ss.$$.fragment,s),Ta=d(s),ls=r(s,"P",{});var Zs=i(ls);dt=n(Zs,"The "),ra=r(Zs,"CODE",{});var je=i(ra);mt=n(je,"audio"),je.forEach(t),gt=n(Zs," column contains a 1-dimensional "),ia=r(Zs,"CODE",{});var be=i(ia);_t=n(be,"array"),be.forEach(t),$t=n(Zs," of the speech signal that must be called to load and resample the audio file."),Zs.forEach(t),Pa=d(s),hs=r(s,"H2",{class:!0});var Qa=i(hs);gs=r(Qa,"A",{id:!0,class:!0,href:!0});var ve=i(gs);pa=r(ve,"SPAN",{});var we=i(pa);y(Is.$$.fragment,we),we.forEach(t),ve.forEach(t),jt=d(Qa),ca=r(Qa,"SPAN",{});var ye=i(ca);bt=n(ye,"Preprocess"),ye.forEach(t),Qa.forEach(t),Ca=d(s),Js=r(s,"P",{});var ke=i(Js);vt=n(ke,"Load the Wav2Vec2 processor to process the audio signal and transcribed text:"),ke.forEach(t),Da=d(s),y(Ls.$$.fragment,s),Sa=d(s),_s=r(s,"P",{});var Xa=i(_s);wt=n(Xa,"The "),Rs=r(Xa,"A",{href:!0,rel:!0});var xe=i(Rs);yt=n(xe,"MInDS-14"),xe.forEach(t),kt=n(Xa," dataset has a sampling rate of 8000khz. You will need to resample the dataset to use the pretrained Wav2Vec2 model:"),Xa.forEach(t),Ia=d(s),y(Os.$$.fragment,s),La=d(s),Ks=r(s,"P",{});var Ee=i(Ks);xt=n(Ee,"The preprocessing function needs to:"),Ee.forEach(t),Ra=d(s),os=r(s,"OL",{});var sa=i(os);Ms=r(sa,"LI",{});var Za=i(Ms);Et=n(Za,"Call the "),ha=r(Za,"CODE",{});var Ae=i(ha);At=n(Ae,"audio"),Ae.forEach(t),qt=n(Za," column to load and resample the audio file."),Za.forEach(t),Tt=d(sa),Ws=r(sa,"LI",{});var st=i(Ws);Pt=n(st,"Extract the "),fa=r(st,"CODE",{});var qe=i(fa);Ct=n(qe,"input_values"),qe.forEach(t),Dt=n(st," from the audio file."),st.forEach(t),St=d(sa),ua=r(sa,"LI",{});var Te=i(ua);It=n(Te,"Typically, when you call the processor, you call the feature extractor. Since you also want to tokenize text, instruct the processor to call the tokenizer instead with a context manager."),Te.forEach(t),sa.forEach(t),Oa=d(s),y(zs.$$.fragment,s),Ma=d(s),rs=r(s,"P",{});var aa=i(rs);Lt=n(aa,"Use \u{1F917} Datasets "),Us=r(aa,"A",{href:!0,rel:!0});var Pe=i(Us);Rt=n(Pe,"map"),Pe.forEach(t),Ot=n(aa," function to apply the preprocessing function over the entire dataset. You can speed up the map function by increasing the number of processes with "),da=r(aa,"CODE",{});var Ce=i(da);Mt=n(Ce,"num_proc"),Ce.forEach(t),Wt=n(aa,". Remove the columns you don\u2019t need:"),aa.forEach(t),Wa=d(s),y(Fs.$$.fragment,s),za=d(s),J=r(s,"P",{});var ws=i(J);zt=n(ws,"\u{1F917} Transformers doesn\u2019t have a data collator for automatic speech recognition, so you will need to create one. You can adapt the "),Qs=r(ws,"A",{href:!0});var De=i(Qs);Ut=n(De,"DataCollatorWithPadding"),De.forEach(t),Ft=n(ws," to create a batch of examples for automatic speech recognition. It will also dynamically pad your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),ma=r(ws,"CODE",{});var Se=i(ma);Yt=n(Se,"tokenizer"),Se.forEach(t),Vt=n(ws," function by setting "),ga=r(ws,"CODE",{});var Ie=i(ga);Nt=n(Ie,"padding=True"),Ie.forEach(t),Ht=n(ws,", dynamic padding is more efficient."),ws.forEach(t),Ua=d(s),is=r(s,"P",{});var ta=i(is);Bt=n(ta,"Unlike other data collators, this specific data collator needs to apply a different padding method to "),_a=r(ta,"CODE",{});var Le=i(_a);Gt=n(Le,"input_values"),Le.forEach(t),Jt=n(ta," and "),$a=r(ta,"CODE",{});var Re=i($a);Kt=n(Re,"labels"),Re.forEach(t),Qt=n(ta,". You can apply a different padding method with a context manager:"),ta.forEach(t),Fa=d(s),y(Ys.$$.fragment,s),Ya=d(s),$s=r(s,"P",{});var at=i($s);Xt=n(at,"Create a batch of examples and dynamically pad them with "),ja=r(at,"CODE",{});var Oe=i(ja);Zt=n(Oe,"DataCollatorForCTCWithPadding"),Oe.forEach(t),se=n(at,":"),at.forEach(t),Va=d(s),y(Vs.$$.fragment,s),Na=d(s),fs=r(s,"H2",{class:!0});var tt=i(fs);js=r(tt,"A",{id:!0,class:!0,href:!0});var Me=i(js);ba=r(Me,"SPAN",{});var We=i(ba);y(Ns.$$.fragment,We),We.forEach(t),Me.forEach(t),ae=d(tt),va=r(tt,"SPAN",{});var ze=i(va);te=n(ze,"Train"),ze.forEach(t),tt.forEach(t),Ha=d(s),y(bs.$$.fragment,s),Ba=d(s),y(vs.$$.fragment,s),this.h()},h(){m(c,"name","hf:doc:metadata"),m(c,"content",JSON.stringify(an)),m($,"id","automatic-speech-recognition"),m($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m($,"href","#automatic-speech-recognition"),m(h,"class","relative group"),m(I,"href","https://huggingface.co/facebook/wav2vec2-base"),m(I,"rel","nofollow"),m(Q,"href","https://huggingface.co/datasets/PolyAI/minds14"),m(Q,"rel","nofollow"),m(N,"id","load-minds14-dataset"),m(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(N,"href","#load-minds14-dataset"),m(M,"class","relative group"),m(B,"href","https://huggingface.co/datasets/PolyAI/minds14"),m(B,"rel","nofollow"),m(gs,"id","preprocess"),m(gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(gs,"href","#preprocess"),m(hs,"class","relative group"),m(Rs,"href","https://huggingface.co/datasets/PolyAI/minds14"),m(Rs,"rel","nofollow"),m(Us,"href","https://huggingface.co/docs/datasets/v2.5.1/en/package_reference/main_classes#datasets.Dataset.map"),m(Us,"rel","nofollow"),m(Qs,"href","/docs/transformers/v4.22.1/en/main_classes/data_collator#transformers.DataCollatorWithPadding"),m(js,"id","train"),m(js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(js,"href","#train"),m(fs,"class","relative group")},m(s,l){a(document.head,c),p(s,j,l),p(s,h,l),a(h,$),a($,v),k(g,v,null),a(h,q),a(h,T),a(T,P),p(s,_,l),k(D,s,l),p(s,C,l),p(s,K,l),a(K,as),p(s,us,l),p(s,S,l),a(S,R),a(S,I),a(I,ys),a(S,ts),a(S,Q),a(Q,ks),a(S,xs),p(s,Y,l),k(V,s,l),p(s,X,l),p(s,M,l),a(M,N),a(N,cs),k(L,cs,null),a(M,Es),a(M,H),a(H,As),p(s,ds,l),p(s,W,l),a(W,Z),a(W,B),a(B,f),a(W,b),p(s,es,l),k(ns,s,l),p(s,qs,l),p(s,z,l),a(z,ms),p(s,Ts,l),k(G,s,l),p(s,Ps,l),p(s,ss,l),a(ss,Bs),p(s,ka,l),k(Cs,s,l),p(s,xa,l),p(s,O,l),a(O,nt),a(O,ea),a(ea,lt),a(O,ot),a(O,na),a(na,rt),a(O,it),a(O,la),a(la,pt),a(O,ct),a(O,oa),a(oa,ht),a(O,ft),p(s,Ea,l),k(Ds,s,l),p(s,Aa,l),p(s,Gs,l),a(Gs,ut),p(s,qa,l),k(Ss,s,l),p(s,Ta,l),p(s,ls,l),a(ls,dt),a(ls,ra),a(ra,mt),a(ls,gt),a(ls,ia),a(ia,_t),a(ls,$t),p(s,Pa,l),p(s,hs,l),a(hs,gs),a(gs,pa),k(Is,pa,null),a(hs,jt),a(hs,ca),a(ca,bt),p(s,Ca,l),p(s,Js,l),a(Js,vt),p(s,Da,l),k(Ls,s,l),p(s,Sa,l),p(s,_s,l),a(_s,wt),a(_s,Rs),a(Rs,yt),a(_s,kt),p(s,Ia,l),k(Os,s,l),p(s,La,l),p(s,Ks,l),a(Ks,xt),p(s,Ra,l),p(s,os,l),a(os,Ms),a(Ms,Et),a(Ms,ha),a(ha,At),a(Ms,qt),a(os,Tt),a(os,Ws),a(Ws,Pt),a(Ws,fa),a(fa,Ct),a(Ws,Dt),a(os,St),a(os,ua),a(ua,It),p(s,Oa,l),k(zs,s,l),p(s,Ma,l),p(s,rs,l),a(rs,Lt),a(rs,Us),a(Us,Rt),a(rs,Ot),a(rs,da),a(da,Mt),a(rs,Wt),p(s,Wa,l),k(Fs,s,l),p(s,za,l),p(s,J,l),a(J,zt),a(J,Qs),a(Qs,Ut),a(J,Ft),a(J,ma),a(ma,Yt),a(J,Vt),a(J,ga),a(ga,Nt),a(J,Ht),p(s,Ua,l),p(s,is,l),a(is,Bt),a(is,_a),a(_a,Gt),a(is,Jt),a(is,$a),a($a,Kt),a(is,Qt),p(s,Fa,l),k(Ys,s,l),p(s,Ya,l),p(s,$s,l),a($s,Xt),a($s,ja),a(ja,Zt),a($s,se),p(s,Va,l),k(Vs,s,l),p(s,Na,l),p(s,fs,l),a(fs,js),a(js,ba),k(Ns,ba,null),a(fs,ae),a(fs,va),a(va,te),p(s,Ha,l),k(bs,s,l),p(s,Ba,l),k(vs,s,l),Ga=!0},p(s,[l]){const Hs={};l&2&&(Hs.$$scope={dirty:l,ctx:s}),V.$set(Hs);const wa={};l&2&&(wa.$$scope={dirty:l,ctx:s}),bs.$set(wa);const ya={};l&2&&(ya.$$scope={dirty:l,ctx:s}),vs.$set(ya)},i(s){Ga||(x(g.$$.fragment,s),x(D.$$.fragment,s),x(V.$$.fragment,s),x(L.$$.fragment,s),x(ns.$$.fragment,s),x(G.$$.fragment,s),x(Cs.$$.fragment,s),x(Ds.$$.fragment,s),x(Ss.$$.fragment,s),x(Is.$$.fragment,s),x(Ls.$$.fragment,s),x(Os.$$.fragment,s),x(zs.$$.fragment,s),x(Fs.$$.fragment,s),x(Ys.$$.fragment,s),x(Vs.$$.fragment,s),x(Ns.$$.fragment,s),x(bs.$$.fragment,s),x(vs.$$.fragment,s),Ga=!0)},o(s){E(g.$$.fragment,s),E(D.$$.fragment,s),E(V.$$.fragment,s),E(L.$$.fragment,s),E(ns.$$.fragment,s),E(G.$$.fragment,s),E(Cs.$$.fragment,s),E(Ds.$$.fragment,s),E(Ss.$$.fragment,s),E(Is.$$.fragment,s),E(Ls.$$.fragment,s),E(Os.$$.fragment,s),E(zs.$$.fragment,s),E(Fs.$$.fragment,s),E(Ys.$$.fragment,s),E(Vs.$$.fragment,s),E(Ns.$$.fragment,s),E(bs.$$.fragment,s),E(vs.$$.fragment,s),Ga=!1},d(s){t(c),s&&t(j),s&&t(h),A(g),s&&t(_),A(D,s),s&&t(C),s&&t(K),s&&t(us),s&&t(S),s&&t(Y),A(V,s),s&&t(X),s&&t(M),A(L),s&&t(ds),s&&t(W),s&&t(es),A(ns,s),s&&t(qs),s&&t(z),s&&t(Ts),A(G,s),s&&t(Ps),s&&t(ss),s&&t(ka),A(Cs,s),s&&t(xa),s&&t(O),s&&t(Ea),A(Ds,s),s&&t(Aa),s&&t(Gs),s&&t(qa),A(Ss,s),s&&t(Ta),s&&t(ls),s&&t(Pa),s&&t(hs),A(Is),s&&t(Ca),s&&t(Js),s&&t(Da),A(Ls,s),s&&t(Sa),s&&t(_s),s&&t(Ia),A(Os,s),s&&t(La),s&&t(Ks),s&&t(Ra),s&&t(os),s&&t(Oa),A(zs,s),s&&t(Ma),s&&t(rs),s&&t(Wa),A(Fs,s),s&&t(za),s&&t(J),s&&t(Ua),s&&t(is),s&&t(Fa),A(Ys,s),s&&t(Ya),s&&t($s),s&&t(Va),A(Vs,s),s&&t(Na),s&&t(fs),A(Ns),s&&t(Ha),A(bs,s),s&&t(Ba),A(vs,s)}}}const an={local:"automatic-speech-recognition",sections:[{local:"load-minds14-dataset",title:"Load MInDS-14 dataset"},{local:"preprocess",title:"Preprocess"},{local:"train",title:"Train"}],title:"Automatic speech recognition"};function tn(F){return Ne(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class cn extends Ue{constructor(c){super();Fe(this,c,tn,sn,Ye,{})}}export{cn as default,an as metadata};
