import{S as La,i as Oa,s as Wa,e as i,k as g,w as x,t as n,M as Ba,c as p,d as a,m as b,a as f,x as z,h as l,b as w,G as t,g as c,y as A,q,o as C,B as P,v as Na,L as Ma}from"../../chunks/vendor-hf-doc-builder.js";import{T as pt}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Ra}from"../../chunks/Youtube-hf-doc-builder.js";import{I as At}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ie}from"../../chunks/CodeBlock-hf-doc-builder.js";import{F as Ia,M as qt}from"../../chunks/Markdown-hf-doc-builder.js";function Ua(S){let s,m,r,u,$;return{c(){s=i("p"),m=n("See the text classification "),r=i("a"),u=n("task page"),$=n(" for more information about other forms of text classification and their associated models, datasets, and metrics."),this.h()},l(_){s=p(_,"P",{});var v=f(s);m=l(v,"See the text classification "),r=p(v,"A",{href:!0,rel:!0});var T=f(r);u=l(T,"task page"),T.forEach(a),$=l(v," for more information about other forms of text classification and their associated models, datasets, and metrics."),v.forEach(a),this.h()},h(){w(r,"href","https://huggingface.co/tasks/text-classification"),w(r,"rel","nofollow")},m(_,v){c(_,s,v),t(s,m),t(s,r),t(r,u),t(s,$)},d(_){_&&a(s)}}}function Ga(S){let s,m;return s=new ie({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`}}),{c(){x(s.$$.fragment)},l(r){z(s.$$.fragment,r)},m(r,u){A(s,r,u),m=!0},p:Ma,i(r){m||(q(s.$$.fragment,r),m=!0)},o(r){C(s.$$.fragment,r),m=!1},d(r){P(s,r)}}}function Ha(S){let s,m;return s=new qt({props:{$$slots:{default:[Ga]},$$scope:{ctx:S}}}),{c(){x(s.$$.fragment)},l(r){z(s.$$.fragment,r)},m(r,u){A(s,r,u),m=!0},p(r,u){const $={};u&2&&($.$$scope={dirty:u,ctx:r}),s.$set($)},i(r){m||(q(s.$$.fragment,r),m=!0)},o(r){C(s.$$.fragment,r),m=!1},d(r){P(s,r)}}}function Ya(S){let s,m;return s=new ie({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){x(s.$$.fragment)},l(r){z(s.$$.fragment,r)},m(r,u){A(s,r,u),m=!0},p:Ma,i(r){m||(q(s.$$.fragment,r),m=!0)},o(r){C(s.$$.fragment,r),m=!1},d(r){P(s,r)}}}function Ka(S){let s,m;return s=new qt({props:{$$slots:{default:[Ya]},$$scope:{ctx:S}}}),{c(){x(s.$$.fragment)},l(r){z(s.$$.fragment,r)},m(r,u){A(s,r,u),m=!0},p(r,u){const $={};u&2&&($.$$scope={dirty:u,ctx:r}),s.$set($)},i(r){m||(q(s.$$.fragment,r),m=!0)},o(r){C(s.$$.fragment,r),m=!1},d(r){P(s,r)}}}function Va(S){let s,m,r,u,$,_,v,T;return{c(){s=i("p"),m=n("If you aren\u2019t familiar with fine-tuning a model with the "),r=i("a"),u=n("Trainer"),$=n(", take a look at the basic tutorial "),_=i("a"),v=n("here"),T=n("!"),this.h()},l(E){s=p(E,"P",{});var j=f(s);m=l(j,"If you aren\u2019t familiar with fine-tuning a model with the "),r=p(j,"A",{href:!0});var D=f(r);u=l(D,"Trainer"),D.forEach(a),$=l(j,", take a look at the basic tutorial "),_=p(j,"A",{href:!0});var I=f(_);v=l(I,"here"),I.forEach(a),T=l(j,"!"),j.forEach(a),this.h()},h(){w(r,"href","/docs/transformers/v4.22.1/en/main_classes/trainer#transformers.Trainer"),w(_,"href","../training#finetune-with-trainer")},m(E,j){c(E,s,j),t(s,m),t(s,r),t(r,u),t(s,$),t(s,_),t(_,v),t(s,T)},d(E){E&&a(s)}}}function Ja(S){let s,m,r,u,$,_,v;return{c(){s=i("p"),m=i("a"),r=n("Trainer"),u=n(" will apply dynamic padding by default when you pass "),$=i("code"),_=n("tokenizer"),v=n(" to it. In this case, you don\u2019t need to specify a data collator explicitly."),this.h()},l(T){s=p(T,"P",{});var E=f(s);m=p(E,"A",{href:!0});var j=f(m);r=l(j,"Trainer"),j.forEach(a),u=l(E," will apply dynamic padding by default when you pass "),$=p(E,"CODE",{});var D=f($);_=l(D,"tokenizer"),D.forEach(a),v=l(E," to it. In this case, you don\u2019t need to specify a data collator explicitly."),E.forEach(a),this.h()},h(){w(m,"href","/docs/transformers/v4.22.1/en/main_classes/trainer#transformers.Trainer")},m(T,E){c(T,s,E),t(s,m),t(m,r),t(s,u),t(s,$),t($,_),t(s,v)},d(T){T&&a(s)}}}function Qa(S){let s,m,r,u,$,_,v,T,E,j,D,I,K,L,V,N,R,J,Q,ue,M,de,ae,pe,O,fe,F,X,W,G,_e,se,Z,H,B,Y;return v=new ie({props:{code:`from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),E=new pt({props:{$$slots:{default:[Va]},$$scope:{ctx:S}}}),Z=new ie({props:{code:`training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=5,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_imdb["train"],
    eval_dataset=tokenized_imdb["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">5</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=tokenized_imdb[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),B=new pt({props:{$$slots:{default:[Ja]},$$scope:{ctx:S}}}),{c(){s=i("p"),m=n("Load DistilBERT with "),r=i("a"),u=n("AutoModelForSequenceClassification"),$=n(" along with the number of expected labels:"),_=g(),x(v.$$.fragment),T=g(),x(E.$$.fragment),j=g(),D=i("p"),I=n("At this point, only three steps remain:"),K=g(),L=i("ol"),V=i("li"),N=n("Define your training hyperparameters in "),R=i("a"),J=n("TrainingArguments"),Q=n("."),ue=g(),M=i("li"),de=n("Pass the training arguments to "),ae=i("a"),pe=n("Trainer"),O=n(" along with the model, dataset, tokenizer, and data collator."),fe=g(),F=i("li"),X=n("Call "),W=i("a"),G=n("train()"),_e=n(" to fine-tune your model."),se=g(),x(Z.$$.fragment),H=g(),x(B.$$.fragment),this.h()},l(h){s=p(h,"P",{});var y=f(s);m=l(y,"Load DistilBERT with "),r=p(y,"A",{href:!0});var ee=f(r);u=l(ee,"AutoModelForSequenceClassification"),ee.forEach(a),$=l(y," along with the number of expected labels:"),y.forEach(a),_=b(h),z(v.$$.fragment,h),T=b(h),z(E.$$.fragment,h),j=b(h),D=p(h,"P",{});var te=f(D);I=l(te,"At this point, only three steps remain:"),te.forEach(a),K=b(h),L=p(h,"OL",{});var re=f(L);V=p(re,"LI",{});var oe=f(V);N=l(oe,"Define your training hyperparameters in "),R=p(oe,"A",{href:!0});var U=f(R);J=l(U,"TrainingArguments"),U.forEach(a),Q=l(oe,"."),oe.forEach(a),ue=b(re),M=p(re,"LI",{});var ne=f(M);de=l(ne,"Pass the training arguments to "),ae=p(ne,"A",{href:!0});var o=f(ae);pe=l(o,"Trainer"),o.forEach(a),O=l(ne," along with the model, dataset, tokenizer, and data collator."),ne.forEach(a),fe=b(re),F=p(re,"LI",{});var k=f(F);X=l(k,"Call "),W=p(k,"A",{href:!0});var ce=f(W);G=l(ce,"train()"),ce.forEach(a),_e=l(k," to fine-tune your model."),k.forEach(a),re.forEach(a),se=b(h),z(Z.$$.fragment,h),H=b(h),z(B.$$.fragment,h),this.h()},h(){w(r,"href","/docs/transformers/v4.22.1/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),w(R,"href","/docs/transformers/v4.22.1/en/main_classes/trainer#transformers.TrainingArguments"),w(ae,"href","/docs/transformers/v4.22.1/en/main_classes/trainer#transformers.Trainer"),w(W,"href","/docs/transformers/v4.22.1/en/main_classes/trainer#transformers.Trainer.train")},m(h,y){c(h,s,y),t(s,m),t(s,r),t(r,u),t(s,$),c(h,_,y),A(v,h,y),c(h,T,y),A(E,h,y),c(h,j,y),c(h,D,y),t(D,I),c(h,K,y),c(h,L,y),t(L,V),t(V,N),t(V,R),t(R,J),t(V,Q),t(L,ue),t(L,M),t(M,de),t(M,ae),t(ae,pe),t(M,O),t(L,fe),t(L,F),t(F,X),t(F,W),t(W,G),t(F,_e),c(h,se,y),A(Z,h,y),c(h,H,y),A(B,h,y),Y=!0},p(h,y){const ee={};y&2&&(ee.$$scope={dirty:y,ctx:h}),E.$set(ee);const te={};y&2&&(te.$$scope={dirty:y,ctx:h}),B.$set(te)},i(h){Y||(q(v.$$.fragment,h),q(E.$$.fragment,h),q(Z.$$.fragment,h),q(B.$$.fragment,h),Y=!0)},o(h){C(v.$$.fragment,h),C(E.$$.fragment,h),C(Z.$$.fragment,h),C(B.$$.fragment,h),Y=!1},d(h){h&&a(s),h&&a(_),P(v,h),h&&a(T),P(E,h),h&&a(j),h&&a(D),h&&a(K),h&&a(L),h&&a(se),P(Z,h),h&&a(H),P(B,h)}}}function Xa(S){let s,m;return s=new qt({props:{$$slots:{default:[Qa]},$$scope:{ctx:S}}}),{c(){x(s.$$.fragment)},l(r){z(s.$$.fragment,r)},m(r,u){A(s,r,u),m=!0},p(r,u){const $={};u&2&&($.$$scope={dirty:u,ctx:r}),s.$set($)},i(r){m||(q(s.$$.fragment,r),m=!0)},o(r){C(s.$$.fragment,r),m=!1},d(r){P(s,r)}}}function Za(S){let s,m,r,u,$;return{c(){s=i("p"),m=n("If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),r=i("a"),u=n("here"),$=n("!"),this.h()},l(_){s=p(_,"P",{});var v=f(s);m=l(v,"If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),r=p(v,"A",{href:!0});var T=f(r);u=l(T,"here"),T.forEach(a),$=l(v,"!"),v.forEach(a),this.h()},h(){w(r,"href","training#finetune-with-keras")},m(_,v){c(_,s,v),t(s,m),t(s,r),t(r,u),t(s,$)},d(_){_&&a(s)}}}function es(S){let s,m,r,u,$,_,v,T,E,j,D,I,K,L,V,N,R,J,Q,ue,M,de,ae,pe,O,fe,F,X,W,G,_e,se,Z,H,B,Y,h,y,ee,te,re,oe,U,ne;return j=new ie({props:{code:`tf_train_set = model.prepare_tf_dataset(
    tokenized_imdb["train"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_set = model.prepare_tf_dataset(
    tokenized_imdb["test"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = model.prepare_tf_dataset(
<span class="hljs-meta">... </span>    tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_validation_set = model.prepare_tf_dataset(
<span class="hljs-meta">... </span>    tokenized_imdb[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`}}),I=new pt({props:{$$slots:{default:[Za]},$$scope:{ctx:S}}}),R=new ie({props:{code:`from transformers import create_optimizer
import tensorflow as tf

batch_size = 16
num_epochs = 5
batches_per_epoch = len(tokenized_imdb["train"]) // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = <span class="hljs-number">16</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_epochs = <span class="hljs-number">5</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>batches_per_epoch = <span class="hljs-built_in">len</span>(tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
<span class="hljs-meta">&gt;&gt;&gt; </span>total_train_steps = <span class="hljs-built_in">int</span>(batches_per_epoch * num_epochs)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer, schedule = create_optimizer(init_lr=<span class="hljs-number">2e-5</span>, num_warmup_steps=<span class="hljs-number">0</span>, num_train_steps=total_train_steps)`}}),O=new ie({props:{code:`from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),H=new ie({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),U=new ie({props:{code:"model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=<span class="hljs-number">3</span>)'}}),{c(){s=i("p"),m=n("To fine-tune a model in TensorFlow, start by converting your datasets to the "),r=i("code"),u=n("tf.data.Dataset"),$=n(" format with "),_=i("a"),v=n("prepare_tf_dataset()"),T=n("."),E=g(),x(j.$$.fragment),D=g(),x(I.$$.fragment),K=g(),L=i("p"),V=n("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),N=g(),x(R.$$.fragment),J=g(),Q=i("p"),ue=n("Load DistilBERT with "),M=i("a"),de=n("TFAutoModelForSequenceClassification"),ae=n(" along with the number of expected labels:"),pe=g(),x(O.$$.fragment),fe=g(),F=i("p"),X=n("Configure the model for training with "),W=i("a"),G=i("code"),_e=n("compile"),se=n(":"),Z=g(),x(H.$$.fragment),B=g(),Y=i("p"),h=n("Call "),y=i("a"),ee=i("code"),te=n("fit"),re=n(" to fine-tune the model:"),oe=g(),x(U.$$.fragment),this.h()},l(o){s=p(o,"P",{});var k=f(s);m=l(k,"To fine-tune a model in TensorFlow, start by converting your datasets to the "),r=p(k,"CODE",{});var ce=f(r);u=l(ce,"tf.data.Dataset"),ce.forEach(a),$=l(k," format with "),_=p(k,"A",{href:!0});var $e=f(_);v=l($e,"prepare_tf_dataset()"),$e.forEach(a),T=l(k,"."),k.forEach(a),E=b(o),z(j.$$.fragment,o),D=b(o),z(I.$$.fragment,o),K=b(o),L=p(o,"P",{});var ve=f(L);V=l(ve,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),ve.forEach(a),N=b(o),z(R.$$.fragment,o),J=b(o),Q=p(o,"P",{});var ge=f(Q);ue=l(ge,"Load DistilBERT with "),M=p(ge,"A",{href:!0});var Be=f(M);de=l(Be,"TFAutoModelForSequenceClassification"),Be.forEach(a),ae=l(ge," along with the number of expected labels:"),ge.forEach(a),pe=b(o),z(O.$$.fragment,o),fe=b(o),F=p(o,"P",{});var je=f(F);X=l(je,"Configure the model for training with "),W=p(je,"A",{href:!0,rel:!0});var Ne=f(W);G=p(Ne,"CODE",{});var me=f(G);_e=l(me,"compile"),me.forEach(a),Ne.forEach(a),se=l(je,":"),je.forEach(a),Z=b(o),z(H.$$.fragment,o),B=b(o),Y=p(o,"P",{});var be=f(Y);h=l(be,"Call "),y=p(be,"A",{href:!0,rel:!0});var Re=f(y);ee=p(Re,"CODE",{});var Ue=f(ee);te=l(Ue,"fit"),Ue.forEach(a),Re.forEach(a),re=l(be," to fine-tune the model:"),be.forEach(a),oe=b(o),z(U.$$.fragment,o),this.h()},h(){w(_,"href","/docs/transformers/v4.22.1/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset"),w(M,"href","/docs/transformers/v4.22.1/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification"),w(W,"href","https://keras.io/api/models/model_training_apis/#compile-method"),w(W,"rel","nofollow"),w(y,"href","https://keras.io/api/models/model_training_apis/#fit-method"),w(y,"rel","nofollow")},m(o,k){c(o,s,k),t(s,m),t(s,r),t(r,u),t(s,$),t(s,_),t(_,v),t(s,T),c(o,E,k),A(j,o,k),c(o,D,k),A(I,o,k),c(o,K,k),c(o,L,k),t(L,V),c(o,N,k),A(R,o,k),c(o,J,k),c(o,Q,k),t(Q,ue),t(Q,M),t(M,de),t(Q,ae),c(o,pe,k),A(O,o,k),c(o,fe,k),c(o,F,k),t(F,X),t(F,W),t(W,G),t(G,_e),t(F,se),c(o,Z,k),A(H,o,k),c(o,B,k),c(o,Y,k),t(Y,h),t(Y,y),t(y,ee),t(ee,te),t(Y,re),c(o,oe,k),A(U,o,k),ne=!0},p(o,k){const ce={};k&2&&(ce.$$scope={dirty:k,ctx:o}),I.$set(ce)},i(o){ne||(q(j.$$.fragment,o),q(I.$$.fragment,o),q(R.$$.fragment,o),q(O.$$.fragment,o),q(H.$$.fragment,o),q(U.$$.fragment,o),ne=!0)},o(o){C(j.$$.fragment,o),C(I.$$.fragment,o),C(R.$$.fragment,o),C(O.$$.fragment,o),C(H.$$.fragment,o),C(U.$$.fragment,o),ne=!1},d(o){o&&a(s),o&&a(E),P(j,o),o&&a(D),P(I,o),o&&a(K),o&&a(L),o&&a(N),P(R,o),o&&a(J),o&&a(Q),o&&a(pe),P(O,o),o&&a(fe),o&&a(F),o&&a(Z),P(H,o),o&&a(B),o&&a(Y),o&&a(oe),P(U,o)}}}function ts(S){let s,m;return s=new qt({props:{$$slots:{default:[es]},$$scope:{ctx:S}}}),{c(){x(s.$$.fragment)},l(r){z(s.$$.fragment,r)},m(r,u){A(s,r,u),m=!0},p(r,u){const $={};u&2&&($.$$scope={dirty:u,ctx:r}),s.$set($)},i(r){m||(q(s.$$.fragment,r),m=!0)},o(r){C(s.$$.fragment,r),m=!1},d(r){P(s,r)}}}function as(S){let s,m,r,u,$,_,v,T;return{c(){s=i("p"),m=n(`For a more in-depth example of how to fine-tune a model for text classification, take a look at the corresponding
`),r=i("a"),u=n("PyTorch notebook"),$=n(`
or `),_=i("a"),v=n("TensorFlow notebook"),T=n("."),this.h()},l(E){s=p(E,"P",{});var j=f(s);m=l(j,`For a more in-depth example of how to fine-tune a model for text classification, take a look at the corresponding
`),r=p(j,"A",{href:!0,rel:!0});var D=f(r);u=l(D,"PyTorch notebook"),D.forEach(a),$=l(j,`
or `),_=p(j,"A",{href:!0,rel:!0});var I=f(_);v=l(I,"TensorFlow notebook"),I.forEach(a),T=l(j,"."),j.forEach(a),this.h()},h(){w(r,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb"),w(r,"rel","nofollow"),w(_,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb"),w(_,"rel","nofollow")},m(E,j){c(E,s,j),t(s,m),t(s,r),t(r,u),t(s,$),t(s,_),t(_,v),t(s,T)},d(E){E&&a(s)}}}function ss(S){let s,m,r,u,$,_,v,T,E,j,D,I,K,L,V,N,R,J,Q,ue,M,de,ae,pe,O,fe,F,X,W,G,_e,se,Z,H,B,Y,h,y,ee,te,re,oe,U,ne,o,k,ce,$e,ve,ge,Be,je,Ne,me,be,Re,Ue,Ye,Ct,Pt,Ke,Dt,St,ft,ke,Ee,Ve,De,Ft,Je,It,ct,Te,Mt,Qe,Lt,Ot,mt,Se,ht,xe,Wt,Xe,Bt,Nt,ut,Fe,dt,he,Rt,Ie,Ut,Gt,Ze,Ht,Yt,et,Kt,Vt,_t,Me,$t,le,Jt,Ge,Qt,Xt,tt,Zt,ea,at,ta,aa,st,sa,ra,gt,ze,bt,ye,Ae,rt,Le,oa,ot,na,wt,qe,vt,Ce,kt;return _=new At({}),D=new Ra({props:{id:"leNG9fN9FQU"}}),O=new pt({props:{$$slots:{default:[Ua]},$$scope:{ctx:S}}}),G=new At({}),y=new ie({props:{code:`from datasets import load_dataset

imdb = load_dataset("imdb")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>imdb = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>)`}}),U=new ie({props:{code:'imdb["test"][0]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>imdb[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-number">0</span>]
{
    <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn&#x27;t match the background, and painfully one-dimensional characters cannot be overcome with a &#x27;sci-fi&#x27; setting. (I&#x27;m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It&#x27;s not. It&#x27;s clich\xE9d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It&#x27;s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it&#x27;s rubbish as they have to always say \\&quot;Gene Roddenberry&#x27;s Earth...\\&quot; otherwise people would not continue watching. Roddenberry&#x27;s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.&quot;</span>,
}`}}),De=new At({}),Se=new ie({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),Fe=new ie({props:{code:`def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>], truncation=<span class="hljs-literal">True</span>)`}}),Me=new ie({props:{code:"tokenized_imdb = imdb.map(preprocess_function, batched=True)",highlighted:'tokenized_imdb = imdb.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)'}}),ze=new Ia({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ka],pytorch:[Ha]},$$scope:{ctx:S}}}),Le=new At({}),qe=new Ia({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[ts],pytorch:[Xa]},$$scope:{ctx:S}}}),Ce=new pt({props:{$$slots:{default:[as]},$$scope:{ctx:S}}}),{c(){s=i("meta"),m=g(),r=i("h1"),u=i("a"),$=i("span"),x(_.$$.fragment),v=g(),T=i("span"),E=n("Text classification"),j=g(),x(D.$$.fragment),I=g(),K=i("p"),L=n("Text classification is a common NLP task that assigns a label or class to text. There are many practical applications of text classification widely used in production by some of today\u2019s largest companies. One of the most popular forms of text classification is sentiment analysis, which assigns a label like positive, negative, or neutral to a sequence of text."),V=g(),N=i("p"),R=n("This guide will show you how to fine-tune "),J=i("a"),Q=n("DistilBERT"),ue=n(" on the "),M=i("a"),de=n("IMDb"),ae=n(" dataset to determine whether a movie review is positive or negative."),pe=g(),x(O.$$.fragment),fe=g(),F=i("h2"),X=i("a"),W=i("span"),x(G.$$.fragment),_e=g(),se=i("span"),Z=n("Load IMDb dataset"),H=g(),B=i("p"),Y=n("Load the IMDb dataset from the \u{1F917} Datasets library:"),h=g(),x(y.$$.fragment),ee=g(),te=i("p"),re=n("Then take a look at an example:"),oe=g(),x(U.$$.fragment),ne=g(),o=i("p"),k=n("There are two fields in this dataset:"),ce=g(),$e=i("ul"),ve=i("li"),ge=i("code"),Be=n("text"),je=n(": a string containing the text of the movie review."),Ne=g(),me=i("li"),be=i("code"),Re=n("label"),Ue=n(": a value that can either be "),Ye=i("code"),Ct=n("0"),Pt=n(" for a negative review or "),Ke=i("code"),Dt=n("1"),St=n(" for a positive review."),ft=g(),ke=i("h2"),Ee=i("a"),Ve=i("span"),x(De.$$.fragment),Ft=g(),Je=i("span"),It=n("Preprocess"),ct=g(),Te=i("p"),Mt=n("Load the DistilBERT tokenizer to process the "),Qe=i("code"),Lt=n("text"),Ot=n(" field:"),mt=g(),x(Se.$$.fragment),ht=g(),xe=i("p"),Wt=n("Create a preprocessing function to tokenize "),Xe=i("code"),Bt=n("text"),Nt=n(" and truncate sequences to be no longer than DistilBERT\u2019s maximum input length:"),ut=g(),x(Fe.$$.fragment),dt=g(),he=i("p"),Rt=n("Use \u{1F917} Datasets "),Ie=i("a"),Ut=n("map"),Gt=n(" function to apply the preprocessing function over the entire dataset. You can speed up the "),Ze=i("code"),Ht=n("map"),Yt=n(" function by setting "),et=i("code"),Kt=n("batched=True"),Vt=n(" to process multiple elements of the dataset at once:"),_t=g(),x(Me.$$.fragment),$t=g(),le=i("p"),Jt=n("Use "),Ge=i("a"),Qt=n("DataCollatorWithPadding"),Xt=n(" to create a batch of examples. It will also "),tt=i("em"),Zt=n("dynamically pad"),ea=n(" your text to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),at=i("code"),ta=n("tokenizer"),aa=n(" function by setting "),st=i("code"),sa=n("padding=True"),ra=n(", dynamic padding is more efficient."),gt=g(),x(ze.$$.fragment),bt=g(),ye=i("h2"),Ae=i("a"),rt=i("span"),x(Le.$$.fragment),oa=g(),ot=i("span"),na=n("Train"),wt=g(),x(qe.$$.fragment),vt=g(),x(Ce.$$.fragment),this.h()},l(e){const d=Ba('[data-svelte="svelte-1phssyn"]',document.head);s=p(d,"META",{name:!0,content:!0}),d.forEach(a),m=b(e),r=p(e,"H1",{class:!0});var Oe=f(r);u=p(Oe,"A",{id:!0,class:!0,href:!0});var nt=f(u);$=p(nt,"SPAN",{});var lt=f($);z(_.$$.fragment,lt),lt.forEach(a),nt.forEach(a),v=b(Oe),T=p(Oe,"SPAN",{});var it=f(T);E=l(it,"Text classification"),it.forEach(a),Oe.forEach(a),j=b(e),z(D.$$.fragment,e),I=b(e),K=p(e,"P",{});var ia=f(K);L=l(ia,"Text classification is a common NLP task that assigns a label or class to text. There are many practical applications of text classification widely used in production by some of today\u2019s largest companies. One of the most popular forms of text classification is sentiment analysis, which assigns a label like positive, negative, or neutral to a sequence of text."),ia.forEach(a),V=b(e),N=p(e,"P",{});var He=f(N);R=l(He,"This guide will show you how to fine-tune "),J=p(He,"A",{href:!0,rel:!0});var pa=f(J);Q=l(pa,"DistilBERT"),pa.forEach(a),ue=l(He," on the "),M=p(He,"A",{href:!0,rel:!0});var fa=f(M);de=l(fa,"IMDb"),fa.forEach(a),ae=l(He," dataset to determine whether a movie review is positive or negative."),He.forEach(a),pe=b(e),z(O.$$.fragment,e),fe=b(e),F=p(e,"H2",{class:!0});var yt=f(F);X=p(yt,"A",{id:!0,class:!0,href:!0});var ca=f(X);W=p(ca,"SPAN",{});var ma=f(W);z(G.$$.fragment,ma),ma.forEach(a),ca.forEach(a),_e=b(yt),se=p(yt,"SPAN",{});var ha=f(se);Z=l(ha,"Load IMDb dataset"),ha.forEach(a),yt.forEach(a),H=b(e),B=p(e,"P",{});var ua=f(B);Y=l(ua,"Load the IMDb dataset from the \u{1F917} Datasets library:"),ua.forEach(a),h=b(e),z(y.$$.fragment,e),ee=b(e),te=p(e,"P",{});var da=f(te);re=l(da,"Then take a look at an example:"),da.forEach(a),oe=b(e),z(U.$$.fragment,e),ne=b(e),o=p(e,"P",{});var _a=f(o);k=l(_a,"There are two fields in this dataset:"),_a.forEach(a),ce=b(e),$e=p(e,"UL",{});var jt=f($e);ve=p(jt,"LI",{});var la=f(ve);ge=p(la,"CODE",{});var $a=f(ge);Be=l($a,"text"),$a.forEach(a),je=l(la,": a string containing the text of the movie review."),la.forEach(a),Ne=b(jt),me=p(jt,"LI",{});var We=f(me);be=p(We,"CODE",{});var ga=f(be);Re=l(ga,"label"),ga.forEach(a),Ue=l(We,": a value that can either be "),Ye=p(We,"CODE",{});var ba=f(Ye);Ct=l(ba,"0"),ba.forEach(a),Pt=l(We," for a negative review or "),Ke=p(We,"CODE",{});var wa=f(Ke);Dt=l(wa,"1"),wa.forEach(a),St=l(We," for a positive review."),We.forEach(a),jt.forEach(a),ft=b(e),ke=p(e,"H2",{class:!0});var Et=f(ke);Ee=p(Et,"A",{id:!0,class:!0,href:!0});var va=f(Ee);Ve=p(va,"SPAN",{});var ka=f(Ve);z(De.$$.fragment,ka),ka.forEach(a),va.forEach(a),Ft=b(Et),Je=p(Et,"SPAN",{});var ya=f(Je);It=l(ya,"Preprocess"),ya.forEach(a),Et.forEach(a),ct=b(e),Te=p(e,"P",{});var Tt=f(Te);Mt=l(Tt,"Load the DistilBERT tokenizer to process the "),Qe=p(Tt,"CODE",{});var ja=f(Qe);Lt=l(ja,"text"),ja.forEach(a),Ot=l(Tt," field:"),Tt.forEach(a),mt=b(e),z(Se.$$.fragment,e),ht=b(e),xe=p(e,"P",{});var xt=f(xe);Wt=l(xt,"Create a preprocessing function to tokenize "),Xe=p(xt,"CODE",{});var Ea=f(Xe);Bt=l(Ea,"text"),Ea.forEach(a),Nt=l(xt," and truncate sequences to be no longer than DistilBERT\u2019s maximum input length:"),xt.forEach(a),ut=b(e),z(Fe.$$.fragment,e),dt=b(e),he=p(e,"P",{});var Pe=f(he);Rt=l(Pe,"Use \u{1F917} Datasets "),Ie=p(Pe,"A",{href:!0,rel:!0});var Ta=f(Ie);Ut=l(Ta,"map"),Ta.forEach(a),Gt=l(Pe," function to apply the preprocessing function over the entire dataset. You can speed up the "),Ze=p(Pe,"CODE",{});var xa=f(Ze);Ht=l(xa,"map"),xa.forEach(a),Yt=l(Pe," function by setting "),et=p(Pe,"CODE",{});var za=f(et);Kt=l(za,"batched=True"),za.forEach(a),Vt=l(Pe," to process multiple elements of the dataset at once:"),Pe.forEach(a),_t=b(e),z(Me.$$.fragment,e),$t=b(e),le=p(e,"P",{});var we=f(le);Jt=l(we,"Use "),Ge=p(we,"A",{href:!0});var Aa=f(Ge);Qt=l(Aa,"DataCollatorWithPadding"),Aa.forEach(a),Xt=l(we," to create a batch of examples. It will also "),tt=p(we,"EM",{});var qa=f(tt);Zt=l(qa,"dynamically pad"),qa.forEach(a),ea=l(we," your text to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),at=p(we,"CODE",{});var Ca=f(at);ta=l(Ca,"tokenizer"),Ca.forEach(a),aa=l(we," function by setting "),st=p(we,"CODE",{});var Pa=f(st);sa=l(Pa,"padding=True"),Pa.forEach(a),ra=l(we,", dynamic padding is more efficient."),we.forEach(a),gt=b(e),z(ze.$$.fragment,e),bt=b(e),ye=p(e,"H2",{class:!0});var zt=f(ye);Ae=p(zt,"A",{id:!0,class:!0,href:!0});var Da=f(Ae);rt=p(Da,"SPAN",{});var Sa=f(rt);z(Le.$$.fragment,Sa),Sa.forEach(a),Da.forEach(a),oa=b(zt),ot=p(zt,"SPAN",{});var Fa=f(ot);na=l(Fa,"Train"),Fa.forEach(a),zt.forEach(a),wt=b(e),z(qe.$$.fragment,e),vt=b(e),z(Ce.$$.fragment,e),this.h()},h(){w(s,"name","hf:doc:metadata"),w(s,"content",JSON.stringify(rs)),w(u,"id","text-classification"),w(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(u,"href","#text-classification"),w(r,"class","relative group"),w(J,"href","https://huggingface.co/distilbert-base-uncased"),w(J,"rel","nofollow"),w(M,"href","https://huggingface.co/datasets/imdb"),w(M,"rel","nofollow"),w(X,"id","load-imdb-dataset"),w(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(X,"href","#load-imdb-dataset"),w(F,"class","relative group"),w(Ee,"id","preprocess"),w(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(Ee,"href","#preprocess"),w(ke,"class","relative group"),w(Ie,"href","https://huggingface.co/docs/datasets/v2.4.0/en/package_reference/main_classes#datasets.Dataset.map"),w(Ie,"rel","nofollow"),w(Ge,"href","/docs/transformers/v4.22.1/en/main_classes/data_collator#transformers.DataCollatorWithPadding"),w(Ae,"id","train"),w(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(Ae,"href","#train"),w(ye,"class","relative group")},m(e,d){t(document.head,s),c(e,m,d),c(e,r,d),t(r,u),t(u,$),A(_,$,null),t(r,v),t(r,T),t(T,E),c(e,j,d),A(D,e,d),c(e,I,d),c(e,K,d),t(K,L),c(e,V,d),c(e,N,d),t(N,R),t(N,J),t(J,Q),t(N,ue),t(N,M),t(M,de),t(N,ae),c(e,pe,d),A(O,e,d),c(e,fe,d),c(e,F,d),t(F,X),t(X,W),A(G,W,null),t(F,_e),t(F,se),t(se,Z),c(e,H,d),c(e,B,d),t(B,Y),c(e,h,d),A(y,e,d),c(e,ee,d),c(e,te,d),t(te,re),c(e,oe,d),A(U,e,d),c(e,ne,d),c(e,o,d),t(o,k),c(e,ce,d),c(e,$e,d),t($e,ve),t(ve,ge),t(ge,Be),t(ve,je),t($e,Ne),t($e,me),t(me,be),t(be,Re),t(me,Ue),t(me,Ye),t(Ye,Ct),t(me,Pt),t(me,Ke),t(Ke,Dt),t(me,St),c(e,ft,d),c(e,ke,d),t(ke,Ee),t(Ee,Ve),A(De,Ve,null),t(ke,Ft),t(ke,Je),t(Je,It),c(e,ct,d),c(e,Te,d),t(Te,Mt),t(Te,Qe),t(Qe,Lt),t(Te,Ot),c(e,mt,d),A(Se,e,d),c(e,ht,d),c(e,xe,d),t(xe,Wt),t(xe,Xe),t(Xe,Bt),t(xe,Nt),c(e,ut,d),A(Fe,e,d),c(e,dt,d),c(e,he,d),t(he,Rt),t(he,Ie),t(Ie,Ut),t(he,Gt),t(he,Ze),t(Ze,Ht),t(he,Yt),t(he,et),t(et,Kt),t(he,Vt),c(e,_t,d),A(Me,e,d),c(e,$t,d),c(e,le,d),t(le,Jt),t(le,Ge),t(Ge,Qt),t(le,Xt),t(le,tt),t(tt,Zt),t(le,ea),t(le,at),t(at,ta),t(le,aa),t(le,st),t(st,sa),t(le,ra),c(e,gt,d),A(ze,e,d),c(e,bt,d),c(e,ye,d),t(ye,Ae),t(Ae,rt),A(Le,rt,null),t(ye,oa),t(ye,ot),t(ot,na),c(e,wt,d),A(qe,e,d),c(e,vt,d),A(Ce,e,d),kt=!0},p(e,[d]){const Oe={};d&2&&(Oe.$$scope={dirty:d,ctx:e}),O.$set(Oe);const nt={};d&2&&(nt.$$scope={dirty:d,ctx:e}),ze.$set(nt);const lt={};d&2&&(lt.$$scope={dirty:d,ctx:e}),qe.$set(lt);const it={};d&2&&(it.$$scope={dirty:d,ctx:e}),Ce.$set(it)},i(e){kt||(q(_.$$.fragment,e),q(D.$$.fragment,e),q(O.$$.fragment,e),q(G.$$.fragment,e),q(y.$$.fragment,e),q(U.$$.fragment,e),q(De.$$.fragment,e),q(Se.$$.fragment,e),q(Fe.$$.fragment,e),q(Me.$$.fragment,e),q(ze.$$.fragment,e),q(Le.$$.fragment,e),q(qe.$$.fragment,e),q(Ce.$$.fragment,e),kt=!0)},o(e){C(_.$$.fragment,e),C(D.$$.fragment,e),C(O.$$.fragment,e),C(G.$$.fragment,e),C(y.$$.fragment,e),C(U.$$.fragment,e),C(De.$$.fragment,e),C(Se.$$.fragment,e),C(Fe.$$.fragment,e),C(Me.$$.fragment,e),C(ze.$$.fragment,e),C(Le.$$.fragment,e),C(qe.$$.fragment,e),C(Ce.$$.fragment,e),kt=!1},d(e){a(s),e&&a(m),e&&a(r),P(_),e&&a(j),P(D,e),e&&a(I),e&&a(K),e&&a(V),e&&a(N),e&&a(pe),P(O,e),e&&a(fe),e&&a(F),P(G),e&&a(H),e&&a(B),e&&a(h),P(y,e),e&&a(ee),e&&a(te),e&&a(oe),P(U,e),e&&a(ne),e&&a(o),e&&a(ce),e&&a($e),e&&a(ft),e&&a(ke),P(De),e&&a(ct),e&&a(Te),e&&a(mt),P(Se,e),e&&a(ht),e&&a(xe),e&&a(ut),P(Fe,e),e&&a(dt),e&&a(he),e&&a(_t),P(Me,e),e&&a($t),e&&a(le),e&&a(gt),P(ze,e),e&&a(bt),e&&a(ye),P(Le),e&&a(wt),P(qe,e),e&&a(vt),P(Ce,e)}}}const rs={local:"text-classification",sections:[{local:"load-imdb-dataset",title:"Load IMDb dataset"},{local:"preprocess",title:"Preprocess"},{local:"train",title:"Train"}],title:"Text classification"};function os(S){return Na(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ms extends La{constructor(s){super();Oa(this,s,os,ss,Wa,{})}}export{ms as default,rs as metadata};
