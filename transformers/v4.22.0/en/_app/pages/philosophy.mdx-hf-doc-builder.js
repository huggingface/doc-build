import{S as br,i as _r,s as wr,e as a,k as d,w as hr,t as r,M as gr,c as s,d as t,m as c,a as n,x as fr,h as l,b as h,G as e,g as f,y as ur,L as Er,q as pr,o as mr,B as vr,v as kr}from"../chunks/vendor-hf-doc-builder.js";import{I as yr}from"../chunks/IconCopyLink-hf-doc-builder.js";function Ar(ha){let k,Xe,A,x,we,S,gt,ge,Et,Qe,Q,kt,Ve,_,Ee,At,Pt,ke,Tt,It,Ae,Lt,Ze,V,xt,et,Z,Pe,Mt,tt,p,u,$t,ee,Ft,Ot,te,zt,qt,oe,St,Ht,ae,Nt,Ct,re,Ut,jt,Dt,P,Kt,Te,Rt,Wt,H,Gt,Yt,Bt,b,Jt,se,Xt,Qt,le,Vt,Zt,Ie,eo,to,oo,N,ao,C,ro,so,ot,U,Le,lo,at,M,xe,no,io,j,co,Me,ho,fo,rt,ne,uo,st,w,D,$e,po,mo,K,Fe,vo,yo,Oe,bo,_o,R,ze,wo,go,W,qe,Eo,ko,Se,Ao,Po,He,Ne,To,lt,T,$,Ce,G,Io,Ue,Lo,nt,ie,xo,it,g,m,je,Mo,$o,Y,Fo,Oo,B,zo,qo,J,So,Ho,No,de,De,Co,Uo,jo,v,Ke,Do,Ko,ce,Ro,Wo,he,Go,Yo,fe,Bo,Jo,dt,ue,Xo,ct,E,F,Re,Qo,Vo,X,Zo,ea,ta,O,We,oa,aa,Ge,ra,sa,la,pe,Ye,na,ia,ht;return S=new yr({}),G=new yr({}),{c(){k=a("meta"),Xe=d(),A=a("h1"),x=a("a"),we=a("span"),hr(S.$$.fragment),gt=d(),ge=a("span"),Et=r("Philosophy"),Qe=d(),Q=a("p"),kt=r("\u{1F917} Transformers is an opinionated library built for:"),Ve=d(),_=a("ul"),Ee=a("li"),At=r("machine learning researchers and educators seeking to use, study or extend large-scale Transformers models."),Pt=d(),ke=a("li"),Tt=r("hands-on practitioners who want to fine-tune those models or serve them in production, or both."),It=d(),Ae=a("li"),Lt=r("engineers who just want to download a pretrained model and use it to solve a given machine learning task."),Ze=d(),V=a("p"),xt=r("The library was designed with two strong goals in mind:"),et=d(),Z=a("ol"),Pe=a("li"),Mt=r("Be as easy and fast to use as possible:"),tt=d(),p=a("ul"),u=a("li"),$t=r(`We strongly limited the number of user-facing abstractions to learn, in fact, there are almost no abstractions,
just three standard classes required to use each model: `),ee=a("a"),Ft=r("configuration"),Ot=r(`,
`),te=a("a"),zt=r("models"),qt=r(", and a preprocessing class ("),oe=a("a"),St=r("tokenizer"),Ht=r(" for NLP, "),ae=a("a"),Nt=r("feature extractor"),Ct=r(" for vision and audio, and "),re=a("a"),Ut=r("processor"),jt=r(" for multimodal inputs)."),Dt=d(),P=a("li"),Kt=r(`All of these classes can be initialized in a simple and unified way from pretrained instances by using a common
`),Te=a("code"),Rt=r("from_pretrained()"),Wt=r(` method which downloads (if needed), caches and
loads the related class instance and associated data (configurations\u2019 hyperparameters, tokenizers\u2019 vocabulary,
and models\u2019 weights) from a pretrained checkpoint provided on `),H=a("a"),Gt=r("Hugging Face Hub"),Yt=r(" or your own saved checkpoint."),Bt=d(),b=a("li"),Jt=r("On top of those three base classes, the library provides two APIs: "),se=a("a"),Xt=r("pipeline()"),Qt=r(` for quickly
using a model for inference on a given task and `),le=a("a"),Vt=r("Trainer"),Zt=r(" to quickly train or fine-tune a PyTorch model (all TensorFlow models are compatible with "),Ie=a("code"),eo=r("Keras.fit"),to=r(")."),oo=d(),N=a("li"),ao=r(`As a consequence, this library is NOT a modular toolbox of building blocks for neural nets. If you want to
extend or build upon the library, just use regular Python, PyTorch, TensorFlow, Keras modules and inherit from the base
classes of the library to reuse functionalities like model loading and saving. If you\u2019d like to learn more about our coding philosophy for models, check out our `),C=a("a"),ro=r("Repeat Yourself"),so=r(" blog post."),ot=d(),U=a("ol"),Le=a("li"),lo=r("Provide state-of-the-art models with performances as close as possible to the original models:"),at=d(),M=a("ul"),xe=a("li"),no=r(`We provide at least one example for each architecture which reproduces a result provided by the official authors
of said architecture.`),io=d(),j=a("li"),co=r(`The code is usually as close to the original code base as possible which means some PyTorch code may be not as
`),Me=a("em"),ho=r("pytorchic"),fo=r(" as it could be as a result of being converted TensorFlow code and vice versa."),rt=d(),ne=a("p"),uo=r("A few other goals:"),st=d(),w=a("ul"),D=a("li"),$e=a("p"),po=r("Expose the models\u2019 internals as consistently as possible:"),mo=d(),K=a("ul"),Fe=a("li"),vo=r("We give access, using a single API, to the full hidden-states and attention weights."),yo=d(),Oe=a("li"),bo=r("The preprocessing classes and base model APIs are standardized to easily switch between models."),_o=d(),R=a("li"),ze=a("p"),wo=r("Incorporate a subjective selection of promising tools for fine-tuning and investigating these models:"),go=d(),W=a("ul"),qe=a("li"),Eo=r("A simple and consistent way to add new tokens to the vocabulary and embeddings for fine-tuning."),ko=d(),Se=a("li"),Ao=r("Simple ways to mask and prune Transformer heads."),Po=d(),He=a("li"),Ne=a("p"),To=r("Easily switch between PyTorch, TensorFlow 2.0 and Flax, allowing training with one framework and inference with another."),lt=d(),T=a("h2"),$=a("a"),Ce=a("span"),hr(G.$$.fragment),Io=d(),Ue=a("span"),Lo=r("Main concepts"),nt=d(),ie=a("p"),xo=r("The library is built around three types of classes for each model:"),it=d(),g=a("ul"),m=a("li"),je=a("strong"),Mo=r("Model classes"),$o=r(" can be PyTorch models ("),Y=a("a"),Fo=r("torch.nn.Module"),Oo=r("), Keras models ("),B=a("a"),zo=r("tf.keras.Model"),qo=r(") or JAX/Flax models ("),J=a("a"),So=r("flax.linen.Module"),Ho=r(") that work with the pretrained weights provided in the library."),No=d(),de=a("li"),De=a("strong"),Co=r("Configuration classes"),Uo=r(" store the hyperparameters required to build a model (such as the number of layers and hidden size). You don\u2019t always need to instantiate these yourself. In particular, if you are using a pretrained model without any modification, creating the model will automatically take care of instantiating the configuration (which is part of the model)."),jo=d(),v=a("li"),Ke=a("strong"),Do=r("Preprocessing classes"),Ko=r(" convert the raw data into a format accepted by the model. A "),ce=a("a"),Ro=r("tokenizer"),Wo=r(" stores the vocabulary for each model and provide methods for encoding and decoding strings in a list of token embedding indices to be fed to a model. "),he=a("a"),Go=r("Feature extractors"),Yo=r(" preprocess audio or vision inputs, and a "),fe=a("a"),Bo=r("processor"),Jo=r(" handles multimodal inputs."),dt=d(),ue=a("p"),Xo=r("All these classes can be instantiated from pretrained instances, saved locally, and shared on the Hub with three methods:"),ct=d(),E=a("ul"),F=a("li"),Re=a("code"),Qo=r("from_pretrained()"),Vo=r(` lets you instantiate a model, configuration, and preprocessing class from a pretrained version either
provided by the library itself (the supported models can be found on the `),X=a("a"),Zo=r("Model Hub"),ea=r(`) or
stored locally (or on a server) by the user.`),ta=d(),O=a("li"),We=a("code"),oa=r("save_pretrained()"),aa=r(` lets you save a model, configuration, and preprocessing class locally so that it can be reloaded using
`),Ge=a("code"),ra=r("from_pretrained()"),sa=r("."),la=d(),pe=a("li"),Ye=a("code"),na=r("push_to_hub()"),ia=r(" lets you share a model, configuration, and a preprocessing class to the Hub, so it is easily accessible to everyone."),this.h()},l(o){const i=gr('[data-svelte="svelte-1phssyn"]',document.head);k=s(i,"META",{name:!0,content:!0}),i.forEach(t),Xe=c(o),A=s(o,"H1",{class:!0});var ft=n(A);x=s(ft,"A",{id:!0,class:!0,href:!0});var fa=n(x);we=s(fa,"SPAN",{});var ua=n(we);fr(S.$$.fragment,ua),ua.forEach(t),fa.forEach(t),gt=c(ft),ge=s(ft,"SPAN",{});var pa=n(ge);Et=l(pa,"Philosophy"),pa.forEach(t),ft.forEach(t),Qe=c(o),Q=s(o,"P",{});var ma=n(Q);kt=l(ma,"\u{1F917} Transformers is an opinionated library built for:"),ma.forEach(t),Ve=c(o),_=s(o,"UL",{});var me=n(_);Ee=s(me,"LI",{});var va=n(Ee);At=l(va,"machine learning researchers and educators seeking to use, study or extend large-scale Transformers models."),va.forEach(t),Pt=c(me),ke=s(me,"LI",{});var ya=n(ke);Tt=l(ya,"hands-on practitioners who want to fine-tune those models or serve them in production, or both."),ya.forEach(t),It=c(me),Ae=s(me,"LI",{});var ba=n(Ae);Lt=l(ba,"engineers who just want to download a pretrained model and use it to solve a given machine learning task."),ba.forEach(t),me.forEach(t),Ze=c(o),V=s(o,"P",{});var _a=n(V);xt=l(_a,"The library was designed with two strong goals in mind:"),_a.forEach(t),et=c(o),Z=s(o,"OL",{});var wa=n(Z);Pe=s(wa,"LI",{});var ga=n(Pe);Mt=l(ga,"Be as easy and fast to use as possible:"),ga.forEach(t),wa.forEach(t),tt=c(o),p=s(o,"UL",{});var z=n(p);u=s(z,"LI",{});var y=n(u);$t=l(y,`We strongly limited the number of user-facing abstractions to learn, in fact, there are almost no abstractions,
just three standard classes required to use each model: `),ee=s(y,"A",{href:!0});var Ea=n(ee);Ft=l(Ea,"configuration"),Ea.forEach(t),Ot=l(y,`,
`),te=s(y,"A",{href:!0});var ka=n(te);zt=l(ka,"models"),ka.forEach(t),qt=l(y,", and a preprocessing class ("),oe=s(y,"A",{href:!0});var Aa=n(oe);St=l(Aa,"tokenizer"),Aa.forEach(t),Ht=l(y," for NLP, "),ae=s(y,"A",{href:!0});var Pa=n(ae);Nt=l(Pa,"feature extractor"),Pa.forEach(t),Ct=l(y," for vision and audio, and "),re=s(y,"A",{href:!0});var Ta=n(re);Ut=l(Ta,"processor"),Ta.forEach(t),jt=l(y," for multimodal inputs)."),y.forEach(t),Dt=c(z),P=s(z,"LI",{});var ve=n(P);Kt=l(ve,`All of these classes can be initialized in a simple and unified way from pretrained instances by using a common
`),Te=s(ve,"CODE",{});var Ia=n(Te);Rt=l(Ia,"from_pretrained()"),Ia.forEach(t),Wt=l(ve,` method which downloads (if needed), caches and
loads the related class instance and associated data (configurations\u2019 hyperparameters, tokenizers\u2019 vocabulary,
and models\u2019 weights) from a pretrained checkpoint provided on `),H=s(ve,"A",{href:!0,rel:!0});var La=n(H);Gt=l(La,"Hugging Face Hub"),La.forEach(t),Yt=l(ve," or your own saved checkpoint."),ve.forEach(t),Bt=c(z),b=s(z,"LI",{});var q=n(b);Jt=l(q,"On top of those three base classes, the library provides two APIs: "),se=s(q,"A",{href:!0});var xa=n(se);Xt=l(xa,"pipeline()"),xa.forEach(t),Qt=l(q,` for quickly
using a model for inference on a given task and `),le=s(q,"A",{href:!0});var Ma=n(le);Vt=l(Ma,"Trainer"),Ma.forEach(t),Zt=l(q," to quickly train or fine-tune a PyTorch model (all TensorFlow models are compatible with "),Ie=s(q,"CODE",{});var $a=n(Ie);eo=l($a,"Keras.fit"),$a.forEach(t),to=l(q,")."),q.forEach(t),oo=c(z),N=s(z,"LI",{});var ut=n(N);ao=l(ut,`As a consequence, this library is NOT a modular toolbox of building blocks for neural nets. If you want to
extend or build upon the library, just use regular Python, PyTorch, TensorFlow, Keras modules and inherit from the base
classes of the library to reuse functionalities like model loading and saving. If you\u2019d like to learn more about our coding philosophy for models, check out our `),C=s(ut,"A",{href:!0,rel:!0});var Fa=n(C);ro=l(Fa,"Repeat Yourself"),Fa.forEach(t),so=l(ut," blog post."),ut.forEach(t),z.forEach(t),ot=c(o),U=s(o,"OL",{start:!0});var Oa=n(U);Le=s(Oa,"LI",{});var za=n(Le);lo=l(za,"Provide state-of-the-art models with performances as close as possible to the original models:"),za.forEach(t),Oa.forEach(t),at=c(o),M=s(o,"UL",{});var pt=n(M);xe=s(pt,"LI",{});var qa=n(xe);no=l(qa,`We provide at least one example for each architecture which reproduces a result provided by the official authors
of said architecture.`),qa.forEach(t),io=c(pt),j=s(pt,"LI",{});var mt=n(j);co=l(mt,`The code is usually as close to the original code base as possible which means some PyTorch code may be not as
`),Me=s(mt,"EM",{});var Sa=n(Me);ho=l(Sa,"pytorchic"),Sa.forEach(t),fo=l(mt," as it could be as a result of being converted TensorFlow code and vice versa."),mt.forEach(t),pt.forEach(t),rt=c(o),ne=s(o,"P",{});var Ha=n(ne);uo=l(Ha,"A few other goals:"),Ha.forEach(t),st=c(o),w=s(o,"UL",{});var ye=n(w);D=s(ye,"LI",{});var vt=n(D);$e=s(vt,"P",{});var Na=n($e);po=l(Na,"Expose the models\u2019 internals as consistently as possible:"),Na.forEach(t),mo=c(vt),K=s(vt,"UL",{});var yt=n(K);Fe=s(yt,"LI",{});var Ca=n(Fe);vo=l(Ca,"We give access, using a single API, to the full hidden-states and attention weights."),Ca.forEach(t),yo=c(yt),Oe=s(yt,"LI",{});var Ua=n(Oe);bo=l(Ua,"The preprocessing classes and base model APIs are standardized to easily switch between models."),Ua.forEach(t),yt.forEach(t),vt.forEach(t),_o=c(ye),R=s(ye,"LI",{});var bt=n(R);ze=s(bt,"P",{});var ja=n(ze);wo=l(ja,"Incorporate a subjective selection of promising tools for fine-tuning and investigating these models:"),ja.forEach(t),go=c(bt),W=s(bt,"UL",{});var _t=n(W);qe=s(_t,"LI",{});var Da=n(qe);Eo=l(Da,"A simple and consistent way to add new tokens to the vocabulary and embeddings for fine-tuning."),Da.forEach(t),ko=c(_t),Se=s(_t,"LI",{});var Ka=n(Se);Ao=l(Ka,"Simple ways to mask and prune Transformer heads."),Ka.forEach(t),_t.forEach(t),bt.forEach(t),Po=c(ye),He=s(ye,"LI",{});var Ra=n(He);Ne=s(Ra,"P",{});var Wa=n(Ne);To=l(Wa,"Easily switch between PyTorch, TensorFlow 2.0 and Flax, allowing training with one framework and inference with another."),Wa.forEach(t),Ra.forEach(t),ye.forEach(t),lt=c(o),T=s(o,"H2",{class:!0});var wt=n(T);$=s(wt,"A",{id:!0,class:!0,href:!0});var Ga=n($);Ce=s(Ga,"SPAN",{});var Ya=n(Ce);fr(G.$$.fragment,Ya),Ya.forEach(t),Ga.forEach(t),Io=c(wt),Ue=s(wt,"SPAN",{});var Ba=n(Ue);Lo=l(Ba,"Main concepts"),Ba.forEach(t),wt.forEach(t),nt=c(o),ie=s(o,"P",{});var Ja=n(ie);xo=l(Ja,"The library is built around three types of classes for each model:"),Ja.forEach(t),it=c(o),g=s(o,"UL",{});var be=n(g);m=s(be,"LI",{});var I=n(m);je=s(I,"STRONG",{});var Xa=n(je);Mo=l(Xa,"Model classes"),Xa.forEach(t),$o=l(I," can be PyTorch models ("),Y=s(I,"A",{href:!0,rel:!0});var Qa=n(Y);Fo=l(Qa,"torch.nn.Module"),Qa.forEach(t),Oo=l(I,"), Keras models ("),B=s(I,"A",{href:!0,rel:!0});var Va=n(B);zo=l(Va,"tf.keras.Model"),Va.forEach(t),qo=l(I,") or JAX/Flax models ("),J=s(I,"A",{href:!0,rel:!0});var Za=n(J);So=l(Za,"flax.linen.Module"),Za.forEach(t),Ho=l(I,") that work with the pretrained weights provided in the library."),I.forEach(t),No=c(be),de=s(be,"LI",{});var da=n(de);De=s(da,"STRONG",{});var er=n(De);Co=l(er,"Configuration classes"),er.forEach(t),Uo=l(da," store the hyperparameters required to build a model (such as the number of layers and hidden size). You don\u2019t always need to instantiate these yourself. In particular, if you are using a pretrained model without any modification, creating the model will automatically take care of instantiating the configuration (which is part of the model)."),da.forEach(t),jo=c(be),v=s(be,"LI",{});var L=n(v);Ke=s(L,"STRONG",{});var tr=n(Ke);Do=l(tr,"Preprocessing classes"),tr.forEach(t),Ko=l(L," convert the raw data into a format accepted by the model. A "),ce=s(L,"A",{href:!0});var or=n(ce);Ro=l(or,"tokenizer"),or.forEach(t),Wo=l(L," stores the vocabulary for each model and provide methods for encoding and decoding strings in a list of token embedding indices to be fed to a model. "),he=s(L,"A",{href:!0});var ar=n(he);Go=l(ar,"Feature extractors"),ar.forEach(t),Yo=l(L," preprocess audio or vision inputs, and a "),fe=s(L,"A",{href:!0});var rr=n(fe);Bo=l(rr,"processor"),rr.forEach(t),Jo=l(L," handles multimodal inputs."),L.forEach(t),be.forEach(t),dt=c(o),ue=s(o,"P",{});var sr=n(ue);Xo=l(sr,"All these classes can be instantiated from pretrained instances, saved locally, and shared on the Hub with three methods:"),sr.forEach(t),ct=c(o),E=s(o,"UL",{});var _e=n(E);F=s(_e,"LI",{});var Be=n(F);Re=s(Be,"CODE",{});var lr=n(Re);Qo=l(lr,"from_pretrained()"),lr.forEach(t),Vo=l(Be,` lets you instantiate a model, configuration, and preprocessing class from a pretrained version either
provided by the library itself (the supported models can be found on the `),X=s(Be,"A",{href:!0,rel:!0});var nr=n(X);Zo=l(nr,"Model Hub"),nr.forEach(t),ea=l(Be,`) or
stored locally (or on a server) by the user.`),Be.forEach(t),ta=c(_e),O=s(_e,"LI",{});var Je=n(O);We=s(Je,"CODE",{});var ir=n(We);oa=l(ir,"save_pretrained()"),ir.forEach(t),aa=l(Je,` lets you save a model, configuration, and preprocessing class locally so that it can be reloaded using
`),Ge=s(Je,"CODE",{});var dr=n(Ge);ra=l(dr,"from_pretrained()"),dr.forEach(t),sa=l(Je,"."),Je.forEach(t),la=c(_e),pe=s(_e,"LI",{});var ca=n(pe);Ye=s(ca,"CODE",{});var cr=n(Ye);na=l(cr,"push_to_hub()"),cr.forEach(t),ia=l(ca," lets you share a model, configuration, and a preprocessing class to the Hub, so it is easily accessible to everyone."),ca.forEach(t),_e.forEach(t),this.h()},h(){h(k,"name","hf:doc:metadata"),h(k,"content",JSON.stringify(Pr)),h(x,"id","philosophy"),h(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(x,"href","#philosophy"),h(A,"class","relative group"),h(ee,"href","main_classes/configuration"),h(te,"href","main_classes/model"),h(oe,"href","main_classes/tokenizer"),h(ae,"href","main_classes/feature_extractor"),h(re,"href","main_classes/processors"),h(H,"href","https://huggingface.co/models"),h(H,"rel","nofollow"),h(se,"href","/docs/transformers/v4.22.0/en/main_classes/pipelines#transformers.pipeline"),h(le,"href","/docs/transformers/v4.22.0/en/main_classes/trainer#transformers.Trainer"),h(C,"href","https://huggingface.co/blog/transformers-design-philosophy"),h(C,"rel","nofollow"),h(U,"start","2"),h($,"id","main-concepts"),h($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h($,"href","#main-concepts"),h(T,"class","relative group"),h(Y,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),h(Y,"rel","nofollow"),h(B,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),h(B,"rel","nofollow"),h(J,"href","https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html"),h(J,"rel","nofollow"),h(ce,"href","main_classes/tokenizer"),h(he,"href","main_classes/feature_extractor"),h(fe,"href","main_classes/processors"),h(X,"href","https://huggingface.co/models"),h(X,"rel","nofollow")},m(o,i){e(document.head,k),f(o,Xe,i),f(o,A,i),e(A,x),e(x,we),ur(S,we,null),e(A,gt),e(A,ge),e(ge,Et),f(o,Qe,i),f(o,Q,i),e(Q,kt),f(o,Ve,i),f(o,_,i),e(_,Ee),e(Ee,At),e(_,Pt),e(_,ke),e(ke,Tt),e(_,It),e(_,Ae),e(Ae,Lt),f(o,Ze,i),f(o,V,i),e(V,xt),f(o,et,i),f(o,Z,i),e(Z,Pe),e(Pe,Mt),f(o,tt,i),f(o,p,i),e(p,u),e(u,$t),e(u,ee),e(ee,Ft),e(u,Ot),e(u,te),e(te,zt),e(u,qt),e(u,oe),e(oe,St),e(u,Ht),e(u,ae),e(ae,Nt),e(u,Ct),e(u,re),e(re,Ut),e(u,jt),e(p,Dt),e(p,P),e(P,Kt),e(P,Te),e(Te,Rt),e(P,Wt),e(P,H),e(H,Gt),e(P,Yt),e(p,Bt),e(p,b),e(b,Jt),e(b,se),e(se,Xt),e(b,Qt),e(b,le),e(le,Vt),e(b,Zt),e(b,Ie),e(Ie,eo),e(b,to),e(p,oo),e(p,N),e(N,ao),e(N,C),e(C,ro),e(N,so),f(o,ot,i),f(o,U,i),e(U,Le),e(Le,lo),f(o,at,i),f(o,M,i),e(M,xe),e(xe,no),e(M,io),e(M,j),e(j,co),e(j,Me),e(Me,ho),e(j,fo),f(o,rt,i),f(o,ne,i),e(ne,uo),f(o,st,i),f(o,w,i),e(w,D),e(D,$e),e($e,po),e(D,mo),e(D,K),e(K,Fe),e(Fe,vo),e(K,yo),e(K,Oe),e(Oe,bo),e(w,_o),e(w,R),e(R,ze),e(ze,wo),e(R,go),e(R,W),e(W,qe),e(qe,Eo),e(W,ko),e(W,Se),e(Se,Ao),e(w,Po),e(w,He),e(He,Ne),e(Ne,To),f(o,lt,i),f(o,T,i),e(T,$),e($,Ce),ur(G,Ce,null),e(T,Io),e(T,Ue),e(Ue,Lo),f(o,nt,i),f(o,ie,i),e(ie,xo),f(o,it,i),f(o,g,i),e(g,m),e(m,je),e(je,Mo),e(m,$o),e(m,Y),e(Y,Fo),e(m,Oo),e(m,B),e(B,zo),e(m,qo),e(m,J),e(J,So),e(m,Ho),e(g,No),e(g,de),e(de,De),e(De,Co),e(de,Uo),e(g,jo),e(g,v),e(v,Ke),e(Ke,Do),e(v,Ko),e(v,ce),e(ce,Ro),e(v,Wo),e(v,he),e(he,Go),e(v,Yo),e(v,fe),e(fe,Bo),e(v,Jo),f(o,dt,i),f(o,ue,i),e(ue,Xo),f(o,ct,i),f(o,E,i),e(E,F),e(F,Re),e(Re,Qo),e(F,Vo),e(F,X),e(X,Zo),e(F,ea),e(E,ta),e(E,O),e(O,We),e(We,oa),e(O,aa),e(O,Ge),e(Ge,ra),e(O,sa),e(E,la),e(E,pe),e(pe,Ye),e(Ye,na),e(pe,ia),ht=!0},p:Er,i(o){ht||(pr(S.$$.fragment,o),pr(G.$$.fragment,o),ht=!0)},o(o){mr(S.$$.fragment,o),mr(G.$$.fragment,o),ht=!1},d(o){t(k),o&&t(Xe),o&&t(A),vr(S),o&&t(Qe),o&&t(Q),o&&t(Ve),o&&t(_),o&&t(Ze),o&&t(V),o&&t(et),o&&t(Z),o&&t(tt),o&&t(p),o&&t(ot),o&&t(U),o&&t(at),o&&t(M),o&&t(rt),o&&t(ne),o&&t(st),o&&t(w),o&&t(lt),o&&t(T),vr(G),o&&t(nt),o&&t(ie),o&&t(it),o&&t(g),o&&t(dt),o&&t(ue),o&&t(ct),o&&t(E)}}}const Pr={local:"philosophy",sections:[{local:"main-concepts",title:"Main concepts"}],title:"Philosophy"};function Tr(ha){return kr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class xr extends br{constructor(k){super();_r(this,k,Tr,Ar,wr,{})}}export{xr as default,Pr as metadata};
