import{S as fr,i as hr,s as $r,e as l,k as f,w as T,t as s,M as gr,c as i,d as a,m as h,a as c,x as M,h as o,b as _,G as r,g as u,y as S,q as L,o as O,B as D,v as _r,L as dr}from"../chunks/vendor-hf-doc-builder.js";import{T as vr}from"../chunks/Tip-hf-doc-builder.js";import{I as Be}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as ve}from"../chunks/CodeBlock-hf-doc-builder.js";import{F as br,M as mr}from"../chunks/Markdown-hf-doc-builder.js";import"../chunks/IconTensorflow-hf-doc-builder.js";function kr(Y){let n,k,p,g,q,v,R,x;return{c(){n=l("p"),k=s("Recuerda, la arquitectura se refiere al esqueleto del modelo y los checkpoints son los pesos para una arquitectura dada. Por ejemplo, "),p=l("a"),g=s("BERT"),q=s(" es una arquitectura, mientras que "),v=l("code"),R=s("bert-base-uncased"),x=s(" es un checkpoint. Modelo es un t\xE9rmino general que puede significar una arquitectura o un checkpoint."),this.h()},l(E){n=i(E,"P",{});var A=c(n);k=o(A,"Recuerda, la arquitectura se refiere al esqueleto del modelo y los checkpoints son los pesos para una arquitectura dada. Por ejemplo, "),p=i(A,"A",{href:!0,rel:!0});var w=c(p);g=o(w,"BERT"),w.forEach(a),q=o(A," es una arquitectura, mientras que "),v=i(A,"CODE",{});var N=c(v);R=o(N,"bert-base-uncased"),N.forEach(a),x=o(A," es un checkpoint. Modelo es un t\xE9rmino general que puede significar una arquitectura o un checkpoint."),A.forEach(a),this.h()},h(){_(p,"href","https://huggingface.co/bert-base-uncased"),_(p,"rel","nofollow")},m(E,A){u(E,n,A),r(n,k),r(n,p),r(p,g),r(n,q),r(n,v),r(v,R),r(n,x)},d(E){E&&a(n)}}}function jr(Y){let n,k,p,g,q,v,R,x,E,A,w,N,C,H,P,B,U,y,I,$,W,F,Z,X,j,V,ee,z,J,ae,K;return C=new ve({props:{code:`from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),y=new ve({props:{code:`from transformers import AutoModelForTokenClassification

model = AutoModelForTokenClassification.from_pretrained("distilbert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),{c(){n=l("p"),k=s("Finalmente, las clases "),p=l("code"),g=s("AutoModelFor"),q=s(" te permiten cargar un modelo preentrenado para una tarea dada (revisa "),v=l("a"),R=s("aqu\xED"),x=s(" para conocer la lista completa de tareas disponibles). Por ejemplo, cargue un modelo para clasificaci\xF3n de secuencias con "),E=l("code"),A=s("AutoModelForSequenceClassification.from_pretrained()"),w=s(":"),N=f(),T(C.$$.fragment),H=f(),P=l("p"),B=s("Reutiliza f\xE1cilmente el mismo checkpoint para cargar una aquitectura para alguna tarea diferente:"),U=f(),T(y.$$.fragment),I=f(),$=l("p"),W=s("Generalmente recomendamos utilizar las clases "),F=l("code"),Z=s("AutoTokenizer"),X=s(" y "),j=l("code"),V=s("AutoModelFor"),ee=s(" para cargar instancias pre-entrenadas de modelos. \xC9sto asegurar\xE1 que cargues la arquitectura correcta en cada ocasi\xF3n. En el siguiente "),z=l("a"),J=s("tutorial"),ae=s(", aprende a usar tu tokenizador reci\xE9n cargado, el extractor de caracter\xEDsticas y el procesador para preprocesar un dataset para fine-tuning."),this.h()},l(t){n=i(t,"P",{});var m=c(n);k=o(m,"Finalmente, las clases "),p=i(m,"CODE",{});var oe=c(p);g=o(oe,"AutoModelFor"),oe.forEach(a),q=o(m," te permiten cargar un modelo preentrenado para una tarea dada (revisa "),v=i(m,"A",{href:!0});var re=c(v);R=o(re,"aqu\xED"),re.forEach(a),x=o(m," para conocer la lista completa de tareas disponibles). Por ejemplo, cargue un modelo para clasificaci\xF3n de secuencias con "),E=i(m,"CODE",{});var ne=c(E);A=o(ne,"AutoModelForSequenceClassification.from_pretrained()"),ne.forEach(a),w=o(m,":"),m.forEach(a),N=h(t),M(C.$$.fragment,t),H=h(t),P=i(t,"P",{});var se=c(P);B=o(se,"Reutiliza f\xE1cilmente el mismo checkpoint para cargar una aquitectura para alguna tarea diferente:"),se.forEach(a),U=h(t),M(y.$$.fragment,t),I=h(t),$=i(t,"P",{});var b=c($);W=o(b,"Generalmente recomendamos utilizar las clases "),F=i(b,"CODE",{});var G=c(F);Z=o(G,"AutoTokenizer"),G.forEach(a),X=o(b," y "),j=i(b,"CODE",{});var te=c(j);V=o(te,"AutoModelFor"),te.forEach(a),ee=o(b," para cargar instancias pre-entrenadas de modelos. \xC9sto asegurar\xE1 que cargues la arquitectura correcta en cada ocasi\xF3n. En el siguiente "),z=i(b,"A",{href:!0});var Q=c(z);J=o(Q,"tutorial"),Q.forEach(a),ae=o(b,", aprende a usar tu tokenizador reci\xE9n cargado, el extractor de caracter\xEDsticas y el procesador para preprocesar un dataset para fine-tuning."),b.forEach(a),this.h()},h(){_(v,"href","model_doc/auto"),_(z,"href","preprocessing")},m(t,m){u(t,n,m),r(n,k),r(n,p),r(p,g),r(n,q),r(n,v),r(v,R),r(n,x),r(n,E),r(E,A),r(n,w),u(t,N,m),S(C,t,m),u(t,H,m),u(t,P,m),r(P,B),u(t,U,m),S(y,t,m),u(t,I,m),u(t,$,m),r($,W),r($,F),r(F,Z),r($,X),r($,j),r(j,V),r($,ee),r($,z),r(z,J),r($,ae),K=!0},p:dr,i(t){K||(L(C.$$.fragment,t),L(y.$$.fragment,t),K=!0)},o(t){O(C.$$.fragment,t),O(y.$$.fragment,t),K=!1},d(t){t&&a(n),t&&a(N),D(C,t),t&&a(H),t&&a(P),t&&a(U),D(y,t),t&&a(I),t&&a($)}}}function qr(Y){let n,k;return n=new mr({props:{$$slots:{default:[jr]},$$scope:{ctx:Y}}}),{c(){T(n.$$.fragment)},l(p){M(n.$$.fragment,p)},m(p,g){S(n,p,g),k=!0},p(p,g){const q={};g&2&&(q.$$scope={dirty:g,ctx:p}),n.$set(q)},i(p){k||(L(n.$$.fragment,p),k=!0)},o(p){O(n.$$.fragment,p),k=!1},d(p){D(n,p)}}}function Er(Y){let n,k,p,g,q,v,R,x,E,A,w,N,C,H,P,B,U,y,I,$,W,F,Z,X,j,V,ee,z,J,ae,K;return C=new ve({props:{code:`from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),y=new ve({props:{code:`from transformers import TFAutoModelForTokenClassification

model = TFAutoModelForTokenClassification.from_pretrained("distilbert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),{c(){n=l("p"),k=s("Finalmente, la clase "),p=l("code"),g=s("TFAutoModelFor"),q=s(" te permite cargar tu modelo pre-entrenado para una tarea dada (revisa "),v=l("a"),R=s("aqu\xED"),x=s(" para conocer la lista completa de tareas disponibles). Por ejemplo, carga un modelo para clasificaci\xF3n de secuencias con "),E=l("code"),A=s("TFAutoModelForSequenceClassification.from_pretrained()"),w=s(":"),N=f(),T(C.$$.fragment),H=f(),P=l("p"),B=s("Reutiliza f\xE1cilmente el mismo checkpoint para cargar una aquitectura para alguna tarea diferente:"),U=f(),T(y.$$.fragment),I=f(),$=l("p"),W=s("Generalmente recomendamos utilizar las clases "),F=l("code"),Z=s("AutoTokenizer"),X=s(" y "),j=l("code"),V=s("TFAutoModelFor"),ee=s(" para cargar instancias de modelos pre-entrenados. \xC9sto asegurar\xE1 que cargues la arquitectura correcta cada vez. En el siguiente "),z=l("a"),J=s("tutorial"),ae=s(", aprende a usar tu tokenizador reci\xE9n cargado, el extractor de caracter\xEDsticas y el procesador para preprocesar un dataset para fine-tuning."),this.h()},l(t){n=i(t,"P",{});var m=c(n);k=o(m,"Finalmente, la clase "),p=i(m,"CODE",{});var oe=c(p);g=o(oe,"TFAutoModelFor"),oe.forEach(a),q=o(m," te permite cargar tu modelo pre-entrenado para una tarea dada (revisa "),v=i(m,"A",{href:!0});var re=c(v);R=o(re,"aqu\xED"),re.forEach(a),x=o(m," para conocer la lista completa de tareas disponibles). Por ejemplo, carga un modelo para clasificaci\xF3n de secuencias con "),E=i(m,"CODE",{});var ne=c(E);A=o(ne,"TFAutoModelForSequenceClassification.from_pretrained()"),ne.forEach(a),w=o(m,":"),m.forEach(a),N=h(t),M(C.$$.fragment,t),H=h(t),P=i(t,"P",{});var se=c(P);B=o(se,"Reutiliza f\xE1cilmente el mismo checkpoint para cargar una aquitectura para alguna tarea diferente:"),se.forEach(a),U=h(t),M(y.$$.fragment,t),I=h(t),$=i(t,"P",{});var b=c($);W=o(b,"Generalmente recomendamos utilizar las clases "),F=i(b,"CODE",{});var G=c(F);Z=o(G,"AutoTokenizer"),G.forEach(a),X=o(b," y "),j=i(b,"CODE",{});var te=c(j);V=o(te,"TFAutoModelFor"),te.forEach(a),ee=o(b," para cargar instancias de modelos pre-entrenados. \xC9sto asegurar\xE1 que cargues la arquitectura correcta cada vez. En el siguiente "),z=i(b,"A",{href:!0});var Q=c(z);J=o(Q,"tutorial"),Q.forEach(a),ae=o(b,", aprende a usar tu tokenizador reci\xE9n cargado, el extractor de caracter\xEDsticas y el procesador para preprocesar un dataset para fine-tuning."),b.forEach(a),this.h()},h(){_(v,"href","model_doc/auto"),_(z,"href","preprocessing")},m(t,m){u(t,n,m),r(n,k),r(n,p),r(p,g),r(n,q),r(n,v),r(v,R),r(n,x),r(n,E),r(E,A),r(n,w),u(t,N,m),S(C,t,m),u(t,H,m),u(t,P,m),r(P,B),u(t,U,m),S(y,t,m),u(t,I,m),u(t,$,m),r($,W),r($,F),r(F,Z),r($,X),r($,j),r(j,V),r($,ee),r($,z),r(z,J),r($,ae),K=!0},p:dr,i(t){K||(L(C.$$.fragment,t),L(y.$$.fragment,t),K=!0)},o(t){O(C.$$.fragment,t),O(y.$$.fragment,t),K=!1},d(t){t&&a(n),t&&a(N),D(C,t),t&&a(H),t&&a(P),t&&a(U),D(y,t),t&&a(I),t&&a($)}}}function Ar(Y){let n,k;return n=new mr({props:{$$slots:{default:[Er]},$$scope:{ctx:Y}}}),{c(){T(n.$$.fragment)},l(p){M(n.$$.fragment,p)},m(p,g){S(n,p,g),k=!0},p(p,g){const q={};g&2&&(q.$$scope={dirty:g,ctx:p}),n.$set(q)},i(p){k||(L(n.$$.fragment,p),k=!0)},o(p){O(n.$$.fragment,p),k=!1},d(p){D(n,p)}}}function wr(Y){let n,k,p,g,q,v,R,x,E,A,w,N,C,H,P,B,U,y,I,$,W,F,Z,X,j,V,ee,z,J,ae,K,t,m,oe,re,ne,se,b,G,te,Q,ha,Te,$a,Ue,ye,ga,Ve,pe,_a,Me,va,ba,Je,be,Ke,Fe,ka,Qe,ke,We,le,ue,Se,je,ja,Le,qa,Xe,xe,Ea,Ye,de,Aa,Oe,wa,Ca,Ze,qe,ea,ie,me,De,Ee,ya,Ne,Fa,aa,fe,xa,Pe,Pa,za,ra,he,Ta,Re,Ma,Sa,ta,Ae,sa,ce,$e,Ie,we,La,Ge,Oa,oa,ge,na;return v=new Be({}),$=new vr({props:{$$slots:{default:[kr]},$$scope:{ctx:Y}}}),Q=new Be({}),be=new ve({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)`}}),ke=new ve({props:{code:`sequence = "In a hole in the ground there lived a hobbit."
print(tokenizer(sequence))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>sequence = <span class="hljs-string">&quot;In a hole in the ground there lived a hobbit.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tokenizer(sequence))
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">1037</span>, <span class="hljs-number">4920</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">2598</span>, <span class="hljs-number">2045</span>, <span class="hljs-number">2973</span>, <span class="hljs-number">1037</span>, <span class="hljs-number">7570</span>, <span class="hljs-number">10322</span>, <span class="hljs-number">4183</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),je=new Be({}),qe=new ve({props:{code:`from transformers import AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained(
    "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition&quot;</span>
<span class="hljs-meta">... </span>)`}}),Ee=new Be({}),Ae=new ve({props:{code:`from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/layoutlmv2-base-uncased&quot;</span>)`}}),we=new Be({}),ge=new br({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ar],pytorch:[qr]},$$scope:{ctx:Y}}}),{c(){n=l("meta"),k=f(),p=l("h1"),g=l("a"),q=l("span"),T(v.$$.fragment),R=f(),x=l("span"),E=s("Carga instancias preentrenadas con un AutoClass"),A=f(),w=l("p"),N=s("Con tantas arquitecturas diferentes de Transformer puede ser retador crear una para tu checkpoint. Como parte de la filosof\xEDa central de \u{1F917} Transformers para hacer que la biblioteca sea f\xE1cil, simple y flexible de usar; una "),C=l("code"),H=s("AutoClass"),P=s(" autom\xE1ticamente infiere y carga la arquitectura correcta desde un checkpoint dado. El m\xE9todo "),B=l("code"),U=s("from_pretrained"),y=s(" te permite cargar r\xE1pidamente un modelo preentrenado para cualquier arquitectura, por lo que no tendr\xE1s que dedicar tiempo y recursos para entrenar uno desde cero. Producir este tipo de c\xF3digo con checkpoint implica que si funciona con uno, funcionar\xE1 tambi\xE9n con otro (siempre que haya sido entrenado para una tarea similar) incluso si la arquitectura es distinta."),I=f(),T($.$$.fragment),W=f(),F=l("p"),Z=s("En este tutorial, aprender\xE1s a:"),X=f(),j=l("ul"),V=l("li"),ee=s("Cargar un tokenizador pre-entrenado."),z=f(),J=l("li"),ae=s("Cargar un extractor de caracter\xEDsticas (feature extractor en ingl\xE9s) pre-entrenado."),K=f(),t=l("li"),m=s("Cargar un procesador pre-entrenado."),oe=f(),re=l("li"),ne=s("Cargar un modelo pre-entrenado."),se=f(),b=l("h2"),G=l("a"),te=l("span"),T(Q.$$.fragment),ha=f(),Te=l("span"),$a=s("AutoTokenizer"),Ue=f(),ye=l("p"),ga=s("Casi cualquier tarea de Procesamiento de Lenguaje Natural comienza con un tokenizador. Un tokenizador convierte tu input a un formato que puede ser procesado por el modelo."),Ve=f(),pe=l("p"),_a=s("Carga un tokenizador con "),Me=l("code"),va=s("AutoTokenizer.from_pretrained()"),ba=s(":"),Je=f(),T(be.$$.fragment),Ke=f(),Fe=l("p"),ka=s("Luego tokeniza tu input como lo mostrado a continuaci\xF3n:"),Qe=f(),T(ke.$$.fragment),We=f(),le=l("h2"),ue=l("a"),Se=l("span"),T(je.$$.fragment),ja=f(),Le=l("span"),qa=s("AutoFeatureExtractor"),Xe=f(),xe=l("p"),Ea=s("Para tareas de audio y visi\xF3n, un extractor de caracter\xEDsticas procesa la se\xF1al de audio o imagen al formato de input correcto."),Ye=f(),de=l("p"),Aa=s("Carga un extractor de caracter\xEDsticas con "),Oe=l("code"),wa=s("AutoFeatureExtractor.from_pretrained()"),Ca=s(":"),Ze=f(),T(qe.$$.fragment),ea=f(),ie=l("h2"),me=l("a"),De=l("span"),T(Ee.$$.fragment),ya=f(),Ne=l("span"),Fa=s("AutoProcessor"),aa=f(),fe=l("p"),xa=s("Las tareas multimodales requieren un procesador que combine dos tipos de herramientas de preprocesamiento. Por ejemplo, el modelo "),Pe=l("a"),Pa=s("LayoutLMV2"),za=s(" requiere que un extractor de caracter\xEDsticas maneje las im\xE1genes y que un tokenizador maneje el texto; un procesador combina ambas."),ra=f(),he=l("p"),Ta=s("Carga un procesador con "),Re=l("code"),Ma=s("AutoProcessor.from_pretrained()"),Sa=s(":"),ta=f(),T(Ae.$$.fragment),sa=f(),ce=l("h2"),$e=l("a"),Ie=l("span"),T(we.$$.fragment),La=f(),Ge=l("span"),Oa=s("AutoModel"),oa=f(),T(ge.$$.fragment),this.h()},l(e){const d=gr('[data-svelte="svelte-1phssyn"]',document.head);n=i(d,"META",{name:!0,content:!0}),d.forEach(a),k=h(e),p=i(e,"H1",{class:!0});var Ce=c(p);g=i(Ce,"A",{id:!0,class:!0,href:!0});var He=c(g);q=i(He,"SPAN",{});var Da=c(q);M(v.$$.fragment,Da),Da.forEach(a),He.forEach(a),R=h(Ce),x=i(Ce,"SPAN",{});var Na=c(x);E=o(Na,"Carga instancias preentrenadas con un AutoClass"),Na.forEach(a),Ce.forEach(a),A=h(e),w=i(e,"P",{});var ze=c(w);N=o(ze,"Con tantas arquitecturas diferentes de Transformer puede ser retador crear una para tu checkpoint. Como parte de la filosof\xEDa central de \u{1F917} Transformers para hacer que la biblioteca sea f\xE1cil, simple y flexible de usar; una "),C=i(ze,"CODE",{});var Ra=c(C);H=o(Ra,"AutoClass"),Ra.forEach(a),P=o(ze," autom\xE1ticamente infiere y carga la arquitectura correcta desde un checkpoint dado. El m\xE9todo "),B=i(ze,"CODE",{});var Ia=c(B);U=o(Ia,"from_pretrained"),Ia.forEach(a),y=o(ze," te permite cargar r\xE1pidamente un modelo preentrenado para cualquier arquitectura, por lo que no tendr\xE1s que dedicar tiempo y recursos para entrenar uno desde cero. Producir este tipo de c\xF3digo con checkpoint implica que si funciona con uno, funcionar\xE1 tambi\xE9n con otro (siempre que haya sido entrenado para una tarea similar) incluso si la arquitectura es distinta."),ze.forEach(a),I=h(e),M($.$$.fragment,e),W=h(e),F=i(e,"P",{});var Ga=c(F);Z=o(Ga,"En este tutorial, aprender\xE1s a:"),Ga.forEach(a),X=h(e),j=i(e,"UL",{});var _e=c(j);V=i(_e,"LI",{});var Ha=c(V);ee=o(Ha,"Cargar un tokenizador pre-entrenado."),Ha.forEach(a),z=h(_e),J=i(_e,"LI",{});var Ba=c(J);ae=o(Ba,"Cargar un extractor de caracter\xEDsticas (feature extractor en ingl\xE9s) pre-entrenado."),Ba.forEach(a),K=h(_e),t=i(_e,"LI",{});var Ua=c(t);m=o(Ua,"Cargar un procesador pre-entrenado."),Ua.forEach(a),oe=h(_e),re=i(_e,"LI",{});var Va=c(re);ne=o(Va,"Cargar un modelo pre-entrenado."),Va.forEach(a),_e.forEach(a),se=h(e),b=i(e,"H2",{class:!0});var la=c(b);G=i(la,"A",{id:!0,class:!0,href:!0});var Ja=c(G);te=i(Ja,"SPAN",{});var Ka=c(te);M(Q.$$.fragment,Ka),Ka.forEach(a),Ja.forEach(a),ha=h(la),Te=i(la,"SPAN",{});var Qa=c(Te);$a=o(Qa,"AutoTokenizer"),Qa.forEach(a),la.forEach(a),Ue=h(e),ye=i(e,"P",{});var Wa=c(ye);ga=o(Wa,"Casi cualquier tarea de Procesamiento de Lenguaje Natural comienza con un tokenizador. Un tokenizador convierte tu input a un formato que puede ser procesado por el modelo."),Wa.forEach(a),Ve=h(e),pe=i(e,"P",{});var ia=c(pe);_a=o(ia,"Carga un tokenizador con "),Me=i(ia,"CODE",{});var Xa=c(Me);va=o(Xa,"AutoTokenizer.from_pretrained()"),Xa.forEach(a),ba=o(ia,":"),ia.forEach(a),Je=h(e),M(be.$$.fragment,e),Ke=h(e),Fe=i(e,"P",{});var Ya=c(Fe);ka=o(Ya,"Luego tokeniza tu input como lo mostrado a continuaci\xF3n:"),Ya.forEach(a),Qe=h(e),M(ke.$$.fragment,e),We=h(e),le=i(e,"H2",{class:!0});var ca=c(le);ue=i(ca,"A",{id:!0,class:!0,href:!0});var Za=c(ue);Se=i(Za,"SPAN",{});var er=c(Se);M(je.$$.fragment,er),er.forEach(a),Za.forEach(a),ja=h(ca),Le=i(ca,"SPAN",{});var ar=c(Le);qa=o(ar,"AutoFeatureExtractor"),ar.forEach(a),ca.forEach(a),Xe=h(e),xe=i(e,"P",{});var rr=c(xe);Ea=o(rr,"Para tareas de audio y visi\xF3n, un extractor de caracter\xEDsticas procesa la se\xF1al de audio o imagen al formato de input correcto."),rr.forEach(a),Ye=h(e),de=i(e,"P",{});var pa=c(de);Aa=o(pa,"Carga un extractor de caracter\xEDsticas con "),Oe=i(pa,"CODE",{});var tr=c(Oe);wa=o(tr,"AutoFeatureExtractor.from_pretrained()"),tr.forEach(a),Ca=o(pa,":"),pa.forEach(a),Ze=h(e),M(qe.$$.fragment,e),ea=h(e),ie=i(e,"H2",{class:!0});var ua=c(ie);me=i(ua,"A",{id:!0,class:!0,href:!0});var sr=c(me);De=i(sr,"SPAN",{});var or=c(De);M(Ee.$$.fragment,or),or.forEach(a),sr.forEach(a),ya=h(ua),Ne=i(ua,"SPAN",{});var nr=c(Ne);Fa=o(nr,"AutoProcessor"),nr.forEach(a),ua.forEach(a),aa=h(e),fe=i(e,"P",{});var da=c(fe);xa=o(da,"Las tareas multimodales requieren un procesador que combine dos tipos de herramientas de preprocesamiento. Por ejemplo, el modelo "),Pe=i(da,"A",{href:!0});var lr=c(Pe);Pa=o(lr,"LayoutLMV2"),lr.forEach(a),za=o(da," requiere que un extractor de caracter\xEDsticas maneje las im\xE1genes y que un tokenizador maneje el texto; un procesador combina ambas."),da.forEach(a),ra=h(e),he=i(e,"P",{});var ma=c(he);Ta=o(ma,"Carga un procesador con "),Re=i(ma,"CODE",{});var ir=c(Re);Ma=o(ir,"AutoProcessor.from_pretrained()"),ir.forEach(a),Sa=o(ma,":"),ma.forEach(a),ta=h(e),M(Ae.$$.fragment,e),sa=h(e),ce=i(e,"H2",{class:!0});var fa=c(ce);$e=i(fa,"A",{id:!0,class:!0,href:!0});var cr=c($e);Ie=i(cr,"SPAN",{});var pr=c(Ie);M(we.$$.fragment,pr),pr.forEach(a),cr.forEach(a),La=h(fa),Ge=i(fa,"SPAN",{});var ur=c(Ge);Oa=o(ur,"AutoModel"),ur.forEach(a),fa.forEach(a),oa=h(e),M(ge.$$.fragment,e),this.h()},h(){_(n,"name","hf:doc:metadata"),_(n,"content",JSON.stringify(Cr)),_(g,"id","carga-instancias-preentrenadas-con-un-autoclass"),_(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(g,"href","#carga-instancias-preentrenadas-con-un-autoclass"),_(p,"class","relative group"),_(G,"id","autotokenizer"),_(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(G,"href","#autotokenizer"),_(b,"class","relative group"),_(ue,"id","autofeatureextractor"),_(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(ue,"href","#autofeatureextractor"),_(le,"class","relative group"),_(me,"id","autoprocessor"),_(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_(me,"href","#autoprocessor"),_(ie,"class","relative group"),_(Pe,"href","model_doc/layoutlmv2"),_($e,"id","automodel"),_($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),_($e,"href","#automodel"),_(ce,"class","relative group")},m(e,d){r(document.head,n),u(e,k,d),u(e,p,d),r(p,g),r(g,q),S(v,q,null),r(p,R),r(p,x),r(x,E),u(e,A,d),u(e,w,d),r(w,N),r(w,C),r(C,H),r(w,P),r(w,B),r(B,U),r(w,y),u(e,I,d),S($,e,d),u(e,W,d),u(e,F,d),r(F,Z),u(e,X,d),u(e,j,d),r(j,V),r(V,ee),r(j,z),r(j,J),r(J,ae),r(j,K),r(j,t),r(t,m),r(j,oe),r(j,re),r(re,ne),u(e,se,d),u(e,b,d),r(b,G),r(G,te),S(Q,te,null),r(b,ha),r(b,Te),r(Te,$a),u(e,Ue,d),u(e,ye,d),r(ye,ga),u(e,Ve,d),u(e,pe,d),r(pe,_a),r(pe,Me),r(Me,va),r(pe,ba),u(e,Je,d),S(be,e,d),u(e,Ke,d),u(e,Fe,d),r(Fe,ka),u(e,Qe,d),S(ke,e,d),u(e,We,d),u(e,le,d),r(le,ue),r(ue,Se),S(je,Se,null),r(le,ja),r(le,Le),r(Le,qa),u(e,Xe,d),u(e,xe,d),r(xe,Ea),u(e,Ye,d),u(e,de,d),r(de,Aa),r(de,Oe),r(Oe,wa),r(de,Ca),u(e,Ze,d),S(qe,e,d),u(e,ea,d),u(e,ie,d),r(ie,me),r(me,De),S(Ee,De,null),r(ie,ya),r(ie,Ne),r(Ne,Fa),u(e,aa,d),u(e,fe,d),r(fe,xa),r(fe,Pe),r(Pe,Pa),r(fe,za),u(e,ra,d),u(e,he,d),r(he,Ta),r(he,Re),r(Re,Ma),r(he,Sa),u(e,ta,d),S(Ae,e,d),u(e,sa,d),u(e,ce,d),r(ce,$e),r($e,Ie),S(we,Ie,null),r(ce,La),r(ce,Ge),r(Ge,Oa),u(e,oa,d),S(ge,e,d),na=!0},p(e,[d]){const Ce={};d&2&&(Ce.$$scope={dirty:d,ctx:e}),$.$set(Ce);const He={};d&2&&(He.$$scope={dirty:d,ctx:e}),ge.$set(He)},i(e){na||(L(v.$$.fragment,e),L($.$$.fragment,e),L(Q.$$.fragment,e),L(be.$$.fragment,e),L(ke.$$.fragment,e),L(je.$$.fragment,e),L(qe.$$.fragment,e),L(Ee.$$.fragment,e),L(Ae.$$.fragment,e),L(we.$$.fragment,e),L(ge.$$.fragment,e),na=!0)},o(e){O(v.$$.fragment,e),O($.$$.fragment,e),O(Q.$$.fragment,e),O(be.$$.fragment,e),O(ke.$$.fragment,e),O(je.$$.fragment,e),O(qe.$$.fragment,e),O(Ee.$$.fragment,e),O(Ae.$$.fragment,e),O(we.$$.fragment,e),O(ge.$$.fragment,e),na=!1},d(e){a(n),e&&a(k),e&&a(p),D(v),e&&a(A),e&&a(w),e&&a(I),D($,e),e&&a(W),e&&a(F),e&&a(X),e&&a(j),e&&a(se),e&&a(b),D(Q),e&&a(Ue),e&&a(ye),e&&a(Ve),e&&a(pe),e&&a(Je),D(be,e),e&&a(Ke),e&&a(Fe),e&&a(Qe),D(ke,e),e&&a(We),e&&a(le),D(je),e&&a(Xe),e&&a(xe),e&&a(Ye),e&&a(de),e&&a(Ze),D(qe,e),e&&a(ea),e&&a(ie),D(Ee),e&&a(aa),e&&a(fe),e&&a(ra),e&&a(he),e&&a(ta),D(Ae,e),e&&a(sa),e&&a(ce),D(we),e&&a(oa),D(ge,e)}}}const Cr={local:"carga-instancias-preentrenadas-con-un-autoclass",sections:[{local:"autotokenizer",title:"AutoTokenizer"},{local:"autofeatureextractor",title:"AutoFeatureExtractor"},{local:"autoprocessor",title:"AutoProcessor"},{local:"automodel",title:"AutoModel"}],title:"Carga instancias preentrenadas con un AutoClass"};function yr(Y){return _r(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Sr extends fr{constructor(n){super();hr(this,n,yr,wr,$r,{})}}export{Sr as default,Cr as metadata};
