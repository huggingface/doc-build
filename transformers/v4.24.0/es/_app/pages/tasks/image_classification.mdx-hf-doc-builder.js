import{S as kt,i as xt,s as At,e as r,k as m,w as $,t as l,M as Pt,c as o,d as a,m as c,a as i,x as v,h as n,b as u,G as s,g as p,y as b,q as j,o as E,B as w,v as Dt}from"../../chunks/vendor-hf-doc-builder.js";import{T as Gs}from"../../chunks/Tip-hf-doc-builder.js";import{Y as zt}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Ma}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as P}from"../../chunks/CodeBlock-hf-doc-builder.js";function Tt(R){let f,C,d,_,y;return{c(){f=r("p"),C=l("Consulta la "),d=r("a"),_=l("p\xE1gina de la tarea"),y=l(" de clasificaci\xF3n de im\xE1genes para obtener m\xE1s informaci\xF3n sobre sus modelos, datasets y m\xE9tricas asociadas."),this.h()},l(h){f=o(h,"P",{});var g=i(f);C=n(g,"Consulta la "),d=o(g,"A",{href:!0,rel:!0});var q=i(d);_=n(q,"p\xE1gina de la tarea"),q.forEach(a),y=n(g," de clasificaci\xF3n de im\xE1genes para obtener m\xE1s informaci\xF3n sobre sus modelos, datasets y m\xE9tricas asociadas."),g.forEach(a),this.h()},h(){u(d,"href","https://huggingface.co/tasks/audio-classification"),u(d,"rel","nofollow")},m(h,g){p(h,f,g),s(f,C),s(f,d),s(d,_),s(f,y)},d(h){h&&a(f)}}}function Ft(R){let f,C,d,_,y,h,g,q;return{c(){f=r("p"),C=l("Si no est\xE1s familiarizado con el fine-tuning de un modelo con el "),d=r("code"),_=l("Trainer"),y=l(", echa un vistazo al tutorial b\xE1sico "),h=r("a"),g=l("aqu\xED"),q=l("!"),this.h()},l(I){f=o(I,"P",{});var x=i(f);C=n(x,"Si no est\xE1s familiarizado con el fine-tuning de un modelo con el "),d=o(x,"CODE",{});var D=i(d);_=n(D,"Trainer"),D.forEach(a),y=n(x,", echa un vistazo al tutorial b\xE1sico "),h=o(x,"A",{href:!0});var W=i(h);g=n(W,"aqu\xED"),W.forEach(a),q=n(x,"!"),x.forEach(a),this.h()},h(){u(h,"href","../training#finetune-with-trainer")},m(I,x){p(I,f,x),s(f,C),s(f,d),s(d,_),s(f,y),s(f,h),s(h,g),s(f,q)},d(I){I&&a(f)}}}function It(R){let f,C,d,_,y;return{c(){f=r("p"),C=l("Para ver un ejemplo m\xE1s a profundidad de c\xF3mo hacer fine-tune a un modelo para clasificaci\xF3n de im\xE1genes, echa un vistazo al correspondiente "),d=r("a"),_=l("PyTorch notebook"),y=l("."),this.h()},l(h){f=o(h,"P",{});var g=i(f);C=n(g,"Para ver un ejemplo m\xE1s a profundidad de c\xF3mo hacer fine-tune a un modelo para clasificaci\xF3n de im\xE1genes, echa un vistazo al correspondiente "),d=o(g,"A",{href:!0,rel:!0});var q=i(d);_=n(q,"PyTorch notebook"),q.forEach(a),y=n(g,"."),g.forEach(a),this.h()},h(){u(d,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb"),u(d,"rel","nofollow")},m(h,g){p(h,f,g),s(f,C),s(f,d),s(d,_),s(f,y)},d(h){h&&a(f)}}}function Ot(R){let f,C,d,_,y,h,g,q,I,x,D,W,be,Ba,Xe,z,Va,X,Ua,Ga,Z,Ha,Ja,Ze,N,ea,O,M,Pe,ee,Ya,De,Ka,aa,je,Qa,sa,ae,ta,Ee,Wa,la,se,na,we,Xa,ra,te,oa,T,Za,ze,es,as,Te,ss,ts,ia,le,pa,ye,ls,ma,ne,ca,B,ns,Fe,rs,os,fa,L,V,Ie,re,is,Oe,ps,da,Ce,ms,ua,oe,ha,U,cs,ie,Le,fs,ds,ga,pe,_a,G,us,Se,hs,gs,$a,me,va,H,_s,ce,Re,$s,vs,ba,fe,ja,J,bs,Ne,js,Es,Ea,de,wa,S,Y,Me,ue,ws,Be,ys,ya,he,Ca,K,qa,qe,Cs,ka,F,k,qs,Ve,ks,xs,Ue,As,Ps,Ge,Ds,zs,He,Ts,Fs,Je,Is,Os,Ls,ge,Ss,Ye,Rs,Ns,Ms,_e,Bs,Ke,Vs,Us,xa,$e,Aa,Q,Pa;return h=new Ma({}),D=new zt({props:{id:"tjAIM7BOYhw"}}),N=new Gs({props:{$$slots:{default:[Tt]},$$scope:{ctx:R}}}),ee=new Ma({}),ae=new P({props:{code:`from datasets import load_dataset

food = load_dataset("food101", split="train[:5000]")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>food = load_dataset(<span class="hljs-string">&quot;food101&quot;</span>, split=<span class="hljs-string">&quot;train[:5000]&quot;</span>)`}}),se=new P({props:{code:"food = food.train_test_split(test_size=0.2)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>food = food.train_test_split(test_size=<span class="hljs-number">0.2</span>)'}}),te=new P({props:{code:'food["train"][0]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>food[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;image&#x27;</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at <span class="hljs-number">0x7F52AFC8AC50</span>&gt;,
 <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">79</span>}`}}),le=new P({props:{code:`labels = food["train"].features["label"].names
label2id, id2label = dict(), dict()
for i, label in enumerate(labels):
    label2id[label] = str(i)
    id2label[str(i)] = label`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = food[<span class="hljs-string">&quot;train&quot;</span>].features[<span class="hljs-string">&quot;label&quot;</span>].names
<span class="hljs-meta">&gt;&gt;&gt; </span>label2id, id2label = <span class="hljs-built_in">dict</span>(), <span class="hljs-built_in">dict</span>()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels):
<span class="hljs-meta">... </span>    label2id[label] = <span class="hljs-built_in">str</span>(i)
<span class="hljs-meta">... </span>    id2label[<span class="hljs-built_in">str</span>(i)] = label`}}),ne=new P({props:{code:"id2label[str(79)]",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>id2label[<span class="hljs-built_in">str</span>(<span class="hljs-number">79</span>)]
<span class="hljs-string">&#x27;prime_rib&#x27;</span>`}}),re=new Ma({}),oe=new P({props:{code:`from transformers import AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained("google/vit-base-patch16-224-in21k")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>)`}}),pe=new P({props:{code:`from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor

normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
_transforms = Compose([RandomResizedCrop(feature_extractor.size), ToTensor(), normalize])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> RandomResizedCrop, Compose, Normalize, ToTensor

<span class="hljs-meta">&gt;&gt;&gt; </span>normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
<span class="hljs-meta">&gt;&gt;&gt; </span>_transforms = Compose([RandomResizedCrop(feature_extractor.size), ToTensor(), normalize])`}}),me=new P({props:{code:`def transforms(examples):
    examples["pixel_values"] = [_transforms(img.convert("RGB")) for img in examples["image"]]
    del examples["image"]
    return examples`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">transforms</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    examples[<span class="hljs-string">&quot;pixel_values&quot;</span>] = [_transforms(img.convert(<span class="hljs-string">&quot;RGB&quot;</span>)) <span class="hljs-keyword">for</span> img <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">del</span> examples[<span class="hljs-string">&quot;image&quot;</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> examples`}}),fe=new P({props:{code:"food = food.with_transform(transforms)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>food = food.with_transform(transforms)'}}),de=new P({props:{code:`from transformers import DefaultDataCollator

data_collator = DefaultDataCollator()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DefaultDataCollator

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DefaultDataCollator()`}}),ue=new Ma({}),he=new P({props:{code:`from transformers import AutoModelForImageClassification, TrainingArguments, Trainer

model = AutoModelForImageClassification.from_pretrained(
    "google/vit-base-patch16-224-in21k",
    num_labels=len(labels),
    id2label=id2label,
    label2id=label2id,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForImageClassification, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>,
<span class="hljs-meta">... </span>    num_labels=<span class="hljs-built_in">len</span>(labels),
<span class="hljs-meta">... </span>    id2label=id2label,
<span class="hljs-meta">... </span>    label2id=label2id,
<span class="hljs-meta">... </span>)`}}),K=new Gs({props:{$$slots:{default:[Ft]},$$scope:{ctx:R}}}),$e=new P({props:{code:`training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=16,
    evaluation_strategy="steps",
    num_train_epochs=4,
    fp16=True,
    save_steps=100,
    eval_steps=100,
    logging_steps=10,
    learning_rate=2e-4,
    save_total_limit=2,
    remove_unused_columns=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=food["train"],
    eval_dataset=food["test"],
    tokenizer=feature_extractor,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;steps&quot;</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">4</span>,
<span class="hljs-meta">... </span>    fp16=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    save_steps=<span class="hljs-number">100</span>,
<span class="hljs-meta">... </span>    eval_steps=<span class="hljs-number">100</span>,
<span class="hljs-meta">... </span>    logging_steps=<span class="hljs-number">10</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-4</span>,
<span class="hljs-meta">... </span>    save_total_limit=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>    remove_unused_columns=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>    train_dataset=food[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=food[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=feature_extractor,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),Q=new Gs({props:{$$slots:{default:[It]},$$scope:{ctx:R}}}),{c(){f=r("meta"),C=m(),d=r("h1"),_=r("a"),y=r("span"),$(h.$$.fragment),g=m(),q=r("span"),I=l("Clasificaci\xF3n de im\xE1genes"),x=m(),$(D.$$.fragment),W=m(),be=r("p"),Ba=l("La clasificaci\xF3n de im\xE1genes asigna una etiqueta o clase a una imagen. A diferencia de la clasificaci\xF3n de texto o audio, las entradas son los valores de los p\xEDxeles que representan una imagen. La clasificaci\xF3n de im\xE1genes tiene muchos usos, como la detecci\xF3n de da\xF1os tras una cat\xE1strofe, el control de la salud de los cultivos o la b\xFAsqueda de signos de enfermedad en im\xE1genes m\xE9dicas."),Xe=m(),z=r("p"),Va=l("Esta gu\xEDa te mostrar\xE1 como hacer fine-tune al "),X=r("a"),Ua=l("ViT"),Ga=l(" en el dataset "),Z=r("a"),Ha=l("Food-101"),Ja=l(" para clasificar un alimento en una imagen."),Ze=m(),$(N.$$.fragment),ea=m(),O=r("h2"),M=r("a"),Pe=r("span"),$(ee.$$.fragment),Ya=m(),De=r("span"),Ka=l("Carga el dataset Food-101"),aa=m(),je=r("p"),Qa=l("Carga solo las primeras 5000 im\xE1genes del dataset Food-101 de la biblioteca \u{1F917} de Datasets ya que es bastante grande:"),sa=m(),$(ae.$$.fragment),ta=m(),Ee=r("p"),Wa=l("Divide el dataset en un train y un test set:"),la=m(),$(se.$$.fragment),na=m(),we=r("p"),Xa=l("A continuaci\xF3n, observa un ejemplo:"),ra=m(),$(te.$$.fragment),oa=m(),T=r("p"),Za=l("El campo "),ze=r("code"),es=l("image"),as=l(" contiene una imagen PIL, y cada "),Te=r("code"),ss=l("label"),ts=l(" es un n\xFAmero entero que representa una clase. Crea un diccionario que asigne un nombre de label a un entero y viceversa. El mapeo ayudar\xE1 al modelo a recuperar el nombre de label a partir del n\xFAmero de la misma:"),ia=m(),$(le.$$.fragment),pa=m(),ye=r("p"),ls=l("Ahora puedes convertir el n\xFAmero de label en un nombre de label para obtener m\xE1s informaci\xF3n:"),ma=m(),$(ne.$$.fragment),ca=m(),B=r("p"),ns=l("Cada clase de alimento - o label - corresponde a un n\xFAmero; "),Fe=r("code"),rs=l("79"),os=l(" indica una costilla de primera en el ejemplo anterior."),fa=m(),L=r("h2"),V=r("a"),Ie=r("span"),$(re.$$.fragment),is=m(),Oe=r("span"),ps=l("Preprocesa"),da=m(),Ce=r("p"),ms=l("Carga el feature extractor de ViT para procesar la imagen en un tensor:"),ua=m(),$(oe.$$.fragment),ha=m(),U=r("p"),cs=l("Aplica varias transformaciones de imagen al dataset para hacer el modelo m\xE1s robusto contra el overfitting. En este caso se utilizar\xE1 el m\xF3dulo "),ie=r("a"),Le=r("code"),fs=l("transforms"),ds=l(" de torchvision. Recorta una parte aleatoria de la imagen, cambia su tama\xF1o y normal\xEDzala con la media y la desviaci\xF3n est\xE1ndar de la imagen:"),ga=m(),$(pe.$$.fragment),_a=m(),G=r("p"),us=l("Crea una funci\xF3n de preprocesamiento que aplique las transformaciones y devuelva los "),Se=r("code"),hs=l("pixel_values"),gs=l(" - los inputs al modelo - de la imagen:"),$a=m(),$(me.$$.fragment),va=m(),H=r("p"),_s=l("Utiliza el m\xE9todo "),ce=r("a"),Re=r("code"),$s=l("with_transform"),vs=l(" de \u{1F917} Dataset para aplicar las transformaciones sobre todo el dataset. Las transformaciones se aplican sobre la marcha cuando se carga un elemento del dataset:"),ba=m(),$(fe.$$.fragment),ja=m(),J=r("p"),bs=l("Utiliza "),Ne=r("code"),js=l("DefaultDataCollator"),Es=l(" para crear un batch de ejemplos. A diferencia de otros data collators en \u{1F917} Transformers, el DefaultDataCollator no aplica un preprocesamiento adicional como el padding."),Ea=m(),$(de.$$.fragment),wa=m(),S=r("h2"),Y=r("a"),Me=r("span"),$(ue.$$.fragment),ws=m(),Be=r("span"),ys=l("Entrena"),ya=l(`

Carga ViT con \`AutoModelForImageClassification\`. Especifica el n\xFAmero de labels, y pasa al modelo el mapping entre el n\xFAmero de label y la clase de label:

	`),$(he.$$.fragment),Ca=m(),$(K.$$.fragment),qa=m(),qe=r("p"),Cs=l("Al llegar a este punto, solo quedan tres pasos:"),ka=m(),F=r("ol"),k=r("li"),qs=l("Define tus hiperpar\xE1metros de entrenamiento en "),Ve=r("code"),ks=l("TrainingArguments"),xs=l(". Es importante que no elimines las columnas que no se utilicen, ya que esto har\xE1 que desaparezca la columna "),Ue=r("code"),As=l("image"),Ps=l(". Sin la columna "),Ge=r("code"),Ds=l("image"),zs=l(" no puedes crear "),He=r("code"),Ts=l("pixel_values"),Fs=l(". Establece "),Je=r("code"),Is=l("remove_unused_columns=False"),Os=l(" para evitar este comportamiento."),Ls=m(),ge=r("li"),Ss=l("Pasa los training arguments al "),Ye=r("code"),Rs=l("Trainer"),Ns=l(" junto con el modelo, los datasets, tokenizer y data collator."),Ms=m(),_e=r("li"),Bs=l("Llama "),Ke=r("code"),Vs=l("train()"),Us=l(" para hacer fine-tune de tu modelo."),xa=m(),$($e.$$.fragment),Aa=m(),$(Q.$$.fragment),this.h()},l(e){const t=Pt('[data-svelte="svelte-1phssyn"]',document.head);f=o(t,"META",{name:!0,content:!0}),t.forEach(a),C=c(e),d=o(e,"H1",{class:!0});var ve=i(d);_=o(ve,"A",{id:!0,class:!0,href:!0});var Qe=i(_);y=o(Qe,"SPAN",{});var We=i(y);v(h.$$.fragment,We),We.forEach(a),Qe.forEach(a),g=c(ve),q=o(ve,"SPAN",{});var Hs=i(q);I=n(Hs,"Clasificaci\xF3n de im\xE1genes"),Hs.forEach(a),ve.forEach(a),x=c(e),v(D.$$.fragment,e),W=c(e),be=o(e,"P",{});var Js=i(be);Ba=n(Js,"La clasificaci\xF3n de im\xE1genes asigna una etiqueta o clase a una imagen. A diferencia de la clasificaci\xF3n de texto o audio, las entradas son los valores de los p\xEDxeles que representan una imagen. La clasificaci\xF3n de im\xE1genes tiene muchos usos, como la detecci\xF3n de da\xF1os tras una cat\xE1strofe, el control de la salud de los cultivos o la b\xFAsqueda de signos de enfermedad en im\xE1genes m\xE9dicas."),Js.forEach(a),Xe=c(e),z=o(e,"P",{});var ke=i(z);Va=n(ke,"Esta gu\xEDa te mostrar\xE1 como hacer fine-tune al "),X=o(ke,"A",{href:!0,rel:!0});var Ys=i(X);Ua=n(Ys,"ViT"),Ys.forEach(a),Ga=n(ke," en el dataset "),Z=o(ke,"A",{href:!0,rel:!0});var Ks=i(Z);Ha=n(Ks,"Food-101"),Ks.forEach(a),Ja=n(ke," para clasificar un alimento en una imagen."),ke.forEach(a),Ze=c(e),v(N.$$.fragment,e),ea=c(e),O=o(e,"H2",{class:!0});var Da=i(O);M=o(Da,"A",{id:!0,class:!0,href:!0});var Qs=i(M);Pe=o(Qs,"SPAN",{});var Ws=i(Pe);v(ee.$$.fragment,Ws),Ws.forEach(a),Qs.forEach(a),Ya=c(Da),De=o(Da,"SPAN",{});var Xs=i(De);Ka=n(Xs,"Carga el dataset Food-101"),Xs.forEach(a),Da.forEach(a),aa=c(e),je=o(e,"P",{});var Zs=i(je);Qa=n(Zs,"Carga solo las primeras 5000 im\xE1genes del dataset Food-101 de la biblioteca \u{1F917} de Datasets ya que es bastante grande:"),Zs.forEach(a),sa=c(e),v(ae.$$.fragment,e),ta=c(e),Ee=o(e,"P",{});var et=i(Ee);Wa=n(et,"Divide el dataset en un train y un test set:"),et.forEach(a),la=c(e),v(se.$$.fragment,e),na=c(e),we=o(e,"P",{});var at=i(we);Xa=n(at,"A continuaci\xF3n, observa un ejemplo:"),at.forEach(a),ra=c(e),v(te.$$.fragment,e),oa=c(e),T=o(e,"P",{});var xe=i(T);Za=n(xe,"El campo "),ze=o(xe,"CODE",{});var st=i(ze);es=n(st,"image"),st.forEach(a),as=n(xe," contiene una imagen PIL, y cada "),Te=o(xe,"CODE",{});var tt=i(Te);ss=n(tt,"label"),tt.forEach(a),ts=n(xe," es un n\xFAmero entero que representa una clase. Crea un diccionario que asigne un nombre de label a un entero y viceversa. El mapeo ayudar\xE1 al modelo a recuperar el nombre de label a partir del n\xFAmero de la misma:"),xe.forEach(a),ia=c(e),v(le.$$.fragment,e),pa=c(e),ye=o(e,"P",{});var lt=i(ye);ls=n(lt,"Ahora puedes convertir el n\xFAmero de label en un nombre de label para obtener m\xE1s informaci\xF3n:"),lt.forEach(a),ma=c(e),v(ne.$$.fragment,e),ca=c(e),B=o(e,"P",{});var za=i(B);ns=n(za,"Cada clase de alimento - o label - corresponde a un n\xFAmero; "),Fe=o(za,"CODE",{});var nt=i(Fe);rs=n(nt,"79"),nt.forEach(a),os=n(za," indica una costilla de primera en el ejemplo anterior."),za.forEach(a),fa=c(e),L=o(e,"H2",{class:!0});var Ta=i(L);V=o(Ta,"A",{id:!0,class:!0,href:!0});var rt=i(V);Ie=o(rt,"SPAN",{});var ot=i(Ie);v(re.$$.fragment,ot),ot.forEach(a),rt.forEach(a),is=c(Ta),Oe=o(Ta,"SPAN",{});var it=i(Oe);ps=n(it,"Preprocesa"),it.forEach(a),Ta.forEach(a),da=c(e),Ce=o(e,"P",{});var pt=i(Ce);ms=n(pt,"Carga el feature extractor de ViT para procesar la imagen en un tensor:"),pt.forEach(a),ua=c(e),v(oe.$$.fragment,e),ha=c(e),U=o(e,"P",{});var Fa=i(U);cs=n(Fa,"Aplica varias transformaciones de imagen al dataset para hacer el modelo m\xE1s robusto contra el overfitting. En este caso se utilizar\xE1 el m\xF3dulo "),ie=o(Fa,"A",{href:!0,rel:!0});var mt=i(ie);Le=o(mt,"CODE",{});var ct=i(Le);fs=n(ct,"transforms"),ct.forEach(a),mt.forEach(a),ds=n(Fa," de torchvision. Recorta una parte aleatoria de la imagen, cambia su tama\xF1o y normal\xEDzala con la media y la desviaci\xF3n est\xE1ndar de la imagen:"),Fa.forEach(a),ga=c(e),v(pe.$$.fragment,e),_a=c(e),G=o(e,"P",{});var Ia=i(G);us=n(Ia,"Crea una funci\xF3n de preprocesamiento que aplique las transformaciones y devuelva los "),Se=o(Ia,"CODE",{});var ft=i(Se);hs=n(ft,"pixel_values"),ft.forEach(a),gs=n(Ia," - los inputs al modelo - de la imagen:"),Ia.forEach(a),$a=c(e),v(me.$$.fragment,e),va=c(e),H=o(e,"P",{});var Oa=i(H);_s=n(Oa,"Utiliza el m\xE9todo "),ce=o(Oa,"A",{href:!0,rel:!0});var dt=i(ce);Re=o(dt,"CODE",{});var ut=i(Re);$s=n(ut,"with_transform"),ut.forEach(a),dt.forEach(a),vs=n(Oa," de \u{1F917} Dataset para aplicar las transformaciones sobre todo el dataset. Las transformaciones se aplican sobre la marcha cuando se carga un elemento del dataset:"),Oa.forEach(a),ba=c(e),v(fe.$$.fragment,e),ja=c(e),J=o(e,"P",{});var La=i(J);bs=n(La,"Utiliza "),Ne=o(La,"CODE",{});var ht=i(Ne);js=n(ht,"DefaultDataCollator"),ht.forEach(a),Es=n(La," para crear un batch de ejemplos. A diferencia de otros data collators en \u{1F917} Transformers, el DefaultDataCollator no aplica un preprocesamiento adicional como el padding."),La.forEach(a),Ea=c(e),v(de.$$.fragment,e),wa=c(e),S=o(e,"H2",{class:!0});var Sa=i(S);Y=o(Sa,"A",{id:!0,class:!0,href:!0});var gt=i(Y);Me=o(gt,"SPAN",{});var _t=i(Me);v(ue.$$.fragment,_t),_t.forEach(a),gt.forEach(a),ws=c(Sa),Be=o(Sa,"SPAN",{});var $t=i(Be);ys=n($t,"Entrena"),$t.forEach(a),Sa.forEach(a),ya=n(e,`

Carga ViT con \`AutoModelForImageClassification\`. Especifica el n\xFAmero de labels, y pasa al modelo el mapping entre el n\xFAmero de label y la clase de label:

	`),v(he.$$.fragment,e),Ca=c(e),v(K.$$.fragment,e),qa=c(e),qe=o(e,"P",{});var vt=i(qe);Cs=n(vt,"Al llegar a este punto, solo quedan tres pasos:"),vt.forEach(a),ka=c(e),F=o(e,"OL",{});var Ae=i(F);k=o(Ae,"LI",{});var A=i(k);qs=n(A,"Define tus hiperpar\xE1metros de entrenamiento en "),Ve=o(A,"CODE",{});var bt=i(Ve);ks=n(bt,"TrainingArguments"),bt.forEach(a),xs=n(A,". Es importante que no elimines las columnas que no se utilicen, ya que esto har\xE1 que desaparezca la columna "),Ue=o(A,"CODE",{});var jt=i(Ue);As=n(jt,"image"),jt.forEach(a),Ps=n(A,". Sin la columna "),Ge=o(A,"CODE",{});var Et=i(Ge);Ds=n(Et,"image"),Et.forEach(a),zs=n(A," no puedes crear "),He=o(A,"CODE",{});var wt=i(He);Ts=n(wt,"pixel_values"),wt.forEach(a),Fs=n(A,". Establece "),Je=o(A,"CODE",{});var yt=i(Je);Is=n(yt,"remove_unused_columns=False"),yt.forEach(a),Os=n(A," para evitar este comportamiento."),A.forEach(a),Ls=c(Ae),ge=o(Ae,"LI",{});var Ra=i(ge);Ss=n(Ra,"Pasa los training arguments al "),Ye=o(Ra,"CODE",{});var Ct=i(Ye);Rs=n(Ct,"Trainer"),Ct.forEach(a),Ns=n(Ra," junto con el modelo, los datasets, tokenizer y data collator."),Ra.forEach(a),Ms=c(Ae),_e=o(Ae,"LI",{});var Na=i(_e);Bs=n(Na,"Llama "),Ke=o(Na,"CODE",{});var qt=i(Ke);Vs=n(qt,"train()"),qt.forEach(a),Us=n(Na," para hacer fine-tune de tu modelo."),Na.forEach(a),Ae.forEach(a),xa=c(e),v($e.$$.fragment,e),Aa=c(e),v(Q.$$.fragment,e),this.h()},h(){u(f,"name","hf:doc:metadata"),u(f,"content",JSON.stringify(Lt)),u(_,"id","clasificacin-de-imgenes"),u(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(_,"href","#clasificacin-de-imgenes"),u(d,"class","relative group"),u(X,"href","https://huggingface.co/docs/transformers/v4.16.2/en/model_doc/vit"),u(X,"rel","nofollow"),u(Z,"href","https://huggingface.co/datasets/food101"),u(Z,"rel","nofollow"),u(M,"id","carga-el-dataset-food101"),u(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(M,"href","#carga-el-dataset-food101"),u(O,"class","relative group"),u(V,"id","preprocesa"),u(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(V,"href","#preprocesa"),u(L,"class","relative group"),u(ie,"href","https://pytorch.org/vision/stable/transforms.html"),u(ie,"rel","nofollow"),u(ce,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html?#datasets.Dataset.with_transform"),u(ce,"rel","nofollow"),u(Y,"id","entrena"),u(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Y,"href","#entrena"),u(S,"class","relative group")},m(e,t){s(document.head,f),p(e,C,t),p(e,d,t),s(d,_),s(_,y),b(h,y,null),s(d,g),s(d,q),s(q,I),p(e,x,t),b(D,e,t),p(e,W,t),p(e,be,t),s(be,Ba),p(e,Xe,t),p(e,z,t),s(z,Va),s(z,X),s(X,Ua),s(z,Ga),s(z,Z),s(Z,Ha),s(z,Ja),p(e,Ze,t),b(N,e,t),p(e,ea,t),p(e,O,t),s(O,M),s(M,Pe),b(ee,Pe,null),s(O,Ya),s(O,De),s(De,Ka),p(e,aa,t),p(e,je,t),s(je,Qa),p(e,sa,t),b(ae,e,t),p(e,ta,t),p(e,Ee,t),s(Ee,Wa),p(e,la,t),b(se,e,t),p(e,na,t),p(e,we,t),s(we,Xa),p(e,ra,t),b(te,e,t),p(e,oa,t),p(e,T,t),s(T,Za),s(T,ze),s(ze,es),s(T,as),s(T,Te),s(Te,ss),s(T,ts),p(e,ia,t),b(le,e,t),p(e,pa,t),p(e,ye,t),s(ye,ls),p(e,ma,t),b(ne,e,t),p(e,ca,t),p(e,B,t),s(B,ns),s(B,Fe),s(Fe,rs),s(B,os),p(e,fa,t),p(e,L,t),s(L,V),s(V,Ie),b(re,Ie,null),s(L,is),s(L,Oe),s(Oe,ps),p(e,da,t),p(e,Ce,t),s(Ce,ms),p(e,ua,t),b(oe,e,t),p(e,ha,t),p(e,U,t),s(U,cs),s(U,ie),s(ie,Le),s(Le,fs),s(U,ds),p(e,ga,t),b(pe,e,t),p(e,_a,t),p(e,G,t),s(G,us),s(G,Se),s(Se,hs),s(G,gs),p(e,$a,t),b(me,e,t),p(e,va,t),p(e,H,t),s(H,_s),s(H,ce),s(ce,Re),s(Re,$s),s(H,vs),p(e,ba,t),b(fe,e,t),p(e,ja,t),p(e,J,t),s(J,bs),s(J,Ne),s(Ne,js),s(J,Es),p(e,Ea,t),b(de,e,t),p(e,wa,t),p(e,S,t),s(S,Y),s(Y,Me),b(ue,Me,null),s(S,ws),s(S,Be),s(Be,ys),p(e,ya,t),b(he,e,t),p(e,Ca,t),b(K,e,t),p(e,qa,t),p(e,qe,t),s(qe,Cs),p(e,ka,t),p(e,F,t),s(F,k),s(k,qs),s(k,Ve),s(Ve,ks),s(k,xs),s(k,Ue),s(Ue,As),s(k,Ps),s(k,Ge),s(Ge,Ds),s(k,zs),s(k,He),s(He,Ts),s(k,Fs),s(k,Je),s(Je,Is),s(k,Os),s(F,Ls),s(F,ge),s(ge,Ss),s(ge,Ye),s(Ye,Rs),s(ge,Ns),s(F,Ms),s(F,_e),s(_e,Bs),s(_e,Ke),s(Ke,Vs),s(_e,Us),p(e,xa,t),b($e,e,t),p(e,Aa,t),b(Q,e,t),Pa=!0},p(e,[t]){const ve={};t&2&&(ve.$$scope={dirty:t,ctx:e}),N.$set(ve);const Qe={};t&2&&(Qe.$$scope={dirty:t,ctx:e}),K.$set(Qe);const We={};t&2&&(We.$$scope={dirty:t,ctx:e}),Q.$set(We)},i(e){Pa||(j(h.$$.fragment,e),j(D.$$.fragment,e),j(N.$$.fragment,e),j(ee.$$.fragment,e),j(ae.$$.fragment,e),j(se.$$.fragment,e),j(te.$$.fragment,e),j(le.$$.fragment,e),j(ne.$$.fragment,e),j(re.$$.fragment,e),j(oe.$$.fragment,e),j(pe.$$.fragment,e),j(me.$$.fragment,e),j(fe.$$.fragment,e),j(de.$$.fragment,e),j(ue.$$.fragment,e),j(he.$$.fragment,e),j(K.$$.fragment,e),j($e.$$.fragment,e),j(Q.$$.fragment,e),Pa=!0)},o(e){E(h.$$.fragment,e),E(D.$$.fragment,e),E(N.$$.fragment,e),E(ee.$$.fragment,e),E(ae.$$.fragment,e),E(se.$$.fragment,e),E(te.$$.fragment,e),E(le.$$.fragment,e),E(ne.$$.fragment,e),E(re.$$.fragment,e),E(oe.$$.fragment,e),E(pe.$$.fragment,e),E(me.$$.fragment,e),E(fe.$$.fragment,e),E(de.$$.fragment,e),E(ue.$$.fragment,e),E(he.$$.fragment,e),E(K.$$.fragment,e),E($e.$$.fragment,e),E(Q.$$.fragment,e),Pa=!1},d(e){a(f),e&&a(C),e&&a(d),w(h),e&&a(x),w(D,e),e&&a(W),e&&a(be),e&&a(Xe),e&&a(z),e&&a(Ze),w(N,e),e&&a(ea),e&&a(O),w(ee),e&&a(aa),e&&a(je),e&&a(sa),w(ae,e),e&&a(ta),e&&a(Ee),e&&a(la),w(se,e),e&&a(na),e&&a(we),e&&a(ra),w(te,e),e&&a(oa),e&&a(T),e&&a(ia),w(le,e),e&&a(pa),e&&a(ye),e&&a(ma),w(ne,e),e&&a(ca),e&&a(B),e&&a(fa),e&&a(L),w(re),e&&a(da),e&&a(Ce),e&&a(ua),w(oe,e),e&&a(ha),e&&a(U),e&&a(ga),w(pe,e),e&&a(_a),e&&a(G),e&&a($a),w(me,e),e&&a(va),e&&a(H),e&&a(ba),w(fe,e),e&&a(ja),e&&a(J),e&&a(Ea),w(de,e),e&&a(wa),e&&a(S),w(ue),e&&a(ya),w(he,e),e&&a(Ca),w(K,e),e&&a(qa),e&&a(qe),e&&a(ka),e&&a(F),e&&a(xa),w($e,e),e&&a(Aa),w(Q,e)}}}const Lt={local:"clasificacin-de-imgenes",sections:[{local:"carga-el-dataset-food101",title:"Carga el dataset Food-101"},{local:"preprocesa",title:"Preprocesa"},{local:"entrena",title:"Entrena"}],title:"Clasificaci\xF3n de im\xE1genes"};function St(R){return Dt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ut extends kt{constructor(f){super();xt(this,f,St,Ot,At,{})}}export{Ut as default,Lt as metadata};
