import{S as Ce,i as De,s as Se,e as o,k as p,w as je,t as s,M as qe,c as r,d as t,m as h,a as l,x as Ne,h as n,b as i,G as a,g as c,y as Oe,L as Ue,q as He,o as Ke,B as ze,v as We}from"../chunks/vendor-hf-doc-builder.js";import{I as Je}from"../chunks/IconCopyLink-hf-doc-builder.js";function Fe(Te){let m,j,v,E,M,_,Q,C,V,q,L,X,N,u,w,Y,P,Z,ee,G,ae,T,te,oe,k,re,b,le,O,y,se,x,ne,ie,U,f,D,de,ce,S,pe,he,R,ue,B,fe,me,H,g,ve,A,Ee,ye,K;return _=new Je({}),{c(){m=o("meta"),j=p(),v=o("h1"),E=o("a"),M=o("span"),je(_.$$.fragment),Q=p(),C=o("span"),V=s("BERTolog\xEDa"),q=p(),L=o("p"),X=s(`Hay un creciente campo de estudio empe\xF1ado en la investigaci\xF3n del funcionamiento interno de los transformers de gran escala como BERT
(que algunos llaman \u201CBERTolog\xEDa\u201D). Algunos buenos ejemplos de este campo son:`),N=p(),u=o("ul"),w=o("li"),Y=s(`BERT Rediscovers the Classical NLP Pipeline por Ian Tenney, Dipanjan Das, Ellie Pavlick:
`),P=o("a"),Z=s("https://arxiv.org/abs/1905.05950"),ee=p(),G=o("li"),ae=s("Are Sixteen Heads Really Better than One? por Paul Michel, Omer Levy, Graham Neubig: "),T=o("a"),te=s("https://arxiv.org/abs/1905.10650"),oe=p(),k=o("li"),re=s(`What Does BERT Look At? An Analysis of BERT\u2019s Attention por Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D.
Manning: `),b=o("a"),le=s("https://arxiv.org/abs/1906.04341"),O=p(),y=o("p"),se=s(`Para asistir al desarrollo de este nuevo campo, hemos incluido algunas features adicionales en los modelos BERT/GPT/GPT-2 para
ayudar a acceder a las representaciones internas, principalmente adaptado de la gran obra de Paul Michel
(`),x=o("a"),ne=s("https://arxiv.org/abs/1905.10650"),ie=s("):"),U=p(),f=o("ul"),D=o("li"),de=s("accediendo a todos los hidden-states de BERT/GPT/GPT-2,"),ce=p(),S=o("li"),pe=s("accediendo a todos los pesos de atenci\xF3n para cada head de BERT/GPT/GPT-2,"),he=p(),R=o("li"),ue=s(`adquiriendo los valores de salida y gradientes de las heads para poder computar la m\xE9trica de importancia de las heads y realizar la poda de heads como se explica
en `),B=o("a"),fe=s("https://arxiv.org/abs/1905.10650"),me=s("."),H=p(),g=o("p"),ve=s("Para ayudarte a entender y usar estas features, hemos a\xF1adido un script espec\xEDfico de ejemplo: "),A=o("a"),Ee=s("bertology.py"),ye=s(` mientras extraes informaci\xF3n y cortas un modelo pre-entrenado en
GLUE.`),this.h()},l(e){const d=qe('[data-svelte="svelte-1phssyn"]',document.head);m=r(d,"META",{name:!0,content:!0}),d.forEach(t),j=h(e),v=r(e,"H1",{class:!0});var z=l(v);E=r(z,"A",{id:!0,class:!0,href:!0});var be=l(E);M=r(be,"SPAN",{});var xe=l(M);Ne(_.$$.fragment,xe),xe.forEach(t),be.forEach(t),Q=h(z),C=r(z,"SPAN",{});var Re=l(C);V=n(Re,"BERTolog\xEDa"),Re.forEach(t),z.forEach(t),q=h(e),L=r(e,"P",{});var Be=l(L);X=n(Be,`Hay un creciente campo de estudio empe\xF1ado en la investigaci\xF3n del funcionamiento interno de los transformers de gran escala como BERT
(que algunos llaman \u201CBERTolog\xEDa\u201D). Algunos buenos ejemplos de este campo son:`),Be.forEach(t),N=h(e),u=r(e,"UL",{});var I=l(u);w=r(I,"LI",{});var ge=l(w);Y=n(ge,`BERT Rediscovers the Classical NLP Pipeline por Ian Tenney, Dipanjan Das, Ellie Pavlick:
`),P=r(ge,"A",{href:!0,rel:!0});var Ae=l(P);Z=n(Ae,"https://arxiv.org/abs/1905.05950"),Ae.forEach(t),ge.forEach(t),ee=h(I),G=r(I,"LI",{});var _e=l(G);ae=n(_e,"Are Sixteen Heads Really Better than One? por Paul Michel, Omer Levy, Graham Neubig: "),T=r(_e,"A",{href:!0,rel:!0});var Le=l(T);te=n(Le,"https://arxiv.org/abs/1905.10650"),Le.forEach(t),_e.forEach(t),oe=h(I),k=r(I,"LI",{});var Pe=l(k);re=n(Pe,`What Does BERT Look At? An Analysis of BERT\u2019s Attention por Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D.
Manning: `),b=r(Pe,"A",{href:!0,rel:!0});var we=l(b);le=n(we,"https://arxiv.org/abs/1906.04341"),we.forEach(t),Pe.forEach(t),I.forEach(t),O=h(e),y=r(e,"P",{});var W=l(y);se=n(W,`Para asistir al desarrollo de este nuevo campo, hemos incluido algunas features adicionales en los modelos BERT/GPT/GPT-2 para
ayudar a acceder a las representaciones internas, principalmente adaptado de la gran obra de Paul Michel
(`),x=r(W,"A",{href:!0,rel:!0});var Ge=l(x);ne=n(Ge,"https://arxiv.org/abs/1905.10650"),Ge.forEach(t),ie=n(W,"):"),W.forEach(t),U=h(e),f=r(e,"UL",{});var $=l(f);D=r($,"LI",{});var ke=l(D);de=n(ke,"accediendo a todos los hidden-states de BERT/GPT/GPT-2,"),ke.forEach(t),ce=h($),S=r($,"LI",{});var Ie=l(S);pe=n(Ie,"accediendo a todos los pesos de atenci\xF3n para cada head de BERT/GPT/GPT-2,"),Ie.forEach(t),he=h($),R=r($,"LI",{});var J=l(R);ue=n(J,`adquiriendo los valores de salida y gradientes de las heads para poder computar la m\xE9trica de importancia de las heads y realizar la poda de heads como se explica
en `),B=r(J,"A",{href:!0,rel:!0});var $e=l(B);fe=n($e,"https://arxiv.org/abs/1905.10650"),$e.forEach(t),me=n(J,"."),J.forEach(t),$.forEach(t),H=h(e),g=r(e,"P",{});var F=l(g);ve=n(F,"Para ayudarte a entender y usar estas features, hemos a\xF1adido un script espec\xEDfico de ejemplo: "),A=r(F,"A",{href:!0,rel:!0});var Me=l(A);Ee=n(Me,"bertology.py"),Me.forEach(t),ye=n(F,` mientras extraes informaci\xF3n y cortas un modelo pre-entrenado en
GLUE.`),F.forEach(t),this.h()},h(){i(m,"name","hf:doc:metadata"),i(m,"content",JSON.stringify(Qe)),i(E,"id","bertologa"),i(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(E,"href","#bertologa"),i(v,"class","relative group"),i(P,"href","https://arxiv.org/abs/1905.05950"),i(P,"rel","nofollow"),i(T,"href","https://arxiv.org/abs/1905.10650"),i(T,"rel","nofollow"),i(b,"href","https://arxiv.org/abs/1906.04341"),i(b,"rel","nofollow"),i(x,"href","https://arxiv.org/abs/1905.10650"),i(x,"rel","nofollow"),i(B,"href","https://arxiv.org/abs/1905.10650"),i(B,"rel","nofollow"),i(A,"href","https://github.com/huggingface/transformers/tree/main/examples/research_projects/bertology/run_bertology.py"),i(A,"rel","nofollow")},m(e,d){a(document.head,m),c(e,j,d),c(e,v,d),a(v,E),a(E,M),Oe(_,M,null),a(v,Q),a(v,C),a(C,V),c(e,q,d),c(e,L,d),a(L,X),c(e,N,d),c(e,u,d),a(u,w),a(w,Y),a(w,P),a(P,Z),a(u,ee),a(u,G),a(G,ae),a(G,T),a(T,te),a(u,oe),a(u,k),a(k,re),a(k,b),a(b,le),c(e,O,d),c(e,y,d),a(y,se),a(y,x),a(x,ne),a(y,ie),c(e,U,d),c(e,f,d),a(f,D),a(D,de),a(f,ce),a(f,S),a(S,pe),a(f,he),a(f,R),a(R,ue),a(R,B),a(B,fe),a(R,me),c(e,H,d),c(e,g,d),a(g,ve),a(g,A),a(A,Ee),a(g,ye),K=!0},p:Ue,i(e){K||(He(_.$$.fragment,e),K=!0)},o(e){Ke(_.$$.fragment,e),K=!1},d(e){t(m),e&&t(j),e&&t(v),ze(_),e&&t(q),e&&t(L),e&&t(N),e&&t(u),e&&t(O),e&&t(y),e&&t(U),e&&t(f),e&&t(H),e&&t(g)}}}const Qe={local:"bertologa",title:"BERTolog\xEDa"};function Ve(Te){return We(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ze extends Ce{constructor(m){super();De(this,m,Ve,Fe,Se,{})}}export{Ze as default,Qe as metadata};
