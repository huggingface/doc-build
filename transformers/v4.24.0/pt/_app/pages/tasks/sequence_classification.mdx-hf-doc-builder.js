import{S as Ba,i as Ra,s as La,e as i,k as $,w as q,t as r,M as Wa,c as p,d as a,m as g,a as c,x as C,h as n,b as w,G as t,g as m,y as T,q as D,o as y,B as P,v as Ua,L as Na}from"../../chunks/vendor-hf-doc-builder.js";import{T as ut}from"../../chunks/Tip-hf-doc-builder.js";import{Y as Va}from"../../chunks/Youtube-hf-doc-builder.js";import{I as St}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as le}from"../../chunks/CodeBlock-hf-doc-builder.js";import{F as Ia,M as Ot}from"../../chunks/Markdown-hf-doc-builder.js";function Ga(S){let s,f,o,u,b;return{c(){s=i("p"),f=r("Consulte a "),o=i("a"),u=r("p\xE1gina de tarefas de classifica\xE7\xE3o de texto"),b=r(" para obter mais informa\xE7\xF5es sobre outras formas de classifica\xE7\xE3o de texto e seus modelos, conjuntos de dados e m\xE9tricas associados."),this.h()},l(_){s=p(_,"P",{});var v=c(s);f=n(v,"Consulte a "),o=p(v,"A",{href:!0,rel:!0});var z=c(o);u=n(z,"p\xE1gina de tarefas de classifica\xE7\xE3o de texto"),z.forEach(a),b=n(v," para obter mais informa\xE7\xF5es sobre outras formas de classifica\xE7\xE3o de texto e seus modelos, conjuntos de dados e m\xE9tricas associados."),v.forEach(a),this.h()},h(){w(o,"href","https://huggingface.co/tasks/text-classification"),w(o,"rel","nofollow")},m(_,v){m(_,s,v),t(s,f),t(s,o),t(o,u),t(s,b)},d(_){_&&a(s)}}}function Ha(S){let s,f;return s=new le({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`}}),{c(){q(s.$$.fragment)},l(o){C(s.$$.fragment,o)},m(o,u){T(s,o,u),f=!0},p:Na,i(o){f||(D(s.$$.fragment,o),f=!0)},o(o){y(s.$$.fragment,o),f=!1},d(o){P(s,o)}}}function Ka(S){let s,f;return s=new Ot({props:{$$slots:{default:[Ha]},$$scope:{ctx:S}}}),{c(){q(s.$$.fragment)},l(o){C(s.$$.fragment,o)},m(o,u){T(s,o,u),f=!0},p(o,u){const b={};u&2&&(b.$$scope={dirty:u,ctx:o}),s.$set(b)},i(o){f||(D(s.$$.fragment,o),f=!0)},o(o){y(s.$$.fragment,o),f=!1},d(o){P(s,o)}}}function Ja(S){let s,f;return s=new le({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){q(s.$$.fragment)},l(o){C(s.$$.fragment,o)},m(o,u){T(s,o,u),f=!0},p:Na,i(o){f||(D(s.$$.fragment,o),f=!0)},o(o){y(s.$$.fragment,o),f=!1},d(o){P(s,o)}}}function Ya(S){let s,f;return s=new Ot({props:{$$slots:{default:[Ja]},$$scope:{ctx:S}}}),{c(){q(s.$$.fragment)},l(o){C(s.$$.fragment,o)},m(o,u){T(s,o,u),f=!0},p(o,u){const b={};u&2&&(b.$$scope={dirty:u,ctx:o}),s.$set(b)},i(o){f||(D(s.$$.fragment,o),f=!0)},o(o){y(s.$$.fragment,o),f=!1},d(o){P(s,o)}}}function Qa(S){let s,f,o,u,b,_,v,z;return{c(){s=i("p"),f=r("Se voc\xEA n\xE3o estiver familiarizado com o fine-tuning de um modelo com o "),o=i("code"),u=r("Trainer"),b=r(", d\xEA uma olhada no tutorial b\xE1sico "),_=i("a"),v=r("aqui"),z=r("!"),this.h()},l(x){s=p(x,"P",{});var j=c(s);f=n(j,"Se voc\xEA n\xE3o estiver familiarizado com o fine-tuning de um modelo com o "),o=p(j,"CODE",{});var A=c(o);u=n(A,"Trainer"),A.forEach(a),b=n(j,", d\xEA uma olhada no tutorial b\xE1sico "),_=p(j,"A",{href:!0});var I=c(_);v=n(I,"aqui"),I.forEach(a),z=n(j,"!"),j.forEach(a),this.h()},h(){w(_,"href","../training#finetune-with-trainer")},m(x,j){m(x,s,j),t(s,f),t(s,o),t(o,u),t(s,b),t(s,_),t(_,v),t(s,z)},d(x){x&&a(s)}}}function Xa(S){let s,f,o,u,b,_,v,z;return{c(){s=i("p"),f=r("O "),o=i("code"),u=r("Trainer"),b=r(" aplicar\xE1 o preenchimento din\xE2mico por padr\xE3o quando voc\xEA definir o argumento "),_=i("code"),v=r("tokenizer"),z=r(" dele. Nesse caso, voc\xEA n\xE3o precisa especificar um data collator explicitamente.")},l(x){s=p(x,"P",{});var j=c(s);f=n(j,"O "),o=p(j,"CODE",{});var A=c(o);u=n(A,"Trainer"),A.forEach(a),b=n(j," aplicar\xE1 o preenchimento din\xE2mico por padr\xE3o quando voc\xEA definir o argumento "),_=p(j,"CODE",{});var I=c(_);v=n(I,"tokenizer"),I.forEach(a),z=n(j," dele. Nesse caso, voc\xEA n\xE3o precisa especificar um data collator explicitamente."),j.forEach(a)},m(x,j){m(x,s,j),t(s,f),t(s,o),t(o,u),t(s,b),t(s,_),t(_,v),t(s,z)},d(x){x&&a(s)}}}function Za(S){let s,f,o,u,b,_,v,z,x,j,A,I,Y,M,H,O,se,U,be,fe,F,ue,V,he,G,_e,N,K,R,Q,X,oe,L,ee,W,$e;return v=new le({props:{code:`from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),x=new ut({props:{$$slots:{default:[Qa]},$$scope:{ctx:S}}}),L=new le({props:{code:`training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=5,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_imdb["train"],
    eval_dataset=tokenized_imdb["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">5</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=tokenized_imdb[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),W=new ut({props:{$$slots:{default:[Xa]},$$scope:{ctx:S}}}),{c(){s=i("p"),f=r("Carregue o DistilBERT com "),o=i("code"),u=r("AutoModelForSequenceClassification"),b=r(" junto com o n\xFAmero de r\xF3tulos esperados:"),_=$(),q(v.$$.fragment),z=$(),q(x.$$.fragment),j=$(),A=i("p"),I=r("Nesse ponto, restam apenas tr\xEAs passos:"),Y=$(),M=i("ol"),H=i("li"),O=r("Definir seus hiperpar\xE2metros de treinamento em "),se=i("code"),U=r("TrainingArguments"),be=r("."),fe=$(),F=i("li"),ue=r("Passar os argumentos de treinamento para o "),V=i("code"),he=r("Trainer"),G=r(" junto com o modelo, conjunto de dados, tokenizador e o data collator."),_e=$(),N=i("li"),K=r("Chamar a fun\xE7\xE3o "),R=i("code"),Q=r("train()"),X=r(" para executar o fine-tuning do seu modelo."),oe=$(),q(L.$$.fragment),ee=$(),q(W.$$.fragment)},l(d){s=p(d,"P",{});var k=c(s);f=n(k,"Carregue o DistilBERT com "),o=p(k,"CODE",{});var te=c(o);u=n(te,"AutoModelForSequenceClassification"),te.forEach(a),b=n(k," junto com o n\xFAmero de r\xF3tulos esperados:"),k.forEach(a),_=g(d),C(v.$$.fragment,d),z=g(d),C(x.$$.fragment,d),j=g(d),A=p(d,"P",{});var B=c(A);I=n(B,"Nesse ponto, restam apenas tr\xEAs passos:"),B.forEach(a),Y=g(d),M=p(d,"OL",{});var re=c(M);H=p(re,"LI",{});var J=c(H);O=n(J,"Definir seus hiperpar\xE2metros de treinamento em "),se=p(J,"CODE",{});var Z=c(se);U=n(Z,"TrainingArguments"),Z.forEach(a),be=n(J,"."),J.forEach(a),fe=g(re),F=p(re,"LI",{});var ie=c(F);ue=n(ie,"Passar os argumentos de treinamento para o "),V=p(ie,"CODE",{});var pe=c(V);he=n(pe,"Trainer"),pe.forEach(a),G=n(ie," junto com o modelo, conjunto de dados, tokenizador e o data collator."),ie.forEach(a),_e=g(re),N=p(re,"LI",{});var ce=c(N);K=n(ce,"Chamar a fun\xE7\xE3o "),R=p(ce,"CODE",{});var ae=c(R);Q=n(ae,"train()"),ae.forEach(a),X=n(ce," para executar o fine-tuning do seu modelo."),ce.forEach(a),re.forEach(a),oe=g(d),C(L.$$.fragment,d),ee=g(d),C(W.$$.fragment,d)},m(d,k){m(d,s,k),t(s,f),t(s,o),t(o,u),t(s,b),m(d,_,k),T(v,d,k),m(d,z,k),T(x,d,k),m(d,j,k),m(d,A,k),t(A,I),m(d,Y,k),m(d,M,k),t(M,H),t(H,O),t(H,se),t(se,U),t(H,be),t(M,fe),t(M,F),t(F,ue),t(F,V),t(V,he),t(F,G),t(M,_e),t(M,N),t(N,K),t(N,R),t(R,Q),t(N,X),m(d,oe,k),T(L,d,k),m(d,ee,k),T(W,d,k),$e=!0},p(d,k){const te={};k&2&&(te.$$scope={dirty:k,ctx:d}),x.$set(te);const B={};k&2&&(B.$$scope={dirty:k,ctx:d}),W.$set(B)},i(d){$e||(D(v.$$.fragment,d),D(x.$$.fragment,d),D(L.$$.fragment,d),D(W.$$.fragment,d),$e=!0)},o(d){y(v.$$.fragment,d),y(x.$$.fragment,d),y(L.$$.fragment,d),y(W.$$.fragment,d),$e=!1},d(d){d&&a(s),d&&a(_),P(v,d),d&&a(z),P(x,d),d&&a(j),d&&a(A),d&&a(Y),d&&a(M),d&&a(oe),P(L,d),d&&a(ee),P(W,d)}}}function es(S){let s,f;return s=new Ot({props:{$$slots:{default:[Za]},$$scope:{ctx:S}}}),{c(){q(s.$$.fragment)},l(o){C(s.$$.fragment,o)},m(o,u){T(s,o,u),f=!0},p(o,u){const b={};u&2&&(b.$$scope={dirty:u,ctx:o}),s.$set(b)},i(o){f||(D(s.$$.fragment,o),f=!0)},o(o){y(s.$$.fragment,o),f=!1},d(o){P(s,o)}}}function ts(S){let s,f,o,u,b;return{c(){s=i("p"),f=r("Se voc\xEA n\xE3o estiver familiarizado com o fine-tuning de um modelo com o Keras, d\xEA uma olhada no tutorial b\xE1sico "),o=i("a"),u=r("aqui"),b=r("!"),this.h()},l(_){s=p(_,"P",{});var v=c(s);f=n(v,"Se voc\xEA n\xE3o estiver familiarizado com o fine-tuning de um modelo com o Keras, d\xEA uma olhada no tutorial b\xE1sico "),o=p(v,"A",{href:!0});var z=c(o);u=n(z,"aqui"),z.forEach(a),b=n(v,"!"),v.forEach(a),this.h()},h(){w(o,"href","training#finetune-with-keras")},m(_,v){m(_,s,v),t(s,f),t(s,o),t(o,u),t(s,b)},d(_){_&&a(s)}}}function as(S){let s,f,o,u,b,_,v,z,x,j,A,I,Y,M,H,O,se,U,be,fe,F,ue,V,he,G,_e,N,K,R,Q,X,oe,L,ee,W,$e,d,k,te,B,re,J,Z,ie,pe,ce,ae,me;return M=new le({props:{code:`tf_train_set = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_set = tokenized_imdb["test"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_validation_set = tokenized_imdb[<span class="hljs-string">&quot;test&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`}}),O=new ut({props:{$$slots:{default:[ts]},$$scope:{ctx:S}}}),F=new le({props:{code:`from transformers import create_optimizer
import tensorflow as tf

batch_size = 16
num_epochs = 5
batches_per_epoch = len(tokenized_imdb["train"]) // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = <span class="hljs-number">16</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_epochs = <span class="hljs-number">5</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>batches_per_epoch = <span class="hljs-built_in">len</span>(tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
<span class="hljs-meta">&gt;&gt;&gt; </span>total_train_steps = <span class="hljs-built_in">int</span>(batches_per_epoch * num_epochs)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer, schedule = create_optimizer(init_lr=<span class="hljs-number">2e-5</span>, num_warmup_steps=<span class="hljs-number">0</span>, num_train_steps=total_train_steps)`}}),R=new le({props:{code:`from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),k=new le({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),ae=new le({props:{code:"model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=<span class="hljs-number">3</span>)'}}),{c(){s=i("p"),f=r("Para executar o fine-tuning de um modelo no TensorFlow, comece convertendo seu conjunto de dados para o formato "),o=i("code"),u=r("tf.data.Dataset"),b=r(" com "),_=i("a"),v=i("code"),z=r("to_tf_dataset"),x=r(". Nessa execu\xE7\xE3o voc\xEA dever\xE1 especificar as entradas e r\xF3tulos (no par\xE2metro "),j=i("code"),A=r("columns"),I=r("), se deseja embaralhar o conjunto de dados, o tamanho do batch e o data collator:"),Y=$(),q(M.$$.fragment),H=$(),q(O.$$.fragment),se=$(),U=i("p"),be=r("Configure o otimizador e alguns hiperpar\xE2metros de treinamento:"),fe=$(),q(F.$$.fragment),ue=$(),V=i("p"),he=r("Carregue o DistilBERT com "),G=i("code"),_e=r("TFAutoModelForSequenceClassification"),N=r(" junto com o n\xFAmero de r\xF3tulos esperados:"),K=$(),q(R.$$.fragment),Q=$(),X=i("p"),oe=r("Configure o modelo para treinamento com o m\xE9todo "),L=i("a"),ee=i("code"),W=r("compile"),$e=r(":"),d=$(),q(k.$$.fragment),te=$(),B=i("p"),re=r("Chame o m\xE9todo "),J=i("a"),Z=i("code"),ie=r("fit"),pe=r(" para executar o fine-tuning do modelo:"),ce=$(),q(ae.$$.fragment),this.h()},l(l){s=p(l,"P",{});var E=c(s);f=n(E,"Para executar o fine-tuning de um modelo no TensorFlow, comece convertendo seu conjunto de dados para o formato "),o=p(E,"CODE",{});var Ee=c(o);u=n(Ee,"tf.data.Dataset"),Ee.forEach(a),b=n(E," com "),_=p(E,"A",{href:!0,rel:!0});var Ue=c(_);v=p(Ue,"CODE",{});var Ve=c(v);z=n(Ve,"to_tf_dataset"),Ve.forEach(a),Ue.forEach(a),x=n(E,". Nessa execu\xE7\xE3o voc\xEA dever\xE1 especificar as entradas e r\xF3tulos (no par\xE2metro "),j=p(E,"CODE",{});var de=c(j);A=n(de,"columns"),de.forEach(a),I=n(E,"), se deseja embaralhar o conjunto de dados, o tamanho do batch e o data collator:"),E.forEach(a),Y=g(l),C(M.$$.fragment,l),H=g(l),C(O.$$.fragment,l),se=g(l),U=p(l,"P",{});var ze=c(U);be=n(ze,"Configure o otimizador e alguns hiperpar\xE2metros de treinamento:"),ze.forEach(a),fe=g(l),C(F.$$.fragment,l),ue=g(l),V=p(l,"P",{});var qe=c(V);he=n(qe,"Carregue o DistilBERT com "),G=p(qe,"CODE",{});var Ge=c(G);_e=n(Ge,"TFAutoModelForSequenceClassification"),Ge.forEach(a),N=n(qe," junto com o n\xFAmero de r\xF3tulos esperados:"),qe.forEach(a),K=g(l),C(R.$$.fragment,l),Q=g(l),X=p(l,"P",{});var ve=c(X);oe=n(ve,"Configure o modelo para treinamento com o m\xE9todo "),L=p(ve,"A",{href:!0,rel:!0});var He=c(L);ee=p(He,"CODE",{});var Ke=c(ee);W=n(Ke,"compile"),Ke.forEach(a),He.forEach(a),$e=n(ve,":"),ve.forEach(a),d=g(l),C(k.$$.fragment,l),te=g(l),B=p(l,"P",{});var je=c(B);re=n(je,"Chame o m\xE9todo "),J=p(je,"A",{href:!0,rel:!0});var Je=c(J);Z=p(Je,"CODE",{});var Ye=c(Z);ie=n(Ye,"fit"),Ye.forEach(a),Je.forEach(a),pe=n(je," para executar o fine-tuning do modelo:"),je.forEach(a),ce=g(l),C(ae.$$.fragment,l),this.h()},h(){w(_,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.to_tf_dataset"),w(_,"rel","nofollow"),w(L,"href","https://keras.io/api/models/model_training_apis/#compile-method"),w(L,"rel","nofollow"),w(J,"href","https://keras.io/api/models/model_training_apis/#fit-method"),w(J,"rel","nofollow")},m(l,E){m(l,s,E),t(s,f),t(s,o),t(o,u),t(s,b),t(s,_),t(_,v),t(v,z),t(s,x),t(s,j),t(j,A),t(s,I),m(l,Y,E),T(M,l,E),m(l,H,E),T(O,l,E),m(l,se,E),m(l,U,E),t(U,be),m(l,fe,E),T(F,l,E),m(l,ue,E),m(l,V,E),t(V,he),t(V,G),t(G,_e),t(V,N),m(l,K,E),T(R,l,E),m(l,Q,E),m(l,X,E),t(X,oe),t(X,L),t(L,ee),t(ee,W),t(X,$e),m(l,d,E),T(k,l,E),m(l,te,E),m(l,B,E),t(B,re),t(B,J),t(J,Z),t(Z,ie),t(B,pe),m(l,ce,E),T(ae,l,E),me=!0},p(l,E){const Ee={};E&2&&(Ee.$$scope={dirty:E,ctx:l}),O.$set(Ee)},i(l){me||(D(M.$$.fragment,l),D(O.$$.fragment,l),D(F.$$.fragment,l),D(R.$$.fragment,l),D(k.$$.fragment,l),D(ae.$$.fragment,l),me=!0)},o(l){y(M.$$.fragment,l),y(O.$$.fragment,l),y(F.$$.fragment,l),y(R.$$.fragment,l),y(k.$$.fragment,l),y(ae.$$.fragment,l),me=!1},d(l){l&&a(s),l&&a(Y),P(M,l),l&&a(H),P(O,l),l&&a(se),l&&a(U),l&&a(fe),P(F,l),l&&a(ue),l&&a(V),l&&a(K),P(R,l),l&&a(Q),l&&a(X),l&&a(d),P(k,l),l&&a(te),l&&a(B),l&&a(ce),P(ae,l)}}}function ss(S){let s,f;return s=new Ot({props:{$$slots:{default:[as]},$$scope:{ctx:S}}}),{c(){q(s.$$.fragment)},l(o){C(s.$$.fragment,o)},m(o,u){T(s,o,u),f=!0},p(o,u){const b={};u&2&&(b.$$scope={dirty:u,ctx:o}),s.$set(b)},i(o){f||(D(s.$$.fragment,o),f=!0)},o(o){y(s.$$.fragment,o),f=!1},d(o){P(s,o)}}}function os(S){let s,f,o,u,b,_,v,z;return{c(){s=i("p"),f=r("Para obter um exemplo mais aprofundado de como executar o fine-tuning de um modelo para classifica\xE7\xE3o de texto, d\xEA uma olhada nesse "),o=i("a"),u=r("notebook utilizando PyTorch"),b=r(" ou nesse "),_=i("a"),v=r("notebook utilizando TensorFlow"),z=r("."),this.h()},l(x){s=p(x,"P",{});var j=c(s);f=n(j,"Para obter um exemplo mais aprofundado de como executar o fine-tuning de um modelo para classifica\xE7\xE3o de texto, d\xEA uma olhada nesse "),o=p(j,"A",{href:!0,rel:!0});var A=c(o);u=n(A,"notebook utilizando PyTorch"),A.forEach(a),b=n(j," ou nesse "),_=p(j,"A",{href:!0,rel:!0});var I=c(_);v=n(I,"notebook utilizando TensorFlow"),I.forEach(a),z=n(j,"."),j.forEach(a),this.h()},h(){w(o,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb"),w(o,"rel","nofollow"),w(_,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb"),w(_,"rel","nofollow")},m(x,j){m(x,s,j),t(s,f),t(s,o),t(o,u),t(s,b),t(s,_),t(_,v),t(s,z)},d(x){x&&a(s)}}}function rs(S){let s,f,o,u,b,_,v,z,x,j,A,I,Y,M,H,O,se,U,be,fe,F,ue,V,he,G,_e,N,K,R,Q,X,oe,L,ee,W,$e,d,k,te,B,re,J,Z,ie,pe,ce,ae,me,l,E,Ee,Ue,Ve,de,ze,qe,Ge,ve,He,Ke,je,Je,Ye,ht,we,Ce,Xe,Fe,Ft,Ze,Mt,_t,Te,It,et,Nt,Bt,$t,Me,gt,De,Rt,tt,Lt,Wt,bt,Ie,vt,ge,Ut,Ne,at,Vt,Gt,st,Ht,Kt,ot,Jt,Yt,jt,Be,kt,ne,Qt,rt,Xt,Zt,nt,ea,ta,lt,aa,sa,it,oa,ra,Et,ye,wt,xe,Pe,pt,Re,na,ct,la,xt,Ae,zt,Se,qt;return _=new St({}),A=new Va({props:{id:"leNG9fN9FQU"}}),G=new ut({props:{$$slots:{default:[Ga]},$$scope:{ctx:S}}}),Q=new St({}),k=new le({props:{code:`from datasets import load_dataset

imdb = load_dataset("imdb")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>imdb = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>)`}}),Z=new le({props:{code:'imdb["test"][0]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>imdb[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-number">0</span>]
{
    <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn&#x27;t match the background, and painfully one-dimensional characters cannot be overcome with a &#x27;sci-fi&#x27; setting. (I&#x27;m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It&#x27;s not. It&#x27;s clich\xE9d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It&#x27;s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it&#x27;s rubbish as they have to always say \\&quot;Gene Roddenberry&#x27;s Earth...\\&quot; otherwise people would not continue watching. Roddenberry&#x27;s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.&quot;</span>,
}`}}),Fe=new St({}),Me=new le({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),Ie=new le({props:{code:`def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>], truncation=<span class="hljs-literal">True</span>)`}}),Be=new le({props:{code:"tokenized_imdb = imdb.map(preprocess_function, batched=True)",highlighted:'tokenized_imdb = imdb.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)'}}),ye=new Ia({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ya],pytorch:[Ka]},$$scope:{ctx:S}}}),Re=new St({}),Ae=new Ia({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[ss],pytorch:[es]},$$scope:{ctx:S}}}),Se=new ut({props:{$$slots:{default:[os]},$$scope:{ctx:S}}}),{c(){s=i("meta"),f=$(),o=i("h1"),u=i("a"),b=i("span"),q(_.$$.fragment),v=$(),z=i("span"),x=r("Classifica\xE7\xE3o de texto"),j=$(),q(A.$$.fragment),I=$(),Y=i("p"),M=r("A classifica\xE7\xE3o de texto \xE9 uma tarefa comum de NLP que atribui um r\xF3tulo ou classe a um texto. Existem muitas aplica\xE7\xF5es pr\xE1ticas de classifica\xE7\xE3o de texto amplamente utilizadas em produ\xE7\xE3o por algumas das maiores empresas da atualidade. Uma das formas mais populares de classifica\xE7\xE3o de texto \xE9 a an\xE1lise de sentimento, que atribui um r\xF3tulo como positivo, negativo ou neutro a um texto."),H=$(),O=i("p"),se=r("Este guia mostrar\xE1 como realizar o fine-tuning do "),U=i("a"),be=r("DistilBERT"),fe=r(" no conjunto de dados "),F=i("a"),ue=r("IMDb"),V=r(" para determinar se a cr\xEDtica de filme \xE9 positiva ou negativa."),he=$(),q(G.$$.fragment),_e=$(),N=i("h2"),K=i("a"),R=i("span"),q(Q.$$.fragment),X=$(),oe=i("span"),L=r("Carregue o conjunto de dados IMDb"),ee=$(),W=i("p"),$e=r("Carregue o conjunto de dados IMDb utilizando a biblioteca \u{1F917} Datasets:"),d=$(),q(k.$$.fragment),te=$(),B=i("p"),re=r("Em seguida, d\xEA uma olhada em um exemplo:"),J=$(),q(Z.$$.fragment),ie=$(),pe=i("p"),ce=r("Existem dois campos neste dataset:"),ae=$(),me=i("ul"),l=i("li"),E=i("code"),Ee=r("text"),Ue=r(": uma string contendo o texto da cr\xEDtica do filme."),Ve=$(),de=i("li"),ze=i("code"),qe=r("label"),Ge=r(": um valor que pode ser "),ve=i("code"),He=r("0"),Ke=r(" para uma cr\xEDtica negativa ou "),je=i("code"),Je=r("1"),Ye=r(" para uma cr\xEDtica positiva."),ht=$(),we=i("h2"),Ce=i("a"),Xe=i("span"),q(Fe.$$.fragment),Ft=$(),Ze=i("span"),Mt=r("Pr\xE9-processamento dos dados"),_t=$(),Te=i("p"),It=r("Carregue o tokenizador do DistilBERT para processar o campo "),et=i("code"),Nt=r("text"),Bt=r(":"),$t=$(),q(Me.$$.fragment),gt=$(),De=i("p"),Rt=r("Crie uma fun\xE7\xE3o de pr\xE9-processamento para tokenizar o campo "),tt=i("code"),Lt=r("text"),Wt=r(" e truncar as sequ\xEAncias para que n\xE3o sejam maiores que o comprimento m\xE1ximo de entrada do DistilBERT:"),bt=$(),q(Ie.$$.fragment),vt=$(),ge=i("p"),Ut=r("Use a fun\xE7\xE3o "),Ne=i("a"),at=i("code"),Vt=r("map"),Gt=r(" do \u{1F917} Datasets para aplicar a fun\xE7\xE3o de pr\xE9-processamento em todo o conjunto de dados. Voc\xEA pode acelerar a fun\xE7\xE3o "),st=i("code"),Ht=r("map"),Kt=r(" definindo "),ot=i("code"),Jt=r("batched=True"),Yt=r(" para processar v\xE1rios elementos do conjunto de dados de uma s\xF3 vez:"),jt=$(),q(Be.$$.fragment),kt=$(),ne=i("p"),Qt=r("Use o "),rt=i("code"),Xt=r("DataCollatorWithPadding"),Zt=r(" para criar um batch de exemplos. Ele tamb\xE9m "),nt=i("em"),ea=r("preencher\xE1 dinamicamente"),ta=r(" seu texto at\xE9 o comprimento do elemento mais longo em seu batch, para que os exemplos do batch tenham um comprimento uniforme. Embora seja poss\xEDvel preencher seu texto com a fun\xE7\xE3o "),lt=i("code"),aa=r("tokenizer"),sa=r(" definindo "),it=i("code"),oa=r("padding=True"),ra=r(", o preenchimento din\xE2mico utilizando um data collator \xE9 mais eficiente."),Et=$(),q(ye.$$.fragment),wt=$(),xe=i("h2"),Pe=i("a"),pt=i("span"),q(Re.$$.fragment),na=$(),ct=i("span"),la=r("Train"),xt=$(),q(Ae.$$.fragment),zt=$(),q(Se.$$.fragment),this.h()},l(e){const h=Wa('[data-svelte="svelte-1phssyn"]',document.head);s=p(h,"META",{name:!0,content:!0}),h.forEach(a),f=g(e),o=p(e,"H1",{class:!0});var Le=c(o);u=p(Le,"A",{id:!0,class:!0,href:!0});var mt=c(u);b=p(mt,"SPAN",{});var dt=c(b);C(_.$$.fragment,dt),dt.forEach(a),mt.forEach(a),v=g(Le),z=p(Le,"SPAN",{});var ft=c(z);x=n(ft,"Classifica\xE7\xE3o de texto"),ft.forEach(a),Le.forEach(a),j=g(e),C(A.$$.fragment,e),I=g(e),Y=p(e,"P",{});var pa=c(Y);M=n(pa,"A classifica\xE7\xE3o de texto \xE9 uma tarefa comum de NLP que atribui um r\xF3tulo ou classe a um texto. Existem muitas aplica\xE7\xF5es pr\xE1ticas de classifica\xE7\xE3o de texto amplamente utilizadas em produ\xE7\xE3o por algumas das maiores empresas da atualidade. Uma das formas mais populares de classifica\xE7\xE3o de texto \xE9 a an\xE1lise de sentimento, que atribui um r\xF3tulo como positivo, negativo ou neutro a um texto."),pa.forEach(a),H=g(e),O=p(e,"P",{});var Qe=c(O);se=n(Qe,"Este guia mostrar\xE1 como realizar o fine-tuning do "),U=p(Qe,"A",{href:!0,rel:!0});var ca=c(U);be=n(ca,"DistilBERT"),ca.forEach(a),fe=n(Qe," no conjunto de dados "),F=p(Qe,"A",{href:!0,rel:!0});var ma=c(F);ue=n(ma,"IMDb"),ma.forEach(a),V=n(Qe," para determinar se a cr\xEDtica de filme \xE9 positiva ou negativa."),Qe.forEach(a),he=g(e),C(G.$$.fragment,e),_e=g(e),N=p(e,"H2",{class:!0});var Ct=c(N);K=p(Ct,"A",{id:!0,class:!0,href:!0});var da=c(K);R=p(da,"SPAN",{});var fa=c(R);C(Q.$$.fragment,fa),fa.forEach(a),da.forEach(a),X=g(Ct),oe=p(Ct,"SPAN",{});var ua=c(oe);L=n(ua,"Carregue o conjunto de dados IMDb"),ua.forEach(a),Ct.forEach(a),ee=g(e),W=p(e,"P",{});var ha=c(W);$e=n(ha,"Carregue o conjunto de dados IMDb utilizando a biblioteca \u{1F917} Datasets:"),ha.forEach(a),d=g(e),C(k.$$.fragment,e),te=g(e),B=p(e,"P",{});var _a=c(B);re=n(_a,"Em seguida, d\xEA uma olhada em um exemplo:"),_a.forEach(a),J=g(e),C(Z.$$.fragment,e),ie=g(e),pe=p(e,"P",{});var $a=c(pe);ce=n($a,"Existem dois campos neste dataset:"),$a.forEach(a),ae=g(e),me=p(e,"UL",{});var Tt=c(me);l=p(Tt,"LI",{});var ia=c(l);E=p(ia,"CODE",{});var ga=c(E);Ee=n(ga,"text"),ga.forEach(a),Ue=n(ia,": uma string contendo o texto da cr\xEDtica do filme."),ia.forEach(a),Ve=g(Tt),de=p(Tt,"LI",{});var We=c(de);ze=p(We,"CODE",{});var ba=c(ze);qe=n(ba,"label"),ba.forEach(a),Ge=n(We,": um valor que pode ser "),ve=p(We,"CODE",{});var va=c(ve);He=n(va,"0"),va.forEach(a),Ke=n(We," para uma cr\xEDtica negativa ou "),je=p(We,"CODE",{});var ja=c(je);Je=n(ja,"1"),ja.forEach(a),Ye=n(We," para uma cr\xEDtica positiva."),We.forEach(a),Tt.forEach(a),ht=g(e),we=p(e,"H2",{class:!0});var Dt=c(we);Ce=p(Dt,"A",{id:!0,class:!0,href:!0});var ka=c(Ce);Xe=p(ka,"SPAN",{});var Ea=c(Xe);C(Fe.$$.fragment,Ea),Ea.forEach(a),ka.forEach(a),Ft=g(Dt),Ze=p(Dt,"SPAN",{});var wa=c(Ze);Mt=n(wa,"Pr\xE9-processamento dos dados"),wa.forEach(a),Dt.forEach(a),_t=g(e),Te=p(e,"P",{});var yt=c(Te);It=n(yt,"Carregue o tokenizador do DistilBERT para processar o campo "),et=p(yt,"CODE",{});var xa=c(et);Nt=n(xa,"text"),xa.forEach(a),Bt=n(yt,":"),yt.forEach(a),$t=g(e),C(Me.$$.fragment,e),gt=g(e),De=p(e,"P",{});var Pt=c(De);Rt=n(Pt,"Crie uma fun\xE7\xE3o de pr\xE9-processamento para tokenizar o campo "),tt=p(Pt,"CODE",{});var za=c(tt);Lt=n(za,"text"),za.forEach(a),Wt=n(Pt," e truncar as sequ\xEAncias para que n\xE3o sejam maiores que o comprimento m\xE1ximo de entrada do DistilBERT:"),Pt.forEach(a),bt=g(e),C(Ie.$$.fragment,e),vt=g(e),ge=p(e,"P",{});var Oe=c(ge);Ut=n(Oe,"Use a fun\xE7\xE3o "),Ne=p(Oe,"A",{href:!0,rel:!0});var qa=c(Ne);at=p(qa,"CODE",{});var Ca=c(at);Vt=n(Ca,"map"),Ca.forEach(a),qa.forEach(a),Gt=n(Oe," do \u{1F917} Datasets para aplicar a fun\xE7\xE3o de pr\xE9-processamento em todo o conjunto de dados. Voc\xEA pode acelerar a fun\xE7\xE3o "),st=p(Oe,"CODE",{});var Ta=c(st);Ht=n(Ta,"map"),Ta.forEach(a),Kt=n(Oe," definindo "),ot=p(Oe,"CODE",{});var Da=c(ot);Jt=n(Da,"batched=True"),Da.forEach(a),Yt=n(Oe," para processar v\xE1rios elementos do conjunto de dados de uma s\xF3 vez:"),Oe.forEach(a),jt=g(e),C(Be.$$.fragment,e),kt=g(e),ne=p(e,"P",{});var ke=c(ne);Qt=n(ke,"Use o "),rt=p(ke,"CODE",{});var ya=c(rt);Xt=n(ya,"DataCollatorWithPadding"),ya.forEach(a),Zt=n(ke," para criar um batch de exemplos. Ele tamb\xE9m "),nt=p(ke,"EM",{});var Pa=c(nt);ea=n(Pa,"preencher\xE1 dinamicamente"),Pa.forEach(a),ta=n(ke," seu texto at\xE9 o comprimento do elemento mais longo em seu batch, para que os exemplos do batch tenham um comprimento uniforme. Embora seja poss\xEDvel preencher seu texto com a fun\xE7\xE3o "),lt=p(ke,"CODE",{});var Aa=c(lt);aa=n(Aa,"tokenizer"),Aa.forEach(a),sa=n(ke," definindo "),it=p(ke,"CODE",{});var Sa=c(it);oa=n(Sa,"padding=True"),Sa.forEach(a),ra=n(ke,", o preenchimento din\xE2mico utilizando um data collator \xE9 mais eficiente."),ke.forEach(a),Et=g(e),C(ye.$$.fragment,e),wt=g(e),xe=p(e,"H2",{class:!0});var At=c(xe);Pe=p(At,"A",{id:!0,class:!0,href:!0});var Oa=c(Pe);pt=p(Oa,"SPAN",{});var Fa=c(pt);C(Re.$$.fragment,Fa),Fa.forEach(a),Oa.forEach(a),na=g(At),ct=p(At,"SPAN",{});var Ma=c(ct);la=n(Ma,"Train"),Ma.forEach(a),At.forEach(a),xt=g(e),C(Ae.$$.fragment,e),zt=g(e),C(Se.$$.fragment,e),this.h()},h(){w(s,"name","hf:doc:metadata"),w(s,"content",JSON.stringify(ns)),w(u,"id","classificao-de-texto"),w(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(u,"href","#classificao-de-texto"),w(o,"class","relative group"),w(U,"href","https://huggingface.co/distilbert-base-uncased"),w(U,"rel","nofollow"),w(F,"href","https://huggingface.co/datasets/imdb"),w(F,"rel","nofollow"),w(K,"id","carregue-o-conjunto-de-dados-imdb"),w(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(K,"href","#carregue-o-conjunto-de-dados-imdb"),w(N,"class","relative group"),w(Ce,"id","prprocessamento-dos-dados"),w(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(Ce,"href","#prprocessamento-dos-dados"),w(we,"class","relative group"),w(Ne,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map"),w(Ne,"rel","nofollow"),w(Pe,"id","train"),w(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(Pe,"href","#train"),w(xe,"class","relative group")},m(e,h){t(document.head,s),m(e,f,h),m(e,o,h),t(o,u),t(u,b),T(_,b,null),t(o,v),t(o,z),t(z,x),m(e,j,h),T(A,e,h),m(e,I,h),m(e,Y,h),t(Y,M),m(e,H,h),m(e,O,h),t(O,se),t(O,U),t(U,be),t(O,fe),t(O,F),t(F,ue),t(O,V),m(e,he,h),T(G,e,h),m(e,_e,h),m(e,N,h),t(N,K),t(K,R),T(Q,R,null),t(N,X),t(N,oe),t(oe,L),m(e,ee,h),m(e,W,h),t(W,$e),m(e,d,h),T(k,e,h),m(e,te,h),m(e,B,h),t(B,re),m(e,J,h),T(Z,e,h),m(e,ie,h),m(e,pe,h),t(pe,ce),m(e,ae,h),m(e,me,h),t(me,l),t(l,E),t(E,Ee),t(l,Ue),t(me,Ve),t(me,de),t(de,ze),t(ze,qe),t(de,Ge),t(de,ve),t(ve,He),t(de,Ke),t(de,je),t(je,Je),t(de,Ye),m(e,ht,h),m(e,we,h),t(we,Ce),t(Ce,Xe),T(Fe,Xe,null),t(we,Ft),t(we,Ze),t(Ze,Mt),m(e,_t,h),m(e,Te,h),t(Te,It),t(Te,et),t(et,Nt),t(Te,Bt),m(e,$t,h),T(Me,e,h),m(e,gt,h),m(e,De,h),t(De,Rt),t(De,tt),t(tt,Lt),t(De,Wt),m(e,bt,h),T(Ie,e,h),m(e,vt,h),m(e,ge,h),t(ge,Ut),t(ge,Ne),t(Ne,at),t(at,Vt),t(ge,Gt),t(ge,st),t(st,Ht),t(ge,Kt),t(ge,ot),t(ot,Jt),t(ge,Yt),m(e,jt,h),T(Be,e,h),m(e,kt,h),m(e,ne,h),t(ne,Qt),t(ne,rt),t(rt,Xt),t(ne,Zt),t(ne,nt),t(nt,ea),t(ne,ta),t(ne,lt),t(lt,aa),t(ne,sa),t(ne,it),t(it,oa),t(ne,ra),m(e,Et,h),T(ye,e,h),m(e,wt,h),m(e,xe,h),t(xe,Pe),t(Pe,pt),T(Re,pt,null),t(xe,na),t(xe,ct),t(ct,la),m(e,xt,h),T(Ae,e,h),m(e,zt,h),T(Se,e,h),qt=!0},p(e,[h]){const Le={};h&2&&(Le.$$scope={dirty:h,ctx:e}),G.$set(Le);const mt={};h&2&&(mt.$$scope={dirty:h,ctx:e}),ye.$set(mt);const dt={};h&2&&(dt.$$scope={dirty:h,ctx:e}),Ae.$set(dt);const ft={};h&2&&(ft.$$scope={dirty:h,ctx:e}),Se.$set(ft)},i(e){qt||(D(_.$$.fragment,e),D(A.$$.fragment,e),D(G.$$.fragment,e),D(Q.$$.fragment,e),D(k.$$.fragment,e),D(Z.$$.fragment,e),D(Fe.$$.fragment,e),D(Me.$$.fragment,e),D(Ie.$$.fragment,e),D(Be.$$.fragment,e),D(ye.$$.fragment,e),D(Re.$$.fragment,e),D(Ae.$$.fragment,e),D(Se.$$.fragment,e),qt=!0)},o(e){y(_.$$.fragment,e),y(A.$$.fragment,e),y(G.$$.fragment,e),y(Q.$$.fragment,e),y(k.$$.fragment,e),y(Z.$$.fragment,e),y(Fe.$$.fragment,e),y(Me.$$.fragment,e),y(Ie.$$.fragment,e),y(Be.$$.fragment,e),y(ye.$$.fragment,e),y(Re.$$.fragment,e),y(Ae.$$.fragment,e),y(Se.$$.fragment,e),qt=!1},d(e){a(s),e&&a(f),e&&a(o),P(_),e&&a(j),P(A,e),e&&a(I),e&&a(Y),e&&a(H),e&&a(O),e&&a(he),P(G,e),e&&a(_e),e&&a(N),P(Q),e&&a(ee),e&&a(W),e&&a(d),P(k,e),e&&a(te),e&&a(B),e&&a(J),P(Z,e),e&&a(ie),e&&a(pe),e&&a(ae),e&&a(me),e&&a(ht),e&&a(we),P(Fe),e&&a(_t),e&&a(Te),e&&a($t),P(Me,e),e&&a(gt),e&&a(De),e&&a(bt),P(Ie,e),e&&a(vt),e&&a(ge),e&&a(jt),P(Be,e),e&&a(kt),e&&a(ne),e&&a(Et),P(ye,e),e&&a(wt),e&&a(xe),P(Re),e&&a(xt),P(Ae,e),e&&a(zt),P(Se,e)}}}const ns={local:"classificao-de-texto",sections:[{local:"carregue-o-conjunto-de-dados-imdb",title:"Carregue o conjunto de dados IMDb"},{local:"prprocessamento-dos-dados",title:"Pr\xE9-processamento dos dados"},{local:"train",title:"Train"}],title:"Classifica\xE7\xE3o de texto"};function ls(S){return Ua(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class us extends Ba{constructor(s){super();Ra(this,s,ls,rs,La,{})}}export{us as default,ns as metadata};
