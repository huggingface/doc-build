import{S as bo,i as Po,s as wo,e as r,k as m,w as C,t as i,M as qo,c as t,d as o,m as d,a as n,x as O,h as l,b as f,G as a,g as p,y as F,L as yo,q as x,o as B,B as J,v as Ao}from"../chunks/vendor-hf-doc-builder.js";import{I as ao}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as be}from"../chunks/CodeBlock-hf-doc-builder.js";function So(so){let h,re,z,_,I,b,Pe,G,we,te,c,qe,R,ye,Ae,P,Se,Ne,ne,U,Ce,ie,w,le,D,Oe,pe,v,$,Q,q,Fe,X,xe,me,u,Be,Y,Je,Ue,Z,De,Ke,de,y,fe,j,Me,K,Ve,Le,ce,g,T,ee,A,We,oe,He,ue,M,Ie,ke,S,he,k,Ge,ae,Re,Qe,se,Xe,Ye,ze,N,ve,E,Ze,V,eo,oo,ge;return b=new ao({}),w=new be({props:{code:`from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace

tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

tokenizer.pre_tokenizer = Whitespace()
files = [...]
tokenizer.train(files, trainer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.models <span class="hljs-keyword">import</span> BPE
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.trainers <span class="hljs-keyword">import</span> BpeTrainer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.pre_tokenizers <span class="hljs-keyword">import</span> Whitespace

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = Tokenizer(BPE(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = BpeTrainer(special_tokens=[<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.pre_tokenizer = Whitespace()
<span class="hljs-meta">&gt;&gt;&gt; </span>files = [...]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.train(files, trainer)`}}),q=new ao({}),y=new be({props:{code:`from transformers import PreTrainedTokenizerFast

fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)`}}),A=new ao({}),S=new be({props:{code:'tokenizer.save("tokenizer.json")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)'}}),N=new be({props:{code:`from transformers import PreTrainedTokenizerFast

fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="tokenizer.json")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=<span class="hljs-string">&quot;tokenizer.json&quot;</span>)`}}),{c(){h=r("meta"),re=m(),z=r("h1"),_=r("a"),I=r("span"),C(b.$$.fragment),Pe=m(),G=r("span"),we=i("Usando os Tokenizers do \u{1F917} Tokenizers"),te=m(),c=r("p"),qe=i("O "),R=r("code"),ye=i("PreTrainedTokenizerFast"),Ae=i(" depende da biblioteca "),P=r("a"),Se=i("\u{1F917} Tokenizers"),Ne=i(". O Tokenizer obtido da biblioteca \u{1F917} Tokenizers pode ser carregado facilmente pelo \u{1F917} Transformers."),ne=m(),U=r("p"),Ce=i("Antes de entrar nos detalhes, vamos come\xE7ar criando um tokenizer fict\xEDcio em algumas linhas:"),ie=m(),C(w.$$.fragment),le=m(),D=r("p"),Oe=i("Agora temos um tokenizer treinado nos arquivos que foram definidos. N\xF3s podemos continuar usando nessa execu\xE7\xE3o ou salvar em um arquivo JSON para re-utilizar no futuro."),pe=m(),v=r("h2"),$=r("a"),Q=r("span"),C(q.$$.fragment),Fe=m(),X=r("span"),xe=i("Carregando diretamente de um objeto tokenizer"),me=m(),u=r("p"),Be=i("Vamos ver como aproveitar esse objeto tokenizer na biblioteca \u{1F917} Transformers. A classe "),Y=r("code"),Je=i("PreTrainedTokenizerFast"),Ue=i(" permite uma instancia\xE7\xE3o f\xE1cil, aceitando o objeto "),Z=r("em"),De=i("tokenizer"),Ke=i(" instanciado como um argumento:"),de=m(),C(y.$$.fragment),fe=m(),j=r("p"),Me=i("Esse objeto pode ser utilizado com todos os m\xE9todos compartilhados pelos tokenizers dos \u{1F917} Transformers! V\xE1 para "),K=r("a"),Ve=i("a p\xE1gina do tokenizer"),Le=i(" para mais informa\xE7\xF5es."),ce=m(),g=r("h2"),T=r("a"),ee=r("span"),C(A.$$.fragment),We=m(),oe=r("span"),He=i("Carregando de um arquivo JSON"),ue=m(),M=r("p"),Ie=i("Para carregar um tokenizer de um arquivo JSON vamos primeiro come\xE7ar salvando nosso tokenizer:"),ke=m(),C(S.$$.fragment),he=m(),k=r("p"),Ge=i("A pasta para qual salvamos esse arquivo pode ser passada para o m\xE9todo de inicializa\xE7\xE3o do "),ae=r("code"),Re=i("PreTrainedTokenizerFast"),Qe=i(" usando o "),se=r("code"),Xe=i("tokenizer_file"),Ye=i(" par\xE2metro:"),ze=m(),C(N.$$.fragment),ve=m(),E=r("p"),Ze=i("Esse objeto pode ser utilizado com todos os m\xE9todos compartilhados pelos tokenizers dos \u{1F917} Transformers! V\xE1 para "),V=r("a"),eo=i("a p\xE1gina do tokenizer"),oo=i(" para mais informa\xE7\xF5es."),this.h()},l(e){const s=qo('[data-svelte="svelte-1phssyn"]',document.head);h=t(s,"META",{name:!0,content:!0}),s.forEach(o),re=d(e),z=t(e,"H1",{class:!0});var _e=n(z);_=t(_e,"A",{id:!0,class:!0,href:!0});var ro=n(_);I=t(ro,"SPAN",{});var to=n(I);O(b.$$.fragment,to),to.forEach(o),ro.forEach(o),Pe=d(_e),G=t(_e,"SPAN",{});var no=n(G);we=l(no,"Usando os Tokenizers do \u{1F917} Tokenizers"),no.forEach(o),_e.forEach(o),te=d(e),c=t(e,"P",{});var L=n(c);qe=l(L,"O "),R=t(L,"CODE",{});var io=n(R);ye=l(io,"PreTrainedTokenizerFast"),io.forEach(o),Ae=l(L," depende da biblioteca "),P=t(L,"A",{href:!0,rel:!0});var lo=n(P);Se=l(lo,"\u{1F917} Tokenizers"),lo.forEach(o),Ne=l(L,". O Tokenizer obtido da biblioteca \u{1F917} Tokenizers pode ser carregado facilmente pelo \u{1F917} Transformers."),L.forEach(o),ne=d(e),U=t(e,"P",{});var po=n(U);Ce=l(po,"Antes de entrar nos detalhes, vamos come\xE7ar criando um tokenizer fict\xEDcio em algumas linhas:"),po.forEach(o),ie=d(e),O(w.$$.fragment,e),le=d(e),D=t(e,"P",{});var mo=n(D);Oe=l(mo,"Agora temos um tokenizer treinado nos arquivos que foram definidos. N\xF3s podemos continuar usando nessa execu\xE7\xE3o ou salvar em um arquivo JSON para re-utilizar no futuro."),mo.forEach(o),pe=d(e),v=t(e,"H2",{class:!0});var $e=n(v);$=t($e,"A",{id:!0,class:!0,href:!0});var fo=n($);Q=t(fo,"SPAN",{});var co=n(Q);O(q.$$.fragment,co),co.forEach(o),fo.forEach(o),Fe=d($e),X=t($e,"SPAN",{});var uo=n(X);xe=l(uo,"Carregando diretamente de um objeto tokenizer"),uo.forEach(o),$e.forEach(o),me=d(e),u=t(e,"P",{});var W=n(u);Be=l(W,"Vamos ver como aproveitar esse objeto tokenizer na biblioteca \u{1F917} Transformers. A classe "),Y=t(W,"CODE",{});var ko=n(Y);Je=l(ko,"PreTrainedTokenizerFast"),ko.forEach(o),Ue=l(W," permite uma instancia\xE7\xE3o f\xE1cil, aceitando o objeto "),Z=t(W,"EM",{});var ho=n(Z);De=l(ho,"tokenizer"),ho.forEach(o),Ke=l(W," instanciado como um argumento:"),W.forEach(o),de=d(e),O(y.$$.fragment,e),fe=d(e),j=t(e,"P",{});var je=n(j);Me=l(je,"Esse objeto pode ser utilizado com todos os m\xE9todos compartilhados pelos tokenizers dos \u{1F917} Transformers! V\xE1 para "),K=t(je,"A",{href:!0});var zo=n(K);Ve=l(zo,"a p\xE1gina do tokenizer"),zo.forEach(o),Le=l(je," para mais informa\xE7\xF5es."),je.forEach(o),ce=d(e),g=t(e,"H2",{class:!0});var Te=n(g);T=t(Te,"A",{id:!0,class:!0,href:!0});var vo=n(T);ee=t(vo,"SPAN",{});var go=n(ee);O(A.$$.fragment,go),go.forEach(o),vo.forEach(o),We=d(Te),oe=t(Te,"SPAN",{});var _o=n(oe);He=l(_o,"Carregando de um arquivo JSON"),_o.forEach(o),Te.forEach(o),ue=d(e),M=t(e,"P",{});var $o=n(M);Ie=l($o,"Para carregar um tokenizer de um arquivo JSON vamos primeiro come\xE7ar salvando nosso tokenizer:"),$o.forEach(o),ke=d(e),O(S.$$.fragment,e),he=d(e),k=t(e,"P",{});var H=n(k);Ge=l(H,"A pasta para qual salvamos esse arquivo pode ser passada para o m\xE9todo de inicializa\xE7\xE3o do "),ae=t(H,"CODE",{});var jo=n(ae);Re=l(jo,"PreTrainedTokenizerFast"),jo.forEach(o),Qe=l(H," usando o "),se=t(H,"CODE",{});var To=n(se);Xe=l(To,"tokenizer_file"),To.forEach(o),Ye=l(H," par\xE2metro:"),H.forEach(o),ze=d(e),O(N.$$.fragment,e),ve=d(e),E=t(e,"P",{});var Ee=n(E);Ze=l(Ee,"Esse objeto pode ser utilizado com todos os m\xE9todos compartilhados pelos tokenizers dos \u{1F917} Transformers! V\xE1 para "),V=t(Ee,"A",{href:!0});var Eo=n(V);eo=l(Eo,"a p\xE1gina do tokenizer"),Eo.forEach(o),oo=l(Ee," para mais informa\xE7\xF5es."),Ee.forEach(o),this.h()},h(){f(h,"name","hf:doc:metadata"),f(h,"content",JSON.stringify(No)),f(_,"id","usando-os-tokenizers-do-tokenizers"),f(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(_,"href","#usando-os-tokenizers-do-tokenizers"),f(z,"class","relative group"),f(P,"href","https://huggingface.co/docs/tokenizers"),f(P,"rel","nofollow"),f($,"id","carregando-diretamente-de-um-objeto-tokenizer"),f($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f($,"href","#carregando-diretamente-de-um-objeto-tokenizer"),f(v,"class","relative group"),f(K,"href","main_classes/tokenizer"),f(T,"id","carregando-de-um-arquivo-json"),f(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(T,"href","#carregando-de-um-arquivo-json"),f(g,"class","relative group"),f(V,"href","main_classes/tokenizer")},m(e,s){a(document.head,h),p(e,re,s),p(e,z,s),a(z,_),a(_,I),F(b,I,null),a(z,Pe),a(z,G),a(G,we),p(e,te,s),p(e,c,s),a(c,qe),a(c,R),a(R,ye),a(c,Ae),a(c,P),a(P,Se),a(c,Ne),p(e,ne,s),p(e,U,s),a(U,Ce),p(e,ie,s),F(w,e,s),p(e,le,s),p(e,D,s),a(D,Oe),p(e,pe,s),p(e,v,s),a(v,$),a($,Q),F(q,Q,null),a(v,Fe),a(v,X),a(X,xe),p(e,me,s),p(e,u,s),a(u,Be),a(u,Y),a(Y,Je),a(u,Ue),a(u,Z),a(Z,De),a(u,Ke),p(e,de,s),F(y,e,s),p(e,fe,s),p(e,j,s),a(j,Me),a(j,K),a(K,Ve),a(j,Le),p(e,ce,s),p(e,g,s),a(g,T),a(T,ee),F(A,ee,null),a(g,We),a(g,oe),a(oe,He),p(e,ue,s),p(e,M,s),a(M,Ie),p(e,ke,s),F(S,e,s),p(e,he,s),p(e,k,s),a(k,Ge),a(k,ae),a(ae,Re),a(k,Qe),a(k,se),a(se,Xe),a(k,Ye),p(e,ze,s),F(N,e,s),p(e,ve,s),p(e,E,s),a(E,Ze),a(E,V),a(V,eo),a(E,oo),ge=!0},p:yo,i(e){ge||(x(b.$$.fragment,e),x(w.$$.fragment,e),x(q.$$.fragment,e),x(y.$$.fragment,e),x(A.$$.fragment,e),x(S.$$.fragment,e),x(N.$$.fragment,e),ge=!0)},o(e){B(b.$$.fragment,e),B(w.$$.fragment,e),B(q.$$.fragment,e),B(y.$$.fragment,e),B(A.$$.fragment,e),B(S.$$.fragment,e),B(N.$$.fragment,e),ge=!1},d(e){o(h),e&&o(re),e&&o(z),J(b),e&&o(te),e&&o(c),e&&o(ne),e&&o(U),e&&o(ie),J(w,e),e&&o(le),e&&o(D),e&&o(pe),e&&o(v),J(q),e&&o(me),e&&o(u),e&&o(de),J(y,e),e&&o(fe),e&&o(j),e&&o(ce),e&&o(g),J(A),e&&o(ue),e&&o(M),e&&o(ke),J(S,e),e&&o(he),e&&o(k),e&&o(ze),J(N,e),e&&o(ve),e&&o(E)}}}const No={local:"usando-os-tokenizers-do-tokenizers",sections:[{local:"carregando-diretamente-de-um-objeto-tokenizer",title:"Carregando diretamente de um objeto tokenizer"},{local:"carregando-de-um-arquivo-json",title:"Carregando de um arquivo JSON"}],title:"Usando os Tokenizers do \u{1F917} Tokenizers"};function Co(so){return Ao(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Bo extends bo{constructor(h){super();Po(this,h,Co,So,wo,{})}}export{Bo as default,No as metadata};
