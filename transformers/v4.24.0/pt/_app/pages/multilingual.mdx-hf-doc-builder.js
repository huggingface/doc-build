import{S as Yr,i as Zr,s as en,e as t,k as d,w as u,t as i,M as on,c as s,d as a,m as p,a as l,x as f,h as r,b as c,G as o,g as m,y as g,L as an,q as h,o as _,B as v,v as tn}from"../chunks/vendor-hf-doc-builder.js";import{I as re}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as z}from"../chunks/CodeBlock-hf-doc-builder.js";import{D as sn}from"../chunks/DocNotebookDropdown-hf-doc-builder.js";function ln(wi){let L,fa,O,S,po,ne,Lt,co,Ot,ga,me,ha,y,Ct,uo,Tt,Pt,de,Dt,Xt,_a,C,I,fo,pe,At,go,Nt,va,Le,St,ba,T,B,ho,ce,It,_o,Bt,Ea,Oe,Rt,ka,b,Ce,vo,Ft,Ht,Gt,Te,bo,Wt,Ut,Jt,Pe,Eo,Kt,Qt,Vt,De,ko,Yt,Zt,es,Xe,$o,os,as,ts,Ae,Mo,ss,ls,is,Ne,xo,rs,ns,$a,$,ms,wo,ds,ps,zo,cs,us,yo,fs,gs,Ma,R,hs,qo,_s,vs,xa,ue,wa,F,bs,jo,Es,ks,za,fe,ya,Se,$s,qa,ge,ja,M,Ms,Lo,xs,ws,Oo,zs,ys,Co,qs,js,La,he,Oa,H,Ls,To,Os,Cs,Ca,_e,Ta,q,Ts,ve,Ps,Ds,Po,Xs,As,Pa,P,G,Do,be,Ns,Xo,Ss,Da,Ie,Is,Xa,W,Be,Ao,Bs,Rs,Fs,Re,No,Hs,Gs,Aa,Fe,Ws,Na,D,U,So,Ee,Us,Io,Js,Sa,He,Ks,Ia,J,Ge,Bo,Qs,Vs,Ys,We,Ro,Zs,el,Ba,Ue,ol,Ra,X,K,Fo,ke,al,Ho,tl,Fa,Je,sl,Ha,Q,Ke,Go,ll,il,rl,Qe,Wo,nl,ml,Ga,Ve,dl,Wa,A,V,Uo,$e,pl,Jo,cl,Ua,Ye,ul,Ja,Y,Ze,Ko,fl,gl,hl,eo,Qo,_l,vl,Ka,Z,bl,Vo,El,kl,Qa,Me,Va,oo,$l,Ya,xe,Za,x,Ml,Yo,xl,wl,Zo,zl,yl,ea,ql,jl,et,we,ot,N,ee,oa,ze,Ll,aa,Ol,at,ao,Cl,tt,E,to,ta,Tl,Pl,Dl,so,sa,Xl,Al,Nl,lo,la,Sl,Il,Bl,io,ia,Rl,Fl,Hl,ra,na,Gl,st,oe,Wl,ma,Ul,Jl,lt,ye,it,ro,Kl,rt,qe,nt,w,Ql,da,Vl,Yl,pa,Zl,ei,ca,oi,ai,mt,je,dt,ae,ti,ua,si,li,pt;return ne=new re({}),me=new sn({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/pt/multilingual.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/pt/pytorch/multilingual.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/pt/tensorflow/multilingual.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/pt/multilingual.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/pt/pytorch/multilingual.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/pt/tensorflow/multilingual.ipynb"}]}}),pe=new re({}),ce=new re({}),ue=new z({props:{code:`import torch
from transformers import XLMTokenizer, XLMWithLMHeadModel

tokenizer = XLMTokenizer.from_pretrained("xlm-clm-enfr-1024")
model = XLMWithLMHeadModel.from_pretrained("xlm-clm-enfr-1024")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMTokenizer, XLMWithLMHeadModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMTokenizer.from_pretrained(<span class="hljs-string">&quot;xlm-clm-enfr-1024&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMWithLMHeadModel.from_pretrained(<span class="hljs-string">&quot;xlm-clm-enfr-1024&quot;</span>)`}}),fe=new z({props:{code:"print(tokenizer.lang2id)",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tokenizer.lang2id)
{<span class="hljs-string">&#x27;en&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;fr&#x27;</span>: <span class="hljs-number">1</span>}`}}),ge=new z({props:{code:'input_ids = torch.tensor([tokenizer.encode("Wikipedia was used to")])  # batch size of 1',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.tensor([tokenizer.encode(<span class="hljs-string">&quot;Wikipedia was used to&quot;</span>)])  <span class="hljs-comment"># batch size of 1</span>'}}),he=new z({props:{code:`language_id = tokenizer.lang2id["en"]  # 0
langs = torch.tensor([language_id] * input_ids.shape[1])  # torch.tensor([0, 0, 0, ..., 0])

# We reshape it to be of size (batch_size, sequence_length)
langs = langs.view(1, -1)  # is now of shape [1, sequence_length] (we have a batch size of 1)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>language_id = tokenizer.lang2id[<span class="hljs-string">&quot;en&quot;</span>]  <span class="hljs-comment"># 0</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>langs = torch.tensor([language_id] * input_ids.shape[<span class="hljs-number">1</span>])  <span class="hljs-comment"># torch.tensor([0, 0, 0, ..., 0])</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># We reshape it to be of size (batch_size, sequence_length)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>langs = langs.view(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)  <span class="hljs-comment"># is now of shape [1, sequence_length] (we have a batch size of 1)</span>`}}),_e=new z({props:{code:"outputs = model(input_ids, langs=langs)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids, langs=langs)'}}),be=new re({}),Ee=new re({}),ke=new re({}),$e=new re({}),Me=new z({props:{code:`from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
chinese_text = "\u4E0D\u8981\u63D2\u624B\u5DEB\u5E2B\u7684\u4E8B\u52D9, \u56E0\u70BA\u4ED6\u5011\u662F\u5FAE\u5999\u7684, \u5F88\u5FEB\u5C31\u6703\u767C\u6012."

tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M", src_lang="zh")
model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> M2M100ForConditionalGeneration, M2M100Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>en_text = <span class="hljs-string">&quot;Do not meddle in the affairs of wizards, for they are subtle and quick to anger.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>chinese_text = <span class="hljs-string">&quot;\u4E0D\u8981\u63D2\u624B\u5DEB\u5E2B\u7684\u4E8B\u52D9, \u56E0\u70BA\u4ED6\u5011\u662F\u5FAE\u5999\u7684, \u5F88\u5FEB\u5C31\u6703\u767C\u6012.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = M2M100Tokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/m2m100_418M&quot;</span>, src_lang=<span class="hljs-string">&quot;zh&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = M2M100ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;facebook/m2m100_418M&quot;</span>)`}}),xe=new z({props:{code:'encoded_zh = tokenizer(chinese_text, return_tensors="pt")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_zh = tokenizer(chinese_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)'}}),we=new z({props:{code:`generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id("en"))
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(<span class="hljs-string">&quot;en&quot;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generated_tokens, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-string">&#x27;Do not interfere with the matters of the witches, because they are delicate and will soon be angry.&#x27;</span>`}}),ze=new re({}),ye=new z({props:{code:`from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

en_text = "Do not meddle in the affairs of wizards, for they are subtle and quick to anger."
fi_text = "\xC4l\xE4 sekaannu velhojen asioihin, sill\xE4 ne ovat hienovaraisia ja nopeasti vihaisia."

tokenizer = AutoTokenizer.from_pretrained("facebook/mbart-large-50-many-to-many-mmt", src_lang="fi_FI")
model = AutoModelForSeq2SeqLM.from_pretrained("facebook/mbart-large-50-many-to-many-mmt")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>en_text = <span class="hljs-string">&quot;Do not meddle in the affairs of wizards, for they are subtle and quick to anger.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>fi_text = <span class="hljs-string">&quot;\xC4l\xE4 sekaannu velhojen asioihin, sill\xE4 ne ovat hienovaraisia ja nopeasti vihaisia.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/mbart-large-50-many-to-many-mmt&quot;</span>, src_lang=<span class="hljs-string">&quot;fi_FI&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;facebook/mbart-large-50-many-to-many-mmt&quot;</span>)`}}),qe=new z({props:{code:'encoded_en = tokenizer(en_text, return_tensors="pt")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_en = tokenizer(en_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)'}}),je=new z({props:{code:`generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id("en_XX"))
tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.lang_code_to_id(<span class="hljs-string">&quot;en_XX&quot;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generated_tokens, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-string">&quot;Don&#x27;t interfere with the wizard&#x27;s affairs, because they are subtle, will soon get angry.&quot;</span>`}}),{c(){L=t("meta"),fa=d(),O=t("h1"),S=t("a"),po=t("span"),u(ne.$$.fragment),Lt=d(),co=t("span"),Ot=i("Modelos multilingu\xEDsticos para infer\xEAncia"),ga=d(),u(me.$$.fragment),ha=d(),y=t("p"),Ct=i(`Existem v\xE1rios modelos multilingu\xEDsticos no \u{1F917} Transformers e seus usos para infer\xEAncia diferem dos modelos monol\xEDngues.
No entanto, nem `),uo=t("em"),Tt=i("todos"),Pt=i(` os usos dos modelos multil\xEDngues s\xE3o t\xE3o diferentes.
Alguns modelos, como o `),de=t("a"),Dt=i("bert-base-multilingual-uncased"),Xt=i(`,
podem ser usados como se fossem monol\xEDngues. Este guia ir\xE1 te ajudar a usar modelos multil\xEDngues cujo uso difere
para o prop\xF3sito de infer\xEAncia.`),_a=d(),C=t("h2"),I=t("a"),fo=t("span"),u(pe.$$.fragment),At=d(),go=t("span"),Nt=i("XLM"),va=d(),Le=t("p"),St=i(`O XLM tem dez checkpoints diferentes dos quais apenas um \xE9 monol\xEDngue.
Os nove checkpoints restantes do modelo s\xE3o subdivididos em duas categorias:
checkpoints que usam de language embeddings e os que n\xE3o.`),ba=d(),T=t("h3"),B=t("a"),ho=t("span"),u(ce.$$.fragment),It=d(),_o=t("span"),Bt=i("XLM com language embeddings"),Ea=d(),Oe=t("p"),Rt=i("Os seguintes modelos de XLM usam language embeddings para especificar a linguagem utilizada para a infer\xEAncia."),ka=d(),b=t("ul"),Ce=t("li"),vo=t("code"),Ft=i("xlm-mlm-ende-1024"),Ht=i(" (Masked language modeling, English-German)"),Gt=d(),Te=t("li"),bo=t("code"),Wt=i("xlm-mlm-enfr-1024"),Ut=i(" (Masked language modeling, English-French)"),Jt=d(),Pe=t("li"),Eo=t("code"),Kt=i("xlm-mlm-enro-1024"),Qt=i(" (Masked language modeling, English-Romanian)"),Vt=d(),De=t("li"),ko=t("code"),Yt=i("xlm-mlm-xnli15-1024"),Zt=i(" (Masked language modeling, XNLI languages)"),es=d(),Xe=t("li"),$o=t("code"),os=i("xlm-mlm-tlm-xnli15-1024"),as=i(" (Masked language modeling + translation, XNLI languages)"),ts=d(),Ae=t("li"),Mo=t("code"),ss=i("xlm-clm-enfr-1024"),ls=i(" (Causal language modeling, English-French)"),is=d(),Ne=t("li"),xo=t("code"),rs=i("xlm-clm-ende-1024"),ns=i(" (Causal language modeling, English-German)"),$a=d(),$=t("p"),ms=i("Os language embeddings s\xE3o representados por um tensor de mesma dimens\xE3o que os "),wo=t("code"),ds=i("input_ids"),ps=i(` passados ao modelo.
Os valores destes tensores dependem do idioma utilizado e se identificam pelos atributos `),zo=t("code"),cs=i("lang2id"),us=i(" e "),yo=t("code"),fs=i("id2lang"),gs=i(" do tokenizador."),Ma=d(),R=t("p"),hs=i("Neste exemplo, carregamos o checkpoint "),qo=t("code"),_s=i("xlm-clm-enfr-1024"),vs=i("(Causal language modeling, English-French):"),xa=d(),u(ue.$$.fragment),wa=d(),F=t("p"),bs=i("O atributo "),jo=t("code"),Es=i("lang2id"),ks=i(" do tokenizador mostra os idiomas deste modelo e seus ids:"),za=d(),u(fe.$$.fragment),ya=d(),Se=t("p"),$s=i("Em seguida, cria-se um input de exemplo:"),qa=d(),u(ge.$$.fragment),ja=d(),M=t("p"),Ms=i("Estabelece-se o id do idioma, por exemplo "),Lo=t("code"),xs=i('"en"'),ws=i(`, e utiliza-se o mesmo para definir a language embedding.
A language embedding \xE9 um tensor preenchido com `),Oo=t("code"),zs=i("0"),ys=i(`, que \xE9 o id de idioma para o ingl\xEAs.
Este tensor deve ser do mesmo tamanho que os `),Co=t("code"),qs=i("input_ids"),js=i("."),La=d(),u(he.$$.fragment),Oa=d(),H=t("p"),Ls=i("Agora voc\xEA pode passar os "),To=t("code"),Os=i("input_ids"),Cs=i(" e a language embedding ao modelo:"),Ca=d(),u(_e.$$.fragment),Ta=d(),q=t("p"),Ts=i("O script "),ve=t("a"),Ps=i("run_generation.py"),Ds=i(" pode gerar um texto com language embeddings utilizando os checkpoints "),Po=t("code"),Xs=i("xlm-clm"),As=i("."),Pa=d(),P=t("h3"),G=t("a"),Do=t("span"),u(be.$$.fragment),Ns=d(),Xo=t("span"),Ss=i("XLM sem language embeddings"),Da=d(),Ie=t("p"),Is=i("Os seguintes modelos XLM n\xE3o requerem o uso de language embeddings durante a infer\xEAncia:"),Xa=d(),W=t("ul"),Be=t("li"),Ao=t("code"),Bs=i("xlm-mlm-17-1280"),Rs=i(" (Modelagem de linguagem com m\xE1scara, 17 idiomas)"),Fs=d(),Re=t("li"),No=t("code"),Hs=i("xlm-mlm-100-1280"),Gs=i(" (Modelagem de linguagem com m\xE1scara, 100 idiomas)"),Aa=d(),Fe=t("p"),Ws=i("Estes modelos s\xE3o utilizados para representa\xE7\xF5es gen\xE9ricas de frase diferentemente dos checkpoints XLM anteriores."),Na=d(),D=t("h2"),U=t("a"),So=t("span"),u(Ee.$$.fragment),Us=d(),Io=t("span"),Js=i("BERT"),Sa=d(),He=t("p"),Ks=i("Os seguintes modelos do BERT podem ser utilizados para tarefas multilingu\xEDsticas:"),Ia=d(),J=t("ul"),Ge=t("li"),Bo=t("code"),Qs=i("bert-base-multilingual-uncased"),Vs=i(" (Modelagem de linguagem com m\xE1scara + Previs\xE3o de frases, 102 idiomas)"),Ys=d(),We=t("li"),Ro=t("code"),Zs=i("bert-base-multilingual-cased"),el=i(" (Modelagem de linguagem com m\xE1scara + Previs\xE3o de frases, 104 idiomas)"),Ba=d(),Ue=t("p"),ol=i(`Estes modelos n\xE3o requerem language embeddings durante a infer\xEAncia. Devem identificar a linguagem a partir
do contexto e realizar a infer\xEAncia em sequ\xEAncia.`),Ra=d(),X=t("h2"),K=t("a"),Fo=t("span"),u(ke.$$.fragment),al=d(),Ho=t("span"),tl=i("XLM-RoBERTa"),Fa=d(),Je=t("p"),sl=i("Os seguintes modelos do XLM-RoBERTa podem ser utilizados para tarefas multilingu\xEDsticas:"),Ha=d(),Q=t("ul"),Ke=t("li"),Go=t("code"),ll=i("xlm-roberta-base"),il=i(" (Modelagem de linguagem com m\xE1scara, 100 idiomas)"),rl=d(),Qe=t("li"),Wo=t("code"),nl=i("xlm-roberta-large"),ml=i(" Modelagem de linguagem com m\xE1scara, 100 idiomas)"),Ga=d(),Ve=t("p"),dl=i(`O XLM-RoBERTa foi treinado com 2,5 TB de dados do CommonCrawl rec\xE9m-criados e testados em 100 idiomas.
Proporciona fortes vantagens sobre os modelos multilingu\xEDsticos publicados anteriormente como o mBERT e o XLM em tarefas
subsequentes como a classifica\xE7\xE3o, a rotulagem de sequ\xEAncias e \xE0 respostas a perguntas.`),Wa=d(),A=t("h2"),V=t("a"),Uo=t("span"),u($e.$$.fragment),pl=d(),Jo=t("span"),cl=i("M2M100"),Ua=d(),Ye=t("p"),ul=i("Os seguintes modelos de M2M100 podem ser utilizados para tradu\xE7\xF5es multilingu\xEDsticas:"),Ja=d(),Y=t("ul"),Ze=t("li"),Ko=t("code"),fl=i("facebook/m2m100_418M"),gl=i(" (Tradu\xE7\xE3o)"),hl=d(),eo=t("li"),Qo=t("code"),_l=i("facebook/m2m100_1.2B"),vl=i(" (Tradu\xE7\xE3o)"),Ka=d(),Z=t("p"),bl=i("Neste exemplo, o checkpoint "),Vo=t("code"),El=i("facebook/m2m100_418M"),kl=i(` \xE9 carregado para traduzir do mandarim ao ingl\xEAs. \xC9 poss\xEDvel
estabelecer o idioma de origem no tokenizador:`),Qa=d(),u(Me.$$.fragment),Va=d(),oo=t("p"),$l=i("Tokeniza\xE7\xE3o do texto:"),Ya=d(),u(xe.$$.fragment),Za=d(),x=t("p"),Ml=i(`O M2M100 for\xE7a o id do idioma de destino como o primeiro token gerado para traduzir ao idioma de destino.
\xC9 definido o `),Yo=t("code"),xl=i("forced_bos_token_id"),wl=i(" como "),Zo=t("code"),zl=i("en"),yl=i(" no m\xE9todo "),ea=t("code"),ql=i("generate"),jl=i(" para traduzir ao ingl\xEAs."),et=d(),u(we.$$.fragment),ot=d(),N=t("h2"),ee=t("a"),oa=t("span"),u(ze.$$.fragment),Ll=d(),aa=t("span"),Ol=i("MBart"),at=d(),ao=t("p"),Cl=i("Os seguintes modelos do MBart podem ser utilizados para tradu\xE7\xE3o multilingu\xEDstica:"),tt=d(),E=t("ul"),to=t("li"),ta=t("code"),Tl=i("facebook/mbart-large-50-one-to-many-mmt"),Pl=i(" (Tradu\xE7\xE3o autom\xE1tica multilingu\xEDstica de um a v\xE1rios, 50 idiomas)"),Dl=d(),so=t("li"),sa=t("code"),Xl=i("facebook/mbart-large-50-many-to-many-mmt"),Al=i(" (Tradu\xE7\xE3o autom\xE1tica multilingu\xEDstica de v\xE1rios a v\xE1rios, 50 idiomas)"),Nl=d(),lo=t("li"),la=t("code"),Sl=i("facebook/mbart-large-50-many-to-one-mmt"),Il=i(" (Tradu\xE7\xE3o autom\xE1tica multilingu\xEDstica v\xE1rios a um, 50 idiomas)"),Bl=d(),io=t("li"),ia=t("code"),Rl=i("facebook/mbart-large-50"),Fl=i(" (Tradu\xE7\xE3o multilingu\xEDstica, 50 idiomas)"),Hl=d(),ra=t("li"),na=t("code"),Gl=i("facebook/mbart-large-cc25"),st=d(),oe=t("p"),Wl=i("Neste exemplo, carrega-se o checkpoint "),ma=t("code"),Ul=i("facebook/mbart-large-50-many-to-many-mmt"),Jl=i(` para traduzir do finland\xEAs ao ingl\xEAs.
Pode-se definir o idioma de origem no tokenizador:`),lt=d(),u(ye.$$.fragment),it=d(),ro=t("p"),Kl=i("Tokenizando o texto:"),rt=d(),u(qe.$$.fragment),nt=d(),w=t("p"),Ql=i(`O MBart for\xE7a o id do idioma de destino como o primeiro token gerado para traduzir ao idioma de destino.
\xC9 definido o `),da=t("code"),Vl=i("forced_bos_token_id"),Yl=i(" como "),pa=t("code"),Zl=i("en"),ei=i(" no m\xE9todo "),ca=t("code"),oi=i("generate"),ai=i(" para traduzir ao ingl\xEAs."),mt=d(),u(je.$$.fragment),dt=d(),ae=t("p"),ti=i("Se estiver usando o checkpoint "),ua=t("code"),si=i("facebook/mbart-large-50-many-to-one-mmt"),li=i(` n\xE3o ser\xE1 necess\xE1rio for\xE7ar o id do idioma de destino
como sendo o primeiro token generado, caso contr\xE1rio a usagem \xE9 a mesma.`),this.h()},l(e){const n=on('[data-svelte="svelte-1phssyn"]',document.head);L=s(n,"META",{name:!0,content:!0}),n.forEach(a),fa=p(e),O=s(e,"H1",{class:!0});var ct=l(O);S=s(ct,"A",{id:!0,class:!0,href:!0});var zi=l(S);po=s(zi,"SPAN",{});var yi=l(po);f(ne.$$.fragment,yi),yi.forEach(a),zi.forEach(a),Lt=p(ct),co=s(ct,"SPAN",{});var qi=l(co);Ot=r(qi,"Modelos multilingu\xEDsticos para infer\xEAncia"),qi.forEach(a),ct.forEach(a),ga=p(e),f(me.$$.fragment,e),ha=p(e),y=s(e,"P",{});var no=l(y);Ct=r(no,`Existem v\xE1rios modelos multilingu\xEDsticos no \u{1F917} Transformers e seus usos para infer\xEAncia diferem dos modelos monol\xEDngues.
No entanto, nem `),uo=s(no,"EM",{});var ji=l(uo);Tt=r(ji,"todos"),ji.forEach(a),Pt=r(no,` os usos dos modelos multil\xEDngues s\xE3o t\xE3o diferentes.
Alguns modelos, como o `),de=s(no,"A",{href:!0,rel:!0});var Li=l(de);Dt=r(Li,"bert-base-multilingual-uncased"),Li.forEach(a),Xt=r(no,`,
podem ser usados como se fossem monol\xEDngues. Este guia ir\xE1 te ajudar a usar modelos multil\xEDngues cujo uso difere
para o prop\xF3sito de infer\xEAncia.`),no.forEach(a),_a=p(e),C=s(e,"H2",{class:!0});var ut=l(C);I=s(ut,"A",{id:!0,class:!0,href:!0});var Oi=l(I);fo=s(Oi,"SPAN",{});var Ci=l(fo);f(pe.$$.fragment,Ci),Ci.forEach(a),Oi.forEach(a),At=p(ut),go=s(ut,"SPAN",{});var Ti=l(go);Nt=r(Ti,"XLM"),Ti.forEach(a),ut.forEach(a),va=p(e),Le=s(e,"P",{});var Pi=l(Le);St=r(Pi,`O XLM tem dez checkpoints diferentes dos quais apenas um \xE9 monol\xEDngue.
Os nove checkpoints restantes do modelo s\xE3o subdivididos em duas categorias:
checkpoints que usam de language embeddings e os que n\xE3o.`),Pi.forEach(a),ba=p(e),T=s(e,"H3",{class:!0});var ft=l(T);B=s(ft,"A",{id:!0,class:!0,href:!0});var Di=l(B);ho=s(Di,"SPAN",{});var Xi=l(ho);f(ce.$$.fragment,Xi),Xi.forEach(a),Di.forEach(a),It=p(ft),_o=s(ft,"SPAN",{});var Ai=l(_o);Bt=r(Ai,"XLM com language embeddings"),Ai.forEach(a),ft.forEach(a),Ea=p(e),Oe=s(e,"P",{});var Ni=l(Oe);Rt=r(Ni,"Os seguintes modelos de XLM usam language embeddings para especificar a linguagem utilizada para a infer\xEAncia."),Ni.forEach(a),ka=p(e),b=s(e,"UL",{});var k=l(b);Ce=s(k,"LI",{});var ii=l(Ce);vo=s(ii,"CODE",{});var Si=l(vo);Ft=r(Si,"xlm-mlm-ende-1024"),Si.forEach(a),Ht=r(ii," (Masked language modeling, English-German)"),ii.forEach(a),Gt=p(k),Te=s(k,"LI",{});var ri=l(Te);bo=s(ri,"CODE",{});var Ii=l(bo);Wt=r(Ii,"xlm-mlm-enfr-1024"),Ii.forEach(a),Ut=r(ri," (Masked language modeling, English-French)"),ri.forEach(a),Jt=p(k),Pe=s(k,"LI",{});var ni=l(Pe);Eo=s(ni,"CODE",{});var Bi=l(Eo);Kt=r(Bi,"xlm-mlm-enro-1024"),Bi.forEach(a),Qt=r(ni," (Masked language modeling, English-Romanian)"),ni.forEach(a),Vt=p(k),De=s(k,"LI",{});var mi=l(De);ko=s(mi,"CODE",{});var Ri=l(ko);Yt=r(Ri,"xlm-mlm-xnli15-1024"),Ri.forEach(a),Zt=r(mi," (Masked language modeling, XNLI languages)"),mi.forEach(a),es=p(k),Xe=s(k,"LI",{});var di=l(Xe);$o=s(di,"CODE",{});var Fi=l($o);os=r(Fi,"xlm-mlm-tlm-xnli15-1024"),Fi.forEach(a),as=r(di," (Masked language modeling + translation, XNLI languages)"),di.forEach(a),ts=p(k),Ae=s(k,"LI",{});var pi=l(Ae);Mo=s(pi,"CODE",{});var Hi=l(Mo);ss=r(Hi,"xlm-clm-enfr-1024"),Hi.forEach(a),ls=r(pi," (Causal language modeling, English-French)"),pi.forEach(a),is=p(k),Ne=s(k,"LI",{});var ci=l(Ne);xo=s(ci,"CODE",{});var Gi=l(xo);rs=r(Gi,"xlm-clm-ende-1024"),Gi.forEach(a),ns=r(ci," (Causal language modeling, English-German)"),ci.forEach(a),k.forEach(a),$a=p(e),$=s(e,"P",{});var te=l($);ms=r(te,"Os language embeddings s\xE3o representados por um tensor de mesma dimens\xE3o que os "),wo=s(te,"CODE",{});var Wi=l(wo);ds=r(Wi,"input_ids"),Wi.forEach(a),ps=r(te,` passados ao modelo.
Os valores destes tensores dependem do idioma utilizado e se identificam pelos atributos `),zo=s(te,"CODE",{});var Ui=l(zo);cs=r(Ui,"lang2id"),Ui.forEach(a),us=r(te," e "),yo=s(te,"CODE",{});var Ji=l(yo);fs=r(Ji,"id2lang"),Ji.forEach(a),gs=r(te," do tokenizador."),te.forEach(a),Ma=p(e),R=s(e,"P",{});var gt=l(R);hs=r(gt,"Neste exemplo, carregamos o checkpoint "),qo=s(gt,"CODE",{});var Ki=l(qo);_s=r(Ki,"xlm-clm-enfr-1024"),Ki.forEach(a),vs=r(gt,"(Causal language modeling, English-French):"),gt.forEach(a),xa=p(e),f(ue.$$.fragment,e),wa=p(e),F=s(e,"P",{});var ht=l(F);bs=r(ht,"O atributo "),jo=s(ht,"CODE",{});var Qi=l(jo);Es=r(Qi,"lang2id"),Qi.forEach(a),ks=r(ht," do tokenizador mostra os idiomas deste modelo e seus ids:"),ht.forEach(a),za=p(e),f(fe.$$.fragment,e),ya=p(e),Se=s(e,"P",{});var Vi=l(Se);$s=r(Vi,"Em seguida, cria-se um input de exemplo:"),Vi.forEach(a),qa=p(e),f(ge.$$.fragment,e),ja=p(e),M=s(e,"P",{});var se=l(M);Ms=r(se,"Estabelece-se o id do idioma, por exemplo "),Lo=s(se,"CODE",{});var Yi=l(Lo);xs=r(Yi,'"en"'),Yi.forEach(a),ws=r(se,`, e utiliza-se o mesmo para definir a language embedding.
A language embedding \xE9 um tensor preenchido com `),Oo=s(se,"CODE",{});var Zi=l(Oo);zs=r(Zi,"0"),Zi.forEach(a),ys=r(se,`, que \xE9 o id de idioma para o ingl\xEAs.
Este tensor deve ser do mesmo tamanho que os `),Co=s(se,"CODE",{});var er=l(Co);qs=r(er,"input_ids"),er.forEach(a),js=r(se,"."),se.forEach(a),La=p(e),f(he.$$.fragment,e),Oa=p(e),H=s(e,"P",{});var _t=l(H);Ls=r(_t,"Agora voc\xEA pode passar os "),To=s(_t,"CODE",{});var or=l(To);Os=r(or,"input_ids"),or.forEach(a),Cs=r(_t," e a language embedding ao modelo:"),_t.forEach(a),Ca=p(e),f(_e.$$.fragment,e),Ta=p(e),q=s(e,"P",{});var mo=l(q);Ts=r(mo,"O script "),ve=s(mo,"A",{href:!0,rel:!0});var ar=l(ve);Ps=r(ar,"run_generation.py"),ar.forEach(a),Ds=r(mo," pode gerar um texto com language embeddings utilizando os checkpoints "),Po=s(mo,"CODE",{});var tr=l(Po);Xs=r(tr,"xlm-clm"),tr.forEach(a),As=r(mo,"."),mo.forEach(a),Pa=p(e),P=s(e,"H3",{class:!0});var vt=l(P);G=s(vt,"A",{id:!0,class:!0,href:!0});var sr=l(G);Do=s(sr,"SPAN",{});var lr=l(Do);f(be.$$.fragment,lr),lr.forEach(a),sr.forEach(a),Ns=p(vt),Xo=s(vt,"SPAN",{});var ir=l(Xo);Ss=r(ir,"XLM sem language embeddings"),ir.forEach(a),vt.forEach(a),Da=p(e),Ie=s(e,"P",{});var rr=l(Ie);Is=r(rr,"Os seguintes modelos XLM n\xE3o requerem o uso de language embeddings durante a infer\xEAncia:"),rr.forEach(a),Xa=p(e),W=s(e,"UL",{});var bt=l(W);Be=s(bt,"LI",{});var ui=l(Be);Ao=s(ui,"CODE",{});var nr=l(Ao);Bs=r(nr,"xlm-mlm-17-1280"),nr.forEach(a),Rs=r(ui," (Modelagem de linguagem com m\xE1scara, 17 idiomas)"),ui.forEach(a),Fs=p(bt),Re=s(bt,"LI",{});var fi=l(Re);No=s(fi,"CODE",{});var mr=l(No);Hs=r(mr,"xlm-mlm-100-1280"),mr.forEach(a),Gs=r(fi," (Modelagem de linguagem com m\xE1scara, 100 idiomas)"),fi.forEach(a),bt.forEach(a),Aa=p(e),Fe=s(e,"P",{});var dr=l(Fe);Ws=r(dr,"Estes modelos s\xE3o utilizados para representa\xE7\xF5es gen\xE9ricas de frase diferentemente dos checkpoints XLM anteriores."),dr.forEach(a),Na=p(e),D=s(e,"H2",{class:!0});var Et=l(D);U=s(Et,"A",{id:!0,class:!0,href:!0});var pr=l(U);So=s(pr,"SPAN",{});var cr=l(So);f(Ee.$$.fragment,cr),cr.forEach(a),pr.forEach(a),Us=p(Et),Io=s(Et,"SPAN",{});var ur=l(Io);Js=r(ur,"BERT"),ur.forEach(a),Et.forEach(a),Sa=p(e),He=s(e,"P",{});var fr=l(He);Ks=r(fr,"Os seguintes modelos do BERT podem ser utilizados para tarefas multilingu\xEDsticas:"),fr.forEach(a),Ia=p(e),J=s(e,"UL",{});var kt=l(J);Ge=s(kt,"LI",{});var gi=l(Ge);Bo=s(gi,"CODE",{});var gr=l(Bo);Qs=r(gr,"bert-base-multilingual-uncased"),gr.forEach(a),Vs=r(gi," (Modelagem de linguagem com m\xE1scara + Previs\xE3o de frases, 102 idiomas)"),gi.forEach(a),Ys=p(kt),We=s(kt,"LI",{});var hi=l(We);Ro=s(hi,"CODE",{});var hr=l(Ro);Zs=r(hr,"bert-base-multilingual-cased"),hr.forEach(a),el=r(hi," (Modelagem de linguagem com m\xE1scara + Previs\xE3o de frases, 104 idiomas)"),hi.forEach(a),kt.forEach(a),Ba=p(e),Ue=s(e,"P",{});var _r=l(Ue);ol=r(_r,`Estes modelos n\xE3o requerem language embeddings durante a infer\xEAncia. Devem identificar a linguagem a partir
do contexto e realizar a infer\xEAncia em sequ\xEAncia.`),_r.forEach(a),Ra=p(e),X=s(e,"H2",{class:!0});var $t=l(X);K=s($t,"A",{id:!0,class:!0,href:!0});var vr=l(K);Fo=s(vr,"SPAN",{});var br=l(Fo);f(ke.$$.fragment,br),br.forEach(a),vr.forEach(a),al=p($t),Ho=s($t,"SPAN",{});var Er=l(Ho);tl=r(Er,"XLM-RoBERTa"),Er.forEach(a),$t.forEach(a),Fa=p(e),Je=s(e,"P",{});var kr=l(Je);sl=r(kr,"Os seguintes modelos do XLM-RoBERTa podem ser utilizados para tarefas multilingu\xEDsticas:"),kr.forEach(a),Ha=p(e),Q=s(e,"UL",{});var Mt=l(Q);Ke=s(Mt,"LI",{});var _i=l(Ke);Go=s(_i,"CODE",{});var $r=l(Go);ll=r($r,"xlm-roberta-base"),$r.forEach(a),il=r(_i," (Modelagem de linguagem com m\xE1scara, 100 idiomas)"),_i.forEach(a),rl=p(Mt),Qe=s(Mt,"LI",{});var vi=l(Qe);Wo=s(vi,"CODE",{});var Mr=l(Wo);nl=r(Mr,"xlm-roberta-large"),Mr.forEach(a),ml=r(vi," Modelagem de linguagem com m\xE1scara, 100 idiomas)"),vi.forEach(a),Mt.forEach(a),Ga=p(e),Ve=s(e,"P",{});var xr=l(Ve);dl=r(xr,`O XLM-RoBERTa foi treinado com 2,5 TB de dados do CommonCrawl rec\xE9m-criados e testados em 100 idiomas.
Proporciona fortes vantagens sobre os modelos multilingu\xEDsticos publicados anteriormente como o mBERT e o XLM em tarefas
subsequentes como a classifica\xE7\xE3o, a rotulagem de sequ\xEAncias e \xE0 respostas a perguntas.`),xr.forEach(a),Wa=p(e),A=s(e,"H2",{class:!0});var xt=l(A);V=s(xt,"A",{id:!0,class:!0,href:!0});var wr=l(V);Uo=s(wr,"SPAN",{});var zr=l(Uo);f($e.$$.fragment,zr),zr.forEach(a),wr.forEach(a),pl=p(xt),Jo=s(xt,"SPAN",{});var yr=l(Jo);cl=r(yr,"M2M100"),yr.forEach(a),xt.forEach(a),Ua=p(e),Ye=s(e,"P",{});var qr=l(Ye);ul=r(qr,"Os seguintes modelos de M2M100 podem ser utilizados para tradu\xE7\xF5es multilingu\xEDsticas:"),qr.forEach(a),Ja=p(e),Y=s(e,"UL",{});var wt=l(Y);Ze=s(wt,"LI",{});var bi=l(Ze);Ko=s(bi,"CODE",{});var jr=l(Ko);fl=r(jr,"facebook/m2m100_418M"),jr.forEach(a),gl=r(bi," (Tradu\xE7\xE3o)"),bi.forEach(a),hl=p(wt),eo=s(wt,"LI",{});var Ei=l(eo);Qo=s(Ei,"CODE",{});var Lr=l(Qo);_l=r(Lr,"facebook/m2m100_1.2B"),Lr.forEach(a),vl=r(Ei," (Tradu\xE7\xE3o)"),Ei.forEach(a),wt.forEach(a),Ka=p(e),Z=s(e,"P",{});var zt=l(Z);bl=r(zt,"Neste exemplo, o checkpoint "),Vo=s(zt,"CODE",{});var Or=l(Vo);El=r(Or,"facebook/m2m100_418M"),Or.forEach(a),kl=r(zt,` \xE9 carregado para traduzir do mandarim ao ingl\xEAs. \xC9 poss\xEDvel
estabelecer o idioma de origem no tokenizador:`),zt.forEach(a),Qa=p(e),f(Me.$$.fragment,e),Va=p(e),oo=s(e,"P",{});var Cr=l(oo);$l=r(Cr,"Tokeniza\xE7\xE3o do texto:"),Cr.forEach(a),Ya=p(e),f(xe.$$.fragment,e),Za=p(e),x=s(e,"P",{});var le=l(x);Ml=r(le,`O M2M100 for\xE7a o id do idioma de destino como o primeiro token gerado para traduzir ao idioma de destino.
\xC9 definido o `),Yo=s(le,"CODE",{});var Tr=l(Yo);xl=r(Tr,"forced_bos_token_id"),Tr.forEach(a),wl=r(le," como "),Zo=s(le,"CODE",{});var Pr=l(Zo);zl=r(Pr,"en"),Pr.forEach(a),yl=r(le," no m\xE9todo "),ea=s(le,"CODE",{});var Dr=l(ea);ql=r(Dr,"generate"),Dr.forEach(a),jl=r(le," para traduzir ao ingl\xEAs."),le.forEach(a),et=p(e),f(we.$$.fragment,e),ot=p(e),N=s(e,"H2",{class:!0});var yt=l(N);ee=s(yt,"A",{id:!0,class:!0,href:!0});var Xr=l(ee);oa=s(Xr,"SPAN",{});var Ar=l(oa);f(ze.$$.fragment,Ar),Ar.forEach(a),Xr.forEach(a),Ll=p(yt),aa=s(yt,"SPAN",{});var Nr=l(aa);Ol=r(Nr,"MBart"),Nr.forEach(a),yt.forEach(a),at=p(e),ao=s(e,"P",{});var Sr=l(ao);Cl=r(Sr,"Os seguintes modelos do MBart podem ser utilizados para tradu\xE7\xE3o multilingu\xEDstica:"),Sr.forEach(a),tt=p(e),E=s(e,"UL",{});var j=l(E);to=s(j,"LI",{});var ki=l(to);ta=s(ki,"CODE",{});var Ir=l(ta);Tl=r(Ir,"facebook/mbart-large-50-one-to-many-mmt"),Ir.forEach(a),Pl=r(ki," (Tradu\xE7\xE3o autom\xE1tica multilingu\xEDstica de um a v\xE1rios, 50 idiomas)"),ki.forEach(a),Dl=p(j),so=s(j,"LI",{});var $i=l(so);sa=s($i,"CODE",{});var Br=l(sa);Xl=r(Br,"facebook/mbart-large-50-many-to-many-mmt"),Br.forEach(a),Al=r($i," (Tradu\xE7\xE3o autom\xE1tica multilingu\xEDstica de v\xE1rios a v\xE1rios, 50 idiomas)"),$i.forEach(a),Nl=p(j),lo=s(j,"LI",{});var Mi=l(lo);la=s(Mi,"CODE",{});var Rr=l(la);Sl=r(Rr,"facebook/mbart-large-50-many-to-one-mmt"),Rr.forEach(a),Il=r(Mi," (Tradu\xE7\xE3o autom\xE1tica multilingu\xEDstica v\xE1rios a um, 50 idiomas)"),Mi.forEach(a),Bl=p(j),io=s(j,"LI",{});var xi=l(io);ia=s(xi,"CODE",{});var Fr=l(ia);Rl=r(Fr,"facebook/mbart-large-50"),Fr.forEach(a),Fl=r(xi," (Tradu\xE7\xE3o multilingu\xEDstica, 50 idiomas)"),xi.forEach(a),Hl=p(j),ra=s(j,"LI",{});var Hr=l(ra);na=s(Hr,"CODE",{});var Gr=l(na);Gl=r(Gr,"facebook/mbart-large-cc25"),Gr.forEach(a),Hr.forEach(a),j.forEach(a),st=p(e),oe=s(e,"P",{});var qt=l(oe);Wl=r(qt,"Neste exemplo, carrega-se o checkpoint "),ma=s(qt,"CODE",{});var Wr=l(ma);Ul=r(Wr,"facebook/mbart-large-50-many-to-many-mmt"),Wr.forEach(a),Jl=r(qt,` para traduzir do finland\xEAs ao ingl\xEAs.
Pode-se definir o idioma de origem no tokenizador:`),qt.forEach(a),lt=p(e),f(ye.$$.fragment,e),it=p(e),ro=s(e,"P",{});var Ur=l(ro);Kl=r(Ur,"Tokenizando o texto:"),Ur.forEach(a),rt=p(e),f(qe.$$.fragment,e),nt=p(e),w=s(e,"P",{});var ie=l(w);Ql=r(ie,`O MBart for\xE7a o id do idioma de destino como o primeiro token gerado para traduzir ao idioma de destino.
\xC9 definido o `),da=s(ie,"CODE",{});var Jr=l(da);Vl=r(Jr,"forced_bos_token_id"),Jr.forEach(a),Yl=r(ie," como "),pa=s(ie,"CODE",{});var Kr=l(pa);Zl=r(Kr,"en"),Kr.forEach(a),ei=r(ie," no m\xE9todo "),ca=s(ie,"CODE",{});var Qr=l(ca);oi=r(Qr,"generate"),Qr.forEach(a),ai=r(ie," para traduzir ao ingl\xEAs."),ie.forEach(a),mt=p(e),f(je.$$.fragment,e),dt=p(e),ae=s(e,"P",{});var jt=l(ae);ti=r(jt,"Se estiver usando o checkpoint "),ua=s(jt,"CODE",{});var Vr=l(ua);si=r(Vr,"facebook/mbart-large-50-many-to-one-mmt"),Vr.forEach(a),li=r(jt,` n\xE3o ser\xE1 necess\xE1rio for\xE7ar o id do idioma de destino
como sendo o primeiro token generado, caso contr\xE1rio a usagem \xE9 a mesma.`),jt.forEach(a),this.h()},h(){c(L,"name","hf:doc:metadata"),c(L,"content",JSON.stringify(rn)),c(S,"id","modelos-multilingusticos-para-inferncia"),c(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(S,"href","#modelos-multilingusticos-para-inferncia"),c(O,"class","relative group"),c(de,"href","https://huggingface.co/bert-base-multilingual-uncased"),c(de,"rel","nofollow"),c(I,"id","xlm"),c(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(I,"href","#xlm"),c(C,"class","relative group"),c(B,"id","xlm-com-language-embeddings"),c(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(B,"href","#xlm-com-language-embeddings"),c(T,"class","relative group"),c(ve,"href","https://github.com/huggingface/transformers/tree/master/examples/pytorch/text-generation/run_generation.py"),c(ve,"rel","nofollow"),c(G,"id","xlm-sem-language-embeddings"),c(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(G,"href","#xlm-sem-language-embeddings"),c(P,"class","relative group"),c(U,"id","bert"),c(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(U,"href","#bert"),c(D,"class","relative group"),c(K,"id","xlmroberta"),c(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(K,"href","#xlmroberta"),c(X,"class","relative group"),c(V,"id","m2m100"),c(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(V,"href","#m2m100"),c(A,"class","relative group"),c(ee,"id","mbart"),c(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ee,"href","#mbart"),c(N,"class","relative group")},m(e,n){o(document.head,L),m(e,fa,n),m(e,O,n),o(O,S),o(S,po),g(ne,po,null),o(O,Lt),o(O,co),o(co,Ot),m(e,ga,n),g(me,e,n),m(e,ha,n),m(e,y,n),o(y,Ct),o(y,uo),o(uo,Tt),o(y,Pt),o(y,de),o(de,Dt),o(y,Xt),m(e,_a,n),m(e,C,n),o(C,I),o(I,fo),g(pe,fo,null),o(C,At),o(C,go),o(go,Nt),m(e,va,n),m(e,Le,n),o(Le,St),m(e,ba,n),m(e,T,n),o(T,B),o(B,ho),g(ce,ho,null),o(T,It),o(T,_o),o(_o,Bt),m(e,Ea,n),m(e,Oe,n),o(Oe,Rt),m(e,ka,n),m(e,b,n),o(b,Ce),o(Ce,vo),o(vo,Ft),o(Ce,Ht),o(b,Gt),o(b,Te),o(Te,bo),o(bo,Wt),o(Te,Ut),o(b,Jt),o(b,Pe),o(Pe,Eo),o(Eo,Kt),o(Pe,Qt),o(b,Vt),o(b,De),o(De,ko),o(ko,Yt),o(De,Zt),o(b,es),o(b,Xe),o(Xe,$o),o($o,os),o(Xe,as),o(b,ts),o(b,Ae),o(Ae,Mo),o(Mo,ss),o(Ae,ls),o(b,is),o(b,Ne),o(Ne,xo),o(xo,rs),o(Ne,ns),m(e,$a,n),m(e,$,n),o($,ms),o($,wo),o(wo,ds),o($,ps),o($,zo),o(zo,cs),o($,us),o($,yo),o(yo,fs),o($,gs),m(e,Ma,n),m(e,R,n),o(R,hs),o(R,qo),o(qo,_s),o(R,vs),m(e,xa,n),g(ue,e,n),m(e,wa,n),m(e,F,n),o(F,bs),o(F,jo),o(jo,Es),o(F,ks),m(e,za,n),g(fe,e,n),m(e,ya,n),m(e,Se,n),o(Se,$s),m(e,qa,n),g(ge,e,n),m(e,ja,n),m(e,M,n),o(M,Ms),o(M,Lo),o(Lo,xs),o(M,ws),o(M,Oo),o(Oo,zs),o(M,ys),o(M,Co),o(Co,qs),o(M,js),m(e,La,n),g(he,e,n),m(e,Oa,n),m(e,H,n),o(H,Ls),o(H,To),o(To,Os),o(H,Cs),m(e,Ca,n),g(_e,e,n),m(e,Ta,n),m(e,q,n),o(q,Ts),o(q,ve),o(ve,Ps),o(q,Ds),o(q,Po),o(Po,Xs),o(q,As),m(e,Pa,n),m(e,P,n),o(P,G),o(G,Do),g(be,Do,null),o(P,Ns),o(P,Xo),o(Xo,Ss),m(e,Da,n),m(e,Ie,n),o(Ie,Is),m(e,Xa,n),m(e,W,n),o(W,Be),o(Be,Ao),o(Ao,Bs),o(Be,Rs),o(W,Fs),o(W,Re),o(Re,No),o(No,Hs),o(Re,Gs),m(e,Aa,n),m(e,Fe,n),o(Fe,Ws),m(e,Na,n),m(e,D,n),o(D,U),o(U,So),g(Ee,So,null),o(D,Us),o(D,Io),o(Io,Js),m(e,Sa,n),m(e,He,n),o(He,Ks),m(e,Ia,n),m(e,J,n),o(J,Ge),o(Ge,Bo),o(Bo,Qs),o(Ge,Vs),o(J,Ys),o(J,We),o(We,Ro),o(Ro,Zs),o(We,el),m(e,Ba,n),m(e,Ue,n),o(Ue,ol),m(e,Ra,n),m(e,X,n),o(X,K),o(K,Fo),g(ke,Fo,null),o(X,al),o(X,Ho),o(Ho,tl),m(e,Fa,n),m(e,Je,n),o(Je,sl),m(e,Ha,n),m(e,Q,n),o(Q,Ke),o(Ke,Go),o(Go,ll),o(Ke,il),o(Q,rl),o(Q,Qe),o(Qe,Wo),o(Wo,nl),o(Qe,ml),m(e,Ga,n),m(e,Ve,n),o(Ve,dl),m(e,Wa,n),m(e,A,n),o(A,V),o(V,Uo),g($e,Uo,null),o(A,pl),o(A,Jo),o(Jo,cl),m(e,Ua,n),m(e,Ye,n),o(Ye,ul),m(e,Ja,n),m(e,Y,n),o(Y,Ze),o(Ze,Ko),o(Ko,fl),o(Ze,gl),o(Y,hl),o(Y,eo),o(eo,Qo),o(Qo,_l),o(eo,vl),m(e,Ka,n),m(e,Z,n),o(Z,bl),o(Z,Vo),o(Vo,El),o(Z,kl),m(e,Qa,n),g(Me,e,n),m(e,Va,n),m(e,oo,n),o(oo,$l),m(e,Ya,n),g(xe,e,n),m(e,Za,n),m(e,x,n),o(x,Ml),o(x,Yo),o(Yo,xl),o(x,wl),o(x,Zo),o(Zo,zl),o(x,yl),o(x,ea),o(ea,ql),o(x,jl),m(e,et,n),g(we,e,n),m(e,ot,n),m(e,N,n),o(N,ee),o(ee,oa),g(ze,oa,null),o(N,Ll),o(N,aa),o(aa,Ol),m(e,at,n),m(e,ao,n),o(ao,Cl),m(e,tt,n),m(e,E,n),o(E,to),o(to,ta),o(ta,Tl),o(to,Pl),o(E,Dl),o(E,so),o(so,sa),o(sa,Xl),o(so,Al),o(E,Nl),o(E,lo),o(lo,la),o(la,Sl),o(lo,Il),o(E,Bl),o(E,io),o(io,ia),o(ia,Rl),o(io,Fl),o(E,Hl),o(E,ra),o(ra,na),o(na,Gl),m(e,st,n),m(e,oe,n),o(oe,Wl),o(oe,ma),o(ma,Ul),o(oe,Jl),m(e,lt,n),g(ye,e,n),m(e,it,n),m(e,ro,n),o(ro,Kl),m(e,rt,n),g(qe,e,n),m(e,nt,n),m(e,w,n),o(w,Ql),o(w,da),o(da,Vl),o(w,Yl),o(w,pa),o(pa,Zl),o(w,ei),o(w,ca),o(ca,oi),o(w,ai),m(e,mt,n),g(je,e,n),m(e,dt,n),m(e,ae,n),o(ae,ti),o(ae,ua),o(ua,si),o(ae,li),pt=!0},p:an,i(e){pt||(h(ne.$$.fragment,e),h(me.$$.fragment,e),h(pe.$$.fragment,e),h(ce.$$.fragment,e),h(ue.$$.fragment,e),h(fe.$$.fragment,e),h(ge.$$.fragment,e),h(he.$$.fragment,e),h(_e.$$.fragment,e),h(be.$$.fragment,e),h(Ee.$$.fragment,e),h(ke.$$.fragment,e),h($e.$$.fragment,e),h(Me.$$.fragment,e),h(xe.$$.fragment,e),h(we.$$.fragment,e),h(ze.$$.fragment,e),h(ye.$$.fragment,e),h(qe.$$.fragment,e),h(je.$$.fragment,e),pt=!0)},o(e){_(ne.$$.fragment,e),_(me.$$.fragment,e),_(pe.$$.fragment,e),_(ce.$$.fragment,e),_(ue.$$.fragment,e),_(fe.$$.fragment,e),_(ge.$$.fragment,e),_(he.$$.fragment,e),_(_e.$$.fragment,e),_(be.$$.fragment,e),_(Ee.$$.fragment,e),_(ke.$$.fragment,e),_($e.$$.fragment,e),_(Me.$$.fragment,e),_(xe.$$.fragment,e),_(we.$$.fragment,e),_(ze.$$.fragment,e),_(ye.$$.fragment,e),_(qe.$$.fragment,e),_(je.$$.fragment,e),pt=!1},d(e){a(L),e&&a(fa),e&&a(O),v(ne),e&&a(ga),v(me,e),e&&a(ha),e&&a(y),e&&a(_a),e&&a(C),v(pe),e&&a(va),e&&a(Le),e&&a(ba),e&&a(T),v(ce),e&&a(Ea),e&&a(Oe),e&&a(ka),e&&a(b),e&&a($a),e&&a($),e&&a(Ma),e&&a(R),e&&a(xa),v(ue,e),e&&a(wa),e&&a(F),e&&a(za),v(fe,e),e&&a(ya),e&&a(Se),e&&a(qa),v(ge,e),e&&a(ja),e&&a(M),e&&a(La),v(he,e),e&&a(Oa),e&&a(H),e&&a(Ca),v(_e,e),e&&a(Ta),e&&a(q),e&&a(Pa),e&&a(P),v(be),e&&a(Da),e&&a(Ie),e&&a(Xa),e&&a(W),e&&a(Aa),e&&a(Fe),e&&a(Na),e&&a(D),v(Ee),e&&a(Sa),e&&a(He),e&&a(Ia),e&&a(J),e&&a(Ba),e&&a(Ue),e&&a(Ra),e&&a(X),v(ke),e&&a(Fa),e&&a(Je),e&&a(Ha),e&&a(Q),e&&a(Ga),e&&a(Ve),e&&a(Wa),e&&a(A),v($e),e&&a(Ua),e&&a(Ye),e&&a(Ja),e&&a(Y),e&&a(Ka),e&&a(Z),e&&a(Qa),v(Me,e),e&&a(Va),e&&a(oo),e&&a(Ya),v(xe,e),e&&a(Za),e&&a(x),e&&a(et),v(we,e),e&&a(ot),e&&a(N),v(ze),e&&a(at),e&&a(ao),e&&a(tt),e&&a(E),e&&a(st),e&&a(oe),e&&a(lt),v(ye,e),e&&a(it),e&&a(ro),e&&a(rt),v(qe,e),e&&a(nt),e&&a(w),e&&a(mt),v(je,e),e&&a(dt),e&&a(ae)}}}const rn={local:"modelos-multilingusticos-para-inferncia",sections:[{local:"xlm",sections:[{local:"xlm-com-language-embeddings",title:"XLM com language embeddings"},{local:"xlm-sem-language-embeddings",title:"XLM sem language embeddings"}],title:"XLM"},{local:"bert",title:"BERT"},{local:"xlmroberta",title:"XLM-RoBERTa"},{local:"m2m100",title:"M2M100"},{local:"mbart",title:"MBart"}],title:"Modelos multilingu\xEDsticos para infer\xEAncia"};function nn(wi){return tn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class un extends Yr{constructor(L){super();Zr(this,L,nn,ln,en,{})}}export{un as default,rn as metadata};
