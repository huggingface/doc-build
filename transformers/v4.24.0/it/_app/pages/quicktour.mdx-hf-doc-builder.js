import{S as yp,i as Sp,s as Mp,e as r,k as d,w as k,t as l,M as Dp,c as n,d as a,m as $,a as p,x as b,h as i,b as z,G as t,g as u,y as E,q as w,o as j,B as A,v as xp,L as Pe}from"../chunks/vendor-hf-doc-builder.js";import{T as va}from"../chunks/Tip-hf-doc-builder.js";import{Y as Pp}from"../chunks/Youtube-hf-doc-builder.js";import{I as ft}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as U}from"../chunks/CodeBlock-hf-doc-builder.js";import{D as Fp}from"../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as _a,M as ge}from"../chunks/Markdown-hf-doc-builder.js";import"../chunks/IconTensorflow-hf-doc-builder.js";function Op(y){let o,m;return{c(){o=r("p"),m=l(`Tutti gli esempi di codice presenti in questa documentazione hanno un pulsante in alto a sinistra che permette di selezionare tra PyTorch e TensorFlow. Se
questo non \xE8 presente, ci si aspetta che il codice funzioni per entrambi i backend senza alcun cambiamento.`)},l(s){o=n(s,"P",{});var f=p(o);m=i(f,`Tutti gli esempi di codice presenti in questa documentazione hanno un pulsante in alto a sinistra che permette di selezionare tra PyTorch e TensorFlow. Se
questo non \xE8 presente, ci si aspetta che il codice funzioni per entrambi i backend senza alcun cambiamento.`),f.forEach(a)},m(s,f){u(s,o,f),t(o,m)},d(s){s&&a(o)}}}function Ip(y){let o,m,s,f,h,_,P,x;return{c(){o=r("p"),m=l("Per maggiori dettagli legati alla "),s=r("code"),f=l("pipeline()"),h=l(" e ai compiti ad essa associati, fai riferimento alla documentazione "),_=r("a"),P=l("qui"),x=l("."),this.h()},l(q){o=n(q,"P",{});var M=p(o);m=i(M,"Per maggiori dettagli legati alla "),s=n(M,"CODE",{});var F=p(s);f=i(F,"pipeline()"),F.forEach(a),h=i(M," e ai compiti ad essa associati, fai riferimento alla documentazione "),_=n(M,"A",{href:!0});var O=p(_);P=i(O,"qui"),O.forEach(a),x=i(M,"."),M.forEach(a),this.h()},h(){z(_,"href","./main_classes/pipelines")},m(q,M){u(q,o,M),t(o,m),t(o,s),t(s,f),t(o,h),t(o,_),t(_,P),t(o,x)},d(q){q&&a(o)}}}function Lp(y){let o,m;return o=new U({props:{code:"pip install torch",highlighted:"pip install torch"}}),{c(){k(o.$$.fragment)},l(s){b(o.$$.fragment,s)},m(s,f){E(o,s,f),m=!0},p:Pe,i(s){m||(w(o.$$.fragment,s),m=!0)},o(s){j(o.$$.fragment,s),m=!1},d(s){A(o,s)}}}function Np(y){let o,m;return o=new ge({props:{$$slots:{default:[Lp]},$$scope:{ctx:y}}}),{c(){k(o.$$.fragment)},l(s){b(o.$$.fragment,s)},m(s,f){E(o,s,f),m=!0},p(s,f){const h={};f&2&&(h.$$scope={dirty:f,ctx:s}),o.$set(h)},i(s){m||(w(o.$$.fragment,s),m=!0)},o(s){j(o.$$.fragment,s),m=!1},d(s){A(o,s)}}}function Rp(y){let o,m;return o=new U({props:{code:"pip install tensorflow",highlighted:"pip install tensorflow"}}),{c(){k(o.$$.fragment)},l(s){b(o.$$.fragment,s)},m(s,f){E(o,s,f),m=!0},p:Pe,i(s){m||(w(o.$$.fragment,s),m=!0)},o(s){j(o.$$.fragment,s),m=!1},d(s){A(o,s)}}}function Up(y){let o,m;return o=new ge({props:{$$slots:{default:[Rp]},$$scope:{ctx:y}}}),{c(){k(o.$$.fragment)},l(s){b(o.$$.fragment,s)},m(s,f){E(o,s,f),m=!0},p(s,f){const h={};f&2&&(h.$$scope={dirty:f,ctx:s}),o.$set(h)},i(s){m||(w(o.$$.fragment,s),m=!0)},o(s){j(o.$$.fragment,s),m=!1},d(s){A(o,s)}}}function Gp(y){let o,m,s,f,h,_,P,x,q,M,F,O,L,R;return L=new U({props:{code:`from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),{c(){o=r("p"),m=l("Usa "),s=r("code"),f=l("AutoModelForSequenceClassification"),h=l(" e "),_=r("code"),P=l("AutoTokenizer"),x=l(" per caricare il modello pre-allenato e il suo tokenizer associato (maggiori informazioni su una "),q=r("code"),M=l("AutoClass"),F=l(" in seguito):"),O=d(),k(L.$$.fragment)},l(T){o=n(T,"P",{});var D=p(o);m=i(D,"Usa "),s=n(D,"CODE",{});var g=p(s);f=i(g,"AutoModelForSequenceClassification"),g.forEach(a),h=i(D," e "),_=n(D,"CODE",{});var S=p(_);P=i(S,"AutoTokenizer"),S.forEach(a),x=i(D," per caricare il modello pre-allenato e il suo tokenizer associato (maggiori informazioni su una "),q=n(D,"CODE",{});var Q=p(q);M=i(Q,"AutoClass"),Q.forEach(a),F=i(D," in seguito):"),D.forEach(a),O=$(T),b(L.$$.fragment,T)},m(T,D){u(T,o,D),t(o,m),t(o,s),t(s,f),t(o,h),t(o,_),t(_,P),t(o,x),t(o,q),t(q,M),t(o,F),u(T,O,D),E(L,T,D),R=!0},p:Pe,i(T){R||(w(L.$$.fragment,T),R=!0)},o(T){j(L.$$.fragment,T),R=!1},d(T){T&&a(o),T&&a(O),A(L,T)}}}function Qp(y){let o,m;return o=new ge({props:{$$slots:{default:[Gp]},$$scope:{ctx:y}}}),{c(){k(o.$$.fragment)},l(s){b(o.$$.fragment,s)},m(s,f){E(o,s,f),m=!0},p(s,f){const h={};f&2&&(h.$$scope={dirty:f,ctx:s}),o.$set(h)},i(s){m||(w(o.$$.fragment,s),m=!0)},o(s){j(o.$$.fragment,s),m=!1},d(s){A(o,s)}}}function Hp(y){let o,m,s,f,h,_,P,x,q,M,F,O,L,R;return L=new U({props:{code:`from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),{c(){o=r("p"),m=l("Usa "),s=r("code"),f=l("TFAutoModelForSequenceClassification"),h=l(" e "),_=r("code"),P=l("AutoTokenizer"),x=l(" per caricare il modello pre-allenato e il suo tokenizer associato (maggiori informazioni su una "),q=r("code"),M=l("TFAutoClass"),F=l(" in seguito):"),O=d(),k(L.$$.fragment)},l(T){o=n(T,"P",{});var D=p(o);m=i(D,"Usa "),s=n(D,"CODE",{});var g=p(s);f=i(g,"TFAutoModelForSequenceClassification"),g.forEach(a),h=i(D," e "),_=n(D,"CODE",{});var S=p(_);P=i(S,"AutoTokenizer"),S.forEach(a),x=i(D," per caricare il modello pre-allenato e il suo tokenizer associato (maggiori informazioni su una "),q=n(D,"CODE",{});var Q=p(q);M=i(Q,"TFAutoClass"),Q.forEach(a),F=i(D," in seguito):"),D.forEach(a),O=$(T),b(L.$$.fragment,T)},m(T,D){u(T,o,D),t(o,m),t(o,s),t(s,f),t(o,h),t(o,_),t(_,P),t(o,x),t(o,q),t(q,M),t(o,F),u(T,O,D),E(L,T,D),R=!0},p:Pe,i(T){R||(w(L.$$.fragment,T),R=!0)},o(T){j(L.$$.fragment,T),R=!1},d(T){T&&a(o),T&&a(O),A(L,T)}}}function Bp(y){let o,m;return o=new ge({props:{$$slots:{default:[Hp]},$$scope:{ctx:y}}}),{c(){k(o.$$.fragment)},l(s){b(o.$$.fragment,s)},m(s,f){E(o,s,f),m=!0},p(s,f){const h={};f&2&&(h.$$scope={dirty:f,ctx:s}),o.$set(h)},i(s){m||(w(o.$$.fragment,s),m=!0)},o(s){j(o.$$.fragment,s),m=!1},d(s){A(o,s)}}}function Kp(y){let o,m;return o=new U({props:{code:`pt_batch = tokenizer(
    ["Siamo molto felici di mostrarti la libreria \u{1F917} Transformers.", "Speriamo te non la odierai."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;Siamo molto felici di mostrarti la libreria \u{1F917} Transformers.&quot;</span>, <span class="hljs-string">&quot;Speriamo te non la odierai.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`}}),{c(){k(o.$$.fragment)},l(s){b(o.$$.fragment,s)},m(s,f){E(o,s,f),m=!0},p:Pe,i(s){m||(w(o.$$.fragment,s),m=!0)},o(s){j(o.$$.fragment,s),m=!1},d(s){A(o,s)}}}function Vp(y){let o,m;return o=new ge({props:{$$slots:{default:[Kp]},$$scope:{ctx:y}}}),{c(){k(o.$$.fragment)},l(s){b(o.$$.fragment,s)},m(s,f){E(o,s,f),m=!0},p(s,f){const h={};f&2&&(h.$$scope={dirty:f,ctx:s}),o.$set(h)},i(s){m||(w(o.$$.fragment,s),m=!0)},o(s){j(o.$$.fragment,s),m=!1},d(s){A(o,s)}}}function Yp(y){let o,m;return o=new U({props:{code:`tf_batch = tokenizer(
    ["Siamo molto felici di mostrarti la libreria \u{1F917} Transformers.", "Speriamo te non la odierai."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;Siamo molto felici di mostrarti la libreria \u{1F917} Transformers.&quot;</span>, <span class="hljs-string">&quot;Speriamo te non la odierai.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`}}),{c(){k(o.$$.fragment)},l(s){b(o.$$.fragment,s)},m(s,f){E(o,s,f),m=!0},p:Pe,i(s){m||(w(o.$$.fragment,s),m=!0)},o(s){j(o.$$.fragment,s),m=!1},d(s){A(o,s)}}}function Jp(y){let o,m;return o=new ge({props:{$$slots:{default:[Yp]},$$scope:{ctx:y}}}),{c(){k(o.$$.fragment)},l(s){b(o.$$.fragment,s)},m(s,f){E(o,s,f),m=!0},p(s,f){const h={};f&2&&(h.$$scope={dirty:f,ctx:s}),o.$set(h)},i(s){m||(w(o.$$.fragment,s),m=!0)},o(s){j(o.$$.fragment,s),m=!1},d(s){A(o,s)}}}function Zp(y){let o,m,s,f,h,_,P,x;return{c(){o=r("p"),m=l("Guarda il "),s=r("a"),f=l("task summary"),h=l(" per sapere quale classe di "),_=r("code"),P=l("AutoModel"),x=l(" utilizzare per quale compito."),this.h()},l(q){o=n(q,"P",{});var M=p(o);m=i(M,"Guarda il "),s=n(M,"A",{href:!0});var F=p(s);f=i(F,"task summary"),F.forEach(a),h=i(M," per sapere quale classe di "),_=n(M,"CODE",{});var O=p(_);P=i(O,"AutoModel"),O.forEach(a),x=i(M," utilizzare per quale compito."),M.forEach(a),this.h()},h(){z(s,"href","./task_summary")},m(q,M){u(q,o,M),t(o,m),t(o,s),t(s,f),t(o,h),t(o,_),t(_,P),t(o,x)},d(q){q&&a(o)}}}function Wp(y){let o,m,s,f,h,_,P,x,q,M,F,O,L,R,T,D,g,S,Q,G,V,K,te,Y,H,ee,J,Z,fe,se,$e,le,ae,ie,he,C,I,re;return D=new U({props:{code:`from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`}}),S=new va({props:{$$slots:{default:[Zp]},$$scope:{ctx:y}}}),ee=new U({props:{code:"pt_outputs = pt_model(**pt_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)'}}),I=new U({props:{code:`from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">0.0041</span>, <span class="hljs-number">0.0037</span>, <span class="hljs-number">0.0203</span>, <span class="hljs-number">0.2005</span>, <span class="hljs-number">0.7713</span>],
        [<span class="hljs-number">0.3766</span>, <span class="hljs-number">0.3292</span>, <span class="hljs-number">0.1832</span>, <span class="hljs-number">0.0558</span>, <span class="hljs-number">0.0552</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)`}}),{c(){o=r("p"),m=l("\u{1F917} Transformers fornisce un metodo semplice e unificato per caricare istanze pre-allenate. Questo significa che puoi caricare un "),s=r("code"),f=l("AutoModel"),h=l(" come caricheresti un "),_=r("code"),P=l("AutoTokenizer"),x=l(". L\u2019unica differenza \xE8 selezionare l\u2019"),q=r("code"),M=l("AutoModel"),F=l(" corretto per il compito di interesse. Dato che stai facendo classificazione di testi, o sequenze, carica "),O=r("code"),L=l("AutoModelForSequenceClassification"),R=l(":"),T=d(),k(D.$$.fragment),g=d(),k(S.$$.fragment),Q=d(),G=r("p"),V=l("Ora puoi passare il tuo lotto di input pre-processati direttamente al modello. Devi solo spacchettare il dizionario aggiungendo "),K=r("code"),te=l("**"),Y=l(":"),H=d(),k(ee.$$.fragment),J=d(),Z=r("p"),fe=l("Il modello produrr\xE0 le attivazioni finali nell\u2019attributo "),se=r("code"),$e=l("logits"),le=l(". Applica la funzione softmax a "),ae=r("code"),ie=l("logits"),he=l(" per ottenere le probabilit\xE0:"),C=d(),k(I.$$.fragment)},l(v){o=n(v,"P",{});var N=p(o);m=i(N,"\u{1F917} Transformers fornisce un metodo semplice e unificato per caricare istanze pre-allenate. Questo significa che puoi caricare un "),s=n(N,"CODE",{});var pe=p(s);f=i(pe,"AutoModel"),pe.forEach(a),h=i(N," come caricheresti un "),_=n(N,"CODE",{});var ye=p(_);P=i(ye,"AutoTokenizer"),ye.forEach(a),x=i(N,". L\u2019unica differenza \xE8 selezionare l\u2019"),q=n(N,"CODE",{});var de=p(q);M=i(de,"AutoModel"),de.forEach(a),F=i(N," corretto per il compito di interesse. Dato che stai facendo classificazione di testi, o sequenze, carica "),O=n(N,"CODE",{});var _e=p(O);L=i(_e,"AutoModelForSequenceClassification"),_e.forEach(a),R=i(N,":"),N.forEach(a),T=$(v),b(D.$$.fragment,v),g=$(v),b(S.$$.fragment,v),Q=$(v),G=n(v,"P",{});var ne=p(G);V=i(ne,"Ora puoi passare il tuo lotto di input pre-processati direttamente al modello. Devi solo spacchettare il dizionario aggiungendo "),K=n(ne,"CODE",{});var Le=p(K);te=i(Le,"**"),Le.forEach(a),Y=i(ne,":"),ne.forEach(a),H=$(v),b(ee.$$.fragment,v),J=$(v),Z=n(v,"P",{});var ve=p(Z);fe=i(ve,"Il modello produrr\xE0 le attivazioni finali nell\u2019attributo "),se=n(ve,"CODE",{});var Kt=p(se);$e=i(Kt,"logits"),Kt.forEach(a),le=i(ve,". Applica la funzione softmax a "),ae=n(ve,"CODE",{});var dt=p(ae);ie=i(dt,"logits"),dt.forEach(a),he=i(ve," per ottenere le probabilit\xE0:"),ve.forEach(a),C=$(v),b(I.$$.fragment,v)},m(v,N){u(v,o,N),t(o,m),t(o,s),t(s,f),t(o,h),t(o,_),t(_,P),t(o,x),t(o,q),t(q,M),t(o,F),t(o,O),t(O,L),t(o,R),u(v,T,N),E(D,v,N),u(v,g,N),E(S,v,N),u(v,Q,N),u(v,G,N),t(G,V),t(G,K),t(K,te),t(G,Y),u(v,H,N),E(ee,v,N),u(v,J,N),u(v,Z,N),t(Z,fe),t(Z,se),t(se,$e),t(Z,le),t(Z,ae),t(ae,ie),t(Z,he),u(v,C,N),E(I,v,N),re=!0},p(v,N){const pe={};N&2&&(pe.$$scope={dirty:N,ctx:v}),S.$set(pe)},i(v){re||(w(D.$$.fragment,v),w(S.$$.fragment,v),w(ee.$$.fragment,v),w(I.$$.fragment,v),re=!0)},o(v){j(D.$$.fragment,v),j(S.$$.fragment,v),j(ee.$$.fragment,v),j(I.$$.fragment,v),re=!1},d(v){v&&a(o),v&&a(T),A(D,v),v&&a(g),A(S,v),v&&a(Q),v&&a(G),v&&a(H),A(ee,v),v&&a(J),v&&a(Z),v&&a(C),A(I,v)}}}function Xp(y){let o,m;return o=new ge({props:{$$slots:{default:[Wp]},$$scope:{ctx:y}}}),{c(){k(o.$$.fragment)},l(s){b(o.$$.fragment,s)},m(s,f){E(o,s,f),m=!0},p(s,f){const h={};f&2&&(h.$$scope={dirty:f,ctx:s}),o.$set(h)},i(s){m||(w(o.$$.fragment,s),m=!0)},o(s){j(o.$$.fragment,s),m=!1},d(s){A(o,s)}}}function ec(y){let o,m,s,f,h,_,P,x;return{c(){o=r("p"),m=l("Guarda il "),s=r("a"),f=l("task summary"),h=l(" per sapere quale classe di "),_=r("code"),P=l("AutoModel"),x=l(" utilizzare per quale compito."),this.h()},l(q){o=n(q,"P",{});var M=p(o);m=i(M,"Guarda il "),s=n(M,"A",{href:!0});var F=p(s);f=i(F,"task summary"),F.forEach(a),h=i(M," per sapere quale classe di "),_=n(M,"CODE",{});var O=p(_);P=i(O,"AutoModel"),O.forEach(a),x=i(M," utilizzare per quale compito."),M.forEach(a),this.h()},h(){z(s,"href","./task_summary")},m(q,M){u(q,o,M),t(o,m),t(o,s),t(s,f),t(o,h),t(o,_),t(_,P),t(o,x)},d(q){q&&a(o)}}}function tc(y){let o,m,s,f,h,_,P,x,q,M,F,O,L,R,T,D,g,S,Q,G,V,K,te,Y,H,ee,J,Z,fe,se,$e,le,ae,ie,he;return D=new U({props:{code:`from transformers import TFAutoModelForSequenceClassification

nome_del_modello = "nlptown/bert-base-multilingual-uncased-sentiment"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(nome_del_modello)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>nome_del_modello = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(nome_del_modello)`}}),S=new va({props:{$$slots:{default:[ec]},$$scope:{ctx:y}}}),te=new U({props:{code:"tf_outputs = tf_model(tf_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)'}}),ie=new U({props:{code:`import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
tf_predictions`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions`}}),{c(){o=r("p"),m=l("\u{1F917} Transformers fornisce un metodo semplice e unificato per caricare istanze pre-allenate. Questo significa che puoi caricare un "),s=r("code"),f=l("TFAutoModel"),h=l(" come caricheresti un "),_=r("code"),P=l("AutoTokenizer"),x=l(". L\u2019unica differenza \xE8 selezionare il "),q=r("code"),M=l("TFAutoModel"),F=l(" corretto per il compito di interesse. Dato che stai facendo classificazione di testi, o sequenze, carica "),O=r("code"),L=l("TFAutoModelForSequenceClassification"),R=l(":"),T=d(),k(D.$$.fragment),g=d(),k(S.$$.fragment),Q=d(),G=r("p"),V=l("Ora puoi passare il tuo lotto di input pre-processati direttamente al modello passando le chiavi del dizionario al tensore:"),K=d(),k(te.$$.fragment),Y=d(),H=r("p"),ee=l("Il modello produrr\xE0 le attivazioni finali nell\u2019attributo "),J=r("code"),Z=l("logits"),fe=l(". Applica la funzione softmax a "),se=r("code"),$e=l("logits"),le=l(" per ottenere le probabilit\xE0:"),ae=d(),k(ie.$$.fragment)},l(C){o=n(C,"P",{});var I=p(o);m=i(I,"\u{1F917} Transformers fornisce un metodo semplice e unificato per caricare istanze pre-allenate. Questo significa che puoi caricare un "),s=n(I,"CODE",{});var re=p(s);f=i(re,"TFAutoModel"),re.forEach(a),h=i(I," come caricheresti un "),_=n(I,"CODE",{});var v=p(_);P=i(v,"AutoTokenizer"),v.forEach(a),x=i(I,". L\u2019unica differenza \xE8 selezionare il "),q=n(I,"CODE",{});var N=p(q);M=i(N,"TFAutoModel"),N.forEach(a),F=i(I," corretto per il compito di interesse. Dato che stai facendo classificazione di testi, o sequenze, carica "),O=n(I,"CODE",{});var pe=p(O);L=i(pe,"TFAutoModelForSequenceClassification"),pe.forEach(a),R=i(I,":"),I.forEach(a),T=$(C),b(D.$$.fragment,C),g=$(C),b(S.$$.fragment,C),Q=$(C),G=n(C,"P",{});var ye=p(G);V=i(ye,"Ora puoi passare il tuo lotto di input pre-processati direttamente al modello passando le chiavi del dizionario al tensore:"),ye.forEach(a),K=$(C),b(te.$$.fragment,C),Y=$(C),H=n(C,"P",{});var de=p(H);ee=i(de,"Il modello produrr\xE0 le attivazioni finali nell\u2019attributo "),J=n(de,"CODE",{});var _e=p(J);Z=i(_e,"logits"),_e.forEach(a),fe=i(de,". Applica la funzione softmax a "),se=n(de,"CODE",{});var ne=p(se);$e=i(ne,"logits"),ne.forEach(a),le=i(de," per ottenere le probabilit\xE0:"),de.forEach(a),ae=$(C),b(ie.$$.fragment,C)},m(C,I){u(C,o,I),t(o,m),t(o,s),t(s,f),t(o,h),t(o,_),t(_,P),t(o,x),t(o,q),t(q,M),t(o,F),t(o,O),t(O,L),t(o,R),u(C,T,I),E(D,C,I),u(C,g,I),E(S,C,I),u(C,Q,I),u(C,G,I),t(G,V),u(C,K,I),E(te,C,I),u(C,Y,I),u(C,H,I),t(H,ee),t(H,J),t(J,Z),t(H,fe),t(H,se),t(se,$e),t(H,le),u(C,ae,I),E(ie,C,I),he=!0},p(C,I){const re={};I&2&&(re.$$scope={dirty:I,ctx:C}),S.$set(re)},i(C){he||(w(D.$$.fragment,C),w(S.$$.fragment,C),w(te.$$.fragment,C),w(ie.$$.fragment,C),he=!0)},o(C){j(D.$$.fragment,C),j(S.$$.fragment,C),j(te.$$.fragment,C),j(ie.$$.fragment,C),he=!1},d(C){C&&a(o),C&&a(T),A(D,C),C&&a(g),A(S,C),C&&a(Q),C&&a(G),C&&a(K),A(te,C),C&&a(Y),C&&a(H),C&&a(ae),A(ie,C)}}}function ac(y){let o,m;return o=new ge({props:{$$slots:{default:[tc]},$$scope:{ctx:y}}}),{c(){k(o.$$.fragment)},l(s){b(o.$$.fragment,s)},m(s,f){E(o,s,f),m=!0},p(s,f){const h={};f&2&&(h.$$scope={dirty:f,ctx:s}),o.$set(h)},i(s){m||(w(o.$$.fragment,s),m=!0)},o(s){j(o.$$.fragment,s),m=!1},d(s){A(o,s)}}}function oc(y){let o,m,s,f,h;return{c(){o=r("p"),m=l("Tutti i modelli di \u{1F917} Transformers (PyTorch e TensorFlow) restituiscono i tensori "),s=r("em"),f=l("prima"),h=l(` della funzione finale
di attivazione (come la softmax) perch\xE9 la funzione di attivazione finale viene spesso unita a quella di perdita.`)},l(_){o=n(_,"P",{});var P=p(o);m=i(P,"Tutti i modelli di \u{1F917} Transformers (PyTorch e TensorFlow) restituiscono i tensori "),s=n(P,"EM",{});var x=p(s);f=i(x,"prima"),x.forEach(a),h=i(P,` della funzione finale
di attivazione (come la softmax) perch\xE9 la funzione di attivazione finale viene spesso unita a quella di perdita.`),P.forEach(a)},m(_,P){u(_,o,P),t(o,m),t(o,s),t(s,f),t(o,h)},d(_){_&&a(o)}}}function sc(y){let o,m,s,f,h;return{c(){o=r("p"),m=l(`Gli output del modello di \u{1F917} Transformers sono delle dataclasses speciali in modo che i loro attributi vengano auto-completati all\u2019interno di un IDE.
Gli output del modello si comportano anche come una tupla o un dizionario (ad esempio, puoi indicizzare con un intero, una slice o una stringa) nel qual caso gli attributi che sono `),s=r("code"),f=l("None"),h=l(" vengono ignorati.")},l(_){o=n(_,"P",{});var P=p(o);m=i(P,`Gli output del modello di \u{1F917} Transformers sono delle dataclasses speciali in modo che i loro attributi vengano auto-completati all\u2019interno di un IDE.
Gli output del modello si comportano anche come una tupla o un dizionario (ad esempio, puoi indicizzare con un intero, una slice o una stringa) nel qual caso gli attributi che sono `),s=n(P,"CODE",{});var x=p(s);f=i(x,"None"),x.forEach(a),h=i(P," vengono ignorati."),P.forEach(a)},m(_,P){u(_,o,P),t(o,m),t(o,s),t(s,f),t(o,h)},d(_){_&&a(o)}}}function lc(y){let o,m,s,f,h,_,P,x,q,M,F,O,L,R,T,D;return P=new U({props:{code:`pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)`}}),T=new U({props:{code:'pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)'}}),{c(){o=r("p"),m=l("Una volta completato il fine-tuning del tuo modello, puoi salvarlo con il suo tokenizer utilizzando "),s=r("code"),f=l("PreTrainedModel.save_pretrained()"),h=l(":"),_=d(),k(P.$$.fragment),x=d(),q=r("p"),M=l("Quando desideri utilizzare il tuo modello nuovamente, puoi ri-caricarlo con "),F=r("code"),O=l("PreTrainedModel.from_pretrained()"),L=l(":"),R=d(),k(T.$$.fragment)},l(g){o=n(g,"P",{});var S=p(o);m=i(S,"Una volta completato il fine-tuning del tuo modello, puoi salvarlo con il suo tokenizer utilizzando "),s=n(S,"CODE",{});var Q=p(s);f=i(Q,"PreTrainedModel.save_pretrained()"),Q.forEach(a),h=i(S,":"),S.forEach(a),_=$(g),b(P.$$.fragment,g),x=$(g),q=n(g,"P",{});var G=p(q);M=i(G,"Quando desideri utilizzare il tuo modello nuovamente, puoi ri-caricarlo con "),F=n(G,"CODE",{});var V=p(F);O=i(V,"PreTrainedModel.from_pretrained()"),V.forEach(a),L=i(G,":"),G.forEach(a),R=$(g),b(T.$$.fragment,g)},m(g,S){u(g,o,S),t(o,m),t(o,s),t(s,f),t(o,h),u(g,_,S),E(P,g,S),u(g,x,S),u(g,q,S),t(q,M),t(q,F),t(F,O),t(q,L),u(g,R,S),E(T,g,S),D=!0},p:Pe,i(g){D||(w(P.$$.fragment,g),w(T.$$.fragment,g),D=!0)},o(g){j(P.$$.fragment,g),j(T.$$.fragment,g),D=!1},d(g){g&&a(o),g&&a(_),A(P,g),g&&a(x),g&&a(q),g&&a(R),A(T,g)}}}function ic(y){let o,m;return o=new ge({props:{$$slots:{default:[lc]},$$scope:{ctx:y}}}),{c(){k(o.$$.fragment)},l(s){b(o.$$.fragment,s)},m(s,f){E(o,s,f),m=!0},p(s,f){const h={};f&2&&(h.$$scope={dirty:f,ctx:s}),o.$set(h)},i(s){m||(w(o.$$.fragment,s),m=!0)},o(s){j(o.$$.fragment,s),m=!1},d(s){A(o,s)}}}function rc(y){let o,m,s,f,h,_,P,x,q,M,F,O,L,R,T,D;return P=new U({props:{code:`tf_save_directory = "./tf_save_pretrained"
tokenizer.save_pretrained(tf_save_directory)
tf_model.save_pretrained(tf_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`}}),T=new U({props:{code:'tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)'}}),{c(){o=r("p"),m=l("Una volta completato il fine-tuning del tuo modello, puoi salvarlo con il suo tokenizer utilizzando "),s=r("code"),f=l("TFPreTrainedModel.save_pretrained()"),h=l(":"),_=d(),k(P.$$.fragment),x=d(),q=r("p"),M=l("Quando desideri utilizzare il tuo modello nuovamente, puoi ri-caricarlo con "),F=r("code"),O=l("TFPreTrainedModel.from_pretrained()"),L=l(":"),R=d(),k(T.$$.fragment)},l(g){o=n(g,"P",{});var S=p(o);m=i(S,"Una volta completato il fine-tuning del tuo modello, puoi salvarlo con il suo tokenizer utilizzando "),s=n(S,"CODE",{});var Q=p(s);f=i(Q,"TFPreTrainedModel.save_pretrained()"),Q.forEach(a),h=i(S,":"),S.forEach(a),_=$(g),b(P.$$.fragment,g),x=$(g),q=n(g,"P",{});var G=p(q);M=i(G,"Quando desideri utilizzare il tuo modello nuovamente, puoi ri-caricarlo con "),F=n(G,"CODE",{});var V=p(F);O=i(V,"TFPreTrainedModel.from_pretrained()"),V.forEach(a),L=i(G,":"),G.forEach(a),R=$(g),b(T.$$.fragment,g)},m(g,S){u(g,o,S),t(o,m),t(o,s),t(s,f),t(o,h),u(g,_,S),E(P,g,S),u(g,x,S),u(g,q,S),t(q,M),t(q,F),t(F,O),t(q,L),u(g,R,S),E(T,g,S),D=!0},p:Pe,i(g){D||(w(P.$$.fragment,g),w(T.$$.fragment,g),D=!0)},o(g){j(P.$$.fragment,g),j(T.$$.fragment,g),D=!1},d(g){g&&a(o),g&&a(_),A(P,g),g&&a(x),g&&a(q),g&&a(R),A(T,g)}}}function nc(y){let o,m;return o=new ge({props:{$$slots:{default:[rc]},$$scope:{ctx:y}}}),{c(){k(o.$$.fragment)},l(s){b(o.$$.fragment,s)},m(s,f){E(o,s,f),m=!0},p(s,f){const h={};f&2&&(h.$$scope={dirty:f,ctx:s}),o.$set(h)},i(s){m||(w(o.$$.fragment,s),m=!0)},o(s){j(o.$$.fragment,s),m=!1},d(s){A(o,s)}}}function pc(y){let o,m;return o=new U({props:{code:`from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`}}),{c(){k(o.$$.fragment)},l(s){b(o.$$.fragment,s)},m(s,f){E(o,s,f),m=!0},p:Pe,i(s){m||(w(o.$$.fragment,s),m=!0)},o(s){j(o.$$.fragment,s),m=!1},d(s){A(o,s)}}}function cc(y){let o,m;return o=new ge({props:{$$slots:{default:[pc]},$$scope:{ctx:y}}}),{c(){k(o.$$.fragment)},l(s){b(o.$$.fragment,s)},m(s,f){E(o,s,f),m=!0},p(s,f){const h={};f&2&&(h.$$scope={dirty:f,ctx:s}),o.$set(h)},i(s){m||(w(o.$$.fragment,s),m=!0)},o(s){j(o.$$.fragment,s),m=!1},d(s){A(o,s)}}}function uc(y){let o,m;return o=new U({props:{code:`from transformers import TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`}}),{c(){k(o.$$.fragment)},l(s){b(o.$$.fragment,s)},m(s,f){E(o,s,f),m=!0},p:Pe,i(s){m||(w(o.$$.fragment,s),m=!0)},o(s){j(o.$$.fragment,s),m=!1},d(s){A(o,s)}}}function mc(y){let o,m;return o=new ge({props:{$$slots:{default:[uc]},$$scope:{ctx:y}}}),{c(){k(o.$$.fragment)},l(s){b(o.$$.fragment,s)},m(s,f){E(o,s,f),m=!0},p(s,f){const h={};f&2&&(h.$$scope={dirty:f,ctx:s}),o.$set(h)},i(s){m||(w(o.$$.fragment,s),m=!0)},o(s){j(o.$$.fragment,s),m=!1},d(s){A(o,s)}}}function fc(y){let o,m,s,f,h,_,P,x,q,M,F,O,L,R,T,D,g,S,Q,G,V,K,te,Y,H,ee,J,Z,fe,se,$e,le,ae,ie,he,C,I,re,v,N,pe,ye,de,_e,ne,Le,ve,Kt,dt,B,za,Xs,el,ka,tl,al,ba,ol,sl,Ea,ll,il,wa,rl,nl,ja,pl,cl,Aa,ul,ml,qa,fl,Co,$t,Ta,dl,$l,Po,ze,Ca,hl,gl,Pa,_l,vl,ya,zl,yo,ht,Sa,kl,bl,So,Ne,Ma,El,wl,Da,jl,Mo,Re,Do,Se,Ue,xa,gt,Al,Fa,ql,xo,Ge,Tl,Oa,Cl,Pl,Fo,Vt,yl,Oo,Qe,Io,He,Sl,Ia,Ml,Dl,Lo,_t,No,ke,xl,vt,Fl,Ol,La,Il,Ll,Ro,zt,Uo,Be,Nl,Na,Rl,Ul,Go,kt,Qo,be,Gl,Ra,Ql,Hl,bt,Bl,Kl,Ho,Et,Bo,Ke,Vl,Ua,Yl,Jl,Ko,wt,Vo,Ee,Zl,jt,Wl,Xl,At,ei,ti,Yo,qt,Jo,Ve,ai,Ga,oi,si,Zo,Tt,Wo,Yt,li,Xo,Ct,es,Ye,ii,Jt,ri,ni,ts,Me,Je,Qa,Pt,pi,Ha,ci,as,ce,ui,Ba,mi,fi,yt,di,$i,Ka,hi,gi,St,_i,vi,os,Mt,ss,Ze,ls,we,zi,Va,ki,bi,Ya,Ei,wi,is,Dt,rs,je,ji,Zt,Ai,qi,Wt,Ti,Ci,ns,De,We,Ja,xt,Pi,Za,yi,ps,Ft,cs,W,Si,Wa,Mi,Di,Xa,xi,Fi,eo,Oi,Ii,Xt,Li,Ni,to,Ri,Ui,ao,Gi,Qi,us,Ae,Hi,oo,Bi,Ki,so,Vi,Yi,ms,xe,Xe,lo,Ot,Ji,io,Zi,fs,qe,Wi,ro,Xi,er,ea,tr,ar,ds,et,or,no,sr,lr,$s,It,hs,tt,ir,po,rr,nr,gs,ta,pr,_s,Lt,vs,aa,cr,zs,at,oa,sa,ur,mr,fr,la,ia,dr,$r,ks,ot,hr,co,gr,_r,bs,st,Es,lt,vr,ra,zr,kr,ws,Fe,it,uo,Nt,br,mo,Er,js,rt,As,nt,qs,X,wr,Rt,fo,jr,Ar,Ut,$o,qr,Tr,ho,Cr,Pr,go,yr,Sr,Gt,Mr,Dr,na,xr,Fr,Ts,pt,Cs,Oe,ct,_o,Qt,Or,vo,Ir,Ps,ut,ys,Te,Lr,zo,Nr,Rr,ko,Ur,Gr,Ss,mt,Ms;return _=new ft({}),F=new Fp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/it/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/it/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/it/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/it/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/it/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/it/tensorflow/quicktour.ipynb"}]}}),K=new va({props:{$$slots:{default:[Op]},$$scope:{ctx:y}}}),J=new ft({}),I=new Pp({props:{id:"tiZFewofSLM"}}),Re=new va({props:{$$slots:{default:[Ip]},$$scope:{ctx:y}}}),gt=new ft({}),Qe=new _a({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Up],pytorch:[Np]},$$scope:{ctx:y}}}),_t=new U({props:{code:`from transformers import pipeline

classificatore = pipeline("sentiment-analysis", model="MilaNLProc/feel-it-italian-sentiment")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classificatore = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=<span class="hljs-string">&quot;MilaNLProc/feel-it-italian-sentiment&quot;</span>)`}}),zt=new U({props:{code:'classificatore("Siamo molto felici di mostrarti la libreria \u{1F917} Transformers.")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classificatore(<span class="hljs-string">&quot;Siamo molto felici di mostrarti la libreria \u{1F917} Transformers.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;positive&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9997</span>}]`}}),kt=new U({props:{code:`risultati = classificatore(
    ["Siamo molto felici di mostrarti la libreria \u{1F917} Transformers.", "Speriamo te non la odierai."]
)
for risultato in risultati:
    print(f"etichetta: {risultato['label']}, con punteggio: {round(risultato['score'], 4)}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>risultati = classificatore(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;Siamo molto felici di mostrarti la libreria \u{1F917} Transformers.&quot;</span>, <span class="hljs-string">&quot;Speriamo te non la odierai.&quot;</span>]
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> risultato <span class="hljs-keyword">in</span> risultati:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;etichetta: <span class="hljs-subst">{risultato[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, con punteggio: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(risultato[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
etichetta: positive, con punteggio: <span class="hljs-number">0.9998</span>
etichetta: negative, con punteggio: <span class="hljs-number">0.9998</span>`}}),Et=new U({props:{code:"pip install datasets ",highlighted:"pip install datasets "}}),wt=new U({props:{code:`import torch
from transformers import pipeline

riconoscitore_vocale = pipeline(
    "automatic-speech-recognition", model="radiogroup-crits/wav2vec2-xls-r-1b-italian-doc4lm-5gram"
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>riconoscitore_vocale = pipeline(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;radiogroup-crits/wav2vec2-xls-r-1b-italian-doc4lm-5gram&quot;</span>
<span class="hljs-meta">... </span>)`}}),qt=new U({props:{code:`from datasets import load_dataset, Audio

dataset = load_dataset("PolyAI/minds14", name="it-IT", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;it-IT&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Tt=new U({props:{code:'dataset = dataset.cast_column("audio", Audio(sampling_rate=riconoscitore_vocale.feature_extractor.sampling_rate))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=riconoscitore_vocale.feature_extractor.sampling_rate))'}}),Ct=new U({props:{code:`risultato = riconoscitore_vocale(dataset[:4]["audio"])
print([d["text"] for d in risultato])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>risultato = riconoscitore_vocale(dataset[:<span class="hljs-number">4</span>][<span class="hljs-string">&quot;audio&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>([d[<span class="hljs-string">&quot;text&quot;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> risultato])
[<span class="hljs-string">&#x27;dovrei caricare dei soldi sul mio conto corrente&#x27;</span>, <span class="hljs-string">&#x27;buongiorno e senza vorrei depositare denaro sul mio conto corrente come devo fare per cortesia&#x27;</span>, <span class="hljs-string">&#x27;s\xEC salve vorrei depositare del denaro sul mio conto&#x27;</span>, <span class="hljs-string">&#x27;e buon pomeriggio vorrei depositare dei soldi sul mio conto bancario volleo sapere come posso fare se e posso farlo online ed un altro conto o andandoo tramite bancomut&#x27;</span>]`}}),Pt=new ft({}),Mt=new U({props:{code:'model_name = "nlptown/bert-base-multilingual-uncased-sentiment"',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>'}}),Ze=new _a({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Bp],pytorch:[Qp]},$$scope:{ctx:y}}}),Dt=new U({props:{code:`classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;5 stars&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.7273</span>}]`}}),xt=new ft({}),Ft=new Pp({props:{id:"AhChOFRegn4"}}),Ot=new ft({}),It=new U({props:{code:`from transformers import AutoTokenizer

nome_del_modello = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(nome_del_modello)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>nome_del_modello = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(nome_del_modello)`}}),Lt=new U({props:{code:`encoding = tokenizer("Siamo molto felici di mostrarti la libreria \u{1F917} Transformers.")
print(encoding)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;Siamo molto felici di mostrarti la libreria \u{1F917} Transformers.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">56821</span>, <span class="hljs-number">10132</span>, <span class="hljs-number">14407</span>, <span class="hljs-number">13019</span>, <span class="hljs-number">13007</span>, <span class="hljs-number">10120</span>, <span class="hljs-number">47201</span>, <span class="hljs-number">10330</span>, <span class="hljs-number">10106</span>, <span class="hljs-number">91686</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
<span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),st=new _a({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Jp],pytorch:[Vp]},$$scope:{ctx:y}}}),Nt=new ft({}),rt=new _a({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[ac],pytorch:[Xp]},$$scope:{ctx:y}}}),nt=new va({props:{$$slots:{default:[oc]},$$scope:{ctx:y}}}),pt=new va({props:{$$slots:{default:[sc]},$$scope:{ctx:y}}}),Qt=new ft({}),ut=new _a({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[nc],pytorch:[ic]},$$scope:{ctx:y}}}),mt=new _a({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[mc],pytorch:[cc]},$$scope:{ctx:y}}}),{c(){o=r("meta"),m=d(),s=r("h1"),f=r("a"),h=r("span"),k(_.$$.fragment),P=d(),x=r("span"),q=l("Quick tour"),M=d(),k(F.$$.fragment),O=d(),L=r("p"),R=l("Entra in azione con \u{1F917} Transformers! Inizia utilizzando "),T=r("code"),D=l("pipeline()"),g=l(" per un\u2019inferenza veloce, carica un modello pre-allenato e un tokenizer con una "),S=r("a"),Q=l("AutoClass"),G=l(" per risolvere i tuoi compiti legati a testo, immagini o audio."),V=d(),k(K.$$.fragment),te=d(),Y=r("h2"),H=r("a"),ee=r("span"),k(J.$$.fragment),Z=d(),fe=r("span"),se=l("Pipeline"),$e=d(),le=r("p"),ae=r("code"),ie=l("pipeline()"),he=l(" \xE8 il modo pi\xF9 semplice per utilizzare un modello pre-allenato per un dato compito."),C=d(),k(I.$$.fragment),re=d(),v=r("p"),N=l("La "),pe=r("code"),ye=l("pipeline()"),de=l(" supporta molti compiti comuni:"),_e=d(),ne=r("p"),Le=r("strong"),ve=l("Testo"),Kt=l(":"),dt=d(),B=r("ul"),za=r("li"),Xs=l("Analisi del Sentimento (Sentiment Analysis, in inglese): classifica la polarit\xE0 di un testo dato."),el=d(),ka=r("li"),tl=l("Generazione del Testo (Text Generation, in inglese): genera del testo a partire da un dato input."),al=d(),ba=r("li"),ol=l("Riconoscimento di Entit\xE0 (Name Entity Recognition o NER, in inglese): etichetta ogni parola con l\u2019entit\xE0 che questa rappresenta (persona, data, luogo, ecc.)."),sl=d(),Ea=r("li"),ll=l("Rispondere a Domande (Question answering, in inglese): estrae la risposta da un contesto, dato del contesto e una domanda."),il=d(),wa=r("li"),rl=l("Riempimento di Maschere (Fill-mask, in inglese): riempie gli spazi mancanti in un testo che ha parole mascherate."),nl=d(),ja=r("li"),pl=l("Riassumere (Summarization, in inglese): genera una sintesi di una lunga sequenza di testo o di un documento."),cl=d(),Aa=r("li"),ul=l("Traduzione (Translation, in inglese): traduce un testo in un\u2019altra lingua."),ml=d(),qa=r("li"),fl=l("Estrazione di Caratteristiche (Feature Extraction, in inglese): crea un tensore che rappresenta un testo."),Co=d(),$t=r("p"),Ta=r("strong"),dl=l("Immagini"),$l=l(":"),Po=d(),ze=r("ul"),Ca=r("li"),hl=l("Classificazione di Immagini (Image Classification, in inglese): classifica un\u2019immagine."),gl=d(),Pa=r("li"),_l=l("Segmentazione di Immagini (Image Segmentation, in inglese): classifica ogni pixel di un\u2019immagine."),vl=d(),ya=r("li"),zl=l("Rilevazione di Oggetti (Object Detection, in inglese): rileva oggetti all\u2019interno di un\u2019immagine."),yo=d(),ht=r("p"),Sa=r("strong"),kl=l("Audio"),bl=l(":"),So=d(),Ne=r("ul"),Ma=r("li"),El=l("Classificazione di Audio (Audio Classification, in inglese): assegna un\u2019etichetta ad un segmento di audio dato."),wl=d(),Da=r("li"),jl=l("Riconoscimento Vocale Automatico (Automatic Speech Recognition o ASR, in inglese): trascrive il contenuto di un audio dato in un testo."),Mo=d(),k(Re.$$.fragment),Do=d(),Se=r("h3"),Ue=r("a"),xa=r("span"),k(gt.$$.fragment),Al=d(),Fa=r("span"),ql=l("Utilizzo della Pipeline"),xo=d(),Ge=r("p"),Tl=l("Nel seguente esempio, utilizzerai la "),Oa=r("code"),Cl=l("pipeline()"),Pl=l(" per l\u2019analisi del sentimento."),Fo=d(),Vt=r("p"),yl=l("Installa le seguenti dipendenze se non lo hai gi\xE0 fatto:"),Oo=d(),k(Qe.$$.fragment),Io=d(),He=r("p"),Sl=l("Importa "),Ia=r("code"),Ml=l("pipeline()"),Dl=l(" e specifica il compito che vuoi completare:"),Lo=d(),k(_t.$$.fragment),No=d(),ke=r("p"),xl=l("La pipeline scarica e salva il "),vt=r("a"),Fl=l("modello pre-allenato"),Ol=l(" e il tokenizer per l\u2019analisi del sentimento. Se non avessimo scelto un modello, la pipeline ne avrebbe scelto uno di default. Ora puoi utilizzare il "),La=r("code"),Il=l("classifier"),Ll=l(" sul tuo testo obiettivo:"),Ro=d(),k(zt.$$.fragment),Uo=d(),Be=r("p"),Nl=l("Per pi\xF9 di una frase, passa una lista di frasi alla "),Na=r("code"),Rl=l("pipeline()"),Ul=l(" la quale restituir\xE0 una lista di dizionari:"),Go=d(),k(kt.$$.fragment),Qo=d(),be=r("p"),Gl=l("La "),Ra=r("code"),Ql=l("pipeline()"),Hl=l(" pu\xF2 anche iterare su un dataset intero. Inizia installando la libreria "),bt=r("a"),Bl=l("\u{1F917} Datasets"),Kl=l(":"),Ho=d(),k(Et.$$.fragment),Bo=d(),Ke=r("p"),Vl=l("Crea una "),Ua=r("code"),Yl=l("pipeline()"),Jl=l(" con il compito che vuoi risolvere e con il modello che vuoi utilizzare."),Ko=d(),k(wt.$$.fragment),Vo=d(),Ee=r("p"),Zl=l("Poi, carica un dataset (vedi \u{1F917} Datasets "),jt=r("a"),Wl=l("Quick Start"),Xl=l(" per maggiori dettagli) sul quale vuoi iterare. Per esempio, carichiamo il dataset "),At=r("a"),ei=l("MInDS-14"),ti=l(":"),Yo=d(),k(qt.$$.fragment),Jo=d(),Ve=r("p"),ai=l("Dobbiamo assicurarci che la frequenza di campionamento del set di dati corrisponda alla frequenza di campionamento con cui \xE8 stato addestrato "),Ga=r("code"),oi=l("radiogroup-crits/wav2vec2-xls-r-1b-italian-doc4lm-5gram"),si=l("."),Zo=d(),k(Tt.$$.fragment),Wo=d(),Yt=r("p"),li=l(`I file audio vengono caricati automaticamente e ri-campionati quando chiamiamo la colonna \u201Caudio\u201D.
Estraiamo i vettori delle forme d\u2019onda grezze delle prime 4 osservazioni e passiamoli come lista alla pipeline:`),Xo=d(),k(Ct.$$.fragment),es=d(),Ye=r("p"),ii=l("Per un dataset pi\xF9 grande dove gli input sono di dimensione maggiore (come nel parlato/audio o nella visione), dovrai passare un generatore al posto di una lista che carica tutti gli input in memoria. Guarda la "),Jt=r("a"),ri=l("documentazione della pipeline"),ni=l(" per maggiori informazioni."),ts=d(),Me=r("h3"),Je=r("a"),Qa=r("span"),k(Pt.$$.fragment),pi=d(),Ha=r("span"),ci=l("Utilizzare un altro modello e tokenizer nella pipeline"),as=d(),ce=r("p"),ui=l("La "),Ba=r("code"),mi=l("pipeline()"),fi=l(" pu\xF2 ospitare qualsiasi modello del "),yt=r("a"),di=l("Model Hub"),$i=l(", rendendo semplice l\u2019adattamento della "),Ka=r("code"),hi=l("pipeline()"),gi=l(" per altri casi d\u2019uso. Per esempio, se si vuole un modello capace di trattare testo in francese, usa i tag presenti nel Model Hub in modo da filtrare per ottenere un modello appropriato. Il miglior risultato filtrato restituisce un modello multi-lingua "),St=r("a"),_i=l("BERT model"),vi=l(" fine-tuned per l\u2019analisi del sentimento. Ottimo, utilizziamo questo modello!"),os=d(),k(Mt.$$.fragment),ss=d(),k(Ze.$$.fragment),ls=d(),we=r("p"),zi=l("Poi puoi specificare il modello e il tokenizer nella "),Va=r("code"),ki=l("pipeline()"),bi=l(", e applicare il "),Ya=r("code"),Ei=l("classifier"),wi=l(" sul tuo testo obiettivo:"),is=d(),k(Dt.$$.fragment),rs=d(),je=r("p"),ji=l("Se non riesci a trovare un modello per il tuo caso d\u2019uso, dovrai fare fine-tuning di un modello pre-allenato sui tuoi dati. Dai un\u2019occhiata al nostro tutorial "),Zt=r("a"),Ai=l("fine-tuning tutorial"),qi=l(" per imparare come. Infine, dopo che hai completato il fine-tuning del tuo modello pre-allenato, considera per favore di condividerlo (vedi il tutorial "),Wt=r("a"),Ti=l("qui"),Ci=l(") con la comunit\xE0 sul Model Hub per democratizzare l\u2019NLP! \u{1F917}"),ns=d(),De=r("h2"),We=r("a"),Ja=r("span"),k(xt.$$.fragment),Pi=d(),Za=r("span"),yi=l("AutoClass"),ps=d(),k(Ft.$$.fragment),cs=d(),W=r("p"),Si=l("Al suo interno, le classi "),Wa=r("code"),Mi=l("AutoModelForSequenceClassification"),Di=l(" e "),Xa=r("code"),xi=l("AutoTokenizer"),Fi=l(" lavorano assieme per dare potere alla "),eo=r("code"),Oi=l("pipeline()"),Ii=l(". Una "),Xt=r("a"),Li=l("AutoClass"),Ni=l(" \xE8 una scorciatoia che automaticamente recupera l\u2019architettura di un modello pre-allenato a partire dal suo nome o path. Hai solo bisogno di selezionare la "),to=r("code"),Ri=l("AutoClass"),Ui=l(" appropriata per il tuo compito e il suo tokenizer associato con "),ao=r("code"),Gi=l("AutoTokenizer"),Qi=l("."),us=d(),Ae=r("p"),Hi=l("Ritorniamo al nostro esempio e vediamo come puoi utilizzare la "),oo=r("code"),Bi=l("AutoClass"),Ki=l(" per replicare i risultati della "),so=r("code"),Vi=l("pipeline()"),Yi=l("."),ms=d(),xe=r("h3"),Xe=r("a"),lo=r("span"),k(Ot.$$.fragment),Ji=d(),io=r("span"),Zi=l("AutoTokenizer"),fs=d(),qe=r("p"),Wi=l("Un tokenizer \xE8 responsabile dell\u2019elaborazione del testo in modo da trasformarlo in un formato comprensibile dal modello. Per prima cosa, il tokenizer divider\xE0 il testo in parole chiamate "),ro=r("em"),Xi=l("token"),er=l(". Ci sono diverse regole che governano il processo di tokenizzazione, tra cui come dividere una parola e a quale livello (impara di pi\xF9 sulla tokenizzazione "),ea=r("a"),tr=l("qui"),ar=l("). La cosa pi\xF9 importante da ricordare comunque \xE8 che hai bisogno di inizializzare il tokenizer con lo stesso nome del modello in modo da assicurarti che stai utilizzando le stesse regole di tokenizzazione con cui il modello \xE8 stato pre-allenato."),ds=d(),et=r("p"),or=l("Carica un tokenizer con "),no=r("code"),sr=l("AutoTokenizer"),lr=l(":"),$s=d(),k(It.$$.fragment),hs=d(),tt=r("p"),ir=l("Dopodich\xE9, il tokenizer converte i token in numeri in modo da costruire un tensore come input del modello. Questo \xE8 conosciuto come il "),po=r("em"),rr=l("vocabolario"),nr=l(" del modello."),gs=d(),ta=r("p"),pr=l("Passa il tuo testo al tokenizer:"),_s=d(),k(Lt.$$.fragment),vs=d(),aa=r("p"),cr=l("Il tokenizer restituir\xE0 un dizionario contenente:"),zs=d(),at=r("ul"),oa=r("li"),sa=r("a"),ur=l("input_ids"),mr=l(": rappresentazioni numeriche dei tuoi token."),fr=d(),la=r("li"),ia=r("a"),dr=l("attention_mask"),$r=l(": indica quali token devono essere presi in considerazione."),ks=d(),ot=r("p"),hr=l("Come con la "),co=r("code"),gr=l("pipeline()"),_r=l(", il tokenizer accetter\xE0 una lista di input. In pi\xF9, il tokenizer pu\xF2 anche completare (pad, in inglese) e troncare il testo in modo da restituire un lotto (batch, in inglese) di lunghezza uniforme:"),bs=d(),k(st.$$.fragment),Es=d(),lt=r("p"),vr=l("Leggi il tutorial sul "),ra=r("a"),zr=l("preprocessing"),kr=l(" per maggiori dettagli sulla tokenizzazione."),ws=d(),Fe=r("h3"),it=r("a"),uo=r("span"),k(Nt.$$.fragment),br=d(),mo=r("span"),Er=l("AutoModel"),js=d(),k(rt.$$.fragment),As=d(),k(nt.$$.fragment),qs=d(),X=r("p"),wr=l("I modelli sono "),Rt=r("a"),fo=r("code"),jr=l("torch.nn.Module"),Ar=l(" o "),Ut=r("a"),$o=r("code"),qr=l("tf.keras.Model"),Tr=l(" standard cos\xEC puoi utilizzarli all\u2019interno del tuo training loop usuale. Tuttavia, per rendere le cose pi\xF9 semplici, \u{1F917} Transformers fornisce una classe "),ho=r("code"),Cr=l("Trainer"),Pr=l(" per PyTorch che aggiunge delle funzionalit\xE0 per l\u2019allenamento distribuito, precisione mista, e altro ancora. Per TensorFlow, puoi utilizzare il metodo "),go=r("code"),yr=l("fit"),Sr=l(" di "),Gt=r("a"),Mr=l("Keras"),Dr=l(". Fai riferimento al "),na=r("a"),xr=l("tutorial per il training"),Fr=l(" per maggiori dettagli."),Ts=d(),k(pt.$$.fragment),Cs=d(),Oe=r("h3"),ct=r("a"),_o=r("span"),k(Qt.$$.fragment),Or=d(),vo=r("span"),Ir=l("Salva un modello"),Ps=d(),k(ut.$$.fragment),ys=d(),Te=r("p"),Lr=l("Una caratteristica particolarmente interessante di \u{1F917} Transformers \xE8 la sua abilit\xE0 di salvare un modello e ri-caricarlo sia come modello di PyTorch che di TensorFlow. I parametri "),zo=r("code"),Nr=l("from_pt"),Rr=l(" o "),ko=r("code"),Ur=l("from_tf"),Gr=l(" possono convertire un modello da un framework all\u2019altro:"),Ss=d(),k(mt.$$.fragment),this.h()},l(e){const c=Dp('[data-svelte="svelte-1phssyn"]',document.head);o=n(c,"META",{name:!0,content:!0}),c.forEach(a),m=$(e),s=n(e,"H1",{class:!0});var Ht=p(s);f=n(Ht,"A",{id:!0,class:!0,href:!0});var bo=p(f);h=n(bo,"SPAN",{});var Eo=p(h);b(_.$$.fragment,Eo),Eo.forEach(a),bo.forEach(a),P=$(Ht),x=n(Ht,"SPAN",{});var wo=p(x);q=i(wo,"Quick tour"),wo.forEach(a),Ht.forEach(a),M=$(e),b(F.$$.fragment,e),O=$(e),L=n(e,"P",{});var Ie=p(L);R=i(Ie,"Entra in azione con \u{1F917} Transformers! Inizia utilizzando "),T=n(Ie,"CODE",{});var jo=p(T);D=i(jo,"pipeline()"),jo.forEach(a),g=i(Ie," per un\u2019inferenza veloce, carica un modello pre-allenato e un tokenizer con una "),S=n(Ie,"A",{href:!0});var Ao=p(S);Q=i(Ao,"AutoClass"),Ao.forEach(a),G=i(Ie," per risolvere i tuoi compiti legati a testo, immagini o audio."),Ie.forEach(a),V=$(e),b(K.$$.fragment,e),te=$(e),Y=n(e,"H2",{class:!0});var Bt=p(Y);H=n(Bt,"A",{id:!0,class:!0,href:!0});var qo=p(H);ee=n(qo,"SPAN",{});var To=p(ee);b(J.$$.fragment,To),To.forEach(a),qo.forEach(a),Z=$(Bt),fe=n(Bt,"SPAN",{});var Jr=p(fe);se=i(Jr,"Pipeline"),Jr.forEach(a),Bt.forEach(a),$e=$(e),le=n(e,"P",{});var Qr=p(le);ae=n(Qr,"CODE",{});var Zr=p(ae);ie=i(Zr,"pipeline()"),Zr.forEach(a),he=i(Qr," \xE8 il modo pi\xF9 semplice per utilizzare un modello pre-allenato per un dato compito."),Qr.forEach(a),C=$(e),b(I.$$.fragment,e),re=$(e),v=n(e,"P",{});var Ds=p(v);N=i(Ds,"La "),pe=n(Ds,"CODE",{});var Wr=p(pe);ye=i(Wr,"pipeline()"),Wr.forEach(a),de=i(Ds," supporta molti compiti comuni:"),Ds.forEach(a),_e=$(e),ne=n(e,"P",{});var Hr=p(ne);Le=n(Hr,"STRONG",{});var Xr=p(Le);ve=i(Xr,"Testo"),Xr.forEach(a),Kt=i(Hr,":"),Hr.forEach(a),dt=$(e),B=n(e,"UL",{});var oe=p(B);za=n(oe,"LI",{});var en=p(za);Xs=i(en,"Analisi del Sentimento (Sentiment Analysis, in inglese): classifica la polarit\xE0 di un testo dato."),en.forEach(a),el=$(oe),ka=n(oe,"LI",{});var tn=p(ka);tl=i(tn,"Generazione del Testo (Text Generation, in inglese): genera del testo a partire da un dato input."),tn.forEach(a),al=$(oe),ba=n(oe,"LI",{});var an=p(ba);ol=i(an,"Riconoscimento di Entit\xE0 (Name Entity Recognition o NER, in inglese): etichetta ogni parola con l\u2019entit\xE0 che questa rappresenta (persona, data, luogo, ecc.)."),an.forEach(a),sl=$(oe),Ea=n(oe,"LI",{});var on=p(Ea);ll=i(on,"Rispondere a Domande (Question answering, in inglese): estrae la risposta da un contesto, dato del contesto e una domanda."),on.forEach(a),il=$(oe),wa=n(oe,"LI",{});var sn=p(wa);rl=i(sn,"Riempimento di Maschere (Fill-mask, in inglese): riempie gli spazi mancanti in un testo che ha parole mascherate."),sn.forEach(a),nl=$(oe),ja=n(oe,"LI",{});var ln=p(ja);pl=i(ln,"Riassumere (Summarization, in inglese): genera una sintesi di una lunga sequenza di testo o di un documento."),ln.forEach(a),cl=$(oe),Aa=n(oe,"LI",{});var rn=p(Aa);ul=i(rn,"Traduzione (Translation, in inglese): traduce un testo in un\u2019altra lingua."),rn.forEach(a),ml=$(oe),qa=n(oe,"LI",{});var nn=p(qa);fl=i(nn,"Estrazione di Caratteristiche (Feature Extraction, in inglese): crea un tensore che rappresenta un testo."),nn.forEach(a),oe.forEach(a),Co=$(e),$t=n(e,"P",{});var Br=p($t);Ta=n(Br,"STRONG",{});var pn=p(Ta);dl=i(pn,"Immagini"),pn.forEach(a),$l=i(Br,":"),Br.forEach(a),Po=$(e),ze=n(e,"UL",{});var pa=p(ze);Ca=n(pa,"LI",{});var cn=p(Ca);hl=i(cn,"Classificazione di Immagini (Image Classification, in inglese): classifica un\u2019immagine."),cn.forEach(a),gl=$(pa),Pa=n(pa,"LI",{});var un=p(Pa);_l=i(un,"Segmentazione di Immagini (Image Segmentation, in inglese): classifica ogni pixel di un\u2019immagine."),un.forEach(a),vl=$(pa),ya=n(pa,"LI",{});var mn=p(ya);zl=i(mn,"Rilevazione di Oggetti (Object Detection, in inglese): rileva oggetti all\u2019interno di un\u2019immagine."),mn.forEach(a),pa.forEach(a),yo=$(e),ht=n(e,"P",{});var Kr=p(ht);Sa=n(Kr,"STRONG",{});var fn=p(Sa);kl=i(fn,"Audio"),fn.forEach(a),bl=i(Kr,":"),Kr.forEach(a),So=$(e),Ne=n(e,"UL",{});var xs=p(Ne);Ma=n(xs,"LI",{});var dn=p(Ma);El=i(dn,"Classificazione di Audio (Audio Classification, in inglese): assegna un\u2019etichetta ad un segmento di audio dato."),dn.forEach(a),wl=$(xs),Da=n(xs,"LI",{});var $n=p(Da);jl=i($n,"Riconoscimento Vocale Automatico (Automatic Speech Recognition o ASR, in inglese): trascrive il contenuto di un audio dato in un testo."),$n.forEach(a),xs.forEach(a),Mo=$(e),b(Re.$$.fragment,e),Do=$(e),Se=n(e,"H3",{class:!0});var Fs=p(Se);Ue=n(Fs,"A",{id:!0,class:!0,href:!0});var hn=p(Ue);xa=n(hn,"SPAN",{});var gn=p(xa);b(gt.$$.fragment,gn),gn.forEach(a),hn.forEach(a),Al=$(Fs),Fa=n(Fs,"SPAN",{});var _n=p(Fa);ql=i(_n,"Utilizzo della Pipeline"),_n.forEach(a),Fs.forEach(a),xo=$(e),Ge=n(e,"P",{});var Os=p(Ge);Tl=i(Os,"Nel seguente esempio, utilizzerai la "),Oa=n(Os,"CODE",{});var vn=p(Oa);Cl=i(vn,"pipeline()"),vn.forEach(a),Pl=i(Os," per l\u2019analisi del sentimento."),Os.forEach(a),Fo=$(e),Vt=n(e,"P",{});var zn=p(Vt);yl=i(zn,"Installa le seguenti dipendenze se non lo hai gi\xE0 fatto:"),zn.forEach(a),Oo=$(e),b(Qe.$$.fragment,e),Io=$(e),He=n(e,"P",{});var Is=p(He);Sl=i(Is,"Importa "),Ia=n(Is,"CODE",{});var kn=p(Ia);Ml=i(kn,"pipeline()"),kn.forEach(a),Dl=i(Is," e specifica il compito che vuoi completare:"),Is.forEach(a),Lo=$(e),b(_t.$$.fragment,e),No=$(e),ke=n(e,"P",{});var ca=p(ke);xl=i(ca,"La pipeline scarica e salva il "),vt=n(ca,"A",{href:!0,rel:!0});var bn=p(vt);Fl=i(bn,"modello pre-allenato"),bn.forEach(a),Ol=i(ca," e il tokenizer per l\u2019analisi del sentimento. Se non avessimo scelto un modello, la pipeline ne avrebbe scelto uno di default. Ora puoi utilizzare il "),La=n(ca,"CODE",{});var En=p(La);Il=i(En,"classifier"),En.forEach(a),Ll=i(ca," sul tuo testo obiettivo:"),ca.forEach(a),Ro=$(e),b(zt.$$.fragment,e),Uo=$(e),Be=n(e,"P",{});var Ls=p(Be);Nl=i(Ls,"Per pi\xF9 di una frase, passa una lista di frasi alla "),Na=n(Ls,"CODE",{});var wn=p(Na);Rl=i(wn,"pipeline()"),wn.forEach(a),Ul=i(Ls," la quale restituir\xE0 una lista di dizionari:"),Ls.forEach(a),Go=$(e),b(kt.$$.fragment,e),Qo=$(e),be=n(e,"P",{});var ua=p(be);Gl=i(ua,"La "),Ra=n(ua,"CODE",{});var jn=p(Ra);Ql=i(jn,"pipeline()"),jn.forEach(a),Hl=i(ua," pu\xF2 anche iterare su un dataset intero. Inizia installando la libreria "),bt=n(ua,"A",{href:!0,rel:!0});var An=p(bt);Bl=i(An,"\u{1F917} Datasets"),An.forEach(a),Kl=i(ua,":"),ua.forEach(a),Ho=$(e),b(Et.$$.fragment,e),Bo=$(e),Ke=n(e,"P",{});var Ns=p(Ke);Vl=i(Ns,"Crea una "),Ua=n(Ns,"CODE",{});var qn=p(Ua);Yl=i(qn,"pipeline()"),qn.forEach(a),Jl=i(Ns," con il compito che vuoi risolvere e con il modello che vuoi utilizzare."),Ns.forEach(a),Ko=$(e),b(wt.$$.fragment,e),Vo=$(e),Ee=n(e,"P",{});var ma=p(Ee);Zl=i(ma,"Poi, carica un dataset (vedi \u{1F917} Datasets "),jt=n(ma,"A",{href:!0,rel:!0});var Tn=p(jt);Wl=i(Tn,"Quick Start"),Tn.forEach(a),Xl=i(ma," per maggiori dettagli) sul quale vuoi iterare. Per esempio, carichiamo il dataset "),At=n(ma,"A",{href:!0,rel:!0});var Cn=p(At);ei=i(Cn,"MInDS-14"),Cn.forEach(a),ti=i(ma,":"),ma.forEach(a),Yo=$(e),b(qt.$$.fragment,e),Jo=$(e),Ve=n(e,"P",{});var Rs=p(Ve);ai=i(Rs,"Dobbiamo assicurarci che la frequenza di campionamento del set di dati corrisponda alla frequenza di campionamento con cui \xE8 stato addestrato "),Ga=n(Rs,"CODE",{});var Pn=p(Ga);oi=i(Pn,"radiogroup-crits/wav2vec2-xls-r-1b-italian-doc4lm-5gram"),Pn.forEach(a),si=i(Rs,"."),Rs.forEach(a),Zo=$(e),b(Tt.$$.fragment,e),Wo=$(e),Yt=n(e,"P",{});var yn=p(Yt);li=i(yn,`I file audio vengono caricati automaticamente e ri-campionati quando chiamiamo la colonna \u201Caudio\u201D.
Estraiamo i vettori delle forme d\u2019onda grezze delle prime 4 osservazioni e passiamoli come lista alla pipeline:`),yn.forEach(a),Xo=$(e),b(Ct.$$.fragment,e),es=$(e),Ye=n(e,"P",{});var Us=p(Ye);ii=i(Us,"Per un dataset pi\xF9 grande dove gli input sono di dimensione maggiore (come nel parlato/audio o nella visione), dovrai passare un generatore al posto di una lista che carica tutti gli input in memoria. Guarda la "),Jt=n(Us,"A",{href:!0});var Sn=p(Jt);ri=i(Sn,"documentazione della pipeline"),Sn.forEach(a),ni=i(Us," per maggiori informazioni."),Us.forEach(a),ts=$(e),Me=n(e,"H3",{class:!0});var Gs=p(Me);Je=n(Gs,"A",{id:!0,class:!0,href:!0});var Mn=p(Je);Qa=n(Mn,"SPAN",{});var Dn=p(Qa);b(Pt.$$.fragment,Dn),Dn.forEach(a),Mn.forEach(a),pi=$(Gs),Ha=n(Gs,"SPAN",{});var xn=p(Ha);ci=i(xn,"Utilizzare un altro modello e tokenizer nella pipeline"),xn.forEach(a),Gs.forEach(a),as=$(e),ce=n(e,"P",{});var Ce=p(ce);ui=i(Ce,"La "),Ba=n(Ce,"CODE",{});var Fn=p(Ba);mi=i(Fn,"pipeline()"),Fn.forEach(a),fi=i(Ce," pu\xF2 ospitare qualsiasi modello del "),yt=n(Ce,"A",{href:!0,rel:!0});var On=p(yt);di=i(On,"Model Hub"),On.forEach(a),$i=i(Ce,", rendendo semplice l\u2019adattamento della "),Ka=n(Ce,"CODE",{});var In=p(Ka);hi=i(In,"pipeline()"),In.forEach(a),gi=i(Ce," per altri casi d\u2019uso. Per esempio, se si vuole un modello capace di trattare testo in francese, usa i tag presenti nel Model Hub in modo da filtrare per ottenere un modello appropriato. Il miglior risultato filtrato restituisce un modello multi-lingua "),St=n(Ce,"A",{href:!0,rel:!0});var Ln=p(St);_i=i(Ln,"BERT model"),Ln.forEach(a),vi=i(Ce," fine-tuned per l\u2019analisi del sentimento. Ottimo, utilizziamo questo modello!"),Ce.forEach(a),os=$(e),b(Mt.$$.fragment,e),ss=$(e),b(Ze.$$.fragment,e),ls=$(e),we=n(e,"P",{});var fa=p(we);zi=i(fa,"Poi puoi specificare il modello e il tokenizer nella "),Va=n(fa,"CODE",{});var Nn=p(Va);ki=i(Nn,"pipeline()"),Nn.forEach(a),bi=i(fa,", e applicare il "),Ya=n(fa,"CODE",{});var Rn=p(Ya);Ei=i(Rn,"classifier"),Rn.forEach(a),wi=i(fa," sul tuo testo obiettivo:"),fa.forEach(a),is=$(e),b(Dt.$$.fragment,e),rs=$(e),je=n(e,"P",{});var da=p(je);ji=i(da,"Se non riesci a trovare un modello per il tuo caso d\u2019uso, dovrai fare fine-tuning di un modello pre-allenato sui tuoi dati. Dai un\u2019occhiata al nostro tutorial "),Zt=n(da,"A",{href:!0});var Un=p(Zt);Ai=i(Un,"fine-tuning tutorial"),Un.forEach(a),qi=i(da," per imparare come. Infine, dopo che hai completato il fine-tuning del tuo modello pre-allenato, considera per favore di condividerlo (vedi il tutorial "),Wt=n(da,"A",{href:!0});var Gn=p(Wt);Ti=i(Gn,"qui"),Gn.forEach(a),Ci=i(da,") con la comunit\xE0 sul Model Hub per democratizzare l\u2019NLP! \u{1F917}"),da.forEach(a),ns=$(e),De=n(e,"H2",{class:!0});var Qs=p(De);We=n(Qs,"A",{id:!0,class:!0,href:!0});var Qn=p(We);Ja=n(Qn,"SPAN",{});var Hn=p(Ja);b(xt.$$.fragment,Hn),Hn.forEach(a),Qn.forEach(a),Pi=$(Qs),Za=n(Qs,"SPAN",{});var Bn=p(Za);yi=i(Bn,"AutoClass"),Bn.forEach(a),Qs.forEach(a),ps=$(e),b(Ft.$$.fragment,e),cs=$(e),W=n(e,"P",{});var ue=p(W);Si=i(ue,"Al suo interno, le classi "),Wa=n(ue,"CODE",{});var Kn=p(Wa);Mi=i(Kn,"AutoModelForSequenceClassification"),Kn.forEach(a),Di=i(ue," e "),Xa=n(ue,"CODE",{});var Vn=p(Xa);xi=i(Vn,"AutoTokenizer"),Vn.forEach(a),Fi=i(ue," lavorano assieme per dare potere alla "),eo=n(ue,"CODE",{});var Yn=p(eo);Oi=i(Yn,"pipeline()"),Yn.forEach(a),Ii=i(ue,". Una "),Xt=n(ue,"A",{href:!0});var Jn=p(Xt);Li=i(Jn,"AutoClass"),Jn.forEach(a),Ni=i(ue," \xE8 una scorciatoia che automaticamente recupera l\u2019architettura di un modello pre-allenato a partire dal suo nome o path. Hai solo bisogno di selezionare la "),to=n(ue,"CODE",{});var Zn=p(to);Ri=i(Zn,"AutoClass"),Zn.forEach(a),Ui=i(ue," appropriata per il tuo compito e il suo tokenizer associato con "),ao=n(ue,"CODE",{});var Wn=p(ao);Gi=i(Wn,"AutoTokenizer"),Wn.forEach(a),Qi=i(ue,"."),ue.forEach(a),us=$(e),Ae=n(e,"P",{});var $a=p(Ae);Hi=i($a,"Ritorniamo al nostro esempio e vediamo come puoi utilizzare la "),oo=n($a,"CODE",{});var Xn=p(oo);Bi=i(Xn,"AutoClass"),Xn.forEach(a),Ki=i($a," per replicare i risultati della "),so=n($a,"CODE",{});var ep=p(so);Vi=i(ep,"pipeline()"),ep.forEach(a),Yi=i($a,"."),$a.forEach(a),ms=$(e),xe=n(e,"H3",{class:!0});var Hs=p(xe);Xe=n(Hs,"A",{id:!0,class:!0,href:!0});var tp=p(Xe);lo=n(tp,"SPAN",{});var ap=p(lo);b(Ot.$$.fragment,ap),ap.forEach(a),tp.forEach(a),Ji=$(Hs),io=n(Hs,"SPAN",{});var op=p(io);Zi=i(op,"AutoTokenizer"),op.forEach(a),Hs.forEach(a),fs=$(e),qe=n(e,"P",{});var ha=p(qe);Wi=i(ha,"Un tokenizer \xE8 responsabile dell\u2019elaborazione del testo in modo da trasformarlo in un formato comprensibile dal modello. Per prima cosa, il tokenizer divider\xE0 il testo in parole chiamate "),ro=n(ha,"EM",{});var sp=p(ro);Xi=i(sp,"token"),sp.forEach(a),er=i(ha,". Ci sono diverse regole che governano il processo di tokenizzazione, tra cui come dividere una parola e a quale livello (impara di pi\xF9 sulla tokenizzazione "),ea=n(ha,"A",{href:!0});var lp=p(ea);tr=i(lp,"qui"),lp.forEach(a),ar=i(ha,"). La cosa pi\xF9 importante da ricordare comunque \xE8 che hai bisogno di inizializzare il tokenizer con lo stesso nome del modello in modo da assicurarti che stai utilizzando le stesse regole di tokenizzazione con cui il modello \xE8 stato pre-allenato."),ha.forEach(a),ds=$(e),et=n(e,"P",{});var Bs=p(et);or=i(Bs,"Carica un tokenizer con "),no=n(Bs,"CODE",{});var ip=p(no);sr=i(ip,"AutoTokenizer"),ip.forEach(a),lr=i(Bs,":"),Bs.forEach(a),$s=$(e),b(It.$$.fragment,e),hs=$(e),tt=n(e,"P",{});var Ks=p(tt);ir=i(Ks,"Dopodich\xE9, il tokenizer converte i token in numeri in modo da costruire un tensore come input del modello. Questo \xE8 conosciuto come il "),po=n(Ks,"EM",{});var rp=p(po);rr=i(rp,"vocabolario"),rp.forEach(a),nr=i(Ks," del modello."),Ks.forEach(a),gs=$(e),ta=n(e,"P",{});var np=p(ta);pr=i(np,"Passa il tuo testo al tokenizer:"),np.forEach(a),_s=$(e),b(Lt.$$.fragment,e),vs=$(e),aa=n(e,"P",{});var pp=p(aa);cr=i(pp,"Il tokenizer restituir\xE0 un dizionario contenente:"),pp.forEach(a),zs=$(e),at=n(e,"UL",{});var Vs=p(at);oa=n(Vs,"LI",{});var Vr=p(oa);sa=n(Vr,"A",{href:!0});var cp=p(sa);ur=i(cp,"input_ids"),cp.forEach(a),mr=i(Vr,": rappresentazioni numeriche dei tuoi token."),Vr.forEach(a),fr=$(Vs),la=n(Vs,"LI",{});var Yr=p(la);ia=n(Yr,"A",{href:!0});var up=p(ia);dr=i(up,"attention_mask"),up.forEach(a),$r=i(Yr,": indica quali token devono essere presi in considerazione."),Yr.forEach(a),Vs.forEach(a),ks=$(e),ot=n(e,"P",{});var Ys=p(ot);hr=i(Ys,"Come con la "),co=n(Ys,"CODE",{});var mp=p(co);gr=i(mp,"pipeline()"),mp.forEach(a),_r=i(Ys,", il tokenizer accetter\xE0 una lista di input. In pi\xF9, il tokenizer pu\xF2 anche completare (pad, in inglese) e troncare il testo in modo da restituire un lotto (batch, in inglese) di lunghezza uniforme:"),Ys.forEach(a),bs=$(e),b(st.$$.fragment,e),Es=$(e),lt=n(e,"P",{});var Js=p(lt);vr=i(Js,"Leggi il tutorial sul "),ra=n(Js,"A",{href:!0});var fp=p(ra);zr=i(fp,"preprocessing"),fp.forEach(a),kr=i(Js," per maggiori dettagli sulla tokenizzazione."),Js.forEach(a),ws=$(e),Fe=n(e,"H3",{class:!0});var Zs=p(Fe);it=n(Zs,"A",{id:!0,class:!0,href:!0});var dp=p(it);uo=n(dp,"SPAN",{});var $p=p(uo);b(Nt.$$.fragment,$p),$p.forEach(a),dp.forEach(a),br=$(Zs),mo=n(Zs,"SPAN",{});var hp=p(mo);Er=i(hp,"AutoModel"),hp.forEach(a),Zs.forEach(a),js=$(e),b(rt.$$.fragment,e),As=$(e),b(nt.$$.fragment,e),qs=$(e),X=n(e,"P",{});var me=p(X);wr=i(me,"I modelli sono "),Rt=n(me,"A",{href:!0,rel:!0});var gp=p(Rt);fo=n(gp,"CODE",{});var _p=p(fo);jr=i(_p,"torch.nn.Module"),_p.forEach(a),gp.forEach(a),Ar=i(me," o "),Ut=n(me,"A",{href:!0,rel:!0});var vp=p(Ut);$o=n(vp,"CODE",{});var zp=p($o);qr=i(zp,"tf.keras.Model"),zp.forEach(a),vp.forEach(a),Tr=i(me," standard cos\xEC puoi utilizzarli all\u2019interno del tuo training loop usuale. Tuttavia, per rendere le cose pi\xF9 semplici, \u{1F917} Transformers fornisce una classe "),ho=n(me,"CODE",{});var kp=p(ho);Cr=i(kp,"Trainer"),kp.forEach(a),Pr=i(me," per PyTorch che aggiunge delle funzionalit\xE0 per l\u2019allenamento distribuito, precisione mista, e altro ancora. Per TensorFlow, puoi utilizzare il metodo "),go=n(me,"CODE",{});var bp=p(go);yr=i(bp,"fit"),bp.forEach(a),Sr=i(me," di "),Gt=n(me,"A",{href:!0,rel:!0});var Ep=p(Gt);Mr=i(Ep,"Keras"),Ep.forEach(a),Dr=i(me,". Fai riferimento al "),na=n(me,"A",{href:!0});var wp=p(na);xr=i(wp,"tutorial per il training"),wp.forEach(a),Fr=i(me," per maggiori dettagli."),me.forEach(a),Ts=$(e),b(pt.$$.fragment,e),Cs=$(e),Oe=n(e,"H3",{class:!0});var Ws=p(Oe);ct=n(Ws,"A",{id:!0,class:!0,href:!0});var jp=p(ct);_o=n(jp,"SPAN",{});var Ap=p(_o);b(Qt.$$.fragment,Ap),Ap.forEach(a),jp.forEach(a),Or=$(Ws),vo=n(Ws,"SPAN",{});var qp=p(vo);Ir=i(qp,"Salva un modello"),qp.forEach(a),Ws.forEach(a),Ps=$(e),b(ut.$$.fragment,e),ys=$(e),Te=n(e,"P",{});var ga=p(Te);Lr=i(ga,"Una caratteristica particolarmente interessante di \u{1F917} Transformers \xE8 la sua abilit\xE0 di salvare un modello e ri-caricarlo sia come modello di PyTorch che di TensorFlow. I parametri "),zo=n(ga,"CODE",{});var Tp=p(zo);Nr=i(Tp,"from_pt"),Tp.forEach(a),Rr=i(ga," o "),ko=n(ga,"CODE",{});var Cp=p(ko);Ur=i(Cp,"from_tf"),Cp.forEach(a),Gr=i(ga," possono convertire un modello da un framework all\u2019altro:"),ga.forEach(a),Ss=$(e),b(mt.$$.fragment,e),this.h()},h(){z(o,"name","hf:doc:metadata"),z(o,"content",JSON.stringify(dc)),z(f,"id","quick-tour"),z(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),z(f,"href","#quick-tour"),z(s,"class","relative group"),z(S,"href","./model_doc/auto"),z(H,"id","pipeline"),z(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),z(H,"href","#pipeline"),z(Y,"class","relative group"),z(Ue,"id","utilizzo-della-pipeline"),z(Ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),z(Ue,"href","#utilizzo-della-pipeline"),z(Se,"class","relative group"),z(vt,"href","https://huggingface.co/MilaNLProc/feel-it-italian-sentiment"),z(vt,"rel","nofollow"),z(bt,"href","https://huggingface.co/docs/datasets/"),z(bt,"rel","nofollow"),z(jt,"href","https://huggingface.co/docs/datasets/quickstart.html"),z(jt,"rel","nofollow"),z(At,"href","https://huggingface.co/datasets/PolyAI/minds14"),z(At,"rel","nofollow"),z(Jt,"href","./main_classes/pipelines"),z(Je,"id","utilizzare-un-altro-modello-e-tokenizer-nella-pipeline"),z(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),z(Je,"href","#utilizzare-un-altro-modello-e-tokenizer-nella-pipeline"),z(Me,"class","relative group"),z(yt,"href","https://huggingface.co/models"),z(yt,"rel","nofollow"),z(St,"href","https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"),z(St,"rel","nofollow"),z(Zt,"href","./training"),z(Wt,"href","./model_sharing"),z(We,"id","autoclass"),z(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),z(We,"href","#autoclass"),z(De,"class","relative group"),z(Xt,"href","./model_doc/auto"),z(Xe,"id","autotokenizer"),z(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),z(Xe,"href","#autotokenizer"),z(xe,"class","relative group"),z(ea,"href","./tokenizer_summary"),z(sa,"href","./glossary#input-ids"),z(ia,"href",".glossary#attention-mask"),z(ra,"href","./preprocessing"),z(it,"id","automodel"),z(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),z(it,"href","#automodel"),z(Fe,"class","relative group"),z(Rt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),z(Rt,"rel","nofollow"),z(Ut,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),z(Ut,"rel","nofollow"),z(Gt,"href","https://keras.io/"),z(Gt,"rel","nofollow"),z(na,"href","./training"),z(ct,"id","salva-un-modello"),z(ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),z(ct,"href","#salva-un-modello"),z(Oe,"class","relative group")},m(e,c){t(document.head,o),u(e,m,c),u(e,s,c),t(s,f),t(f,h),E(_,h,null),t(s,P),t(s,x),t(x,q),u(e,M,c),E(F,e,c),u(e,O,c),u(e,L,c),t(L,R),t(L,T),t(T,D),t(L,g),t(L,S),t(S,Q),t(L,G),u(e,V,c),E(K,e,c),u(e,te,c),u(e,Y,c),t(Y,H),t(H,ee),E(J,ee,null),t(Y,Z),t(Y,fe),t(fe,se),u(e,$e,c),u(e,le,c),t(le,ae),t(ae,ie),t(le,he),u(e,C,c),E(I,e,c),u(e,re,c),u(e,v,c),t(v,N),t(v,pe),t(pe,ye),t(v,de),u(e,_e,c),u(e,ne,c),t(ne,Le),t(Le,ve),t(ne,Kt),u(e,dt,c),u(e,B,c),t(B,za),t(za,Xs),t(B,el),t(B,ka),t(ka,tl),t(B,al),t(B,ba),t(ba,ol),t(B,sl),t(B,Ea),t(Ea,ll),t(B,il),t(B,wa),t(wa,rl),t(B,nl),t(B,ja),t(ja,pl),t(B,cl),t(B,Aa),t(Aa,ul),t(B,ml),t(B,qa),t(qa,fl),u(e,Co,c),u(e,$t,c),t($t,Ta),t(Ta,dl),t($t,$l),u(e,Po,c),u(e,ze,c),t(ze,Ca),t(Ca,hl),t(ze,gl),t(ze,Pa),t(Pa,_l),t(ze,vl),t(ze,ya),t(ya,zl),u(e,yo,c),u(e,ht,c),t(ht,Sa),t(Sa,kl),t(ht,bl),u(e,So,c),u(e,Ne,c),t(Ne,Ma),t(Ma,El),t(Ne,wl),t(Ne,Da),t(Da,jl),u(e,Mo,c),E(Re,e,c),u(e,Do,c),u(e,Se,c),t(Se,Ue),t(Ue,xa),E(gt,xa,null),t(Se,Al),t(Se,Fa),t(Fa,ql),u(e,xo,c),u(e,Ge,c),t(Ge,Tl),t(Ge,Oa),t(Oa,Cl),t(Ge,Pl),u(e,Fo,c),u(e,Vt,c),t(Vt,yl),u(e,Oo,c),E(Qe,e,c),u(e,Io,c),u(e,He,c),t(He,Sl),t(He,Ia),t(Ia,Ml),t(He,Dl),u(e,Lo,c),E(_t,e,c),u(e,No,c),u(e,ke,c),t(ke,xl),t(ke,vt),t(vt,Fl),t(ke,Ol),t(ke,La),t(La,Il),t(ke,Ll),u(e,Ro,c),E(zt,e,c),u(e,Uo,c),u(e,Be,c),t(Be,Nl),t(Be,Na),t(Na,Rl),t(Be,Ul),u(e,Go,c),E(kt,e,c),u(e,Qo,c),u(e,be,c),t(be,Gl),t(be,Ra),t(Ra,Ql),t(be,Hl),t(be,bt),t(bt,Bl),t(be,Kl),u(e,Ho,c),E(Et,e,c),u(e,Bo,c),u(e,Ke,c),t(Ke,Vl),t(Ke,Ua),t(Ua,Yl),t(Ke,Jl),u(e,Ko,c),E(wt,e,c),u(e,Vo,c),u(e,Ee,c),t(Ee,Zl),t(Ee,jt),t(jt,Wl),t(Ee,Xl),t(Ee,At),t(At,ei),t(Ee,ti),u(e,Yo,c),E(qt,e,c),u(e,Jo,c),u(e,Ve,c),t(Ve,ai),t(Ve,Ga),t(Ga,oi),t(Ve,si),u(e,Zo,c),E(Tt,e,c),u(e,Wo,c),u(e,Yt,c),t(Yt,li),u(e,Xo,c),E(Ct,e,c),u(e,es,c),u(e,Ye,c),t(Ye,ii),t(Ye,Jt),t(Jt,ri),t(Ye,ni),u(e,ts,c),u(e,Me,c),t(Me,Je),t(Je,Qa),E(Pt,Qa,null),t(Me,pi),t(Me,Ha),t(Ha,ci),u(e,as,c),u(e,ce,c),t(ce,ui),t(ce,Ba),t(Ba,mi),t(ce,fi),t(ce,yt),t(yt,di),t(ce,$i),t(ce,Ka),t(Ka,hi),t(ce,gi),t(ce,St),t(St,_i),t(ce,vi),u(e,os,c),E(Mt,e,c),u(e,ss,c),E(Ze,e,c),u(e,ls,c),u(e,we,c),t(we,zi),t(we,Va),t(Va,ki),t(we,bi),t(we,Ya),t(Ya,Ei),t(we,wi),u(e,is,c),E(Dt,e,c),u(e,rs,c),u(e,je,c),t(je,ji),t(je,Zt),t(Zt,Ai),t(je,qi),t(je,Wt),t(Wt,Ti),t(je,Ci),u(e,ns,c),u(e,De,c),t(De,We),t(We,Ja),E(xt,Ja,null),t(De,Pi),t(De,Za),t(Za,yi),u(e,ps,c),E(Ft,e,c),u(e,cs,c),u(e,W,c),t(W,Si),t(W,Wa),t(Wa,Mi),t(W,Di),t(W,Xa),t(Xa,xi),t(W,Fi),t(W,eo),t(eo,Oi),t(W,Ii),t(W,Xt),t(Xt,Li),t(W,Ni),t(W,to),t(to,Ri),t(W,Ui),t(W,ao),t(ao,Gi),t(W,Qi),u(e,us,c),u(e,Ae,c),t(Ae,Hi),t(Ae,oo),t(oo,Bi),t(Ae,Ki),t(Ae,so),t(so,Vi),t(Ae,Yi),u(e,ms,c),u(e,xe,c),t(xe,Xe),t(Xe,lo),E(Ot,lo,null),t(xe,Ji),t(xe,io),t(io,Zi),u(e,fs,c),u(e,qe,c),t(qe,Wi),t(qe,ro),t(ro,Xi),t(qe,er),t(qe,ea),t(ea,tr),t(qe,ar),u(e,ds,c),u(e,et,c),t(et,or),t(et,no),t(no,sr),t(et,lr),u(e,$s,c),E(It,e,c),u(e,hs,c),u(e,tt,c),t(tt,ir),t(tt,po),t(po,rr),t(tt,nr),u(e,gs,c),u(e,ta,c),t(ta,pr),u(e,_s,c),E(Lt,e,c),u(e,vs,c),u(e,aa,c),t(aa,cr),u(e,zs,c),u(e,at,c),t(at,oa),t(oa,sa),t(sa,ur),t(oa,mr),t(at,fr),t(at,la),t(la,ia),t(ia,dr),t(la,$r),u(e,ks,c),u(e,ot,c),t(ot,hr),t(ot,co),t(co,gr),t(ot,_r),u(e,bs,c),E(st,e,c),u(e,Es,c),u(e,lt,c),t(lt,vr),t(lt,ra),t(ra,zr),t(lt,kr),u(e,ws,c),u(e,Fe,c),t(Fe,it),t(it,uo),E(Nt,uo,null),t(Fe,br),t(Fe,mo),t(mo,Er),u(e,js,c),E(rt,e,c),u(e,As,c),E(nt,e,c),u(e,qs,c),u(e,X,c),t(X,wr),t(X,Rt),t(Rt,fo),t(fo,jr),t(X,Ar),t(X,Ut),t(Ut,$o),t($o,qr),t(X,Tr),t(X,ho),t(ho,Cr),t(X,Pr),t(X,go),t(go,yr),t(X,Sr),t(X,Gt),t(Gt,Mr),t(X,Dr),t(X,na),t(na,xr),t(X,Fr),u(e,Ts,c),E(pt,e,c),u(e,Cs,c),u(e,Oe,c),t(Oe,ct),t(ct,_o),E(Qt,_o,null),t(Oe,Or),t(Oe,vo),t(vo,Ir),u(e,Ps,c),E(ut,e,c),u(e,ys,c),u(e,Te,c),t(Te,Lr),t(Te,zo),t(zo,Nr),t(Te,Rr),t(Te,ko),t(ko,Ur),t(Te,Gr),u(e,Ss,c),E(mt,e,c),Ms=!0},p(e,[c]){const Ht={};c&2&&(Ht.$$scope={dirty:c,ctx:e}),K.$set(Ht);const bo={};c&2&&(bo.$$scope={dirty:c,ctx:e}),Re.$set(bo);const Eo={};c&2&&(Eo.$$scope={dirty:c,ctx:e}),Qe.$set(Eo);const wo={};c&2&&(wo.$$scope={dirty:c,ctx:e}),Ze.$set(wo);const Ie={};c&2&&(Ie.$$scope={dirty:c,ctx:e}),st.$set(Ie);const jo={};c&2&&(jo.$$scope={dirty:c,ctx:e}),rt.$set(jo);const Ao={};c&2&&(Ao.$$scope={dirty:c,ctx:e}),nt.$set(Ao);const Bt={};c&2&&(Bt.$$scope={dirty:c,ctx:e}),pt.$set(Bt);const qo={};c&2&&(qo.$$scope={dirty:c,ctx:e}),ut.$set(qo);const To={};c&2&&(To.$$scope={dirty:c,ctx:e}),mt.$set(To)},i(e){Ms||(w(_.$$.fragment,e),w(F.$$.fragment,e),w(K.$$.fragment,e),w(J.$$.fragment,e),w(I.$$.fragment,e),w(Re.$$.fragment,e),w(gt.$$.fragment,e),w(Qe.$$.fragment,e),w(_t.$$.fragment,e),w(zt.$$.fragment,e),w(kt.$$.fragment,e),w(Et.$$.fragment,e),w(wt.$$.fragment,e),w(qt.$$.fragment,e),w(Tt.$$.fragment,e),w(Ct.$$.fragment,e),w(Pt.$$.fragment,e),w(Mt.$$.fragment,e),w(Ze.$$.fragment,e),w(Dt.$$.fragment,e),w(xt.$$.fragment,e),w(Ft.$$.fragment,e),w(Ot.$$.fragment,e),w(It.$$.fragment,e),w(Lt.$$.fragment,e),w(st.$$.fragment,e),w(Nt.$$.fragment,e),w(rt.$$.fragment,e),w(nt.$$.fragment,e),w(pt.$$.fragment,e),w(Qt.$$.fragment,e),w(ut.$$.fragment,e),w(mt.$$.fragment,e),Ms=!0)},o(e){j(_.$$.fragment,e),j(F.$$.fragment,e),j(K.$$.fragment,e),j(J.$$.fragment,e),j(I.$$.fragment,e),j(Re.$$.fragment,e),j(gt.$$.fragment,e),j(Qe.$$.fragment,e),j(_t.$$.fragment,e),j(zt.$$.fragment,e),j(kt.$$.fragment,e),j(Et.$$.fragment,e),j(wt.$$.fragment,e),j(qt.$$.fragment,e),j(Tt.$$.fragment,e),j(Ct.$$.fragment,e),j(Pt.$$.fragment,e),j(Mt.$$.fragment,e),j(Ze.$$.fragment,e),j(Dt.$$.fragment,e),j(xt.$$.fragment,e),j(Ft.$$.fragment,e),j(Ot.$$.fragment,e),j(It.$$.fragment,e),j(Lt.$$.fragment,e),j(st.$$.fragment,e),j(Nt.$$.fragment,e),j(rt.$$.fragment,e),j(nt.$$.fragment,e),j(pt.$$.fragment,e),j(Qt.$$.fragment,e),j(ut.$$.fragment,e),j(mt.$$.fragment,e),Ms=!1},d(e){a(o),e&&a(m),e&&a(s),A(_),e&&a(M),A(F,e),e&&a(O),e&&a(L),e&&a(V),A(K,e),e&&a(te),e&&a(Y),A(J),e&&a($e),e&&a(le),e&&a(C),A(I,e),e&&a(re),e&&a(v),e&&a(_e),e&&a(ne),e&&a(dt),e&&a(B),e&&a(Co),e&&a($t),e&&a(Po),e&&a(ze),e&&a(yo),e&&a(ht),e&&a(So),e&&a(Ne),e&&a(Mo),A(Re,e),e&&a(Do),e&&a(Se),A(gt),e&&a(xo),e&&a(Ge),e&&a(Fo),e&&a(Vt),e&&a(Oo),A(Qe,e),e&&a(Io),e&&a(He),e&&a(Lo),A(_t,e),e&&a(No),e&&a(ke),e&&a(Ro),A(zt,e),e&&a(Uo),e&&a(Be),e&&a(Go),A(kt,e),e&&a(Qo),e&&a(be),e&&a(Ho),A(Et,e),e&&a(Bo),e&&a(Ke),e&&a(Ko),A(wt,e),e&&a(Vo),e&&a(Ee),e&&a(Yo),A(qt,e),e&&a(Jo),e&&a(Ve),e&&a(Zo),A(Tt,e),e&&a(Wo),e&&a(Yt),e&&a(Xo),A(Ct,e),e&&a(es),e&&a(Ye),e&&a(ts),e&&a(Me),A(Pt),e&&a(as),e&&a(ce),e&&a(os),A(Mt,e),e&&a(ss),A(Ze,e),e&&a(ls),e&&a(we),e&&a(is),A(Dt,e),e&&a(rs),e&&a(je),e&&a(ns),e&&a(De),A(xt),e&&a(ps),A(Ft,e),e&&a(cs),e&&a(W),e&&a(us),e&&a(Ae),e&&a(ms),e&&a(xe),A(Ot),e&&a(fs),e&&a(qe),e&&a(ds),e&&a(et),e&&a($s),A(It,e),e&&a(hs),e&&a(tt),e&&a(gs),e&&a(ta),e&&a(_s),A(Lt,e),e&&a(vs),e&&a(aa),e&&a(zs),e&&a(at),e&&a(ks),e&&a(ot),e&&a(bs),A(st,e),e&&a(Es),e&&a(lt),e&&a(ws),e&&a(Fe),A(Nt),e&&a(js),A(rt,e),e&&a(As),A(nt,e),e&&a(qs),e&&a(X),e&&a(Ts),A(pt,e),e&&a(Cs),e&&a(Oe),A(Qt),e&&a(Ps),A(ut,e),e&&a(ys),e&&a(Te),e&&a(Ss),A(mt,e)}}}const dc={local:"quick-tour",sections:[{local:"pipeline",sections:[{local:"utilizzo-della-pipeline",title:"Utilizzo della Pipeline"},{local:"utilizzare-un-altro-modello-e-tokenizer-nella-pipeline",title:"Utilizzare un altro modello e tokenizer nella pipeline"}],title:"Pipeline"},{local:"autoclass",sections:[{local:"autotokenizer",title:"AutoTokenizer"},{local:"automodel",title:"AutoModel"},{local:"salva-un-modello",title:"Salva un modello"}],title:"AutoClass"}],title:"Quick tour"};function $c(y){return xp(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class wc extends yp{constructor(o){super();Sp(this,o,$c,fc,Mp,{})}}export{wc as default,dc as metadata};
