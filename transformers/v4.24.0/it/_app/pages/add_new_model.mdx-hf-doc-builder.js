import{S as yP,i as qP,s as LP,e as r,k as d,w as u,t,M as IP,c as l,d as i,m as p,a as n,x as f,h as a,b as m,N as CP,G as o,g as c,y as v,q as g,o as h,B as _,v as OP}from"../chunks/vendor-hf-doc-builder.js";import{T as AP}from"../chunks/Tip-hf-doc-builder.js";import{I as pe}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as w}from"../chunks/CodeBlock-hf-doc-builder.js";function NP(Pd){let k,Ye,C,F,qe,j,ui;return{c(){k=r("p"),Ye=t("Nel caso siate su Windows, sostituite "),C=r("code"),F=t("RUN_SLOW=1"),qe=t(" con "),j=r("code"),ui=t("SET RUN_SLOW=1")},l(le){k=l(le,"P",{});var ne=n(k);Ye=a(ne,"Nel caso siate su Windows, sostituite "),C=l(ne,"CODE",{});var fi=n(C);F=a(fi,"RUN_SLOW=1"),fi.forEach(i),qe=a(ne," con "),j=l(ne,"CODE",{});var M=n(j);ui=a(M,"SET RUN_SLOW=1"),M.forEach(i),ne.forEach(i)},m(le,ne){c(le,k,ne),o(k,Ye),o(k,C),o(C,F),o(k,qe),o(k,j),o(j,ui)},d(le){le&&i(k)}}}function BP(Pd){let k,Ye,C,F,qe,j,ui,le,ne,fi,M,Vf,Ir,Xf,Zf,Cr,Yf,ev,$d,eo,ov,vi,iv,tv,Td,Ft,av,kd,Q,Or,rv,lv,Ar,nv,sv,Nr,cv,dv,se,pv,Br,mv,uv,Dr,fv,vv,Sr,gv,hv,yd,oo,_v,gi,bv,Ev,qd,Qt,zv,Ld,Le,io,jr,hi,wv,Mr,Pv,Id,Gt,$v,Cd,to,Tv,Ht,kv,yv,Od,me,Rr,qv,Lv,Ur,Iv,Cv,_i,Ov,Fr,Av,Nv,Ad,ao,Bv,Qr,Dv,Sv,Nd,xt,jv,Bd,Ie,ro,Gr,bi,Mv,Hr,Rv,Dd,O,Uv,xr,Fv,Qv,Wr,Gv,Hv,xv,Wv,Jr,Jv,Kv,Sd,Wt,Vv,jd,Jt,C5,Md,b,Xv,Kr,Zv,Yv,Vr,eg,og,Xr,ig,tg,Zr,ag,rg,Yr,lg,ng,el,sg,cg,ol,dg,pg,il,mg,ug,tl,fg,vg,al,gg,hg,rl,_g,bg,ll,Eg,zg,nl,wg,Pg,sl,$g,Tg,cl,kg,yg,Rd,Ei,Ud,y,qg,dl,Lg,Ig,pl,Cg,Og,ml,Ag,Ng,ul,Bg,Dg,fl,Sg,jg,Fd,Ce,lo,vl,zi,Mg,gl,Rg,Qd,Kt,Ug,Gd,A,Oe,Fg,hl,Qg,Gg,wi,Hg,xg,Wg,Ae,Jg,_l,Kg,Vg,bl,Xg,Zg,Yg,El,eh,oh,Ne,ih,zl,th,ah,wl,rh,lh,nh,Pl,sh,Hd,Be,no,$l,Pi,ch,Tl,dh,xd,Vt,ph,Wd,De,so,kl,$i,mh,yl,uh,Jd,Xt,fh,Kd,co,Ti,ki,vh,gh,yi,hh,_h,qi,Li,bh,Eh,Ii,zh,Vd,Zt,wh,Xd,ue,ce,Ph,Ci,$h,Th,Oi,kh,yh,ql,qh,Lh,Ih,Ll,Ch,Oh,Il,Ah,Zd,Yt,Nh,Yd,ea,Bh,ep,E,Cl,Ol,Al,Dh,Sh,Nl,oa,Bl,jh,Mh,Dl,ia,Sl,Rh,Uh,jl,ta,Ml,Fh,Qh,Rl,aa,Ul,Gh,Hh,Fl,ra,Ql,xh,Wh,Gl,la,Hl,Jh,Kh,xl,na,Wl,Vh,Xh,Jl,sa,Kl,Zh,Yh,Vl,ca,Xl,e_,o_,Zl,da,Yl,i_,t_,en,pa,on,a_,r_,tn,ma,an,l_,n_,rn,ua,ln,s_,op,N,c_,nn,d_,p_,sn,m_,u_,cn,f_,v_,dn,g_,h_,ip,Se,po,pn,Ai,__,mn,b_,tp,mo,E_,un,z_,w_,ap,B,je,P_,fn,$_,T_,fa,k_,y_,q_,Ni,L_,vn,I_,C_,O_,gn,A_,N_,Me,B_,Bi,D_,S_,hn,j_,M_,R_,_n,U_,rp,va,F_,lp,Re,uo,bn,Di,Q_,En,G_,np,fo,zn,Si,H_,ji,x_,W_,J_,wn,Mi,K_,Pn,V_,X_,sp,Ri,cp,Ui,$n,Z_,dp,Fi,pp,ga,Y_,mp,Qi,up,Gi,Ue,e1,Tn,o1,i1,Hi,t1,a1,fp,xi,kn,r1,l1,vp,Wi,Fe,n1,yn,s1,c1,qn,d1,p1,gp,Ji,hp,vo,m1,Ln,u1,f1,_p,Qe,go,In,Ki,v1,Cn,g1,bp,q,h1,On,_1,b1,An,E1,z1,Nn,w1,P1,Bn,$1,T1,Dn,k1,y1,Ep,ho,q1,Sn,L1,I1,zp,L,jn,C1,O1,Mn,A1,N1,Rn,B1,D1,Un,S1,j1,R,M1,Fn,R1,U1,Qn,F1,Q1,Gn,G1,H1,Hn,x1,W1,J1,Ge,K1,xn,V1,X1,Wn,Z1,Y1,wp,_o,eb,Jn,ob,ib,Pp,ha,tb,$p,_a,ab,Tp,bo,Vi,Xi,rb,lb,Zi,nb,sb,Kn,cb,kp,ba,db,yp,Eo,pb,Vn,mb,ub,qp,zo,fb,Xn,vb,gb,Lp,Yi,Ip,Ea,hb,Cp,wo,Zn,_b,bb,He,Eb,Yn,zb,wb,es,Pb,$b,Op,za,Tb,Ap,Po,kb,os,yb,qb,Np,G,is,Lb,Ib,ts,Cb,Ob,as,Ab,Nb,rs,Bb,Bp,$o,Db,et,Sb,jb,Dp,To,Mb,ot,Rb,Ub,Sp,wa,Fb,jp,I,ls,Qb,Gb,ns,Hb,xb,ss,Wb,Jb,cs,Kb,Vb,it,Xb,ds,Zb,Yb,e0,ps,o0,Mp,xe,i0,ms,t0,a0,us,r0,Rp,Pa,l0,Up,tt,Fp,ko,n0,fs,s0,c0,Qp,D,U,d0,at,p0,m0,vs,u0,f0,gs,v0,g0,rt,h0,_0,b0,hs,E0,z0,$,w0,_s,P0,$0,bs,T0,k0,Es,y0,q0,zs,L0,I0,ws,C0,O0,Ps,A0,N0,$s,B0,D0,Ts,S0,j0,ks,M0,R0,ys,U0,F0,Q0,qs,G0,H0,de,x0,Ls,W0,J0,Is,K0,V0,Cs,X0,Z0,Gp,yo,Y0,Os,e2,o2,Hp,We,qo,As,lt,i2,Ns,t2,xp,$a,a2,Wp,nt,Jp,Lo,r2,Ta,l2,n2,Kp,ka,s2,Vp,Io,ya,Bs,c2,d2,p2,qa,Ds,m2,u2,Xp,fe,f2,Ss,v2,g2,st,h2,_2,Zp,La,js,b2,Yp,ve,E2,Ms,z2,w2,Rs,P2,$2,em,Ia,T2,om,Ca,Us,k2,im,ct,tm,dt,Fs,y2,am,pt,rm,mt,Qs,q2,lm,ut,nm,ft,Gs,L2,sm,vt,cm,Je,Hs,xs,I2,C2,Ws,Js,O2,dm,Oa,A2,pm,gt,mm,Aa,N2,um,Na,B2,fm,Ba,D2,vm,Da,Ks,S2,gm,H,j2,M2,R2,Vs,U2,F2,Xs,Q2,G2,hm,x,H2,Zs,x2,W2,Ys,J2,K2,ec,V2,X2,_m,Ke,oc,Z2,Y2,ic,eE,oE,bm,ht,Em,ge,iE,tc,tE,aE,ac,rE,lE,zm,Sa,rc,nE,wm,he,sE,lc,cE,dE,nc,pE,mE,Pm,Co,ja,uE,_t,fE,vE,Ma,gE,bt,hE,$m,Oo,_E,sc,bE,EE,Tm,Et,km,W,zE,cc,wE,PE,dc,$E,TE,pc,kE,yE,ym,zt,qm,Ra,qE,Lm,wt,Im,Ua,LE,Cm,Pt,Om,Fa,IE,Am,$t,Nm,Tt,CE,mc,OE,Bm,kt,Dm,J,AE,uc,NE,BE,fc,DE,SE,vc,jE,ME,Sm,yt,jm,Qa,RE,Mm,qt,Rm,Ga,UE,Um,Ao,FE,gc,QE,GE,Fm,K,HE,hc,xE,WE,_c,JE,KE,bc,VE,XE,Qm,V,ZE,Ec,YE,e3,zc,o3,i3,wc,t3,a3,Gm,Lt,Hm,Ha,Pc,r3,xm,No,l3,xa,n3,s3,Wm,It,Jm,X,c3,$c,d3,p3,Tc,m3,u3,kc,f3,v3,Km,_e,g3,yc,h3,_3,qc,b3,E3,Vm,Z,Ve,z3,Lc,w3,P3,Ic,$3,T3,k3,Cc,y3,q3,Oc,L3,I3,Y,C3,Ac,O3,A3,Nc,N3,B3,Bc,D3,S3,Ct,j3,Xm,Bo,M3,Dc,R3,U3,Zm,Do,F3,Sc,Q3,G3,Ym,Wa,jc,H3,eu,So,x3,Mc,W3,J3,ou,Ot,iu,Ja,K3,tu,jo,At,V3,Rc,X3,Z3,Y3,Uc,e4,au,Mo,o4,Fc,i4,t4,ru,Nt,lu,Ro,nu,ee,a4,Qc,r4,l4,Gc,n4,s4,Hc,c4,d4,su,Uo,xc,p4,m4,Wc,u4,cu,Ka,Jc,f4,du,Fo,v4,Kc,g4,h4,pu,Va,_4,mu,Qo,b4,Vc,E4,z4,uu,Bt,fu,Go,w4,Xc,P4,$4,vu,Dt,gu,Ho,T4,Zc,k4,y4,hu,xo,q4,Yc,L4,I4,_u,Xa,ed,C4,bu,oe,O4,od,A4,N4,id,B4,D4,td,S4,j4,Eu,Za,ad,M4,zu,be,R4,rd,U4,F4,ld,Q4,G4,wu,Wo,H4,nd,x4,W4,Pu,Ya,sd,J4,$u,Jo,K4,cd,V4,X4,Tu,St,ku,er,Z4,yu,jt,qu,or,Y4,Lu,ir,e5,Iu,tr,o5,Cu,ar,dd,i5,Ou,S,t5,rr,a5,r5,pd,l5,n5,md,s5,c5,ud,d5,p5,Au,Mt,Nu,Ko,m5,fd,u5,f5,Bu,lr,vd,v5,Du,Vo,g5,gd,h5,_5,Su,nr,hd,b5,ju,sr,E5,Mu,Xe,Xo,_d,Rt,z5,bd,w5,Ru,cr,P5,Uu,dr,Ed,$5,Fu;return j=new pe({}),hi=new pe({}),bi=new pe({}),Ei=new w({props:{code:`model = BrandNewBertModel.from_pretrained("brandy/brand_new_bert")
model.config  # il modello ha accesso al suo config`,highlighted:`model = BrandNewBertModel.from_pretrained(<span class="hljs-string">&quot;brandy/brand_new_bert&quot;</span>)
model.config  <span class="hljs-comment"># il modello ha accesso al suo config</span>`}}),zi=new pe({}),Pi=new pe({}),$i=new pe({}),Ai=new pe({}),Di=new pe({}),Ri=new w({props:{code:`git clone https://github.com/[your Github handle]/transformers.git
cd transformers
git remote add upstream https://github.com/huggingface/transformers.git`,highlighted:`git <span class="hljs-built_in">clone</span> https://github.com/[your Github handle]/transformers.git
<span class="hljs-built_in">cd</span> transformers
git remote add upstream https://github.com/huggingface/transformers.git`}}),Fi=new w({props:{code:`python -m venv .env
source .env/bin/activate
pip install -e ".[dev]"`,highlighted:`python -m venv .<span class="hljs-built_in">env</span>
<span class="hljs-built_in">source</span> .<span class="hljs-built_in">env</span>/bin/activate
pip install -e <span class="hljs-string">&quot;.[dev]&quot;</span>`}}),Qi=new w({props:{code:"cd ..",highlighted:'<span class="hljs-built_in">cd</span> ..'}}),Ji=new w({props:{code:`git clone https://github.com/org_that_created_brand_new_bert_org/brand_new_bert.git 
cd brand_new_bert
pip install -e .`,highlighted:`git <span class="hljs-built_in">clone</span> https://github.com/org_that_created_brand_new_bert_org/brand_new_bert.git 
<span class="hljs-built_in">cd</span> brand_new_bert
pip install -e .`}}),Ki=new pe({}),Yi=new w({props:{code:`model = BrandNewBertModel.load_pretrained_checkpoint("/path/to/checkpoint/")
input_ids = [0, 4, 5, 2, 3, 7, 9]  # vector of input ids
original_output = model.predict(input_ids)`,highlighted:`model = BrandNewBertModel.load_pretrained_checkpoint(<span class="hljs-string">&quot;/path/to/checkpoint/&quot;</span>)
input_ids = [<span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>]  <span class="hljs-comment"># vector of input ids</span>
original_output = model.predict(input_ids)`}}),tt=new w({props:{code:`[[
 [-0.1465, -0.6501,  0.1993,  ...,  0.1451,  0.3430,  0.6024],
 [-0.4417, -0.5920,  0.3450,  ..., -0.3062,  0.6182,  0.7132],
 [-0.5009, -0.7122,  0.4548,  ..., -0.3662,  0.6091,  0.7648],
 ...,
 [-0.5613, -0.6332,  0.4324,  ..., -0.3792,  0.7372,  0.9288],
 [-0.5416, -0.6345,  0.4180,  ..., -0.3564,  0.6992,  0.9191],
 [-0.5334, -0.6403,  0.4271,  ..., -0.3339,  0.6533,  0.8694]]],`,highlighted:`<span class="hljs-comment">[<span class="hljs-comment">[
 <span class="hljs-comment">[-0.1465, -0.6501,  0.1993,  ...,  0.1451,  0.3430,  0.6024]</span>,
 <span class="hljs-comment">[-0.4417, -0.5920,  0.3450,  ..., -0.3062,  0.6182,  0.7132]</span>,
 <span class="hljs-comment">[-0.5009, -0.7122,  0.4548,  ..., -0.3662,  0.6091,  0.7648]</span>,
 ...,
 <span class="hljs-comment">[-0.5613, -0.6332,  0.4324,  ..., -0.3792,  0.7372,  0.9288]</span>,
 <span class="hljs-comment">[-0.5416, -0.6345,  0.4180,  ..., -0.3564,  0.6992,  0.9191]</span>,
 <span class="hljs-comment">[-0.5334, -0.6403,  0.4271,  ..., -0.3339,  0.6533,  0.8694]</span>]</span>]</span>,`}}),lt=new pe({}),nt=new w({props:{code:"cd transformers",highlighted:'<span class="hljs-built_in">cd</span> transformers'}}),ct=new w({props:{code:"git checkout -b add_brand_new_bert ",highlighted:"git checkout -b add_brand_new_bert "}}),pt=new w({props:{code:`git add . 
git commit `,highlighted:`git add . 
git commit `}}),ut=new w({props:{code:`git fetch upstream 
git rebase upstream/main `,highlighted:`git fetch upstream 
git rebase upstream/main `}}),vt=new w({props:{code:"git push -u origin a-descriptive-name-for-my-changes",highlighted:"git push -u origin a-descriptive-name-for-my-changes"}}),gt=new w({props:{code:`git fetch upstream
git merge upstream/main`,highlighted:`git fetch upstream
git merge upstream/main`}}),ht=new w({props:{code:`from transformers import BrandNewBertModel, BrandNewBertConfig

model = BrandNewBertModel(BrandNewBertConfig())`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BrandNewBertModel, BrandNewBertConfig

model = BrandNewBertModel(BrandNewBertConfig())`}}),Et=new w({props:{code:`from torch import nn


class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.dense = nn.Linear(10, 10)
        self.intermediate = nn.Linear(10, 10)
        self.layer_norm = nn.LayerNorm(10)`,highlighted:`<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn


<span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleModel</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):
        <span class="hljs-built_in">super</span>().__init__()
        self.dense = nn.Linear(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)
        self.intermediate = nn.Linear(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>)
        self.layer_norm = nn.LayerNorm(<span class="hljs-number">10</span>)`}}),zt=new w({props:{code:`model = SimpleModel()

print(model)`,highlighted:`model = SimpleModel()

<span class="hljs-built_in">print</span>(model)`}}),wt=new w({props:{code:`SimpleModel(
  (dense): Linear(in_features=10, out_features=10, bias=True)
  (intermediate): Linear(in_features=10, out_features=10, bias=True)
  (layer_norm): LayerNorm((10,), eps=1e-05, elementwise_affine=True)
)`,highlighted:`SimpleModel(
  (dense): Linear(<span class="hljs-attribute">in_features</span>=10, <span class="hljs-attribute">out_features</span>=10, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)
  (intermediate): Linear(<span class="hljs-attribute">in_features</span>=10, <span class="hljs-attribute">out_features</span>=10, <span class="hljs-attribute">bias</span>=<span class="hljs-literal">True</span>)
  (layer_norm): LayerNorm((10,), <span class="hljs-attribute">eps</span>=1e-05, <span class="hljs-attribute">elementwise_affine</span>=<span class="hljs-literal">True</span>)
)`}}),Pt=new w({props:{code:"print(model.dense.weight.data)",highlighted:'<span class="hljs-built_in">print</span>(model.dense.weight.data)'}}),$t=new w({props:{code:`tensor([[-0.0818,  0.2207, -0.0749, -0.0030,  0.0045, -0.1569, -0.1598,  0.0212,
         -0.2077,  0.2157],
        [ 0.1044,  0.0201,  0.0990,  0.2482,  0.3116,  0.2509,  0.2866, -0.2190,
          0.2166, -0.0212],
        [-0.2000,  0.1107, -0.1999, -0.3119,  0.1559,  0.0993,  0.1776, -0.1950,
         -0.1023, -0.0447],
        [-0.0888, -0.1092,  0.2281,  0.0336,  0.1817, -0.0115,  0.2096,  0.1415,
         -0.1876, -0.2467],
        [ 0.2208, -0.2352, -0.1426, -0.2636, -0.2889, -0.2061, -0.2849, -0.0465,
          0.2577,  0.0402],
        [ 0.1502,  0.2465,  0.2566,  0.0693,  0.2352, -0.0530,  0.1859, -0.0604,
          0.2132,  0.1680],
        [ 0.1733, -0.2407, -0.1721,  0.1484,  0.0358, -0.0633, -0.0721, -0.0090,
          0.2707, -0.2509],
        [-0.1173,  0.1561,  0.2945,  0.0595, -0.1996,  0.2988, -0.0802,  0.0407,
          0.1829, -0.1568],
        [-0.1164, -0.2228, -0.0403,  0.0428,  0.1339,  0.0047,  0.1967,  0.2923,
          0.0333, -0.0536],
        [-0.1492, -0.1616,  0.1057,  0.1950, -0.2807, -0.2710, -0.1586,  0.0739,
          0.2220,  0.2358]]).`,highlighted:`tensor([[<span class="hljs-string">-0</span>.0818,  0.2207, <span class="hljs-string">-0</span>.0749, <span class="hljs-string">-0</span>.0030,  0.0045, <span class="hljs-string">-0</span>.1569, <span class="hljs-string">-0</span>.1598,  0.0212,
         <span class="hljs-string">-0</span>.2077,  0.2157],
        [ 0.1044,  0.0201,  0.0990,  0.2482,  0.3116,  0.2509,  0.2866, <span class="hljs-string">-0</span>.2190,
          0.2166, <span class="hljs-string">-0</span>.0212],
        [<span class="hljs-string">-0</span>.2000,  0.1107, <span class="hljs-string">-0</span>.1999, <span class="hljs-string">-0</span>.3119,  0.1559,  0.0993,  0.1776, <span class="hljs-string">-0</span>.1950,
         <span class="hljs-string">-0</span>.1023, <span class="hljs-string">-0</span>.0447],
        [<span class="hljs-string">-0</span>.0888, <span class="hljs-string">-0</span>.1092,  0.2281,  0.0336,  0.1817, <span class="hljs-string">-0</span>.0115,  0.2096,  0.1415,
         <span class="hljs-string">-0</span>.1876, <span class="hljs-string">-0</span>.2467],
        [ 0.2208, <span class="hljs-string">-0</span>.2352, <span class="hljs-string">-0</span>.1426, <span class="hljs-string">-0</span>.2636, <span class="hljs-string">-0</span>.2889, <span class="hljs-string">-0</span>.2061, <span class="hljs-string">-0</span>.2849, <span class="hljs-string">-0</span>.0465,
          0.2577,  0.0402],
        [ 0.1502,  0.2465,  0.2566,  0.0693,  0.2352, <span class="hljs-string">-0</span>.0530,  0.1859, <span class="hljs-string">-0</span>.0604,
          0.2132,  0.1680],
        [ 0.1733, <span class="hljs-string">-0</span>.2407, <span class="hljs-string">-0</span>.1721,  0.1484,  0.0358, <span class="hljs-string">-0</span>.0633, <span class="hljs-string">-0</span>.0721, <span class="hljs-string">-0</span>.0090,
          0.2707, <span class="hljs-string">-0</span>.2509],
        [<span class="hljs-string">-0</span>.1173,  0.1561,  0.2945,  0.0595, <span class="hljs-string">-0</span>.1996,  0.2988, <span class="hljs-string">-0</span>.0802,  0.0407,
          0.1829, <span class="hljs-string">-0</span>.1568],
        [<span class="hljs-string">-0</span>.1164, <span class="hljs-string">-0</span>.2228, <span class="hljs-string">-0</span>.0403,  0.0428,  0.1339,  0.0047,  0.1967,  0.2923,
          0.0333, <span class="hljs-string">-0</span>.0536],
        [<span class="hljs-string">-0</span>.1492, <span class="hljs-string">-0</span>.1616,  0.1057,  0.1950, <span class="hljs-string">-0</span>.2807, <span class="hljs-string">-0</span>.2710, <span class="hljs-string">-0</span>.1586,  0.0739,
          0.2220,  0.2358]]).`}}),kt=new w({props:{code:`# retrieve matching layer weights, e.g. by
# recursive algorithm
layer_name = "dense"
pretrained_weight = array_of_dense_layer

model_pointer = getattr(model, "dense")

model_pointer.weight.data = torch.from_numpy(pretrained_weight)`,highlighted:`<span class="hljs-comment"># retrieve matching layer weights, e.g. by</span>
<span class="hljs-comment"># recursive algorithm</span>
layer_name = <span class="hljs-string">&quot;dense&quot;</span>
pretrained_weight = array_of_dense_layer

model_pointer = <span class="hljs-built_in">getattr</span>(model, <span class="hljs-string">&quot;dense&quot;</span>)

model_pointer.weight.data = torch.from_numpy(pretrained_weight)`}}),yt=new w({props:{code:`assert (
    model_pointer.weight.shape == pretrained_weight.shape
), f"Pointer shape of random weight {model_pointer.shape} and array shape of checkpoint weight {pretrained_weight.shape} mismatched"`,highlighted:`<span class="hljs-keyword">assert</span> (
    model_pointer.weight.shape == pretrained_weight.shape
), <span class="hljs-string">f&quot;Pointer shape of random weight <span class="hljs-subst">{model_pointer.shape}</span> and array shape of checkpoint weight <span class="hljs-subst">{pretrained_weight.shape}</span> mismatched&quot;</span>`}}),qt=new w({props:{code:'logger.info(f"Initialize PyTorch weight {layer_name} from {pretrained_weight.name}")',highlighted:'logger.info(<span class="hljs-string">f&quot;Initialize PyTorch weight <span class="hljs-subst">{layer_name}</span> from <span class="hljs-subst">{pretrained_weight.name}</span>&quot;</span>)'}}),Lt=new w({props:{code:'model.save_pretrained("/path/to/converted/checkpoint/folder")',highlighted:'model.save_pretrained(<span class="hljs-string">&quot;/path/to/converted/checkpoint/folder&quot;</span>)'}}),It=new w({props:{code:`model = BrandNewBertModel.from_pretrained("/path/to/converted/checkpoint/folder")
input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]
output = model(input_ids).last_hidden_states`,highlighted:`model = BrandNewBertModel.from_pretrained(<span class="hljs-string">&quot;/path/to/converted/checkpoint/folder&quot;</span>)
input_ids = [<span class="hljs-number">0</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">7</span>, <span class="hljs-number">19</span>]
output = model(input_ids).last_hidden_states`}}),Ot=new w({props:{code:"pytest tests/test_modeling_brand_new_bert.py",highlighted:"pytest tests/test_modeling_brand_new_bert.py"}}),Nt=new w({props:{code:"RUN_SLOW=1 pytest -sv tests/test_modeling_brand_new_bert.py::BrandNewBertModelIntegrationTests",highlighted:"RUN_SLOW=1 pytest -sv tests/test_modeling_brand_new_bert.py::BrandNewBertModelIntegrationTests"}}),Ro=new AP({props:{$$slots:{default:[NP]},$$scope:{ctx:Pd}}}),Bt=new w({props:{code:`input_str = "This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words."
model = BrandNewBertModel.load_pretrained_checkpoint("/path/to/checkpoint/")
input_ids = model.tokenize(input_str)`,highlighted:`input_str = <span class="hljs-string">&quot;This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words.&quot;</span>
model = BrandNewBertModel.load_pretrained_checkpoint(<span class="hljs-string">&quot;/path/to/checkpoint/&quot;</span>)
input_ids = model.tokenize(input_str)`}}),Dt=new w({props:{code:`from transformers import BrandNewBertTokenizer

input_str = "This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words."

tokenizer = BrandNewBertTokenizer.from_pretrained("/path/to/tokenizer/folder/")

input_ids = tokenizer(input_str).input_ids`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BrandNewBertTokenizer

input_str = <span class="hljs-string">&quot;This is a long example input string containing special characters .$?-, numbers 2872 234 12 and words.&quot;</span>

tokenizer = BrandNewBertTokenizer.from_pretrained(<span class="hljs-string">&quot;/path/to/tokenizer/folder/&quot;</span>)

input_ids = tokenizer(input_str).input_ids`}}),St=new w({props:{code:"make style",highlighted:"make style"}}),jt=new w({props:{code:"make quality",highlighted:"make quality"}}),Mt=new w({props:{code:`brand_new_bert.push_to_hub(
    repo_path_or_name="brand_new_bert",
    # Uncomment the following line to push to an organization
    # organization="<ORGANIZATION>",
    commit_message="Add model",
    use_temp_dir=True,
)`,highlighted:`brand_new_bert.push_to_hub(
    repo_path_or_name=<span class="hljs-string">&quot;brand_new_bert&quot;</span>,
    <span class="hljs-comment"># Uncomment the following line to push to an organization</span>
    <span class="hljs-comment"># organization=&quot;&lt;ORGANIZATION&gt;&quot;,</span>
    commit_message=<span class="hljs-string">&quot;Add model&quot;</span>,
    use_temp_dir=<span class="hljs-literal">True</span>,
)`}}),Rt=new pe({}),{c(){k=r("meta"),Ye=d(),C=r("h1"),F=r("a"),qe=r("span"),u(j.$$.fragment),ui=d(),le=r("span"),ne=t("Come aggiungere un modello a \u{1F917} Transformers?"),fi=d(),M=r("p"),Vf=t(`Aggiungere un nuovo modello \xE9 spesso difficile e richiede una profonda conoscenza della libreria \u{1F917} Transformers e anche
della repository originale del modello. A Hugging Face cerchiamo di dare alla community sempre pi\xFA poteri per aggiungere
modelli independentemente. Quindi, per alcuni nuovi modelli che la community vuole aggiungere a \u{1F917} Transformers, abbiamo
creato una specifica `),Ir=r("em"),Xf=t("call-for-model-addition"),Zf=t(` che spiega passo dopo passo come aggiungere il modello richiesto. Con
questo `),Cr=r("em"),Yf=t("call-for-model-addition"),ev=t(` vogliamo insegnare a volenterosi e esperti collaboratori della community come implementare
un modello in \u{1F917} Transformers.`),$d=d(),eo=r("p"),ov=t("Se questo \xE9 qualcosa che pu\xF2 interessarvi, siete liberi di controllare l\u2019attuale \u201Ccalls-for-model-addition\u201D "),vi=r("a"),iv=t("qui"),tv=t(`
e contattarci.`),Td=d(),Ft=r("p"),av=t(`Se il modello sar\xE0 selezionato, allora potrete lavorare insieme a un membro di Hugging Face per integrare il modello in \u{1F917}
Transformers. Cos\xEC facendo, ci guadagnerai in una comprensione totale, sia teorica che pratica, del modello proposto. Inoltre,
sarai l\u2019artefice di un importante contributo open-source a \u{1F917} Transformers. Durante l\u2019implementazione avrai l\u2019opportunit\xE0 di:`),kd=d(),Q=r("ul"),Or=r("li"),rv=t("ottenere pi\xF9 comprensione delle best practices in open-source"),lv=d(),Ar=r("li"),nv=t("capire i principi di design di una della librerie NLP pi\xF9 popolari"),sv=d(),Nr=r("li"),cv=t("capire come efficientemente testare complessi modelli NLP"),dv=d(),se=r("li"),pv=t("capire come integrare utilit Python come "),Br=r("code"),mv=t("black"),uv=t(", "),Dr=r("code"),fv=t("isort"),vv=t(", "),Sr=r("code"),gv=t("make fix-copies"),hv=t(" in una libreria per garantire sempre di avere un codice leggibile e pulito"),yd=d(),oo=r("p"),_v=t(`Siamo anche contenti se vuoi aggiungere un modello che non pu\xF2 essere trovato nella cartella \u201Ccalls-for-model-addition\u201D.
Le seguenti sezioni spiegano in dettaglio come aggiungere un nuovo modello. Pu\xF2 anche essere molto utile controllare modelli
gi\xE0 aggiunti `),gi=r("a"),bv=t("qui"),Ev=t(`,
per capire se richiamano il modello che vorreste aggiungere.`),qd=d(),Qt=r("p"),zv=t("Per cominciare, vediamo una panoramica general della libreria Transformers."),Ld=d(),Le=r("h2"),io=r("a"),jr=r("span"),u(hi.$$.fragment),wv=d(),Mr=r("span"),Pv=t("Panoramica generale su \u{1F917} Transformers"),Id=d(),Gt=r("p"),$v=t(`Prima di tutto, vediamo in generale \u{1F917} Transformers. \u{1F917} Transformers \xE9 una libreria molto strutturata, quindi
pu\xE0 essere che a volte ci sia un disaccordo con alcune filosofie della libreria o scelte di design. Dalla nostra esperienza,
tuttavia, abbiamo trovato che le scelte fondamentali di design della libreria sono cruciali per usare \u{1F917} Transformers efficacemente
su larga scala, mantenendo i costi a un livello accettabile.`),Cd=d(),to=r("p"),Tv=t("Un buon primo punto di partenza per capire al meglio la libreria \xE9 leggere la "),Ht=r("a"),kv=t("documentazione sulla nostra filosofia"),yv=t(`
Da qui, ci sono alcune scelte sul modo di lavorare che cerchiamo di applicare a tutti i modelli:`),Od=d(),me=r("ul"),Rr=r("li"),qv=t("La composizione \xE9 generalmente favorita sulla sovra-astrazione"),Lv=d(),Ur=r("li"),Iv=t("Duplicare il codice non \xE9 sempre male, soprattutto se migliora notevolmente la leggibilit\xE0 e accessibilit\xE0 del modello"),Cv=d(),_i=r("li"),Ov=t(`Tutti i files creati per il nuovo modello devono il piu possibile \u201Ccompatti\u201D. Questo vuol dire che quando qualcuno legger\xE1 il codice
di uno specifico modello, potr\xE1 vedere solo il corrispettivo file `),Fr=r("code"),Av=t("modeling_....py"),Nv=t(" senza avere multiple dipendenze."),Ad=d(),ao=r("p"),Bv=t("La cosa pi\xFA importante, \xE9 che consideriamo la libreria non solo un mezzo per dare un prodotto, "),Qr=r("em"),Dv=t("per esempio"),Sv=t(` dare la possibilit\xE0
di usare BERT per inferenza, ma \xE9 anche il prodotto reale che noi vogliamo migliorare sempre pi\xF9. Quindi, quando aggiungi
un modello, non sei solo la persona che user\xE0 il modello, ma rappresenti anche tutti coloro che leggeranno,
cercheranno di capire e modificare il tuo modello.`),Nd=d(),xt=r("p"),jv=t("Tenendo questi principi in mente, immergiamoci nel design generale della libreria."),Bd=d(),Ie=r("h3"),ro=r("a"),Gr=r("span"),u(bi.$$.fragment),Mv=d(),Hr=r("span"),Rv=t("Panoramica sui modelli"),Dd=d(),O=r("p"),Uv=t(`Per aggiungere con successo un modello, \xE9 importante capire l\u2019interazione tra il tuo modello e la sua configurazione,
`),xr=r("code"),Fv=t("PreTrainedModel"),Qv=t(", e "),Wr=r("code"),Gv=t("PretrainedConfig"),Hv=t(". Per dare un esempio, chiameremo il modello da aggiungere a \u{1F917} Transformers"),xv=r("br"),Wv=d(),Jr=r("code"),Jv=t("BrandNewBert"),Kv=t("."),Sd=d(),Wt=r("p"),Vv=t("Diamo un\u2019occhiata:"),jd=d(),Jt=r("img"),Md=d(),b=r("p"),Xv=t(`Come potete vedere, ci basiamo sull\u2019ereditariet\xE0 in \u{1F917} Transformers, tenendo per\xF2 il livello di astrazione a un minimo
assoluto.  Non ci sono mai pi\xF9 di due livelli di astrazione per ogni modello nella libreria. `),Kr=r("code"),Zv=t("BrandNewBertModel"),Yv=t(` eredita
da `),Vr=r("code"),eg=t("BrandNewBertPreTrainedModel"),og=t(" che, a sua volta, eredita da "),Xr=r("code"),ig=t("PreTrainedModel"),tg=t(` -  semplice no?
Come regola generale, vogliamo essere sicuri che un nuovo modello dipenda solo da `),Zr=r("code"),ag=t("PreTrainedModel"),rg=t(`. Le funzionalit\xE0
importanti che sono automaticamente conferite a ogni nuovo modello sono `),Yr=r("code"),lg=t("from_pretrained()"),ng=t(`
e `),el=r("code"),sg=t("save_pretrained()"),cg=t(`, che sono usate per serializzazione e deserializzazione. Tutte le altre importanti
funzionalit\xE0, come ad esempio `),ol=r("code"),dg=t("BrandNewBertModel.forward"),pg=t(` devono essere definite completamente nel nuovo script
`),il=r("code"),mg=t("modeling_brand_new_bert.py"),ug=t(`. Inoltre, vogliamo essere sicuri che un modello con uno specifico head layer, come
`),tl=r("code"),fg=t("BrandNewBertForMaskedLM"),vg=t(" non erediti da "),al=r("code"),gg=t("BrandNewBertModel"),hg=t(", ma piuttosto usi "),rl=r("code"),_g=t("BrandNewBertModel"),bg=t(`
come componente che pu\xF2 essere chiamata nel passaggio forward per mantenere il livello di astrazione basso. Ogni
nuovo modello richieste una classe di configurazione, chiamata `),ll=r("code"),Eg=t("BrandNewBertConfig"),zg=t(`. Questa configurazione \xE9 sempre
mantenuta come un attributo in `),nl=r("code"),wg=t("PreTrainedModel"),Pg=t(", e quindi pu\xF2 essere accessibile tramite l\u2019attributo "),sl=r("code"),$g=t("config"),Tg=t(`
per tutte le classi che ereditano da `),cl=r("code"),kg=t("BrandNewBertPreTrainedModel"),yg=t(":"),Rd=d(),u(Ei.$$.fragment),Ud=d(),y=r("p"),qg=t(`Analogamente al modello, la configurazione eredita le funzionalit\xE0 base di serializzazione e deserializzazione da
`),dl=r("code"),Lg=t("PretrainedConfig"),Ig=t(`. \xC9 da notare che la configurazione e il modello sono sempre serializzati in due formati differenti -
il modello \xE9 serializzato in un file `),pl=r("em"),Cg=t("pytorch_model.bin"),Og=t(" mentre la configurazione con "),ml=r("em"),Ag=t("config.json"),Ng=t(`. Chiamando
`),ul=r("code"),Bg=t("save_pretrained()"),Dg=t(" automaticamente chiamer\xE0 "),fl=r("code"),Sg=t("save_pretrained()"),jg=t(`, cosicch\xE9 sia il
modello che la configurazione siano salvati.`),Fd=d(),Ce=r("h3"),lo=r("a"),vl=r("span"),u(zi.$$.fragment),Mg=d(),gl=r("span"),Rg=t("Stile per il codice"),Qd=d(),Kt=r("p"),Ug=t(`Quando codifichi un nuovo modello, tieni presente che Transformers ha una sua struttura di fondo come libreria, perci\xF2
ci sono alcuni fatti da considerare su come scrivere un codice :-)`),Gd=d(),A=r("ol"),Oe=r("li"),Fg=t(`Il forward pass del tuo modello dev\u2019essere scritto completamente nel file del modello, mentre dev\u2019essere indipendente
da altri modelli nella libreria. Se vuoi riutilizzare un blocco di codice da un altro modello, copia e incolla il codice con un commento `),hl=r("code"),Qg=t("# Copied from"),Gg=t(" in cima al codice (guarda "),wi=r("a"),Hg=t("qui"),xg=t(`
per un ottimo esempio).`),Wg=d(),Ae=r("li"),Jg=t(`Il codice dev\u2019essere interamente comprensibile, anche da persone che non parlano in inglese. Questo significa che le
variabili devono avere un nome descrittivo e bisogna evitare abbreviazioni. Per esempio, `),_l=r("code"),Kg=t("activation"),Vg=t(` \xE9 molto meglio
che `),bl=r("code"),Xg=t("act"),Zg=t(". Le variabili con una lettera sono da evitare fortemente, almeno che non sia per un indce in un for loop."),Yg=d(),El=r("li"),eh=t("Generamente \xE9 meglio avere un codice esplicito e pi\xFA lungo che un codice corto e magico."),oh=d(),Ne=r("li"),ih=t("Evita di subclassare "),zl=r("code"),th=t("nn.Sequential"),ah=t(" in Pytorch, puoi subclassare "),wl=r("code"),rh=t("nn.Module"),lh=t(` e scrivere il forward pass, cosicch\xE9
chiunque pu\xF2 effettuare debug sul tuo codice, aggiungendo print o breaking points.`),nh=d(),Pl=r("li"),sh=t(`La tua function-signature dev\u2019essere type-annoted. Per il resto, \xE9 meglio preferire variabili con un nome accettabile
piuttosto che annotazioni per aumentare la comprensione e leggibilit\xE0 del codice.`),Hd=d(),Be=r("h3"),no=r("a"),$l=r("span"),u(Pi.$$.fragment),ch=d(),Tl=r("span"),dh=t("Panoramica sui tokenizers"),xd=d(),Vt=r("p"),ph=t("Questa sezione sar\xE0 creata al piu presto :-("),Wd=d(),De=r("h2"),so=r("a"),kl=r("span"),u($i.$$.fragment),mh=d(),yl=r("span"),uh=t("Aggiungere un modello a \u{1F917} Transformers passo dopo passo"),Jd=d(),Xt=r("p"),fh=t("Ci sono differenti modi per aggiungere un modello a Hugging Face. Qui trovi una lista di blog posts da parte della community su come aggiungere un modello:"),Kd=d(),co=r("ol"),Ti=r("li"),ki=r("a"),vh=t("Aggiungere GPT2"),gh=t(" scritto da "),yi=r("a"),hh=t("Thomas"),_h=d(),qi=r("li"),Li=r("a"),bh=t("Aggiungere WMT19 MT"),Eh=t(" scritto da "),Ii=r("a"),zh=t("Stas"),Vd=d(),Zt=r("p"),wh=t("Per esperienza, possiamo dirti che quando si aggiunge un modello \xE9 meglio tenere a mente le seguenti considerazioni:"),Xd=d(),ue=r("ul"),ce=r("li"),Ph=t(`Non sfondare una porta gi\xE1 aperta! La maggior parte del codice che aggiungerai per un nuovo modello \u{1F917} Transformers
esiste gi\xE0 da qualche parte in \u{1F917} Transformers. Prendi un po\u2019 di tempo per trovare codici simili in modelli e tokenizers esistenti e fare un copia-incolla. Ricorda che `),Ci=r("a"),$h=t("grep"),Th=t(" e "),Oi=r("a"),kh=t("rg"),yh=t(" sono tuoi buoni amici. Inoltre, ricorda che pu\xF3 essere molto probabile che il tokenizer per il tuo modello sia basato sull\u2019implementazione di un altro modello, e il codice del tuo modello stesso su un altro ancora. "),ql=r("em"),qh=t("Per esempio"),Lh=t(" il modello FSMT \xE9 basato su BART, mentre il tokenizer di FSMT \xE9 basato su XLM."),Ih=d(),Ll=r("li"),Ch=t("Ricorda che qui \xE9 piu una sfida ingegneristica che scientifica. Spendi pi\xFA tempo per create un efficiente ambiente di debugging piuttosto che cercare di capire tutti gli aspetti teorici dell\u2019articolo del modello."),Oh=d(),Il=r("li"),Ah=t("Chiedi aiuto se sei in panne! I modelli sono la parte principale di \u{1F917} Transformers, perci\xF2 qui a Hugging Face siamo pi\xF9 che contenti di aiutarti in ogni passo per aggiungere il tuo modello. Non esitare a chiedere se vedi che non riesci a progredire."),Zd=d(),Yt=r("p"),Nh=t("Di seguito, diamo una ricetta generale per aiutare a portare un modello in \u{1F917} Transformers."),Yd=d(),ea=r("p"),Bh=t("La lista seguente \xE9 un sommario di tutto quello che \xE9 stato fatto per aggiungere un modello, e pu\xF2 essere usata come To-Do List:"),ep=d(),E=r("ul"),Cl=r("li"),Ol=r("ol"),Al=r("li"),Dh=t("\u2610 (Opzionale) Capire gli aspetti teorici del modello"),Sh=d(),Nl=r("li"),oa=r("ol"),Bl=r("li"),jh=t("\u2610 Preparare l\u2019ambiente dev per transformers"),Mh=d(),Dl=r("li"),ia=r("ol"),Sl=r("li"),Rh=t("\u2610 Preparare l\u2019ambiente debugging della repository originale"),Uh=d(),jl=r("li"),ta=r("ol"),Ml=r("li"),Fh=t("\u2610 Create uno script che gestisca con successo il forward pass usando la repository originale e checkpoint"),Qh=d(),Rl=r("li"),aa=r("ol"),Ul=r("li"),Gh=t("\u2610 Aggiungere con successo lo scheletro del modello a Transformers"),Hh=d(),Fl=r("li"),ra=r("ol"),Ql=r("li"),xh=t("\u2610 Convertire i checkpoint original a Transformers checkpoint"),Wh=d(),Gl=r("li"),la=r("ol"),Hl=r("li"),Jh=t("\u2610 Effettuare con successo la forward pass in Transformers, di modo che dia un output identico al checkpoint originale"),Kh=d(),xl=r("li"),na=r("ol"),Wl=r("li"),Vh=t("\u2610 Finire i tests per il modello in Transformers"),Xh=d(),Jl=r("li"),sa=r("ol"),Kl=r("li"),Zh=t("\u2610 Aggiungere con successo Tokenizer in Transformers"),Yh=d(),Vl=r("li"),ca=r("ol"),Xl=r("li"),e_=t("\u2610 Testare e provare gli integration tests da capo a fine"),o_=d(),Zl=r("li"),da=r("ol"),Yl=r("li"),i_=t("\u2610 Completare i docs"),t_=d(),en=r("li"),pa=r("ol"),on=r("li"),a_=t("\u2610 Caricare i moedl weights all\u2019hub"),r_=d(),tn=r("li"),ma=r("ol"),an=r("li"),l_=t("\u2610 Sottomettere una pull request"),n_=d(),rn=r("li"),ua=r("ol"),ln=r("li"),s_=t("\u2610 (Opzionale) Aggiungere un notebook con una demo"),op=d(),N=r("p"),c_=t("Per cominciare di solito consigliamo "),nn=r("code"),d_=t("BrandNewBert"),p_=t(", partendo dalla teoria, di modo da avere una buona comprensione della teoria generale. TUttavia, se preferisci imparare l\u2019aspetto teorico del modello mentre "),sn=r("em"),m_=t("lavori"),u_=t(" sul modello \xE9 ok immergersi direttamente nel codice di "),cn=r("code"),f_=t("BrandNewBert"),v_=t(". Questa opzione pu\xF3 essere buona se le tue skills ingegneristiche sono meglio che quelle teoriche, o se il paper "),dn=r("code"),g_=t("BrandNewBert"),h_=t(" ti d\xE1 problemi, o se semplicemente ti piace programmare pi\xFA che leggere articoli scientifici."),ip=d(),Se=r("h3"),po=r("a"),pn=r("span"),u(Ai.$$.fragment),__=d(),mn=r("span"),b_=t("1. (Opzionale) Aspetti teorici di BrandNewBert"),tp=d(),mo=r("p"),E_=t("Allora con calma, prendi un po\u2019 di tempo per leggere l\u2019articolo su "),un=r("em"),z_=t("BrandNewBert"),w_=t(" . Sicuramente, alcune sezioni dell\u2019articolo sono molto complesse, ma non preoccuparti! L\u2019obiettivo non \xE9 avere una compresione immensa della teoria alla base, ma estrarre le informazioni necessarie per re-implementare con successo il modello in \u{1F917} Transformers. Quindi, non impazzire sugli aspetti teorici, ma piuttosto focalizzati su quelli pratici, ossia:"),ap=d(),B=r("ul"),je=r("li"),P_=t("Che tipo di modello \xE9 "),fn=r("em"),$_=t("brand_new_bert"),T_=t("? \xC9 solo un encoder in stile BERT? O tipo decoder come GPT2? O encoder e decoder stile BART? Dai un\u2019occhiata a "),fa=r("a"),k_=t("model_summary"),y_=t(" se non sei famigliare con le differenze tra questi modelli"),q_=d(),Ni=r("li"),L_=t("Quali sono le applicazioni di "),vn=r("em"),I_=t("brand_new_bert"),C_=t("? Classificazione di testo? Generazione di testo? O per tasks del genere seq2seq?"),O_=d(),gn=r("li"),A_=t("Quali sono le nuove aggiunte al modello che lo rendono diverso da BERT/GPT-2/BART?"),N_=d(),Me=r("li"),B_=t("Quali modelli estistenti in "),Bi=r("a"),D_=t("\u{1F917} Transformers models"),S_=t(" sono molto simili a "),hn=r("em"),j_=t("brand_new_bert"),M_=t("?"),R_=d(),_n=r("li"),U_=t("Che tipo di tokenizer si usa in questo caso? Un sentencepiece tokenizer? O un word piece tokenizer? Il tokenizer \xE9 lo stesso di BERT o BART?"),rp=d(),va=r("p"),F_=t("Una volta che senti che hai avuto una bella overview dell\u2019architettura del modello, puoi scrivere senza problemi al team di Hugging Face per ogni domanda che tu hai. Questo pu\xF3 includere domande sull\u2019architettura del modello, o sull\u2019attention layer, etc. Saremo molto felici di aiutarti :)"),lp=d(),Re=r("h3"),uo=r("a"),bn=r("span"),u(Di.$$.fragment),Q_=d(),En=r("span"),G_=t("2. Prepare il tuo ambiente"),np=d(),fo=r("ol"),zn=r("li"),Si=r("p"),H_=t("Forka la "),ji=r("a"),x_=t("repository"),W_=t(" cliccando sul tasto \u2018Fork\u2019 nella pagina della repository. Questo crea una copia del codice nel tuo account GitHub"),J_=d(),wn=r("li"),Mi=r("p"),K_=t("Clona il tuo fork "),Pn=r("code"),V_=t("transfomers"),X_=t(" sul tuo dico locale, e aggiungi la repository base come remota:"),sp=d(),u(Ri.$$.fragment),cp=d(),Ui=r("ol"),$n=r("li"),Z_=t("Crea un ambiente di sviluppo, per esempio tramite questo comando:"),dp=d(),u(Fi.$$.fragment),pp=d(),ga=r("p"),Y_=t("quindi torna alla directory principale:"),mp=d(),u(Qi.$$.fragment),up=d(),Gi=r("ol"),Ue=r("li"),e1=t("Attenzione, raccomandiamo di aggiungere la versione di PyTorch di "),Tn=r("em"),o1=t("brand_new_bert"),i1=t(" a Transfomers. Per installare PyTorch, basta seguire queste istruzioni "),Hi=r("a"),t1=t("https://pytorch.org/get-started/locally/"),a1=t("."),fp=d(),xi=r("p"),kn=r("strong"),r1=t("Nota bene:"),l1=t(" Non c\u2019\xE9 bisogno di installare o avere installato CUDA. Il nuovo modello pu\xF2 funzionare senza problemi su una CPU."),vp=d(),Wi=r("ol"),Fe=r("li"),n1=t("Per trasferire "),yn=r("em"),s1=t("brand_new_bert"),c1=t(" To port "),qn=r("em"),d1=t("brand_new_bert"),p1=t(" avrai bisogno anche accesso alla sua repository originale:"),gp=d(),u(Ji.$$.fragment),hp=d(),vo=r("p"),m1=t("Ok, ora hai un ambiente di sviluppo per portare "),Ln=r("em"),u1=t("brand_new_bert"),f1=t(" in \u{1F917} Transformers."),_p=d(),Qe=r("h3"),go=r("a"),In=r("span"),u(Ki.$$.fragment),v1=d(),Cn=r("span"),g1=t("3.-4. Provare un pretrained checkpoint usando la repo originale"),bp=d(),q=r("p"),h1=t("Per cominciare, comincerai a lavorare sulla repo originale di "),On=r("em"),_1=t("brand_new_bert"),b1=t(". Come spesso accade, l\u2019implementazione originale \xE9 molto sullo stile \u201Cricerca\u201D. Questo significa che a volte la documentazione non \xE9 al top, magari manca qualche cosa e il codice pu\xF3 essere difficile da capire. Tuttavia, questa \xE9 e dev\u2019essere la motivazione per reimplementare "),An=r("em"),E1=t("brand_new_bert"),z1=t(". In Hugging Face, uno degli obiettivi principali \xE9 di "),Nn=r("em"),w1=t("mettere le persone sulle spalle dei giganti"),P1=t(", il che si traduce, in questo contesto, di prendere un modello funzionante e riscriverlo e renderlo il pi\xFA possibile "),Bn=r("strong"),$1=t("accessibile, user-friendly, e leggibile"),T1=t(". Questa \xE9 la top motivazione per re-implementare modelli in \u{1F917} Transformers - cercare di creare nuove complesse tecnologie NLP accessibili a "),Dn=r("strong"),k1=t("chiunque"),y1=t("."),Ep=d(),ho=r("p"),q1=t("Riuscire a far girare il modello pretrained originale dalla repository ufficiale \xE9 spesso il passo "),Sn=r("strong"),L1=t("piu arduo"),I1=t(". Dalla nostra esperienza, \xE9 molto importante spendere un p\u2019 di tempo per diventare familiari con il codice base originale. Come test, prova a capire i seguenti punti:"),zp=d(),L=r("ul"),jn=r("li"),C1=t("Dove si trovano i pretrained weights?"),O1=d(),Mn=r("li"),A1=t("Come caricare i pretrained weights nel modello corrispondente?"),N1=d(),Rn=r("li"),B1=t("Come girare un tokenizer independentemente dal modello?"),D1=d(),Un=r("li"),S1=t("Prova a tracciare un singolo forward pass, cosicch\xE9 potrai sapere che classi e funzioni sono richieste per un semplice forward pass. Di solito, dovrai reimplementare queste funzioni e basta"),j1=d(),R=r("li"),M1=t("Prova a localizzare i componenti importanti del modello: Dove si trova la classe del modello? Ci sono sotto classi nel modello "),Fn=r("em"),R1=t("per esempio"),U1=t(" EngoderModel, DecoderMOdel? Dove si trova il self-attention layer? Ci sono molteplici differenti layer di attention, "),Qn=r("em"),F1=t("per esempio"),Q1=d(),Gn=r("em"),G1=t("self-attention"),H1=t(", "),Hn=r("em"),x1=t("cross-attention"),W1=t("\u2026?"),J1=d(),Ge=r("li"),K1=t("Come puoi fare debug sul modello nell\u2019ambiente originale della repo? Devi aggiungere dei "),xn=r("em"),V1=t("print"),X1=t(" o puoi usare "),Wn=r("em"),Z1=t("ipdb"),Y1=t(" come debugger interattivo, o vabene anche un IDE efficiente per debug come PyCharm?"),wp=d(),_o=r("p"),eb=t("\xC9 molto importante che prima di cominciare a trasferire il modello nuovo tu spenda tempo a fare debug del codice originale in maniera "),Jn=r("strong"),ob=t("efficiente"),ib=t("! Inoltre, ricorda che tutta la library \xE9 open-soruce, quindi non temere di aprire issue o fare una pull request nella repo originale. Tutti coloro che mantengono la repository saranno pi\xFA che felici di avere qualcuno che guarda e gioca con i loro codici!"),Pp=d(),ha=r("p"),tb=t("A questo punto, sta a te decidere quale ambiente per debug vuoi usare. Noi consilgiamo di evitare setup con GPU, che potrebbero costare assai, lavorare su una CPU pu\xF3 essere un ottimo punto di partenza per indagare la repository originale e per cominciare a scrivere il codice per \u{1F917} Transformers. Solo alla fine, quando il modello \xE9 stato portato con successo in  \u{1F917} Transformers, allora si potr\xE1 verificare il suo funzionamento su GPU."),$p=d(),_a=r("p"),ab=t("In generale ci sono due possibili ambienti di debug per il testare il modello originale:"),Tp=d(),bo=r("ul"),Vi=r("li"),Xi=r("a"),rb=t("Jupyter notebooks"),lb=t(" / "),Zi=r("a"),nb=t("google colab"),sb=d(),Kn=r("li"),cb=t("Scripts locali in Python"),kp=d(),ba=r("p"),db=t("Il vantaggio dei Jupyter notebooks \xE9 la possibilit\xE0 di eseguire cella per cella, il che pu\xF2 essere utile per decomporre tutte le componenti logiche, cosi da a vere un ciclo di debug pi\xF9 rapido, siccome si possono salvare i risultati da steps intermedi. Inoltre, i notebooks spesso sono molto facili da condividere con altri contributors, il che pu\xF2 essere molto utile se vuoi chiedere aiuto al team di Hugging Face. Se sei famigliare con Jupyter notebooks allora racommandiamo di lavorare in questa maniera."),yp=d(),Eo=r("p"),pb=t("Ovviamente se non siete abituati a lavorare con i notebook, questo pu\xF2 essere uno svantaggio nell\u2019usare questa tecnologia, sprecando un sacco di tempo per setup e portare tutto al nuovo ambiente, siccome non potreste neanche usare dei tools di debug come "),Vn=r("code"),mb=t("ipdb"),ub=t("."),qp=d(),zo=r("p"),fb=t("Per ogni pratica code-base, \xE9 sempre meglio come primo step caricare un "),Xn=r("strong"),vb=t("piccolo"),gb=t(" checkpoint pretrained e cercare di riprodurre un singolo forward pass usando un vettore fittizio di IDs fatti da numeri interi. Un esempio per uno script simile, in pseudocodice \xE9:"),Lp=d(),u(Yi.$$.fragment),Ip=d(),Ea=r("p"),hb=t("Per quanto riguarda la strategia di debugging, si pu\xF2 scegliere tra:"),Cp=d(),wo=r("ul"),Zn=r("li"),_b=t("Decomporre il modello originario in piccole componenenti e testare ognuna di esse"),bb=d(),He=r("li"),Eb=t("Decomporre il modello originario nel "),Yn=r("em"),zb=t("tokenizer"),wb=t(" originale e nel "),es=r("em"),Pb=t("modello"),$b=t(` originale, testare un forward pass su questi,
e usare dei print statement o breakpoints intermedi per verificare`),Op=d(),za=r("p"),Tb=t(`Ancora una volta, siete liberi di scegliere quale strategia sia ottimale per voi. Spesso una strategia \xE9 piu
avvantaggiosa di un\u2019altra, ma tutto dipende dall\u2019code-base originario.`),Ap=d(),Po=r("p"),kb=t("Se il code-base vi permette di decomporre il modello in piccole sub-componenenti, "),os=r("em"),yb=t("per esempio"),qb=t(` se il code-base
originario pu\xF2 essere facilmente testato in eager mode, allora vale la pena effettuare un debugging di questo genere.
Ricordate che ci sono dei vantaggi nel decidere di prendere la strada piu impegnativa sin da subito:`),Np=d(),G=r("ul"),is=r("li"),Lb=t(`negli stage piu finali, quando bisogner\xE0 comparare il modello originario all\u2019implementazione in Hugging Face, potrete verificare
automaticamente ogni componente, individualmente, di modo che ci sia una corrispondenza 1:1`),Ib=d(),ts=r("li"),Cb=t("avrete l\u2019opportunit\xE0 di decomporre un problema molto grande in piccoli passi, cos\xEC da strutturare meglio il vostro lavoro"),Ob=d(),as=r("li"),Ab=t(`separare il modello in componenti logiche vi aiuter\xE0 ad avere un\u2019ottima overview sul design del modello, quindi una migliore
comprensione del modello stesso`),Nb=d(),rs=r("li"),Bb=t(`verso gli stage finali i test fatti componente per componente vi aiuter\xE0 ad essere sicuri di non andare avanti e indietro
nell\u2019implementazione, cos\xEC da continuare la modifica del codice senza interruzione`),Bp=d(),$o=r("p"),Db=t("Un ottimo esempio di come questo pu\xF2 essere fatto \xE9 dato da "),et=r("a"),Sb=t("Lysandre"),jb=t(`
per il modello ELECTRA`),Dp=d(),To=r("p"),Mb=t(`Tuttavia, se il code-base originale \xE9 molto complesso o le componenti intermedie possono essere testate solo in tramite
compilazione, potrebbe richiedere parecchio tempo o addirittura essere impossibile separare il modello in piccole sotto-componenti.
Un buon esempio \xE9 `),ot=r("a"),Rb=t("MeshTensorFlow di T5"),Ub=t(`. Questa libreria
\xE9 molto complessa e non offre un metodo semplice di decomposizione in sotto-componenti. Per simili librerie, potrete fare
affidamento ai print statements.`),Sp=d(),wa=r("p"),Fb=t(`In ogni caso, indipendentemente da quale strategia scegliete, la procedura raccomandata \xE9 di cominciare a fare debug dal
primo layer al layer finale.
\xC9 consigliato recuperare gli output dai layers, tramite print o sotto-componenti, nel seguente ordine:`),jp=d(),I=r("ol"),ls=r("li"),Qb=t("Recuperare gli IDs di input dati al modello"),Gb=d(),ns=r("li"),Hb=t("Recuperare i word embeddings"),xb=d(),ss=r("li"),Wb=t("Recuperare l\u2019input del primo Transformer layer"),Jb=d(),cs=r("li"),Kb=t("Recuperare l\u2019output del primo Transformer layer"),Vb=d(),it=r("li"),Xb=t("Recuperare l\u2019output dei seguenti "),ds=r("code"),Zb=t("n - 1"),Yb=t(" Transformer layers"),e0=d(),ps=r("li"),o0=t("Recuperare l\u2019output dell\u2019intero BrandNewBert Model"),Mp=d(),xe=r("p"),i0=t("Gli IDs in input dovrebbero essere un arrary di interi, "),ms=r("em"),t0=t("per esempio"),a0=d(),us=r("code"),r0=t("input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]"),Rp=d(),Pa=r("p"),l0=t("Gli output dei seguenti layer di solito dovrebbero essere degli array di float multi-dimensionali come questo:"),Up=d(),u(tt.$$.fragment),Fp=d(),ko=r("p"),n0=t(`Ci aspettiamo che ogni modello aggiunto a \u{1F917} Transformers passi con successo un paio di test d\u2019integrazione. Questo
significa che il modello originale e la sua implementazione in \u{1F917} Transformers abbiano lo stesso output con una precisione
di 0.001! Siccome \xE9 normale che lo stesso esatto modello, scritto in librerie diverse, possa dare output leggermente
diversi, la tolleranza accettata \xE9 1e-3 (0.001). Ricordate che i due modelli devono dare output quasi identici. Dunque,
\xE9 molto conveniente comparare gli output intermedi di \u{1F917} Transformers molteplici volte con gli output intermedi del
modello originale di `),fs=r("em"),s0=t("brand_new_bert"),c0=t(`. Di seguito vi diamo alcuni consigli per avere un ambiente di debug il piu efficiente
possibile:`),Qp=d(),D=r("ul"),U=r("li"),d0=t(`Trovate la migliore strategia per fare debug dei risultati intermedi. Per esempio, \xE9 la repository originale scritta in PyTorch?
Se si, molto probabilmente dovrete dedicare un po\u2019 di tempo per scrivere degli script piu lunghi, cos\xEC da decomporre il
modello originale in piccole sotto-componenti, in modo da poter recuperare i valori intermedi. Oppure, la repo originale
\xE9 scritta in Tensorflow 1? Se \xE9 cos\xEC dovrete fare affidamento ai print di Tensorflow `),at=r("a"),p0=t("tf.print"),m0=t(`
per avere i valori intermedi. Altro caso, la repo \xE9 scritta in Jax? Allora assicuratevi che il modello non sia in `),vs=r("strong"),u0=t("jit"),f0=t(`
quanto testate il foward pass, `),gs=r("em"),v0=t("per esempio"),g0=t(" controllate "),rt=r("a"),h0=t("questo link"),_0=t("."),b0=d(),hs=r("li"),E0=t(`Usate i pi\xF9 piccoli pretrained checkpoint che potete trovare. Piu piccolo \xE9 il checkpoint, piu velocemente sar\xE0 il vostro
ciclo di debug. Non \xE9 efficiente avere un pretrained model cos\xEC gigante che per il forward pass impieghi piu di 10 secondi.
Nel caso in cui i checkpoints siano molto grandi, e non si possa trovare di meglio, allora \xE9 buona consuetudine ricorrere
a fare un dummy model nel nuovo ambiente, con weights inizializzati random e salvare quei weights per comprare la versione \u{1F917} Transformers
con il vostro modello`),z0=d(),$=r("li"),w0=t(`Accertatevi di usare la via piu semplice per chiamare il forward pass nella repo originale. Sarebbe opportuno trovare
la funzione originaria che chiami `),_s=r("strong"),P0=t("solo"),$0=t(" un singolo forward pass, "),bs=r("em"),T0=t("per esempio"),k0=t(` questa funzione spesso viene chiamata
`),Es=r("code"),y0=t("predict"),q0=t(", "),zs=r("code"),L0=t("evaluate"),I0=t(", "),ws=r("code"),C0=t("forward"),O0=t(" o "),Ps=r("code"),A0=t("__call__"),N0=t(". Siate sicuri di non fare debug su una funzione che chiami "),$s=r("code"),B0=t("forward"),D0=t(` molteplici
volte, `),Ts=r("em"),S0=t("per esempio"),j0=t(" per generare testo, come "),ks=r("code"),M0=t("autoregressive_sample"),R0=t(", "),ys=r("code"),U0=t("generate"),F0=t("."),Q0=d(),qs=r("li"),G0=t(`Cercate di separare la tokenization dal forward pass del modello. Se la repo originaria mostra esempio dove potete dare
come input una stringa, provate a cercare dove nella forward call la stringa viene cambiata in input ids e cominciate il
debug da questo punto. Questo vi garantisce un ottimo punto di partenza per scrivere un piccolo script personale dove dare
gli input al modello, anziche delle stringhe in input.`),H0=d(),de=r("li"),x0=t("Assicuratevi che il debugging "),Ls=r("strong"),W0=t("non"),J0=t(` sia in training mode. Spesso questo potra il modello a dare degli output random, per
via dei molteplici dropout layers. Assicuratevi che il forward pass nell\u2019ambiente di debug sia `),Is=r("strong"),K0=t("deterministico"),V0=t(`, cosicche
i dropout non siano usati. Alternativamente, potete usare `),Cs=r("em"),X0=t("transformers.utils.set_seed"),Z0=t(` se la vecchia e nuova implementazione
sono nello stesso framework.`),Gp=d(),yo=r("p"),Y0=t("La seguente sezione vi da ulteriori dettagli e accorgimenti su come potete fare tutto questo per "),Os=r("em"),e2=t("brand_new_bert"),o2=t("."),Hp=d(),We=r("h3"),qo=r("a"),As=r("span"),u(lt.$$.fragment),i2=d(),Ns=r("span"),t2=t("5.-14. Trasferire BrandNewBert in \u{1F917} Transformers"),xp=d(),$a=r("p"),a2=t("Allora cominciamo ad aggiungere un nuovo codice in \u{1F917} Transformers. Andate nel vostro fork clone di \u{1F917} Transformers:"),Wp=d(),u(nt.$$.fragment),Jp=d(),Lo=r("p"),r2=t(`Nel caso speciale in cui stiate aggiungendo un modello, la cui architettura sia identica a una di un modello gi\xE0 esistente,
dovrete solo aggiugnere uno script di conversione, come descritto `),Ta=r("a"),l2=t("qui"),n2=t(`.
In questo caso, potete riutilizzare l\u2019intera architettura del modello gia esistente.`),Kp=d(),ka=r("p"),s2=t("Se questo non \xE9 il caso, cominciamo con il generare un nuovo modello. Avrete due opzioni:"),Vp=d(),Io=r("ul"),ya=r("li"),Bs=r("code"),c2=t("transformers-cli add-new-model-like"),d2=t(" per aggiungere un nuovo modello come uno che gia esiste"),p2=d(),qa=r("li"),Ds=r("code"),m2=t("transformers-cli add-new-model"),u2=t(" per aggiungere un nuovo modello da un nostro template (questo assomigliera a BERT o Bart, in base al modello che selezionerete)"),Xp=d(),fe=r("p"),f2=t(`In entrambi i casi, l\u2019output vi dar\xE0 un questionario da riempire con informazioni basi sul modello. Il secondo comando richiede di installare
un `),Ss=r("code"),v2=t("cookiecutter"),g2=t(" - maggiori informazioni "),st=r("a"),h2=t("qui"),_2=t("."),Zp=d(),La=r("p"),js=r("strong"),b2=t("Aprire una Pull Request in main huggingface/transformers repo"),Yp=d(),ve=r("p"),E2=t(`Prime di cominciare ad adattare il codice automaticamente generato, aprite una nuova PR come \u201CWork in progress (WIP)\u201C,
`),Ms=r("em"),z2=t("per esempio"),w2=t(" \u201D[WIP] Aggiungere "),Rs=r("em"),P2=t("brand_new_bert"),$2=t(`\u201D, cosicch\xE9 il team di Hugging Face possa lavorare al vostro fianco nell\u2019
integrare il modello in \u{1F917} Transformers.`),em=d(),Ia=r("p"),T2=t("Questi sarebbero gli step generali da seguire:"),om=d(),Ca=r("ol"),Us=r("li"),k2=t("Creare un branch dal main branch con un nome descrittivo"),im=d(),u(ct.$$.fragment),tm=d(),dt=r("ol"),Fs=r("li"),y2=t("Commit del codice automaticamente generato"),am=d(),u(pt.$$.fragment),rm=d(),mt=r("ol"),Qs=r("li"),q2=t("Fare fetch e rebase del main esistente"),lm=d(),u(ut.$$.fragment),nm=d(),ft=r("ol"),Gs=r("li"),L2=t("Push dei cambiamenti al proprio account:"),sm=d(),u(vt.$$.fragment),cm=d(),Je=r("ol"),Hs=r("li"),xs=r("p"),I2=t(`Una volte che siete soddisfatti dei nuovi cambiamenti, andate sulla webpage del vostro fork su GitHub. Cliccate \u201CPull request\u201D.
Assiuratevi di aggiungere alcuni membri di Hugging Face come reviewers, nel riguardo alla destra della pagina della PR, cosicche il team
Hugging Face verr\xE0 notificato anche per i futuri cambiamenti.`),C2=d(),Ws=r("li"),Js=r("p"),O2=t("Cambiare la PR a draft, cliccando su \u201CConvert to draft\u201D alla destra della pagina della PR"),dm=d(),Oa=r("p"),A2=t(`Da quel punto in poi, ricordate di fare commit di ogni progresso e cambiamento, cosicche venga mostrato nella PR. Inoltre,
ricordatevi di tenere aggiornato il vostro lavoro con il main esistente:`),pm=d(),u(gt.$$.fragment),mm=d(),Aa=r("p"),N2=t(`In generale, tutte le domande che avrete riguardo al modello o l\u2019implementazione dovranno essere fatte nella vostra PR
e discusse/risolte nella PR stessa. In questa maniera, il team di Hugging Face sar\xE0 sempre notificato quando farete commit
di un nuovo codice o se avrete qualche domanda. \xC9 molto utile indicare al team di Hugging Face il codice a cui fate riferimento
nella domanda, cosicche il team potra facilmente capire il problema o la domanda.`),um=d(),Na=r("p"),B2=t(`Per fare questo andate sulla tab \u201CFiles changed\u201D, dove potrete vedere tutti i vostri cambiamenti al codice, andate sulla linea
dove volete chiedere una domanda, e cliccate sul simbolo \u201D+\u201D per aggiungere un commento. Ogni volta che una domanda o problema
\xE9 stato risolto, cliccate sul bottone \u201CResolve\u201D.`),fm=d(),Ba=r("p"),D2=t(`In questa stessa maniera, Hugging Face aprir\xE0 domande o commenti nel rivedere il vostro codice. Mi raccomando, chiedete pi\xF9
domande possibili nella pagina della vostra PR. Se avete domande molto generali, non molto utili per il pubblico, siete liberi
di chiedere al team Hugging Face direttamente su slack o email.`),vm=d(),Da=r("p"),Ks=r("strong"),S2=t("5. Adattare i codici per brand_new_bert"),gm=d(),H=r("p"),j2=t("Per prima cosa, ci focalizzeremo sul modello e non sui tokenizer. Tutto il codice relative dovrebbe trovarsi in"),M2=r("br"),R2=d(),Vs=r("code"),U2=t("src/transformers/models/brand_new_bert/modeling_brand_new_bert.py"),F2=t(` e
`),Xs=r("code"),Q2=t("src/transformers/models/brand_new_bert/configuration_brand_new_bert.py"),G2=t("."),hm=d(),x=r("p"),H2=t(`Ora potete finalmente cominciare il codice :). Il codice generato in
`),Zs=r("code"),x2=t("src/transformers/models/brand_new_bert/modeling_brand_new_bert.py"),W2=t(` avr\xE0 sia la stessa architettura di BERT se \xE9 un
modello encoder-only o BART se \xE9 encoder-decoder. A questo punto, ricordatevi cio che avete imparato all\u2019inizio, riguardo
agli aspetti teorici del modello: `),Ys=r("em"),J2=t("In che maniera il modello che sto implmementando \xE9 diverso da BERT o BART?"),K2=t(`. Implementare
questi cambi  spesso vuol dire cambiare il layer `),ec=r("em"),V2=t("self-attention"),X2=t(`, l\u2019ordine dei layer di normalizzazione e cos\xEC via\u2026
Ancora una volta ripetiamo, \xE9 molto utile vedere architetture simili di modelli gia esistenti in Transformers per avere
un\u2019idea migliore su come implementare il modello.`),_m=d(),Ke=r("p"),oc=r("strong"),Z2=t("Notate"),Y2=t(` che a questo punto non dovete avere subito un codice tutto corretto o pulito. Piuttosto, \xE9 consigliato cominciare con un
codice poco pulito, con copia-incolla del codice originale in `),ic=r("code"),eE=t("src/transformers/models/brand_new_bert/modeling_brand_new_bert.py"),oE=t(`
fino a che non avrete tutto il codice necessario. In base alla nostra esperienza, \xE9 molto meglio aggiungere una prima bozza
del codice richiesto e poi correggere e migliorare iterativamente. L\u2019unica cosa essenziale che deve funzionare qui \xE9 la seguente
instanza:`),bm=d(),u(ht.$$.fragment),Em=d(),ge=r("p"),iE=t("Questo comando creer\xE0 un modello con i parametri di default definiti in "),tc=r("code"),tE=t("BrandNewBergConfig()"),aE=t(` e weights random. Questo garantisce
che `),ac=r("code"),rE=t("init()"),lE=t(" di tutte le componenti funzioni correttamente."),zm=d(),Sa=r("p"),rc=r("strong"),nE=t("6. Scrivere uno script di conversione"),wm=d(),he=r("p"),sE=t("Il prossimo step \xE9 scrivere uno script per convertire il checkpoint che avete usato per fare debug su "),lc=r("em"),cE=t("brand_new_berts"),dE=t(` nella
repo originale in un checkpoint per la nuova implementazione di `),nc=r("em"),pE=t("brand_new_bert"),mE=t(` in \u{1F917} Transformers. Non \xE9 consigliato scrivere
lo script di conversione da zero, ma piuttosto cercate e guardate script gia esistenti in \u{1F917} Transformers, cos\xEC da trovarne
uno simile al vostro modello. Di solito basta fare una copia di uno script gia esistente e adattarlo al vostro caso.
Non esistate a chiedre al team di Hugging Face a riguardo.`),Pm=d(),Co=r("ul"),ja=r("li"),uE=t("Se state convertendo un modello da TensorFlow a PyTorch, un ottimo inizio \xE9 vedere "),_t=r("a"),fE=t("questo script di conversione per BERT"),vE=d(),Ma=r("li"),gE=t("Se state convertendo un modello da PyTorch a PyTorch, "),bt=r("a"),hE=t("lo script di conversione di BART pu\xF2 esservi utile"),$m=d(),Oo=r("p"),_E=t(`Qui di seguito spiegheremo come i modelli PyTorch salvano i weights per ogni layer e come i nomi dei layer sono definiti. In PyTorch,
il nomde del layer \xE9 definito dal nome della class attribute che date al layer. Definiamo un modello dummy in PyTorch,
chiamato `),sc=r("code"),bE=t("SimpleModel"),EE=t(":"),Tm=d(),u(Et.$$.fragment),km=d(),W=r("p"),zE=t("Ora possiamo creare un\u2019instanza di questa definizione di modo da inizializzare a random weights: "),cc=r("code"),wE=t("dense"),PE=t(", "),dc=r("code"),$E=t("intermediate"),TE=t(", "),pc=r("code"),kE=t("layer_norm"),yE=t(`.
Possiamo usare print per vedere l\u2019architettura del modello:`),ym=d(),u(zt.$$.fragment),qm=d(),Ra=r("p"),qE=t("Da cui si ottiene:"),Lm=d(),u(wt.$$.fragment),Im=d(),Ua=r("p"),LE=t(`Si pu\xF2 vedere come i nomi dei layers siano definiti dal nome della class attribute in PyTorch. I valori dei weights di uno
specifico layer possono essere visualizzati:`),Cm=d(),u(Pt.$$.fragment),Om=d(),Fa=r("p"),IE=t("ad esempio:"),Am=d(),u($t.$$.fragment),Nm=d(),Tt=r("p"),CE=t(`Nello script di conversione, dovreste riempire quei valori di inizializzazione random con gli stessi weights del corrispondente
layer nel checkpoint. `),mc=r("em"),OE=t("Per esempio"),Bm=d(),u(kt.$$.fragment),Dm=d(),J=r("p"),AE=t(`Cos\xEC facendo, dovete verificare che ogni inizializzazione random di un peso del modello PyTorch e il suo corrispondente peso nel pretrained checkpoint
siano esattamente gli stessi e uguali in `),uc=r("strong"),NE=t("dimensione/shape e nome"),BE=t(". Per fare questo, \xE9 "),fc=r("strong"),DE=t("necessario"),SE=t(" aggiungere un "),vc=r("code"),jE=t("assert"),ME=t(`
per la dimensione/shape e nome:`),Sm=d(),u(yt.$$.fragment),jm=d(),Qa=r("p"),RE=t("Inoltre, dovrete fare il print sia dei nomi che dei weights per essere sicuri che siano gli stessi:"),Mm=d(),u(qt.$$.fragment),Rm=d(),Ga=r("p"),UE=t(`Se la dimensione o il nome non sono uguali, probabilmente avete sbagliato ad assegnare il peso nel checkpoint o nel layer costrutture di
\u{1F917} Transformers.`),Um=d(),Ao=r("p"),FE=t("Una dimensione sbagliata pu\xF2 essere dovuta ad un errore nei parameteri in "),gc=r("code"),QE=t("BrandNewBertConfig()"),GE=t(`. Tuttavia, pu\xF2 essere anche
che l\u2019implementazione del layer in PyTorch richieda di fare una transposizione della matrice dei weights.`),Fm=d(),K=r("p"),HE=t("Infine, controllate "),hc=r("strong"),xE=t("tutti"),WE=t(` che tutti i weights inizializzati e fate print di tutti i weights del checkpoint che non sono stati
usati per l\u2019inizializzazione, di modo da essere sicuri che il modello sia correttamente convertito. \xC9 normale che ci siano
errori nel test di conversione, fai per un errore in `),_c=r("code"),JE=t("BrandNewBertConfig()"),KE=t(`, o un errore nell\u2019architettura in \u{1F917} Transformers,
o un bug in `),bc=r("code"),VE=t("init()"),XE=t("."),Qm=d(),V=r("p"),ZE=t(`Questo step dev\u2019essere fatto tramite iterazioni fino a che non si raggiungano gli stessi valori per i weights. Una volta che
il checkpoint \xE9 stato correttamente caricato in \u{1F917} Transformers, potete salvare il modello in una cartella di vostra scelta
`),Ec=r("code"),YE=t("/path/to/converted/checkpoint/folder"),e3=t(` che contenga sia
`),zc=r("code"),o3=t("pytorch_model.bin"),i3=t(" che "),wc=r("code"),t3=t("config.json"),a3=t(":"),Gm=d(),u(Lt.$$.fragment),Hm=d(),Ha=r("p"),Pc=r("strong"),r3=t("7. Implementare il forward pass"),xm=d(),No=r("p"),l3=t(`Una volta che i weights pretrained sono stati correttamente caricati in \u{1F917} Transformers, dovrete assicurarvi che il forward pass
sia correttamente implementato. `),xa=r("a"),n3=t("Qui"),s3=t(`, avete give creato e provato
uno script che testi il forward pass del modello usando la repo originaria. Ora dovrete fare lo stesso con uno script analogo
usando l\u2019implementazione in \u{1F917} Transformers anzich\xE9 l\u2019originale. Piu o meno lo script dovrebbe essere:`),Wm=d(),u(It.$$.fragment),Jm=d(),X=r("p"),c3=t(`Di solito l\u2019output da \u{1F917} Transformers non \xE9 uguale uguale all\u2019output originario, sopratto la prima volta. Non vi abbattete -
\xE9 normale! Prima di tutto assicuratevi che non ci siano errori o che non vengano segnalati degli errori nella forward pass.
Spesso capita che ci siano dimensioni sbagliate o data type sbagliati, `),$c=r("em"),d3=t("ad esempio"),p3=d(),Tc=r("code"),m3=t("torch.long"),u3=t(" anziche "),kc=r("code"),f3=t("torch.float32"),v3=t(`.
Non esistate a chiedere al team Hugging Face!`),Km=d(),_e=r("p"),g3=t(`Nella parte finale assicuratevi che l\u2019implementazione \u{1F917} Transformers funzioni correttamente cosi da testare che gli output
siano equivalenti a una precisione di `),yc=r("code"),h3=t("1e-3"),_3=t(". Controllate che "),qc=r("code"),b3=t("outputs.shape"),E3=t(` siano le stesse tra \u{1F917} Transformers e l\u2019implementazione
originaria. Poi, controllate che i valori in output siano identici. Questa \xE9 sicuramente la parte pi\xF9 difficile, qui una serie
di errori comuni quando gli output non sono uguali:`),Vm=d(),Z=r("ul"),Ve=r("li"),z3=t("Alcuni layers non sono stati aggiunti, "),Lc=r("em"),w3=t("ad esempio"),P3=t(" un "),Ic=r("em"),$3=t("activation"),T3=t(" layer non \xE9 stato aggiunto, o ci si \xE9 scordati di una connessione"),k3=d(),Cc=r("li"),y3=t("La matrice del word embedding non \xE9 stata ripareggiata"),q3=d(),Oc=r("li"),L3=t("Ci sono degli embeddings posizionali sbagliati perch\xE9 l\u2019implementazione originaria ha un offset"),I3=d(),Y=r("li"),C3=t("Il dropout \xE9 in azione durante il forward pass. Per sistemare questo errore controllate che "),Ac=r("em"),O3=t("model.training = False"),A3=t(` e che
il dropout non sia stato attivato nel forward pass, `),Nc=r("em"),N3=t("per esempio"),B3=t(" passate "),Bc=r("em"),D3=t("self.training"),S3=t(" a "),Ct=r("a"),j3=t("PyTorch\u2019s functional dropout"),Xm=d(),Bo=r("p"),M3=t(`La miglior maniera per sistemare il problema \xE9 di vedere all\u2019implementazione originaria del forward pass e in \u{1F917} Transformers
fianco a fianco e vedere se ci sono delle differenze. In teoria, con debug e print degli output intermedie di entrambe le
implementazioni nel forward pass nell\u2019esatta posizione del network dovrebbe aiutarvi a vedere dove ci sono differenze tra
i due frameworks. Come prima mossa controllate che `),Dc=r("code"),R3=t("input_ids"),U3=t(` siano identici in entrambi gli scripts. Da l\xEC andate fino
all\u2019ultimo layer. Potrete notare una differenza tra le due implementazioni a quel punto.`),Zm=d(),Do=r("p"),F3=t("Una volta che lo stesso output \xE9 stato ragguingi, verificate gli output con "),Sc=r("code"),Q3=t("torch.allclose(original_output, output, atol=1e-3)"),G3=t(`.
A questo punto se \xE9 tutto a posto: complimenti! Le parti seguenti saranno una passeggiata \u{1F60A}.`),Ym=d(),Wa=r("p"),jc=r("strong"),H3=t("8. Aggiungere i test necessari per il modello"),eu=d(),So=r("p"),x3=t(`A questo punto avete aggiunto con successo il vostro nuovo modello. Tuttavia, \xE9 molto probabile che il modello non sia
del tutto ok con il design richiesto. Per essere sicuri che l\u2019implementazione sia consona e compatibile con \u{1F917} Transformers \xE9
necessario implementare dei tests. Il Cookiecutter dovrebbe fornire automaticamente dei file per test per il vostro modello,
di solito nella folder `),Mc=r("code"),W3=t("tests/test_modeling_brand_new_bert.py"),J3=t(". Provate questo per verificare l\u2019ok nei test piu comuni:"),ou=d(),u(Ot.$$.fragment),iu=d(),Ja=r("p"),K3=t("Una volta sistemati i test comuni, bisogna assicurarsi che il vostro lavoro sia correttamente testato cosicch\xE8:"),tu=d(),jo=r("ul"),At=r("li"),V3=t("a) La community puo capire in maniera semplice il vostro lavoro controllando tests specifici del modello "),Rc=r("em"),X3=t("brand_new_bert"),Z3=t(","),Y3=d(),Uc=r("li"),e4=t("b) Implementazioni future del vostro modello non rompano alcune feature importante del modello."),au=d(),Mo=r("p"),o4=t(`Per prima cosa agguingete dei test d\u2019integrazione. Questi sono essenziali perche fanno la stessa funzione degli scripts di
debug usati precedentemente. Un template per questi tests esiste gia nel Cookiecutter ed \xE9 sotto il nome di `),Fc=r("code"),i4=t("BrandNewBertModelIntegrationTests"),t4=t(`,
voi dovrete solo completarlo. Una volta che questi tests sono OK, provate:`),ru=d(),u(Nt.$$.fragment),lu=d(),u(Ro.$$.fragment),nu=d(),ee=r("p"),a4=t("Di seguito, tutte le features che sono utili e necessarire per "),Qc=r("em"),r4=t("brand_new_bert"),l4=t(` devono essere testate in test separati,
contenuti in `),Gc=r("code"),n4=t("BrandNewBertModelTester"),s4=t("/ "),Hc=r("code"),c4=t("BrandNewBertModelTest"),d4=t(". spesso la gente si scorda questi test, ma ricordate che sono utili per:"),su=d(),Uo=r("ul"),xc=r("li"),p4=t("Aiuta gli utenti a capire il vostro codice meglio, richiamando l\u2019attenzione su queste nuove features"),m4=d(),Wc=r("li"),u4=t("Developers e contributors futuri potranno velocemente testare nuove implementazioni del modello testanto questi casi speciali."),cu=d(),Ka=r("p"),Jc=r("strong"),f4=t("9. Implementare il tokenizer"),du=d(),Fo=r("p"),v4=t("A questo punto avremo bisogno un tokenizer per "),Kc=r("em"),g4=t("brand_new_bert"),h4=t(". Di solito il tokenizer \xE9 uguale ad altri modelli in \u{1F917} Transformers."),pu=d(),Va=r("p"),_4=t("\xC9 importante che troviate il file con il tokenizer originale e che lo carichiate in \u{1F917} Transformers."),mu=d(),Qo=r("p"),b4=t(`Per controllare che il tokenizer funzioni in modo corretto, create uno script nella repo originaria che riceva come input
una stringa e ritorni gli `),Vc=r("code"),E4=t("input_ids"),z4=t(". Piu o meno questo potrebbe essere il codice:"),uu=d(),u(Bt.$$.fragment),fu=d(),Go=r("p"),w4=t(`Potrebbe richiedere un po\u2019 di tempo, ma guardate ancora alla repo originaria per trovare la funzione corretta del tokenizer.
A volte capita di dover riscrivere il tokenizer nella repo originaria, di modo da avere come output gli `),Xc=r("code"),P4=t("input_ids"),$4=t(`.
A quel punto uno script analogo \xE9 necessario in \u{1F917} Transformers:`),vu=d(),u(Dt.$$.fragment),gu=d(),Ho=r("p"),T4=t("Una volta che "),Zc=r("code"),k4=t("input_ids"),y4=t(" sono uguali, bisogna aggiungere un test per il tokenizer."),hu=d(),xo=r("p"),q4=t("Il file test per tokenizer di "),Yc=r("em"),L4=t("brand_new_brand"),I4=t(" dovrebbe avere un paio di hard-coded test d\u2019integrazione."),_u=d(),Xa=r("p"),ed=r("strong"),C4=t("10. Test end-to-end"),bu=d(),oe=r("p"),O4=t("Ora che avete il tokenizer, dovrete aggiungere dei test d\u2019integrazione per l\u2019intero workflow in "),od=r("code"),A4=t("tests/test_modeling_brand_new_bert.py"),N4=t(` in \u{1F917} Transformer.
Questi test devono mostrare che un significante campione text-to-text funzioni come ci si aspetta nell\u2019implementazione di  \u{1F917} Transformers.
`),id=r("em"),B4=t("Per esempio"),D4=t(` potreste usare dei source-to-target-translation, o un sommario di un articolo, o un domanda-risposta e cosi via.
Se nessuno dei checkpoints \xE9 stato ultra parametrizzato per task simili, allora i tests per il modello sono piu che sufficienti.
Nello step finale dovete assicurarvi che il modello sia totalmente funzionale, e consigliamo anche di provare a testare su GPU.
Puo succedere che ci si scordi un `),td=r("code"),S4=t(".to(self.device)"),j4=t(` ad esempio. Se non avete accesso a GPU, il team Hugging Face puo provvedere
a testare questo aspetto per voi.`),Eu=d(),Za=r("p"),ad=r("strong"),M4=t("11. Aggiungere una Docstring"),zu=d(),be=r("p"),R4=t(`Siete quasi alla fine! L\u2019ultima cosa rimasta \xE9 avere una bella docstring e una pagina doc. Il Cookiecutter dovrebbe provvedere gi\xE0
un template chiamato `),rd=r("code"),U4=t("docs/source/model_doc/brand_new_bert.rst"),F4=t(`, che dovrete compilare. La prima cosa che un utente far\xE0
per usare il vostro modello sar\xE0 dare una bella lettura al doc. Quindi proponete una documentazione chiara e concisa. \xC9 molto
utile per la community avere anche delle `),ld=r("em"),Q4=t("Tips"),G4=t(` per mostrare come il modello puo\u2019 essere usato. Non esitate a chiedere a Hugging Face
riguardo alle docstirng.`),wu=d(),Wo=r("p"),H4=t("Quindi, assicuratevi che la docstring sia stata aggiunta a "),nd=r("code"),x4=t("src/transformers/models/brand_new_bert/modeling_brand_new_bert.py"),W4=t(`.
Assicuratevi che la docstring sia corretta e che includa tutti i necessari input e output. Abbiamo una guida dettagliata per
scrivere la documentazione e docstring.`),Pu=d(),Ya=r("p"),sd=r("strong"),J4=t("Rifattorizzare il codice"),$u=d(),Jo=r("p"),K4=t("Perfetto! Ora che abbiamo tutto per "),cd=r("em"),V4=t("brand_new_bert"),X4=t(" controllate che lo stile del codice sia ok:"),Tu=d(),u(St.$$.fragment),ku=d(),er=r("p"),Z4=t("E che il codice passi i quality check:"),yu=d(),u(jt.$$.fragment),qu=d(),or=r("p"),Y4=t(`A volte capita che manchino delle informazioninella docstring o alcuni nomi sbagliati, questo far\xE0 fallire i tests sopra.
Ripetiamo: chiedete pure a Hugging Face, saremo lieti di aiutarvi.`),Lu=d(),ir=r("p"),e5=t("Per ultimo, fare del refactoring del codice una volta che \xE9 stato creato."),Iu=d(),tr=r("p"),o5=t("Avete finito con il codice, congratulazioni! \u{1F389} Siete fantasticiiiiiii! \u{1F60E}"),Cu=d(),ar=r("p"),dd=r("strong"),i5=t("12. Caricare il modello sul model hub"),Ou=d(),S=r("p"),t5=t(`In questa ultima parte dovrete convertire e caricare il modello, con tutti i checkpoints, nel model hub e aggiungere una
model card per ogni checkpoint caricato. Leggete la nostra guida `),rr=r("a"),a5=t("Model sharing and uploading Page"),r5=t(` per
avere familiarit\xE0 con l\u2019hub. Di solito in questa parte lavorate a fianco di Hugging face per decidere un nome che sia ok
per ogni checkpoint, per ottenere i permessi necessari per caricare il modello nell\u2019organizzazione dell\u2019autore di `),pd=r("em"),l5=t("brand_new_bert"),n5=t(`.
Il metodo `),md=r("code"),s5=t("push_to_hub"),c5=t(", presente in tutti i modelli "),ud=r("code"),d5=t("transformers"),p5=t(", \xE9 una maniera rapida e indolore per caricare il vostro checkpoint sull\u2019hub:"),Au=d(),u(Mt.$$.fragment),Nu=d(),Ko=r("p"),m5=t(`Vale la pena spendere un po\u2019 di tempo per creare una model card ad-hoc per ogni checkpoint. Le model cards dovrebbero
suggerire le caratteristiche specifiche del checkpoint, `),fd=r("em"),u5=t("per esempio"),f5=t(` su che dataset il checkpoint \xE9 stato pretrained o fine-tuned.
O che su che genere di task il modello lavoro? E anche buona pratica includere del codice su come usare il modello correttamente.`),Bu=d(),lr=r("p"),vd=r("strong"),v5=t("13. (Opzionale) Aggiungere un notebook"),Du=d(),Vo=r("p"),g5=t("\xC9 molto utile aggiungere un notebook, che dimostri in dettaglio come "),gd=r("em"),h5=t("brand_new_bert"),_5=t(` si utilizzi per fare inferenza e/o
fine-tuned su specifiche task. Non \xE9 una cosa obbligatoria da avere nella vostra PR, ma \xE9 molto utile per la community.`),Su=d(),nr=r("p"),hd=r("strong"),b5=t("14. Sottomettere la PR"),ju=d(),sr=r("p"),E5=t(`L\u2019ultimissimo step! Ovvero il merge della PR nel main. Di solito il team Hugging face a questo punto vi avr\xE0 gia aiutato,
ma \xE9 ok prendere un po\u2019 di tempo per pulire la descirzione e commenti nel codice.`),Mu=d(),Xe=r("h3"),Xo=r("a"),_d=r("span"),u(Rt.$$.fragment),z5=d(),bd=r("span"),w5=t("Condividete il vostro lavoro!!"),Ru=d(),cr=r("p"),P5=t(`\xC9 ora tempo di prendere un po\u2019 di credito dalla communit\xE0 per il vostro lavoro! Caricare e implementare un nuovo modello
\xE9 un grandissimo contributo per Transformers e l\u2019intera community NLP. Il codice e la conversione dei modelli pre-trained sara
sicuramente utilizzato da centinaia o migliaia di sviluppatori e ricercatori. Siate fieri e orgogliosi di condividere il vostro
traguardo con l\u2019intera community :)`),Uu=d(),dr=r("p"),Ed=r("strong"),$5=t("Avete create un altro modello che \xE9 super facile da usare per tutti quanti nella community! \u{1F92F}"),this.h()},l(e){const s=IP('[data-svelte="svelte-1phssyn"]',document.head);k=l(s,"META",{name:!0,content:!0}),s.forEach(i),Ye=p(e),C=l(e,"H1",{class:!0});var Ut=n(C);F=l(Ut,"A",{id:!0,class:!0,href:!0});var O5=n(F);qe=l(O5,"SPAN",{});var A5=n(qe);f(j.$$.fragment,A5),A5.forEach(i),O5.forEach(i),ui=p(Ut),le=l(Ut,"SPAN",{});var N5=n(le);ne=a(N5,"Come aggiungere un modello a \u{1F917} Transformers?"),N5.forEach(i),Ut.forEach(i),fi=p(e),M=l(e,"P",{});var pr=n(M);Vf=a(pr,`Aggiungere un nuovo modello \xE9 spesso difficile e richiede una profonda conoscenza della libreria \u{1F917} Transformers e anche
della repository originale del modello. A Hugging Face cerchiamo di dare alla community sempre pi\xFA poteri per aggiungere
modelli independentemente. Quindi, per alcuni nuovi modelli che la community vuole aggiungere a \u{1F917} Transformers, abbiamo
creato una specifica `),Ir=l(pr,"EM",{});var B5=n(Ir);Xf=a(B5,"call-for-model-addition"),B5.forEach(i),Zf=a(pr,` che spiega passo dopo passo come aggiungere il modello richiesto. Con
questo `),Cr=l(pr,"EM",{});var D5=n(Cr);Yf=a(D5,"call-for-model-addition"),D5.forEach(i),ev=a(pr,` vogliamo insegnare a volenterosi e esperti collaboratori della community come implementare
un modello in \u{1F917} Transformers.`),pr.forEach(i),$d=p(e),eo=l(e,"P",{});var Qu=n(eo);ov=a(Qu,"Se questo \xE9 qualcosa che pu\xF2 interessarvi, siete liberi di controllare l\u2019attuale \u201Ccalls-for-model-addition\u201D "),vi=l(Qu,"A",{href:!0,rel:!0});var S5=n(vi);iv=a(S5,"qui"),S5.forEach(i),tv=a(Qu,`
e contattarci.`),Qu.forEach(i),Td=p(e),Ft=l(e,"P",{});var j5=n(Ft);av=a(j5,`Se il modello sar\xE0 selezionato, allora potrete lavorare insieme a un membro di Hugging Face per integrare il modello in \u{1F917}
Transformers. Cos\xEC facendo, ci guadagnerai in una comprensione totale, sia teorica che pratica, del modello proposto. Inoltre,
sarai l\u2019artefice di un importante contributo open-source a \u{1F917} Transformers. Durante l\u2019implementazione avrai l\u2019opportunit\xE0 di:`),j5.forEach(i),kd=p(e),Q=l(e,"UL",{});var Zo=n(Q);Or=l(Zo,"LI",{});var M5=n(Or);rv=a(M5,"ottenere pi\xF9 comprensione delle best practices in open-source"),M5.forEach(i),lv=p(Zo),Ar=l(Zo,"LI",{});var R5=n(Ar);nv=a(R5,"capire i principi di design di una della librerie NLP pi\xF9 popolari"),R5.forEach(i),sv=p(Zo),Nr=l(Zo,"LI",{});var U5=n(Nr);cv=a(U5,"capire come efficientemente testare complessi modelli NLP"),U5.forEach(i),dv=p(Zo),se=l(Zo,"LI",{});var Yo=n(se);pv=a(Yo,"capire come integrare utilit Python come "),Br=l(Yo,"CODE",{});var F5=n(Br);mv=a(F5,"black"),F5.forEach(i),uv=a(Yo,", "),Dr=l(Yo,"CODE",{});var Q5=n(Dr);fv=a(Q5,"isort"),Q5.forEach(i),vv=a(Yo,", "),Sr=l(Yo,"CODE",{});var G5=n(Sr);gv=a(G5,"make fix-copies"),G5.forEach(i),hv=a(Yo," in una libreria per garantire sempre di avere un codice leggibile e pulito"),Yo.forEach(i),Zo.forEach(i),yd=p(e),oo=l(e,"P",{});var Gu=n(oo);_v=a(Gu,`Siamo anche contenti se vuoi aggiungere un modello che non pu\xF2 essere trovato nella cartella \u201Ccalls-for-model-addition\u201D.
Le seguenti sezioni spiegano in dettaglio come aggiungere un nuovo modello. Pu\xF2 anche essere molto utile controllare modelli
gi\xE0 aggiunti `),gi=l(Gu,"A",{href:!0,rel:!0});var H5=n(gi);bv=a(H5,"qui"),H5.forEach(i),Ev=a(Gu,`,
per capire se richiamano il modello che vorreste aggiungere.`),Gu.forEach(i),qd=p(e),Qt=l(e,"P",{});var x5=n(Qt);zv=a(x5,"Per cominciare, vediamo una panoramica general della libreria Transformers."),x5.forEach(i),Ld=p(e),Le=l(e,"H2",{class:!0});var Hu=n(Le);io=l(Hu,"A",{id:!0,class:!0,href:!0});var W5=n(io);jr=l(W5,"SPAN",{});var J5=n(jr);f(hi.$$.fragment,J5),J5.forEach(i),W5.forEach(i),wv=p(Hu),Mr=l(Hu,"SPAN",{});var K5=n(Mr);Pv=a(K5,"Panoramica generale su \u{1F917} Transformers"),K5.forEach(i),Hu.forEach(i),Id=p(e),Gt=l(e,"P",{});var V5=n(Gt);$v=a(V5,`Prima di tutto, vediamo in generale \u{1F917} Transformers. \u{1F917} Transformers \xE9 una libreria molto strutturata, quindi
pu\xE0 essere che a volte ci sia un disaccordo con alcune filosofie della libreria o scelte di design. Dalla nostra esperienza,
tuttavia, abbiamo trovato che le scelte fondamentali di design della libreria sono cruciali per usare \u{1F917} Transformers efficacemente
su larga scala, mantenendo i costi a un livello accettabile.`),V5.forEach(i),Cd=p(e),to=l(e,"P",{});var xu=n(to);Tv=a(xu,"Un buon primo punto di partenza per capire al meglio la libreria \xE9 leggere la "),Ht=l(xu,"A",{href:!0});var X5=n(Ht);kv=a(X5,"documentazione sulla nostra filosofia"),X5.forEach(i),yv=a(xu,`
Da qui, ci sono alcune scelte sul modo di lavorare che cerchiamo di applicare a tutti i modelli:`),xu.forEach(i),Od=p(e),me=l(e,"UL",{});var mr=n(me);Rr=l(mr,"LI",{});var Z5=n(Rr);qv=a(Z5,"La composizione \xE9 generalmente favorita sulla sovra-astrazione"),Z5.forEach(i),Lv=p(mr),Ur=l(mr,"LI",{});var Y5=n(Ur);Iv=a(Y5,"Duplicare il codice non \xE9 sempre male, soprattutto se migliora notevolmente la leggibilit\xE0 e accessibilit\xE0 del modello"),Y5.forEach(i),Cv=p(mr),_i=l(mr,"LI",{});var Wu=n(_i);Ov=a(Wu,`Tutti i files creati per il nuovo modello devono il piu possibile \u201Ccompatti\u201D. Questo vuol dire che quando qualcuno legger\xE1 il codice
di uno specifico modello, potr\xE1 vedere solo il corrispettivo file `),Fr=l(Wu,"CODE",{});var e6=n(Fr);Av=a(e6,"modeling_....py"),e6.forEach(i),Nv=a(Wu," senza avere multiple dipendenze."),Wu.forEach(i),mr.forEach(i),Ad=p(e),ao=l(e,"P",{});var Ju=n(ao);Bv=a(Ju,"La cosa pi\xFA importante, \xE9 che consideriamo la libreria non solo un mezzo per dare un prodotto, "),Qr=l(Ju,"EM",{});var o6=n(Qr);Dv=a(o6,"per esempio"),o6.forEach(i),Sv=a(Ju,` dare la possibilit\xE0
di usare BERT per inferenza, ma \xE9 anche il prodotto reale che noi vogliamo migliorare sempre pi\xF9. Quindi, quando aggiungi
un modello, non sei solo la persona che user\xE0 il modello, ma rappresenti anche tutti coloro che leggeranno,
cercheranno di capire e modificare il tuo modello.`),Ju.forEach(i),Nd=p(e),xt=l(e,"P",{});var i6=n(xt);jv=a(i6,"Tenendo questi principi in mente, immergiamoci nel design generale della libreria."),i6.forEach(i),Bd=p(e),Ie=l(e,"H3",{class:!0});var Ku=n(Ie);ro=l(Ku,"A",{id:!0,class:!0,href:!0});var t6=n(ro);Gr=l(t6,"SPAN",{});var a6=n(Gr);f(bi.$$.fragment,a6),a6.forEach(i),t6.forEach(i),Mv=p(Ku),Hr=l(Ku,"SPAN",{});var r6=n(Hr);Rv=a(r6,"Panoramica sui modelli"),r6.forEach(i),Ku.forEach(i),Dd=p(e),O=l(e,"P",{});var Ee=n(O);Uv=a(Ee,`Per aggiungere con successo un modello, \xE9 importante capire l\u2019interazione tra il tuo modello e la sua configurazione,
`),xr=l(Ee,"CODE",{});var l6=n(xr);Fv=a(l6,"PreTrainedModel"),l6.forEach(i),Qv=a(Ee,", e "),Wr=l(Ee,"CODE",{});var n6=n(Wr);Gv=a(n6,"PretrainedConfig"),n6.forEach(i),Hv=a(Ee,". Per dare un esempio, chiameremo il modello da aggiungere a \u{1F917} Transformers"),xv=l(Ee,"BR",{}),Wv=p(Ee),Jr=l(Ee,"CODE",{});var s6=n(Jr);Jv=a(s6,"BrandNewBert"),s6.forEach(i),Kv=a(Ee,"."),Ee.forEach(i),Sd=p(e),Wt=l(e,"P",{});var c6=n(Wt);Vv=a(c6,"Diamo un\u2019occhiata:"),c6.forEach(i),jd=p(e),Jt=l(e,"IMG",{src:!0}),Md=p(e),b=l(e,"P",{});var z=n(b);Xv=a(z,`Come potete vedere, ci basiamo sull\u2019ereditariet\xE0 in \u{1F917} Transformers, tenendo per\xF2 il livello di astrazione a un minimo
assoluto.  Non ci sono mai pi\xF9 di due livelli di astrazione per ogni modello nella libreria. `),Kr=l(z,"CODE",{});var d6=n(Kr);Zv=a(d6,"BrandNewBertModel"),d6.forEach(i),Yv=a(z,` eredita
da `),Vr=l(z,"CODE",{});var p6=n(Vr);eg=a(p6,"BrandNewBertPreTrainedModel"),p6.forEach(i),og=a(z," che, a sua volta, eredita da "),Xr=l(z,"CODE",{});var m6=n(Xr);ig=a(m6,"PreTrainedModel"),m6.forEach(i),tg=a(z,` -  semplice no?
Come regola generale, vogliamo essere sicuri che un nuovo modello dipenda solo da `),Zr=l(z,"CODE",{});var u6=n(Zr);ag=a(u6,"PreTrainedModel"),u6.forEach(i),rg=a(z,`. Le funzionalit\xE0
importanti che sono automaticamente conferite a ogni nuovo modello sono `),Yr=l(z,"CODE",{});var f6=n(Yr);lg=a(f6,"from_pretrained()"),f6.forEach(i),ng=a(z,`
e `),el=l(z,"CODE",{});var v6=n(el);sg=a(v6,"save_pretrained()"),v6.forEach(i),cg=a(z,`, che sono usate per serializzazione e deserializzazione. Tutte le altre importanti
funzionalit\xE0, come ad esempio `),ol=l(z,"CODE",{});var g6=n(ol);dg=a(g6,"BrandNewBertModel.forward"),g6.forEach(i),pg=a(z,` devono essere definite completamente nel nuovo script
`),il=l(z,"CODE",{});var h6=n(il);mg=a(h6,"modeling_brand_new_bert.py"),h6.forEach(i),ug=a(z,`. Inoltre, vogliamo essere sicuri che un modello con uno specifico head layer, come
`),tl=l(z,"CODE",{});var _6=n(tl);fg=a(_6,"BrandNewBertForMaskedLM"),_6.forEach(i),vg=a(z," non erediti da "),al=l(z,"CODE",{});var b6=n(al);gg=a(b6,"BrandNewBertModel"),b6.forEach(i),hg=a(z,", ma piuttosto usi "),rl=l(z,"CODE",{});var E6=n(rl);_g=a(E6,"BrandNewBertModel"),E6.forEach(i),bg=a(z,`
come componente che pu\xF2 essere chiamata nel passaggio forward per mantenere il livello di astrazione basso. Ogni
nuovo modello richieste una classe di configurazione, chiamata `),ll=l(z,"CODE",{});var z6=n(ll);Eg=a(z6,"BrandNewBertConfig"),z6.forEach(i),zg=a(z,`. Questa configurazione \xE9 sempre
mantenuta come un attributo in `),nl=l(z,"CODE",{});var w6=n(nl);wg=a(w6,"PreTrainedModel"),w6.forEach(i),Pg=a(z,", e quindi pu\xF2 essere accessibile tramite l\u2019attributo "),sl=l(z,"CODE",{});var P6=n(sl);$g=a(P6,"config"),P6.forEach(i),Tg=a(z,`
per tutte le classi che ereditano da `),cl=l(z,"CODE",{});var $6=n(cl);kg=a($6,"BrandNewBertPreTrainedModel"),$6.forEach(i),yg=a(z,":"),z.forEach(i),Rd=p(e),f(Ei.$$.fragment,e),Ud=p(e),y=l(e,"P",{});var ie=n(y);qg=a(ie,`Analogamente al modello, la configurazione eredita le funzionalit\xE0 base di serializzazione e deserializzazione da
`),dl=l(ie,"CODE",{});var T6=n(dl);Lg=a(T6,"PretrainedConfig"),T6.forEach(i),Ig=a(ie,`. \xC9 da notare che la configurazione e il modello sono sempre serializzati in due formati differenti -
il modello \xE9 serializzato in un file `),pl=l(ie,"EM",{});var k6=n(pl);Cg=a(k6,"pytorch_model.bin"),k6.forEach(i),Og=a(ie," mentre la configurazione con "),ml=l(ie,"EM",{});var y6=n(ml);Ag=a(y6,"config.json"),y6.forEach(i),Ng=a(ie,`. Chiamando
`),ul=l(ie,"CODE",{});var q6=n(ul);Bg=a(q6,"save_pretrained()"),q6.forEach(i),Dg=a(ie," automaticamente chiamer\xE0 "),fl=l(ie,"CODE",{});var L6=n(fl);Sg=a(L6,"save_pretrained()"),L6.forEach(i),jg=a(ie,`, cosicch\xE9 sia il
modello che la configurazione siano salvati.`),ie.forEach(i),Fd=p(e),Ce=l(e,"H3",{class:!0});var Vu=n(Ce);lo=l(Vu,"A",{id:!0,class:!0,href:!0});var I6=n(lo);vl=l(I6,"SPAN",{});var C6=n(vl);f(zi.$$.fragment,C6),C6.forEach(i),I6.forEach(i),Mg=p(Vu),gl=l(Vu,"SPAN",{});var O6=n(gl);Rg=a(O6,"Stile per il codice"),O6.forEach(i),Vu.forEach(i),Qd=p(e),Kt=l(e,"P",{});var A6=n(Kt);Ug=a(A6,`Quando codifichi un nuovo modello, tieni presente che Transformers ha una sua struttura di fondo come libreria, perci\xF2
ci sono alcuni fatti da considerare su come scrivere un codice :-)`),A6.forEach(i),Gd=p(e),A=l(e,"OL",{});var ze=n(A);Oe=l(ze,"LI",{});var ur=n(Oe);Fg=a(ur,`Il forward pass del tuo modello dev\u2019essere scritto completamente nel file del modello, mentre dev\u2019essere indipendente
da altri modelli nella libreria. Se vuoi riutilizzare un blocco di codice da un altro modello, copia e incolla il codice con un commento `),hl=l(ur,"CODE",{});var N6=n(hl);Qg=a(N6,"# Copied from"),N6.forEach(i),Gg=a(ur," in cima al codice (guarda "),wi=l(ur,"A",{href:!0,rel:!0});var B6=n(wi);Hg=a(B6,"qui"),B6.forEach(i),xg=a(ur,`
per un ottimo esempio).`),ur.forEach(i),Wg=p(ze),Ae=l(ze,"LI",{});var fr=n(Ae);Jg=a(fr,`Il codice dev\u2019essere interamente comprensibile, anche da persone che non parlano in inglese. Questo significa che le
variabili devono avere un nome descrittivo e bisogna evitare abbreviazioni. Per esempio, `),_l=l(fr,"CODE",{});var D6=n(_l);Kg=a(D6,"activation"),D6.forEach(i),Vg=a(fr,` \xE9 molto meglio
che `),bl=l(fr,"CODE",{});var S6=n(bl);Xg=a(S6,"act"),S6.forEach(i),Zg=a(fr,". Le variabili con una lettera sono da evitare fortemente, almeno che non sia per un indce in un for loop."),fr.forEach(i),Yg=p(ze),El=l(ze,"LI",{});var j6=n(El);eh=a(j6,"Generamente \xE9 meglio avere un codice esplicito e pi\xFA lungo che un codice corto e magico."),j6.forEach(i),oh=p(ze),Ne=l(ze,"LI",{});var vr=n(Ne);ih=a(vr,"Evita di subclassare "),zl=l(vr,"CODE",{});var M6=n(zl);th=a(M6,"nn.Sequential"),M6.forEach(i),ah=a(vr," in Pytorch, puoi subclassare "),wl=l(vr,"CODE",{});var R6=n(wl);rh=a(R6,"nn.Module"),R6.forEach(i),lh=a(vr,` e scrivere il forward pass, cosicch\xE9
chiunque pu\xF2 effettuare debug sul tuo codice, aggiungendo print o breaking points.`),vr.forEach(i),nh=p(ze),Pl=l(ze,"LI",{});var U6=n(Pl);sh=a(U6,`La tua function-signature dev\u2019essere type-annoted. Per il resto, \xE9 meglio preferire variabili con un nome accettabile
piuttosto che annotazioni per aumentare la comprensione e leggibilit\xE0 del codice.`),U6.forEach(i),ze.forEach(i),Hd=p(e),Be=l(e,"H3",{class:!0});var Xu=n(Be);no=l(Xu,"A",{id:!0,class:!0,href:!0});var F6=n(no);$l=l(F6,"SPAN",{});var Q6=n($l);f(Pi.$$.fragment,Q6),Q6.forEach(i),F6.forEach(i),ch=p(Xu),Tl=l(Xu,"SPAN",{});var G6=n(Tl);dh=a(G6,"Panoramica sui tokenizers"),G6.forEach(i),Xu.forEach(i),xd=p(e),Vt=l(e,"P",{});var H6=n(Vt);ph=a(H6,"Questa sezione sar\xE0 creata al piu presto :-("),H6.forEach(i),Wd=p(e),De=l(e,"H2",{class:!0});var Zu=n(De);so=l(Zu,"A",{id:!0,class:!0,href:!0});var x6=n(so);kl=l(x6,"SPAN",{});var W6=n(kl);f($i.$$.fragment,W6),W6.forEach(i),x6.forEach(i),mh=p(Zu),yl=l(Zu,"SPAN",{});var J6=n(yl);uh=a(J6,"Aggiungere un modello a \u{1F917} Transformers passo dopo passo"),J6.forEach(i),Zu.forEach(i),Jd=p(e),Xt=l(e,"P",{});var K6=n(Xt);fh=a(K6,"Ci sono differenti modi per aggiungere un modello a Hugging Face. Qui trovi una lista di blog posts da parte della community su come aggiungere un modello:"),K6.forEach(i),Kd=p(e),co=l(e,"OL",{});var Yu=n(co);Ti=l(Yu,"LI",{});var ef=n(Ti);ki=l(ef,"A",{href:!0,rel:!0});var V6=n(ki);vh=a(V6,"Aggiungere GPT2"),V6.forEach(i),gh=a(ef," scritto da "),yi=l(ef,"A",{href:!0,rel:!0});var X6=n(yi);hh=a(X6,"Thomas"),X6.forEach(i),ef.forEach(i),_h=p(Yu),qi=l(Yu,"LI",{});var of=n(qi);Li=l(of,"A",{href:!0,rel:!0});var Z6=n(Li);bh=a(Z6,"Aggiungere WMT19 MT"),Z6.forEach(i),Eh=a(of," scritto da "),Ii=l(of,"A",{href:!0,rel:!0});var Y6=n(Ii);zh=a(Y6,"Stas"),Y6.forEach(i),of.forEach(i),Yu.forEach(i),Vd=p(e),Zt=l(e,"P",{});var e7=n(Zt);wh=a(e7,"Per esperienza, possiamo dirti che quando si aggiunge un modello \xE9 meglio tenere a mente le seguenti considerazioni:"),e7.forEach(i),Xd=p(e),ue=l(e,"UL",{});var gr=n(ue);ce=l(gr,"LI",{});var ei=n(ce);Ph=a(ei,`Non sfondare una porta gi\xE1 aperta! La maggior parte del codice che aggiungerai per un nuovo modello \u{1F917} Transformers
esiste gi\xE0 da qualche parte in \u{1F917} Transformers. Prendi un po\u2019 di tempo per trovare codici simili in modelli e tokenizers esistenti e fare un copia-incolla. Ricorda che `),Ci=l(ei,"A",{href:!0,rel:!0});var o7=n(Ci);$h=a(o7,"grep"),o7.forEach(i),Th=a(ei," e "),Oi=l(ei,"A",{href:!0,rel:!0});var i7=n(Oi);kh=a(i7,"rg"),i7.forEach(i),yh=a(ei," sono tuoi buoni amici. Inoltre, ricorda che pu\xF3 essere molto probabile che il tokenizer per il tuo modello sia basato sull\u2019implementazione di un altro modello, e il codice del tuo modello stesso su un altro ancora. "),ql=l(ei,"EM",{});var t7=n(ql);qh=a(t7,"Per esempio"),t7.forEach(i),Lh=a(ei," il modello FSMT \xE9 basato su BART, mentre il tokenizer di FSMT \xE9 basato su XLM."),ei.forEach(i),Ih=p(gr),Ll=l(gr,"LI",{});var a7=n(Ll);Ch=a(a7,"Ricorda che qui \xE9 piu una sfida ingegneristica che scientifica. Spendi pi\xFA tempo per create un efficiente ambiente di debugging piuttosto che cercare di capire tutti gli aspetti teorici dell\u2019articolo del modello."),a7.forEach(i),Oh=p(gr),Il=l(gr,"LI",{});var r7=n(Il);Ah=a(r7,"Chiedi aiuto se sei in panne! I modelli sono la parte principale di \u{1F917} Transformers, perci\xF2 qui a Hugging Face siamo pi\xF9 che contenti di aiutarti in ogni passo per aggiungere il tuo modello. Non esitare a chiedere se vedi che non riesci a progredire."),r7.forEach(i),gr.forEach(i),Zd=p(e),Yt=l(e,"P",{});var l7=n(Yt);Nh=a(l7,"Di seguito, diamo una ricetta generale per aiutare a portare un modello in \u{1F917} Transformers."),l7.forEach(i),Yd=p(e),ea=l(e,"P",{});var n7=n(ea);Bh=a(n7,"La lista seguente \xE9 un sommario di tutto quello che \xE9 stato fatto per aggiungere un modello, e pu\xF2 essere usata come To-Do List:"),n7.forEach(i),ep=p(e),E=l(e,"UL",{});var P=n(E);Cl=l(P,"LI",{});var s7=n(Cl);Ol=l(s7,"OL",{});var c7=n(Ol);Al=l(c7,"LI",{});var d7=n(Al);Dh=a(d7,"\u2610 (Opzionale) Capire gli aspetti teorici del modello"),d7.forEach(i),c7.forEach(i),s7.forEach(i),Sh=p(P),Nl=l(P,"LI",{});var p7=n(Nl);oa=l(p7,"OL",{start:!0});var m7=n(oa);Bl=l(m7,"LI",{});var u7=n(Bl);jh=a(u7,"\u2610 Preparare l\u2019ambiente dev per transformers"),u7.forEach(i),m7.forEach(i),p7.forEach(i),Mh=p(P),Dl=l(P,"LI",{});var f7=n(Dl);ia=l(f7,"OL",{start:!0});var v7=n(ia);Sl=l(v7,"LI",{});var g7=n(Sl);Rh=a(g7,"\u2610 Preparare l\u2019ambiente debugging della repository originale"),g7.forEach(i),v7.forEach(i),f7.forEach(i),Uh=p(P),jl=l(P,"LI",{});var h7=n(jl);ta=l(h7,"OL",{start:!0});var _7=n(ta);Ml=l(_7,"LI",{});var b7=n(Ml);Fh=a(b7,"\u2610 Create uno script che gestisca con successo il forward pass usando la repository originale e checkpoint"),b7.forEach(i),_7.forEach(i),h7.forEach(i),Qh=p(P),Rl=l(P,"LI",{});var E7=n(Rl);aa=l(E7,"OL",{start:!0});var z7=n(aa);Ul=l(z7,"LI",{});var w7=n(Ul);Gh=a(w7,"\u2610 Aggiungere con successo lo scheletro del modello a Transformers"),w7.forEach(i),z7.forEach(i),E7.forEach(i),Hh=p(P),Fl=l(P,"LI",{});var P7=n(Fl);ra=l(P7,"OL",{start:!0});var $7=n(ra);Ql=l($7,"LI",{});var T7=n(Ql);xh=a(T7,"\u2610 Convertire i checkpoint original a Transformers checkpoint"),T7.forEach(i),$7.forEach(i),P7.forEach(i),Wh=p(P),Gl=l(P,"LI",{});var k7=n(Gl);la=l(k7,"OL",{start:!0});var y7=n(la);Hl=l(y7,"LI",{});var q7=n(Hl);Jh=a(q7,"\u2610 Effettuare con successo la forward pass in Transformers, di modo che dia un output identico al checkpoint originale"),q7.forEach(i),y7.forEach(i),k7.forEach(i),Kh=p(P),xl=l(P,"LI",{});var L7=n(xl);na=l(L7,"OL",{start:!0});var I7=n(na);Wl=l(I7,"LI",{});var C7=n(Wl);Vh=a(C7,"\u2610 Finire i tests per il modello in Transformers"),C7.forEach(i),I7.forEach(i),L7.forEach(i),Xh=p(P),Jl=l(P,"LI",{});var O7=n(Jl);sa=l(O7,"OL",{start:!0});var A7=n(sa);Kl=l(A7,"LI",{});var N7=n(Kl);Zh=a(N7,"\u2610 Aggiungere con successo Tokenizer in Transformers"),N7.forEach(i),A7.forEach(i),O7.forEach(i),Yh=p(P),Vl=l(P,"LI",{});var B7=n(Vl);ca=l(B7,"OL",{start:!0});var D7=n(ca);Xl=l(D7,"LI",{});var S7=n(Xl);e_=a(S7,"\u2610 Testare e provare gli integration tests da capo a fine"),S7.forEach(i),D7.forEach(i),B7.forEach(i),o_=p(P),Zl=l(P,"LI",{});var j7=n(Zl);da=l(j7,"OL",{start:!0});var M7=n(da);Yl=l(M7,"LI",{});var R7=n(Yl);i_=a(R7,"\u2610 Completare i docs"),R7.forEach(i),M7.forEach(i),j7.forEach(i),t_=p(P),en=l(P,"LI",{});var U7=n(en);pa=l(U7,"OL",{start:!0});var F7=n(pa);on=l(F7,"LI",{});var Q7=n(on);a_=a(Q7,"\u2610 Caricare i moedl weights all\u2019hub"),Q7.forEach(i),F7.forEach(i),U7.forEach(i),r_=p(P),tn=l(P,"LI",{});var G7=n(tn);ma=l(G7,"OL",{start:!0});var H7=n(ma);an=l(H7,"LI",{});var x7=n(an);l_=a(x7,"\u2610 Sottomettere una pull request"),x7.forEach(i),H7.forEach(i),G7.forEach(i),n_=p(P),rn=l(P,"LI",{});var W7=n(rn);ua=l(W7,"OL",{start:!0});var J7=n(ua);ln=l(J7,"LI",{});var K7=n(ln);s_=a(K7,"\u2610 (Opzionale) Aggiungere un notebook con una demo"),K7.forEach(i),J7.forEach(i),W7.forEach(i),P.forEach(i),op=p(e),N=l(e,"P",{});var we=n(N);c_=a(we,"Per cominciare di solito consigliamo "),nn=l(we,"CODE",{});var V7=n(nn);d_=a(V7,"BrandNewBert"),V7.forEach(i),p_=a(we,", partendo dalla teoria, di modo da avere una buona comprensione della teoria generale. TUttavia, se preferisci imparare l\u2019aspetto teorico del modello mentre "),sn=l(we,"EM",{});var X7=n(sn);m_=a(X7,"lavori"),X7.forEach(i),u_=a(we," sul modello \xE9 ok immergersi direttamente nel codice di "),cn=l(we,"CODE",{});var Z7=n(cn);f_=a(Z7,"BrandNewBert"),Z7.forEach(i),v_=a(we,". Questa opzione pu\xF3 essere buona se le tue skills ingegneristiche sono meglio che quelle teoriche, o se il paper "),dn=l(we,"CODE",{});var Y7=n(dn);g_=a(Y7,"BrandNewBert"),Y7.forEach(i),h_=a(we," ti d\xE1 problemi, o se semplicemente ti piace programmare pi\xFA che leggere articoli scientifici."),we.forEach(i),ip=p(e),Se=l(e,"H3",{class:!0});var tf=n(Se);po=l(tf,"A",{id:!0,class:!0,href:!0});var ez=n(po);pn=l(ez,"SPAN",{});var oz=n(pn);f(Ai.$$.fragment,oz),oz.forEach(i),ez.forEach(i),__=p(tf),mn=l(tf,"SPAN",{});var iz=n(mn);b_=a(iz,"1. (Opzionale) Aspetti teorici di BrandNewBert"),iz.forEach(i),tf.forEach(i),tp=p(e),mo=l(e,"P",{});var af=n(mo);E_=a(af,"Allora con calma, prendi un po\u2019 di tempo per leggere l\u2019articolo su "),un=l(af,"EM",{});var tz=n(un);z_=a(tz,"BrandNewBert"),tz.forEach(i),w_=a(af," . Sicuramente, alcune sezioni dell\u2019articolo sono molto complesse, ma non preoccuparti! L\u2019obiettivo non \xE9 avere una compresione immensa della teoria alla base, ma estrarre le informazioni necessarie per re-implementare con successo il modello in \u{1F917} Transformers. Quindi, non impazzire sugli aspetti teorici, ma piuttosto focalizzati su quelli pratici, ossia:"),af.forEach(i),ap=p(e),B=l(e,"UL",{});var Pe=n(B);je=l(Pe,"LI",{});var hr=n(je);P_=a(hr,"Che tipo di modello \xE9 "),fn=l(hr,"EM",{});var az=n(fn);$_=a(az,"brand_new_bert"),az.forEach(i),T_=a(hr,"? \xC9 solo un encoder in stile BERT? O tipo decoder come GPT2? O encoder e decoder stile BART? Dai un\u2019occhiata a "),fa=l(hr,"A",{href:!0});var rz=n(fa);k_=a(rz,"model_summary"),rz.forEach(i),y_=a(hr," se non sei famigliare con le differenze tra questi modelli"),hr.forEach(i),q_=p(Pe),Ni=l(Pe,"LI",{});var rf=n(Ni);L_=a(rf,"Quali sono le applicazioni di "),vn=l(rf,"EM",{});var lz=n(vn);I_=a(lz,"brand_new_bert"),lz.forEach(i),C_=a(rf,"? Classificazione di testo? Generazione di testo? O per tasks del genere seq2seq?"),rf.forEach(i),O_=p(Pe),gn=l(Pe,"LI",{});var nz=n(gn);A_=a(nz,"Quali sono le nuove aggiunte al modello che lo rendono diverso da BERT/GPT-2/BART?"),nz.forEach(i),N_=p(Pe),Me=l(Pe,"LI",{});var _r=n(Me);B_=a(_r,"Quali modelli estistenti in "),Bi=l(_r,"A",{href:!0,rel:!0});var sz=n(Bi);D_=a(sz,"\u{1F917} Transformers models"),sz.forEach(i),S_=a(_r," sono molto simili a "),hn=l(_r,"EM",{});var cz=n(hn);j_=a(cz,"brand_new_bert"),cz.forEach(i),M_=a(_r,"?"),_r.forEach(i),R_=p(Pe),_n=l(Pe,"LI",{});var dz=n(_n);U_=a(dz,"Che tipo di tokenizer si usa in questo caso? Un sentencepiece tokenizer? O un word piece tokenizer? Il tokenizer \xE9 lo stesso di BERT o BART?"),dz.forEach(i),Pe.forEach(i),rp=p(e),va=l(e,"P",{});var pz=n(va);F_=a(pz,"Una volta che senti che hai avuto una bella overview dell\u2019architettura del modello, puoi scrivere senza problemi al team di Hugging Face per ogni domanda che tu hai. Questo pu\xF3 includere domande sull\u2019architettura del modello, o sull\u2019attention layer, etc. Saremo molto felici di aiutarti :)"),pz.forEach(i),lp=p(e),Re=l(e,"H3",{class:!0});var lf=n(Re);uo=l(lf,"A",{id:!0,class:!0,href:!0});var mz=n(uo);bn=l(mz,"SPAN",{});var uz=n(bn);f(Di.$$.fragment,uz),uz.forEach(i),mz.forEach(i),Q_=p(lf),En=l(lf,"SPAN",{});var fz=n(En);G_=a(fz,"2. Prepare il tuo ambiente"),fz.forEach(i),lf.forEach(i),np=p(e),fo=l(e,"OL",{});var nf=n(fo);zn=l(nf,"LI",{});var vz=n(zn);Si=l(vz,"P",{});var sf=n(Si);H_=a(sf,"Forka la "),ji=l(sf,"A",{href:!0,rel:!0});var gz=n(ji);x_=a(gz,"repository"),gz.forEach(i),W_=a(sf," cliccando sul tasto \u2018Fork\u2019 nella pagina della repository. Questo crea una copia del codice nel tuo account GitHub"),sf.forEach(i),vz.forEach(i),J_=p(nf),wn=l(nf,"LI",{});var hz=n(wn);Mi=l(hz,"P",{});var cf=n(Mi);K_=a(cf,"Clona il tuo fork "),Pn=l(cf,"CODE",{});var _z=n(Pn);V_=a(_z,"transfomers"),_z.forEach(i),X_=a(cf," sul tuo dico locale, e aggiungi la repository base come remota:"),cf.forEach(i),hz.forEach(i),nf.forEach(i),sp=p(e),f(Ri.$$.fragment,e),cp=p(e),Ui=l(e,"OL",{start:!0});var bz=n(Ui);$n=l(bz,"LI",{});var Ez=n($n);Z_=a(Ez,"Crea un ambiente di sviluppo, per esempio tramite questo comando:"),Ez.forEach(i),bz.forEach(i),dp=p(e),f(Fi.$$.fragment,e),pp=p(e),ga=l(e,"P",{});var zz=n(ga);Y_=a(zz,"quindi torna alla directory principale:"),zz.forEach(i),mp=p(e),f(Qi.$$.fragment,e),up=p(e),Gi=l(e,"OL",{start:!0});var wz=n(Gi);Ue=l(wz,"LI",{});var br=n(Ue);e1=a(br,"Attenzione, raccomandiamo di aggiungere la versione di PyTorch di "),Tn=l(br,"EM",{});var Pz=n(Tn);o1=a(Pz,"brand_new_bert"),Pz.forEach(i),i1=a(br," a Transfomers. Per installare PyTorch, basta seguire queste istruzioni "),Hi=l(br,"A",{href:!0,rel:!0});var $z=n(Hi);t1=a($z,"https://pytorch.org/get-started/locally/"),$z.forEach(i),a1=a(br,"."),br.forEach(i),wz.forEach(i),fp=p(e),xi=l(e,"P",{});var T5=n(xi);kn=l(T5,"STRONG",{});var Tz=n(kn);r1=a(Tz,"Nota bene:"),Tz.forEach(i),l1=a(T5," Non c\u2019\xE9 bisogno di installare o avere installato CUDA. Il nuovo modello pu\xF2 funzionare senza problemi su una CPU."),T5.forEach(i),vp=p(e),Wi=l(e,"OL",{start:!0});var kz=n(Wi);Fe=l(kz,"LI",{});var Er=n(Fe);n1=a(Er,"Per trasferire "),yn=l(Er,"EM",{});var yz=n(yn);s1=a(yz,"brand_new_bert"),yz.forEach(i),c1=a(Er," To port "),qn=l(Er,"EM",{});var qz=n(qn);d1=a(qz,"brand_new_bert"),qz.forEach(i),p1=a(Er," avrai bisogno anche accesso alla sua repository originale:"),Er.forEach(i),kz.forEach(i),gp=p(e),f(Ji.$$.fragment,e),hp=p(e),vo=l(e,"P",{});var df=n(vo);m1=a(df,"Ok, ora hai un ambiente di sviluppo per portare "),Ln=l(df,"EM",{});var Lz=n(Ln);u1=a(Lz,"brand_new_bert"),Lz.forEach(i),f1=a(df," in \u{1F917} Transformers."),df.forEach(i),_p=p(e),Qe=l(e,"H3",{class:!0});var pf=n(Qe);go=l(pf,"A",{id:!0,class:!0,href:!0});var Iz=n(go);In=l(Iz,"SPAN",{});var Cz=n(In);f(Ki.$$.fragment,Cz),Cz.forEach(i),Iz.forEach(i),v1=p(pf),Cn=l(pf,"SPAN",{});var Oz=n(Cn);g1=a(Oz,"3.-4. Provare un pretrained checkpoint usando la repo originale"),Oz.forEach(i),pf.forEach(i),bp=p(e),q=l(e,"P",{});var te=n(q);h1=a(te,"Per cominciare, comincerai a lavorare sulla repo originale di "),On=l(te,"EM",{});var Az=n(On);_1=a(Az,"brand_new_bert"),Az.forEach(i),b1=a(te,". Come spesso accade, l\u2019implementazione originale \xE9 molto sullo stile \u201Cricerca\u201D. Questo significa che a volte la documentazione non \xE9 al top, magari manca qualche cosa e il codice pu\xF3 essere difficile da capire. Tuttavia, questa \xE9 e dev\u2019essere la motivazione per reimplementare "),An=l(te,"EM",{});var Nz=n(An);E1=a(Nz,"brand_new_bert"),Nz.forEach(i),z1=a(te,". In Hugging Face, uno degli obiettivi principali \xE9 di "),Nn=l(te,"EM",{});var Bz=n(Nn);w1=a(Bz,"mettere le persone sulle spalle dei giganti"),Bz.forEach(i),P1=a(te,", il che si traduce, in questo contesto, di prendere un modello funzionante e riscriverlo e renderlo il pi\xFA possibile "),Bn=l(te,"STRONG",{});var Dz=n(Bn);$1=a(Dz,"accessibile, user-friendly, e leggibile"),Dz.forEach(i),T1=a(te,". Questa \xE9 la top motivazione per re-implementare modelli in \u{1F917} Transformers - cercare di creare nuove complesse tecnologie NLP accessibili a "),Dn=l(te,"STRONG",{});var Sz=n(Dn);k1=a(Sz,"chiunque"),Sz.forEach(i),y1=a(te,"."),te.forEach(i),Ep=p(e),ho=l(e,"P",{});var mf=n(ho);q1=a(mf,"Riuscire a far girare il modello pretrained originale dalla repository ufficiale \xE9 spesso il passo "),Sn=l(mf,"STRONG",{});var jz=n(Sn);L1=a(jz,"piu arduo"),jz.forEach(i),I1=a(mf,". Dalla nostra esperienza, \xE9 molto importante spendere un p\u2019 di tempo per diventare familiari con il codice base originale. Come test, prova a capire i seguenti punti:"),mf.forEach(i),zp=p(e),L=l(e,"UL",{});var ae=n(L);jn=l(ae,"LI",{});var Mz=n(jn);C1=a(Mz,"Dove si trovano i pretrained weights?"),Mz.forEach(i),O1=p(ae),Mn=l(ae,"LI",{});var Rz=n(Mn);A1=a(Rz,"Come caricare i pretrained weights nel modello corrispondente?"),Rz.forEach(i),N1=p(ae),Rn=l(ae,"LI",{});var Uz=n(Rn);B1=a(Uz,"Come girare un tokenizer independentemente dal modello?"),Uz.forEach(i),D1=p(ae),Un=l(ae,"LI",{});var Fz=n(Un);S1=a(Fz,"Prova a tracciare un singolo forward pass, cosicch\xE9 potrai sapere che classi e funzioni sono richieste per un semplice forward pass. Di solito, dovrai reimplementare queste funzioni e basta"),Fz.forEach(i),j1=p(ae),R=l(ae,"LI",{});var $e=n(R);M1=a($e,"Prova a localizzare i componenti importanti del modello: Dove si trova la classe del modello? Ci sono sotto classi nel modello "),Fn=l($e,"EM",{});var Qz=n(Fn);R1=a(Qz,"per esempio"),Qz.forEach(i),U1=a($e," EngoderModel, DecoderMOdel? Dove si trova il self-attention layer? Ci sono molteplici differenti layer di attention, "),Qn=l($e,"EM",{});var Gz=n(Qn);F1=a(Gz,"per esempio"),Gz.forEach(i),Q1=p($e),Gn=l($e,"EM",{});var Hz=n(Gn);G1=a(Hz,"self-attention"),Hz.forEach(i),H1=a($e,", "),Hn=l($e,"EM",{});var xz=n(Hn);x1=a(xz,"cross-attention"),xz.forEach(i),W1=a($e,"\u2026?"),$e.forEach(i),J1=p(ae),Ge=l(ae,"LI",{});var zr=n(Ge);K1=a(zr,"Come puoi fare debug sul modello nell\u2019ambiente originale della repo? Devi aggiungere dei "),xn=l(zr,"EM",{});var Wz=n(xn);V1=a(Wz,"print"),Wz.forEach(i),X1=a(zr," o puoi usare "),Wn=l(zr,"EM",{});var Jz=n(Wn);Z1=a(Jz,"ipdb"),Jz.forEach(i),Y1=a(zr," come debugger interattivo, o vabene anche un IDE efficiente per debug come PyCharm?"),zr.forEach(i),ae.forEach(i),wp=p(e),_o=l(e,"P",{});var uf=n(_o);eb=a(uf,"\xC9 molto importante che prima di cominciare a trasferire il modello nuovo tu spenda tempo a fare debug del codice originale in maniera "),Jn=l(uf,"STRONG",{});var Kz=n(Jn);ob=a(Kz,"efficiente"),Kz.forEach(i),ib=a(uf,"! Inoltre, ricorda che tutta la library \xE9 open-soruce, quindi non temere di aprire issue o fare una pull request nella repo originale. Tutti coloro che mantengono la repository saranno pi\xFA che felici di avere qualcuno che guarda e gioca con i loro codici!"),uf.forEach(i),Pp=p(e),ha=l(e,"P",{});var Vz=n(ha);tb=a(Vz,"A questo punto, sta a te decidere quale ambiente per debug vuoi usare. Noi consilgiamo di evitare setup con GPU, che potrebbero costare assai, lavorare su una CPU pu\xF3 essere un ottimo punto di partenza per indagare la repository originale e per cominciare a scrivere il codice per \u{1F917} Transformers. Solo alla fine, quando il modello \xE9 stato portato con successo in  \u{1F917} Transformers, allora si potr\xE1 verificare il suo funzionamento su GPU."),Vz.forEach(i),$p=p(e),_a=l(e,"P",{});var Xz=n(_a);ab=a(Xz,"In generale ci sono due possibili ambienti di debug per il testare il modello originale:"),Xz.forEach(i),Tp=p(e),bo=l(e,"UL",{});var ff=n(bo);Vi=l(ff,"LI",{});var vf=n(Vi);Xi=l(vf,"A",{href:!0,rel:!0});var Zz=n(Xi);rb=a(Zz,"Jupyter notebooks"),Zz.forEach(i),lb=a(vf," / "),Zi=l(vf,"A",{href:!0,rel:!0});var Yz=n(Zi);nb=a(Yz,"google colab"),Yz.forEach(i),vf.forEach(i),sb=p(ff),Kn=l(ff,"LI",{});var ew=n(Kn);cb=a(ew,"Scripts locali in Python"),ew.forEach(i),ff.forEach(i),kp=p(e),ba=l(e,"P",{});var ow=n(ba);db=a(ow,"Il vantaggio dei Jupyter notebooks \xE9 la possibilit\xE0 di eseguire cella per cella, il che pu\xF2 essere utile per decomporre tutte le componenti logiche, cosi da a vere un ciclo di debug pi\xF9 rapido, siccome si possono salvare i risultati da steps intermedi. Inoltre, i notebooks spesso sono molto facili da condividere con altri contributors, il che pu\xF2 essere molto utile se vuoi chiedere aiuto al team di Hugging Face. Se sei famigliare con Jupyter notebooks allora racommandiamo di lavorare in questa maniera."),ow.forEach(i),yp=p(e),Eo=l(e,"P",{});var gf=n(Eo);pb=a(gf,"Ovviamente se non siete abituati a lavorare con i notebook, questo pu\xF2 essere uno svantaggio nell\u2019usare questa tecnologia, sprecando un sacco di tempo per setup e portare tutto al nuovo ambiente, siccome non potreste neanche usare dei tools di debug come "),Vn=l(gf,"CODE",{});var iw=n(Vn);mb=a(iw,"ipdb"),iw.forEach(i),ub=a(gf,"."),gf.forEach(i),qp=p(e),zo=l(e,"P",{});var hf=n(zo);fb=a(hf,"Per ogni pratica code-base, \xE9 sempre meglio come primo step caricare un "),Xn=l(hf,"STRONG",{});var tw=n(Xn);vb=a(tw,"piccolo"),tw.forEach(i),gb=a(hf," checkpoint pretrained e cercare di riprodurre un singolo forward pass usando un vettore fittizio di IDs fatti da numeri interi. Un esempio per uno script simile, in pseudocodice \xE9:"),hf.forEach(i),Lp=p(e),f(Yi.$$.fragment,e),Ip=p(e),Ea=l(e,"P",{});var aw=n(Ea);hb=a(aw,"Per quanto riguarda la strategia di debugging, si pu\xF2 scegliere tra:"),aw.forEach(i),Cp=p(e),wo=l(e,"UL",{});var _f=n(wo);Zn=l(_f,"LI",{});var rw=n(Zn);_b=a(rw,"Decomporre il modello originario in piccole componenenti e testare ognuna di esse"),rw.forEach(i),bb=p(_f),He=l(_f,"LI",{});var wr=n(He);Eb=a(wr,"Decomporre il modello originario nel "),Yn=l(wr,"EM",{});var lw=n(Yn);zb=a(lw,"tokenizer"),lw.forEach(i),wb=a(wr," originale e nel "),es=l(wr,"EM",{});var nw=n(es);Pb=a(nw,"modello"),nw.forEach(i),$b=a(wr,` originale, testare un forward pass su questi,
e usare dei print statement o breakpoints intermedi per verificare`),wr.forEach(i),_f.forEach(i),Op=p(e),za=l(e,"P",{});var sw=n(za);Tb=a(sw,`Ancora una volta, siete liberi di scegliere quale strategia sia ottimale per voi. Spesso una strategia \xE9 piu
avvantaggiosa di un\u2019altra, ma tutto dipende dall\u2019code-base originario.`),sw.forEach(i),Ap=p(e),Po=l(e,"P",{});var bf=n(Po);kb=a(bf,"Se il code-base vi permette di decomporre il modello in piccole sub-componenenti, "),os=l(bf,"EM",{});var cw=n(os);yb=a(cw,"per esempio"),cw.forEach(i),qb=a(bf,` se il code-base
originario pu\xF2 essere facilmente testato in eager mode, allora vale la pena effettuare un debugging di questo genere.
Ricordate che ci sono dei vantaggi nel decidere di prendere la strada piu impegnativa sin da subito:`),bf.forEach(i),Np=p(e),G=l(e,"UL",{});var oi=n(G);is=l(oi,"LI",{});var dw=n(is);Lb=a(dw,`negli stage piu finali, quando bisogner\xE0 comparare il modello originario all\u2019implementazione in Hugging Face, potrete verificare
automaticamente ogni componente, individualmente, di modo che ci sia una corrispondenza 1:1`),dw.forEach(i),Ib=p(oi),ts=l(oi,"LI",{});var pw=n(ts);Cb=a(pw,"avrete l\u2019opportunit\xE0 di decomporre un problema molto grande in piccoli passi, cos\xEC da strutturare meglio il vostro lavoro"),pw.forEach(i),Ob=p(oi),as=l(oi,"LI",{});var mw=n(as);Ab=a(mw,`separare il modello in componenti logiche vi aiuter\xE0 ad avere un\u2019ottima overview sul design del modello, quindi una migliore
comprensione del modello stesso`),mw.forEach(i),Nb=p(oi),rs=l(oi,"LI",{});var uw=n(rs);Bb=a(uw,`verso gli stage finali i test fatti componente per componente vi aiuter\xE0 ad essere sicuri di non andare avanti e indietro
nell\u2019implementazione, cos\xEC da continuare la modifica del codice senza interruzione`),uw.forEach(i),oi.forEach(i),Bp=p(e),$o=l(e,"P",{});var Ef=n($o);Db=a(Ef,"Un ottimo esempio di come questo pu\xF2 essere fatto \xE9 dato da "),et=l(Ef,"A",{href:!0,rel:!0});var fw=n(et);Sb=a(fw,"Lysandre"),fw.forEach(i),jb=a(Ef,`
per il modello ELECTRA`),Ef.forEach(i),Dp=p(e),To=l(e,"P",{});var zf=n(To);Mb=a(zf,`Tuttavia, se il code-base originale \xE9 molto complesso o le componenti intermedie possono essere testate solo in tramite
compilazione, potrebbe richiedere parecchio tempo o addirittura essere impossibile separare il modello in piccole sotto-componenti.
Un buon esempio \xE9 `),ot=l(zf,"A",{href:!0,rel:!0});var vw=n(ot);Rb=a(vw,"MeshTensorFlow di T5"),vw.forEach(i),Ub=a(zf,`. Questa libreria
\xE9 molto complessa e non offre un metodo semplice di decomposizione in sotto-componenti. Per simili librerie, potrete fare
affidamento ai print statements.`),zf.forEach(i),Sp=p(e),wa=l(e,"P",{});var gw=n(wa);Fb=a(gw,`In ogni caso, indipendentemente da quale strategia scegliete, la procedura raccomandata \xE9 di cominciare a fare debug dal
primo layer al layer finale.
\xC9 consigliato recuperare gli output dai layers, tramite print o sotto-componenti, nel seguente ordine:`),gw.forEach(i),jp=p(e),I=l(e,"OL",{});var re=n(I);ls=l(re,"LI",{});var hw=n(ls);Qb=a(hw,"Recuperare gli IDs di input dati al modello"),hw.forEach(i),Gb=p(re),ns=l(re,"LI",{});var _w=n(ns);Hb=a(_w,"Recuperare i word embeddings"),_w.forEach(i),xb=p(re),ss=l(re,"LI",{});var bw=n(ss);Wb=a(bw,"Recuperare l\u2019input del primo Transformer layer"),bw.forEach(i),Jb=p(re),cs=l(re,"LI",{});var Ew=n(cs);Kb=a(Ew,"Recuperare l\u2019output del primo Transformer layer"),Ew.forEach(i),Vb=p(re),it=l(re,"LI",{});var wf=n(it);Xb=a(wf,"Recuperare l\u2019output dei seguenti "),ds=l(wf,"CODE",{});var zw=n(ds);Zb=a(zw,"n - 1"),zw.forEach(i),Yb=a(wf," Transformer layers"),wf.forEach(i),e0=p(re),ps=l(re,"LI",{});var ww=n(ps);o0=a(ww,"Recuperare l\u2019output dell\u2019intero BrandNewBert Model"),ww.forEach(i),re.forEach(i),Mp=p(e),xe=l(e,"P",{});var zd=n(xe);i0=a(zd,"Gli IDs in input dovrebbero essere un arrary di interi, "),ms=l(zd,"EM",{});var Pw=n(ms);t0=a(Pw,"per esempio"),Pw.forEach(i),a0=p(zd),us=l(zd,"CODE",{});var $w=n(us);r0=a($w,"input_ids = [0, 4, 4, 3, 2, 4, 1, 7, 19]"),$w.forEach(i),zd.forEach(i),Rp=p(e),Pa=l(e,"P",{});var Tw=n(Pa);l0=a(Tw,"Gli output dei seguenti layer di solito dovrebbero essere degli array di float multi-dimensionali come questo:"),Tw.forEach(i),Up=p(e),f(tt.$$.fragment,e),Fp=p(e),ko=l(e,"P",{});var Pf=n(ko);n0=a(Pf,`Ci aspettiamo che ogni modello aggiunto a \u{1F917} Transformers passi con successo un paio di test d\u2019integrazione. Questo
significa che il modello originale e la sua implementazione in \u{1F917} Transformers abbiano lo stesso output con una precisione
di 0.001! Siccome \xE9 normale che lo stesso esatto modello, scritto in librerie diverse, possa dare output leggermente
diversi, la tolleranza accettata \xE9 1e-3 (0.001). Ricordate che i due modelli devono dare output quasi identici. Dunque,
\xE9 molto conveniente comparare gli output intermedi di \u{1F917} Transformers molteplici volte con gli output intermedi del
modello originale di `),fs=l(Pf,"EM",{});var kw=n(fs);s0=a(kw,"brand_new_bert"),kw.forEach(i),c0=a(Pf,`. Di seguito vi diamo alcuni consigli per avere un ambiente di debug il piu efficiente
possibile:`),Pf.forEach(i),Qp=p(e),D=l(e,"UL",{});var Te=n(D);U=l(Te,"LI",{});var ke=n(U);d0=a(ke,`Trovate la migliore strategia per fare debug dei risultati intermedi. Per esempio, \xE9 la repository originale scritta in PyTorch?
Se si, molto probabilmente dovrete dedicare un po\u2019 di tempo per scrivere degli script piu lunghi, cos\xEC da decomporre il
modello originale in piccole sotto-componenti, in modo da poter recuperare i valori intermedi. Oppure, la repo originale
\xE9 scritta in Tensorflow 1? Se \xE9 cos\xEC dovrete fare affidamento ai print di Tensorflow `),at=l(ke,"A",{href:!0,rel:!0});var yw=n(at);p0=a(yw,"tf.print"),yw.forEach(i),m0=a(ke,`
per avere i valori intermedi. Altro caso, la repo \xE9 scritta in Jax? Allora assicuratevi che il modello non sia in `),vs=l(ke,"STRONG",{});var qw=n(vs);u0=a(qw,"jit"),qw.forEach(i),f0=a(ke,`
quanto testate il foward pass, `),gs=l(ke,"EM",{});var Lw=n(gs);v0=a(Lw,"per esempio"),Lw.forEach(i),g0=a(ke," controllate "),rt=l(ke,"A",{href:!0,rel:!0});var Iw=n(rt);h0=a(Iw,"questo link"),Iw.forEach(i),_0=a(ke,"."),ke.forEach(i),b0=p(Te),hs=l(Te,"LI",{});var Cw=n(hs);E0=a(Cw,`Usate i pi\xF9 piccoli pretrained checkpoint che potete trovare. Piu piccolo \xE9 il checkpoint, piu velocemente sar\xE0 il vostro
ciclo di debug. Non \xE9 efficiente avere un pretrained model cos\xEC gigante che per il forward pass impieghi piu di 10 secondi.
Nel caso in cui i checkpoints siano molto grandi, e non si possa trovare di meglio, allora \xE9 buona consuetudine ricorrere
a fare un dummy model nel nuovo ambiente, con weights inizializzati random e salvare quei weights per comprare la versione \u{1F917} Transformers
con il vostro modello`),Cw.forEach(i),z0=p(Te),$=l(Te,"LI",{});var T=n($);w0=a(T,`Accertatevi di usare la via piu semplice per chiamare il forward pass nella repo originale. Sarebbe opportuno trovare
la funzione originaria che chiami `),_s=l(T,"STRONG",{});var Ow=n(_s);P0=a(Ow,"solo"),Ow.forEach(i),$0=a(T," un singolo forward pass, "),bs=l(T,"EM",{});var Aw=n(bs);T0=a(Aw,"per esempio"),Aw.forEach(i),k0=a(T,` questa funzione spesso viene chiamata
`),Es=l(T,"CODE",{});var Nw=n(Es);y0=a(Nw,"predict"),Nw.forEach(i),q0=a(T,", "),zs=l(T,"CODE",{});var Bw=n(zs);L0=a(Bw,"evaluate"),Bw.forEach(i),I0=a(T,", "),ws=l(T,"CODE",{});var Dw=n(ws);C0=a(Dw,"forward"),Dw.forEach(i),O0=a(T," o "),Ps=l(T,"CODE",{});var Sw=n(Ps);A0=a(Sw,"__call__"),Sw.forEach(i),N0=a(T,". Siate sicuri di non fare debug su una funzione che chiami "),$s=l(T,"CODE",{});var jw=n($s);B0=a(jw,"forward"),jw.forEach(i),D0=a(T,` molteplici
volte, `),Ts=l(T,"EM",{});var Mw=n(Ts);S0=a(Mw,"per esempio"),Mw.forEach(i),j0=a(T," per generare testo, come "),ks=l(T,"CODE",{});var Rw=n(ks);M0=a(Rw,"autoregressive_sample"),Rw.forEach(i),R0=a(T,", "),ys=l(T,"CODE",{});var Uw=n(ys);U0=a(Uw,"generate"),Uw.forEach(i),F0=a(T,"."),T.forEach(i),Q0=p(Te),qs=l(Te,"LI",{});var Fw=n(qs);G0=a(Fw,`Cercate di separare la tokenization dal forward pass del modello. Se la repo originaria mostra esempio dove potete dare
come input una stringa, provate a cercare dove nella forward call la stringa viene cambiata in input ids e cominciate il
debug da questo punto. Questo vi garantisce un ottimo punto di partenza per scrivere un piccolo script personale dove dare
gli input al modello, anziche delle stringhe in input.`),Fw.forEach(i),H0=p(Te),de=l(Te,"LI",{});var ii=n(de);x0=a(ii,"Assicuratevi che il debugging "),Ls=l(ii,"STRONG",{});var Qw=n(Ls);W0=a(Qw,"non"),Qw.forEach(i),J0=a(ii,` sia in training mode. Spesso questo potra il modello a dare degli output random, per
via dei molteplici dropout layers. Assicuratevi che il forward pass nell\u2019ambiente di debug sia `),Is=l(ii,"STRONG",{});var Gw=n(Is);K0=a(Gw,"deterministico"),Gw.forEach(i),V0=a(ii,`, cosicche
i dropout non siano usati. Alternativamente, potete usare `),Cs=l(ii,"EM",{});var Hw=n(Cs);X0=a(Hw,"transformers.utils.set_seed"),Hw.forEach(i),Z0=a(ii,` se la vecchia e nuova implementazione
sono nello stesso framework.`),ii.forEach(i),Te.forEach(i),Gp=p(e),yo=l(e,"P",{});var $f=n(yo);Y0=a($f,"La seguente sezione vi da ulteriori dettagli e accorgimenti su come potete fare tutto questo per "),Os=l($f,"EM",{});var xw=n(Os);e2=a(xw,"brand_new_bert"),xw.forEach(i),o2=a($f,"."),$f.forEach(i),Hp=p(e),We=l(e,"H3",{class:!0});var Tf=n(We);qo=l(Tf,"A",{id:!0,class:!0,href:!0});var Ww=n(qo);As=l(Ww,"SPAN",{});var Jw=n(As);f(lt.$$.fragment,Jw),Jw.forEach(i),Ww.forEach(i),i2=p(Tf),Ns=l(Tf,"SPAN",{});var Kw=n(Ns);t2=a(Kw,"5.-14. Trasferire BrandNewBert in \u{1F917} Transformers"),Kw.forEach(i),Tf.forEach(i),xp=p(e),$a=l(e,"P",{});var Vw=n($a);a2=a(Vw,"Allora cominciamo ad aggiungere un nuovo codice in \u{1F917} Transformers. Andate nel vostro fork clone di \u{1F917} Transformers:"),Vw.forEach(i),Wp=p(e),f(nt.$$.fragment,e),Jp=p(e),Lo=l(e,"P",{});var kf=n(Lo);r2=a(kf,`Nel caso speciale in cui stiate aggiungendo un modello, la cui architettura sia identica a una di un modello gi\xE0 esistente,
dovrete solo aggiugnere uno script di conversione, come descritto `),Ta=l(kf,"A",{href:!0});var Xw=n(Ta);l2=a(Xw,"qui"),Xw.forEach(i),n2=a(kf,`.
In questo caso, potete riutilizzare l\u2019intera architettura del modello gia esistente.`),kf.forEach(i),Kp=p(e),ka=l(e,"P",{});var Zw=n(ka);s2=a(Zw,"Se questo non \xE9 il caso, cominciamo con il generare un nuovo modello. Avrete due opzioni:"),Zw.forEach(i),Vp=p(e),Io=l(e,"UL",{});var yf=n(Io);ya=l(yf,"LI",{});var k5=n(ya);Bs=l(k5,"CODE",{});var Yw=n(Bs);c2=a(Yw,"transformers-cli add-new-model-like"),Yw.forEach(i),d2=a(k5," per aggiungere un nuovo modello come uno che gia esiste"),k5.forEach(i),p2=p(yf),qa=l(yf,"LI",{});var y5=n(qa);Ds=l(y5,"CODE",{});var e9=n(Ds);m2=a(e9,"transformers-cli add-new-model"),e9.forEach(i),u2=a(y5," per aggiungere un nuovo modello da un nostro template (questo assomigliera a BERT o Bart, in base al modello che selezionerete)"),y5.forEach(i),yf.forEach(i),Xp=p(e),fe=l(e,"P",{});var Pr=n(fe);f2=a(Pr,`In entrambi i casi, l\u2019output vi dar\xE0 un questionario da riempire con informazioni basi sul modello. Il secondo comando richiede di installare
un `),Ss=l(Pr,"CODE",{});var o9=n(Ss);v2=a(o9,"cookiecutter"),o9.forEach(i),g2=a(Pr," - maggiori informazioni "),st=l(Pr,"A",{href:!0,rel:!0});var i9=n(st);h2=a(i9,"qui"),i9.forEach(i),_2=a(Pr,"."),Pr.forEach(i),Zp=p(e),La=l(e,"P",{});var t9=n(La);js=l(t9,"STRONG",{});var a9=n(js);b2=a(a9,"Aprire una Pull Request in main huggingface/transformers repo"),a9.forEach(i),t9.forEach(i),Yp=p(e),ve=l(e,"P",{});var $r=n(ve);E2=a($r,`Prime di cominciare ad adattare il codice automaticamente generato, aprite una nuova PR come \u201CWork in progress (WIP)\u201C,
`),Ms=l($r,"EM",{});var r9=n(Ms);z2=a(r9,"per esempio"),r9.forEach(i),w2=a($r," \u201D[WIP] Aggiungere "),Rs=l($r,"EM",{});var l9=n(Rs);P2=a(l9,"brand_new_bert"),l9.forEach(i),$2=a($r,`\u201D, cosicch\xE9 il team di Hugging Face possa lavorare al vostro fianco nell\u2019
integrare il modello in \u{1F917} Transformers.`),$r.forEach(i),em=p(e),Ia=l(e,"P",{});var n9=n(Ia);T2=a(n9,"Questi sarebbero gli step generali da seguire:"),n9.forEach(i),om=p(e),Ca=l(e,"OL",{});var s9=n(Ca);Us=l(s9,"LI",{});var c9=n(Us);k2=a(c9,"Creare un branch dal main branch con un nome descrittivo"),c9.forEach(i),s9.forEach(i),im=p(e),f(ct.$$.fragment,e),tm=p(e),dt=l(e,"OL",{start:!0});var d9=n(dt);Fs=l(d9,"LI",{});var p9=n(Fs);y2=a(p9,"Commit del codice automaticamente generato"),p9.forEach(i),d9.forEach(i),am=p(e),f(pt.$$.fragment,e),rm=p(e),mt=l(e,"OL",{start:!0});var m9=n(mt);Qs=l(m9,"LI",{});var u9=n(Qs);q2=a(u9,"Fare fetch e rebase del main esistente"),u9.forEach(i),m9.forEach(i),lm=p(e),f(ut.$$.fragment,e),nm=p(e),ft=l(e,"OL",{start:!0});var f9=n(ft);Gs=l(f9,"LI",{});var v9=n(Gs);L2=a(v9,"Push dei cambiamenti al proprio account:"),v9.forEach(i),f9.forEach(i),sm=p(e),f(vt.$$.fragment,e),cm=p(e),Je=l(e,"OL",{start:!0});var qf=n(Je);Hs=l(qf,"LI",{});var g9=n(Hs);xs=l(g9,"P",{});var h9=n(xs);I2=a(h9,`Una volte che siete soddisfatti dei nuovi cambiamenti, andate sulla webpage del vostro fork su GitHub. Cliccate \u201CPull request\u201D.
Assiuratevi di aggiungere alcuni membri di Hugging Face come reviewers, nel riguardo alla destra della pagina della PR, cosicche il team
Hugging Face verr\xE0 notificato anche per i futuri cambiamenti.`),h9.forEach(i),g9.forEach(i),C2=p(qf),Ws=l(qf,"LI",{});var _9=n(Ws);Js=l(_9,"P",{});var b9=n(Js);O2=a(b9,"Cambiare la PR a draft, cliccando su \u201CConvert to draft\u201D alla destra della pagina della PR"),b9.forEach(i),_9.forEach(i),qf.forEach(i),dm=p(e),Oa=l(e,"P",{});var E9=n(Oa);A2=a(E9,`Da quel punto in poi, ricordate di fare commit di ogni progresso e cambiamento, cosicche venga mostrato nella PR. Inoltre,
ricordatevi di tenere aggiornato il vostro lavoro con il main esistente:`),E9.forEach(i),pm=p(e),f(gt.$$.fragment,e),mm=p(e),Aa=l(e,"P",{});var z9=n(Aa);N2=a(z9,`In generale, tutte le domande che avrete riguardo al modello o l\u2019implementazione dovranno essere fatte nella vostra PR
e discusse/risolte nella PR stessa. In questa maniera, il team di Hugging Face sar\xE0 sempre notificato quando farete commit
di un nuovo codice o se avrete qualche domanda. \xC9 molto utile indicare al team di Hugging Face il codice a cui fate riferimento
nella domanda, cosicche il team potra facilmente capire il problema o la domanda.`),z9.forEach(i),um=p(e),Na=l(e,"P",{});var w9=n(Na);B2=a(w9,`Per fare questo andate sulla tab \u201CFiles changed\u201D, dove potrete vedere tutti i vostri cambiamenti al codice, andate sulla linea
dove volete chiedere una domanda, e cliccate sul simbolo \u201D+\u201D per aggiungere un commento. Ogni volta che una domanda o problema
\xE9 stato risolto, cliccate sul bottone \u201CResolve\u201D.`),w9.forEach(i),fm=p(e),Ba=l(e,"P",{});var P9=n(Ba);D2=a(P9,`In questa stessa maniera, Hugging Face aprir\xE0 domande o commenti nel rivedere il vostro codice. Mi raccomando, chiedete pi\xF9
domande possibili nella pagina della vostra PR. Se avete domande molto generali, non molto utili per il pubblico, siete liberi
di chiedere al team Hugging Face direttamente su slack o email.`),P9.forEach(i),vm=p(e),Da=l(e,"P",{});var $9=n(Da);Ks=l($9,"STRONG",{});var T9=n(Ks);S2=a(T9,"5. Adattare i codici per brand_new_bert"),T9.forEach(i),$9.forEach(i),gm=p(e),H=l(e,"P",{});var ti=n(H);j2=a(ti,"Per prima cosa, ci focalizzeremo sul modello e non sui tokenizer. Tutto il codice relative dovrebbe trovarsi in"),M2=l(ti,"BR",{}),R2=p(ti),Vs=l(ti,"CODE",{});var k9=n(Vs);U2=a(k9,"src/transformers/models/brand_new_bert/modeling_brand_new_bert.py"),k9.forEach(i),F2=a(ti,` e
`),Xs=l(ti,"CODE",{});var y9=n(Xs);Q2=a(y9,"src/transformers/models/brand_new_bert/configuration_brand_new_bert.py"),y9.forEach(i),G2=a(ti,"."),ti.forEach(i),hm=p(e),x=l(e,"P",{});var ai=n(x);H2=a(ai,`Ora potete finalmente cominciare il codice :). Il codice generato in
`),Zs=l(ai,"CODE",{});var q9=n(Zs);x2=a(q9,"src/transformers/models/brand_new_bert/modeling_brand_new_bert.py"),q9.forEach(i),W2=a(ai,` avr\xE0 sia la stessa architettura di BERT se \xE9 un
modello encoder-only o BART se \xE9 encoder-decoder. A questo punto, ricordatevi cio che avete imparato all\u2019inizio, riguardo
agli aspetti teorici del modello: `),Ys=l(ai,"EM",{});var L9=n(Ys);J2=a(L9,"In che maniera il modello che sto implmementando \xE9 diverso da BERT o BART?"),L9.forEach(i),K2=a(ai,`. Implementare
questi cambi  spesso vuol dire cambiare il layer `),ec=l(ai,"EM",{});var I9=n(ec);V2=a(I9,"self-attention"),I9.forEach(i),X2=a(ai,`, l\u2019ordine dei layer di normalizzazione e cos\xEC via\u2026
Ancora una volta ripetiamo, \xE9 molto utile vedere architetture simili di modelli gia esistenti in Transformers per avere
un\u2019idea migliore su come implementare il modello.`),ai.forEach(i),_m=p(e),Ke=l(e,"P",{});var wd=n(Ke);oc=l(wd,"STRONG",{});var C9=n(oc);Z2=a(C9,"Notate"),C9.forEach(i),Y2=a(wd,` che a questo punto non dovete avere subito un codice tutto corretto o pulito. Piuttosto, \xE9 consigliato cominciare con un
codice poco pulito, con copia-incolla del codice originale in `),ic=l(wd,"CODE",{});var O9=n(ic);eE=a(O9,"src/transformers/models/brand_new_bert/modeling_brand_new_bert.py"),O9.forEach(i),oE=a(wd,`
fino a che non avrete tutto il codice necessario. In base alla nostra esperienza, \xE9 molto meglio aggiungere una prima bozza
del codice richiesto e poi correggere e migliorare iterativamente. L\u2019unica cosa essenziale che deve funzionare qui \xE9 la seguente
instanza:`),wd.forEach(i),bm=p(e),f(ht.$$.fragment,e),Em=p(e),ge=l(e,"P",{});var Tr=n(ge);iE=a(Tr,"Questo comando creer\xE0 un modello con i parametri di default definiti in "),tc=l(Tr,"CODE",{});var A9=n(tc);tE=a(A9,"BrandNewBergConfig()"),A9.forEach(i),aE=a(Tr,` e weights random. Questo garantisce
che `),ac=l(Tr,"CODE",{});var N9=n(ac);rE=a(N9,"init()"),N9.forEach(i),lE=a(Tr," di tutte le componenti funzioni correttamente."),Tr.forEach(i),zm=p(e),Sa=l(e,"P",{});var B9=n(Sa);rc=l(B9,"STRONG",{});var D9=n(rc);nE=a(D9,"6. Scrivere uno script di conversione"),D9.forEach(i),B9.forEach(i),wm=p(e),he=l(e,"P",{});var kr=n(he);sE=a(kr,"Il prossimo step \xE9 scrivere uno script per convertire il checkpoint che avete usato per fare debug su "),lc=l(kr,"EM",{});var S9=n(lc);cE=a(S9,"brand_new_berts"),S9.forEach(i),dE=a(kr,` nella
repo originale in un checkpoint per la nuova implementazione di `),nc=l(kr,"EM",{});var j9=n(nc);pE=a(j9,"brand_new_bert"),j9.forEach(i),mE=a(kr,` in \u{1F917} Transformers. Non \xE9 consigliato scrivere
lo script di conversione da zero, ma piuttosto cercate e guardate script gia esistenti in \u{1F917} Transformers, cos\xEC da trovarne
uno simile al vostro modello. Di solito basta fare una copia di uno script gia esistente e adattarlo al vostro caso.
Non esistate a chiedre al team di Hugging Face a riguardo.`),kr.forEach(i),Pm=p(e),Co=l(e,"UL",{});var Lf=n(Co);ja=l(Lf,"LI",{});var q5=n(ja);uE=a(q5,"Se state convertendo un modello da TensorFlow a PyTorch, un ottimo inizio \xE9 vedere "),_t=l(q5,"A",{href:!0,rel:!0});var M9=n(_t);fE=a(M9,"questo script di conversione per BERT"),M9.forEach(i),q5.forEach(i),vE=p(Lf),Ma=l(Lf,"LI",{});var L5=n(Ma);gE=a(L5,"Se state convertendo un modello da PyTorch a PyTorch, "),bt=l(L5,"A",{href:!0,rel:!0});var R9=n(bt);hE=a(R9,"lo script di conversione di BART pu\xF2 esservi utile"),R9.forEach(i),L5.forEach(i),Lf.forEach(i),$m=p(e),Oo=l(e,"P",{});var If=n(Oo);_E=a(If,`Qui di seguito spiegheremo come i modelli PyTorch salvano i weights per ogni layer e come i nomi dei layer sono definiti. In PyTorch,
il nomde del layer \xE9 definito dal nome della class attribute che date al layer. Definiamo un modello dummy in PyTorch,
chiamato `),sc=l(If,"CODE",{});var U9=n(sc);bE=a(U9,"SimpleModel"),U9.forEach(i),EE=a(If,":"),If.forEach(i),Tm=p(e),f(Et.$$.fragment,e),km=p(e),W=l(e,"P",{});var ri=n(W);zE=a(ri,"Ora possiamo creare un\u2019instanza di questa definizione di modo da inizializzare a random weights: "),cc=l(ri,"CODE",{});var F9=n(cc);wE=a(F9,"dense"),F9.forEach(i),PE=a(ri,", "),dc=l(ri,"CODE",{});var Q9=n(dc);$E=a(Q9,"intermediate"),Q9.forEach(i),TE=a(ri,", "),pc=l(ri,"CODE",{});var G9=n(pc);kE=a(G9,"layer_norm"),G9.forEach(i),yE=a(ri,`.
Possiamo usare print per vedere l\u2019architettura del modello:`),ri.forEach(i),ym=p(e),f(zt.$$.fragment,e),qm=p(e),Ra=l(e,"P",{});var H9=n(Ra);qE=a(H9,"Da cui si ottiene:"),H9.forEach(i),Lm=p(e),f(wt.$$.fragment,e),Im=p(e),Ua=l(e,"P",{});var x9=n(Ua);LE=a(x9,`Si pu\xF2 vedere come i nomi dei layers siano definiti dal nome della class attribute in PyTorch. I valori dei weights di uno
specifico layer possono essere visualizzati:`),x9.forEach(i),Cm=p(e),f(Pt.$$.fragment,e),Om=p(e),Fa=l(e,"P",{});var W9=n(Fa);IE=a(W9,"ad esempio:"),W9.forEach(i),Am=p(e),f($t.$$.fragment,e),Nm=p(e),Tt=l(e,"P",{});var I5=n(Tt);CE=a(I5,`Nello script di conversione, dovreste riempire quei valori di inizializzazione random con gli stessi weights del corrispondente
layer nel checkpoint. `),mc=l(I5,"EM",{});var J9=n(mc);OE=a(J9,"Per esempio"),J9.forEach(i),I5.forEach(i),Bm=p(e),f(kt.$$.fragment,e),Dm=p(e),J=l(e,"P",{});var li=n(J);AE=a(li,`Cos\xEC facendo, dovete verificare che ogni inizializzazione random di un peso del modello PyTorch e il suo corrispondente peso nel pretrained checkpoint
siano esattamente gli stessi e uguali in `),uc=l(li,"STRONG",{});var K9=n(uc);NE=a(K9,"dimensione/shape e nome"),K9.forEach(i),BE=a(li,". Per fare questo, \xE9 "),fc=l(li,"STRONG",{});var V9=n(fc);DE=a(V9,"necessario"),V9.forEach(i),SE=a(li," aggiungere un "),vc=l(li,"CODE",{});var X9=n(vc);jE=a(X9,"assert"),X9.forEach(i),ME=a(li,`
per la dimensione/shape e nome:`),li.forEach(i),Sm=p(e),f(yt.$$.fragment,e),jm=p(e),Qa=l(e,"P",{});var Z9=n(Qa);RE=a(Z9,"Inoltre, dovrete fare il print sia dei nomi che dei weights per essere sicuri che siano gli stessi:"),Z9.forEach(i),Mm=p(e),f(qt.$$.fragment,e),Rm=p(e),Ga=l(e,"P",{});var Y9=n(Ga);UE=a(Y9,`Se la dimensione o il nome non sono uguali, probabilmente avete sbagliato ad assegnare il peso nel checkpoint o nel layer costrutture di
\u{1F917} Transformers.`),Y9.forEach(i),Um=p(e),Ao=l(e,"P",{});var Cf=n(Ao);FE=a(Cf,"Una dimensione sbagliata pu\xF2 essere dovuta ad un errore nei parameteri in "),gc=l(Cf,"CODE",{});var e8=n(gc);QE=a(e8,"BrandNewBertConfig()"),e8.forEach(i),GE=a(Cf,`. Tuttavia, pu\xF2 essere anche
che l\u2019implementazione del layer in PyTorch richieda di fare una transposizione della matrice dei weights.`),Cf.forEach(i),Fm=p(e),K=l(e,"P",{});var ni=n(K);HE=a(ni,"Infine, controllate "),hc=l(ni,"STRONG",{});var o8=n(hc);xE=a(o8,"tutti"),o8.forEach(i),WE=a(ni,` che tutti i weights inizializzati e fate print di tutti i weights del checkpoint che non sono stati
usati per l\u2019inizializzazione, di modo da essere sicuri che il modello sia correttamente convertito. \xC9 normale che ci siano
errori nel test di conversione, fai per un errore in `),_c=l(ni,"CODE",{});var i8=n(_c);JE=a(i8,"BrandNewBertConfig()"),i8.forEach(i),KE=a(ni,`, o un errore nell\u2019architettura in \u{1F917} Transformers,
o un bug in `),bc=l(ni,"CODE",{});var t8=n(bc);VE=a(t8,"init()"),t8.forEach(i),XE=a(ni,"."),ni.forEach(i),Qm=p(e),V=l(e,"P",{});var si=n(V);ZE=a(si,`Questo step dev\u2019essere fatto tramite iterazioni fino a che non si raggiungano gli stessi valori per i weights. Una volta che
il checkpoint \xE9 stato correttamente caricato in \u{1F917} Transformers, potete salvare il modello in una cartella di vostra scelta
`),Ec=l(si,"CODE",{});var a8=n(Ec);YE=a(a8,"/path/to/converted/checkpoint/folder"),a8.forEach(i),e3=a(si,` che contenga sia
`),zc=l(si,"CODE",{});var r8=n(zc);o3=a(r8,"pytorch_model.bin"),r8.forEach(i),i3=a(si," che "),wc=l(si,"CODE",{});var l8=n(wc);t3=a(l8,"config.json"),l8.forEach(i),a3=a(si,":"),si.forEach(i),Gm=p(e),f(Lt.$$.fragment,e),Hm=p(e),Ha=l(e,"P",{});var n8=n(Ha);Pc=l(n8,"STRONG",{});var s8=n(Pc);r3=a(s8,"7. Implementare il forward pass"),s8.forEach(i),n8.forEach(i),xm=p(e),No=l(e,"P",{});var Of=n(No);l3=a(Of,`Una volta che i weights pretrained sono stati correttamente caricati in \u{1F917} Transformers, dovrete assicurarvi che il forward pass
sia correttamente implementato. `),xa=l(Of,"A",{href:!0});var c8=n(xa);n3=a(c8,"Qui"),c8.forEach(i),s3=a(Of,`, avete give creato e provato
uno script che testi il forward pass del modello usando la repo originaria. Ora dovrete fare lo stesso con uno script analogo
usando l\u2019implementazione in \u{1F917} Transformers anzich\xE9 l\u2019originale. Piu o meno lo script dovrebbe essere:`),Of.forEach(i),Wm=p(e),f(It.$$.fragment,e),Jm=p(e),X=l(e,"P",{});var ci=n(X);c3=a(ci,`Di solito l\u2019output da \u{1F917} Transformers non \xE9 uguale uguale all\u2019output originario, sopratto la prima volta. Non vi abbattete -
\xE9 normale! Prima di tutto assicuratevi che non ci siano errori o che non vengano segnalati degli errori nella forward pass.
Spesso capita che ci siano dimensioni sbagliate o data type sbagliati, `),$c=l(ci,"EM",{});var d8=n($c);d3=a(d8,"ad esempio"),d8.forEach(i),p3=p(ci),Tc=l(ci,"CODE",{});var p8=n(Tc);m3=a(p8,"torch.long"),p8.forEach(i),u3=a(ci," anziche "),kc=l(ci,"CODE",{});var m8=n(kc);f3=a(m8,"torch.float32"),m8.forEach(i),v3=a(ci,`.
Non esistate a chiedere al team Hugging Face!`),ci.forEach(i),Km=p(e),_e=l(e,"P",{});var yr=n(_e);g3=a(yr,`Nella parte finale assicuratevi che l\u2019implementazione \u{1F917} Transformers funzioni correttamente cosi da testare che gli output
siano equivalenti a una precisione di `),yc=l(yr,"CODE",{});var u8=n(yc);h3=a(u8,"1e-3"),u8.forEach(i),_3=a(yr,". Controllate che "),qc=l(yr,"CODE",{});var f8=n(qc);b3=a(f8,"outputs.shape"),f8.forEach(i),E3=a(yr,` siano le stesse tra \u{1F917} Transformers e l\u2019implementazione
originaria. Poi, controllate che i valori in output siano identici. Questa \xE9 sicuramente la parte pi\xF9 difficile, qui una serie
di errori comuni quando gli output non sono uguali:`),yr.forEach(i),Vm=p(e),Z=l(e,"UL",{});var di=n(Z);Ve=l(di,"LI",{});var qr=n(Ve);z3=a(qr,"Alcuni layers non sono stati aggiunti, "),Lc=l(qr,"EM",{});var v8=n(Lc);w3=a(v8,"ad esempio"),v8.forEach(i),P3=a(qr," un "),Ic=l(qr,"EM",{});var g8=n(Ic);$3=a(g8,"activation"),g8.forEach(i),T3=a(qr," layer non \xE9 stato aggiunto, o ci si \xE9 scordati di una connessione"),qr.forEach(i),k3=p(di),Cc=l(di,"LI",{});var h8=n(Cc);y3=a(h8,"La matrice del word embedding non \xE9 stata ripareggiata"),h8.forEach(i),q3=p(di),Oc=l(di,"LI",{});var _8=n(Oc);L3=a(_8,"Ci sono degli embeddings posizionali sbagliati perch\xE9 l\u2019implementazione originaria ha un offset"),_8.forEach(i),I3=p(di),Y=l(di,"LI",{});var Ze=n(Y);C3=a(Ze,"Il dropout \xE9 in azione durante il forward pass. Per sistemare questo errore controllate che "),Ac=l(Ze,"EM",{});var b8=n(Ac);O3=a(b8,"model.training = False"),b8.forEach(i),A3=a(Ze,` e che
il dropout non sia stato attivato nel forward pass, `),Nc=l(Ze,"EM",{});var E8=n(Nc);N3=a(E8,"per esempio"),E8.forEach(i),B3=a(Ze," passate "),Bc=l(Ze,"EM",{});var z8=n(Bc);D3=a(z8,"self.training"),z8.forEach(i),S3=a(Ze," a "),Ct=l(Ze,"A",{href:!0,rel:!0});var w8=n(Ct);j3=a(w8,"PyTorch\u2019s functional dropout"),w8.forEach(i),Ze.forEach(i),di.forEach(i),Xm=p(e),Bo=l(e,"P",{});var Af=n(Bo);M3=a(Af,`La miglior maniera per sistemare il problema \xE9 di vedere all\u2019implementazione originaria del forward pass e in \u{1F917} Transformers
fianco a fianco e vedere se ci sono delle differenze. In teoria, con debug e print degli output intermedie di entrambe le
implementazioni nel forward pass nell\u2019esatta posizione del network dovrebbe aiutarvi a vedere dove ci sono differenze tra
i due frameworks. Come prima mossa controllate che `),Dc=l(Af,"CODE",{});var P8=n(Dc);R3=a(P8,"input_ids"),P8.forEach(i),U3=a(Af,` siano identici in entrambi gli scripts. Da l\xEC andate fino
all\u2019ultimo layer. Potrete notare una differenza tra le due implementazioni a quel punto.`),Af.forEach(i),Zm=p(e),Do=l(e,"P",{});var Nf=n(Do);F3=a(Nf,"Una volta che lo stesso output \xE9 stato ragguingi, verificate gli output con "),Sc=l(Nf,"CODE",{});var $8=n(Sc);Q3=a($8,"torch.allclose(original_output, output, atol=1e-3)"),$8.forEach(i),G3=a(Nf,`.
A questo punto se \xE9 tutto a posto: complimenti! Le parti seguenti saranno una passeggiata \u{1F60A}.`),Nf.forEach(i),Ym=p(e),Wa=l(e,"P",{});var T8=n(Wa);jc=l(T8,"STRONG",{});var k8=n(jc);H3=a(k8,"8. Aggiungere i test necessari per il modello"),k8.forEach(i),T8.forEach(i),eu=p(e),So=l(e,"P",{});var Bf=n(So);x3=a(Bf,`A questo punto avete aggiunto con successo il vostro nuovo modello. Tuttavia, \xE9 molto probabile che il modello non sia
del tutto ok con il design richiesto. Per essere sicuri che l\u2019implementazione sia consona e compatibile con \u{1F917} Transformers \xE9
necessario implementare dei tests. Il Cookiecutter dovrebbe fornire automaticamente dei file per test per il vostro modello,
di solito nella folder `),Mc=l(Bf,"CODE",{});var y8=n(Mc);W3=a(y8,"tests/test_modeling_brand_new_bert.py"),y8.forEach(i),J3=a(Bf,". Provate questo per verificare l\u2019ok nei test piu comuni:"),Bf.forEach(i),ou=p(e),f(Ot.$$.fragment,e),iu=p(e),Ja=l(e,"P",{});var q8=n(Ja);K3=a(q8,"Una volta sistemati i test comuni, bisogna assicurarsi che il vostro lavoro sia correttamente testato cosicch\xE8:"),q8.forEach(i),tu=p(e),jo=l(e,"UL",{});var Df=n(jo);At=l(Df,"LI",{});var Sf=n(At);V3=a(Sf,"a) La community puo capire in maniera semplice il vostro lavoro controllando tests specifici del modello "),Rc=l(Sf,"EM",{});var L8=n(Rc);X3=a(L8,"brand_new_bert"),L8.forEach(i),Z3=a(Sf,","),Sf.forEach(i),Y3=p(Df),Uc=l(Df,"LI",{});var I8=n(Uc);e4=a(I8,"b) Implementazioni future del vostro modello non rompano alcune feature importante del modello."),I8.forEach(i),Df.forEach(i),au=p(e),Mo=l(e,"P",{});var jf=n(Mo);o4=a(jf,`Per prima cosa agguingete dei test d\u2019integrazione. Questi sono essenziali perche fanno la stessa funzione degli scripts di
debug usati precedentemente. Un template per questi tests esiste gia nel Cookiecutter ed \xE9 sotto il nome di `),Fc=l(jf,"CODE",{});var C8=n(Fc);i4=a(C8,"BrandNewBertModelIntegrationTests"),C8.forEach(i),t4=a(jf,`,
voi dovrete solo completarlo. Una volta che questi tests sono OK, provate:`),jf.forEach(i),ru=p(e),f(Nt.$$.fragment,e),lu=p(e),f(Ro.$$.fragment,e),nu=p(e),ee=l(e,"P",{});var pi=n(ee);a4=a(pi,"Di seguito, tutte le features che sono utili e necessarire per "),Qc=l(pi,"EM",{});var O8=n(Qc);r4=a(O8,"brand_new_bert"),O8.forEach(i),l4=a(pi,` devono essere testate in test separati,
contenuti in `),Gc=l(pi,"CODE",{});var A8=n(Gc);n4=a(A8,"BrandNewBertModelTester"),A8.forEach(i),s4=a(pi,"/ "),Hc=l(pi,"CODE",{});var N8=n(Hc);c4=a(N8,"BrandNewBertModelTest"),N8.forEach(i),d4=a(pi,". spesso la gente si scorda questi test, ma ricordate che sono utili per:"),pi.forEach(i),su=p(e),Uo=l(e,"UL",{});var Mf=n(Uo);xc=l(Mf,"LI",{});var B8=n(xc);p4=a(B8,"Aiuta gli utenti a capire il vostro codice meglio, richiamando l\u2019attenzione su queste nuove features"),B8.forEach(i),m4=p(Mf),Wc=l(Mf,"LI",{});var D8=n(Wc);u4=a(D8,"Developers e contributors futuri potranno velocemente testare nuove implementazioni del modello testanto questi casi speciali."),D8.forEach(i),Mf.forEach(i),cu=p(e),Ka=l(e,"P",{});var S8=n(Ka);Jc=l(S8,"STRONG",{});var j8=n(Jc);f4=a(j8,"9. Implementare il tokenizer"),j8.forEach(i),S8.forEach(i),du=p(e),Fo=l(e,"P",{});var Rf=n(Fo);v4=a(Rf,"A questo punto avremo bisogno un tokenizer per "),Kc=l(Rf,"EM",{});var M8=n(Kc);g4=a(M8,"brand_new_bert"),M8.forEach(i),h4=a(Rf,". Di solito il tokenizer \xE9 uguale ad altri modelli in \u{1F917} Transformers."),Rf.forEach(i),pu=p(e),Va=l(e,"P",{});var R8=n(Va);_4=a(R8,"\xC9 importante che troviate il file con il tokenizer originale e che lo carichiate in \u{1F917} Transformers."),R8.forEach(i),mu=p(e),Qo=l(e,"P",{});var Uf=n(Qo);b4=a(Uf,`Per controllare che il tokenizer funzioni in modo corretto, create uno script nella repo originaria che riceva come input
una stringa e ritorni gli `),Vc=l(Uf,"CODE",{});var U8=n(Vc);E4=a(U8,"input_ids"),U8.forEach(i),z4=a(Uf,". Piu o meno questo potrebbe essere il codice:"),Uf.forEach(i),uu=p(e),f(Bt.$$.fragment,e),fu=p(e),Go=l(e,"P",{});var Ff=n(Go);w4=a(Ff,`Potrebbe richiedere un po\u2019 di tempo, ma guardate ancora alla repo originaria per trovare la funzione corretta del tokenizer.
A volte capita di dover riscrivere il tokenizer nella repo originaria, di modo da avere come output gli `),Xc=l(Ff,"CODE",{});var F8=n(Xc);P4=a(F8,"input_ids"),F8.forEach(i),$4=a(Ff,`.
A quel punto uno script analogo \xE9 necessario in \u{1F917} Transformers:`),Ff.forEach(i),vu=p(e),f(Dt.$$.fragment,e),gu=p(e),Ho=l(e,"P",{});var Qf=n(Ho);T4=a(Qf,"Una volta che "),Zc=l(Qf,"CODE",{});var Q8=n(Zc);k4=a(Q8,"input_ids"),Q8.forEach(i),y4=a(Qf," sono uguali, bisogna aggiungere un test per il tokenizer."),Qf.forEach(i),hu=p(e),xo=l(e,"P",{});var Gf=n(xo);q4=a(Gf,"Il file test per tokenizer di "),Yc=l(Gf,"EM",{});var G8=n(Yc);L4=a(G8,"brand_new_brand"),G8.forEach(i),I4=a(Gf," dovrebbe avere un paio di hard-coded test d\u2019integrazione."),Gf.forEach(i),_u=p(e),Xa=l(e,"P",{});var H8=n(Xa);ed=l(H8,"STRONG",{});var x8=n(ed);C4=a(x8,"10. Test end-to-end"),x8.forEach(i),H8.forEach(i),bu=p(e),oe=l(e,"P",{});var mi=n(oe);O4=a(mi,"Ora che avete il tokenizer, dovrete aggiungere dei test d\u2019integrazione per l\u2019intero workflow in "),od=l(mi,"CODE",{});var W8=n(od);A4=a(W8,"tests/test_modeling_brand_new_bert.py"),W8.forEach(i),N4=a(mi,` in \u{1F917} Transformer.
Questi test devono mostrare che un significante campione text-to-text funzioni come ci si aspetta nell\u2019implementazione di  \u{1F917} Transformers.
`),id=l(mi,"EM",{});var J8=n(id);B4=a(J8,"Per esempio"),J8.forEach(i),D4=a(mi,` potreste usare dei source-to-target-translation, o un sommario di un articolo, o un domanda-risposta e cosi via.
Se nessuno dei checkpoints \xE9 stato ultra parametrizzato per task simili, allora i tests per il modello sono piu che sufficienti.
Nello step finale dovete assicurarvi che il modello sia totalmente funzionale, e consigliamo anche di provare a testare su GPU.
Puo succedere che ci si scordi un `),td=l(mi,"CODE",{});var K8=n(td);S4=a(K8,".to(self.device)"),K8.forEach(i),j4=a(mi,` ad esempio. Se non avete accesso a GPU, il team Hugging Face puo provvedere
a testare questo aspetto per voi.`),mi.forEach(i),Eu=p(e),Za=l(e,"P",{});var V8=n(Za);ad=l(V8,"STRONG",{});var X8=n(ad);M4=a(X8,"11. Aggiungere una Docstring"),X8.forEach(i),V8.forEach(i),zu=p(e),be=l(e,"P",{});var Lr=n(be);R4=a(Lr,`Siete quasi alla fine! L\u2019ultima cosa rimasta \xE9 avere una bella docstring e una pagina doc. Il Cookiecutter dovrebbe provvedere gi\xE0
un template chiamato `),rd=l(Lr,"CODE",{});var Z8=n(rd);U4=a(Z8,"docs/source/model_doc/brand_new_bert.rst"),Z8.forEach(i),F4=a(Lr,`, che dovrete compilare. La prima cosa che un utente far\xE0
per usare il vostro modello sar\xE0 dare una bella lettura al doc. Quindi proponete una documentazione chiara e concisa. \xC9 molto
utile per la community avere anche delle `),ld=l(Lr,"EM",{});var Y8=n(ld);Q4=a(Y8,"Tips"),Y8.forEach(i),G4=a(Lr,` per mostrare come il modello puo\u2019 essere usato. Non esitate a chiedere a Hugging Face
riguardo alle docstirng.`),Lr.forEach(i),wu=p(e),Wo=l(e,"P",{});var Hf=n(Wo);H4=a(Hf,"Quindi, assicuratevi che la docstring sia stata aggiunta a "),nd=l(Hf,"CODE",{});var eP=n(nd);x4=a(eP,"src/transformers/models/brand_new_bert/modeling_brand_new_bert.py"),eP.forEach(i),W4=a(Hf,`.
Assicuratevi che la docstring sia corretta e che includa tutti i necessari input e output. Abbiamo una guida dettagliata per
scrivere la documentazione e docstring.`),Hf.forEach(i),Pu=p(e),Ya=l(e,"P",{});var oP=n(Ya);sd=l(oP,"STRONG",{});var iP=n(sd);J4=a(iP,"Rifattorizzare il codice"),iP.forEach(i),oP.forEach(i),$u=p(e),Jo=l(e,"P",{});var xf=n(Jo);K4=a(xf,"Perfetto! Ora che abbiamo tutto per "),cd=l(xf,"EM",{});var tP=n(cd);V4=a(tP,"brand_new_bert"),tP.forEach(i),X4=a(xf," controllate che lo stile del codice sia ok:"),xf.forEach(i),Tu=p(e),f(St.$$.fragment,e),ku=p(e),er=l(e,"P",{});var aP=n(er);Z4=a(aP,"E che il codice passi i quality check:"),aP.forEach(i),yu=p(e),f(jt.$$.fragment,e),qu=p(e),or=l(e,"P",{});var rP=n(or);Y4=a(rP,`A volte capita che manchino delle informazioninella docstring o alcuni nomi sbagliati, questo far\xE0 fallire i tests sopra.
Ripetiamo: chiedete pure a Hugging Face, saremo lieti di aiutarvi.`),rP.forEach(i),Lu=p(e),ir=l(e,"P",{});var lP=n(ir);e5=a(lP,"Per ultimo, fare del refactoring del codice una volta che \xE9 stato creato."),lP.forEach(i),Iu=p(e),tr=l(e,"P",{});var nP=n(tr);o5=a(nP,"Avete finito con il codice, congratulazioni! \u{1F389} Siete fantasticiiiiiii! \u{1F60E}"),nP.forEach(i),Cu=p(e),ar=l(e,"P",{});var sP=n(ar);dd=l(sP,"STRONG",{});var cP=n(dd);i5=a(cP,"12. Caricare il modello sul model hub"),cP.forEach(i),sP.forEach(i),Ou=p(e),S=l(e,"P",{});var ye=n(S);t5=a(ye,`In questa ultima parte dovrete convertire e caricare il modello, con tutti i checkpoints, nel model hub e aggiungere una
model card per ogni checkpoint caricato. Leggete la nostra guida `),rr=l(ye,"A",{href:!0});var dP=n(rr);a5=a(dP,"Model sharing and uploading Page"),dP.forEach(i),r5=a(ye,` per
avere familiarit\xE0 con l\u2019hub. Di solito in questa parte lavorate a fianco di Hugging face per decidere un nome che sia ok
per ogni checkpoint, per ottenere i permessi necessari per caricare il modello nell\u2019organizzazione dell\u2019autore di `),pd=l(ye,"EM",{});var pP=n(pd);l5=a(pP,"brand_new_bert"),pP.forEach(i),n5=a(ye,`.
Il metodo `),md=l(ye,"CODE",{});var mP=n(md);s5=a(mP,"push_to_hub"),mP.forEach(i),c5=a(ye,", presente in tutti i modelli "),ud=l(ye,"CODE",{});var uP=n(ud);d5=a(uP,"transformers"),uP.forEach(i),p5=a(ye,", \xE9 una maniera rapida e indolore per caricare il vostro checkpoint sull\u2019hub:"),ye.forEach(i),Au=p(e),f(Mt.$$.fragment,e),Nu=p(e),Ko=l(e,"P",{});var Wf=n(Ko);m5=a(Wf,`Vale la pena spendere un po\u2019 di tempo per creare una model card ad-hoc per ogni checkpoint. Le model cards dovrebbero
suggerire le caratteristiche specifiche del checkpoint, `),fd=l(Wf,"EM",{});var fP=n(fd);u5=a(fP,"per esempio"),fP.forEach(i),f5=a(Wf,` su che dataset il checkpoint \xE9 stato pretrained o fine-tuned.
O che su che genere di task il modello lavoro? E anche buona pratica includere del codice su come usare il modello correttamente.`),Wf.forEach(i),Bu=p(e),lr=l(e,"P",{});var vP=n(lr);vd=l(vP,"STRONG",{});var gP=n(vd);v5=a(gP,"13. (Opzionale) Aggiungere un notebook"),gP.forEach(i),vP.forEach(i),Du=p(e),Vo=l(e,"P",{});var Jf=n(Vo);g5=a(Jf,"\xC9 molto utile aggiungere un notebook, che dimostri in dettaglio come "),gd=l(Jf,"EM",{});var hP=n(gd);h5=a(hP,"brand_new_bert"),hP.forEach(i),_5=a(Jf,` si utilizzi per fare inferenza e/o
fine-tuned su specifiche task. Non \xE9 una cosa obbligatoria da avere nella vostra PR, ma \xE9 molto utile per la community.`),Jf.forEach(i),Su=p(e),nr=l(e,"P",{});var _P=n(nr);hd=l(_P,"STRONG",{});var bP=n(hd);b5=a(bP,"14. Sottomettere la PR"),bP.forEach(i),_P.forEach(i),ju=p(e),sr=l(e,"P",{});var EP=n(sr);E5=a(EP,`L\u2019ultimissimo step! Ovvero il merge della PR nel main. Di solito il team Hugging face a questo punto vi avr\xE0 gia aiutato,
ma \xE9 ok prendere un po\u2019 di tempo per pulire la descirzione e commenti nel codice.`),EP.forEach(i),Mu=p(e),Xe=l(e,"H3",{class:!0});var Kf=n(Xe);Xo=l(Kf,"A",{id:!0,class:!0,href:!0});var zP=n(Xo);_d=l(zP,"SPAN",{});var wP=n(_d);f(Rt.$$.fragment,wP),wP.forEach(i),zP.forEach(i),z5=p(Kf),bd=l(Kf,"SPAN",{});var PP=n(bd);w5=a(PP,"Condividete il vostro lavoro!!"),PP.forEach(i),Kf.forEach(i),Ru=p(e),cr=l(e,"P",{});var $P=n(cr);P5=a($P,`\xC9 ora tempo di prendere un po\u2019 di credito dalla communit\xE0 per il vostro lavoro! Caricare e implementare un nuovo modello
\xE9 un grandissimo contributo per Transformers e l\u2019intera community NLP. Il codice e la conversione dei modelli pre-trained sara
sicuramente utilizzato da centinaia o migliaia di sviluppatori e ricercatori. Siate fieri e orgogliosi di condividere il vostro
traguardo con l\u2019intera community :)`),$P.forEach(i),Uu=p(e),dr=l(e,"P",{});var TP=n(dr);Ed=l(TP,"STRONG",{});var kP=n(Ed);$5=a(kP,"Avete create un altro modello che \xE9 super facile da usare per tutti quanti nella community! \u{1F92F}"),kP.forEach(i),TP.forEach(i),this.h()},h(){m(k,"name","hf:doc:metadata"),m(k,"content",JSON.stringify(DP)),m(F,"id","come-aggiungere-un-modello-a-transformers"),m(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(F,"href","#come-aggiungere-un-modello-a-transformers"),m(C,"class","relative group"),m(vi,"href","https://github.com/huggingface/transformers/tree/main/templates/adding_a_new_model/open_model_proposals/README.md"),m(vi,"rel","nofollow"),m(gi,"href","https://github.com/huggingface/transformers/pulls?q=is%3Apr+label%3A%22PR+for+Model+Addition%22+is%3Aclosed"),m(gi,"rel","nofollow"),m(io,"id","panoramica-generale-su-transformers"),m(io,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(io,"href","#panoramica-generale-su-transformers"),m(Le,"class","relative group"),m(Ht,"href","filosofia"),m(ro,"id","panoramica-sui-modelli"),m(ro,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ro,"href","#panoramica-sui-modelli"),m(Ie,"class","relative group"),CP(Jt.src,C5="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_overview.png")||m(Jt,"src",C5),m(lo,"id","stile-per-il-codice"),m(lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(lo,"href","#stile-per-il-codice"),m(Ce,"class","relative group"),m(wi,"href","https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/roberta/modeling_roberta.py#L160"),m(wi,"rel","nofollow"),m(no,"id","panoramica-sui-tokenizers"),m(no,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(no,"href","#panoramica-sui-tokenizers"),m(Be,"class","relative group"),m(so,"id","aggiungere-un-modello-a-transformers-passo-dopo-passo"),m(so,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(so,"href","#aggiungere-un-modello-a-transformers-passo-dopo-passo"),m(De,"class","relative group"),m(ki,"href","https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28"),m(ki,"rel","nofollow"),m(yi,"href","https://huggingface.co/thomwolf"),m(yi,"rel","nofollow"),m(Li,"href","https://huggingface.co/blog/porting-fsmt"),m(Li,"rel","nofollow"),m(Ii,"href","https://huggingface.co/stas"),m(Ii,"rel","nofollow"),m(Ci,"href","https://www.gnu.org/software/grep/"),m(Ci,"rel","nofollow"),m(Oi,"href","https://github.com/BurntSushi/ripgrep"),m(Oi,"rel","nofollow"),m(oa,"start","2"),m(ia,"start","3"),m(ta,"start","4"),m(aa,"start","5"),m(ra,"start","6"),m(la,"start","7"),m(na,"start","8"),m(sa,"start","9"),m(ca,"start","10"),m(da,"start","11"),m(pa,"start","12"),m(ma,"start","13"),m(ua,"start","14"),m(po,"id","1-opzionale-aspetti-teorici-di-brandnewbert"),m(po,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(po,"href","#1-opzionale-aspetti-teorici-di-brandnewbert"),m(Se,"class","relative group"),m(fa,"href","model_summary"),m(Bi,"href","https://huggingface.co/transformers/#contents"),m(Bi,"rel","nofollow"),m(uo,"id","2-prepare-il-tuo-ambiente"),m(uo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(uo,"href","#2-prepare-il-tuo-ambiente"),m(Re,"class","relative group"),m(ji,"href","https://github.com/huggingface/transformers"),m(ji,"rel","nofollow"),m(Ui,"start","3"),m(Hi,"href","https://pytorch.org/get-started/locally/"),m(Hi,"rel","nofollow"),m(Gi,"start","4"),m(Wi,"start","5"),m(go,"id","34-provare-un-pretrained-checkpoint-usando-la-repo-originale"),m(go,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(go,"href","#34-provare-un-pretrained-checkpoint-usando-la-repo-originale"),m(Qe,"class","relative group"),m(Xi,"href","https://jupyter.org/"),m(Xi,"rel","nofollow"),m(Zi,"href","https://colab.research.google.com/notebooks/intro.ipynb"),m(Zi,"rel","nofollow"),m(et,"href","https://gist.github.com/LysandreJik/db4c948f6b4483960de5cbac598ad4ed"),m(et,"rel","nofollow"),m(ot,"href","https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow"),m(ot,"rel","nofollow"),m(at,"href","https://www.tensorflow.org/api_docs/python/tf/print"),m(at,"rel","nofollow"),m(rt,"href","https://github.com/google/jax/issues/196"),m(rt,"rel","nofollow"),m(qo,"id","514-trasferire-brandnewbert-in-transformers"),m(qo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(qo,"href","#514-trasferire-brandnewbert-in-transformers"),m(We,"class","relative group"),m(Ta,"href","#write-a-conversion-script"),m(st,"href","https://github.com/huggingface/transformers/tree/main/templates/adding_a_new_model"),m(st,"rel","nofollow"),m(dt,"start","2"),m(mt,"start","3"),m(ft,"start","4"),m(Je,"start","5"),m(_t,"href","https://github.com/huggingface/transformers/blob/7acfa95afb8194f8f9c1f4d2c6028224dbed35a2/src/transformers/models/bert/modeling_bert.py#L91"),m(_t,"rel","nofollow"),m(bt,"href","https://github.com/huggingface/transformers/blob/main/src/transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py"),m(bt,"rel","nofollow"),m(xa,"href","#provare-un-pretrained-checkpoint-usando-la-repo-originale"),m(Ct,"href","https://pytorch.org/docs/stable/nn.functional.html?highlight=dropout#torch.nn.functional.dropout"),m(Ct,"rel","nofollow"),m(rr,"href","model_sharing"),m(Xo,"id","condividete-il-vostro-lavoro"),m(Xo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Xo,"href","#condividete-il-vostro-lavoro"),m(Xe,"class","relative group")},m(e,s){o(document.head,k),c(e,Ye,s),c(e,C,s),o(C,F),o(F,qe),v(j,qe,null),o(C,ui),o(C,le),o(le,ne),c(e,fi,s),c(e,M,s),o(M,Vf),o(M,Ir),o(Ir,Xf),o(M,Zf),o(M,Cr),o(Cr,Yf),o(M,ev),c(e,$d,s),c(e,eo,s),o(eo,ov),o(eo,vi),o(vi,iv),o(eo,tv),c(e,Td,s),c(e,Ft,s),o(Ft,av),c(e,kd,s),c(e,Q,s),o(Q,Or),o(Or,rv),o(Q,lv),o(Q,Ar),o(Ar,nv),o(Q,sv),o(Q,Nr),o(Nr,cv),o(Q,dv),o(Q,se),o(se,pv),o(se,Br),o(Br,mv),o(se,uv),o(se,Dr),o(Dr,fv),o(se,vv),o(se,Sr),o(Sr,gv),o(se,hv),c(e,yd,s),c(e,oo,s),o(oo,_v),o(oo,gi),o(gi,bv),o(oo,Ev),c(e,qd,s),c(e,Qt,s),o(Qt,zv),c(e,Ld,s),c(e,Le,s),o(Le,io),o(io,jr),v(hi,jr,null),o(Le,wv),o(Le,Mr),o(Mr,Pv),c(e,Id,s),c(e,Gt,s),o(Gt,$v),c(e,Cd,s),c(e,to,s),o(to,Tv),o(to,Ht),o(Ht,kv),o(to,yv),c(e,Od,s),c(e,me,s),o(me,Rr),o(Rr,qv),o(me,Lv),o(me,Ur),o(Ur,Iv),o(me,Cv),o(me,_i),o(_i,Ov),o(_i,Fr),o(Fr,Av),o(_i,Nv),c(e,Ad,s),c(e,ao,s),o(ao,Bv),o(ao,Qr),o(Qr,Dv),o(ao,Sv),c(e,Nd,s),c(e,xt,s),o(xt,jv),c(e,Bd,s),c(e,Ie,s),o(Ie,ro),o(ro,Gr),v(bi,Gr,null),o(Ie,Mv),o(Ie,Hr),o(Hr,Rv),c(e,Dd,s),c(e,O,s),o(O,Uv),o(O,xr),o(xr,Fv),o(O,Qv),o(O,Wr),o(Wr,Gv),o(O,Hv),o(O,xv),o(O,Wv),o(O,Jr),o(Jr,Jv),o(O,Kv),c(e,Sd,s),c(e,Wt,s),o(Wt,Vv),c(e,jd,s),c(e,Jt,s),c(e,Md,s),c(e,b,s),o(b,Xv),o(b,Kr),o(Kr,Zv),o(b,Yv),o(b,Vr),o(Vr,eg),o(b,og),o(b,Xr),o(Xr,ig),o(b,tg),o(b,Zr),o(Zr,ag),o(b,rg),o(b,Yr),o(Yr,lg),o(b,ng),o(b,el),o(el,sg),o(b,cg),o(b,ol),o(ol,dg),o(b,pg),o(b,il),o(il,mg),o(b,ug),o(b,tl),o(tl,fg),o(b,vg),o(b,al),o(al,gg),o(b,hg),o(b,rl),o(rl,_g),o(b,bg),o(b,ll),o(ll,Eg),o(b,zg),o(b,nl),o(nl,wg),o(b,Pg),o(b,sl),o(sl,$g),o(b,Tg),o(b,cl),o(cl,kg),o(b,yg),c(e,Rd,s),v(Ei,e,s),c(e,Ud,s),c(e,y,s),o(y,qg),o(y,dl),o(dl,Lg),o(y,Ig),o(y,pl),o(pl,Cg),o(y,Og),o(y,ml),o(ml,Ag),o(y,Ng),o(y,ul),o(ul,Bg),o(y,Dg),o(y,fl),o(fl,Sg),o(y,jg),c(e,Fd,s),c(e,Ce,s),o(Ce,lo),o(lo,vl),v(zi,vl,null),o(Ce,Mg),o(Ce,gl),o(gl,Rg),c(e,Qd,s),c(e,Kt,s),o(Kt,Ug),c(e,Gd,s),c(e,A,s),o(A,Oe),o(Oe,Fg),o(Oe,hl),o(hl,Qg),o(Oe,Gg),o(Oe,wi),o(wi,Hg),o(Oe,xg),o(A,Wg),o(A,Ae),o(Ae,Jg),o(Ae,_l),o(_l,Kg),o(Ae,Vg),o(Ae,bl),o(bl,Xg),o(Ae,Zg),o(A,Yg),o(A,El),o(El,eh),o(A,oh),o(A,Ne),o(Ne,ih),o(Ne,zl),o(zl,th),o(Ne,ah),o(Ne,wl),o(wl,rh),o(Ne,lh),o(A,nh),o(A,Pl),o(Pl,sh),c(e,Hd,s),c(e,Be,s),o(Be,no),o(no,$l),v(Pi,$l,null),o(Be,ch),o(Be,Tl),o(Tl,dh),c(e,xd,s),c(e,Vt,s),o(Vt,ph),c(e,Wd,s),c(e,De,s),o(De,so),o(so,kl),v($i,kl,null),o(De,mh),o(De,yl),o(yl,uh),c(e,Jd,s),c(e,Xt,s),o(Xt,fh),c(e,Kd,s),c(e,co,s),o(co,Ti),o(Ti,ki),o(ki,vh),o(Ti,gh),o(Ti,yi),o(yi,hh),o(co,_h),o(co,qi),o(qi,Li),o(Li,bh),o(qi,Eh),o(qi,Ii),o(Ii,zh),c(e,Vd,s),c(e,Zt,s),o(Zt,wh),c(e,Xd,s),c(e,ue,s),o(ue,ce),o(ce,Ph),o(ce,Ci),o(Ci,$h),o(ce,Th),o(ce,Oi),o(Oi,kh),o(ce,yh),o(ce,ql),o(ql,qh),o(ce,Lh),o(ue,Ih),o(ue,Ll),o(Ll,Ch),o(ue,Oh),o(ue,Il),o(Il,Ah),c(e,Zd,s),c(e,Yt,s),o(Yt,Nh),c(e,Yd,s),c(e,ea,s),o(ea,Bh),c(e,ep,s),c(e,E,s),o(E,Cl),o(Cl,Ol),o(Ol,Al),o(Al,Dh),o(E,Sh),o(E,Nl),o(Nl,oa),o(oa,Bl),o(Bl,jh),o(E,Mh),o(E,Dl),o(Dl,ia),o(ia,Sl),o(Sl,Rh),o(E,Uh),o(E,jl),o(jl,ta),o(ta,Ml),o(Ml,Fh),o(E,Qh),o(E,Rl),o(Rl,aa),o(aa,Ul),o(Ul,Gh),o(E,Hh),o(E,Fl),o(Fl,ra),o(ra,Ql),o(Ql,xh),o(E,Wh),o(E,Gl),o(Gl,la),o(la,Hl),o(Hl,Jh),o(E,Kh),o(E,xl),o(xl,na),o(na,Wl),o(Wl,Vh),o(E,Xh),o(E,Jl),o(Jl,sa),o(sa,Kl),o(Kl,Zh),o(E,Yh),o(E,Vl),o(Vl,ca),o(ca,Xl),o(Xl,e_),o(E,o_),o(E,Zl),o(Zl,da),o(da,Yl),o(Yl,i_),o(E,t_),o(E,en),o(en,pa),o(pa,on),o(on,a_),o(E,r_),o(E,tn),o(tn,ma),o(ma,an),o(an,l_),o(E,n_),o(E,rn),o(rn,ua),o(ua,ln),o(ln,s_),c(e,op,s),c(e,N,s),o(N,c_),o(N,nn),o(nn,d_),o(N,p_),o(N,sn),o(sn,m_),o(N,u_),o(N,cn),o(cn,f_),o(N,v_),o(N,dn),o(dn,g_),o(N,h_),c(e,ip,s),c(e,Se,s),o(Se,po),o(po,pn),v(Ai,pn,null),o(Se,__),o(Se,mn),o(mn,b_),c(e,tp,s),c(e,mo,s),o(mo,E_),o(mo,un),o(un,z_),o(mo,w_),c(e,ap,s),c(e,B,s),o(B,je),o(je,P_),o(je,fn),o(fn,$_),o(je,T_),o(je,fa),o(fa,k_),o(je,y_),o(B,q_),o(B,Ni),o(Ni,L_),o(Ni,vn),o(vn,I_),o(Ni,C_),o(B,O_),o(B,gn),o(gn,A_),o(B,N_),o(B,Me),o(Me,B_),o(Me,Bi),o(Bi,D_),o(Me,S_),o(Me,hn),o(hn,j_),o(Me,M_),o(B,R_),o(B,_n),o(_n,U_),c(e,rp,s),c(e,va,s),o(va,F_),c(e,lp,s),c(e,Re,s),o(Re,uo),o(uo,bn),v(Di,bn,null),o(Re,Q_),o(Re,En),o(En,G_),c(e,np,s),c(e,fo,s),o(fo,zn),o(zn,Si),o(Si,H_),o(Si,ji),o(ji,x_),o(Si,W_),o(fo,J_),o(fo,wn),o(wn,Mi),o(Mi,K_),o(Mi,Pn),o(Pn,V_),o(Mi,X_),c(e,sp,s),v(Ri,e,s),c(e,cp,s),c(e,Ui,s),o(Ui,$n),o($n,Z_),c(e,dp,s),v(Fi,e,s),c(e,pp,s),c(e,ga,s),o(ga,Y_),c(e,mp,s),v(Qi,e,s),c(e,up,s),c(e,Gi,s),o(Gi,Ue),o(Ue,e1),o(Ue,Tn),o(Tn,o1),o(Ue,i1),o(Ue,Hi),o(Hi,t1),o(Ue,a1),c(e,fp,s),c(e,xi,s),o(xi,kn),o(kn,r1),o(xi,l1),c(e,vp,s),c(e,Wi,s),o(Wi,Fe),o(Fe,n1),o(Fe,yn),o(yn,s1),o(Fe,c1),o(Fe,qn),o(qn,d1),o(Fe,p1),c(e,gp,s),v(Ji,e,s),c(e,hp,s),c(e,vo,s),o(vo,m1),o(vo,Ln),o(Ln,u1),o(vo,f1),c(e,_p,s),c(e,Qe,s),o(Qe,go),o(go,In),v(Ki,In,null),o(Qe,v1),o(Qe,Cn),o(Cn,g1),c(e,bp,s),c(e,q,s),o(q,h1),o(q,On),o(On,_1),o(q,b1),o(q,An),o(An,E1),o(q,z1),o(q,Nn),o(Nn,w1),o(q,P1),o(q,Bn),o(Bn,$1),o(q,T1),o(q,Dn),o(Dn,k1),o(q,y1),c(e,Ep,s),c(e,ho,s),o(ho,q1),o(ho,Sn),o(Sn,L1),o(ho,I1),c(e,zp,s),c(e,L,s),o(L,jn),o(jn,C1),o(L,O1),o(L,Mn),o(Mn,A1),o(L,N1),o(L,Rn),o(Rn,B1),o(L,D1),o(L,Un),o(Un,S1),o(L,j1),o(L,R),o(R,M1),o(R,Fn),o(Fn,R1),o(R,U1),o(R,Qn),o(Qn,F1),o(R,Q1),o(R,Gn),o(Gn,G1),o(R,H1),o(R,Hn),o(Hn,x1),o(R,W1),o(L,J1),o(L,Ge),o(Ge,K1),o(Ge,xn),o(xn,V1),o(Ge,X1),o(Ge,Wn),o(Wn,Z1),o(Ge,Y1),c(e,wp,s),c(e,_o,s),o(_o,eb),o(_o,Jn),o(Jn,ob),o(_o,ib),c(e,Pp,s),c(e,ha,s),o(ha,tb),c(e,$p,s),c(e,_a,s),o(_a,ab),c(e,Tp,s),c(e,bo,s),o(bo,Vi),o(Vi,Xi),o(Xi,rb),o(Vi,lb),o(Vi,Zi),o(Zi,nb),o(bo,sb),o(bo,Kn),o(Kn,cb),c(e,kp,s),c(e,ba,s),o(ba,db),c(e,yp,s),c(e,Eo,s),o(Eo,pb),o(Eo,Vn),o(Vn,mb),o(Eo,ub),c(e,qp,s),c(e,zo,s),o(zo,fb),o(zo,Xn),o(Xn,vb),o(zo,gb),c(e,Lp,s),v(Yi,e,s),c(e,Ip,s),c(e,Ea,s),o(Ea,hb),c(e,Cp,s),c(e,wo,s),o(wo,Zn),o(Zn,_b),o(wo,bb),o(wo,He),o(He,Eb),o(He,Yn),o(Yn,zb),o(He,wb),o(He,es),o(es,Pb),o(He,$b),c(e,Op,s),c(e,za,s),o(za,Tb),c(e,Ap,s),c(e,Po,s),o(Po,kb),o(Po,os),o(os,yb),o(Po,qb),c(e,Np,s),c(e,G,s),o(G,is),o(is,Lb),o(G,Ib),o(G,ts),o(ts,Cb),o(G,Ob),o(G,as),o(as,Ab),o(G,Nb),o(G,rs),o(rs,Bb),c(e,Bp,s),c(e,$o,s),o($o,Db),o($o,et),o(et,Sb),o($o,jb),c(e,Dp,s),c(e,To,s),o(To,Mb),o(To,ot),o(ot,Rb),o(To,Ub),c(e,Sp,s),c(e,wa,s),o(wa,Fb),c(e,jp,s),c(e,I,s),o(I,ls),o(ls,Qb),o(I,Gb),o(I,ns),o(ns,Hb),o(I,xb),o(I,ss),o(ss,Wb),o(I,Jb),o(I,cs),o(cs,Kb),o(I,Vb),o(I,it),o(it,Xb),o(it,ds),o(ds,Zb),o(it,Yb),o(I,e0),o(I,ps),o(ps,o0),c(e,Mp,s),c(e,xe,s),o(xe,i0),o(xe,ms),o(ms,t0),o(xe,a0),o(xe,us),o(us,r0),c(e,Rp,s),c(e,Pa,s),o(Pa,l0),c(e,Up,s),v(tt,e,s),c(e,Fp,s),c(e,ko,s),o(ko,n0),o(ko,fs),o(fs,s0),o(ko,c0),c(e,Qp,s),c(e,D,s),o(D,U),o(U,d0),o(U,at),o(at,p0),o(U,m0),o(U,vs),o(vs,u0),o(U,f0),o(U,gs),o(gs,v0),o(U,g0),o(U,rt),o(rt,h0),o(U,_0),o(D,b0),o(D,hs),o(hs,E0),o(D,z0),o(D,$),o($,w0),o($,_s),o(_s,P0),o($,$0),o($,bs),o(bs,T0),o($,k0),o($,Es),o(Es,y0),o($,q0),o($,zs),o(zs,L0),o($,I0),o($,ws),o(ws,C0),o($,O0),o($,Ps),o(Ps,A0),o($,N0),o($,$s),o($s,B0),o($,D0),o($,Ts),o(Ts,S0),o($,j0),o($,ks),o(ks,M0),o($,R0),o($,ys),o(ys,U0),o($,F0),o(D,Q0),o(D,qs),o(qs,G0),o(D,H0),o(D,de),o(de,x0),o(de,Ls),o(Ls,W0),o(de,J0),o(de,Is),o(Is,K0),o(de,V0),o(de,Cs),o(Cs,X0),o(de,Z0),c(e,Gp,s),c(e,yo,s),o(yo,Y0),o(yo,Os),o(Os,e2),o(yo,o2),c(e,Hp,s),c(e,We,s),o(We,qo),o(qo,As),v(lt,As,null),o(We,i2),o(We,Ns),o(Ns,t2),c(e,xp,s),c(e,$a,s),o($a,a2),c(e,Wp,s),v(nt,e,s),c(e,Jp,s),c(e,Lo,s),o(Lo,r2),o(Lo,Ta),o(Ta,l2),o(Lo,n2),c(e,Kp,s),c(e,ka,s),o(ka,s2),c(e,Vp,s),c(e,Io,s),o(Io,ya),o(ya,Bs),o(Bs,c2),o(ya,d2),o(Io,p2),o(Io,qa),o(qa,Ds),o(Ds,m2),o(qa,u2),c(e,Xp,s),c(e,fe,s),o(fe,f2),o(fe,Ss),o(Ss,v2),o(fe,g2),o(fe,st),o(st,h2),o(fe,_2),c(e,Zp,s),c(e,La,s),o(La,js),o(js,b2),c(e,Yp,s),c(e,ve,s),o(ve,E2),o(ve,Ms),o(Ms,z2),o(ve,w2),o(ve,Rs),o(Rs,P2),o(ve,$2),c(e,em,s),c(e,Ia,s),o(Ia,T2),c(e,om,s),c(e,Ca,s),o(Ca,Us),o(Us,k2),c(e,im,s),v(ct,e,s),c(e,tm,s),c(e,dt,s),o(dt,Fs),o(Fs,y2),c(e,am,s),v(pt,e,s),c(e,rm,s),c(e,mt,s),o(mt,Qs),o(Qs,q2),c(e,lm,s),v(ut,e,s),c(e,nm,s),c(e,ft,s),o(ft,Gs),o(Gs,L2),c(e,sm,s),v(vt,e,s),c(e,cm,s),c(e,Je,s),o(Je,Hs),o(Hs,xs),o(xs,I2),o(Je,C2),o(Je,Ws),o(Ws,Js),o(Js,O2),c(e,dm,s),c(e,Oa,s),o(Oa,A2),c(e,pm,s),v(gt,e,s),c(e,mm,s),c(e,Aa,s),o(Aa,N2),c(e,um,s),c(e,Na,s),o(Na,B2),c(e,fm,s),c(e,Ba,s),o(Ba,D2),c(e,vm,s),c(e,Da,s),o(Da,Ks),o(Ks,S2),c(e,gm,s),c(e,H,s),o(H,j2),o(H,M2),o(H,R2),o(H,Vs),o(Vs,U2),o(H,F2),o(H,Xs),o(Xs,Q2),o(H,G2),c(e,hm,s),c(e,x,s),o(x,H2),o(x,Zs),o(Zs,x2),o(x,W2),o(x,Ys),o(Ys,J2),o(x,K2),o(x,ec),o(ec,V2),o(x,X2),c(e,_m,s),c(e,Ke,s),o(Ke,oc),o(oc,Z2),o(Ke,Y2),o(Ke,ic),o(ic,eE),o(Ke,oE),c(e,bm,s),v(ht,e,s),c(e,Em,s),c(e,ge,s),o(ge,iE),o(ge,tc),o(tc,tE),o(ge,aE),o(ge,ac),o(ac,rE),o(ge,lE),c(e,zm,s),c(e,Sa,s),o(Sa,rc),o(rc,nE),c(e,wm,s),c(e,he,s),o(he,sE),o(he,lc),o(lc,cE),o(he,dE),o(he,nc),o(nc,pE),o(he,mE),c(e,Pm,s),c(e,Co,s),o(Co,ja),o(ja,uE),o(ja,_t),o(_t,fE),o(Co,vE),o(Co,Ma),o(Ma,gE),o(Ma,bt),o(bt,hE),c(e,$m,s),c(e,Oo,s),o(Oo,_E),o(Oo,sc),o(sc,bE),o(Oo,EE),c(e,Tm,s),v(Et,e,s),c(e,km,s),c(e,W,s),o(W,zE),o(W,cc),o(cc,wE),o(W,PE),o(W,dc),o(dc,$E),o(W,TE),o(W,pc),o(pc,kE),o(W,yE),c(e,ym,s),v(zt,e,s),c(e,qm,s),c(e,Ra,s),o(Ra,qE),c(e,Lm,s),v(wt,e,s),c(e,Im,s),c(e,Ua,s),o(Ua,LE),c(e,Cm,s),v(Pt,e,s),c(e,Om,s),c(e,Fa,s),o(Fa,IE),c(e,Am,s),v($t,e,s),c(e,Nm,s),c(e,Tt,s),o(Tt,CE),o(Tt,mc),o(mc,OE),c(e,Bm,s),v(kt,e,s),c(e,Dm,s),c(e,J,s),o(J,AE),o(J,uc),o(uc,NE),o(J,BE),o(J,fc),o(fc,DE),o(J,SE),o(J,vc),o(vc,jE),o(J,ME),c(e,Sm,s),v(yt,e,s),c(e,jm,s),c(e,Qa,s),o(Qa,RE),c(e,Mm,s),v(qt,e,s),c(e,Rm,s),c(e,Ga,s),o(Ga,UE),c(e,Um,s),c(e,Ao,s),o(Ao,FE),o(Ao,gc),o(gc,QE),o(Ao,GE),c(e,Fm,s),c(e,K,s),o(K,HE),o(K,hc),o(hc,xE),o(K,WE),o(K,_c),o(_c,JE),o(K,KE),o(K,bc),o(bc,VE),o(K,XE),c(e,Qm,s),c(e,V,s),o(V,ZE),o(V,Ec),o(Ec,YE),o(V,e3),o(V,zc),o(zc,o3),o(V,i3),o(V,wc),o(wc,t3),o(V,a3),c(e,Gm,s),v(Lt,e,s),c(e,Hm,s),c(e,Ha,s),o(Ha,Pc),o(Pc,r3),c(e,xm,s),c(e,No,s),o(No,l3),o(No,xa),o(xa,n3),o(No,s3),c(e,Wm,s),v(It,e,s),c(e,Jm,s),c(e,X,s),o(X,c3),o(X,$c),o($c,d3),o(X,p3),o(X,Tc),o(Tc,m3),o(X,u3),o(X,kc),o(kc,f3),o(X,v3),c(e,Km,s),c(e,_e,s),o(_e,g3),o(_e,yc),o(yc,h3),o(_e,_3),o(_e,qc),o(qc,b3),o(_e,E3),c(e,Vm,s),c(e,Z,s),o(Z,Ve),o(Ve,z3),o(Ve,Lc),o(Lc,w3),o(Ve,P3),o(Ve,Ic),o(Ic,$3),o(Ve,T3),o(Z,k3),o(Z,Cc),o(Cc,y3),o(Z,q3),o(Z,Oc),o(Oc,L3),o(Z,I3),o(Z,Y),o(Y,C3),o(Y,Ac),o(Ac,O3),o(Y,A3),o(Y,Nc),o(Nc,N3),o(Y,B3),o(Y,Bc),o(Bc,D3),o(Y,S3),o(Y,Ct),o(Ct,j3),c(e,Xm,s),c(e,Bo,s),o(Bo,M3),o(Bo,Dc),o(Dc,R3),o(Bo,U3),c(e,Zm,s),c(e,Do,s),o(Do,F3),o(Do,Sc),o(Sc,Q3),o(Do,G3),c(e,Ym,s),c(e,Wa,s),o(Wa,jc),o(jc,H3),c(e,eu,s),c(e,So,s),o(So,x3),o(So,Mc),o(Mc,W3),o(So,J3),c(e,ou,s),v(Ot,e,s),c(e,iu,s),c(e,Ja,s),o(Ja,K3),c(e,tu,s),c(e,jo,s),o(jo,At),o(At,V3),o(At,Rc),o(Rc,X3),o(At,Z3),o(jo,Y3),o(jo,Uc),o(Uc,e4),c(e,au,s),c(e,Mo,s),o(Mo,o4),o(Mo,Fc),o(Fc,i4),o(Mo,t4),c(e,ru,s),v(Nt,e,s),c(e,lu,s),v(Ro,e,s),c(e,nu,s),c(e,ee,s),o(ee,a4),o(ee,Qc),o(Qc,r4),o(ee,l4),o(ee,Gc),o(Gc,n4),o(ee,s4),o(ee,Hc),o(Hc,c4),o(ee,d4),c(e,su,s),c(e,Uo,s),o(Uo,xc),o(xc,p4),o(Uo,m4),o(Uo,Wc),o(Wc,u4),c(e,cu,s),c(e,Ka,s),o(Ka,Jc),o(Jc,f4),c(e,du,s),c(e,Fo,s),o(Fo,v4),o(Fo,Kc),o(Kc,g4),o(Fo,h4),c(e,pu,s),c(e,Va,s),o(Va,_4),c(e,mu,s),c(e,Qo,s),o(Qo,b4),o(Qo,Vc),o(Vc,E4),o(Qo,z4),c(e,uu,s),v(Bt,e,s),c(e,fu,s),c(e,Go,s),o(Go,w4),o(Go,Xc),o(Xc,P4),o(Go,$4),c(e,vu,s),v(Dt,e,s),c(e,gu,s),c(e,Ho,s),o(Ho,T4),o(Ho,Zc),o(Zc,k4),o(Ho,y4),c(e,hu,s),c(e,xo,s),o(xo,q4),o(xo,Yc),o(Yc,L4),o(xo,I4),c(e,_u,s),c(e,Xa,s),o(Xa,ed),o(ed,C4),c(e,bu,s),c(e,oe,s),o(oe,O4),o(oe,od),o(od,A4),o(oe,N4),o(oe,id),o(id,B4),o(oe,D4),o(oe,td),o(td,S4),o(oe,j4),c(e,Eu,s),c(e,Za,s),o(Za,ad),o(ad,M4),c(e,zu,s),c(e,be,s),o(be,R4),o(be,rd),o(rd,U4),o(be,F4),o(be,ld),o(ld,Q4),o(be,G4),c(e,wu,s),c(e,Wo,s),o(Wo,H4),o(Wo,nd),o(nd,x4),o(Wo,W4),c(e,Pu,s),c(e,Ya,s),o(Ya,sd),o(sd,J4),c(e,$u,s),c(e,Jo,s),o(Jo,K4),o(Jo,cd),o(cd,V4),o(Jo,X4),c(e,Tu,s),v(St,e,s),c(e,ku,s),c(e,er,s),o(er,Z4),c(e,yu,s),v(jt,e,s),c(e,qu,s),c(e,or,s),o(or,Y4),c(e,Lu,s),c(e,ir,s),o(ir,e5),c(e,Iu,s),c(e,tr,s),o(tr,o5),c(e,Cu,s),c(e,ar,s),o(ar,dd),o(dd,i5),c(e,Ou,s),c(e,S,s),o(S,t5),o(S,rr),o(rr,a5),o(S,r5),o(S,pd),o(pd,l5),o(S,n5),o(S,md),o(md,s5),o(S,c5),o(S,ud),o(ud,d5),o(S,p5),c(e,Au,s),v(Mt,e,s),c(e,Nu,s),c(e,Ko,s),o(Ko,m5),o(Ko,fd),o(fd,u5),o(Ko,f5),c(e,Bu,s),c(e,lr,s),o(lr,vd),o(vd,v5),c(e,Du,s),c(e,Vo,s),o(Vo,g5),o(Vo,gd),o(gd,h5),o(Vo,_5),c(e,Su,s),c(e,nr,s),o(nr,hd),o(hd,b5),c(e,ju,s),c(e,sr,s),o(sr,E5),c(e,Mu,s),c(e,Xe,s),o(Xe,Xo),o(Xo,_d),v(Rt,_d,null),o(Xe,z5),o(Xe,bd),o(bd,w5),c(e,Ru,s),c(e,cr,s),o(cr,P5),c(e,Uu,s),c(e,dr,s),o(dr,Ed),o(Ed,$5),Fu=!0},p(e,[s]){const Ut={};s&2&&(Ut.$$scope={dirty:s,ctx:e}),Ro.$set(Ut)},i(e){Fu||(g(j.$$.fragment,e),g(hi.$$.fragment,e),g(bi.$$.fragment,e),g(Ei.$$.fragment,e),g(zi.$$.fragment,e),g(Pi.$$.fragment,e),g($i.$$.fragment,e),g(Ai.$$.fragment,e),g(Di.$$.fragment,e),g(Ri.$$.fragment,e),g(Fi.$$.fragment,e),g(Qi.$$.fragment,e),g(Ji.$$.fragment,e),g(Ki.$$.fragment,e),g(Yi.$$.fragment,e),g(tt.$$.fragment,e),g(lt.$$.fragment,e),g(nt.$$.fragment,e),g(ct.$$.fragment,e),g(pt.$$.fragment,e),g(ut.$$.fragment,e),g(vt.$$.fragment,e),g(gt.$$.fragment,e),g(ht.$$.fragment,e),g(Et.$$.fragment,e),g(zt.$$.fragment,e),g(wt.$$.fragment,e),g(Pt.$$.fragment,e),g($t.$$.fragment,e),g(kt.$$.fragment,e),g(yt.$$.fragment,e),g(qt.$$.fragment,e),g(Lt.$$.fragment,e),g(It.$$.fragment,e),g(Ot.$$.fragment,e),g(Nt.$$.fragment,e),g(Ro.$$.fragment,e),g(Bt.$$.fragment,e),g(Dt.$$.fragment,e),g(St.$$.fragment,e),g(jt.$$.fragment,e),g(Mt.$$.fragment,e),g(Rt.$$.fragment,e),Fu=!0)},o(e){h(j.$$.fragment,e),h(hi.$$.fragment,e),h(bi.$$.fragment,e),h(Ei.$$.fragment,e),h(zi.$$.fragment,e),h(Pi.$$.fragment,e),h($i.$$.fragment,e),h(Ai.$$.fragment,e),h(Di.$$.fragment,e),h(Ri.$$.fragment,e),h(Fi.$$.fragment,e),h(Qi.$$.fragment,e),h(Ji.$$.fragment,e),h(Ki.$$.fragment,e),h(Yi.$$.fragment,e),h(tt.$$.fragment,e),h(lt.$$.fragment,e),h(nt.$$.fragment,e),h(ct.$$.fragment,e),h(pt.$$.fragment,e),h(ut.$$.fragment,e),h(vt.$$.fragment,e),h(gt.$$.fragment,e),h(ht.$$.fragment,e),h(Et.$$.fragment,e),h(zt.$$.fragment,e),h(wt.$$.fragment,e),h(Pt.$$.fragment,e),h($t.$$.fragment,e),h(kt.$$.fragment,e),h(yt.$$.fragment,e),h(qt.$$.fragment,e),h(Lt.$$.fragment,e),h(It.$$.fragment,e),h(Ot.$$.fragment,e),h(Nt.$$.fragment,e),h(Ro.$$.fragment,e),h(Bt.$$.fragment,e),h(Dt.$$.fragment,e),h(St.$$.fragment,e),h(jt.$$.fragment,e),h(Mt.$$.fragment,e),h(Rt.$$.fragment,e),Fu=!1},d(e){i(k),e&&i(Ye),e&&i(C),_(j),e&&i(fi),e&&i(M),e&&i($d),e&&i(eo),e&&i(Td),e&&i(Ft),e&&i(kd),e&&i(Q),e&&i(yd),e&&i(oo),e&&i(qd),e&&i(Qt),e&&i(Ld),e&&i(Le),_(hi),e&&i(Id),e&&i(Gt),e&&i(Cd),e&&i(to),e&&i(Od),e&&i(me),e&&i(Ad),e&&i(ao),e&&i(Nd),e&&i(xt),e&&i(Bd),e&&i(Ie),_(bi),e&&i(Dd),e&&i(O),e&&i(Sd),e&&i(Wt),e&&i(jd),e&&i(Jt),e&&i(Md),e&&i(b),e&&i(Rd),_(Ei,e),e&&i(Ud),e&&i(y),e&&i(Fd),e&&i(Ce),_(zi),e&&i(Qd),e&&i(Kt),e&&i(Gd),e&&i(A),e&&i(Hd),e&&i(Be),_(Pi),e&&i(xd),e&&i(Vt),e&&i(Wd),e&&i(De),_($i),e&&i(Jd),e&&i(Xt),e&&i(Kd),e&&i(co),e&&i(Vd),e&&i(Zt),e&&i(Xd),e&&i(ue),e&&i(Zd),e&&i(Yt),e&&i(Yd),e&&i(ea),e&&i(ep),e&&i(E),e&&i(op),e&&i(N),e&&i(ip),e&&i(Se),_(Ai),e&&i(tp),e&&i(mo),e&&i(ap),e&&i(B),e&&i(rp),e&&i(va),e&&i(lp),e&&i(Re),_(Di),e&&i(np),e&&i(fo),e&&i(sp),_(Ri,e),e&&i(cp),e&&i(Ui),e&&i(dp),_(Fi,e),e&&i(pp),e&&i(ga),e&&i(mp),_(Qi,e),e&&i(up),e&&i(Gi),e&&i(fp),e&&i(xi),e&&i(vp),e&&i(Wi),e&&i(gp),_(Ji,e),e&&i(hp),e&&i(vo),e&&i(_p),e&&i(Qe),_(Ki),e&&i(bp),e&&i(q),e&&i(Ep),e&&i(ho),e&&i(zp),e&&i(L),e&&i(wp),e&&i(_o),e&&i(Pp),e&&i(ha),e&&i($p),e&&i(_a),e&&i(Tp),e&&i(bo),e&&i(kp),e&&i(ba),e&&i(yp),e&&i(Eo),e&&i(qp),e&&i(zo),e&&i(Lp),_(Yi,e),e&&i(Ip),e&&i(Ea),e&&i(Cp),e&&i(wo),e&&i(Op),e&&i(za),e&&i(Ap),e&&i(Po),e&&i(Np),e&&i(G),e&&i(Bp),e&&i($o),e&&i(Dp),e&&i(To),e&&i(Sp),e&&i(wa),e&&i(jp),e&&i(I),e&&i(Mp),e&&i(xe),e&&i(Rp),e&&i(Pa),e&&i(Up),_(tt,e),e&&i(Fp),e&&i(ko),e&&i(Qp),e&&i(D),e&&i(Gp),e&&i(yo),e&&i(Hp),e&&i(We),_(lt),e&&i(xp),e&&i($a),e&&i(Wp),_(nt,e),e&&i(Jp),e&&i(Lo),e&&i(Kp),e&&i(ka),e&&i(Vp),e&&i(Io),e&&i(Xp),e&&i(fe),e&&i(Zp),e&&i(La),e&&i(Yp),e&&i(ve),e&&i(em),e&&i(Ia),e&&i(om),e&&i(Ca),e&&i(im),_(ct,e),e&&i(tm),e&&i(dt),e&&i(am),_(pt,e),e&&i(rm),e&&i(mt),e&&i(lm),_(ut,e),e&&i(nm),e&&i(ft),e&&i(sm),_(vt,e),e&&i(cm),e&&i(Je),e&&i(dm),e&&i(Oa),e&&i(pm),_(gt,e),e&&i(mm),e&&i(Aa),e&&i(um),e&&i(Na),e&&i(fm),e&&i(Ba),e&&i(vm),e&&i(Da),e&&i(gm),e&&i(H),e&&i(hm),e&&i(x),e&&i(_m),e&&i(Ke),e&&i(bm),_(ht,e),e&&i(Em),e&&i(ge),e&&i(zm),e&&i(Sa),e&&i(wm),e&&i(he),e&&i(Pm),e&&i(Co),e&&i($m),e&&i(Oo),e&&i(Tm),_(Et,e),e&&i(km),e&&i(W),e&&i(ym),_(zt,e),e&&i(qm),e&&i(Ra),e&&i(Lm),_(wt,e),e&&i(Im),e&&i(Ua),e&&i(Cm),_(Pt,e),e&&i(Om),e&&i(Fa),e&&i(Am),_($t,e),e&&i(Nm),e&&i(Tt),e&&i(Bm),_(kt,e),e&&i(Dm),e&&i(J),e&&i(Sm),_(yt,e),e&&i(jm),e&&i(Qa),e&&i(Mm),_(qt,e),e&&i(Rm),e&&i(Ga),e&&i(Um),e&&i(Ao),e&&i(Fm),e&&i(K),e&&i(Qm),e&&i(V),e&&i(Gm),_(Lt,e),e&&i(Hm),e&&i(Ha),e&&i(xm),e&&i(No),e&&i(Wm),_(It,e),e&&i(Jm),e&&i(X),e&&i(Km),e&&i(_e),e&&i(Vm),e&&i(Z),e&&i(Xm),e&&i(Bo),e&&i(Zm),e&&i(Do),e&&i(Ym),e&&i(Wa),e&&i(eu),e&&i(So),e&&i(ou),_(Ot,e),e&&i(iu),e&&i(Ja),e&&i(tu),e&&i(jo),e&&i(au),e&&i(Mo),e&&i(ru),_(Nt,e),e&&i(lu),_(Ro,e),e&&i(nu),e&&i(ee),e&&i(su),e&&i(Uo),e&&i(cu),e&&i(Ka),e&&i(du),e&&i(Fo),e&&i(pu),e&&i(Va),e&&i(mu),e&&i(Qo),e&&i(uu),_(Bt,e),e&&i(fu),e&&i(Go),e&&i(vu),_(Dt,e),e&&i(gu),e&&i(Ho),e&&i(hu),e&&i(xo),e&&i(_u),e&&i(Xa),e&&i(bu),e&&i(oe),e&&i(Eu),e&&i(Za),e&&i(zu),e&&i(be),e&&i(wu),e&&i(Wo),e&&i(Pu),e&&i(Ya),e&&i($u),e&&i(Jo),e&&i(Tu),_(St,e),e&&i(ku),e&&i(er),e&&i(yu),_(jt,e),e&&i(qu),e&&i(or),e&&i(Lu),e&&i(ir),e&&i(Iu),e&&i(tr),e&&i(Cu),e&&i(ar),e&&i(Ou),e&&i(S),e&&i(Au),_(Mt,e),e&&i(Nu),e&&i(Ko),e&&i(Bu),e&&i(lr),e&&i(Du),e&&i(Vo),e&&i(Su),e&&i(nr),e&&i(ju),e&&i(sr),e&&i(Mu),e&&i(Xe),_(Rt),e&&i(Ru),e&&i(cr),e&&i(Uu),e&&i(dr)}}}const DP={local:"come-aggiungere-un-modello-a-transformers",sections:[{local:"panoramica-generale-su-transformers",sections:[{local:"panoramica-sui-modelli",title:"Panoramica sui modelli"},{local:"stile-per-il-codice",title:"Stile per il codice"},{local:"panoramica-sui-tokenizers",title:"Panoramica sui tokenizers"}],title:"Panoramica generale su \u{1F917} Transformers"},{local:"aggiungere-un-modello-a-transformers-passo-dopo-passo",sections:[{local:"1-opzionale-aspetti-teorici-di-brandnewbert",title:"1. (Opzionale) Aspetti teorici di BrandNewBert "},{local:"2-prepare-il-tuo-ambiente",title:"2. Prepare il tuo ambiente"},{local:"34-provare-un-pretrained-checkpoint-usando-la-repo-originale",title:"3.-4. Provare un pretrained checkpoint usando la repo originale "},{local:"514-trasferire-brandnewbert-in-transformers",title:"5.-14. Trasferire BrandNewBert in \u{1F917} Transformers"},{local:"condividete-il-vostro-lavoro",title:"Condividete il vostro lavoro!!"}],title:"Aggiungere un modello a \u{1F917} Transformers passo dopo passo "}],title:"Come aggiungere un modello a \u{1F917} Transformers?"};function SP(Pd){return OP(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class FP extends yP{constructor(k){super();qP(this,k,SP,BP,LP,{})}}export{FP as default,DP as metadata};
