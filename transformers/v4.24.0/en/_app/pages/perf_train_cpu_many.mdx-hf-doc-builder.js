import{S as nl,i as rl,s as ol,e as r,k as p,w as P,t as a,M as ll,c as o,d as n,m as h,a as l,x as C,h as i,b as s,G as t,g as f,y as k,q as D,o as A,B as I,v as al}from"../chunks/vendor-hf-doc-builder.js";import{T as il}from"../chunks/Tip-hf-doc-builder.js";import{I as bt}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as Ze}from"../chunks/CodeBlock-hf-doc-builder.js";function sl(yt){let d,H;return{c(){d=r("p"),H=a(`oneccl_bindings_for_pytorch 1.12.0 prebuilt wheel does not work with PyTorch 1.12.1 (it is for PyTorch 1.12.0)
PyTorch 1.12.1 should work with oneccl_bindings_for_pytorch 1.12.100`)},l(_){d=o(_,"P",{});var y=l(d);H=i(y,`oneccl_bindings_for_pytorch 1.12.0 prebuilt wheel does not work with PyTorch 1.12.1 (it is for PyTorch 1.12.0)
PyTorch 1.12.1 should work with oneccl_bindings_for_pytorch 1.12.100`),y.forEach(n)},m(_,y){f(_,d,y),t(d,H)},d(_){_&&n(d)}}}function cl(yt){let d,H,_,y,et,J,nn,tt,rn,xt,me,on,wt,N,W,nt,Y,ln,rt,an,Et,R,Q,sn,cn,Z,pn,hn,ee,fn,dn,$t,M,_n,ot,un,mn,lt,vn,gn,Tt,j,bn,te,yn,xn,Pt,O,B,at,ne,wn,it,En,Ct,ve,$n,kt,K,st,u,ge,Tn,Pn,be,Cn,kn,ye,Dn,An,xe,In,Rn,we,Ln,Mn,Ee,Un,Nn,L,m,$e,On,Sn,ct,qn,Te,Hn,Wn,Pe,jn,Bn,Ce,Kn,Gn,ke,zn,Vn,v,De,Xn,Fn,pt,Jn,Ae,Yn,Qn,Ie,Zn,er,Re,tr,nr,Le,rr,or,g,Me,lr,ar,ht,ir,Ue,sr,cr,Ne,pr,hr,Oe,fr,dr,Se,_r,ur,b,qe,mr,vr,He,gr,br,We,yr,xr,je,wr,Er,Be,$r,Tr,ft,Dt,re,At,U,Pr,dt,Cr,kr,oe,Dr,Ar,It,G,Rt,S,z,_t,le,Ir,ut,Rr,Lt,Ke,Lr,Mt,Ge,Mr,Ut,ae,Nt,ze,Ur,Ot,ie,St,Ve,Nr,qt,q,V,mt,se,Or,vt,Sr,Ht,ce,qr,pe,Hr,Wt,Xe,Wr,jt,he,Bt,Fe,jr,Kt,Je,Br,Gt,fe,zt,X,Kr,gt,Gr,zr,Vt,de,Xt;return J=new bt({}),Y=new bt({}),ne=new bt({}),re=new Ze({props:{code:"pip install oneccl_bind_pt=={pytorch_version} -f https://software.intel.com/ipex-whl-stable",highlighted:'pip install oneccl_bind_pt=={<span class="hljs-attribute">pytorch_version} -f https</span>://software<span class="hljs-variable">.intel</span><span class="hljs-variable">.com</span>/ipex-whl-stable'}}),G=new il({props:{warning:!0,$$slots:{default:[sl]},$$scope:{ctx:yt}}}),le=new bt({}),ae=new Ze({props:{code:`oneccl_bindings_for_pytorch_path=$(python -c "from oneccl_bindings_for_pytorch import cwd; print(cwd)")
source $oneccl_bindings_for_pytorch_path/env/setvars.sh`,highlighted:`oneccl_bindings_for_pytorch_path=$(<span class="hljs-keyword">python</span> -c <span class="hljs-string">&quot;from oneccl_bindings_for_pytorch import cwd; print(cwd)&quot;</span>)
<span class="hljs-keyword">source</span> $oneccl_bindings_for_pytorch_path/<span class="hljs-keyword">env</span>/setvars.sh`}}),ie=new Ze({props:{code:`torch_ccl_path=$(python -c "import torch; import torch_ccl; import os;  print(os.path.abspath(os.path.dirname(torch_ccl.__file__)))")
source $torch_ccl_path/env/setvars.sh`,highlighted:`torch_ccl_path=$(<span class="hljs-keyword">python</span> -c <span class="hljs-string">&quot;import torch; import torch_ccl; import os;  print(os.path.abspath(os.path.dirname(torch_ccl.__file__)))&quot;</span>)
<span class="hljs-keyword">source</span> $torch_ccl_path/<span class="hljs-keyword">env</span>/setvars.sh`}}),se=new bt({}),he=new Ze({props:{code:` export CCL_WORKER_COUNT=1
 export MASTER_ADDR=127.0.0.1
 mpirun -n 2 -genv OMP_NUM_THREADS=23 \\
 python3 run_qa.py \\
 --model_name_or_path bert-large-uncased \\
 --dataset_name squad \\
 --do_train \\
 --do_eval \\
 --per_device_train_batch_size 12  \\
 --learning_rate 3e-5  \\
 --num_train_epochs 2  \\
 --max_seq_length 384 \\
 --doc_stride 128  \\
 --output_dir /tmp/debug_squad/ \\
 --no_cuda \\
 --xpu_backend ccl`,highlighted:` export CCL_WORKER_COUNT=1
 export MASTER_ADDR=127.0.0.1
 mpirun -n 2 -genv OMP_NUM_THREADS=23 \\
 python3 run_qa.py \\
 --model_name_or_path bert-large-uncased \\
 --dataset_name squad \\
 --do_train \\
 --do_eval \\
 --per_device_train_batch_size 12  \\
 --learning_rate 3e-5  \\
 --num_train_epochs 2  \\
 --max_seq_length 384 \\
 --doc_stride 128  \\
 --output_dir /tmp/debug_squad/ \\
 --no_cuda \\
 --xpu_backend ccl`}}),fe=new Ze({props:{code:` cat hostfile
 xxx.xxx.xxx.xxx #node0 ip
 xxx.xxx.xxx.xxx #node1 ip`,highlighted:` cat hostfile
 xxx.xxx.xxx.xxx #node0 ip
 xxx.xxx.xxx.xxx #node1 ip`}}),de=new Ze({props:{code:` export CCL_WORKER_COUNT=1
 export MASTER_ADDR=xxx.xxx.xxx.xxx #node0 ip
 mpirun -f hostfile -n 4 -ppn 2 \\
 -genv OMP_NUM_THREADS=23 \\
 python3 run_qa.py \\
 --model_name_or_path bert-large-uncased \\
 --dataset_name squad \\
 --do_train \\
 --do_eval \\
 --per_device_train_batch_size 12  \\
 --learning_rate 3e-5  \\
 --num_train_epochs 2  \\
 --max_seq_length 384 \\
 --doc_stride 128  \\
 --output_dir /tmp/debug_squad/ \\
 --no_cuda \\
 --xpu_backend ccl`,highlighted:` export CCL_WORKER_COUNT=1
 export MASTER_ADDR=xxx.xxx.xxx.xxx #node0 ip
 mpirun -f hostfile -n 4 -ppn 2 \\
 -genv OMP_NUM_THREADS=23 \\
 python3 run_qa.py \\
 --model_name_or_path bert-large-uncased \\
 --dataset_name squad \\
 --do_train \\
 --do_eval \\
 --per_device_train_batch_size 12  \\
 --learning_rate 3e-5  \\
 --num_train_epochs 2  \\
 --max_seq_length 384 \\
 --doc_stride 128  \\
 --output_dir /tmp/debug_squad/ \\
 --no_cuda \\
 --xpu_backend ccl`}}),{c(){d=r("meta"),H=p(),_=r("h1"),y=r("a"),et=r("span"),P(J.$$.fragment),nn=p(),tt=r("span"),rn=a("Efficient Training on Multiple CPUs"),xt=p(),me=r("p"),on=a("When training on a single CPU is too slow, we can use multiple CPUs. This guide focuses on PyTorch-based DDP enabling distributed CPU training efficiently."),wt=p(),N=r("h2"),W=r("a"),nt=r("span"),P(Y.$$.fragment),ln=p(),rt=r("span"),an=a("Intel\xAE oneCCL Bindings for PyTorch"),Et=p(),R=r("p"),Q=r("a"),sn=a("Intel\xAE oneCCL"),cn=a(" (collective communications library) is a library for efficient distributed deep learning training implementing such collectives like allreduce, allgather, alltoall. For more information on oneCCL, please refer to the "),Z=r("a"),pn=a("oneCCL documentation"),hn=a(" and "),ee=r("a"),fn=a("oneCCL specification"),dn=a("."),$t=p(),M=r("p"),_n=a("Module "),ot=r("code"),un=a("oneccl_bindings_for_pytorch"),mn=a(" ("),lt=r("code"),vn=a("torch_ccl"),gn=a(" before version 1.12)  implements PyTorch C10D ProcessGroup API and can be dynamically loaded as external ProcessGroup and only works on Linux platform now"),Tt=p(),j=r("p"),bn=a("Check more detailed information for "),te=r("a"),yn=a("oneccl_bind_pt"),xn=a("."),Pt=p(),O=r("h3"),B=r("a"),at=r("span"),P(ne.$$.fragment),wn=p(),it=r("span"),En=a("Intel\xAE oneCCL Bindings for PyTorch installation:"),Ct=p(),ve=r("p"),$n=a("Wheel files are available for the following Python versions:"),kt=p(),K=r("table"),st=r("thead"),u=r("tr"),ge=r("th"),Tn=a("Extension Version"),Pn=p(),be=r("th"),Cn=a("Python 3.6"),kn=p(),ye=r("th"),Dn=a("Python 3.7"),An=p(),xe=r("th"),In=a("Python 3.8"),Rn=p(),we=r("th"),Ln=a("Python 3.9"),Mn=p(),Ee=r("th"),Un=a("Python 3.10"),Nn=p(),L=r("tbody"),m=r("tr"),$e=r("td"),On=a("1.12.100"),Sn=p(),ct=r("td"),qn=p(),Te=r("td"),Hn=a("\u221A"),Wn=p(),Pe=r("td"),jn=a("\u221A"),Bn=p(),Ce=r("td"),Kn=a("\u221A"),Gn=p(),ke=r("td"),zn=a("\u221A"),Vn=p(),v=r("tr"),De=r("td"),Xn=a("1.12.0"),Fn=p(),pt=r("td"),Jn=p(),Ae=r("td"),Yn=a("\u221A"),Qn=p(),Ie=r("td"),Zn=a("\u221A"),er=p(),Re=r("td"),tr=a("\u221A"),nr=p(),Le=r("td"),rr=a("\u221A"),or=p(),g=r("tr"),Me=r("td"),lr=a("1.11.0"),ar=p(),ht=r("td"),ir=p(),Ue=r("td"),sr=a("\u221A"),cr=p(),Ne=r("td"),pr=a("\u221A"),hr=p(),Oe=r("td"),fr=a("\u221A"),dr=p(),Se=r("td"),_r=a("\u221A"),ur=p(),b=r("tr"),qe=r("td"),mr=a("1.10.0"),vr=p(),He=r("td"),gr=a("\u221A"),br=p(),We=r("td"),yr=a("\u221A"),xr=p(),je=r("td"),wr=a("\u221A"),Er=p(),Be=r("td"),$r=a("\u221A"),Tr=p(),ft=r("td"),Dt=p(),P(re.$$.fragment),At=p(),U=r("p"),Pr=a("where "),dt=r("code"),Cr=a("{pytorch_version}"),kr=a(` should be your PyTorch version, for instance 1.12.0.
Check more approaches for `),oe=r("a"),Dr=a("oneccl_bind_pt installation"),Ar=a(`.
Versions of oneCCL and PyTorch must match.`),It=p(),P(G.$$.fragment),Rt=p(),S=r("h2"),z=r("a"),_t=r("span"),P(le.$$.fragment),Ir=p(),ut=r("span"),Rr=a("Intel\xAE MPI library"),Lt=a(`

Use this standards-based MPI implementation to deliver flexible, efficient, scalable cluster messaging on Intel\xAE architecture. This component is part of the Intel\xAE oneAPI HPC Toolkit.
`),Ke=r("p"),Lr=a("oneccl_bindings_for_pytorch is installed along with the MPI tool set. Need to source the environment before using it."),Mt=p(),Ge=r("p"),Mr=a("for Intel\xAE oneCCL >= 1.12.0"),Ut=p(),P(ae.$$.fragment),Nt=p(),ze=r("p"),Ur=a("for Intel\xAE oneCCL whose version < 1.12.0"),Ot=p(),P(ie.$$.fragment),St=p(),Ve=r("p"),Nr=a("The following \u201CUsage in Trainer\u201D takes mpirun in Intel\xAE MPI library as an example."),qt=p(),q=r("h2"),V=r("a"),mt=r("span"),P(se.$$.fragment),Or=p(),vt=r("span"),Sr=a("Usage in Trainer"),Ht=a(`

To enable multi CPU distributed training in the Trainer with the ccl backend, users should add **\`--xpu_backend ccl\`** in the command arguments.
`),ce=r("p"),qr=a("Let\u2019s see an example with the "),pe=r("a"),Hr=a("question-answering example"),Wt=p(),Xe=r("p"),Wr=a("The following command enables training with 2 processes on one Xeon node, with one process running per one socket. The variables OMP_NUM_THREADS/CCL_WORKER_COUNT can be tuned for optimal performance."),jt=p(),P(he.$$.fragment),Bt=p(),Fe=r("p"),jr=a("The following command enables training with a total of four processes on two Xeons (node0 and node1, taking node0 as the main process), ppn (processes per node) is set to 2, with one process running per one socket. The variables OMP_NUM_THREADS/CCL_WORKER_COUNT can be tuned for optimal performance."),Kt=p(),Je=r("p"),Br=a("In node0, you need to create a configuration file which contains the IP addresses of each node (for example hostfile) and pass that configuration file path as an argument."),Gt=p(),P(fe.$$.fragment),zt=p(),X=r("p"),Kr=a("Now, run the following command in node0 and "),gt=r("strong"),Gr=a("4DDP"),zr=a(" will be enabled in node0 and node1:"),Vt=p(),P(de.$$.fragment),this.h()},l(e){const c=ll('[data-svelte="svelte-1phssyn"]',document.head);d=o(c,"META",{name:!0,content:!0}),c.forEach(n),H=h(e),_=o(e,"H1",{class:!0});var _e=l(_);y=o(_e,"A",{id:!0,class:!0,href:!0});var Xr=l(y);et=o(Xr,"SPAN",{});var Fr=l(et);C(J.$$.fragment,Fr),Fr.forEach(n),Xr.forEach(n),nn=h(_e),tt=o(_e,"SPAN",{});var Jr=l(tt);rn=i(Jr,"Efficient Training on Multiple CPUs"),Jr.forEach(n),_e.forEach(n),xt=h(e),me=o(e,"P",{});var Yr=l(me);on=i(Yr,"When training on a single CPU is too slow, we can use multiple CPUs. This guide focuses on PyTorch-based DDP enabling distributed CPU training efficiently."),Yr.forEach(n),wt=h(e),N=o(e,"H2",{class:!0});var Ft=l(N);W=o(Ft,"A",{id:!0,class:!0,href:!0});var Qr=l(W);nt=o(Qr,"SPAN",{});var Zr=l(nt);C(Y.$$.fragment,Zr),Zr.forEach(n),Qr.forEach(n),ln=h(Ft),rt=o(Ft,"SPAN",{});var eo=l(rt);an=i(eo,"Intel\xAE oneCCL Bindings for PyTorch"),eo.forEach(n),Ft.forEach(n),Et=h(e),R=o(e,"P",{});var ue=l(R);Q=o(ue,"A",{href:!0,rel:!0});var to=l(Q);sn=i(to,"Intel\xAE oneCCL"),to.forEach(n),cn=i(ue," (collective communications library) is a library for efficient distributed deep learning training implementing such collectives like allreduce, allgather, alltoall. For more information on oneCCL, please refer to the "),Z=o(ue,"A",{href:!0,rel:!0});var no=l(Z);pn=i(no,"oneCCL documentation"),no.forEach(n),hn=i(ue," and "),ee=o(ue,"A",{href:!0,rel:!0});var ro=l(ee);fn=i(ro,"oneCCL specification"),ro.forEach(n),dn=i(ue,"."),ue.forEach(n),$t=h(e),M=o(e,"P",{});var Ye=l(M);_n=i(Ye,"Module "),ot=o(Ye,"CODE",{});var oo=l(ot);un=i(oo,"oneccl_bindings_for_pytorch"),oo.forEach(n),mn=i(Ye," ("),lt=o(Ye,"CODE",{});var lo=l(lt);vn=i(lo,"torch_ccl"),lo.forEach(n),gn=i(Ye," before version 1.12)  implements PyTorch C10D ProcessGroup API and can be dynamically loaded as external ProcessGroup and only works on Linux platform now"),Ye.forEach(n),Tt=h(e),j=o(e,"P",{});var Jt=l(j);bn=i(Jt,"Check more detailed information for "),te=o(Jt,"A",{href:!0,rel:!0});var ao=l(te);yn=i(ao,"oneccl_bind_pt"),ao.forEach(n),xn=i(Jt,"."),Jt.forEach(n),Pt=h(e),O=o(e,"H3",{class:!0});var Yt=l(O);B=o(Yt,"A",{id:!0,class:!0,href:!0});var io=l(B);at=o(io,"SPAN",{});var so=l(at);C(ne.$$.fragment,so),so.forEach(n),io.forEach(n),wn=h(Yt),it=o(Yt,"SPAN",{});var co=l(it);En=i(co,"Intel\xAE oneCCL Bindings for PyTorch installation:"),co.forEach(n),Yt.forEach(n),Ct=h(e),ve=o(e,"P",{});var po=l(ve);$n=i(po,"Wheel files are available for the following Python versions:"),po.forEach(n),kt=h(e),K=o(e,"TABLE",{});var Qt=l(K);st=o(Qt,"THEAD",{});var ho=l(st);u=o(ho,"TR",{});var x=l(u);ge=o(x,"TH",{align:!0});var fo=l(ge);Tn=i(fo,"Extension Version"),fo.forEach(n),Pn=h(x),be=o(x,"TH",{align:!0});var _o=l(be);Cn=i(_o,"Python 3.6"),_o.forEach(n),kn=h(x),ye=o(x,"TH",{align:!0});var uo=l(ye);Dn=i(uo,"Python 3.7"),uo.forEach(n),An=h(x),xe=o(x,"TH",{align:!0});var mo=l(xe);In=i(mo,"Python 3.8"),mo.forEach(n),Rn=h(x),we=o(x,"TH",{align:!0});var vo=l(we);Ln=i(vo,"Python 3.9"),vo.forEach(n),Mn=h(x),Ee=o(x,"TH",{align:!0});var go=l(Ee);Un=i(go,"Python 3.10"),go.forEach(n),x.forEach(n),ho.forEach(n),Nn=h(Qt),L=o(Qt,"TBODY",{});var F=l(L);m=o(F,"TR",{});var w=l(m);$e=o(w,"TD",{align:!0});var bo=l($e);On=i(bo,"1.12.100"),bo.forEach(n),Sn=h(w),ct=o(w,"TD",{align:!0}),l(ct).forEach(n),qn=h(w),Te=o(w,"TD",{align:!0});var yo=l(Te);Hn=i(yo,"\u221A"),yo.forEach(n),Wn=h(w),Pe=o(w,"TD",{align:!0});var xo=l(Pe);jn=i(xo,"\u221A"),xo.forEach(n),Bn=h(w),Ce=o(w,"TD",{align:!0});var wo=l(Ce);Kn=i(wo,"\u221A"),wo.forEach(n),Gn=h(w),ke=o(w,"TD",{align:!0});var Eo=l(ke);zn=i(Eo,"\u221A"),Eo.forEach(n),w.forEach(n),Vn=h(F),v=o(F,"TR",{});var E=l(v);De=o(E,"TD",{align:!0});var $o=l(De);Xn=i($o,"1.12.0"),$o.forEach(n),Fn=h(E),pt=o(E,"TD",{align:!0}),l(pt).forEach(n),Jn=h(E),Ae=o(E,"TD",{align:!0});var To=l(Ae);Yn=i(To,"\u221A"),To.forEach(n),Qn=h(E),Ie=o(E,"TD",{align:!0});var Po=l(Ie);Zn=i(Po,"\u221A"),Po.forEach(n),er=h(E),Re=o(E,"TD",{align:!0});var Co=l(Re);tr=i(Co,"\u221A"),Co.forEach(n),nr=h(E),Le=o(E,"TD",{align:!0});var ko=l(Le);rr=i(ko,"\u221A"),ko.forEach(n),E.forEach(n),or=h(F),g=o(F,"TR",{});var $=l(g);Me=o($,"TD",{align:!0});var Do=l(Me);lr=i(Do,"1.11.0"),Do.forEach(n),ar=h($),ht=o($,"TD",{align:!0}),l(ht).forEach(n),ir=h($),Ue=o($,"TD",{align:!0});var Ao=l(Ue);sr=i(Ao,"\u221A"),Ao.forEach(n),cr=h($),Ne=o($,"TD",{align:!0});var Io=l(Ne);pr=i(Io,"\u221A"),Io.forEach(n),hr=h($),Oe=o($,"TD",{align:!0});var Ro=l(Oe);fr=i(Ro,"\u221A"),Ro.forEach(n),dr=h($),Se=o($,"TD",{align:!0});var Lo=l(Se);_r=i(Lo,"\u221A"),Lo.forEach(n),$.forEach(n),ur=h(F),b=o(F,"TR",{});var T=l(b);qe=o(T,"TD",{align:!0});var Mo=l(qe);mr=i(Mo,"1.10.0"),Mo.forEach(n),vr=h(T),He=o(T,"TD",{align:!0});var Uo=l(He);gr=i(Uo,"\u221A"),Uo.forEach(n),br=h(T),We=o(T,"TD",{align:!0});var No=l(We);yr=i(No,"\u221A"),No.forEach(n),xr=h(T),je=o(T,"TD",{align:!0});var Oo=l(je);wr=i(Oo,"\u221A"),Oo.forEach(n),Er=h(T),Be=o(T,"TD",{align:!0});var So=l(Be);$r=i(So,"\u221A"),So.forEach(n),Tr=h(T),ft=o(T,"TD",{align:!0}),l(ft).forEach(n),T.forEach(n),F.forEach(n),Qt.forEach(n),Dt=h(e),C(re.$$.fragment,e),At=h(e),U=o(e,"P",{});var Qe=l(U);Pr=i(Qe,"where "),dt=o(Qe,"CODE",{});var qo=l(dt);Cr=i(qo,"{pytorch_version}"),qo.forEach(n),kr=i(Qe,` should be your PyTorch version, for instance 1.12.0.
Check more approaches for `),oe=o(Qe,"A",{href:!0,rel:!0});var Ho=l(oe);Dr=i(Ho,"oneccl_bind_pt installation"),Ho.forEach(n),Ar=i(Qe,`.
Versions of oneCCL and PyTorch must match.`),Qe.forEach(n),It=h(e),C(G.$$.fragment,e),Rt=h(e),S=o(e,"H2",{class:!0});var Zt=l(S);z=o(Zt,"A",{id:!0,class:!0,href:!0});var Wo=l(z);_t=o(Wo,"SPAN",{});var jo=l(_t);C(le.$$.fragment,jo),jo.forEach(n),Wo.forEach(n),Ir=h(Zt),ut=o(Zt,"SPAN",{});var Bo=l(ut);Rr=i(Bo,"Intel\xAE MPI library"),Bo.forEach(n),Zt.forEach(n),Lt=i(e,`

Use this standards-based MPI implementation to deliver flexible, efficient, scalable cluster messaging on Intel\xAE architecture. This component is part of the Intel\xAE oneAPI HPC Toolkit.
`),Ke=o(e,"P",{});var Ko=l(Ke);Lr=i(Ko,"oneccl_bindings_for_pytorch is installed along with the MPI tool set. Need to source the environment before using it."),Ko.forEach(n),Mt=h(e),Ge=o(e,"P",{});var Go=l(Ge);Mr=i(Go,"for Intel\xAE oneCCL >= 1.12.0"),Go.forEach(n),Ut=h(e),C(ae.$$.fragment,e),Nt=h(e),ze=o(e,"P",{});var zo=l(ze);Ur=i(zo,"for Intel\xAE oneCCL whose version < 1.12.0"),zo.forEach(n),Ot=h(e),C(ie.$$.fragment,e),St=h(e),Ve=o(e,"P",{});var Vo=l(Ve);Nr=i(Vo,"The following \u201CUsage in Trainer\u201D takes mpirun in Intel\xAE MPI library as an example."),Vo.forEach(n),qt=h(e),q=o(e,"H2",{class:!0});var en=l(q);V=o(en,"A",{id:!0,class:!0,href:!0});var Xo=l(V);mt=o(Xo,"SPAN",{});var Fo=l(mt);C(se.$$.fragment,Fo),Fo.forEach(n),Xo.forEach(n),Or=h(en),vt=o(en,"SPAN",{});var Jo=l(vt);Sr=i(Jo,"Usage in Trainer"),Jo.forEach(n),en.forEach(n),Ht=i(e,`

To enable multi CPU distributed training in the Trainer with the ccl backend, users should add **\`--xpu_backend ccl\`** in the command arguments.
`),ce=o(e,"P",{});var Vr=l(ce);qr=i(Vr,"Let\u2019s see an example with the "),pe=o(Vr,"A",{href:!0,rel:!0});var Yo=l(pe);Hr=i(Yo,"question-answering example"),Yo.forEach(n),Vr.forEach(n),Wt=h(e),Xe=o(e,"P",{});var Qo=l(Xe);Wr=i(Qo,"The following command enables training with 2 processes on one Xeon node, with one process running per one socket. The variables OMP_NUM_THREADS/CCL_WORKER_COUNT can be tuned for optimal performance."),Qo.forEach(n),jt=h(e),C(he.$$.fragment,e),Bt=h(e),Fe=o(e,"P",{});var Zo=l(Fe);jr=i(Zo,"The following command enables training with a total of four processes on two Xeons (node0 and node1, taking node0 as the main process), ppn (processes per node) is set to 2, with one process running per one socket. The variables OMP_NUM_THREADS/CCL_WORKER_COUNT can be tuned for optimal performance."),Zo.forEach(n),Kt=h(e),Je=o(e,"P",{});var el=l(Je);Br=i(el,"In node0, you need to create a configuration file which contains the IP addresses of each node (for example hostfile) and pass that configuration file path as an argument."),el.forEach(n),Gt=h(e),C(fe.$$.fragment,e),zt=h(e),X=o(e,"P",{});var tn=l(X);Kr=i(tn,"Now, run the following command in node0 and "),gt=o(tn,"STRONG",{});var tl=l(gt);Gr=i(tl,"4DDP"),tl.forEach(n),zr=i(tn," will be enabled in node0 and node1:"),tn.forEach(n),Vt=h(e),C(de.$$.fragment,e),this.h()},h(){s(d,"name","hf:doc:metadata"),s(d,"content",JSON.stringify(pl)),s(y,"id","efficient-training-on-multiple-cpus"),s(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(y,"href","#efficient-training-on-multiple-cpus"),s(_,"class","relative group"),s(W,"id","intel-oneccl-bindings-for-pytorch"),s(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(W,"href","#intel-oneccl-bindings-for-pytorch"),s(N,"class","relative group"),s(Q,"href","https://github.com/oneapi-src/oneCCL"),s(Q,"rel","nofollow"),s(Z,"href","https://spec.oneapi.com/versions/latest/elements/oneCCL/source/index.html"),s(Z,"rel","nofollow"),s(ee,"href","https://spec.oneapi.com/versions/latest/elements/oneCCL/source/index.html"),s(ee,"rel","nofollow"),s(te,"href","https://github.com/intel/torch-ccl"),s(te,"rel","nofollow"),s(B,"id","intel-oneccl-bindings-for-pytorch-installation"),s(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(B,"href","#intel-oneccl-bindings-for-pytorch-installation"),s(O,"class","relative group"),s(ge,"align","center"),s(be,"align","center"),s(ye,"align","center"),s(xe,"align","center"),s(we,"align","center"),s(Ee,"align","center"),s($e,"align","center"),s(ct,"align","center"),s(Te,"align","center"),s(Pe,"align","center"),s(Ce,"align","center"),s(ke,"align","center"),s(De,"align","center"),s(pt,"align","center"),s(Ae,"align","center"),s(Ie,"align","center"),s(Re,"align","center"),s(Le,"align","center"),s(Me,"align","center"),s(ht,"align","center"),s(Ue,"align","center"),s(Ne,"align","center"),s(Oe,"align","center"),s(Se,"align","center"),s(qe,"align","center"),s(He,"align","center"),s(We,"align","center"),s(je,"align","center"),s(Be,"align","center"),s(ft,"align","center"),s(oe,"href","https://github.com/intel/torch-ccl"),s(oe,"rel","nofollow"),s(z,"id","intel-mpi-library"),s(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(z,"href","#intel-mpi-library"),s(S,"class","relative group"),s(V,"id","usage-in-trainer"),s(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),s(V,"href","#usage-in-trainer"),s(q,"class","relative group"),s(pe,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering"),s(pe,"rel","nofollow")},m(e,c){t(document.head,d),f(e,H,c),f(e,_,c),t(_,y),t(y,et),k(J,et,null),t(_,nn),t(_,tt),t(tt,rn),f(e,xt,c),f(e,me,c),t(me,on),f(e,wt,c),f(e,N,c),t(N,W),t(W,nt),k(Y,nt,null),t(N,ln),t(N,rt),t(rt,an),f(e,Et,c),f(e,R,c),t(R,Q),t(Q,sn),t(R,cn),t(R,Z),t(Z,pn),t(R,hn),t(R,ee),t(ee,fn),t(R,dn),f(e,$t,c),f(e,M,c),t(M,_n),t(M,ot),t(ot,un),t(M,mn),t(M,lt),t(lt,vn),t(M,gn),f(e,Tt,c),f(e,j,c),t(j,bn),t(j,te),t(te,yn),t(j,xn),f(e,Pt,c),f(e,O,c),t(O,B),t(B,at),k(ne,at,null),t(O,wn),t(O,it),t(it,En),f(e,Ct,c),f(e,ve,c),t(ve,$n),f(e,kt,c),f(e,K,c),t(K,st),t(st,u),t(u,ge),t(ge,Tn),t(u,Pn),t(u,be),t(be,Cn),t(u,kn),t(u,ye),t(ye,Dn),t(u,An),t(u,xe),t(xe,In),t(u,Rn),t(u,we),t(we,Ln),t(u,Mn),t(u,Ee),t(Ee,Un),t(K,Nn),t(K,L),t(L,m),t(m,$e),t($e,On),t(m,Sn),t(m,ct),t(m,qn),t(m,Te),t(Te,Hn),t(m,Wn),t(m,Pe),t(Pe,jn),t(m,Bn),t(m,Ce),t(Ce,Kn),t(m,Gn),t(m,ke),t(ke,zn),t(L,Vn),t(L,v),t(v,De),t(De,Xn),t(v,Fn),t(v,pt),t(v,Jn),t(v,Ae),t(Ae,Yn),t(v,Qn),t(v,Ie),t(Ie,Zn),t(v,er),t(v,Re),t(Re,tr),t(v,nr),t(v,Le),t(Le,rr),t(L,or),t(L,g),t(g,Me),t(Me,lr),t(g,ar),t(g,ht),t(g,ir),t(g,Ue),t(Ue,sr),t(g,cr),t(g,Ne),t(Ne,pr),t(g,hr),t(g,Oe),t(Oe,fr),t(g,dr),t(g,Se),t(Se,_r),t(L,ur),t(L,b),t(b,qe),t(qe,mr),t(b,vr),t(b,He),t(He,gr),t(b,br),t(b,We),t(We,yr),t(b,xr),t(b,je),t(je,wr),t(b,Er),t(b,Be),t(Be,$r),t(b,Tr),t(b,ft),f(e,Dt,c),k(re,e,c),f(e,At,c),f(e,U,c),t(U,Pr),t(U,dt),t(dt,Cr),t(U,kr),t(U,oe),t(oe,Dr),t(U,Ar),f(e,It,c),k(G,e,c),f(e,Rt,c),f(e,S,c),t(S,z),t(z,_t),k(le,_t,null),t(S,Ir),t(S,ut),t(ut,Rr),f(e,Lt,c),f(e,Ke,c),t(Ke,Lr),f(e,Mt,c),f(e,Ge,c),t(Ge,Mr),f(e,Ut,c),k(ae,e,c),f(e,Nt,c),f(e,ze,c),t(ze,Ur),f(e,Ot,c),k(ie,e,c),f(e,St,c),f(e,Ve,c),t(Ve,Nr),f(e,qt,c),f(e,q,c),t(q,V),t(V,mt),k(se,mt,null),t(q,Or),t(q,vt),t(vt,Sr),f(e,Ht,c),f(e,ce,c),t(ce,qr),t(ce,pe),t(pe,Hr),f(e,Wt,c),f(e,Xe,c),t(Xe,Wr),f(e,jt,c),k(he,e,c),f(e,Bt,c),f(e,Fe,c),t(Fe,jr),f(e,Kt,c),f(e,Je,c),t(Je,Br),f(e,Gt,c),k(fe,e,c),f(e,zt,c),f(e,X,c),t(X,Kr),t(X,gt),t(gt,Gr),t(X,zr),f(e,Vt,c),k(de,e,c),Xt=!0},p(e,[c]){const _e={};c&2&&(_e.$$scope={dirty:c,ctx:e}),G.$set(_e)},i(e){Xt||(D(J.$$.fragment,e),D(Y.$$.fragment,e),D(ne.$$.fragment,e),D(re.$$.fragment,e),D(G.$$.fragment,e),D(le.$$.fragment,e),D(ae.$$.fragment,e),D(ie.$$.fragment,e),D(se.$$.fragment,e),D(he.$$.fragment,e),D(fe.$$.fragment,e),D(de.$$.fragment,e),Xt=!0)},o(e){A(J.$$.fragment,e),A(Y.$$.fragment,e),A(ne.$$.fragment,e),A(re.$$.fragment,e),A(G.$$.fragment,e),A(le.$$.fragment,e),A(ae.$$.fragment,e),A(ie.$$.fragment,e),A(se.$$.fragment,e),A(he.$$.fragment,e),A(fe.$$.fragment,e),A(de.$$.fragment,e),Xt=!1},d(e){n(d),e&&n(H),e&&n(_),I(J),e&&n(xt),e&&n(me),e&&n(wt),e&&n(N),I(Y),e&&n(Et),e&&n(R),e&&n($t),e&&n(M),e&&n(Tt),e&&n(j),e&&n(Pt),e&&n(O),I(ne),e&&n(Ct),e&&n(ve),e&&n(kt),e&&n(K),e&&n(Dt),I(re,e),e&&n(At),e&&n(U),e&&n(It),I(G,e),e&&n(Rt),e&&n(S),I(le),e&&n(Lt),e&&n(Ke),e&&n(Mt),e&&n(Ge),e&&n(Ut),I(ae,e),e&&n(Nt),e&&n(ze),e&&n(Ot),I(ie,e),e&&n(St),e&&n(Ve),e&&n(qt),e&&n(q),I(se),e&&n(Ht),e&&n(ce),e&&n(Wt),e&&n(Xe),e&&n(jt),I(he,e),e&&n(Bt),e&&n(Fe),e&&n(Kt),e&&n(Je),e&&n(Gt),I(fe,e),e&&n(zt),e&&n(X),e&&n(Vt),I(de,e)}}}const pl={local:"efficient-training-on-multiple-cpus",sections:[{local:"intel-oneccl-bindings-for-pytorch",sections:[{local:"intel-oneccl-bindings-for-pytorch-installation",title:"Intel\xAE oneCCL Bindings for PyTorch installation:"}],title:"Intel\xAE oneCCL Bindings for PyTorch"},{local:"intel-mpi-library",title:"Intel\xAE MPI library"},{local:"usage-in-trainer",title:"Usage in Trainer"}],title:"Efficient Training on Multiple CPUs"};function hl(yt){return al(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ml extends nl{constructor(d){super();rl(this,d,hl,cl,ol,{})}}export{ml as default,pl as metadata};
