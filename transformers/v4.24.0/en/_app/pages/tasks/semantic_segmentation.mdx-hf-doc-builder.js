import{S as fr,i as ur,s as dr,e as o,k as m,w as u,t as l,M as gr,c as i,d as s,m as c,a as p,x as d,h as n,b as h,N as cr,G as a,g as r,y as g,q as _,o as b,B as v,v as _r}from"../../chunks/vendor-hf-doc-builder.js";import{T as hr}from"../../chunks/Tip-hf-doc-builder.js";import{Y as br}from"../../chunks/Youtube-hf-doc-builder.js";import{I as Js}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as y}from"../../chunks/CodeBlock-hf-doc-builder.js";import{D as vr}from"../../chunks/DocNotebookDropdown-hf-doc-builder.js";function jr(Je){let f,S,j,w,T;return{c(){f=o("p"),S=l("See the image segmentation "),j=o("a"),w=l("task page"),T=l(" for more information about its associated models, datasets, and metrics."),this.h()},l($){f=i($,"P",{});var P=p(f);S=n(P,"See the image segmentation "),j=i(P,"A",{href:!0,rel:!0});var A=p(j);w=n(A,"task page"),A.forEach(s),T=n(P," for more information about its associated models, datasets, and metrics."),P.forEach(s),this.h()},h(){h(j,"href","https://huggingface.co/tasks/image-segmentation"),h(j,"rel","nofollow")},m($,P){r($,f,P),a(f,S),a(f,j),a(j,w),a(f,T)},d($){$&&s(f)}}}function $r(Je){let f,S,j,w,T,$,P,A;return{c(){f=o("p"),S=l("If you aren\u2019t familiar with finetuning a model with the "),j=o("a"),w=l("Trainer"),T=l(", take a look at the basic tutorial "),$=o("a"),P=l("here"),A=l("!"),this.h()},l(G){f=i(G,"P",{});var q=p(f);S=n(q,"If you aren\u2019t familiar with finetuning a model with the "),j=i(q,"A",{href:!0});var I=p(j);w=n(I,"Trainer"),I.forEach(s),T=n(q,", take a look at the basic tutorial "),$=i(q,"A",{href:!0});var oe=p($);P=n(oe,"here"),oe.forEach(s),A=n(q,"!"),q.forEach(s),this.h()},h(){h(j,"href","/docs/transformers/v4.24.0/en/main_classes/trainer#transformers.Trainer"),h($,"href","../training#finetune-with-trainer")},m(G,q){r(G,f,q),a(f,S),a(f,j),a(j,w),a(f,T),a(f,$),a($,P),a(f,A)},d(G){G&&s(f)}}}function yr(Je){let f,S,j,w,T,$,P,A,G,q,I,oe,ie,Rs,Re,nt,Ys,z,rt,pe,ot,it,me,pt,mt,Vs,Y,Ws,Ye,ct,Ks,ce,Qs,B,V,fs,he,ht,us,ft,Xs,Ve,ut,Zs,fe,ea,We,dt,sa,ue,aa,Ke,gt,ta,de,la,k,_t,ds,bt,vt,gs,jt,$t,_s,yt,wt,bs,kt,Et,vs,xt,Pt,na,O,Tt,js,qt,St,$s,At,Ft,ra,ge,oa,H,W,ys,_e,Ct,ws,Dt,ia,L,It,ks,zt,Ot,Es,Lt,Nt,pa,be,ma,N,Mt,ve,xs,Ut,Gt,je,Bt,Ht,ca,$e,ha,E,Jt,Ps,Rt,Yt,Ts,Vt,Wt,qs,Kt,Qt,Ss,Xt,Zt,As,el,sl,fa,ye,ua,M,al,Fs,tl,ll,we,nl,rl,da,ke,ga,J,K,Cs,Ee,ol,Ds,il,_a,Q,pl,Qe,ml,cl,ba,xe,va,X,ja,x,hl,Xe,fl,ul,Is,dl,gl,zs,_l,bl,Os,vl,jl,Ls,$l,yl,$a,Z,wl,Ns,kl,El,ya,Pe,wa,ee,xl,Te,Pl,Tl,ka,Ze,ql,Ea,qe,xa,U,Sl,Se,Al,Fl,Ae,Cl,Dl,Pa,Fe,Ta,se,Il,es,zl,Ol,qa,Ce,Sa,ae,Ll,ss,Nl,Ml,Aa,De,Fa,R,te,Ms,Ie,Ul,Us,Gl,Ca,as,Bl,Da,ts,Hl,Ia,ze,za,Oe,ls,sn,Oa,le,Jl,Gs,Rl,Yl,La,Le,Na,ne,Vl,Bs,Wl,Kl,Ma,Ne,Ua,ns,Ql,Ga,Me,Ba,re,Xl,Ue,Zl,en,Ha,Ge,Ja,Be,rs,an,Ra;return $=new Js({}),I=new vr({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/semantic_segmentation.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/semantic_segmentation.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/semantic_segmentation.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/semantic_segmentation.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/semantic_segmentation.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/semantic_segmentation.ipynb"}]}}),ie=new br({props:{id:"dKE8SIt9C-w"}}),Y=new hr({props:{$$slots:{default:[jr]},$$scope:{ctx:Je}}}),ce=new y({props:{code:"pip install -q datasets transformers evaluate",highlighted:"pip install -q datasets transformers evaluate"}}),he=new Js({}),fe=new y({props:{code:`from datasets import load_dataset

ds = load_dataset("scene_parse_150", split="train[:50]")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&quot;scene_parse_150&quot;</span>, split=<span class="hljs-string">&quot;train[:50]&quot;</span>)`}}),ue=new y({props:{code:`ds = ds.train_test_split(test_size=0.2)
train_ds = ds["train"]
test_ds = ds["test"]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.train_test_split(test_size=<span class="hljs-number">0.2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_ds = ds[<span class="hljs-string">&quot;train&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>test_ds = ds[<span class="hljs-string">&quot;test&quot;</span>]`}}),de=new y({props:{code:"train_ds[0]",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>train_ds[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;image&#x27;</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x683 at <span class="hljs-number">0x7F9B0C201F90</span>&gt;,
 <span class="hljs-string">&#x27;annotation&#x27;</span>: &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=512x683 at <span class="hljs-number">0x7F9B0C201DD0</span>&gt;,
 <span class="hljs-string">&#x27;scene_category&#x27;</span>: <span class="hljs-number">368</span>}`}}),ge=new y({props:{code:`import json
from huggingface_hub import cached_download, hf_hub_url

repo_id = "huggingface/label-files"
filename = "ade20k-hf-doc-builder.json"
id2label = json.load(open(cached_download(hf_hub_url(repo_id, filename, repo_type="dataset")), "r"))
id2label = {int(k): v for k, v in id2label.items()}
label2id = {v: k for k, v in id2label.items()}
num_labels = len(id2label)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> json
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> cached_download, hf_hub_url

<span class="hljs-meta">&gt;&gt;&gt; </span>repo_id = <span class="hljs-string">&quot;huggingface/label-files&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>filename = <span class="hljs-string">&quot;ade20k-hf-doc-builder.json&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>id2label = json.load(<span class="hljs-built_in">open</span>(cached_download(hf_hub_url(repo_id, filename, repo_type=<span class="hljs-string">&quot;dataset&quot;</span>)), <span class="hljs-string">&quot;r&quot;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>id2label = {<span class="hljs-built_in">int</span>(k): v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> id2label.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>label2id = {v: k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> id2label.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(id2label)`}}),_e=new Js({}),be=new y({props:{code:`from transformers import AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained("nvidia/mit-b0", reduce_labels=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;nvidia/mit-b0&quot;</span>, reduce_labels=<span class="hljs-literal">True</span>)`}}),$e=new y({props:{code:`from torchvision.transforms import ColorJitter

jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ColorJitter

<span class="hljs-meta">&gt;&gt;&gt; </span>jitter = ColorJitter(brightness=<span class="hljs-number">0.25</span>, contrast=<span class="hljs-number">0.25</span>, saturation=<span class="hljs-number">0.25</span>, hue=<span class="hljs-number">0.1</span>)`}}),ye=new y({props:{code:`def train_transforms(example_batch):
    images = [jitter(x) for x in example_batch["image"]]
    labels = [x for x in example_batch["annotation"]]
    inputs = feature_extractor(images, labels)
    return inputs


def val_transforms(example_batch):
    images = [x for x in example_batch["image"]]
    labels = [x for x in example_batch["annotation"]]
    inputs = feature_extractor(images, labels)
    return inputs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_transforms</span>(<span class="hljs-params">example_batch</span>):
<span class="hljs-meta">... </span>    images = [jitter(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> example_batch[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    labels = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> example_batch[<span class="hljs-string">&quot;annotation&quot;</span>]]
<span class="hljs-meta">... </span>    inputs = feature_extractor(images, labels)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> inputs


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">val_transforms</span>(<span class="hljs-params">example_batch</span>):
<span class="hljs-meta">... </span>    images = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> example_batch[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    labels = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> example_batch[<span class="hljs-string">&quot;annotation&quot;</span>]]
<span class="hljs-meta">... </span>    inputs = feature_extractor(images, labels)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> inputs`}}),ke=new y({props:{code:`train_ds.set_transform(train_transforms)
test_ds.set_transform(val_transforms)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>train_ds.set_transform(train_transforms)
<span class="hljs-meta">&gt;&gt;&gt; </span>test_ds.set_transform(val_transforms)`}}),Ee=new Js({}),xe=new y({props:{code:`from transformers import AutoModelForSemanticSegmentation

pretrained_model_name = "nvidia/mit-b0"
model = AutoModelForSemanticSegmentation.from_pretrained(
    pretrained_model_name, id2label=id2label, label2id=label2id
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span>pretrained_model_name = <span class="hljs-string">&quot;nvidia/mit-b0&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    pretrained_model_name, id2label=id2label, label2id=label2id
<span class="hljs-meta">... </span>)`}}),X=new hr({props:{$$slots:{default:[$r]},$$scope:{ctx:Je}}}),Pe=new y({props:{code:`from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="segformer-b0-scene-parse-150",
    learning_rate=6e-5,
    num_train_epochs=50,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    save_total_limit=3,
    evaluation_strategy="steps",
    save_strategy="steps",
    save_steps=20,
    eval_steps=20,
    logging_steps=1,
    eval_accumulation_steps=5,
    remove_unused_columns=False,
    push_to_hub=True,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments

<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;segformer-b0-scene-parse-150&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">6e-5</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">50</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>    save_total_limit=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;steps&quot;</span>,
<span class="hljs-meta">... </span>    save_strategy=<span class="hljs-string">&quot;steps&quot;</span>,
<span class="hljs-meta">... </span>    save_steps=<span class="hljs-number">20</span>,
<span class="hljs-meta">... </span>    eval_steps=<span class="hljs-number">20</span>,
<span class="hljs-meta">... </span>    logging_steps=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    eval_accumulation_steps=<span class="hljs-number">5</span>,
<span class="hljs-meta">... </span>    remove_unused_columns=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    push_to_hub=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>)`}}),qe=new y({props:{code:`import evaluate

metric = evaluate.load("mean_iou")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate

<span class="hljs-meta">&gt;&gt;&gt; </span>metric = evaluate.load(<span class="hljs-string">&quot;mean_iou&quot;</span>)`}}),Fe=new y({props:{code:`def compute_metrics(eval_pred):
    with torch.no_grad():
        logits, labels = eval_pred
        logits_tensor = torch.from_numpy(logits)
        logits_tensor = nn.functional.interpolate(
            logits_tensor,
            size=labels.shape[-2:],
            mode="bilinear",
            align_corners=False,
        ).argmax(dim=1)

        pred_labels = logits_tensor.detach().cpu().numpy()
        metrics = metric.compute(
            predictions=pred_labels,
            references=labels,
            num_labels=num_labels,
            ignore_index=255,
            reduce_labels=False,
        )
        for key, value in metrics.items():
            if type(value) is np.ndarray:
                metrics[key] = value.tolist()
        return metrics`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>        logits, labels = eval_pred
<span class="hljs-meta">... </span>        logits_tensor = torch.from_numpy(logits)
<span class="hljs-meta">... </span>        logits_tensor = nn.functional.interpolate(
<span class="hljs-meta">... </span>            logits_tensor,
<span class="hljs-meta">... </span>            size=labels.shape[-<span class="hljs-number">2</span>:],
<span class="hljs-meta">... </span>            mode=<span class="hljs-string">&quot;bilinear&quot;</span>,
<span class="hljs-meta">... </span>            align_corners=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>        ).argmax(dim=<span class="hljs-number">1</span>)

<span class="hljs-meta">... </span>        pred_labels = logits_tensor.detach().cpu().numpy()
<span class="hljs-meta">... </span>        metrics = metric.compute(
<span class="hljs-meta">... </span>            predictions=pred_labels,
<span class="hljs-meta">... </span>            references=labels,
<span class="hljs-meta">... </span>            num_labels=num_labels,
<span class="hljs-meta">... </span>            ignore_index=<span class="hljs-number">255</span>,
<span class="hljs-meta">... </span>            reduce_labels=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>        )
<span class="hljs-meta">... </span>        <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> metrics.items():
<span class="hljs-meta">... </span>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(value) <span class="hljs-keyword">is</span> np.ndarray:
<span class="hljs-meta">... </span>                metrics[key] = value.tolist()
<span class="hljs-meta">... </span>        <span class="hljs-keyword">return</span> metrics`}}),Ce=new y({props:{code:`from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    compute_metrics=compute_metrics,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=train_ds,
<span class="hljs-meta">... </span>    eval_dataset=test_ds,
<span class="hljs-meta">... </span>    compute_metrics=compute_metrics,
<span class="hljs-meta">... </span>)`}}),De=new y({props:{code:"trainer.train()",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()'}}),Ie=new Js({}),ze=new y({props:{code:`image = ds[0]["image"]
image`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>image = ds[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;image&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>image`}}),Le=new y({props:{code:`device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # use GPU if available, otherwise use a CPU
encoding = feature_extractor(image, return_tensors="pt")
pixel_values = encoding.pixel_values.to(device)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)  <span class="hljs-comment"># use GPU if available, otherwise use a CPU</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = encoding.pixel_values.to(device)`}}),Ne=new y({props:{code:`outputs = model(pixel_values=pixel_values)
logits = outputs.logits.cpu()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(pixel_values=pixel_values)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits.cpu()`}}),Me=new y({props:{code:`upsampled_logits = nn.functional.interpolate(
    logits,
    size=image.size[::-1],
    mode="bilinear",
    align_corners=False,
)

pred_seg = upsampled_logits.argmax(dim=1)[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>upsampled_logits = nn.functional.interpolate(
<span class="hljs-meta">... </span>    logits,
<span class="hljs-meta">... </span>    size=image.size[::-<span class="hljs-number">1</span>],
<span class="hljs-meta">... </span>    mode=<span class="hljs-string">&quot;bilinear&quot;</span>,
<span class="hljs-meta">... </span>    align_corners=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>pred_seg = upsampled_logits.argmax(dim=<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]`}}),Ge=new y({props:{code:`import matplotlib.pyplot as plt

color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)
palette = np.array(ade_palette())
for label, color in enumerate(palette):
    color_seg[pred_seg == label, :] = color
color_seg = color_seg[..., ::-1]  # convert to BGR

img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map
img = img.astype(np.uint8)

plt.figure(figsize=(15, 10))
plt.imshow(img)
plt.show()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

<span class="hljs-meta">&gt;&gt;&gt; </span>color_seg = np.zeros((pred_seg.shape[<span class="hljs-number">0</span>], pred_seg.shape[<span class="hljs-number">1</span>], <span class="hljs-number">3</span>), dtype=np.uint8)
<span class="hljs-meta">&gt;&gt;&gt; </span>palette = np.array(ade_palette())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> label, color <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(palette):
<span class="hljs-meta">... </span>    color_seg[pred_seg == label, :] = color
<span class="hljs-meta">&gt;&gt;&gt; </span>color_seg = color_seg[..., ::-<span class="hljs-number">1</span>]  <span class="hljs-comment"># convert to BGR</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>img = np.array(image) * <span class="hljs-number">0.5</span> + color_seg * <span class="hljs-number">0.5</span>  <span class="hljs-comment"># plot the image with the segmentation map</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>img = img.astype(np.uint8)

<span class="hljs-meta">&gt;&gt;&gt; </span>plt.figure(figsize=(<span class="hljs-number">15</span>, <span class="hljs-number">10</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>plt.imshow(img)
<span class="hljs-meta">&gt;&gt;&gt; </span>plt.show()`}}),{c(){f=o("meta"),S=m(),j=o("h1"),w=o("a"),T=o("span"),u($.$$.fragment),P=m(),A=o("span"),G=l("Semantic segmentation"),q=m(),u(I.$$.fragment),oe=m(),u(ie.$$.fragment),Rs=m(),Re=o("p"),nt=l("Semantic segmentation assigns a label or class to each individual pixel of an image. There are several types of segmentation, and in the case of semantic segmentation, no distinction is made between unique instances of the same object. Both objects are given the same label (for example, \u201Ccar\u201D instead of \u201Ccar-1\u201D and \u201Ccar-2\u201D). Common real-world applications of semantic segmentation include training self-driving cars to identify pedestrians and important traffic information, identifying cells and abnormalities in medical imagery, and monitoring environmental changes from satellite imagery."),Ys=m(),z=o("p"),rt=l("This guide will show you how to finetune "),pe=o("a"),ot=l("SegFormer"),it=l(" on the "),me=o("a"),pt=l("SceneParse150"),mt=l(" dataset."),Vs=m(),u(Y.$$.fragment),Ws=m(),Ye=o("p"),ct=l("Before you begin, make sure you have all the necessary libraries installed:"),Ks=m(),u(ce.$$.fragment),Qs=m(),B=o("h2"),V=o("a"),fs=o("span"),u(he.$$.fragment),ht=m(),us=o("span"),ft=l("Load SceneParse150 dataset"),Xs=m(),Ve=o("p"),ut=l("Load the first 50 examples of the SceneParse150 dataset from the \u{1F917} Datasets library so you can quickly train and test a model:"),Zs=m(),u(fe.$$.fragment),ea=m(),We=o("p"),dt=l("Split this dataset into a train and test set:"),sa=m(),u(ue.$$.fragment),aa=m(),Ke=o("p"),gt=l("Then take a look at an example:"),ta=m(),u(de.$$.fragment),la=m(),k=o("p"),_t=l("There is an "),ds=o("code"),bt=l("image"),vt=l(", an "),gs=o("code"),jt=l("annotation"),$t=l(" (this is the segmentation map or label), and a "),_s=o("code"),yt=l("scene_category"),wt=l(" field that describes the image scene, like \u201Ckitchen\u201D or \u201Coffice\u201D. In this guide, you\u2019ll only need "),bs=o("code"),kt=l("image"),Et=l(" and "),vs=o("code"),xt=l("annotation"),Pt=l(", both of which are PIL images."),na=m(),O=o("p"),Tt=l("You\u2019ll also want to create a dictionary that maps a label id to a label class which will be useful when you set up the model later. Download the mappings from the Hub and create the "),js=o("code"),qt=l("id2label"),St=l(" and "),$s=o("code"),At=l("label2id"),Ft=l(" dictionaries:"),ra=m(),u(ge.$$.fragment),oa=m(),H=o("h2"),W=o("a"),ys=o("span"),u(_e.$$.fragment),Ct=m(),ws=o("span"),Dt=l("Preprocess"),ia=m(),L=o("p"),It=l("Next, load a SegFormer feature extractor to prepare the images and annotations for the model. Some datasets, like this one, use the zero-index as the background class. However, the background class isn\u2019t included in the 150 classes, so you\u2019ll need to set "),ks=o("code"),zt=l("reduce_labels=True"),Ot=l(" to subtract one from all the labels. The zero-index is replaced by "),Es=o("code"),Lt=l("255"),Nt=l(" so it\u2019s ignored by SegFormer\u2019s loss function:"),pa=m(),u(be.$$.fragment),ma=m(),N=o("p"),Mt=l("It is common to apply some data augmentations to an image dataset to make a model more robust against overfitting. In this guide, you\u2019ll use the "),ve=o("a"),xs=o("code"),Ut=l("ColorJitter"),Gt=l(" function from "),je=o("a"),Bt=l("torchvision"),Ht=l(" to randomly change the color properties of an image:"),ca=m(),u($e.$$.fragment),ha=m(),E=o("p"),Jt=l("Now create two preprocessing functions to prepare the images and annotations for the model. These functions convert the images into "),Ps=o("code"),Rt=l("pixel_values"),Yt=l(" and annotations to "),Ts=o("code"),Vt=l("labels"),Wt=l(". For the training set, "),qs=o("code"),Kt=l("jitter"),Qt=l(" is applied before providing the images to the feature extractor. For the test set, the feature extractor crops and normalizes the "),Ss=o("code"),Xt=l("images"),Zt=l(", and only crops the "),As=o("code"),el=l("labels"),sl=l(" because no data augmentation is applied during testing."),fa=m(),u(ye.$$.fragment),ua=m(),M=o("p"),al=l("To apply the "),Fs=o("code"),tl=l("jitter"),ll=l(" over the entire dataset, use the \u{1F917} Datasets "),we=o("a"),nl=l("set_transform"),rl=l(" function. The transform is applied on the fly which is faster and consumes less disk space:"),da=m(),u(ke.$$.fragment),ga=m(),J=o("h2"),K=o("a"),Cs=o("span"),u(Ee.$$.fragment),ol=m(),Ds=o("span"),il=l("Train"),_a=m(),Q=o("p"),pl=l("Load SegFormer with "),Qe=o("a"),ml=l("AutoModelForSemanticSegmentation"),cl=l(", and pass the model the mapping between label ids and label classes:"),ba=m(),u(xe.$$.fragment),va=m(),u(X.$$.fragment),ja=m(),x=o("p"),hl=l("Define your training hyperparameters in "),Xe=o("a"),fl=l("TrainingArguments"),ul=l(". It is important not to remove unused columns because this will drop the "),Is=o("code"),dl=l("image"),gl=l(" column. Without the "),zs=o("code"),_l=l("image"),bl=l(" column, you can\u2019t create "),Os=o("code"),vl=l("pixel_values"),jl=l(". Set "),Ls=o("code"),$l=l("remove_unused_columns=False"),yl=l(" to prevent this behavior!"),$a=m(),Z=o("p"),wl=l("To save and push a model under your namespace to the Hub, set "),Ns=o("code"),kl=l("push_to_hub=True"),El=l(":"),ya=m(),u(Pe.$$.fragment),wa=m(),ee=o("p"),xl=l("To evaluate model performance during training, you\u2019ll need to create a function to compute and report metrics. For semantic segmentation, you\u2019ll typically compute the "),Te=o("a"),Pl=l("mean Intersection over Union"),Tl=l(" (IoU). The mean IoU measures the overlapping area between the predicted and ground truth segmentation maps."),ka=m(),Ze=o("p"),ql=l("Load the mean IoU from the \u{1F917} Evaluate library:"),Ea=m(),u(qe.$$.fragment),xa=m(),U=o("p"),Sl=l("Then create a function to "),Se=o("a"),Al=l("compute"),Fl=l(" the metrics. Your predictions need to be converted to logits first, and then reshaped to match the size of the labels before you can call "),Ae=o("a"),Cl=l("compute"),Dl=l(":"),Pa=m(),u(Fe.$$.fragment),Ta=m(),se=o("p"),Il=l("Pass your model, training arguments, datasets, and metrics function to the "),es=o("a"),zl=l("Trainer"),Ol=l(":"),qa=m(),u(Ce.$$.fragment),Sa=m(),ae=o("p"),Ll=l("Lastly, call "),ss=o("a"),Nl=l("train()"),Ml=l(" to finetune your model:"),Aa=m(),u(De.$$.fragment),Fa=m(),R=o("h2"),te=o("a"),Ms=o("span"),u(Ie.$$.fragment),Ul=m(),Us=o("span"),Gl=l("Inference"),Ca=m(),as=o("p"),Bl=l("Great, now that you\u2019ve finetuned a model, you can use it for inference!"),Da=m(),ts=o("p"),Hl=l("Load an image for inference:"),Ia=m(),u(ze.$$.fragment),za=m(),Oe=o("div"),ls=o("img"),Oa=m(),le=o("p"),Jl=l("Process the image with a feature extractor and place the "),Gs=o("code"),Rl=l("pixel_values"),Yl=l(" on a GPU:"),La=m(),u(Le.$$.fragment),Na=m(),ne=o("p"),Vl=l("Pass your input to the model and return the "),Bs=o("code"),Wl=l("logits"),Kl=l(":"),Ma=m(),u(Ne.$$.fragment),Ua=m(),ns=o("p"),Ql=l("Next, rescale the logits to the original image size:"),Ga=m(),u(Me.$$.fragment),Ba=m(),re=o("p"),Xl=l("To visualize the results, load the "),Ue=o("a"),Zl=l("dataset color palette"),en=l(" that maps each class to their RGB values. Then you can combine and plot your image and the predicted segmentation map:"),Ha=m(),u(Ge.$$.fragment),Ja=m(),Be=o("div"),rs=o("img"),this.h()},l(e){const t=gr('[data-svelte="svelte-1phssyn"]',document.head);f=i(t,"META",{name:!0,content:!0}),t.forEach(s),S=c(e),j=i(e,"H1",{class:!0});var He=p(j);w=i(He,"A",{id:!0,class:!0,href:!0});var Hs=p(w);T=i(Hs,"SPAN",{});var tn=p(T);d($.$$.fragment,tn),tn.forEach(s),Hs.forEach(s),P=c(He),A=i(He,"SPAN",{});var ln=p(A);G=n(ln,"Semantic segmentation"),ln.forEach(s),He.forEach(s),q=c(e),d(I.$$.fragment,e),oe=c(e),d(ie.$$.fragment,e),Rs=c(e),Re=i(e,"P",{});var nn=p(Re);nt=n(nn,"Semantic segmentation assigns a label or class to each individual pixel of an image. There are several types of segmentation, and in the case of semantic segmentation, no distinction is made between unique instances of the same object. Both objects are given the same label (for example, \u201Ccar\u201D instead of \u201Ccar-1\u201D and \u201Ccar-2\u201D). Common real-world applications of semantic segmentation include training self-driving cars to identify pedestrians and important traffic information, identifying cells and abnormalities in medical imagery, and monitoring environmental changes from satellite imagery."),nn.forEach(s),Ys=c(e),z=i(e,"P",{});var os=p(z);rt=n(os,"This guide will show you how to finetune "),pe=i(os,"A",{href:!0,rel:!0});var rn=p(pe);ot=n(rn,"SegFormer"),rn.forEach(s),it=n(os," on the "),me=i(os,"A",{href:!0,rel:!0});var on=p(me);pt=n(on,"SceneParse150"),on.forEach(s),mt=n(os," dataset."),os.forEach(s),Vs=c(e),d(Y.$$.fragment,e),Ws=c(e),Ye=i(e,"P",{});var pn=p(Ye);ct=n(pn,"Before you begin, make sure you have all the necessary libraries installed:"),pn.forEach(s),Ks=c(e),d(ce.$$.fragment,e),Qs=c(e),B=i(e,"H2",{class:!0});var Ya=p(B);V=i(Ya,"A",{id:!0,class:!0,href:!0});var mn=p(V);fs=i(mn,"SPAN",{});var cn=p(fs);d(he.$$.fragment,cn),cn.forEach(s),mn.forEach(s),ht=c(Ya),us=i(Ya,"SPAN",{});var hn=p(us);ft=n(hn,"Load SceneParse150 dataset"),hn.forEach(s),Ya.forEach(s),Xs=c(e),Ve=i(e,"P",{});var fn=p(Ve);ut=n(fn,"Load the first 50 examples of the SceneParse150 dataset from the \u{1F917} Datasets library so you can quickly train and test a model:"),fn.forEach(s),Zs=c(e),d(fe.$$.fragment,e),ea=c(e),We=i(e,"P",{});var un=p(We);dt=n(un,"Split this dataset into a train and test set:"),un.forEach(s),sa=c(e),d(ue.$$.fragment,e),aa=c(e),Ke=i(e,"P",{});var dn=p(Ke);gt=n(dn,"Then take a look at an example:"),dn.forEach(s),ta=c(e),d(de.$$.fragment,e),la=c(e),k=i(e,"P",{});var F=p(k);_t=n(F,"There is an "),ds=i(F,"CODE",{});var gn=p(ds);bt=n(gn,"image"),gn.forEach(s),vt=n(F,", an "),gs=i(F,"CODE",{});var _n=p(gs);jt=n(_n,"annotation"),_n.forEach(s),$t=n(F," (this is the segmentation map or label), and a "),_s=i(F,"CODE",{});var bn=p(_s);yt=n(bn,"scene_category"),bn.forEach(s),wt=n(F," field that describes the image scene, like \u201Ckitchen\u201D or \u201Coffice\u201D. In this guide, you\u2019ll only need "),bs=i(F,"CODE",{});var vn=p(bs);kt=n(vn,"image"),vn.forEach(s),Et=n(F," and "),vs=i(F,"CODE",{});var jn=p(vs);xt=n(jn,"annotation"),jn.forEach(s),Pt=n(F,", both of which are PIL images."),F.forEach(s),na=c(e),O=i(e,"P",{});var is=p(O);Tt=n(is,"You\u2019ll also want to create a dictionary that maps a label id to a label class which will be useful when you set up the model later. Download the mappings from the Hub and create the "),js=i(is,"CODE",{});var $n=p(js);qt=n($n,"id2label"),$n.forEach(s),St=n(is," and "),$s=i(is,"CODE",{});var yn=p($s);At=n(yn,"label2id"),yn.forEach(s),Ft=n(is," dictionaries:"),is.forEach(s),ra=c(e),d(ge.$$.fragment,e),oa=c(e),H=i(e,"H2",{class:!0});var Va=p(H);W=i(Va,"A",{id:!0,class:!0,href:!0});var wn=p(W);ys=i(wn,"SPAN",{});var kn=p(ys);d(_e.$$.fragment,kn),kn.forEach(s),wn.forEach(s),Ct=c(Va),ws=i(Va,"SPAN",{});var En=p(ws);Dt=n(En,"Preprocess"),En.forEach(s),Va.forEach(s),ia=c(e),L=i(e,"P",{});var ps=p(L);It=n(ps,"Next, load a SegFormer feature extractor to prepare the images and annotations for the model. Some datasets, like this one, use the zero-index as the background class. However, the background class isn\u2019t included in the 150 classes, so you\u2019ll need to set "),ks=i(ps,"CODE",{});var xn=p(ks);zt=n(xn,"reduce_labels=True"),xn.forEach(s),Ot=n(ps," to subtract one from all the labels. The zero-index is replaced by "),Es=i(ps,"CODE",{});var Pn=p(Es);Lt=n(Pn,"255"),Pn.forEach(s),Nt=n(ps," so it\u2019s ignored by SegFormer\u2019s loss function:"),ps.forEach(s),pa=c(e),d(be.$$.fragment,e),ma=c(e),N=i(e,"P",{});var ms=p(N);Mt=n(ms,"It is common to apply some data augmentations to an image dataset to make a model more robust against overfitting. In this guide, you\u2019ll use the "),ve=i(ms,"A",{href:!0,rel:!0});var Tn=p(ve);xs=i(Tn,"CODE",{});var qn=p(xs);Ut=n(qn,"ColorJitter"),qn.forEach(s),Tn.forEach(s),Gt=n(ms," function from "),je=i(ms,"A",{href:!0,rel:!0});var Sn=p(je);Bt=n(Sn,"torchvision"),Sn.forEach(s),Ht=n(ms," to randomly change the color properties of an image:"),ms.forEach(s),ca=c(e),d($e.$$.fragment,e),ha=c(e),E=i(e,"P",{});var C=p(E);Jt=n(C,"Now create two preprocessing functions to prepare the images and annotations for the model. These functions convert the images into "),Ps=i(C,"CODE",{});var An=p(Ps);Rt=n(An,"pixel_values"),An.forEach(s),Yt=n(C," and annotations to "),Ts=i(C,"CODE",{});var Fn=p(Ts);Vt=n(Fn,"labels"),Fn.forEach(s),Wt=n(C,". For the training set, "),qs=i(C,"CODE",{});var Cn=p(qs);Kt=n(Cn,"jitter"),Cn.forEach(s),Qt=n(C," is applied before providing the images to the feature extractor. For the test set, the feature extractor crops and normalizes the "),Ss=i(C,"CODE",{});var Dn=p(Ss);Xt=n(Dn,"images"),Dn.forEach(s),Zt=n(C,", and only crops the "),As=i(C,"CODE",{});var In=p(As);el=n(In,"labels"),In.forEach(s),sl=n(C," because no data augmentation is applied during testing."),C.forEach(s),fa=c(e),d(ye.$$.fragment,e),ua=c(e),M=i(e,"P",{});var cs=p(M);al=n(cs,"To apply the "),Fs=i(cs,"CODE",{});var zn=p(Fs);tl=n(zn,"jitter"),zn.forEach(s),ll=n(cs," over the entire dataset, use the \u{1F917} Datasets "),we=i(cs,"A",{href:!0,rel:!0});var On=p(we);nl=n(On,"set_transform"),On.forEach(s),rl=n(cs," function. The transform is applied on the fly which is faster and consumes less disk space:"),cs.forEach(s),da=c(e),d(ke.$$.fragment,e),ga=c(e),J=i(e,"H2",{class:!0});var Wa=p(J);K=i(Wa,"A",{id:!0,class:!0,href:!0});var Ln=p(K);Cs=i(Ln,"SPAN",{});var Nn=p(Cs);d(Ee.$$.fragment,Nn),Nn.forEach(s),Ln.forEach(s),ol=c(Wa),Ds=i(Wa,"SPAN",{});var Mn=p(Ds);il=n(Mn,"Train"),Mn.forEach(s),Wa.forEach(s),_a=c(e),Q=i(e,"P",{});var Ka=p(Q);pl=n(Ka,"Load SegFormer with "),Qe=i(Ka,"A",{href:!0});var Un=p(Qe);ml=n(Un,"AutoModelForSemanticSegmentation"),Un.forEach(s),cl=n(Ka,", and pass the model the mapping between label ids and label classes:"),Ka.forEach(s),ba=c(e),d(xe.$$.fragment,e),va=c(e),d(X.$$.fragment,e),ja=c(e),x=i(e,"P",{});var D=p(x);hl=n(D,"Define your training hyperparameters in "),Xe=i(D,"A",{href:!0});var Gn=p(Xe);fl=n(Gn,"TrainingArguments"),Gn.forEach(s),ul=n(D,". It is important not to remove unused columns because this will drop the "),Is=i(D,"CODE",{});var Bn=p(Is);dl=n(Bn,"image"),Bn.forEach(s),gl=n(D," column. Without the "),zs=i(D,"CODE",{});var Hn=p(zs);_l=n(Hn,"image"),Hn.forEach(s),bl=n(D," column, you can\u2019t create "),Os=i(D,"CODE",{});var Jn=p(Os);vl=n(Jn,"pixel_values"),Jn.forEach(s),jl=n(D,". Set "),Ls=i(D,"CODE",{});var Rn=p(Ls);$l=n(Rn,"remove_unused_columns=False"),Rn.forEach(s),yl=n(D," to prevent this behavior!"),D.forEach(s),$a=c(e),Z=i(e,"P",{});var Qa=p(Z);wl=n(Qa,"To save and push a model under your namespace to the Hub, set "),Ns=i(Qa,"CODE",{});var Yn=p(Ns);kl=n(Yn,"push_to_hub=True"),Yn.forEach(s),El=n(Qa,":"),Qa.forEach(s),ya=c(e),d(Pe.$$.fragment,e),wa=c(e),ee=i(e,"P",{});var Xa=p(ee);xl=n(Xa,"To evaluate model performance during training, you\u2019ll need to create a function to compute and report metrics. For semantic segmentation, you\u2019ll typically compute the "),Te=i(Xa,"A",{href:!0,rel:!0});var Vn=p(Te);Pl=n(Vn,"mean Intersection over Union"),Vn.forEach(s),Tl=n(Xa," (IoU). The mean IoU measures the overlapping area between the predicted and ground truth segmentation maps."),Xa.forEach(s),ka=c(e),Ze=i(e,"P",{});var Wn=p(Ze);ql=n(Wn,"Load the mean IoU from the \u{1F917} Evaluate library:"),Wn.forEach(s),Ea=c(e),d(qe.$$.fragment,e),xa=c(e),U=i(e,"P",{});var hs=p(U);Sl=n(hs,"Then create a function to "),Se=i(hs,"A",{href:!0,rel:!0});var Kn=p(Se);Al=n(Kn,"compute"),Kn.forEach(s),Fl=n(hs," the metrics. Your predictions need to be converted to logits first, and then reshaped to match the size of the labels before you can call "),Ae=i(hs,"A",{href:!0,rel:!0});var Qn=p(Ae);Cl=n(Qn,"compute"),Qn.forEach(s),Dl=n(hs,":"),hs.forEach(s),Pa=c(e),d(Fe.$$.fragment,e),Ta=c(e),se=i(e,"P",{});var Za=p(se);Il=n(Za,"Pass your model, training arguments, datasets, and metrics function to the "),es=i(Za,"A",{href:!0});var Xn=p(es);zl=n(Xn,"Trainer"),Xn.forEach(s),Ol=n(Za,":"),Za.forEach(s),qa=c(e),d(Ce.$$.fragment,e),Sa=c(e),ae=i(e,"P",{});var et=p(ae);Ll=n(et,"Lastly, call "),ss=i(et,"A",{href:!0});var Zn=p(ss);Nl=n(Zn,"train()"),Zn.forEach(s),Ml=n(et," to finetune your model:"),et.forEach(s),Aa=c(e),d(De.$$.fragment,e),Fa=c(e),R=i(e,"H2",{class:!0});var st=p(R);te=i(st,"A",{id:!0,class:!0,href:!0});var er=p(te);Ms=i(er,"SPAN",{});var sr=p(Ms);d(Ie.$$.fragment,sr),sr.forEach(s),er.forEach(s),Ul=c(st),Us=i(st,"SPAN",{});var ar=p(Us);Gl=n(ar,"Inference"),ar.forEach(s),st.forEach(s),Ca=c(e),as=i(e,"P",{});var tr=p(as);Bl=n(tr,"Great, now that you\u2019ve finetuned a model, you can use it for inference!"),tr.forEach(s),Da=c(e),ts=i(e,"P",{});var lr=p(ts);Hl=n(lr,"Load an image for inference:"),lr.forEach(s),Ia=c(e),d(ze.$$.fragment,e),za=c(e),Oe=i(e,"DIV",{class:!0});var nr=p(Oe);ls=i(nr,"IMG",{src:!0,alt:!0}),nr.forEach(s),Oa=c(e),le=i(e,"P",{});var at=p(le);Jl=n(at,"Process the image with a feature extractor and place the "),Gs=i(at,"CODE",{});var rr=p(Gs);Rl=n(rr,"pixel_values"),rr.forEach(s),Yl=n(at," on a GPU:"),at.forEach(s),La=c(e),d(Le.$$.fragment,e),Na=c(e),ne=i(e,"P",{});var tt=p(ne);Vl=n(tt,"Pass your input to the model and return the "),Bs=i(tt,"CODE",{});var or=p(Bs);Wl=n(or,"logits"),or.forEach(s),Kl=n(tt,":"),tt.forEach(s),Ma=c(e),d(Ne.$$.fragment,e),Ua=c(e),ns=i(e,"P",{});var ir=p(ns);Ql=n(ir,"Next, rescale the logits to the original image size:"),ir.forEach(s),Ga=c(e),d(Me.$$.fragment,e),Ba=c(e),re=i(e,"P",{});var lt=p(re);Xl=n(lt,"To visualize the results, load the "),Ue=i(lt,"A",{href:!0,rel:!0});var pr=p(Ue);Zl=n(pr,"dataset color palette"),pr.forEach(s),en=n(lt," that maps each class to their RGB values. Then you can combine and plot your image and the predicted segmentation map:"),lt.forEach(s),Ha=c(e),d(Ge.$$.fragment,e),Ja=c(e),Be=i(e,"DIV",{class:!0});var mr=p(Be);rs=i(mr,"IMG",{src:!0,alt:!0}),mr.forEach(s),this.h()},h(){h(f,"name","hf:doc:metadata"),h(f,"content",JSON.stringify(wr)),h(w,"id","semantic-segmentation"),h(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(w,"href","#semantic-segmentation"),h(j,"class","relative group"),h(pe,"href","https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer"),h(pe,"rel","nofollow"),h(me,"href","https://huggingface.co/datasets/scene_parse_150"),h(me,"rel","nofollow"),h(V,"id","load-sceneparse150-dataset"),h(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(V,"href","#load-sceneparse150-dataset"),h(B,"class","relative group"),h(W,"id","preprocess"),h(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(W,"href","#preprocess"),h(H,"class","relative group"),h(ve,"href","https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html"),h(ve,"rel","nofollow"),h(je,"href","https://pytorch.org/vision/stable/index.html"),h(je,"rel","nofollow"),h(we,"href","https://huggingface.co/docs/datasets/v2.6.1/en/package_reference/main_classes#datasets.Dataset.set_transform"),h(we,"rel","nofollow"),h(K,"id","train"),h(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(K,"href","#train"),h(J,"class","relative group"),h(Qe,"href","/docs/transformers/v4.24.0/en/model_doc/auto#transformers.AutoModelForSemanticSegmentation"),h(Xe,"href","/docs/transformers/v4.24.0/en/main_classes/trainer#transformers.TrainingArguments"),h(Te,"href","https://huggingface.co/spaces/evaluate-metric/mean_iou"),h(Te,"rel","nofollow"),h(Se,"href","https://huggingface.co/docs/evaluate/v0.3.0/en/package_reference/main_classes#evaluate.EvaluationModule.compute"),h(Se,"rel","nofollow"),h(Ae,"href","https://huggingface.co/docs/evaluate/v0.3.0/en/package_reference/main_classes#evaluate.EvaluationModule.compute"),h(Ae,"rel","nofollow"),h(es,"href","/docs/transformers/v4.24.0/en/main_classes/trainer#transformers.Trainer"),h(ss,"href","/docs/transformers/v4.24.0/en/main_classes/trainer#transformers.Trainer.train"),h(te,"id","inference"),h(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(te,"href","#inference"),h(R,"class","relative group"),cr(ls.src,sn="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png")||h(ls,"src",sn),h(ls,"alt","Image of bedroom"),h(Oe,"class","flex justify-center"),h(Ue,"href","https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51"),h(Ue,"rel","nofollow"),cr(rs.src,an="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-preds.png")||h(rs,"src",an),h(rs,"alt","Image of bedroom overlayed with segmentation map"),h(Be,"class","flex justify-center")},m(e,t){a(document.head,f),r(e,S,t),r(e,j,t),a(j,w),a(w,T),g($,T,null),a(j,P),a(j,A),a(A,G),r(e,q,t),g(I,e,t),r(e,oe,t),g(ie,e,t),r(e,Rs,t),r(e,Re,t),a(Re,nt),r(e,Ys,t),r(e,z,t),a(z,rt),a(z,pe),a(pe,ot),a(z,it),a(z,me),a(me,pt),a(z,mt),r(e,Vs,t),g(Y,e,t),r(e,Ws,t),r(e,Ye,t),a(Ye,ct),r(e,Ks,t),g(ce,e,t),r(e,Qs,t),r(e,B,t),a(B,V),a(V,fs),g(he,fs,null),a(B,ht),a(B,us),a(us,ft),r(e,Xs,t),r(e,Ve,t),a(Ve,ut),r(e,Zs,t),g(fe,e,t),r(e,ea,t),r(e,We,t),a(We,dt),r(e,sa,t),g(ue,e,t),r(e,aa,t),r(e,Ke,t),a(Ke,gt),r(e,ta,t),g(de,e,t),r(e,la,t),r(e,k,t),a(k,_t),a(k,ds),a(ds,bt),a(k,vt),a(k,gs),a(gs,jt),a(k,$t),a(k,_s),a(_s,yt),a(k,wt),a(k,bs),a(bs,kt),a(k,Et),a(k,vs),a(vs,xt),a(k,Pt),r(e,na,t),r(e,O,t),a(O,Tt),a(O,js),a(js,qt),a(O,St),a(O,$s),a($s,At),a(O,Ft),r(e,ra,t),g(ge,e,t),r(e,oa,t),r(e,H,t),a(H,W),a(W,ys),g(_e,ys,null),a(H,Ct),a(H,ws),a(ws,Dt),r(e,ia,t),r(e,L,t),a(L,It),a(L,ks),a(ks,zt),a(L,Ot),a(L,Es),a(Es,Lt),a(L,Nt),r(e,pa,t),g(be,e,t),r(e,ma,t),r(e,N,t),a(N,Mt),a(N,ve),a(ve,xs),a(xs,Ut),a(N,Gt),a(N,je),a(je,Bt),a(N,Ht),r(e,ca,t),g($e,e,t),r(e,ha,t),r(e,E,t),a(E,Jt),a(E,Ps),a(Ps,Rt),a(E,Yt),a(E,Ts),a(Ts,Vt),a(E,Wt),a(E,qs),a(qs,Kt),a(E,Qt),a(E,Ss),a(Ss,Xt),a(E,Zt),a(E,As),a(As,el),a(E,sl),r(e,fa,t),g(ye,e,t),r(e,ua,t),r(e,M,t),a(M,al),a(M,Fs),a(Fs,tl),a(M,ll),a(M,we),a(we,nl),a(M,rl),r(e,da,t),g(ke,e,t),r(e,ga,t),r(e,J,t),a(J,K),a(K,Cs),g(Ee,Cs,null),a(J,ol),a(J,Ds),a(Ds,il),r(e,_a,t),r(e,Q,t),a(Q,pl),a(Q,Qe),a(Qe,ml),a(Q,cl),r(e,ba,t),g(xe,e,t),r(e,va,t),g(X,e,t),r(e,ja,t),r(e,x,t),a(x,hl),a(x,Xe),a(Xe,fl),a(x,ul),a(x,Is),a(Is,dl),a(x,gl),a(x,zs),a(zs,_l),a(x,bl),a(x,Os),a(Os,vl),a(x,jl),a(x,Ls),a(Ls,$l),a(x,yl),r(e,$a,t),r(e,Z,t),a(Z,wl),a(Z,Ns),a(Ns,kl),a(Z,El),r(e,ya,t),g(Pe,e,t),r(e,wa,t),r(e,ee,t),a(ee,xl),a(ee,Te),a(Te,Pl),a(ee,Tl),r(e,ka,t),r(e,Ze,t),a(Ze,ql),r(e,Ea,t),g(qe,e,t),r(e,xa,t),r(e,U,t),a(U,Sl),a(U,Se),a(Se,Al),a(U,Fl),a(U,Ae),a(Ae,Cl),a(U,Dl),r(e,Pa,t),g(Fe,e,t),r(e,Ta,t),r(e,se,t),a(se,Il),a(se,es),a(es,zl),a(se,Ol),r(e,qa,t),g(Ce,e,t),r(e,Sa,t),r(e,ae,t),a(ae,Ll),a(ae,ss),a(ss,Nl),a(ae,Ml),r(e,Aa,t),g(De,e,t),r(e,Fa,t),r(e,R,t),a(R,te),a(te,Ms),g(Ie,Ms,null),a(R,Ul),a(R,Us),a(Us,Gl),r(e,Ca,t),r(e,as,t),a(as,Bl),r(e,Da,t),r(e,ts,t),a(ts,Hl),r(e,Ia,t),g(ze,e,t),r(e,za,t),r(e,Oe,t),a(Oe,ls),r(e,Oa,t),r(e,le,t),a(le,Jl),a(le,Gs),a(Gs,Rl),a(le,Yl),r(e,La,t),g(Le,e,t),r(e,Na,t),r(e,ne,t),a(ne,Vl),a(ne,Bs),a(Bs,Wl),a(ne,Kl),r(e,Ma,t),g(Ne,e,t),r(e,Ua,t),r(e,ns,t),a(ns,Ql),r(e,Ga,t),g(Me,e,t),r(e,Ba,t),r(e,re,t),a(re,Xl),a(re,Ue),a(Ue,Zl),a(re,en),r(e,Ha,t),g(Ge,e,t),r(e,Ja,t),r(e,Be,t),a(Be,rs),Ra=!0},p(e,[t]){const He={};t&2&&(He.$$scope={dirty:t,ctx:e}),Y.$set(He);const Hs={};t&2&&(Hs.$$scope={dirty:t,ctx:e}),X.$set(Hs)},i(e){Ra||(_($.$$.fragment,e),_(I.$$.fragment,e),_(ie.$$.fragment,e),_(Y.$$.fragment,e),_(ce.$$.fragment,e),_(he.$$.fragment,e),_(fe.$$.fragment,e),_(ue.$$.fragment,e),_(de.$$.fragment,e),_(ge.$$.fragment,e),_(_e.$$.fragment,e),_(be.$$.fragment,e),_($e.$$.fragment,e),_(ye.$$.fragment,e),_(ke.$$.fragment,e),_(Ee.$$.fragment,e),_(xe.$$.fragment,e),_(X.$$.fragment,e),_(Pe.$$.fragment,e),_(qe.$$.fragment,e),_(Fe.$$.fragment,e),_(Ce.$$.fragment,e),_(De.$$.fragment,e),_(Ie.$$.fragment,e),_(ze.$$.fragment,e),_(Le.$$.fragment,e),_(Ne.$$.fragment,e),_(Me.$$.fragment,e),_(Ge.$$.fragment,e),Ra=!0)},o(e){b($.$$.fragment,e),b(I.$$.fragment,e),b(ie.$$.fragment,e),b(Y.$$.fragment,e),b(ce.$$.fragment,e),b(he.$$.fragment,e),b(fe.$$.fragment,e),b(ue.$$.fragment,e),b(de.$$.fragment,e),b(ge.$$.fragment,e),b(_e.$$.fragment,e),b(be.$$.fragment,e),b($e.$$.fragment,e),b(ye.$$.fragment,e),b(ke.$$.fragment,e),b(Ee.$$.fragment,e),b(xe.$$.fragment,e),b(X.$$.fragment,e),b(Pe.$$.fragment,e),b(qe.$$.fragment,e),b(Fe.$$.fragment,e),b(Ce.$$.fragment,e),b(De.$$.fragment,e),b(Ie.$$.fragment,e),b(ze.$$.fragment,e),b(Le.$$.fragment,e),b(Ne.$$.fragment,e),b(Me.$$.fragment,e),b(Ge.$$.fragment,e),Ra=!1},d(e){s(f),e&&s(S),e&&s(j),v($),e&&s(q),v(I,e),e&&s(oe),v(ie,e),e&&s(Rs),e&&s(Re),e&&s(Ys),e&&s(z),e&&s(Vs),v(Y,e),e&&s(Ws),e&&s(Ye),e&&s(Ks),v(ce,e),e&&s(Qs),e&&s(B),v(he),e&&s(Xs),e&&s(Ve),e&&s(Zs),v(fe,e),e&&s(ea),e&&s(We),e&&s(sa),v(ue,e),e&&s(aa),e&&s(Ke),e&&s(ta),v(de,e),e&&s(la),e&&s(k),e&&s(na),e&&s(O),e&&s(ra),v(ge,e),e&&s(oa),e&&s(H),v(_e),e&&s(ia),e&&s(L),e&&s(pa),v(be,e),e&&s(ma),e&&s(N),e&&s(ca),v($e,e),e&&s(ha),e&&s(E),e&&s(fa),v(ye,e),e&&s(ua),e&&s(M),e&&s(da),v(ke,e),e&&s(ga),e&&s(J),v(Ee),e&&s(_a),e&&s(Q),e&&s(ba),v(xe,e),e&&s(va),v(X,e),e&&s(ja),e&&s(x),e&&s($a),e&&s(Z),e&&s(ya),v(Pe,e),e&&s(wa),e&&s(ee),e&&s(ka),e&&s(Ze),e&&s(Ea),v(qe,e),e&&s(xa),e&&s(U),e&&s(Pa),v(Fe,e),e&&s(Ta),e&&s(se),e&&s(qa),v(Ce,e),e&&s(Sa),e&&s(ae),e&&s(Aa),v(De,e),e&&s(Fa),e&&s(R),v(Ie),e&&s(Ca),e&&s(as),e&&s(Da),e&&s(ts),e&&s(Ia),v(ze,e),e&&s(za),e&&s(Oe),e&&s(Oa),e&&s(le),e&&s(La),v(Le,e),e&&s(Na),e&&s(ne),e&&s(Ma),v(Ne,e),e&&s(Ua),e&&s(ns),e&&s(Ga),v(Me,e),e&&s(Ba),e&&s(re),e&&s(Ha),v(Ge,e),e&&s(Ja),e&&s(Be)}}}const wr={local:"semantic-segmentation",sections:[{local:"load-sceneparse150-dataset",title:"Load SceneParse150 dataset"},{local:"preprocess",title:"Preprocess"},{local:"train",title:"Train"},{local:"inference",title:"Inference"}],title:"Semantic segmentation"};function kr(Je){return _r(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ar extends fr{constructor(f){super();ur(this,f,kr,yr,dr,{})}}export{Ar as default,wr as metadata};
