import{S as zo,i as jo,s as qo,e as a,k as c,w as v,t as s,M as Lo,c as o,d as r,m as l,a as n,x as b,h as i,b as m,G as e,g,y,q as $,o as E,B as w,v as No,L as Mo}from"../../chunks/vendor-hf-doc-builder.js";import{T as Oo}from"../../chunks/Tip-hf-doc-builder.js";import{D as T}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Po}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Ra}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Do}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Ao(H){let f,k,_,u,I;return u=new Po({props:{code:`# We can't instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let's show the examples on a
# derived class: *Wav2Vec2FeatureExtractor*
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h"
)  # Download feature_extraction_config from huggingface.co and cache.
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "./test/saved_model/"
)  # E.g. feature_extractor (or model) was saved using *save_pretrained('./test/saved_model/')*
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("./test/saved_model/preprocessor_config.json")
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h", return_attention_mask=False, foo=False
)
assert feature_extractor.return_attention_mask is False
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h", return_attention_mask=False, foo=False, return_unused_kwargs=True
)
assert feature_extractor.return_attention_mask is False
assert unused_kwargs == {"foo": False}`,highlighted:`<span class="hljs-comment"># We can&#x27;t instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let&#x27;s show the examples on a</span>
<span class="hljs-comment"># derived class: *Wav2Vec2FeatureExtractor*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>
)  <span class="hljs-comment"># Download feature_extraction_config from huggingface.co and cache.</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;./test/saved_model/&quot;</span>
)  <span class="hljs-comment"># E.g. feature_extractor (or model) was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/preprocessor_config.json&quot;</span>)
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
<span class="hljs-keyword">assert</span> unused_kwargs == {<span class="hljs-string">&quot;foo&quot;</span>: <span class="hljs-literal">False</span>}`}}),{c(){f=a("p"),k=s("Examples:"),_=c(),v(u.$$.fragment)},l(d){f=o(d,"P",{});var F=n(f);k=i(F,"Examples:"),F.forEach(r),_=l(d),b(u.$$.fragment,d)},m(d,F){g(d,f,F),e(f,k),g(d,_,F),y(u,d,F),I=!0},p:Mo,i(d){I||($(u.$$.fragment,d),I=!0)},o(d){E(u.$$.fragment,d),I=!1},d(d){d&&r(f),d&&r(_),w(u,d)}}}function So(H){let f,k,_,u,I;return u=new Po({props:{code:`from transformers import AutoFeatureExtractor

feature extractor = AutoFeatureExtractor.from_pretrained("bert-base-cased")

# Push the feature extractor to your namespace with the name "my-finetuned-bert".
feature extractor.push_to_hub("my-finetuned-bert")

# Push the feature extractor to an organization with the name "my-finetuned-bert".
feature extractor.push_to_hub("huggingface/my-finetuned-bert")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

feature extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-comment"># Push the feature extractor to your namespace with the name &quot;my-finetuned-bert&quot;.</span>
feature extractor.push_to_hub(<span class="hljs-string">&quot;my-finetuned-bert&quot;</span>)

<span class="hljs-comment"># Push the feature extractor to an organization with the name &quot;my-finetuned-bert&quot;.</span>
feature extractor.push_to_hub(<span class="hljs-string">&quot;huggingface/my-finetuned-bert&quot;</span>)`}}),{c(){f=a("p"),k=s("Examples:"),_=c(),v(u.$$.fragment)},l(d){f=o(d,"P",{});var F=n(f);k=i(F,"Examples:"),F.forEach(r),_=l(d),b(u.$$.fragment,d)},m(d,F){g(d,f,F),e(f,k),g(d,_,F),y(u,d,F),I=!0},p:Mo,i(d){I||($(u.$$.fragment,d),I=!0)},o(d){E(u.$$.fragment,d),I=!1},d(d){d&&r(f),d&&r(_),w(u,d)}}}function Co(H){let f,k;return{c(){f=a("p"),k=s("This API is experimental and may have some slight breaking changes in the next releases.")},l(_){f=o(_,"P",{});var u=n(f);k=i(u,"This API is experimental and may have some slight breaking changes in the next releases."),u.forEach(r)},m(_,u){g(_,f,u),e(f,k)},d(_){_&&r(f)}}}function Vo(H){let f,k,_,u,I,d,F,Ge,Xt,Et,je,Zt,wt,qe,er,Ft,O,B,Ye,le,tr,Ke,rr,kt,A,de,ar,D,or,Qe,nr,sr,Xe,ir,cr,Ze,lr,dr,Tt,M,me,mr,P,pr,et,fr,ur,tt,hr,gr,rt,_r,xr,vr,at,br,It,S,pe,yr,C,$r,ot,Er,wr,nt,Fr,kr,Dt,V,fe,Tr,W,Ir,st,Dr,Mr,it,Pr,zr,Mt,U,ue,jr,he,qr,ct,Lr,Nr,Pt,R,G,lt,ge,Or,dt,Ar,zt,h,_e,Sr,mt,Cr,Vr,Y,xe,Wr,ve,Ur,Le,Rr,Jr,Hr,K,be,Br,ye,Gr,Ne,Yr,Kr,Qr,q,$e,Xr,z,Zr,Oe,ea,ta,pt,ra,aa,Ae,oa,na,sa,Q,ia,X,Ee,ca,j,la,ft,da,ma,Se,pa,fa,ut,ua,ha,ga,L,we,_a,Fe,xa,ht,va,ba,ya,Z,$a,N,ke,Ea,Te,wa,gt,Fa,ka,Ta,ee,Ia,te,Ie,Da,J,Ma,_t,Pa,za,Ce,ja,qa,La,re,De,Na,xt,Oa,Aa,ae,Me,Sa,vt,Ca,Va,oe,Pe,Wa,bt,Ua,jt;return d=new Ra({}),le=new Ra({}),de=new T({props:{name:"transformers.image_transforms.center_crop",anchor:"transformers.image_transforms.center_crop",parameters:[{name:"image",val:": ndarray"},{name:"size",val:": typing.Tuple[int, int]"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"return_numpy",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.image_transforms.center_crop.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to crop.`,name:"image"},{anchor:"transformers.image_transforms.center_crop.size",description:`<strong>size</strong> (<code>Tuple[int, int]</code>) &#x2014;
The target size for the cropped image.`,name:"size"},{anchor:"transformers.image_transforms.center_crop.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.
If unset, will use the inferred format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.image_transforms.center_crop.return_numpy",description:`<strong>return_numpy</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the cropped image as a numpy array. Used for backwards compatibility with the
previous ImageFeatureExtractionMixin method.<ul>
<li>Unset: will return the same type as the input image.</li>
<li><code>True</code>: will return a numpy array.</li>
<li><code>False</code>: will return a <code>PIL.Image.Image</code> object.</li>
</ul>`,name:"return_numpy"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/image_transforms.py#L322",returnDescription:`
<p>The cropped image.</p>
`,returnType:`
<p><code>np.ndarray</code></p>
`}}),me=new T({props:{name:"transformers.image_transforms.normalize",anchor:"transformers.image_transforms.normalize",parameters:[{name:"image",val:": ndarray"},{name:"mean",val:": typing.Union[float, typing.Iterable[float]]"},{name:"std",val:": typing.Union[float, typing.Iterable[float]]"},{name:"data_format",val:": typing.Optional[transformers.image_utils.ChannelDimension] = None"}],parametersDescription:[{anchor:"transformers.image_transforms.normalize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to normalize.`,name:"image"},{anchor:"transformers.image_transforms.normalize.mean",description:`<strong>mean</strong> (<code>float</code> or <code>Iterable[float]</code>) &#x2014;
The mean to use for normalization.`,name:"mean"},{anchor:"transformers.image_transforms.normalize.std",description:`<strong>std</strong> (<code>float</code> or <code>Iterable[float]</code>) &#x2014;
The standard deviation to use for normalization.`,name:"std"},{anchor:"transformers.image_transforms.normalize.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the output image. If <code>None</code>, will use the inferred format from the input.`,name:"data_format"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/image_transforms.py#L266"}}),pe=new T({props:{name:"transformers.rescale",anchor:"transformers.rescale",parameters:[{name:"image",val:": ndarray"},{name:"scale",val:": float"},{name:"data_format",val:": typing.Optional[transformers.image_utils.ChannelDimension] = None"},{name:"dtype",val:" = <class 'numpy.float32'>"}],parametersDescription:[{anchor:"transformers.rescale.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to rescale.`,name:"image"},{anchor:"transformers.rescale.scale",description:`<strong>scale</strong> (<code>float</code>) &#x2014;
The scale to use for rescaling the image.`,name:"scale"},{anchor:"transformers.rescale.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the image. If not provided, it will be the same as the input image.`,name:"data_format"},{anchor:"transformers.rescale.dtype",description:`<strong>dtype</strong> (<code>np.dtype</code>, <em>optional</em>, defaults to <code>np.float32</code>) &#x2014;
The dtype of the output image. Defaults to <code>np.float32</code>. Used for backwards compatibility with feature
extractors.`,name:"dtype"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/image_transforms.py#L80",returnDescription:`
<p>The rescaled image.</p>
`,returnType:`
<p><code>np.ndarray</code></p>
`}}),fe=new T({props:{name:"transformers.resize",anchor:"transformers.resize",parameters:[{name:"image",val:""},{name:"size",val:": typing.Tuple[int, int]"},{name:"resample",val:" = <Resampling.BILINEAR: 2>"},{name:"data_format",val:": typing.Optional[transformers.image_utils.ChannelDimension] = None"},{name:"return_numpy",val:": bool = True"}],parametersDescription:[{anchor:"transformers.resize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.resize.size",description:`<strong>size</strong> (<code>Tuple[int, int]</code>) &#x2014;
The size to use for resizing the image.`,name:"size"},{anchor:"transformers.resize.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.Resampling.BILINEAR</code>) &#x2014;
The filter to user for resampling.`,name:"resample"},{anchor:"transformers.resize.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the output image. If <code>None</code>, will use the inferred format from the input.`,name:"data_format"},{anchor:"transformers.resize.return_numpy",description:`<strong>return_numpy</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return the resized image as a numpy array. If False a <code>PIL.Image.Image</code> object is
returned.`,name:"return_numpy"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/image_transforms.py#L217",returnDescription:`
<p>The resized image.</p>
`,returnType:`
<p><code>np.ndarray</code></p>
`}}),ue=new T({props:{name:"transformers.to_pil_image",anchor:"transformers.to_pil_image",parameters:[{name:"image",val:": typing.Union[numpy.ndarray, PIL.Image.Image, ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor'), ForwardRef('jnp.Tensor')]"},{name:"do_rescale",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.to_pil_image.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>numpy.ndarray</code> or <code>torch.Tensor</code> or <code>tf.Tensor</code>) &#x2014;
The image to convert to the <code>PIL.Image</code> format.`,name:"image"},{anchor:"transformers.to_pil_image.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values integers between 0 and 255). Will default
to <code>True</code> if the image type is a floating type, <code>False</code> otherwise.`,name:"do_rescale"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/image_transforms.py#L110",returnDescription:`
<p>The converted image.</p>
`,returnType:`
<p><code>PIL.Image.Image</code></p>
`}}),ge=new Ra({}),_e=new T({props:{name:"class transformers.FeatureExtractionMixin",anchor:"transformers.FeatureExtractionMixin",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/feature_extraction_utils.py#L200"}}),xe=new T({props:{name:"from_dict",anchor:"transformers.FeatureExtractionMixin.from_dict",parameters:[{name:"feature_extractor_dict",val:": typing.Dict[str, typing.Any]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.from_dict.feature_extractor_dict",description:`<strong>feature_extractor_dict</strong> (<code>Dict[str, Any]</code>) &#x2014;
Dictionary that will be used to instantiate the feature extractor object. Such a dictionary can be
retrieved from a pretrained checkpoint by leveraging the
<a href="/docs/transformers/v4.24.0/en/internal/image_processing_utils#transformers.FeatureExtractionMixin.to_dict">to_dict()</a> method.`,name:"feature_extractor_dict"},{anchor:"transformers.FeatureExtractionMixin.from_dict.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>) &#x2014;
Additional parameters from which to initialize the feature extractor object.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/feature_extraction_utils.py#L447",returnDescription:`
<p>The feature extractor object instantiated from those
parameters.</p>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"
>FeatureExtractionMixin</a></p>
`}}),be=new T({props:{name:"from_json_file",anchor:"transformers.FeatureExtractionMixin.from_json_file",parameters:[{name:"json_file",val:": typing.Union[str, os.PathLike]"}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.from_json_file.json_file",description:`<strong>json_file</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Path to the JSON file containing the parameters.`,name:"json_file"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/feature_extraction_utils.py#L496",returnDescription:`
<p>The feature_extractor
object instantiated from that JSON file.</p>
`,returnType:`
<p>A feature extractor of type <a
  href="/docs/transformers/v4.24.0/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"
>FeatureExtractionMixin</a></p>
`}}),$e=new T({props:{name:"from_pretrained",anchor:"transformers.FeatureExtractionMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/v4.24.0/en/internal/image_processing_utils#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, or not specified, will use
the token generated when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/feature_extraction_utils.py#L224",returnDescription:`
<p>A feature extractor of type <a
  href="/docs/transformers/v4.24.0/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"
>FeatureExtractionMixin</a>.</p>
`}}),Q=new Do({props:{anchor:"transformers.FeatureExtractionMixin.from_pretrained.example",$$slots:{default:[Ao]},$$scope:{ctx:H}}}),Ee=new T({props:{name:"get_feature_extractor_dict",anchor:"transformers.FeatureExtractionMixin.get_feature_extractor_dict",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.get_feature_extractor_dict.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
The identifier of the pre-trained checkpoint from which we want the dictionary of parameters.`,name:"pretrained_model_name_or_path"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/feature_extraction_utils.py#L354",returnDescription:`
<p>The dictionary(ies) that will be used to instantiate the feature extractor object.</p>
`,returnType:`
<p><code>Tuple[Dict, Dict]</code></p>
`}}),we=new T({props:{name:"push_to_hub",anchor:"transformers.FeatureExtractionMixin.push_to_hub",parameters:[{name:"repo_id",val:": str"},{name:"use_temp_dir",val:": typing.Optional[bool] = None"},{name:"commit_message",val:": typing.Optional[str] = None"},{name:"private",val:": typing.Optional[bool] = None"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"max_shard_size",val:": typing.Union[int, str, NoneType] = '10GB'"},{name:"create_pr",val:": bool = False"},{name:"**deprecated_kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.push_to_hub.repo_id",description:`<strong>repo_id</strong> (<code>str</code>) &#x2014;
The name of the repository you want to push your feature extractor to. It should contain your organization name
when pushing to a given organization.`,name:"repo_id"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.use_temp_dir",description:`<strong>use_temp_dir</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.
Will default to <code>True</code> if there is no directory named like <code>repo_id</code>, <code>False</code> otherwise.`,name:"use_temp_dir"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.commit_message",description:`<strong>commit_message</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Message to commit while pushing. Will default to <code>&quot;Upload feature extractor&quot;</code>.`,name:"commit_message"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.private",description:`<strong>private</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not the repository created should be private (requires a paying subscription).`,name:"private"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.use_auth_token",description:`<strong>use_auth_token</strong> (<code>bool</code> or <code>str</code>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>). Will default to <code>True</code> if <code>repo_url</code>
is not specified.`,name:"use_auth_token"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.max_shard_size",description:`<strong>max_shard_size</strong> (<code>int</code> or <code>str</code>, <em>optional</em>, defaults to <code>&quot;10GB&quot;</code>) &#x2014;
Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard
will then be each of size lower than this size. If expressed as a string, needs to be digits followed
by a unit (like <code>&quot;5MB&quot;</code>).`,name:"max_shard_size"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.create_pr",description:`<strong>create_pr</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to create a PR with the uploaded files or directly commit.`,name:"create_pr"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/utils/hub.py#L712"}}),Z=new Do({props:{anchor:"transformers.FeatureExtractionMixin.push_to_hub.example",$$slots:{default:[So]},$$scope:{ctx:H}}}),ke=new T({props:{name:"register_for_auto_class",anchor:"transformers.FeatureExtractionMixin.register_for_auto_class",parameters:[{name:"auto_class",val:" = 'AutoFeatureExtractor'"}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.register_for_auto_class.auto_class",description:`<strong>auto_class</strong> (<code>str</code> or <code>type</code>, <em>optional</em>, defaults to <code>&quot;AutoFeatureExtractor&quot;</code>) &#x2014;
The auto class to register this new feature extractor with.`,name:"auto_class"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/feature_extraction_utils.py#L550"}}),ee=new Oo({props:{warning:!0,$$slots:{default:[Co]},$$scope:{ctx:H}}}),Ie=new T({props:{name:"save_pretrained",anchor:"transformers.FeatureExtractionMixin.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike]"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the feature extractor JSON file will be saved (will be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.FeatureExtractionMixin.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).
kwargs &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/v4.24.0/en/internal/image_processing_utils#transformers.FeatureExtractionMixin.push_to_hub">push_to_hub()</a> method.`,name:"push_to_hub"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/feature_extraction_utils.py#L310"}}),De=new T({props:{name:"to_dict",anchor:"transformers.FeatureExtractionMixin.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/feature_extraction_utils.py#L484",returnDescription:`
<p>Dictionary of all the attributes that make up this feature extractor instance.</p>
`,returnType:`
<p><code>Dict[str, Any]</code></p>
`}}),Me=new T({props:{name:"to_json_file",anchor:"transformers.FeatureExtractionMixin.to_json_file",parameters:[{name:"json_file_path",val:": typing.Union[str, os.PathLike]"}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.to_json_file.json_file_path",description:`<strong>json_file_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Path to the JSON file in which this feature_extractor instance&#x2019;s parameters will be saved.`,name:"json_file_path"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/feature_extraction_utils.py#L536"}}),Pe=new T({props:{name:"to_json_string",anchor:"transformers.FeatureExtractionMixin.to_json_string",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/feature_extraction_utils.py#L515",returnDescription:`
<p>String containing all the attributes that make up this feature_extractor instance in JSON format.</p>
`,returnType:`
<p><code>str</code></p>
`}}),{c(){f=a("meta"),k=c(),_=a("h1"),u=a("a"),I=a("span"),v(d.$$.fragment),F=c(),Ge=a("span"),Xt=s("Utilities for Image Processors"),Et=c(),je=a("p"),Zt=s(`This page lists all the utility functions used by the image processors, mainly the functional
transformations used to process the images.`),wt=c(),qe=a("p"),er=s("Most of those are only useful if you are studying the code of the image processors in the library."),Ft=c(),O=a("h2"),B=a("a"),Ye=a("span"),v(le.$$.fragment),tr=c(),Ke=a("span"),rr=s("Image Transformations"),kt=c(),A=a("div"),v(de.$$.fragment),ar=c(),D=a("p"),or=s("Crops the "),Qe=a("code"),nr=s("image"),sr=s(" to the specified "),Xe=a("code"),ir=s("size"),cr=s(` using a center crop. Note that if the image is too small to be cropped to
the size given, it will be padded (so the returned result will always be of size `),Ze=a("code"),lr=s("size"),dr=s(")."),Tt=c(),M=a("div"),v(me.$$.fragment),mr=c(),P=a("p"),pr=s("Normalizes "),et=a("code"),fr=s("image"),ur=s(" using the mean and standard deviation specified by "),tt=a("code"),hr=s("mean"),gr=s(" and "),rt=a("code"),_r=s("std"),xr=s("."),vr=c(),at=a("p"),br=s("image = (image - mean) / std"),It=c(),S=a("div"),v(pe.$$.fragment),yr=c(),C=a("p"),$r=s("Rescales "),ot=a("code"),Er=s("image"),wr=s(" by "),nt=a("code"),Fr=s("scale"),kr=s("."),Dt=c(),V=a("div"),v(fe.$$.fragment),Tr=c(),W=a("p"),Ir=s("Resizes "),st=a("code"),Dr=s("image"),Mr=s(" to (h, w) specified by "),it=a("code"),Pr=s("size"),zr=s(" using the PIL library."),Mt=c(),U=a("div"),v(ue.$$.fragment),jr=c(),he=a("p"),qr=s("Converts "),ct=a("code"),Lr=s("image"),Nr=s(` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),Pt=c(),R=a("h2"),G=a("a"),lt=a("span"),v(ge.$$.fragment),Or=c(),dt=a("span"),Ar=s("ImageProcessorMixin"),zt=c(),h=a("div"),v(_e.$$.fragment),Sr=c(),mt=a("p"),Cr=s(`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),Vr=c(),Y=a("div"),v(xe.$$.fragment),Wr=c(),ve=a("p"),Ur=s("Instantiates a type of "),Le=a("a"),Rr=s("FeatureExtractionMixin"),Jr=s(` from a Python dictionary of
parameters.`),Hr=c(),K=a("div"),v(be.$$.fragment),Br=c(),ye=a("p"),Gr=s("Instantiates a feature extractor of type "),Ne=a("a"),Yr=s("FeatureExtractionMixin"),Kr=s(` from the path to
a JSON file of parameters.`),Qr=c(),q=a("div"),v($e.$$.fragment),Xr=c(),z=a("p"),Zr=s("Instantiate a type of "),Oe=a("a"),ea=s("FeatureExtractionMixin"),ta=s(" from a feature extractor, "),pt=a("em"),ra=s("e.g."),aa=s(` a
derived class of `),Ae=a("a"),oa=s("SequenceFeatureExtractor"),na=s("."),sa=c(),v(Q.$$.fragment),ia=c(),X=a("div"),v(Ee.$$.fragment),ca=c(),j=a("p"),la=s("From a "),ft=a("code"),da=s("pretrained_model_name_or_path"),ma=s(`, resolve to a dictionary of parameters, to be used for instantiating a
feature extractor of type `),Se=a("a"),pa=s("FeatureExtractionMixin"),fa=s(" using "),ut=a("code"),ua=s("from_dict"),ha=s("."),ga=c(),L=a("div"),v(we.$$.fragment),_a=c(),Fe=a("p"),xa=s(`Upload the feature extractor file to the \u{1F917} Model Hub while synchronizing a local clone of the repo in
`),ht=a("code"),va=s("repo_path_or_name"),ba=s("."),ya=c(),v(Z.$$.fragment),$a=c(),N=a("div"),v(ke.$$.fragment),Ea=c(),Te=a("p"),wa=s(`Register this class with a given auto class. This should only be used for custom feature extractors as the ones
in the library are already mapped with `),gt=a("code"),Fa=s("AutoFeatureExtractor"),ka=s("."),Ta=c(),v(ee.$$.fragment),Ia=c(),te=a("div"),v(Ie.$$.fragment),Da=c(),J=a("p"),Ma=s("Save a feature_extractor object to the directory "),_t=a("code"),Pa=s("save_directory"),za=s(`, so that it can be re-loaded using the
`),Ce=a("a"),ja=s("from_pretrained()"),qa=s(" class method."),La=c(),re=a("div"),v(De.$$.fragment),Na=c(),xt=a("p"),Oa=s("Serializes this instance to a Python dictionary."),Aa=c(),ae=a("div"),v(Me.$$.fragment),Sa=c(),vt=a("p"),Ca=s("Save this instance to a JSON file."),Va=c(),oe=a("div"),v(Pe.$$.fragment),Wa=c(),bt=a("p"),Ua=s("Serializes this instance to a JSON string."),this.h()},l(t){const p=Lo('[data-svelte="svelte-1phssyn"]',document.head);f=o(p,"META",{name:!0,content:!0}),p.forEach(r),k=l(t),_=o(t,"H1",{class:!0});var ze=n(_);u=o(ze,"A",{id:!0,class:!0,href:!0});var yt=n(u);I=o(yt,"SPAN",{});var $t=n(I);b(d.$$.fragment,$t),$t.forEach(r),yt.forEach(r),F=l(ze),Ge=o(ze,"SPAN",{});var Ja=n(Ge);Xt=i(Ja,"Utilities for Image Processors"),Ja.forEach(r),ze.forEach(r),Et=l(t),je=o(t,"P",{});var Ha=n(je);Zt=i(Ha,`This page lists all the utility functions used by the image processors, mainly the functional
transformations used to process the images.`),Ha.forEach(r),wt=l(t),qe=o(t,"P",{});var Ba=n(qe);er=i(Ba,"Most of those are only useful if you are studying the code of the image processors in the library."),Ba.forEach(r),Ft=l(t),O=o(t,"H2",{class:!0});var qt=n(O);B=o(qt,"A",{id:!0,class:!0,href:!0});var Ga=n(B);Ye=o(Ga,"SPAN",{});var Ya=n(Ye);b(le.$$.fragment,Ya),Ya.forEach(r),Ga.forEach(r),tr=l(qt),Ke=o(qt,"SPAN",{});var Ka=n(Ke);rr=i(Ka,"Image Transformations"),Ka.forEach(r),qt.forEach(r),kt=l(t),A=o(t,"DIV",{class:!0});var Lt=n(A);b(de.$$.fragment,Lt),ar=l(Lt),D=o(Lt,"P",{});var ne=n(D);or=i(ne,"Crops the "),Qe=o(ne,"CODE",{});var Qa=n(Qe);nr=i(Qa,"image"),Qa.forEach(r),sr=i(ne," to the specified "),Xe=o(ne,"CODE",{});var Xa=n(Xe);ir=i(Xa,"size"),Xa.forEach(r),cr=i(ne,` using a center crop. Note that if the image is too small to be cropped to
the size given, it will be padded (so the returned result will always be of size `),Ze=o(ne,"CODE",{});var Za=n(Ze);lr=i(Za,"size"),Za.forEach(r),dr=i(ne,")."),ne.forEach(r),Lt.forEach(r),Tt=l(t),M=o(t,"DIV",{class:!0});var Ve=n(M);b(me.$$.fragment,Ve),mr=l(Ve),P=o(Ve,"P",{});var se=n(P);pr=i(se,"Normalizes "),et=o(se,"CODE",{});var eo=n(et);fr=i(eo,"image"),eo.forEach(r),ur=i(se," using the mean and standard deviation specified by "),tt=o(se,"CODE",{});var to=n(tt);hr=i(to,"mean"),to.forEach(r),gr=i(se," and "),rt=o(se,"CODE",{});var ro=n(rt);_r=i(ro,"std"),ro.forEach(r),xr=i(se,"."),se.forEach(r),vr=l(Ve),at=o(Ve,"P",{});var ao=n(at);br=i(ao,"image = (image - mean) / std"),ao.forEach(r),Ve.forEach(r),It=l(t),S=o(t,"DIV",{class:!0});var Nt=n(S);b(pe.$$.fragment,Nt),yr=l(Nt),C=o(Nt,"P",{});var We=n(C);$r=i(We,"Rescales "),ot=o(We,"CODE",{});var oo=n(ot);Er=i(oo,"image"),oo.forEach(r),wr=i(We," by "),nt=o(We,"CODE",{});var no=n(nt);Fr=i(no,"scale"),no.forEach(r),kr=i(We,"."),We.forEach(r),Nt.forEach(r),Dt=l(t),V=o(t,"DIV",{class:!0});var Ot=n(V);b(fe.$$.fragment,Ot),Tr=l(Ot),W=o(Ot,"P",{});var Ue=n(W);Ir=i(Ue,"Resizes "),st=o(Ue,"CODE",{});var so=n(st);Dr=i(so,"image"),so.forEach(r),Mr=i(Ue," to (h, w) specified by "),it=o(Ue,"CODE",{});var io=n(it);Pr=i(io,"size"),io.forEach(r),zr=i(Ue," using the PIL library."),Ue.forEach(r),Ot.forEach(r),Mt=l(t),U=o(t,"DIV",{class:!0});var At=n(U);b(ue.$$.fragment,At),jr=l(At),he=o(At,"P",{});var St=n(he);qr=i(St,"Converts "),ct=o(St,"CODE",{});var co=n(ct);Lr=i(co,"image"),co.forEach(r),Nr=i(St,` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),St.forEach(r),At.forEach(r),Pt=l(t),R=o(t,"H2",{class:!0});var Ct=n(R);G=o(Ct,"A",{id:!0,class:!0,href:!0});var lo=n(G);lt=o(lo,"SPAN",{});var mo=n(lt);b(ge.$$.fragment,mo),mo.forEach(r),lo.forEach(r),Or=l(Ct),dt=o(Ct,"SPAN",{});var po=n(dt);Ar=i(po,"ImageProcessorMixin"),po.forEach(r),Ct.forEach(r),zt=l(t),h=o(t,"DIV",{class:!0});var x=n(h);b(_e.$$.fragment,x),Sr=l(x),mt=o(x,"P",{});var fo=n(mt);Cr=i(fo,`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),fo.forEach(r),Vr=l(x),Y=o(x,"DIV",{class:!0});var Vt=n(Y);b(xe.$$.fragment,Vt),Wr=l(Vt),ve=o(Vt,"P",{});var Wt=n(ve);Ur=i(Wt,"Instantiates a type of "),Le=o(Wt,"A",{href:!0});var uo=n(Le);Rr=i(uo,"FeatureExtractionMixin"),uo.forEach(r),Jr=i(Wt,` from a Python dictionary of
parameters.`),Wt.forEach(r),Vt.forEach(r),Hr=l(x),K=o(x,"DIV",{class:!0});var Ut=n(K);b(be.$$.fragment,Ut),Br=l(Ut),ye=o(Ut,"P",{});var Rt=n(ye);Gr=i(Rt,"Instantiates a feature extractor of type "),Ne=o(Rt,"A",{href:!0});var ho=n(Ne);Yr=i(ho,"FeatureExtractionMixin"),ho.forEach(r),Kr=i(Rt,` from the path to
a JSON file of parameters.`),Rt.forEach(r),Ut.forEach(r),Qr=l(x),q=o(x,"DIV",{class:!0});var Re=n(q);b($e.$$.fragment,Re),Xr=l(Re),z=o(Re,"P",{});var ie=n(z);Zr=i(ie,"Instantiate a type of "),Oe=o(ie,"A",{href:!0});var go=n(Oe);ea=i(go,"FeatureExtractionMixin"),go.forEach(r),ta=i(ie," from a feature extractor, "),pt=o(ie,"EM",{});var _o=n(pt);ra=i(_o,"e.g."),_o.forEach(r),aa=i(ie,` a
derived class of `),Ae=o(ie,"A",{href:!0});var xo=n(Ae);oa=i(xo,"SequenceFeatureExtractor"),xo.forEach(r),na=i(ie,"."),ie.forEach(r),sa=l(Re),b(Q.$$.fragment,Re),Re.forEach(r),ia=l(x),X=o(x,"DIV",{class:!0});var Jt=n(X);b(Ee.$$.fragment,Jt),ca=l(Jt),j=o(Jt,"P",{});var ce=n(j);la=i(ce,"From a "),ft=o(ce,"CODE",{});var vo=n(ft);da=i(vo,"pretrained_model_name_or_path"),vo.forEach(r),ma=i(ce,`, resolve to a dictionary of parameters, to be used for instantiating a
feature extractor of type `),Se=o(ce,"A",{href:!0});var bo=n(Se);pa=i(bo,"FeatureExtractionMixin"),bo.forEach(r),fa=i(ce," using "),ut=o(ce,"CODE",{});var yo=n(ut);ua=i(yo,"from_dict"),yo.forEach(r),ha=i(ce,"."),ce.forEach(r),Jt.forEach(r),ga=l(x),L=o(x,"DIV",{class:!0});var Je=n(L);b(we.$$.fragment,Je),_a=l(Je),Fe=o(Je,"P",{});var Ht=n(Fe);xa=i(Ht,`Upload the feature extractor file to the \u{1F917} Model Hub while synchronizing a local clone of the repo in
`),ht=o(Ht,"CODE",{});var $o=n(ht);va=i($o,"repo_path_or_name"),$o.forEach(r),ba=i(Ht,"."),Ht.forEach(r),ya=l(Je),b(Z.$$.fragment,Je),Je.forEach(r),$a=l(x),N=o(x,"DIV",{class:!0});var He=n(N);b(ke.$$.fragment,He),Ea=l(He),Te=o(He,"P",{});var Bt=n(Te);wa=i(Bt,`Register this class with a given auto class. This should only be used for custom feature extractors as the ones
in the library are already mapped with `),gt=o(Bt,"CODE",{});var Eo=n(gt);Fa=i(Eo,"AutoFeatureExtractor"),Eo.forEach(r),ka=i(Bt,"."),Bt.forEach(r),Ta=l(He),b(ee.$$.fragment,He),He.forEach(r),Ia=l(x),te=o(x,"DIV",{class:!0});var Gt=n(te);b(Ie.$$.fragment,Gt),Da=l(Gt),J=o(Gt,"P",{});var Be=n(J);Ma=i(Be,"Save a feature_extractor object to the directory "),_t=o(Be,"CODE",{});var wo=n(_t);Pa=i(wo,"save_directory"),wo.forEach(r),za=i(Be,`, so that it can be re-loaded using the
`),Ce=o(Be,"A",{href:!0});var Fo=n(Ce);ja=i(Fo,"from_pretrained()"),Fo.forEach(r),qa=i(Be," class method."),Be.forEach(r),Gt.forEach(r),La=l(x),re=o(x,"DIV",{class:!0});var Yt=n(re);b(De.$$.fragment,Yt),Na=l(Yt),xt=o(Yt,"P",{});var ko=n(xt);Oa=i(ko,"Serializes this instance to a Python dictionary."),ko.forEach(r),Yt.forEach(r),Aa=l(x),ae=o(x,"DIV",{class:!0});var Kt=n(ae);b(Me.$$.fragment,Kt),Sa=l(Kt),vt=o(Kt,"P",{});var To=n(vt);Ca=i(To,"Save this instance to a JSON file."),To.forEach(r),Kt.forEach(r),Va=l(x),oe=o(x,"DIV",{class:!0});var Qt=n(oe);b(Pe.$$.fragment,Qt),Wa=l(Qt),bt=o(Qt,"P",{});var Io=n(bt);Ua=i(Io,"Serializes this instance to a JSON string."),Io.forEach(r),Qt.forEach(r),x.forEach(r),this.h()},h(){m(f,"name","hf:doc:metadata"),m(f,"content",JSON.stringify(Wo)),m(u,"id","utilities-for-image-processors"),m(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(u,"href","#utilities-for-image-processors"),m(_,"class","relative group"),m(B,"id","transformers.image_transforms.center_crop"),m(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(B,"href","#transformers.image_transforms.center_crop"),m(O,"class","relative group"),m(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(G,"id","transformers.FeatureExtractionMixin"),m(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(G,"href","#transformers.FeatureExtractionMixin"),m(R,"class","relative group"),m(Le,"href","/docs/transformers/v4.24.0/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"),m(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(Ne,"href","/docs/transformers/v4.24.0/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"),m(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(Oe,"href","/docs/transformers/v4.24.0/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"),m(Ae,"href","/docs/transformers/v4.24.0/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor"),m(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(Se,"href","/docs/transformers/v4.24.0/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"),m(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(Ce,"href","/docs/transformers/v4.24.0/en/internal/image_processing_utils#transformers.FeatureExtractionMixin.from_pretrained"),m(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(h,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,p){e(document.head,f),g(t,k,p),g(t,_,p),e(_,u),e(u,I),y(d,I,null),e(_,F),e(_,Ge),e(Ge,Xt),g(t,Et,p),g(t,je,p),e(je,Zt),g(t,wt,p),g(t,qe,p),e(qe,er),g(t,Ft,p),g(t,O,p),e(O,B),e(B,Ye),y(le,Ye,null),e(O,tr),e(O,Ke),e(Ke,rr),g(t,kt,p),g(t,A,p),y(de,A,null),e(A,ar),e(A,D),e(D,or),e(D,Qe),e(Qe,nr),e(D,sr),e(D,Xe),e(Xe,ir),e(D,cr),e(D,Ze),e(Ze,lr),e(D,dr),g(t,Tt,p),g(t,M,p),y(me,M,null),e(M,mr),e(M,P),e(P,pr),e(P,et),e(et,fr),e(P,ur),e(P,tt),e(tt,hr),e(P,gr),e(P,rt),e(rt,_r),e(P,xr),e(M,vr),e(M,at),e(at,br),g(t,It,p),g(t,S,p),y(pe,S,null),e(S,yr),e(S,C),e(C,$r),e(C,ot),e(ot,Er),e(C,wr),e(C,nt),e(nt,Fr),e(C,kr),g(t,Dt,p),g(t,V,p),y(fe,V,null),e(V,Tr),e(V,W),e(W,Ir),e(W,st),e(st,Dr),e(W,Mr),e(W,it),e(it,Pr),e(W,zr),g(t,Mt,p),g(t,U,p),y(ue,U,null),e(U,jr),e(U,he),e(he,qr),e(he,ct),e(ct,Lr),e(he,Nr),g(t,Pt,p),g(t,R,p),e(R,G),e(G,lt),y(ge,lt,null),e(R,Or),e(R,dt),e(dt,Ar),g(t,zt,p),g(t,h,p),y(_e,h,null),e(h,Sr),e(h,mt),e(mt,Cr),e(h,Vr),e(h,Y),y(xe,Y,null),e(Y,Wr),e(Y,ve),e(ve,Ur),e(ve,Le),e(Le,Rr),e(ve,Jr),e(h,Hr),e(h,K),y(be,K,null),e(K,Br),e(K,ye),e(ye,Gr),e(ye,Ne),e(Ne,Yr),e(ye,Kr),e(h,Qr),e(h,q),y($e,q,null),e(q,Xr),e(q,z),e(z,Zr),e(z,Oe),e(Oe,ea),e(z,ta),e(z,pt),e(pt,ra),e(z,aa),e(z,Ae),e(Ae,oa),e(z,na),e(q,sa),y(Q,q,null),e(h,ia),e(h,X),y(Ee,X,null),e(X,ca),e(X,j),e(j,la),e(j,ft),e(ft,da),e(j,ma),e(j,Se),e(Se,pa),e(j,fa),e(j,ut),e(ut,ua),e(j,ha),e(h,ga),e(h,L),y(we,L,null),e(L,_a),e(L,Fe),e(Fe,xa),e(Fe,ht),e(ht,va),e(Fe,ba),e(L,ya),y(Z,L,null),e(h,$a),e(h,N),y(ke,N,null),e(N,Ea),e(N,Te),e(Te,wa),e(Te,gt),e(gt,Fa),e(Te,ka),e(N,Ta),y(ee,N,null),e(h,Ia),e(h,te),y(Ie,te,null),e(te,Da),e(te,J),e(J,Ma),e(J,_t),e(_t,Pa),e(J,za),e(J,Ce),e(Ce,ja),e(J,qa),e(h,La),e(h,re),y(De,re,null),e(re,Na),e(re,xt),e(xt,Oa),e(h,Aa),e(h,ae),y(Me,ae,null),e(ae,Sa),e(ae,vt),e(vt,Ca),e(h,Va),e(h,oe),y(Pe,oe,null),e(oe,Wa),e(oe,bt),e(bt,Ua),jt=!0},p(t,[p]){const ze={};p&2&&(ze.$$scope={dirty:p,ctx:t}),Q.$set(ze);const yt={};p&2&&(yt.$$scope={dirty:p,ctx:t}),Z.$set(yt);const $t={};p&2&&($t.$$scope={dirty:p,ctx:t}),ee.$set($t)},i(t){jt||($(d.$$.fragment,t),$(le.$$.fragment,t),$(de.$$.fragment,t),$(me.$$.fragment,t),$(pe.$$.fragment,t),$(fe.$$.fragment,t),$(ue.$$.fragment,t),$(ge.$$.fragment,t),$(_e.$$.fragment,t),$(xe.$$.fragment,t),$(be.$$.fragment,t),$($e.$$.fragment,t),$(Q.$$.fragment,t),$(Ee.$$.fragment,t),$(we.$$.fragment,t),$(Z.$$.fragment,t),$(ke.$$.fragment,t),$(ee.$$.fragment,t),$(Ie.$$.fragment,t),$(De.$$.fragment,t),$(Me.$$.fragment,t),$(Pe.$$.fragment,t),jt=!0)},o(t){E(d.$$.fragment,t),E(le.$$.fragment,t),E(de.$$.fragment,t),E(me.$$.fragment,t),E(pe.$$.fragment,t),E(fe.$$.fragment,t),E(ue.$$.fragment,t),E(ge.$$.fragment,t),E(_e.$$.fragment,t),E(xe.$$.fragment,t),E(be.$$.fragment,t),E($e.$$.fragment,t),E(Q.$$.fragment,t),E(Ee.$$.fragment,t),E(we.$$.fragment,t),E(Z.$$.fragment,t),E(ke.$$.fragment,t),E(ee.$$.fragment,t),E(Ie.$$.fragment,t),E(De.$$.fragment,t),E(Me.$$.fragment,t),E(Pe.$$.fragment,t),jt=!1},d(t){r(f),t&&r(k),t&&r(_),w(d),t&&r(Et),t&&r(je),t&&r(wt),t&&r(qe),t&&r(Ft),t&&r(O),w(le),t&&r(kt),t&&r(A),w(de),t&&r(Tt),t&&r(M),w(me),t&&r(It),t&&r(S),w(pe),t&&r(Dt),t&&r(V),w(fe),t&&r(Mt),t&&r(U),w(ue),t&&r(Pt),t&&r(R),w(ge),t&&r(zt),t&&r(h),w(_e),w(xe),w(be),w($e),w(Q),w(Ee),w(we),w(Z),w(ke),w(ee),w(Ie),w(De),w(Me),w(Pe)}}}const Wo={local:"utilities-for-image-processors",sections:[{local:"transformers.image_transforms.center_crop",title:"Image Transformations"},{local:"transformers.FeatureExtractionMixin",title:"ImageProcessorMixin"}],title:"Utilities for Image Processors"};function Uo(H){return No(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ko extends zo{constructor(f){super();jo(this,f,Uo,Vo,qo,{})}}export{Ko as default,Wo as metadata};
