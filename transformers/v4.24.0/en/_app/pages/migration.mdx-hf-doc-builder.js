import{S as hw,i as fw,s as pw,e as l,k as c,w as p,t as r,M as mw,c as s,d as o,m as h,a as i,x as m,h as a,b as f,G as e,g as d,y as u,L as uw,q as _,o as v,B as E,v as _w}from"../chunks/vendor-hf-doc-builder.js";import{I as $}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as x}from"../chunks/CodeBlock-hf-doc-builder.js";function vw(DE){let me,Fi,ue,Ge,Pr,no,Ec,Lr,bc,Bi,_e,We,Ir,co,wc,Ue,yc,Sr,kc,Tc,Mr,gc,qi,Zo,$c,Ni,ve,Xe,jr,ho,Cc,Fr,Oc,Hi,er,Dc,Ri,tr,xc,Gi,Ke,Br,Ac,zc,qr,Pc,Wi,Ee,Je,Nr,fo,Lc,Hr,Ic,Ui,Qe,po,Sc,Ve,Mc,Rr,jc,Fc,Bc,qc,be,Nc,Gr,Hc,Rc,Wr,Gc,Wc,Xi,Ye,Uc,Ur,Xc,Kc,Ki,mo,Ji,Ze,Jc,Xr,Qc,Vc,Qi,uo,Vi,we,et,Kr,_o,Yc,Jr,Zc,Yi,A,eh,Qr,th,oh,Vr,rh,ah,Yr,lh,sh,Zi,tt,ih,Zr,nh,dh,en,T,ea,ta,ch,hh,oa,ra,fh,ph,aa,la,mh,uh,sa,ia,_h,vh,na,da,Eh,bh,ca,ha,wh,yh,fa,pa,kh,Th,ma,ua,gh,tn,ye,ot,_a,vo,$h,va,Ch,on,V,Oh,Ea,Dh,xh,ba,Ah,zh,rn,rt,Ph,wa,Lh,Ih,an,Eo,ln,at,Sh,ya,Mh,jh,sn,bo,nn,or,Fh,dn,wo,cn,ke,lt,ka,yo,Bh,Ta,qh,hn,st,Nh,ga,Hh,Rh,fn,rr,Gh,pn,Te,it,$a,ko,Wh,Ca,Uh,mn,nt,Xh,Oa,Kh,Jh,un,dt,Qh,Da,Vh,Yh,_n,To,vn,ct,Zh,xa,ef,tf,En,go,bn,ge,ht,Aa,$o,of,$e,rf,za,af,lf,Pa,sf,nf,wn,ft,df,Co,La,cf,hf,ff,yn,pt,pf,Ia,mf,uf,kn,Ce,mt,Sa,Oo,_f,Ma,vf,Tn,z,Ef,ja,bf,wf,Fa,yf,kf,Ba,Tf,gf,gn,ut,$f,qa,Cf,Of,$n,Do,Cn,_t,Df,Na,xf,Af,On,xo,Dn,ar,zf,xn,Ao,An,Oe,vt,Ha,zo,Pf,Ra,Lf,zn,Et,If,Po,Sf,Mf,Pn,lr,jf,Ln,sr,Ff,In,b,P,Ga,Bf,qf,Wa,Nf,Hf,Ua,Rf,Gf,Xa,Wf,Uf,Xf,L,Ka,Kf,Jf,Ja,Qf,Vf,Qa,Yf,Zf,Va,ep,tp,op,Y,Ya,rp,ap,Za,lp,sp,el,ip,np,dp,Z,tl,cp,hp,ol,fp,pp,rl,mp,up,_p,ee,al,vp,Ep,ll,bp,wp,sl,yp,kp,Tp,te,il,gp,$p,nl,Cp,Op,dl,Dp,xp,Ap,oe,cl,zp,Pp,hl,Lp,Ip,fl,Sp,Mp,jp,re,pl,Fp,Bp,ml,qp,Np,ul,Hp,Rp,Gp,ae,_l,Wp,Up,vl,Xp,Kp,El,Jp,Qp,Vp,le,bl,Yp,Zp,wl,em,tm,yl,om,rm,am,se,kl,lm,sm,Tl,im,nm,gl,dm,cm,Sn,ir,hm,Mn,I,bt,$l,fm,pm,Cl,mm,um,_m,wt,Ol,vm,Em,Dl,bm,wm,ym,yt,xl,km,Tm,Al,gm,$m,Cm,kt,zl,Om,Dm,Pl,xm,Am,jn,nr,zm,Fn,ie,De,Pm,Ll,Lm,Im,Il,Sm,Mm,jm,xe,Fm,Sl,Bm,qm,Ml,Nm,Hm,Rm,Ae,Gm,jl,Wm,Um,Fl,Xm,Km,Bn,Tt,Jm,Bl,Qm,Vm,qn,g,j,Ym,ql,Zm,eu,Nl,tu,ou,Hl,ru,au,lu,F,su,Rl,iu,nu,Gl,du,cu,Wl,hu,fu,pu,ze,mu,Ul,uu,_u,Xl,vu,Eu,bu,B,wu,Kl,yu,ku,Jl,Tu,gu,Ql,$u,Cu,Ou,q,Du,Vl,xu,Au,Yl,zu,Pu,Zl,Lu,Iu,Su,N,Mu,es,ju,Fu,ts,Bu,qu,os,Nu,Hu,Ru,H,Gu,rs,Wu,Uu,as,Xu,Ku,ls,Ju,Qu,Vu,R,Yu,ss,Zu,e_,is,t_,o_,ns,r_,a_,Nn,gt,l_,ds,s_,i_,Hn,D,G,n_,cs,d_,c_,hs,h_,f_,fs,p_,m_,u_,W,__,ps,v_,E_,ms,b_,w_,us,y_,k_,T_,U,g_,_s,$_,C_,vs,O_,D_,Es,x_,A_,z_,X,P_,bs,L_,I_,ws,S_,M_,ys,j_,F_,B_,K,q_,ks,N_,H_,Ts,R_,G_,gs,W_,U_,Rn,$t,X_,$s,K_,J_,Gn,dr,J,Q_,Cs,V_,Y_,Os,Z_,ev,Ds,tv,ov,Wn,cr,rv,Un,Ct,Pe,av,xs,lv,sv,As,iv,nv,dv,Le,cv,zs,hv,fv,Ps,pv,mv,Xn,hr,uv,Kn,fr,Q,_v,Ls,vv,Ev,Is,bv,wv,Ss,yv,kv,Jn,Ie,Ot,Ms,Lo,Tv,js,gv,Qn,Dt,$v,Fs,Cv,Ov,Vn,Se,xt,Bs,Io,Dv,Me,xv,qs,Av,zv,Ns,Pv,Lv,Yn,S,Iv,Hs,Sv,Mv,Rs,jv,Fv,Gs,Bv,qv,Zn,At,Nv,Ws,Hv,Rv,ed,zt,Gv,Us,Wv,Uv,td,je,Pt,Xs,So,Xv,Ks,Kv,od,Lt,Jv,Js,Qv,Vv,rd,Fe,It,Qs,Mo,Yv,pr,Zv,Vs,e1,ad,ne,t1,Ys,o1,r1,Zs,a1,l1,ld,St,s1,jo,i1,n1,sd,Mt,d1,ei,c1,h1,id,de,f1,ti,p1,m1,oi,u1,_1,nd,Fo,dd,Be,jt,ri,Bo,v1,ai,E1,cd,Ft,b1,li,w1,y1,hd,Bt,si,qe,k1,ii,T1,g1,ni,$1,C1,O1,di,w,D1,ci,x1,A1,hi,z1,P1,fi,L1,I1,pi,S1,M1,mi,j1,F1,ui,B1,q1,_i,N1,H1,vi,R1,G1,Ei,W1,U1,bi,X1,K1,fd,qt,J1,wi,Q1,V1,pd,mr,Y1,md,qo,ud,Ne,Nt,yi,No,Z1,ki,eE,_d,M,tE,Ti,oE,rE,gi,aE,lE,$i,sE,iE,vd,ce,Ci,nE,dE,Oi,cE,hE,Di,fE,Ed,he,pE,xi,mE,uE,Ai,_E,vE,bd,Ht,EE,Ho,bE,wE,wd,fe,yE,zi,kE,TE,Pi,gE,$E,yd,Ro,kd;return no=new $({}),co=new $({}),ho=new $({}),fo=new $({}),mo=new x({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}}),uo=new x({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased", use_fast=False)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, use_fast=<span class="hljs-literal">False</span>)`}}),_o=new $({}),vo=new $({}),Eo=new x({props:{code:"pip install transformers",highlighted:"pip install transformers"}}),bo=new x({props:{code:"pip install transformers[sentencepiece]",highlighted:"pip install transformers[sentencepiece]"}}),wo=new x({props:{code:"pip install transformers sentencepiece",highlighted:"pip install transformers sentencepiece"}}),yo=new $({}),ko=new $({}),To=new x({props:{code:"from transformers.modeling_bert import BertLayer",highlighted:"from transformers.modeling_bert import BertLayer"}}),go=new x({props:{code:"from transformers.models.bert.modeling_bert import BertLayer",highlighted:"from transformers.models.bert.modeling_bert import BertLayer"}}),$o=new $({}),Oo=new $({}),Do=new x({props:{code:`model = BertModel.from_pretrained("bert-base-cased")
outputs = model(**inputs)`,highlighted:`model = BertModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
outputs = model(**inputs)`}}),xo=new x({props:{code:`model = BertModel.from_pretrained("bert-base-cased")
outputs = model(**inputs, return_dict=False)`,highlighted:`model = BertModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
outputs = model(**inputs, return_dict=False)`}}),Ao=new x({props:{code:`model = BertModel.from_pretrained("bert-base-cased", return_dict=False)
outputs = model(**inputs)`,highlighted:`model = BertModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, return_dict=False)
outputs = model(**inputs)`}}),zo=new $({}),Lo=new $({}),Io=new $({}),So=new $({}),Mo=new $({}),Fo=new x({props:{code:`# Let's load our model
model = BertForSequenceClassification.from_pretrained("bert-base-uncased")

# If you used to have this line in pytorch-pretrained-bert:
loss = model(input_ids, labels=labels)

# Now just use this line in \u{1F917} Transformers to extract the loss from the output tuple:
outputs = model(input_ids, labels=labels)
loss = outputs[0]

# In \u{1F917} Transformers you can also have access to the logits:
loss, logits = outputs[:2]

# And even the attention weights if you configure the model to output them (and other outputs too, see the docstrings and documentation)
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", output_attentions=True)
outputs = model(input_ids, labels=labels)
loss, logits, attentions = outputs`,highlighted:`<span class="hljs-comment"># Let&#x27;s load our model</span>
model = BertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-comment"># If you used to have this line in pytorch-pretrained-bert:</span>
loss = model(input_ids, labels=labels)

<span class="hljs-comment"># Now just use this line in \u{1F917} Transformers to extract the loss from the output tuple:</span>
outputs = model(input_ids, labels=labels)
loss = outputs[<span class="hljs-number">0</span>]

<span class="hljs-comment"># In \u{1F917} Transformers you can also have access to the logits:</span>
loss, logits = outputs[:<span class="hljs-number">2</span>]

<span class="hljs-comment"># And even the attention weights if you configure the model to output them (and other outputs too, see the docstrings and documentation)</span>
model = BertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
outputs = model(input_ids, labels=labels)
loss, logits, attentions = outputs`}}),Bo=new $({}),qo=new x({props:{code:`### Let's load a model and tokenizer
model = BertForSequenceClassification.from_pretrained("bert-base-uncased")
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

### Do some stuff to our model and tokenizer
# Ex: add new tokens to the vocabulary and embeddings of our model
tokenizer.add_tokens(["[SPECIAL_TOKEN_1]", "[SPECIAL_TOKEN_2]"])
model.resize_token_embeddings(len(tokenizer))
# Train our model
train(model)

### Now let's save our model and tokenizer to a directory
model.save_pretrained("./my_saved_model_directory/")
tokenizer.save_pretrained("./my_saved_model_directory/")

### Reload the model and the tokenizer
model = BertForSequenceClassification.from_pretrained("./my_saved_model_directory/")
tokenizer = BertTokenizer.from_pretrained("./my_saved_model_directory/")`,highlighted:`<span class="hljs-comment">### Let&#x27;s load a model and tokenizer</span>
model = BertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)
tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-comment">### Do some stuff to our model and tokenizer</span>
<span class="hljs-comment"># Ex: add new tokens to the vocabulary and embeddings of our model</span>
tokenizer.add_tokens([<span class="hljs-string">&quot;[SPECIAL_TOKEN_1]&quot;</span>, <span class="hljs-string">&quot;[SPECIAL_TOKEN_2]&quot;</span>])
model.resize_token_embeddings(<span class="hljs-built_in">len</span>(tokenizer))
<span class="hljs-comment"># Train our model</span>
train(model)

<span class="hljs-comment">### Now let&#x27;s save our model and tokenizer to a directory</span>
model.save_pretrained(<span class="hljs-string">&quot;./my_saved_model_directory/&quot;</span>)
tokenizer.save_pretrained(<span class="hljs-string">&quot;./my_saved_model_directory/&quot;</span>)

<span class="hljs-comment">### Reload the model and the tokenizer</span>
model = BertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./my_saved_model_directory/&quot;</span>)
tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;./my_saved_model_directory/&quot;</span>)`}}),No=new $({}),Ro=new x({props:{code:`# Parameters:
lr = 1e-3
max_grad_norm = 1.0
num_training_steps = 1000
num_warmup_steps = 100
warmup_proportion = float(num_warmup_steps) / float(num_training_steps)  # 0.1

### Previously BertAdam optimizer was instantiated like this:
optimizer = BertAdam(
    model.parameters(),
    lr=lr,
    schedule="warmup_linear",
    warmup=warmup_proportion,
    num_training_steps=num_training_steps,
)
### and used like this:
for batch in train_data:
    loss = model(batch)
    loss.backward()
    optimizer.step()

### In \u{1F917} Transformers, optimizer and schedules are split and instantiated like this:
optimizer = AdamW(
    model.parameters(), lr=lr, correct_bias=False
)  # To reproduce BertAdam specific behavior set correct_bias=False
scheduler = get_linear_schedule_with_warmup(
    optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps
)  # PyTorch scheduler
### and used like this:
for batch in train_data:
    loss = model(batch)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(
        model.parameters(), max_grad_norm
    )  # Gradient clipping is not in AdamW anymore (so you can use amp without issue)
    optimizer.step()
    scheduler.step()`,highlighted:`<span class="hljs-comment"># Parameters:</span>
lr = <span class="hljs-number">1e-3</span>
max_grad_norm = <span class="hljs-number">1.0</span>
num_training_steps = <span class="hljs-number">1000</span>
num_warmup_steps = <span class="hljs-number">100</span>
warmup_proportion = <span class="hljs-built_in">float</span>(num_warmup_steps) / <span class="hljs-built_in">float</span>(num_training_steps)  <span class="hljs-comment"># 0.1</span>

<span class="hljs-comment">### Previously BertAdam optimizer was instantiated like this:</span>
optimizer = BertAdam(
    model.parameters(),
    lr=lr,
    schedule=<span class="hljs-string">&quot;warmup_linear&quot;</span>,
    warmup=warmup_proportion,
    num_training_steps=num_training_steps,
)
<span class="hljs-comment">### and used like this:</span>
<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_data:
    loss = model(batch)
    loss.backward()
    optimizer.step()

<span class="hljs-comment">### In \u{1F917} Transformers, optimizer and schedules are split and instantiated like this:</span>
optimizer = AdamW(
    model.parameters(), lr=lr, correct_bias=<span class="hljs-literal">False</span>
)  <span class="hljs-comment"># To reproduce BertAdam specific behavior set correct_bias=False</span>
scheduler = get_linear_schedule_with_warmup(
    optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps
)  <span class="hljs-comment"># PyTorch scheduler</span>
<span class="hljs-comment">### and used like this:</span>
<span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_data:
    loss = model(batch)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(
        model.parameters(), max_grad_norm
    )  <span class="hljs-comment"># Gradient clipping is not in AdamW anymore (so you can use amp without issue)</span>
    optimizer.step()
    scheduler.step()`}}),{c(){me=l("meta"),Fi=c(),ue=l("h1"),Ge=l("a"),Pr=l("span"),p(no.$$.fragment),Ec=c(),Lr=l("span"),bc=r("Migrating from previous packages"),Bi=c(),_e=l("h2"),We=l("a"),Ir=l("span"),p(co.$$.fragment),wc=c(),Ue=l("span"),yc=r("Migrating from transformers "),Sr=l("code"),kc=r("v3.x"),Tc=r(" to "),Mr=l("code"),gc=r("v4.x"),qi=c(),Zo=l("p"),$c=r(`A couple of changes were introduced when the switch from version 3 to version 4 was done. Below is a summary of the
expected changes:`),Ni=c(),ve=l("h4"),Xe=l("a"),jr=l("span"),p(ho.$$.fragment),Cc=c(),Fr=l("span"),Oc=r("1. AutoTokenizers and pipelines now use fast (rust) tokenizers by default."),Hi=c(),er=l("p"),Dc=r("The python and rust tokenizers have roughly the same API, but the rust tokenizers have a more complete feature set."),Ri=c(),tr=l("p"),xc=r("This introduces two breaking changes:"),Gi=c(),Ke=l("ul"),Br=l("li"),Ac=r("The handling of overflowing tokens between the python and rust tokenizers is different."),zc=c(),qr=l("li"),Pc=r("The rust tokenizers do not accept integers in the encoding methods."),Wi=c(),Ee=l("h5"),Je=l("a"),Nr=l("span"),p(fo.$$.fragment),Lc=c(),Hr=l("span"),Ic=r("How to obtain the same behavior as v3.x in v4.x"),Ui=c(),Qe=l("ul"),po=l("li"),Sc=r("The pipelines now contain additional features out of the box. See the "),Ve=l("a"),Mc=r("token-classification pipeline with the "),Rr=l("code"),jc=r("grouped_entities"),Fc=r(" flag"),Bc=r("."),qc=c(),be=l("li"),Nc=r("The auto-tokenizers now return rust tokenizers. In order to obtain the python tokenizers instead, the user may use the "),Gr=l("code"),Hc=r("use_fast"),Rc=r(" flag by setting it to "),Wr=l("code"),Gc=r("False"),Wc=r(":"),Xi=c(),Ye=l("p"),Uc=r("In version "),Ur=l("code"),Xc=r("v3.x"),Kc=r(":"),Ki=c(),p(mo.$$.fragment),Ji=c(),Ze=l("p"),Jc=r("to obtain the same in version "),Xr=l("code"),Qc=r("v4.x"),Vc=r(":"),Qi=c(),p(uo.$$.fragment),Vi=c(),we=l("h4"),et=l("a"),Kr=l("span"),p(_o.$$.fragment),Yc=c(),Jr=l("span"),Zc=r("2. SentencePiece is removed from the required dependencies"),Yi=c(),A=l("p"),eh=r("The requirement on the SentencePiece dependency has been lifted from the "),Qr=l("code"),th=r("setup.py"),oh=r(". This is done so that we may have a channel on anaconda cloud without relying on "),Vr=l("code"),rh=r("conda-forge"),ah=r(". This means that the tokenizers that depend on the SentencePiece library will not be available with a standard "),Yr=l("code"),lh=r("transformers"),sh=r(" installation."),Zi=c(),tt=l("p"),ih=r("This includes the "),Zr=l("strong"),nh=r("slow"),dh=r(" versions of:"),en=c(),T=l("ul"),ea=l("li"),ta=l("code"),ch=r("XLNetTokenizer"),hh=c(),oa=l("li"),ra=l("code"),fh=r("AlbertTokenizer"),ph=c(),aa=l("li"),la=l("code"),mh=r("CamembertTokenizer"),uh=c(),sa=l("li"),ia=l("code"),_h=r("MBartTokenizer"),vh=c(),na=l("li"),da=l("code"),Eh=r("PegasusTokenizer"),bh=c(),ca=l("li"),ha=l("code"),wh=r("T5Tokenizer"),yh=c(),fa=l("li"),pa=l("code"),kh=r("ReformerTokenizer"),Th=c(),ma=l("li"),ua=l("code"),gh=r("XLMRobertaTokenizer"),tn=c(),ye=l("h5"),ot=l("a"),_a=l("span"),p(vo.$$.fragment),$h=c(),va=l("span"),Ch=r("How to obtain the same behavior as v3.x in v4.x"),on=c(),V=l("p"),Oh=r("In order to obtain the same behavior as version "),Ea=l("code"),Dh=r("v3.x"),xh=r(", you should install "),ba=l("code"),Ah=r("sentencepiece"),zh=r(" additionally:"),rn=c(),rt=l("p"),Ph=r("In version "),wa=l("code"),Lh=r("v3.x"),Ih=r(":"),an=c(),p(Eo.$$.fragment),ln=c(),at=l("p"),Sh=r("to obtain the same in version "),ya=l("code"),Mh=r("v4.x"),jh=r(":"),sn=c(),p(bo.$$.fragment),nn=c(),or=l("p"),Fh=r("or"),dn=c(),p(wo.$$.fragment),cn=c(),ke=l("h4"),lt=l("a"),ka=l("span"),p(yo.$$.fragment),Bh=c(),Ta=l("span"),qh=r("3. The architecture of the repo has been updated so that each model resides in its folder"),hn=c(),st=l("p"),Nh=r("The past and foreseeable addition of new models means that the number of files in the directory "),ga=l("code"),Hh=r("src/transformers"),Rh=r(" keeps growing and becomes harder to navigate and understand. We made the choice to put each model and the files accompanying it in their own sub-directories."),fn=c(),rr=l("p"),Gh=r("This is a breaking change as importing intermediary layers using a model\u2019s module directly needs to be done via a different path."),pn=c(),Te=l("h5"),it=l("a"),$a=l("span"),p(ko.$$.fragment),Wh=c(),Ca=l("span"),Uh=r("How to obtain the same behavior as v3.x in v4.x"),mn=c(),nt=l("p"),Xh=r("In order to obtain the same behavior as version "),Oa=l("code"),Kh=r("v3.x"),Jh=r(", you should update the path used to access the layers."),un=c(),dt=l("p"),Qh=r("In version "),Da=l("code"),Vh=r("v3.x"),Yh=r(":"),_n=c(),p(To.$$.fragment),vn=c(),ct=l("p"),Zh=r("to obtain the same in version "),xa=l("code"),ef=r("v4.x"),tf=r(":"),En=c(),p(go.$$.fragment),bn=c(),ge=l("h4"),ht=l("a"),Aa=l("span"),p($o.$$.fragment),of=c(),$e=l("span"),rf=r("4. Switching the "),za=l("code"),af=r("return_dict"),lf=r(" argument to "),Pa=l("code"),sf=r("True"),nf=r(" by default"),wn=c(),ft=l("p"),df=r("The "),Co=l("a"),La=l("code"),cf=r("return_dict"),hf=r(" argument"),ff=r(" enables the return of dict-like python objects containing the model outputs, instead of the standard tuples. This object is self-documented as keys can be used to retrieve values, while also behaving as a tuple as users may retrieve objects by index or by slice."),yn=c(),pt=l("p"),pf=r("This is a breaking change as the limitation of that tuple is that it cannot be unpacked: "),Ia=l("code"),mf=r("value0, value1 = outputs"),uf=r(" will not work."),kn=c(),Ce=l("h5"),mt=l("a"),Sa=l("span"),p(Oo.$$.fragment),_f=c(),Ma=l("span"),vf=r("How to obtain the same behavior as v3.x in v4.x"),Tn=c(),z=l("p"),Ef=r("In order to obtain the same behavior as version "),ja=l("code"),bf=r("v3.x"),wf=r(", you should specify the "),Fa=l("code"),yf=r("return_dict"),kf=r(" argument to "),Ba=l("code"),Tf=r("False"),gf=r(", either in the model configuration or during the forward pass."),gn=c(),ut=l("p"),$f=r("In version "),qa=l("code"),Cf=r("v3.x"),Of=r(":"),$n=c(),p(Do.$$.fragment),Cn=c(),_t=l("p"),Df=r("to obtain the same in version "),Na=l("code"),xf=r("v4.x"),Af=r(":"),On=c(),p(xo.$$.fragment),Dn=c(),ar=l("p"),zf=r("or"),xn=c(),p(Ao.$$.fragment),An=c(),Oe=l("h4"),vt=l("a"),Ha=l("span"),p(zo.$$.fragment),Pf=c(),Ra=l("span"),Lf=r("5. Removed some deprecated attributes"),zn=c(),Et=l("p"),If=r("Attributes that were deprecated have been removed if they had been deprecated for at least a month. The full list of deprecated attributes can be found in "),Po=l("a"),Sf=r("#8604"),Mf=r("."),Pn=c(),lr=l("p"),jf=r("Here is a list of these attributes/methods/arguments and what their replacements should be:"),Ln=c(),sr=l("p"),Ff=r("In several models, the labels become consistent with the other models:"),In=c(),b=l("ul"),P=l("li"),Ga=l("code"),Bf=r("masked_lm_labels"),qf=r(" becomes "),Wa=l("code"),Nf=r("labels"),Hf=r(" in "),Ua=l("code"),Rf=r("AlbertForMaskedLM"),Gf=r(" and "),Xa=l("code"),Wf=r("AlbertForPreTraining"),Uf=r("."),Xf=c(),L=l("li"),Ka=l("code"),Kf=r("masked_lm_labels"),Jf=r(" becomes "),Ja=l("code"),Qf=r("labels"),Vf=r(" in "),Qa=l("code"),Yf=r("BertForMaskedLM"),Zf=r(" and "),Va=l("code"),ep=r("BertForPreTraining"),tp=r("."),op=c(),Y=l("li"),Ya=l("code"),rp=r("masked_lm_labels"),ap=r(" becomes "),Za=l("code"),lp=r("labels"),sp=r(" in "),el=l("code"),ip=r("DistilBertForMaskedLM"),np=r("."),dp=c(),Z=l("li"),tl=l("code"),cp=r("masked_lm_labels"),hp=r(" becomes "),ol=l("code"),fp=r("labels"),pp=r(" in "),rl=l("code"),mp=r("ElectraForMaskedLM"),up=r("."),_p=c(),ee=l("li"),al=l("code"),vp=r("masked_lm_labels"),Ep=r(" becomes "),ll=l("code"),bp=r("labels"),wp=r(" in "),sl=l("code"),yp=r("LongformerForMaskedLM"),kp=r("."),Tp=c(),te=l("li"),il=l("code"),gp=r("masked_lm_labels"),$p=r(" becomes "),nl=l("code"),Cp=r("labels"),Op=r(" in "),dl=l("code"),Dp=r("MobileBertForMaskedLM"),xp=r("."),Ap=c(),oe=l("li"),cl=l("code"),zp=r("masked_lm_labels"),Pp=r(" becomes "),hl=l("code"),Lp=r("labels"),Ip=r(" in "),fl=l("code"),Sp=r("RobertaForMaskedLM"),Mp=r("."),jp=c(),re=l("li"),pl=l("code"),Fp=r("lm_labels"),Bp=r(" becomes "),ml=l("code"),qp=r("labels"),Np=r(" in "),ul=l("code"),Hp=r("BartForConditionalGeneration"),Rp=r("."),Gp=c(),ae=l("li"),_l=l("code"),Wp=r("lm_labels"),Up=r(" becomes "),vl=l("code"),Xp=r("labels"),Kp=r(" in "),El=l("code"),Jp=r("GPT2DoubleHeadsModel"),Qp=r("."),Vp=c(),le=l("li"),bl=l("code"),Yp=r("lm_labels"),Zp=r(" becomes "),wl=l("code"),em=r("labels"),tm=r(" in "),yl=l("code"),om=r("OpenAIGPTDoubleHeadsModel"),rm=r("."),am=c(),se=l("li"),kl=l("code"),lm=r("lm_labels"),sm=r(" becomes "),Tl=l("code"),im=r("labels"),nm=r(" in "),gl=l("code"),dm=r("T5ForConditionalGeneration"),cm=r("."),Sn=c(),ir=l("p"),hm=r("In several models, the caching mechanism becomes consistent with the other models:"),Mn=c(),I=l("ul"),bt=l("li"),$l=l("code"),fm=r("decoder_cached_states"),pm=r(" becomes "),Cl=l("code"),mm=r("past_key_values"),um=r(" in all BART-like, FSMT and T5 models."),_m=c(),wt=l("li"),Ol=l("code"),vm=r("decoder_past_key_values"),Em=r(" becomes "),Dl=l("code"),bm=r("past_key_values"),wm=r(" in all BART-like, FSMT and T5 models."),ym=c(),yt=l("li"),xl=l("code"),km=r("past"),Tm=r(" becomes "),Al=l("code"),gm=r("past_key_values"),$m=r(" in all CTRL models."),Cm=c(),kt=l("li"),zl=l("code"),Om=r("past"),Dm=r(" becomes "),Pl=l("code"),xm=r("past_key_values"),Am=r(" in all GPT-2 models."),jn=c(),nr=l("p"),zm=r("Regarding the tokenizer classes:"),Fn=c(),ie=l("ul"),De=l("li"),Pm=r("The tokenizer attribute "),Ll=l("code"),Lm=r("max_len"),Im=r(" becomes "),Il=l("code"),Sm=r("model_max_length"),Mm=r("."),jm=c(),xe=l("li"),Fm=r("The tokenizer attribute "),Sl=l("code"),Bm=r("return_lengths"),qm=r(" becomes "),Ml=l("code"),Nm=r("return_length"),Hm=r("."),Rm=c(),Ae=l("li"),Gm=r("The tokenizer encoding argument "),jl=l("code"),Wm=r("is_pretokenized"),Um=r(" becomes "),Fl=l("code"),Xm=r("is_split_into_words"),Km=r("."),Bn=c(),Tt=l("p"),Jm=r("Regarding the "),Bl=l("code"),Qm=r("Trainer"),Vm=r(" class:"),qn=c(),g=l("ul"),j=l("li"),Ym=r("The "),ql=l("code"),Zm=r("Trainer"),eu=r(" argument "),Nl=l("code"),tu=r("tb_writer"),ou=r(" is removed in favor of the callback "),Hl=l("code"),ru=r("TensorBoardCallback(tb_writer=...)"),au=r("."),lu=c(),F=l("li"),su=r("The "),Rl=l("code"),iu=r("Trainer"),nu=r(" argument "),Gl=l("code"),du=r("prediction_loss_only"),cu=r(" is removed in favor of the class argument "),Wl=l("code"),hu=r("args.prediction_loss_only"),fu=r("."),pu=c(),ze=l("li"),mu=r("The "),Ul=l("code"),uu=r("Trainer"),_u=r(" attribute "),Xl=l("code"),vu=r("data_collator"),Eu=r(" should be a callable."),bu=c(),B=l("li"),wu=r("The "),Kl=l("code"),yu=r("Trainer"),ku=r(" method "),Jl=l("code"),Tu=r("_log"),gu=r(" is deprecated in favor of "),Ql=l("code"),$u=r("log"),Cu=r("."),Ou=c(),q=l("li"),Du=r("The "),Vl=l("code"),xu=r("Trainer"),Au=r(" method "),Yl=l("code"),zu=r("_training_step"),Pu=r(" is deprecated in favor of "),Zl=l("code"),Lu=r("training_step"),Iu=r("."),Su=c(),N=l("li"),Mu=r("The "),es=l("code"),ju=r("Trainer"),Fu=r(" method "),ts=l("code"),Bu=r("_prediction_loop"),qu=r(" is deprecated in favor of "),os=l("code"),Nu=r("prediction_loop"),Hu=r("."),Ru=c(),H=l("li"),Gu=r("The "),rs=l("code"),Wu=r("Trainer"),Uu=r(" method "),as=l("code"),Xu=r("is_local_master"),Ku=r(" is deprecated in favor of "),ls=l("code"),Ju=r("is_local_process_zero"),Qu=r("."),Vu=c(),R=l("li"),Yu=r("The "),ss=l("code"),Zu=r("Trainer"),e_=r(" method "),is=l("code"),t_=r("is_world_master"),o_=r(" is deprecated in favor of "),ns=l("code"),r_=r("is_world_process_zero"),a_=r("."),Nn=c(),gt=l("p"),l_=r("Regarding the "),ds=l("code"),s_=r("TFTrainer"),i_=r(" class:"),Hn=c(),D=l("ul"),G=l("li"),n_=r("The "),cs=l("code"),d_=r("TFTrainer"),c_=r(" argument "),hs=l("code"),h_=r("prediction_loss_only"),f_=r(" is removed in favor of the class argument "),fs=l("code"),p_=r("args.prediction_loss_only"),m_=r("."),u_=c(),W=l("li"),__=r("The "),ps=l("code"),v_=r("Trainer"),E_=r(" method "),ms=l("code"),b_=r("_log"),w_=r(" is deprecated in favor of "),us=l("code"),y_=r("log"),k_=r("."),T_=c(),U=l("li"),g_=r("The "),_s=l("code"),$_=r("TFTrainer"),C_=r(" method "),vs=l("code"),O_=r("_prediction_loop"),D_=r(" is deprecated in favor of "),Es=l("code"),x_=r("prediction_loop"),A_=r("."),z_=c(),X=l("li"),P_=r("The "),bs=l("code"),L_=r("TFTrainer"),I_=r(" method "),ws=l("code"),S_=r("_setup_wandb"),M_=r(" is deprecated in favor of "),ys=l("code"),j_=r("setup_wandb"),F_=r("."),B_=c(),K=l("li"),q_=r("The "),ks=l("code"),N_=r("TFTrainer"),H_=r(" method "),Ts=l("code"),R_=r("_run_model"),G_=r(" is deprecated in favor of "),gs=l("code"),W_=r("run_model"),U_=r("."),Rn=c(),$t=l("p"),X_=r("Regarding the "),$s=l("code"),K_=r("TrainingArguments"),J_=r(" class:"),Gn=c(),dr=l("ul"),J=l("li"),Q_=r("The "),Cs=l("code"),V_=r("TrainingArguments"),Y_=r(" argument "),Os=l("code"),Z_=r("evaluate_during_training"),ev=r(" is deprecated in favor of "),Ds=l("code"),tv=r("evaluation_strategy"),ov=r("."),Wn=c(),cr=l("p"),rv=r("Regarding the Transfo-XL model:"),Un=c(),Ct=l("ul"),Pe=l("li"),av=r("The Transfo-XL configuration attribute "),xs=l("code"),lv=r("tie_weight"),sv=r(" becomes "),As=l("code"),iv=r("tie_words_embeddings"),nv=r("."),dv=c(),Le=l("li"),cv=r("The Transfo-XL modeling method "),zs=l("code"),hv=r("reset_length"),fv=r(" becomes "),Ps=l("code"),pv=r("reset_memory_length"),mv=r("."),Xn=c(),hr=l("p"),uv=r("Regarding pipelines:"),Kn=c(),fr=l("ul"),Q=l("li"),_v=r("The "),Ls=l("code"),vv=r("FillMaskPipeline"),Ev=r(" argument "),Is=l("code"),bv=r("topk"),wv=r(" becomes "),Ss=l("code"),yv=r("top_k"),kv=r("."),Jn=c(),Ie=l("h2"),Ot=l("a"),Ms=l("span"),p(Lo.$$.fragment),Tv=c(),js=l("span"),gv=r("Migrating from pytorch-transformers to \u{1F917} Transformers"),Qn=c(),Dt=l("p"),$v=r("Here is a quick summary of what you should take care of when migrating from "),Fs=l("code"),Cv=r("pytorch-transformers"),Ov=r(" to \u{1F917} Transformers."),Vn=c(),Se=l("h3"),xt=l("a"),Bs=l("span"),p(Io.$$.fragment),Dv=c(),Me=l("span"),xv=r("Positional order of some models' keywords inputs ("),qs=l("code"),Av=r("attention_mask"),zv=r(", "),Ns=l("code"),Pv=r("token_type_ids"),Lv=r("...) changed"),Yn=c(),S=l("p"),Iv=r("To be able to use Torchscript (see #1010, #1204 and #1195) the specific order of some models "),Hs=l("strong"),Sv=r("keywords inputs"),Mv=r(" ("),Rs=l("code"),jv=r("attention_mask"),Fv=r(", "),Gs=l("code"),Bv=r("token_type_ids"),qv=r("\u2026) has been changed."),Zn=c(),At=l("p"),Nv=r("If you used to call the models with keyword names for keyword arguments, e.g. "),Ws=l("code"),Hv=r("model(inputs_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)"),Rv=r(", this should not cause any change."),ed=c(),zt=l("p"),Gv=r("If you used to call the models with positional inputs for keyword arguments, e.g. "),Us=l("code"),Wv=r("model(inputs_ids, attention_mask, token_type_ids)"),Uv=r(", you may have to double check the exact order of input arguments."),td=c(),je=l("h2"),Pt=l("a"),Xs=l("span"),p(So.$$.fragment),Xv=c(),Ks=l("span"),Kv=r("Migrating from pytorch-pretrained-bert"),od=c(),Lt=l("p"),Jv=r("Here is a quick summary of what you should take care of when migrating from "),Js=l("code"),Qv=r("pytorch-pretrained-bert"),Vv=r(" to \u{1F917} Transformers"),rd=c(),Fe=l("h3"),It=l("a"),Qs=l("span"),p(Mo.$$.fragment),Yv=c(),pr=l("span"),Zv=r("Models always output "),Vs=l("code"),e1=r("tuples"),ad=c(),ne=l("p"),t1=r("The main breaking change when migrating from "),Ys=l("code"),o1=r("pytorch-pretrained-bert"),r1=r(" to \u{1F917} Transformers is that the models forward method always outputs a "),Zs=l("code"),a1=r("tuple"),l1=r(" with various elements depending on the model and the configuration parameters."),ld=c(),St=l("p"),s1=r("The exact content of the tuples for each model are detailed in the models\u2019 docstrings and the "),jo=l("a"),i1=r("documentation"),n1=r("."),sd=c(),Mt=l("p"),d1=r("In pretty much every case, you will be fine by taking the first element of the output as the output you previously used in "),ei=l("code"),c1=r("pytorch-pretrained-bert"),h1=r("."),id=c(),de=l("p"),f1=r("Here is a "),ti=l("code"),p1=r("pytorch-pretrained-bert"),m1=r(" to \u{1F917} Transformers conversion example for a "),oi=l("code"),u1=r("BertForSequenceClassification"),_1=r(" classification model:"),nd=c(),p(Fo.$$.fragment),dd=c(),Be=l("h3"),jt=l("a"),ri=l("span"),p(Bo.$$.fragment),v1=c(),ai=l("span"),E1=r("Serialization"),cd=c(),Ft=l("p"),b1=r("Breaking change in the "),li=l("code"),w1=r("from_pretrained()"),y1=r("method:"),hd=c(),Bt=l("ol"),si=l("li"),qe=l("p"),k1=r("Models are now set in evaluation mode by default when instantiated with the "),ii=l("code"),T1=r("from_pretrained()"),g1=r(" method. To train them don\u2019t forget to set them back in training mode ("),ni=l("code"),$1=r("model.train()"),C1=r(") to activate the dropout modules."),O1=c(),di=l("li"),w=l("p"),D1=r("The additional "),ci=l("code"),x1=r("*inputs"),A1=r(" and "),hi=l("code"),z1=r("**kwargs"),P1=r(" arguments supplied to the "),fi=l("code"),L1=r("from_pretrained()"),I1=r(" method used to be directly passed to the underlying model\u2019s class "),pi=l("code"),S1=r("__init__()"),M1=r(" method. They are now used to update the model configuration attribute first which can break derived model classes build based on the previous "),mi=l("code"),j1=r("BertForSequenceClassification"),F1=r(" examples. More precisely, the positional arguments "),ui=l("code"),B1=r("*inputs"),q1=r(" provided to "),_i=l("code"),N1=r("from_pretrained()"),H1=r(" are directly forwarded the model "),vi=l("code"),R1=r("__init__()"),G1=r(" method while the keyword arguments "),Ei=l("code"),W1=r("**kwargs"),U1=r(" (i) which match configuration class attributes are used to update said attributes (ii) which don\u2019t match any configuration class attributes are forwarded to the model "),bi=l("code"),X1=r("__init__()"),K1=r(" method."),fd=c(),qt=l("p"),J1=r("Also, while not a breaking change, the serialization methods have been standardized and you probably should switch to the new method "),wi=l("code"),Q1=r("save_pretrained(save_directory)"),V1=r(" if you were using any other serialization method before."),pd=c(),mr=l("p"),Y1=r("Here is an example:"),md=c(),p(qo.$$.fragment),ud=c(),Ne=l("h3"),Nt=l("a"),yi=l("span"),p(No.$$.fragment),Z1=c(),ki=l("span"),eE=r("Optimizers: BertAdam & OpenAIAdam are now AdamW, schedules are standard PyTorch schedules"),_d=c(),M=l("p"),tE=r("The two optimizers previously included, "),Ti=l("code"),oE=r("BertAdam"),rE=r(" and "),gi=l("code"),aE=r("OpenAIAdam"),lE=r(", have been replaced by a single "),$i=l("code"),sE=r("AdamW"),iE=r(" optimizer which has a few differences:"),vd=c(),ce=l("ul"),Ci=l("li"),nE=r("it only implements weights decay correction,"),dE=c(),Oi=l("li"),cE=r("schedules are now externals (see below),"),hE=c(),Di=l("li"),fE=r("gradient clipping is now also external (see below)."),Ed=c(),he=l("p"),pE=r("The new optimizer "),xi=l("code"),mE=r("AdamW"),uE=r(" matches PyTorch "),Ai=l("code"),_E=r("Adam"),vE=r(" optimizer API and let you use standard PyTorch or apex methods for the schedule and clipping."),bd=c(),Ht=l("p"),EE=r("The schedules are now standard "),Ho=l("a"),bE=r("PyTorch learning rate schedulers"),wE=r(" and not part of the optimizer anymore."),wd=c(),fe=l("p"),yE=r("Here is a conversion examples from "),zi=l("code"),kE=r("BertAdam"),TE=r(" with a linear warmup and decay schedule to "),Pi=l("code"),gE=r("AdamW"),$E=r(" and the same schedule:"),yd=c(),p(Ro.$$.fragment),this.h()},l(t){const n=mw('[data-svelte="svelte-1phssyn"]',document.head);me=s(n,"META",{name:!0,content:!0}),n.forEach(o),Fi=h(t),ue=s(t,"H1",{class:!0});var Td=i(ue);Ge=s(Td,"A",{id:!0,class:!0,href:!0});var xE=i(Ge);Pr=s(xE,"SPAN",{});var AE=i(Pr);m(no.$$.fragment,AE),AE.forEach(o),xE.forEach(o),Ec=h(Td),Lr=s(Td,"SPAN",{});var zE=i(Lr);bc=a(zE,"Migrating from previous packages"),zE.forEach(o),Td.forEach(o),Bi=h(t),_e=s(t,"H2",{class:!0});var gd=i(_e);We=s(gd,"A",{id:!0,class:!0,href:!0});var PE=i(We);Ir=s(PE,"SPAN",{});var LE=i(Ir);m(co.$$.fragment,LE),LE.forEach(o),PE.forEach(o),wc=h(gd),Ue=s(gd,"SPAN",{});var Li=i(Ue);yc=a(Li,"Migrating from transformers "),Sr=s(Li,"CODE",{});var IE=i(Sr);kc=a(IE,"v3.x"),IE.forEach(o),Tc=a(Li," to "),Mr=s(Li,"CODE",{});var SE=i(Mr);gc=a(SE,"v4.x"),SE.forEach(o),Li.forEach(o),gd.forEach(o),qi=h(t),Zo=s(t,"P",{});var ME=i(Zo);$c=a(ME,`A couple of changes were introduced when the switch from version 3 to version 4 was done. Below is a summary of the
expected changes:`),ME.forEach(o),Ni=h(t),ve=s(t,"H4",{class:!0});var $d=i(ve);Xe=s($d,"A",{id:!0,class:!0,href:!0});var jE=i(Xe);jr=s(jE,"SPAN",{});var FE=i(jr);m(ho.$$.fragment,FE),FE.forEach(o),jE.forEach(o),Cc=h($d),Fr=s($d,"SPAN",{});var BE=i(Fr);Oc=a(BE,"1. AutoTokenizers and pipelines now use fast (rust) tokenizers by default."),BE.forEach(o),$d.forEach(o),Hi=h(t),er=s(t,"P",{});var qE=i(er);Dc=a(qE,"The python and rust tokenizers have roughly the same API, but the rust tokenizers have a more complete feature set."),qE.forEach(o),Ri=h(t),tr=s(t,"P",{});var NE=i(tr);xc=a(NE,"This introduces two breaking changes:"),NE.forEach(o),Gi=h(t),Ke=s(t,"UL",{});var Cd=i(Ke);Br=s(Cd,"LI",{});var HE=i(Br);Ac=a(HE,"The handling of overflowing tokens between the python and rust tokenizers is different."),HE.forEach(o),zc=h(Cd),qr=s(Cd,"LI",{});var RE=i(qr);Pc=a(RE,"The rust tokenizers do not accept integers in the encoding methods."),RE.forEach(o),Cd.forEach(o),Wi=h(t),Ee=s(t,"H5",{class:!0});var Od=i(Ee);Je=s(Od,"A",{id:!0,class:!0,href:!0});var GE=i(Je);Nr=s(GE,"SPAN",{});var WE=i(Nr);m(fo.$$.fragment,WE),WE.forEach(o),GE.forEach(o),Lc=h(Od),Hr=s(Od,"SPAN",{});var UE=i(Hr);Ic=a(UE,"How to obtain the same behavior as v3.x in v4.x"),UE.forEach(o),Od.forEach(o),Ui=h(t),Qe=s(t,"UL",{});var Dd=i(Qe);po=s(Dd,"LI",{});var xd=i(po);Sc=a(xd,"The pipelines now contain additional features out of the box. See the "),Ve=s(xd,"A",{href:!0});var Ad=i(Ve);Mc=a(Ad,"token-classification pipeline with the "),Rr=s(Ad,"CODE",{});var XE=i(Rr);jc=a(XE,"grouped_entities"),XE.forEach(o),Fc=a(Ad," flag"),Ad.forEach(o),Bc=a(xd,"."),xd.forEach(o),qc=h(Dd),be=s(Dd,"LI",{});var ur=i(be);Nc=a(ur,"The auto-tokenizers now return rust tokenizers. In order to obtain the python tokenizers instead, the user may use the "),Gr=s(ur,"CODE",{});var KE=i(Gr);Hc=a(KE,"use_fast"),KE.forEach(o),Rc=a(ur," flag by setting it to "),Wr=s(ur,"CODE",{});var JE=i(Wr);Gc=a(JE,"False"),JE.forEach(o),Wc=a(ur,":"),ur.forEach(o),Dd.forEach(o),Xi=h(t),Ye=s(t,"P",{});var zd=i(Ye);Uc=a(zd,"In version "),Ur=s(zd,"CODE",{});var QE=i(Ur);Xc=a(QE,"v3.x"),QE.forEach(o),Kc=a(zd,":"),zd.forEach(o),Ki=h(t),m(mo.$$.fragment,t),Ji=h(t),Ze=s(t,"P",{});var Pd=i(Ze);Jc=a(Pd,"to obtain the same in version "),Xr=s(Pd,"CODE",{});var VE=i(Xr);Qc=a(VE,"v4.x"),VE.forEach(o),Vc=a(Pd,":"),Pd.forEach(o),Qi=h(t),m(uo.$$.fragment,t),Vi=h(t),we=s(t,"H4",{class:!0});var Ld=i(we);et=s(Ld,"A",{id:!0,class:!0,href:!0});var YE=i(et);Kr=s(YE,"SPAN",{});var ZE=i(Kr);m(_o.$$.fragment,ZE),ZE.forEach(o),YE.forEach(o),Yc=h(Ld),Jr=s(Ld,"SPAN",{});var eb=i(Jr);Zc=a(eb,"2. SentencePiece is removed from the required dependencies"),eb.forEach(o),Ld.forEach(o),Yi=h(t),A=s(t,"P",{});var Rt=i(A);eh=a(Rt,"The requirement on the SentencePiece dependency has been lifted from the "),Qr=s(Rt,"CODE",{});var tb=i(Qr);th=a(tb,"setup.py"),tb.forEach(o),oh=a(Rt,". This is done so that we may have a channel on anaconda cloud without relying on "),Vr=s(Rt,"CODE",{});var ob=i(Vr);rh=a(ob,"conda-forge"),ob.forEach(o),ah=a(Rt,". This means that the tokenizers that depend on the SentencePiece library will not be available with a standard "),Yr=s(Rt,"CODE",{});var rb=i(Yr);lh=a(rb,"transformers"),rb.forEach(o),sh=a(Rt," installation."),Rt.forEach(o),Zi=h(t),tt=s(t,"P",{});var Id=i(tt);ih=a(Id,"This includes the "),Zr=s(Id,"STRONG",{});var ab=i(Zr);nh=a(ab,"slow"),ab.forEach(o),dh=a(Id," versions of:"),Id.forEach(o),en=h(t),T=s(t,"UL",{});var C=i(T);ea=s(C,"LI",{});var lb=i(ea);ta=s(lb,"CODE",{});var sb=i(ta);ch=a(sb,"XLNetTokenizer"),sb.forEach(o),lb.forEach(o),hh=h(C),oa=s(C,"LI",{});var ib=i(oa);ra=s(ib,"CODE",{});var nb=i(ra);fh=a(nb,"AlbertTokenizer"),nb.forEach(o),ib.forEach(o),ph=h(C),aa=s(C,"LI",{});var db=i(aa);la=s(db,"CODE",{});var cb=i(la);mh=a(cb,"CamembertTokenizer"),cb.forEach(o),db.forEach(o),uh=h(C),sa=s(C,"LI",{});var hb=i(sa);ia=s(hb,"CODE",{});var fb=i(ia);_h=a(fb,"MBartTokenizer"),fb.forEach(o),hb.forEach(o),vh=h(C),na=s(C,"LI",{});var pb=i(na);da=s(pb,"CODE",{});var mb=i(da);Eh=a(mb,"PegasusTokenizer"),mb.forEach(o),pb.forEach(o),bh=h(C),ca=s(C,"LI",{});var ub=i(ca);ha=s(ub,"CODE",{});var _b=i(ha);wh=a(_b,"T5Tokenizer"),_b.forEach(o),ub.forEach(o),yh=h(C),fa=s(C,"LI",{});var vb=i(fa);pa=s(vb,"CODE",{});var Eb=i(pa);kh=a(Eb,"ReformerTokenizer"),Eb.forEach(o),vb.forEach(o),Th=h(C),ma=s(C,"LI",{});var bb=i(ma);ua=s(bb,"CODE",{});var wb=i(ua);gh=a(wb,"XLMRobertaTokenizer"),wb.forEach(o),bb.forEach(o),C.forEach(o),tn=h(t),ye=s(t,"H5",{class:!0});var Sd=i(ye);ot=s(Sd,"A",{id:!0,class:!0,href:!0});var yb=i(ot);_a=s(yb,"SPAN",{});var kb=i(_a);m(vo.$$.fragment,kb),kb.forEach(o),yb.forEach(o),$h=h(Sd),va=s(Sd,"SPAN",{});var Tb=i(va);Ch=a(Tb,"How to obtain the same behavior as v3.x in v4.x"),Tb.forEach(o),Sd.forEach(o),on=h(t),V=s(t,"P",{});var _r=i(V);Oh=a(_r,"In order to obtain the same behavior as version "),Ea=s(_r,"CODE",{});var gb=i(Ea);Dh=a(gb,"v3.x"),gb.forEach(o),xh=a(_r,", you should install "),ba=s(_r,"CODE",{});var $b=i(ba);Ah=a($b,"sentencepiece"),$b.forEach(o),zh=a(_r," additionally:"),_r.forEach(o),rn=h(t),rt=s(t,"P",{});var Md=i(rt);Ph=a(Md,"In version "),wa=s(Md,"CODE",{});var Cb=i(wa);Lh=a(Cb,"v3.x"),Cb.forEach(o),Ih=a(Md,":"),Md.forEach(o),an=h(t),m(Eo.$$.fragment,t),ln=h(t),at=s(t,"P",{});var jd=i(at);Sh=a(jd,"to obtain the same in version "),ya=s(jd,"CODE",{});var Ob=i(ya);Mh=a(Ob,"v4.x"),Ob.forEach(o),jh=a(jd,":"),jd.forEach(o),sn=h(t),m(bo.$$.fragment,t),nn=h(t),or=s(t,"P",{});var Db=i(or);Fh=a(Db,"or"),Db.forEach(o),dn=h(t),m(wo.$$.fragment,t),cn=h(t),ke=s(t,"H4",{class:!0});var Fd=i(ke);lt=s(Fd,"A",{id:!0,class:!0,href:!0});var xb=i(lt);ka=s(xb,"SPAN",{});var Ab=i(ka);m(yo.$$.fragment,Ab),Ab.forEach(o),xb.forEach(o),Bh=h(Fd),Ta=s(Fd,"SPAN",{});var zb=i(Ta);qh=a(zb,"3. The architecture of the repo has been updated so that each model resides in its folder"),zb.forEach(o),Fd.forEach(o),hn=h(t),st=s(t,"P",{});var Bd=i(st);Nh=a(Bd,"The past and foreseeable addition of new models means that the number of files in the directory "),ga=s(Bd,"CODE",{});var Pb=i(ga);Hh=a(Pb,"src/transformers"),Pb.forEach(o),Rh=a(Bd," keeps growing and becomes harder to navigate and understand. We made the choice to put each model and the files accompanying it in their own sub-directories."),Bd.forEach(o),fn=h(t),rr=s(t,"P",{});var Lb=i(rr);Gh=a(Lb,"This is a breaking change as importing intermediary layers using a model\u2019s module directly needs to be done via a different path."),Lb.forEach(o),pn=h(t),Te=s(t,"H5",{class:!0});var qd=i(Te);it=s(qd,"A",{id:!0,class:!0,href:!0});var Ib=i(it);$a=s(Ib,"SPAN",{});var Sb=i($a);m(ko.$$.fragment,Sb),Sb.forEach(o),Ib.forEach(o),Wh=h(qd),Ca=s(qd,"SPAN",{});var Mb=i(Ca);Uh=a(Mb,"How to obtain the same behavior as v3.x in v4.x"),Mb.forEach(o),qd.forEach(o),mn=h(t),nt=s(t,"P",{});var Nd=i(nt);Xh=a(Nd,"In order to obtain the same behavior as version "),Oa=s(Nd,"CODE",{});var jb=i(Oa);Kh=a(jb,"v3.x"),jb.forEach(o),Jh=a(Nd,", you should update the path used to access the layers."),Nd.forEach(o),un=h(t),dt=s(t,"P",{});var Hd=i(dt);Qh=a(Hd,"In version "),Da=s(Hd,"CODE",{});var Fb=i(Da);Vh=a(Fb,"v3.x"),Fb.forEach(o),Yh=a(Hd,":"),Hd.forEach(o),_n=h(t),m(To.$$.fragment,t),vn=h(t),ct=s(t,"P",{});var Rd=i(ct);Zh=a(Rd,"to obtain the same in version "),xa=s(Rd,"CODE",{});var Bb=i(xa);ef=a(Bb,"v4.x"),Bb.forEach(o),tf=a(Rd,":"),Rd.forEach(o),En=h(t),m(go.$$.fragment,t),bn=h(t),ge=s(t,"H4",{class:!0});var Gd=i(ge);ht=s(Gd,"A",{id:!0,class:!0,href:!0});var qb=i(ht);Aa=s(qb,"SPAN",{});var Nb=i(Aa);m($o.$$.fragment,Nb),Nb.forEach(o),qb.forEach(o),of=h(Gd),$e=s(Gd,"SPAN",{});var vr=i($e);rf=a(vr,"4. Switching the "),za=s(vr,"CODE",{});var Hb=i(za);af=a(Hb,"return_dict"),Hb.forEach(o),lf=a(vr," argument to "),Pa=s(vr,"CODE",{});var Rb=i(Pa);sf=a(Rb,"True"),Rb.forEach(o),nf=a(vr," by default"),vr.forEach(o),Gd.forEach(o),wn=h(t),ft=s(t,"P",{});var Wd=i(ft);df=a(Wd,"The "),Co=s(Wd,"A",{href:!0});var CE=i(Co);La=s(CE,"CODE",{});var Gb=i(La);cf=a(Gb,"return_dict"),Gb.forEach(o),hf=a(CE," argument"),CE.forEach(o),ff=a(Wd," enables the return of dict-like python objects containing the model outputs, instead of the standard tuples. This object is self-documented as keys can be used to retrieve values, while also behaving as a tuple as users may retrieve objects by index or by slice."),Wd.forEach(o),yn=h(t),pt=s(t,"P",{});var Ud=i(pt);pf=a(Ud,"This is a breaking change as the limitation of that tuple is that it cannot be unpacked: "),Ia=s(Ud,"CODE",{});var Wb=i(Ia);mf=a(Wb,"value0, value1 = outputs"),Wb.forEach(o),uf=a(Ud," will not work."),Ud.forEach(o),kn=h(t),Ce=s(t,"H5",{class:!0});var Xd=i(Ce);mt=s(Xd,"A",{id:!0,class:!0,href:!0});var Ub=i(mt);Sa=s(Ub,"SPAN",{});var Xb=i(Sa);m(Oo.$$.fragment,Xb),Xb.forEach(o),Ub.forEach(o),_f=h(Xd),Ma=s(Xd,"SPAN",{});var Kb=i(Ma);vf=a(Kb,"How to obtain the same behavior as v3.x in v4.x"),Kb.forEach(o),Xd.forEach(o),Tn=h(t),z=s(t,"P",{});var Gt=i(z);Ef=a(Gt,"In order to obtain the same behavior as version "),ja=s(Gt,"CODE",{});var Jb=i(ja);bf=a(Jb,"v3.x"),Jb.forEach(o),wf=a(Gt,", you should specify the "),Fa=s(Gt,"CODE",{});var Qb=i(Fa);yf=a(Qb,"return_dict"),Qb.forEach(o),kf=a(Gt," argument to "),Ba=s(Gt,"CODE",{});var Vb=i(Ba);Tf=a(Vb,"False"),Vb.forEach(o),gf=a(Gt,", either in the model configuration or during the forward pass."),Gt.forEach(o),gn=h(t),ut=s(t,"P",{});var Kd=i(ut);$f=a(Kd,"In version "),qa=s(Kd,"CODE",{});var Yb=i(qa);Cf=a(Yb,"v3.x"),Yb.forEach(o),Of=a(Kd,":"),Kd.forEach(o),$n=h(t),m(Do.$$.fragment,t),Cn=h(t),_t=s(t,"P",{});var Jd=i(_t);Df=a(Jd,"to obtain the same in version "),Na=s(Jd,"CODE",{});var Zb=i(Na);xf=a(Zb,"v4.x"),Zb.forEach(o),Af=a(Jd,":"),Jd.forEach(o),On=h(t),m(xo.$$.fragment,t),Dn=h(t),ar=s(t,"P",{});var e3=i(ar);zf=a(e3,"or"),e3.forEach(o),xn=h(t),m(Ao.$$.fragment,t),An=h(t),Oe=s(t,"H4",{class:!0});var Qd=i(Oe);vt=s(Qd,"A",{id:!0,class:!0,href:!0});var t3=i(vt);Ha=s(t3,"SPAN",{});var o3=i(Ha);m(zo.$$.fragment,o3),o3.forEach(o),t3.forEach(o),Pf=h(Qd),Ra=s(Qd,"SPAN",{});var r3=i(Ra);Lf=a(r3,"5. Removed some deprecated attributes"),r3.forEach(o),Qd.forEach(o),zn=h(t),Et=s(t,"P",{});var Vd=i(Et);If=a(Vd,"Attributes that were deprecated have been removed if they had been deprecated for at least a month. The full list of deprecated attributes can be found in "),Po=s(Vd,"A",{href:!0,rel:!0});var a3=i(Po);Sf=a(a3,"#8604"),a3.forEach(o),Mf=a(Vd,"."),Vd.forEach(o),Pn=h(t),lr=s(t,"P",{});var l3=i(lr);jf=a(l3,"Here is a list of these attributes/methods/arguments and what their replacements should be:"),l3.forEach(o),Ln=h(t),sr=s(t,"P",{});var s3=i(sr);Ff=a(s3,"In several models, the labels become consistent with the other models:"),s3.forEach(o),In=h(t),b=s(t,"UL",{});var y=i(b);P=s(y,"LI",{});var He=i(P);Ga=s(He,"CODE",{});var i3=i(Ga);Bf=a(i3,"masked_lm_labels"),i3.forEach(o),qf=a(He," becomes "),Wa=s(He,"CODE",{});var n3=i(Wa);Nf=a(n3,"labels"),n3.forEach(o),Hf=a(He," in "),Ua=s(He,"CODE",{});var d3=i(Ua);Rf=a(d3,"AlbertForMaskedLM"),d3.forEach(o),Gf=a(He," and "),Xa=s(He,"CODE",{});var c3=i(Xa);Wf=a(c3,"AlbertForPreTraining"),c3.forEach(o),Uf=a(He,"."),He.forEach(o),Xf=h(y),L=s(y,"LI",{});var Re=i(L);Ka=s(Re,"CODE",{});var h3=i(Ka);Kf=a(h3,"masked_lm_labels"),h3.forEach(o),Jf=a(Re," becomes "),Ja=s(Re,"CODE",{});var f3=i(Ja);Qf=a(f3,"labels"),f3.forEach(o),Vf=a(Re," in "),Qa=s(Re,"CODE",{});var p3=i(Qa);Yf=a(p3,"BertForMaskedLM"),p3.forEach(o),Zf=a(Re," and "),Va=s(Re,"CODE",{});var m3=i(Va);ep=a(m3,"BertForPreTraining"),m3.forEach(o),tp=a(Re,"."),Re.forEach(o),op=h(y),Y=s(y,"LI",{});var Go=i(Y);Ya=s(Go,"CODE",{});var u3=i(Ya);rp=a(u3,"masked_lm_labels"),u3.forEach(o),ap=a(Go," becomes "),Za=s(Go,"CODE",{});var _3=i(Za);lp=a(_3,"labels"),_3.forEach(o),sp=a(Go," in "),el=s(Go,"CODE",{});var v3=i(el);ip=a(v3,"DistilBertForMaskedLM"),v3.forEach(o),np=a(Go,"."),Go.forEach(o),dp=h(y),Z=s(y,"LI",{});var Wo=i(Z);tl=s(Wo,"CODE",{});var E3=i(tl);cp=a(E3,"masked_lm_labels"),E3.forEach(o),hp=a(Wo," becomes "),ol=s(Wo,"CODE",{});var b3=i(ol);fp=a(b3,"labels"),b3.forEach(o),pp=a(Wo," in "),rl=s(Wo,"CODE",{});var w3=i(rl);mp=a(w3,"ElectraForMaskedLM"),w3.forEach(o),up=a(Wo,"."),Wo.forEach(o),_p=h(y),ee=s(y,"LI",{});var Uo=i(ee);al=s(Uo,"CODE",{});var y3=i(al);vp=a(y3,"masked_lm_labels"),y3.forEach(o),Ep=a(Uo," becomes "),ll=s(Uo,"CODE",{});var k3=i(ll);bp=a(k3,"labels"),k3.forEach(o),wp=a(Uo," in "),sl=s(Uo,"CODE",{});var T3=i(sl);yp=a(T3,"LongformerForMaskedLM"),T3.forEach(o),kp=a(Uo,"."),Uo.forEach(o),Tp=h(y),te=s(y,"LI",{});var Xo=i(te);il=s(Xo,"CODE",{});var g3=i(il);gp=a(g3,"masked_lm_labels"),g3.forEach(o),$p=a(Xo," becomes "),nl=s(Xo,"CODE",{});var $3=i(nl);Cp=a($3,"labels"),$3.forEach(o),Op=a(Xo," in "),dl=s(Xo,"CODE",{});var C3=i(dl);Dp=a(C3,"MobileBertForMaskedLM"),C3.forEach(o),xp=a(Xo,"."),Xo.forEach(o),Ap=h(y),oe=s(y,"LI",{});var Ko=i(oe);cl=s(Ko,"CODE",{});var O3=i(cl);zp=a(O3,"masked_lm_labels"),O3.forEach(o),Pp=a(Ko," becomes "),hl=s(Ko,"CODE",{});var D3=i(hl);Lp=a(D3,"labels"),D3.forEach(o),Ip=a(Ko," in "),fl=s(Ko,"CODE",{});var x3=i(fl);Sp=a(x3,"RobertaForMaskedLM"),x3.forEach(o),Mp=a(Ko,"."),Ko.forEach(o),jp=h(y),re=s(y,"LI",{});var Jo=i(re);pl=s(Jo,"CODE",{});var A3=i(pl);Fp=a(A3,"lm_labels"),A3.forEach(o),Bp=a(Jo," becomes "),ml=s(Jo,"CODE",{});var z3=i(ml);qp=a(z3,"labels"),z3.forEach(o),Np=a(Jo," in "),ul=s(Jo,"CODE",{});var P3=i(ul);Hp=a(P3,"BartForConditionalGeneration"),P3.forEach(o),Rp=a(Jo,"."),Jo.forEach(o),Gp=h(y),ae=s(y,"LI",{});var Qo=i(ae);_l=s(Qo,"CODE",{});var L3=i(_l);Wp=a(L3,"lm_labels"),L3.forEach(o),Up=a(Qo," becomes "),vl=s(Qo,"CODE",{});var I3=i(vl);Xp=a(I3,"labels"),I3.forEach(o),Kp=a(Qo," in "),El=s(Qo,"CODE",{});var S3=i(El);Jp=a(S3,"GPT2DoubleHeadsModel"),S3.forEach(o),Qp=a(Qo,"."),Qo.forEach(o),Vp=h(y),le=s(y,"LI",{});var Vo=i(le);bl=s(Vo,"CODE",{});var M3=i(bl);Yp=a(M3,"lm_labels"),M3.forEach(o),Zp=a(Vo," becomes "),wl=s(Vo,"CODE",{});var j3=i(wl);em=a(j3,"labels"),j3.forEach(o),tm=a(Vo," in "),yl=s(Vo,"CODE",{});var F3=i(yl);om=a(F3,"OpenAIGPTDoubleHeadsModel"),F3.forEach(o),rm=a(Vo,"."),Vo.forEach(o),am=h(y),se=s(y,"LI",{});var Yo=i(se);kl=s(Yo,"CODE",{});var B3=i(kl);lm=a(B3,"lm_labels"),B3.forEach(o),sm=a(Yo," becomes "),Tl=s(Yo,"CODE",{});var q3=i(Tl);im=a(q3,"labels"),q3.forEach(o),nm=a(Yo," in "),gl=s(Yo,"CODE",{});var N3=i(gl);dm=a(N3,"T5ForConditionalGeneration"),N3.forEach(o),cm=a(Yo,"."),Yo.forEach(o),y.forEach(o),Sn=h(t),ir=s(t,"P",{});var H3=i(ir);hm=a(H3,"In several models, the caching mechanism becomes consistent with the other models:"),H3.forEach(o),Mn=h(t),I=s(t,"UL",{});var Wt=i(I);bt=s(Wt,"LI",{});var Ii=i(bt);$l=s(Ii,"CODE",{});var R3=i($l);fm=a(R3,"decoder_cached_states"),R3.forEach(o),pm=a(Ii," becomes "),Cl=s(Ii,"CODE",{});var G3=i(Cl);mm=a(G3,"past_key_values"),G3.forEach(o),um=a(Ii," in all BART-like, FSMT and T5 models."),Ii.forEach(o),_m=h(Wt),wt=s(Wt,"LI",{});var Si=i(wt);Ol=s(Si,"CODE",{});var W3=i(Ol);vm=a(W3,"decoder_past_key_values"),W3.forEach(o),Em=a(Si," becomes "),Dl=s(Si,"CODE",{});var U3=i(Dl);bm=a(U3,"past_key_values"),U3.forEach(o),wm=a(Si," in all BART-like, FSMT and T5 models."),Si.forEach(o),ym=h(Wt),yt=s(Wt,"LI",{});var Mi=i(yt);xl=s(Mi,"CODE",{});var X3=i(xl);km=a(X3,"past"),X3.forEach(o),Tm=a(Mi," becomes "),Al=s(Mi,"CODE",{});var K3=i(Al);gm=a(K3,"past_key_values"),K3.forEach(o),$m=a(Mi," in all CTRL models."),Mi.forEach(o),Cm=h(Wt),kt=s(Wt,"LI",{});var ji=i(kt);zl=s(ji,"CODE",{});var J3=i(zl);Om=a(J3,"past"),J3.forEach(o),Dm=a(ji," becomes "),Pl=s(ji,"CODE",{});var Q3=i(Pl);xm=a(Q3,"past_key_values"),Q3.forEach(o),Am=a(ji," in all GPT-2 models."),ji.forEach(o),Wt.forEach(o),jn=h(t),nr=s(t,"P",{});var V3=i(nr);zm=a(V3,"Regarding the tokenizer classes:"),V3.forEach(o),Fn=h(t),ie=s(t,"UL",{});var Er=i(ie);De=s(Er,"LI",{});var br=i(De);Pm=a(br,"The tokenizer attribute "),Ll=s(br,"CODE",{});var Y3=i(Ll);Lm=a(Y3,"max_len"),Y3.forEach(o),Im=a(br," becomes "),Il=s(br,"CODE",{});var Z3=i(Il);Sm=a(Z3,"model_max_length"),Z3.forEach(o),Mm=a(br,"."),br.forEach(o),jm=h(Er),xe=s(Er,"LI",{});var wr=i(xe);Fm=a(wr,"The tokenizer attribute "),Sl=s(wr,"CODE",{});var e4=i(Sl);Bm=a(e4,"return_lengths"),e4.forEach(o),qm=a(wr," becomes "),Ml=s(wr,"CODE",{});var t4=i(Ml);Nm=a(t4,"return_length"),t4.forEach(o),Hm=a(wr,"."),wr.forEach(o),Rm=h(Er),Ae=s(Er,"LI",{});var yr=i(Ae);Gm=a(yr,"The tokenizer encoding argument "),jl=s(yr,"CODE",{});var o4=i(jl);Wm=a(o4,"is_pretokenized"),o4.forEach(o),Um=a(yr," becomes "),Fl=s(yr,"CODE",{});var r4=i(Fl);Xm=a(r4,"is_split_into_words"),r4.forEach(o),Km=a(yr,"."),yr.forEach(o),Er.forEach(o),Bn=h(t),Tt=s(t,"P",{});var Yd=i(Tt);Jm=a(Yd,"Regarding the "),Bl=s(Yd,"CODE",{});var a4=i(Bl);Qm=a(a4,"Trainer"),a4.forEach(o),Vm=a(Yd," class:"),Yd.forEach(o),qn=h(t),g=s(t,"UL",{});var O=i(g);j=s(O,"LI",{});var Ut=i(j);Ym=a(Ut,"The "),ql=s(Ut,"CODE",{});var l4=i(ql);Zm=a(l4,"Trainer"),l4.forEach(o),eu=a(Ut," argument "),Nl=s(Ut,"CODE",{});var s4=i(Nl);tu=a(s4,"tb_writer"),s4.forEach(o),ou=a(Ut," is removed in favor of the callback "),Hl=s(Ut,"CODE",{});var i4=i(Hl);ru=a(i4,"TensorBoardCallback(tb_writer=...)"),i4.forEach(o),au=a(Ut,"."),Ut.forEach(o),lu=h(O),F=s(O,"LI",{});var Xt=i(F);su=a(Xt,"The "),Rl=s(Xt,"CODE",{});var n4=i(Rl);iu=a(n4,"Trainer"),n4.forEach(o),nu=a(Xt," argument "),Gl=s(Xt,"CODE",{});var d4=i(Gl);du=a(d4,"prediction_loss_only"),d4.forEach(o),cu=a(Xt," is removed in favor of the class argument "),Wl=s(Xt,"CODE",{});var c4=i(Wl);hu=a(c4,"args.prediction_loss_only"),c4.forEach(o),fu=a(Xt,"."),Xt.forEach(o),pu=h(O),ze=s(O,"LI",{});var kr=i(ze);mu=a(kr,"The "),Ul=s(kr,"CODE",{});var h4=i(Ul);uu=a(h4,"Trainer"),h4.forEach(o),_u=a(kr," attribute "),Xl=s(kr,"CODE",{});var f4=i(Xl);vu=a(f4,"data_collator"),f4.forEach(o),Eu=a(kr," should be a callable."),kr.forEach(o),bu=h(O),B=s(O,"LI",{});var Kt=i(B);wu=a(Kt,"The "),Kl=s(Kt,"CODE",{});var p4=i(Kl);yu=a(p4,"Trainer"),p4.forEach(o),ku=a(Kt," method "),Jl=s(Kt,"CODE",{});var m4=i(Jl);Tu=a(m4,"_log"),m4.forEach(o),gu=a(Kt," is deprecated in favor of "),Ql=s(Kt,"CODE",{});var u4=i(Ql);$u=a(u4,"log"),u4.forEach(o),Cu=a(Kt,"."),Kt.forEach(o),Ou=h(O),q=s(O,"LI",{});var Jt=i(q);Du=a(Jt,"The "),Vl=s(Jt,"CODE",{});var _4=i(Vl);xu=a(_4,"Trainer"),_4.forEach(o),Au=a(Jt," method "),Yl=s(Jt,"CODE",{});var v4=i(Yl);zu=a(v4,"_training_step"),v4.forEach(o),Pu=a(Jt," is deprecated in favor of "),Zl=s(Jt,"CODE",{});var E4=i(Zl);Lu=a(E4,"training_step"),E4.forEach(o),Iu=a(Jt,"."),Jt.forEach(o),Su=h(O),N=s(O,"LI",{});var Qt=i(N);Mu=a(Qt,"The "),es=s(Qt,"CODE",{});var b4=i(es);ju=a(b4,"Trainer"),b4.forEach(o),Fu=a(Qt," method "),ts=s(Qt,"CODE",{});var w4=i(ts);Bu=a(w4,"_prediction_loop"),w4.forEach(o),qu=a(Qt," is deprecated in favor of "),os=s(Qt,"CODE",{});var y4=i(os);Nu=a(y4,"prediction_loop"),y4.forEach(o),Hu=a(Qt,"."),Qt.forEach(o),Ru=h(O),H=s(O,"LI",{});var Vt=i(H);Gu=a(Vt,"The "),rs=s(Vt,"CODE",{});var k4=i(rs);Wu=a(k4,"Trainer"),k4.forEach(o),Uu=a(Vt," method "),as=s(Vt,"CODE",{});var T4=i(as);Xu=a(T4,"is_local_master"),T4.forEach(o),Ku=a(Vt," is deprecated in favor of "),ls=s(Vt,"CODE",{});var g4=i(ls);Ju=a(g4,"is_local_process_zero"),g4.forEach(o),Qu=a(Vt,"."),Vt.forEach(o),Vu=h(O),R=s(O,"LI",{});var Yt=i(R);Yu=a(Yt,"The "),ss=s(Yt,"CODE",{});var $4=i(ss);Zu=a($4,"Trainer"),$4.forEach(o),e_=a(Yt," method "),is=s(Yt,"CODE",{});var C4=i(is);t_=a(C4,"is_world_master"),C4.forEach(o),o_=a(Yt," is deprecated in favor of "),ns=s(Yt,"CODE",{});var O4=i(ns);r_=a(O4,"is_world_process_zero"),O4.forEach(o),a_=a(Yt,"."),Yt.forEach(o),O.forEach(o),Nn=h(t),gt=s(t,"P",{});var Zd=i(gt);l_=a(Zd,"Regarding the "),ds=s(Zd,"CODE",{});var D4=i(ds);s_=a(D4,"TFTrainer"),D4.forEach(o),i_=a(Zd," class:"),Zd.forEach(o),Hn=h(t),D=s(t,"UL",{});var pe=i(D);G=s(pe,"LI",{});var Zt=i(G);n_=a(Zt,"The "),cs=s(Zt,"CODE",{});var x4=i(cs);d_=a(x4,"TFTrainer"),x4.forEach(o),c_=a(Zt," argument "),hs=s(Zt,"CODE",{});var A4=i(hs);h_=a(A4,"prediction_loss_only"),A4.forEach(o),f_=a(Zt," is removed in favor of the class argument "),fs=s(Zt,"CODE",{});var z4=i(fs);p_=a(z4,"args.prediction_loss_only"),z4.forEach(o),m_=a(Zt,"."),Zt.forEach(o),u_=h(pe),W=s(pe,"LI",{});var eo=i(W);__=a(eo,"The "),ps=s(eo,"CODE",{});var P4=i(ps);v_=a(P4,"Trainer"),P4.forEach(o),E_=a(eo," method "),ms=s(eo,"CODE",{});var L4=i(ms);b_=a(L4,"_log"),L4.forEach(o),w_=a(eo," is deprecated in favor of "),us=s(eo,"CODE",{});var I4=i(us);y_=a(I4,"log"),I4.forEach(o),k_=a(eo,"."),eo.forEach(o),T_=h(pe),U=s(pe,"LI",{});var to=i(U);g_=a(to,"The "),_s=s(to,"CODE",{});var S4=i(_s);$_=a(S4,"TFTrainer"),S4.forEach(o),C_=a(to," method "),vs=s(to,"CODE",{});var M4=i(vs);O_=a(M4,"_prediction_loop"),M4.forEach(o),D_=a(to," is deprecated in favor of "),Es=s(to,"CODE",{});var j4=i(Es);x_=a(j4,"prediction_loop"),j4.forEach(o),A_=a(to,"."),to.forEach(o),z_=h(pe),X=s(pe,"LI",{});var oo=i(X);P_=a(oo,"The "),bs=s(oo,"CODE",{});var F4=i(bs);L_=a(F4,"TFTrainer"),F4.forEach(o),I_=a(oo," method "),ws=s(oo,"CODE",{});var B4=i(ws);S_=a(B4,"_setup_wandb"),B4.forEach(o),M_=a(oo," is deprecated in favor of "),ys=s(oo,"CODE",{});var q4=i(ys);j_=a(q4,"setup_wandb"),q4.forEach(o),F_=a(oo,"."),oo.forEach(o),B_=h(pe),K=s(pe,"LI",{});var ro=i(K);q_=a(ro,"The "),ks=s(ro,"CODE",{});var N4=i(ks);N_=a(N4,"TFTrainer"),N4.forEach(o),H_=a(ro," method "),Ts=s(ro,"CODE",{});var H4=i(Ts);R_=a(H4,"_run_model"),H4.forEach(o),G_=a(ro," is deprecated in favor of "),gs=s(ro,"CODE",{});var R4=i(gs);W_=a(R4,"run_model"),R4.forEach(o),U_=a(ro,"."),ro.forEach(o),pe.forEach(o),Rn=h(t),$t=s(t,"P",{});var ec=i($t);X_=a(ec,"Regarding the "),$s=s(ec,"CODE",{});var G4=i($s);K_=a(G4,"TrainingArguments"),G4.forEach(o),J_=a(ec," class:"),ec.forEach(o),Gn=h(t),dr=s(t,"UL",{});var W4=i(dr);J=s(W4,"LI",{});var ao=i(J);Q_=a(ao,"The "),Cs=s(ao,"CODE",{});var U4=i(Cs);V_=a(U4,"TrainingArguments"),U4.forEach(o),Y_=a(ao," argument "),Os=s(ao,"CODE",{});var X4=i(Os);Z_=a(X4,"evaluate_during_training"),X4.forEach(o),ev=a(ao," is deprecated in favor of "),Ds=s(ao,"CODE",{});var K4=i(Ds);tv=a(K4,"evaluation_strategy"),K4.forEach(o),ov=a(ao,"."),ao.forEach(o),W4.forEach(o),Wn=h(t),cr=s(t,"P",{});var J4=i(cr);rv=a(J4,"Regarding the Transfo-XL model:"),J4.forEach(o),Un=h(t),Ct=s(t,"UL",{});var tc=i(Ct);Pe=s(tc,"LI",{});var Tr=i(Pe);av=a(Tr,"The Transfo-XL configuration attribute "),xs=s(Tr,"CODE",{});var Q4=i(xs);lv=a(Q4,"tie_weight"),Q4.forEach(o),sv=a(Tr," becomes "),As=s(Tr,"CODE",{});var V4=i(As);iv=a(V4,"tie_words_embeddings"),V4.forEach(o),nv=a(Tr,"."),Tr.forEach(o),dv=h(tc),Le=s(tc,"LI",{});var gr=i(Le);cv=a(gr,"The Transfo-XL modeling method "),zs=s(gr,"CODE",{});var Y4=i(zs);hv=a(Y4,"reset_length"),Y4.forEach(o),fv=a(gr," becomes "),Ps=s(gr,"CODE",{});var Z4=i(Ps);pv=a(Z4,"reset_memory_length"),Z4.forEach(o),mv=a(gr,"."),gr.forEach(o),tc.forEach(o),Xn=h(t),hr=s(t,"P",{});var e2=i(hr);uv=a(e2,"Regarding pipelines:"),e2.forEach(o),Kn=h(t),fr=s(t,"UL",{});var t2=i(fr);Q=s(t2,"LI",{});var lo=i(Q);_v=a(lo,"The "),Ls=s(lo,"CODE",{});var o2=i(Ls);vv=a(o2,"FillMaskPipeline"),o2.forEach(o),Ev=a(lo," argument "),Is=s(lo,"CODE",{});var r2=i(Is);bv=a(r2,"topk"),r2.forEach(o),wv=a(lo," becomes "),Ss=s(lo,"CODE",{});var a2=i(Ss);yv=a(a2,"top_k"),a2.forEach(o),kv=a(lo,"."),lo.forEach(o),t2.forEach(o),Jn=h(t),Ie=s(t,"H2",{class:!0});var oc=i(Ie);Ot=s(oc,"A",{id:!0,class:!0,href:!0});var l2=i(Ot);Ms=s(l2,"SPAN",{});var s2=i(Ms);m(Lo.$$.fragment,s2),s2.forEach(o),l2.forEach(o),Tv=h(oc),js=s(oc,"SPAN",{});var i2=i(js);gv=a(i2,"Migrating from pytorch-transformers to \u{1F917} Transformers"),i2.forEach(o),oc.forEach(o),Qn=h(t),Dt=s(t,"P",{});var rc=i(Dt);$v=a(rc,"Here is a quick summary of what you should take care of when migrating from "),Fs=s(rc,"CODE",{});var n2=i(Fs);Cv=a(n2,"pytorch-transformers"),n2.forEach(o),Ov=a(rc," to \u{1F917} Transformers."),rc.forEach(o),Vn=h(t),Se=s(t,"H3",{class:!0});var ac=i(Se);xt=s(ac,"A",{id:!0,class:!0,href:!0});var d2=i(xt);Bs=s(d2,"SPAN",{});var c2=i(Bs);m(Io.$$.fragment,c2),c2.forEach(o),d2.forEach(o),Dv=h(ac),Me=s(ac,"SPAN",{});var $r=i(Me);xv=a($r,"Positional order of some models' keywords inputs ("),qs=s($r,"CODE",{});var h2=i(qs);Av=a(h2,"attention_mask"),h2.forEach(o),zv=a($r,", "),Ns=s($r,"CODE",{});var f2=i(Ns);Pv=a(f2,"token_type_ids"),f2.forEach(o),Lv=a($r,"...) changed"),$r.forEach(o),ac.forEach(o),Yn=h(t),S=s(t,"P",{});var so=i(S);Iv=a(so,"To be able to use Torchscript (see #1010, #1204 and #1195) the specific order of some models "),Hs=s(so,"STRONG",{});var p2=i(Hs);Sv=a(p2,"keywords inputs"),p2.forEach(o),Mv=a(so," ("),Rs=s(so,"CODE",{});var m2=i(Rs);jv=a(m2,"attention_mask"),m2.forEach(o),Fv=a(so,", "),Gs=s(so,"CODE",{});var u2=i(Gs);Bv=a(u2,"token_type_ids"),u2.forEach(o),qv=a(so,"\u2026) has been changed."),so.forEach(o),Zn=h(t),At=s(t,"P",{});var lc=i(At);Nv=a(lc,"If you used to call the models with keyword names for keyword arguments, e.g. "),Ws=s(lc,"CODE",{});var _2=i(Ws);Hv=a(_2,"model(inputs_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)"),_2.forEach(o),Rv=a(lc,", this should not cause any change."),lc.forEach(o),ed=h(t),zt=s(t,"P",{});var sc=i(zt);Gv=a(sc,"If you used to call the models with positional inputs for keyword arguments, e.g. "),Us=s(sc,"CODE",{});var v2=i(Us);Wv=a(v2,"model(inputs_ids, attention_mask, token_type_ids)"),v2.forEach(o),Uv=a(sc,", you may have to double check the exact order of input arguments."),sc.forEach(o),td=h(t),je=s(t,"H2",{class:!0});var ic=i(je);Pt=s(ic,"A",{id:!0,class:!0,href:!0});var E2=i(Pt);Xs=s(E2,"SPAN",{});var b2=i(Xs);m(So.$$.fragment,b2),b2.forEach(o),E2.forEach(o),Xv=h(ic),Ks=s(ic,"SPAN",{});var w2=i(Ks);Kv=a(w2,"Migrating from pytorch-pretrained-bert"),w2.forEach(o),ic.forEach(o),od=h(t),Lt=s(t,"P",{});var nc=i(Lt);Jv=a(nc,"Here is a quick summary of what you should take care of when migrating from "),Js=s(nc,"CODE",{});var y2=i(Js);Qv=a(y2,"pytorch-pretrained-bert"),y2.forEach(o),Vv=a(nc," to \u{1F917} Transformers"),nc.forEach(o),rd=h(t),Fe=s(t,"H3",{class:!0});var dc=i(Fe);It=s(dc,"A",{id:!0,class:!0,href:!0});var k2=i(It);Qs=s(k2,"SPAN",{});var T2=i(Qs);m(Mo.$$.fragment,T2),T2.forEach(o),k2.forEach(o),Yv=h(dc),pr=s(dc,"SPAN",{});var OE=i(pr);Zv=a(OE,"Models always output "),Vs=s(OE,"CODE",{});var g2=i(Vs);e1=a(g2,"tuples"),g2.forEach(o),OE.forEach(o),dc.forEach(o),ad=h(t),ne=s(t,"P",{});var Cr=i(ne);t1=a(Cr,"The main breaking change when migrating from "),Ys=s(Cr,"CODE",{});var $2=i(Ys);o1=a($2,"pytorch-pretrained-bert"),$2.forEach(o),r1=a(Cr," to \u{1F917} Transformers is that the models forward method always outputs a "),Zs=s(Cr,"CODE",{});var C2=i(Zs);a1=a(C2,"tuple"),C2.forEach(o),l1=a(Cr," with various elements depending on the model and the configuration parameters."),Cr.forEach(o),ld=h(t),St=s(t,"P",{});var cc=i(St);s1=a(cc,"The exact content of the tuples for each model are detailed in the models\u2019 docstrings and the "),jo=s(cc,"A",{href:!0,rel:!0});var O2=i(jo);i1=a(O2,"documentation"),O2.forEach(o),n1=a(cc,"."),cc.forEach(o),sd=h(t),Mt=s(t,"P",{});var hc=i(Mt);d1=a(hc,"In pretty much every case, you will be fine by taking the first element of the output as the output you previously used in "),ei=s(hc,"CODE",{});var D2=i(ei);c1=a(D2,"pytorch-pretrained-bert"),D2.forEach(o),h1=a(hc,"."),hc.forEach(o),id=h(t),de=s(t,"P",{});var Or=i(de);f1=a(Or,"Here is a "),ti=s(Or,"CODE",{});var x2=i(ti);p1=a(x2,"pytorch-pretrained-bert"),x2.forEach(o),m1=a(Or," to \u{1F917} Transformers conversion example for a "),oi=s(Or,"CODE",{});var A2=i(oi);u1=a(A2,"BertForSequenceClassification"),A2.forEach(o),_1=a(Or," classification model:"),Or.forEach(o),nd=h(t),m(Fo.$$.fragment,t),dd=h(t),Be=s(t,"H3",{class:!0});var fc=i(Be);jt=s(fc,"A",{id:!0,class:!0,href:!0});var z2=i(jt);ri=s(z2,"SPAN",{});var P2=i(ri);m(Bo.$$.fragment,P2),P2.forEach(o),z2.forEach(o),v1=h(fc),ai=s(fc,"SPAN",{});var L2=i(ai);E1=a(L2,"Serialization"),L2.forEach(o),fc.forEach(o),cd=h(t),Ft=s(t,"P",{});var pc=i(Ft);b1=a(pc,"Breaking change in the "),li=s(pc,"CODE",{});var I2=i(li);w1=a(I2,"from_pretrained()"),I2.forEach(o),y1=a(pc,"method:"),pc.forEach(o),hd=h(t),Bt=s(t,"OL",{});var mc=i(Bt);si=s(mc,"LI",{});var S2=i(si);qe=s(S2,"P",{});var Dr=i(qe);k1=a(Dr,"Models are now set in evaluation mode by default when instantiated with the "),ii=s(Dr,"CODE",{});var M2=i(ii);T1=a(M2,"from_pretrained()"),M2.forEach(o),g1=a(Dr," method. To train them don\u2019t forget to set them back in training mode ("),ni=s(Dr,"CODE",{});var j2=i(ni);$1=a(j2,"model.train()"),j2.forEach(o),C1=a(Dr,") to activate the dropout modules."),Dr.forEach(o),S2.forEach(o),O1=h(mc),di=s(mc,"LI",{});var F2=i(di);w=s(F2,"P",{});var k=i(w);D1=a(k,"The additional "),ci=s(k,"CODE",{});var B2=i(ci);x1=a(B2,"*inputs"),B2.forEach(o),A1=a(k," and "),hi=s(k,"CODE",{});var q2=i(hi);z1=a(q2,"**kwargs"),q2.forEach(o),P1=a(k," arguments supplied to the "),fi=s(k,"CODE",{});var N2=i(fi);L1=a(N2,"from_pretrained()"),N2.forEach(o),I1=a(k," method used to be directly passed to the underlying model\u2019s class "),pi=s(k,"CODE",{});var H2=i(pi);S1=a(H2,"__init__()"),H2.forEach(o),M1=a(k," method. They are now used to update the model configuration attribute first which can break derived model classes build based on the previous "),mi=s(k,"CODE",{});var R2=i(mi);j1=a(R2,"BertForSequenceClassification"),R2.forEach(o),F1=a(k," examples. More precisely, the positional arguments "),ui=s(k,"CODE",{});var G2=i(ui);B1=a(G2,"*inputs"),G2.forEach(o),q1=a(k," provided to "),_i=s(k,"CODE",{});var W2=i(_i);N1=a(W2,"from_pretrained()"),W2.forEach(o),H1=a(k," are directly forwarded the model "),vi=s(k,"CODE",{});var U2=i(vi);R1=a(U2,"__init__()"),U2.forEach(o),G1=a(k," method while the keyword arguments "),Ei=s(k,"CODE",{});var X2=i(Ei);W1=a(X2,"**kwargs"),X2.forEach(o),U1=a(k," (i) which match configuration class attributes are used to update said attributes (ii) which don\u2019t match any configuration class attributes are forwarded to the model "),bi=s(k,"CODE",{});var K2=i(bi);X1=a(K2,"__init__()"),K2.forEach(o),K1=a(k," method."),k.forEach(o),F2.forEach(o),mc.forEach(o),fd=h(t),qt=s(t,"P",{});var uc=i(qt);J1=a(uc,"Also, while not a breaking change, the serialization methods have been standardized and you probably should switch to the new method "),wi=s(uc,"CODE",{});var J2=i(wi);Q1=a(J2,"save_pretrained(save_directory)"),J2.forEach(o),V1=a(uc," if you were using any other serialization method before."),uc.forEach(o),pd=h(t),mr=s(t,"P",{});var Q2=i(mr);Y1=a(Q2,"Here is an example:"),Q2.forEach(o),md=h(t),m(qo.$$.fragment,t),ud=h(t),Ne=s(t,"H3",{class:!0});var _c=i(Ne);Nt=s(_c,"A",{id:!0,class:!0,href:!0});var V2=i(Nt);yi=s(V2,"SPAN",{});var Y2=i(yi);m(No.$$.fragment,Y2),Y2.forEach(o),V2.forEach(o),Z1=h(_c),ki=s(_c,"SPAN",{});var Z2=i(ki);eE=a(Z2,"Optimizers: BertAdam & OpenAIAdam are now AdamW, schedules are standard PyTorch schedules"),Z2.forEach(o),_c.forEach(o),_d=h(t),M=s(t,"P",{});var io=i(M);tE=a(io,"The two optimizers previously included, "),Ti=s(io,"CODE",{});var ew=i(Ti);oE=a(ew,"BertAdam"),ew.forEach(o),rE=a(io," and "),gi=s(io,"CODE",{});var tw=i(gi);aE=a(tw,"OpenAIAdam"),tw.forEach(o),lE=a(io,", have been replaced by a single "),$i=s(io,"CODE",{});var ow=i($i);sE=a(ow,"AdamW"),ow.forEach(o),iE=a(io," optimizer which has a few differences:"),io.forEach(o),vd=h(t),ce=s(t,"UL",{});var xr=i(ce);Ci=s(xr,"LI",{});var rw=i(Ci);nE=a(rw,"it only implements weights decay correction,"),rw.forEach(o),dE=h(xr),Oi=s(xr,"LI",{});var aw=i(Oi);cE=a(aw,"schedules are now externals (see below),"),aw.forEach(o),hE=h(xr),Di=s(xr,"LI",{});var lw=i(Di);fE=a(lw,"gradient clipping is now also external (see below)."),lw.forEach(o),xr.forEach(o),Ed=h(t),he=s(t,"P",{});var Ar=i(he);pE=a(Ar,"The new optimizer "),xi=s(Ar,"CODE",{});var sw=i(xi);mE=a(sw,"AdamW"),sw.forEach(o),uE=a(Ar," matches PyTorch "),Ai=s(Ar,"CODE",{});var iw=i(Ai);_E=a(iw,"Adam"),iw.forEach(o),vE=a(Ar," optimizer API and let you use standard PyTorch or apex methods for the schedule and clipping."),Ar.forEach(o),bd=h(t),Ht=s(t,"P",{});var vc=i(Ht);EE=a(vc,"The schedules are now standard "),Ho=s(vc,"A",{href:!0,rel:!0});var nw=i(Ho);bE=a(nw,"PyTorch learning rate schedulers"),nw.forEach(o),wE=a(vc," and not part of the optimizer anymore."),vc.forEach(o),wd=h(t),fe=s(t,"P",{});var zr=i(fe);yE=a(zr,"Here is a conversion examples from "),zi=s(zr,"CODE",{});var dw=i(zi);kE=a(dw,"BertAdam"),dw.forEach(o),TE=a(zr," with a linear warmup and decay schedule to "),Pi=s(zr,"CODE",{});var cw=i(Pi);gE=a(cw,"AdamW"),cw.forEach(o),$E=a(zr," and the same schedule:"),zr.forEach(o),yd=h(t),m(Ro.$$.fragment,t),this.h()},h(){f(me,"name","hf:doc:metadata"),f(me,"content",JSON.stringify(Ew)),f(Ge,"id","migrating-from-previous-packages"),f(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ge,"href","#migrating-from-previous-packages"),f(ue,"class","relative group"),f(We,"id","migrating-from-transformers-v3x-to-v4x"),f(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(We,"href","#migrating-from-transformers-v3x-to-v4x"),f(_e,"class","relative group"),f(Xe,"id","1-autotokenizers-and-pipelines-now-use-fast-rust-tokenizers-by-default"),f(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Xe,"href","#1-autotokenizers-and-pipelines-now-use-fast-rust-tokenizers-by-default"),f(ve,"class","relative group"),f(Je,"id","how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Je,"href","#how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(Ee,"class","relative group"),f(Ve,"href","main_classes/pipelines#transformers.TokenClassificationPipeline"),f(et,"id","2-sentencepiece-is-removed-from-the-required-dependencies"),f(et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(et,"href","#2-sentencepiece-is-removed-from-the-required-dependencies"),f(we,"class","relative group"),f(ot,"id","how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ot,"href","#how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(ye,"class","relative group"),f(lt,"id","3-the-architecture-of-the-repo-has-been-updated-so-that-each-model-resides-in-its-folder"),f(lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(lt,"href","#3-the-architecture-of-the-repo-has-been-updated-so-that-each-model-resides-in-its-folder"),f(ke,"class","relative group"),f(it,"id","how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(it,"href","#how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(Te,"class","relative group"),f(ht,"id","4-switching-the-returndict-argument-to-true-by-default"),f(ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(ht,"href","#4-switching-the-returndict-argument-to-true-by-default"),f(ge,"class","relative group"),f(Co,"href","main_classes/output"),f(mt,"id","how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(mt,"href","#how-to-obtain-the-same-behavior-as-v3x-in-v4x"),f(Ce,"class","relative group"),f(vt,"id","5-removed-some-deprecated-attributes"),f(vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(vt,"href","#5-removed-some-deprecated-attributes"),f(Oe,"class","relative group"),f(Po,"href","https://github.com/huggingface/transformers/pull/8604"),f(Po,"rel","nofollow"),f(Ot,"id","migrating-from-pytorchtransformers-to-transformers"),f(Ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ot,"href","#migrating-from-pytorchtransformers-to-transformers"),f(Ie,"class","relative group"),f(xt,"id","positional-order-of-some-models-keywords-inputs-attentionmask-tokentypeids-changed"),f(xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(xt,"href","#positional-order-of-some-models-keywords-inputs-attentionmask-tokentypeids-changed"),f(Se,"class","relative group"),f(Pt,"id","migrating-from-pytorchpretrainedbert"),f(Pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Pt,"href","#migrating-from-pytorchpretrainedbert"),f(je,"class","relative group"),f(It,"id","models-always-output-tuples"),f(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(It,"href","#models-always-output-tuples"),f(Fe,"class","relative group"),f(jo,"href","https://huggingface.co/transformers/"),f(jo,"rel","nofollow"),f(jt,"id","serialization"),f(jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(jt,"href","#serialization"),f(Be,"class","relative group"),f(Nt,"id","optimizers-bertadam-openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules"),f(Nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Nt,"href","#optimizers-bertadam-openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules"),f(Ne,"class","relative group"),f(Ho,"href","https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"),f(Ho,"rel","nofollow")},m(t,n){e(document.head,me),d(t,Fi,n),d(t,ue,n),e(ue,Ge),e(Ge,Pr),u(no,Pr,null),e(ue,Ec),e(ue,Lr),e(Lr,bc),d(t,Bi,n),d(t,_e,n),e(_e,We),e(We,Ir),u(co,Ir,null),e(_e,wc),e(_e,Ue),e(Ue,yc),e(Ue,Sr),e(Sr,kc),e(Ue,Tc),e(Ue,Mr),e(Mr,gc),d(t,qi,n),d(t,Zo,n),e(Zo,$c),d(t,Ni,n),d(t,ve,n),e(ve,Xe),e(Xe,jr),u(ho,jr,null),e(ve,Cc),e(ve,Fr),e(Fr,Oc),d(t,Hi,n),d(t,er,n),e(er,Dc),d(t,Ri,n),d(t,tr,n),e(tr,xc),d(t,Gi,n),d(t,Ke,n),e(Ke,Br),e(Br,Ac),e(Ke,zc),e(Ke,qr),e(qr,Pc),d(t,Wi,n),d(t,Ee,n),e(Ee,Je),e(Je,Nr),u(fo,Nr,null),e(Ee,Lc),e(Ee,Hr),e(Hr,Ic),d(t,Ui,n),d(t,Qe,n),e(Qe,po),e(po,Sc),e(po,Ve),e(Ve,Mc),e(Ve,Rr),e(Rr,jc),e(Ve,Fc),e(po,Bc),e(Qe,qc),e(Qe,be),e(be,Nc),e(be,Gr),e(Gr,Hc),e(be,Rc),e(be,Wr),e(Wr,Gc),e(be,Wc),d(t,Xi,n),d(t,Ye,n),e(Ye,Uc),e(Ye,Ur),e(Ur,Xc),e(Ye,Kc),d(t,Ki,n),u(mo,t,n),d(t,Ji,n),d(t,Ze,n),e(Ze,Jc),e(Ze,Xr),e(Xr,Qc),e(Ze,Vc),d(t,Qi,n),u(uo,t,n),d(t,Vi,n),d(t,we,n),e(we,et),e(et,Kr),u(_o,Kr,null),e(we,Yc),e(we,Jr),e(Jr,Zc),d(t,Yi,n),d(t,A,n),e(A,eh),e(A,Qr),e(Qr,th),e(A,oh),e(A,Vr),e(Vr,rh),e(A,ah),e(A,Yr),e(Yr,lh),e(A,sh),d(t,Zi,n),d(t,tt,n),e(tt,ih),e(tt,Zr),e(Zr,nh),e(tt,dh),d(t,en,n),d(t,T,n),e(T,ea),e(ea,ta),e(ta,ch),e(T,hh),e(T,oa),e(oa,ra),e(ra,fh),e(T,ph),e(T,aa),e(aa,la),e(la,mh),e(T,uh),e(T,sa),e(sa,ia),e(ia,_h),e(T,vh),e(T,na),e(na,da),e(da,Eh),e(T,bh),e(T,ca),e(ca,ha),e(ha,wh),e(T,yh),e(T,fa),e(fa,pa),e(pa,kh),e(T,Th),e(T,ma),e(ma,ua),e(ua,gh),d(t,tn,n),d(t,ye,n),e(ye,ot),e(ot,_a),u(vo,_a,null),e(ye,$h),e(ye,va),e(va,Ch),d(t,on,n),d(t,V,n),e(V,Oh),e(V,Ea),e(Ea,Dh),e(V,xh),e(V,ba),e(ba,Ah),e(V,zh),d(t,rn,n),d(t,rt,n),e(rt,Ph),e(rt,wa),e(wa,Lh),e(rt,Ih),d(t,an,n),u(Eo,t,n),d(t,ln,n),d(t,at,n),e(at,Sh),e(at,ya),e(ya,Mh),e(at,jh),d(t,sn,n),u(bo,t,n),d(t,nn,n),d(t,or,n),e(or,Fh),d(t,dn,n),u(wo,t,n),d(t,cn,n),d(t,ke,n),e(ke,lt),e(lt,ka),u(yo,ka,null),e(ke,Bh),e(ke,Ta),e(Ta,qh),d(t,hn,n),d(t,st,n),e(st,Nh),e(st,ga),e(ga,Hh),e(st,Rh),d(t,fn,n),d(t,rr,n),e(rr,Gh),d(t,pn,n),d(t,Te,n),e(Te,it),e(it,$a),u(ko,$a,null),e(Te,Wh),e(Te,Ca),e(Ca,Uh),d(t,mn,n),d(t,nt,n),e(nt,Xh),e(nt,Oa),e(Oa,Kh),e(nt,Jh),d(t,un,n),d(t,dt,n),e(dt,Qh),e(dt,Da),e(Da,Vh),e(dt,Yh),d(t,_n,n),u(To,t,n),d(t,vn,n),d(t,ct,n),e(ct,Zh),e(ct,xa),e(xa,ef),e(ct,tf),d(t,En,n),u(go,t,n),d(t,bn,n),d(t,ge,n),e(ge,ht),e(ht,Aa),u($o,Aa,null),e(ge,of),e(ge,$e),e($e,rf),e($e,za),e(za,af),e($e,lf),e($e,Pa),e(Pa,sf),e($e,nf),d(t,wn,n),d(t,ft,n),e(ft,df),e(ft,Co),e(Co,La),e(La,cf),e(Co,hf),e(ft,ff),d(t,yn,n),d(t,pt,n),e(pt,pf),e(pt,Ia),e(Ia,mf),e(pt,uf),d(t,kn,n),d(t,Ce,n),e(Ce,mt),e(mt,Sa),u(Oo,Sa,null),e(Ce,_f),e(Ce,Ma),e(Ma,vf),d(t,Tn,n),d(t,z,n),e(z,Ef),e(z,ja),e(ja,bf),e(z,wf),e(z,Fa),e(Fa,yf),e(z,kf),e(z,Ba),e(Ba,Tf),e(z,gf),d(t,gn,n),d(t,ut,n),e(ut,$f),e(ut,qa),e(qa,Cf),e(ut,Of),d(t,$n,n),u(Do,t,n),d(t,Cn,n),d(t,_t,n),e(_t,Df),e(_t,Na),e(Na,xf),e(_t,Af),d(t,On,n),u(xo,t,n),d(t,Dn,n),d(t,ar,n),e(ar,zf),d(t,xn,n),u(Ao,t,n),d(t,An,n),d(t,Oe,n),e(Oe,vt),e(vt,Ha),u(zo,Ha,null),e(Oe,Pf),e(Oe,Ra),e(Ra,Lf),d(t,zn,n),d(t,Et,n),e(Et,If),e(Et,Po),e(Po,Sf),e(Et,Mf),d(t,Pn,n),d(t,lr,n),e(lr,jf),d(t,Ln,n),d(t,sr,n),e(sr,Ff),d(t,In,n),d(t,b,n),e(b,P),e(P,Ga),e(Ga,Bf),e(P,qf),e(P,Wa),e(Wa,Nf),e(P,Hf),e(P,Ua),e(Ua,Rf),e(P,Gf),e(P,Xa),e(Xa,Wf),e(P,Uf),e(b,Xf),e(b,L),e(L,Ka),e(Ka,Kf),e(L,Jf),e(L,Ja),e(Ja,Qf),e(L,Vf),e(L,Qa),e(Qa,Yf),e(L,Zf),e(L,Va),e(Va,ep),e(L,tp),e(b,op),e(b,Y),e(Y,Ya),e(Ya,rp),e(Y,ap),e(Y,Za),e(Za,lp),e(Y,sp),e(Y,el),e(el,ip),e(Y,np),e(b,dp),e(b,Z),e(Z,tl),e(tl,cp),e(Z,hp),e(Z,ol),e(ol,fp),e(Z,pp),e(Z,rl),e(rl,mp),e(Z,up),e(b,_p),e(b,ee),e(ee,al),e(al,vp),e(ee,Ep),e(ee,ll),e(ll,bp),e(ee,wp),e(ee,sl),e(sl,yp),e(ee,kp),e(b,Tp),e(b,te),e(te,il),e(il,gp),e(te,$p),e(te,nl),e(nl,Cp),e(te,Op),e(te,dl),e(dl,Dp),e(te,xp),e(b,Ap),e(b,oe),e(oe,cl),e(cl,zp),e(oe,Pp),e(oe,hl),e(hl,Lp),e(oe,Ip),e(oe,fl),e(fl,Sp),e(oe,Mp),e(b,jp),e(b,re),e(re,pl),e(pl,Fp),e(re,Bp),e(re,ml),e(ml,qp),e(re,Np),e(re,ul),e(ul,Hp),e(re,Rp),e(b,Gp),e(b,ae),e(ae,_l),e(_l,Wp),e(ae,Up),e(ae,vl),e(vl,Xp),e(ae,Kp),e(ae,El),e(El,Jp),e(ae,Qp),e(b,Vp),e(b,le),e(le,bl),e(bl,Yp),e(le,Zp),e(le,wl),e(wl,em),e(le,tm),e(le,yl),e(yl,om),e(le,rm),e(b,am),e(b,se),e(se,kl),e(kl,lm),e(se,sm),e(se,Tl),e(Tl,im),e(se,nm),e(se,gl),e(gl,dm),e(se,cm),d(t,Sn,n),d(t,ir,n),e(ir,hm),d(t,Mn,n),d(t,I,n),e(I,bt),e(bt,$l),e($l,fm),e(bt,pm),e(bt,Cl),e(Cl,mm),e(bt,um),e(I,_m),e(I,wt),e(wt,Ol),e(Ol,vm),e(wt,Em),e(wt,Dl),e(Dl,bm),e(wt,wm),e(I,ym),e(I,yt),e(yt,xl),e(xl,km),e(yt,Tm),e(yt,Al),e(Al,gm),e(yt,$m),e(I,Cm),e(I,kt),e(kt,zl),e(zl,Om),e(kt,Dm),e(kt,Pl),e(Pl,xm),e(kt,Am),d(t,jn,n),d(t,nr,n),e(nr,zm),d(t,Fn,n),d(t,ie,n),e(ie,De),e(De,Pm),e(De,Ll),e(Ll,Lm),e(De,Im),e(De,Il),e(Il,Sm),e(De,Mm),e(ie,jm),e(ie,xe),e(xe,Fm),e(xe,Sl),e(Sl,Bm),e(xe,qm),e(xe,Ml),e(Ml,Nm),e(xe,Hm),e(ie,Rm),e(ie,Ae),e(Ae,Gm),e(Ae,jl),e(jl,Wm),e(Ae,Um),e(Ae,Fl),e(Fl,Xm),e(Ae,Km),d(t,Bn,n),d(t,Tt,n),e(Tt,Jm),e(Tt,Bl),e(Bl,Qm),e(Tt,Vm),d(t,qn,n),d(t,g,n),e(g,j),e(j,Ym),e(j,ql),e(ql,Zm),e(j,eu),e(j,Nl),e(Nl,tu),e(j,ou),e(j,Hl),e(Hl,ru),e(j,au),e(g,lu),e(g,F),e(F,su),e(F,Rl),e(Rl,iu),e(F,nu),e(F,Gl),e(Gl,du),e(F,cu),e(F,Wl),e(Wl,hu),e(F,fu),e(g,pu),e(g,ze),e(ze,mu),e(ze,Ul),e(Ul,uu),e(ze,_u),e(ze,Xl),e(Xl,vu),e(ze,Eu),e(g,bu),e(g,B),e(B,wu),e(B,Kl),e(Kl,yu),e(B,ku),e(B,Jl),e(Jl,Tu),e(B,gu),e(B,Ql),e(Ql,$u),e(B,Cu),e(g,Ou),e(g,q),e(q,Du),e(q,Vl),e(Vl,xu),e(q,Au),e(q,Yl),e(Yl,zu),e(q,Pu),e(q,Zl),e(Zl,Lu),e(q,Iu),e(g,Su),e(g,N),e(N,Mu),e(N,es),e(es,ju),e(N,Fu),e(N,ts),e(ts,Bu),e(N,qu),e(N,os),e(os,Nu),e(N,Hu),e(g,Ru),e(g,H),e(H,Gu),e(H,rs),e(rs,Wu),e(H,Uu),e(H,as),e(as,Xu),e(H,Ku),e(H,ls),e(ls,Ju),e(H,Qu),e(g,Vu),e(g,R),e(R,Yu),e(R,ss),e(ss,Zu),e(R,e_),e(R,is),e(is,t_),e(R,o_),e(R,ns),e(ns,r_),e(R,a_),d(t,Nn,n),d(t,gt,n),e(gt,l_),e(gt,ds),e(ds,s_),e(gt,i_),d(t,Hn,n),d(t,D,n),e(D,G),e(G,n_),e(G,cs),e(cs,d_),e(G,c_),e(G,hs),e(hs,h_),e(G,f_),e(G,fs),e(fs,p_),e(G,m_),e(D,u_),e(D,W),e(W,__),e(W,ps),e(ps,v_),e(W,E_),e(W,ms),e(ms,b_),e(W,w_),e(W,us),e(us,y_),e(W,k_),e(D,T_),e(D,U),e(U,g_),e(U,_s),e(_s,$_),e(U,C_),e(U,vs),e(vs,O_),e(U,D_),e(U,Es),e(Es,x_),e(U,A_),e(D,z_),e(D,X),e(X,P_),e(X,bs),e(bs,L_),e(X,I_),e(X,ws),e(ws,S_),e(X,M_),e(X,ys),e(ys,j_),e(X,F_),e(D,B_),e(D,K),e(K,q_),e(K,ks),e(ks,N_),e(K,H_),e(K,Ts),e(Ts,R_),e(K,G_),e(K,gs),e(gs,W_),e(K,U_),d(t,Rn,n),d(t,$t,n),e($t,X_),e($t,$s),e($s,K_),e($t,J_),d(t,Gn,n),d(t,dr,n),e(dr,J),e(J,Q_),e(J,Cs),e(Cs,V_),e(J,Y_),e(J,Os),e(Os,Z_),e(J,ev),e(J,Ds),e(Ds,tv),e(J,ov),d(t,Wn,n),d(t,cr,n),e(cr,rv),d(t,Un,n),d(t,Ct,n),e(Ct,Pe),e(Pe,av),e(Pe,xs),e(xs,lv),e(Pe,sv),e(Pe,As),e(As,iv),e(Pe,nv),e(Ct,dv),e(Ct,Le),e(Le,cv),e(Le,zs),e(zs,hv),e(Le,fv),e(Le,Ps),e(Ps,pv),e(Le,mv),d(t,Xn,n),d(t,hr,n),e(hr,uv),d(t,Kn,n),d(t,fr,n),e(fr,Q),e(Q,_v),e(Q,Ls),e(Ls,vv),e(Q,Ev),e(Q,Is),e(Is,bv),e(Q,wv),e(Q,Ss),e(Ss,yv),e(Q,kv),d(t,Jn,n),d(t,Ie,n),e(Ie,Ot),e(Ot,Ms),u(Lo,Ms,null),e(Ie,Tv),e(Ie,js),e(js,gv),d(t,Qn,n),d(t,Dt,n),e(Dt,$v),e(Dt,Fs),e(Fs,Cv),e(Dt,Ov),d(t,Vn,n),d(t,Se,n),e(Se,xt),e(xt,Bs),u(Io,Bs,null),e(Se,Dv),e(Se,Me),e(Me,xv),e(Me,qs),e(qs,Av),e(Me,zv),e(Me,Ns),e(Ns,Pv),e(Me,Lv),d(t,Yn,n),d(t,S,n),e(S,Iv),e(S,Hs),e(Hs,Sv),e(S,Mv),e(S,Rs),e(Rs,jv),e(S,Fv),e(S,Gs),e(Gs,Bv),e(S,qv),d(t,Zn,n),d(t,At,n),e(At,Nv),e(At,Ws),e(Ws,Hv),e(At,Rv),d(t,ed,n),d(t,zt,n),e(zt,Gv),e(zt,Us),e(Us,Wv),e(zt,Uv),d(t,td,n),d(t,je,n),e(je,Pt),e(Pt,Xs),u(So,Xs,null),e(je,Xv),e(je,Ks),e(Ks,Kv),d(t,od,n),d(t,Lt,n),e(Lt,Jv),e(Lt,Js),e(Js,Qv),e(Lt,Vv),d(t,rd,n),d(t,Fe,n),e(Fe,It),e(It,Qs),u(Mo,Qs,null),e(Fe,Yv),e(Fe,pr),e(pr,Zv),e(pr,Vs),e(Vs,e1),d(t,ad,n),d(t,ne,n),e(ne,t1),e(ne,Ys),e(Ys,o1),e(ne,r1),e(ne,Zs),e(Zs,a1),e(ne,l1),d(t,ld,n),d(t,St,n),e(St,s1),e(St,jo),e(jo,i1),e(St,n1),d(t,sd,n),d(t,Mt,n),e(Mt,d1),e(Mt,ei),e(ei,c1),e(Mt,h1),d(t,id,n),d(t,de,n),e(de,f1),e(de,ti),e(ti,p1),e(de,m1),e(de,oi),e(oi,u1),e(de,_1),d(t,nd,n),u(Fo,t,n),d(t,dd,n),d(t,Be,n),e(Be,jt),e(jt,ri),u(Bo,ri,null),e(Be,v1),e(Be,ai),e(ai,E1),d(t,cd,n),d(t,Ft,n),e(Ft,b1),e(Ft,li),e(li,w1),e(Ft,y1),d(t,hd,n),d(t,Bt,n),e(Bt,si),e(si,qe),e(qe,k1),e(qe,ii),e(ii,T1),e(qe,g1),e(qe,ni),e(ni,$1),e(qe,C1),e(Bt,O1),e(Bt,di),e(di,w),e(w,D1),e(w,ci),e(ci,x1),e(w,A1),e(w,hi),e(hi,z1),e(w,P1),e(w,fi),e(fi,L1),e(w,I1),e(w,pi),e(pi,S1),e(w,M1),e(w,mi),e(mi,j1),e(w,F1),e(w,ui),e(ui,B1),e(w,q1),e(w,_i),e(_i,N1),e(w,H1),e(w,vi),e(vi,R1),e(w,G1),e(w,Ei),e(Ei,W1),e(w,U1),e(w,bi),e(bi,X1),e(w,K1),d(t,fd,n),d(t,qt,n),e(qt,J1),e(qt,wi),e(wi,Q1),e(qt,V1),d(t,pd,n),d(t,mr,n),e(mr,Y1),d(t,md,n),u(qo,t,n),d(t,ud,n),d(t,Ne,n),e(Ne,Nt),e(Nt,yi),u(No,yi,null),e(Ne,Z1),e(Ne,ki),e(ki,eE),d(t,_d,n),d(t,M,n),e(M,tE),e(M,Ti),e(Ti,oE),e(M,rE),e(M,gi),e(gi,aE),e(M,lE),e(M,$i),e($i,sE),e(M,iE),d(t,vd,n),d(t,ce,n),e(ce,Ci),e(Ci,nE),e(ce,dE),e(ce,Oi),e(Oi,cE),e(ce,hE),e(ce,Di),e(Di,fE),d(t,Ed,n),d(t,he,n),e(he,pE),e(he,xi),e(xi,mE),e(he,uE),e(he,Ai),e(Ai,_E),e(he,vE),d(t,bd,n),d(t,Ht,n),e(Ht,EE),e(Ht,Ho),e(Ho,bE),e(Ht,wE),d(t,wd,n),d(t,fe,n),e(fe,yE),e(fe,zi),e(zi,kE),e(fe,TE),e(fe,Pi),e(Pi,gE),e(fe,$E),d(t,yd,n),u(Ro,t,n),kd=!0},p:uw,i(t){kd||(_(no.$$.fragment,t),_(co.$$.fragment,t),_(ho.$$.fragment,t),_(fo.$$.fragment,t),_(mo.$$.fragment,t),_(uo.$$.fragment,t),_(_o.$$.fragment,t),_(vo.$$.fragment,t),_(Eo.$$.fragment,t),_(bo.$$.fragment,t),_(wo.$$.fragment,t),_(yo.$$.fragment,t),_(ko.$$.fragment,t),_(To.$$.fragment,t),_(go.$$.fragment,t),_($o.$$.fragment,t),_(Oo.$$.fragment,t),_(Do.$$.fragment,t),_(xo.$$.fragment,t),_(Ao.$$.fragment,t),_(zo.$$.fragment,t),_(Lo.$$.fragment,t),_(Io.$$.fragment,t),_(So.$$.fragment,t),_(Mo.$$.fragment,t),_(Fo.$$.fragment,t),_(Bo.$$.fragment,t),_(qo.$$.fragment,t),_(No.$$.fragment,t),_(Ro.$$.fragment,t),kd=!0)},o(t){v(no.$$.fragment,t),v(co.$$.fragment,t),v(ho.$$.fragment,t),v(fo.$$.fragment,t),v(mo.$$.fragment,t),v(uo.$$.fragment,t),v(_o.$$.fragment,t),v(vo.$$.fragment,t),v(Eo.$$.fragment,t),v(bo.$$.fragment,t),v(wo.$$.fragment,t),v(yo.$$.fragment,t),v(ko.$$.fragment,t),v(To.$$.fragment,t),v(go.$$.fragment,t),v($o.$$.fragment,t),v(Oo.$$.fragment,t),v(Do.$$.fragment,t),v(xo.$$.fragment,t),v(Ao.$$.fragment,t),v(zo.$$.fragment,t),v(Lo.$$.fragment,t),v(Io.$$.fragment,t),v(So.$$.fragment,t),v(Mo.$$.fragment,t),v(Fo.$$.fragment,t),v(Bo.$$.fragment,t),v(qo.$$.fragment,t),v(No.$$.fragment,t),v(Ro.$$.fragment,t),kd=!1},d(t){o(me),t&&o(Fi),t&&o(ue),E(no),t&&o(Bi),t&&o(_e),E(co),t&&o(qi),t&&o(Zo),t&&o(Ni),t&&o(ve),E(ho),t&&o(Hi),t&&o(er),t&&o(Ri),t&&o(tr),t&&o(Gi),t&&o(Ke),t&&o(Wi),t&&o(Ee),E(fo),t&&o(Ui),t&&o(Qe),t&&o(Xi),t&&o(Ye),t&&o(Ki),E(mo,t),t&&o(Ji),t&&o(Ze),t&&o(Qi),E(uo,t),t&&o(Vi),t&&o(we),E(_o),t&&o(Yi),t&&o(A),t&&o(Zi),t&&o(tt),t&&o(en),t&&o(T),t&&o(tn),t&&o(ye),E(vo),t&&o(on),t&&o(V),t&&o(rn),t&&o(rt),t&&o(an),E(Eo,t),t&&o(ln),t&&o(at),t&&o(sn),E(bo,t),t&&o(nn),t&&o(or),t&&o(dn),E(wo,t),t&&o(cn),t&&o(ke),E(yo),t&&o(hn),t&&o(st),t&&o(fn),t&&o(rr),t&&o(pn),t&&o(Te),E(ko),t&&o(mn),t&&o(nt),t&&o(un),t&&o(dt),t&&o(_n),E(To,t),t&&o(vn),t&&o(ct),t&&o(En),E(go,t),t&&o(bn),t&&o(ge),E($o),t&&o(wn),t&&o(ft),t&&o(yn),t&&o(pt),t&&o(kn),t&&o(Ce),E(Oo),t&&o(Tn),t&&o(z),t&&o(gn),t&&o(ut),t&&o($n),E(Do,t),t&&o(Cn),t&&o(_t),t&&o(On),E(xo,t),t&&o(Dn),t&&o(ar),t&&o(xn),E(Ao,t),t&&o(An),t&&o(Oe),E(zo),t&&o(zn),t&&o(Et),t&&o(Pn),t&&o(lr),t&&o(Ln),t&&o(sr),t&&o(In),t&&o(b),t&&o(Sn),t&&o(ir),t&&o(Mn),t&&o(I),t&&o(jn),t&&o(nr),t&&o(Fn),t&&o(ie),t&&o(Bn),t&&o(Tt),t&&o(qn),t&&o(g),t&&o(Nn),t&&o(gt),t&&o(Hn),t&&o(D),t&&o(Rn),t&&o($t),t&&o(Gn),t&&o(dr),t&&o(Wn),t&&o(cr),t&&o(Un),t&&o(Ct),t&&o(Xn),t&&o(hr),t&&o(Kn),t&&o(fr),t&&o(Jn),t&&o(Ie),E(Lo),t&&o(Qn),t&&o(Dt),t&&o(Vn),t&&o(Se),E(Io),t&&o(Yn),t&&o(S),t&&o(Zn),t&&o(At),t&&o(ed),t&&o(zt),t&&o(td),t&&o(je),E(So),t&&o(od),t&&o(Lt),t&&o(rd),t&&o(Fe),E(Mo),t&&o(ad),t&&o(ne),t&&o(ld),t&&o(St),t&&o(sd),t&&o(Mt),t&&o(id),t&&o(de),t&&o(nd),E(Fo,t),t&&o(dd),t&&o(Be),E(Bo),t&&o(cd),t&&o(Ft),t&&o(hd),t&&o(Bt),t&&o(fd),t&&o(qt),t&&o(pd),t&&o(mr),t&&o(md),E(qo,t),t&&o(ud),t&&o(Ne),E(No),t&&o(_d),t&&o(M),t&&o(vd),t&&o(ce),t&&o(Ed),t&&o(he),t&&o(bd),t&&o(Ht),t&&o(wd),t&&o(fe),t&&o(yd),E(Ro,t)}}}const Ew={local:"migrating-from-previous-packages",sections:[{local:"migrating-from-transformers-v3x-to-v4x",sections:[{local:"1-autotokenizers-and-pipelines-now-use-fast-rust-tokenizers-by-default",sections:[{local:"how-to-obtain-the-same-behavior-as-v3x-in-v4x",title:"How to obtain the same behavior as v3.x in v4.x"}],title:"1. AutoTokenizers and pipelines now use fast (rust) tokenizers by default."},{local:"2-sentencepiece-is-removed-from-the-required-dependencies",sections:[{local:"how-to-obtain-the-same-behavior-as-v3x-in-v4x",title:"How to obtain the same behavior as v3.x in v4.x"}],title:"2. SentencePiece is removed from the required dependencies"},{local:"3-the-architecture-of-the-repo-has-been-updated-so-that-each-model-resides-in-its-folder",sections:[{local:"how-to-obtain-the-same-behavior-as-v3x-in-v4x",title:"How to obtain the same behavior as v3.x in v4.x"}],title:"3. The architecture of the repo has been updated so that each model resides in its folder"},{local:"4-switching-the-returndict-argument-to-true-by-default",sections:[{local:"how-to-obtain-the-same-behavior-as-v3x-in-v4x",title:"How to obtain the same behavior as v3.x in v4.x"}],title:"4. Switching the `return_dict` argument to `True` by default"},{local:"5-removed-some-deprecated-attributes",title:"5. Removed some deprecated attributes"}],title:"Migrating from transformers `v3.x` to `v4.x`"},{local:"migrating-from-pytorchtransformers-to-transformers",sections:[{local:"positional-order-of-some-models-keywords-inputs-attentionmask-tokentypeids-changed",title:"Positional order of some models' keywords inputs (`attention_mask`, `token_type_ids`...) changed"}],title:"Migrating from pytorch-transformers to \u{1F917} Transformers"},{local:"migrating-from-pytorchpretrainedbert",sections:[{local:"models-always-output-tuples",title:"Models always output `tuples`"},{local:"serialization",title:"Serialization"},{local:"optimizers-bertadam-openaiadam-are-now-adamw-schedules-are-standard-pytorch-schedules",title:"Optimizers: BertAdam & OpenAIAdam are now AdamW, schedules are standard PyTorch schedules"}],title:"Migrating from pytorch-pretrained-bert"}],title:"Migrating from previous packages"};function bw(DE){return _w(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Tw extends hw{constructor(me){super();fw(this,me,bw,vw,pw,{})}}export{Tw as default,Ew as metadata};
