import{S as Bt,i as Ft,s as Ot,e as r,k as p,w as ae,t as s,M as Ht,c as n,d as t,m as c,a,x as ie,h as l,b as i,G as o,g as f,y as se,L as Rt,q as le,o as he,B as fe,v as Dt}from"../chunks/vendor-hf-doc-builder.js";import{I as pe}from"../chunks/IconCopyLink-hf-doc-builder.js";function Kt(_t){let d,ce,m,P,F,j,Le,O,Ne,de,M,Ue,me,u,w,H,A,Xe,R,Je,ue,E,Ge,S,Me,Be,_e,_,g,D,q,Fe,K,Oe,ve,T,He,z,Re,De,ye,v,b,Q,C,Ke,V,Qe,Pe,I,Ve,L,We,Ye,we,y,$,W,N,Ze,Y,et,Ee,U,tt,X,ot,ge,x,J,Z,rt,nt,B,at,ee,it,st,G,te,lt,ht,k,ft,oe,pt,ct,re,dt,Te;return j=new pe({}),A=new pe({}),q=new pe({}),C=new pe({}),N=new pe({}),{c(){d=r("meta"),ce=p(),m=r("h1"),P=r("a"),F=r("span"),ae(j.$$.fragment),Le=p(),O=r("span"),Ne=s("Efficient Inference on CPU"),de=p(),M=r("p"),Ue=s("This guide focuses on inferencing large models efficiently on CPU."),me=p(),u=r("h2"),w=r("a"),H=r("span"),ae(A.$$.fragment),Xe=p(),R=r("span"),Je=s("PyTorch JIT-mode (TorchScript)"),ue=s(`

TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency.
Comparing to default eager mode, jit mode in PyTorch normally yields better performance for model inference from optimization methodologies like operator fusion.
`),E=r("p"),Ge=s("For a gentle introduction to TorchScript, see the Introduction to "),S=r("a"),Me=s("PyTorch TorchScript tutorial"),Be=s("."),_e=p(),_=r("h3"),g=r("a"),D=r("span"),ae(q.$$.fragment),Fe=p(),K=r("span"),Oe=s("IPEX Graph Optimization with JIT-mode"),ve=s(`

Intel\xAE Extension for PyTorch provides further optimizations in jit mode for Transformers series models. It is highly recommended for users to take advantage of Intel\xAE Extension for PyTorch with jit mode. Some frequently used operator patterns from Transformers models are already supported in Intel\xAE Extension for PyTorch with jit mode fusions. Those fusion patterns like Multi-head-attention fusion, Concat Linear, Linear+Add, Linear+Gelu, Add+LayerNorm fusion and etc. are enabled and perform well. The benefit of the fusion is delivered to users in a transparent fashion. According to the analysis, ~70% of most popular NLP tasks in question-answering, text-classification, and token-classification can get performance benefits with these fusion patterns for both Float32 precision and BFloat16 Mixed precision.
`),T=r("p"),He=s("Check more detailed information for "),z=r("a"),Re=s("IPEX Graph Optimization"),De=s("."),ye=p(),v=r("h4"),b=r("a"),Q=r("span"),ae(C.$$.fragment),Ke=p(),V=r("span"),Qe=s("IPEX installation:"),Pe=p(),I=r("p"),Ve=s("IPEX release is following PyTorch, check the approaches for "),L=r("a"),We=s("IPEX installation"),Ye=s("."),we=p(),y=r("h3"),$=r("a"),W=r("span"),ae(N.$$.fragment),Ze=p(),Y=r("span"),et=s("Usage of JIT-mode"),Ee=s(`

To enable jit mode in Trainer, users should add \`jit_mode_eval\` in Trainer command arguments.
`),U=r("p"),tt=s("Take an example of the use cases on "),X=r("a"),ot=s("Transformers question-answering"),ge=p(),x=r("ul"),J=r("li"),Z=r("p"),rt=s("Inference using jit mode on CPU:"),nt=p(),B=r("pre"),at=s(`python run_qa.py \\
--model_name_or_path csarron/bert-base-uncased-squad-v1 \\
--dataset_name squad \\
--do_eval \\
--max_seq_length 384 \\
--doc_stride 128 \\
--output_dir /tmp/ \\
--no_cuda \\
`),ee=r("b"),it=s("--jit_mode_eval "),st=p(),G=r("li"),te=r("p"),lt=s("Inference with IPEX using jit mode on CPU:"),ht=p(),k=r("pre"),ft=s(`python run_qa.py \\
--model_name_or_path csarron/bert-base-uncased-squad-v1 \\
--dataset_name squad \\
--do_eval \\
--max_seq_length 384 \\
--doc_stride 128 \\
--output_dir /tmp/ \\
--no_cuda \\
`),oe=r("b"),pt=s("--use_ipex \\"),ct=s(`
`),re=r("b"),dt=s("--jit_mode_eval"),this.h()},l(e){const h=Ht('[data-svelte="svelte-1phssyn"]',document.head);d=n(h,"META",{name:!0,content:!0}),h.forEach(t),ce=c(e),m=n(e,"H1",{class:!0});var be=a(m);P=n(be,"A",{id:!0,class:!0,href:!0});var vt=a(P);F=n(vt,"SPAN",{});var yt=a(F);ie(j.$$.fragment,yt),yt.forEach(t),vt.forEach(t),Le=c(be),O=n(be,"SPAN",{});var Pt=a(O);Ne=l(Pt,"Efficient Inference on CPU"),Pt.forEach(t),be.forEach(t),de=c(e),M=n(e,"P",{});var wt=a(M);Ue=l(wt,"This guide focuses on inferencing large models efficiently on CPU."),wt.forEach(t),me=c(e),u=n(e,"H2",{class:!0});var Ie=a(u);w=n(Ie,"A",{id:!0,class:!0,href:!0});var Et=a(w);H=n(Et,"SPAN",{});var gt=a(H);ie(A.$$.fragment,gt),gt.forEach(t),Et.forEach(t),Xe=c(Ie),R=n(Ie,"SPAN",{});var Tt=a(R);Je=l(Tt,"PyTorch JIT-mode (TorchScript)"),Tt.forEach(t),Ie.forEach(t),ue=l(e,`

TorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency.
Comparing to default eager mode, jit mode in PyTorch normally yields better performance for model inference from optimization methodologies like operator fusion.
`),E=n(e,"P",{});var $e=a(E);Ge=l($e,"For a gentle introduction to TorchScript, see the Introduction to "),S=n($e,"A",{href:!0,rel:!0});var bt=a(S);Me=l(bt,"PyTorch TorchScript tutorial"),bt.forEach(t),Be=l($e,"."),$e.forEach(t),_e=c(e),_=n(e,"H3",{class:!0});var xe=a(_);g=n(xe,"A",{id:!0,class:!0,href:!0});var It=a(g);D=n(It,"SPAN",{});var $t=a(D);ie(q.$$.fragment,$t),$t.forEach(t),It.forEach(t),Fe=c(xe),K=n(xe,"SPAN",{});var xt=a(K);Oe=l(xt,"IPEX Graph Optimization with JIT-mode"),xt.forEach(t),xe.forEach(t),ve=l(e,`

Intel\xAE Extension for PyTorch provides further optimizations in jit mode for Transformers series models. It is highly recommended for users to take advantage of Intel\xAE Extension for PyTorch with jit mode. Some frequently used operator patterns from Transformers models are already supported in Intel\xAE Extension for PyTorch with jit mode fusions. Those fusion patterns like Multi-head-attention fusion, Concat Linear, Linear+Add, Linear+Gelu, Add+LayerNorm fusion and etc. are enabled and perform well. The benefit of the fusion is delivered to users in a transparent fashion. According to the analysis, ~70% of most popular NLP tasks in question-answering, text-classification, and token-classification can get performance benefits with these fusion patterns for both Float32 precision and BFloat16 Mixed precision.
`),T=n(e,"P",{});var ke=a(T);He=l(ke,"Check more detailed information for "),z=n(ke,"A",{href:!0,rel:!0});var kt=a(z);Re=l(kt,"IPEX Graph Optimization"),kt.forEach(t),De=l(ke,"."),ke.forEach(t),ye=c(e),v=n(e,"H4",{class:!0});var je=a(v);b=n(je,"A",{id:!0,class:!0,href:!0});var jt=a(b);Q=n(jt,"SPAN",{});var At=a(Q);ie(C.$$.fragment,At),At.forEach(t),jt.forEach(t),Ke=c(je),V=n(je,"SPAN",{});var St=a(V);Qe=l(St,"IPEX installation:"),St.forEach(t),je.forEach(t),Pe=c(e),I=n(e,"P",{});var Ae=a(I);Ve=l(Ae,"IPEX release is following PyTorch, check the approaches for "),L=n(Ae,"A",{href:!0,rel:!0});var qt=a(L);We=l(qt,"IPEX installation"),qt.forEach(t),Ye=l(Ae,"."),Ae.forEach(t),we=c(e),y=n(e,"H3",{class:!0});var Se=a(y);$=n(Se,"A",{id:!0,class:!0,href:!0});var zt=a($);W=n(zt,"SPAN",{});var Ct=a(W);ie(N.$$.fragment,Ct),Ct.forEach(t),zt.forEach(t),Ze=c(Se),Y=n(Se,"SPAN",{});var Lt=a(Y);et=l(Lt,"Usage of JIT-mode"),Lt.forEach(t),Se.forEach(t),Ee=l(e,`

To enable jit mode in Trainer, users should add \`jit_mode_eval\` in Trainer command arguments.
`),U=n(e,"P",{});var mt=a(U);tt=l(mt,"Take an example of the use cases on "),X=n(mt,"A",{href:!0,rel:!0});var Nt=a(X);ot=l(Nt,"Transformers question-answering"),Nt.forEach(t),mt.forEach(t),ge=c(e),x=n(e,"UL",{});var qe=a(x);J=n(qe,"LI",{});var ze=a(J);Z=n(ze,"P",{});var Ut=a(Z);rt=l(Ut,"Inference using jit mode on CPU:"),Ut.forEach(t),nt=c(ze),B=n(ze,"PRE",{});var ut=a(B);at=l(ut,`python run_qa.py \\
--model_name_or_path csarron/bert-base-uncased-squad-v1 \\
--dataset_name squad \\
--do_eval \\
--max_seq_length 384 \\
--doc_stride 128 \\
--output_dir /tmp/ \\
--no_cuda \\
`),ee=n(ut,"B",{});var Xt=a(ee);it=l(Xt,"--jit_mode_eval "),Xt.forEach(t),ut.forEach(t),ze.forEach(t),st=c(qe),G=n(qe,"LI",{});var Ce=a(G);te=n(Ce,"P",{});var Jt=a(te);lt=l(Jt,"Inference with IPEX using jit mode on CPU:"),Jt.forEach(t),ht=c(Ce),k=n(Ce,"PRE",{});var ne=a(k);ft=l(ne,`python run_qa.py \\
--model_name_or_path csarron/bert-base-uncased-squad-v1 \\
--dataset_name squad \\
--do_eval \\
--max_seq_length 384 \\
--doc_stride 128 \\
--output_dir /tmp/ \\
--no_cuda \\
`),oe=n(ne,"B",{});var Gt=a(oe);pt=l(Gt,"--use_ipex \\"),Gt.forEach(t),ct=l(ne,`
`),re=n(ne,"B",{});var Mt=a(re);dt=l(Mt,"--jit_mode_eval"),Mt.forEach(t),ne.forEach(t),Ce.forEach(t),qe.forEach(t),this.h()},h(){i(d,"name","hf:doc:metadata"),i(d,"content",JSON.stringify(Qt)),i(P,"id","efficient-inference-on-cpu"),i(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(P,"href","#efficient-inference-on-cpu"),i(m,"class","relative group"),i(w,"id","pytorch-jitmode-torchscript"),i(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(w,"href","#pytorch-jitmode-torchscript"),i(u,"class","relative group"),i(S,"href","https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html#tracing-modules"),i(S,"rel","nofollow"),i(g,"id","ipex-graph-optimization-with-jitmode"),i(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(g,"href","#ipex-graph-optimization-with-jitmode"),i(_,"class","relative group"),i(z,"href","https://intel.github.io/intel-extension-for-pytorch/1.11.200/tutorials/features/graph_optimization.html"),i(z,"rel","nofollow"),i(b,"id","ipex-installation"),i(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(b,"href","#ipex-installation"),i(v,"class","relative group"),i(L,"href","https://intel.github.io/intel-extension-for-pytorch/"),i(L,"rel","nofollow"),i($,"id","usage-of-jitmode"),i($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i($,"href","#usage-of-jitmode"),i(y,"class","relative group"),i(X,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering"),i(X,"rel","nofollow")},m(e,h){o(document.head,d),f(e,ce,h),f(e,m,h),o(m,P),o(P,F),se(j,F,null),o(m,Le),o(m,O),o(O,Ne),f(e,de,h),f(e,M,h),o(M,Ue),f(e,me,h),f(e,u,h),o(u,w),o(w,H),se(A,H,null),o(u,Xe),o(u,R),o(R,Je),f(e,ue,h),f(e,E,h),o(E,Ge),o(E,S),o(S,Me),o(E,Be),f(e,_e,h),f(e,_,h),o(_,g),o(g,D),se(q,D,null),o(_,Fe),o(_,K),o(K,Oe),f(e,ve,h),f(e,T,h),o(T,He),o(T,z),o(z,Re),o(T,De),f(e,ye,h),f(e,v,h),o(v,b),o(b,Q),se(C,Q,null),o(v,Ke),o(v,V),o(V,Qe),f(e,Pe,h),f(e,I,h),o(I,Ve),o(I,L),o(L,We),o(I,Ye),f(e,we,h),f(e,y,h),o(y,$),o($,W),se(N,W,null),o(y,Ze),o(y,Y),o(Y,et),f(e,Ee,h),f(e,U,h),o(U,tt),o(U,X),o(X,ot),f(e,ge,h),f(e,x,h),o(x,J),o(J,Z),o(Z,rt),o(J,nt),o(J,B),o(B,at),o(B,ee),o(ee,it),o(x,st),o(x,G),o(G,te),o(te,lt),o(G,ht),o(G,k),o(k,ft),o(k,oe),o(oe,pt),o(k,ct),o(k,re),o(re,dt),Te=!0},p:Rt,i(e){Te||(le(j.$$.fragment,e),le(A.$$.fragment,e),le(q.$$.fragment,e),le(C.$$.fragment,e),le(N.$$.fragment,e),Te=!0)},o(e){he(j.$$.fragment,e),he(A.$$.fragment,e),he(q.$$.fragment,e),he(C.$$.fragment,e),he(N.$$.fragment,e),Te=!1},d(e){t(d),e&&t(ce),e&&t(m),fe(j),e&&t(de),e&&t(M),e&&t(me),e&&t(u),fe(A),e&&t(ue),e&&t(E),e&&t(_e),e&&t(_),fe(q),e&&t(ve),e&&t(T),e&&t(ye),e&&t(v),fe(C),e&&t(Pe),e&&t(I),e&&t(we),e&&t(y),fe(N),e&&t(Ee),e&&t(U),e&&t(ge),e&&t(x)}}}const Qt={local:"efficient-inference-on-cpu",sections:[{local:"pytorch-jitmode-torchscript",sections:[{local:"ipex-graph-optimization-with-jitmode",sections:[{local:"ipex-installation",title:"IPEX installation:"}],title:"IPEX Graph Optimization with JIT-mode"},{local:"usage-of-jitmode",title:"Usage of JIT-mode"}],title:"PyTorch JIT-mode (TorchScript)"}],title:"Efficient Inference on CPU"};function Vt(_t){return Dt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Zt extends Bt{constructor(d){super();Ft(this,d,Vt,Kt,Ot,{})}}export{Zt as default,Qt as metadata};
