import{S as ys,i as $s,s as zs,e as o,k as l,w as g,t as a,M as Bs,c as n,d as t,m as d,a as s,x as k,h as i,b as c,G as e,g as h,y as v,q as b,o as T,B as w,v as Rs,L as Ts}from"../../chunks/vendor-hf-doc-builder.js";import{D}from"../../chunks/Docstring-hf-doc-builder.js";import{C as ws}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as ct}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as bs}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function qs(Se){let u,E,y,p,R;return p=new ws({props:{code:`0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),{c(){u=o("p"),E=a("pair mask has the following format:"),y=l(),g(p.$$.fragment)},l(m){u=n(m,"P",{});var $=s(u);E=i($,"pair mask has the following format:"),$.forEach(t),y=d(m),k(p.$$.fragment,m)},m(m,$){h(m,u,$),e(u,E),h(m,y,$),v(p,m,$),R=!0},p:Ts,i(m){R||(b(p.$$.fragment,m),R=!0)},o(m){T(p.$$.fragment,m),R=!1},d(m){m&&t(u),m&&t(y),w(p,m)}}}function Es(Se){let u,E,y,p,R;return p=new ws({props:{code:`0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),{c(){u=o("p"),E=a("pair mask has the following format:"),y=l(),g(p.$$.fragment)},l(m){u=n(m,"P",{});var $=s(u);E=i($,"pair mask has the following format:"),$.forEach(t),y=d(m),k(p.$$.fragment,m)},m(m,$){h(m,u,$),e(u,E),h(m,y,$),v(p,m,$),R=!0},p:Ts,i(m){R||(b(p.$$.fragment,m),R=!0)},o(m){T(p.$$.fragment,m),R=!1},d(m){m&&t(u),m&&t(y),w(p,m)}}}function xs(Se){let u,E,y,p,R,m,$,lt,pr,jt,M,X,dt,ce,ur,mt,_r,Nt,J,gr,le,kr,vr,Ot,A,br,de,Tr,wr,me,yr,$r,Ut,j,G,ft,fe,zr,ht,Br,Wt,L,he,Rr,N,qr,Me,Er,xr,pe,Pr,Cr,Lr,O,Dr,je,Ar,Ir,Ne,Fr,Sr,Vt,U,Q,pt,ue,Mr,ut,jr,Ht,_,_e,Nr,_t,Or,Ur,Y,Oe,Wr,Vr,Ue,Hr,Kr,Xr,ge,Jr,We,Gr,Qr,Yr,I,ke,Zr,gt,eo,to,ve,Ve,ro,kt,oo,no,He,so,vt,ao,io,Z,be,co,bt,lo,mo,x,Te,fo,Tt,ho,po,ee,uo,W,_o,wt,go,ko,yt,vo,bo,To,te,we,wo,ye,yo,$t,$o,zo,Kt,V,re,zt,$e,Bo,Bt,Ro,Xt,z,ze,qo,Be,Eo,Rt,xo,Po,Co,oe,Ke,Lo,Do,Xe,Ao,Io,Fo,Re,So,Je,Mo,jo,No,F,qe,Oo,qt,Uo,Wo,Ee,Ge,Vo,Et,Ho,Ko,Qe,Xo,xt,Jo,Go,P,xe,Qo,Pt,Yo,Zo,ne,en,H,tn,Ct,rn,on,Lt,nn,sn,Jt,K,se,Dt,Pe,an,At,cn,Gt,q,Ce,ln,It,dn,mn,Le,fn,Ye,hn,pn,un,De,_n,Ae,gn,kn,vn,Ze,Ie,Qt;return m=new ct({}),ce=new ct({}),fe=new ct({}),he=new D({props:{name:"class transformers.RetriBertConfig",anchor:"transformers.RetriBertConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 8"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"share_encoders",val:" = True"},{name:"projection_dim",val:" = 128"},{name:"pad_token_id",val:" = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.RetriBertConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the RetriBERT model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.24.0/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a>`,name:"vocab_size"},{anchor:"transformers.RetriBertConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.RetriBertConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.RetriBertConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.RetriBertConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.RetriBertConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.RetriBertConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.RetriBertConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.RetriBertConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.RetriBertConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <em>token_type_ids</em> passed into <a href="/docs/transformers/v4.24.0/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.RetriBertConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.RetriBertConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.RetriBertConfig.share_encoders",description:`<strong>share_encoders</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use the same Bert-type encoder for the queries and document`,name:"share_encoders"},{anchor:"transformers.RetriBertConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Final dimension of the query and document representation after projection`,name:"projection_dim"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/retribert/configuration_retribert.py#L31"}}),ue=new ct({}),_e=new D({props:{name:"class transformers.RetriBertTokenizer",anchor:"transformers.RetriBertTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.RetriBertTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.RetriBertTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.RetriBertTokenizer.do_basic_tokenize",description:`<strong>do_basic_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to do basic tokenization before WordPiece.`,name:"do_basic_tokenize"},{anchor:"transformers.RetriBertTokenizer.never_split",description:`<strong>never_split</strong> (<code>Iterable</code>, <em>optional</em>) &#x2014;
Collection of tokens which will never be split during tokenization. Only has an effect when
<code>do_basic_tokenize=True</code>`,name:"never_split"},{anchor:"transformers.RetriBertTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.RetriBertTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.RetriBertTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.RetriBertTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.RetriBertTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.RetriBertTokenizer.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see this
<a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">issue</a>).`,name:"tokenize_chinese_chars"},{anchor:"transformers.RetriBertTokenizer.strip_accents",description:`<strong>strip_accents</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"strip_accents"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/retribert/tokenization_retribert.py#L70"}}),ke=new D({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.RetriBertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.RetriBertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.RetriBertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/retribert/tokenization_retribert.py#L213",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),be=new D({props:{name:"convert_tokens_to_string",anchor:"transformers.RetriBertTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/retribert/tokenization_retribert.py#L207"}}),Te=new D({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.RetriBertTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.RetriBertTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.RetriBertTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/retribert/tokenization_retribert.py#L268",returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ee=new bs({props:{anchor:"transformers.RetriBertTokenizer.create_token_type_ids_from_sequences.example",$$slots:{default:[qs]},$$scope:{ctx:Se}}}),we=new D({props:{name:"get_special_tokens_mask",anchor:"transformers.RetriBertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.RetriBertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.RetriBertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.RetriBertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/retribert/tokenization_retribert.py#L239",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),$e=new ct({}),ze=new D({props:{name:"class transformers.RetriBertTokenizerFast",anchor:"transformers.RetriBertTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.RetriBertTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.RetriBertTokenizerFast.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.RetriBertTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.RetriBertTokenizerFast.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.RetriBertTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.RetriBertTokenizerFast.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.RetriBertTokenizerFast.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.RetriBertTokenizerFast.clean_text",description:`<strong>clean_text</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to clean the text before tokenization by removing any control characters and replacing all
whitespaces by the classic one.`,name:"clean_text"},{anchor:"transformers.RetriBertTokenizerFast.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see <a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">this
issue</a>).`,name:"tokenize_chinese_chars"},{anchor:"transformers.RetriBertTokenizerFast.strip_accents",description:`<strong>strip_accents</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"strip_accents"},{anchor:"transformers.RetriBertTokenizerFast.wordpieces_prefix",description:`<strong>wordpieces_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;##&quot;</code>) &#x2014;
The prefix for subwords.`,name:"wordpieces_prefix"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/retribert/tokenization_retribert_fast.py#L54"}}),qe=new D({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.RetriBertTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:""},{name:"token_ids_1",val:" = None"}],parametersDescription:[{anchor:"transformers.RetriBertTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.RetriBertTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/retribert/tokenization_retribert_fast.py#L148",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),xe=new D({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.RetriBertTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.RetriBertTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.RetriBertTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/retribert/tokenization_retribert_fast.py#L173",returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ne=new bs({props:{anchor:"transformers.RetriBertTokenizerFast.create_token_type_ids_from_sequences.example",$$slots:{default:[Es]},$$scope:{ctx:Se}}}),Pe=new ct({}),Ce=new D({props:{name:"class transformers.RetriBertModel",anchor:"transformers.RetriBertModel",parameters:[{name:"config",val:": RetriBertConfig"}],parametersDescription:[{anchor:"transformers.RetriBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/retribert/modeling_retribert.py#L88"}}),Ie=new D({props:{name:"forward",anchor:"transformers.RetriBertModel.forward",parameters:[{name:"input_ids_query",val:": LongTensor"},{name:"attention_mask_query",val:": typing.Optional[torch.FloatTensor]"},{name:"input_ids_doc",val:": LongTensor"},{name:"attention_mask_doc",val:": typing.Optional[torch.FloatTensor]"},{name:"checkpoint_batch_size",val:": int = -1"}],parametersDescription:[{anchor:"transformers.RetriBertModel.forward.input_ids_query",description:`<strong>input_ids_query</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary for the queries in a batch.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/retribert#transformers.RetriBertTokenizer">RetriBertTokenizer</a>. See <a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids_query"},{anchor:"transformers.RetriBertModel.forward.attention_mask_query",description:`<strong>attention_mask_query</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask_query"},{anchor:"transformers.RetriBertModel.forward.input_ids_doc",description:`<strong>input_ids_doc</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary for the documents in a batch.`,name:"input_ids_doc"},{anchor:"transformers.RetriBertModel.forward.attention_mask_doc",description:`<strong>attention_mask_doc</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on documents padding token indices.`,name:"attention_mask_doc"},{anchor:"transformers.RetriBertModel.forward.checkpoint_batch_size",description:`<strong>checkpoint_batch_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>-1</code>) &#x2014;
If greater than 0, uses gradient checkpointing to only compute sequence representation on
<code>checkpoint_batch_size</code> examples at a time on the GPU. All query representations are still compared to
all document representations in the batch.`,name:"checkpoint_batch_size"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/retribert/modeling_retribert.py#L176",returnDescription:`
<p>The bidirectional cross-entropy loss obtained while trying to match each query to its
corresponding document and each document to its corresponding query in the batch</p>
`,returnType:`
<p>\`torch.FloatTensor\u201C</p>
`}}),{c(){u=o("meta"),E=l(),y=o("h1"),p=o("a"),R=o("span"),g(m.$$.fragment),$=l(),lt=o("span"),pr=a("RetriBERT"),jt=l(),M=o("h2"),X=o("a"),dt=o("span"),g(ce.$$.fragment),ur=l(),mt=o("span"),_r=a("Overview"),Nt=l(),J=o("p"),gr=a("The RetriBERT model was proposed in the blog post "),le=o("a"),kr=a(`Explain Anything Like I\u2019m Five: A Model for Open Domain Long Form
Question Answering`),vr=a(`. RetriBERT is a small model that uses either a single or
pair of BERT encoders with lower-dimension projection for dense semantic indexing of text.`),Ot=l(),A=o("p"),br=a("This model was contributed by "),de=o("a"),Tr=a("yjernite"),wr=a(`. Code to train and use the model can be
found `),me=o("a"),yr=a("here"),$r=a("."),Ut=l(),j=o("h2"),G=o("a"),ft=o("span"),g(fe.$$.fragment),zr=l(),ht=o("span"),Br=a("RetriBertConfig"),Wt=l(),L=o("div"),g(he.$$.fragment),Rr=l(),N=o("p"),qr=a("This is the configuration class to store the configuration of a "),Me=o("a"),Er=a("RetriBertModel"),xr=a(`. It is used to instantiate a
RetriBertModel model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the RetriBERT
`),pe=o("a"),Pr=a("yjernite/retribert-base-uncased"),Cr=a(" architecture."),Lr=l(),O=o("p"),Dr=a("Configuration objects inherit from "),je=o("a"),Ar=a("PretrainedConfig"),Ir=a(` and can be used to control the model outputs. Read the
documentation from `),Ne=o("a"),Fr=a("PretrainedConfig"),Sr=a(" for more information."),Vt=l(),U=o("h2"),Q=o("a"),pt=o("span"),g(ue.$$.fragment),Mr=l(),ut=o("span"),jr=a("RetriBertTokenizer"),Ht=l(),_=o("div"),g(_e.$$.fragment),Nr=l(),_t=o("p"),Or=a("Constructs a RetriBERT tokenizer."),Ur=l(),Y=o("p"),Oe=o("a"),Wr=a("RetriBertTokenizer"),Vr=a(" is identical to "),Ue=o("a"),Hr=a("BertTokenizer"),Kr=a(` and runs end-to-end tokenization: punctuation splitting
and wordpiece.`),Xr=l(),ge=o("p"),Jr=a("This tokenizer inherits from "),We=o("a"),Gr=a("PreTrainedTokenizer"),Qr=a(` which contains most of the main methods. Users should refer
to: this superclass for more information regarding those methods.`),Yr=l(),I=o("div"),g(ke.$$.fragment),Zr=l(),gt=o("p"),eo=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),to=l(),ve=o("ul"),Ve=o("li"),ro=a("single sequence: "),kt=o("code"),oo=a("[CLS] X [SEP]"),no=l(),He=o("li"),so=a("pair of sequences: "),vt=o("code"),ao=a("[CLS] A [SEP] B [SEP]"),io=l(),Z=o("div"),g(be.$$.fragment),co=l(),bt=o("p"),lo=a("Converts a sequence of tokens (string) in a single string."),mo=l(),x=o("div"),g(Te.$$.fragment),fo=l(),Tt=o("p"),ho=a("Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence"),po=l(),g(ee.$$.fragment),uo=l(),W=o("p"),_o=a("If "),wt=o("code"),go=a("token_ids_1"),ko=a(" is "),yt=o("code"),vo=a("None"),bo=a(", this method only returns the first portion of the mask (0s)."),To=l(),te=o("div"),g(we.$$.fragment),wo=l(),ye=o("p"),yo=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),$t=o("code"),$o=a("prepare_for_model"),zo=a(" method."),Kt=l(),V=o("h2"),re=o("a"),zt=o("span"),g($e.$$.fragment),Bo=l(),Bt=o("span"),Ro=a("RetriBertTokenizerFast"),Xt=l(),z=o("div"),g(ze.$$.fragment),qo=l(),Be=o("p"),Eo=a("Construct a \u201Cfast\u201D RetriBERT tokenizer (backed by HuggingFace\u2019s "),Rt=o("em"),xo=a("tokenizers"),Po=a(" library)."),Co=l(),oe=o("p"),Ke=o("a"),Lo=a("RetriBertTokenizerFast"),Do=a(" is identical to "),Xe=o("a"),Ao=a("BertTokenizerFast"),Io=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Fo=l(),Re=o("p"),So=a("This tokenizer inherits from "),Je=o("a"),Mo=a("PreTrainedTokenizerFast"),jo=a(` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),No=l(),F=o("div"),g(qe.$$.fragment),Oo=l(),qt=o("p"),Uo=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),Wo=l(),Ee=o("ul"),Ge=o("li"),Vo=a("single sequence: "),Et=o("code"),Ho=a("[CLS] X [SEP]"),Ko=l(),Qe=o("li"),Xo=a("pair of sequences: "),xt=o("code"),Jo=a("[CLS] A [SEP] B [SEP]"),Go=l(),P=o("div"),g(xe.$$.fragment),Qo=l(),Pt=o("p"),Yo=a("Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence"),Zo=l(),g(ne.$$.fragment),en=l(),H=o("p"),tn=a("If "),Ct=o("code"),rn=a("token_ids_1"),on=a(" is "),Lt=o("code"),nn=a("None"),sn=a(", this method only returns the first portion of the mask (0s)."),Jt=l(),K=o("h2"),se=o("a"),Dt=o("span"),g(Pe.$$.fragment),an=l(),At=o("span"),cn=a("RetriBertModel"),Gt=l(),q=o("div"),g(Ce.$$.fragment),ln=l(),It=o("p"),dn=a("Bert Based model to embed queries or document for document retrieval."),mn=l(),Le=o("p"),fn=a("This model inherits from "),Ye=o("a"),hn=a("PreTrainedModel"),pn=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),un=l(),De=o("p"),_n=a("This model is also a PyTorch "),Ae=o("a"),gn=a("torch.nn.Module"),kn=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),vn=l(),Ze=o("div"),g(Ie.$$.fragment),this.h()},l(r){const f=Bs('[data-svelte="svelte-1phssyn"]',document.head);u=n(f,"META",{name:!0,content:!0}),f.forEach(t),E=d(r),y=n(r,"H1",{class:!0});var Fe=s(y);p=n(Fe,"A",{id:!0,class:!0,href:!0});var Ft=s(p);R=n(Ft,"SPAN",{});var $n=s(R);k(m.$$.fragment,$n),$n.forEach(t),Ft.forEach(t),$=d(Fe),lt=n(Fe,"SPAN",{});var zn=s(lt);pr=i(zn,"RetriBERT"),zn.forEach(t),Fe.forEach(t),jt=d(r),M=n(r,"H2",{class:!0});var Yt=s(M);X=n(Yt,"A",{id:!0,class:!0,href:!0});var Bn=s(X);dt=n(Bn,"SPAN",{});var Rn=s(dt);k(ce.$$.fragment,Rn),Rn.forEach(t),Bn.forEach(t),ur=d(Yt),mt=n(Yt,"SPAN",{});var qn=s(mt);_r=i(qn,"Overview"),qn.forEach(t),Yt.forEach(t),Nt=d(r),J=n(r,"P",{});var Zt=s(J);gr=i(Zt,"The RetriBERT model was proposed in the blog post "),le=n(Zt,"A",{href:!0,rel:!0});var En=s(le);kr=i(En,`Explain Anything Like I\u2019m Five: A Model for Open Domain Long Form
Question Answering`),En.forEach(t),vr=i(Zt,`. RetriBERT is a small model that uses either a single or
pair of BERT encoders with lower-dimension projection for dense semantic indexing of text.`),Zt.forEach(t),Ot=d(r),A=n(r,"P",{});var et=s(A);br=i(et,"This model was contributed by "),de=n(et,"A",{href:!0,rel:!0});var xn=s(de);Tr=i(xn,"yjernite"),xn.forEach(t),wr=i(et,`. Code to train and use the model can be
found `),me=n(et,"A",{href:!0,rel:!0});var Pn=s(me);yr=i(Pn,"here"),Pn.forEach(t),$r=i(et,"."),et.forEach(t),Ut=d(r),j=n(r,"H2",{class:!0});var er=s(j);G=n(er,"A",{id:!0,class:!0,href:!0});var Cn=s(G);ft=n(Cn,"SPAN",{});var Ln=s(ft);k(fe.$$.fragment,Ln),Ln.forEach(t),Cn.forEach(t),zr=d(er),ht=n(er,"SPAN",{});var Dn=s(ht);Br=i(Dn,"RetriBertConfig"),Dn.forEach(t),er.forEach(t),Wt=d(r),L=n(r,"DIV",{class:!0});var tt=s(L);k(he.$$.fragment,tt),Rr=d(tt),N=n(tt,"P",{});var rt=s(N);qr=i(rt,"This is the configuration class to store the configuration of a "),Me=n(rt,"A",{href:!0});var An=s(Me);Er=i(An,"RetriBertModel"),An.forEach(t),xr=i(rt,`. It is used to instantiate a
RetriBertModel model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the RetriBERT
`),pe=n(rt,"A",{href:!0,rel:!0});var In=s(pe);Pr=i(In,"yjernite/retribert-base-uncased"),In.forEach(t),Cr=i(rt," architecture."),rt.forEach(t),Lr=d(tt),O=n(tt,"P",{});var ot=s(O);Dr=i(ot,"Configuration objects inherit from "),je=n(ot,"A",{href:!0});var Fn=s(je);Ar=i(Fn,"PretrainedConfig"),Fn.forEach(t),Ir=i(ot,` and can be used to control the model outputs. Read the
documentation from `),Ne=n(ot,"A",{href:!0});var Sn=s(Ne);Fr=i(Sn,"PretrainedConfig"),Sn.forEach(t),Sr=i(ot," for more information."),ot.forEach(t),tt.forEach(t),Vt=d(r),U=n(r,"H2",{class:!0});var tr=s(U);Q=n(tr,"A",{id:!0,class:!0,href:!0});var Mn=s(Q);pt=n(Mn,"SPAN",{});var jn=s(pt);k(ue.$$.fragment,jn),jn.forEach(t),Mn.forEach(t),Mr=d(tr),ut=n(tr,"SPAN",{});var Nn=s(ut);jr=i(Nn,"RetriBertTokenizer"),Nn.forEach(t),tr.forEach(t),Ht=d(r),_=n(r,"DIV",{class:!0});var B=s(_);k(_e.$$.fragment,B),Nr=d(B),_t=n(B,"P",{});var On=s(_t);Or=i(On,"Constructs a RetriBERT tokenizer."),On.forEach(t),Ur=d(B),Y=n(B,"P",{});var St=s(Y);Oe=n(St,"A",{href:!0});var Un=s(Oe);Wr=i(Un,"RetriBertTokenizer"),Un.forEach(t),Vr=i(St," is identical to "),Ue=n(St,"A",{href:!0});var Wn=s(Ue);Hr=i(Wn,"BertTokenizer"),Wn.forEach(t),Kr=i(St,` and runs end-to-end tokenization: punctuation splitting
and wordpiece.`),St.forEach(t),Xr=d(B),ge=n(B,"P",{});var rr=s(ge);Jr=i(rr,"This tokenizer inherits from "),We=n(rr,"A",{href:!0});var Vn=s(We);Gr=i(Vn,"PreTrainedTokenizer"),Vn.forEach(t),Qr=i(rr,` which contains most of the main methods. Users should refer
to: this superclass for more information regarding those methods.`),rr.forEach(t),Yr=d(B),I=n(B,"DIV",{class:!0});var nt=s(I);k(ke.$$.fragment,nt),Zr=d(nt),gt=n(nt,"P",{});var Hn=s(gt);eo=i(Hn,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),Hn.forEach(t),to=d(nt),ve=n(nt,"UL",{});var or=s(ve);Ve=n(or,"LI",{});var bn=s(Ve);ro=i(bn,"single sequence: "),kt=n(bn,"CODE",{});var Kn=s(kt);oo=i(Kn,"[CLS] X [SEP]"),Kn.forEach(t),bn.forEach(t),no=d(or),He=n(or,"LI",{});var Tn=s(He);so=i(Tn,"pair of sequences: "),vt=n(Tn,"CODE",{});var Xn=s(vt);ao=i(Xn,"[CLS] A [SEP] B [SEP]"),Xn.forEach(t),Tn.forEach(t),or.forEach(t),nt.forEach(t),io=d(B),Z=n(B,"DIV",{class:!0});var nr=s(Z);k(be.$$.fragment,nr),co=d(nr),bt=n(nr,"P",{});var Jn=s(bt);lo=i(Jn,"Converts a sequence of tokens (string) in a single string."),Jn.forEach(t),nr.forEach(t),mo=d(B),x=n(B,"DIV",{class:!0});var ae=s(x);k(Te.$$.fragment,ae),fo=d(ae),Tt=n(ae,"P",{});var Gn=s(Tt);ho=i(Gn,"Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence"),Gn.forEach(t),po=d(ae),k(ee.$$.fragment,ae),uo=d(ae),W=n(ae,"P",{});var st=s(W);_o=i(st,"If "),wt=n(st,"CODE",{});var Qn=s(wt);go=i(Qn,"token_ids_1"),Qn.forEach(t),ko=i(st," is "),yt=n(st,"CODE",{});var Yn=s(yt);vo=i(Yn,"None"),Yn.forEach(t),bo=i(st,", this method only returns the first portion of the mask (0s)."),st.forEach(t),ae.forEach(t),To=d(B),te=n(B,"DIV",{class:!0});var sr=s(te);k(we.$$.fragment,sr),wo=d(sr),ye=n(sr,"P",{});var ar=s(ye);yo=i(ar,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),$t=n(ar,"CODE",{});var Zn=s($t);$o=i(Zn,"prepare_for_model"),Zn.forEach(t),zo=i(ar," method."),ar.forEach(t),sr.forEach(t),B.forEach(t),Kt=d(r),V=n(r,"H2",{class:!0});var ir=s(V);re=n(ir,"A",{id:!0,class:!0,href:!0});var es=s(re);zt=n(es,"SPAN",{});var ts=s(zt);k($e.$$.fragment,ts),ts.forEach(t),es.forEach(t),Bo=d(ir),Bt=n(ir,"SPAN",{});var rs=s(Bt);Ro=i(rs,"RetriBertTokenizerFast"),rs.forEach(t),ir.forEach(t),Xt=d(r),z=n(r,"DIV",{class:!0});var C=s(z);k(ze.$$.fragment,C),qo=d(C),Be=n(C,"P",{});var cr=s(Be);Eo=i(cr,"Construct a \u201Cfast\u201D RetriBERT tokenizer (backed by HuggingFace\u2019s "),Rt=n(cr,"EM",{});var os=s(Rt);xo=i(os,"tokenizers"),os.forEach(t),Po=i(cr," library)."),cr.forEach(t),Co=d(C),oe=n(C,"P",{});var Mt=s(oe);Ke=n(Mt,"A",{href:!0});var ns=s(Ke);Lo=i(ns,"RetriBertTokenizerFast"),ns.forEach(t),Do=i(Mt," is identical to "),Xe=n(Mt,"A",{href:!0});var ss=s(Xe);Ao=i(ss,"BertTokenizerFast"),ss.forEach(t),Io=i(Mt,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Mt.forEach(t),Fo=d(C),Re=n(C,"P",{});var lr=s(Re);So=i(lr,"This tokenizer inherits from "),Je=n(lr,"A",{href:!0});var as=s(Je);Mo=i(as,"PreTrainedTokenizerFast"),as.forEach(t),jo=i(lr,` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),lr.forEach(t),No=d(C),F=n(C,"DIV",{class:!0});var at=s(F);k(qe.$$.fragment,at),Oo=d(at),qt=n(at,"P",{});var is=s(qt);Uo=i(is,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),is.forEach(t),Wo=d(at),Ee=n(at,"UL",{});var dr=s(Ee);Ge=n(dr,"LI",{});var wn=s(Ge);Vo=i(wn,"single sequence: "),Et=n(wn,"CODE",{});var cs=s(Et);Ho=i(cs,"[CLS] X [SEP]"),cs.forEach(t),wn.forEach(t),Ko=d(dr),Qe=n(dr,"LI",{});var yn=s(Qe);Xo=i(yn,"pair of sequences: "),xt=n(yn,"CODE",{});var ls=s(xt);Jo=i(ls,"[CLS] A [SEP] B [SEP]"),ls.forEach(t),yn.forEach(t),dr.forEach(t),at.forEach(t),Go=d(C),P=n(C,"DIV",{class:!0});var ie=s(P);k(xe.$$.fragment,ie),Qo=d(ie),Pt=n(ie,"P",{});var ds=s(Pt);Yo=i(ds,"Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence"),ds.forEach(t),Zo=d(ie),k(ne.$$.fragment,ie),en=d(ie),H=n(ie,"P",{});var it=s(H);tn=i(it,"If "),Ct=n(it,"CODE",{});var ms=s(Ct);rn=i(ms,"token_ids_1"),ms.forEach(t),on=i(it," is "),Lt=n(it,"CODE",{});var fs=s(Lt);nn=i(fs,"None"),fs.forEach(t),sn=i(it,", this method only returns the first portion of the mask (0s)."),it.forEach(t),ie.forEach(t),C.forEach(t),Jt=d(r),K=n(r,"H2",{class:!0});var mr=s(K);se=n(mr,"A",{id:!0,class:!0,href:!0});var hs=s(se);Dt=n(hs,"SPAN",{});var ps=s(Dt);k(Pe.$$.fragment,ps),ps.forEach(t),hs.forEach(t),an=d(mr),At=n(mr,"SPAN",{});var us=s(At);cn=i(us,"RetriBertModel"),us.forEach(t),mr.forEach(t),Gt=d(r),q=n(r,"DIV",{class:!0});var S=s(q);k(Ce.$$.fragment,S),ln=d(S),It=n(S,"P",{});var _s=s(It);dn=i(_s,"Bert Based model to embed queries or document for document retrieval."),_s.forEach(t),mn=d(S),Le=n(S,"P",{});var fr=s(Le);fn=i(fr,"This model inherits from "),Ye=n(fr,"A",{href:!0});var gs=s(Ye);hn=i(gs,"PreTrainedModel"),gs.forEach(t),pn=i(fr,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),fr.forEach(t),un=d(S),De=n(S,"P",{});var hr=s(De);_n=i(hr,"This model is also a PyTorch "),Ae=n(hr,"A",{href:!0,rel:!0});var ks=s(Ae);gn=i(ks,"torch.nn.Module"),ks.forEach(t),kn=i(hr,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),hr.forEach(t),vn=d(S),Ze=n(S,"DIV",{class:!0});var vs=s(Ze);k(Ie.$$.fragment,vs),vs.forEach(t),S.forEach(t),this.h()},h(){c(u,"name","hf:doc:metadata"),c(u,"content",JSON.stringify(Ps)),c(p,"id","retribert"),c(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(p,"href","#retribert"),c(y,"class","relative group"),c(X,"id","overview"),c(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(X,"href","#overview"),c(M,"class","relative group"),c(le,"href","https://yjernite.github.io/lfqa.html"),c(le,"rel","nofollow"),c(de,"href","https://huggingface.co/yjernite"),c(de,"rel","nofollow"),c(me,"href","https://github.com/huggingface/transformers/tree/main/examples/research-projects/distillation"),c(me,"rel","nofollow"),c(G,"id","transformers.RetriBertConfig"),c(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(G,"href","#transformers.RetriBertConfig"),c(j,"class","relative group"),c(Me,"href","/docs/transformers/v4.24.0/en/model_doc/retribert#transformers.RetriBertModel"),c(pe,"href","https://huggingface.co/yjernite/retribert-base-uncased"),c(pe,"rel","nofollow"),c(je,"href","/docs/transformers/v4.24.0/en/main_classes/configuration#transformers.PretrainedConfig"),c(Ne,"href","/docs/transformers/v4.24.0/en/main_classes/configuration#transformers.PretrainedConfig"),c(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Q,"id","transformers.RetriBertTokenizer"),c(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Q,"href","#transformers.RetriBertTokenizer"),c(U,"class","relative group"),c(Oe,"href","/docs/transformers/v4.24.0/en/model_doc/retribert#transformers.RetriBertTokenizer"),c(Ue,"href","/docs/transformers/v4.24.0/en/model_doc/bert#transformers.BertTokenizer"),c(We,"href","/docs/transformers/v4.24.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(_,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(re,"id","transformers.RetriBertTokenizerFast"),c(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(re,"href","#transformers.RetriBertTokenizerFast"),c(V,"class","relative group"),c(Ke,"href","/docs/transformers/v4.24.0/en/model_doc/retribert#transformers.RetriBertTokenizerFast"),c(Xe,"href","/docs/transformers/v4.24.0/en/model_doc/bert#transformers.BertTokenizerFast"),c(Je,"href","/docs/transformers/v4.24.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),c(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(se,"id","transformers.RetriBertModel"),c(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(se,"href","#transformers.RetriBertModel"),c(K,"class","relative group"),c(Ye,"href","/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel"),c(Ae,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ae,"rel","nofollow"),c(Ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(r,f){e(document.head,u),h(r,E,f),h(r,y,f),e(y,p),e(p,R),v(m,R,null),e(y,$),e(y,lt),e(lt,pr),h(r,jt,f),h(r,M,f),e(M,X),e(X,dt),v(ce,dt,null),e(M,ur),e(M,mt),e(mt,_r),h(r,Nt,f),h(r,J,f),e(J,gr),e(J,le),e(le,kr),e(J,vr),h(r,Ot,f),h(r,A,f),e(A,br),e(A,de),e(de,Tr),e(A,wr),e(A,me),e(me,yr),e(A,$r),h(r,Ut,f),h(r,j,f),e(j,G),e(G,ft),v(fe,ft,null),e(j,zr),e(j,ht),e(ht,Br),h(r,Wt,f),h(r,L,f),v(he,L,null),e(L,Rr),e(L,N),e(N,qr),e(N,Me),e(Me,Er),e(N,xr),e(N,pe),e(pe,Pr),e(N,Cr),e(L,Lr),e(L,O),e(O,Dr),e(O,je),e(je,Ar),e(O,Ir),e(O,Ne),e(Ne,Fr),e(O,Sr),h(r,Vt,f),h(r,U,f),e(U,Q),e(Q,pt),v(ue,pt,null),e(U,Mr),e(U,ut),e(ut,jr),h(r,Ht,f),h(r,_,f),v(_e,_,null),e(_,Nr),e(_,_t),e(_t,Or),e(_,Ur),e(_,Y),e(Y,Oe),e(Oe,Wr),e(Y,Vr),e(Y,Ue),e(Ue,Hr),e(Y,Kr),e(_,Xr),e(_,ge),e(ge,Jr),e(ge,We),e(We,Gr),e(ge,Qr),e(_,Yr),e(_,I),v(ke,I,null),e(I,Zr),e(I,gt),e(gt,eo),e(I,to),e(I,ve),e(ve,Ve),e(Ve,ro),e(Ve,kt),e(kt,oo),e(ve,no),e(ve,He),e(He,so),e(He,vt),e(vt,ao),e(_,io),e(_,Z),v(be,Z,null),e(Z,co),e(Z,bt),e(bt,lo),e(_,mo),e(_,x),v(Te,x,null),e(x,fo),e(x,Tt),e(Tt,ho),e(x,po),v(ee,x,null),e(x,uo),e(x,W),e(W,_o),e(W,wt),e(wt,go),e(W,ko),e(W,yt),e(yt,vo),e(W,bo),e(_,To),e(_,te),v(we,te,null),e(te,wo),e(te,ye),e(ye,yo),e(ye,$t),e($t,$o),e(ye,zo),h(r,Kt,f),h(r,V,f),e(V,re),e(re,zt),v($e,zt,null),e(V,Bo),e(V,Bt),e(Bt,Ro),h(r,Xt,f),h(r,z,f),v(ze,z,null),e(z,qo),e(z,Be),e(Be,Eo),e(Be,Rt),e(Rt,xo),e(Be,Po),e(z,Co),e(z,oe),e(oe,Ke),e(Ke,Lo),e(oe,Do),e(oe,Xe),e(Xe,Ao),e(oe,Io),e(z,Fo),e(z,Re),e(Re,So),e(Re,Je),e(Je,Mo),e(Re,jo),e(z,No),e(z,F),v(qe,F,null),e(F,Oo),e(F,qt),e(qt,Uo),e(F,Wo),e(F,Ee),e(Ee,Ge),e(Ge,Vo),e(Ge,Et),e(Et,Ho),e(Ee,Ko),e(Ee,Qe),e(Qe,Xo),e(Qe,xt),e(xt,Jo),e(z,Go),e(z,P),v(xe,P,null),e(P,Qo),e(P,Pt),e(Pt,Yo),e(P,Zo),v(ne,P,null),e(P,en),e(P,H),e(H,tn),e(H,Ct),e(Ct,rn),e(H,on),e(H,Lt),e(Lt,nn),e(H,sn),h(r,Jt,f),h(r,K,f),e(K,se),e(se,Dt),v(Pe,Dt,null),e(K,an),e(K,At),e(At,cn),h(r,Gt,f),h(r,q,f),v(Ce,q,null),e(q,ln),e(q,It),e(It,dn),e(q,mn),e(q,Le),e(Le,fn),e(Le,Ye),e(Ye,hn),e(Le,pn),e(q,un),e(q,De),e(De,_n),e(De,Ae),e(Ae,gn),e(De,kn),e(q,vn),e(q,Ze),v(Ie,Ze,null),Qt=!0},p(r,[f]){const Fe={};f&2&&(Fe.$$scope={dirty:f,ctx:r}),ee.$set(Fe);const Ft={};f&2&&(Ft.$$scope={dirty:f,ctx:r}),ne.$set(Ft)},i(r){Qt||(b(m.$$.fragment,r),b(ce.$$.fragment,r),b(fe.$$.fragment,r),b(he.$$.fragment,r),b(ue.$$.fragment,r),b(_e.$$.fragment,r),b(ke.$$.fragment,r),b(be.$$.fragment,r),b(Te.$$.fragment,r),b(ee.$$.fragment,r),b(we.$$.fragment,r),b($e.$$.fragment,r),b(ze.$$.fragment,r),b(qe.$$.fragment,r),b(xe.$$.fragment,r),b(ne.$$.fragment,r),b(Pe.$$.fragment,r),b(Ce.$$.fragment,r),b(Ie.$$.fragment,r),Qt=!0)},o(r){T(m.$$.fragment,r),T(ce.$$.fragment,r),T(fe.$$.fragment,r),T(he.$$.fragment,r),T(ue.$$.fragment,r),T(_e.$$.fragment,r),T(ke.$$.fragment,r),T(be.$$.fragment,r),T(Te.$$.fragment,r),T(ee.$$.fragment,r),T(we.$$.fragment,r),T($e.$$.fragment,r),T(ze.$$.fragment,r),T(qe.$$.fragment,r),T(xe.$$.fragment,r),T(ne.$$.fragment,r),T(Pe.$$.fragment,r),T(Ce.$$.fragment,r),T(Ie.$$.fragment,r),Qt=!1},d(r){t(u),r&&t(E),r&&t(y),w(m),r&&t(jt),r&&t(M),w(ce),r&&t(Nt),r&&t(J),r&&t(Ot),r&&t(A),r&&t(Ut),r&&t(j),w(fe),r&&t(Wt),r&&t(L),w(he),r&&t(Vt),r&&t(U),w(ue),r&&t(Ht),r&&t(_),w(_e),w(ke),w(be),w(Te),w(ee),w(we),r&&t(Kt),r&&t(V),w($e),r&&t(Xt),r&&t(z),w(ze),w(qe),w(xe),w(ne),r&&t(Jt),r&&t(K),w(Pe),r&&t(Gt),r&&t(q),w(Ce),w(Ie)}}}const Ps={local:"retribert",sections:[{local:"overview",title:"Overview"},{local:"transformers.RetriBertConfig",title:"RetriBertConfig"},{local:"transformers.RetriBertTokenizer",title:"RetriBertTokenizer"},{local:"transformers.RetriBertTokenizerFast",title:"RetriBertTokenizerFast"},{local:"transformers.RetriBertModel",title:"RetriBertModel"}],title:"RetriBERT"};function Cs(Se){return Rs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ss extends ys{constructor(u){super();$s(this,u,Cs,xs,zs,{})}}export{Ss as default,Ps as metadata};
