import{S as $M,i as yM,s as DM,e as r,k as p,w,t as o,M as FM,c as a,d as t,m as h,a as i,x as T,h as s,b as c,G as e,g as _,y as $,q as y,o as D,B as F,v as EM,L as ue}from"../../chunks/vendor-hf-doc-builder.js";import{T as Se}from"../../chunks/Tip-hf-doc-builder.js";import{D as Y}from"../../chunks/Docstring-hf-doc-builder.js";import{C as me}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Ne}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as fe}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";import{P as Sg}from"../../chunks/PipelineTag-hf-doc-builder.js";function BM(B){let d,b,f,m,v;return m=new me({props:{code:`from transformers import DistilBertConfig, DistilBertModel

# Initializing a DistilBERT configuration
configuration = DistilBertConfig()

# Initializing a model (with random weights) from the configuration
model = DistilBertModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertConfig, DistilBertModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a DistilBERT configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = DistilBertConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){d=r("p"),b=o("Examples:"),f=p(),w(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Examples:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),v=!0},p:ue,i(l){v||(y(m.$$.fragment,l),v=!0)},o(l){D(m.$$.fragment,l),v=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function xM(B){let d,b,f,m,v;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),v=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),v=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,v)},d(l){l&&t(d)}}}function MM(B){let d,b,f,m,v;return m=new me({props:{code:`from transformers import DistilBertTokenizer, DistilBertModel
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertModel.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertModel.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){d=r("p"),b=o("Example:"),f=p(),w(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),v=!0},p:ue,i(l){v||(y(m.$$.fragment,l),v=!0)},o(l){D(m.$$.fragment,l),v=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function zM(B){let d,b,f,m,v;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),v=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),v=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,v)},d(l){l&&t(d)}}}function jM(B){let d,b,f,m,v;return m=new me({props:{code:`from transformers import DistilBertTokenizer, DistilBertForMaskedLM
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForMaskedLM.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

# retrieve index of [MASK]
mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]

predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)
tokenizer.decode(predicted_token_id)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve index of [MASK]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[<span class="hljs-number">0</span>].nonzero(as_tuple=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_id = logits[<span class="hljs-number">0</span>, mask_token_index].argmax(axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predicted_token_id)
`}}),{c(){d=r("p"),b=o("Example:"),f=p(),w(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),v=!0},p:ue,i(l){v||(y(m.$$.fragment,l),v=!0)},o(l){D(m.$$.fragment,l),v=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function CM(B){let d,b;return d=new me({props:{code:`labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]
# mask labels of non-[MASK] tokens
labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

outputs = model(**inputs, labels=labels)
round(outputs.loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># mask labels of non-[MASK] tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -<span class="hljs-number">100</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(outputs.loss.item(), <span class="hljs-number">2</span>)
`}}),{c(){w(d.$$.fragment)},l(f){T(d.$$.fragment,f)},m(f,m){$(d,f,m),b=!0},p:ue,i(f){b||(y(d.$$.fragment,f),b=!0)},o(f){D(d.$$.fragment,f),b=!1},d(f){F(d,f)}}}function PM(B){let d,b,f,m,v;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),v=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),v=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,v)},d(l){l&&t(d)}}}function qM(B){let d,b,f,m,v;return m=new me({props:{code:`import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
`}}),{c(){d=r("p"),b=o("Example of single-label classification:"),f=p(),w(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example of single-label classification:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),v=!0},p:ue,i(l){v||(y(m.$$.fragment,l),v=!0)},o(l){D(m.$$.fragment,l),v=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function AM(B){let d,b;return d=new me({props:{code:`# To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`
num_labels = len(model.config.id2label)
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=num_labels)

labels = torch.tensor([1])
loss = model(**inputs, labels=labels).loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=num_labels)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),{c(){w(d.$$.fragment)},l(f){T(d.$$.fragment,f)},m(f,m){$(d,f,m),b=!0},p:ue,i(f){b||(y(d.$$.fragment,f),b=!0)},o(f){D(d.$$.fragment,f),b=!1},d(f){F(d,f)}}}function OM(B){let d,b,f,m,v;return m=new me({props:{code:`import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", problem_type="multi_label_classification")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
`}}),{c(){d=r("p"),b=o("Example of multi-label classification:"),f=p(),w(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example of multi-label classification:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),v=!0},p:ue,i(l){v||(y(m.$$.fragment,l),v=!0)},o(l){D(m.$$.fragment,l),v=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function LM(B){let d,b;return d=new me({props:{code:`# To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`
num_labels = len(model.config.id2label)
model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=num_labels, problem_type="multi_label_classification"
)

labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(
    torch.float
)
loss = model(**inputs, labels=labels).loss
loss.backward()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=num_labels, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(
<span class="hljs-meta">... </span>    torch.<span class="hljs-built_in">float</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span>loss.backward()`}}),{c(){w(d.$$.fragment)},l(f){T(d.$$.fragment,f)},m(f,m){$(d,f,m),b=!0},p:ue,i(f){b||(y(d.$$.fragment,f),b=!0)},o(f){D(d.$$.fragment,f),b=!1},d(f){F(d,f)}}}function IM(B){let d,b,f,m,v;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),v=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),v=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,v)},d(l){l&&t(d)}}}function SM(B){let d,b,f,m,v;return m=new me({props:{code:`from transformers import DistilBertTokenizer, DistilBertForMultipleChoice
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-cased")
model = DistilBertForMultipleChoice.from_pretrained("distilbert-base-cased")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."
labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

encoding = tokenizer([[prompt, choice0], [prompt, choice1]], return_tensors="pt", padding=True)
outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1

# the linear classifier still needs to be trained
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;distilbert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># choice0 is correct (according to Wikipedia ;)), batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([[prompt, choice0], [prompt, choice1]], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v.unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}, labels=labels)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){d=r("p"),b=o("Examples:"),f=p(),w(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Examples:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),v=!0},p:ue,i(l){v||(y(m.$$.fragment,l),v=!0)},o(l){D(m.$$.fragment,l),v=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function NM(B){let d,b,f,m,v;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),v=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),v=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,v)},d(l){l&&t(d)}}}function RM(B){let d,b,f,m,v;return m=new me({props:{code:`from transformers import DistilBertTokenizer, DistilBertForTokenClassification
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForTokenClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer(
    "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="pt"
)

with torch.no_grad():
    logits = model(**inputs).logits

predicted_token_class_ids = logits.argmax(-1)

# Note that tokens are classified rather then input words which means that
# there might be more predicted token classes than words.
# Multiple token classes might account for the same word
predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]
predicted_tokens_classes
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;HuggingFace is a company based in Paris and New York&quot;</span>, add_special_tokens=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_class_ids = logits.argmax(-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Note that tokens are classified rather then input words which means that</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># there might be more predicted token classes than words.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multiple token classes might account for the same word</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes = [model.config.id2label[t.item()] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> predicted_token_class_ids[<span class="hljs-number">0</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes
`}}),{c(){d=r("p"),b=o("Example:"),f=p(),w(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),v=!0},p:ue,i(l){v||(y(m.$$.fragment,l),v=!0)},o(l){D(m.$$.fragment,l),v=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function WM(B){let d,b;return d=new me({props:{code:`labels = predicted_token_class_ids
loss = model(**inputs, labels=labels).loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = predicted_token_class_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),{c(){w(d.$$.fragment)},l(f){T(d.$$.fragment,f)},m(f,m){$(d,f,m),b=!0},p:ue,i(f){b||(y(d.$$.fragment,f),b=!0)},o(f){D(d.$$.fragment,f),b=!1},d(f){F(d,f)}}}function QM(B){let d,b,f,m,v;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),v=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),v=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,v)},d(l){l&&t(d)}}}function UM(B){let d,b,f,m,v;return m=new me({props:{code:`from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

inputs = tokenizer(question, text, return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs)

answer_start_index = outputs.start_logits.argmax()
answer_end_index = outputs.end_logits.argmax()

predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
tokenizer.decode(predict_answer_tokens)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>answer_start_index = outputs.start_logits.argmax()
<span class="hljs-meta">&gt;&gt;&gt; </span>answer_end_index = outputs.end_logits.argmax()

<span class="hljs-meta">&gt;&gt;&gt; </span>predict_answer_tokens = inputs.input_ids[<span class="hljs-number">0</span>, answer_start_index : answer_end_index + <span class="hljs-number">1</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predict_answer_tokens)
`}}),{c(){d=r("p"),b=o("Example:"),f=p(),w(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),v=!0},p:ue,i(l){v||(y(m.$$.fragment,l),v=!0)},o(l){D(m.$$.fragment,l),v=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function HM(B){let d,b;return d=new me({props:{code:`# target is "nice puppet"
target_start_index = torch.tensor([14])
target_end_index = torch.tensor([15])

outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
loss = outputs.loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target is &quot;nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_start_index = torch.tensor([<span class="hljs-number">14</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>target_end_index = torch.tensor([<span class="hljs-number">15</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),{c(){w(d.$$.fragment)},l(f){T(d.$$.fragment,f)},m(f,m){$(d,f,m),b=!0},p:ue,i(f){b||(y(d.$$.fragment,f),b=!0)},o(f){D(d.$$.fragment,f),b=!1},d(f){F(d,f)}}}function KM(B){let d,b,f,m,v,l,u,M,we,ge,I,re,oe,E,Te,Q,$e,_e,O,ye,ae,H,De,ie,K,Fe,de,V,Ee,be,ee,j,q,le,U,Be,ve,W,xe,ke,C,se,J,ce,Me,G,pe,ze,S,he,X,je,ne,P,Ce,A,Pe,qe;return{c(){d=r("p"),b=o("TensorFlow models and layers in "),f=r("code"),m=o("transformers"),v=o(" accept two formats as input:"),l=p(),u=r("ul"),M=r("li"),we=o("having all inputs as keyword arguments (like PyTorch models), or"),ge=p(),I=r("li"),re=o("having all inputs as a list, tuple or dict in the first positional argument."),oe=p(),E=r("p"),Te=o(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=r("code"),$e=o("model.fit()"),_e=o(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=r("code"),ye=o("model.fit()"),ae=o(` supports! If, however, you want to use the second
format outside of Keras methods like `),H=r("code"),De=o("fit()"),ie=o(" and "),K=r("code"),Fe=o("predict()"),de=o(`, such as when creating your own layers or models with
the Keras `),V=r("code"),Ee=o("Functional"),be=o(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),ee=p(),j=r("ul"),q=r("li"),le=o("a single Tensor with "),U=r("code"),Be=o("input_ids"),ve=o(" only and nothing else: "),W=r("code"),xe=o("model(input_ids)"),ke=p(),C=r("li"),se=o(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=r("code"),ce=o("model([input_ids, attention_mask])"),Me=o(" or "),G=r("code"),pe=o("model([input_ids, attention_mask, token_type_ids])"),ze=p(),S=r("li"),he=o(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=r("code"),je=o('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ne=p(),P=r("p"),Ce=o(`Note that when creating models and layers with
`),A=r("a"),Pe=o("subclassing"),qe=o(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(k){d=a(k,"P",{});var x=i(d);b=s(x,"TensorFlow models and layers in "),f=a(x,"CODE",{});var He=i(f);m=s(He,"transformers"),He.forEach(t),v=s(x," accept two formats as input:"),x.forEach(t),l=h(k),u=a(k,"UL",{});var Z=i(u);M=a(Z,"LI",{});var Ke=i(M);we=s(Ke,"having all inputs as keyword arguments (like PyTorch models), or"),Ke.forEach(t),ge=h(Z),I=a(Z,"LI",{});var Ve=i(I);re=s(Ve,"having all inputs as a list, tuple or dict in the first positional argument."),Ve.forEach(t),Z.forEach(t),oe=h(k),E=a(k,"P",{});var z=i(E);Te=s(z,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=a(z,"CODE",{});var Je=i(Q);$e=s(Je,"model.fit()"),Je.forEach(t),_e=s(z,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=a(z,"CODE",{});var Ge=i(O);ye=s(Ge,"model.fit()"),Ge.forEach(t),ae=s(z,` supports! If, however, you want to use the second
format outside of Keras methods like `),H=a(z,"CODE",{});var Le=i(H);De=s(Le,"fit()"),Le.forEach(t),ie=s(z," and "),K=a(z,"CODE",{});var Xe=i(K);Fe=s(Xe,"predict()"),Xe.forEach(t),de=s(z,`, such as when creating your own layers or models with
the Keras `),V=a(z,"CODE",{});var Ye=i(V);Ee=s(Ye,"Functional"),Ye.forEach(t),be=s(z,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),z.forEach(t),ee=h(k),j=a(k,"UL",{});var L=i(j);q=a(L,"LI",{});var N=i(q);le=s(N,"a single Tensor with "),U=a(N,"CODE",{});var Oe=i(U);Be=s(Oe,"input_ids"),Oe.forEach(t),ve=s(N," only and nothing else: "),W=a(N,"CODE",{});var Re=i(W);xe=s(Re,"model(input_ids)"),Re.forEach(t),N.forEach(t),ke=h(L),C=a(L,"LI",{});var R=i(C);se=s(R,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=a(R,"CODE",{});var Ze=i(J);ce=s(Ze,"model([input_ids, attention_mask])"),Ze.forEach(t),Me=s(R," or "),G=a(R,"CODE",{});var We=i(G);pe=s(We,"model([input_ids, attention_mask, token_type_ids])"),We.forEach(t),R.forEach(t),ze=h(L),S=a(L,"LI",{});var Ae=i(S);he=s(Ae,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=a(Ae,"CODE",{});var Ue=i(X);je=s(Ue,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ue.forEach(t),Ae.forEach(t),L.forEach(t),ne=h(k),P=a(k,"P",{});var te=i(P);Ce=s(te,`Note that when creating models and layers with
`),A=a(te,"A",{href:!0,rel:!0});var et=i(A);Pe=s(et,"subclassing"),et.forEach(t),qe=s(te,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),te.forEach(t),this.h()},h(){c(A,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),c(A,"rel","nofollow")},m(k,x){_(k,d,x),e(d,b),e(d,f),e(f,m),e(d,v),_(k,l,x),_(k,u,x),e(u,M),e(M,we),e(u,ge),e(u,I),e(I,re),_(k,oe,x),_(k,E,x),e(E,Te),e(E,Q),e(Q,$e),e(E,_e),e(E,O),e(O,ye),e(E,ae),e(E,H),e(H,De),e(E,ie),e(E,K),e(K,Fe),e(E,de),e(E,V),e(V,Ee),e(E,be),_(k,ee,x),_(k,j,x),e(j,q),e(q,le),e(q,U),e(U,Be),e(q,ve),e(q,W),e(W,xe),e(j,ke),e(j,C),e(C,se),e(C,J),e(J,ce),e(C,Me),e(C,G),e(G,pe),e(j,ze),e(j,S),e(S,he),e(S,X),e(X,je),_(k,ne,x),_(k,P,x),e(P,Ce),e(P,A),e(A,Pe),e(P,qe)},d(k){k&&t(d),k&&t(l),k&&t(u),k&&t(oe),k&&t(E),k&&t(ee),k&&t(j),k&&t(ne),k&&t(P)}}}function VM(B){let d,b,f,m,v;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),v=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),v=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,v)},d(l){l&&t(d)}}}function JM(B){let d,b,f,m,v;return m=new me({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertModel
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertModel.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertModel.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){d=r("p"),b=o("Example:"),f=p(),w(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),v=!0},p:ue,i(l){v||(y(m.$$.fragment,l),v=!0)},o(l){D(m.$$.fragment,l),v=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function GM(B){let d,b,f,m,v,l,u,M,we,ge,I,re,oe,E,Te,Q,$e,_e,O,ye,ae,H,De,ie,K,Fe,de,V,Ee,be,ee,j,q,le,U,Be,ve,W,xe,ke,C,se,J,ce,Me,G,pe,ze,S,he,X,je,ne,P,Ce,A,Pe,qe;return{c(){d=r("p"),b=o("TensorFlow models and layers in "),f=r("code"),m=o("transformers"),v=o(" accept two formats as input:"),l=p(),u=r("ul"),M=r("li"),we=o("having all inputs as keyword arguments (like PyTorch models), or"),ge=p(),I=r("li"),re=o("having all inputs as a list, tuple or dict in the first positional argument."),oe=p(),E=r("p"),Te=o(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=r("code"),$e=o("model.fit()"),_e=o(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=r("code"),ye=o("model.fit()"),ae=o(` supports! If, however, you want to use the second
format outside of Keras methods like `),H=r("code"),De=o("fit()"),ie=o(" and "),K=r("code"),Fe=o("predict()"),de=o(`, such as when creating your own layers or models with
the Keras `),V=r("code"),Ee=o("Functional"),be=o(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),ee=p(),j=r("ul"),q=r("li"),le=o("a single Tensor with "),U=r("code"),Be=o("input_ids"),ve=o(" only and nothing else: "),W=r("code"),xe=o("model(input_ids)"),ke=p(),C=r("li"),se=o(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=r("code"),ce=o("model([input_ids, attention_mask])"),Me=o(" or "),G=r("code"),pe=o("model([input_ids, attention_mask, token_type_ids])"),ze=p(),S=r("li"),he=o(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=r("code"),je=o('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ne=p(),P=r("p"),Ce=o(`Note that when creating models and layers with
`),A=r("a"),Pe=o("subclassing"),qe=o(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(k){d=a(k,"P",{});var x=i(d);b=s(x,"TensorFlow models and layers in "),f=a(x,"CODE",{});var He=i(f);m=s(He,"transformers"),He.forEach(t),v=s(x," accept two formats as input:"),x.forEach(t),l=h(k),u=a(k,"UL",{});var Z=i(u);M=a(Z,"LI",{});var Ke=i(M);we=s(Ke,"having all inputs as keyword arguments (like PyTorch models), or"),Ke.forEach(t),ge=h(Z),I=a(Z,"LI",{});var Ve=i(I);re=s(Ve,"having all inputs as a list, tuple or dict in the first positional argument."),Ve.forEach(t),Z.forEach(t),oe=h(k),E=a(k,"P",{});var z=i(E);Te=s(z,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=a(z,"CODE",{});var Je=i(Q);$e=s(Je,"model.fit()"),Je.forEach(t),_e=s(z,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=a(z,"CODE",{});var Ge=i(O);ye=s(Ge,"model.fit()"),Ge.forEach(t),ae=s(z,` supports! If, however, you want to use the second
format outside of Keras methods like `),H=a(z,"CODE",{});var Le=i(H);De=s(Le,"fit()"),Le.forEach(t),ie=s(z," and "),K=a(z,"CODE",{});var Xe=i(K);Fe=s(Xe,"predict()"),Xe.forEach(t),de=s(z,`, such as when creating your own layers or models with
the Keras `),V=a(z,"CODE",{});var Ye=i(V);Ee=s(Ye,"Functional"),Ye.forEach(t),be=s(z,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),z.forEach(t),ee=h(k),j=a(k,"UL",{});var L=i(j);q=a(L,"LI",{});var N=i(q);le=s(N,"a single Tensor with "),U=a(N,"CODE",{});var Oe=i(U);Be=s(Oe,"input_ids"),Oe.forEach(t),ve=s(N," only and nothing else: "),W=a(N,"CODE",{});var Re=i(W);xe=s(Re,"model(input_ids)"),Re.forEach(t),N.forEach(t),ke=h(L),C=a(L,"LI",{});var R=i(C);se=s(R,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=a(R,"CODE",{});var Ze=i(J);ce=s(Ze,"model([input_ids, attention_mask])"),Ze.forEach(t),Me=s(R," or "),G=a(R,"CODE",{});var We=i(G);pe=s(We,"model([input_ids, attention_mask, token_type_ids])"),We.forEach(t),R.forEach(t),ze=h(L),S=a(L,"LI",{});var Ae=i(S);he=s(Ae,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=a(Ae,"CODE",{});var Ue=i(X);je=s(Ue,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ue.forEach(t),Ae.forEach(t),L.forEach(t),ne=h(k),P=a(k,"P",{});var te=i(P);Ce=s(te,`Note that when creating models and layers with
`),A=a(te,"A",{href:!0,rel:!0});var et=i(A);Pe=s(et,"subclassing"),et.forEach(t),qe=s(te,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),te.forEach(t),this.h()},h(){c(A,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),c(A,"rel","nofollow")},m(k,x){_(k,d,x),e(d,b),e(d,f),e(f,m),e(d,v),_(k,l,x),_(k,u,x),e(u,M),e(M,we),e(u,ge),e(u,I),e(I,re),_(k,oe,x),_(k,E,x),e(E,Te),e(E,Q),e(Q,$e),e(E,_e),e(E,O),e(O,ye),e(E,ae),e(E,H),e(H,De),e(E,ie),e(E,K),e(K,Fe),e(E,de),e(E,V),e(V,Ee),e(E,be),_(k,ee,x),_(k,j,x),e(j,q),e(q,le),e(q,U),e(U,Be),e(q,ve),e(q,W),e(W,xe),e(j,ke),e(j,C),e(C,se),e(C,J),e(J,ce),e(C,Me),e(C,G),e(G,pe),e(j,ze),e(j,S),e(S,he),e(S,X),e(X,je),_(k,ne,x),_(k,P,x),e(P,Ce),e(P,A),e(A,Pe),e(P,qe)},d(k){k&&t(d),k&&t(l),k&&t(u),k&&t(oe),k&&t(E),k&&t(ee),k&&t(j),k&&t(ne),k&&t(P)}}}function XM(B){let d,b,f,m,v;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),v=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),v=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,v)},d(l){l&&t(d)}}}function YM(B){let d,b,f,m,v;return m=new me({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForMaskedLM
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForMaskedLM.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="tf")
logits = model(**inputs).logits

# retrieve index of [MASK]
mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])
selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)

predicted_token_id = tf.math.argmax(selected_logits, axis=-1)
tokenizer.decode(predicted_token_id)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve index of [MASK]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>selected_logits = tf.gather_nd(logits[<span class="hljs-number">0</span>], indices=mask_token_index)

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_id = tf.math.argmax(selected_logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predicted_token_id)
`}}),{c(){d=r("p"),b=o("Example:"),f=p(),w(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),v=!0},p:ue,i(l){v||(y(m.$$.fragment,l),v=!0)},o(l){D(m.$$.fragment,l),v=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function ZM(B){let d,b;return d=new me({props:{code:`labels = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]
# mask labels of non-[MASK] tokens
labels = tf.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

outputs = model(**inputs, labels=labels)
round(float(outputs.loss), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># mask labels of non-[MASK] tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tf.where(inputs.input_ids == tokenizer.mask_token_id, labels, -<span class="hljs-number">100</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(outputs.loss), <span class="hljs-number">2</span>)
`}}),{c(){w(d.$$.fragment)},l(f){T(d.$$.fragment,f)},m(f,m){$(d,f,m),b=!0},p:ue,i(f){b||(y(d.$$.fragment,f),b=!0)},o(f){D(d.$$.fragment,f),b=!1},d(f){F(d,f)}}}function e3(B){let d,b,f,m,v,l,u,M,we,ge,I,re,oe,E,Te,Q,$e,_e,O,ye,ae,H,De,ie,K,Fe,de,V,Ee,be,ee,j,q,le,U,Be,ve,W,xe,ke,C,se,J,ce,Me,G,pe,ze,S,he,X,je,ne,P,Ce,A,Pe,qe;return{c(){d=r("p"),b=o("TensorFlow models and layers in "),f=r("code"),m=o("transformers"),v=o(" accept two formats as input:"),l=p(),u=r("ul"),M=r("li"),we=o("having all inputs as keyword arguments (like PyTorch models), or"),ge=p(),I=r("li"),re=o("having all inputs as a list, tuple or dict in the first positional argument."),oe=p(),E=r("p"),Te=o(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=r("code"),$e=o("model.fit()"),_e=o(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=r("code"),ye=o("model.fit()"),ae=o(` supports! If, however, you want to use the second
format outside of Keras methods like `),H=r("code"),De=o("fit()"),ie=o(" and "),K=r("code"),Fe=o("predict()"),de=o(`, such as when creating your own layers or models with
the Keras `),V=r("code"),Ee=o("Functional"),be=o(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),ee=p(),j=r("ul"),q=r("li"),le=o("a single Tensor with "),U=r("code"),Be=o("input_ids"),ve=o(" only and nothing else: "),W=r("code"),xe=o("model(input_ids)"),ke=p(),C=r("li"),se=o(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=r("code"),ce=o("model([input_ids, attention_mask])"),Me=o(" or "),G=r("code"),pe=o("model([input_ids, attention_mask, token_type_ids])"),ze=p(),S=r("li"),he=o(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=r("code"),je=o('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ne=p(),P=r("p"),Ce=o(`Note that when creating models and layers with
`),A=r("a"),Pe=o("subclassing"),qe=o(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(k){d=a(k,"P",{});var x=i(d);b=s(x,"TensorFlow models and layers in "),f=a(x,"CODE",{});var He=i(f);m=s(He,"transformers"),He.forEach(t),v=s(x," accept two formats as input:"),x.forEach(t),l=h(k),u=a(k,"UL",{});var Z=i(u);M=a(Z,"LI",{});var Ke=i(M);we=s(Ke,"having all inputs as keyword arguments (like PyTorch models), or"),Ke.forEach(t),ge=h(Z),I=a(Z,"LI",{});var Ve=i(I);re=s(Ve,"having all inputs as a list, tuple or dict in the first positional argument."),Ve.forEach(t),Z.forEach(t),oe=h(k),E=a(k,"P",{});var z=i(E);Te=s(z,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=a(z,"CODE",{});var Je=i(Q);$e=s(Je,"model.fit()"),Je.forEach(t),_e=s(z,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=a(z,"CODE",{});var Ge=i(O);ye=s(Ge,"model.fit()"),Ge.forEach(t),ae=s(z,` supports! If, however, you want to use the second
format outside of Keras methods like `),H=a(z,"CODE",{});var Le=i(H);De=s(Le,"fit()"),Le.forEach(t),ie=s(z," and "),K=a(z,"CODE",{});var Xe=i(K);Fe=s(Xe,"predict()"),Xe.forEach(t),de=s(z,`, such as when creating your own layers or models with
the Keras `),V=a(z,"CODE",{});var Ye=i(V);Ee=s(Ye,"Functional"),Ye.forEach(t),be=s(z,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),z.forEach(t),ee=h(k),j=a(k,"UL",{});var L=i(j);q=a(L,"LI",{});var N=i(q);le=s(N,"a single Tensor with "),U=a(N,"CODE",{});var Oe=i(U);Be=s(Oe,"input_ids"),Oe.forEach(t),ve=s(N," only and nothing else: "),W=a(N,"CODE",{});var Re=i(W);xe=s(Re,"model(input_ids)"),Re.forEach(t),N.forEach(t),ke=h(L),C=a(L,"LI",{});var R=i(C);se=s(R,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=a(R,"CODE",{});var Ze=i(J);ce=s(Ze,"model([input_ids, attention_mask])"),Ze.forEach(t),Me=s(R," or "),G=a(R,"CODE",{});var We=i(G);pe=s(We,"model([input_ids, attention_mask, token_type_ids])"),We.forEach(t),R.forEach(t),ze=h(L),S=a(L,"LI",{});var Ae=i(S);he=s(Ae,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=a(Ae,"CODE",{});var Ue=i(X);je=s(Ue,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ue.forEach(t),Ae.forEach(t),L.forEach(t),ne=h(k),P=a(k,"P",{});var te=i(P);Ce=s(te,`Note that when creating models and layers with
`),A=a(te,"A",{href:!0,rel:!0});var et=i(A);Pe=s(et,"subclassing"),et.forEach(t),qe=s(te,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),te.forEach(t),this.h()},h(){c(A,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),c(A,"rel","nofollow")},m(k,x){_(k,d,x),e(d,b),e(d,f),e(f,m),e(d,v),_(k,l,x),_(k,u,x),e(u,M),e(M,we),e(u,ge),e(u,I),e(I,re),_(k,oe,x),_(k,E,x),e(E,Te),e(E,Q),e(Q,$e),e(E,_e),e(E,O),e(O,ye),e(E,ae),e(E,H),e(H,De),e(E,ie),e(E,K),e(K,Fe),e(E,de),e(E,V),e(V,Ee),e(E,be),_(k,ee,x),_(k,j,x),e(j,q),e(q,le),e(q,U),e(U,Be),e(q,ve),e(q,W),e(W,xe),e(j,ke),e(j,C),e(C,se),e(C,J),e(J,ce),e(C,Me),e(C,G),e(G,pe),e(j,ze),e(j,S),e(S,he),e(S,X),e(X,je),_(k,ne,x),_(k,P,x),e(P,Ce),e(P,A),e(A,Pe),e(P,qe)},d(k){k&&t(d),k&&t(l),k&&t(u),k&&t(oe),k&&t(E),k&&t(ee),k&&t(j),k&&t(ne),k&&t(P)}}}function t3(B){let d,b,f,m,v;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),v=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),v=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,v)},d(l){l&&t(d)}}}function o3(B){let d,b,f,m,v;return m=new me({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")

logits = model(**inputs).logits

predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])
model.config.id2label[predicted_class_id]
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = <span class="hljs-built_in">int</span>(tf.math.argmax(logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
`}}),{c(){d=r("p"),b=o("Example:"),f=p(),w(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),v=!0},p:ue,i(l){v||(y(m.$$.fragment,l),v=!0)},o(l){D(m.$$.fragment,l),v=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function s3(B){let d,b;return d=new me({props:{code:`# To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`
num_labels = len(model.config.id2label)
model = TFDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=num_labels)

labels = tf.constant(1)
loss = model(**inputs, labels=labels).loss
round(float(loss), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=num_labels)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tf.constant(<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(loss), <span class="hljs-number">2</span>)
`}}),{c(){w(d.$$.fragment)},l(f){T(d.$$.fragment,f)},m(f,m){$(d,f,m),b=!0},p:ue,i(f){b||(y(d.$$.fragment,f),b=!0)},o(f){D(d.$$.fragment,f),b=!1},d(f){F(d,f)}}}function n3(B){let d,b,f,m,v,l,u,M,we,ge,I,re,oe,E,Te,Q,$e,_e,O,ye,ae,H,De,ie,K,Fe,de,V,Ee,be,ee,j,q,le,U,Be,ve,W,xe,ke,C,se,J,ce,Me,G,pe,ze,S,he,X,je,ne,P,Ce,A,Pe,qe;return{c(){d=r("p"),b=o("TensorFlow models and layers in "),f=r("code"),m=o("transformers"),v=o(" accept two formats as input:"),l=p(),u=r("ul"),M=r("li"),we=o("having all inputs as keyword arguments (like PyTorch models), or"),ge=p(),I=r("li"),re=o("having all inputs as a list, tuple or dict in the first positional argument."),oe=p(),E=r("p"),Te=o(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=r("code"),$e=o("model.fit()"),_e=o(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=r("code"),ye=o("model.fit()"),ae=o(` supports! If, however, you want to use the second
format outside of Keras methods like `),H=r("code"),De=o("fit()"),ie=o(" and "),K=r("code"),Fe=o("predict()"),de=o(`, such as when creating your own layers or models with
the Keras `),V=r("code"),Ee=o("Functional"),be=o(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),ee=p(),j=r("ul"),q=r("li"),le=o("a single Tensor with "),U=r("code"),Be=o("input_ids"),ve=o(" only and nothing else: "),W=r("code"),xe=o("model(input_ids)"),ke=p(),C=r("li"),se=o(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=r("code"),ce=o("model([input_ids, attention_mask])"),Me=o(" or "),G=r("code"),pe=o("model([input_ids, attention_mask, token_type_ids])"),ze=p(),S=r("li"),he=o(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=r("code"),je=o('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ne=p(),P=r("p"),Ce=o(`Note that when creating models and layers with
`),A=r("a"),Pe=o("subclassing"),qe=o(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(k){d=a(k,"P",{});var x=i(d);b=s(x,"TensorFlow models and layers in "),f=a(x,"CODE",{});var He=i(f);m=s(He,"transformers"),He.forEach(t),v=s(x," accept two formats as input:"),x.forEach(t),l=h(k),u=a(k,"UL",{});var Z=i(u);M=a(Z,"LI",{});var Ke=i(M);we=s(Ke,"having all inputs as keyword arguments (like PyTorch models), or"),Ke.forEach(t),ge=h(Z),I=a(Z,"LI",{});var Ve=i(I);re=s(Ve,"having all inputs as a list, tuple or dict in the first positional argument."),Ve.forEach(t),Z.forEach(t),oe=h(k),E=a(k,"P",{});var z=i(E);Te=s(z,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=a(z,"CODE",{});var Je=i(Q);$e=s(Je,"model.fit()"),Je.forEach(t),_e=s(z,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=a(z,"CODE",{});var Ge=i(O);ye=s(Ge,"model.fit()"),Ge.forEach(t),ae=s(z,` supports! If, however, you want to use the second
format outside of Keras methods like `),H=a(z,"CODE",{});var Le=i(H);De=s(Le,"fit()"),Le.forEach(t),ie=s(z," and "),K=a(z,"CODE",{});var Xe=i(K);Fe=s(Xe,"predict()"),Xe.forEach(t),de=s(z,`, such as when creating your own layers or models with
the Keras `),V=a(z,"CODE",{});var Ye=i(V);Ee=s(Ye,"Functional"),Ye.forEach(t),be=s(z,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),z.forEach(t),ee=h(k),j=a(k,"UL",{});var L=i(j);q=a(L,"LI",{});var N=i(q);le=s(N,"a single Tensor with "),U=a(N,"CODE",{});var Oe=i(U);Be=s(Oe,"input_ids"),Oe.forEach(t),ve=s(N," only and nothing else: "),W=a(N,"CODE",{});var Re=i(W);xe=s(Re,"model(input_ids)"),Re.forEach(t),N.forEach(t),ke=h(L),C=a(L,"LI",{});var R=i(C);se=s(R,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=a(R,"CODE",{});var Ze=i(J);ce=s(Ze,"model([input_ids, attention_mask])"),Ze.forEach(t),Me=s(R," or "),G=a(R,"CODE",{});var We=i(G);pe=s(We,"model([input_ids, attention_mask, token_type_ids])"),We.forEach(t),R.forEach(t),ze=h(L),S=a(L,"LI",{});var Ae=i(S);he=s(Ae,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=a(Ae,"CODE",{});var Ue=i(X);je=s(Ue,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ue.forEach(t),Ae.forEach(t),L.forEach(t),ne=h(k),P=a(k,"P",{});var te=i(P);Ce=s(te,`Note that when creating models and layers with
`),A=a(te,"A",{href:!0,rel:!0});var et=i(A);Pe=s(et,"subclassing"),et.forEach(t),qe=s(te,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),te.forEach(t),this.h()},h(){c(A,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),c(A,"rel","nofollow")},m(k,x){_(k,d,x),e(d,b),e(d,f),e(f,m),e(d,v),_(k,l,x),_(k,u,x),e(u,M),e(M,we),e(u,ge),e(u,I),e(I,re),_(k,oe,x),_(k,E,x),e(E,Te),e(E,Q),e(Q,$e),e(E,_e),e(E,O),e(O,ye),e(E,ae),e(E,H),e(H,De),e(E,ie),e(E,K),e(K,Fe),e(E,de),e(E,V),e(V,Ee),e(E,be),_(k,ee,x),_(k,j,x),e(j,q),e(q,le),e(q,U),e(U,Be),e(q,ve),e(q,W),e(W,xe),e(j,ke),e(j,C),e(C,se),e(C,J),e(J,ce),e(C,Me),e(C,G),e(G,pe),e(j,ze),e(j,S),e(S,he),e(S,X),e(X,je),_(k,ne,x),_(k,P,x),e(P,Ce),e(P,A),e(A,Pe),e(P,qe)},d(k){k&&t(d),k&&t(l),k&&t(u),k&&t(oe),k&&t(E),k&&t(ee),k&&t(j),k&&t(ne),k&&t(P)}}}function r3(B){let d,b,f,m,v;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),v=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),v=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,v)},d(l){l&&t(d)}}}function a3(B){let d,b,f,m,v;return m=new me({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForMultipleChoice
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForMultipleChoice.from_pretrained("distilbert-base-uncased")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="tf", padding=True)
inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}
outputs = model(inputs)  # batch size is 1

# the linear classifier still needs to be trained
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;tf&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {k: tf.expand_dims(v, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){d=r("p"),b=o("Example:"),f=p(),w(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),v=!0},p:ue,i(l){v||(y(m.$$.fragment,l),v=!0)},o(l){D(m.$$.fragment,l),v=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function i3(B){let d,b,f,m,v,l,u,M,we,ge,I,re,oe,E,Te,Q,$e,_e,O,ye,ae,H,De,ie,K,Fe,de,V,Ee,be,ee,j,q,le,U,Be,ve,W,xe,ke,C,se,J,ce,Me,G,pe,ze,S,he,X,je,ne,P,Ce,A,Pe,qe;return{c(){d=r("p"),b=o("TensorFlow models and layers in "),f=r("code"),m=o("transformers"),v=o(" accept two formats as input:"),l=p(),u=r("ul"),M=r("li"),we=o("having all inputs as keyword arguments (like PyTorch models), or"),ge=p(),I=r("li"),re=o("having all inputs as a list, tuple or dict in the first positional argument."),oe=p(),E=r("p"),Te=o(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=r("code"),$e=o("model.fit()"),_e=o(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=r("code"),ye=o("model.fit()"),ae=o(` supports! If, however, you want to use the second
format outside of Keras methods like `),H=r("code"),De=o("fit()"),ie=o(" and "),K=r("code"),Fe=o("predict()"),de=o(`, such as when creating your own layers or models with
the Keras `),V=r("code"),Ee=o("Functional"),be=o(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),ee=p(),j=r("ul"),q=r("li"),le=o("a single Tensor with "),U=r("code"),Be=o("input_ids"),ve=o(" only and nothing else: "),W=r("code"),xe=o("model(input_ids)"),ke=p(),C=r("li"),se=o(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=r("code"),ce=o("model([input_ids, attention_mask])"),Me=o(" or "),G=r("code"),pe=o("model([input_ids, attention_mask, token_type_ids])"),ze=p(),S=r("li"),he=o(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=r("code"),je=o('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ne=p(),P=r("p"),Ce=o(`Note that when creating models and layers with
`),A=r("a"),Pe=o("subclassing"),qe=o(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(k){d=a(k,"P",{});var x=i(d);b=s(x,"TensorFlow models and layers in "),f=a(x,"CODE",{});var He=i(f);m=s(He,"transformers"),He.forEach(t),v=s(x," accept two formats as input:"),x.forEach(t),l=h(k),u=a(k,"UL",{});var Z=i(u);M=a(Z,"LI",{});var Ke=i(M);we=s(Ke,"having all inputs as keyword arguments (like PyTorch models), or"),Ke.forEach(t),ge=h(Z),I=a(Z,"LI",{});var Ve=i(I);re=s(Ve,"having all inputs as a list, tuple or dict in the first positional argument."),Ve.forEach(t),Z.forEach(t),oe=h(k),E=a(k,"P",{});var z=i(E);Te=s(z,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=a(z,"CODE",{});var Je=i(Q);$e=s(Je,"model.fit()"),Je.forEach(t),_e=s(z,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=a(z,"CODE",{});var Ge=i(O);ye=s(Ge,"model.fit()"),Ge.forEach(t),ae=s(z,` supports! If, however, you want to use the second
format outside of Keras methods like `),H=a(z,"CODE",{});var Le=i(H);De=s(Le,"fit()"),Le.forEach(t),ie=s(z," and "),K=a(z,"CODE",{});var Xe=i(K);Fe=s(Xe,"predict()"),Xe.forEach(t),de=s(z,`, such as when creating your own layers or models with
the Keras `),V=a(z,"CODE",{});var Ye=i(V);Ee=s(Ye,"Functional"),Ye.forEach(t),be=s(z,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),z.forEach(t),ee=h(k),j=a(k,"UL",{});var L=i(j);q=a(L,"LI",{});var N=i(q);le=s(N,"a single Tensor with "),U=a(N,"CODE",{});var Oe=i(U);Be=s(Oe,"input_ids"),Oe.forEach(t),ve=s(N," only and nothing else: "),W=a(N,"CODE",{});var Re=i(W);xe=s(Re,"model(input_ids)"),Re.forEach(t),N.forEach(t),ke=h(L),C=a(L,"LI",{});var R=i(C);se=s(R,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=a(R,"CODE",{});var Ze=i(J);ce=s(Ze,"model([input_ids, attention_mask])"),Ze.forEach(t),Me=s(R," or "),G=a(R,"CODE",{});var We=i(G);pe=s(We,"model([input_ids, attention_mask, token_type_ids])"),We.forEach(t),R.forEach(t),ze=h(L),S=a(L,"LI",{});var Ae=i(S);he=s(Ae,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=a(Ae,"CODE",{});var Ue=i(X);je=s(Ue,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ue.forEach(t),Ae.forEach(t),L.forEach(t),ne=h(k),P=a(k,"P",{});var te=i(P);Ce=s(te,`Note that when creating models and layers with
`),A=a(te,"A",{href:!0,rel:!0});var et=i(A);Pe=s(et,"subclassing"),et.forEach(t),qe=s(te,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),te.forEach(t),this.h()},h(){c(A,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),c(A,"rel","nofollow")},m(k,x){_(k,d,x),e(d,b),e(d,f),e(f,m),e(d,v),_(k,l,x),_(k,u,x),e(u,M),e(M,we),e(u,ge),e(u,I),e(I,re),_(k,oe,x),_(k,E,x),e(E,Te),e(E,Q),e(Q,$e),e(E,_e),e(E,O),e(O,ye),e(E,ae),e(E,H),e(H,De),e(E,ie),e(E,K),e(K,Fe),e(E,de),e(E,V),e(V,Ee),e(E,be),_(k,ee,x),_(k,j,x),e(j,q),e(q,le),e(q,U),e(U,Be),e(q,ve),e(q,W),e(W,xe),e(j,ke),e(j,C),e(C,se),e(C,J),e(J,ce),e(C,Me),e(C,G),e(G,pe),e(j,ze),e(j,S),e(S,he),e(S,X),e(X,je),_(k,ne,x),_(k,P,x),e(P,Ce),e(P,A),e(A,Pe),e(P,qe)},d(k){k&&t(d),k&&t(l),k&&t(u),k&&t(oe),k&&t(E),k&&t(ee),k&&t(j),k&&t(ne),k&&t(P)}}}function l3(B){let d,b,f,m,v;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),v=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),v=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,v)},d(l){l&&t(d)}}}function d3(B){let d,b,f,m,v;return m=new me({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForTokenClassification
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForTokenClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer(
    "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="tf"
)

logits = model(**inputs).logits
predicted_token_class_ids = tf.math.argmax(logits, axis=-1)

# Note that tokens are classified rather then input words which means that
# there might be more predicted token classes than words.
# Multiple token classes might account for the same word
predicted_tokens_classes = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]
predicted_tokens_classes
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;HuggingFace is a company based in Paris and New York&quot;</span>, add_special_tokens=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_class_ids = tf.math.argmax(logits, axis=-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Note that tokens are classified rather then input words which means that</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># there might be more predicted token classes than words.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multiple token classes might account for the same word</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes = [model.config.id2label[t] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> predicted_token_class_ids[<span class="hljs-number">0</span>].numpy().tolist()]
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes
`}}),{c(){d=r("p"),b=o("Example:"),f=p(),w(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),v=!0},p:ue,i(l){v||(y(m.$$.fragment,l),v=!0)},o(l){D(m.$$.fragment,l),v=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function c3(B){let d,b;return d=new me({props:{code:`labels = predicted_token_class_ids
loss = tf.math.reduce_mean(model(**inputs, labels=labels).loss)
round(float(loss), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = predicted_token_class_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = tf.math.reduce_mean(model(**inputs, labels=labels).loss)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(loss), <span class="hljs-number">2</span>)
`}}),{c(){w(d.$$.fragment)},l(f){T(d.$$.fragment,f)},m(f,m){$(d,f,m),b=!0},p:ue,i(f){b||(y(d.$$.fragment,f),b=!0)},o(f){D(d.$$.fragment,f),b=!1},d(f){F(d,f)}}}function p3(B){let d,b,f,m,v,l,u,M,we,ge,I,re,oe,E,Te,Q,$e,_e,O,ye,ae,H,De,ie,K,Fe,de,V,Ee,be,ee,j,q,le,U,Be,ve,W,xe,ke,C,se,J,ce,Me,G,pe,ze,S,he,X,je,ne,P,Ce,A,Pe,qe;return{c(){d=r("p"),b=o("TensorFlow models and layers in "),f=r("code"),m=o("transformers"),v=o(" accept two formats as input:"),l=p(),u=r("ul"),M=r("li"),we=o("having all inputs as keyword arguments (like PyTorch models), or"),ge=p(),I=r("li"),re=o("having all inputs as a list, tuple or dict in the first positional argument."),oe=p(),E=r("p"),Te=o(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=r("code"),$e=o("model.fit()"),_e=o(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=r("code"),ye=o("model.fit()"),ae=o(` supports! If, however, you want to use the second
format outside of Keras methods like `),H=r("code"),De=o("fit()"),ie=o(" and "),K=r("code"),Fe=o("predict()"),de=o(`, such as when creating your own layers or models with
the Keras `),V=r("code"),Ee=o("Functional"),be=o(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),ee=p(),j=r("ul"),q=r("li"),le=o("a single Tensor with "),U=r("code"),Be=o("input_ids"),ve=o(" only and nothing else: "),W=r("code"),xe=o("model(input_ids)"),ke=p(),C=r("li"),se=o(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=r("code"),ce=o("model([input_ids, attention_mask])"),Me=o(" or "),G=r("code"),pe=o("model([input_ids, attention_mask, token_type_ids])"),ze=p(),S=r("li"),he=o(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=r("code"),je=o('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ne=p(),P=r("p"),Ce=o(`Note that when creating models and layers with
`),A=r("a"),Pe=o("subclassing"),qe=o(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(k){d=a(k,"P",{});var x=i(d);b=s(x,"TensorFlow models and layers in "),f=a(x,"CODE",{});var He=i(f);m=s(He,"transformers"),He.forEach(t),v=s(x," accept two formats as input:"),x.forEach(t),l=h(k),u=a(k,"UL",{});var Z=i(u);M=a(Z,"LI",{});var Ke=i(M);we=s(Ke,"having all inputs as keyword arguments (like PyTorch models), or"),Ke.forEach(t),ge=h(Z),I=a(Z,"LI",{});var Ve=i(I);re=s(Ve,"having all inputs as a list, tuple or dict in the first positional argument."),Ve.forEach(t),Z.forEach(t),oe=h(k),E=a(k,"P",{});var z=i(E);Te=s(z,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=a(z,"CODE",{});var Je=i(Q);$e=s(Je,"model.fit()"),Je.forEach(t),_e=s(z,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=a(z,"CODE",{});var Ge=i(O);ye=s(Ge,"model.fit()"),Ge.forEach(t),ae=s(z,` supports! If, however, you want to use the second
format outside of Keras methods like `),H=a(z,"CODE",{});var Le=i(H);De=s(Le,"fit()"),Le.forEach(t),ie=s(z," and "),K=a(z,"CODE",{});var Xe=i(K);Fe=s(Xe,"predict()"),Xe.forEach(t),de=s(z,`, such as when creating your own layers or models with
the Keras `),V=a(z,"CODE",{});var Ye=i(V);Ee=s(Ye,"Functional"),Ye.forEach(t),be=s(z,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),z.forEach(t),ee=h(k),j=a(k,"UL",{});var L=i(j);q=a(L,"LI",{});var N=i(q);le=s(N,"a single Tensor with "),U=a(N,"CODE",{});var Oe=i(U);Be=s(Oe,"input_ids"),Oe.forEach(t),ve=s(N," only and nothing else: "),W=a(N,"CODE",{});var Re=i(W);xe=s(Re,"model(input_ids)"),Re.forEach(t),N.forEach(t),ke=h(L),C=a(L,"LI",{});var R=i(C);se=s(R,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=a(R,"CODE",{});var Ze=i(J);ce=s(Ze,"model([input_ids, attention_mask])"),Ze.forEach(t),Me=s(R," or "),G=a(R,"CODE",{});var We=i(G);pe=s(We,"model([input_ids, attention_mask, token_type_ids])"),We.forEach(t),R.forEach(t),ze=h(L),S=a(L,"LI",{});var Ae=i(S);he=s(Ae,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=a(Ae,"CODE",{});var Ue=i(X);je=s(Ue,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ue.forEach(t),Ae.forEach(t),L.forEach(t),ne=h(k),P=a(k,"P",{});var te=i(P);Ce=s(te,`Note that when creating models and layers with
`),A=a(te,"A",{href:!0,rel:!0});var et=i(A);Pe=s(et,"subclassing"),et.forEach(t),qe=s(te,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),te.forEach(t),this.h()},h(){c(A,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),c(A,"rel","nofollow")},m(k,x){_(k,d,x),e(d,b),e(d,f),e(f,m),e(d,v),_(k,l,x),_(k,u,x),e(u,M),e(M,we),e(u,ge),e(u,I),e(I,re),_(k,oe,x),_(k,E,x),e(E,Te),e(E,Q),e(Q,$e),e(E,_e),e(E,O),e(O,ye),e(E,ae),e(E,H),e(H,De),e(E,ie),e(E,K),e(K,Fe),e(E,de),e(E,V),e(V,Ee),e(E,be),_(k,ee,x),_(k,j,x),e(j,q),e(q,le),e(q,U),e(U,Be),e(q,ve),e(q,W),e(W,xe),e(j,ke),e(j,C),e(C,se),e(C,J),e(J,ce),e(C,Me),e(C,G),e(G,pe),e(j,ze),e(j,S),e(S,he),e(S,X),e(X,je),_(k,ne,x),_(k,P,x),e(P,Ce),e(P,A),e(A,Pe),e(P,qe)},d(k){k&&t(d),k&&t(l),k&&t(u),k&&t(oe),k&&t(E),k&&t(ee),k&&t(j),k&&t(ne),k&&t(P)}}}function h3(B){let d,b,f,m,v;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),v=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),v=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,v)},d(l){l&&t(d)}}}function f3(B){let d,b,f,m,v;return m=new me({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

inputs = tokenizer(question, text, return_tensors="tf")
outputs = model(**inputs)

answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])
answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])

predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
tokenizer.decode(predict_answer_tokens)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>answer_start_index = <span class="hljs-built_in">int</span>(tf.math.argmax(outputs.start_logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>answer_end_index = <span class="hljs-built_in">int</span>(tf.math.argmax(outputs.end_logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>predict_answer_tokens = inputs.input_ids[<span class="hljs-number">0</span>, answer_start_index : answer_end_index + <span class="hljs-number">1</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predict_answer_tokens)
`}}),{c(){d=r("p"),b=o("Example:"),f=p(),w(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),v=!0},p:ue,i(l){v||(y(m.$$.fragment,l),v=!0)},o(l){D(m.$$.fragment,l),v=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function u3(B){let d,b;return d=new me({props:{code:`# target is "nice puppet"
target_start_index = tf.constant([14])
target_end_index = tf.constant([15])

outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
loss = tf.math.reduce_mean(outputs.loss)
round(float(loss), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target is &quot;nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_start_index = tf.constant([<span class="hljs-number">14</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>target_end_index = tf.constant([<span class="hljs-number">15</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = tf.math.reduce_mean(outputs.loss)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(loss), <span class="hljs-number">2</span>)
`}}),{c(){w(d.$$.fragment)},l(f){T(d.$$.fragment,f)},m(f,m){$(d,f,m),b=!0},p:ue,i(f){b||(y(d.$$.fragment,f),b=!0)},o(f){D(d.$$.fragment,f),b=!1},d(f){F(d,f)}}}function m3(B){let d,b,f,m,v;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),v=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),v=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,v)},d(l){l&&t(d)}}}function g3(B){let d,b,f,m,v;return m=new me({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertModel

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertModel.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertModel.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){d=r("p"),b=o("Example:"),f=p(),w(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),v=!0},p:ue,i(l){v||(y(m.$$.fragment,l),v=!0)},o(l){D(m.$$.fragment,l),v=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function _3(B){let d,b,f,m,v;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),v=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),v=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,v)},d(l){l&&t(d)}}}function b3(B){let d,b,f,m,v;return m=new me({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForMaskedLM

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForMaskedLM.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){d=r("p"),b=o("Example:"),f=p(),w(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),v=!0},p:ue,i(l){v||(y(m.$$.fragment,l),v=!0)},o(l){D(m.$$.fragment,l),v=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function v3(B){let d,b,f,m,v;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),v=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),v=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,v)},d(l){l&&t(d)}}}function k3(B){let d,b,f,m,v;return m=new me({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForSequenceClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){d=r("p"),b=o("Example:"),f=p(),w(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),v=!0},p:ue,i(l){v||(y(m.$$.fragment,l),v=!0)},o(l){D(m.$$.fragment,l),v=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function w3(B){let d,b,f,m,v;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),v=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),v=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,v)},d(l){l&&t(d)}}}function T3(B){let d,b,f,m,v;return m=new me({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForMultipleChoice

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForMultipleChoice.from_pretrained("distilbert-base-uncased")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="jax", padding=True)
outputs = model(**{k: v[None, :] for k, v in encoding.items()})

logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;jax&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v[<span class="hljs-literal">None</span>, :] <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()})

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){d=r("p"),b=o("Example:"),f=p(),w(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),v=!0},p:ue,i(l){v||(y(m.$$.fragment,l),v=!0)},o(l){D(m.$$.fragment,l),v=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function $3(B){let d,b,f,m,v;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),v=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),v=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,v)},d(l){l&&t(d)}}}function y3(B){let d,b,f,m,v;return m=new me({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForTokenClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForTokenClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){d=r("p"),b=o("Example:"),f=p(),w(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),v=!0},p:ue,i(l){v||(y(m.$$.fragment,l),v=!0)},o(l){D(m.$$.fragment,l),v=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function D3(B){let d,b,f,m,v;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),v=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),v=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,v)},d(l){l&&t(d)}}}function F3(B){let d,b,f,m,v;return m=new me({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForQuestionAnswering

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
inputs = tokenizer(question, text, return_tensors="jax")

outputs = model(**inputs)
start_scores = outputs.start_logits
end_scores = outputs.end_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_scores = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_scores = outputs.end_logits`}}),{c(){d=r("p"),b=o("Example:"),f=p(),w(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),v=!0},p:ue,i(l){v||(y(m.$$.fragment,l),v=!0)},o(l){D(m.$$.fragment,l),v=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function E3(B){let d,b,f,m,v,l,u,M,we,ge,I,re,oe,E,Te,Q,$e,_e,O,ye,ae,H,De,ie,K,Fe,de,V,Ee,be,ee,j,q,le,U,Be,ve,W,xe,ke,C,se,J,ce,Me,G,pe,ze,S,he,X,je,ne,P,Ce,A,Pe,qe,k,x,He,Z,Ke,Ve,z,Je,Ge,Le,Xe,Ye,L,N,Oe,Re,R,Ze,We,Ae,Ue,te,et,gu,Er,_u,Ie,Br,Ng,xr,Rg,Wg,Qg,Mr,Ug,zr,Hg,Kg,Vg,jr,Jg,Cr,Gg,Xg,Yg,Pr,Zg,qr,e_,t_,o_,Ar,s_,Or,n_,r_,a_,Lr,i_,Ir,l_,d_,c_,Sr,p_,Nr,h_,f_,u_,fo,Nd,m_,g_,Rr,__,b_,Wr,v_,k_,w_,uo,Rd,T_,$_,Qr,y_,D_,Ur,F_,E_,B_,mo,Wd,x_,M_,Hr,z_,j_,Kr,C_,P_,bu,Vr,vu,Ot,go,Qd,q_,A_,Jr,O_,L_,Gr,I_,S_,N_,_o,Ud,R_,W_,Xr,Q_,U_,Yr,H_,K_,V_,zs,Hd,J_,G_,Zr,X_,Y_,Z_,Kd,ea,eb,tb,ku,ta,wu,Lt,bo,Vd,ob,sb,oa,nb,rb,sa,ab,ib,lb,vo,Jd,db,cb,na,pb,hb,ra,fb,ub,mb,ko,Gd,gb,_b,aa,bb,vb,ia,kb,wb,Tb,Xd,la,$b,yb,Tu,da,$u,It,wo,Yd,Db,Fb,ca,Eb,Bb,pa,xb,Mb,zb,To,Zd,jb,Cb,ha,Pb,qb,fa,Ab,Ob,Lb,js,ec,Ib,Sb,ua,Nb,Rb,Wb,tc,ma,Qb,Ub,yu,oc,bp,Hb,Du,Cs,$o,sc,Kb,Vb,ga,Jb,Gb,_a,Xb,Yb,Zb,yo,nc,ev,tv,ba,ov,sv,va,nv,rv,Fu,rc,av,Eu,Do,ka,iv,wa,lv,dv,cv,Ta,pv,$a,hv,fv,uv,ya,mv,Da,gv,_v,Bu,ac,bv,xu,Ps,Fa,vv,Ea,kv,wv,Tv,Ba,$v,xa,yv,Dv,Mu,ic,Fv,zu,Fo,Ma,Ev,za,Bv,xv,Mv,ja,zv,Ca,jv,Cv,Pv,Pa,qv,qa,Av,Ov,ju,Ro,qs,vp,Aa,Lv,kp,Iv,Cu,Pt,Oa,Sv,oo,Nv,lc,Rv,Wv,dc,Qv,Uv,La,Hv,Kv,Vv,Wo,Jv,cc,Gv,Xv,pc,Yv,Zv,ek,As,Pu,Qo,Os,wp,Ia,tk,Tp,ok,qu,qt,Sa,sk,$p,nk,rk,Ls,hc,ak,ik,fc,lk,dk,ck,Na,pk,uc,hk,fk,Au,Uo,Is,yp,Ra,uk,Dp,mk,Ou,At,Wa,gk,Qa,_k,Fp,bk,vk,kk,Ss,mc,wk,Tk,gc,$k,yk,Dk,Ua,Fk,_c,Ek,Bk,Lu,Ho,Ns,Ep,Ha,xk,Bp,Mk,Iu,ut,Ka,zk,xp,jk,Ck,Va,Pk,bc,qk,Ak,Ok,Ja,Lk,Ga,Ik,Sk,Nk,St,Xa,Rk,Ko,Wk,vc,Qk,Uk,Mp,Hk,Kk,Vk,Rs,Jk,Ws,Su,Vo,Qs,zp,Ya,Gk,jp,Xk,Nu,mt,Za,Yk,ei,Zk,Cp,ew,tw,ow,ti,sw,kc,nw,rw,aw,oi,iw,si,lw,dw,cw,kt,ni,pw,Jo,hw,wc,fw,uw,Pp,mw,gw,_w,Us,bw,Hs,vw,Ks,Ru,Go,Vs,qp,ri,kw,Ap,ww,Wu,gt,ai,Tw,Op,$w,yw,ii,Dw,Tc,Fw,Ew,Bw,li,xw,di,Mw,zw,jw,it,ci,Cw,Xo,Pw,$c,qw,Aw,Lp,Ow,Lw,Iw,Js,Sw,Gs,Nw,Xs,Rw,Ys,Ww,Zs,Qu,Yo,en,Ip,pi,Qw,Sp,Uw,Uu,_t,hi,Hw,Np,Kw,Vw,fi,Jw,yc,Gw,Xw,Yw,ui,Zw,mi,eT,tT,oT,Nt,gi,sT,Zo,nT,Dc,rT,aT,Rp,iT,lT,dT,tn,cT,on,Hu,es,sn,Wp,_i,pT,Qp,hT,Ku,bt,bi,fT,Up,uT,mT,vi,gT,Fc,_T,bT,vT,ki,kT,wi,wT,TT,$T,wt,Ti,yT,ts,DT,Ec,FT,ET,Hp,BT,xT,MT,nn,zT,rn,jT,an,Vu,os,ln,Kp,$i,CT,Vp,PT,Ju,vt,yi,qT,ss,AT,Jp,OT,LT,Gp,IT,ST,NT,Di,RT,Bc,WT,QT,UT,Fi,HT,Ei,KT,VT,JT,Tt,Bi,GT,ns,XT,xc,YT,ZT,Xp,e$,t$,o$,dn,s$,cn,n$,pn,Gu,rs,hn,Yp,xi,r$,Zp,a$,Xu,lt,Mi,i$,eh,l$,d$,zi,c$,Mc,p$,h$,f$,ji,u$,Ci,m$,g$,_$,fn,b$,Rt,Pi,v$,as,k$,zc,w$,T$,th,$$,y$,D$,un,F$,mn,Yu,is,gn,oh,qi,E$,sh,B$,Zu,dt,Ai,x$,Oi,M$,nh,z$,j$,C$,Li,P$,jc,q$,A$,O$,Ii,L$,Si,I$,S$,N$,_n,R$,$t,Ni,W$,ls,Q$,Cc,U$,H$,rh,K$,V$,J$,bn,G$,vn,X$,kn,em,ds,wn,ah,Ri,Y$,ih,Z$,tm,ct,Wi,ey,lh,ty,oy,Qi,sy,Pc,ny,ry,ay,Ui,iy,Hi,ly,dy,cy,Tn,py,yt,Ki,hy,cs,fy,qc,uy,my,dh,gy,_y,by,$n,vy,yn,ky,Dn,om,ps,Fn,ch,Vi,wy,ph,Ty,sm,pt,Ji,$y,hh,yy,Dy,Gi,Fy,Ac,Ey,By,xy,Xi,My,Yi,zy,jy,Cy,En,Py,Wt,Zi,qy,hs,Ay,Oc,Oy,Ly,fh,Iy,Sy,Ny,Bn,Ry,xn,nm,fs,Mn,uh,el,Wy,mh,Qy,rm,ht,tl,Uy,gh,Hy,Ky,ol,Vy,Lc,Jy,Gy,Xy,sl,Yy,nl,Zy,e2,t2,zn,o2,Dt,rl,s2,us,n2,Ic,r2,a2,_h,i2,l2,d2,jn,c2,Cn,p2,Pn,am,ms,qn,bh,al,h2,vh,f2,im,ft,il,u2,gs,m2,kh,g2,_2,wh,b2,v2,k2,ll,w2,Sc,T2,$2,y2,dl,D2,cl,F2,E2,B2,An,x2,Ft,pl,M2,_s,z2,Nc,j2,C2,Th,P2,q2,A2,On,O2,Ln,L2,In,lm,bs,Sn,$h,hl,I2,yh,S2,dm,tt,fl,N2,Dh,R2,W2,ul,Q2,Rc,U2,H2,K2,ml,V2,gl,J2,G2,X2,Fh,Y2,Z2,so,Eh,_l,e1,t1,Bh,bl,o1,s1,xh,vl,n1,r1,Mh,kl,a1,i1,Qt,wl,l1,vs,d1,zh,c1,p1,jh,h1,f1,u1,Nn,m1,Rn,cm,ks,Wn,Ch,Tl,g1,Ph,_1,pm,ot,$l,b1,yl,v1,qh,k1,w1,T1,Dl,$1,Wc,y1,D1,F1,Fl,E1,El,B1,x1,M1,Ah,z1,j1,no,Oh,Bl,C1,P1,Lh,xl,q1,A1,Ih,Ml,O1,L1,Sh,zl,I1,S1,Ut,jl,N1,ws,R1,Nh,W1,Q1,Rh,U1,H1,K1,Qn,V1,Un,hm,Ts,Hn,Wh,Cl,J1,Qh,G1,fm,st,Pl,X1,Uh,Y1,Z1,ql,e4,Qc,t4,o4,s4,Al,n4,Ol,r4,a4,i4,Hh,l4,d4,ro,Kh,Ll,c4,p4,Vh,Il,h4,f4,Jh,Sl,u4,m4,Gh,Nl,g4,_4,Ht,Rl,b4,$s,v4,Xh,k4,w4,Yh,T4,$4,y4,Kn,D4,Vn,um,ys,Jn,Zh,Wl,F4,ef,E4,mm,nt,Ql,B4,tf,x4,M4,Ul,z4,Uc,j4,C4,P4,Hl,q4,Kl,A4,O4,L4,of,I4,S4,ao,sf,Vl,N4,R4,nf,Jl,W4,Q4,rf,Gl,U4,H4,af,Xl,K4,V4,Kt,Yl,J4,Ds,G4,lf,X4,Y4,df,Z4,e0,t0,Gn,o0,Xn,gm,Fs,Yn,cf,Zl,s0,pf,n0,_m,rt,ed,r0,hf,a0,i0,td,l0,Hc,d0,c0,p0,od,h0,sd,f0,u0,m0,ff,g0,_0,io,uf,nd,b0,v0,mf,rd,k0,w0,gf,ad,T0,$0,_f,id,y0,D0,Vt,ld,F0,Es,E0,bf,B0,x0,vf,M0,z0,j0,Zn,C0,er,bm,Bs,tr,kf,dd,P0,wf,q0,vm,at,cd,A0,xs,O0,Tf,L0,I0,$f,S0,N0,R0,pd,W0,Kc,Q0,U0,H0,hd,K0,fd,V0,J0,G0,yf,X0,Y0,lo,Df,ud,Z0,eD,Ff,md,tD,oD,Ef,gd,sD,nD,Bf,_d,rD,aD,Jt,bd,iD,Ms,lD,xf,dD,cD,Mf,pD,hD,fD,or,uD,sr,km;return l=new Ne({}),E=new Ne({}),R=new Ne({}),Er=new Sg({props:{pipeline:"text-classification"}}),Vr=new Sg({props:{pipeline:"token-classification"}}),ta=new Sg({props:{pipeline:"fill-mask"}}),da=new Sg({props:{pipeline:"question-answering"}}),Aa=new Ne({}),Oa=new Y({props:{name:"class transformers.DistilBertConfig",anchor:"transformers.DistilBertConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"max_position_embeddings",val:" = 512"},{name:"sinusoidal_pos_embds",val:" = False"},{name:"n_layers",val:" = 6"},{name:"n_heads",val:" = 12"},{name:"dim",val:" = 768"},{name:"hidden_dim",val:" = 3072"},{name:"dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.1"},{name:"activation",val:" = 'gelu'"},{name:"initializer_range",val:" = 0.02"},{name:"qa_dropout",val:" = 0.1"},{name:"seq_classif_dropout",val:" = 0.2"},{name:"pad_token_id",val:" = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DistilBertConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the DistilBERT model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> or <a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a>.`,name:"vocab_size"},{anchor:"transformers.DistilBertConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.DistilBertConfig.sinusoidal_pos_embds",description:`<strong>sinusoidal_pos_embds</strong> (<code>boolean</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use sinusoidal positional embeddings.`,name:"sinusoidal_pos_embds"},{anchor:"transformers.DistilBertConfig.n_layers",description:`<strong>n_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"n_layers"},{anchor:"transformers.DistilBertConfig.n_heads",description:`<strong>n_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"n_heads"},{anchor:"transformers.DistilBertConfig.dim",description:`<strong>dim</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"dim"},{anchor:"transformers.DistilBertConfig.hidden_dim",description:`<strong>hidden_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
The size of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the Transformer encoder.`,name:"hidden_dim"},{anchor:"transformers.DistilBertConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.DistilBertConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.DistilBertConfig.activation",description:`<strong>activation</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation"},{anchor:"transformers.DistilBertConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DistilBertConfig.qa_dropout",description:`<strong>qa_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilities used in the question answering model <a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a>.`,name:"qa_dropout"},{anchor:"transformers.DistilBertConfig.seq_classif_dropout",description:`<strong>seq_classif_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.2) &#x2014;
The dropout probabilities used in the sequence classification and the multiple choice model
<a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a>.`,name:"seq_classif_dropout"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/configuration_distilbert.py#L45"}}),As=new fe({props:{anchor:"transformers.DistilBertConfig.example",$$slots:{default:[BM]},$$scope:{ctx:B}}}),Ia=new Ne({}),Sa=new Y({props:{name:"class transformers.DistilBertTokenizer",anchor:"transformers.DistilBertTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/tokenization_distilbert.py#L62"}}),Ra=new Ne({}),Wa=new Y({props:{name:"class transformers.DistilBertTokenizerFast",anchor:"transformers.DistilBertTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/tokenization_distilbert_fast.py#L79"}}),Ha=new Ne({}),Ka=new Y({props:{name:"class transformers.DistilBertModel",anchor:"transformers.DistilBertModel",parameters:[{name:"config",val:": PretrainedConfig"}],parametersDescription:[{anchor:"transformers.DistilBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_distilbert.py#L453"}}),Xa=new Y({props:{name:"forward",anchor:"transformers.DistilBertModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DistilBertModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_distilbert.py#L525",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Rs=new Se({props:{$$slots:{default:[xM]},$$scope:{ctx:B}}}),Ws=new fe({props:{anchor:"transformers.DistilBertModel.forward.example",$$slots:{default:[MM]},$$scope:{ctx:B}}}),Ya=new Ne({}),Za=new Y({props:{name:"class transformers.DistilBertForMaskedLM",anchor:"transformers.DistilBertForMaskedLM",parameters:[{name:"config",val:": PretrainedConfig"}],parametersDescription:[{anchor:"transformers.DistilBertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_distilbert.py#L581"}}),ni=new Y({props:{name:"forward",anchor:"transformers.DistilBertForMaskedLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DistilBertForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForMaskedLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForMaskedLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_distilbert.py#L623",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Us=new Se({props:{$$slots:{default:[zM]},$$scope:{ctx:B}}}),Hs=new fe({props:{anchor:"transformers.DistilBertForMaskedLM.forward.example",$$slots:{default:[jM]},$$scope:{ctx:B}}}),Ks=new fe({props:{anchor:"transformers.DistilBertForMaskedLM.forward.example-2",$$slots:{default:[CM]},$$scope:{ctx:B}}}),ri=new Ne({}),ai=new Y({props:{name:"class transformers.DistilBertForSequenceClassification",anchor:"transformers.DistilBertForSequenceClassification",parameters:[{name:"config",val:": PretrainedConfig"}],parametersDescription:[{anchor:"transformers.DistilBertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_distilbert.py#L687"}}),ci=new Y({props:{name:"forward",anchor:"transformers.DistilBertForSequenceClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DistilBertForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForSequenceClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_distilbert.py#L721",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Js=new Se({props:{$$slots:{default:[PM]},$$scope:{ctx:B}}}),Gs=new fe({props:{anchor:"transformers.DistilBertForSequenceClassification.forward.example",$$slots:{default:[qM]},$$scope:{ctx:B}}}),Xs=new fe({props:{anchor:"transformers.DistilBertForSequenceClassification.forward.example-2",$$slots:{default:[AM]},$$scope:{ctx:B}}}),Ys=new fe({props:{anchor:"transformers.DistilBertForSequenceClassification.forward.example-3",$$slots:{default:[OM]},$$scope:{ctx:B}}}),Zs=new fe({props:{anchor:"transformers.DistilBertForSequenceClassification.forward.example-4",$$slots:{default:[LM]},$$scope:{ctx:B}}}),pi=new Ne({}),hi=new Y({props:{name:"class transformers.DistilBertForMultipleChoice",anchor:"transformers.DistilBertForMultipleChoice",parameters:[{name:"config",val:": PretrainedConfig"}],parametersDescription:[{anchor:"transformers.DistilBertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_distilbert.py#L1017"}}),gi=new Y({props:{name:"forward",anchor:"transformers.DistilBertForMultipleChoice.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DistilBertForMultipleChoice.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForMultipleChoice.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForMultipleChoice.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForMultipleChoice.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForMultipleChoice.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForMultipleChoice.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForMultipleChoice.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForMultipleChoice.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices-1]</code> where <code>num_choices</code> is the size of the second dimension of the input tensors. (See
<code>input_ids</code> above)`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_distilbert.py#L1049",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),tn=new Se({props:{$$slots:{default:[IM]},$$scope:{ctx:B}}}),on=new fe({props:{anchor:"transformers.DistilBertForMultipleChoice.forward.example",$$slots:{default:[SM]},$$scope:{ctx:B}}}),_i=new Ne({}),bi=new Y({props:{name:"class transformers.DistilBertForTokenClassification",anchor:"transformers.DistilBertForTokenClassification",parameters:[{name:"config",val:": PretrainedConfig"}],parametersDescription:[{anchor:"transformers.DistilBertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_distilbert.py#L922"}}),Ti=new Y({props:{name:"forward",anchor:"transformers.DistilBertForTokenClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DistilBertForTokenClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>({0})</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForTokenClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>({0})</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForTokenClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForTokenClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>({0}, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForTokenClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForTokenClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForTokenClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForTokenClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_distilbert.py#L954",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),nn=new Se({props:{$$slots:{default:[NM]},$$scope:{ctx:B}}}),rn=new fe({props:{anchor:"transformers.DistilBertForTokenClassification.forward.example",$$slots:{default:[RM]},$$scope:{ctx:B}}}),an=new fe({props:{anchor:"transformers.DistilBertForTokenClassification.forward.example-2",$$slots:{default:[WM]},$$scope:{ctx:B}}}),$i=new Ne({}),yi=new Y({props:{name:"class transformers.DistilBertForQuestionAnswering",anchor:"transformers.DistilBertForQuestionAnswering",parameters:[{name:"config",val:": PretrainedConfig"}],parametersDescription:[{anchor:"transformers.DistilBertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_distilbert.py#L805"}}),Bi=new Y({props:{name:"forward",anchor:"transformers.DistilBertForQuestionAnswering.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"start_positions",val:": typing.Optional[torch.Tensor] = None"},{name:"end_positions",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DistilBertForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_distilbert.py#L837",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),dn=new Se({props:{$$slots:{default:[QM]},$$scope:{ctx:B}}}),cn=new fe({props:{anchor:"transformers.DistilBertForQuestionAnswering.forward.example",$$slots:{default:[UM]},$$scope:{ctx:B}}}),pn=new fe({props:{anchor:"transformers.DistilBertForQuestionAnswering.forward.example-2",$$slots:{default:[HM]},$$scope:{ctx:B}}}),xi=new Ne({}),Mi=new Y({props:{name:"class transformers.TFDistilBertModel",anchor:"transformers.TFDistilBertModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDistilBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_tf_distilbert.py#L537"}}),fn=new Se({props:{$$slots:{default:[KM]},$$scope:{ctx:B}}}),Pi=new Y({props:{name:"call",anchor:"transformers.TFDistilBertModel.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"training",val:": typing.Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFDistilBertModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertModel.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_tf_distilbert.py#L542",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),un=new Se({props:{$$slots:{default:[VM]},$$scope:{ctx:B}}}),mn=new fe({props:{anchor:"transformers.TFDistilBertModel.call.example",$$slots:{default:[JM]},$$scope:{ctx:B}}}),qi=new Ne({}),Ai=new Y({props:{name:"class transformers.TFDistilBertForMaskedLM",anchor:"transformers.TFDistilBertForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDistilBertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_tf_distilbert.py#L624"}}),_n=new Se({props:{$$slots:{default:[GM]},$$scope:{ctx:B}}}),Ni=new Y({props:{name:"call",anchor:"transformers.TFDistilBertForMaskedLM.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFDistilBertForMaskedLM.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForMaskedLM.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForMaskedLM.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForMaskedLM.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForMaskedLM.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForMaskedLM.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForMaskedLM.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForMaskedLM.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForMaskedLM.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_tf_distilbert.py#L644",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),bn=new Se({props:{$$slots:{default:[XM]},$$scope:{ctx:B}}}),vn=new fe({props:{anchor:"transformers.TFDistilBertForMaskedLM.call.example",$$slots:{default:[YM]},$$scope:{ctx:B}}}),kn=new fe({props:{anchor:"transformers.TFDistilBertForMaskedLM.call.example-2",$$slots:{default:[ZM]},$$scope:{ctx:B}}}),Ri=new Ne({}),Wi=new Y({props:{name:"class transformers.TFDistilBertForSequenceClassification",anchor:"transformers.TFDistilBertForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDistilBertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_tf_distilbert.py#L714"}}),Tn=new Se({props:{$$slots:{default:[e3]},$$scope:{ctx:B}}}),Ki=new Y({props:{name:"call",anchor:"transformers.TFDistilBertForSequenceClassification.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFDistilBertForSequenceClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_tf_distilbert.py#L731",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),$n=new Se({props:{$$slots:{default:[t3]},$$scope:{ctx:B}}}),yn=new fe({props:{anchor:"transformers.TFDistilBertForSequenceClassification.call.example",$$slots:{default:[o3]},$$scope:{ctx:B}}}),Dn=new fe({props:{anchor:"transformers.TFDistilBertForSequenceClassification.call.example-2",$$slots:{default:[s3]},$$scope:{ctx:B}}}),Vi=new Ne({}),Ji=new Y({props:{name:"class transformers.TFDistilBertForMultipleChoice",anchor:"transformers.TFDistilBertForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDistilBertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_tf_distilbert.py#L877"}}),En=new Se({props:{$$slots:{default:[n3]},$$scope:{ctx:B}}}),Zi=new Y({props:{name:"call",anchor:"transformers.TFDistilBertForMultipleChoice.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFDistilBertForMultipleChoice.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices]</code>
where <code>num_choices</code> is the size of the second dimension of the input tensors. (See <code>input_ids</code> above)`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_tf_distilbert.py#L903",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <em>(batch_size, )</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Bn=new Se({props:{$$slots:{default:[r3]},$$scope:{ctx:B}}}),xn=new fe({props:{anchor:"transformers.TFDistilBertForMultipleChoice.call.example",$$slots:{default:[a3]},$$scope:{ctx:B}}}),el=new Ne({}),tl=new Y({props:{name:"class transformers.TFDistilBertForTokenClassification",anchor:"transformers.TFDistilBertForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDistilBertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_tf_distilbert.py#L801"}}),zn=new Se({props:{$$slots:{default:[i3]},$$scope:{ctx:B}}}),rl=new Y({props:{name:"call",anchor:"transformers.TFDistilBertForTokenClassification.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFDistilBertForTokenClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForTokenClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForTokenClassification.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForTokenClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForTokenClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForTokenClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForTokenClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForTokenClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForTokenClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_tf_distilbert.py#L812",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of unmasked labels, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),jn=new Se({props:{$$slots:{default:[l3]},$$scope:{ctx:B}}}),Cn=new fe({props:{anchor:"transformers.TFDistilBertForTokenClassification.call.example",$$slots:{default:[d3]},$$scope:{ctx:B}}}),Pn=new fe({props:{anchor:"transformers.TFDistilBertForTokenClassification.call.example-2",$$slots:{default:[c3]},$$scope:{ctx:B}}}),al=new Ne({}),il=new Y({props:{name:"class transformers.TFDistilBertForQuestionAnswering",anchor:"transformers.TFDistilBertForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDistilBertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_tf_distilbert.py#L1002"}}),An=new Se({props:{$$slots:{default:[p3]},$$scope:{ctx:B}}}),pl=new Y({props:{name:"call",anchor:"transformers.TFDistilBertForQuestionAnswering.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"start_positions",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"end_positions",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFDistilBertForQuestionAnswering.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.start_positions",description:`<strong>start_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.end_positions",description:`<strong>end_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_tf_distilbert.py#L1013",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>start_positions</code> and <code>end_positions</code> are provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),On=new Se({props:{$$slots:{default:[h3]},$$scope:{ctx:B}}}),Ln=new fe({props:{anchor:"transformers.TFDistilBertForQuestionAnswering.call.example",$$slots:{default:[f3]},$$scope:{ctx:B}}}),In=new fe({props:{anchor:"transformers.TFDistilBertForQuestionAnswering.call.example-2",$$slots:{default:[u3]},$$scope:{ctx:B}}}),hl=new Ne({}),fl=new Y({props:{name:"class transformers.FlaxDistilBertModel",anchor:"transformers.FlaxDistilBertModel",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxDistilBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_flax_distilbert.py#L535"}}),wl=new Y({props:{name:"__call__",anchor:"transformers.FlaxDistilBertModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlaxDistilBertModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_flax_distilbert.py#L458"}}),Nn=new Se({props:{$$slots:{default:[m3]},$$scope:{ctx:B}}}),Rn=new fe({props:{anchor:"transformers.FlaxDistilBertModel.__call__.example",$$slots:{default:[g3]},$$scope:{ctx:B}}}),Tl=new Ne({}),$l=new Y({props:{name:"class transformers.FlaxDistilBertForMaskedLM",anchor:"transformers.FlaxDistilBertForMaskedLM",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxDistilBertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_flax_distilbert.py#L608"}}),jl=new Y({props:{name:"__call__",anchor:"transformers.FlaxDistilBertForMaskedLM.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_flax_distilbert.py#L458",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Qn=new Se({props:{$$slots:{default:[_3]},$$scope:{ctx:B}}}),Un=new fe({props:{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.example",$$slots:{default:[b3]},$$scope:{ctx:B}}}),Cl=new Ne({}),Pl=new Y({props:{name:"class transformers.FlaxDistilBertForSequenceClassification",anchor:"transformers.FlaxDistilBertForSequenceClassification",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxDistilBertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_flax_distilbert.py#L677"}}),Rl=new Y({props:{name:"__call__",anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_flax_distilbert.py#L458",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"
>transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"
>transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Kn=new Se({props:{$$slots:{default:[v3]},$$scope:{ctx:B}}}),Vn=new fe({props:{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.example",$$slots:{default:[k3]},$$scope:{ctx:B}}}),Wl=new Ne({}),Ql=new Y({props:{name:"class transformers.FlaxDistilBertForMultipleChoice",anchor:"transformers.FlaxDistilBertForMultipleChoice",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxDistilBertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_flax_distilbert.py#L757"}}),Yl=new Y({props:{name:"__call__",anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_flax_distilbert.py#L458",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput"
>transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput"
>transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Gn=new Se({props:{$$slots:{default:[w3]},$$scope:{ctx:B}}}),Xn=new fe({props:{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.example",$$slots:{default:[T3]},$$scope:{ctx:B}}}),Zl=new Ne({}),ed=new Y({props:{name:"class transformers.FlaxDistilBertForTokenClassification",anchor:"transformers.FlaxDistilBertForTokenClassification",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxDistilBertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_flax_distilbert.py#L823"}}),ld=new Y({props:{name:"__call__",anchor:"transformers.FlaxDistilBertForTokenClassification.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_flax_distilbert.py#L458",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_flax_outputs.FlaxTokenClassifierOutput"
>transformers.modeling_flax_outputs.FlaxTokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_flax_outputs.FlaxTokenClassifierOutput"
>transformers.modeling_flax_outputs.FlaxTokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Zn=new Se({props:{$$slots:{default:[$3]},$$scope:{ctx:B}}}),er=new fe({props:{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.example",$$slots:{default:[y3]},$$scope:{ctx:B}}}),dd=new Ne({}),cd=new Y({props:{name:"class transformers.FlaxDistilBertForQuestionAnswering",anchor:"transformers.FlaxDistilBertForQuestionAnswering",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxDistilBertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_flax_distilbert.py#L893"}}),bd=new Y({props:{name:"__call__",anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.24.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/distilbert/modeling_flax_distilbert.py#L458",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput"
>transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput"
>transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),or=new Se({props:{$$slots:{default:[D3]},$$scope:{ctx:B}}}),sr=new fe({props:{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.example",$$slots:{default:[F3]},$$scope:{ctx:B}}}),{c(){d=r("meta"),b=p(),f=r("h1"),m=r("a"),v=r("span"),w(l.$$.fragment),u=p(),M=r("span"),we=o("DistilBERT"),ge=p(),I=r("h2"),re=r("a"),oe=r("span"),w(E.$$.fragment),Te=p(),Q=r("span"),$e=o("Overview"),_e=p(),O=r("p"),ye=o("The DistilBERT model was proposed in the blog post "),ae=r("a"),H=o(`Smaller, faster, cheaper, lighter: Introducing DistilBERT, a
distilled version of BERT`),De=o(", and the paper "),ie=r("a"),K=o(`DistilBERT, a
distilled version of BERT: smaller, faster, cheaper and lighter`),Fe=o(`. DistilBERT is a
small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than
`),de=r("em"),V=o("bert-base-uncased"),Ee=o(`, runs 60% faster while preserving over 95% of BERT\u2019s performances as measured on the GLUE language
understanding benchmark.`),be=p(),ee=r("p"),j=o("The abstract from the paper is the following:"),q=p(),le=r("p"),U=r("em"),Be=o(`As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP),
operating these large models in on-the-edge and/or under constrained computational training or inference budgets
remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation
model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger
counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage
knowledge distillation during the pretraining phase and show that it is possible to reduce the size of a BERT model by
40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive
biases learned by larger models during pretraining, we introduce a triple loss combining language modeling,
distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we
demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device
study.`),ve=p(),W=r("p"),xe=o("Tips:"),ke=p(),C=r("ul"),se=r("li"),J=o("DistilBERT doesn\u2019t have "),ce=r("code"),Me=o("token_type_ids"),G=o(`, you don\u2019t need to indicate which token belongs to which segment. Just
separate your segments with the separation token `),pe=r("code"),ze=o("tokenizer.sep_token"),S=o(" (or "),he=r("code"),X=o("[SEP]"),je=o(")."),ne=p(),P=r("li"),Ce=o("DistilBERT doesn\u2019t have options to select the input positions ("),A=r("code"),Pe=o("position_ids"),qe=o(` input). This could be added if
necessary though, just let us know if you need this option.`),k=p(),x=r("p"),He=o("This model was contributed by "),Z=r("a"),Ke=o("victorsanh"),Ve=o(`. This model jax version was
contributed by `),z=r("a"),Je=o("kamalkraj"),Ge=o(". The original code can be found "),Le=r("a"),Xe=o("here"),Ye=o("."),L=p(),N=r("h2"),Oe=r("a"),Re=r("span"),w(R.$$.fragment),Ze=p(),We=r("span"),Ae=o("Resources"),Ue=p(),te=r("p"),et=o("A list of official Hugging Face and community (indicated by \u{1F30E}) resources to help you get started with DistilBERT. If you\u2019re interested in submitting a resource to be included here, please feel free to open a Pull Request and we\u2019ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource."),gu=p(),w(Er.$$.fragment),_u=p(),Ie=r("ul"),Br=r("li"),Ng=o("A blog post on "),xr=r("a"),Rg=o("Getting Started with Sentiment Analysis using Python"),Wg=o(" with DistilBERT."),Qg=p(),Mr=r("li"),Ug=o("A blog post on how to "),zr=r("a"),Hg=o("train DistilBERT with Blurr for sequence classification"),Kg=o("."),Vg=p(),jr=r("li"),Jg=o("A blog post on how to use "),Cr=r("a"),Gg=o("Ray to tune DistilBERT hyperparameters"),Xg=o("."),Yg=p(),Pr=r("li"),Zg=o("A blog post on how to "),qr=r("a"),e_=o("train DistilBERT with Hugging Face and Amazon SageMaker"),t_=o("."),o_=p(),Ar=r("li"),s_=o("A notebook on how to "),Or=r("a"),n_=o("finetune DistilBERT for multi-label classification"),r_=o(". \u{1F30E}"),a_=p(),Lr=r("li"),i_=o("A notebook on how to "),Ir=r("a"),l_=o("finetune DistilBERT for multiclass classification with PyTorch"),d_=o(". \u{1F30E}"),c_=p(),Sr=r("li"),p_=o("A notebook on how to "),Nr=r("a"),h_=o("finetune DistilBERT for text classification in TensorFlow"),f_=o(". \u{1F30E}"),u_=p(),fo=r("li"),Nd=r("a"),m_=o("DistilBertForSequenceClassification"),g_=o(" is supported by this "),Rr=r("a"),__=o("example script"),b_=o(" and "),Wr=r("a"),v_=o("notebook"),k_=o("."),w_=p(),uo=r("li"),Rd=r("a"),T_=o("TFDistilBertForSequenceClassification"),$_=o(" is supported by this "),Qr=r("a"),y_=o("example script"),D_=o(" and "),Ur=r("a"),F_=o("notebook"),E_=o("."),B_=p(),mo=r("li"),Wd=r("a"),x_=o("FlaxDistilBertForSequenceClassification"),M_=o(" is supported by this "),Hr=r("a"),z_=o("example script"),j_=o(" and "),Kr=r("a"),C_=o("notebook"),P_=o("."),bu=p(),w(Vr.$$.fragment),vu=p(),Ot=r("ul"),go=r("li"),Qd=r("a"),q_=o("DistilBertForTokenClassification"),A_=o(" is supported by this "),Jr=r("a"),O_=o("example script"),L_=o(" and "),Gr=r("a"),I_=o("notebook"),S_=o("."),N_=p(),_o=r("li"),Ud=r("a"),R_=o("TFDistilBertForTokenClassification"),W_=o(" is supported by this "),Xr=r("a"),Q_=o("example script"),U_=o(" and "),Yr=r("a"),H_=o("notebook"),K_=o("."),V_=p(),zs=r("li"),Hd=r("a"),J_=o("FlaxDistilBertForTokenClassification"),G_=o(" is supported by this "),Zr=r("a"),X_=o("example script"),Y_=o("."),Z_=p(),Kd=r("li"),ea=r("a"),eb=o("Token classification"),tb=o(" chapter of the \u{1F917} Hugging Face Course."),ku=p(),w(ta.$$.fragment),wu=p(),Lt=r("ul"),bo=r("li"),Vd=r("a"),ob=o("DistilBertForMaskedLM"),sb=o(" is supported by this "),oa=r("a"),nb=o("example script"),rb=o(" and "),sa=r("a"),ab=o("notebook"),ib=o("."),lb=p(),vo=r("li"),Jd=r("a"),db=o("TFDistilBertForMaskedLM"),cb=o(" is supported by this "),na=r("a"),pb=o("example script"),hb=o(" and "),ra=r("a"),fb=o("notebook"),ub=o("."),mb=p(),ko=r("li"),Gd=r("a"),gb=o("FlaxDistilBertForMaskedLM"),_b=o(" is supported by this "),aa=r("a"),bb=o("example script"),vb=o(" and "),ia=r("a"),kb=o("notebook"),wb=o("."),Tb=p(),Xd=r("li"),la=r("a"),$b=o("Masked language modeling"),yb=o(" chapter of the \u{1F917} Hugging Face Course."),Tu=p(),w(da.$$.fragment),$u=p(),It=r("ul"),wo=r("li"),Yd=r("a"),Db=o("DistilBertForQuestionAnswering"),Fb=o(" is supported by this "),ca=r("a"),Eb=o("example script"),Bb=o(" and "),pa=r("a"),xb=o("notebook"),Mb=o("."),zb=p(),To=r("li"),Zd=r("a"),jb=o("TFDistilBertForQuestionAnswering"),Cb=o(" is supported by this "),ha=r("a"),Pb=o("example script"),qb=o(" and "),fa=r("a"),Ab=o("notebook"),Ob=o("."),Lb=p(),js=r("li"),ec=r("a"),Ib=o("FlaxDistilBertForQuestionAnswering"),Sb=o(" is supported by this "),ua=r("a"),Nb=o("example script"),Rb=o("."),Wb=p(),tc=r("li"),ma=r("a"),Qb=o("Question answering"),Ub=o(" chapter of the \u{1F917} Hugging Face Course."),yu=p(),oc=r("p"),bp=r("strong"),Hb=o("Multiple choice"),Du=p(),Cs=r("ul"),$o=r("li"),sc=r("a"),Kb=o("DistilBertForMultipleChoice"),Vb=o(" is supported by this "),ga=r("a"),Jb=o("example script"),Gb=o(" and "),_a=r("a"),Xb=o("notebook"),Yb=o("."),Zb=p(),yo=r("li"),nc=r("a"),ev=o("TFDistilBertForMultipleChoice"),tv=o(" is supported by this "),ba=r("a"),ov=o("example script"),sv=o(" and "),va=r("a"),nv=o("notebook"),rv=o("."),Fu=p(),rc=r("p"),av=o("\u2697\uFE0F Optimization"),Eu=p(),Do=r("ul"),ka=r("li"),iv=o("A blog post on how to "),wa=r("a"),lv=o("quantize DistilBERT with \u{1F917} Optimum and Intel"),dv=o("."),cv=p(),Ta=r("li"),pv=o("A blog post on how "),$a=r("a"),hv=o("Optimizing Transformers for GPUs with \u{1F917} Optimum"),fv=o("."),uv=p(),ya=r("li"),mv=o("A blog post on "),Da=r("a"),gv=o("Optimizing Transformers with Hugging Face Optimum"),_v=o("."),Bu=p(),ac=r("p"),bv=o("\u26A1\uFE0F Inference"),xu=p(),Ps=r("ul"),Fa=r("li"),vv=o("A blog post on how to "),Ea=r("a"),kv=o("Accelerate BERT inference with Hugging Face Transformers and AWS Inferentia"),wv=o(" with DistilBERT."),Tv=p(),Ba=r("li"),$v=o("A blog post on "),xa=r("a"),yv=o("Serverless Inference with Hugging Face\u2019s Transformers, DistilBERT and Amazon SageMaker"),Dv=o("."),Mu=p(),ic=r("p"),Fv=o("\u{1F680} Deploy"),zu=p(),Fo=r("ul"),Ma=r("li"),Ev=o("A blog post on how to "),za=r("a"),Bv=o("deploy DistilBERT on Google Cloud"),xv=o("."),Mv=p(),ja=r("li"),zv=o("A blog post on how to "),Ca=r("a"),jv=o("deploy DistilBERT with Amazon SageMaker"),Cv=o("."),Pv=p(),Pa=r("li"),qv=o("A blog post on how to "),qa=r("a"),Av=o("Deploy BERT with Hugging Face Transformers, Amazon SageMaker and Terraform module"),Ov=o("."),ju=p(),Ro=r("h2"),qs=r("a"),vp=r("span"),w(Aa.$$.fragment),Lv=p(),kp=r("span"),Iv=o("DistilBertConfig"),Cu=p(),Pt=r("div"),w(Oa.$$.fragment),Sv=p(),oo=r("p"),Nv=o("This is the configuration class to store the configuration of a "),lc=r("a"),Rv=o("DistilBertModel"),Wv=o(" or a "),dc=r("a"),Qv=o("TFDistilBertModel"),Uv=o(`. It
is used to instantiate a DistilBERT model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the DistilBERT
`),La=r("a"),Hv=o("distilbert-base-uncased"),Kv=o(" architecture."),Vv=p(),Wo=r("p"),Jv=o("Configuration objects inherit from "),cc=r("a"),Gv=o("PretrainedConfig"),Xv=o(` and can be used to control the model outputs. Read the
documentation from `),pc=r("a"),Yv=o("PretrainedConfig"),Zv=o(" for more information."),ek=p(),w(As.$$.fragment),Pu=p(),Qo=r("h2"),Os=r("a"),wp=r("span"),w(Ia.$$.fragment),tk=p(),Tp=r("span"),ok=o("DistilBertTokenizer"),qu=p(),qt=r("div"),w(Sa.$$.fragment),sk=p(),$p=r("p"),nk=o("Construct a DistilBERT tokenizer."),rk=p(),Ls=r("p"),hc=r("a"),ak=o("DistilBertTokenizer"),ik=o(" is identical to "),fc=r("a"),lk=o("BertTokenizer"),dk=o(` and runs end-to-end tokenization: punctuation splitting
and wordpiece.`),ck=p(),Na=r("p"),pk=o("Refer to superclass "),uc=r("a"),hk=o("BertTokenizer"),fk=o(" for usage examples and documentation concerning parameters."),Au=p(),Uo=r("h2"),Is=r("a"),yp=r("span"),w(Ra.$$.fragment),uk=p(),Dp=r("span"),mk=o("DistilBertTokenizerFast"),Ou=p(),At=r("div"),w(Wa.$$.fragment),gk=p(),Qa=r("p"),_k=o("Construct a \u201Cfast\u201D DistilBERT tokenizer (backed by HuggingFace\u2019s "),Fp=r("em"),bk=o("tokenizers"),vk=o(" library)."),kk=p(),Ss=r("p"),mc=r("a"),wk=o("DistilBertTokenizerFast"),Tk=o(" is identical to "),gc=r("a"),$k=o("BertTokenizerFast"),yk=o(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Dk=p(),Ua=r("p"),Fk=o("Refer to superclass "),_c=r("a"),Ek=o("BertTokenizerFast"),Bk=o(" for usage examples and documentation concerning parameters."),Lu=p(),Ho=r("h2"),Ns=r("a"),Ep=r("span"),w(Ha.$$.fragment),xk=p(),Bp=r("span"),Mk=o("DistilBertModel"),Iu=p(),ut=r("div"),w(Ka.$$.fragment),zk=p(),xp=r("p"),jk=o("The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),Ck=p(),Va=r("p"),Pk=o("This model inherits from "),bc=r("a"),qk=o("PreTrainedModel"),Ak=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ok=p(),Ja=r("p"),Lk=o("This model is also a PyTorch "),Ga=r("a"),Ik=o("torch.nn.Module"),Sk=o(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Nk=p(),St=r("div"),w(Xa.$$.fragment),Rk=p(),Ko=r("p"),Wk=o("The "),vc=r("a"),Qk=o("DistilBertModel"),Uk=o(" forward method, overrides the "),Mp=r("code"),Hk=o("__call__"),Kk=o(" special method."),Vk=p(),w(Rs.$$.fragment),Jk=p(),w(Ws.$$.fragment),Su=p(),Vo=r("h2"),Qs=r("a"),zp=r("span"),w(Ya.$$.fragment),Gk=p(),jp=r("span"),Xk=o("DistilBertForMaskedLM"),Nu=p(),mt=r("div"),w(Za.$$.fragment),Yk=p(),ei=r("p"),Zk=o("DistilBert Model with a "),Cp=r("code"),ew=o("masked language modeling"),tw=o(" head on top."),ow=p(),ti=r("p"),sw=o("This model inherits from "),kc=r("a"),nw=o("PreTrainedModel"),rw=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),aw=p(),oi=r("p"),iw=o("This model is also a PyTorch "),si=r("a"),lw=o("torch.nn.Module"),dw=o(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),cw=p(),kt=r("div"),w(ni.$$.fragment),pw=p(),Jo=r("p"),hw=o("The "),wc=r("a"),fw=o("DistilBertForMaskedLM"),uw=o(" forward method, overrides the "),Pp=r("code"),mw=o("__call__"),gw=o(" special method."),_w=p(),w(Us.$$.fragment),bw=p(),w(Hs.$$.fragment),vw=p(),w(Ks.$$.fragment),Ru=p(),Go=r("h2"),Vs=r("a"),qp=r("span"),w(ri.$$.fragment),kw=p(),Ap=r("span"),ww=o("DistilBertForSequenceClassification"),Wu=p(),gt=r("div"),w(ai.$$.fragment),Tw=p(),Op=r("p"),$w=o(`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),yw=p(),ii=r("p"),Dw=o("This model inherits from "),Tc=r("a"),Fw=o("PreTrainedModel"),Ew=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Bw=p(),li=r("p"),xw=o("This model is also a PyTorch "),di=r("a"),Mw=o("torch.nn.Module"),zw=o(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),jw=p(),it=r("div"),w(ci.$$.fragment),Cw=p(),Xo=r("p"),Pw=o("The "),$c=r("a"),qw=o("DistilBertForSequenceClassification"),Aw=o(" forward method, overrides the "),Lp=r("code"),Ow=o("__call__"),Lw=o(" special method."),Iw=p(),w(Js.$$.fragment),Sw=p(),w(Gs.$$.fragment),Nw=p(),w(Xs.$$.fragment),Rw=p(),w(Ys.$$.fragment),Ww=p(),w(Zs.$$.fragment),Qu=p(),Yo=r("h2"),en=r("a"),Ip=r("span"),w(pi.$$.fragment),Qw=p(),Sp=r("span"),Uw=o("DistilBertForMultipleChoice"),Uu=p(),_t=r("div"),w(hi.$$.fragment),Hw=p(),Np=r("p"),Kw=o(`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),Vw=p(),fi=r("p"),Jw=o("This model inherits from "),yc=r("a"),Gw=o("PreTrainedModel"),Xw=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Yw=p(),ui=r("p"),Zw=o("This model is also a PyTorch "),mi=r("a"),eT=o("torch.nn.Module"),tT=o(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),oT=p(),Nt=r("div"),w(gi.$$.fragment),sT=p(),Zo=r("p"),nT=o("The "),Dc=r("a"),rT=o("DistilBertForMultipleChoice"),aT=o(" forward method, overrides the "),Rp=r("code"),iT=o("__call__"),lT=o(" special method."),dT=p(),w(tn.$$.fragment),cT=p(),w(on.$$.fragment),Hu=p(),es=r("h2"),sn=r("a"),Wp=r("span"),w(_i.$$.fragment),pT=p(),Qp=r("span"),hT=o("DistilBertForTokenClassification"),Ku=p(),bt=r("div"),w(bi.$$.fragment),fT=p(),Up=r("p"),uT=o(`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),mT=p(),vi=r("p"),gT=o("This model inherits from "),Fc=r("a"),_T=o("PreTrainedModel"),bT=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),vT=p(),ki=r("p"),kT=o("This model is also a PyTorch "),wi=r("a"),wT=o("torch.nn.Module"),TT=o(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),$T=p(),wt=r("div"),w(Ti.$$.fragment),yT=p(),ts=r("p"),DT=o("The "),Ec=r("a"),FT=o("DistilBertForTokenClassification"),ET=o(" forward method, overrides the "),Hp=r("code"),BT=o("__call__"),xT=o(" special method."),MT=p(),w(nn.$$.fragment),zT=p(),w(rn.$$.fragment),jT=p(),w(an.$$.fragment),Vu=p(),os=r("h2"),ln=r("a"),Kp=r("span"),w($i.$$.fragment),CT=p(),Vp=r("span"),PT=o("DistilBertForQuestionAnswering"),Ju=p(),vt=r("div"),w(yi.$$.fragment),qT=p(),ss=r("p"),AT=o(`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),Jp=r("code"),OT=o("span start logits"),LT=o(" and "),Gp=r("code"),IT=o("span end logits"),ST=o(")."),NT=p(),Di=r("p"),RT=o("This model inherits from "),Bc=r("a"),WT=o("PreTrainedModel"),QT=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),UT=p(),Fi=r("p"),HT=o("This model is also a PyTorch "),Ei=r("a"),KT=o("torch.nn.Module"),VT=o(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),JT=p(),Tt=r("div"),w(Bi.$$.fragment),GT=p(),ns=r("p"),XT=o("The "),xc=r("a"),YT=o("DistilBertForQuestionAnswering"),ZT=o(" forward method, overrides the "),Xp=r("code"),e$=o("__call__"),t$=o(" special method."),o$=p(),w(dn.$$.fragment),s$=p(),w(cn.$$.fragment),n$=p(),w(pn.$$.fragment),Gu=p(),rs=r("h2"),hn=r("a"),Yp=r("span"),w(xi.$$.fragment),r$=p(),Zp=r("span"),a$=o("TFDistilBertModel"),Xu=p(),lt=r("div"),w(Mi.$$.fragment),i$=p(),eh=r("p"),l$=o("The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),d$=p(),zi=r("p"),c$=o("This model inherits from "),Mc=r("a"),p$=o("TFPreTrainedModel"),h$=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),f$=p(),ji=r("p"),u$=o("This model is also a "),Ci=r("a"),m$=o("tf.keras.Model"),g$=o(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),_$=p(),w(fn.$$.fragment),b$=p(),Rt=r("div"),w(Pi.$$.fragment),v$=p(),as=r("p"),k$=o("The "),zc=r("a"),w$=o("TFDistilBertModel"),T$=o(" forward method, overrides the "),th=r("code"),$$=o("__call__"),y$=o(" special method."),D$=p(),w(un.$$.fragment),F$=p(),w(mn.$$.fragment),Yu=p(),is=r("h2"),gn=r("a"),oh=r("span"),w(qi.$$.fragment),E$=p(),sh=r("span"),B$=o("TFDistilBertForMaskedLM"),Zu=p(),dt=r("div"),w(Ai.$$.fragment),x$=p(),Oi=r("p"),M$=o("DistilBert Model with a "),nh=r("code"),z$=o("masked language modeling"),j$=o(" head on top."),C$=p(),Li=r("p"),P$=o("This model inherits from "),jc=r("a"),q$=o("TFPreTrainedModel"),A$=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),O$=p(),Ii=r("p"),L$=o("This model is also a "),Si=r("a"),I$=o("tf.keras.Model"),S$=o(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),N$=p(),w(_n.$$.fragment),R$=p(),$t=r("div"),w(Ni.$$.fragment),W$=p(),ls=r("p"),Q$=o("The "),Cc=r("a"),U$=o("TFDistilBertForMaskedLM"),H$=o(" forward method, overrides the "),rh=r("code"),K$=o("__call__"),V$=o(" special method."),J$=p(),w(bn.$$.fragment),G$=p(),w(vn.$$.fragment),X$=p(),w(kn.$$.fragment),em=p(),ds=r("h2"),wn=r("a"),ah=r("span"),w(Ri.$$.fragment),Y$=p(),ih=r("span"),Z$=o("TFDistilBertForSequenceClassification"),tm=p(),ct=r("div"),w(Wi.$$.fragment),ey=p(),lh=r("p"),ty=o(`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),oy=p(),Qi=r("p"),sy=o("This model inherits from "),Pc=r("a"),ny=o("TFPreTrainedModel"),ry=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ay=p(),Ui=r("p"),iy=o("This model is also a "),Hi=r("a"),ly=o("tf.keras.Model"),dy=o(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),cy=p(),w(Tn.$$.fragment),py=p(),yt=r("div"),w(Ki.$$.fragment),hy=p(),cs=r("p"),fy=o("The "),qc=r("a"),uy=o("TFDistilBertForSequenceClassification"),my=o(" forward method, overrides the "),dh=r("code"),gy=o("__call__"),_y=o(" special method."),by=p(),w($n.$$.fragment),vy=p(),w(yn.$$.fragment),ky=p(),w(Dn.$$.fragment),om=p(),ps=r("h2"),Fn=r("a"),ch=r("span"),w(Vi.$$.fragment),wy=p(),ph=r("span"),Ty=o("TFDistilBertForMultipleChoice"),sm=p(),pt=r("div"),w(Ji.$$.fragment),$y=p(),hh=r("p"),yy=o(`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),Dy=p(),Gi=r("p"),Fy=o("This model inherits from "),Ac=r("a"),Ey=o("TFPreTrainedModel"),By=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),xy=p(),Xi=r("p"),My=o("This model is also a "),Yi=r("a"),zy=o("tf.keras.Model"),jy=o(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Cy=p(),w(En.$$.fragment),Py=p(),Wt=r("div"),w(Zi.$$.fragment),qy=p(),hs=r("p"),Ay=o("The "),Oc=r("a"),Oy=o("TFDistilBertForMultipleChoice"),Ly=o(" forward method, overrides the "),fh=r("code"),Iy=o("__call__"),Sy=o(" special method."),Ny=p(),w(Bn.$$.fragment),Ry=p(),w(xn.$$.fragment),nm=p(),fs=r("h2"),Mn=r("a"),uh=r("span"),w(el.$$.fragment),Wy=p(),mh=r("span"),Qy=o("TFDistilBertForTokenClassification"),rm=p(),ht=r("div"),w(tl.$$.fragment),Uy=p(),gh=r("p"),Hy=o(`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),Ky=p(),ol=r("p"),Vy=o("This model inherits from "),Lc=r("a"),Jy=o("TFPreTrainedModel"),Gy=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Xy=p(),sl=r("p"),Yy=o("This model is also a "),nl=r("a"),Zy=o("tf.keras.Model"),e2=o(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),t2=p(),w(zn.$$.fragment),o2=p(),Dt=r("div"),w(rl.$$.fragment),s2=p(),us=r("p"),n2=o("The "),Ic=r("a"),r2=o("TFDistilBertForTokenClassification"),a2=o(" forward method, overrides the "),_h=r("code"),i2=o("__call__"),l2=o(" special method."),d2=p(),w(jn.$$.fragment),c2=p(),w(Cn.$$.fragment),p2=p(),w(Pn.$$.fragment),am=p(),ms=r("h2"),qn=r("a"),bh=r("span"),w(al.$$.fragment),h2=p(),vh=r("span"),f2=o("TFDistilBertForQuestionAnswering"),im=p(),ft=r("div"),w(il.$$.fragment),u2=p(),gs=r("p"),m2=o(`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layer on top of the hidden-states output to compute `),kh=r("code"),g2=o("span start logits"),_2=o(" and "),wh=r("code"),b2=o("span end logits"),v2=o(")."),k2=p(),ll=r("p"),w2=o("This model inherits from "),Sc=r("a"),T2=o("TFPreTrainedModel"),$2=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),y2=p(),dl=r("p"),D2=o("This model is also a "),cl=r("a"),F2=o("tf.keras.Model"),E2=o(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),B2=p(),w(An.$$.fragment),x2=p(),Ft=r("div"),w(pl.$$.fragment),M2=p(),_s=r("p"),z2=o("The "),Nc=r("a"),j2=o("TFDistilBertForQuestionAnswering"),C2=o(" forward method, overrides the "),Th=r("code"),P2=o("__call__"),q2=o(" special method."),A2=p(),w(On.$$.fragment),O2=p(),w(Ln.$$.fragment),L2=p(),w(In.$$.fragment),lm=p(),bs=r("h2"),Sn=r("a"),$h=r("span"),w(hl.$$.fragment),I2=p(),yh=r("span"),S2=o("FlaxDistilBertModel"),dm=p(),tt=r("div"),w(fl.$$.fragment),N2=p(),Dh=r("p"),R2=o("The bare DistilBert Model transformer outputting raw hidden-states without any specific head on top."),W2=p(),ul=r("p"),Q2=o("This model inherits from "),Rc=r("a"),U2=o("FlaxPreTrainedModel"),H2=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),K2=p(),ml=r("p"),V2=o("This model is also a Flax Linen "),gl=r("a"),J2=o("flax.linen.Module"),G2=o(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),X2=p(),Fh=r("p"),Y2=o("Finally, this model supports inherent JAX features such as:"),Z2=p(),so=r("ul"),Eh=r("li"),_l=r("a"),e1=o("Just-In-Time (JIT) compilation"),t1=p(),Bh=r("li"),bl=r("a"),o1=o("Automatic Differentiation"),s1=p(),xh=r("li"),vl=r("a"),n1=o("Vectorization"),r1=p(),Mh=r("li"),kl=r("a"),a1=o("Parallelization"),i1=p(),Qt=r("div"),w(wl.$$.fragment),l1=p(),vs=r("p"),d1=o("The "),zh=r("code"),c1=o("FlaxDistilBertPreTrainedModel"),p1=o(" forward method, overrides the "),jh=r("code"),h1=o("__call__"),f1=o(" special method."),u1=p(),w(Nn.$$.fragment),m1=p(),w(Rn.$$.fragment),cm=p(),ks=r("h2"),Wn=r("a"),Ch=r("span"),w(Tl.$$.fragment),g1=p(),Ph=r("span"),_1=o("FlaxDistilBertForMaskedLM"),pm=p(),ot=r("div"),w($l.$$.fragment),b1=p(),yl=r("p"),v1=o("DistilBert Model with a "),qh=r("code"),k1=o("language modeling"),w1=o(" head on top."),T1=p(),Dl=r("p"),$1=o("This model inherits from "),Wc=r("a"),y1=o("FlaxPreTrainedModel"),D1=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),F1=p(),Fl=r("p"),E1=o("This model is also a Flax Linen "),El=r("a"),B1=o("flax.linen.Module"),x1=o(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),M1=p(),Ah=r("p"),z1=o("Finally, this model supports inherent JAX features such as:"),j1=p(),no=r("ul"),Oh=r("li"),Bl=r("a"),C1=o("Just-In-Time (JIT) compilation"),P1=p(),Lh=r("li"),xl=r("a"),q1=o("Automatic Differentiation"),A1=p(),Ih=r("li"),Ml=r("a"),O1=o("Vectorization"),L1=p(),Sh=r("li"),zl=r("a"),I1=o("Parallelization"),S1=p(),Ut=r("div"),w(jl.$$.fragment),N1=p(),ws=r("p"),R1=o("The "),Nh=r("code"),W1=o("FlaxDistilBertPreTrainedModel"),Q1=o(" forward method, overrides the "),Rh=r("code"),U1=o("__call__"),H1=o(" special method."),K1=p(),w(Qn.$$.fragment),V1=p(),w(Un.$$.fragment),hm=p(),Ts=r("h2"),Hn=r("a"),Wh=r("span"),w(Cl.$$.fragment),J1=p(),Qh=r("span"),G1=o("FlaxDistilBertForSequenceClassification"),fm=p(),st=r("div"),w(Pl.$$.fragment),X1=p(),Uh=r("p"),Y1=o(`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),Z1=p(),ql=r("p"),e4=o("This model inherits from "),Qc=r("a"),t4=o("FlaxPreTrainedModel"),o4=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),s4=p(),Al=r("p"),n4=o("This model is also a Flax Linen "),Ol=r("a"),r4=o("flax.linen.Module"),a4=o(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),i4=p(),Hh=r("p"),l4=o("Finally, this model supports inherent JAX features such as:"),d4=p(),ro=r("ul"),Kh=r("li"),Ll=r("a"),c4=o("Just-In-Time (JIT) compilation"),p4=p(),Vh=r("li"),Il=r("a"),h4=o("Automatic Differentiation"),f4=p(),Jh=r("li"),Sl=r("a"),u4=o("Vectorization"),m4=p(),Gh=r("li"),Nl=r("a"),g4=o("Parallelization"),_4=p(),Ht=r("div"),w(Rl.$$.fragment),b4=p(),$s=r("p"),v4=o("The "),Xh=r("code"),k4=o("FlaxDistilBertPreTrainedModel"),w4=o(" forward method, overrides the "),Yh=r("code"),T4=o("__call__"),$4=o(" special method."),y4=p(),w(Kn.$$.fragment),D4=p(),w(Vn.$$.fragment),um=p(),ys=r("h2"),Jn=r("a"),Zh=r("span"),w(Wl.$$.fragment),F4=p(),ef=r("span"),E4=o("FlaxDistilBertForMultipleChoice"),mm=p(),nt=r("div"),w(Ql.$$.fragment),B4=p(),tf=r("p"),x4=o(`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),M4=p(),Ul=r("p"),z4=o("This model inherits from "),Uc=r("a"),j4=o("FlaxPreTrainedModel"),C4=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),P4=p(),Hl=r("p"),q4=o("This model is also a Flax Linen "),Kl=r("a"),A4=o("flax.linen.Module"),O4=o(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),L4=p(),of=r("p"),I4=o("Finally, this model supports inherent JAX features such as:"),S4=p(),ao=r("ul"),sf=r("li"),Vl=r("a"),N4=o("Just-In-Time (JIT) compilation"),R4=p(),nf=r("li"),Jl=r("a"),W4=o("Automatic Differentiation"),Q4=p(),rf=r("li"),Gl=r("a"),U4=o("Vectorization"),H4=p(),af=r("li"),Xl=r("a"),K4=o("Parallelization"),V4=p(),Kt=r("div"),w(Yl.$$.fragment),J4=p(),Ds=r("p"),G4=o("The "),lf=r("code"),X4=o("FlaxDistilBertPreTrainedModel"),Y4=o(" forward method, overrides the "),df=r("code"),Z4=o("__call__"),e0=o(" special method."),t0=p(),w(Gn.$$.fragment),o0=p(),w(Xn.$$.fragment),gm=p(),Fs=r("h2"),Yn=r("a"),cf=r("span"),w(Zl.$$.fragment),s0=p(),pf=r("span"),n0=o("FlaxDistilBertForTokenClassification"),_m=p(),rt=r("div"),w(ed.$$.fragment),r0=p(),hf=r("p"),a0=o(`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),i0=p(),td=r("p"),l0=o("This model inherits from "),Hc=r("a"),d0=o("FlaxPreTrainedModel"),c0=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),p0=p(),od=r("p"),h0=o("This model is also a Flax Linen "),sd=r("a"),f0=o("flax.linen.Module"),u0=o(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),m0=p(),ff=r("p"),g0=o("Finally, this model supports inherent JAX features such as:"),_0=p(),io=r("ul"),uf=r("li"),nd=r("a"),b0=o("Just-In-Time (JIT) compilation"),v0=p(),mf=r("li"),rd=r("a"),k0=o("Automatic Differentiation"),w0=p(),gf=r("li"),ad=r("a"),T0=o("Vectorization"),$0=p(),_f=r("li"),id=r("a"),y0=o("Parallelization"),D0=p(),Vt=r("div"),w(ld.$$.fragment),F0=p(),Es=r("p"),E0=o("The "),bf=r("code"),B0=o("FlaxDistilBertPreTrainedModel"),x0=o(" forward method, overrides the "),vf=r("code"),M0=o("__call__"),z0=o(" special method."),j0=p(),w(Zn.$$.fragment),C0=p(),w(er.$$.fragment),bm=p(),Bs=r("h2"),tr=r("a"),kf=r("span"),w(dd.$$.fragment),P0=p(),wf=r("span"),q0=o("FlaxDistilBertForQuestionAnswering"),vm=p(),at=r("div"),w(cd.$$.fragment),A0=p(),xs=r("p"),O0=o(`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),Tf=r("code"),L0=o("span start logits"),I0=o(" and "),$f=r("code"),S0=o("span end logits"),N0=o(")."),R0=p(),pd=r("p"),W0=o("This model inherits from "),Kc=r("a"),Q0=o("FlaxPreTrainedModel"),U0=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),H0=p(),hd=r("p"),K0=o("This model is also a Flax Linen "),fd=r("a"),V0=o("flax.linen.Module"),J0=o(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),G0=p(),yf=r("p"),X0=o("Finally, this model supports inherent JAX features such as:"),Y0=p(),lo=r("ul"),Df=r("li"),ud=r("a"),Z0=o("Just-In-Time (JIT) compilation"),eD=p(),Ff=r("li"),md=r("a"),tD=o("Automatic Differentiation"),oD=p(),Ef=r("li"),gd=r("a"),sD=o("Vectorization"),nD=p(),Bf=r("li"),_d=r("a"),rD=o("Parallelization"),aD=p(),Jt=r("div"),w(bd.$$.fragment),iD=p(),Ms=r("p"),lD=o("The "),xf=r("code"),dD=o("FlaxDistilBertPreTrainedModel"),cD=o(" forward method, overrides the "),Mf=r("code"),pD=o("__call__"),hD=o(" special method."),fD=p(),w(or.$$.fragment),uD=p(),w(sr.$$.fragment),this.h()},l(n){const g=FM('[data-svelte="svelte-1phssyn"]',document.head);d=a(g,"META",{name:!0,content:!0}),g.forEach(t),b=h(n),f=a(n,"H1",{class:!0});var vd=i(f);m=a(vd,"A",{id:!0,class:!0,href:!0});var zf=i(m);v=a(zf,"SPAN",{});var jf=i(v);T(l.$$.fragment,jf),jf.forEach(t),zf.forEach(t),u=h(vd),M=a(vd,"SPAN",{});var Cf=i(M);we=s(Cf,"DistilBERT"),Cf.forEach(t),vd.forEach(t),ge=h(n),I=a(n,"H2",{class:!0});var kd=i(I);re=a(kd,"A",{id:!0,class:!0,href:!0});var Pf=i(re);oe=a(Pf,"SPAN",{});var qf=i(oe);T(E.$$.fragment,qf),qf.forEach(t),Pf.forEach(t),Te=h(kd),Q=a(kd,"SPAN",{});var Af=i(Q);$e=s(Af,"Overview"),Af.forEach(t),kd.forEach(t),_e=h(n),O=a(n,"P",{});var co=i(O);ye=s(co,"The DistilBERT model was proposed in the blog post "),ae=a(co,"A",{href:!0,rel:!0});var Of=i(ae);H=s(Of,`Smaller, faster, cheaper, lighter: Introducing DistilBERT, a
distilled version of BERT`),Of.forEach(t),De=s(co,", and the paper "),ie=a(co,"A",{href:!0,rel:!0});var Lf=i(ie);K=s(Lf,`DistilBERT, a
distilled version of BERT: smaller, faster, cheaper and lighter`),Lf.forEach(t),Fe=s(co,`. DistilBERT is a
small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than
`),de=a(co,"EM",{});var If=i(de);V=s(If,"bert-base-uncased"),If.forEach(t),Ee=s(co,`, runs 60% faster while preserving over 95% of BERT\u2019s performances as measured on the GLUE language
understanding benchmark.`),co.forEach(t),be=h(n),ee=a(n,"P",{});var Sf=i(ee);j=s(Sf,"The abstract from the paper is the following:"),Sf.forEach(t),q=h(n),le=a(n,"P",{});var Nf=i(le);U=a(Nf,"EM",{});var Rf=i(U);Be=s(Rf,`As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP),
operating these large models in on-the-edge and/or under constrained computational training or inference budgets
remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation
model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger
counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage
knowledge distillation during the pretraining phase and show that it is possible to reduce the size of a BERT model by
40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive
biases learned by larger models during pretraining, we introduce a triple loss combining language modeling,
distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we
demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device
study.`),Rf.forEach(t),Nf.forEach(t),ve=h(n),W=a(n,"P",{});var Wf=i(W);xe=s(Wf,"Tips:"),Wf.forEach(t),ke=h(n),C=a(n,"UL",{});var wd=i(C);se=a(wd,"LI",{});var po=i(se);J=s(po,"DistilBERT doesn\u2019t have "),ce=a(po,"CODE",{});var Qf=i(ce);Me=s(Qf,"token_type_ids"),Qf.forEach(t),G=s(po,`, you don\u2019t need to indicate which token belongs to which segment. Just
separate your segments with the separation token `),pe=a(po,"CODE",{});var Uf=i(pe);ze=s(Uf,"tokenizer.sep_token"),Uf.forEach(t),S=s(po," (or "),he=a(po,"CODE",{});var Hf=i(he);X=s(Hf,"[SEP]"),Hf.forEach(t),je=s(po,")."),po.forEach(t),ne=h(wd),P=a(wd,"LI",{});var Td=i(P);Ce=s(Td,"DistilBERT doesn\u2019t have options to select the input positions ("),A=a(Td,"CODE",{});var Kf=i(A);Pe=s(Kf,"position_ids"),Kf.forEach(t),qe=s(Td,` input). This could be added if
necessary though, just let us know if you need this option.`),Td.forEach(t),wd.forEach(t),k=h(n),x=a(n,"P",{});var ho=i(x);He=s(ho,"This model was contributed by "),Z=a(ho,"A",{href:!0,rel:!0});var Vf=i(Z);Ke=s(Vf,"victorsanh"),Vf.forEach(t),Ve=s(ho,`. This model jax version was
contributed by `),z=a(ho,"A",{href:!0,rel:!0});var Jf=i(z);Je=s(Jf,"kamalkraj"),Jf.forEach(t),Ge=s(ho,". The original code can be found "),Le=a(ho,"A",{href:!0,rel:!0});var Gf=i(Le);Xe=s(Gf,"here"),Gf.forEach(t),Ye=s(ho,"."),ho.forEach(t),L=h(n),N=a(n,"H2",{class:!0});var $d=i(N);Oe=a($d,"A",{id:!0,class:!0,href:!0});var Xf=i(Oe);Re=a(Xf,"SPAN",{});var Yf=i(Re);T(R.$$.fragment,Yf),Yf.forEach(t),Xf.forEach(t),Ze=h($d),We=a($d,"SPAN",{});var Zf=i(We);Ae=s(Zf,"Resources"),Zf.forEach(t),$d.forEach(t),Ue=h(n),te=a(n,"P",{});var eu=i(te);et=s(eu,"A list of official Hugging Face and community (indicated by \u{1F30E}) resources to help you get started with DistilBERT. If you\u2019re interested in submitting a resource to be included here, please feel free to open a Pull Request and we\u2019ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource."),eu.forEach(t),gu=h(n),T(Er.$$.fragment,n),_u=h(n),Ie=a(n,"UL",{});var Qe=i(Ie);Br=a(Qe,"LI",{});var yd=i(Br);Ng=s(yd,"A blog post on "),xr=a(yd,"A",{href:!0,rel:!0});var tu=i(xr);Rg=s(tu,"Getting Started with Sentiment Analysis using Python"),tu.forEach(t),Wg=s(yd," with DistilBERT."),yd.forEach(t),Qg=h(Qe),Mr=a(Qe,"LI",{});var Dd=i(Mr);Ug=s(Dd,"A blog post on how to "),zr=a(Dd,"A",{href:!0,rel:!0});var ou=i(zr);Hg=s(ou,"train DistilBERT with Blurr for sequence classification"),ou.forEach(t),Kg=s(Dd,"."),Dd.forEach(t),Vg=h(Qe),jr=a(Qe,"LI",{});var Fd=i(jr);Jg=s(Fd,"A blog post on how to use "),Cr=a(Fd,"A",{href:!0,rel:!0});var su=i(Cr);Gg=s(su,"Ray to tune DistilBERT hyperparameters"),su.forEach(t),Xg=s(Fd,"."),Fd.forEach(t),Yg=h(Qe),Pr=a(Qe,"LI",{});var Ed=i(Pr);Zg=s(Ed,"A blog post on how to "),qr=a(Ed,"A",{href:!0,rel:!0});var nu=i(qr);e_=s(nu,"train DistilBERT with Hugging Face and Amazon SageMaker"),nu.forEach(t),t_=s(Ed,"."),Ed.forEach(t),o_=h(Qe),Ar=a(Qe,"LI",{});var Bd=i(Ar);s_=s(Bd,"A notebook on how to "),Or=a(Bd,"A",{href:!0,rel:!0});var ru=i(Or);n_=s(ru,"finetune DistilBERT for multi-label classification"),ru.forEach(t),r_=s(Bd,". \u{1F30E}"),Bd.forEach(t),a_=h(Qe),Lr=a(Qe,"LI",{});var xd=i(Lr);i_=s(xd,"A notebook on how to "),Ir=a(xd,"A",{href:!0,rel:!0});var au=i(Ir);l_=s(au,"finetune DistilBERT for multiclass classification with PyTorch"),au.forEach(t),d_=s(xd,". \u{1F30E}"),xd.forEach(t),c_=h(Qe),Sr=a(Qe,"LI",{});var Md=i(Sr);p_=s(Md,"A notebook on how to "),Nr=a(Md,"A",{href:!0,rel:!0});var iu=i(Nr);h_=s(iu,"finetune DistilBERT for text classification in TensorFlow"),iu.forEach(t),f_=s(Md,". \u{1F30E}"),Md.forEach(t),u_=h(Qe),fo=a(Qe,"LI",{});var Eo=i(fo);Nd=a(Eo,"A",{href:!0});var lu=i(Nd);m_=s(lu,"DistilBertForSequenceClassification"),lu.forEach(t),g_=s(Eo," is supported by this "),Rr=a(Eo,"A",{href:!0,rel:!0});var du=i(Rr);__=s(du,"example script"),du.forEach(t),b_=s(Eo," and "),Wr=a(Eo,"A",{href:!0,rel:!0});var cu=i(Wr);v_=s(cu,"notebook"),cu.forEach(t),k_=s(Eo,"."),Eo.forEach(t),w_=h(Qe),uo=a(Qe,"LI",{});var Bo=i(uo);Rd=a(Bo,"A",{href:!0});var pu=i(Rd);T_=s(pu,"TFDistilBertForSequenceClassification"),pu.forEach(t),$_=s(Bo," is supported by this "),Qr=a(Bo,"A",{href:!0,rel:!0});var bD=i(Qr);y_=s(bD,"example script"),bD.forEach(t),D_=s(Bo," and "),Ur=a(Bo,"A",{href:!0,rel:!0});var vD=i(Ur);F_=s(vD,"notebook"),vD.forEach(t),E_=s(Bo,"."),Bo.forEach(t),B_=h(Qe),mo=a(Qe,"LI",{});var zd=i(mo);Wd=a(zd,"A",{href:!0});var kD=i(Wd);x_=s(kD,"FlaxDistilBertForSequenceClassification"),kD.forEach(t),M_=s(zd," is supported by this "),Hr=a(zd,"A",{href:!0,rel:!0});var wD=i(Hr);z_=s(wD,"example script"),wD.forEach(t),j_=s(zd," and "),Kr=a(zd,"A",{href:!0,rel:!0});var TD=i(Kr);C_=s(TD,"notebook"),TD.forEach(t),P_=s(zd,"."),zd.forEach(t),Qe.forEach(t),bu=h(n),T(Vr.$$.fragment,n),vu=h(n),Ot=a(n,"UL",{});var nr=i(Ot);go=a(nr,"LI",{});var jd=i(go);Qd=a(jd,"A",{href:!0});var $D=i(Qd);q_=s($D,"DistilBertForTokenClassification"),$D.forEach(t),A_=s(jd," is supported by this "),Jr=a(jd,"A",{href:!0,rel:!0});var yD=i(Jr);O_=s(yD,"example script"),yD.forEach(t),L_=s(jd," and "),Gr=a(jd,"A",{href:!0,rel:!0});var DD=i(Gr);I_=s(DD,"notebook"),DD.forEach(t),S_=s(jd,"."),jd.forEach(t),N_=h(nr),_o=a(nr,"LI",{});var Cd=i(_o);Ud=a(Cd,"A",{href:!0});var FD=i(Ud);R_=s(FD,"TFDistilBertForTokenClassification"),FD.forEach(t),W_=s(Cd," is supported by this "),Xr=a(Cd,"A",{href:!0,rel:!0});var ED=i(Xr);Q_=s(ED,"example script"),ED.forEach(t),U_=s(Cd," and "),Yr=a(Cd,"A",{href:!0,rel:!0});var BD=i(Yr);H_=s(BD,"notebook"),BD.forEach(t),K_=s(Cd,"."),Cd.forEach(t),V_=h(nr),zs=a(nr,"LI",{});var hu=i(zs);Hd=a(hu,"A",{href:!0});var xD=i(Hd);J_=s(xD,"FlaxDistilBertForTokenClassification"),xD.forEach(t),G_=s(hu," is supported by this "),Zr=a(hu,"A",{href:!0,rel:!0});var MD=i(Zr);X_=s(MD,"example script"),MD.forEach(t),Y_=s(hu,"."),hu.forEach(t),Z_=h(nr),Kd=a(nr,"LI",{});var mD=i(Kd);ea=a(mD,"A",{href:!0,rel:!0});var zD=i(ea);eb=s(zD,"Token classification"),zD.forEach(t),tb=s(mD," chapter of the \u{1F917} Hugging Face Course."),mD.forEach(t),nr.forEach(t),ku=h(n),T(ta.$$.fragment,n),wu=h(n),Lt=a(n,"UL",{});var rr=i(Lt);bo=a(rr,"LI",{});var Pd=i(bo);Vd=a(Pd,"A",{href:!0});var jD=i(Vd);ob=s(jD,"DistilBertForMaskedLM"),jD.forEach(t),sb=s(Pd," is supported by this "),oa=a(Pd,"A",{href:!0,rel:!0});var CD=i(oa);nb=s(CD,"example script"),CD.forEach(t),rb=s(Pd," and "),sa=a(Pd,"A",{href:!0,rel:!0});var PD=i(sa);ab=s(PD,"notebook"),PD.forEach(t),ib=s(Pd,"."),Pd.forEach(t),lb=h(rr),vo=a(rr,"LI",{});var qd=i(vo);Jd=a(qd,"A",{href:!0});var qD=i(Jd);db=s(qD,"TFDistilBertForMaskedLM"),qD.forEach(t),cb=s(qd," is supported by this "),na=a(qd,"A",{href:!0,rel:!0});var AD=i(na);pb=s(AD,"example script"),AD.forEach(t),hb=s(qd," and "),ra=a(qd,"A",{href:!0,rel:!0});var OD=i(ra);fb=s(OD,"notebook"),OD.forEach(t),ub=s(qd,"."),qd.forEach(t),mb=h(rr),ko=a(rr,"LI",{});var Ad=i(ko);Gd=a(Ad,"A",{href:!0});var LD=i(Gd);gb=s(LD,"FlaxDistilBertForMaskedLM"),LD.forEach(t),_b=s(Ad," is supported by this "),aa=a(Ad,"A",{href:!0,rel:!0});var ID=i(aa);bb=s(ID,"example script"),ID.forEach(t),vb=s(Ad," and "),ia=a(Ad,"A",{href:!0,rel:!0});var SD=i(ia);kb=s(SD,"notebook"),SD.forEach(t),wb=s(Ad,"."),Ad.forEach(t),Tb=h(rr),Xd=a(rr,"LI",{});var gD=i(Xd);la=a(gD,"A",{href:!0,rel:!0});var ND=i(la);$b=s(ND,"Masked language modeling"),ND.forEach(t),yb=s(gD," chapter of the \u{1F917} Hugging Face Course."),gD.forEach(t),rr.forEach(t),Tu=h(n),T(da.$$.fragment,n),$u=h(n),It=a(n,"UL",{});var ar=i(It);wo=a(ar,"LI",{});var Od=i(wo);Yd=a(Od,"A",{href:!0});var RD=i(Yd);Db=s(RD,"DistilBertForQuestionAnswering"),RD.forEach(t),Fb=s(Od," is supported by this "),ca=a(Od,"A",{href:!0,rel:!0});var WD=i(ca);Eb=s(WD,"example script"),WD.forEach(t),Bb=s(Od," and "),pa=a(Od,"A",{href:!0,rel:!0});var QD=i(pa);xb=s(QD,"notebook"),QD.forEach(t),Mb=s(Od,"."),Od.forEach(t),zb=h(ar),To=a(ar,"LI",{});var Ld=i(To);Zd=a(Ld,"A",{href:!0});var UD=i(Zd);jb=s(UD,"TFDistilBertForQuestionAnswering"),UD.forEach(t),Cb=s(Ld," is supported by this "),ha=a(Ld,"A",{href:!0,rel:!0});var HD=i(ha);Pb=s(HD,"example script"),HD.forEach(t),qb=s(Ld," and "),fa=a(Ld,"A",{href:!0,rel:!0});var KD=i(fa);Ab=s(KD,"notebook"),KD.forEach(t),Ob=s(Ld,"."),Ld.forEach(t),Lb=h(ar),js=a(ar,"LI",{});var fu=i(js);ec=a(fu,"A",{href:!0});var VD=i(ec);Ib=s(VD,"FlaxDistilBertForQuestionAnswering"),VD.forEach(t),Sb=s(fu," is supported by this "),ua=a(fu,"A",{href:!0,rel:!0});var JD=i(ua);Nb=s(JD,"example script"),JD.forEach(t),Rb=s(fu,"."),fu.forEach(t),Wb=h(ar),tc=a(ar,"LI",{});var _D=i(tc);ma=a(_D,"A",{href:!0,rel:!0});var GD=i(ma);Qb=s(GD,"Question answering"),GD.forEach(t),Ub=s(_D," chapter of the \u{1F917} Hugging Face Course."),_D.forEach(t),ar.forEach(t),yu=h(n),oc=a(n,"P",{});var XD=i(oc);bp=a(XD,"STRONG",{});var YD=i(bp);Hb=s(YD,"Multiple choice"),YD.forEach(t),XD.forEach(t),Du=h(n),Cs=a(n,"UL",{});var wm=i(Cs);$o=a(wm,"LI",{});var Id=i($o);sc=a(Id,"A",{href:!0});var ZD=i(sc);Kb=s(ZD,"DistilBertForMultipleChoice"),ZD.forEach(t),Vb=s(Id," is supported by this "),ga=a(Id,"A",{href:!0,rel:!0});var eF=i(ga);Jb=s(eF,"example script"),eF.forEach(t),Gb=s(Id," and "),_a=a(Id,"A",{href:!0,rel:!0});var tF=i(_a);Xb=s(tF,"notebook"),tF.forEach(t),Yb=s(Id,"."),Id.forEach(t),Zb=h(wm),yo=a(wm,"LI",{});var Sd=i(yo);nc=a(Sd,"A",{href:!0});var oF=i(nc);ev=s(oF,"TFDistilBertForMultipleChoice"),oF.forEach(t),tv=s(Sd," is supported by this "),ba=a(Sd,"A",{href:!0,rel:!0});var sF=i(ba);ov=s(sF,"example script"),sF.forEach(t),sv=s(Sd," and "),va=a(Sd,"A",{href:!0,rel:!0});var nF=i(va);nv=s(nF,"notebook"),nF.forEach(t),rv=s(Sd,"."),Sd.forEach(t),wm.forEach(t),Fu=h(n),rc=a(n,"P",{});var rF=i(rc);av=s(rF,"\u2697\uFE0F Optimization"),rF.forEach(t),Eu=h(n),Do=a(n,"UL",{});var Vc=i(Do);ka=a(Vc,"LI",{});var Tm=i(ka);iv=s(Tm,"A blog post on how to "),wa=a(Tm,"A",{href:!0,rel:!0});var aF=i(wa);lv=s(aF,"quantize DistilBERT with \u{1F917} Optimum and Intel"),aF.forEach(t),dv=s(Tm,"."),Tm.forEach(t),cv=h(Vc),Ta=a(Vc,"LI",{});var $m=i(Ta);pv=s($m,"A blog post on how "),$a=a($m,"A",{href:!0,rel:!0});var iF=i($a);hv=s(iF,"Optimizing Transformers for GPUs with \u{1F917} Optimum"),iF.forEach(t),fv=s($m,"."),$m.forEach(t),uv=h(Vc),ya=a(Vc,"LI",{});var ym=i(ya);mv=s(ym,"A blog post on "),Da=a(ym,"A",{href:!0,rel:!0});var lF=i(Da);gv=s(lF,"Optimizing Transformers with Hugging Face Optimum"),lF.forEach(t),_v=s(ym,"."),ym.forEach(t),Vc.forEach(t),Bu=h(n),ac=a(n,"P",{});var dF=i(ac);bv=s(dF,"\u26A1\uFE0F Inference"),dF.forEach(t),xu=h(n),Ps=a(n,"UL",{});var Dm=i(Ps);Fa=a(Dm,"LI",{});var Fm=i(Fa);vv=s(Fm,"A blog post on how to "),Ea=a(Fm,"A",{href:!0,rel:!0});var cF=i(Ea);kv=s(cF,"Accelerate BERT inference with Hugging Face Transformers and AWS Inferentia"),cF.forEach(t),wv=s(Fm," with DistilBERT."),Fm.forEach(t),Tv=h(Dm),Ba=a(Dm,"LI",{});var Em=i(Ba);$v=s(Em,"A blog post on "),xa=a(Em,"A",{href:!0,rel:!0});var pF=i(xa);yv=s(pF,"Serverless Inference with Hugging Face\u2019s Transformers, DistilBERT and Amazon SageMaker"),pF.forEach(t),Dv=s(Em,"."),Em.forEach(t),Dm.forEach(t),Mu=h(n),ic=a(n,"P",{});var hF=i(ic);Fv=s(hF,"\u{1F680} Deploy"),hF.forEach(t),zu=h(n),Fo=a(n,"UL",{});var Jc=i(Fo);Ma=a(Jc,"LI",{});var Bm=i(Ma);Ev=s(Bm,"A blog post on how to "),za=a(Bm,"A",{href:!0,rel:!0});var fF=i(za);Bv=s(fF,"deploy DistilBERT on Google Cloud"),fF.forEach(t),xv=s(Bm,"."),Bm.forEach(t),Mv=h(Jc),ja=a(Jc,"LI",{});var xm=i(ja);zv=s(xm,"A blog post on how to "),Ca=a(xm,"A",{href:!0,rel:!0});var uF=i(Ca);jv=s(uF,"deploy DistilBERT with Amazon SageMaker"),uF.forEach(t),Cv=s(xm,"."),xm.forEach(t),Pv=h(Jc),Pa=a(Jc,"LI",{});var Mm=i(Pa);qv=s(Mm,"A blog post on how to "),qa=a(Mm,"A",{href:!0,rel:!0});var mF=i(qa);Av=s(mF,"Deploy BERT with Hugging Face Transformers, Amazon SageMaker and Terraform module"),mF.forEach(t),Ov=s(Mm,"."),Mm.forEach(t),Jc.forEach(t),ju=h(n),Ro=a(n,"H2",{class:!0});var zm=i(Ro);qs=a(zm,"A",{id:!0,class:!0,href:!0});var gF=i(qs);vp=a(gF,"SPAN",{});var _F=i(vp);T(Aa.$$.fragment,_F),_F.forEach(t),gF.forEach(t),Lv=h(zm),kp=a(zm,"SPAN",{});var bF=i(kp);Iv=s(bF,"DistilBertConfig"),bF.forEach(t),zm.forEach(t),Cu=h(n),Pt=a(n,"DIV",{class:!0});var ir=i(Pt);T(Oa.$$.fragment,ir),Sv=h(ir),oo=a(ir,"P",{});var lr=i(oo);Nv=s(lr,"This is the configuration class to store the configuration of a "),lc=a(lr,"A",{href:!0});var vF=i(lc);Rv=s(vF,"DistilBertModel"),vF.forEach(t),Wv=s(lr," or a "),dc=a(lr,"A",{href:!0});var kF=i(dc);Qv=s(kF,"TFDistilBertModel"),kF.forEach(t),Uv=s(lr,`. It
is used to instantiate a DistilBERT model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the DistilBERT
`),La=a(lr,"A",{href:!0,rel:!0});var wF=i(La);Hv=s(wF,"distilbert-base-uncased"),wF.forEach(t),Kv=s(lr," architecture."),lr.forEach(t),Vv=h(ir),Wo=a(ir,"P",{});var Gc=i(Wo);Jv=s(Gc,"Configuration objects inherit from "),cc=a(Gc,"A",{href:!0});var TF=i(cc);Gv=s(TF,"PretrainedConfig"),TF.forEach(t),Xv=s(Gc,` and can be used to control the model outputs. Read the
documentation from `),pc=a(Gc,"A",{href:!0});var $F=i(pc);Yv=s($F,"PretrainedConfig"),$F.forEach(t),Zv=s(Gc," for more information."),Gc.forEach(t),ek=h(ir),T(As.$$.fragment,ir),ir.forEach(t),Pu=h(n),Qo=a(n,"H2",{class:!0});var jm=i(Qo);Os=a(jm,"A",{id:!0,class:!0,href:!0});var yF=i(Os);wp=a(yF,"SPAN",{});var DF=i(wp);T(Ia.$$.fragment,DF),DF.forEach(t),yF.forEach(t),tk=h(jm),Tp=a(jm,"SPAN",{});var FF=i(Tp);ok=s(FF,"DistilBertTokenizer"),FF.forEach(t),jm.forEach(t),qu=h(n),qt=a(n,"DIV",{class:!0});var dr=i(qt);T(Sa.$$.fragment,dr),sk=h(dr),$p=a(dr,"P",{});var EF=i($p);nk=s(EF,"Construct a DistilBERT tokenizer."),EF.forEach(t),rk=h(dr),Ls=a(dr,"P",{});var uu=i(Ls);hc=a(uu,"A",{href:!0});var BF=i(hc);ak=s(BF,"DistilBertTokenizer"),BF.forEach(t),ik=s(uu," is identical to "),fc=a(uu,"A",{href:!0});var xF=i(fc);lk=s(xF,"BertTokenizer"),xF.forEach(t),dk=s(uu,` and runs end-to-end tokenization: punctuation splitting
and wordpiece.`),uu.forEach(t),ck=h(dr),Na=a(dr,"P",{});var Cm=i(Na);pk=s(Cm,"Refer to superclass "),uc=a(Cm,"A",{href:!0});var MF=i(uc);hk=s(MF,"BertTokenizer"),MF.forEach(t),fk=s(Cm," for usage examples and documentation concerning parameters."),Cm.forEach(t),dr.forEach(t),Au=h(n),Uo=a(n,"H2",{class:!0});var Pm=i(Uo);Is=a(Pm,"A",{id:!0,class:!0,href:!0});var zF=i(Is);yp=a(zF,"SPAN",{});var jF=i(yp);T(Ra.$$.fragment,jF),jF.forEach(t),zF.forEach(t),uk=h(Pm),Dp=a(Pm,"SPAN",{});var CF=i(Dp);mk=s(CF,"DistilBertTokenizerFast"),CF.forEach(t),Pm.forEach(t),Ou=h(n),At=a(n,"DIV",{class:!0});var cr=i(At);T(Wa.$$.fragment,cr),gk=h(cr),Qa=a(cr,"P",{});var qm=i(Qa);_k=s(qm,"Construct a \u201Cfast\u201D DistilBERT tokenizer (backed by HuggingFace\u2019s "),Fp=a(qm,"EM",{});var PF=i(Fp);bk=s(PF,"tokenizers"),PF.forEach(t),vk=s(qm," library)."),qm.forEach(t),kk=h(cr),Ss=a(cr,"P",{});var mu=i(Ss);mc=a(mu,"A",{href:!0});var qF=i(mc);wk=s(qF,"DistilBertTokenizerFast"),qF.forEach(t),Tk=s(mu," is identical to "),gc=a(mu,"A",{href:!0});var AF=i(gc);$k=s(AF,"BertTokenizerFast"),AF.forEach(t),yk=s(mu,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),mu.forEach(t),Dk=h(cr),Ua=a(cr,"P",{});var Am=i(Ua);Fk=s(Am,"Refer to superclass "),_c=a(Am,"A",{href:!0});var OF=i(_c);Ek=s(OF,"BertTokenizerFast"),OF.forEach(t),Bk=s(Am," for usage examples and documentation concerning parameters."),Am.forEach(t),cr.forEach(t),Lu=h(n),Ho=a(n,"H2",{class:!0});var Om=i(Ho);Ns=a(Om,"A",{id:!0,class:!0,href:!0});var LF=i(Ns);Ep=a(LF,"SPAN",{});var IF=i(Ep);T(Ha.$$.fragment,IF),IF.forEach(t),LF.forEach(t),xk=h(Om),Bp=a(Om,"SPAN",{});var SF=i(Bp);Mk=s(SF,"DistilBertModel"),SF.forEach(t),Om.forEach(t),Iu=h(n),ut=a(n,"DIV",{class:!0});var xo=i(ut);T(Ka.$$.fragment,xo),zk=h(xo),xp=a(xo,"P",{});var NF=i(xp);jk=s(NF,"The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),NF.forEach(t),Ck=h(xo),Va=a(xo,"P",{});var Lm=i(Va);Pk=s(Lm,"This model inherits from "),bc=a(Lm,"A",{href:!0});var RF=i(bc);qk=s(RF,"PreTrainedModel"),RF.forEach(t),Ak=s(Lm,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Lm.forEach(t),Ok=h(xo),Ja=a(xo,"P",{});var Im=i(Ja);Lk=s(Im,"This model is also a PyTorch "),Ga=a(Im,"A",{href:!0,rel:!0});var WF=i(Ga);Ik=s(WF,"torch.nn.Module"),WF.forEach(t),Sk=s(Im,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Im.forEach(t),Nk=h(xo),St=a(xo,"DIV",{class:!0});var pr=i(St);T(Xa.$$.fragment,pr),Rk=h(pr),Ko=a(pr,"P",{});var Xc=i(Ko);Wk=s(Xc,"The "),vc=a(Xc,"A",{href:!0});var QF=i(vc);Qk=s(QF,"DistilBertModel"),QF.forEach(t),Uk=s(Xc," forward method, overrides the "),Mp=a(Xc,"CODE",{});var UF=i(Mp);Hk=s(UF,"__call__"),UF.forEach(t),Kk=s(Xc," special method."),Xc.forEach(t),Vk=h(pr),T(Rs.$$.fragment,pr),Jk=h(pr),T(Ws.$$.fragment,pr),pr.forEach(t),xo.forEach(t),Su=h(n),Vo=a(n,"H2",{class:!0});var Sm=i(Vo);Qs=a(Sm,"A",{id:!0,class:!0,href:!0});var HF=i(Qs);zp=a(HF,"SPAN",{});var KF=i(zp);T(Ya.$$.fragment,KF),KF.forEach(t),HF.forEach(t),Gk=h(Sm),jp=a(Sm,"SPAN",{});var VF=i(jp);Xk=s(VF,"DistilBertForMaskedLM"),VF.forEach(t),Sm.forEach(t),Nu=h(n),mt=a(n,"DIV",{class:!0});var Mo=i(mt);T(Za.$$.fragment,Mo),Yk=h(Mo),ei=a(Mo,"P",{});var Nm=i(ei);Zk=s(Nm,"DistilBert Model with a "),Cp=a(Nm,"CODE",{});var JF=i(Cp);ew=s(JF,"masked language modeling"),JF.forEach(t),tw=s(Nm," head on top."),Nm.forEach(t),ow=h(Mo),ti=a(Mo,"P",{});var Rm=i(ti);sw=s(Rm,"This model inherits from "),kc=a(Rm,"A",{href:!0});var GF=i(kc);nw=s(GF,"PreTrainedModel"),GF.forEach(t),rw=s(Rm,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Rm.forEach(t),aw=h(Mo),oi=a(Mo,"P",{});var Wm=i(oi);iw=s(Wm,"This model is also a PyTorch "),si=a(Wm,"A",{href:!0,rel:!0});var XF=i(si);lw=s(XF,"torch.nn.Module"),XF.forEach(t),dw=s(Wm,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Wm.forEach(t),cw=h(Mo),kt=a(Mo,"DIV",{class:!0});var zo=i(kt);T(ni.$$.fragment,zo),pw=h(zo),Jo=a(zo,"P",{});var Yc=i(Jo);hw=s(Yc,"The "),wc=a(Yc,"A",{href:!0});var YF=i(wc);fw=s(YF,"DistilBertForMaskedLM"),YF.forEach(t),uw=s(Yc," forward method, overrides the "),Pp=a(Yc,"CODE",{});var ZF=i(Pp);mw=s(ZF,"__call__"),ZF.forEach(t),gw=s(Yc," special method."),Yc.forEach(t),_w=h(zo),T(Us.$$.fragment,zo),bw=h(zo),T(Hs.$$.fragment,zo),vw=h(zo),T(Ks.$$.fragment,zo),zo.forEach(t),Mo.forEach(t),Ru=h(n),Go=a(n,"H2",{class:!0});var Qm=i(Go);Vs=a(Qm,"A",{id:!0,class:!0,href:!0});var eE=i(Vs);qp=a(eE,"SPAN",{});var tE=i(qp);T(ri.$$.fragment,tE),tE.forEach(t),eE.forEach(t),kw=h(Qm),Ap=a(Qm,"SPAN",{});var oE=i(Ap);ww=s(oE,"DistilBertForSequenceClassification"),oE.forEach(t),Qm.forEach(t),Wu=h(n),gt=a(n,"DIV",{class:!0});var jo=i(gt);T(ai.$$.fragment,jo),Tw=h(jo),Op=a(jo,"P",{});var sE=i(Op);$w=s(sE,`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),sE.forEach(t),yw=h(jo),ii=a(jo,"P",{});var Um=i(ii);Dw=s(Um,"This model inherits from "),Tc=a(Um,"A",{href:!0});var nE=i(Tc);Fw=s(nE,"PreTrainedModel"),nE.forEach(t),Ew=s(Um,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Um.forEach(t),Bw=h(jo),li=a(jo,"P",{});var Hm=i(li);xw=s(Hm,"This model is also a PyTorch "),di=a(Hm,"A",{href:!0,rel:!0});var rE=i(di);Mw=s(rE,"torch.nn.Module"),rE.forEach(t),zw=s(Hm,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Hm.forEach(t),jw=h(jo),it=a(jo,"DIV",{class:!0});var Et=i(it);T(ci.$$.fragment,Et),Cw=h(Et),Xo=a(Et,"P",{});var Zc=i(Xo);Pw=s(Zc,"The "),$c=a(Zc,"A",{href:!0});var aE=i($c);qw=s(aE,"DistilBertForSequenceClassification"),aE.forEach(t),Aw=s(Zc," forward method, overrides the "),Lp=a(Zc,"CODE",{});var iE=i(Lp);Ow=s(iE,"__call__"),iE.forEach(t),Lw=s(Zc," special method."),Zc.forEach(t),Iw=h(Et),T(Js.$$.fragment,Et),Sw=h(Et),T(Gs.$$.fragment,Et),Nw=h(Et),T(Xs.$$.fragment,Et),Rw=h(Et),T(Ys.$$.fragment,Et),Ww=h(Et),T(Zs.$$.fragment,Et),Et.forEach(t),jo.forEach(t),Qu=h(n),Yo=a(n,"H2",{class:!0});var Km=i(Yo);en=a(Km,"A",{id:!0,class:!0,href:!0});var lE=i(en);Ip=a(lE,"SPAN",{});var dE=i(Ip);T(pi.$$.fragment,dE),dE.forEach(t),lE.forEach(t),Qw=h(Km),Sp=a(Km,"SPAN",{});var cE=i(Sp);Uw=s(cE,"DistilBertForMultipleChoice"),cE.forEach(t),Km.forEach(t),Uu=h(n),_t=a(n,"DIV",{class:!0});var Co=i(_t);T(hi.$$.fragment,Co),Hw=h(Co),Np=a(Co,"P",{});var pE=i(Np);Kw=s(pE,`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),pE.forEach(t),Vw=h(Co),fi=a(Co,"P",{});var Vm=i(fi);Jw=s(Vm,"This model inherits from "),yc=a(Vm,"A",{href:!0});var hE=i(yc);Gw=s(hE,"PreTrainedModel"),hE.forEach(t),Xw=s(Vm,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Vm.forEach(t),Yw=h(Co),ui=a(Co,"P",{});var Jm=i(ui);Zw=s(Jm,"This model is also a PyTorch "),mi=a(Jm,"A",{href:!0,rel:!0});var fE=i(mi);eT=s(fE,"torch.nn.Module"),fE.forEach(t),tT=s(Jm,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Jm.forEach(t),oT=h(Co),Nt=a(Co,"DIV",{class:!0});var hr=i(Nt);T(gi.$$.fragment,hr),sT=h(hr),Zo=a(hr,"P",{});var ep=i(Zo);nT=s(ep,"The "),Dc=a(ep,"A",{href:!0});var uE=i(Dc);rT=s(uE,"DistilBertForMultipleChoice"),uE.forEach(t),aT=s(ep," forward method, overrides the "),Rp=a(ep,"CODE",{});var mE=i(Rp);iT=s(mE,"__call__"),mE.forEach(t),lT=s(ep," special method."),ep.forEach(t),dT=h(hr),T(tn.$$.fragment,hr),cT=h(hr),T(on.$$.fragment,hr),hr.forEach(t),Co.forEach(t),Hu=h(n),es=a(n,"H2",{class:!0});var Gm=i(es);sn=a(Gm,"A",{id:!0,class:!0,href:!0});var gE=i(sn);Wp=a(gE,"SPAN",{});var _E=i(Wp);T(_i.$$.fragment,_E),_E.forEach(t),gE.forEach(t),pT=h(Gm),Qp=a(Gm,"SPAN",{});var bE=i(Qp);hT=s(bE,"DistilBertForTokenClassification"),bE.forEach(t),Gm.forEach(t),Ku=h(n),bt=a(n,"DIV",{class:!0});var Po=i(bt);T(bi.$$.fragment,Po),fT=h(Po),Up=a(Po,"P",{});var vE=i(Up);uT=s(vE,`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),vE.forEach(t),mT=h(Po),vi=a(Po,"P",{});var Xm=i(vi);gT=s(Xm,"This model inherits from "),Fc=a(Xm,"A",{href:!0});var kE=i(Fc);_T=s(kE,"PreTrainedModel"),kE.forEach(t),bT=s(Xm,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Xm.forEach(t),vT=h(Po),ki=a(Po,"P",{});var Ym=i(ki);kT=s(Ym,"This model is also a PyTorch "),wi=a(Ym,"A",{href:!0,rel:!0});var wE=i(wi);wT=s(wE,"torch.nn.Module"),wE.forEach(t),TT=s(Ym,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ym.forEach(t),$T=h(Po),wt=a(Po,"DIV",{class:!0});var qo=i(wt);T(Ti.$$.fragment,qo),yT=h(qo),ts=a(qo,"P",{});var tp=i(ts);DT=s(tp,"The "),Ec=a(tp,"A",{href:!0});var TE=i(Ec);FT=s(TE,"DistilBertForTokenClassification"),TE.forEach(t),ET=s(tp," forward method, overrides the "),Hp=a(tp,"CODE",{});var $E=i(Hp);BT=s($E,"__call__"),$E.forEach(t),xT=s(tp," special method."),tp.forEach(t),MT=h(qo),T(nn.$$.fragment,qo),zT=h(qo),T(rn.$$.fragment,qo),jT=h(qo),T(an.$$.fragment,qo),qo.forEach(t),Po.forEach(t),Vu=h(n),os=a(n,"H2",{class:!0});var Zm=i(os);ln=a(Zm,"A",{id:!0,class:!0,href:!0});var yE=i(ln);Kp=a(yE,"SPAN",{});var DE=i(Kp);T($i.$$.fragment,DE),DE.forEach(t),yE.forEach(t),CT=h(Zm),Vp=a(Zm,"SPAN",{});var FE=i(Vp);PT=s(FE,"DistilBertForQuestionAnswering"),FE.forEach(t),Zm.forEach(t),Ju=h(n),vt=a(n,"DIV",{class:!0});var Ao=i(vt);T(yi.$$.fragment,Ao),qT=h(Ao),ss=a(Ao,"P",{});var op=i(ss);AT=s(op,`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),Jp=a(op,"CODE",{});var EE=i(Jp);OT=s(EE,"span start logits"),EE.forEach(t),LT=s(op," and "),Gp=a(op,"CODE",{});var BE=i(Gp);IT=s(BE,"span end logits"),BE.forEach(t),ST=s(op,")."),op.forEach(t),NT=h(Ao),Di=a(Ao,"P",{});var eg=i(Di);RT=s(eg,"This model inherits from "),Bc=a(eg,"A",{href:!0});var xE=i(Bc);WT=s(xE,"PreTrainedModel"),xE.forEach(t),QT=s(eg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),eg.forEach(t),UT=h(Ao),Fi=a(Ao,"P",{});var tg=i(Fi);HT=s(tg,"This model is also a PyTorch "),Ei=a(tg,"A",{href:!0,rel:!0});var ME=i(Ei);KT=s(ME,"torch.nn.Module"),ME.forEach(t),VT=s(tg,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),tg.forEach(t),JT=h(Ao),Tt=a(Ao,"DIV",{class:!0});var Oo=i(Tt);T(Bi.$$.fragment,Oo),GT=h(Oo),ns=a(Oo,"P",{});var sp=i(ns);XT=s(sp,"The "),xc=a(sp,"A",{href:!0});var zE=i(xc);YT=s(zE,"DistilBertForQuestionAnswering"),zE.forEach(t),ZT=s(sp," forward method, overrides the "),Xp=a(sp,"CODE",{});var jE=i(Xp);e$=s(jE,"__call__"),jE.forEach(t),t$=s(sp," special method."),sp.forEach(t),o$=h(Oo),T(dn.$$.fragment,Oo),s$=h(Oo),T(cn.$$.fragment,Oo),n$=h(Oo),T(pn.$$.fragment,Oo),Oo.forEach(t),Ao.forEach(t),Gu=h(n),rs=a(n,"H2",{class:!0});var og=i(rs);hn=a(og,"A",{id:!0,class:!0,href:!0});var CE=i(hn);Yp=a(CE,"SPAN",{});var PE=i(Yp);T(xi.$$.fragment,PE),PE.forEach(t),CE.forEach(t),r$=h(og),Zp=a(og,"SPAN",{});var qE=i(Zp);a$=s(qE,"TFDistilBertModel"),qE.forEach(t),og.forEach(t),Xu=h(n),lt=a(n,"DIV",{class:!0});var Gt=i(lt);T(Mi.$$.fragment,Gt),i$=h(Gt),eh=a(Gt,"P",{});var AE=i(eh);l$=s(AE,"The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),AE.forEach(t),d$=h(Gt),zi=a(Gt,"P",{});var sg=i(zi);c$=s(sg,"This model inherits from "),Mc=a(sg,"A",{href:!0});var OE=i(Mc);p$=s(OE,"TFPreTrainedModel"),OE.forEach(t),h$=s(sg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),sg.forEach(t),f$=h(Gt),ji=a(Gt,"P",{});var ng=i(ji);u$=s(ng,"This model is also a "),Ci=a(ng,"A",{href:!0,rel:!0});var LE=i(Ci);m$=s(LE,"tf.keras.Model"),LE.forEach(t),g$=s(ng,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),ng.forEach(t),_$=h(Gt),T(fn.$$.fragment,Gt),b$=h(Gt),Rt=a(Gt,"DIV",{class:!0});var fr=i(Rt);T(Pi.$$.fragment,fr),v$=h(fr),as=a(fr,"P",{});var np=i(as);k$=s(np,"The "),zc=a(np,"A",{href:!0});var IE=i(zc);w$=s(IE,"TFDistilBertModel"),IE.forEach(t),T$=s(np," forward method, overrides the "),th=a(np,"CODE",{});var SE=i(th);$$=s(SE,"__call__"),SE.forEach(t),y$=s(np," special method."),np.forEach(t),D$=h(fr),T(un.$$.fragment,fr),F$=h(fr),T(mn.$$.fragment,fr),fr.forEach(t),Gt.forEach(t),Yu=h(n),is=a(n,"H2",{class:!0});var rg=i(is);gn=a(rg,"A",{id:!0,class:!0,href:!0});var NE=i(gn);oh=a(NE,"SPAN",{});var RE=i(oh);T(qi.$$.fragment,RE),RE.forEach(t),NE.forEach(t),E$=h(rg),sh=a(rg,"SPAN",{});var WE=i(sh);B$=s(WE,"TFDistilBertForMaskedLM"),WE.forEach(t),rg.forEach(t),Zu=h(n),dt=a(n,"DIV",{class:!0});var Xt=i(dt);T(Ai.$$.fragment,Xt),x$=h(Xt),Oi=a(Xt,"P",{});var ag=i(Oi);M$=s(ag,"DistilBert Model with a "),nh=a(ag,"CODE",{});var QE=i(nh);z$=s(QE,"masked language modeling"),QE.forEach(t),j$=s(ag," head on top."),ag.forEach(t),C$=h(Xt),Li=a(Xt,"P",{});var ig=i(Li);P$=s(ig,"This model inherits from "),jc=a(ig,"A",{href:!0});var UE=i(jc);q$=s(UE,"TFPreTrainedModel"),UE.forEach(t),A$=s(ig,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ig.forEach(t),O$=h(Xt),Ii=a(Xt,"P",{});var lg=i(Ii);L$=s(lg,"This model is also a "),Si=a(lg,"A",{href:!0,rel:!0});var HE=i(Si);I$=s(HE,"tf.keras.Model"),HE.forEach(t),S$=s(lg,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),lg.forEach(t),N$=h(Xt),T(_n.$$.fragment,Xt),R$=h(Xt),$t=a(Xt,"DIV",{class:!0});var Lo=i($t);T(Ni.$$.fragment,Lo),W$=h(Lo),ls=a(Lo,"P",{});var rp=i(ls);Q$=s(rp,"The "),Cc=a(rp,"A",{href:!0});var KE=i(Cc);U$=s(KE,"TFDistilBertForMaskedLM"),KE.forEach(t),H$=s(rp," forward method, overrides the "),rh=a(rp,"CODE",{});var VE=i(rh);K$=s(VE,"__call__"),VE.forEach(t),V$=s(rp," special method."),rp.forEach(t),J$=h(Lo),T(bn.$$.fragment,Lo),G$=h(Lo),T(vn.$$.fragment,Lo),X$=h(Lo),T(kn.$$.fragment,Lo),Lo.forEach(t),Xt.forEach(t),em=h(n),ds=a(n,"H2",{class:!0});var dg=i(ds);wn=a(dg,"A",{id:!0,class:!0,href:!0});var JE=i(wn);ah=a(JE,"SPAN",{});var GE=i(ah);T(Ri.$$.fragment,GE),GE.forEach(t),JE.forEach(t),Y$=h(dg),ih=a(dg,"SPAN",{});var XE=i(ih);Z$=s(XE,"TFDistilBertForSequenceClassification"),XE.forEach(t),dg.forEach(t),tm=h(n),ct=a(n,"DIV",{class:!0});var Yt=i(ct);T(Wi.$$.fragment,Yt),ey=h(Yt),lh=a(Yt,"P",{});var YE=i(lh);ty=s(YE,`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),YE.forEach(t),oy=h(Yt),Qi=a(Yt,"P",{});var cg=i(Qi);sy=s(cg,"This model inherits from "),Pc=a(cg,"A",{href:!0});var ZE=i(Pc);ny=s(ZE,"TFPreTrainedModel"),ZE.forEach(t),ry=s(cg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),cg.forEach(t),ay=h(Yt),Ui=a(Yt,"P",{});var pg=i(Ui);iy=s(pg,"This model is also a "),Hi=a(pg,"A",{href:!0,rel:!0});var eB=i(Hi);ly=s(eB,"tf.keras.Model"),eB.forEach(t),dy=s(pg,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),pg.forEach(t),cy=h(Yt),T(Tn.$$.fragment,Yt),py=h(Yt),yt=a(Yt,"DIV",{class:!0});var Io=i(yt);T(Ki.$$.fragment,Io),hy=h(Io),cs=a(Io,"P",{});var ap=i(cs);fy=s(ap,"The "),qc=a(ap,"A",{href:!0});var tB=i(qc);uy=s(tB,"TFDistilBertForSequenceClassification"),tB.forEach(t),my=s(ap," forward method, overrides the "),dh=a(ap,"CODE",{});var oB=i(dh);gy=s(oB,"__call__"),oB.forEach(t),_y=s(ap," special method."),ap.forEach(t),by=h(Io),T($n.$$.fragment,Io),vy=h(Io),T(yn.$$.fragment,Io),ky=h(Io),T(Dn.$$.fragment,Io),Io.forEach(t),Yt.forEach(t),om=h(n),ps=a(n,"H2",{class:!0});var hg=i(ps);Fn=a(hg,"A",{id:!0,class:!0,href:!0});var sB=i(Fn);ch=a(sB,"SPAN",{});var nB=i(ch);T(Vi.$$.fragment,nB),nB.forEach(t),sB.forEach(t),wy=h(hg),ph=a(hg,"SPAN",{});var rB=i(ph);Ty=s(rB,"TFDistilBertForMultipleChoice"),rB.forEach(t),hg.forEach(t),sm=h(n),pt=a(n,"DIV",{class:!0});var Zt=i(pt);T(Ji.$$.fragment,Zt),$y=h(Zt),hh=a(Zt,"P",{});var aB=i(hh);yy=s(aB,`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),aB.forEach(t),Dy=h(Zt),Gi=a(Zt,"P",{});var fg=i(Gi);Fy=s(fg,"This model inherits from "),Ac=a(fg,"A",{href:!0});var iB=i(Ac);Ey=s(iB,"TFPreTrainedModel"),iB.forEach(t),By=s(fg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),fg.forEach(t),xy=h(Zt),Xi=a(Zt,"P",{});var ug=i(Xi);My=s(ug,"This model is also a "),Yi=a(ug,"A",{href:!0,rel:!0});var lB=i(Yi);zy=s(lB,"tf.keras.Model"),lB.forEach(t),jy=s(ug,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),ug.forEach(t),Cy=h(Zt),T(En.$$.fragment,Zt),Py=h(Zt),Wt=a(Zt,"DIV",{class:!0});var ur=i(Wt);T(Zi.$$.fragment,ur),qy=h(ur),hs=a(ur,"P",{});var ip=i(hs);Ay=s(ip,"The "),Oc=a(ip,"A",{href:!0});var dB=i(Oc);Oy=s(dB,"TFDistilBertForMultipleChoice"),dB.forEach(t),Ly=s(ip," forward method, overrides the "),fh=a(ip,"CODE",{});var cB=i(fh);Iy=s(cB,"__call__"),cB.forEach(t),Sy=s(ip," special method."),ip.forEach(t),Ny=h(ur),T(Bn.$$.fragment,ur),Ry=h(ur),T(xn.$$.fragment,ur),ur.forEach(t),Zt.forEach(t),nm=h(n),fs=a(n,"H2",{class:!0});var mg=i(fs);Mn=a(mg,"A",{id:!0,class:!0,href:!0});var pB=i(Mn);uh=a(pB,"SPAN",{});var hB=i(uh);T(el.$$.fragment,hB),hB.forEach(t),pB.forEach(t),Wy=h(mg),mh=a(mg,"SPAN",{});var fB=i(mh);Qy=s(fB,"TFDistilBertForTokenClassification"),fB.forEach(t),mg.forEach(t),rm=h(n),ht=a(n,"DIV",{class:!0});var eo=i(ht);T(tl.$$.fragment,eo),Uy=h(eo),gh=a(eo,"P",{});var uB=i(gh);Hy=s(uB,`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),uB.forEach(t),Ky=h(eo),ol=a(eo,"P",{});var gg=i(ol);Vy=s(gg,"This model inherits from "),Lc=a(gg,"A",{href:!0});var mB=i(Lc);Jy=s(mB,"TFPreTrainedModel"),mB.forEach(t),Gy=s(gg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),gg.forEach(t),Xy=h(eo),sl=a(eo,"P",{});var _g=i(sl);Yy=s(_g,"This model is also a "),nl=a(_g,"A",{href:!0,rel:!0});var gB=i(nl);Zy=s(gB,"tf.keras.Model"),gB.forEach(t),e2=s(_g,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),_g.forEach(t),t2=h(eo),T(zn.$$.fragment,eo),o2=h(eo),Dt=a(eo,"DIV",{class:!0});var So=i(Dt);T(rl.$$.fragment,So),s2=h(So),us=a(So,"P",{});var lp=i(us);n2=s(lp,"The "),Ic=a(lp,"A",{href:!0});var _B=i(Ic);r2=s(_B,"TFDistilBertForTokenClassification"),_B.forEach(t),a2=s(lp," forward method, overrides the "),_h=a(lp,"CODE",{});var bB=i(_h);i2=s(bB,"__call__"),bB.forEach(t),l2=s(lp," special method."),lp.forEach(t),d2=h(So),T(jn.$$.fragment,So),c2=h(So),T(Cn.$$.fragment,So),p2=h(So),T(Pn.$$.fragment,So),So.forEach(t),eo.forEach(t),am=h(n),ms=a(n,"H2",{class:!0});var bg=i(ms);qn=a(bg,"A",{id:!0,class:!0,href:!0});var vB=i(qn);bh=a(vB,"SPAN",{});var kB=i(bh);T(al.$$.fragment,kB),kB.forEach(t),vB.forEach(t),h2=h(bg),vh=a(bg,"SPAN",{});var wB=i(vh);f2=s(wB,"TFDistilBertForQuestionAnswering"),wB.forEach(t),bg.forEach(t),im=h(n),ft=a(n,"DIV",{class:!0});var to=i(ft);T(il.$$.fragment,to),u2=h(to),gs=a(to,"P",{});var dp=i(gs);m2=s(dp,`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layer on top of the hidden-states output to compute `),kh=a(dp,"CODE",{});var TB=i(kh);g2=s(TB,"span start logits"),TB.forEach(t),_2=s(dp," and "),wh=a(dp,"CODE",{});var $B=i(wh);b2=s($B,"span end logits"),$B.forEach(t),v2=s(dp,")."),dp.forEach(t),k2=h(to),ll=a(to,"P",{});var vg=i(ll);w2=s(vg,"This model inherits from "),Sc=a(vg,"A",{href:!0});var yB=i(Sc);T2=s(yB,"TFPreTrainedModel"),yB.forEach(t),$2=s(vg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),vg.forEach(t),y2=h(to),dl=a(to,"P",{});var kg=i(dl);D2=s(kg,"This model is also a "),cl=a(kg,"A",{href:!0,rel:!0});var DB=i(cl);F2=s(DB,"tf.keras.Model"),DB.forEach(t),E2=s(kg,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),kg.forEach(t),B2=h(to),T(An.$$.fragment,to),x2=h(to),Ft=a(to,"DIV",{class:!0});var No=i(Ft);T(pl.$$.fragment,No),M2=h(No),_s=a(No,"P",{});var cp=i(_s);z2=s(cp,"The "),Nc=a(cp,"A",{href:!0});var FB=i(Nc);j2=s(FB,"TFDistilBertForQuestionAnswering"),FB.forEach(t),C2=s(cp," forward method, overrides the "),Th=a(cp,"CODE",{});var EB=i(Th);P2=s(EB,"__call__"),EB.forEach(t),q2=s(cp," special method."),cp.forEach(t),A2=h(No),T(On.$$.fragment,No),O2=h(No),T(Ln.$$.fragment,No),L2=h(No),T(In.$$.fragment,No),No.forEach(t),to.forEach(t),lm=h(n),bs=a(n,"H2",{class:!0});var wg=i(bs);Sn=a(wg,"A",{id:!0,class:!0,href:!0});var BB=i(Sn);$h=a(BB,"SPAN",{});var xB=i($h);T(hl.$$.fragment,xB),xB.forEach(t),BB.forEach(t),I2=h(wg),yh=a(wg,"SPAN",{});var MB=i(yh);S2=s(MB,"FlaxDistilBertModel"),MB.forEach(t),wg.forEach(t),dm=h(n),tt=a(n,"DIV",{class:!0});var Bt=i(tt);T(fl.$$.fragment,Bt),N2=h(Bt),Dh=a(Bt,"P",{});var zB=i(Dh);R2=s(zB,"The bare DistilBert Model transformer outputting raw hidden-states without any specific head on top."),zB.forEach(t),W2=h(Bt),ul=a(Bt,"P",{});var Tg=i(ul);Q2=s(Tg,"This model inherits from "),Rc=a(Tg,"A",{href:!0});var jB=i(Rc);U2=s(jB,"FlaxPreTrainedModel"),jB.forEach(t),H2=s(Tg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),Tg.forEach(t),K2=h(Bt),ml=a(Bt,"P",{});var $g=i(ml);V2=s($g,"This model is also a Flax Linen "),gl=a($g,"A",{href:!0,rel:!0});var CB=i(gl);J2=s(CB,"flax.linen.Module"),CB.forEach(t),G2=s($g,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),$g.forEach(t),X2=h(Bt),Fh=a(Bt,"P",{});var PB=i(Fh);Y2=s(PB,"Finally, this model supports inherent JAX features such as:"),PB.forEach(t),Z2=h(Bt),so=a(Bt,"UL",{});var mr=i(so);Eh=a(mr,"LI",{});var qB=i(Eh);_l=a(qB,"A",{href:!0,rel:!0});var AB=i(_l);e1=s(AB,"Just-In-Time (JIT) compilation"),AB.forEach(t),qB.forEach(t),t1=h(mr),Bh=a(mr,"LI",{});var OB=i(Bh);bl=a(OB,"A",{href:!0,rel:!0});var LB=i(bl);o1=s(LB,"Automatic Differentiation"),LB.forEach(t),OB.forEach(t),s1=h(mr),xh=a(mr,"LI",{});var IB=i(xh);vl=a(IB,"A",{href:!0,rel:!0});var SB=i(vl);n1=s(SB,"Vectorization"),SB.forEach(t),IB.forEach(t),r1=h(mr),Mh=a(mr,"LI",{});var NB=i(Mh);kl=a(NB,"A",{href:!0,rel:!0});var RB=i(kl);a1=s(RB,"Parallelization"),RB.forEach(t),NB.forEach(t),mr.forEach(t),i1=h(Bt),Qt=a(Bt,"DIV",{class:!0});var gr=i(Qt);T(wl.$$.fragment,gr),l1=h(gr),vs=a(gr,"P",{});var pp=i(vs);d1=s(pp,"The "),zh=a(pp,"CODE",{});var WB=i(zh);c1=s(WB,"FlaxDistilBertPreTrainedModel"),WB.forEach(t),p1=s(pp," forward method, overrides the "),jh=a(pp,"CODE",{});var QB=i(jh);h1=s(QB,"__call__"),QB.forEach(t),f1=s(pp," special method."),pp.forEach(t),u1=h(gr),T(Nn.$$.fragment,gr),m1=h(gr),T(Rn.$$.fragment,gr),gr.forEach(t),Bt.forEach(t),cm=h(n),ks=a(n,"H2",{class:!0});var yg=i(ks);Wn=a(yg,"A",{id:!0,class:!0,href:!0});var UB=i(Wn);Ch=a(UB,"SPAN",{});var HB=i(Ch);T(Tl.$$.fragment,HB),HB.forEach(t),UB.forEach(t),g1=h(yg),Ph=a(yg,"SPAN",{});var KB=i(Ph);_1=s(KB,"FlaxDistilBertForMaskedLM"),KB.forEach(t),yg.forEach(t),pm=h(n),ot=a(n,"DIV",{class:!0});var xt=i(ot);T($l.$$.fragment,xt),b1=h(xt),yl=a(xt,"P",{});var Dg=i(yl);v1=s(Dg,"DistilBert Model with a "),qh=a(Dg,"CODE",{});var VB=i(qh);k1=s(VB,"language modeling"),VB.forEach(t),w1=s(Dg," head on top."),Dg.forEach(t),T1=h(xt),Dl=a(xt,"P",{});var Fg=i(Dl);$1=s(Fg,"This model inherits from "),Wc=a(Fg,"A",{href:!0});var JB=i(Wc);y1=s(JB,"FlaxPreTrainedModel"),JB.forEach(t),D1=s(Fg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),Fg.forEach(t),F1=h(xt),Fl=a(xt,"P",{});var Eg=i(Fl);E1=s(Eg,"This model is also a Flax Linen "),El=a(Eg,"A",{href:!0,rel:!0});var GB=i(El);B1=s(GB,"flax.linen.Module"),GB.forEach(t),x1=s(Eg,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Eg.forEach(t),M1=h(xt),Ah=a(xt,"P",{});var XB=i(Ah);z1=s(XB,"Finally, this model supports inherent JAX features such as:"),XB.forEach(t),j1=h(xt),no=a(xt,"UL",{});var _r=i(no);Oh=a(_r,"LI",{});var YB=i(Oh);Bl=a(YB,"A",{href:!0,rel:!0});var ZB=i(Bl);C1=s(ZB,"Just-In-Time (JIT) compilation"),ZB.forEach(t),YB.forEach(t),P1=h(_r),Lh=a(_r,"LI",{});var ex=i(Lh);xl=a(ex,"A",{href:!0,rel:!0});var tx=i(xl);q1=s(tx,"Automatic Differentiation"),tx.forEach(t),ex.forEach(t),A1=h(_r),Ih=a(_r,"LI",{});var ox=i(Ih);Ml=a(ox,"A",{href:!0,rel:!0});var sx=i(Ml);O1=s(sx,"Vectorization"),sx.forEach(t),ox.forEach(t),L1=h(_r),Sh=a(_r,"LI",{});var nx=i(Sh);zl=a(nx,"A",{href:!0,rel:!0});var rx=i(zl);I1=s(rx,"Parallelization"),rx.forEach(t),nx.forEach(t),_r.forEach(t),S1=h(xt),Ut=a(xt,"DIV",{class:!0});var br=i(Ut);T(jl.$$.fragment,br),N1=h(br),ws=a(br,"P",{});var hp=i(ws);R1=s(hp,"The "),Nh=a(hp,"CODE",{});var ax=i(Nh);W1=s(ax,"FlaxDistilBertPreTrainedModel"),ax.forEach(t),Q1=s(hp," forward method, overrides the "),Rh=a(hp,"CODE",{});var ix=i(Rh);U1=s(ix,"__call__"),ix.forEach(t),H1=s(hp," special method."),hp.forEach(t),K1=h(br),T(Qn.$$.fragment,br),V1=h(br),T(Un.$$.fragment,br),br.forEach(t),xt.forEach(t),hm=h(n),Ts=a(n,"H2",{class:!0});var Bg=i(Ts);Hn=a(Bg,"A",{id:!0,class:!0,href:!0});var lx=i(Hn);Wh=a(lx,"SPAN",{});var dx=i(Wh);T(Cl.$$.fragment,dx),dx.forEach(t),lx.forEach(t),J1=h(Bg),Qh=a(Bg,"SPAN",{});var cx=i(Qh);G1=s(cx,"FlaxDistilBertForSequenceClassification"),cx.forEach(t),Bg.forEach(t),fm=h(n),st=a(n,"DIV",{class:!0});var Mt=i(st);T(Pl.$$.fragment,Mt),X1=h(Mt),Uh=a(Mt,"P",{});var px=i(Uh);Y1=s(px,`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),px.forEach(t),Z1=h(Mt),ql=a(Mt,"P",{});var xg=i(ql);e4=s(xg,"This model inherits from "),Qc=a(xg,"A",{href:!0});var hx=i(Qc);t4=s(hx,"FlaxPreTrainedModel"),hx.forEach(t),o4=s(xg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),xg.forEach(t),s4=h(Mt),Al=a(Mt,"P",{});var Mg=i(Al);n4=s(Mg,"This model is also a Flax Linen "),Ol=a(Mg,"A",{href:!0,rel:!0});var fx=i(Ol);r4=s(fx,"flax.linen.Module"),fx.forEach(t),a4=s(Mg,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Mg.forEach(t),i4=h(Mt),Hh=a(Mt,"P",{});var ux=i(Hh);l4=s(ux,"Finally, this model supports inherent JAX features such as:"),ux.forEach(t),d4=h(Mt),ro=a(Mt,"UL",{});var vr=i(ro);Kh=a(vr,"LI",{});var mx=i(Kh);Ll=a(mx,"A",{href:!0,rel:!0});var gx=i(Ll);c4=s(gx,"Just-In-Time (JIT) compilation"),gx.forEach(t),mx.forEach(t),p4=h(vr),Vh=a(vr,"LI",{});var _x=i(Vh);Il=a(_x,"A",{href:!0,rel:!0});var bx=i(Il);h4=s(bx,"Automatic Differentiation"),bx.forEach(t),_x.forEach(t),f4=h(vr),Jh=a(vr,"LI",{});var vx=i(Jh);Sl=a(vx,"A",{href:!0,rel:!0});var kx=i(Sl);u4=s(kx,"Vectorization"),kx.forEach(t),vx.forEach(t),m4=h(vr),Gh=a(vr,"LI",{});var wx=i(Gh);Nl=a(wx,"A",{href:!0,rel:!0});var Tx=i(Nl);g4=s(Tx,"Parallelization"),Tx.forEach(t),wx.forEach(t),vr.forEach(t),_4=h(Mt),Ht=a(Mt,"DIV",{class:!0});var kr=i(Ht);T(Rl.$$.fragment,kr),b4=h(kr),$s=a(kr,"P",{});var fp=i($s);v4=s(fp,"The "),Xh=a(fp,"CODE",{});var $x=i(Xh);k4=s($x,"FlaxDistilBertPreTrainedModel"),$x.forEach(t),w4=s(fp," forward method, overrides the "),Yh=a(fp,"CODE",{});var yx=i(Yh);T4=s(yx,"__call__"),yx.forEach(t),$4=s(fp," special method."),fp.forEach(t),y4=h(kr),T(Kn.$$.fragment,kr),D4=h(kr),T(Vn.$$.fragment,kr),kr.forEach(t),Mt.forEach(t),um=h(n),ys=a(n,"H2",{class:!0});var zg=i(ys);Jn=a(zg,"A",{id:!0,class:!0,href:!0});var Dx=i(Jn);Zh=a(Dx,"SPAN",{});var Fx=i(Zh);T(Wl.$$.fragment,Fx),Fx.forEach(t),Dx.forEach(t),F4=h(zg),ef=a(zg,"SPAN",{});var Ex=i(ef);E4=s(Ex,"FlaxDistilBertForMultipleChoice"),Ex.forEach(t),zg.forEach(t),mm=h(n),nt=a(n,"DIV",{class:!0});var zt=i(nt);T(Ql.$$.fragment,zt),B4=h(zt),tf=a(zt,"P",{});var Bx=i(tf);x4=s(Bx,`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),Bx.forEach(t),M4=h(zt),Ul=a(zt,"P",{});var jg=i(Ul);z4=s(jg,"This model inherits from "),Uc=a(jg,"A",{href:!0});var xx=i(Uc);j4=s(xx,"FlaxPreTrainedModel"),xx.forEach(t),C4=s(jg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),jg.forEach(t),P4=h(zt),Hl=a(zt,"P",{});var Cg=i(Hl);q4=s(Cg,"This model is also a Flax Linen "),Kl=a(Cg,"A",{href:!0,rel:!0});var Mx=i(Kl);A4=s(Mx,"flax.linen.Module"),Mx.forEach(t),O4=s(Cg,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Cg.forEach(t),L4=h(zt),of=a(zt,"P",{});var zx=i(of);I4=s(zx,"Finally, this model supports inherent JAX features such as:"),zx.forEach(t),S4=h(zt),ao=a(zt,"UL",{});var wr=i(ao);sf=a(wr,"LI",{});var jx=i(sf);Vl=a(jx,"A",{href:!0,rel:!0});var Cx=i(Vl);N4=s(Cx,"Just-In-Time (JIT) compilation"),Cx.forEach(t),jx.forEach(t),R4=h(wr),nf=a(wr,"LI",{});var Px=i(nf);Jl=a(Px,"A",{href:!0,rel:!0});var qx=i(Jl);W4=s(qx,"Automatic Differentiation"),qx.forEach(t),Px.forEach(t),Q4=h(wr),rf=a(wr,"LI",{});var Ax=i(rf);Gl=a(Ax,"A",{href:!0,rel:!0});var Ox=i(Gl);U4=s(Ox,"Vectorization"),Ox.forEach(t),Ax.forEach(t),H4=h(wr),af=a(wr,"LI",{});var Lx=i(af);Xl=a(Lx,"A",{href:!0,rel:!0});var Ix=i(Xl);K4=s(Ix,"Parallelization"),Ix.forEach(t),Lx.forEach(t),wr.forEach(t),V4=h(zt),Kt=a(zt,"DIV",{class:!0});var Tr=i(Kt);T(Yl.$$.fragment,Tr),J4=h(Tr),Ds=a(Tr,"P",{});var up=i(Ds);G4=s(up,"The "),lf=a(up,"CODE",{});var Sx=i(lf);X4=s(Sx,"FlaxDistilBertPreTrainedModel"),Sx.forEach(t),Y4=s(up," forward method, overrides the "),df=a(up,"CODE",{});var Nx=i(df);Z4=s(Nx,"__call__"),Nx.forEach(t),e0=s(up," special method."),up.forEach(t),t0=h(Tr),T(Gn.$$.fragment,Tr),o0=h(Tr),T(Xn.$$.fragment,Tr),Tr.forEach(t),zt.forEach(t),gm=h(n),Fs=a(n,"H2",{class:!0});var Pg=i(Fs);Yn=a(Pg,"A",{id:!0,class:!0,href:!0});var Rx=i(Yn);cf=a(Rx,"SPAN",{});var Wx=i(cf);T(Zl.$$.fragment,Wx),Wx.forEach(t),Rx.forEach(t),s0=h(Pg),pf=a(Pg,"SPAN",{});var Qx=i(pf);n0=s(Qx,"FlaxDistilBertForTokenClassification"),Qx.forEach(t),Pg.forEach(t),_m=h(n),rt=a(n,"DIV",{class:!0});var jt=i(rt);T(ed.$$.fragment,jt),r0=h(jt),hf=a(jt,"P",{});var Ux=i(hf);a0=s(Ux,`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),Ux.forEach(t),i0=h(jt),td=a(jt,"P",{});var qg=i(td);l0=s(qg,"This model inherits from "),Hc=a(qg,"A",{href:!0});var Hx=i(Hc);d0=s(Hx,"FlaxPreTrainedModel"),Hx.forEach(t),c0=s(qg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),qg.forEach(t),p0=h(jt),od=a(jt,"P",{});var Ag=i(od);h0=s(Ag,"This model is also a Flax Linen "),sd=a(Ag,"A",{href:!0,rel:!0});var Kx=i(sd);f0=s(Kx,"flax.linen.Module"),Kx.forEach(t),u0=s(Ag,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Ag.forEach(t),m0=h(jt),ff=a(jt,"P",{});var Vx=i(ff);g0=s(Vx,"Finally, this model supports inherent JAX features such as:"),Vx.forEach(t),_0=h(jt),io=a(jt,"UL",{});var $r=i(io);uf=a($r,"LI",{});var Jx=i(uf);nd=a(Jx,"A",{href:!0,rel:!0});var Gx=i(nd);b0=s(Gx,"Just-In-Time (JIT) compilation"),Gx.forEach(t),Jx.forEach(t),v0=h($r),mf=a($r,"LI",{});var Xx=i(mf);rd=a(Xx,"A",{href:!0,rel:!0});var Yx=i(rd);k0=s(Yx,"Automatic Differentiation"),Yx.forEach(t),Xx.forEach(t),w0=h($r),gf=a($r,"LI",{});var Zx=i(gf);ad=a(Zx,"A",{href:!0,rel:!0});var eM=i(ad);T0=s(eM,"Vectorization"),eM.forEach(t),Zx.forEach(t),$0=h($r),_f=a($r,"LI",{});var tM=i(_f);id=a(tM,"A",{href:!0,rel:!0});var oM=i(id);y0=s(oM,"Parallelization"),oM.forEach(t),tM.forEach(t),$r.forEach(t),D0=h(jt),Vt=a(jt,"DIV",{class:!0});var yr=i(Vt);T(ld.$$.fragment,yr),F0=h(yr),Es=a(yr,"P",{});var mp=i(Es);E0=s(mp,"The "),bf=a(mp,"CODE",{});var sM=i(bf);B0=s(sM,"FlaxDistilBertPreTrainedModel"),sM.forEach(t),x0=s(mp," forward method, overrides the "),vf=a(mp,"CODE",{});var nM=i(vf);M0=s(nM,"__call__"),nM.forEach(t),z0=s(mp," special method."),mp.forEach(t),j0=h(yr),T(Zn.$$.fragment,yr),C0=h(yr),T(er.$$.fragment,yr),yr.forEach(t),jt.forEach(t),bm=h(n),Bs=a(n,"H2",{class:!0});var Og=i(Bs);tr=a(Og,"A",{id:!0,class:!0,href:!0});var rM=i(tr);kf=a(rM,"SPAN",{});var aM=i(kf);T(dd.$$.fragment,aM),aM.forEach(t),rM.forEach(t),P0=h(Og),wf=a(Og,"SPAN",{});var iM=i(wf);q0=s(iM,"FlaxDistilBertForQuestionAnswering"),iM.forEach(t),Og.forEach(t),vm=h(n),at=a(n,"DIV",{class:!0});var Ct=i(at);T(cd.$$.fragment,Ct),A0=h(Ct),xs=a(Ct,"P",{});var gp=i(xs);O0=s(gp,`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),Tf=a(gp,"CODE",{});var lM=i(Tf);L0=s(lM,"span start logits"),lM.forEach(t),I0=s(gp," and "),$f=a(gp,"CODE",{});var dM=i($f);S0=s(dM,"span end logits"),dM.forEach(t),N0=s(gp,")."),gp.forEach(t),R0=h(Ct),pd=a(Ct,"P",{});var Lg=i(pd);W0=s(Lg,"This model inherits from "),Kc=a(Lg,"A",{href:!0});var cM=i(Kc);Q0=s(cM,"FlaxPreTrainedModel"),cM.forEach(t),U0=s(Lg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),Lg.forEach(t),H0=h(Ct),hd=a(Ct,"P",{});var Ig=i(hd);K0=s(Ig,"This model is also a Flax Linen "),fd=a(Ig,"A",{href:!0,rel:!0});var pM=i(fd);V0=s(pM,"flax.linen.Module"),pM.forEach(t),J0=s(Ig,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Ig.forEach(t),G0=h(Ct),yf=a(Ct,"P",{});var hM=i(yf);X0=s(hM,"Finally, this model supports inherent JAX features such as:"),hM.forEach(t),Y0=h(Ct),lo=a(Ct,"UL",{});var Dr=i(lo);Df=a(Dr,"LI",{});var fM=i(Df);ud=a(fM,"A",{href:!0,rel:!0});var uM=i(ud);Z0=s(uM,"Just-In-Time (JIT) compilation"),uM.forEach(t),fM.forEach(t),eD=h(Dr),Ff=a(Dr,"LI",{});var mM=i(Ff);md=a(mM,"A",{href:!0,rel:!0});var gM=i(md);tD=s(gM,"Automatic Differentiation"),gM.forEach(t),mM.forEach(t),oD=h(Dr),Ef=a(Dr,"LI",{});var _M=i(Ef);gd=a(_M,"A",{href:!0,rel:!0});var bM=i(gd);sD=s(bM,"Vectorization"),bM.forEach(t),_M.forEach(t),nD=h(Dr),Bf=a(Dr,"LI",{});var vM=i(Bf);_d=a(vM,"A",{href:!0,rel:!0});var kM=i(_d);rD=s(kM,"Parallelization"),kM.forEach(t),vM.forEach(t),Dr.forEach(t),aD=h(Ct),Jt=a(Ct,"DIV",{class:!0});var Fr=i(Jt);T(bd.$$.fragment,Fr),iD=h(Fr),Ms=a(Fr,"P",{});var _p=i(Ms);lD=s(_p,"The "),xf=a(_p,"CODE",{});var wM=i(xf);dD=s(wM,"FlaxDistilBertPreTrainedModel"),wM.forEach(t),cD=s(_p," forward method, overrides the "),Mf=a(_p,"CODE",{});var TM=i(Mf);pD=s(TM,"__call__"),TM.forEach(t),hD=s(_p," special method."),_p.forEach(t),fD=h(Fr),T(or.$$.fragment,Fr),uD=h(Fr),T(sr.$$.fragment,Fr),Fr.forEach(t),Ct.forEach(t),this.h()},h(){c(d,"name","hf:doc:metadata"),c(d,"content",JSON.stringify(B3)),c(m,"id","distilbert"),c(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(m,"href","#distilbert"),c(f,"class","relative group"),c(re,"id","overview"),c(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(re,"href","#overview"),c(I,"class","relative group"),c(ae,"href","https://medium.com/huggingface/distilbert-8cf3380435b5"),c(ae,"rel","nofollow"),c(ie,"href","https://arxiv.org/abs/1910.01108"),c(ie,"rel","nofollow"),c(Z,"href","https://huggingface.co/victorsanh"),c(Z,"rel","nofollow"),c(z,"href","https://huggingface.co/kamalkraj"),c(z,"rel","nofollow"),c(Le,"href","https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation"),c(Le,"rel","nofollow"),c(Oe,"id","resources"),c(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Oe,"href","#resources"),c(N,"class","relative group"),c(xr,"href","https://huggingface.co/blog/sentiment-analysis-python"),c(xr,"rel","nofollow"),c(zr,"href","https://huggingface.co/blog/fastai"),c(zr,"rel","nofollow"),c(Cr,"href","https://huggingface.co/blog/ray-tune"),c(Cr,"rel","nofollow"),c(qr,"href","https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face"),c(qr,"rel","nofollow"),c(Or,"href","https://colab.research.google.com/github/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb"),c(Or,"rel","nofollow"),c(Ir,"href","https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb"),c(Ir,"rel","nofollow"),c(Nr,"href","https://colab.research.google.com/github/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb"),c(Nr,"rel","nofollow"),c(Nd,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),c(Rr,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification"),c(Rr,"rel","nofollow"),c(Wr,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb"),c(Wr,"rel","nofollow"),c(Rd,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),c(Qr,"href","https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification"),c(Qr,"rel","nofollow"),c(Ur,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb"),c(Ur,"rel","nofollow"),c(Wd,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification"),c(Hr,"href","https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification"),c(Hr,"rel","nofollow"),c(Kr,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb"),c(Kr,"rel","nofollow"),c(Qd,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),c(Jr,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification"),c(Jr,"rel","nofollow"),c(Gr,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb"),c(Gr,"rel","nofollow"),c(Ud,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),c(Xr,"href","https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification"),c(Xr,"rel","nofollow"),c(Yr,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb"),c(Yr,"rel","nofollow"),c(Hd,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification"),c(Zr,"href","https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification"),c(Zr,"rel","nofollow"),c(ea,"href","https://huggingface.co/course/chapter7/2?fw=pt"),c(ea,"rel","nofollow"),c(Vd,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(oa,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling"),c(oa,"rel","nofollow"),c(sa,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb"),c(sa,"rel","nofollow"),c(Jd,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(na,"href","https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy"),c(na,"rel","nofollow"),c(ra,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb"),c(ra,"rel","nofollow"),c(Gd,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM"),c(aa,"href","https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling"),c(aa,"rel","nofollow"),c(ia,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb"),c(ia,"rel","nofollow"),c(la,"href","https://huggingface.co/course/chapter7/3?fw=pt"),c(la,"rel","nofollow"),c(Yd,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),c(ca,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering"),c(ca,"rel","nofollow"),c(pa,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb"),c(pa,"rel","nofollow"),c(Zd,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),c(ha,"href","https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering"),c(ha,"rel","nofollow"),c(fa,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb"),c(fa,"rel","nofollow"),c(ec,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering"),c(ua,"href","https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering"),c(ua,"rel","nofollow"),c(ma,"href","https://huggingface.co/course/chapter7/7?fw=pt"),c(ma,"rel","nofollow"),c(sc,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),c(ga,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice"),c(ga,"rel","nofollow"),c(_a,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb"),c(_a,"rel","nofollow"),c(nc,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),c(ba,"href","https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice"),c(ba,"rel","nofollow"),c(va,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb"),c(va,"rel","nofollow"),c(wa,"href","https://huggingface.co/blog/intel"),c(wa,"rel","nofollow"),c($a,"href","https://www.philschmid.de/optimizing-transformers-with-optimum-gpu"),c($a,"rel","nofollow"),c(Da,"href","https://www.philschmid.de/optimizing-transformers-with-optimum"),c(Da,"rel","nofollow"),c(Ea,"href","https://huggingface.co/blog/bert-inferentia-sagemaker"),c(Ea,"rel","nofollow"),c(xa,"href","https://www.philschmid.de/sagemaker-serverless-huggingface-distilbert"),c(xa,"rel","nofollow"),c(za,"href","https://huggingface.co/blog/how-to-deploy-a-pipeline-to-google-clouds"),c(za,"rel","nofollow"),c(Ca,"href","https://huggingface.co/blog/deploy-hugging-face-models-easily-with-amazon-sagemaker"),c(Ca,"rel","nofollow"),c(qa,"href","https://www.philschmid.de/terraform-huggingface-amazon-sagemaker"),c(qa,"rel","nofollow"),c(qs,"id","transformers.DistilBertConfig"),c(qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qs,"href","#transformers.DistilBertConfig"),c(Ro,"class","relative group"),c(lc,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertModel"),c(dc,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(La,"href","https://huggingface.co/distilbert-base-uncased"),c(La,"rel","nofollow"),c(cc,"href","/docs/transformers/v4.24.0/en/main_classes/configuration#transformers.PretrainedConfig"),c(pc,"href","/docs/transformers/v4.24.0/en/main_classes/configuration#transformers.PretrainedConfig"),c(Pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Os,"id","transformers.DistilBertTokenizer"),c(Os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Os,"href","#transformers.DistilBertTokenizer"),c(Qo,"class","relative group"),c(hc,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertTokenizer"),c(fc,"href","/docs/transformers/v4.24.0/en/model_doc/bert#transformers.BertTokenizer"),c(uc,"href","/docs/transformers/v4.24.0/en/model_doc/bert#transformers.BertTokenizer"),c(qt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Is,"id","transformers.DistilBertTokenizerFast"),c(Is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Is,"href","#transformers.DistilBertTokenizerFast"),c(Uo,"class","relative group"),c(mc,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),c(gc,"href","/docs/transformers/v4.24.0/en/model_doc/bert#transformers.BertTokenizerFast"),c(_c,"href","/docs/transformers/v4.24.0/en/model_doc/bert#transformers.BertTokenizerFast"),c(At,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Ns,"id","transformers.DistilBertModel"),c(Ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ns,"href","#transformers.DistilBertModel"),c(Ho,"class","relative group"),c(bc,"href","/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel"),c(Ga,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ga,"rel","nofollow"),c(vc,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertModel"),c(St,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Qs,"id","transformers.DistilBertForMaskedLM"),c(Qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qs,"href","#transformers.DistilBertForMaskedLM"),c(Vo,"class","relative group"),c(kc,"href","/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel"),c(si,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(si,"rel","nofollow"),c(wc,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(kt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(mt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Vs,"id","transformers.DistilBertForSequenceClassification"),c(Vs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Vs,"href","#transformers.DistilBertForSequenceClassification"),c(Go,"class","relative group"),c(Tc,"href","/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel"),c(di,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(di,"rel","nofollow"),c($c,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),c(it,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(gt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(en,"id","transformers.DistilBertForMultipleChoice"),c(en,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(en,"href","#transformers.DistilBertForMultipleChoice"),c(Yo,"class","relative group"),c(yc,"href","/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel"),c(mi,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(mi,"rel","nofollow"),c(Dc,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),c(Nt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(_t,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(sn,"id","transformers.DistilBertForTokenClassification"),c(sn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(sn,"href","#transformers.DistilBertForTokenClassification"),c(es,"class","relative group"),c(Fc,"href","/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel"),c(wi,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(wi,"rel","nofollow"),c(Ec,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),c(wt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(bt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ln,"id","transformers.DistilBertForQuestionAnswering"),c(ln,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ln,"href","#transformers.DistilBertForQuestionAnswering"),c(os,"class","relative group"),c(Bc,"href","/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel"),c(Ei,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ei,"rel","nofollow"),c(xc,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),c(Tt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(vt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(hn,"id","transformers.TFDistilBertModel"),c(hn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(hn,"href","#transformers.TFDistilBertModel"),c(rs,"class","relative group"),c(Mc,"href","/docs/transformers/v4.24.0/en/main_classes/model#transformers.TFPreTrainedModel"),c(Ci,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Ci,"rel","nofollow"),c(zc,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(Rt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(lt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(gn,"id","transformers.TFDistilBertForMaskedLM"),c(gn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(gn,"href","#transformers.TFDistilBertForMaskedLM"),c(is,"class","relative group"),c(jc,"href","/docs/transformers/v4.24.0/en/main_classes/model#transformers.TFPreTrainedModel"),c(Si,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Si,"rel","nofollow"),c(Cc,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c($t,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(dt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(wn,"id","transformers.TFDistilBertForSequenceClassification"),c(wn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(wn,"href","#transformers.TFDistilBertForSequenceClassification"),c(ds,"class","relative group"),c(Pc,"href","/docs/transformers/v4.24.0/en/main_classes/model#transformers.TFPreTrainedModel"),c(Hi,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Hi,"rel","nofollow"),c(qc,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),c(yt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ct,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Fn,"id","transformers.TFDistilBertForMultipleChoice"),c(Fn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fn,"href","#transformers.TFDistilBertForMultipleChoice"),c(ps,"class","relative group"),c(Ac,"href","/docs/transformers/v4.24.0/en/main_classes/model#transformers.TFPreTrainedModel"),c(Yi,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Yi,"rel","nofollow"),c(Oc,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),c(Wt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Mn,"id","transformers.TFDistilBertForTokenClassification"),c(Mn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Mn,"href","#transformers.TFDistilBertForTokenClassification"),c(fs,"class","relative group"),c(Lc,"href","/docs/transformers/v4.24.0/en/main_classes/model#transformers.TFPreTrainedModel"),c(nl,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(nl,"rel","nofollow"),c(Ic,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),c(Dt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ht,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(qn,"id","transformers.TFDistilBertForQuestionAnswering"),c(qn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qn,"href","#transformers.TFDistilBertForQuestionAnswering"),c(ms,"class","relative group"),c(Sc,"href","/docs/transformers/v4.24.0/en/main_classes/model#transformers.TFPreTrainedModel"),c(cl,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(cl,"rel","nofollow"),c(Nc,"href","/docs/transformers/v4.24.0/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),c(Ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Sn,"id","transformers.FlaxDistilBertModel"),c(Sn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Sn,"href","#transformers.FlaxDistilBertModel"),c(bs,"class","relative group"),c(Rc,"href","/docs/transformers/v4.24.0/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(gl,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(gl,"rel","nofollow"),c(_l,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(_l,"rel","nofollow"),c(bl,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(bl,"rel","nofollow"),c(vl,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(vl,"rel","nofollow"),c(kl,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(kl,"rel","nofollow"),c(Qt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(tt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Wn,"id","transformers.FlaxDistilBertForMaskedLM"),c(Wn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Wn,"href","#transformers.FlaxDistilBertForMaskedLM"),c(ks,"class","relative group"),c(Wc,"href","/docs/transformers/v4.24.0/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(El,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(El,"rel","nofollow"),c(Bl,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(Bl,"rel","nofollow"),c(xl,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(xl,"rel","nofollow"),c(Ml,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(Ml,"rel","nofollow"),c(zl,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(zl,"rel","nofollow"),c(Ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ot,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Hn,"id","transformers.FlaxDistilBertForSequenceClassification"),c(Hn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Hn,"href","#transformers.FlaxDistilBertForSequenceClassification"),c(Ts,"class","relative group"),c(Qc,"href","/docs/transformers/v4.24.0/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(Ol,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(Ol,"rel","nofollow"),c(Ll,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(Ll,"rel","nofollow"),c(Il,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(Il,"rel","nofollow"),c(Sl,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(Sl,"rel","nofollow"),c(Nl,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(Nl,"rel","nofollow"),c(Ht,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(st,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Jn,"id","transformers.FlaxDistilBertForMultipleChoice"),c(Jn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Jn,"href","#transformers.FlaxDistilBertForMultipleChoice"),c(ys,"class","relative group"),c(Uc,"href","/docs/transformers/v4.24.0/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(Kl,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(Kl,"rel","nofollow"),c(Vl,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(Vl,"rel","nofollow"),c(Jl,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(Jl,"rel","nofollow"),c(Gl,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(Gl,"rel","nofollow"),c(Xl,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(Xl,"rel","nofollow"),c(Kt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(nt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Yn,"id","transformers.FlaxDistilBertForTokenClassification"),c(Yn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Yn,"href","#transformers.FlaxDistilBertForTokenClassification"),c(Fs,"class","relative group"),c(Hc,"href","/docs/transformers/v4.24.0/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(sd,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(sd,"rel","nofollow"),c(nd,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(nd,"rel","nofollow"),c(rd,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(rd,"rel","nofollow"),c(ad,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(ad,"rel","nofollow"),c(id,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(id,"rel","nofollow"),c(Vt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(rt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(tr,"id","transformers.FlaxDistilBertForQuestionAnswering"),c(tr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(tr,"href","#transformers.FlaxDistilBertForQuestionAnswering"),c(Bs,"class","relative group"),c(Kc,"href","/docs/transformers/v4.24.0/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(fd,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(fd,"rel","nofollow"),c(ud,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(ud,"rel","nofollow"),c(md,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(md,"rel","nofollow"),c(gd,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(gd,"rel","nofollow"),c(_d,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(_d,"rel","nofollow"),c(Jt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(at,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(n,g){e(document.head,d),_(n,b,g),_(n,f,g),e(f,m),e(m,v),$(l,v,null),e(f,u),e(f,M),e(M,we),_(n,ge,g),_(n,I,g),e(I,re),e(re,oe),$(E,oe,null),e(I,Te),e(I,Q),e(Q,$e),_(n,_e,g),_(n,O,g),e(O,ye),e(O,ae),e(ae,H),e(O,De),e(O,ie),e(ie,K),e(O,Fe),e(O,de),e(de,V),e(O,Ee),_(n,be,g),_(n,ee,g),e(ee,j),_(n,q,g),_(n,le,g),e(le,U),e(U,Be),_(n,ve,g),_(n,W,g),e(W,xe),_(n,ke,g),_(n,C,g),e(C,se),e(se,J),e(se,ce),e(ce,Me),e(se,G),e(se,pe),e(pe,ze),e(se,S),e(se,he),e(he,X),e(se,je),e(C,ne),e(C,P),e(P,Ce),e(P,A),e(A,Pe),e(P,qe),_(n,k,g),_(n,x,g),e(x,He),e(x,Z),e(Z,Ke),e(x,Ve),e(x,z),e(z,Je),e(x,Ge),e(x,Le),e(Le,Xe),e(x,Ye),_(n,L,g),_(n,N,g),e(N,Oe),e(Oe,Re),$(R,Re,null),e(N,Ze),e(N,We),e(We,Ae),_(n,Ue,g),_(n,te,g),e(te,et),_(n,gu,g),$(Er,n,g),_(n,_u,g),_(n,Ie,g),e(Ie,Br),e(Br,Ng),e(Br,xr),e(xr,Rg),e(Br,Wg),e(Ie,Qg),e(Ie,Mr),e(Mr,Ug),e(Mr,zr),e(zr,Hg),e(Mr,Kg),e(Ie,Vg),e(Ie,jr),e(jr,Jg),e(jr,Cr),e(Cr,Gg),e(jr,Xg),e(Ie,Yg),e(Ie,Pr),e(Pr,Zg),e(Pr,qr),e(qr,e_),e(Pr,t_),e(Ie,o_),e(Ie,Ar),e(Ar,s_),e(Ar,Or),e(Or,n_),e(Ar,r_),e(Ie,a_),e(Ie,Lr),e(Lr,i_),e(Lr,Ir),e(Ir,l_),e(Lr,d_),e(Ie,c_),e(Ie,Sr),e(Sr,p_),e(Sr,Nr),e(Nr,h_),e(Sr,f_),e(Ie,u_),e(Ie,fo),e(fo,Nd),e(Nd,m_),e(fo,g_),e(fo,Rr),e(Rr,__),e(fo,b_),e(fo,Wr),e(Wr,v_),e(fo,k_),e(Ie,w_),e(Ie,uo),e(uo,Rd),e(Rd,T_),e(uo,$_),e(uo,Qr),e(Qr,y_),e(uo,D_),e(uo,Ur),e(Ur,F_),e(uo,E_),e(Ie,B_),e(Ie,mo),e(mo,Wd),e(Wd,x_),e(mo,M_),e(mo,Hr),e(Hr,z_),e(mo,j_),e(mo,Kr),e(Kr,C_),e(mo,P_),_(n,bu,g),$(Vr,n,g),_(n,vu,g),_(n,Ot,g),e(Ot,go),e(go,Qd),e(Qd,q_),e(go,A_),e(go,Jr),e(Jr,O_),e(go,L_),e(go,Gr),e(Gr,I_),e(go,S_),e(Ot,N_),e(Ot,_o),e(_o,Ud),e(Ud,R_),e(_o,W_),e(_o,Xr),e(Xr,Q_),e(_o,U_),e(_o,Yr),e(Yr,H_),e(_o,K_),e(Ot,V_),e(Ot,zs),e(zs,Hd),e(Hd,J_),e(zs,G_),e(zs,Zr),e(Zr,X_),e(zs,Y_),e(Ot,Z_),e(Ot,Kd),e(Kd,ea),e(ea,eb),e(Kd,tb),_(n,ku,g),$(ta,n,g),_(n,wu,g),_(n,Lt,g),e(Lt,bo),e(bo,Vd),e(Vd,ob),e(bo,sb),e(bo,oa),e(oa,nb),e(bo,rb),e(bo,sa),e(sa,ab),e(bo,ib),e(Lt,lb),e(Lt,vo),e(vo,Jd),e(Jd,db),e(vo,cb),e(vo,na),e(na,pb),e(vo,hb),e(vo,ra),e(ra,fb),e(vo,ub),e(Lt,mb),e(Lt,ko),e(ko,Gd),e(Gd,gb),e(ko,_b),e(ko,aa),e(aa,bb),e(ko,vb),e(ko,ia),e(ia,kb),e(ko,wb),e(Lt,Tb),e(Lt,Xd),e(Xd,la),e(la,$b),e(Xd,yb),_(n,Tu,g),$(da,n,g),_(n,$u,g),_(n,It,g),e(It,wo),e(wo,Yd),e(Yd,Db),e(wo,Fb),e(wo,ca),e(ca,Eb),e(wo,Bb),e(wo,pa),e(pa,xb),e(wo,Mb),e(It,zb),e(It,To),e(To,Zd),e(Zd,jb),e(To,Cb),e(To,ha),e(ha,Pb),e(To,qb),e(To,fa),e(fa,Ab),e(To,Ob),e(It,Lb),e(It,js),e(js,ec),e(ec,Ib),e(js,Sb),e(js,ua),e(ua,Nb),e(js,Rb),e(It,Wb),e(It,tc),e(tc,ma),e(ma,Qb),e(tc,Ub),_(n,yu,g),_(n,oc,g),e(oc,bp),e(bp,Hb),_(n,Du,g),_(n,Cs,g),e(Cs,$o),e($o,sc),e(sc,Kb),e($o,Vb),e($o,ga),e(ga,Jb),e($o,Gb),e($o,_a),e(_a,Xb),e($o,Yb),e(Cs,Zb),e(Cs,yo),e(yo,nc),e(nc,ev),e(yo,tv),e(yo,ba),e(ba,ov),e(yo,sv),e(yo,va),e(va,nv),e(yo,rv),_(n,Fu,g),_(n,rc,g),e(rc,av),_(n,Eu,g),_(n,Do,g),e(Do,ka),e(ka,iv),e(ka,wa),e(wa,lv),e(ka,dv),e(Do,cv),e(Do,Ta),e(Ta,pv),e(Ta,$a),e($a,hv),e(Ta,fv),e(Do,uv),e(Do,ya),e(ya,mv),e(ya,Da),e(Da,gv),e(ya,_v),_(n,Bu,g),_(n,ac,g),e(ac,bv),_(n,xu,g),_(n,Ps,g),e(Ps,Fa),e(Fa,vv),e(Fa,Ea),e(Ea,kv),e(Fa,wv),e(Ps,Tv),e(Ps,Ba),e(Ba,$v),e(Ba,xa),e(xa,yv),e(Ba,Dv),_(n,Mu,g),_(n,ic,g),e(ic,Fv),_(n,zu,g),_(n,Fo,g),e(Fo,Ma),e(Ma,Ev),e(Ma,za),e(za,Bv),e(Ma,xv),e(Fo,Mv),e(Fo,ja),e(ja,zv),e(ja,Ca),e(Ca,jv),e(ja,Cv),e(Fo,Pv),e(Fo,Pa),e(Pa,qv),e(Pa,qa),e(qa,Av),e(Pa,Ov),_(n,ju,g),_(n,Ro,g),e(Ro,qs),e(qs,vp),$(Aa,vp,null),e(Ro,Lv),e(Ro,kp),e(kp,Iv),_(n,Cu,g),_(n,Pt,g),$(Oa,Pt,null),e(Pt,Sv),e(Pt,oo),e(oo,Nv),e(oo,lc),e(lc,Rv),e(oo,Wv),e(oo,dc),e(dc,Qv),e(oo,Uv),e(oo,La),e(La,Hv),e(oo,Kv),e(Pt,Vv),e(Pt,Wo),e(Wo,Jv),e(Wo,cc),e(cc,Gv),e(Wo,Xv),e(Wo,pc),e(pc,Yv),e(Wo,Zv),e(Pt,ek),$(As,Pt,null),_(n,Pu,g),_(n,Qo,g),e(Qo,Os),e(Os,wp),$(Ia,wp,null),e(Qo,tk),e(Qo,Tp),e(Tp,ok),_(n,qu,g),_(n,qt,g),$(Sa,qt,null),e(qt,sk),e(qt,$p),e($p,nk),e(qt,rk),e(qt,Ls),e(Ls,hc),e(hc,ak),e(Ls,ik),e(Ls,fc),e(fc,lk),e(Ls,dk),e(qt,ck),e(qt,Na),e(Na,pk),e(Na,uc),e(uc,hk),e(Na,fk),_(n,Au,g),_(n,Uo,g),e(Uo,Is),e(Is,yp),$(Ra,yp,null),e(Uo,uk),e(Uo,Dp),e(Dp,mk),_(n,Ou,g),_(n,At,g),$(Wa,At,null),e(At,gk),e(At,Qa),e(Qa,_k),e(Qa,Fp),e(Fp,bk),e(Qa,vk),e(At,kk),e(At,Ss),e(Ss,mc),e(mc,wk),e(Ss,Tk),e(Ss,gc),e(gc,$k),e(Ss,yk),e(At,Dk),e(At,Ua),e(Ua,Fk),e(Ua,_c),e(_c,Ek),e(Ua,Bk),_(n,Lu,g),_(n,Ho,g),e(Ho,Ns),e(Ns,Ep),$(Ha,Ep,null),e(Ho,xk),e(Ho,Bp),e(Bp,Mk),_(n,Iu,g),_(n,ut,g),$(Ka,ut,null),e(ut,zk),e(ut,xp),e(xp,jk),e(ut,Ck),e(ut,Va),e(Va,Pk),e(Va,bc),e(bc,qk),e(Va,Ak),e(ut,Ok),e(ut,Ja),e(Ja,Lk),e(Ja,Ga),e(Ga,Ik),e(Ja,Sk),e(ut,Nk),e(ut,St),$(Xa,St,null),e(St,Rk),e(St,Ko),e(Ko,Wk),e(Ko,vc),e(vc,Qk),e(Ko,Uk),e(Ko,Mp),e(Mp,Hk),e(Ko,Kk),e(St,Vk),$(Rs,St,null),e(St,Jk),$(Ws,St,null),_(n,Su,g),_(n,Vo,g),e(Vo,Qs),e(Qs,zp),$(Ya,zp,null),e(Vo,Gk),e(Vo,jp),e(jp,Xk),_(n,Nu,g),_(n,mt,g),$(Za,mt,null),e(mt,Yk),e(mt,ei),e(ei,Zk),e(ei,Cp),e(Cp,ew),e(ei,tw),e(mt,ow),e(mt,ti),e(ti,sw),e(ti,kc),e(kc,nw),e(ti,rw),e(mt,aw),e(mt,oi),e(oi,iw),e(oi,si),e(si,lw),e(oi,dw),e(mt,cw),e(mt,kt),$(ni,kt,null),e(kt,pw),e(kt,Jo),e(Jo,hw),e(Jo,wc),e(wc,fw),e(Jo,uw),e(Jo,Pp),e(Pp,mw),e(Jo,gw),e(kt,_w),$(Us,kt,null),e(kt,bw),$(Hs,kt,null),e(kt,vw),$(Ks,kt,null),_(n,Ru,g),_(n,Go,g),e(Go,Vs),e(Vs,qp),$(ri,qp,null),e(Go,kw),e(Go,Ap),e(Ap,ww),_(n,Wu,g),_(n,gt,g),$(ai,gt,null),e(gt,Tw),e(gt,Op),e(Op,$w),e(gt,yw),e(gt,ii),e(ii,Dw),e(ii,Tc),e(Tc,Fw),e(ii,Ew),e(gt,Bw),e(gt,li),e(li,xw),e(li,di),e(di,Mw),e(li,zw),e(gt,jw),e(gt,it),$(ci,it,null),e(it,Cw),e(it,Xo),e(Xo,Pw),e(Xo,$c),e($c,qw),e(Xo,Aw),e(Xo,Lp),e(Lp,Ow),e(Xo,Lw),e(it,Iw),$(Js,it,null),e(it,Sw),$(Gs,it,null),e(it,Nw),$(Xs,it,null),e(it,Rw),$(Ys,it,null),e(it,Ww),$(Zs,it,null),_(n,Qu,g),_(n,Yo,g),e(Yo,en),e(en,Ip),$(pi,Ip,null),e(Yo,Qw),e(Yo,Sp),e(Sp,Uw),_(n,Uu,g),_(n,_t,g),$(hi,_t,null),e(_t,Hw),e(_t,Np),e(Np,Kw),e(_t,Vw),e(_t,fi),e(fi,Jw),e(fi,yc),e(yc,Gw),e(fi,Xw),e(_t,Yw),e(_t,ui),e(ui,Zw),e(ui,mi),e(mi,eT),e(ui,tT),e(_t,oT),e(_t,Nt),$(gi,Nt,null),e(Nt,sT),e(Nt,Zo),e(Zo,nT),e(Zo,Dc),e(Dc,rT),e(Zo,aT),e(Zo,Rp),e(Rp,iT),e(Zo,lT),e(Nt,dT),$(tn,Nt,null),e(Nt,cT),$(on,Nt,null),_(n,Hu,g),_(n,es,g),e(es,sn),e(sn,Wp),$(_i,Wp,null),e(es,pT),e(es,Qp),e(Qp,hT),_(n,Ku,g),_(n,bt,g),$(bi,bt,null),e(bt,fT),e(bt,Up),e(Up,uT),e(bt,mT),e(bt,vi),e(vi,gT),e(vi,Fc),e(Fc,_T),e(vi,bT),e(bt,vT),e(bt,ki),e(ki,kT),e(ki,wi),e(wi,wT),e(ki,TT),e(bt,$T),e(bt,wt),$(Ti,wt,null),e(wt,yT),e(wt,ts),e(ts,DT),e(ts,Ec),e(Ec,FT),e(ts,ET),e(ts,Hp),e(Hp,BT),e(ts,xT),e(wt,MT),$(nn,wt,null),e(wt,zT),$(rn,wt,null),e(wt,jT),$(an,wt,null),_(n,Vu,g),_(n,os,g),e(os,ln),e(ln,Kp),$($i,Kp,null),e(os,CT),e(os,Vp),e(Vp,PT),_(n,Ju,g),_(n,vt,g),$(yi,vt,null),e(vt,qT),e(vt,ss),e(ss,AT),e(ss,Jp),e(Jp,OT),e(ss,LT),e(ss,Gp),e(Gp,IT),e(ss,ST),e(vt,NT),e(vt,Di),e(Di,RT),e(Di,Bc),e(Bc,WT),e(Di,QT),e(vt,UT),e(vt,Fi),e(Fi,HT),e(Fi,Ei),e(Ei,KT),e(Fi,VT),e(vt,JT),e(vt,Tt),$(Bi,Tt,null),e(Tt,GT),e(Tt,ns),e(ns,XT),e(ns,xc),e(xc,YT),e(ns,ZT),e(ns,Xp),e(Xp,e$),e(ns,t$),e(Tt,o$),$(dn,Tt,null),e(Tt,s$),$(cn,Tt,null),e(Tt,n$),$(pn,Tt,null),_(n,Gu,g),_(n,rs,g),e(rs,hn),e(hn,Yp),$(xi,Yp,null),e(rs,r$),e(rs,Zp),e(Zp,a$),_(n,Xu,g),_(n,lt,g),$(Mi,lt,null),e(lt,i$),e(lt,eh),e(eh,l$),e(lt,d$),e(lt,zi),e(zi,c$),e(zi,Mc),e(Mc,p$),e(zi,h$),e(lt,f$),e(lt,ji),e(ji,u$),e(ji,Ci),e(Ci,m$),e(ji,g$),e(lt,_$),$(fn,lt,null),e(lt,b$),e(lt,Rt),$(Pi,Rt,null),e(Rt,v$),e(Rt,as),e(as,k$),e(as,zc),e(zc,w$),e(as,T$),e(as,th),e(th,$$),e(as,y$),e(Rt,D$),$(un,Rt,null),e(Rt,F$),$(mn,Rt,null),_(n,Yu,g),_(n,is,g),e(is,gn),e(gn,oh),$(qi,oh,null),e(is,E$),e(is,sh),e(sh,B$),_(n,Zu,g),_(n,dt,g),$(Ai,dt,null),e(dt,x$),e(dt,Oi),e(Oi,M$),e(Oi,nh),e(nh,z$),e(Oi,j$),e(dt,C$),e(dt,Li),e(Li,P$),e(Li,jc),e(jc,q$),e(Li,A$),e(dt,O$),e(dt,Ii),e(Ii,L$),e(Ii,Si),e(Si,I$),e(Ii,S$),e(dt,N$),$(_n,dt,null),e(dt,R$),e(dt,$t),$(Ni,$t,null),e($t,W$),e($t,ls),e(ls,Q$),e(ls,Cc),e(Cc,U$),e(ls,H$),e(ls,rh),e(rh,K$),e(ls,V$),e($t,J$),$(bn,$t,null),e($t,G$),$(vn,$t,null),e($t,X$),$(kn,$t,null),_(n,em,g),_(n,ds,g),e(ds,wn),e(wn,ah),$(Ri,ah,null),e(ds,Y$),e(ds,ih),e(ih,Z$),_(n,tm,g),_(n,ct,g),$(Wi,ct,null),e(ct,ey),e(ct,lh),e(lh,ty),e(ct,oy),e(ct,Qi),e(Qi,sy),e(Qi,Pc),e(Pc,ny),e(Qi,ry),e(ct,ay),e(ct,Ui),e(Ui,iy),e(Ui,Hi),e(Hi,ly),e(Ui,dy),e(ct,cy),$(Tn,ct,null),e(ct,py),e(ct,yt),$(Ki,yt,null),e(yt,hy),e(yt,cs),e(cs,fy),e(cs,qc),e(qc,uy),e(cs,my),e(cs,dh),e(dh,gy),e(cs,_y),e(yt,by),$($n,yt,null),e(yt,vy),$(yn,yt,null),e(yt,ky),$(Dn,yt,null),_(n,om,g),_(n,ps,g),e(ps,Fn),e(Fn,ch),$(Vi,ch,null),e(ps,wy),e(ps,ph),e(ph,Ty),_(n,sm,g),_(n,pt,g),$(Ji,pt,null),e(pt,$y),e(pt,hh),e(hh,yy),e(pt,Dy),e(pt,Gi),e(Gi,Fy),e(Gi,Ac),e(Ac,Ey),e(Gi,By),e(pt,xy),e(pt,Xi),e(Xi,My),e(Xi,Yi),e(Yi,zy),e(Xi,jy),e(pt,Cy),$(En,pt,null),e(pt,Py),e(pt,Wt),$(Zi,Wt,null),e(Wt,qy),e(Wt,hs),e(hs,Ay),e(hs,Oc),e(Oc,Oy),e(hs,Ly),e(hs,fh),e(fh,Iy),e(hs,Sy),e(Wt,Ny),$(Bn,Wt,null),e(Wt,Ry),$(xn,Wt,null),_(n,nm,g),_(n,fs,g),e(fs,Mn),e(Mn,uh),$(el,uh,null),e(fs,Wy),e(fs,mh),e(mh,Qy),_(n,rm,g),_(n,ht,g),$(tl,ht,null),e(ht,Uy),e(ht,gh),e(gh,Hy),e(ht,Ky),e(ht,ol),e(ol,Vy),e(ol,Lc),e(Lc,Jy),e(ol,Gy),e(ht,Xy),e(ht,sl),e(sl,Yy),e(sl,nl),e(nl,Zy),e(sl,e2),e(ht,t2),$(zn,ht,null),e(ht,o2),e(ht,Dt),$(rl,Dt,null),e(Dt,s2),e(Dt,us),e(us,n2),e(us,Ic),e(Ic,r2),e(us,a2),e(us,_h),e(_h,i2),e(us,l2),e(Dt,d2),$(jn,Dt,null),e(Dt,c2),$(Cn,Dt,null),e(Dt,p2),$(Pn,Dt,null),_(n,am,g),_(n,ms,g),e(ms,qn),e(qn,bh),$(al,bh,null),e(ms,h2),e(ms,vh),e(vh,f2),_(n,im,g),_(n,ft,g),$(il,ft,null),e(ft,u2),e(ft,gs),e(gs,m2),e(gs,kh),e(kh,g2),e(gs,_2),e(gs,wh),e(wh,b2),e(gs,v2),e(ft,k2),e(ft,ll),e(ll,w2),e(ll,Sc),e(Sc,T2),e(ll,$2),e(ft,y2),e(ft,dl),e(dl,D2),e(dl,cl),e(cl,F2),e(dl,E2),e(ft,B2),$(An,ft,null),e(ft,x2),e(ft,Ft),$(pl,Ft,null),e(Ft,M2),e(Ft,_s),e(_s,z2),e(_s,Nc),e(Nc,j2),e(_s,C2),e(_s,Th),e(Th,P2),e(_s,q2),e(Ft,A2),$(On,Ft,null),e(Ft,O2),$(Ln,Ft,null),e(Ft,L2),$(In,Ft,null),_(n,lm,g),_(n,bs,g),e(bs,Sn),e(Sn,$h),$(hl,$h,null),e(bs,I2),e(bs,yh),e(yh,S2),_(n,dm,g),_(n,tt,g),$(fl,tt,null),e(tt,N2),e(tt,Dh),e(Dh,R2),e(tt,W2),e(tt,ul),e(ul,Q2),e(ul,Rc),e(Rc,U2),e(ul,H2),e(tt,K2),e(tt,ml),e(ml,V2),e(ml,gl),e(gl,J2),e(ml,G2),e(tt,X2),e(tt,Fh),e(Fh,Y2),e(tt,Z2),e(tt,so),e(so,Eh),e(Eh,_l),e(_l,e1),e(so,t1),e(so,Bh),e(Bh,bl),e(bl,o1),e(so,s1),e(so,xh),e(xh,vl),e(vl,n1),e(so,r1),e(so,Mh),e(Mh,kl),e(kl,a1),e(tt,i1),e(tt,Qt),$(wl,Qt,null),e(Qt,l1),e(Qt,vs),e(vs,d1),e(vs,zh),e(zh,c1),e(vs,p1),e(vs,jh),e(jh,h1),e(vs,f1),e(Qt,u1),$(Nn,Qt,null),e(Qt,m1),$(Rn,Qt,null),_(n,cm,g),_(n,ks,g),e(ks,Wn),e(Wn,Ch),$(Tl,Ch,null),e(ks,g1),e(ks,Ph),e(Ph,_1),_(n,pm,g),_(n,ot,g),$($l,ot,null),e(ot,b1),e(ot,yl),e(yl,v1),e(yl,qh),e(qh,k1),e(yl,w1),e(ot,T1),e(ot,Dl),e(Dl,$1),e(Dl,Wc),e(Wc,y1),e(Dl,D1),e(ot,F1),e(ot,Fl),e(Fl,E1),e(Fl,El),e(El,B1),e(Fl,x1),e(ot,M1),e(ot,Ah),e(Ah,z1),e(ot,j1),e(ot,no),e(no,Oh),e(Oh,Bl),e(Bl,C1),e(no,P1),e(no,Lh),e(Lh,xl),e(xl,q1),e(no,A1),e(no,Ih),e(Ih,Ml),e(Ml,O1),e(no,L1),e(no,Sh),e(Sh,zl),e(zl,I1),e(ot,S1),e(ot,Ut),$(jl,Ut,null),e(Ut,N1),e(Ut,ws),e(ws,R1),e(ws,Nh),e(Nh,W1),e(ws,Q1),e(ws,Rh),e(Rh,U1),e(ws,H1),e(Ut,K1),$(Qn,Ut,null),e(Ut,V1),$(Un,Ut,null),_(n,hm,g),_(n,Ts,g),e(Ts,Hn),e(Hn,Wh),$(Cl,Wh,null),e(Ts,J1),e(Ts,Qh),e(Qh,G1),_(n,fm,g),_(n,st,g),$(Pl,st,null),e(st,X1),e(st,Uh),e(Uh,Y1),e(st,Z1),e(st,ql),e(ql,e4),e(ql,Qc),e(Qc,t4),e(ql,o4),e(st,s4),e(st,Al),e(Al,n4),e(Al,Ol),e(Ol,r4),e(Al,a4),e(st,i4),e(st,Hh),e(Hh,l4),e(st,d4),e(st,ro),e(ro,Kh),e(Kh,Ll),e(Ll,c4),e(ro,p4),e(ro,Vh),e(Vh,Il),e(Il,h4),e(ro,f4),e(ro,Jh),e(Jh,Sl),e(Sl,u4),e(ro,m4),e(ro,Gh),e(Gh,Nl),e(Nl,g4),e(st,_4),e(st,Ht),$(Rl,Ht,null),e(Ht,b4),e(Ht,$s),e($s,v4),e($s,Xh),e(Xh,k4),e($s,w4),e($s,Yh),e(Yh,T4),e($s,$4),e(Ht,y4),$(Kn,Ht,null),e(Ht,D4),$(Vn,Ht,null),_(n,um,g),_(n,ys,g),e(ys,Jn),e(Jn,Zh),$(Wl,Zh,null),e(ys,F4),e(ys,ef),e(ef,E4),_(n,mm,g),_(n,nt,g),$(Ql,nt,null),e(nt,B4),e(nt,tf),e(tf,x4),e(nt,M4),e(nt,Ul),e(Ul,z4),e(Ul,Uc),e(Uc,j4),e(Ul,C4),e(nt,P4),e(nt,Hl),e(Hl,q4),e(Hl,Kl),e(Kl,A4),e(Hl,O4),e(nt,L4),e(nt,of),e(of,I4),e(nt,S4),e(nt,ao),e(ao,sf),e(sf,Vl),e(Vl,N4),e(ao,R4),e(ao,nf),e(nf,Jl),e(Jl,W4),e(ao,Q4),e(ao,rf),e(rf,Gl),e(Gl,U4),e(ao,H4),e(ao,af),e(af,Xl),e(Xl,K4),e(nt,V4),e(nt,Kt),$(Yl,Kt,null),e(Kt,J4),e(Kt,Ds),e(Ds,G4),e(Ds,lf),e(lf,X4),e(Ds,Y4),e(Ds,df),e(df,Z4),e(Ds,e0),e(Kt,t0),$(Gn,Kt,null),e(Kt,o0),$(Xn,Kt,null),_(n,gm,g),_(n,Fs,g),e(Fs,Yn),e(Yn,cf),$(Zl,cf,null),e(Fs,s0),e(Fs,pf),e(pf,n0),_(n,_m,g),_(n,rt,g),$(ed,rt,null),e(rt,r0),e(rt,hf),e(hf,a0),e(rt,i0),e(rt,td),e(td,l0),e(td,Hc),e(Hc,d0),e(td,c0),e(rt,p0),e(rt,od),e(od,h0),e(od,sd),e(sd,f0),e(od,u0),e(rt,m0),e(rt,ff),e(ff,g0),e(rt,_0),e(rt,io),e(io,uf),e(uf,nd),e(nd,b0),e(io,v0),e(io,mf),e(mf,rd),e(rd,k0),e(io,w0),e(io,gf),e(gf,ad),e(ad,T0),e(io,$0),e(io,_f),e(_f,id),e(id,y0),e(rt,D0),e(rt,Vt),$(ld,Vt,null),e(Vt,F0),e(Vt,Es),e(Es,E0),e(Es,bf),e(bf,B0),e(Es,x0),e(Es,vf),e(vf,M0),e(Es,z0),e(Vt,j0),$(Zn,Vt,null),e(Vt,C0),$(er,Vt,null),_(n,bm,g),_(n,Bs,g),e(Bs,tr),e(tr,kf),$(dd,kf,null),e(Bs,P0),e(Bs,wf),e(wf,q0),_(n,vm,g),_(n,at,g),$(cd,at,null),e(at,A0),e(at,xs),e(xs,O0),e(xs,Tf),e(Tf,L0),e(xs,I0),e(xs,$f),e($f,S0),e(xs,N0),e(at,R0),e(at,pd),e(pd,W0),e(pd,Kc),e(Kc,Q0),e(pd,U0),e(at,H0),e(at,hd),e(hd,K0),e(hd,fd),e(fd,V0),e(hd,J0),e(at,G0),e(at,yf),e(yf,X0),e(at,Y0),e(at,lo),e(lo,Df),e(Df,ud),e(ud,Z0),e(lo,eD),e(lo,Ff),e(Ff,md),e(md,tD),e(lo,oD),e(lo,Ef),e(Ef,gd),e(gd,sD),e(lo,nD),e(lo,Bf),e(Bf,_d),e(_d,rD),e(at,aD),e(at,Jt),$(bd,Jt,null),e(Jt,iD),e(Jt,Ms),e(Ms,lD),e(Ms,xf),e(xf,dD),e(Ms,cD),e(Ms,Mf),e(Mf,pD),e(Ms,hD),e(Jt,fD),$(or,Jt,null),e(Jt,uD),$(sr,Jt,null),km=!0},p(n,[g]){const vd={};g&2&&(vd.$$scope={dirty:g,ctx:n}),As.$set(vd);const zf={};g&2&&(zf.$$scope={dirty:g,ctx:n}),Rs.$set(zf);const jf={};g&2&&(jf.$$scope={dirty:g,ctx:n}),Ws.$set(jf);const Cf={};g&2&&(Cf.$$scope={dirty:g,ctx:n}),Us.$set(Cf);const kd={};g&2&&(kd.$$scope={dirty:g,ctx:n}),Hs.$set(kd);const Pf={};g&2&&(Pf.$$scope={dirty:g,ctx:n}),Ks.$set(Pf);const qf={};g&2&&(qf.$$scope={dirty:g,ctx:n}),Js.$set(qf);const Af={};g&2&&(Af.$$scope={dirty:g,ctx:n}),Gs.$set(Af);const co={};g&2&&(co.$$scope={dirty:g,ctx:n}),Xs.$set(co);const Of={};g&2&&(Of.$$scope={dirty:g,ctx:n}),Ys.$set(Of);const Lf={};g&2&&(Lf.$$scope={dirty:g,ctx:n}),Zs.$set(Lf);const If={};g&2&&(If.$$scope={dirty:g,ctx:n}),tn.$set(If);const Sf={};g&2&&(Sf.$$scope={dirty:g,ctx:n}),on.$set(Sf);const Nf={};g&2&&(Nf.$$scope={dirty:g,ctx:n}),nn.$set(Nf);const Rf={};g&2&&(Rf.$$scope={dirty:g,ctx:n}),rn.$set(Rf);const Wf={};g&2&&(Wf.$$scope={dirty:g,ctx:n}),an.$set(Wf);const wd={};g&2&&(wd.$$scope={dirty:g,ctx:n}),dn.$set(wd);const po={};g&2&&(po.$$scope={dirty:g,ctx:n}),cn.$set(po);const Qf={};g&2&&(Qf.$$scope={dirty:g,ctx:n}),pn.$set(Qf);const Uf={};g&2&&(Uf.$$scope={dirty:g,ctx:n}),fn.$set(Uf);const Hf={};g&2&&(Hf.$$scope={dirty:g,ctx:n}),un.$set(Hf);const Td={};g&2&&(Td.$$scope={dirty:g,ctx:n}),mn.$set(Td);const Kf={};g&2&&(Kf.$$scope={dirty:g,ctx:n}),_n.$set(Kf);const ho={};g&2&&(ho.$$scope={dirty:g,ctx:n}),bn.$set(ho);const Vf={};g&2&&(Vf.$$scope={dirty:g,ctx:n}),vn.$set(Vf);const Jf={};g&2&&(Jf.$$scope={dirty:g,ctx:n}),kn.$set(Jf);const Gf={};g&2&&(Gf.$$scope={dirty:g,ctx:n}),Tn.$set(Gf);const $d={};g&2&&($d.$$scope={dirty:g,ctx:n}),$n.$set($d);const Xf={};g&2&&(Xf.$$scope={dirty:g,ctx:n}),yn.$set(Xf);const Yf={};g&2&&(Yf.$$scope={dirty:g,ctx:n}),Dn.$set(Yf);const Zf={};g&2&&(Zf.$$scope={dirty:g,ctx:n}),En.$set(Zf);const eu={};g&2&&(eu.$$scope={dirty:g,ctx:n}),Bn.$set(eu);const Qe={};g&2&&(Qe.$$scope={dirty:g,ctx:n}),xn.$set(Qe);const yd={};g&2&&(yd.$$scope={dirty:g,ctx:n}),zn.$set(yd);const tu={};g&2&&(tu.$$scope={dirty:g,ctx:n}),jn.$set(tu);const Dd={};g&2&&(Dd.$$scope={dirty:g,ctx:n}),Cn.$set(Dd);const ou={};g&2&&(ou.$$scope={dirty:g,ctx:n}),Pn.$set(ou);const Fd={};g&2&&(Fd.$$scope={dirty:g,ctx:n}),An.$set(Fd);const su={};g&2&&(su.$$scope={dirty:g,ctx:n}),On.$set(su);const Ed={};g&2&&(Ed.$$scope={dirty:g,ctx:n}),Ln.$set(Ed);const nu={};g&2&&(nu.$$scope={dirty:g,ctx:n}),In.$set(nu);const Bd={};g&2&&(Bd.$$scope={dirty:g,ctx:n}),Nn.$set(Bd);const ru={};g&2&&(ru.$$scope={dirty:g,ctx:n}),Rn.$set(ru);const xd={};g&2&&(xd.$$scope={dirty:g,ctx:n}),Qn.$set(xd);const au={};g&2&&(au.$$scope={dirty:g,ctx:n}),Un.$set(au);const Md={};g&2&&(Md.$$scope={dirty:g,ctx:n}),Kn.$set(Md);const iu={};g&2&&(iu.$$scope={dirty:g,ctx:n}),Vn.$set(iu);const Eo={};g&2&&(Eo.$$scope={dirty:g,ctx:n}),Gn.$set(Eo);const lu={};g&2&&(lu.$$scope={dirty:g,ctx:n}),Xn.$set(lu);const du={};g&2&&(du.$$scope={dirty:g,ctx:n}),Zn.$set(du);const cu={};g&2&&(cu.$$scope={dirty:g,ctx:n}),er.$set(cu);const Bo={};g&2&&(Bo.$$scope={dirty:g,ctx:n}),or.$set(Bo);const pu={};g&2&&(pu.$$scope={dirty:g,ctx:n}),sr.$set(pu)},i(n){km||(y(l.$$.fragment,n),y(E.$$.fragment,n),y(R.$$.fragment,n),y(Er.$$.fragment,n),y(Vr.$$.fragment,n),y(ta.$$.fragment,n),y(da.$$.fragment,n),y(Aa.$$.fragment,n),y(Oa.$$.fragment,n),y(As.$$.fragment,n),y(Ia.$$.fragment,n),y(Sa.$$.fragment,n),y(Ra.$$.fragment,n),y(Wa.$$.fragment,n),y(Ha.$$.fragment,n),y(Ka.$$.fragment,n),y(Xa.$$.fragment,n),y(Rs.$$.fragment,n),y(Ws.$$.fragment,n),y(Ya.$$.fragment,n),y(Za.$$.fragment,n),y(ni.$$.fragment,n),y(Us.$$.fragment,n),y(Hs.$$.fragment,n),y(Ks.$$.fragment,n),y(ri.$$.fragment,n),y(ai.$$.fragment,n),y(ci.$$.fragment,n),y(Js.$$.fragment,n),y(Gs.$$.fragment,n),y(Xs.$$.fragment,n),y(Ys.$$.fragment,n),y(Zs.$$.fragment,n),y(pi.$$.fragment,n),y(hi.$$.fragment,n),y(gi.$$.fragment,n),y(tn.$$.fragment,n),y(on.$$.fragment,n),y(_i.$$.fragment,n),y(bi.$$.fragment,n),y(Ti.$$.fragment,n),y(nn.$$.fragment,n),y(rn.$$.fragment,n),y(an.$$.fragment,n),y($i.$$.fragment,n),y(yi.$$.fragment,n),y(Bi.$$.fragment,n),y(dn.$$.fragment,n),y(cn.$$.fragment,n),y(pn.$$.fragment,n),y(xi.$$.fragment,n),y(Mi.$$.fragment,n),y(fn.$$.fragment,n),y(Pi.$$.fragment,n),y(un.$$.fragment,n),y(mn.$$.fragment,n),y(qi.$$.fragment,n),y(Ai.$$.fragment,n),y(_n.$$.fragment,n),y(Ni.$$.fragment,n),y(bn.$$.fragment,n),y(vn.$$.fragment,n),y(kn.$$.fragment,n),y(Ri.$$.fragment,n),y(Wi.$$.fragment,n),y(Tn.$$.fragment,n),y(Ki.$$.fragment,n),y($n.$$.fragment,n),y(yn.$$.fragment,n),y(Dn.$$.fragment,n),y(Vi.$$.fragment,n),y(Ji.$$.fragment,n),y(En.$$.fragment,n),y(Zi.$$.fragment,n),y(Bn.$$.fragment,n),y(xn.$$.fragment,n),y(el.$$.fragment,n),y(tl.$$.fragment,n),y(zn.$$.fragment,n),y(rl.$$.fragment,n),y(jn.$$.fragment,n),y(Cn.$$.fragment,n),y(Pn.$$.fragment,n),y(al.$$.fragment,n),y(il.$$.fragment,n),y(An.$$.fragment,n),y(pl.$$.fragment,n),y(On.$$.fragment,n),y(Ln.$$.fragment,n),y(In.$$.fragment,n),y(hl.$$.fragment,n),y(fl.$$.fragment,n),y(wl.$$.fragment,n),y(Nn.$$.fragment,n),y(Rn.$$.fragment,n),y(Tl.$$.fragment,n),y($l.$$.fragment,n),y(jl.$$.fragment,n),y(Qn.$$.fragment,n),y(Un.$$.fragment,n),y(Cl.$$.fragment,n),y(Pl.$$.fragment,n),y(Rl.$$.fragment,n),y(Kn.$$.fragment,n),y(Vn.$$.fragment,n),y(Wl.$$.fragment,n),y(Ql.$$.fragment,n),y(Yl.$$.fragment,n),y(Gn.$$.fragment,n),y(Xn.$$.fragment,n),y(Zl.$$.fragment,n),y(ed.$$.fragment,n),y(ld.$$.fragment,n),y(Zn.$$.fragment,n),y(er.$$.fragment,n),y(dd.$$.fragment,n),y(cd.$$.fragment,n),y(bd.$$.fragment,n),y(or.$$.fragment,n),y(sr.$$.fragment,n),km=!0)},o(n){D(l.$$.fragment,n),D(E.$$.fragment,n),D(R.$$.fragment,n),D(Er.$$.fragment,n),D(Vr.$$.fragment,n),D(ta.$$.fragment,n),D(da.$$.fragment,n),D(Aa.$$.fragment,n),D(Oa.$$.fragment,n),D(As.$$.fragment,n),D(Ia.$$.fragment,n),D(Sa.$$.fragment,n),D(Ra.$$.fragment,n),D(Wa.$$.fragment,n),D(Ha.$$.fragment,n),D(Ka.$$.fragment,n),D(Xa.$$.fragment,n),D(Rs.$$.fragment,n),D(Ws.$$.fragment,n),D(Ya.$$.fragment,n),D(Za.$$.fragment,n),D(ni.$$.fragment,n),D(Us.$$.fragment,n),D(Hs.$$.fragment,n),D(Ks.$$.fragment,n),D(ri.$$.fragment,n),D(ai.$$.fragment,n),D(ci.$$.fragment,n),D(Js.$$.fragment,n),D(Gs.$$.fragment,n),D(Xs.$$.fragment,n),D(Ys.$$.fragment,n),D(Zs.$$.fragment,n),D(pi.$$.fragment,n),D(hi.$$.fragment,n),D(gi.$$.fragment,n),D(tn.$$.fragment,n),D(on.$$.fragment,n),D(_i.$$.fragment,n),D(bi.$$.fragment,n),D(Ti.$$.fragment,n),D(nn.$$.fragment,n),D(rn.$$.fragment,n),D(an.$$.fragment,n),D($i.$$.fragment,n),D(yi.$$.fragment,n),D(Bi.$$.fragment,n),D(dn.$$.fragment,n),D(cn.$$.fragment,n),D(pn.$$.fragment,n),D(xi.$$.fragment,n),D(Mi.$$.fragment,n),D(fn.$$.fragment,n),D(Pi.$$.fragment,n),D(un.$$.fragment,n),D(mn.$$.fragment,n),D(qi.$$.fragment,n),D(Ai.$$.fragment,n),D(_n.$$.fragment,n),D(Ni.$$.fragment,n),D(bn.$$.fragment,n),D(vn.$$.fragment,n),D(kn.$$.fragment,n),D(Ri.$$.fragment,n),D(Wi.$$.fragment,n),D(Tn.$$.fragment,n),D(Ki.$$.fragment,n),D($n.$$.fragment,n),D(yn.$$.fragment,n),D(Dn.$$.fragment,n),D(Vi.$$.fragment,n),D(Ji.$$.fragment,n),D(En.$$.fragment,n),D(Zi.$$.fragment,n),D(Bn.$$.fragment,n),D(xn.$$.fragment,n),D(el.$$.fragment,n),D(tl.$$.fragment,n),D(zn.$$.fragment,n),D(rl.$$.fragment,n),D(jn.$$.fragment,n),D(Cn.$$.fragment,n),D(Pn.$$.fragment,n),D(al.$$.fragment,n),D(il.$$.fragment,n),D(An.$$.fragment,n),D(pl.$$.fragment,n),D(On.$$.fragment,n),D(Ln.$$.fragment,n),D(In.$$.fragment,n),D(hl.$$.fragment,n),D(fl.$$.fragment,n),D(wl.$$.fragment,n),D(Nn.$$.fragment,n),D(Rn.$$.fragment,n),D(Tl.$$.fragment,n),D($l.$$.fragment,n),D(jl.$$.fragment,n),D(Qn.$$.fragment,n),D(Un.$$.fragment,n),D(Cl.$$.fragment,n),D(Pl.$$.fragment,n),D(Rl.$$.fragment,n),D(Kn.$$.fragment,n),D(Vn.$$.fragment,n),D(Wl.$$.fragment,n),D(Ql.$$.fragment,n),D(Yl.$$.fragment,n),D(Gn.$$.fragment,n),D(Xn.$$.fragment,n),D(Zl.$$.fragment,n),D(ed.$$.fragment,n),D(ld.$$.fragment,n),D(Zn.$$.fragment,n),D(er.$$.fragment,n),D(dd.$$.fragment,n),D(cd.$$.fragment,n),D(bd.$$.fragment,n),D(or.$$.fragment,n),D(sr.$$.fragment,n),km=!1},d(n){t(d),n&&t(b),n&&t(f),F(l),n&&t(ge),n&&t(I),F(E),n&&t(_e),n&&t(O),n&&t(be),n&&t(ee),n&&t(q),n&&t(le),n&&t(ve),n&&t(W),n&&t(ke),n&&t(C),n&&t(k),n&&t(x),n&&t(L),n&&t(N),F(R),n&&t(Ue),n&&t(te),n&&t(gu),F(Er,n),n&&t(_u),n&&t(Ie),n&&t(bu),F(Vr,n),n&&t(vu),n&&t(Ot),n&&t(ku),F(ta,n),n&&t(wu),n&&t(Lt),n&&t(Tu),F(da,n),n&&t($u),n&&t(It),n&&t(yu),n&&t(oc),n&&t(Du),n&&t(Cs),n&&t(Fu),n&&t(rc),n&&t(Eu),n&&t(Do),n&&t(Bu),n&&t(ac),n&&t(xu),n&&t(Ps),n&&t(Mu),n&&t(ic),n&&t(zu),n&&t(Fo),n&&t(ju),n&&t(Ro),F(Aa),n&&t(Cu),n&&t(Pt),F(Oa),F(As),n&&t(Pu),n&&t(Qo),F(Ia),n&&t(qu),n&&t(qt),F(Sa),n&&t(Au),n&&t(Uo),F(Ra),n&&t(Ou),n&&t(At),F(Wa),n&&t(Lu),n&&t(Ho),F(Ha),n&&t(Iu),n&&t(ut),F(Ka),F(Xa),F(Rs),F(Ws),n&&t(Su),n&&t(Vo),F(Ya),n&&t(Nu),n&&t(mt),F(Za),F(ni),F(Us),F(Hs),F(Ks),n&&t(Ru),n&&t(Go),F(ri),n&&t(Wu),n&&t(gt),F(ai),F(ci),F(Js),F(Gs),F(Xs),F(Ys),F(Zs),n&&t(Qu),n&&t(Yo),F(pi),n&&t(Uu),n&&t(_t),F(hi),F(gi),F(tn),F(on),n&&t(Hu),n&&t(es),F(_i),n&&t(Ku),n&&t(bt),F(bi),F(Ti),F(nn),F(rn),F(an),n&&t(Vu),n&&t(os),F($i),n&&t(Ju),n&&t(vt),F(yi),F(Bi),F(dn),F(cn),F(pn),n&&t(Gu),n&&t(rs),F(xi),n&&t(Xu),n&&t(lt),F(Mi),F(fn),F(Pi),F(un),F(mn),n&&t(Yu),n&&t(is),F(qi),n&&t(Zu),n&&t(dt),F(Ai),F(_n),F(Ni),F(bn),F(vn),F(kn),n&&t(em),n&&t(ds),F(Ri),n&&t(tm),n&&t(ct),F(Wi),F(Tn),F(Ki),F($n),F(yn),F(Dn),n&&t(om),n&&t(ps),F(Vi),n&&t(sm),n&&t(pt),F(Ji),F(En),F(Zi),F(Bn),F(xn),n&&t(nm),n&&t(fs),F(el),n&&t(rm),n&&t(ht),F(tl),F(zn),F(rl),F(jn),F(Cn),F(Pn),n&&t(am),n&&t(ms),F(al),n&&t(im),n&&t(ft),F(il),F(An),F(pl),F(On),F(Ln),F(In),n&&t(lm),n&&t(bs),F(hl),n&&t(dm),n&&t(tt),F(fl),F(wl),F(Nn),F(Rn),n&&t(cm),n&&t(ks),F(Tl),n&&t(pm),n&&t(ot),F($l),F(jl),F(Qn),F(Un),n&&t(hm),n&&t(Ts),F(Cl),n&&t(fm),n&&t(st),F(Pl),F(Rl),F(Kn),F(Vn),n&&t(um),n&&t(ys),F(Wl),n&&t(mm),n&&t(nt),F(Ql),F(Yl),F(Gn),F(Xn),n&&t(gm),n&&t(Fs),F(Zl),n&&t(_m),n&&t(rt),F(ed),F(ld),F(Zn),F(er),n&&t(bm),n&&t(Bs),F(dd),n&&t(vm),n&&t(at),F(cd),F(bd),F(or),F(sr)}}}const B3={local:"distilbert",sections:[{local:"overview",title:"Overview"},{local:"resources",title:"Resources"},{local:"transformers.DistilBertConfig",title:"DistilBertConfig"},{local:"transformers.DistilBertTokenizer",title:"DistilBertTokenizer"},{local:"transformers.DistilBertTokenizerFast",title:"DistilBertTokenizerFast"},{local:"transformers.DistilBertModel",title:"DistilBertModel"},{local:"transformers.DistilBertForMaskedLM",title:"DistilBertForMaskedLM"},{local:"transformers.DistilBertForSequenceClassification",title:"DistilBertForSequenceClassification"},{local:"transformers.DistilBertForMultipleChoice",title:"DistilBertForMultipleChoice"},{local:"transformers.DistilBertForTokenClassification",title:"DistilBertForTokenClassification"},{local:"transformers.DistilBertForQuestionAnswering",title:"DistilBertForQuestionAnswering"},{local:"transformers.TFDistilBertModel",title:"TFDistilBertModel"},{local:"transformers.TFDistilBertForMaskedLM",title:"TFDistilBertForMaskedLM"},{local:"transformers.TFDistilBertForSequenceClassification",title:"TFDistilBertForSequenceClassification"},{local:"transformers.TFDistilBertForMultipleChoice",title:"TFDistilBertForMultipleChoice"},{local:"transformers.TFDistilBertForTokenClassification",title:"TFDistilBertForTokenClassification"},{local:"transformers.TFDistilBertForQuestionAnswering",title:"TFDistilBertForQuestionAnswering"},{local:"transformers.FlaxDistilBertModel",title:"FlaxDistilBertModel"},{local:"transformers.FlaxDistilBertForMaskedLM",title:"FlaxDistilBertForMaskedLM"},{local:"transformers.FlaxDistilBertForSequenceClassification",title:"FlaxDistilBertForSequenceClassification"},{local:"transformers.FlaxDistilBertForMultipleChoice",title:"FlaxDistilBertForMultipleChoice"},{local:"transformers.FlaxDistilBertForTokenClassification",title:"FlaxDistilBertForTokenClassification"},{local:"transformers.FlaxDistilBertForQuestionAnswering",title:"FlaxDistilBertForQuestionAnswering"}],title:"DistilBERT"};function x3(B){return EM(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class O3 extends $M{constructor(d){super();yM(this,d,x3,E3,DM,{})}}export{O3 as default,B3 as metadata};
