import{S as os,i as ns,s as as,e as n,k as p,w as T,t as r,M as ss,c as a,d as o,m as h,a as s,x as k,h as i,b as l,G as e,g as _,y,q as w,o as P,B as $,v as rs,L as lo}from"../../chunks/vendor-hf-doc-builder.js";import{T as ts}from"../../chunks/Tip-hf-doc-builder.js";import{D as Ve}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Ft}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Ke}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as io}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function is(G){let c,b;return c=new Ft({props:{code:`from transformers import GPTNeoXJapaneseConfig, GPTNeoXJapaneseModel

# Initializing a GPTNeoXJapanese gpt-neox-japanese-2.7b style configuration
configuration = GPTNeoXJapaneseConfig()

# Initializing a model (with random weights) from the gpt-neox-japanese-2.7b style configuration
model = GPTNeoXJapaneseModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTNeoXJapaneseConfig, GPTNeoXJapaneseModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a GPTNeoXJapanese gpt-neox-japanese-2.7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = GPTNeoXJapaneseConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the gpt-neox-japanese-2.7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoXJapaneseModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){T(c.$$.fragment)},l(u){k(c.$$.fragment,u)},m(u,f){y(c,u,f),b=!0},p:lo,i(u){b||(w(c.$$.fragment,u),b=!0)},o(u){P(c.$$.fragment,u),b=!1},d(u){$(c,u)}}}function ls(G){let c,b,u,f,v;return f=new Ft({props:{code:`from transformers import GPTNeoXJapaneseTokenizer

tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained("abeja/gpt-neox-japanese-2.7b")
# You can confirm both \u6176\u5FDC and \u6176\u61C9 are encoded to 17749
tokenizer("\u543E\u8F29\u306F\u732B\u3067\u3042\u308B\u{1F42F}\u3002\u5B9F\u306F\u6176\u5FDC(\u6176\u61C9)\u5927\u5B66\u51FA\u8EAB")["input_ids"]

# Both \u6176\u5FDC and \u6176\u61C9 are decoded to \u6176\u5FDC
tokenizer.decode(tokenizer("\u543E\u8F29\u306F\u732B\u3067\u3042\u308B\u{1F42F}\u3002\u5B9F\u306F\u6176\u5FDC(\u6176\u61C9)\u5927\u5B66\u51FA\u8EAB")["input_ids"])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTNeoXJapaneseTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># You can confirm both \u6176\u5FDC and \u6176\u61C9 are encoded to 17749</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer(<span class="hljs-string">&quot;\u543E\u8F29\u306F\u732B\u3067\u3042\u308B\u{1F42F}\u3002\u5B9F\u306F\u6176\u5FDC(\u6176\u61C9)\u5927\u5B66\u51FA\u8EAB&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
[<span class="hljs-number">30014</span>, <span class="hljs-number">26883</span>, <span class="hljs-number">26638</span>, <span class="hljs-number">27228</span>, <span class="hljs-number">25</span>, <span class="hljs-number">26650</span>, <span class="hljs-number">31732</span>, <span class="hljs-number">31679</span>, <span class="hljs-number">27809</span>, <span class="hljs-number">26638</span>, <span class="hljs-number">17749</span>, <span class="hljs-number">31592</span>, <span class="hljs-number">17749</span>, <span class="hljs-number">31593</span>, <span class="hljs-number">321</span>, <span class="hljs-number">1281</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Both \u6176\u5FDC and \u6176\u61C9 are decoded to \u6176\u5FDC</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(tokenizer(<span class="hljs-string">&quot;\u543E\u8F29\u306F\u732B\u3067\u3042\u308B\u{1F42F}\u3002\u5B9F\u306F\u6176\u5FDC(\u6176\u61C9)\u5927\u5B66\u51FA\u8EAB&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-string">&#x27;\u543E\u8F29\u306F\u732B\u3067\u3042\u308B\u{1F42F}\u3002\u5B9F\u306F\u6176\u5FDC(\u6176\u5FDC)\u5927\u5B66\u51FA\u8EAB&#x27;</span>`}}),{c(){c=n("p"),b=r("Example:"),u=p(),T(f.$$.fragment)},l(d){c=a(d,"P",{});var g=s(c);b=i(g,"Example:"),g.forEach(o),u=h(d),k(f.$$.fragment,d)},m(d,g){_(d,c,g),e(c,b),_(d,u,g),y(f,d,g),v=!0},p:lo,i(d){v||(w(f.$$.fragment,d),v=!0)},o(d){P(f.$$.fragment,d),v=!1},d(d){d&&o(c),d&&o(u),$(f,d)}}}function ds(G){let c,b,u,f,v;return{c(){c=n("p"),b=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n("code"),f=r("Module"),v=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){c=a(d,"P",{});var g=s(c);b=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a(g,"CODE",{});var F=s(u);f=i(F,"Module"),F.forEach(o),v=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(o)},m(d,g){_(d,c,g),e(c,b),e(c,u),e(u,f),e(c,v)},d(d){d&&o(c)}}}function cs(G){let c,b,u,f,v;return f=new Ft({props:{code:`from transformers import GPTNeoXJapaneseTokenizer, GPTNeoXJapaneseModel
import torch

tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained("abeja/gpt-neox-japanese-2.7b")
model = GPTNeoXJapaneseModel.from_pretrained("abeja/gpt-neox-japanese-2.7b")

inputs = tokenizer("\u65E5\u672C\u8A9E\u306EGPT-neox\u304CHugging Face\u3067\u4F7F\u3048\u307E\u3059\u{1F600}", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTNeoXJapaneseTokenizer, GPTNeoXJapaneseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoXJapaneseModel.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;\u65E5\u672C\u8A9E\u306EGPT-neox\u304CHugging Face\u3067\u4F7F\u3048\u307E\u3059\u{1F600}&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){c=n("p"),b=r("Example:"),u=p(),T(f.$$.fragment)},l(d){c=a(d,"P",{});var g=s(c);b=i(g,"Example:"),g.forEach(o),u=h(d),k(f.$$.fragment,d)},m(d,g){_(d,c,g),e(c,b),_(d,u,g),y(f,d,g),v=!0},p:lo,i(d){v||(w(f.$$.fragment,d),v=!0)},o(d){P(f.$$.fragment,d),v=!1},d(d){d&&o(c),d&&o(u),$(f,d)}}}function ps(G){let c,b,u,f,v;return{c(){c=n("p"),b=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n("code"),f=r("Module"),v=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){c=a(d,"P",{});var g=s(c);b=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a(g,"CODE",{});var F=s(u);f=i(F,"Module"),F.forEach(o),v=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(o)},m(d,g){_(d,c,g),e(c,b),e(c,u),e(u,f),e(c,v)},d(d){d&&o(c)}}}function hs(G){let c,b,u,f,v;return f=new Ft({props:{code:`from transformers import GPTNeoXJapaneseTokenizer, GPTNeoXJapaneseForCausalLM, GPTNeoXJapaneseConfig
import torch

tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained("abeja/gpt-neox-japanese-2.7b")
config = GPTNeoXJapaneseConfig.from_pretrained("abeja/gpt-neox-japanese-2.7b")
config.is_decoder = True
model = GPTNeoXJapaneseForCausalLM.from_pretrained("abeja/gpt-neox-japanese-2.7b", config=config)

inputs = tokenizer("\u65E5\u672C\u8A9E\u306EGPT-neox\u304CHugging Face\u3067\u4F7F\u3048\u307E\u3059\u{1F600}", return_tensors="pt")
outputs = model(**inputs)

prediction_logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTNeoXJapaneseTokenizer, GPTNeoXJapaneseForCausalLM, GPTNeoXJapaneseConfig
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config = GPTNeoXJapaneseConfig.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.is_decoder = <span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoXJapaneseForCausalLM.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>, config=config)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;\u65E5\u672C\u8A9E\u306EGPT-neox\u304CHugging Face\u3067\u4F7F\u3048\u307E\u3059\u{1F600}&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>prediction_logits = outputs.logits`}}),{c(){c=n("p"),b=r("Example:"),u=p(),T(f.$$.fragment)},l(d){c=a(d,"P",{});var g=s(c);b=i(g,"Example:"),g.forEach(o),u=h(d),k(f.$$.fragment,d)},m(d,g){_(d,c,g),e(c,b),_(d,u,g),y(f,d,g),v=!0},p:lo,i(d){v||(w(f.$$.fragment,d),v=!0)},o(d){P(f.$$.fragment,d),v=!1},d(d){d&&o(c),d&&o(u),$(f,d)}}}function us(G){let c,b,u,f,v,d,g,F,co,At,O,Z,lt,ge,po,dt,ho,Lt,j,uo,_e,mo,fo,be,go,_o,ct,bo,vo,ve,To,ko,Te,yo,wo,It,N,Po,ke,$o,No,ye,jo,xo,we,Jo,Go,Pe,Xo,zo,$e,Eo,Mo,Ne,Co,qo,Ot,S,ee,pt,je,Fo,ht,Ao,St,te,Lo,ut,Io,Oo,Dt,xe,Bt,D,oe,mt,Je,So,ft,Do,Wt,z,Ge,Bo,B,Wo,gt,Ho,Vo,Xe,Ko,Uo,Ro,W,Yo,Ue,Qo,Zo,Re,en,tn,on,ne,Ht,H,ae,_t,ze,nn,bt,an,Vt,x,Ee,sn,V,rn,Ye,ln,dn,Me,cn,pn,hn,J,vt,un,mn,Tt,fn,gn,kt,_n,bn,yt,vn,Tn,wt,kn,yn,Pt,wn,Pn,se,$n,re,Ce,Nn,$t,jn,Kt,K,ie,Nt,qe,xn,jt,Jn,Ut,A,Fe,Gn,Ae,Xn,Le,zn,En,Mn,E,Ie,Cn,U,qn,Qe,Fn,An,xt,Ln,In,On,le,Sn,de,Rt,R,ce,Jt,Oe,Dn,Gt,Bn,Yt,L,Se,Wn,Y,Hn,Xt,Vn,Kn,De,Un,Rn,Yn,M,Be,Qn,Q,Zn,Ze,ea,ta,zt,oa,na,aa,pe,sa,he,Qt;return d=new Ke({}),ge=new Ke({}),je=new Ke({}),xe=new Ft({props:{code:`from transformers import GPTNeoXJapaneseForCausalLM, GPTNeoXJapaneseTokenizer

model = GPTNeoXJapaneseForCausalLM.from_pretrained("abeja/gpt-neox-japanese-2.7b")
tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained("abeja/gpt-neox-japanese-2.7b")

prompt = "\u4EBA\u3068AI\u304C\u5354\u8ABF\u3059\u308B\u305F\u3081\u306B\u306F\u3001"

input_ids = tokenizer(prompt, return_tensors="pt").input_ids

gen_tokens = model.generate(
    input_ids,
    do_sample=True,
    temperature=0.9,
    max_length=100,
)
gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]

print(gen_text)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTNeoXJapaneseForCausalLM, GPTNeoXJapaneseTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoXJapaneseForCausalLM.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;\u4EBA\u3068AI\u304C\u5354\u8ABF\u3059\u308B\u305F\u3081\u306B\u306F\u3001&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>gen_tokens = model.generate(
<span class="hljs-meta">... </span>    input_ids,
<span class="hljs-meta">... </span>    do_sample=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    temperature=<span class="hljs-number">0.9</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">100</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(gen_text)
\u4EBA\u3068AI\u304C\u5354\u8ABF\u3059\u308B\u305F\u3081\u306B\u306F\u3001AI\u3068\u4EBA\u304C\u5171\u5B58\u3057\u3001AI\u3092\u6B63\u3057\u304F\u7406\u89E3\u3059\u308B\u5FC5\u8981\u304C\u3042\u308A\u307E\u3059\u3002`}}),Je=new Ke({}),Ge=new Ve({props:{name:"class transformers.GPTNeoXJapaneseConfig",anchor:"transformers.GPTNeoXJapaneseConfig",parameters:[{name:"vocab_size",val:" = 32000"},{name:"hidden_size",val:" = 2560"},{name:"num_hidden_layers",val:" = 32"},{name:"num_attention_heads",val:" = 32"},{name:"intermediate_multiple_size",val:" = 4"},{name:"hidden_act",val:" = 'gelu'"},{name:"rotary_pct",val:" = 1.0"},{name:"rotary_emb_base",val:" = 10000"},{name:"max_position_embeddings",val:" = 2048"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"use_cache",val:" = True"},{name:"bos_token_id",val:" = 31996"},{name:"eos_token_id",val:" = 31999"},{name:"weight_tying",val:" = True"},{name:"attention_dropout",val:" = 0.1"},{name:"hidden_dropout",val:" = 0.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.GPTNeoXJapaneseConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32000) &#x2014;
Vocabulary size of the GPTNeoXJapanese model. Defines the number of different tokens that can be
represented by the <code>inputs_ids</code> passed when calling <code>GPTNeoXJapanese</code>.`,name:"vocab_size"},{anchor:"transformers.GPTNeoXJapaneseConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2560) &#x2014;
Dimension of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.GPTNeoXJapaneseConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.GPTNeoXJapaneseConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.GPTNeoXJapaneseConfig.intermediate_multiple_size",description:`<strong>intermediate_multiple_size</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Dimension of the &#x201C;intermediate&#x201D; layer in the Transformer encoder is calculated by hidden_size *
intermediate_multiple_size.`,name:"intermediate_multiple_size"},{anchor:"transformers.GPTNeoXJapaneseConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler.`,name:"hidden_act"},{anchor:"transformers.GPTNeoXJapaneseConfig.rotary_pct",description:`<strong>rotary_pct</strong> (<code>float</code>, <em>optional</em>, defaults to 1.00) &#x2014;
percentage of hidden dimensions to allocate to rotary embeddings`,name:"rotary_pct"},{anchor:"transformers.GPTNeoXJapaneseConfig.rotary_emb_base",description:`<strong>rotary_emb_base</strong> (<code>int</code>, <em>optional</em>, defaults to 10000) &#x2014;
base for computing rotary embeddings frequency`,name:"rotary_emb_base"},{anchor:"transformers.GPTNeoXJapaneseConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
The maximum sequence length that this model might ever be used with.`,name:"max_position_embeddings"},{anchor:"transformers.GPTNeoXJapaneseConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.GPTNeoXJapaneseConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-5) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.GPTNeoXJapaneseConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"},{anchor:"transformers.GPTNeoXJapaneseConfig.weight_tying",description:`<strong>weight_tying</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whhether or not use weight tying between input and output embedding weight`,name:"weight_tying"},{anchor:"transformers.GPTNeoXJapaneseConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention.`,name:"attention_dropout"},{anchor:"transformers.GPTNeoXJapaneseConfig.hidden_dropout",description:`<strong>hidden_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the hidden layer.
Example &#x2014;`,name:"hidden_dropout"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/gpt_neox_japanese/configuration_gpt_neox_japanese.py#L28"}}),ne=new io({props:{anchor:"transformers.GPTNeoXJapaneseConfig.example",$$slots:{default:[is]},$$scope:{ctx:G}}}),ze=new Ke({}),Ee=new Ve({props:{name:"class transformers.GPTNeoXJapaneseTokenizer",anchor:"transformers.GPTNeoXJapaneseTokenizer",parameters:[{name:"vocab_file",val:""},{name:"emoji_file",val:""},{name:"unk_token",val:" = '<|endoftext|>'"},{name:"pad_token",val:" = '<|endoftext|>'"},{name:"bos_token",val:" = '<|startoftext|>'"},{name:"eos_token",val:" = '<|endoftext|>'"},{name:"do_clean_text",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.GPTNeoXJapaneseTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.GPTNeoXJapaneseTokenizer.emoji_file",description:`<strong>emoji_file</strong> (<code>str</code>) &#x2014;
File containing the emoji.`,name:"emoji_file"},{anchor:"transformers.GPTNeoXJapaneseTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|endoftext|&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.GPTNeoXJapaneseTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|endoftext|&gt;&quot;</code>) &#x2014;
The token used for padding`,name:"pad_token"},{anchor:"transformers.GPTNeoXJapaneseTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|startoftext|&gt;&quot;</code>) &#x2014;
The beginning of sequence token.`,name:"bos_token"},{anchor:"transformers.GPTNeoXJapaneseTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|endoftext|&gt;&quot;</code>) &#x2014;
The end of sequence token.`,name:"eos_token"},{anchor:"transformers.GPTNeoXJapaneseTokenizer.do_clean_text",description:`<strong>do_clean_text</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to clean text for URL, EMAIL, TEL, Japanese DATE and Japanese PRICE.`,name:"do_clean_text"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/gpt_neox_japanese/tokenization_gpt_neox_japanese.py#L70"}}),se=new io({props:{anchor:"transformers.GPTNeoXJapaneseTokenizer.example",$$slots:{default:[ls]},$$scope:{ctx:G}}}),Ce=new Ve({props:{name:"convert_tokens_to_string",anchor:"transformers.GPTNeoXJapaneseTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/gpt_neox_japanese/tokenization_gpt_neox_japanese.py#L177"}}),qe=new Ke({}),Fe=new Ve({props:{name:"class transformers.GPTNeoXJapaneseModel",anchor:"transformers.GPTNeoXJapaneseModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.GPTNeoXJapaneseModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig">~GPTNeoXJapaneseConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py#L439"}}),Ie=new Ve({props:{name:"forward",anchor:"transformers.GPTNeoXJapaneseModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.GPTNeoXJapaneseModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseTokenizer">GPTNeoXJapaneseTokenizer</a>.`,name:"input_ids"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.
If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py#L459",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPast"
>transformers.modeling_outputs.BaseModelOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig"
>GPTNeoXJapaneseConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPast"
>transformers.modeling_outputs.BaseModelOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),le=new ts({props:{$$slots:{default:[ds]},$$scope:{ctx:G}}}),de=new io({props:{anchor:"transformers.GPTNeoXJapaneseModel.forward.example",$$slots:{default:[cs]},$$scope:{ctx:G}}}),Oe=new Ke({}),Se=new Ve({props:{name:"class transformers.GPTNeoXJapaneseForCausalLM",anchor:"transformers.GPTNeoXJapaneseForCausalLM",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.GPTNeoXJapaneseForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig">~GPTNeoXJapaneseConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py#L593"}}),Be=new Ve({props:{name:"forward",anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseTokenizer">GPTNeoXJapaneseTokenizer</a>.`,name:"input_ids"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>. The two additional tensors are
only required when the model is used as a decoder in a Sequence to Sequence model.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in
<code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are
ignored (masked), the loss is only computed for the tokens with labels n <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py#L613",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig"
>GPTNeoXJapaneseConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>)</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),pe=new ts({props:{$$slots:{default:[ps]},$$scope:{ctx:G}}}),he=new io({props:{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.example",$$slots:{default:[hs]},$$scope:{ctx:G}}}),{c(){c=n("meta"),b=p(),u=n("h1"),f=n("a"),v=n("span"),T(d.$$.fragment),g=p(),F=n("span"),co=r("GPT-NeoX-Japanese"),At=p(),O=n("h2"),Z=n("a"),lt=n("span"),T(ge.$$.fragment),po=p(),dt=n("span"),ho=r("Overview"),Lt=p(),j=n("p"),uo=r("We introduce GPT-NeoX-Japanese, which is an autoregressive language model for Japanese, trained on top of "),_e=n("a"),mo=r("https://github.com/EleutherAI/gpt-neox"),fo=r(`.
Japanese is a unique language with its large vocabulary and a combination of hiragana, katakana, and kanji writing scripts.
To address this distinct structure of the Japanese language, we use a `),be=n("a"),go=r("special sub-word tokenizer"),_o=r(". We are very grateful to "),ct=n("em"),bo=r("tanreinama"),vo=r(` for open-sourcing this incredibly helpful tokenizer.
Following the recommendations from Google\u2019s research on `),ve=n("a"),To=r("PaLM"),ko=r(", we have removed bias parameters from transformer blocks, achieving better model performance. Please refer "),Te=n("a"),yo=r("this article"),wo=r(" in detail."),It=p(),N=n("p"),Po=r("Development of the model was led by "),ke=n("a"),$o=r("Shinya Otani"),No=r(", "),ye=n("a"),jo=r("Takayoshi Makabe"),xo=r(", "),we=n("a"),Jo=r("Anuj Arora"),Go=r(", and "),Pe=n("a"),Xo=r("Kyo Hattori"),zo=r(" from "),$e=n("a"),Eo=r("ABEJA, Inc."),Mo=r(". For more information on this model-building activity, please refer "),Ne=n("a"),Co=r("here (ja)"),qo=r("."),Ot=p(),S=n("h3"),ee=n("a"),pt=n("span"),T(je.$$.fragment),Fo=p(),ht=n("span"),Ao=r("Generation"),St=p(),te=n("p"),Lo=r("The "),ut=n("code"),Io=r("generate()"),Oo=r(" method can be used to generate text using GPT NeoX Japanese model."),Dt=p(),T(xe.$$.fragment),Bt=p(),D=n("h2"),oe=n("a"),mt=n("span"),T(Je.$$.fragment),So=p(),ft=n("span"),Do=r("GPTNeoXJapaneseConfig"),Wt=p(),z=n("div"),T(Ge.$$.fragment),Bo=p(),B=n("p"),Wo=r("This is the configuration class to store the configuration of a "),gt=n("code"),Ho=r("GPTNeoXModelJapanese"),Vo=r(`. It is used to instantiate
a GPTNeoX model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the GPTNeoXJapanese
`),Xe=n("a"),Ko=r("abeja/gpt-neox-japanese-2.7b"),Uo=r(" architecture."),Ro=p(),W=n("p"),Yo=r("Configuration objects inherit from "),Ue=n("a"),Qo=r("PretrainedConfig"),Zo=r(` and can be used to control the model outputs. Read the
documentation from `),Re=n("a"),en=r("PretrainedConfig"),tn=r(" for more information. Default configs is set as 2.7B model"),on=p(),T(ne.$$.fragment),Ht=p(),H=n("h2"),ae=n("a"),_t=n("span"),T(ze.$$.fragment),nn=p(),bt=n("span"),an=r("GPTNeoXJapaneseTokenizer"),Vt=p(),x=n("div"),T(Ee.$$.fragment),sn=p(),V=n("p"),rn=r("This tokenizer inherits from "),Ye=n("a"),ln=r("PreTrainedTokenizer"),dn=r(` and is based on Japanese special Sub-Word-Encoding that is
used in this repository (`),Me=n("a"),cn=r("https://github.com/tanreinama/Japanese-BPEEncoder_V2"),pn=r(`). Check the repository for details.
Japanese has a relatively large vocabulary and there is no separation between words. Furthermore, the language is a
combination of hiragana, katakana, and kanji, and variants such as \u201C1\u201D and \u201C\u2460\u201D are often used. In order to cope
with these, this tokenizer has the following features`),hn=p(),J=n("ul"),vt=n("li"),un=r("Subword-by-subword segmentation, which is intermediate between byte strings and morphological analysis."),mn=p(),Tt=n("li"),fn=r(`BPEs are created for each Kanji, Hiragana, and Katakana character, and there are no BPEs that cross character
types, such as Kanji + Hiragana or Hiragana + Katakana.`),gn=p(),kt=n("li"),_n=r("All-byte encoding that does not require <unk>."),bn=p(),yt=n("li"),vn=r("Independent of UTF codes such as 2-byte and 3-byte characters"),Tn=p(),wt=n("li"),kn=r("Conversion of heterographs to the same token_id"),yn=p(),Pt=n("li"),wn=r("Emoji and Emoticon are grouped into 12 types as special tags."),Pn=p(),T(se.$$.fragment),$n=p(),re=n("div"),T(Ce.$$.fragment),Nn=p(),$t=n("p"),jn=r("Converts a sequence of tokens (string) in a single string."),Kt=p(),K=n("h2"),ie=n("a"),Nt=n("span"),T(qe.$$.fragment),xn=p(),jt=n("span"),Jn=r("GPTNeoXJapaneseModel"),Ut=p(),A=n("div"),T(Fe.$$.fragment),Gn=p(),Ae=n("p"),Xn=r(`The bare GPTNeoXJapanese Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Le=n("a"),zn=r("torch.nn.Module"),En=r(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Mn=p(),E=n("div"),T(Ie.$$.fragment),Cn=p(),U=n("p"),qn=r("The "),Qe=n("a"),Fn=r("GPTNeoXJapaneseModel"),An=r(" forward method, overrides the "),xt=n("code"),Ln=r("__call__"),In=r(" special method."),On=p(),T(le.$$.fragment),Sn=p(),T(de.$$.fragment),Rt=p(),R=n("h2"),ce=n("a"),Jt=n("span"),T(Oe.$$.fragment),Dn=p(),Gt=n("span"),Bn=r("GPTNeoXJapaneseForCausalLM"),Yt=p(),L=n("div"),T(Se.$$.fragment),Wn=p(),Y=n("p"),Hn=r("GPTNeoXJapanese Model with a "),Xt=n("code"),Vn=r("language modeling"),Kn=r(` head on top for Classifier Model fine-tuning.
This model is a PyTorch `),De=n("a"),Un=r("torch.nn.Module"),Rn=r(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Yn=p(),M=n("div"),T(Be.$$.fragment),Qn=p(),Q=n("p"),Zn=r("The "),Ze=n("a"),ea=r("GPTNeoXJapaneseForCausalLM"),ta=r(" forward method, overrides the "),zt=n("code"),oa=r("__call__"),na=r(" special method."),aa=p(),T(pe.$$.fragment),sa=p(),T(he.$$.fragment),this.h()},l(t){const m=ss('[data-svelte="svelte-1phssyn"]',document.head);c=a(m,"META",{name:!0,content:!0}),m.forEach(o),b=h(t),u=a(t,"H1",{class:!0});var We=s(u);f=a(We,"A",{id:!0,class:!0,href:!0});var Et=s(f);v=a(Et,"SPAN",{});var Mt=s(v);k(d.$$.fragment,Mt),Mt.forEach(o),Et.forEach(o),g=h(We),F=a(We,"SPAN",{});var Ct=s(F);co=i(Ct,"GPT-NeoX-Japanese"),Ct.forEach(o),We.forEach(o),At=h(t),O=a(t,"H2",{class:!0});var He=s(O);Z=a(He,"A",{id:!0,class:!0,href:!0});var qt=s(Z);lt=a(qt,"SPAN",{});var ra=s(lt);k(ge.$$.fragment,ra),ra.forEach(o),qt.forEach(o),po=h(He),dt=a(He,"SPAN",{});var ia=s(dt);ho=i(ia,"Overview"),ia.forEach(o),He.forEach(o),Lt=h(t),j=a(t,"P",{});var C=s(j);uo=i(C,"We introduce GPT-NeoX-Japanese, which is an autoregressive language model for Japanese, trained on top of "),_e=a(C,"A",{href:!0,rel:!0});var la=s(_e);mo=i(la,"https://github.com/EleutherAI/gpt-neox"),la.forEach(o),fo=i(C,`.
Japanese is a unique language with its large vocabulary and a combination of hiragana, katakana, and kanji writing scripts.
To address this distinct structure of the Japanese language, we use a `),be=a(C,"A",{href:!0,rel:!0});var da=s(be);go=i(da,"special sub-word tokenizer"),da.forEach(o),_o=i(C,". We are very grateful to "),ct=a(C,"EM",{});var ca=s(ct);bo=i(ca,"tanreinama"),ca.forEach(o),vo=i(C,` for open-sourcing this incredibly helpful tokenizer.
Following the recommendations from Google\u2019s research on `),ve=a(C,"A",{href:!0,rel:!0});var pa=s(ve);To=i(pa,"PaLM"),pa.forEach(o),ko=i(C,", we have removed bias parameters from transformer blocks, achieving better model performance. Please refer "),Te=a(C,"A",{href:!0,rel:!0});var ha=s(Te);yo=i(ha,"this article"),ha.forEach(o),wo=i(C," in detail."),C.forEach(o),It=h(t),N=a(t,"P",{});var X=s(N);Po=i(X,"Development of the model was led by "),ke=a(X,"A",{href:!0,rel:!0});var ua=s(ke);$o=i(ua,"Shinya Otani"),ua.forEach(o),No=i(X,", "),ye=a(X,"A",{href:!0,rel:!0});var ma=s(ye);jo=i(ma,"Takayoshi Makabe"),ma.forEach(o),xo=i(X,", "),we=a(X,"A",{href:!0,rel:!0});var fa=s(we);Jo=i(fa,"Anuj Arora"),fa.forEach(o),Go=i(X,", and "),Pe=a(X,"A",{href:!0,rel:!0});var ga=s(Pe);Xo=i(ga,"Kyo Hattori"),ga.forEach(o),zo=i(X," from "),$e=a(X,"A",{href:!0,rel:!0});var _a=s($e);Eo=i(_a,"ABEJA, Inc."),_a.forEach(o),Mo=i(X,". For more information on this model-building activity, please refer "),Ne=a(X,"A",{href:!0,rel:!0});var ba=s(Ne);Co=i(ba,"here (ja)"),ba.forEach(o),qo=i(X,"."),X.forEach(o),Ot=h(t),S=a(t,"H3",{class:!0});var Zt=s(S);ee=a(Zt,"A",{id:!0,class:!0,href:!0});var va=s(ee);pt=a(va,"SPAN",{});var Ta=s(pt);k(je.$$.fragment,Ta),Ta.forEach(o),va.forEach(o),Fo=h(Zt),ht=a(Zt,"SPAN",{});var ka=s(ht);Ao=i(ka,"Generation"),ka.forEach(o),Zt.forEach(o),St=h(t),te=a(t,"P",{});var eo=s(te);Lo=i(eo,"The "),ut=a(eo,"CODE",{});var ya=s(ut);Io=i(ya,"generate()"),ya.forEach(o),Oo=i(eo," method can be used to generate text using GPT NeoX Japanese model."),eo.forEach(o),Dt=h(t),k(xe.$$.fragment,t),Bt=h(t),D=a(t,"H2",{class:!0});var to=s(D);oe=a(to,"A",{id:!0,class:!0,href:!0});var wa=s(oe);mt=a(wa,"SPAN",{});var Pa=s(mt);k(Je.$$.fragment,Pa),Pa.forEach(o),wa.forEach(o),So=h(to),ft=a(to,"SPAN",{});var $a=s(ft);Do=i($a,"GPTNeoXJapaneseConfig"),$a.forEach(o),to.forEach(o),Wt=h(t),z=a(t,"DIV",{class:!0});var ue=s(z);k(Ge.$$.fragment,ue),Bo=h(ue),B=a(ue,"P",{});var et=s(B);Wo=i(et,"This is the configuration class to store the configuration of a "),gt=a(et,"CODE",{});var Na=s(gt);Ho=i(Na,"GPTNeoXModelJapanese"),Na.forEach(o),Vo=i(et,`. It is used to instantiate
a GPTNeoX model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the GPTNeoXJapanese
`),Xe=a(et,"A",{href:!0,rel:!0});var ja=s(Xe);Ko=i(ja,"abeja/gpt-neox-japanese-2.7b"),ja.forEach(o),Uo=i(et," architecture."),et.forEach(o),Ro=h(ue),W=a(ue,"P",{});var tt=s(W);Yo=i(tt,"Configuration objects inherit from "),Ue=a(tt,"A",{href:!0});var xa=s(Ue);Qo=i(xa,"PretrainedConfig"),xa.forEach(o),Zo=i(tt,` and can be used to control the model outputs. Read the
documentation from `),Re=a(tt,"A",{href:!0});var Ja=s(Re);en=i(Ja,"PretrainedConfig"),Ja.forEach(o),tn=i(tt," for more information. Default configs is set as 2.7B model"),tt.forEach(o),on=h(ue),k(ne.$$.fragment,ue),ue.forEach(o),Ht=h(t),H=a(t,"H2",{class:!0});var oo=s(H);ae=a(oo,"A",{id:!0,class:!0,href:!0});var Ga=s(ae);_t=a(Ga,"SPAN",{});var Xa=s(_t);k(ze.$$.fragment,Xa),Xa.forEach(o),Ga.forEach(o),nn=h(oo),bt=a(oo,"SPAN",{});var za=s(bt);an=i(za,"GPTNeoXJapaneseTokenizer"),za.forEach(o),oo.forEach(o),Vt=h(t),x=a(t,"DIV",{class:!0});var I=s(x);k(Ee.$$.fragment,I),sn=h(I),V=a(I,"P",{});var ot=s(V);rn=i(ot,"This tokenizer inherits from "),Ye=a(ot,"A",{href:!0});var Ea=s(Ye);ln=i(Ea,"PreTrainedTokenizer"),Ea.forEach(o),dn=i(ot,` and is based on Japanese special Sub-Word-Encoding that is
used in this repository (`),Me=a(ot,"A",{href:!0,rel:!0});var Ma=s(Me);cn=i(Ma,"https://github.com/tanreinama/Japanese-BPEEncoder_V2"),Ma.forEach(o),pn=i(ot,`). Check the repository for details.
Japanese has a relatively large vocabulary and there is no separation between words. Furthermore, the language is a
combination of hiragana, katakana, and kanji, and variants such as \u201C1\u201D and \u201C\u2460\u201D are often used. In order to cope
with these, this tokenizer has the following features`),ot.forEach(o),hn=h(I),J=a(I,"UL",{});var q=s(J);vt=a(q,"LI",{});var Ca=s(vt);un=i(Ca,"Subword-by-subword segmentation, which is intermediate between byte strings and morphological analysis."),Ca.forEach(o),mn=h(q),Tt=a(q,"LI",{});var qa=s(Tt);fn=i(qa,`BPEs are created for each Kanji, Hiragana, and Katakana character, and there are no BPEs that cross character
types, such as Kanji + Hiragana or Hiragana + Katakana.`),qa.forEach(o),gn=h(q),kt=a(q,"LI",{});var Fa=s(kt);_n=i(Fa,"All-byte encoding that does not require <unk>."),Fa.forEach(o),bn=h(q),yt=a(q,"LI",{});var Aa=s(yt);vn=i(Aa,"Independent of UTF codes such as 2-byte and 3-byte characters"),Aa.forEach(o),Tn=h(q),wt=a(q,"LI",{});var La=s(wt);kn=i(La,"Conversion of heterographs to the same token_id"),La.forEach(o),yn=h(q),Pt=a(q,"LI",{});var Ia=s(Pt);wn=i(Ia,"Emoji and Emoticon are grouped into 12 types as special tags."),Ia.forEach(o),q.forEach(o),Pn=h(I),k(se.$$.fragment,I),$n=h(I),re=a(I,"DIV",{class:!0});var no=s(re);k(Ce.$$.fragment,no),Nn=h(no),$t=a(no,"P",{});var Oa=s($t);jn=i(Oa,"Converts a sequence of tokens (string) in a single string."),Oa.forEach(o),no.forEach(o),I.forEach(o),Kt=h(t),K=a(t,"H2",{class:!0});var ao=s(K);ie=a(ao,"A",{id:!0,class:!0,href:!0});var Sa=s(ie);Nt=a(Sa,"SPAN",{});var Da=s(Nt);k(qe.$$.fragment,Da),Da.forEach(o),Sa.forEach(o),xn=h(ao),jt=a(ao,"SPAN",{});var Ba=s(jt);Jn=i(Ba,"GPTNeoXJapaneseModel"),Ba.forEach(o),ao.forEach(o),Ut=h(t),A=a(t,"DIV",{class:!0});var nt=s(A);k(Fe.$$.fragment,nt),Gn=h(nt),Ae=a(nt,"P",{});var so=s(Ae);Xn=i(so,`The bare GPTNeoXJapanese Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Le=a(so,"A",{href:!0,rel:!0});var Wa=s(Le);zn=i(Wa,"torch.nn.Module"),Wa.forEach(o),En=i(so,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),so.forEach(o),Mn=h(nt),E=a(nt,"DIV",{class:!0});var me=s(E);k(Ie.$$.fragment,me),Cn=h(me),U=a(me,"P",{});var at=s(U);qn=i(at,"The "),Qe=a(at,"A",{href:!0});var Ha=s(Qe);Fn=i(Ha,"GPTNeoXJapaneseModel"),Ha.forEach(o),An=i(at," forward method, overrides the "),xt=a(at,"CODE",{});var Va=s(xt);Ln=i(Va,"__call__"),Va.forEach(o),In=i(at," special method."),at.forEach(o),On=h(me),k(le.$$.fragment,me),Sn=h(me),k(de.$$.fragment,me),me.forEach(o),nt.forEach(o),Rt=h(t),R=a(t,"H2",{class:!0});var ro=s(R);ce=a(ro,"A",{id:!0,class:!0,href:!0});var Ka=s(ce);Jt=a(Ka,"SPAN",{});var Ua=s(Jt);k(Oe.$$.fragment,Ua),Ua.forEach(o),Ka.forEach(o),Dn=h(ro),Gt=a(ro,"SPAN",{});var Ra=s(Gt);Bn=i(Ra,"GPTNeoXJapaneseForCausalLM"),Ra.forEach(o),ro.forEach(o),Yt=h(t),L=a(t,"DIV",{class:!0});var st=s(L);k(Se.$$.fragment,st),Wn=h(st),Y=a(st,"P",{});var rt=s(Y);Hn=i(rt,"GPTNeoXJapanese Model with a "),Xt=a(rt,"CODE",{});var Ya=s(Xt);Vn=i(Ya,"language modeling"),Ya.forEach(o),Kn=i(rt,` head on top for Classifier Model fine-tuning.
This model is a PyTorch `),De=a(rt,"A",{href:!0,rel:!0});var Qa=s(De);Un=i(Qa,"torch.nn.Module"),Qa.forEach(o),Rn=i(rt,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),rt.forEach(o),Yn=h(st),M=a(st,"DIV",{class:!0});var fe=s(M);k(Be.$$.fragment,fe),Qn=h(fe),Q=a(fe,"P",{});var it=s(Q);Zn=i(it,"The "),Ze=a(it,"A",{href:!0});var Za=s(Ze);ea=i(Za,"GPTNeoXJapaneseForCausalLM"),Za.forEach(o),ta=i(it," forward method, overrides the "),zt=a(it,"CODE",{});var es=s(zt);oa=i(es,"__call__"),es.forEach(o),na=i(it," special method."),it.forEach(o),aa=h(fe),k(pe.$$.fragment,fe),sa=h(fe),k(he.$$.fragment,fe),fe.forEach(o),st.forEach(o),this.h()},h(){l(c,"name","hf:doc:metadata"),l(c,"content",JSON.stringify(ms)),l(f,"id","gptneoxjapanese"),l(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(f,"href","#gptneoxjapanese"),l(u,"class","relative group"),l(Z,"id","overview"),l(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Z,"href","#overview"),l(O,"class","relative group"),l(_e,"href","https://github.com/EleutherAI/gpt-neox"),l(_e,"rel","nofollow"),l(be,"href","https://github.com/tanreinama/Japanese-BPEEncoder_V2"),l(be,"rel","nofollow"),l(ve,"href","https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html"),l(ve,"rel","nofollow"),l(Te,"href","https://medium.com/ml-abeja/training-a-better-gpt-2-93b157662ae4"),l(Te,"rel","nofollow"),l(ke,"href","https://github.com/SO0529"),l(ke,"rel","nofollow"),l(ye,"href","https://github.com/spider-man-tm"),l(ye,"rel","nofollow"),l(we,"href","https://github.com/Anuj040"),l(we,"rel","nofollow"),l(Pe,"href","https://github.com/go5paopao"),l(Pe,"rel","nofollow"),l($e,"href","https://www.abejainc.com/"),l($e,"rel","nofollow"),l(Ne,"href","https://tech-blog.abeja.asia/entry/abeja-gpt-project-202207"),l(Ne,"rel","nofollow"),l(ee,"id","generation"),l(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ee,"href","#generation"),l(S,"class","relative group"),l(oe,"id","transformers.GPTNeoXJapaneseConfig"),l(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(oe,"href","#transformers.GPTNeoXJapaneseConfig"),l(D,"class","relative group"),l(Xe,"href","https://huggingface.co/abeja/gpt-neox-japanese-2.7b"),l(Xe,"rel","nofollow"),l(Ue,"href","/docs/transformers/v4.24.0/en/main_classes/configuration#transformers.PretrainedConfig"),l(Re,"href","/docs/transformers/v4.24.0/en/main_classes/configuration#transformers.PretrainedConfig"),l(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ae,"id","transformers.GPTNeoXJapaneseTokenizer"),l(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ae,"href","#transformers.GPTNeoXJapaneseTokenizer"),l(H,"class","relative group"),l(Ye,"href","/docs/transformers/v4.24.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),l(Me,"href","https://github.com/tanreinama/Japanese-BPEEncoder_V2"),l(Me,"rel","nofollow"),l(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ie,"id","transformers.GPTNeoXJapaneseModel"),l(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ie,"href","#transformers.GPTNeoXJapaneseModel"),l(K,"class","relative group"),l(Le,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Le,"rel","nofollow"),l(Qe,"href","/docs/transformers/v4.24.0/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseModel"),l(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ce,"id","transformers.GPTNeoXJapaneseForCausalLM"),l(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ce,"href","#transformers.GPTNeoXJapaneseForCausalLM"),l(R,"class","relative group"),l(De,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(De,"rel","nofollow"),l(Ze,"href","/docs/transformers/v4.24.0/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseForCausalLM"),l(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,m){e(document.head,c),_(t,b,m),_(t,u,m),e(u,f),e(f,v),y(d,v,null),e(u,g),e(u,F),e(F,co),_(t,At,m),_(t,O,m),e(O,Z),e(Z,lt),y(ge,lt,null),e(O,po),e(O,dt),e(dt,ho),_(t,Lt,m),_(t,j,m),e(j,uo),e(j,_e),e(_e,mo),e(j,fo),e(j,be),e(be,go),e(j,_o),e(j,ct),e(ct,bo),e(j,vo),e(j,ve),e(ve,To),e(j,ko),e(j,Te),e(Te,yo),e(j,wo),_(t,It,m),_(t,N,m),e(N,Po),e(N,ke),e(ke,$o),e(N,No),e(N,ye),e(ye,jo),e(N,xo),e(N,we),e(we,Jo),e(N,Go),e(N,Pe),e(Pe,Xo),e(N,zo),e(N,$e),e($e,Eo),e(N,Mo),e(N,Ne),e(Ne,Co),e(N,qo),_(t,Ot,m),_(t,S,m),e(S,ee),e(ee,pt),y(je,pt,null),e(S,Fo),e(S,ht),e(ht,Ao),_(t,St,m),_(t,te,m),e(te,Lo),e(te,ut),e(ut,Io),e(te,Oo),_(t,Dt,m),y(xe,t,m),_(t,Bt,m),_(t,D,m),e(D,oe),e(oe,mt),y(Je,mt,null),e(D,So),e(D,ft),e(ft,Do),_(t,Wt,m),_(t,z,m),y(Ge,z,null),e(z,Bo),e(z,B),e(B,Wo),e(B,gt),e(gt,Ho),e(B,Vo),e(B,Xe),e(Xe,Ko),e(B,Uo),e(z,Ro),e(z,W),e(W,Yo),e(W,Ue),e(Ue,Qo),e(W,Zo),e(W,Re),e(Re,en),e(W,tn),e(z,on),y(ne,z,null),_(t,Ht,m),_(t,H,m),e(H,ae),e(ae,_t),y(ze,_t,null),e(H,nn),e(H,bt),e(bt,an),_(t,Vt,m),_(t,x,m),y(Ee,x,null),e(x,sn),e(x,V),e(V,rn),e(V,Ye),e(Ye,ln),e(V,dn),e(V,Me),e(Me,cn),e(V,pn),e(x,hn),e(x,J),e(J,vt),e(vt,un),e(J,mn),e(J,Tt),e(Tt,fn),e(J,gn),e(J,kt),e(kt,_n),e(J,bn),e(J,yt),e(yt,vn),e(J,Tn),e(J,wt),e(wt,kn),e(J,yn),e(J,Pt),e(Pt,wn),e(x,Pn),y(se,x,null),e(x,$n),e(x,re),y(Ce,re,null),e(re,Nn),e(re,$t),e($t,jn),_(t,Kt,m),_(t,K,m),e(K,ie),e(ie,Nt),y(qe,Nt,null),e(K,xn),e(K,jt),e(jt,Jn),_(t,Ut,m),_(t,A,m),y(Fe,A,null),e(A,Gn),e(A,Ae),e(Ae,Xn),e(Ae,Le),e(Le,zn),e(Ae,En),e(A,Mn),e(A,E),y(Ie,E,null),e(E,Cn),e(E,U),e(U,qn),e(U,Qe),e(Qe,Fn),e(U,An),e(U,xt),e(xt,Ln),e(U,In),e(E,On),y(le,E,null),e(E,Sn),y(de,E,null),_(t,Rt,m),_(t,R,m),e(R,ce),e(ce,Jt),y(Oe,Jt,null),e(R,Dn),e(R,Gt),e(Gt,Bn),_(t,Yt,m),_(t,L,m),y(Se,L,null),e(L,Wn),e(L,Y),e(Y,Hn),e(Y,Xt),e(Xt,Vn),e(Y,Kn),e(Y,De),e(De,Un),e(Y,Rn),e(L,Yn),e(L,M),y(Be,M,null),e(M,Qn),e(M,Q),e(Q,Zn),e(Q,Ze),e(Ze,ea),e(Q,ta),e(Q,zt),e(zt,oa),e(Q,na),e(M,aa),y(pe,M,null),e(M,sa),y(he,M,null),Qt=!0},p(t,[m]){const We={};m&2&&(We.$$scope={dirty:m,ctx:t}),ne.$set(We);const Et={};m&2&&(Et.$$scope={dirty:m,ctx:t}),se.$set(Et);const Mt={};m&2&&(Mt.$$scope={dirty:m,ctx:t}),le.$set(Mt);const Ct={};m&2&&(Ct.$$scope={dirty:m,ctx:t}),de.$set(Ct);const He={};m&2&&(He.$$scope={dirty:m,ctx:t}),pe.$set(He);const qt={};m&2&&(qt.$$scope={dirty:m,ctx:t}),he.$set(qt)},i(t){Qt||(w(d.$$.fragment,t),w(ge.$$.fragment,t),w(je.$$.fragment,t),w(xe.$$.fragment,t),w(Je.$$.fragment,t),w(Ge.$$.fragment,t),w(ne.$$.fragment,t),w(ze.$$.fragment,t),w(Ee.$$.fragment,t),w(se.$$.fragment,t),w(Ce.$$.fragment,t),w(qe.$$.fragment,t),w(Fe.$$.fragment,t),w(Ie.$$.fragment,t),w(le.$$.fragment,t),w(de.$$.fragment,t),w(Oe.$$.fragment,t),w(Se.$$.fragment,t),w(Be.$$.fragment,t),w(pe.$$.fragment,t),w(he.$$.fragment,t),Qt=!0)},o(t){P(d.$$.fragment,t),P(ge.$$.fragment,t),P(je.$$.fragment,t),P(xe.$$.fragment,t),P(Je.$$.fragment,t),P(Ge.$$.fragment,t),P(ne.$$.fragment,t),P(ze.$$.fragment,t),P(Ee.$$.fragment,t),P(se.$$.fragment,t),P(Ce.$$.fragment,t),P(qe.$$.fragment,t),P(Fe.$$.fragment,t),P(Ie.$$.fragment,t),P(le.$$.fragment,t),P(de.$$.fragment,t),P(Oe.$$.fragment,t),P(Se.$$.fragment,t),P(Be.$$.fragment,t),P(pe.$$.fragment,t),P(he.$$.fragment,t),Qt=!1},d(t){o(c),t&&o(b),t&&o(u),$(d),t&&o(At),t&&o(O),$(ge),t&&o(Lt),t&&o(j),t&&o(It),t&&o(N),t&&o(Ot),t&&o(S),$(je),t&&o(St),t&&o(te),t&&o(Dt),$(xe,t),t&&o(Bt),t&&o(D),$(Je),t&&o(Wt),t&&o(z),$(Ge),$(ne),t&&o(Ht),t&&o(H),$(ze),t&&o(Vt),t&&o(x),$(Ee),$(se),$(Ce),t&&o(Kt),t&&o(K),$(qe),t&&o(Ut),t&&o(A),$(Fe),$(Ie),$(le),$(de),t&&o(Rt),t&&o(R),$(Oe),t&&o(Yt),t&&o(L),$(Se),$(Be),$(pe),$(he)}}}const ms={local:"gptneoxjapanese",sections:[{local:"overview",sections:[{local:"generation",title:"Generation"}],title:"Overview"},{local:"transformers.GPTNeoXJapaneseConfig",title:"GPTNeoXJapaneseConfig"},{local:"transformers.GPTNeoXJapaneseTokenizer",title:"GPTNeoXJapaneseTokenizer"},{local:"transformers.GPTNeoXJapaneseModel",title:"GPTNeoXJapaneseModel"},{local:"transformers.GPTNeoXJapaneseForCausalLM",title:"GPTNeoXJapaneseForCausalLM"}],title:"GPT-NeoX-Japanese"};function fs(G){return rs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ys extends os{constructor(c){super();ns(this,c,fs,us,as,{})}}export{ys as default,ms as metadata};
