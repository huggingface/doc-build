import{S as lt,i as st,s as ht,e as o,k as p,w as et,t as l,M as ft,c as n,d as t,m as c,a as i,x as tt,h as s,b as f,G as a,g as h,y as at,L as pt,q as ot,o as nt,B as it,v as ct}from"../../chunks/vendor-hf-doc-builder.js";import{I as rt}from"../../chunks/IconCopyLink-hf-doc-builder.js";function ut(Ie){let d,J,m,v,I,b,se,N,he,F,g,P,W,y,fe,q,pe,H,T,ce,x,ue,de,Y,$,me,z,M,R,ge,U,S,ve,X,u,j,Pe,Te,O,_e,we,D,Ge,E,be,ye,Z,A,xe,K,_,De,B,Ee,ke,Q,w,$e,L,Me,Se,V,G,Ae,k,Le,Ce,ee;return b=new rt({}),y=new rt({}),{c(){d=o("meta"),J=p(),m=o("h1"),v=o("a"),I=o("span"),et(b.$$.fragment),se=p(),N=o("span"),he=l("DialoGPT"),F=p(),g=o("h2"),P=o("a"),W=o("span"),et(y.$$.fragment),fe=p(),q=o("span"),pe=l("Overview"),H=p(),T=o("p"),ce=l("DialoGPT was proposed in "),x=o("a"),ue=l("DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation"),de=l(` by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao,
Jianfeng Gao, Jingjing Liu, Bill Dolan. It\u2019s a GPT2 Model trained on 147M conversation-like exchanges extracted from
Reddit.`),Y=p(),$=o("p"),me=l("The abstract from the paper is the following:"),z=p(),M=o("p"),R=o("em"),ge=l(`We present a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained
transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning
from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human
both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems
that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline
systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response
generation and the development of more intelligent open-domain dialogue systems.`),U=p(),S=o("p"),ve=l("Tips:"),X=p(),u=o("ul"),j=o("li"),Pe=l(`DialoGPT is a model with absolute position embeddings so it\u2019s usually advised to pad the inputs on the right rather
than the left.`),Te=p(),O=o("li"),_e=l(`DialoGPT was trained with a causal language modeling (CLM) objective on conversational data and is therefore powerful
at response generation in open-domain dialogue systems.`),we=p(),D=o("li"),Ge=l("DialoGPT enables the user to create a chat bot in just 10 lines of code as shown on "),E=o("a"),be=l("DialoGPT\u2019s model card"),ye=l("."),Z=p(),A=o("p"),xe=l("Training:"),K=p(),_=o("p"),De=l("In order to train or fine-tune DialoGPT, one can use causal language modeling training. To cite the official paper: "),B=o("em"),Ee=l(`We
follow the OpenAI GPT-2 to model a multiturn dialogue session as a long text and frame the generation task as language
modeling. We first concatenate all dialog turns within a dialogue session into a long text x_1,\u2026, x_N (N is the
sequence length), ended by the end-of-text token.`),ke=l(" For more information please confer to the original paper."),Q=p(),w=o("p"),$e=l("DialoGPT\u2019s architecture is based on the GPT2 model, so one can refer to "),L=o("a"),Me=l("GPT2\u2019s documentation page"),Se=l("."),V=p(),G=o("p"),Ae=l("The original code can be found "),k=o("a"),Le=l("here"),Ce=l("."),this.h()},l(e){const r=ft('[data-svelte="svelte-1phssyn"]',document.head);d=n(r,"META",{name:!0,content:!0}),r.forEach(t),J=c(e),m=n(e,"H1",{class:!0});var te=i(m);v=n(te,"A",{id:!0,class:!0,href:!0});var Ne=i(v);I=n(Ne,"SPAN",{});var We=i(I);tt(b.$$.fragment,We),We.forEach(t),Ne.forEach(t),se=c(te),N=n(te,"SPAN",{});var qe=i(N);he=s(qe,"DialoGPT"),qe.forEach(t),te.forEach(t),F=c(e),g=n(e,"H2",{class:!0});var ae=i(g);P=n(ae,"A",{id:!0,class:!0,href:!0});var Re=i(P);W=n(Re,"SPAN",{});var je=i(W);tt(y.$$.fragment,je),je.forEach(t),Re.forEach(t),fe=c(ae),q=n(ae,"SPAN",{});var Oe=i(q);pe=s(Oe,"Overview"),Oe.forEach(t),ae.forEach(t),H=c(e),T=n(e,"P",{});var oe=i(T);ce=s(oe,"DialoGPT was proposed in "),x=n(oe,"A",{href:!0,rel:!0});var Be=i(x);ue=s(Be,"DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation"),Be.forEach(t),de=s(oe,` by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao,
Jianfeng Gao, Jingjing Liu, Bill Dolan. It\u2019s a GPT2 Model trained on 147M conversation-like exchanges extracted from
Reddit.`),oe.forEach(t),Y=c(e),$=n(e,"P",{});var Je=i($);me=s(Je,"The abstract from the paper is the following:"),Je.forEach(t),z=c(e),M=n(e,"P",{});var Fe=i(M);R=n(Fe,"EM",{});var He=i(R);ge=s(He,`We present a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained
transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning
from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human
both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems
that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline
systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response
generation and the development of more intelligent open-domain dialogue systems.`),He.forEach(t),Fe.forEach(t),U=c(e),S=n(e,"P",{});var Ye=i(S);ve=s(Ye,"Tips:"),Ye.forEach(t),X=c(e),u=n(e,"UL",{});var C=i(u);j=n(C,"LI",{});var ze=i(j);Pe=s(ze,`DialoGPT is a model with absolute position embeddings so it\u2019s usually advised to pad the inputs on the right rather
than the left.`),ze.forEach(t),Te=c(C),O=n(C,"LI",{});var Ue=i(O);_e=s(Ue,`DialoGPT was trained with a causal language modeling (CLM) objective on conversational data and is therefore powerful
at response generation in open-domain dialogue systems.`),Ue.forEach(t),we=c(C),D=n(C,"LI",{});var ne=i(D);Ge=s(ne,"DialoGPT enables the user to create a chat bot in just 10 lines of code as shown on "),E=n(ne,"A",{href:!0,rel:!0});var Xe=i(E);be=s(Xe,"DialoGPT\u2019s model card"),Xe.forEach(t),ye=s(ne,"."),ne.forEach(t),C.forEach(t),Z=c(e),A=n(e,"P",{});var Ze=i(A);xe=s(Ze,"Training:"),Ze.forEach(t),K=c(e),_=n(e,"P",{});var ie=i(_);De=s(ie,"In order to train or fine-tune DialoGPT, one can use causal language modeling training. To cite the official paper: "),B=n(ie,"EM",{});var Ke=i(B);Ee=s(Ke,`We
follow the OpenAI GPT-2 to model a multiturn dialogue session as a long text and frame the generation task as language
modeling. We first concatenate all dialog turns within a dialogue session into a long text x_1,\u2026, x_N (N is the
sequence length), ended by the end-of-text token.`),Ke.forEach(t),ke=s(ie," For more information please confer to the original paper."),ie.forEach(t),Q=c(e),w=n(e,"P",{});var re=i(w);$e=s(re,"DialoGPT\u2019s architecture is based on the GPT2 model, so one can refer to "),L=n(re,"A",{href:!0});var Qe=i(L);Me=s(Qe,"GPT2\u2019s documentation page"),Qe.forEach(t),Se=s(re,"."),re.forEach(t),V=c(e),G=n(e,"P",{});var le=i(G);Ae=s(le,"The original code can be found "),k=n(le,"A",{href:!0,rel:!0});var Ve=i(k);Le=s(Ve,"here"),Ve.forEach(t),Ce=s(le,"."),le.forEach(t),this.h()},h(){f(d,"name","hf:doc:metadata"),f(d,"content",JSON.stringify(dt)),f(v,"id","dialogpt"),f(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(v,"href","#dialogpt"),f(m,"class","relative group"),f(P,"id","overview"),f(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(P,"href","#overview"),f(g,"class","relative group"),f(x,"href","https://arxiv.org/abs/1911.00536"),f(x,"rel","nofollow"),f(E,"href","https://huggingface.co/microsoft/DialoGPT-medium"),f(E,"rel","nofollow"),f(L,"href","gpt2"),f(k,"href","https://github.com/microsoft/DialoGPT"),f(k,"rel","nofollow")},m(e,r){a(document.head,d),h(e,J,r),h(e,m,r),a(m,v),a(v,I),at(b,I,null),a(m,se),a(m,N),a(N,he),h(e,F,r),h(e,g,r),a(g,P),a(P,W),at(y,W,null),a(g,fe),a(g,q),a(q,pe),h(e,H,r),h(e,T,r),a(T,ce),a(T,x),a(x,ue),a(T,de),h(e,Y,r),h(e,$,r),a($,me),h(e,z,r),h(e,M,r),a(M,R),a(R,ge),h(e,U,r),h(e,S,r),a(S,ve),h(e,X,r),h(e,u,r),a(u,j),a(j,Pe),a(u,Te),a(u,O),a(O,_e),a(u,we),a(u,D),a(D,Ge),a(D,E),a(E,be),a(D,ye),h(e,Z,r),h(e,A,r),a(A,xe),h(e,K,r),h(e,_,r),a(_,De),a(_,B),a(B,Ee),a(_,ke),h(e,Q,r),h(e,w,r),a(w,$e),a(w,L),a(L,Me),a(w,Se),h(e,V,r),h(e,G,r),a(G,Ae),a(G,k),a(k,Le),a(G,Ce),ee=!0},p:pt,i(e){ee||(ot(b.$$.fragment,e),ot(y.$$.fragment,e),ee=!0)},o(e){nt(b.$$.fragment,e),nt(y.$$.fragment,e),ee=!1},d(e){t(d),e&&t(J),e&&t(m),it(b),e&&t(F),e&&t(g),it(y),e&&t(H),e&&t(T),e&&t(Y),e&&t($),e&&t(z),e&&t(M),e&&t(U),e&&t(S),e&&t(X),e&&t(u),e&&t(Z),e&&t(A),e&&t(K),e&&t(_),e&&t(Q),e&&t(w),e&&t(V),e&&t(G)}}}const dt={local:"dialogpt",sections:[{local:"overview",title:"Overview"}],title:"DialoGPT"};function mt(Ie){return ct(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Pt extends lt{constructor(d){super();st(this,d,mt,ut,ht,{})}}export{Pt as default,dt as metadata};
