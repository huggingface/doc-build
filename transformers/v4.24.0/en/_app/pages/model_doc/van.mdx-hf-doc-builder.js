import{S as Po,i as Io,s as qo,e as n,k as m,w,t as i,M as zo,c as s,d as a,m as u,a as r,x as $,h as l,b as c,N as Lo,G as t,g as p,y,q as V,o as A,B as x,v as Do,L as Za}from"../../chunks/vendor-hf-doc-builder.js";import{T as Fo}from"../../chunks/Tip-hf-doc-builder.js";import{D as nt}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Ja}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as st}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Ka}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Oo(T){let d,b,_,f,v;return f=new Ja({props:{code:`from transformers import VanModel, VanConfig

# Initializing a VAN van-base style configuration
configuration = VanConfig()
# Initializing a model from the van-base style configuration
model = VanModel(configuration)
# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> VanModel, VanConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a VAN van-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = VanConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the van-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VanModel(configuration)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){d=n("p"),b=i("Example:"),_=m(),w(f.$$.fragment)},l(o){d=s(o,"P",{});var g=r(d);b=l(g,"Example:"),g.forEach(a),_=u(o),$(f.$$.fragment,o)},m(o,g){p(o,d,g),t(d,b),p(o,_,g),y(f,o,g),v=!0},p:Za,i(o){v||(V(f.$$.fragment,o),v=!0)},o(o){A(f.$$.fragment,o),v=!1},d(o){o&&a(d),o&&a(_),x(f,o)}}}function So(T){let d,b,_,f,v;return{c(){d=n("p"),b=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=n("code"),f=i("Module"),v=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(o){d=s(o,"P",{});var g=r(d);b=l(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=s(g,"CODE",{});var N=r(_);f=l(N,"Module"),N.forEach(a),v=l(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(a)},m(o,g){p(o,d,g),t(d,b),t(d,_),t(_,f),t(d,v)},d(o){o&&a(d)}}}function Wo(T){let d,b,_,f,v;return f=new Ja({props:{code:`from transformers import AutoFeatureExtractor, VanModel
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

feature_extractor = AutoFeatureExtractor.from_pretrained("Visual-Attention-Network/van-base")
model = VanModel.from_pretrained("Visual-Attention-Network/van-base")

inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
list(last_hidden_states.shape)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor, VanModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;Visual-Attention-Network/van-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VanModel.from_pretrained(<span class="hljs-string">&quot;Visual-Attention-Network/van-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>]`}}),{c(){d=n("p"),b=i("Example:"),_=m(),w(f.$$.fragment)},l(o){d=s(o,"P",{});var g=r(d);b=l(g,"Example:"),g.forEach(a),_=u(o),$(f.$$.fragment,o)},m(o,g){p(o,d,g),t(d,b),p(o,_,g),y(f,o,g),v=!0},p:Za,i(o){v||(V(f.$$.fragment,o),v=!0)},o(o){A(f.$$.fragment,o),v=!1},d(o){o&&a(d),o&&a(_),x(f,o)}}}function Ho(T){let d,b,_,f,v;return{c(){d=n("p"),b=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=n("code"),f=i("Module"),v=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(o){d=s(o,"P",{});var g=r(d);b=l(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=s(g,"CODE",{});var N=r(_);f=l(N,"Module"),N.forEach(a),v=l(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(a)},m(o,g){p(o,d,g),t(d,b),t(d,_),t(_,f),t(d,v)},d(o){o&&a(d)}}}function Uo(T){let d,b,_,f,v;return f=new Ja({props:{code:`from transformers import AutoFeatureExtractor, VanForImageClassification
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

feature_extractor = AutoFeatureExtractor.from_pretrained("Visual-Attention-Network/van-base")
model = VanForImageClassification.from_pretrained("Visual-Attention-Network/van-base")

inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

# model predicts one of the 1000 ImageNet classes
predicted_label = logits.argmax(-1).item()
print(model.config.id2label[predicted_label])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor, VanForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;Visual-Attention-Network/van-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VanForImageClassification.from_pretrained(<span class="hljs-string">&quot;Visual-Attention-Network/van-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`}}),{c(){d=n("p"),b=i("Example:"),_=m(),w(f.$$.fragment)},l(o){d=s(o,"P",{});var g=r(d);b=l(g,"Example:"),g.forEach(a),_=u(o),$(f.$$.fragment,o)},m(o,g){p(o,d,g),t(d,b),p(o,_,g),y(f,o,g),v=!0},p:Za,i(o){v||(V(f.$$.fragment,o),v=!0)},o(o){A(f.$$.fragment,o),v=!1},d(o){o&&a(d),o&&a(_),x(f,o)}}}function Bo(T){let d,b,_,f,v,o,g,N,Mt,rt,P,W,He,oe,Ft,Ue,Pt,it,H,It,ne,qt,zt,lt,Ce,Lt,ct,Te,Dt,dt,ke,se,Ot,re,St,Wt,ht,je,Ht,pt,Ne,ie,Ut,Be,Bt,Rt,ft,U,Gt,le,Kt,Zt,mt,ce,Qa,ut,F,Jt,de,Qt,Xt,he,Yt,ea,gt,I,B,Re,pe,ta,Ge,aa,_t,E,fe,oa,q,na,Me,sa,ra,me,ia,la,ca,z,da,Fe,ha,pa,Pe,fa,ma,ua,R,vt,L,G,Ke,ue,ga,Ze,_a,bt,M,ge,va,_e,ba,ve,wa,$a,ya,k,be,Va,D,Aa,Ie,xa,Ea,Je,Ca,Ta,ka,K,ja,Z,wt,O,J,Qe,we,Na,Xe,Ma,$t,C,$e,Fa,Ye,Pa,Ia,ye,qa,Ve,za,La,Da,j,Ae,Oa,S,Sa,qe,Wa,Ha,et,Ua,Ba,Ra,Q,Ga,X,yt;return o=new st({}),oe=new st({}),pe=new st({}),fe=new nt({props:{name:"class transformers.VanConfig",anchor:"transformers.VanConfig",parameters:[{name:"image_size",val:" = 224"},{name:"num_channels",val:" = 3"},{name:"patch_sizes",val:" = [7, 3, 3, 3]"},{name:"strides",val:" = [4, 2, 2, 2]"},{name:"hidden_sizes",val:" = [64, 128, 320, 512]"},{name:"depths",val:" = [3, 3, 12, 3]"},{name:"mlp_ratios",val:" = [8, 8, 4, 4]"},{name:"hidden_act",val:" = 'gelu'"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"layer_scale_init_value",val:" = 0.01"},{name:"drop_path_rate",val:" = 0.0"},{name:"dropout_rate",val:" = 0.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.VanConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.VanConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.VanConfig.patch_sizes",description:`<strong>patch_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[7, 3, 3, 3]</code>) &#x2014;
Patch size to use in each stage&#x2019;s embedding layer.`,name:"patch_sizes"},{anchor:"transformers.VanConfig.strides",description:`<strong>strides</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[4, 2, 2, 2]</code>) &#x2014;
Stride size to use in each stage&#x2019;s embedding layer to downsample the input.`,name:"strides"},{anchor:"transformers.VanConfig.hidden_sizes",description:`<strong>hidden_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[64, 128, 320, 512]</code>) &#x2014;
Dimensionality (hidden size) at each stage.`,name:"hidden_sizes"},{anchor:"transformers.VanConfig.depths",description:`<strong>depths</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[3, 3, 12, 3]</code>) &#x2014;
Depth (number of layers) for each stage.`,name:"depths"},{anchor:"transformers.VanConfig.mlp_ratios",description:`<strong>mlp_ratios</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[8, 8, 4, 4]</code>) &#x2014;
The expansion ratio for mlp layer at each stage.`,name:"mlp_ratios"},{anchor:"transformers.VanConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in each layer. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>,
<code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.VanConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.VanConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.VanConfig.layer_scale_init_value",description:`<strong>layer_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-2) &#x2014;
The initial value for layer scaling.`,name:"layer_scale_init_value"},{anchor:"transformers.VanConfig.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for stochastic depth.`,name:"drop_path_rate"},{anchor:"transformers.VanConfig.dropout_rate",description:`<strong>dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for dropout.`,name:"dropout_rate"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/van/configuration_van.py#L30"}}),R=new Ka({props:{anchor:"transformers.VanConfig.example",$$slots:{default:[Oo]},$$scope:{ctx:T}}}),ue=new st({}),ge=new nt({props:{name:"class transformers.VanModel",anchor:"transformers.VanModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.VanModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/van#transformers.VanConfig">VanConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/van/modeling_van.py#L426"}}),be=new nt({props:{name:"forward",anchor:"transformers.VanModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor]"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.VanModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/auto#transformers.AutoFeatureExtractor">AutoFeatureExtractor</a>. See
<code>AutoFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.VanModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all stages. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.VanModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/van/modeling_van.py#L436",returnDescription:`
<p>A <code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/van#transformers.VanConfig"
>VanConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state after a pooling operation on the spatial dimensions.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),K=new Fo({props:{$$slots:{default:[So]},$$scope:{ctx:T}}}),Z=new Ka({props:{anchor:"transformers.VanModel.forward.example",$$slots:{default:[Wo]},$$scope:{ctx:T}}}),we=new st({}),$e=new nt({props:{name:"class transformers.VanForImageClassification",anchor:"transformers.VanForImageClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.VanForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.24.0/en/model_doc/van#transformers.VanConfig">VanConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.24.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/van/modeling_van.py#L482"}}),Ae=new nt({props:{name:"forward",anchor:"transformers.VanForImageClassification.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.VanForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/v4.24.0/en/model_doc/auto#transformers.AutoFeatureExtractor">AutoFeatureExtractor</a>. See
<code>AutoFeatureExtractor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.VanForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all stages. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.VanForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.VanForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/van/modeling_van.py#L494",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.24.0/en/model_doc/van#transformers.VanConfig"
>VanConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also
called feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.24.0/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Q=new Fo({props:{$$slots:{default:[Ho]},$$scope:{ctx:T}}}),X=new Ka({props:{anchor:"transformers.VanForImageClassification.forward.example",$$slots:{default:[Uo]},$$scope:{ctx:T}}}),{c(){d=n("meta"),b=m(),_=n("h1"),f=n("a"),v=n("span"),w(o.$$.fragment),g=m(),N=n("span"),Mt=i("VAN"),rt=m(),P=n("h2"),W=n("a"),He=n("span"),w(oe.$$.fragment),Ft=m(),Ue=n("span"),Pt=i("Overview"),it=m(),H=n("p"),It=i("The VAN model was proposed in "),ne=n("a"),qt=i("Visual Attention Network"),zt=i(" by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu."),lt=m(),Ce=n("p"),Lt=i("This paper introduces a new attention layer based on convolution operations able to capture both local and distant relationships. This is done by combining normal and large kernel convolution layers. The latter uses a dilated convolution to capture distant correlations."),ct=m(),Te=n("p"),Dt=i("The abstract from the paper is the following:"),dt=m(),ke=n("p"),se=n("em"),Ot=i("While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel large kernel attention (LKA) module to enable self-adaptive and long-range correlations in self-attention while avoiding the above issues. We further introduce a novel neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN outperforms the state-of-the-art vision transformers and convolutional neural networks with a large margin in extensive experiments, including image classification, object detection, semantic segmentation, instance segmentation, etc. Code is available at "),re=n("a"),St=i("this https URL"),Wt=i("."),ht=m(),je=n("p"),Ht=i("Tips:"),pt=m(),Ne=n("ul"),ie=n("li"),Ut=i("VAN does not have an embedding layer, thus the "),Be=n("code"),Bt=i("hidden_states"),Rt=i(" will have a length equal to the number of stages."),ft=m(),U=n("p"),Gt=i("The figure below illustrates the architecture of a Visual Aattention Layer. Taken from the "),le=n("a"),Kt=i("original paper"),Zt=i("."),mt=m(),ce=n("img"),ut=m(),F=n("p"),Jt=i("This model was contributed by "),de=n("a"),Qt=i("Francesco"),Xt=i(". The original code can be found "),he=n("a"),Yt=i("here"),ea=i("."),gt=m(),I=n("h2"),B=n("a"),Re=n("span"),w(pe.$$.fragment),ta=m(),Ge=n("span"),aa=i("VanConfig"),_t=m(),E=n("div"),w(fe.$$.fragment),oa=m(),q=n("p"),na=i("This is the configuration class to store the configuration of a "),Me=n("a"),sa=i("VanModel"),ra=i(`. It is used to instantiate a VAN model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the VAN
`),me=n("a"),ia=i("Visual-Attention-Network/van-base"),la=i(" architecture."),ca=m(),z=n("p"),da=i("Configuration objects inherit from "),Fe=n("a"),ha=i("PretrainedConfig"),pa=i(` and can be used to control the model outputs. Read the
documentation from `),Pe=n("a"),fa=i("PretrainedConfig"),ma=i(" for more information."),ua=m(),w(R.$$.fragment),vt=m(),L=n("h2"),G=n("a"),Ke=n("span"),w(ue.$$.fragment),ga=m(),Ze=n("span"),_a=i("VanModel"),bt=m(),M=n("div"),w(ge.$$.fragment),va=m(),_e=n("p"),ba=i(`The bare VAN model outputting raw features without any specific head on top. Note, VAN does not have an embedding layer.
This model is a PyTorch `),ve=n("a"),wa=i("torch.nn.Module"),$a=i(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ya=m(),k=n("div"),w(be.$$.fragment),Va=m(),D=n("p"),Aa=i("The "),Ie=n("a"),xa=i("VanModel"),Ea=i(" forward method, overrides the "),Je=n("code"),Ca=i("__call__"),Ta=i(" special method."),ka=m(),w(K.$$.fragment),ja=m(),w(Z.$$.fragment),wt=m(),O=n("h2"),J=n("a"),Qe=n("span"),w(we.$$.fragment),Na=m(),Xe=n("span"),Ma=i("VanForImageClassification"),$t=m(),C=n("div"),w($e.$$.fragment),Fa=m(),Ye=n("p"),Pa=i(`VAN Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`),Ia=m(),ye=n("p"),qa=i("This model is a PyTorch "),Ve=n("a"),za=i("torch.nn.Module"),La=i(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Da=m(),j=n("div"),w(Ae.$$.fragment),Oa=m(),S=n("p"),Sa=i("The "),qe=n("a"),Wa=i("VanForImageClassification"),Ha=i(" forward method, overrides the "),et=n("code"),Ua=i("__call__"),Ba=i(" special method."),Ra=m(),w(Q.$$.fragment),Ga=m(),w(X.$$.fragment),this.h()},l(e){const h=zo('[data-svelte="svelte-1phssyn"]',document.head);d=s(h,"META",{name:!0,content:!0}),h.forEach(a),b=u(e),_=s(e,"H1",{class:!0});var xe=r(_);f=s(xe,"A",{id:!0,class:!0,href:!0});var tt=r(f);v=s(tt,"SPAN",{});var at=r(v);$(o.$$.fragment,at),at.forEach(a),tt.forEach(a),g=u(xe),N=s(xe,"SPAN",{});var ot=r(N);Mt=l(ot,"VAN"),ot.forEach(a),xe.forEach(a),rt=u(e),P=s(e,"H2",{class:!0});var Ee=r(P);W=s(Ee,"A",{id:!0,class:!0,href:!0});var Xa=r(W);He=s(Xa,"SPAN",{});var Ya=r(He);$(oe.$$.fragment,Ya),Ya.forEach(a),Xa.forEach(a),Ft=u(Ee),Ue=s(Ee,"SPAN",{});var eo=r(Ue);Pt=l(eo,"Overview"),eo.forEach(a),Ee.forEach(a),it=u(e),H=s(e,"P",{});var Vt=r(H);It=l(Vt,"The VAN model was proposed in "),ne=s(Vt,"A",{href:!0,rel:!0});var to=r(ne);qt=l(to,"Visual Attention Network"),to.forEach(a),zt=l(Vt," by Meng-Hao Guo, Cheng-Ze Lu, Zheng-Ning Liu, Ming-Ming Cheng, Shi-Min Hu."),Vt.forEach(a),lt=u(e),Ce=s(e,"P",{});var ao=r(Ce);Lt=l(ao,"This paper introduces a new attention layer based on convolution operations able to capture both local and distant relationships. This is done by combining normal and large kernel convolution layers. The latter uses a dilated convolution to capture distant correlations."),ao.forEach(a),ct=u(e),Te=s(e,"P",{});var oo=r(Te);Dt=l(oo,"The abstract from the paper is the following:"),oo.forEach(a),dt=u(e),ke=s(e,"P",{});var no=r(ke);se=s(no,"EM",{});var At=r(se);Ot=l(At,"While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel large kernel attention (LKA) module to enable self-adaptive and long-range correlations in self-attention while avoiding the above issues. We further introduce a novel neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN outperforms the state-of-the-art vision transformers and convolutional neural networks with a large margin in extensive experiments, including image classification, object detection, semantic segmentation, instance segmentation, etc. Code is available at "),re=s(At,"A",{href:!0,rel:!0});var so=r(re);St=l(so,"this https URL"),so.forEach(a),Wt=l(At,"."),At.forEach(a),no.forEach(a),ht=u(e),je=s(e,"P",{});var ro=r(je);Ht=l(ro,"Tips:"),ro.forEach(a),pt=u(e),Ne=s(e,"UL",{});var io=r(Ne);ie=s(io,"LI",{});var xt=r(ie);Ut=l(xt,"VAN does not have an embedding layer, thus the "),Be=s(xt,"CODE",{});var lo=r(Be);Bt=l(lo,"hidden_states"),lo.forEach(a),Rt=l(xt," will have a length equal to the number of stages."),xt.forEach(a),io.forEach(a),ft=u(e),U=s(e,"P",{});var Et=r(U);Gt=l(Et,"The figure below illustrates the architecture of a Visual Aattention Layer. Taken from the "),le=s(Et,"A",{href:!0,rel:!0});var co=r(le);Kt=l(co,"original paper"),co.forEach(a),Zt=l(Et,"."),Et.forEach(a),mt=u(e),ce=s(e,"IMG",{width:!0,src:!0}),ut=u(e),F=s(e,"P",{});var ze=r(F);Jt=l(ze,"This model was contributed by "),de=s(ze,"A",{href:!0,rel:!0});var ho=r(de);Qt=l(ho,"Francesco"),ho.forEach(a),Xt=l(ze,". The original code can be found "),he=s(ze,"A",{href:!0,rel:!0});var po=r(he);Yt=l(po,"here"),po.forEach(a),ea=l(ze,"."),ze.forEach(a),gt=u(e),I=s(e,"H2",{class:!0});var Ct=r(I);B=s(Ct,"A",{id:!0,class:!0,href:!0});var fo=r(B);Re=s(fo,"SPAN",{});var mo=r(Re);$(pe.$$.fragment,mo),mo.forEach(a),fo.forEach(a),ta=u(Ct),Ge=s(Ct,"SPAN",{});var uo=r(Ge);aa=l(uo,"VanConfig"),uo.forEach(a),Ct.forEach(a),_t=u(e),E=s(e,"DIV",{class:!0});var Y=r(E);$(fe.$$.fragment,Y),oa=u(Y),q=s(Y,"P",{});var Le=r(q);na=l(Le,"This is the configuration class to store the configuration of a "),Me=s(Le,"A",{href:!0});var go=r(Me);sa=l(go,"VanModel"),go.forEach(a),ra=l(Le,`. It is used to instantiate a VAN model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the VAN
`),me=s(Le,"A",{href:!0,rel:!0});var _o=r(me);ia=l(_o,"Visual-Attention-Network/van-base"),_o.forEach(a),la=l(Le," architecture."),Le.forEach(a),ca=u(Y),z=s(Y,"P",{});var De=r(z);da=l(De,"Configuration objects inherit from "),Fe=s(De,"A",{href:!0});var vo=r(Fe);ha=l(vo,"PretrainedConfig"),vo.forEach(a),pa=l(De,` and can be used to control the model outputs. Read the
documentation from `),Pe=s(De,"A",{href:!0});var bo=r(Pe);fa=l(bo,"PretrainedConfig"),bo.forEach(a),ma=l(De," for more information."),De.forEach(a),ua=u(Y),$(R.$$.fragment,Y),Y.forEach(a),vt=u(e),L=s(e,"H2",{class:!0});var Tt=r(L);G=s(Tt,"A",{id:!0,class:!0,href:!0});var wo=r(G);Ke=s(wo,"SPAN",{});var $o=r(Ke);$(ue.$$.fragment,$o),$o.forEach(a),wo.forEach(a),ga=u(Tt),Ze=s(Tt,"SPAN",{});var yo=r(Ze);_a=l(yo,"VanModel"),yo.forEach(a),Tt.forEach(a),bt=u(e),M=s(e,"DIV",{class:!0});var Oe=r(M);$(ge.$$.fragment,Oe),va=u(Oe),_e=s(Oe,"P",{});var kt=r(_e);ba=l(kt,`The bare VAN model outputting raw features without any specific head on top. Note, VAN does not have an embedding layer.
This model is a PyTorch `),ve=s(kt,"A",{href:!0,rel:!0});var Vo=r(ve);wa=l(Vo,"torch.nn.Module"),Vo.forEach(a),$a=l(kt,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),kt.forEach(a),ya=u(Oe),k=s(Oe,"DIV",{class:!0});var ee=r(k);$(be.$$.fragment,ee),Va=u(ee),D=s(ee,"P",{});var Se=r(D);Aa=l(Se,"The "),Ie=s(Se,"A",{href:!0});var Ao=r(Ie);xa=l(Ao,"VanModel"),Ao.forEach(a),Ea=l(Se," forward method, overrides the "),Je=s(Se,"CODE",{});var xo=r(Je);Ca=l(xo,"__call__"),xo.forEach(a),Ta=l(Se," special method."),Se.forEach(a),ka=u(ee),$(K.$$.fragment,ee),ja=u(ee),$(Z.$$.fragment,ee),ee.forEach(a),Oe.forEach(a),wt=u(e),O=s(e,"H2",{class:!0});var jt=r(O);J=s(jt,"A",{id:!0,class:!0,href:!0});var Eo=r(J);Qe=s(Eo,"SPAN",{});var Co=r(Qe);$(we.$$.fragment,Co),Co.forEach(a),Eo.forEach(a),Na=u(jt),Xe=s(jt,"SPAN",{});var To=r(Xe);Ma=l(To,"VanForImageClassification"),To.forEach(a),jt.forEach(a),$t=u(e),C=s(e,"DIV",{class:!0});var te=r(C);$($e.$$.fragment,te),Fa=u(te),Ye=s(te,"P",{});var ko=r(Ye);Pa=l(ko,`VAN Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`),ko.forEach(a),Ia=u(te),ye=s(te,"P",{});var Nt=r(ye);qa=l(Nt,"This model is a PyTorch "),Ve=s(Nt,"A",{href:!0,rel:!0});var jo=r(Ve);za=l(jo,"torch.nn.Module"),jo.forEach(a),La=l(Nt,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Nt.forEach(a),Da=u(te),j=s(te,"DIV",{class:!0});var ae=r(j);$(Ae.$$.fragment,ae),Oa=u(ae),S=s(ae,"P",{});var We=r(S);Sa=l(We,"The "),qe=s(We,"A",{href:!0});var No=r(qe);Wa=l(No,"VanForImageClassification"),No.forEach(a),Ha=l(We," forward method, overrides the "),et=s(We,"CODE",{});var Mo=r(et);Ua=l(Mo,"__call__"),Mo.forEach(a),Ba=l(We," special method."),We.forEach(a),Ra=u(ae),$(Q.$$.fragment,ae),Ga=u(ae),$(X.$$.fragment,ae),ae.forEach(a),te.forEach(a),this.h()},h(){c(d,"name","hf:doc:metadata"),c(d,"content",JSON.stringify(Ro)),c(f,"id","van"),c(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(f,"href","#van"),c(_,"class","relative group"),c(W,"id","overview"),c(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(W,"href","#overview"),c(P,"class","relative group"),c(ne,"href","https://arxiv.org/abs/2202.09741"),c(ne,"rel","nofollow"),c(re,"href","https://github.com/Visual-Attention-Network/VAN-Classification"),c(re,"rel","nofollow"),c(le,"href","https://arxiv.org/abs/2202.09741"),c(le,"rel","nofollow"),c(ce,"width","600"),Lo(ce.src,Qa="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/van_architecture.png")||c(ce,"src",Qa),c(de,"href","https://huggingface.co/Francesco"),c(de,"rel","nofollow"),c(he,"href","https://github.com/Visual-Attention-Network/VAN-Classification"),c(he,"rel","nofollow"),c(B,"id","transformers.VanConfig"),c(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(B,"href","#transformers.VanConfig"),c(I,"class","relative group"),c(Me,"href","/docs/transformers/v4.24.0/en/model_doc/van#transformers.VanModel"),c(me,"href","https://huggingface.co/Visual-Attention-Network/van-base"),c(me,"rel","nofollow"),c(Fe,"href","/docs/transformers/v4.24.0/en/main_classes/configuration#transformers.PretrainedConfig"),c(Pe,"href","/docs/transformers/v4.24.0/en/main_classes/configuration#transformers.PretrainedConfig"),c(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(G,"id","transformers.VanModel"),c(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(G,"href","#transformers.VanModel"),c(L,"class","relative group"),c(ve,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(ve,"rel","nofollow"),c(Ie,"href","/docs/transformers/v4.24.0/en/model_doc/van#transformers.VanModel"),c(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(J,"id","transformers.VanForImageClassification"),c(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J,"href","#transformers.VanForImageClassification"),c(O,"class","relative group"),c(Ve,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ve,"rel","nofollow"),c(qe,"href","/docs/transformers/v4.24.0/en/model_doc/van#transformers.VanForImageClassification"),c(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,h){t(document.head,d),p(e,b,h),p(e,_,h),t(_,f),t(f,v),y(o,v,null),t(_,g),t(_,N),t(N,Mt),p(e,rt,h),p(e,P,h),t(P,W),t(W,He),y(oe,He,null),t(P,Ft),t(P,Ue),t(Ue,Pt),p(e,it,h),p(e,H,h),t(H,It),t(H,ne),t(ne,qt),t(H,zt),p(e,lt,h),p(e,Ce,h),t(Ce,Lt),p(e,ct,h),p(e,Te,h),t(Te,Dt),p(e,dt,h),p(e,ke,h),t(ke,se),t(se,Ot),t(se,re),t(re,St),t(se,Wt),p(e,ht,h),p(e,je,h),t(je,Ht),p(e,pt,h),p(e,Ne,h),t(Ne,ie),t(ie,Ut),t(ie,Be),t(Be,Bt),t(ie,Rt),p(e,ft,h),p(e,U,h),t(U,Gt),t(U,le),t(le,Kt),t(U,Zt),p(e,mt,h),p(e,ce,h),p(e,ut,h),p(e,F,h),t(F,Jt),t(F,de),t(de,Qt),t(F,Xt),t(F,he),t(he,Yt),t(F,ea),p(e,gt,h),p(e,I,h),t(I,B),t(B,Re),y(pe,Re,null),t(I,ta),t(I,Ge),t(Ge,aa),p(e,_t,h),p(e,E,h),y(fe,E,null),t(E,oa),t(E,q),t(q,na),t(q,Me),t(Me,sa),t(q,ra),t(q,me),t(me,ia),t(q,la),t(E,ca),t(E,z),t(z,da),t(z,Fe),t(Fe,ha),t(z,pa),t(z,Pe),t(Pe,fa),t(z,ma),t(E,ua),y(R,E,null),p(e,vt,h),p(e,L,h),t(L,G),t(G,Ke),y(ue,Ke,null),t(L,ga),t(L,Ze),t(Ze,_a),p(e,bt,h),p(e,M,h),y(ge,M,null),t(M,va),t(M,_e),t(_e,ba),t(_e,ve),t(ve,wa),t(_e,$a),t(M,ya),t(M,k),y(be,k,null),t(k,Va),t(k,D),t(D,Aa),t(D,Ie),t(Ie,xa),t(D,Ea),t(D,Je),t(Je,Ca),t(D,Ta),t(k,ka),y(K,k,null),t(k,ja),y(Z,k,null),p(e,wt,h),p(e,O,h),t(O,J),t(J,Qe),y(we,Qe,null),t(O,Na),t(O,Xe),t(Xe,Ma),p(e,$t,h),p(e,C,h),y($e,C,null),t(C,Fa),t(C,Ye),t(Ye,Pa),t(C,Ia),t(C,ye),t(ye,qa),t(ye,Ve),t(Ve,za),t(ye,La),t(C,Da),t(C,j),y(Ae,j,null),t(j,Oa),t(j,S),t(S,Sa),t(S,qe),t(qe,Wa),t(S,Ha),t(S,et),t(et,Ua),t(S,Ba),t(j,Ra),y(Q,j,null),t(j,Ga),y(X,j,null),yt=!0},p(e,[h]){const xe={};h&2&&(xe.$$scope={dirty:h,ctx:e}),R.$set(xe);const tt={};h&2&&(tt.$$scope={dirty:h,ctx:e}),K.$set(tt);const at={};h&2&&(at.$$scope={dirty:h,ctx:e}),Z.$set(at);const ot={};h&2&&(ot.$$scope={dirty:h,ctx:e}),Q.$set(ot);const Ee={};h&2&&(Ee.$$scope={dirty:h,ctx:e}),X.$set(Ee)},i(e){yt||(V(o.$$.fragment,e),V(oe.$$.fragment,e),V(pe.$$.fragment,e),V(fe.$$.fragment,e),V(R.$$.fragment,e),V(ue.$$.fragment,e),V(ge.$$.fragment,e),V(be.$$.fragment,e),V(K.$$.fragment,e),V(Z.$$.fragment,e),V(we.$$.fragment,e),V($e.$$.fragment,e),V(Ae.$$.fragment,e),V(Q.$$.fragment,e),V(X.$$.fragment,e),yt=!0)},o(e){A(o.$$.fragment,e),A(oe.$$.fragment,e),A(pe.$$.fragment,e),A(fe.$$.fragment,e),A(R.$$.fragment,e),A(ue.$$.fragment,e),A(ge.$$.fragment,e),A(be.$$.fragment,e),A(K.$$.fragment,e),A(Z.$$.fragment,e),A(we.$$.fragment,e),A($e.$$.fragment,e),A(Ae.$$.fragment,e),A(Q.$$.fragment,e),A(X.$$.fragment,e),yt=!1},d(e){a(d),e&&a(b),e&&a(_),x(o),e&&a(rt),e&&a(P),x(oe),e&&a(it),e&&a(H),e&&a(lt),e&&a(Ce),e&&a(ct),e&&a(Te),e&&a(dt),e&&a(ke),e&&a(ht),e&&a(je),e&&a(pt),e&&a(Ne),e&&a(ft),e&&a(U),e&&a(mt),e&&a(ce),e&&a(ut),e&&a(F),e&&a(gt),e&&a(I),x(pe),e&&a(_t),e&&a(E),x(fe),x(R),e&&a(vt),e&&a(L),x(ue),e&&a(bt),e&&a(M),x(ge),x(be),x(K),x(Z),e&&a(wt),e&&a(O),x(we),e&&a($t),e&&a(C),x($e),x(Ae),x(Q),x(X)}}}const Ro={local:"van",sections:[{local:"overview",title:"Overview"},{local:"transformers.VanConfig",title:"VanConfig"},{local:"transformers.VanModel",title:"VanModel"},{local:"transformers.VanForImageClassification",title:"VanForImageClassification"}],title:"VAN"};function Go(T){return Do(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class en extends Po{constructor(d){super();Io(this,d,Go,Bo,qo,{})}}export{en as default,Ro as metadata};
