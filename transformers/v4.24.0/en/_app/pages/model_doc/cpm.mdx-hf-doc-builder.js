import{S as $s,i as ws,s as ys,e as s,k as p,w as v,t as r,M as Cs,c as a,d as n,m as c,a as o,x as b,h as i,b as m,G as e,g as h,y as $,q as w,o as y,B as C,v as Ls,L as vs}from"../../chunks/vendor-hf-doc-builder.js";import{D as W}from"../../chunks/Docstring-hf-doc-builder.js";import{C as bs}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as vt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as ks}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Es(ue){let u,P,_,f,L;return f=new bs({props:{code:`0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),{c(){u=s("p"),P=r("sequence pair mask has the following format:"),_=p(),v(f.$$.fragment)},l(l){u=a(l,"P",{});var g=o(u);P=i(g,"sequence pair mask has the following format:"),g.forEach(n),_=c(l),b(f.$$.fragment,l)},m(l,g){h(l,u,g),e(u,P),h(l,_,g),$(f,l,g),L=!0},p:vs,i(l){L||(w(f.$$.fragment,l),L=!0)},o(l){y(f.$$.fragment,l),L=!1},d(l){l&&n(u),l&&n(_),C(f,l)}}}function Ps(ue){let u,P,_,f,L;return f=new bs({props:{code:`0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),{c(){u=s("p"),P=r("sequence pair mask has the following format:"),_=p(),v(f.$$.fragment)},l(l){u=a(l,"P",{});var g=o(u);P=i(g,"sequence pair mask has the following format:"),g.forEach(n),_=c(l),b(f.$$.fragment,l)},m(l,g){h(l,u,g),e(u,P),h(l,_,g),$(f,l,g),L=!0},p:vs,i(l){L||(w(f.$$.fragment,l),L=!0)},o(l){y(f.$$.fragment,l),L=!1},d(l){l&&n(u),l&&n(_),C(f,l)}}}function qs(ue){let u,P,_,f,L,l,g,Pe,bt,Ke,I,O,qe,Q,$t,Te,wt,et,j,yt,U,Ct,Lt,tt,_e,Et,nt,ge,ze,Pt,st,N,qt,K,Tt,zt,ee,Dt,at,ke,xt,ot,M,S,De,te,It,xe,Nt,rt,k,ne,Mt,Ie,At,Gt,D,se,Ft,Ne,Ot,jt,ae,ve,St,Me,Ht,Xt,be,Zt,Ae,Bt,Jt,H,oe,Yt,Ge,Rt,Vt,q,re,Wt,Fe,Qt,Ut,X,Kt,A,en,Oe,tn,nn,je,sn,an,on,Z,ie,rn,le,ln,Se,pn,cn,it,G,B,He,pe,mn,Xe,dn,lt,E,ce,hn,Ze,fn,un,x,me,_n,Be,gn,kn,de,$e,vn,Je,bn,$n,we,wn,Ye,yn,Cn,T,he,Ln,Re,En,Pn,J,qn,F,Tn,Ve,zn,Dn,We,xn,In,pt;return l=new vt({}),Q=new vt({}),te=new vt({}),ne=new W({props:{name:"class transformers.CpmTokenizer",anchor:"transformers.CpmTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = False"},{name:"remove_space",val:" = True"},{name:"keep_accents",val:" = False"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"additional_special_tokens",val:" = ['<eop>', '<eod>']"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/cpm/tokenization_cpm.py#L38"}}),se=new W({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.CpmTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.CpmTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.CpmTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/cpm/tokenization_cpm.py#L242",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),oe=new W({props:{name:"convert_tokens_to_string",anchor:"transformers.CpmTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/cpm/tokenization_cpm.py#L236"}}),re=new W({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.CpmTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.CpmTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.CpmTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/cpm/tokenization_cpm.py#L297",returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),X=new ks({props:{anchor:"transformers.CpmTokenizer.create_token_type_ids_from_sequences.example",$$slots:{default:[Es]},$$scope:{ctx:ue}}}),ie=new W({props:{name:"get_special_tokens_mask",anchor:"transformers.CpmTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.CpmTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.CpmTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.CpmTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/cpm/tokenization_cpm.py#L268",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),pe=new vt({}),ce=new W({props:{name:"class transformers.CpmTokenizerFast",anchor:"transformers.CpmTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = False"},{name:"remove_space",val:" = True"},{name:"keep_accents",val:" = False"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"additional_special_tokens",val:" = ['<eop>', '<eod>']"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/cpm/tokenization_cpm_fast.py#L38"}}),me=new W({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.CpmTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.CpmTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.CpmTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/cpm/tokenization_cpm_fast.py#L157",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),he=new W({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.CpmTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.CpmTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.CpmTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.24.0/src/transformers/models/cpm/tokenization_cpm_fast.py#L183",returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),J=new ks({props:{anchor:"transformers.CpmTokenizerFast.create_token_type_ids_from_sequences.example",$$slots:{default:[Ps]},$$scope:{ctx:ue}}}),{c(){u=s("meta"),P=p(),_=s("h1"),f=s("a"),L=s("span"),v(l.$$.fragment),g=p(),Pe=s("span"),bt=r("CPM"),Ke=p(),I=s("h2"),O=s("a"),qe=s("span"),v(Q.$$.fragment),$t=p(),Te=s("span"),wt=r("Overview"),et=p(),j=s("p"),yt=r("The CPM model was proposed in "),U=s("a"),Ct=r("CPM: A Large-scale Generative Chinese Pre-trained Language Model"),Lt=r(` by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin,
Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen,
Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.`),tt=p(),_e=s("p"),Et=r("The abstract from the paper is the following:"),nt=p(),ge=s("p"),ze=s("em"),Pt=r(`Pre-trained Language Models (PLMs) have proven to be beneficial for various downstream NLP tasks. Recently, GPT-3,
with 175 billion parameters and 570GB training data, drew a lot of attention due to the capacity of few-shot (even
zero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is still challenging, as the training corpus
of GPT-3 is primarily English, and the parameters are not publicly available. In this technical report, we release the
Chinese Pre-trained Language Model (CPM) with generative pre-training on large-scale Chinese training data. To the best
of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained
language model, which could facilitate several downstream Chinese NLP tasks, such as conversation, essay generation,
cloze test, and language understanding. Extensive experiments demonstrate that CPM achieves strong performance on many
NLP tasks in the settings of few-shot (even zero-shot) learning.`),st=p(),N=s("p"),qt=r("This model was contributed by "),K=s("a"),Tt=r("canwenxu"),zt=r(`. The original implementation can be found
here: `),ee=s("a"),Dt=r("https://github.com/TsinghuaAI/CPM-Generate"),at=p(),ke=s("p"),xt=r("Note: We only have a tokenizer here, since the model architecture is the same as GPT-2."),ot=p(),M=s("h2"),S=s("a"),De=s("span"),v(te.$$.fragment),It=p(),xe=s("span"),Nt=r("CpmTokenizer"),rt=p(),k=s("div"),v(ne.$$.fragment),Mt=p(),Ie=s("p"),At=r("Runs pre-tokenization with Jieba segmentation tool. It is used in CPM models."),Gt=p(),D=s("div"),v(se.$$.fragment),Ft=p(),Ne=s("p"),Ot=r(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An XLNet sequence has the following format:`),jt=p(),ae=s("ul"),ve=s("li"),St=r("single sequence: "),Me=s("code"),Ht=r("X <sep> <cls>"),Xt=p(),be=s("li"),Zt=r("pair of sequences: "),Ae=s("code"),Bt=r("A <sep> B <sep> <cls>"),Jt=p(),H=s("div"),v(oe.$$.fragment),Yt=p(),Ge=s("p"),Rt=r("Converts a sequence of tokens (strings for sub-words) in a single string."),Vt=p(),q=s("div"),v(re.$$.fragment),Wt=p(),Fe=s("p"),Qt=r("Create a mask from the two sequences passed to be used in a sequence-pair classification task. An XLNet"),Ut=p(),v(X.$$.fragment),Kt=p(),A=s("p"),en=r("If "),Oe=s("code"),tn=r("token_ids_1"),nn=r(" is "),je=s("code"),sn=r("None"),an=r(", this method only returns the first portion of the mask (0s)."),on=p(),Z=s("div"),v(ie.$$.fragment),rn=p(),le=s("p"),ln=r(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Se=s("code"),pn=r("prepare_for_model"),cn=r(" method."),it=p(),G=s("h2"),B=s("a"),He=s("span"),v(pe.$$.fragment),mn=p(),Xe=s("span"),dn=r("CpmTokenizerFast"),lt=p(),E=s("div"),v(ce.$$.fragment),hn=p(),Ze=s("p"),fn=r("Runs pre-tokenization with Jieba segmentation tool. It is used in CPM models."),un=p(),x=s("div"),v(me.$$.fragment),_n=p(),Be=s("p"),gn=r(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An XLNet sequence has the following format:`),kn=p(),de=s("ul"),$e=s("li"),vn=r("single sequence: "),Je=s("code"),bn=r("X <sep> <cls>"),$n=p(),we=s("li"),wn=r("pair of sequences: "),Ye=s("code"),yn=r("A <sep> B <sep> <cls>"),Cn=p(),T=s("div"),v(he.$$.fragment),Ln=p(),Re=s("p"),En=r("Create a mask from the two sequences passed to be used in a sequence-pair classification task. An XLNet"),Pn=p(),v(J.$$.fragment),qn=p(),F=s("p"),Tn=r("If "),Ve=s("code"),zn=r("token_ids_1"),Dn=r(" is "),We=s("code"),xn=r("None"),In=r(", this method only returns the first portion of the mask (0s)."),this.h()},l(t){const d=Cs('[data-svelte="svelte-1phssyn"]',document.head);u=a(d,"META",{name:!0,content:!0}),d.forEach(n),P=c(t),_=a(t,"H1",{class:!0});var fe=o(_);f=a(fe,"A",{id:!0,class:!0,href:!0});var Qe=o(f);L=a(Qe,"SPAN",{});var Fn=o(L);b(l.$$.fragment,Fn),Fn.forEach(n),Qe.forEach(n),g=c(fe),Pe=a(fe,"SPAN",{});var On=o(Pe);bt=i(On,"CPM"),On.forEach(n),fe.forEach(n),Ke=c(t),I=a(t,"H2",{class:!0});var ct=o(I);O=a(ct,"A",{id:!0,class:!0,href:!0});var jn=o(O);qe=a(jn,"SPAN",{});var Sn=o(qe);b(Q.$$.fragment,Sn),Sn.forEach(n),jn.forEach(n),$t=c(ct),Te=a(ct,"SPAN",{});var Hn=o(Te);wt=i(Hn,"Overview"),Hn.forEach(n),ct.forEach(n),et=c(t),j=a(t,"P",{});var mt=o(j);yt=i(mt,"The CPM model was proposed in "),U=a(mt,"A",{href:!0,rel:!0});var Xn=o(U);Ct=i(Xn,"CPM: A Large-scale Generative Chinese Pre-trained Language Model"),Xn.forEach(n),Lt=i(mt,` by Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Yuxian Gu, Deming Ye, Yujia Qin,
Yusheng Su, Haozhe Ji, Jian Guan, Fanchao Qi, Xiaozhi Wang, Yanan Zheng, Guoyang Zeng, Huanqi Cao, Shengqi Chen,
Daixuan Li, Zhenbo Sun, Zhiyuan Liu, Minlie Huang, Wentao Han, Jie Tang, Juanzi Li, Xiaoyan Zhu, Maosong Sun.`),mt.forEach(n),tt=c(t),_e=a(t,"P",{});var Zn=o(_e);Et=i(Zn,"The abstract from the paper is the following:"),Zn.forEach(n),nt=c(t),ge=a(t,"P",{});var Bn=o(ge);ze=a(Bn,"EM",{});var Jn=o(ze);Pt=i(Jn,`Pre-trained Language Models (PLMs) have proven to be beneficial for various downstream NLP tasks. Recently, GPT-3,
with 175 billion parameters and 570GB training data, drew a lot of attention due to the capacity of few-shot (even
zero-shot) learning. However, applying GPT-3 to address Chinese NLP tasks is still challenging, as the training corpus
of GPT-3 is primarily English, and the parameters are not publicly available. In this technical report, we release the
Chinese Pre-trained Language Model (CPM) with generative pre-training on large-scale Chinese training data. To the best
of our knowledge, CPM, with 2.6 billion parameters and 100GB Chinese training data, is the largest Chinese pre-trained
language model, which could facilitate several downstream Chinese NLP tasks, such as conversation, essay generation,
cloze test, and language understanding. Extensive experiments demonstrate that CPM achieves strong performance on many
NLP tasks in the settings of few-shot (even zero-shot) learning.`),Jn.forEach(n),Bn.forEach(n),st=c(t),N=a(t,"P",{});var Ue=o(N);qt=i(Ue,"This model was contributed by "),K=a(Ue,"A",{href:!0,rel:!0});var Yn=o(K);Tt=i(Yn,"canwenxu"),Yn.forEach(n),zt=i(Ue,`. The original implementation can be found
here: `),ee=a(Ue,"A",{href:!0,rel:!0});var Rn=o(ee);Dt=i(Rn,"https://github.com/TsinghuaAI/CPM-Generate"),Rn.forEach(n),Ue.forEach(n),at=c(t),ke=a(t,"P",{});var Vn=o(ke);xt=i(Vn,"Note: We only have a tokenizer here, since the model architecture is the same as GPT-2."),Vn.forEach(n),ot=c(t),M=a(t,"H2",{class:!0});var dt=o(M);S=a(dt,"A",{id:!0,class:!0,href:!0});var Wn=o(S);De=a(Wn,"SPAN",{});var Qn=o(De);b(te.$$.fragment,Qn),Qn.forEach(n),Wn.forEach(n),It=c(dt),xe=a(dt,"SPAN",{});var Un=o(xe);Nt=i(Un,"CpmTokenizer"),Un.forEach(n),dt.forEach(n),rt=c(t),k=a(t,"DIV",{class:!0});var z=o(k);b(ne.$$.fragment,z),Mt=c(z),Ie=a(z,"P",{});var Kn=o(Ie);At=i(Kn,"Runs pre-tokenization with Jieba segmentation tool. It is used in CPM models."),Kn.forEach(n),Gt=c(z),D=a(z,"DIV",{class:!0});var ye=o(D);b(se.$$.fragment,ye),Ft=c(ye),Ne=a(ye,"P",{});var es=o(Ne);Ot=i(es,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An XLNet sequence has the following format:`),es.forEach(n),jt=c(ye),ae=a(ye,"UL",{});var ht=o(ae);ve=a(ht,"LI",{});var Nn=o(ve);St=i(Nn,"single sequence: "),Me=a(Nn,"CODE",{});var ts=o(Me);Ht=i(ts,"X <sep> <cls>"),ts.forEach(n),Nn.forEach(n),Xt=c(ht),be=a(ht,"LI",{});var Mn=o(be);Zt=i(Mn,"pair of sequences: "),Ae=a(Mn,"CODE",{});var ns=o(Ae);Bt=i(ns,"A <sep> B <sep> <cls>"),ns.forEach(n),Mn.forEach(n),ht.forEach(n),ye.forEach(n),Jt=c(z),H=a(z,"DIV",{class:!0});var ft=o(H);b(oe.$$.fragment,ft),Yt=c(ft),Ge=a(ft,"P",{});var ss=o(Ge);Rt=i(ss,"Converts a sequence of tokens (strings for sub-words) in a single string."),ss.forEach(n),ft.forEach(n),Vt=c(z),q=a(z,"DIV",{class:!0});var Y=o(q);b(re.$$.fragment,Y),Wt=c(Y),Fe=a(Y,"P",{});var as=o(Fe);Qt=i(as,"Create a mask from the two sequences passed to be used in a sequence-pair classification task. An XLNet"),as.forEach(n),Ut=c(Y),b(X.$$.fragment,Y),Kt=c(Y),A=a(Y,"P",{});var Ce=o(A);en=i(Ce,"If "),Oe=a(Ce,"CODE",{});var os=o(Oe);tn=i(os,"token_ids_1"),os.forEach(n),nn=i(Ce," is "),je=a(Ce,"CODE",{});var rs=o(je);sn=i(rs,"None"),rs.forEach(n),an=i(Ce,", this method only returns the first portion of the mask (0s)."),Ce.forEach(n),Y.forEach(n),on=c(z),Z=a(z,"DIV",{class:!0});var ut=o(Z);b(ie.$$.fragment,ut),rn=c(ut),le=a(ut,"P",{});var _t=o(le);ln=i(_t,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Se=a(_t,"CODE",{});var is=o(Se);pn=i(is,"prepare_for_model"),is.forEach(n),cn=i(_t," method."),_t.forEach(n),ut.forEach(n),z.forEach(n),it=c(t),G=a(t,"H2",{class:!0});var gt=o(G);B=a(gt,"A",{id:!0,class:!0,href:!0});var ls=o(B);He=a(ls,"SPAN",{});var ps=o(He);b(pe.$$.fragment,ps),ps.forEach(n),ls.forEach(n),mn=c(gt),Xe=a(gt,"SPAN",{});var cs=o(Xe);dn=i(cs,"CpmTokenizerFast"),cs.forEach(n),gt.forEach(n),lt=c(t),E=a(t,"DIV",{class:!0});var R=o(E);b(ce.$$.fragment,R),hn=c(R),Ze=a(R,"P",{});var ms=o(Ze);fn=i(ms,"Runs pre-tokenization with Jieba segmentation tool. It is used in CPM models."),ms.forEach(n),un=c(R),x=a(R,"DIV",{class:!0});var Le=o(x);b(me.$$.fragment,Le),_n=c(Le),Be=a(Le,"P",{});var ds=o(Be);gn=i(ds,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An XLNet sequence has the following format:`),ds.forEach(n),kn=c(Le),de=a(Le,"UL",{});var kt=o(de);$e=a(kt,"LI",{});var An=o($e);vn=i(An,"single sequence: "),Je=a(An,"CODE",{});var hs=o(Je);bn=i(hs,"X <sep> <cls>"),hs.forEach(n),An.forEach(n),$n=c(kt),we=a(kt,"LI",{});var Gn=o(we);wn=i(Gn,"pair of sequences: "),Ye=a(Gn,"CODE",{});var fs=o(Ye);yn=i(fs,"A <sep> B <sep> <cls>"),fs.forEach(n),Gn.forEach(n),kt.forEach(n),Le.forEach(n),Cn=c(R),T=a(R,"DIV",{class:!0});var V=o(T);b(he.$$.fragment,V),Ln=c(V),Re=a(V,"P",{});var us=o(Re);En=i(us,"Create a mask from the two sequences passed to be used in a sequence-pair classification task. An XLNet"),us.forEach(n),Pn=c(V),b(J.$$.fragment,V),qn=c(V),F=a(V,"P",{});var Ee=o(F);Tn=i(Ee,"If "),Ve=a(Ee,"CODE",{});var _s=o(Ve);zn=i(_s,"token_ids_1"),_s.forEach(n),Dn=i(Ee," is "),We=a(Ee,"CODE",{});var gs=o(We);xn=i(gs,"None"),gs.forEach(n),In=i(Ee,", this method only returns the first portion of the mask (0s)."),Ee.forEach(n),V.forEach(n),R.forEach(n),this.h()},h(){m(u,"name","hf:doc:metadata"),m(u,"content",JSON.stringify(Ts)),m(f,"id","cpm"),m(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(f,"href","#cpm"),m(_,"class","relative group"),m(O,"id","overview"),m(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(O,"href","#overview"),m(I,"class","relative group"),m(U,"href","https://arxiv.org/abs/2012.00413"),m(U,"rel","nofollow"),m(K,"href","https://huggingface.co/canwenxu"),m(K,"rel","nofollow"),m(ee,"href","https://github.com/TsinghuaAI/CPM-Generate"),m(ee,"rel","nofollow"),m(S,"id","transformers.CpmTokenizer"),m(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(S,"href","#transformers.CpmTokenizer"),m(M,"class","relative group"),m(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(B,"id","transformers.CpmTokenizerFast"),m(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(B,"href","#transformers.CpmTokenizerFast"),m(G,"class","relative group"),m(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),m(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,d){e(document.head,u),h(t,P,d),h(t,_,d),e(_,f),e(f,L),$(l,L,null),e(_,g),e(_,Pe),e(Pe,bt),h(t,Ke,d),h(t,I,d),e(I,O),e(O,qe),$(Q,qe,null),e(I,$t),e(I,Te),e(Te,wt),h(t,et,d),h(t,j,d),e(j,yt),e(j,U),e(U,Ct),e(j,Lt),h(t,tt,d),h(t,_e,d),e(_e,Et),h(t,nt,d),h(t,ge,d),e(ge,ze),e(ze,Pt),h(t,st,d),h(t,N,d),e(N,qt),e(N,K),e(K,Tt),e(N,zt),e(N,ee),e(ee,Dt),h(t,at,d),h(t,ke,d),e(ke,xt),h(t,ot,d),h(t,M,d),e(M,S),e(S,De),$(te,De,null),e(M,It),e(M,xe),e(xe,Nt),h(t,rt,d),h(t,k,d),$(ne,k,null),e(k,Mt),e(k,Ie),e(Ie,At),e(k,Gt),e(k,D),$(se,D,null),e(D,Ft),e(D,Ne),e(Ne,Ot),e(D,jt),e(D,ae),e(ae,ve),e(ve,St),e(ve,Me),e(Me,Ht),e(ae,Xt),e(ae,be),e(be,Zt),e(be,Ae),e(Ae,Bt),e(k,Jt),e(k,H),$(oe,H,null),e(H,Yt),e(H,Ge),e(Ge,Rt),e(k,Vt),e(k,q),$(re,q,null),e(q,Wt),e(q,Fe),e(Fe,Qt),e(q,Ut),$(X,q,null),e(q,Kt),e(q,A),e(A,en),e(A,Oe),e(Oe,tn),e(A,nn),e(A,je),e(je,sn),e(A,an),e(k,on),e(k,Z),$(ie,Z,null),e(Z,rn),e(Z,le),e(le,ln),e(le,Se),e(Se,pn),e(le,cn),h(t,it,d),h(t,G,d),e(G,B),e(B,He),$(pe,He,null),e(G,mn),e(G,Xe),e(Xe,dn),h(t,lt,d),h(t,E,d),$(ce,E,null),e(E,hn),e(E,Ze),e(Ze,fn),e(E,un),e(E,x),$(me,x,null),e(x,_n),e(x,Be),e(Be,gn),e(x,kn),e(x,de),e(de,$e),e($e,vn),e($e,Je),e(Je,bn),e(de,$n),e(de,we),e(we,wn),e(we,Ye),e(Ye,yn),e(E,Cn),e(E,T),$(he,T,null),e(T,Ln),e(T,Re),e(Re,En),e(T,Pn),$(J,T,null),e(T,qn),e(T,F),e(F,Tn),e(F,Ve),e(Ve,zn),e(F,Dn),e(F,We),e(We,xn),e(F,In),pt=!0},p(t,[d]){const fe={};d&2&&(fe.$$scope={dirty:d,ctx:t}),X.$set(fe);const Qe={};d&2&&(Qe.$$scope={dirty:d,ctx:t}),J.$set(Qe)},i(t){pt||(w(l.$$.fragment,t),w(Q.$$.fragment,t),w(te.$$.fragment,t),w(ne.$$.fragment,t),w(se.$$.fragment,t),w(oe.$$.fragment,t),w(re.$$.fragment,t),w(X.$$.fragment,t),w(ie.$$.fragment,t),w(pe.$$.fragment,t),w(ce.$$.fragment,t),w(me.$$.fragment,t),w(he.$$.fragment,t),w(J.$$.fragment,t),pt=!0)},o(t){y(l.$$.fragment,t),y(Q.$$.fragment,t),y(te.$$.fragment,t),y(ne.$$.fragment,t),y(se.$$.fragment,t),y(oe.$$.fragment,t),y(re.$$.fragment,t),y(X.$$.fragment,t),y(ie.$$.fragment,t),y(pe.$$.fragment,t),y(ce.$$.fragment,t),y(me.$$.fragment,t),y(he.$$.fragment,t),y(J.$$.fragment,t),pt=!1},d(t){n(u),t&&n(P),t&&n(_),C(l),t&&n(Ke),t&&n(I),C(Q),t&&n(et),t&&n(j),t&&n(tt),t&&n(_e),t&&n(nt),t&&n(ge),t&&n(st),t&&n(N),t&&n(at),t&&n(ke),t&&n(ot),t&&n(M),C(te),t&&n(rt),t&&n(k),C(ne),C(se),C(oe),C(re),C(X),C(ie),t&&n(it),t&&n(G),C(pe),t&&n(lt),t&&n(E),C(ce),C(me),C(he),C(J)}}}const Ts={local:"cpm",sections:[{local:"overview",title:"Overview"},{local:"transformers.CpmTokenizer",title:"CpmTokenizer"},{local:"transformers.CpmTokenizerFast",title:"CpmTokenizerFast"}],title:"CPM"};function zs(ue){return Ls(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class As extends $s{constructor(u){super();ws(this,u,zs,qs,ys,{})}}export{As as default,Ts as metadata};
