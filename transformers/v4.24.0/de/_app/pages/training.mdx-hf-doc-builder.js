import{S as Gt,i as Vt,s as Rt,e as r,k as m,w as $,t as p,M as Ut,c as i,d as e,m as f,a as l,x as k,h as u,b as h,G as n,g as d,y as _,q as z,o as S,B as E,v as Zt}from"../chunks/vendor-hf-doc-builder.js";import{T as It}from"../chunks/Tip-hf-doc-builder.js";import{Y as Ft}from"../chunks/Youtube-hf-doc-builder.js";import{I as Gn}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as G}from"../chunks/CodeBlock-hf-doc-builder.js";import{D as Yt}from"../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as Ht,M as xt}from"../chunks/Markdown-hf-doc-builder.js";function Jt(fn){let g,T;return{c(){g=r("p"),T=p(`Es wird eine Warnung angezeigt, dass einige der trainierten Parameter nicht verwendet werden und einige Parameter zuf\xE4llig
initialisiert werden. Machen Sie sich keine Sorgen, das ist v\xF6llig normal! Der vorher trainierte Kopf des BERT-Modells wird verworfen und durch einen zuf\xE4llig initialisierten Klassifikationskopf ersetzt. Sie werden diesen neuen Modellkopf in Ihrer Sequenzklassifizierungsaufgabe feinabstimmen, indem Sie das Wissen des vortrainierten Modells auf ihn \xFCbertragen.`)},l(c){g=i(c,"P",{});var j=l(g);T=u(j,`Es wird eine Warnung angezeigt, dass einige der trainierten Parameter nicht verwendet werden und einige Parameter zuf\xE4llig
initialisiert werden. Machen Sie sich keine Sorgen, das ist v\xF6llig normal! Der vorher trainierte Kopf des BERT-Modells wird verworfen und durch einen zuf\xE4llig initialisierten Klassifikationskopf ersetzt. Sie werden diesen neuen Modellkopf in Ihrer Sequenzklassifizierungsaufgabe feinabstimmen, indem Sie das Wissen des vortrainierten Modells auf ihn \xFCbertragen.`),j.forEach(e)},m(c,j){d(c,g,j),n(g,T)},d(c){c&&e(g)}}}function Xt(fn){let g,T,c,j,y,A,x,P,He,C,M,Ge,O,Vn,hn,q,ve,Sn,N,be,cn,gn,En,Ce,On,Q,De,Ve,Pe,ee,ne,V,R,Qe,en,Re,jn,F,nn,te,Ne,tn,W,de,Ln,K,sn,qn,Fn,pe,se,U,Z,Be,we,Wn,Oe,Me,H,L,Le,Ue,vn,ue,$e,me,an,Y,We,In,fe,J,rn,Ze,ln,Tn,Rn,X,ke,bn,_e,on,ae,B,xn,qe,Ke,yn,Fe,dn,Ie,Ye,An,Dn,I,re,D,he,pn,wn,Kn,ze,Se,Je,ce,ie,Ee,je,Te,un,Pn,ye,Mn,ge,$n,Xe,Cn,le,a,t,v,Ae,Un,Zn,at,oe,Hn,kn,rt;return g=new Ft({props:{id:"nvBXf7s7vTI"}}),A=new Gn({}),Q=new G({props:{code:`from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, num_labels=<span class="hljs-number">5</span>)`}}),Ve=new It({props:{$$slots:{default:[Jt]},$$scope:{ctx:fn}}}),R=new Gn({}),pe=new G({props:{code:`from transformers import TrainingArguments

training_args = TrainingArguments(output_dir="test_trainer")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments

<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(output_dir=<span class="hljs-string">&quot;test_trainer&quot;</span>)`}}),we=new Gn({}),on=new G({props:{code:`import numpy as np
import evaluate

metric = evaluate.load("accuracy")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate

<span class="hljs-meta">&gt;&gt;&gt; </span>metric = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)`}}),re=new G({props:{code:`def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
<span class="hljs-meta">... </span>    logits, labels = eval_pred
<span class="hljs-meta">... </span>    predictions = np.argmax(logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)`}}),Je=new G({props:{code:`from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(output_dir="test_trainer", evaluation_strategy="epoch")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(output_dir=<span class="hljs-string">&quot;test_trainer&quot;</span>, evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>)`}}),Te=new Gn({}),t=new G({props:{code:`trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=small_train_dataset,
<span class="hljs-meta">... </span>    eval_dataset=small_eval_dataset,
<span class="hljs-meta">... </span>    compute_metrics=compute_metrics,
<span class="hljs-meta">... </span>)`}}),kn=new G({props:{code:"trainer.train()",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()'}}),{c(){$(g.$$.fragment),T=m(),c=r("h2"),j=r("a"),y=r("span"),$(A.$$.fragment),x=m(),P=r("span"),He=p("Trainieren mit PyTorch Trainer"),C=m(),M=r("p"),Ge=p("\u{1F917} Transformers bietet eine "),O=r("code"),Vn=p("Trainer"),hn=p("-Klasse, die f\xFCr das Training von \u{1F917} Transformers-Modellen optimiert ist und es einfacher macht, mit dem Training zu beginnen, ohne manuell eine eigene Trainingsschleife zu schreiben. Die "),q=r("code"),ve=p("Trainer"),Sn=p("-API unterst\xFCtzt eine breite Palette von Trainingsoptionen und Funktionen wie Logging, Gradientenakkumulation und gemischte Pr\xE4zision."),N=m(),be=r("p"),cn=p("Beginnen Sie mit dem Laden Ihres Modells und geben Sie die Anzahl der erwarteten Labels an. Aus dem Yelp Review "),gn=r("a"),En=p("dataset card"),Ce=p(" wissen Sie, dass es f\xFCnf Labels gibt:"),On=m(),$(Q.$$.fragment),De=m(),$(Ve.$$.fragment),Pe=m(),ee=r("h3"),ne=r("a"),V=r("span"),$(R.$$.fragment),Qe=m(),en=r("span"),Re=p("Hyperparameter f\xFCr das Training"),jn=m(),F=r("p"),nn=p("Als N\xE4chstes erstellen Sie eine Klasse "),te=r("code"),Ne=p("TrainingArguments"),tn=p(", die alle Hyperparameter enth\xE4lt, die Sie einstellen k\xF6nnen, sowie Flags zur Aktivierung verschiedener Trainingsoptionen. F\xFCr dieses Lernprogramm k\xF6nnen Sie mit den Standard- "),W=r("a"),de=p("Hyperparametern"),Ln=p(" beginnen, aber Sie k\xF6nnen mit diesen experimentieren, um Ihre optimalen Einstellungen zu finden."),K=m(),sn=r("p"),qn=p("Geben Sie an, wo die Kontrollpunkte Ihres Trainings gespeichert werden sollen:"),Fn=m(),$(pe.$$.fragment),se=m(),U=r("h3"),Z=r("a"),Be=r("span"),$(we.$$.fragment),Wn=m(),Oe=r("span"),Me=p("Auswerten"),H=m(),L=r("p"),Le=p("Der "),Ue=r("code"),vn=p("Trainer"),ue=p(" wertet die Leistung des Modells w\xE4hrend des Trainings nicht automatisch aus. Sie m\xFCssen "),$e=r("code"),me=p("Trainer"),an=p(" eine Funktion \xFCbergeben, um Metriken zu berechnen und zu berichten. Die "),Y=r("a"),We=p("\u{1F917} Evaluate"),In=p(" Bibliothek bietet eine einfache "),fe=r("a"),J=r("code"),rn=p("accuracy"),Ze=p(" Funktion, die Sie mit der "),ln=r("code"),Tn=p("evaluate.load"),Rn=p(" Funktion laden k\xF6nnen (siehe diese "),X=r("a"),ke=p("quicktour"),bn=p(" f\xFCr weitere Informationen):"),_e=m(),$(on.$$.fragment),ae=m(),B=r("p"),xn=p("Rufen Sie "),qe=r("code"),Ke=p("compute"),yn=p(" auf "),Fe=r("code"),dn=p("metric"),Ie=p(" auf, um die Genauigkeit Ihrer Vorhersagen zu berechnen. Bevor Sie Ihre Vorhersagen an "),Ye=r("code"),An=p("compute"),Dn=p(" \xFCbergeben, m\xFCssen Sie die Vorhersagen in Logits umwandeln (denken Sie daran, dass alle \u{1F917} Transformers-Modelle Logits zur\xFCckgeben):"),I=m(),$(re.$$.fragment),D=m(),he=r("p"),pn=p("Wenn Sie Ihre Bewertungsmetriken w\xE4hrend der Feinabstimmung \xFCberwachen m\xF6chten, geben Sie den Parameter "),wn=r("code"),Kn=p("evaluation_strategy"),ze=p(" in Ihren Trainingsargumenten an, um die Bewertungsmetrik am Ende jeder Epoche zu ermitteln:"),Se=m(),$(Je.$$.fragment),ce=m(),ie=r("h3"),Ee=r("a"),je=r("span"),$(Te.$$.fragment),un=m(),Pn=r("span"),ye=p("Trainer"),Mn=m(),ge=r("p"),$n=p("Erstellen Sie ein "),Xe=r("code"),Cn=p("Trainer"),le=p("-Objekt mit Ihrem Modell, Trainingsargumenten, Trainings- und Testdatens\xE4tzen und einer Evaluierungsfunktion:"),a=m(),$(t.$$.fragment),v=m(),Ae=r("p"),Un=p("Anschlie\xDFend k\xF6nnen Sie Ihr Modell durch den Aufruf von "),Zn=r("code"),at=p("train()"),oe=p(" optimieren:"),Hn=m(),$(kn.$$.fragment),this.h()},l(o){k(g.$$.fragment,o),T=f(o),c=i(o,"H2",{class:!0});var w=l(c);j=i(w,"A",{id:!0,class:!0,href:!0});var Yn=l(j);y=i(Yn,"SPAN",{});var ht=l(y);k(A.$$.fragment,ht),ht.forEach(e),Yn.forEach(e),x=f(w),P=i(w,"SPAN",{});var Nn=l(P);He=u(Nn,"Trainieren mit PyTorch Trainer"),Nn.forEach(e),w.forEach(e),C=f(o),M=i(o,"P",{});var mn=l(M);Ge=u(mn,"\u{1F917} Transformers bietet eine "),O=i(mn,"CODE",{});var Jn=l(O);Vn=u(Jn,"Trainer"),Jn.forEach(e),hn=u(mn,"-Klasse, die f\xFCr das Training von \u{1F917} Transformers-Modellen optimiert ist und es einfacher macht, mit dem Training zu beginnen, ohne manuell eine eigene Trainingsschleife zu schreiben. Die "),q=i(mn,"CODE",{});var s=l(q);ve=u(s,"Trainer"),s.forEach(e),Sn=u(mn,"-API unterst\xFCtzt eine breite Palette von Trainingsoptionen und Funktionen wie Logging, Gradientenakkumulation und gemischte Pr\xE4zision."),mn.forEach(e),N=f(o),be=i(o,"P",{});var b=l(be);cn=u(b,"Beginnen Sie mit dem Laden Ihres Modells und geben Sie die Anzahl der erwarteten Labels an. Aus dem Yelp Review "),gn=i(b,"A",{href:!0,rel:!0});var it=l(gn);En=u(it,"dataset card"),it.forEach(e),Ce=u(b," wissen Sie, dass es f\xFCnf Labels gibt:"),b.forEach(e),On=f(o),k(Q.$$.fragment,o),De=f(o),k(Ve.$$.fragment,o),Pe=f(o),ee=i(o,"H3",{class:!0});var Xn=l(ee);ne=i(Xn,"A",{id:!0,class:!0,href:!0});var dt=l(ne);V=i(dt,"SPAN",{});var pt=l(V);k(R.$$.fragment,pt),pt.forEach(e),dt.forEach(e),Qe=f(Xn),en=i(Xn,"SPAN",{});var Qn=l(en);Re=u(Qn,"Hyperparameter f\xFCr das Training"),Qn.forEach(e),Xn.forEach(e),jn=f(o),F=i(o,"P",{});var _n=l(F);nn=u(_n,"Als N\xE4chstes erstellen Sie eine Klasse "),te=i(_n,"CODE",{});var ct=l(te);Ne=u(ct,"TrainingArguments"),ct.forEach(e),tn=u(_n,", die alle Hyperparameter enth\xE4lt, die Sie einstellen k\xF6nnen, sowie Flags zur Aktivierung verschiedener Trainingsoptionen. F\xFCr dieses Lernprogramm k\xF6nnen Sie mit den Standard- "),W=i(_n,"A",{href:!0,rel:!0});var et=l(W);de=u(et,"Hyperparametern"),et.forEach(e),Ln=u(_n," beginnen, aber Sie k\xF6nnen mit diesen experimentieren, um Ihre optimalen Einstellungen zu finden."),_n.forEach(e),K=f(o),sn=i(o,"P",{});var lt=l(sn);qn=u(lt,"Geben Sie an, wo die Kontrollpunkte Ihres Trainings gespeichert werden sollen:"),lt.forEach(e),Fn=f(o),k(pe.$$.fragment,o),se=f(o),U=i(o,"H3",{class:!0});var nt=l(U);Z=i(nt,"A",{id:!0,class:!0,href:!0});var tt=l(Z);Be=i(tt,"SPAN",{});var gt=l(Be);k(we.$$.fragment,gt),gt.forEach(e),tt.forEach(e),Wn=f(nt),Oe=i(nt,"SPAN",{});var st=l(Oe);Me=u(st,"Auswerten"),st.forEach(e),nt.forEach(e),H=f(o),L=i(o,"P",{});var xe=l(L);Le=u(xe,"Der "),Ue=i(xe,"CODE",{});var vt=l(Ue);vn=u(vt,"Trainer"),vt.forEach(e),ue=u(xe," wertet die Leistung des Modells w\xE4hrend des Trainings nicht automatisch aus. Sie m\xFCssen "),$e=i(xe,"CODE",{});var zn=l($e);me=u(zn,"Trainer"),zn.forEach(e),an=u(xe," eine Funktion \xFCbergeben, um Metriken zu berechnen und zu berichten. Die "),Y=i(xe,"A",{href:!0,rel:!0});var ut=l(Y);We=u(ut,"\u{1F917} Evaluate"),ut.forEach(e),In=u(xe," Bibliothek bietet eine einfache "),fe=i(xe,"A",{href:!0,rel:!0});var mt=l(fe);J=i(mt,"CODE",{});var bt=l(J);rn=u(bt,"accuracy"),bt.forEach(e),mt.forEach(e),Ze=u(xe," Funktion, die Sie mit der "),ln=i(xe,"CODE",{});var ot=l(ln);Tn=u(ot,"evaluate.load"),ot.forEach(e),Rn=u(xe," Funktion laden k\xF6nnen (siehe diese "),X=i(xe,"A",{href:!0,rel:!0});var ft=l(X);ke=u(ft,"quicktour"),ft.forEach(e),bn=u(xe," f\xFCr weitere Informationen):"),xe.forEach(e),_e=f(o),k(on.$$.fragment,o),ae=f(o),B=i(o,"P",{});var Bn=l(B);xn=u(Bn,"Rufen Sie "),qe=i(Bn,"CODE",{});var $t=l(qe);Ke=u($t,"compute"),$t.forEach(e),yn=u(Bn," auf "),Fe=i(Bn,"CODE",{});var wt=l(Fe);dn=u(wt,"metric"),wt.forEach(e),Ie=u(Bn," auf, um die Genauigkeit Ihrer Vorhersagen zu berechnen. Bevor Sie Ihre Vorhersagen an "),Ye=i(Bn,"CODE",{});var yt=l(Ye);An=u(yt,"compute"),yt.forEach(e),Dn=u(Bn," \xFCbergeben, m\xFCssen Sie die Vorhersagen in Logits umwandeln (denken Sie daran, dass alle \u{1F917} Transformers-Modelle Logits zur\xFCckgeben):"),Bn.forEach(e),I=f(o),k(re.$$.fragment,o),D=f(o),he=i(o,"P",{});var _t=l(he);pn=u(_t,"Wenn Sie Ihre Bewertungsmetriken w\xE4hrend der Feinabstimmung \xFCberwachen m\xF6chten, geben Sie den Parameter "),wn=i(_t,"CODE",{});var zt=l(wn);Kn=u(zt,"evaluation_strategy"),zt.forEach(e),ze=u(_t," in Ihren Trainingsargumenten an, um die Bewertungsmetrik am Ende jeder Epoche zu ermitteln:"),_t.forEach(e),Se=f(o),k(Je.$$.fragment,o),ce=f(o),ie=i(o,"H3",{class:!0});var St=l(ie);Ee=i(St,"A",{id:!0,class:!0,href:!0});var Et=l(Ee);je=i(Et,"SPAN",{});var At=l(je);k(Te.$$.fragment,At),At.forEach(e),Et.forEach(e),un=f(St),Pn=i(St,"SPAN",{});var Dt=l(Pn);ye=u(Dt,"Trainer"),Dt.forEach(e),St.forEach(e),Mn=f(o),ge=i(o,"P",{});var kt=l(ge);$n=u(kt,"Erstellen Sie ein "),Xe=i(kt,"CODE",{});var Pt=l(Xe);Cn=u(Pt,"Trainer"),Pt.forEach(e),le=u(kt,"-Objekt mit Ihrem Modell, Trainingsargumenten, Trainings- und Testdatens\xE4tzen und einer Evaluierungsfunktion:"),kt.forEach(e),a=f(o),k(t.$$.fragment,o),v=f(o),Ae=i(o,"P",{});var jt=l(Ae);Un=u(jt,"Anschlie\xDFend k\xF6nnen Sie Ihr Modell durch den Aufruf von "),Zn=i(jt,"CODE",{});var Mt=l(Zn);at=u(Mt,"train()"),Mt.forEach(e),oe=u(jt," optimieren:"),jt.forEach(e),Hn=f(o),k(kn.$$.fragment,o),this.h()},h(){h(j,"id","trainieren-mit-pytorch-trainer"),h(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(j,"href","#trainieren-mit-pytorch-trainer"),h(c,"class","relative group"),h(gn,"href","https://huggingface.co/datasets/yelp_review_full#data-fields"),h(gn,"rel","nofollow"),h(ne,"id","hyperparameter-fr-das-training"),h(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ne,"href","#hyperparameter-fr-das-training"),h(ee,"class","relative group"),h(W,"href","https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments"),h(W,"rel","nofollow"),h(Z,"id","auswerten"),h(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Z,"href","#auswerten"),h(U,"class","relative group"),h(Y,"href","https://huggingface.co/docs/evaluate/index"),h(Y,"rel","nofollow"),h(fe,"href","https://huggingface.co/spaces/evaluate-metric/accuracy"),h(fe,"rel","nofollow"),h(X,"href","https://huggingface.co/docs/evaluate/a_quick_tour"),h(X,"rel","nofollow"),h(Ee,"id","trainer"),h(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ee,"href","#trainer"),h(ie,"class","relative group")},m(o,w){_(g,o,w),d(o,T,w),d(o,c,w),n(c,j),n(j,y),_(A,y,null),n(c,x),n(c,P),n(P,He),d(o,C,w),d(o,M,w),n(M,Ge),n(M,O),n(O,Vn),n(M,hn),n(M,q),n(q,ve),n(M,Sn),d(o,N,w),d(o,be,w),n(be,cn),n(be,gn),n(gn,En),n(be,Ce),d(o,On,w),_(Q,o,w),d(o,De,w),_(Ve,o,w),d(o,Pe,w),d(o,ee,w),n(ee,ne),n(ne,V),_(R,V,null),n(ee,Qe),n(ee,en),n(en,Re),d(o,jn,w),d(o,F,w),n(F,nn),n(F,te),n(te,Ne),n(F,tn),n(F,W),n(W,de),n(F,Ln),d(o,K,w),d(o,sn,w),n(sn,qn),d(o,Fn,w),_(pe,o,w),d(o,se,w),d(o,U,w),n(U,Z),n(Z,Be),_(we,Be,null),n(U,Wn),n(U,Oe),n(Oe,Me),d(o,H,w),d(o,L,w),n(L,Le),n(L,Ue),n(Ue,vn),n(L,ue),n(L,$e),n($e,me),n(L,an),n(L,Y),n(Y,We),n(L,In),n(L,fe),n(fe,J),n(J,rn),n(L,Ze),n(L,ln),n(ln,Tn),n(L,Rn),n(L,X),n(X,ke),n(L,bn),d(o,_e,w),_(on,o,w),d(o,ae,w),d(o,B,w),n(B,xn),n(B,qe),n(qe,Ke),n(B,yn),n(B,Fe),n(Fe,dn),n(B,Ie),n(B,Ye),n(Ye,An),n(B,Dn),d(o,I,w),_(re,o,w),d(o,D,w),d(o,he,w),n(he,pn),n(he,wn),n(wn,Kn),n(he,ze),d(o,Se,w),_(Je,o,w),d(o,ce,w),d(o,ie,w),n(ie,Ee),n(Ee,je),_(Te,je,null),n(ie,un),n(ie,Pn),n(Pn,ye),d(o,Mn,w),d(o,ge,w),n(ge,$n),n(ge,Xe),n(Xe,Cn),n(ge,le),d(o,a,w),_(t,o,w),d(o,v,w),d(o,Ae,w),n(Ae,Un),n(Ae,Zn),n(Zn,at),n(Ae,oe),d(o,Hn,w),_(kn,o,w),rt=!0},p(o,w){const Yn={};w&2&&(Yn.$$scope={dirty:w,ctx:o}),Ve.$set(Yn)},i(o){rt||(z(g.$$.fragment,o),z(A.$$.fragment,o),z(Q.$$.fragment,o),z(Ve.$$.fragment,o),z(R.$$.fragment,o),z(pe.$$.fragment,o),z(we.$$.fragment,o),z(on.$$.fragment,o),z(re.$$.fragment,o),z(Je.$$.fragment,o),z(Te.$$.fragment,o),z(t.$$.fragment,o),z(kn.$$.fragment,o),rt=!0)},o(o){S(g.$$.fragment,o),S(A.$$.fragment,o),S(Q.$$.fragment,o),S(Ve.$$.fragment,o),S(R.$$.fragment,o),S(pe.$$.fragment,o),S(we.$$.fragment,o),S(on.$$.fragment,o),S(re.$$.fragment,o),S(Je.$$.fragment,o),S(Te.$$.fragment,o),S(t.$$.fragment,o),S(kn.$$.fragment,o),rt=!1},d(o){E(g,o),o&&e(T),o&&e(c),E(A),o&&e(C),o&&e(M),o&&e(N),o&&e(be),o&&e(On),E(Q,o),o&&e(De),E(Ve,o),o&&e(Pe),o&&e(ee),E(R),o&&e(jn),o&&e(F),o&&e(K),o&&e(sn),o&&e(Fn),E(pe,o),o&&e(se),o&&e(U),E(we),o&&e(H),o&&e(L),o&&e(_e),E(on,o),o&&e(ae),o&&e(B),o&&e(I),E(re,o),o&&e(D),o&&e(he),o&&e(Se),E(Je,o),o&&e(ce),o&&e(ie),E(Te),o&&e(Mn),o&&e(ge),o&&e(a),E(t,o),o&&e(v),o&&e(Ae),o&&e(Hn),E(kn,o)}}}function Qt(fn){let g,T;return g=new xt({props:{$$slots:{default:[Xt]},$$scope:{ctx:fn}}}),{c(){$(g.$$.fragment)},l(c){k(g.$$.fragment,c)},m(c,j){_(g,c,j),T=!0},p(c,j){const y={};j&2&&(y.$$scope={dirty:j,ctx:c}),g.$set(y)},i(c){T||(z(g.$$.fragment,c),T=!0)},o(c){S(g.$$.fragment,c),T=!1},d(c){E(g,c)}}}function es(fn){let g,T,c,j,y;return{c(){g=r("p"),T=p("Sie m\xFCssen Ihren Modellen kein Verlustargument \xFCbergeben, wenn Sie sie "),c=r("code"),j=p("compile()"),y=p(`! Hugging-Face-Modelle w\xE4hlen automatisch
einen Loss, der f\xFCr ihre Aufgabe und Modellarchitektur geeignet ist, wenn dieses Argument leer gelassen wird. Sie k\xF6nnen jederzeit au\xDFer Kraft setzen, indem Sie selbst einen Loss angeben, wenn Sie das m\xF6chten!`)},l(A){g=i(A,"P",{});var x=l(g);T=u(x,"Sie m\xFCssen Ihren Modellen kein Verlustargument \xFCbergeben, wenn Sie sie "),c=i(x,"CODE",{});var P=l(c);j=u(P,"compile()"),P.forEach(e),y=u(x,`! Hugging-Face-Modelle w\xE4hlen automatisch
einen Loss, der f\xFCr ihre Aufgabe und Modellarchitektur geeignet ist, wenn dieses Argument leer gelassen wird. Sie k\xF6nnen jederzeit au\xDFer Kraft setzen, indem Sie selbst einen Loss angeben, wenn Sie das m\xF6chten!`),x.forEach(e)},m(A,x){d(A,g,x),n(g,T),n(g,c),n(c,j),n(g,y)},d(A){A&&e(g)}}}function ns(fn){let g,T,c,j,y,A,x,P,He,C,M,Ge,O,Vn,hn,q,ve,Sn,N,be,cn,gn,En,Ce,On,Q,De,Ve,Pe,ee,ne,V,R,Qe,en,Re,jn,F,nn,te,Ne,tn,W,de,Ln,K,sn,qn,Fn,pe,se,U,Z,Be,we,Wn,Oe,Me,H,L,Le,Ue,vn,ue,$e,me,an,Y,We,In,fe,J,rn,Ze,ln,Tn,Rn,X,ke,bn,_e,on,ae,B,xn,qe,Ke,yn,Fe,dn,Ie,Ye,An,Dn,I,re,D,he,pn,wn,Kn,ze,Se,Je,ce,ie,Ee,je,Te,un,Pn,ye,Mn,ge,$n,Xe,Cn,le,a;return c=new Ft({props:{id:"rnTGBy2ax1c"}}),P=new Gn({}),N=new Gn({}),R=new G({props:{code:`from datasets import load_dataset

dataset = load_dataset("glue", "cola")
dataset = dataset["train"]  # Just take the training split for now`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

dataset = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;cola&quot;</span>)
dataset = dataset[<span class="hljs-string">&quot;train&quot;</span>]  <span class="hljs-comment"># Just take the training split for now</span>`}}),F=new G({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
tokenized_data = tokenizer(dataset["text"], return_tensors="np", padding=True)

labels = np.array(dataset["label"])  # Label is already an array of 0 and 1`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
tokenized_data = tokenizer(dataset[<span class="hljs-string">&quot;text&quot;</span>], return_tensors=<span class="hljs-string">&quot;np&quot;</span>, padding=<span class="hljs-literal">True</span>)

labels = np.array(dataset[<span class="hljs-string">&quot;label&quot;</span>])  <span class="hljs-comment"># Label is already an array of 0 and 1</span>`}}),se=new G({props:{code:`from transformers import TFAutoModelForSequenceClassification
from tensorflow.keras.optimizers import Adam

# Load and compile our model
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")
# Lower learning rates are often better for fine-tuning transformers
model.compile(optimizer=Adam(3e-5))

model.fit(tokenized_data, labels)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification
<span class="hljs-keyword">from</span> tensorflow.keras.optimizers <span class="hljs-keyword">import</span> Adam

<span class="hljs-comment"># Load and compile our model</span>
model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-comment"># Lower learning rates are often better for fine-tuning transformers</span>
model.<span class="hljs-built_in">compile</span>(optimizer=Adam(<span class="hljs-number">3e-5</span>))

model.fit(tokenized_data, labels)`}}),Z=new It({props:{$$slots:{default:[es]},$$scope:{ctx:fn}}}),Le=new Gn({}),dn=new G({props:{code:`def tokenize_dataset(data):
    # Keys of the returned dictionary will be added to the dataset as columns
    return tokenizer(data["text"])


dataset = dataset.map(tokenize_dataset)`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_dataset</span>(<span class="hljs-params">data</span>):
    <span class="hljs-comment"># Keys of the returned dictionary will be added to the dataset as columns</span>
    <span class="hljs-keyword">return</span> tokenizer(data[<span class="hljs-string">&quot;text&quot;</span>])


dataset = dataset.<span class="hljs-built_in">map</span>(tokenize_dataset)`}}),I=new G({props:{code:"tf_dataset = model.prepare_tf_dataset(dataset, batch_size=16, shuffle=True, tokenizer=tokenizer)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_dataset = model.prepare_tf_dataset(dataset, batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>, tokenizer=tokenizer)'}}),le=new G({props:{code:`model.compile(optimizer=Adam(3e-5))

model.fit(tf_dataset)`,highlighted:`model.<span class="hljs-built_in">compile</span>(optimizer=Adam(<span class="hljs-number">3e-5</span>))

model.fit(tf_dataset)`}}),{c(){g=r("a"),T=m(),$(c.$$.fragment),j=m(),y=r("h2"),A=r("a"),x=r("span"),$(P.$$.fragment),He=m(),C=r("span"),M=p("Trainieren Sie ein TensorFlow-Modell mit Keras"),Ge=m(),O=r("p"),Vn=p("Sie k\xF6nnen auch \u{1F917} Transformers Modelle in TensorFlow mit der Keras API trainieren!"),hn=m(),q=r("h3"),ve=r("a"),Sn=r("span"),$(N.$$.fragment),be=m(),cn=r("span"),gn=p("Laden von Daten f\xFCr Keras"),En=m(),Ce=r("p"),On=p(`Wenn Sie ein \u{1F917} Transformers Modell mit der Keras API trainieren wollen, m\xFCssen Sie Ihren Datensatz in ein Format konvertieren, das
Keras versteht. Wenn Ihr Datensatz klein ist, k\xF6nnen Sie das Ganze einfach in NumPy-Arrays konvertieren und an Keras \xFCbergeben.
Probieren wir das zuerst aus, bevor wir etwas Komplizierteres tun.`),Q=m(),De=r("p"),Ve=p("Laden Sie zun\xE4chst ein Dataset. Wir werden den CoLA-Datensatz aus dem "),Pe=r("a"),ee=p("GLUE-Benchmark"),ne=p(` verwenden,
da es sich um eine einfache Aufgabe zur Klassifizierung von bin\xE4rem Text handelt, und nehmen vorerst nur den Trainingssplit.`),V=m(),$(R.$$.fragment),Qe=m(),en=r("p"),Re=p(`Als n\xE4chstes laden Sie einen Tokenizer und tokenisieren die Daten als NumPy-Arrays. Beachten Sie, dass die Beschriftungen bereits eine Liste von 0 und 1en sind,
Wir k\xF6nnen sie also ohne Tokenisierung direkt in ein NumPy-Array konvertieren!`),jn=m(),$(F.$$.fragment),nn=m(),te=r("p"),Ne=p("Schlie\xDFlich laden, "),tn=r("a"),W=r("code"),de=p("compile"),Ln=p(" und "),K=r("a"),sn=r("code"),qn=p("fit"),Fn=p(" Sie das Modell:"),pe=m(),$(se.$$.fragment),U=m(),$(Z.$$.fragment),Be=m(),we=r("p"),Wn=p(`Dieser Ansatz eignet sich hervorragend f\xFCr kleinere Datens\xE4tze, aber bei gr\xF6\xDFeren Datens\xE4tzen kann er zu einem Problem werden. Warum?
Weil das tokenisierte Array und die Beschriftungen vollst\xE4ndig in den Speicher geladen werden m\xFCssten, und weil NumPy nicht mit
\u201Cgezackte\u201D Arrays nicht verarbeiten kann, so dass jedes tokenisierte Sample auf die L\xE4nge des l\xE4ngsten Samples im gesamten Datensatz aufgef\xFCllt werden m\xFCsste.
Datensatzes aufgef\xFCllt werden. Dadurch wird das Array noch gr\xF6\xDFer, und all die aufgef\xFCllten Token verlangsamen auch das Training!`),Oe=m(),Me=r("h3"),H=r("a"),L=r("span"),$(Le.$$.fragment),Ue=m(),vn=r("span"),ue=p("Laden von Daten als tf.data.Dataset"),$e=m(),me=r("p"),an=p("Wenn Sie eine Verlangsamung des Trainings vermeiden wollen, k\xF6nnen Sie Ihre Daten stattdessen als "),Y=r("code"),We=p("tf.data.Dataset"),In=p(` laden. Sie k\xF6nnen zwar Ihre eigene
tf.data\u201D-Pipeline schreiben k\xF6nnen, wenn Sie wollen, haben wir zwei bequeme Methoden, um dies zu tun:`),fe=m(),J=r("ul"),rn=r("li"),Ze=r("code"),ln=p("prepare_tf_dataset()"),Tn=p(`: Dies ist die Methode, die wir in den meisten F\xE4llen empfehlen. Da es sich um eine Methode
Ihres Modells ist, kann sie das Modell inspizieren, um automatisch herauszufinden, welche Spalten als Modelleingaben verwendet werden k\xF6nnen, und
verwirft die anderen, um einen einfacheren, leistungsf\xE4higeren Datensatz zu erstellen.`),Rn=m(),X=r("li"),ke=p("[~datasets.Dataset.to_tf_dataset`]: Diese Methode ist eher auf niedriger Ebene angesiedelt und ist n\xFCtzlich, wenn Sie genau kontrollieren wollen, wie\nDataset erstellt wird, indem man genau angibt, welche "),bn=r("code"),_e=p("columns"),on=p(" und "),ae=r("code"),B=p("label_cols"),xn=p(" einbezogen werden sollen."),qe=m(),Ke=r("p"),yn=p("Bevor Sie [~TFPreTrainedModel.prepare_tf_dataset`] verwenden k\xF6nnen, m\xFCssen Sie die Tokenizer-Ausgaben als Spalten zu Ihrem Datensatz hinzuf\xFCgen, wie in\ndem folgenden Codebeispiel:"),Fe=m(),$(dn.$$.fragment),Ie=m(),Ye=r("p"),An=p(`Denken Sie daran, dass Hugging Face-Datens\xE4tze standardm\xE4\xDFig auf der Festplatte gespeichert werden, so dass dies nicht zu einem erh\xF6hten Arbeitsspeicherbedarf f\xFChren wird! Sobald die
Spalten hinzugef\xFCgt wurden, k\xF6nnen Sie Batches aus dem Datensatz streamen und zu jedem Batch Auff\xFCllungen hinzuf\xFCgen, was die Anzahl der Auff\xFCllungs-Token im Vergleich zum Auff\xFCllen des gesamten Datensatzes reduziert.`),Dn=m(),$(I.$$.fragment),re=m(),D=r("p"),he=p("Beachten Sie, dass Sie im obigen Codebeispiel den Tokenizer an "),pn=r("code"),wn=p("prepare_tf_dataset"),Kn=p(` \xFCbergeben m\xFCssen, damit die Stapel beim Laden korrekt aufgef\xFCllt werden k\xF6nnen.
Wenn alle Stichproben in Ihrem Datensatz die gleiche L\xE4nge haben und kein Auff\xFCllen erforderlich ist, k\xF6nnen Sie dieses Argument weglassen.
Wenn Sie etwas Komplexeres als nur das Auff\xFCllen von Stichproben ben\xF6tigen (z. B. das Korrumpieren von Token f\xFCr die maskierte Sprachmodellierung), k\xF6nnen Sie das Argument
Modellierung), k\xF6nnen Sie stattdessen das Argument `),ze=r("code"),Se=p("collate_fn"),Je=p(` verwenden, um eine Funktion zu \xFCbergeben, die aufgerufen wird, um die
Liste von Stichproben in einen Stapel umwandelt und alle gew\xFCnschten Vorverarbeitungen vornimmt. Siehe unsere
`),ce=r("a"),ie=p("examples"),Ee=p(` oder
`),je=r("a"),Te=p("notebooks"),un=p(", um diesen Ansatz in Aktion zu sehen."),Pn=m(),ye=r("p"),Mn=p("Sobald Sie einen "),ge=r("code"),$n=p("tf.data.Dataset"),Xe=p(" erstellt haben, k\xF6nnen Sie das Modell wie zuvor kompilieren und anpassen:"),Cn=m(),$(le.$$.fragment),this.h()},l(t){g=i(t,"A",{id:!0}),l(g).forEach(e),T=f(t),k(c.$$.fragment,t),j=f(t),y=i(t,"H2",{class:!0});var v=l(y);A=i(v,"A",{id:!0,class:!0,href:!0});var Ae=l(A);x=i(Ae,"SPAN",{});var Un=l(x);k(P.$$.fragment,Un),Un.forEach(e),Ae.forEach(e),He=f(v),C=i(v,"SPAN",{});var Zn=l(C);M=u(Zn,"Trainieren Sie ein TensorFlow-Modell mit Keras"),Zn.forEach(e),v.forEach(e),Ge=f(t),O=i(t,"P",{});var at=l(O);Vn=u(at,"Sie k\xF6nnen auch \u{1F917} Transformers Modelle in TensorFlow mit der Keras API trainieren!"),at.forEach(e),hn=f(t),q=i(t,"H3",{class:!0});var oe=l(q);ve=i(oe,"A",{id:!0,class:!0,href:!0});var Hn=l(ve);Sn=i(Hn,"SPAN",{});var kn=l(Sn);k(N.$$.fragment,kn),kn.forEach(e),Hn.forEach(e),be=f(oe),cn=i(oe,"SPAN",{});var rt=l(cn);gn=u(rt,"Laden von Daten f\xFCr Keras"),rt.forEach(e),oe.forEach(e),En=f(t),Ce=i(t,"P",{});var o=l(Ce);On=u(o,`Wenn Sie ein \u{1F917} Transformers Modell mit der Keras API trainieren wollen, m\xFCssen Sie Ihren Datensatz in ein Format konvertieren, das
Keras versteht. Wenn Ihr Datensatz klein ist, k\xF6nnen Sie das Ganze einfach in NumPy-Arrays konvertieren und an Keras \xFCbergeben.
Probieren wir das zuerst aus, bevor wir etwas Komplizierteres tun.`),o.forEach(e),Q=f(t),De=i(t,"P",{});var w=l(De);Ve=u(w,"Laden Sie zun\xE4chst ein Dataset. Wir werden den CoLA-Datensatz aus dem "),Pe=i(w,"A",{href:!0,rel:!0});var Yn=l(Pe);ee=u(Yn,"GLUE-Benchmark"),Yn.forEach(e),ne=u(w,` verwenden,
da es sich um eine einfache Aufgabe zur Klassifizierung von bin\xE4rem Text handelt, und nehmen vorerst nur den Trainingssplit.`),w.forEach(e),V=f(t),k(R.$$.fragment,t),Qe=f(t),en=i(t,"P",{});var ht=l(en);Re=u(ht,`Als n\xE4chstes laden Sie einen Tokenizer und tokenisieren die Daten als NumPy-Arrays. Beachten Sie, dass die Beschriftungen bereits eine Liste von 0 und 1en sind,
Wir k\xF6nnen sie also ohne Tokenisierung direkt in ein NumPy-Array konvertieren!`),ht.forEach(e),jn=f(t),k(F.$$.fragment,t),nn=f(t),te=i(t,"P",{});var Nn=l(te);Ne=u(Nn,"Schlie\xDFlich laden, "),tn=i(Nn,"A",{href:!0,rel:!0});var mn=l(tn);W=i(mn,"CODE",{});var Jn=l(W);de=u(Jn,"compile"),Jn.forEach(e),mn.forEach(e),Ln=u(Nn," und "),K=i(Nn,"A",{href:!0,rel:!0});var s=l(K);sn=i(s,"CODE",{});var b=l(sn);qn=u(b,"fit"),b.forEach(e),s.forEach(e),Fn=u(Nn," Sie das Modell:"),Nn.forEach(e),pe=f(t),k(se.$$.fragment,t),U=f(t),k(Z.$$.fragment,t),Be=f(t),we=i(t,"P",{});var it=l(we);Wn=u(it,`Dieser Ansatz eignet sich hervorragend f\xFCr kleinere Datens\xE4tze, aber bei gr\xF6\xDFeren Datens\xE4tzen kann er zu einem Problem werden. Warum?
Weil das tokenisierte Array und die Beschriftungen vollst\xE4ndig in den Speicher geladen werden m\xFCssten, und weil NumPy nicht mit
\u201Cgezackte\u201D Arrays nicht verarbeiten kann, so dass jedes tokenisierte Sample auf die L\xE4nge des l\xE4ngsten Samples im gesamten Datensatz aufgef\xFCllt werden m\xFCsste.
Datensatzes aufgef\xFCllt werden. Dadurch wird das Array noch gr\xF6\xDFer, und all die aufgef\xFCllten Token verlangsamen auch das Training!`),it.forEach(e),Oe=f(t),Me=i(t,"H3",{class:!0});var Xn=l(Me);H=i(Xn,"A",{id:!0,class:!0,href:!0});var dt=l(H);L=i(dt,"SPAN",{});var pt=l(L);k(Le.$$.fragment,pt),pt.forEach(e),dt.forEach(e),Ue=f(Xn),vn=i(Xn,"SPAN",{});var Qn=l(vn);ue=u(Qn,"Laden von Daten als tf.data.Dataset"),Qn.forEach(e),Xn.forEach(e),$e=f(t),me=i(t,"P",{});var _n=l(me);an=u(_n,"Wenn Sie eine Verlangsamung des Trainings vermeiden wollen, k\xF6nnen Sie Ihre Daten stattdessen als "),Y=i(_n,"CODE",{});var ct=l(Y);We=u(ct,"tf.data.Dataset"),ct.forEach(e),In=u(_n,` laden. Sie k\xF6nnen zwar Ihre eigene
tf.data\u201D-Pipeline schreiben k\xF6nnen, wenn Sie wollen, haben wir zwei bequeme Methoden, um dies zu tun:`),_n.forEach(e),fe=f(t),J=i(t,"UL",{});var et=l(J);rn=i(et,"LI",{});var lt=l(rn);Ze=i(lt,"CODE",{});var nt=l(Ze);ln=u(nt,"prepare_tf_dataset()"),nt.forEach(e),Tn=u(lt,`: Dies ist die Methode, die wir in den meisten F\xE4llen empfehlen. Da es sich um eine Methode
Ihres Modells ist, kann sie das Modell inspizieren, um automatisch herauszufinden, welche Spalten als Modelleingaben verwendet werden k\xF6nnen, und
verwirft die anderen, um einen einfacheren, leistungsf\xE4higeren Datensatz zu erstellen.`),lt.forEach(e),Rn=f(et),X=i(et,"LI",{});var tt=l(X);ke=u(tt,"[~datasets.Dataset.to_tf_dataset`]: Diese Methode ist eher auf niedriger Ebene angesiedelt und ist n\xFCtzlich, wenn Sie genau kontrollieren wollen, wie\nDataset erstellt wird, indem man genau angibt, welche "),bn=i(tt,"CODE",{});var gt=l(bn);_e=u(gt,"columns"),gt.forEach(e),on=u(tt," und "),ae=i(tt,"CODE",{});var st=l(ae);B=u(st,"label_cols"),st.forEach(e),xn=u(tt," einbezogen werden sollen."),tt.forEach(e),et.forEach(e),qe=f(t),Ke=i(t,"P",{});var xe=l(Ke);yn=u(xe,"Bevor Sie [~TFPreTrainedModel.prepare_tf_dataset`] verwenden k\xF6nnen, m\xFCssen Sie die Tokenizer-Ausgaben als Spalten zu Ihrem Datensatz hinzuf\xFCgen, wie in\ndem folgenden Codebeispiel:"),xe.forEach(e),Fe=f(t),k(dn.$$.fragment,t),Ie=f(t),Ye=i(t,"P",{});var vt=l(Ye);An=u(vt,`Denken Sie daran, dass Hugging Face-Datens\xE4tze standardm\xE4\xDFig auf der Festplatte gespeichert werden, so dass dies nicht zu einem erh\xF6hten Arbeitsspeicherbedarf f\xFChren wird! Sobald die
Spalten hinzugef\xFCgt wurden, k\xF6nnen Sie Batches aus dem Datensatz streamen und zu jedem Batch Auff\xFCllungen hinzuf\xFCgen, was die Anzahl der Auff\xFCllungs-Token im Vergleich zum Auff\xFCllen des gesamten Datensatzes reduziert.`),vt.forEach(e),Dn=f(t),k(I.$$.fragment,t),re=f(t),D=i(t,"P",{});var zn=l(D);he=u(zn,"Beachten Sie, dass Sie im obigen Codebeispiel den Tokenizer an "),pn=i(zn,"CODE",{});var ut=l(pn);wn=u(ut,"prepare_tf_dataset"),ut.forEach(e),Kn=u(zn,` \xFCbergeben m\xFCssen, damit die Stapel beim Laden korrekt aufgef\xFCllt werden k\xF6nnen.
Wenn alle Stichproben in Ihrem Datensatz die gleiche L\xE4nge haben und kein Auff\xFCllen erforderlich ist, k\xF6nnen Sie dieses Argument weglassen.
Wenn Sie etwas Komplexeres als nur das Auff\xFCllen von Stichproben ben\xF6tigen (z. B. das Korrumpieren von Token f\xFCr die maskierte Sprachmodellierung), k\xF6nnen Sie das Argument
Modellierung), k\xF6nnen Sie stattdessen das Argument `),ze=i(zn,"CODE",{});var mt=l(ze);Se=u(mt,"collate_fn"),mt.forEach(e),Je=u(zn,` verwenden, um eine Funktion zu \xFCbergeben, die aufgerufen wird, um die
Liste von Stichproben in einen Stapel umwandelt und alle gew\xFCnschten Vorverarbeitungen vornimmt. Siehe unsere
`),ce=i(zn,"A",{href:!0,rel:!0});var bt=l(ce);ie=u(bt,"examples"),bt.forEach(e),Ee=u(zn,` oder
`),je=i(zn,"A",{href:!0,rel:!0});var ot=l(je);Te=u(ot,"notebooks"),ot.forEach(e),un=u(zn,", um diesen Ansatz in Aktion zu sehen."),zn.forEach(e),Pn=f(t),ye=i(t,"P",{});var ft=l(ye);Mn=u(ft,"Sobald Sie einen "),ge=i(ft,"CODE",{});var Bn=l(ge);$n=u(Bn,"tf.data.Dataset"),Bn.forEach(e),Xe=u(ft," erstellt haben, k\xF6nnen Sie das Modell wie zuvor kompilieren und anpassen:"),ft.forEach(e),Cn=f(t),k(le.$$.fragment,t),this.h()},h(){h(g,"id","keras"),h(A,"id","trainieren-sie-ein-tensorflowmodell-mit-keras"),h(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(A,"href","#trainieren-sie-ein-tensorflowmodell-mit-keras"),h(y,"class","relative group"),h(ve,"id","laden-von-daten-fr-keras"),h(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ve,"href","#laden-von-daten-fr-keras"),h(q,"class","relative group"),h(Pe,"href","https://huggingface.co/datasets/glue"),h(Pe,"rel","nofollow"),h(tn,"href","https://keras.io/api/models/model_training_apis/#compile-method"),h(tn,"rel","nofollow"),h(K,"href","https://keras.io/api/models/model_training_apis/#fit-method"),h(K,"rel","nofollow"),h(H,"id","laden-von-daten-als-tfdatadataset"),h(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(H,"href","#laden-von-daten-als-tfdatadataset"),h(Me,"class","relative group"),h(ce,"href","https://github.com/huggingface/transformers/tree/main/examples"),h(ce,"rel","nofollow"),h(je,"href","https://huggingface.co/docs/transformers/notebooks"),h(je,"rel","nofollow")},m(t,v){d(t,g,v),d(t,T,v),_(c,t,v),d(t,j,v),d(t,y,v),n(y,A),n(A,x),_(P,x,null),n(y,He),n(y,C),n(C,M),d(t,Ge,v),d(t,O,v),n(O,Vn),d(t,hn,v),d(t,q,v),n(q,ve),n(ve,Sn),_(N,Sn,null),n(q,be),n(q,cn),n(cn,gn),d(t,En,v),d(t,Ce,v),n(Ce,On),d(t,Q,v),d(t,De,v),n(De,Ve),n(De,Pe),n(Pe,ee),n(De,ne),d(t,V,v),_(R,t,v),d(t,Qe,v),d(t,en,v),n(en,Re),d(t,jn,v),_(F,t,v),d(t,nn,v),d(t,te,v),n(te,Ne),n(te,tn),n(tn,W),n(W,de),n(te,Ln),n(te,K),n(K,sn),n(sn,qn),n(te,Fn),d(t,pe,v),_(se,t,v),d(t,U,v),_(Z,t,v),d(t,Be,v),d(t,we,v),n(we,Wn),d(t,Oe,v),d(t,Me,v),n(Me,H),n(H,L),_(Le,L,null),n(Me,Ue),n(Me,vn),n(vn,ue),d(t,$e,v),d(t,me,v),n(me,an),n(me,Y),n(Y,We),n(me,In),d(t,fe,v),d(t,J,v),n(J,rn),n(rn,Ze),n(Ze,ln),n(rn,Tn),n(J,Rn),n(J,X),n(X,ke),n(X,bn),n(bn,_e),n(X,on),n(X,ae),n(ae,B),n(X,xn),d(t,qe,v),d(t,Ke,v),n(Ke,yn),d(t,Fe,v),_(dn,t,v),d(t,Ie,v),d(t,Ye,v),n(Ye,An),d(t,Dn,v),_(I,t,v),d(t,re,v),d(t,D,v),n(D,he),n(D,pn),n(pn,wn),n(D,Kn),n(D,ze),n(ze,Se),n(D,Je),n(D,ce),n(ce,ie),n(D,Ee),n(D,je),n(je,Te),n(D,un),d(t,Pn,v),d(t,ye,v),n(ye,Mn),n(ye,ge),n(ge,$n),n(ye,Xe),d(t,Cn,v),_(le,t,v),a=!0},p(t,v){const Ae={};v&2&&(Ae.$$scope={dirty:v,ctx:t}),Z.$set(Ae)},i(t){a||(z(c.$$.fragment,t),z(P.$$.fragment,t),z(N.$$.fragment,t),z(R.$$.fragment,t),z(F.$$.fragment,t),z(se.$$.fragment,t),z(Z.$$.fragment,t),z(Le.$$.fragment,t),z(dn.$$.fragment,t),z(I.$$.fragment,t),z(le.$$.fragment,t),a=!0)},o(t){S(c.$$.fragment,t),S(P.$$.fragment,t),S(N.$$.fragment,t),S(R.$$.fragment,t),S(F.$$.fragment,t),S(se.$$.fragment,t),S(Z.$$.fragment,t),S(Le.$$.fragment,t),S(dn.$$.fragment,t),S(I.$$.fragment,t),S(le.$$.fragment,t),a=!1},d(t){t&&e(g),t&&e(T),E(c,t),t&&e(j),t&&e(y),E(P),t&&e(Ge),t&&e(O),t&&e(hn),t&&e(q),E(N),t&&e(En),t&&e(Ce),t&&e(Q),t&&e(De),t&&e(V),E(R,t),t&&e(Qe),t&&e(en),t&&e(jn),E(F,t),t&&e(nn),t&&e(te),t&&e(pe),E(se,t),t&&e(U),E(Z,t),t&&e(Be),t&&e(we),t&&e(Oe),t&&e(Me),E(Le),t&&e($e),t&&e(me),t&&e(fe),t&&e(J),t&&e(qe),t&&e(Ke),t&&e(Fe),E(dn,t),t&&e(Ie),t&&e(Ye),t&&e(Dn),E(I,t),t&&e(re),t&&e(D),t&&e(Pn),t&&e(ye),t&&e(Cn),E(le,t)}}}function ts(fn){let g,T;return g=new xt({props:{$$slots:{default:[ns]},$$scope:{ctx:fn}}}),{c(){$(g.$$.fragment)},l(c){k(g.$$.fragment,c)},m(c,j){_(g,c,j),T=!0},p(c,j){const y={};j&2&&(y.$$scope={dirty:j,ctx:c}),g.$set(y)},i(c){T||(z(g.$$.fragment,c),T=!0)},o(c){S(g.$$.fragment,c),T=!1},d(c){E(g,c)}}}function ss(fn){let g,T,c,j,y,A,x,P;return{c(){g=r("p"),T=p("Holen Sie sich mit einem gehosteten Notebook wie "),c=r("a"),j=p("Colaboratory"),y=p(" oder "),A=r("a"),x=p("SageMaker StudioLab"),P=p(" kostenlosen Zugang zu einem Cloud-GPU, wenn Sie noch keinen haben."),this.h()},l(He){g=i(He,"P",{});var C=l(g);T=u(C,"Holen Sie sich mit einem gehosteten Notebook wie "),c=i(C,"A",{href:!0,rel:!0});var M=l(c);j=u(M,"Colaboratory"),M.forEach(e),y=u(C," oder "),A=i(C,"A",{href:!0,rel:!0});var Ge=l(A);x=u(Ge,"SageMaker StudioLab"),Ge.forEach(e),P=u(C," kostenlosen Zugang zu einem Cloud-GPU, wenn Sie noch keinen haben."),C.forEach(e),this.h()},h(){h(c,"href","https://colab.research.google.com/"),h(c,"rel","nofollow"),h(A,"href","https://studiolab.sagemaker.aws/"),h(A,"rel","nofollow")},m(He,C){d(He,g,C),n(g,T),n(g,c),n(c,j),n(g,y),n(g,A),n(A,x),n(g,P)},d(He){He&&e(g)}}}function as(fn){let g,T,c,j,y,A,x,P,He,C,M,Ge,O,Vn,hn,q,ve,Sn,N,be,cn,gn,En,Ce,On,Q,De,Ve,Pe,ee,ne,V,R,Qe,en,Re,jn,F,nn,te,Ne,tn,W,de,Ln,K,sn,qn,Fn,pe,se,U,Z,Be,we,Wn,Oe,Me,H,L,Le,Ue,vn,ue,$e,me,an,Y,We,In,fe,J,rn,Ze,ln,Tn,Rn,X,ke,bn,_e,on,ae,B,xn,qe,Ke,yn,Fe,dn,Ie,Ye,An,Dn,I,re,D,he,pn,wn,Kn,ze,Se,Je,ce,ie,Ee,je,Te,un,Pn,ye,Mn,ge,$n,Xe,Cn,le,a,t,v,Ae,Un,Zn,at,oe,Hn,kn,rt,o,w,Yn,ht,Nn,mn,Jn;return g=new Ft({props:{id:"Dh9CL8fyG80"}}),M=new G({props:{code:`del model
del pytorch_model
del trainer
torch.cuda.empty_cache()`,highlighted:`<span class="hljs-keyword">del</span> model
<span class="hljs-keyword">del</span> pytorch_model
<span class="hljs-keyword">del</span> trainer
torch.cuda.empty_cache()`}}),Ce=new G({props:{code:'tokenized_datasets = tokenized_datasets.remove_columns(["text"])',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_datasets = tokenized_datasets.remove_columns([<span class="hljs-string">&quot;text&quot;</span>])'}}),ee=new G({props:{code:'tokenized_datasets = tokenized_datasets.rename_column("label", "labels")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_datasets = tokenized_datasets.rename_column(<span class="hljs-string">&quot;label&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>)'}}),Re=new G({props:{code:'tokenized_datasets.set_format("torch")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_datasets.set_format(<span class="hljs-string">&quot;torch&quot;</span>)'}}),Ne=new G({props:{code:`small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>small_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>small_eval_dataset = tokenized_datasets[<span class="hljs-string">&quot;test&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))`}}),K=new Gn({}),Oe=new G({props:{code:`from torch.utils.data import DataLoader

train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)
eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader

<span class="hljs-meta">&gt;&gt;&gt; </span>train_dataloader = DataLoader(small_train_dataset, shuffle=<span class="hljs-literal">True</span>, batch_size=<span class="hljs-number">8</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>eval_dataloader = DataLoader(small_eval_dataset, batch_size=<span class="hljs-number">8</span>)`}}),Ue=new G({props:{code:`from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, num_labels=<span class="hljs-number">5</span>)`}}),an=new Gn({}),ke=new G({props:{code:`from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> AdamW

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">5e-5</span>)`}}),Ke=new G({props:{code:`from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    name="linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> get_scheduler

<span class="hljs-meta">&gt;&gt;&gt; </span>num_epochs = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_training_steps = num_epochs * <span class="hljs-built_in">len</span>(train_dataloader)
<span class="hljs-meta">&gt;&gt;&gt; </span>lr_scheduler = get_scheduler(
<span class="hljs-meta">... </span>    name=<span class="hljs-string">&quot;linear&quot;</span>, optimizer=optimizer, num_warmup_steps=<span class="hljs-number">0</span>, num_training_steps=num_training_steps
<span class="hljs-meta">... </span>)`}}),I=new G({props:{code:`import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.to(device)`}}),D=new It({props:{$$slots:{default:[ss]},$$scope:{ctx:fn}}}),ce=new Gn({}),Xe=new G({props:{code:`from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm

<span class="hljs-meta">&gt;&gt;&gt; </span>progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_training_steps))

<span class="hljs-meta">&gt;&gt;&gt; </span>model.train()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:
<span class="hljs-meta">... </span>        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
<span class="hljs-meta">... </span>        outputs = model(**batch)
<span class="hljs-meta">... </span>        loss = outputs.loss
<span class="hljs-meta">... </span>        loss.backward()

<span class="hljs-meta">... </span>        optimizer.step()
<span class="hljs-meta">... </span>        lr_scheduler.step()
<span class="hljs-meta">... </span>        optimizer.zero_grad()
<span class="hljs-meta">... </span>        progress_bar.update(<span class="hljs-number">1</span>)`}}),v=new Gn({}),mn=new G({props:{code:`import evaluate

metric = evaluate.load("accuracy")
model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate

<span class="hljs-meta">&gt;&gt;&gt; </span>metric = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> eval_dataloader:
<span class="hljs-meta">... </span>    batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
<span class="hljs-meta">... </span>    <span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>        outputs = model(**batch)

<span class="hljs-meta">... </span>    logits = outputs.logits
<span class="hljs-meta">... </span>    predictions = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    metric.add_batch(predictions=predictions, references=batch[<span class="hljs-string">&quot;labels&quot;</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>metric.compute()`}}),{c(){$(g.$$.fragment),T=m(),c=r("p"),j=r("code"),y=p("Trainer"),A=p(" k\xFCmmert sich um die Trainingsschleife und erm\xF6glicht die Feinabstimmung eines Modells in einer einzigen Codezeile. F\xFCr Benutzer, die es vorziehen, ihre eigene Trainingsschleife zu schreiben, k\xF6nnen Sie auch eine Feinabstimmung eines \u{1F917} Transformers-Modells in nativem PyTorch vornehmen."),x=m(),P=r("p"),He=p("An diesem Punkt m\xFCssen Sie m\xF6glicherweise Ihr Notebook neu starten oder den folgenden Code ausf\xFChren, um etwas Speicher freizugeben:"),C=m(),$(M.$$.fragment),Ge=m(),O=r("p"),Vn=p("Als N\xE4chstes m\xFCssen Sie den Datensatz "),hn=r("code"),q=p("tokenized_dataset"),ve=p(" manuell nachbearbeiten, um ihn f\xFCr das Training vorzubereiten."),Sn=m(),N=r("ol"),be=r("li"),cn=r("p"),gn=p("Entfernen Sie die Spalte \u201CText\u201D, da das Modell keinen Rohtext als Eingabe akzeptiert:"),En=m(),$(Ce.$$.fragment),On=m(),Q=r("li"),De=r("p"),Ve=p("Benennen Sie die Spalte \u201CLabel\u201D in \u201CLabels\u201D um, da das Modell erwartet, dass das Argument \u201CLabels\u201D genannt wird:"),Pe=m(),$(ee.$$.fragment),ne=m(),V=r("li"),R=r("p"),Qe=p("Stellen Sie das Format des Datensatzes so ein, dass PyTorch-Tensoren anstelle von Listen zur\xFCckgegeben werden:"),en=m(),$(Re.$$.fragment),jn=m(),F=r("p"),nn=p("Erstellen Sie dann eine kleinere Teilmenge des Datensatzes, wie zuvor gezeigt, um die Feinabstimmung zu beschleunigen:"),te=m(),$(Ne.$$.fragment),tn=m(),W=r("h3"),de=r("a"),Ln=r("span"),$(K.$$.fragment),sn=m(),qn=r("span"),Fn=p("DataLoader"),pe=m(),se=r("p"),U=p("Erstellen Sie einen "),Z=r("code"),Be=p("DataLoader"),we=p(" f\xFCr Ihre Trainings- und Testdatens\xE4tze, damit Sie \xFCber die Datenstapel iterieren k\xF6nnen:"),Wn=m(),$(Oe.$$.fragment),Me=m(),H=r("p"),L=p("Laden Sie Ihr Modell mit der Anzahl der erwarteten Kennzeichnungen:"),Le=m(),$(Ue.$$.fragment),vn=m(),ue=r("h3"),$e=r("a"),me=r("span"),$(an.$$.fragment),Y=m(),We=r("span"),In=p("Optimierer und Lernratensteuerung"),fe=m(),J=r("p"),rn=p("Erstellen Sie einen Optimierer und einen Scheduler f\xFCr die Lernrate, um das Modell fein abzustimmen. Wir verwenden den Optimierer "),Ze=r("a"),ln=r("code"),Tn=p("AdamW"),Rn=p(" aus PyTorch:"),X=m(),$(ke.$$.fragment),bn=m(),_e=r("p"),on=p("Erstellen Sie den Standard-Lernratenplaner aus "),ae=r("code"),B=p("Trainer"),xn=p(":"),qe=m(),$(Ke.$$.fragment),yn=m(),Fe=r("p"),dn=p("Geben Sie schlie\xDFlich "),Ie=r("code"),Ye=p("device"),An=p(" an, um einen Grafikprozessor zu verwenden, wenn Sie Zugang zu einem solchen haben. Andernfalls kann das Training auf einer CPU mehrere Stunden statt ein paar Minuten dauern."),Dn=m(),$(I.$$.fragment),re=m(),$(D.$$.fragment),he=m(),pn=r("p"),wn=p("Gro\xDFartig, Sie sind bereit f\xFCr das Training! \u{1F973}"),Kn=m(),ze=r("h3"),Se=r("a"),Je=r("span"),$(ce.$$.fragment),ie=m(),Ee=r("span"),je=p("Trainingsschleife"),Te=m(),un=r("p"),Pn=p("Um Ihren Trainingsfortschritt zu verfolgen, verwenden Sie die "),ye=r("a"),Mn=p("tqdm"),ge=p(" Bibliothek, um einen Fortschrittsbalken \xFCber die Anzahl der Trainingsschritte hinzuzuf\xFCgen:"),$n=m(),$(Xe.$$.fragment),Cn=m(),le=r("h3"),a=r("a"),t=r("span"),$(v.$$.fragment),Ae=m(),Un=r("span"),Zn=p("Auswertung"),at=m(),oe=r("p"),Hn=p("Genauso wie Sie eine Bewertungsfunktion zu "),kn=r("code"),rt=p("Trainer"),o=p(" hinzugef\xFCgt haben, m\xFCssen Sie dasselbe tun, wenn Sie Ihre eigene Trainingsschleife schreiben. Aber anstatt die Metrik am Ende jeder Epoche zu berechnen und zu melden, werden Sie dieses Mal alle Stapel mit "),w=r("code"),Yn=p("add_batch"),ht=p(" akkumulieren und die Metrik ganz am Ende berechnen."),Nn=m(),$(mn.$$.fragment),this.h()},l(s){k(g.$$.fragment,s),T=f(s),c=i(s,"P",{});var b=l(c);j=i(b,"CODE",{});var it=l(j);y=u(it,"Trainer"),it.forEach(e),A=u(b," k\xFCmmert sich um die Trainingsschleife und erm\xF6glicht die Feinabstimmung eines Modells in einer einzigen Codezeile. F\xFCr Benutzer, die es vorziehen, ihre eigene Trainingsschleife zu schreiben, k\xF6nnen Sie auch eine Feinabstimmung eines \u{1F917} Transformers-Modells in nativem PyTorch vornehmen."),b.forEach(e),x=f(s),P=i(s,"P",{});var Xn=l(P);He=u(Xn,"An diesem Punkt m\xFCssen Sie m\xF6glicherweise Ihr Notebook neu starten oder den folgenden Code ausf\xFChren, um etwas Speicher freizugeben:"),Xn.forEach(e),C=f(s),k(M.$$.fragment,s),Ge=f(s),O=i(s,"P",{});var dt=l(O);Vn=u(dt,"Als N\xE4chstes m\xFCssen Sie den Datensatz "),hn=i(dt,"CODE",{});var pt=l(hn);q=u(pt,"tokenized_dataset"),pt.forEach(e),ve=u(dt," manuell nachbearbeiten, um ihn f\xFCr das Training vorzubereiten."),dt.forEach(e),Sn=f(s),N=i(s,"OL",{});var Qn=l(N);be=i(Qn,"LI",{});var _n=l(be);cn=i(_n,"P",{});var ct=l(cn);gn=u(ct,"Entfernen Sie die Spalte \u201CText\u201D, da das Modell keinen Rohtext als Eingabe akzeptiert:"),ct.forEach(e),En=f(_n),k(Ce.$$.fragment,_n),_n.forEach(e),On=f(Qn),Q=i(Qn,"LI",{});var et=l(Q);De=i(et,"P",{});var lt=l(De);Ve=u(lt,"Benennen Sie die Spalte \u201CLabel\u201D in \u201CLabels\u201D um, da das Modell erwartet, dass das Argument \u201CLabels\u201D genannt wird:"),lt.forEach(e),Pe=f(et),k(ee.$$.fragment,et),et.forEach(e),ne=f(Qn),V=i(Qn,"LI",{});var nt=l(V);R=i(nt,"P",{});var tt=l(R);Qe=u(tt,"Stellen Sie das Format des Datensatzes so ein, dass PyTorch-Tensoren anstelle von Listen zur\xFCckgegeben werden:"),tt.forEach(e),en=f(nt),k(Re.$$.fragment,nt),nt.forEach(e),Qn.forEach(e),jn=f(s),F=i(s,"P",{});var gt=l(F);nn=u(gt,"Erstellen Sie dann eine kleinere Teilmenge des Datensatzes, wie zuvor gezeigt, um die Feinabstimmung zu beschleunigen:"),gt.forEach(e),te=f(s),k(Ne.$$.fragment,s),tn=f(s),W=i(s,"H3",{class:!0});var st=l(W);de=i(st,"A",{id:!0,class:!0,href:!0});var xe=l(de);Ln=i(xe,"SPAN",{});var vt=l(Ln);k(K.$$.fragment,vt),vt.forEach(e),xe.forEach(e),sn=f(st),qn=i(st,"SPAN",{});var zn=l(qn);Fn=u(zn,"DataLoader"),zn.forEach(e),st.forEach(e),pe=f(s),se=i(s,"P",{});var ut=l(se);U=u(ut,"Erstellen Sie einen "),Z=i(ut,"CODE",{});var mt=l(Z);Be=u(mt,"DataLoader"),mt.forEach(e),we=u(ut," f\xFCr Ihre Trainings- und Testdatens\xE4tze, damit Sie \xFCber die Datenstapel iterieren k\xF6nnen:"),ut.forEach(e),Wn=f(s),k(Oe.$$.fragment,s),Me=f(s),H=i(s,"P",{});var bt=l(H);L=u(bt,"Laden Sie Ihr Modell mit der Anzahl der erwarteten Kennzeichnungen:"),bt.forEach(e),Le=f(s),k(Ue.$$.fragment,s),vn=f(s),ue=i(s,"H3",{class:!0});var ot=l(ue);$e=i(ot,"A",{id:!0,class:!0,href:!0});var ft=l($e);me=i(ft,"SPAN",{});var Bn=l(me);k(an.$$.fragment,Bn),Bn.forEach(e),ft.forEach(e),Y=f(ot),We=i(ot,"SPAN",{});var $t=l(We);In=u($t,"Optimierer und Lernratensteuerung"),$t.forEach(e),ot.forEach(e),fe=f(s),J=i(s,"P",{});var wt=l(J);rn=u(wt,"Erstellen Sie einen Optimierer und einen Scheduler f\xFCr die Lernrate, um das Modell fein abzustimmen. Wir verwenden den Optimierer "),Ze=i(wt,"A",{href:!0,rel:!0});var yt=l(Ze);ln=i(yt,"CODE",{});var _t=l(ln);Tn=u(_t,"AdamW"),_t.forEach(e),yt.forEach(e),Rn=u(wt," aus PyTorch:"),wt.forEach(e),X=f(s),k(ke.$$.fragment,s),bn=f(s),_e=i(s,"P",{});var zt=l(_e);on=u(zt,"Erstellen Sie den Standard-Lernratenplaner aus "),ae=i(zt,"CODE",{});var St=l(ae);B=u(St,"Trainer"),St.forEach(e),xn=u(zt,":"),zt.forEach(e),qe=f(s),k(Ke.$$.fragment,s),yn=f(s),Fe=i(s,"P",{});var Et=l(Fe);dn=u(Et,"Geben Sie schlie\xDFlich "),Ie=i(Et,"CODE",{});var At=l(Ie);Ye=u(At,"device"),At.forEach(e),An=u(Et," an, um einen Grafikprozessor zu verwenden, wenn Sie Zugang zu einem solchen haben. Andernfalls kann das Training auf einer CPU mehrere Stunden statt ein paar Minuten dauern."),Et.forEach(e),Dn=f(s),k(I.$$.fragment,s),re=f(s),k(D.$$.fragment,s),he=f(s),pn=i(s,"P",{});var Dt=l(pn);wn=u(Dt,"Gro\xDFartig, Sie sind bereit f\xFCr das Training! \u{1F973}"),Dt.forEach(e),Kn=f(s),ze=i(s,"H3",{class:!0});var kt=l(ze);Se=i(kt,"A",{id:!0,class:!0,href:!0});var Pt=l(Se);Je=i(Pt,"SPAN",{});var jt=l(Je);k(ce.$$.fragment,jt),jt.forEach(e),Pt.forEach(e),ie=f(kt),Ee=i(kt,"SPAN",{});var Mt=l(Ee);je=u(Mt,"Trainingsschleife"),Mt.forEach(e),kt.forEach(e),Te=f(s),un=i(s,"P",{});var Lt=l(un);Pn=u(Lt,"Um Ihren Trainingsfortschritt zu verfolgen, verwenden Sie die "),ye=i(Lt,"A",{href:!0,rel:!0});var Ct=l(ye);Mn=u(Ct,"tqdm"),Ct.forEach(e),ge=u(Lt," Bibliothek, um einen Fortschrittsbalken \xFCber die Anzahl der Trainingsschritte hinzuzuf\xFCgen:"),Lt.forEach(e),$n=f(s),k(Xe.$$.fragment,s),Cn=f(s),le=i(s,"H3",{class:!0});var qt=l(le);a=i(qt,"A",{id:!0,class:!0,href:!0});var Nt=l(a);t=i(Nt,"SPAN",{});var Bt=l(t);k(v.$$.fragment,Bt),Bt.forEach(e),Nt.forEach(e),Ae=f(qt),Un=i(qt,"SPAN",{});var Ot=l(Un);Zn=u(Ot,"Auswertung"),Ot.forEach(e),qt.forEach(e),at=f(s),oe=i(s,"P",{});var Tt=l(oe);Hn=u(Tt,"Genauso wie Sie eine Bewertungsfunktion zu "),kn=i(Tt,"CODE",{});var Wt=l(kn);rt=u(Wt,"Trainer"),Wt.forEach(e),o=u(Tt," hinzugef\xFCgt haben, m\xFCssen Sie dasselbe tun, wenn Sie Ihre eigene Trainingsschleife schreiben. Aber anstatt die Metrik am Ende jeder Epoche zu berechnen und zu melden, werden Sie dieses Mal alle Stapel mit "),w=i(Tt,"CODE",{});var Kt=l(w);Yn=u(Kt,"add_batch"),Kt.forEach(e),ht=u(Tt," akkumulieren und die Metrik ganz am Ende berechnen."),Tt.forEach(e),Nn=f(s),k(mn.$$.fragment,s),this.h()},h(){h(de,"id","dataloader"),h(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(de,"href","#dataloader"),h(W,"class","relative group"),h($e,"id","optimierer-und-lernratensteuerung"),h($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h($e,"href","#optimierer-und-lernratensteuerung"),h(ue,"class","relative group"),h(Ze,"href","https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html"),h(Ze,"rel","nofollow"),h(Se,"id","trainingsschleife"),h(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Se,"href","#trainingsschleife"),h(ze,"class","relative group"),h(ye,"href","https://tqdm.github.io/"),h(ye,"rel","nofollow"),h(a,"id","auswertung"),h(a,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(a,"href","#auswertung"),h(le,"class","relative group")},m(s,b){_(g,s,b),d(s,T,b),d(s,c,b),n(c,j),n(j,y),n(c,A),d(s,x,b),d(s,P,b),n(P,He),d(s,C,b),_(M,s,b),d(s,Ge,b),d(s,O,b),n(O,Vn),n(O,hn),n(hn,q),n(O,ve),d(s,Sn,b),d(s,N,b),n(N,be),n(be,cn),n(cn,gn),n(be,En),_(Ce,be,null),n(N,On),n(N,Q),n(Q,De),n(De,Ve),n(Q,Pe),_(ee,Q,null),n(N,ne),n(N,V),n(V,R),n(R,Qe),n(V,en),_(Re,V,null),d(s,jn,b),d(s,F,b),n(F,nn),d(s,te,b),_(Ne,s,b),d(s,tn,b),d(s,W,b),n(W,de),n(de,Ln),_(K,Ln,null),n(W,sn),n(W,qn),n(qn,Fn),d(s,pe,b),d(s,se,b),n(se,U),n(se,Z),n(Z,Be),n(se,we),d(s,Wn,b),_(Oe,s,b),d(s,Me,b),d(s,H,b),n(H,L),d(s,Le,b),_(Ue,s,b),d(s,vn,b),d(s,ue,b),n(ue,$e),n($e,me),_(an,me,null),n(ue,Y),n(ue,We),n(We,In),d(s,fe,b),d(s,J,b),n(J,rn),n(J,Ze),n(Ze,ln),n(ln,Tn),n(J,Rn),d(s,X,b),_(ke,s,b),d(s,bn,b),d(s,_e,b),n(_e,on),n(_e,ae),n(ae,B),n(_e,xn),d(s,qe,b),_(Ke,s,b),d(s,yn,b),d(s,Fe,b),n(Fe,dn),n(Fe,Ie),n(Ie,Ye),n(Fe,An),d(s,Dn,b),_(I,s,b),d(s,re,b),_(D,s,b),d(s,he,b),d(s,pn,b),n(pn,wn),d(s,Kn,b),d(s,ze,b),n(ze,Se),n(Se,Je),_(ce,Je,null),n(ze,ie),n(ze,Ee),n(Ee,je),d(s,Te,b),d(s,un,b),n(un,Pn),n(un,ye),n(ye,Mn),n(un,ge),d(s,$n,b),_(Xe,s,b),d(s,Cn,b),d(s,le,b),n(le,a),n(a,t),_(v,t,null),n(le,Ae),n(le,Un),n(Un,Zn),d(s,at,b),d(s,oe,b),n(oe,Hn),n(oe,kn),n(kn,rt),n(oe,o),n(oe,w),n(w,Yn),n(oe,ht),d(s,Nn,b),_(mn,s,b),Jn=!0},p(s,b){const it={};b&2&&(it.$$scope={dirty:b,ctx:s}),D.$set(it)},i(s){Jn||(z(g.$$.fragment,s),z(M.$$.fragment,s),z(Ce.$$.fragment,s),z(ee.$$.fragment,s),z(Re.$$.fragment,s),z(Ne.$$.fragment,s),z(K.$$.fragment,s),z(Oe.$$.fragment,s),z(Ue.$$.fragment,s),z(an.$$.fragment,s),z(ke.$$.fragment,s),z(Ke.$$.fragment,s),z(I.$$.fragment,s),z(D.$$.fragment,s),z(ce.$$.fragment,s),z(Xe.$$.fragment,s),z(v.$$.fragment,s),z(mn.$$.fragment,s),Jn=!0)},o(s){S(g.$$.fragment,s),S(M.$$.fragment,s),S(Ce.$$.fragment,s),S(ee.$$.fragment,s),S(Re.$$.fragment,s),S(Ne.$$.fragment,s),S(K.$$.fragment,s),S(Oe.$$.fragment,s),S(Ue.$$.fragment,s),S(an.$$.fragment,s),S(ke.$$.fragment,s),S(Ke.$$.fragment,s),S(I.$$.fragment,s),S(D.$$.fragment,s),S(ce.$$.fragment,s),S(Xe.$$.fragment,s),S(v.$$.fragment,s),S(mn.$$.fragment,s),Jn=!1},d(s){E(g,s),s&&e(T),s&&e(c),s&&e(x),s&&e(P),s&&e(C),E(M,s),s&&e(Ge),s&&e(O),s&&e(Sn),s&&e(N),E(Ce),E(ee),E(Re),s&&e(jn),s&&e(F),s&&e(te),E(Ne,s),s&&e(tn),s&&e(W),E(K),s&&e(pe),s&&e(se),s&&e(Wn),E(Oe,s),s&&e(Me),s&&e(H),s&&e(Le),E(Ue,s),s&&e(vn),s&&e(ue),E(an),s&&e(fe),s&&e(J),s&&e(X),E(ke,s),s&&e(bn),s&&e(_e),s&&e(qe),E(Ke,s),s&&e(yn),s&&e(Fe),s&&e(Dn),E(I,s),s&&e(re),E(D,s),s&&e(he),s&&e(pn),s&&e(Kn),s&&e(ze),E(ce),s&&e(Te),s&&e(un),s&&e($n),E(Xe,s),s&&e(Cn),s&&e(le),E(v),s&&e(at),s&&e(oe),s&&e(Nn),E(mn,s)}}}function rs(fn){let g,T;return g=new xt({props:{$$slots:{default:[as]},$$scope:{ctx:fn}}}),{c(){$(g.$$.fragment)},l(c){k(g.$$.fragment,c)},m(c,j){_(g,c,j),T=!0},p(c,j){const y={};j&2&&(y.$$scope={dirty:j,ctx:c}),g.$set(y)},i(c){T||(z(g.$$.fragment,c),T=!0)},o(c){S(g.$$.fragment,c),T=!1},d(c){E(g,c)}}}function is(fn){let g,T,c,j,y,A,x,P,He,C,M,Ge,O,Vn,hn,q,ve,Sn,N,be,cn,gn,En,Ce,On,Q,De,Ve,Pe,ee,ne,V,R,Qe,en,Re,jn,F,nn,te,Ne,tn,W,de,Ln,K,sn,qn,Fn,pe,se,U,Z,Be,we,Wn,Oe,Me,H,L,Le,Ue,vn,ue,$e,me,an,Y,We,In,fe,J,rn,Ze,ln,Tn,Rn,X,ke,bn,_e,on,ae,B,xn,qe,Ke,yn,Fe,dn,Ie,Ye,An,Dn,I,re,D,he,pn,wn,Kn,ze,Se,Je,ce,ie,Ee,je,Te,un,Pn,ye,Mn,ge,$n,Xe,Cn,le;return A=new Gn({}),M=new Yt({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/de/training.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/de/pytorch/training.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/de/tensorflow/training.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/de/training.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/de/pytorch/training.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/de/tensorflow/training.ipynb"}]}}),Qe=new Gn({}),nn=new Ft({props:{id:"_BZearw7f0w"}}),pe=new G({props:{code:`from datasets import load_dataset

dataset = load_dataset("yelp_review_full")
dataset["train"][100]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;yelp_review_full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">100</span>]
{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\&#x27;s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\&#x27;s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\&quot;serving off their orders\\\\&quot; when they didn\\&#x27;t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\&#x27;t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\&#x27;ve eaten at various McDonalds restaurants for over 30 years. I\\&#x27;ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!&#x27;</span>}`}}),H=new G({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")


def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)


tokenized_datasets = dataset.map(tokenize_function, batched=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>], padding=<span class="hljs-string">&quot;max_length&quot;</span>, truncation=<span class="hljs-literal">True</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_datasets = dataset.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)`}}),ue=new G({props:{code:`small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>small_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>small_eval_dataset = tokenized_datasets[<span class="hljs-string">&quot;test&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))`}}),fe=new Gn({}),ke=new Ht({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[ts],pytorch:[Qt]},$$scope:{ctx:fn}}}),qe=new Gn({}),Ie=new Ht({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[rs]},$$scope:{ctx:fn}}}),he=new Gn({}),{c(){g=r("meta"),T=m(),c=r("h1"),j=r("a"),y=r("span"),$(A.$$.fragment),x=m(),P=r("span"),He=p("Optimierung eines vortrainierten Modells"),C=m(),$(M.$$.fragment),Ge=m(),O=r("p"),Vn=p("Die Verwendung eines vorab trainierten Modells hat erhebliche Vorteile. Es reduziert die Rechenkosten und den CO2-Fu\xDFabdruck und erm\xF6glicht Ihnen die Verwendung von Modellen, die dem neuesten Stand der Technik entsprechen, ohne dass Sie ein Modell von Grund auf neu trainieren m\xFCssen. Transformers bietet Zugang zu Tausenden von vortrainierten Modellen f\xFCr eine Vielzahl von Aufgaben. Wenn Sie ein vorab trainiertes Modell verwenden, trainieren Sie es auf einem f\xFCr Ihre Aufgabe spezifischen Datensatz. Dies wird als Feinabstimmung bezeichnet und ist eine unglaublich leistungsf\xE4hige Trainingstechnik. In diesem Tutorial werden Sie ein vortrainiertes Modell mit einem Deep-Learning-Framework Ihrer Wahl feinabstimmen:"),hn=m(),q=r("ul"),ve=r("li"),Sn=p("Feinabstimmung eines vorab trainierten Modells mit \u{1F917} Transformers "),N=r("code"),be=p("Trainer"),cn=p("."),gn=m(),En=r("li"),Ce=p("Feinabstimmung eines vorab trainierten Modells in TensorFlow mit Keras."),On=m(),Q=r("li"),De=p("Feinabstimmung eines vorab trainierten Modells in nativem PyTorch."),Ve=m(),Pe=r("a"),ee=m(),ne=r("h2"),V=r("a"),R=r("span"),$(Qe.$$.fragment),en=m(),Re=r("span"),jn=p("Vorbereitung eines Datensatzes"),F=m(),$(nn.$$.fragment),te=m(),Ne=r("p"),tn=p("Bevor Sie die Feinabstimmung eines vortrainierten Modells vornehmen k\xF6nnen, m\xFCssen Sie einen Datensatz herunterladen und f\xFCr das Training vorbereiten. Im vorangegangenen Leitfaden haben Sie gelernt, wie man Daten f\xFCr das Training aufbereitet, und jetzt haben Sie die Gelegenheit, diese F\xE4higkeiten zu testen!"),W=m(),de=r("p"),Ln=p("Laden Sie zun\xE4chst den Datensatz "),K=r("a"),sn=p("Yelp Reviews"),qn=p(":"),Fn=m(),$(pe.$$.fragment),se=m(),U=r("p"),Z=p("Wie Sie nun wissen, ben\xF6tigen Sie einen Tokenizer, um den Text zu verarbeiten und eine Auff\xFCll- und Abschneidungsstrategie einzubauen, um mit variablen Sequenzl\xE4ngen umzugehen. Um Ihren Datensatz in einem Schritt zu verarbeiten, verwenden Sie die \u{1F917} Methode Datasets "),Be=r("a"),we=r("code"),Wn=p("map"),Oe=p(", um eine Vorverarbeitungsfunktion auf den gesamten Datensatz anzuwenden:"),Me=m(),$(H.$$.fragment),L=m(),Le=r("p"),Ue=p("Wenn Sie m\xF6chten, k\xF6nnen Sie eine kleinere Teilmenge des gesamten Datensatzes f\xFCr die Feinabstimmung erstellen, um den Zeitaufwand zu verringern:"),vn=m(),$(ue.$$.fragment),$e=m(),me=r("a"),an=m(),Y=r("h2"),We=r("a"),In=r("span"),$(fe.$$.fragment),J=m(),rn=r("span"),Ze=p("Training"),ln=m(),Tn=r("p"),Rn=p(`An dieser Stelle sollten Sie dem Abschnitt folgen, der dem Rahmen entspricht, den Sie verwenden m\xF6chten. Sie k\xF6nnen \xFCber die Links
in der rechten Seitenleiste k\xF6nnen Sie zu dem gew\xFCnschten Abschnitt springen - und wenn Sie den gesamten Inhalt eines bestimmten Frameworks ausblenden m\xF6chten,
klicken Sie einfach auf die Schaltfl\xE4che oben rechts im Block des jeweiligen Frameworks!`),X=m(),$(ke.$$.fragment),bn=m(),_e=r("a"),on=m(),ae=r("h2"),B=r("a"),xn=r("span"),$(qe.$$.fragment),Ke=m(),yn=r("span"),Fe=p("Trainieren in nativem PyTorch"),dn=m(),$(Ie.$$.fragment),Ye=m(),An=r("a"),Dn=m(),I=r("h2"),re=r("a"),D=r("span"),$(he.$$.fragment),pn=m(),wn=r("span"),Kn=p("Zus\xE4tzliche Ressourcen"),ze=m(),Se=r("p"),Je=p("Weitere Beispiele f\xFCr die Feinabstimmung finden Sie unter:"),ce=m(),ie=r("ul"),Ee=r("li"),je=r("p"),Te=r("a"),un=p("\u{1F917} Transformers Examples"),Pn=p(` enth\xE4lt Skripte
um g\xE4ngige NLP-Aufgaben in PyTorch und TensorFlow zu trainieren.`),ye=m(),Mn=r("li"),ge=r("p"),$n=r("a"),Xe=p("\u{1F917} Transformers Notebooks"),Cn=p(" enth\xE4lt verschiedene Notebooks zur Feinabstimmung eines Modells f\xFCr bestimmte Aufgaben in PyTorch und TensorFlow."),this.h()},l(a){const t=Ut('[data-svelte="svelte-1phssyn"]',document.head);g=i(t,"META",{name:!0,content:!0}),t.forEach(e),T=f(a),c=i(a,"H1",{class:!0});var v=l(c);j=i(v,"A",{id:!0,class:!0,href:!0});var Ae=l(j);y=i(Ae,"SPAN",{});var Un=l(y);k(A.$$.fragment,Un),Un.forEach(e),Ae.forEach(e),x=f(v),P=i(v,"SPAN",{});var Zn=l(P);He=u(Zn,"Optimierung eines vortrainierten Modells"),Zn.forEach(e),v.forEach(e),C=f(a),k(M.$$.fragment,a),Ge=f(a),O=i(a,"P",{});var at=l(O);Vn=u(at,"Die Verwendung eines vorab trainierten Modells hat erhebliche Vorteile. Es reduziert die Rechenkosten und den CO2-Fu\xDFabdruck und erm\xF6glicht Ihnen die Verwendung von Modellen, die dem neuesten Stand der Technik entsprechen, ohne dass Sie ein Modell von Grund auf neu trainieren m\xFCssen. Transformers bietet Zugang zu Tausenden von vortrainierten Modellen f\xFCr eine Vielzahl von Aufgaben. Wenn Sie ein vorab trainiertes Modell verwenden, trainieren Sie es auf einem f\xFCr Ihre Aufgabe spezifischen Datensatz. Dies wird als Feinabstimmung bezeichnet und ist eine unglaublich leistungsf\xE4hige Trainingstechnik. In diesem Tutorial werden Sie ein vortrainiertes Modell mit einem Deep-Learning-Framework Ihrer Wahl feinabstimmen:"),at.forEach(e),hn=f(a),q=i(a,"UL",{});var oe=l(q);ve=i(oe,"LI",{});var Hn=l(ve);Sn=u(Hn,"Feinabstimmung eines vorab trainierten Modells mit \u{1F917} Transformers "),N=i(Hn,"CODE",{});var kn=l(N);be=u(kn,"Trainer"),kn.forEach(e),cn=u(Hn,"."),Hn.forEach(e),gn=f(oe),En=i(oe,"LI",{});var rt=l(En);Ce=u(rt,"Feinabstimmung eines vorab trainierten Modells in TensorFlow mit Keras."),rt.forEach(e),On=f(oe),Q=i(oe,"LI",{});var o=l(Q);De=u(o,"Feinabstimmung eines vorab trainierten Modells in nativem PyTorch."),o.forEach(e),oe.forEach(e),Ve=f(a),Pe=i(a,"A",{id:!0}),l(Pe).forEach(e),ee=f(a),ne=i(a,"H2",{class:!0});var w=l(ne);V=i(w,"A",{id:!0,class:!0,href:!0});var Yn=l(V);R=i(Yn,"SPAN",{});var ht=l(R);k(Qe.$$.fragment,ht),ht.forEach(e),Yn.forEach(e),en=f(w),Re=i(w,"SPAN",{});var Nn=l(Re);jn=u(Nn,"Vorbereitung eines Datensatzes"),Nn.forEach(e),w.forEach(e),F=f(a),k(nn.$$.fragment,a),te=f(a),Ne=i(a,"P",{});var mn=l(Ne);tn=u(mn,"Bevor Sie die Feinabstimmung eines vortrainierten Modells vornehmen k\xF6nnen, m\xFCssen Sie einen Datensatz herunterladen und f\xFCr das Training vorbereiten. Im vorangegangenen Leitfaden haben Sie gelernt, wie man Daten f\xFCr das Training aufbereitet, und jetzt haben Sie die Gelegenheit, diese F\xE4higkeiten zu testen!"),mn.forEach(e),W=f(a),de=i(a,"P",{});var Jn=l(de);Ln=u(Jn,"Laden Sie zun\xE4chst den Datensatz "),K=i(Jn,"A",{href:!0,rel:!0});var s=l(K);sn=u(s,"Yelp Reviews"),s.forEach(e),qn=u(Jn,":"),Jn.forEach(e),Fn=f(a),k(pe.$$.fragment,a),se=f(a),U=i(a,"P",{});var b=l(U);Z=u(b,"Wie Sie nun wissen, ben\xF6tigen Sie einen Tokenizer, um den Text zu verarbeiten und eine Auff\xFCll- und Abschneidungsstrategie einzubauen, um mit variablen Sequenzl\xE4ngen umzugehen. Um Ihren Datensatz in einem Schritt zu verarbeiten, verwenden Sie die \u{1F917} Methode Datasets "),Be=i(b,"A",{href:!0,rel:!0});var it=l(Be);we=i(it,"CODE",{});var Xn=l(we);Wn=u(Xn,"map"),Xn.forEach(e),it.forEach(e),Oe=u(b,", um eine Vorverarbeitungsfunktion auf den gesamten Datensatz anzuwenden:"),b.forEach(e),Me=f(a),k(H.$$.fragment,a),L=f(a),Le=i(a,"P",{});var dt=l(Le);Ue=u(dt,"Wenn Sie m\xF6chten, k\xF6nnen Sie eine kleinere Teilmenge des gesamten Datensatzes f\xFCr die Feinabstimmung erstellen, um den Zeitaufwand zu verringern:"),dt.forEach(e),vn=f(a),k(ue.$$.fragment,a),$e=f(a),me=i(a,"A",{id:!0}),l(me).forEach(e),an=f(a),Y=i(a,"H2",{class:!0});var pt=l(Y);We=i(pt,"A",{id:!0,class:!0,href:!0});var Qn=l(We);In=i(Qn,"SPAN",{});var _n=l(In);k(fe.$$.fragment,_n),_n.forEach(e),Qn.forEach(e),J=f(pt),rn=i(pt,"SPAN",{});var ct=l(rn);Ze=u(ct,"Training"),ct.forEach(e),pt.forEach(e),ln=f(a),Tn=i(a,"P",{});var et=l(Tn);Rn=u(et,`An dieser Stelle sollten Sie dem Abschnitt folgen, der dem Rahmen entspricht, den Sie verwenden m\xF6chten. Sie k\xF6nnen \xFCber die Links
in der rechten Seitenleiste k\xF6nnen Sie zu dem gew\xFCnschten Abschnitt springen - und wenn Sie den gesamten Inhalt eines bestimmten Frameworks ausblenden m\xF6chten,
klicken Sie einfach auf die Schaltfl\xE4che oben rechts im Block des jeweiligen Frameworks!`),et.forEach(e),X=f(a),k(ke.$$.fragment,a),bn=f(a),_e=i(a,"A",{id:!0}),l(_e).forEach(e),on=f(a),ae=i(a,"H2",{class:!0});var lt=l(ae);B=i(lt,"A",{id:!0,class:!0,href:!0});var nt=l(B);xn=i(nt,"SPAN",{});var tt=l(xn);k(qe.$$.fragment,tt),tt.forEach(e),nt.forEach(e),Ke=f(lt),yn=i(lt,"SPAN",{});var gt=l(yn);Fe=u(gt,"Trainieren in nativem PyTorch"),gt.forEach(e),lt.forEach(e),dn=f(a),k(Ie.$$.fragment,a),Ye=f(a),An=i(a,"A",{id:!0}),l(An).forEach(e),Dn=f(a),I=i(a,"H2",{class:!0});var st=l(I);re=i(st,"A",{id:!0,class:!0,href:!0});var xe=l(re);D=i(xe,"SPAN",{});var vt=l(D);k(he.$$.fragment,vt),vt.forEach(e),xe.forEach(e),pn=f(st),wn=i(st,"SPAN",{});var zn=l(wn);Kn=u(zn,"Zus\xE4tzliche Ressourcen"),zn.forEach(e),st.forEach(e),ze=f(a),Se=i(a,"P",{});var ut=l(Se);Je=u(ut,"Weitere Beispiele f\xFCr die Feinabstimmung finden Sie unter:"),ut.forEach(e),ce=f(a),ie=i(a,"UL",{});var mt=l(ie);Ee=i(mt,"LI",{});var bt=l(Ee);je=i(bt,"P",{});var ot=l(je);Te=i(ot,"A",{href:!0,rel:!0});var ft=l(Te);un=u(ft,"\u{1F917} Transformers Examples"),ft.forEach(e),Pn=u(ot,` enth\xE4lt Skripte
um g\xE4ngige NLP-Aufgaben in PyTorch und TensorFlow zu trainieren.`),ot.forEach(e),bt.forEach(e),ye=f(mt),Mn=i(mt,"LI",{});var Bn=l(Mn);ge=i(Bn,"P",{});var $t=l(ge);$n=i($t,"A",{href:!0});var wt=l($n);Xe=u(wt,"\u{1F917} Transformers Notebooks"),wt.forEach(e),Cn=u($t," enth\xE4lt verschiedene Notebooks zur Feinabstimmung eines Modells f\xFCr bestimmte Aufgaben in PyTorch und TensorFlow."),$t.forEach(e),Bn.forEach(e),mt.forEach(e),this.h()},h(){h(g,"name","hf:doc:metadata"),h(g,"content",JSON.stringify(ls)),h(j,"id","optimierung-eines-vortrainierten-modells"),h(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(j,"href","#optimierung-eines-vortrainierten-modells"),h(c,"class","relative group"),h(Pe,"id","data-processing"),h(V,"id","vorbereitung-eines-datensatzes"),h(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(V,"href","#vorbereitung-eines-datensatzes"),h(ne,"class","relative group"),h(K,"href","https://huggingface.co/datasets/yelp_review_full"),h(K,"rel","nofollow"),h(Be,"href","https://huggingface.co/docs/datasets/process.html#map"),h(Be,"rel","nofollow"),h(me,"id","trainer"),h(We,"id","training"),h(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(We,"href","#training"),h(Y,"class","relative group"),h(_e,"id","pytorch_native"),h(B,"id","trainieren-in-nativem-pytorch"),h(B,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(B,"href","#trainieren-in-nativem-pytorch"),h(ae,"class","relative group"),h(An,"id","additional-resources"),h(re,"id","zustzliche-ressourcen"),h(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(re,"href","#zustzliche-ressourcen"),h(I,"class","relative group"),h(Te,"href","https://github.com/huggingface/transformers/tree/main/examples"),h(Te,"rel","nofollow"),h($n,"href","notebooks")},m(a,t){n(document.head,g),d(a,T,t),d(a,c,t),n(c,j),n(j,y),_(A,y,null),n(c,x),n(c,P),n(P,He),d(a,C,t),_(M,a,t),d(a,Ge,t),d(a,O,t),n(O,Vn),d(a,hn,t),d(a,q,t),n(q,ve),n(ve,Sn),n(ve,N),n(N,be),n(ve,cn),n(q,gn),n(q,En),n(En,Ce),n(q,On),n(q,Q),n(Q,De),d(a,Ve,t),d(a,Pe,t),d(a,ee,t),d(a,ne,t),n(ne,V),n(V,R),_(Qe,R,null),n(ne,en),n(ne,Re),n(Re,jn),d(a,F,t),_(nn,a,t),d(a,te,t),d(a,Ne,t),n(Ne,tn),d(a,W,t),d(a,de,t),n(de,Ln),n(de,K),n(K,sn),n(de,qn),d(a,Fn,t),_(pe,a,t),d(a,se,t),d(a,U,t),n(U,Z),n(U,Be),n(Be,we),n(we,Wn),n(U,Oe),d(a,Me,t),_(H,a,t),d(a,L,t),d(a,Le,t),n(Le,Ue),d(a,vn,t),_(ue,a,t),d(a,$e,t),d(a,me,t),d(a,an,t),d(a,Y,t),n(Y,We),n(We,In),_(fe,In,null),n(Y,J),n(Y,rn),n(rn,Ze),d(a,ln,t),d(a,Tn,t),n(Tn,Rn),d(a,X,t),_(ke,a,t),d(a,bn,t),d(a,_e,t),d(a,on,t),d(a,ae,t),n(ae,B),n(B,xn),_(qe,xn,null),n(ae,Ke),n(ae,yn),n(yn,Fe),d(a,dn,t),_(Ie,a,t),d(a,Ye,t),d(a,An,t),d(a,Dn,t),d(a,I,t),n(I,re),n(re,D),_(he,D,null),n(I,pn),n(I,wn),n(wn,Kn),d(a,ze,t),d(a,Se,t),n(Se,Je),d(a,ce,t),d(a,ie,t),n(ie,Ee),n(Ee,je),n(je,Te),n(Te,un),n(je,Pn),n(ie,ye),n(ie,Mn),n(Mn,ge),n(ge,$n),n($n,Xe),n(ge,Cn),le=!0},p(a,[t]){const v={};t&2&&(v.$$scope={dirty:t,ctx:a}),ke.$set(v);const Ae={};t&2&&(Ae.$$scope={dirty:t,ctx:a}),Ie.$set(Ae)},i(a){le||(z(A.$$.fragment,a),z(M.$$.fragment,a),z(Qe.$$.fragment,a),z(nn.$$.fragment,a),z(pe.$$.fragment,a),z(H.$$.fragment,a),z(ue.$$.fragment,a),z(fe.$$.fragment,a),z(ke.$$.fragment,a),z(qe.$$.fragment,a),z(Ie.$$.fragment,a),z(he.$$.fragment,a),le=!0)},o(a){S(A.$$.fragment,a),S(M.$$.fragment,a),S(Qe.$$.fragment,a),S(nn.$$.fragment,a),S(pe.$$.fragment,a),S(H.$$.fragment,a),S(ue.$$.fragment,a),S(fe.$$.fragment,a),S(ke.$$.fragment,a),S(qe.$$.fragment,a),S(Ie.$$.fragment,a),S(he.$$.fragment,a),le=!1},d(a){e(g),a&&e(T),a&&e(c),E(A),a&&e(C),E(M,a),a&&e(Ge),a&&e(O),a&&e(hn),a&&e(q),a&&e(Ve),a&&e(Pe),a&&e(ee),a&&e(ne),E(Qe),a&&e(F),E(nn,a),a&&e(te),a&&e(Ne),a&&e(W),a&&e(de),a&&e(Fn),E(pe,a),a&&e(se),a&&e(U),a&&e(Me),E(H,a),a&&e(L),a&&e(Le),a&&e(vn),E(ue,a),a&&e($e),a&&e(me),a&&e(an),a&&e(Y),E(fe),a&&e(ln),a&&e(Tn),a&&e(X),E(ke,a),a&&e(bn),a&&e(_e),a&&e(on),a&&e(ae),E(qe),a&&e(dn),E(Ie,a),a&&e(Ye),a&&e(An),a&&e(Dn),a&&e(I),E(he),a&&e(ze),a&&e(Se),a&&e(ce),a&&e(ie)}}}const ls={local:"optimierung-eines-vortrainierten-modells",sections:[{local:"vorbereitung-eines-datensatzes",title:"Vorbereitung eines Datensatzes"},{local:"training",title:"Training"},{local:"trainieren-mit-pytorch-trainer",sections:[{local:"hyperparameter-fr-das-training",title:"Hyperparameter f\xFCr das Training"},{local:"auswerten",title:"Auswerten"},{local:"trainer",title:"Trainer"}],title:"Trainieren mit PyTorch Trainer"},{local:"trainieren-sie-ein-tensorflowmodell-mit-keras",sections:[{local:"laden-von-daten-fr-keras",title:"Laden von Daten f\xFCr Keras"},{local:"laden-von-daten-als-tfdatadataset",title:"Laden von Daten als tf.data.Dataset"}],title:"Trainieren Sie ein TensorFlow-Modell mit Keras"},{local:"trainieren-in-nativem-pytorch",sections:[{local:"dataloader",title:"DataLoader"},{local:"optimierer-und-lernratensteuerung",title:"Optimierer und Lernratensteuerung"},{local:"trainingsschleife",title:"Trainingsschleife"},{local:"auswertung",title:"Auswertung"}],title:"Trainieren in nativem PyTorch"},{local:"zustzliche-ressourcen",title:"Zus\xE4tzliche Ressourcen"}],title:"Optimierung eines vortrainierten Modells"};function os(fn){return Zt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class gs extends Gt{constructor(g){super();Vt(this,g,os,is,Rt,{})}}export{gs as default,ls as metadata};
