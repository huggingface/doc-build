import{S as wp,i as kp,s as vp,e as l,k as c,w as v,t as a,M as Ep,c as o,d as n,m as h,a as f,x as E,h as i,b as k,G as t,g as u,y as A,q as T,o as z,B as S,v as Ap,L as me}from"../chunks/vendor-hf-doc-builder.js";import{T as An}from"../chunks/Tip-hf-doc-builder.js";import{Y as bp}from"../chunks/Youtube-hf-doc-builder.js";import{I as Ee}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as L}from"../chunks/CodeBlock-hf-doc-builder.js";import{D as Tp}from"../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as Zt,M as oe}from"../chunks/Markdown-hf-doc-builder.js";function zp(j){let s,d;return{c(){s=l("p"),d=a(`Alle in der Dokumentation vorgestellten Codebeispiele haben oben links einen Umschalter f\xFCr PyTorch und TensorFlow. Wenn
nicht, wird erwartet, dass der Code f\xFCr beide Backends ohne \xC4nderungen funktioniert.`)},l(r){s=o(r,"P",{});var m=f(s);d=i(m,`Alle in der Dokumentation vorgestellten Codebeispiele haben oben links einen Umschalter f\xFCr PyTorch und TensorFlow. Wenn
nicht, wird erwartet, dass der Code f\xFCr beide Backends ohne \xC4nderungen funktioniert.`),m.forEach(n)},m(r,m){u(r,s,m),t(s,d)},d(r){r&&n(s)}}}function Sp(j){let s,d,r,m,g,b,w,I;return{c(){s=l("p"),d=a("F\xFCr mehr Details \xFCber die "),r=l("code"),m=a("pipeline()"),g=a(" und assoziierte Aufgaben, schauen Sie in die Dokumentation "),b=l("a"),w=a("hier"),I=a("."),this.h()},l($){s=o($,"P",{});var C=f(s);d=i(C,"F\xFCr mehr Details \xFCber die "),r=o(C,"CODE",{});var F=f(r);m=i(F,"pipeline()"),F.forEach(n),g=i(C," und assoziierte Aufgaben, schauen Sie in die Dokumentation "),b=o(C,"A",{href:!0});var W=f(b);w=i(W,"hier"),W.forEach(n),I=i(C,"."),C.forEach(n),this.h()},h(){k(b,"href","./main_classes/pipelines")},m($,C){u($,s,C),t(s,d),t(s,r),t(r,m),t(s,g),t(s,b),t(b,w),t(s,I)},d($){$&&n(s)}}}function jp(j){let s,d;return s=new L({props:{code:"pip install torch",highlighted:"pip install torch"}}),{c(){v(s.$$.fragment)},l(r){E(s.$$.fragment,r)},m(r,m){A(s,r,m),d=!0},p:me,i(r){d||(T(s.$$.fragment,r),d=!0)},o(r){z(s.$$.fragment,r),d=!1},d(r){S(s,r)}}}function Mp(j){let s,d;return s=new oe({props:{$$slots:{default:[jp]},$$scope:{ctx:j}}}),{c(){v(s.$$.fragment)},l(r){E(s.$$.fragment,r)},m(r,m){A(s,r,m),d=!0},p(r,m){const g={};m&2&&(g.$$scope={dirty:m,ctx:r}),s.$set(g)},i(r){d||(T(s.$$.fragment,r),d=!0)},o(r){z(s.$$.fragment,r),d=!1},d(r){S(s,r)}}}function yp(j){let s,d;return s=new L({props:{code:"pip install tensorflow",highlighted:"pip install tensorflow"}}),{c(){v(s.$$.fragment)},l(r){E(s.$$.fragment,r)},m(r,m){A(s,r,m),d=!0},p:me,i(r){d||(T(s.$$.fragment,r),d=!0)},o(r){z(s.$$.fragment,r),d=!1},d(r){S(s,r)}}}function Dp(j){let s,d;return s=new oe({props:{$$slots:{default:[yp]},$$scope:{ctx:j}}}),{c(){v(s.$$.fragment)},l(r){E(s.$$.fragment,r)},m(r,m){A(s,r,m),d=!0},p(r,m){const g={};m&2&&(g.$$scope={dirty:m,ctx:r}),s.$set(g)},i(r){d||(T(s.$$.fragment,r),d=!0)},o(r){z(s.$$.fragment,r),d=!1},d(r){S(s,r)}}}function Pp(j){let s,d,r,m,g,b,w,I,$,C,F,W,q,B;return q=new L({props:{code:`from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),{c(){s=l("p"),d=a("Use the "),r=l("code"),m=a("AutoModelForSequenceClassification"),g=a(" and "),b=l("code"),w=a("AutoTokenizer"),I=a(" to load the pretrained model and it\u2019s associated tokenizer (more on an "),$=l("code"),C=a("AutoClass"),F=a(" below):"),W=c(),v(q.$$.fragment)},l(M){s=o(M,"P",{});var x=f(s);d=i(x,"Use the "),r=o(x,"CODE",{});var _=f(r);m=i(_,"AutoModelForSequenceClassification"),_.forEach(n),g=i(x," and "),b=o(x,"CODE",{});var P=f(b);w=i(P,"AutoTokenizer"),P.forEach(n),I=i(x," to load the pretrained model and it\u2019s associated tokenizer (more on an "),$=o(x,"CODE",{});var H=f($);C=i(H,"AutoClass"),H.forEach(n),F=i(x," below):"),x.forEach(n),W=h(M),E(q.$$.fragment,M)},m(M,x){u(M,s,x),t(s,d),t(s,r),t(r,m),t(s,g),t(s,b),t(b,w),t(s,I),t(s,$),t($,C),t(s,F),u(M,W,x),A(q,M,x),B=!0},p:me,i(M){B||(T(q.$$.fragment,M),B=!0)},o(M){z(q.$$.fragment,M),B=!1},d(M){M&&n(s),M&&n(W),S(q,M)}}}function Cp(j){let s,d;return s=new oe({props:{$$slots:{default:[Pp]},$$scope:{ctx:j}}}),{c(){v(s.$$.fragment)},l(r){E(s.$$.fragment,r)},m(r,m){A(s,r,m),d=!0},p(r,m){const g={};m&2&&(g.$$scope={dirty:m,ctx:r}),s.$set(g)},i(r){d||(T(s.$$.fragment,r),d=!0)},o(r){z(s.$$.fragment,r),d=!1},d(r){S(s,r)}}}function xp(j){let s,d,r,m,g,b,w,I,$,C,F,W,q,B;return q=new L({props:{code:`from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),{c(){s=l("p"),d=a("Use the "),r=l("code"),m=a("TFAutoModelForSequenceClassification"),g=a(" and "),b=l("code"),w=a("AutoTokenizer"),I=a(" to load the pretrained model and it\u2019s associated tokenizer (more on an "),$=l("code"),C=a("TFAutoClass"),F=a(" below):"),W=c(),v(q.$$.fragment)},l(M){s=o(M,"P",{});var x=f(s);d=i(x,"Use the "),r=o(x,"CODE",{});var _=f(r);m=i(_,"TFAutoModelForSequenceClassification"),_.forEach(n),g=i(x," and "),b=o(x,"CODE",{});var P=f(b);w=i(P,"AutoTokenizer"),P.forEach(n),I=i(x," to load the pretrained model and it\u2019s associated tokenizer (more on an "),$=o(x,"CODE",{});var H=f($);C=i(H,"TFAutoClass"),H.forEach(n),F=i(x," below):"),x.forEach(n),W=h(M),E(q.$$.fragment,M)},m(M,x){u(M,s,x),t(s,d),t(s,r),t(r,m),t(s,g),t(s,b),t(b,w),t(s,I),t(s,$),t($,C),t(s,F),u(M,W,x),A(q,M,x),B=!0},p:me,i(M){B||(T(q.$$.fragment,M),B=!0)},o(M){z(q.$$.fragment,M),B=!1},d(M){M&&n(s),M&&n(W),S(q,M)}}}function Ip(j){let s,d;return s=new oe({props:{$$slots:{default:[xp]},$$scope:{ctx:j}}}),{c(){v(s.$$.fragment)},l(r){E(s.$$.fragment,r)},m(r,m){A(s,r,m),d=!0},p(r,m){const g={};m&2&&(g.$$scope={dirty:m,ctx:r}),s.$set(g)},i(r){d||(T(s.$$.fragment,r),d=!0)},o(r){z(s.$$.fragment,r),d=!1},d(r){S(s,r)}}}function Fp(j){let s,d;return s=new L({props:{code:`pt_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`}}),{c(){v(s.$$.fragment)},l(r){E(s.$$.fragment,r)},m(r,m){A(s,r,m),d=!0},p:me,i(r){d||(T(s.$$.fragment,r),d=!0)},o(r){z(s.$$.fragment,r),d=!1},d(r){S(s,r)}}}function qp(j){let s,d;return s=new oe({props:{$$slots:{default:[Fp]},$$scope:{ctx:j}}}),{c(){v(s.$$.fragment)},l(r){E(s.$$.fragment,r)},m(r,m){A(s,r,m),d=!0},p(r,m){const g={};m&2&&(g.$$scope={dirty:m,ctx:r}),s.$set(g)},i(r){d||(T(s.$$.fragment,r),d=!0)},o(r){z(s.$$.fragment,r),d=!1},d(r){S(s,r)}}}function Op(j){let s,d;return s=new L({props:{code:`tf_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`}}),{c(){v(s.$$.fragment)},l(r){E(s.$$.fragment,r)},m(r,m){A(s,r,m),d=!0},p:me,i(r){d||(T(s.$$.fragment,r),d=!0)},o(r){z(s.$$.fragment,r),d=!1},d(r){S(s,r)}}}function Wp(j){let s,d;return s=new oe({props:{$$slots:{default:[Op]},$$scope:{ctx:j}}}),{c(){v(s.$$.fragment)},l(r){E(s.$$.fragment,r)},m(r,m){A(s,r,m),d=!0},p(r,m){const g={};m&2&&(g.$$scope={dirty:m,ctx:r}),s.$set(g)},i(r){d||(T(s.$$.fragment,r),d=!0)},o(r){z(s.$$.fragment,r),d=!1},d(r){S(s,r)}}}function Np(j){let s,d,r,m,g;return{c(){s=l("p"),d=a("In der "),r=l("a"),m=a("Aufgabenzusammenfassung"),g=a(" steht, welche [AutoModel]-Klasse f\xFCr welche Aufgabe zu verwenden ist."),this.h()},l(b){s=o(b,"P",{});var w=f(s);d=i(w,"In der "),r=o(w,"A",{href:!0});var I=f(r);m=i(I,"Aufgabenzusammenfassung"),I.forEach(n),g=i(w," steht, welche [AutoModel]-Klasse f\xFCr welche Aufgabe zu verwenden ist."),w.forEach(n),this.h()},h(){k(r,"href","./task_summary")},m(b,w){u(b,s,w),t(s,d),t(s,r),t(r,m),t(s,g)},d(b){b&&n(s)}}}function Lp(j){let s,d,r,m,g,b,w,I,$,C,F,W,q,B,M,x,_,P,H,K,V,G,ne,Z,U,ee,te,J,re,y,O,Y;return x=new L({props:{code:`from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`}}),P=new An({props:{$$slots:{default:[Np]},$$scope:{ctx:j}}}),ee=new L({props:{code:"pt_outputs = pt_model(**pt_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)'}}),O=new L({props:{code:`from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">0.0021</span>, <span class="hljs-number">0.0018</span>, <span class="hljs-number">0.0115</span>, <span class="hljs-number">0.2121</span>, <span class="hljs-number">0.7725</span>],
        [<span class="hljs-number">0.2084</span>, <span class="hljs-number">0.1826</span>, <span class="hljs-number">0.1969</span>, <span class="hljs-number">0.1755</span>, <span class="hljs-number">0.2365</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)`}}),{c(){s=l("p"),d=a("\u{1F917} Transformers bietet eine einfache und einheitliche M\xF6glichkeit, vortrainierte Instanzen zu laden. Das bedeutet, dass Sie ein "),r=l("code"),m=a("AutoModel"),g=a(" laden k\xF6nnen, wie Sie einen "),b=l("code"),w=a("AutoTokenizer"),I=a(" laden w\xFCrden. Der einzige Unterschied ist die Auswahl des richtigen "),$=l("code"),C=a("AutoModel"),F=a(" f\xFCr die Aufgabe. Da Sie eine Text- oder Sequenzklassifizierung vornehmen, laden Sie "),W=l("code"),q=a("AutoModelForSequenceClassification"),B=a(":"),M=c(),v(x.$$.fragment),_=c(),v(P.$$.fragment),H=c(),K=l("p"),V=a("Jetzt k\xF6nnen Sie Ihren vorverarbeiteten Stapel von Eingaben direkt an das Modell \xFCbergeben. Sie m\xFCssen nur das W\xF6rterbuch entpacken, indem Sie "),G=l("code"),ne=a("**"),Z=a(" hinzuf\xFCgen:"),U=c(),v(ee.$$.fragment),te=c(),J=l("p"),re=a("Das Modell gibt die endg\xFCltigen Aktivierungen in dem Attribut \u201Clogits\u201D aus. Wenden Sie die Softmax-Funktion auf die \u201Clogits\u201D an, um die Wahrscheinlichkeiten zu erhalten:"),y=c(),v(O.$$.fragment)},l(D){s=o(D,"P",{});var N=f(s);d=i(N,"\u{1F917} Transformers bietet eine einfache und einheitliche M\xF6glichkeit, vortrainierte Instanzen zu laden. Das bedeutet, dass Sie ein "),r=o(N,"CODE",{});var ue=f(r);m=i(ue,"AutoModel"),ue.forEach(n),g=i(N," laden k\xF6nnen, wie Sie einen "),b=o(N,"CODE",{});var ce=f(b);w=i(ce,"AutoTokenizer"),ce.forEach(n),I=i(N," laden w\xFCrden. Der einzige Unterschied ist die Auswahl des richtigen "),$=o(N,"CODE",{});var fe=f($);C=i(fe,"AutoModel"),fe.forEach(n),F=i(N," f\xFCr die Aufgabe. Da Sie eine Text- oder Sequenzklassifizierung vornehmen, laden Sie "),W=o(N,"CODE",{});var dt=f(W);q=i(dt,"AutoModelForSequenceClassification"),dt.forEach(n),B=i(N,":"),N.forEach(n),M=h(D),E(x.$$.fragment,D),_=h(D),E(P.$$.fragment,D),H=h(D),K=o(D,"P",{});var pe=f(K);V=i(pe,"Jetzt k\xF6nnen Sie Ihren vorverarbeiteten Stapel von Eingaben direkt an das Modell \xFCbergeben. Sie m\xFCssen nur das W\xF6rterbuch entpacken, indem Sie "),G=o(pe,"CODE",{});var Jt=f(G);ne=i(Jt,"**"),Jt.forEach(n),Z=i(pe," hinzuf\xFCgen:"),pe.forEach(n),U=h(D),E(ee.$$.fragment,D),te=h(D),J=o(D,"P",{});var Ce=f(J);re=i(Ce,"Das Modell gibt die endg\xFCltigen Aktivierungen in dem Attribut \u201Clogits\u201D aus. Wenden Sie die Softmax-Funktion auf die \u201Clogits\u201D an, um die Wahrscheinlichkeiten zu erhalten:"),Ce.forEach(n),y=h(D),E(O.$$.fragment,D)},m(D,N){u(D,s,N),t(s,d),t(s,r),t(r,m),t(s,g),t(s,b),t(b,w),t(s,I),t(s,$),t($,C),t(s,F),t(s,W),t(W,q),t(s,B),u(D,M,N),A(x,D,N),u(D,_,N),A(P,D,N),u(D,H,N),u(D,K,N),t(K,V),t(K,G),t(G,ne),t(K,Z),u(D,U,N),A(ee,D,N),u(D,te,N),u(D,J,N),t(J,re),u(D,y,N),A(O,D,N),Y=!0},p(D,N){const ue={};N&2&&(ue.$$scope={dirty:N,ctx:D}),P.$set(ue)},i(D){Y||(T(x.$$.fragment,D),T(P.$$.fragment,D),T(ee.$$.fragment,D),T(O.$$.fragment,D),Y=!0)},o(D){z(x.$$.fragment,D),z(P.$$.fragment,D),z(ee.$$.fragment,D),z(O.$$.fragment,D),Y=!1},d(D){D&&n(s),D&&n(M),S(x,D),D&&n(_),S(P,D),D&&n(H),D&&n(K),D&&n(U),S(ee,D),D&&n(te),D&&n(J),D&&n(y),S(O,D)}}}function Bp(j){let s,d;return s=new oe({props:{$$slots:{default:[Lp]},$$scope:{ctx:j}}}),{c(){v(s.$$.fragment)},l(r){E(s.$$.fragment,r)},m(r,m){A(s,r,m),d=!0},p(r,m){const g={};m&2&&(g.$$scope={dirty:m,ctx:r}),s.$set(g)},i(r){d||(T(s.$$.fragment,r),d=!0)},o(r){z(s.$$.fragment,r),d=!1},d(r){S(s,r)}}}function Kp(j){let s,d,r,m,g;return{c(){s=l("p"),d=a("In der "),r=l("a"),m=a("Aufgabenzusammenfassung"),g=a(" steht, welche [AutoModel]-Klasse f\xFCr welche Aufgabe zu verwenden ist."),this.h()},l(b){s=o(b,"P",{});var w=f(s);d=i(w,"In der "),r=o(w,"A",{href:!0});var I=f(r);m=i(I,"Aufgabenzusammenfassung"),I.forEach(n),g=i(w," steht, welche [AutoModel]-Klasse f\xFCr welche Aufgabe zu verwenden ist."),w.forEach(n),this.h()},h(){k(r,"href","./task_summary")},m(b,w){u(b,s,w),t(s,d),t(s,r),t(r,m),t(s,g)},d(b){b&&n(s)}}}function Hp(j){let s,d,r,m,g,b,w,I,$,C,F,W,q,B,M,x,_,P,H,K,V,G,ne,Z,U,ee,te,J,re;return x=new L({props:{code:`from transformers import TFAutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`}}),P=new An({props:{$$slots:{default:[Kp]},$$scope:{ctx:j}}}),ne=new L({props:{code:"tf_outputs = tf_model(tf_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)'}}),J=new L({props:{code:`import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
tf_predictions`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions`}}),{c(){s=l("p"),d=a("\u{1F917} Transformers bietet eine einfache und einheitliche Methode zum Laden von vortrainierten Instanzen. Das bedeutet, dass Sie ein "),r=l("code"),m=a("TFAutoModel"),g=a(" genauso laden k\xF6nnen, wie Sie einen "),b=l("code"),w=a("AutoTokenizer"),I=a(" laden w\xFCrden. Der einzige Unterschied ist die Auswahl des richtigen "),$=l("code"),C=a("TFAutoModel"),F=a(" f\xFCr die Aufgabe. Da Sie Text - oder Sequenz - Klassifizierung machen, laden Sie "),W=l("code"),q=a("TFAutoModelForSequenceClassification"),B=a(":"),M=c(),v(x.$$.fragment),_=c(),v(P.$$.fragment),H=c(),K=l("p"),V=a("Jetzt k\xF6nnen Sie Ihren vorverarbeiteten Stapel von Eingaben direkt an das Modell \xFCbergeben, indem Sie die W\xF6rterbuchschl\xFCssel direkt an die Tensoren \xFCbergeben:"),G=c(),v(ne.$$.fragment),Z=c(),U=l("p"),ee=a("Das Modell gibt die endg\xFCltigen Aktivierungen in dem Attribut \u201Clogits\u201D aus. Wenden Sie die Softmax-Funktion auf die \u201Clogits\u201D an, um die Wahrscheinlichkeiten zu erhalten:"),te=c(),v(J.$$.fragment)},l(y){s=o(y,"P",{});var O=f(s);d=i(O,"\u{1F917} Transformers bietet eine einfache und einheitliche Methode zum Laden von vortrainierten Instanzen. Das bedeutet, dass Sie ein "),r=o(O,"CODE",{});var Y=f(r);m=i(Y,"TFAutoModel"),Y.forEach(n),g=i(O," genauso laden k\xF6nnen, wie Sie einen "),b=o(O,"CODE",{});var D=f(b);w=i(D,"AutoTokenizer"),D.forEach(n),I=i(O," laden w\xFCrden. Der einzige Unterschied ist die Auswahl des richtigen "),$=o(O,"CODE",{});var N=f($);C=i(N,"TFAutoModel"),N.forEach(n),F=i(O," f\xFCr die Aufgabe. Da Sie Text - oder Sequenz - Klassifizierung machen, laden Sie "),W=o(O,"CODE",{});var ue=f(W);q=i(ue,"TFAutoModelForSequenceClassification"),ue.forEach(n),B=i(O,":"),O.forEach(n),M=h(y),E(x.$$.fragment,y),_=h(y),E(P.$$.fragment,y),H=h(y),K=o(y,"P",{});var ce=f(K);V=i(ce,"Jetzt k\xF6nnen Sie Ihren vorverarbeiteten Stapel von Eingaben direkt an das Modell \xFCbergeben, indem Sie die W\xF6rterbuchschl\xFCssel direkt an die Tensoren \xFCbergeben:"),ce.forEach(n),G=h(y),E(ne.$$.fragment,y),Z=h(y),U=o(y,"P",{});var fe=f(U);ee=i(fe,"Das Modell gibt die endg\xFCltigen Aktivierungen in dem Attribut \u201Clogits\u201D aus. Wenden Sie die Softmax-Funktion auf die \u201Clogits\u201D an, um die Wahrscheinlichkeiten zu erhalten:"),fe.forEach(n),te=h(y),E(J.$$.fragment,y)},m(y,O){u(y,s,O),t(s,d),t(s,r),t(r,m),t(s,g),t(s,b),t(b,w),t(s,I),t(s,$),t($,C),t(s,F),t(s,W),t(W,q),t(s,B),u(y,M,O),A(x,y,O),u(y,_,O),A(P,y,O),u(y,H,O),u(y,K,O),t(K,V),u(y,G,O),A(ne,y,O),u(y,Z,O),u(y,U,O),t(U,ee),u(y,te,O),A(J,y,O),re=!0},p(y,O){const Y={};O&2&&(Y.$$scope={dirty:O,ctx:y}),P.$set(Y)},i(y){re||(T(x.$$.fragment,y),T(P.$$.fragment,y),T(ne.$$.fragment,y),T(J.$$.fragment,y),re=!0)},o(y){z(x.$$.fragment,y),z(P.$$.fragment,y),z(ne.$$.fragment,y),z(J.$$.fragment,y),re=!1},d(y){y&&n(s),y&&n(M),S(x,y),y&&n(_),S(P,y),y&&n(H),y&&n(K),y&&n(G),S(ne,y),y&&n(Z),y&&n(U),y&&n(te),S(J,y)}}}function Up(j){let s,d;return s=new oe({props:{$$slots:{default:[Hp]},$$scope:{ctx:j}}}),{c(){v(s.$$.fragment)},l(r){E(s.$$.fragment,r)},m(r,m){A(s,r,m),d=!0},p(r,m){const g={};m&2&&(g.$$scope={dirty:m,ctx:r}),s.$set(g)},i(r){d||(T(s.$$.fragment,r),d=!0)},o(r){z(s.$$.fragment,r),d=!1},d(r){S(s,r)}}}function Rp(j){let s,d,r,m,g;return{c(){s=l("p"),d=a("Alle \u{1F917} Transformers-Modelle (PyTorch oder TensorFlow) geben die Tensoren "),r=l("em"),m=a("vor"),g=a(` der endg\xFCltigen Aktivierungsfunktion
Funktion (wie Softmax) aus, da die endg\xFCltige Aktivierungsfunktion oft mit dem Verlusten verschmolzen ist.`)},l(b){s=o(b,"P",{});var w=f(s);d=i(w,"Alle \u{1F917} Transformers-Modelle (PyTorch oder TensorFlow) geben die Tensoren "),r=o(w,"EM",{});var I=f(r);m=i(I,"vor"),I.forEach(n),g=i(w,` der endg\xFCltigen Aktivierungsfunktion
Funktion (wie Softmax) aus, da die endg\xFCltige Aktivierungsfunktion oft mit dem Verlusten verschmolzen ist.`),w.forEach(n)},m(b,w){u(b,s,w),t(s,d),t(s,r),t(r,m),t(s,g)},d(b){b&&n(s)}}}function Gp(j){let s,d;return{c(){s=l("p"),d=a(`Transformers-Modellausgaben sind spezielle Datenklassen, so dass ihre Attribute in einer IDE automatisch vervollst\xE4ndigt werden.
Die Modellausg\xE4nge verhalten sich auch wie ein Tupel oder ein W\xF6rterbuch (z.B. k\xF6nnen Sie mit einem Integer, einem Slice oder einem String indexieren), wobei die Attribute, die \u201CNone\u201D sind, ignoriert werden.`)},l(r){s=o(r,"P",{});var m=f(s);d=i(m,`Transformers-Modellausgaben sind spezielle Datenklassen, so dass ihre Attribute in einer IDE automatisch vervollst\xE4ndigt werden.
Die Modellausg\xE4nge verhalten sich auch wie ein Tupel oder ein W\xF6rterbuch (z.B. k\xF6nnen Sie mit einem Integer, einem Slice oder einem String indexieren), wobei die Attribute, die \u201CNone\u201D sind, ignoriert werden.`),m.forEach(n)},m(r,m){u(r,s,m),t(s,d)},d(r){r&&n(s)}}}function Vp(j){let s,d,r,m,g,b,w,I,$,C,F,W,q,B,M,x;return w=new L({props:{code:`pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)`}}),M=new L({props:{code:'pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)'}}),{c(){s=l("p"),d=a("Sobald Ihr Modell feinabgestimmt ist, k\xF6nnen Sie es mit seinem Tokenizer speichern, indem Sie "),r=l("code"),m=a("PreTrainedModel.save_pretrained()"),g=a(" verwenden:"),b=c(),v(w.$$.fragment),I=c(),$=l("p"),C=a("Wenn Sie bereit sind, das Modell erneut zu verwenden, laden Sie es mit "),F=l("code"),W=a("PreTrainedModel.from_pretrained()"),q=a(":"),B=c(),v(M.$$.fragment)},l(_){s=o(_,"P",{});var P=f(s);d=i(P,"Sobald Ihr Modell feinabgestimmt ist, k\xF6nnen Sie es mit seinem Tokenizer speichern, indem Sie "),r=o(P,"CODE",{});var H=f(r);m=i(H,"PreTrainedModel.save_pretrained()"),H.forEach(n),g=i(P," verwenden:"),P.forEach(n),b=h(_),E(w.$$.fragment,_),I=h(_),$=o(_,"P",{});var K=f($);C=i(K,"Wenn Sie bereit sind, das Modell erneut zu verwenden, laden Sie es mit "),F=o(K,"CODE",{});var V=f(F);W=i(V,"PreTrainedModel.from_pretrained()"),V.forEach(n),q=i(K,":"),K.forEach(n),B=h(_),E(M.$$.fragment,_)},m(_,P){u(_,s,P),t(s,d),t(s,r),t(r,m),t(s,g),u(_,b,P),A(w,_,P),u(_,I,P),u(_,$,P),t($,C),t($,F),t(F,W),t($,q),u(_,B,P),A(M,_,P),x=!0},p:me,i(_){x||(T(w.$$.fragment,_),T(M.$$.fragment,_),x=!0)},o(_){z(w.$$.fragment,_),z(M.$$.fragment,_),x=!1},d(_){_&&n(s),_&&n(b),S(w,_),_&&n(I),_&&n($),_&&n(B),S(M,_)}}}function Zp(j){let s,d;return s=new oe({props:{$$slots:{default:[Vp]},$$scope:{ctx:j}}}),{c(){v(s.$$.fragment)},l(r){E(s.$$.fragment,r)},m(r,m){A(s,r,m),d=!0},p(r,m){const g={};m&2&&(g.$$scope={dirty:m,ctx:r}),s.$set(g)},i(r){d||(T(s.$$.fragment,r),d=!0)},o(r){z(s.$$.fragment,r),d=!1},d(r){S(s,r)}}}function Jp(j){let s,d,r,m,g,b,w,I,$,C,F,W,q,B,M,x;return w=new L({props:{code:`tf_save_directory = "./tf_save_pretrained"
tokenizer.save_pretrained(tf_save_directory)
tf_model.save_pretrained(tf_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`}}),M=new L({props:{code:'tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)'}}),{c(){s=l("p"),d=a("Sobald Ihr Modell feinabgestimmt ist, k\xF6nnen Sie es mit seinem Tokenizer unter Verwendung von "),r=l("code"),m=a("TFPreTrainedModel.save_pretrained()"),g=a(" speichern:"),b=c(),v(w.$$.fragment),I=c(),$=l("p"),C=a("Wenn Sie bereit sind, das Modell wieder zu verwenden, laden Sie es mit "),F=l("code"),W=a("TFPreTrainedModel.from_pretrained()"),q=a(":"),B=c(),v(M.$$.fragment)},l(_){s=o(_,"P",{});var P=f(s);d=i(P,"Sobald Ihr Modell feinabgestimmt ist, k\xF6nnen Sie es mit seinem Tokenizer unter Verwendung von "),r=o(P,"CODE",{});var H=f(r);m=i(H,"TFPreTrainedModel.save_pretrained()"),H.forEach(n),g=i(P," speichern:"),P.forEach(n),b=h(_),E(w.$$.fragment,_),I=h(_),$=o(_,"P",{});var K=f($);C=i(K,"Wenn Sie bereit sind, das Modell wieder zu verwenden, laden Sie es mit "),F=o(K,"CODE",{});var V=f(F);W=i(V,"TFPreTrainedModel.from_pretrained()"),V.forEach(n),q=i(K,":"),K.forEach(n),B=h(_),E(M.$$.fragment,_)},m(_,P){u(_,s,P),t(s,d),t(s,r),t(r,m),t(s,g),u(_,b,P),A(w,_,P),u(_,I,P),u(_,$,P),t($,C),t($,F),t(F,W),t($,q),u(_,B,P),A(M,_,P),x=!0},p:me,i(_){x||(T(w.$$.fragment,_),T(M.$$.fragment,_),x=!0)},o(_){z(w.$$.fragment,_),z(M.$$.fragment,_),x=!1},d(_){_&&n(s),_&&n(b),S(w,_),_&&n(I),_&&n($),_&&n(B),S(M,_)}}}function Yp(j){let s,d;return s=new oe({props:{$$slots:{default:[Jp]},$$scope:{ctx:j}}}),{c(){v(s.$$.fragment)},l(r){E(s.$$.fragment,r)},m(r,m){A(s,r,m),d=!0},p(r,m){const g={};m&2&&(g.$$scope={dirty:m,ctx:r}),s.$set(g)},i(r){d||(T(s.$$.fragment,r),d=!0)},o(r){z(s.$$.fragment,r),d=!1},d(r){S(s,r)}}}function Qp(j){let s,d;return s=new L({props:{code:`from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`}}),{c(){v(s.$$.fragment)},l(r){E(s.$$.fragment,r)},m(r,m){A(s,r,m),d=!0},p:me,i(r){d||(T(s.$$.fragment,r),d=!0)},o(r){z(s.$$.fragment,r),d=!1},d(r){S(s,r)}}}function Xp(j){let s,d;return s=new oe({props:{$$slots:{default:[Qp]},$$scope:{ctx:j}}}),{c(){v(s.$$.fragment)},l(r){E(s.$$.fragment,r)},m(r,m){A(s,r,m),d=!0},p(r,m){const g={};m&2&&(g.$$scope={dirty:m,ctx:r}),s.$set(g)},i(r){d||(T(s.$$.fragment,r),d=!0)},o(r){z(s.$$.fragment,r),d=!1},d(r){S(s,r)}}}function eu(j){let s,d;return s=new L({props:{code:`from transformers import TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`}}),{c(){v(s.$$.fragment)},l(r){E(s.$$.fragment,r)},m(r,m){A(s,r,m),d=!0},p:me,i(r){d||(T(s.$$.fragment,r),d=!0)},o(r){z(s.$$.fragment,r),d=!1},d(r){S(s,r)}}}function tu(j){let s,d;return s=new oe({props:{$$slots:{default:[eu]},$$scope:{ctx:j}}}),{c(){v(s.$$.fragment)},l(r){E(s.$$.fragment,r)},m(r,m){A(s,r,m),d=!0},p(r,m){const g={};m&2&&(g.$$scope={dirty:m,ctx:r}),s.$set(g)},i(r){d||(T(s.$$.fragment,r),d=!0)},o(r){z(s.$$.fragment,r),d=!1},d(r){S(s,r)}}}function nu(j){let s,d,r,m,g,b,w,I;return w=new L({props:{code:`from transformers import AutoModel

my_model = AutoModel.from_config(my_config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>my_model = AutoModel.from_config(my_config)`}}),{c(){s=l("p"),d=a("Create a model from your custom configuration with "),r=l("code"),m=a("AutoModel.from_config()"),g=a(":"),b=c(),v(w.$$.fragment)},l($){s=o($,"P",{});var C=f(s);d=i(C,"Create a model from your custom configuration with "),r=o(C,"CODE",{});var F=f(r);m=i(F,"AutoModel.from_config()"),F.forEach(n),g=i(C,":"),C.forEach(n),b=h($),E(w.$$.fragment,$)},m($,C){u($,s,C),t(s,d),t(s,r),t(r,m),t(s,g),u($,b,C),A(w,$,C),I=!0},p:me,i($){I||(T(w.$$.fragment,$),I=!0)},o($){z(w.$$.fragment,$),I=!1},d($){$&&n(s),$&&n(b),S(w,$)}}}function su(j){let s,d;return s=new oe({props:{$$slots:{default:[nu]},$$scope:{ctx:j}}}),{c(){v(s.$$.fragment)},l(r){E(s.$$.fragment,r)},m(r,m){A(s,r,m),d=!0},p(r,m){const g={};m&2&&(g.$$scope={dirty:m,ctx:r}),s.$set(g)},i(r){d||(T(s.$$.fragment,r),d=!0)},o(r){z(s.$$.fragment,r),d=!1},d(r){S(s,r)}}}function ru(j){let s,d,r,m,g,b,w,I;return w=new L({props:{code:`from transformers import TFAutoModel

my_model = TFAutoModel.from_config(my_config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>my_model = TFAutoModel.from_config(my_config)`}}),{c(){s=l("p"),d=a("Create a model from your custom configuration with "),r=l("code"),m=a("TFAutoModel.from_config()"),g=a(":"),b=c(),v(w.$$.fragment)},l($){s=o($,"P",{});var C=f(s);d=i(C,"Create a model from your custom configuration with "),r=o(C,"CODE",{});var F=f(r);m=i(F,"TFAutoModel.from_config()"),F.forEach(n),g=i(C,":"),C.forEach(n),b=h($),E(w.$$.fragment,$)},m($,C){u($,s,C),t(s,d),t(s,r),t(r,m),t(s,g),u($,b,C),A(w,$,C),I=!0},p:me,i($){I||(T(w.$$.fragment,$),I=!0)},o($){z(w.$$.fragment,$),I=!1},d($){$&&n(s),$&&n(b),S(w,$)}}}function au(j){let s,d;return s=new oe({props:{$$slots:{default:[ru]},$$scope:{ctx:j}}}),{c(){v(s.$$.fragment)},l(r){E(s.$$.fragment,r)},m(r,m){A(s,r,m),d=!0},p(r,m){const g={};m&2&&(g.$$scope={dirty:m,ctx:r}),s.$set(g)},i(r){d||(T(s.$$.fragment,r),d=!0)},o(r){z(s.$$.fragment,r),d=!1},d(r){S(s,r)}}}function iu(j){let s,d,r,m,g,b,w,I,$,C,F,W,q,B,M,x,_,P,H,K,V,G,ne,Z,U,ee,te,J,re,y,O,Y,D,N,ue,ce,fe,dt,pe,Jt,Ce,ka,va,Ws,mt,Tn,Ea,Aa,Ns,R,zn,Ta,za,Sn,Sa,ja,jn,Ma,ya,Mn,Da,Pa,yn,Ca,xa,Dn,Ia,Fa,Pn,qa,Oa,Cn,Wa,Ls,ct,xn,Na,La,Bs,he,In,Ba,Ka,Fn,Ha,Ua,qn,Ra,Ks,ht,On,Ga,Va,Hs,xe,Wn,Za,Ja,Nn,Ya,Us,Ie,Rs,Ae,Fe,Ln,gt,Qa,Bn,Xa,Gs,qe,ei,Kn,ti,ni,Vs,Yt,si,Zs,Oe,Js,We,ri,Hn,ai,ii,Ys,$t,Qs,Ne,li,_t,oi,fi,Xs,bt,er,Le,pi,Un,ui,di,tr,wt,nr,ge,mi,Rn,ci,hi,kt,gi,$i,sr,vt,rr,Be,_i,Gn,bi,wi,ar,Et,ir,$e,ki,At,vi,Ei,Tt,Ai,Ti,lr,zt,or,Ke,zi,Vn,Si,ji,fr,St,pr,Qt,Mi,ur,jt,dr,He,yi,Xt,Di,Pi,mr,Te,Ue,Zn,Mt,Ci,Jn,xi,cr,ae,Ii,Yn,Fi,qi,yt,Oi,Wi,Qn,Ni,Li,Dt,Bi,Ki,hr,Pt,gr,Re,$r,_e,Hi,Xn,Ui,Ri,es,Gi,Vi,_r,Ct,br,be,Zi,en,Ji,Yi,tn,Qi,Xi,wr,ze,Ge,ts,xt,el,ns,tl,kr,It,vr,Q,nl,ss,sl,rl,rs,al,il,as,ll,ol,nn,is,fl,pl,ls,ul,dl,os,ml,cl,Er,we,hl,fs,gl,$l,ps,_l,bl,Ar,Se,Ve,us,Ft,wl,ds,kl,Tr,de,vl,ms,El,Al,sn,Tl,zl,cs,Sl,jl,zr,qt,Sr,Ze,Ml,hs,yl,Dl,jr,rn,Pl,Mr,Ot,yr,an,Cl,Dr,Je,ln,on,xl,Il,Fl,fn,pn,ql,Ol,Pr,Ye,Wl,gs,Nl,Ll,Cr,Qe,xr,Xe,Bl,un,Kl,Hl,Ir,je,et,$s,Wt,Ul,_s,Rl,Fr,tt,qr,nt,Or,X,Gl,Nt,bs,Vl,Zl,Lt,ws,Jl,Yl,ks,Ql,Xl,vs,eo,to,Bt,no,so,dn,ro,ao,Wr,st,Nr,Me,rt,Es,Kt,io,As,lo,Lr,at,Br,mn,oo,Kr,it,Hr,ye,lt,Ts,Ht,fo,zs,po,Ur,cn,uo,Rr,ke,mo,Ss,co,ho,js,go,$o,Gr,Ut,Vr,ot,Zr,ft,_o,hn,bo,wo,Jr,De,pt,Ms,Rt,ko,ys,vo,Yr,gn,Eo,Qr;return b=new Ee({}),F=new Tp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/de/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/de/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/de/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/de/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/de/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/de/tensorflow/quicktour.ipynb"}]}}),G=new An({props:{$$slots:{default:[zp]},$$scope:{ctx:j}}}),te=new Ee({}),fe=new bp({props:{id:"tiZFewofSLM"}}),Ie=new An({props:{$$slots:{default:[Sp]},$$scope:{ctx:j}}}),gt=new Ee({}),Oe=new Zt({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Dp],pytorch:[Mp]},$$scope:{ctx:j}}}),$t=new L({props:{code:`from transformers import pipeline

classifier = pipeline("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),bt=new L({props:{code:'classifier("We are very happy to show you the \u{1F917} Transformers library.")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9998</span>}]`}}),wt=new L({props:{code:`results = classifier(["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">{result[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, with score: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>`}}),vt=new L({props:{code:"pip install datasets ",highlighted:"pip install datasets "}}),Et=new L({props:{code:`import torch
from transformers import pipeline

speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)`}}),zt=new L({props:{code:`from datasets import load_dataset, Audio

dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),St=new L({props:{code:'dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))'}}),jt=new L({props:{code:`result = speech_recognizer(dataset[:4]["audio"])
print([d["text"] for d in result])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>result = speech_recognizer(dataset[:<span class="hljs-number">4</span>][<span class="hljs-string">&quot;audio&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>([d[<span class="hljs-string">&quot;text&quot;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> result])
[<span class="hljs-string">&#x27;I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT&#x27;</span>, <span class="hljs-string">&quot;FODING HOW I&#x27;D SET UP A JOIN TO HET WITH MY WIFE AND WHERE THE AP MIGHT BE&quot;</span>, <span class="hljs-string">&quot;I I&#x27;D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I&#x27;M NOT SEEING THE OPTION TO DO IT ON THE AP SO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AND I&#x27;M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS&quot;</span>, <span class="hljs-string">&#x27;HOW DO I THURN A JOIN A COUNT&#x27;</span>]`}}),Mt=new Ee({}),Pt=new L({props:{code:'model_name = "nlptown/bert-base-multilingual-uncased-sentiment"',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>'}}),Re=new Zt({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ip],pytorch:[Cp]},$$scope:{ctx:j}}}),Ct=new L({props:{code:`classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;5 stars&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.7273</span>}]`}}),xt=new Ee({}),It=new bp({props:{id:"AhChOFRegn4"}}),Ft=new Ee({}),qt=new L({props:{code:`from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),Ot=new L({props:{code:`encoding = tokenizer("We are very happy to show you the \u{1F917} Transformers library.")
print(encoding)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">11312</span>, <span class="hljs-number">10320</span>, <span class="hljs-number">12495</span>, <span class="hljs-number">19308</span>, <span class="hljs-number">10114</span>, <span class="hljs-number">11391</span>, <span class="hljs-number">10855</span>, <span class="hljs-number">10103</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">13299</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Qe=new Zt({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Wp],pytorch:[qp]},$$scope:{ctx:j}}}),Wt=new Ee({}),tt=new Zt({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Up],pytorch:[Bp]},$$scope:{ctx:j}}}),nt=new An({props:{$$slots:{default:[Rp]},$$scope:{ctx:j}}}),st=new An({props:{$$slots:{default:[Gp]},$$scope:{ctx:j}}}),Kt=new Ee({}),at=new Zt({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Yp],pytorch:[Zp]},$$scope:{ctx:j}}}),it=new Zt({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[tu],pytorch:[Xp]},$$scope:{ctx:j}}}),Ht=new Ee({}),Ut=new L({props:{code:`from transformers import AutoConfig

my_config = AutoConfig.from_pretrained("distilbert-base-uncased", n_heads=12)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>my_config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, n_heads=<span class="hljs-number">12</span>)`}}),ot=new Zt({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[au],pytorch:[su]},$$scope:{ctx:j}}}),Rt=new Ee({}),{c(){s=l("meta"),d=c(),r=l("h1"),m=l("a"),g=l("span"),v(b.$$.fragment),w=c(),I=l("span"),$=a("Schnellstart"),C=c(),v(F.$$.fragment),W=c(),q=l("p"),B=a("Mit \u{1F917} Transformers k\xF6nnen Sie sofort loslegen! Verwenden Sie die "),M=l("code"),x=a("pipeline()"),_=a(" f\xFCr schnelle Inferenz und laden Sie schnell ein vortrainiertes Modell und einen Tokenizer mit einer "),P=l("a"),H=a("AutoClass"),K=a(", um Ihre Text-, Bild- oder Audioaufgabe zu l\xF6sen."),V=c(),v(G.$$.fragment),ne=c(),Z=l("h2"),U=l("a"),ee=l("span"),v(te.$$.fragment),J=c(),re=l("span"),y=a("Pipeline"),O=c(),Y=l("p"),D=l("code"),N=a("pipeline()"),ue=a(" ist der einfachste Weg, ein vortrainiertes Modell f\xFCr eine bestimmte Aufgabe zu verwenden."),ce=c(),v(fe.$$.fragment),dt=c(),pe=l("p"),Jt=a("Die "),Ce=l("code"),ka=a("pipeline()"),va=a(" unterst\xFCtzt viele g\xE4ngige Aufgaben:"),Ws=c(),mt=l("p"),Tn=l("strong"),Ea=a("Text"),Aa=a(":"),Ns=c(),R=l("ul"),zn=l("li"),Ta=a("Stimmungsanalyse: Klassifizierung der Polarit\xE4t eines gegebenen Textes."),za=c(),Sn=l("li"),Sa=a("Textgenerierung (auf Englisch): Generierung von Text aus einer gegebenen Eingabe."),ja=c(),jn=l("li"),Ma=a("Name-Entity-Recognition (NER): Kennzeichnung jedes Worts mit der Entit\xE4t, die es repr\xE4sentiert (Person, Datum, Ort usw.)."),ya=c(),Mn=l("li"),Da=a("Beantwortung von Fragen: Extrahieren der Antwort aus dem Kontext, wenn ein gewisser Kontext und eine Frage gegeben sind."),Pa=c(),yn=l("li"),Ca=a("Fill-mask: Ausf\xFCllen von L\xFCcken in einem Text mit maskierten W\xF6rtern."),xa=c(),Dn=l("li"),Ia=a("Zusammenfassung: Erstellung einer Zusammenfassung einer langen Text- oder Dokumentensequenz."),Fa=c(),Pn=l("li"),qa=a("\xDCbersetzung: \xDCbersetzen eines Textes in eine andere Sprache."),Oa=c(),Cn=l("li"),Wa=a("Merkmalsextraktion: Erstellen einer Tensordarstellung des Textes."),Ls=c(),ct=l("p"),xn=l("strong"),Na=a("Bild"),La=a(":"),Bs=c(),he=l("ul"),In=l("li"),Ba=a("Bildklassifizierung: Klassifizierung eines Bildes."),Ka=c(),Fn=l("li"),Ha=a("Bildsegmentierung: Klassifizierung jedes Pixels in einem Bild."),Ua=c(),qn=l("li"),Ra=a("Objekterkennung: Erkennen von Objekten innerhalb eines Bildes."),Ks=c(),ht=l("p"),On=l("strong"),Ga=a("Audio"),Va=a(":"),Hs=c(),xe=l("ul"),Wn=l("li"),Za=a("Audioklassifizierung: Zuweisung eines Labels zu einem bestimmten Audiosegment."),Ja=c(),Nn=l("li"),Ya=a("Automatische Spracherkennung (ASR): Transkription von Audiodaten in Text."),Us=c(),v(Ie.$$.fragment),Rs=c(),Ae=l("h3"),Fe=l("a"),Ln=l("span"),v(gt.$$.fragment),Qa=c(),Bn=l("span"),Xa=a("Verwendung der Pipeline"),Gs=c(),qe=l("p"),ei=a("Im folgenden Beispiel werden Sie die "),Kn=l("code"),ti=a("pipeline()"),ni=a(" f\xFCr die Stimmungsanalyse verwenden."),Vs=c(),Yt=l("p"),si=a("Installieren Sie die folgenden Abh\xE4ngigkeiten, falls Sie dies nicht bereits getan haben:"),Zs=c(),v(Oe.$$.fragment),Js=c(),We=l("p"),ri=a("Importieren sie die "),Hn=l("code"),ai=a("pipeline()"),ii=a(" und spezifizieren sie die Aufgabe, welche sie l\xF6sen m\xF6chten:"),Ys=c(),v($t.$$.fragment),Qs=c(),Ne=l("p"),li=a("Die Pipeline l\xE4dt ein standardm\xE4\xDFiges [vortrainiertes Modell] ("),_t=l("a"),oi=a("https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),fi=a(") und einen Tokenizer f\xFCr die Stimmungs-Analyse herunter und speichert sie. Jetzt k\xF6nnen Sie den \u201CKlassifikator\u201D auf Ihren Zieltext anwenden:"),Xs=c(),v(bt.$$.fragment),er=c(),Le=l("p"),pi=a("For more than one sentence, pass a list of sentences to the "),Un=l("code"),ui=a("pipeline()"),di=a(" which returns a list of dictionaries:"),tr=c(),v(wt.$$.fragment),nr=c(),ge=l("p"),mi=a("Die "),Rn=l("code"),ci=a("pipeline()"),hi=a(" kann auch \xFCber einen ganzen Datensatz iterieren. Starten wir mit der Installation der "),kt=l("a"),gi=a("\u{1F917} Datasets"),$i=a(" Bibliothek:"),sr=c(),v(vt.$$.fragment),rr=c(),Be=l("p"),_i=a("Erstellen wir eine "),Gn=l("code"),bi=a("pipeline()"),wi=a(" mit der Aufgabe die wir l\xF6sen und dem Modell welches wir nutzen m\xF6chten."),ar=c(),v(Et.$$.fragment),ir=c(),$e=l("p"),ki=a("Als n\xE4chstes laden wir den Datensatz (siehe \u{1F917} Datasets "),At=l("a"),vi=a("Quick Start"),Ei=a(" f\xFCr mehr Details) welches wir nutzen m\xF6chten. Zum Beispiel laden wir den "),Tt=l("a"),Ai=a("MInDS-14"),Ti=a(" Datensatz:"),lr=c(),v(zt.$$.fragment),or=c(),Ke=l("p"),zi=a("Wir m\xFCssen sicherstellen, dass die Abtastrate des Datensatzes der Abtastrate entspricht, mit der "),Vn=l("code"),Si=a("facebook/wav2vec2-base-960h"),ji=a(" trainiert wurde."),fr=c(),v(St.$$.fragment),pr=c(),Qt=l("p"),Mi=a(`Audiodateien werden automatisch geladen und neu abgetastet, wenn die Spalte \u201Caudio\u201D aufgerufen wird.
Extrahieren wir die rohen Wellenform-Arrays der ersten 4 Beispiele und \xFCbergeben wir sie als Liste an die Pipeline:`),ur=c(),v(jt.$$.fragment),dr=c(),He=l("p"),yi=a("Bei einem gr\xF6\xDFeren Datensatz mit vielen Eingaben (wie bei Sprache oder Bildverarbeitung) sollten Sie einen Generator anstelle einer Liste \xFCbergeben, der alle Eingaben in den Speicher l\xE4dt. Weitere Informationen finden Sie in der "),Xt=l("a"),Di=a("Pipeline-Dokumentation"),Pi=a("."),mr=c(),Te=l("h3"),Ue=l("a"),Zn=l("span"),v(Mt.$$.fragment),Ci=c(),Jn=l("span"),xi=a("Ein anderes Modell und einen anderen Tokenizer in der Pipeline verwenden"),cr=c(),ae=l("p"),Ii=a("Die "),Yn=l("code"),Fi=a("pipeline()"),qi=a(" kann jedes Modell aus dem [Model Hub] ("),yt=l("a"),Oi=a("https://huggingface.co/models"),Wi=a(") verwenden, wodurch es einfach ist, die "),Qn=l("code"),Ni=a("pipeline()"),Li=a(" f\xFCr andere Anwendungsf\xE4lle anzupassen. Wenn Sie beispielsweise ein Modell w\xFCnschen, das franz\xF6sischen Text verarbeiten kann, verwenden Sie die Tags im Model Hub, um nach einem geeigneten Modell zu filtern. Das oberste gefilterte Ergebnis liefert ein mehrsprachiges "),Dt=l("a"),Bi=a("BERT-Modell"),Ki=a(", das auf die Stimmungsanalyse abgestimmt ist. Gro\xDFartig, verwenden wir dieses Modell!"),hr=c(),v(Pt.$$.fragment),gr=c(),v(Re.$$.fragment),$r=c(),_e=l("p"),Hi=a("Dann k\xF6nnen Sie das Modell und den Tokenizer in der "),Xn=l("code"),Ui=a("pipeline()"),Ri=a(" angeben und den "),es=l("code"),Gi=a("Klassifikator"),Vi=a(" auf Ihren Zieltext anwenden:"),_r=c(),v(Ct.$$.fragment),br=c(),be=l("p"),Zi=a("Wenn Sie kein Modell f\xFCr Ihren Anwendungsfall finden k\xF6nnen, m\xFCssen Sie ein vortrainiertes Modell auf Ihren Daten feinabstimmen. Schauen Sie sich unser "),en=l("a"),Ji=a("Feinabstimmungs-Tutorial"),Yi=a(" an, um zu erfahren, wie das geht. Und schlie\xDFlich, nachdem Sie Ihr trainiertes Modell verfeinert haben, sollten Sie es mit der Community im Model Hub teilen (siehe Tutorial "),tn=l("a"),Qi=a("hier"),Xi=a("), um NLP f\xFCr alle zu demokratisieren! \u{1F917}"),wr=c(),ze=l("h2"),Ge=l("a"),ts=l("span"),v(xt.$$.fragment),el=c(),ns=l("span"),tl=a("AutoClass"),kr=c(),v(It.$$.fragment),vr=c(),Q=l("p"),nl=a("Unter der Haube arbeiten die Klassen "),ss=l("code"),sl=a("AutoModelForSequenceClassification"),rl=a(" und "),rs=l("code"),al=a("AutoTokenizer"),il=a(" zusammen, um die "),as=l("code"),ll=a("pipeline()"),ol=a(" zu betreiben. Eine "),nn=l("a"),is=l("code"),fl=a("AutoClass"),pl=a(" ist eine Abk\xFCrzung, die automatisch die Architektur eines trainierten Modells aus dessen Namen oder Pfad abruft. Sie m\xFCssen nur die passende "),ls=l("code"),ul=a("AutoClass"),dl=a(" f\xFCr Ihre Aufgabe und den zugeh\xF6rigen Tokenizer mit "),os=l("code"),ml=a("AutoTokenizer"),cl=a(" ausw\xE4hlen."),Er=c(),we=l("p"),hl=a("Kehren wir zu unserem Beispiel zur\xFCck und sehen wir uns an, wie Sie die "),fs=l("code"),gl=a("AutoClass"),$l=a(" verwenden k\xF6nnen, um die Ergebnisse der "),ps=l("code"),_l=a("pipeline()"),bl=a(" zu replizieren."),Ar=c(),Se=l("h3"),Ve=l("a"),us=l("span"),v(Ft.$$.fragment),wl=c(),ds=l("span"),kl=a("AutoTokenizer"),Tr=c(),de=l("p"),vl=a("Ein Tokenizer ist f\xFCr die Vorverarbeitung von Text in ein f\xFCr das Modell verst\xE4ndliches Format zust\xE4ndig. Zun\xE4chst zerlegt der Tokenisierer den Text in W\xF6rter, die "),ms=l("em"),El=a("Token"),Al=a(" genannt werden. Es gibt mehrere Regeln f\xFCr den Tokenisierungsprozess, z. B. wie und auf welcher Ebene ein Wort aufgespalten wird (weitere Informationen \xFCber Tokenisierung "),sn=l("a"),Tl=a("hier"),zl=a(`). Das Wichtigste ist jedoch, dass Sie den Tokenizer mit demselben Modellnamen instanziieren m\xFCssen, um sicherzustellen, dass Sie dieselben Tokenisierungsregeln verwenden, mit denen ein Modell zuvor trainiert wurde.
Laden sie einen Tokenizer mit `),cs=l("code"),Sl=a("AutoTokenizer"),jl=a(":"),zr=c(),v(qt.$$.fragment),Sr=c(),Ze=l("p"),Ml=a("Anschlie\xDFend wandelt der Tokenizer die Token in Zahlen um, um einen Tensor als Eingabe f\xFCr das Modell zu konstruieren. Dieser wird als "),hs=l("em"),yl=a("Vokabular"),Dl=a(" des Modells bezeichnet."),jr=c(),rn=l("p"),Pl=a("\xDCbergeben Sie Ihren Text an den Tokenizer:"),Mr=c(),v(Ot.$$.fragment),yr=c(),an=l("p"),Cl=a("Der Tokenizer gibt ein W\xF6rterbuch zur\xFCck, das Folgendes enth\xE4lt:"),Dr=c(),Je=l("ul"),ln=l("li"),on=l("a"),xl=a("input_ids"),Il=a(": numerische Repr\xE4sentationen Ihrer Token."),Fl=c(),fn=l("li"),pn=l("a"),ql=a("atttention_mask"),Ol=a(": gibt an, welche Token beachtet werden sollen."),Pr=c(),Ye=l("p"),Wl=a("Genau wie die "),gs=l("code"),Nl=a("pipeline()"),Ll=a(" akzeptiert der Tokenizer eine Liste von Eingaben. Dar\xFCber hinaus kann der Tokenizer den Text auch auff\xFCllen und k\xFCrzen, um einen Stapel mit einheitlicher L\xE4nge zur\xFCckzugeben:"),Cr=c(),v(Qe.$$.fragment),xr=c(),Xe=l("p"),Bl=a("Lesen Sie das Tutorial "),un=l("a"),Kl=a("preprocessing"),Hl=a(" f\xFCr weitere Details zur Tokenisierung."),Ir=c(),je=l("h3"),et=l("a"),$s=l("span"),v(Wt.$$.fragment),Ul=c(),_s=l("span"),Rl=a("AutoModel"),Fr=c(),v(tt.$$.fragment),qr=c(),v(nt.$$.fragment),Or=c(),X=l("p"),Gl=a("Modelle sind ein standardm\xE4\xDFiges "),Nt=l("a"),bs=l("code"),Vl=a("torch.nn.Module"),Zl=a(" oder ein "),Lt=l("a"),ws=l("code"),Jl=a("tf.keras.Model"),Yl=a(", sodass Sie sie in Ihrer \xFCblichen Trainingsschleife verwenden k\xF6nnen. Um jedoch die Dinge einfacher zu machen, bietet \u{1F917} Transformers eine "),ks=l("code"),Ql=a("Trainer"),Xl=a("-Klasse f\xFCr PyTorch, die Funktionalit\xE4t f\xFCr verteiltes Training, gemischte Pr\xE4zision und mehr bietet. F\xFCr TensorFlow k\xF6nnen Sie die Methode "),vs=l("code"),eo=a("fit"),to=a(" aus "),Bt=l("a"),no=a("Keras"),so=a(" verwenden. Siehe das "),dn=l("a"),ro=a("training tutorial"),ao=a(" f\xFCr weitere Details."),Wr=c(),v(st.$$.fragment),Nr=c(),Me=l("h3"),rt=l("a"),Es=l("span"),v(Kt.$$.fragment),io=c(),As=l("span"),lo=a("Modell speichern"),Lr=c(),v(at.$$.fragment),Br=c(),mn=l("p"),oo=a("Ein besonders cooles \u{1F917} Transformers-Feature ist die M\xF6glichkeit, ein Modell zu speichern und es entweder als PyTorch- oder TensorFlow-Modell wieder zu laden. Der Parameter \u201Cfrom_pt\u201D oder \u201Cfrom_tf\u201D kann das Modell von einem Framework in das andere konvertieren:"),Kr=c(),v(it.$$.fragment),Hr=c(),ye=l("h2"),lt=l("a"),Ts=l("span"),v(Ht.$$.fragment),fo=c(),zs=l("span"),po=a("Custom model builds"),Ur=c(),cn=l("p"),uo=a("Sie k\xF6nnen die Konfigurationsklasse des Modells \xE4ndern, um zu bestimmen, wie ein Modell aufgebaut ist. Die Konfiguration legt die Attribute eines Modells fest, z. B. die Anzahl der verborgenen Schichten oder der Aufmerksamkeitsk\xF6pfe. Wenn Sie ein Modell aus einer benutzerdefinierten Konfigurationsklasse initialisieren, beginnen Sie bei Null. Die Modellattribute werden zuf\xE4llig initialisiert, und Sie m\xFCssen das Modell trainieren, bevor Sie es verwenden k\xF6nnen, um aussagekr\xE4ftige Ergebnisse zu erhalten."),Rr=c(),ke=l("p"),mo=a("Beginnen Sie mit dem Import von "),Ss=l("code"),co=a("AutoConfig"),ho=a(" und laden Sie dann das trainierte Modell, das Sie \xE4ndern m\xF6chten. Innerhalb von "),js=l("code"),go=a("AutoConfig.from_pretrained()"),$o=a(" k\xF6nnen Sie das Attribut angeben, das Sie \xE4ndern m\xF6chten, z. B. die Anzahl der Aufmerksamkeitsk\xF6pfe:"),Gr=c(),v(Ut.$$.fragment),Vr=c(),v(ot.$$.fragment),Zr=c(),ft=l("p"),_o=a("Weitere Informationen zur Erstellung von benutzerdefinierten Konfigurationen finden Sie in der Anleitung "),hn=l("a"),bo=a("Erstellen einer benutzerdefinierten Architektur"),wo=a("."),Jr=c(),De=l("h2"),pt=l("a"),Ms=l("span"),v(Rt.$$.fragment),ko=c(),ys=l("span"),vo=a("Wie geht es weiter?"),Yr=c(),gn=l("p"),Eo=a("Nachdem Sie nun die \u{1F917} Transformers-Kurztour abgeschlossen haben, schauen Sie sich unsere Anleitungen an und erfahren Sie, wie Sie spezifischere Dinge tun k\xF6nnen, wie das Schreiben eines benutzerdefinierten Modells, die Feinabstimmung eines Modells f\xFCr eine Aufgabe und wie man ein Modell mit einem Skript trainiert. Wenn Sie mehr \xFCber die Kernkonzepte von \u{1F917} Transformers erfahren m\xF6chten, nehmen Sie sich eine Tasse Kaffee und werfen Sie einen Blick auf unsere konzeptionellen Leitf\xE4den!"),this.h()},l(e){const p=Ep('[data-svelte="svelte-1phssyn"]',document.head);s=o(p,"META",{name:!0,content:!0}),p.forEach(n),d=h(e),r=o(e,"H1",{class:!0});var Gt=f(r);m=o(Gt,"A",{id:!0,class:!0,href:!0});var Ds=f(m);g=o(Ds,"SPAN",{});var Ps=f(g);E(b.$$.fragment,Ps),Ps.forEach(n),Ds.forEach(n),w=h(Gt),I=o(Gt,"SPAN",{});var Cs=f(I);$=i(Cs,"Schnellstart"),Cs.forEach(n),Gt.forEach(n),C=h(e),E(F.$$.fragment,e),W=h(e),q=o(e,"P",{});var Pe=f(q);B=i(Pe,"Mit \u{1F917} Transformers k\xF6nnen Sie sofort loslegen! Verwenden Sie die "),M=o(Pe,"CODE",{});var xs=f(M);x=i(xs,"pipeline()"),xs.forEach(n),_=i(Pe," f\xFCr schnelle Inferenz und laden Sie schnell ein vortrainiertes Modell und einen Tokenizer mit einer "),P=o(Pe,"A",{href:!0});var Is=f(P);H=i(Is,"AutoClass"),Is.forEach(n),K=i(Pe,", um Ihre Text-, Bild- oder Audioaufgabe zu l\xF6sen."),Pe.forEach(n),V=h(e),E(G.$$.fragment,e),ne=h(e),Z=o(e,"H2",{class:!0});var Vt=f(Z);U=o(Vt,"A",{id:!0,class:!0,href:!0});var Fs=f(U);ee=o(Fs,"SPAN",{});var qs=f(ee);E(te.$$.fragment,qs),qs.forEach(n),Fs.forEach(n),J=h(Vt),re=o(Vt,"SPAN",{});var Os=f(re);y=i(Os,"Pipeline"),Os.forEach(n),Vt.forEach(n),O=h(e),Y=o(e,"P",{});var Ao=f(Y);D=o(Ao,"CODE",{});var yo=f(D);N=i(yo,"pipeline()"),yo.forEach(n),ue=i(Ao," ist der einfachste Weg, ein vortrainiertes Modell f\xFCr eine bestimmte Aufgabe zu verwenden."),Ao.forEach(n),ce=h(e),E(fe.$$.fragment,e),dt=h(e),pe=o(e,"P",{});var Xr=f(pe);Jt=i(Xr,"Die "),Ce=o(Xr,"CODE",{});var Do=f(Ce);ka=i(Do,"pipeline()"),Do.forEach(n),va=i(Xr," unterst\xFCtzt viele g\xE4ngige Aufgaben:"),Xr.forEach(n),Ws=h(e),mt=o(e,"P",{});var To=f(mt);Tn=o(To,"STRONG",{});var Po=f(Tn);Ea=i(Po,"Text"),Po.forEach(n),Aa=i(To,":"),To.forEach(n),Ns=h(e),R=o(e,"UL",{});var se=f(R);zn=o(se,"LI",{});var Co=f(zn);Ta=i(Co,"Stimmungsanalyse: Klassifizierung der Polarit\xE4t eines gegebenen Textes."),Co.forEach(n),za=h(se),Sn=o(se,"LI",{});var xo=f(Sn);Sa=i(xo,"Textgenerierung (auf Englisch): Generierung von Text aus einer gegebenen Eingabe."),xo.forEach(n),ja=h(se),jn=o(se,"LI",{});var Io=f(jn);Ma=i(Io,"Name-Entity-Recognition (NER): Kennzeichnung jedes Worts mit der Entit\xE4t, die es repr\xE4sentiert (Person, Datum, Ort usw.)."),Io.forEach(n),ya=h(se),Mn=o(se,"LI",{});var Fo=f(Mn);Da=i(Fo,"Beantwortung von Fragen: Extrahieren der Antwort aus dem Kontext, wenn ein gewisser Kontext und eine Frage gegeben sind."),Fo.forEach(n),Pa=h(se),yn=o(se,"LI",{});var qo=f(yn);Ca=i(qo,"Fill-mask: Ausf\xFCllen von L\xFCcken in einem Text mit maskierten W\xF6rtern."),qo.forEach(n),xa=h(se),Dn=o(se,"LI",{});var Oo=f(Dn);Ia=i(Oo,"Zusammenfassung: Erstellung einer Zusammenfassung einer langen Text- oder Dokumentensequenz."),Oo.forEach(n),Fa=h(se),Pn=o(se,"LI",{});var Wo=f(Pn);qa=i(Wo,"\xDCbersetzung: \xDCbersetzen eines Textes in eine andere Sprache."),Wo.forEach(n),Oa=h(se),Cn=o(se,"LI",{});var No=f(Cn);Wa=i(No,"Merkmalsextraktion: Erstellen einer Tensordarstellung des Textes."),No.forEach(n),se.forEach(n),Ls=h(e),ct=o(e,"P",{});var zo=f(ct);xn=o(zo,"STRONG",{});var Lo=f(xn);Na=i(Lo,"Bild"),Lo.forEach(n),La=i(zo,":"),zo.forEach(n),Bs=h(e),he=o(e,"UL",{});var $n=f(he);In=o($n,"LI",{});var Bo=f(In);Ba=i(Bo,"Bildklassifizierung: Klassifizierung eines Bildes."),Bo.forEach(n),Ka=h($n),Fn=o($n,"LI",{});var Ko=f(Fn);Ha=i(Ko,"Bildsegmentierung: Klassifizierung jedes Pixels in einem Bild."),Ko.forEach(n),Ua=h($n),qn=o($n,"LI",{});var Ho=f(qn);Ra=i(Ho,"Objekterkennung: Erkennen von Objekten innerhalb eines Bildes."),Ho.forEach(n),$n.forEach(n),Ks=h(e),ht=o(e,"P",{});var So=f(ht);On=o(So,"STRONG",{});var Uo=f(On);Ga=i(Uo,"Audio"),Uo.forEach(n),Va=i(So,":"),So.forEach(n),Hs=h(e),xe=o(e,"UL",{});var ea=f(xe);Wn=o(ea,"LI",{});var Ro=f(Wn);Za=i(Ro,"Audioklassifizierung: Zuweisung eines Labels zu einem bestimmten Audiosegment."),Ro.forEach(n),Ja=h(ea),Nn=o(ea,"LI",{});var Go=f(Nn);Ya=i(Go,"Automatische Spracherkennung (ASR): Transkription von Audiodaten in Text."),Go.forEach(n),ea.forEach(n),Us=h(e),E(Ie.$$.fragment,e),Rs=h(e),Ae=o(e,"H3",{class:!0});var ta=f(Ae);Fe=o(ta,"A",{id:!0,class:!0,href:!0});var Vo=f(Fe);Ln=o(Vo,"SPAN",{});var Zo=f(Ln);E(gt.$$.fragment,Zo),Zo.forEach(n),Vo.forEach(n),Qa=h(ta),Bn=o(ta,"SPAN",{});var Jo=f(Bn);Xa=i(Jo,"Verwendung der Pipeline"),Jo.forEach(n),ta.forEach(n),Gs=h(e),qe=o(e,"P",{});var na=f(qe);ei=i(na,"Im folgenden Beispiel werden Sie die "),Kn=o(na,"CODE",{});var Yo=f(Kn);ti=i(Yo,"pipeline()"),Yo.forEach(n),ni=i(na," f\xFCr die Stimmungsanalyse verwenden."),na.forEach(n),Vs=h(e),Yt=o(e,"P",{});var Qo=f(Yt);si=i(Qo,"Installieren Sie die folgenden Abh\xE4ngigkeiten, falls Sie dies nicht bereits getan haben:"),Qo.forEach(n),Zs=h(e),E(Oe.$$.fragment,e),Js=h(e),We=o(e,"P",{});var sa=f(We);ri=i(sa,"Importieren sie die "),Hn=o(sa,"CODE",{});var Xo=f(Hn);ai=i(Xo,"pipeline()"),Xo.forEach(n),ii=i(sa," und spezifizieren sie die Aufgabe, welche sie l\xF6sen m\xF6chten:"),sa.forEach(n),Ys=h(e),E($t.$$.fragment,e),Qs=h(e),Ne=o(e,"P",{});var ra=f(Ne);li=i(ra,"Die Pipeline l\xE4dt ein standardm\xE4\xDFiges [vortrainiertes Modell] ("),_t=o(ra,"A",{href:!0,rel:!0});var ef=f(_t);oi=i(ef,"https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),ef.forEach(n),fi=i(ra,") und einen Tokenizer f\xFCr die Stimmungs-Analyse herunter und speichert sie. Jetzt k\xF6nnen Sie den \u201CKlassifikator\u201D auf Ihren Zieltext anwenden:"),ra.forEach(n),Xs=h(e),E(bt.$$.fragment,e),er=h(e),Le=o(e,"P",{});var aa=f(Le);pi=i(aa,"For more than one sentence, pass a list of sentences to the "),Un=o(aa,"CODE",{});var tf=f(Un);ui=i(tf,"pipeline()"),tf.forEach(n),di=i(aa," which returns a list of dictionaries:"),aa.forEach(n),tr=h(e),E(wt.$$.fragment,e),nr=h(e),ge=o(e,"P",{});var _n=f(ge);mi=i(_n,"Die "),Rn=o(_n,"CODE",{});var nf=f(Rn);ci=i(nf,"pipeline()"),nf.forEach(n),hi=i(_n," kann auch \xFCber einen ganzen Datensatz iterieren. Starten wir mit der Installation der "),kt=o(_n,"A",{href:!0,rel:!0});var sf=f(kt);gi=i(sf,"\u{1F917} Datasets"),sf.forEach(n),$i=i(_n," Bibliothek:"),_n.forEach(n),sr=h(e),E(vt.$$.fragment,e),rr=h(e),Be=o(e,"P",{});var ia=f(Be);_i=i(ia,"Erstellen wir eine "),Gn=o(ia,"CODE",{});var rf=f(Gn);bi=i(rf,"pipeline()"),rf.forEach(n),wi=i(ia," mit der Aufgabe die wir l\xF6sen und dem Modell welches wir nutzen m\xF6chten."),ia.forEach(n),ar=h(e),E(Et.$$.fragment,e),ir=h(e),$e=o(e,"P",{});var bn=f($e);ki=i(bn,"Als n\xE4chstes laden wir den Datensatz (siehe \u{1F917} Datasets "),At=o(bn,"A",{href:!0,rel:!0});var af=f(At);vi=i(af,"Quick Start"),af.forEach(n),Ei=i(bn," f\xFCr mehr Details) welches wir nutzen m\xF6chten. Zum Beispiel laden wir den "),Tt=o(bn,"A",{href:!0,rel:!0});var lf=f(Tt);Ai=i(lf,"MInDS-14"),lf.forEach(n),Ti=i(bn," Datensatz:"),bn.forEach(n),lr=h(e),E(zt.$$.fragment,e),or=h(e),Ke=o(e,"P",{});var la=f(Ke);zi=i(la,"Wir m\xFCssen sicherstellen, dass die Abtastrate des Datensatzes der Abtastrate entspricht, mit der "),Vn=o(la,"CODE",{});var of=f(Vn);Si=i(of,"facebook/wav2vec2-base-960h"),of.forEach(n),ji=i(la," trainiert wurde."),la.forEach(n),fr=h(e),E(St.$$.fragment,e),pr=h(e),Qt=o(e,"P",{});var ff=f(Qt);Mi=i(ff,`Audiodateien werden automatisch geladen und neu abgetastet, wenn die Spalte \u201Caudio\u201D aufgerufen wird.
Extrahieren wir die rohen Wellenform-Arrays der ersten 4 Beispiele und \xFCbergeben wir sie als Liste an die Pipeline:`),ff.forEach(n),ur=h(e),E(jt.$$.fragment,e),dr=h(e),He=o(e,"P",{});var oa=f(He);yi=i(oa,"Bei einem gr\xF6\xDFeren Datensatz mit vielen Eingaben (wie bei Sprache oder Bildverarbeitung) sollten Sie einen Generator anstelle einer Liste \xFCbergeben, der alle Eingaben in den Speicher l\xE4dt. Weitere Informationen finden Sie in der "),Xt=o(oa,"A",{href:!0});var pf=f(Xt);Di=i(pf,"Pipeline-Dokumentation"),pf.forEach(n),Pi=i(oa,"."),oa.forEach(n),mr=h(e),Te=o(e,"H3",{class:!0});var fa=f(Te);Ue=o(fa,"A",{id:!0,class:!0,href:!0});var uf=f(Ue);Zn=o(uf,"SPAN",{});var df=f(Zn);E(Mt.$$.fragment,df),df.forEach(n),uf.forEach(n),Ci=h(fa),Jn=o(fa,"SPAN",{});var mf=f(Jn);xi=i(mf,"Ein anderes Modell und einen anderen Tokenizer in der Pipeline verwenden"),mf.forEach(n),fa.forEach(n),cr=h(e),ae=o(e,"P",{});var ve=f(ae);Ii=i(ve,"Die "),Yn=o(ve,"CODE",{});var cf=f(Yn);Fi=i(cf,"pipeline()"),cf.forEach(n),qi=i(ve," kann jedes Modell aus dem [Model Hub] ("),yt=o(ve,"A",{href:!0,rel:!0});var hf=f(yt);Oi=i(hf,"https://huggingface.co/models"),hf.forEach(n),Wi=i(ve,") verwenden, wodurch es einfach ist, die "),Qn=o(ve,"CODE",{});var gf=f(Qn);Ni=i(gf,"pipeline()"),gf.forEach(n),Li=i(ve," f\xFCr andere Anwendungsf\xE4lle anzupassen. Wenn Sie beispielsweise ein Modell w\xFCnschen, das franz\xF6sischen Text verarbeiten kann, verwenden Sie die Tags im Model Hub, um nach einem geeigneten Modell zu filtern. Das oberste gefilterte Ergebnis liefert ein mehrsprachiges "),Dt=o(ve,"A",{href:!0,rel:!0});var $f=f(Dt);Bi=i($f,"BERT-Modell"),$f.forEach(n),Ki=i(ve,", das auf die Stimmungsanalyse abgestimmt ist. Gro\xDFartig, verwenden wir dieses Modell!"),ve.forEach(n),hr=h(e),E(Pt.$$.fragment,e),gr=h(e),E(Re.$$.fragment,e),$r=h(e),_e=o(e,"P",{});var wn=f(_e);Hi=i(wn,"Dann k\xF6nnen Sie das Modell und den Tokenizer in der "),Xn=o(wn,"CODE",{});var _f=f(Xn);Ui=i(_f,"pipeline()"),_f.forEach(n),Ri=i(wn," angeben und den "),es=o(wn,"CODE",{});var bf=f(es);Gi=i(bf,"Klassifikator"),bf.forEach(n),Vi=i(wn," auf Ihren Zieltext anwenden:"),wn.forEach(n),_r=h(e),E(Ct.$$.fragment,e),br=h(e),be=o(e,"P",{});var kn=f(be);Zi=i(kn,"Wenn Sie kein Modell f\xFCr Ihren Anwendungsfall finden k\xF6nnen, m\xFCssen Sie ein vortrainiertes Modell auf Ihren Daten feinabstimmen. Schauen Sie sich unser "),en=o(kn,"A",{href:!0});var wf=f(en);Ji=i(wf,"Feinabstimmungs-Tutorial"),wf.forEach(n),Yi=i(kn," an, um zu erfahren, wie das geht. Und schlie\xDFlich, nachdem Sie Ihr trainiertes Modell verfeinert haben, sollten Sie es mit der Community im Model Hub teilen (siehe Tutorial "),tn=o(kn,"A",{href:!0});var kf=f(tn);Qi=i(kf,"hier"),kf.forEach(n),Xi=i(kn,"), um NLP f\xFCr alle zu demokratisieren! \u{1F917}"),kn.forEach(n),wr=h(e),ze=o(e,"H2",{class:!0});var pa=f(ze);Ge=o(pa,"A",{id:!0,class:!0,href:!0});var vf=f(Ge);ts=o(vf,"SPAN",{});var Ef=f(ts);E(xt.$$.fragment,Ef),Ef.forEach(n),vf.forEach(n),el=h(pa),ns=o(pa,"SPAN",{});var Af=f(ns);tl=i(Af,"AutoClass"),Af.forEach(n),pa.forEach(n),kr=h(e),E(It.$$.fragment,e),vr=h(e),Q=o(e,"P",{});var ie=f(Q);nl=i(ie,"Unter der Haube arbeiten die Klassen "),ss=o(ie,"CODE",{});var Tf=f(ss);sl=i(Tf,"AutoModelForSequenceClassification"),Tf.forEach(n),rl=i(ie," und "),rs=o(ie,"CODE",{});var zf=f(rs);al=i(zf,"AutoTokenizer"),zf.forEach(n),il=i(ie," zusammen, um die "),as=o(ie,"CODE",{});var Sf=f(as);ll=i(Sf,"pipeline()"),Sf.forEach(n),ol=i(ie," zu betreiben. Eine "),nn=o(ie,"A",{href:!0});var jf=f(nn);is=o(jf,"CODE",{});var Mf=f(is);fl=i(Mf,"AutoClass"),Mf.forEach(n),jf.forEach(n),pl=i(ie," ist eine Abk\xFCrzung, die automatisch die Architektur eines trainierten Modells aus dessen Namen oder Pfad abruft. Sie m\xFCssen nur die passende "),ls=o(ie,"CODE",{});var yf=f(ls);ul=i(yf,"AutoClass"),yf.forEach(n),dl=i(ie," f\xFCr Ihre Aufgabe und den zugeh\xF6rigen Tokenizer mit "),os=o(ie,"CODE",{});var Df=f(os);ml=i(Df,"AutoTokenizer"),Df.forEach(n),cl=i(ie," ausw\xE4hlen."),ie.forEach(n),Er=h(e),we=o(e,"P",{});var vn=f(we);hl=i(vn,"Kehren wir zu unserem Beispiel zur\xFCck und sehen wir uns an, wie Sie die "),fs=o(vn,"CODE",{});var Pf=f(fs);gl=i(Pf,"AutoClass"),Pf.forEach(n),$l=i(vn," verwenden k\xF6nnen, um die Ergebnisse der "),ps=o(vn,"CODE",{});var Cf=f(ps);_l=i(Cf,"pipeline()"),Cf.forEach(n),bl=i(vn," zu replizieren."),vn.forEach(n),Ar=h(e),Se=o(e,"H3",{class:!0});var ua=f(Se);Ve=o(ua,"A",{id:!0,class:!0,href:!0});var xf=f(Ve);us=o(xf,"SPAN",{});var If=f(us);E(Ft.$$.fragment,If),If.forEach(n),xf.forEach(n),wl=h(ua),ds=o(ua,"SPAN",{});var Ff=f(ds);kl=i(Ff,"AutoTokenizer"),Ff.forEach(n),ua.forEach(n),Tr=h(e),de=o(e,"P",{});var ut=f(de);vl=i(ut,"Ein Tokenizer ist f\xFCr die Vorverarbeitung von Text in ein f\xFCr das Modell verst\xE4ndliches Format zust\xE4ndig. Zun\xE4chst zerlegt der Tokenisierer den Text in W\xF6rter, die "),ms=o(ut,"EM",{});var qf=f(ms);El=i(qf,"Token"),qf.forEach(n),Al=i(ut," genannt werden. Es gibt mehrere Regeln f\xFCr den Tokenisierungsprozess, z. B. wie und auf welcher Ebene ein Wort aufgespalten wird (weitere Informationen \xFCber Tokenisierung "),sn=o(ut,"A",{href:!0});var Of=f(sn);Tl=i(Of,"hier"),Of.forEach(n),zl=i(ut,`). Das Wichtigste ist jedoch, dass Sie den Tokenizer mit demselben Modellnamen instanziieren m\xFCssen, um sicherzustellen, dass Sie dieselben Tokenisierungsregeln verwenden, mit denen ein Modell zuvor trainiert wurde.
Laden sie einen Tokenizer mit `),cs=o(ut,"CODE",{});var Wf=f(cs);Sl=i(Wf,"AutoTokenizer"),Wf.forEach(n),jl=i(ut,":"),ut.forEach(n),zr=h(e),E(qt.$$.fragment,e),Sr=h(e),Ze=o(e,"P",{});var da=f(Ze);Ml=i(da,"Anschlie\xDFend wandelt der Tokenizer die Token in Zahlen um, um einen Tensor als Eingabe f\xFCr das Modell zu konstruieren. Dieser wird als "),hs=o(da,"EM",{});var Nf=f(hs);yl=i(Nf,"Vokabular"),Nf.forEach(n),Dl=i(da," des Modells bezeichnet."),da.forEach(n),jr=h(e),rn=o(e,"P",{});var Lf=f(rn);Pl=i(Lf,"\xDCbergeben Sie Ihren Text an den Tokenizer:"),Lf.forEach(n),Mr=h(e),E(Ot.$$.fragment,e),yr=h(e),an=o(e,"P",{});var Bf=f(an);Cl=i(Bf,"Der Tokenizer gibt ein W\xF6rterbuch zur\xFCck, das Folgendes enth\xE4lt:"),Bf.forEach(n),Dr=h(e),Je=o(e,"UL",{});var ma=f(Je);ln=o(ma,"LI",{});var jo=f(ln);on=o(jo,"A",{href:!0});var Kf=f(on);xl=i(Kf,"input_ids"),Kf.forEach(n),Il=i(jo,": numerische Repr\xE4sentationen Ihrer Token."),jo.forEach(n),Fl=h(ma),fn=o(ma,"LI",{});var Mo=f(fn);pn=o(Mo,"A",{href:!0});var Hf=f(pn);ql=i(Hf,"atttention_mask"),Hf.forEach(n),Ol=i(Mo,": gibt an, welche Token beachtet werden sollen."),Mo.forEach(n),ma.forEach(n),Pr=h(e),Ye=o(e,"P",{});var ca=f(Ye);Wl=i(ca,"Genau wie die "),gs=o(ca,"CODE",{});var Uf=f(gs);Nl=i(Uf,"pipeline()"),Uf.forEach(n),Ll=i(ca," akzeptiert der Tokenizer eine Liste von Eingaben. Dar\xFCber hinaus kann der Tokenizer den Text auch auff\xFCllen und k\xFCrzen, um einen Stapel mit einheitlicher L\xE4nge zur\xFCckzugeben:"),ca.forEach(n),Cr=h(e),E(Qe.$$.fragment,e),xr=h(e),Xe=o(e,"P",{});var ha=f(Xe);Bl=i(ha,"Lesen Sie das Tutorial "),un=o(ha,"A",{href:!0});var Rf=f(un);Kl=i(Rf,"preprocessing"),Rf.forEach(n),Hl=i(ha," f\xFCr weitere Details zur Tokenisierung."),ha.forEach(n),Ir=h(e),je=o(e,"H3",{class:!0});var ga=f(je);et=o(ga,"A",{id:!0,class:!0,href:!0});var Gf=f(et);$s=o(Gf,"SPAN",{});var Vf=f($s);E(Wt.$$.fragment,Vf),Vf.forEach(n),Gf.forEach(n),Ul=h(ga),_s=o(ga,"SPAN",{});var Zf=f(_s);Rl=i(Zf,"AutoModel"),Zf.forEach(n),ga.forEach(n),Fr=h(e),E(tt.$$.fragment,e),qr=h(e),E(nt.$$.fragment,e),Or=h(e),X=o(e,"P",{});var le=f(X);Gl=i(le,"Modelle sind ein standardm\xE4\xDFiges "),Nt=o(le,"A",{href:!0,rel:!0});var Jf=f(Nt);bs=o(Jf,"CODE",{});var Yf=f(bs);Vl=i(Yf,"torch.nn.Module"),Yf.forEach(n),Jf.forEach(n),Zl=i(le," oder ein "),Lt=o(le,"A",{href:!0,rel:!0});var Qf=f(Lt);ws=o(Qf,"CODE",{});var Xf=f(ws);Jl=i(Xf,"tf.keras.Model"),Xf.forEach(n),Qf.forEach(n),Yl=i(le,", sodass Sie sie in Ihrer \xFCblichen Trainingsschleife verwenden k\xF6nnen. Um jedoch die Dinge einfacher zu machen, bietet \u{1F917} Transformers eine "),ks=o(le,"CODE",{});var ep=f(ks);Ql=i(ep,"Trainer"),ep.forEach(n),Xl=i(le,"-Klasse f\xFCr PyTorch, die Funktionalit\xE4t f\xFCr verteiltes Training, gemischte Pr\xE4zision und mehr bietet. F\xFCr TensorFlow k\xF6nnen Sie die Methode "),vs=o(le,"CODE",{});var tp=f(vs);eo=i(tp,"fit"),tp.forEach(n),to=i(le," aus "),Bt=o(le,"A",{href:!0,rel:!0});var np=f(Bt);no=i(np,"Keras"),np.forEach(n),so=i(le," verwenden. Siehe das "),dn=o(le,"A",{href:!0});var sp=f(dn);ro=i(sp,"training tutorial"),sp.forEach(n),ao=i(le," f\xFCr weitere Details."),le.forEach(n),Wr=h(e),E(st.$$.fragment,e),Nr=h(e),Me=o(e,"H3",{class:!0});var $a=f(Me);rt=o($a,"A",{id:!0,class:!0,href:!0});var rp=f(rt);Es=o(rp,"SPAN",{});var ap=f(Es);E(Kt.$$.fragment,ap),ap.forEach(n),rp.forEach(n),io=h($a),As=o($a,"SPAN",{});var ip=f(As);lo=i(ip,"Modell speichern"),ip.forEach(n),$a.forEach(n),Lr=h(e),E(at.$$.fragment,e),Br=h(e),mn=o(e,"P",{});var lp=f(mn);oo=i(lp,"Ein besonders cooles \u{1F917} Transformers-Feature ist die M\xF6glichkeit, ein Modell zu speichern und es entweder als PyTorch- oder TensorFlow-Modell wieder zu laden. Der Parameter \u201Cfrom_pt\u201D oder \u201Cfrom_tf\u201D kann das Modell von einem Framework in das andere konvertieren:"),lp.forEach(n),Kr=h(e),E(it.$$.fragment,e),Hr=h(e),ye=o(e,"H2",{class:!0});var _a=f(ye);lt=o(_a,"A",{id:!0,class:!0,href:!0});var op=f(lt);Ts=o(op,"SPAN",{});var fp=f(Ts);E(Ht.$$.fragment,fp),fp.forEach(n),op.forEach(n),fo=h(_a),zs=o(_a,"SPAN",{});var pp=f(zs);po=i(pp,"Custom model builds"),pp.forEach(n),_a.forEach(n),Ur=h(e),cn=o(e,"P",{});var up=f(cn);uo=i(up,"Sie k\xF6nnen die Konfigurationsklasse des Modells \xE4ndern, um zu bestimmen, wie ein Modell aufgebaut ist. Die Konfiguration legt die Attribute eines Modells fest, z. B. die Anzahl der verborgenen Schichten oder der Aufmerksamkeitsk\xF6pfe. Wenn Sie ein Modell aus einer benutzerdefinierten Konfigurationsklasse initialisieren, beginnen Sie bei Null. Die Modellattribute werden zuf\xE4llig initialisiert, und Sie m\xFCssen das Modell trainieren, bevor Sie es verwenden k\xF6nnen, um aussagekr\xE4ftige Ergebnisse zu erhalten."),up.forEach(n),Rr=h(e),ke=o(e,"P",{});var En=f(ke);mo=i(En,"Beginnen Sie mit dem Import von "),Ss=o(En,"CODE",{});var dp=f(Ss);co=i(dp,"AutoConfig"),dp.forEach(n),ho=i(En," und laden Sie dann das trainierte Modell, das Sie \xE4ndern m\xF6chten. Innerhalb von "),js=o(En,"CODE",{});var mp=f(js);go=i(mp,"AutoConfig.from_pretrained()"),mp.forEach(n),$o=i(En," k\xF6nnen Sie das Attribut angeben, das Sie \xE4ndern m\xF6chten, z. B. die Anzahl der Aufmerksamkeitsk\xF6pfe:"),En.forEach(n),Gr=h(e),E(Ut.$$.fragment,e),Vr=h(e),E(ot.$$.fragment,e),Zr=h(e),ft=o(e,"P",{});var ba=f(ft);_o=i(ba,"Weitere Informationen zur Erstellung von benutzerdefinierten Konfigurationen finden Sie in der Anleitung "),hn=o(ba,"A",{href:!0});var cp=f(hn);bo=i(cp,"Erstellen einer benutzerdefinierten Architektur"),cp.forEach(n),wo=i(ba,"."),ba.forEach(n),Jr=h(e),De=o(e,"H2",{class:!0});var wa=f(De);pt=o(wa,"A",{id:!0,class:!0,href:!0});var hp=f(pt);Ms=o(hp,"SPAN",{});var gp=f(Ms);E(Rt.$$.fragment,gp),gp.forEach(n),hp.forEach(n),ko=h(wa),ys=o(wa,"SPAN",{});var $p=f(ys);vo=i($p,"Wie geht es weiter?"),$p.forEach(n),wa.forEach(n),Yr=h(e),gn=o(e,"P",{});var _p=f(gn);Eo=i(_p,"Nachdem Sie nun die \u{1F917} Transformers-Kurztour abgeschlossen haben, schauen Sie sich unsere Anleitungen an und erfahren Sie, wie Sie spezifischere Dinge tun k\xF6nnen, wie das Schreiben eines benutzerdefinierten Modells, die Feinabstimmung eines Modells f\xFCr eine Aufgabe und wie man ein Modell mit einem Skript trainiert. Wenn Sie mehr \xFCber die Kernkonzepte von \u{1F917} Transformers erfahren m\xF6chten, nehmen Sie sich eine Tasse Kaffee und werfen Sie einen Blick auf unsere konzeptionellen Leitf\xE4den!"),_p.forEach(n),this.h()},h(){k(s,"name","hf:doc:metadata"),k(s,"content",JSON.stringify(lu)),k(m,"id","schnellstart"),k(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(m,"href","#schnellstart"),k(r,"class","relative group"),k(P,"href","./model_doc/auto"),k(U,"id","pipeline"),k(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(U,"href","#pipeline"),k(Z,"class","relative group"),k(Fe,"id","verwendung-der-pipeline"),k(Fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(Fe,"href","#verwendung-der-pipeline"),k(Ae,"class","relative group"),k(_t,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),k(_t,"rel","nofollow"),k(kt,"href","https://huggingface.co/docs/datasets/"),k(kt,"rel","nofollow"),k(At,"href","https://huggingface.co/docs/datasets/quickstart.html"),k(At,"rel","nofollow"),k(Tt,"href","https://huggingface.co/datasets/PolyAI/minds14"),k(Tt,"rel","nofollow"),k(Xt,"href","./main_classes/pipelines"),k(Ue,"id","ein-anderes-modell-und-einen-anderen-tokenizer-in-der-pipeline-verwenden"),k(Ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(Ue,"href","#ein-anderes-modell-und-einen-anderen-tokenizer-in-der-pipeline-verwenden"),k(Te,"class","relative group"),k(yt,"href","https://huggingface.co/models"),k(yt,"rel","nofollow"),k(Dt,"href","https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"),k(Dt,"rel","nofollow"),k(en,"href","./training"),k(tn,"href","./model_sharing"),k(Ge,"id","autoclass"),k(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(Ge,"href","#autoclass"),k(ze,"class","relative group"),k(nn,"href","./model_doc/auto"),k(Ve,"id","autotokenizer"),k(Ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(Ve,"href","#autotokenizer"),k(Se,"class","relative group"),k(sn,"href","./tokenizer_summary"),k(on,"href","./glossary#input-ids"),k(pn,"href",".glossary#attention-mask"),k(un,"href","./preprocessing"),k(et,"id","automodel"),k(et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(et,"href","#automodel"),k(je,"class","relative group"),k(Nt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),k(Nt,"rel","nofollow"),k(Lt,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),k(Lt,"rel","nofollow"),k(Bt,"href","https://keras.io/"),k(Bt,"rel","nofollow"),k(dn,"href","./training"),k(rt,"id","modell-speichern"),k(rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(rt,"href","#modell-speichern"),k(Me,"class","relative group"),k(lt,"id","custom-model-builds"),k(lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(lt,"href","#custom-model-builds"),k(ye,"class","relative group"),k(hn,"href","./create_a_model"),k(pt,"id","wie-geht-es-weiter"),k(pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(pt,"href","#wie-geht-es-weiter"),k(De,"class","relative group")},m(e,p){t(document.head,s),u(e,d,p),u(e,r,p),t(r,m),t(m,g),A(b,g,null),t(r,w),t(r,I),t(I,$),u(e,C,p),A(F,e,p),u(e,W,p),u(e,q,p),t(q,B),t(q,M),t(M,x),t(q,_),t(q,P),t(P,H),t(q,K),u(e,V,p),A(G,e,p),u(e,ne,p),u(e,Z,p),t(Z,U),t(U,ee),A(te,ee,null),t(Z,J),t(Z,re),t(re,y),u(e,O,p),u(e,Y,p),t(Y,D),t(D,N),t(Y,ue),u(e,ce,p),A(fe,e,p),u(e,dt,p),u(e,pe,p),t(pe,Jt),t(pe,Ce),t(Ce,ka),t(pe,va),u(e,Ws,p),u(e,mt,p),t(mt,Tn),t(Tn,Ea),t(mt,Aa),u(e,Ns,p),u(e,R,p),t(R,zn),t(zn,Ta),t(R,za),t(R,Sn),t(Sn,Sa),t(R,ja),t(R,jn),t(jn,Ma),t(R,ya),t(R,Mn),t(Mn,Da),t(R,Pa),t(R,yn),t(yn,Ca),t(R,xa),t(R,Dn),t(Dn,Ia),t(R,Fa),t(R,Pn),t(Pn,qa),t(R,Oa),t(R,Cn),t(Cn,Wa),u(e,Ls,p),u(e,ct,p),t(ct,xn),t(xn,Na),t(ct,La),u(e,Bs,p),u(e,he,p),t(he,In),t(In,Ba),t(he,Ka),t(he,Fn),t(Fn,Ha),t(he,Ua),t(he,qn),t(qn,Ra),u(e,Ks,p),u(e,ht,p),t(ht,On),t(On,Ga),t(ht,Va),u(e,Hs,p),u(e,xe,p),t(xe,Wn),t(Wn,Za),t(xe,Ja),t(xe,Nn),t(Nn,Ya),u(e,Us,p),A(Ie,e,p),u(e,Rs,p),u(e,Ae,p),t(Ae,Fe),t(Fe,Ln),A(gt,Ln,null),t(Ae,Qa),t(Ae,Bn),t(Bn,Xa),u(e,Gs,p),u(e,qe,p),t(qe,ei),t(qe,Kn),t(Kn,ti),t(qe,ni),u(e,Vs,p),u(e,Yt,p),t(Yt,si),u(e,Zs,p),A(Oe,e,p),u(e,Js,p),u(e,We,p),t(We,ri),t(We,Hn),t(Hn,ai),t(We,ii),u(e,Ys,p),A($t,e,p),u(e,Qs,p),u(e,Ne,p),t(Ne,li),t(Ne,_t),t(_t,oi),t(Ne,fi),u(e,Xs,p),A(bt,e,p),u(e,er,p),u(e,Le,p),t(Le,pi),t(Le,Un),t(Un,ui),t(Le,di),u(e,tr,p),A(wt,e,p),u(e,nr,p),u(e,ge,p),t(ge,mi),t(ge,Rn),t(Rn,ci),t(ge,hi),t(ge,kt),t(kt,gi),t(ge,$i),u(e,sr,p),A(vt,e,p),u(e,rr,p),u(e,Be,p),t(Be,_i),t(Be,Gn),t(Gn,bi),t(Be,wi),u(e,ar,p),A(Et,e,p),u(e,ir,p),u(e,$e,p),t($e,ki),t($e,At),t(At,vi),t($e,Ei),t($e,Tt),t(Tt,Ai),t($e,Ti),u(e,lr,p),A(zt,e,p),u(e,or,p),u(e,Ke,p),t(Ke,zi),t(Ke,Vn),t(Vn,Si),t(Ke,ji),u(e,fr,p),A(St,e,p),u(e,pr,p),u(e,Qt,p),t(Qt,Mi),u(e,ur,p),A(jt,e,p),u(e,dr,p),u(e,He,p),t(He,yi),t(He,Xt),t(Xt,Di),t(He,Pi),u(e,mr,p),u(e,Te,p),t(Te,Ue),t(Ue,Zn),A(Mt,Zn,null),t(Te,Ci),t(Te,Jn),t(Jn,xi),u(e,cr,p),u(e,ae,p),t(ae,Ii),t(ae,Yn),t(Yn,Fi),t(ae,qi),t(ae,yt),t(yt,Oi),t(ae,Wi),t(ae,Qn),t(Qn,Ni),t(ae,Li),t(ae,Dt),t(Dt,Bi),t(ae,Ki),u(e,hr,p),A(Pt,e,p),u(e,gr,p),A(Re,e,p),u(e,$r,p),u(e,_e,p),t(_e,Hi),t(_e,Xn),t(Xn,Ui),t(_e,Ri),t(_e,es),t(es,Gi),t(_e,Vi),u(e,_r,p),A(Ct,e,p),u(e,br,p),u(e,be,p),t(be,Zi),t(be,en),t(en,Ji),t(be,Yi),t(be,tn),t(tn,Qi),t(be,Xi),u(e,wr,p),u(e,ze,p),t(ze,Ge),t(Ge,ts),A(xt,ts,null),t(ze,el),t(ze,ns),t(ns,tl),u(e,kr,p),A(It,e,p),u(e,vr,p),u(e,Q,p),t(Q,nl),t(Q,ss),t(ss,sl),t(Q,rl),t(Q,rs),t(rs,al),t(Q,il),t(Q,as),t(as,ll),t(Q,ol),t(Q,nn),t(nn,is),t(is,fl),t(Q,pl),t(Q,ls),t(ls,ul),t(Q,dl),t(Q,os),t(os,ml),t(Q,cl),u(e,Er,p),u(e,we,p),t(we,hl),t(we,fs),t(fs,gl),t(we,$l),t(we,ps),t(ps,_l),t(we,bl),u(e,Ar,p),u(e,Se,p),t(Se,Ve),t(Ve,us),A(Ft,us,null),t(Se,wl),t(Se,ds),t(ds,kl),u(e,Tr,p),u(e,de,p),t(de,vl),t(de,ms),t(ms,El),t(de,Al),t(de,sn),t(sn,Tl),t(de,zl),t(de,cs),t(cs,Sl),t(de,jl),u(e,zr,p),A(qt,e,p),u(e,Sr,p),u(e,Ze,p),t(Ze,Ml),t(Ze,hs),t(hs,yl),t(Ze,Dl),u(e,jr,p),u(e,rn,p),t(rn,Pl),u(e,Mr,p),A(Ot,e,p),u(e,yr,p),u(e,an,p),t(an,Cl),u(e,Dr,p),u(e,Je,p),t(Je,ln),t(ln,on),t(on,xl),t(ln,Il),t(Je,Fl),t(Je,fn),t(fn,pn),t(pn,ql),t(fn,Ol),u(e,Pr,p),u(e,Ye,p),t(Ye,Wl),t(Ye,gs),t(gs,Nl),t(Ye,Ll),u(e,Cr,p),A(Qe,e,p),u(e,xr,p),u(e,Xe,p),t(Xe,Bl),t(Xe,un),t(un,Kl),t(Xe,Hl),u(e,Ir,p),u(e,je,p),t(je,et),t(et,$s),A(Wt,$s,null),t(je,Ul),t(je,_s),t(_s,Rl),u(e,Fr,p),A(tt,e,p),u(e,qr,p),A(nt,e,p),u(e,Or,p),u(e,X,p),t(X,Gl),t(X,Nt),t(Nt,bs),t(bs,Vl),t(X,Zl),t(X,Lt),t(Lt,ws),t(ws,Jl),t(X,Yl),t(X,ks),t(ks,Ql),t(X,Xl),t(X,vs),t(vs,eo),t(X,to),t(X,Bt),t(Bt,no),t(X,so),t(X,dn),t(dn,ro),t(X,ao),u(e,Wr,p),A(st,e,p),u(e,Nr,p),u(e,Me,p),t(Me,rt),t(rt,Es),A(Kt,Es,null),t(Me,io),t(Me,As),t(As,lo),u(e,Lr,p),A(at,e,p),u(e,Br,p),u(e,mn,p),t(mn,oo),u(e,Kr,p),A(it,e,p),u(e,Hr,p),u(e,ye,p),t(ye,lt),t(lt,Ts),A(Ht,Ts,null),t(ye,fo),t(ye,zs),t(zs,po),u(e,Ur,p),u(e,cn,p),t(cn,uo),u(e,Rr,p),u(e,ke,p),t(ke,mo),t(ke,Ss),t(Ss,co),t(ke,ho),t(ke,js),t(js,go),t(ke,$o),u(e,Gr,p),A(Ut,e,p),u(e,Vr,p),A(ot,e,p),u(e,Zr,p),u(e,ft,p),t(ft,_o),t(ft,hn),t(hn,bo),t(ft,wo),u(e,Jr,p),u(e,De,p),t(De,pt),t(pt,Ms),A(Rt,Ms,null),t(De,ko),t(De,ys),t(ys,vo),u(e,Yr,p),u(e,gn,p),t(gn,Eo),Qr=!0},p(e,[p]){const Gt={};p&2&&(Gt.$$scope={dirty:p,ctx:e}),G.$set(Gt);const Ds={};p&2&&(Ds.$$scope={dirty:p,ctx:e}),Ie.$set(Ds);const Ps={};p&2&&(Ps.$$scope={dirty:p,ctx:e}),Oe.$set(Ps);const Cs={};p&2&&(Cs.$$scope={dirty:p,ctx:e}),Re.$set(Cs);const Pe={};p&2&&(Pe.$$scope={dirty:p,ctx:e}),Qe.$set(Pe);const xs={};p&2&&(xs.$$scope={dirty:p,ctx:e}),tt.$set(xs);const Is={};p&2&&(Is.$$scope={dirty:p,ctx:e}),nt.$set(Is);const Vt={};p&2&&(Vt.$$scope={dirty:p,ctx:e}),st.$set(Vt);const Fs={};p&2&&(Fs.$$scope={dirty:p,ctx:e}),at.$set(Fs);const qs={};p&2&&(qs.$$scope={dirty:p,ctx:e}),it.$set(qs);const Os={};p&2&&(Os.$$scope={dirty:p,ctx:e}),ot.$set(Os)},i(e){Qr||(T(b.$$.fragment,e),T(F.$$.fragment,e),T(G.$$.fragment,e),T(te.$$.fragment,e),T(fe.$$.fragment,e),T(Ie.$$.fragment,e),T(gt.$$.fragment,e),T(Oe.$$.fragment,e),T($t.$$.fragment,e),T(bt.$$.fragment,e),T(wt.$$.fragment,e),T(vt.$$.fragment,e),T(Et.$$.fragment,e),T(zt.$$.fragment,e),T(St.$$.fragment,e),T(jt.$$.fragment,e),T(Mt.$$.fragment,e),T(Pt.$$.fragment,e),T(Re.$$.fragment,e),T(Ct.$$.fragment,e),T(xt.$$.fragment,e),T(It.$$.fragment,e),T(Ft.$$.fragment,e),T(qt.$$.fragment,e),T(Ot.$$.fragment,e),T(Qe.$$.fragment,e),T(Wt.$$.fragment,e),T(tt.$$.fragment,e),T(nt.$$.fragment,e),T(st.$$.fragment,e),T(Kt.$$.fragment,e),T(at.$$.fragment,e),T(it.$$.fragment,e),T(Ht.$$.fragment,e),T(Ut.$$.fragment,e),T(ot.$$.fragment,e),T(Rt.$$.fragment,e),Qr=!0)},o(e){z(b.$$.fragment,e),z(F.$$.fragment,e),z(G.$$.fragment,e),z(te.$$.fragment,e),z(fe.$$.fragment,e),z(Ie.$$.fragment,e),z(gt.$$.fragment,e),z(Oe.$$.fragment,e),z($t.$$.fragment,e),z(bt.$$.fragment,e),z(wt.$$.fragment,e),z(vt.$$.fragment,e),z(Et.$$.fragment,e),z(zt.$$.fragment,e),z(St.$$.fragment,e),z(jt.$$.fragment,e),z(Mt.$$.fragment,e),z(Pt.$$.fragment,e),z(Re.$$.fragment,e),z(Ct.$$.fragment,e),z(xt.$$.fragment,e),z(It.$$.fragment,e),z(Ft.$$.fragment,e),z(qt.$$.fragment,e),z(Ot.$$.fragment,e),z(Qe.$$.fragment,e),z(Wt.$$.fragment,e),z(tt.$$.fragment,e),z(nt.$$.fragment,e),z(st.$$.fragment,e),z(Kt.$$.fragment,e),z(at.$$.fragment,e),z(it.$$.fragment,e),z(Ht.$$.fragment,e),z(Ut.$$.fragment,e),z(ot.$$.fragment,e),z(Rt.$$.fragment,e),Qr=!1},d(e){n(s),e&&n(d),e&&n(r),S(b),e&&n(C),S(F,e),e&&n(W),e&&n(q),e&&n(V),S(G,e),e&&n(ne),e&&n(Z),S(te),e&&n(O),e&&n(Y),e&&n(ce),S(fe,e),e&&n(dt),e&&n(pe),e&&n(Ws),e&&n(mt),e&&n(Ns),e&&n(R),e&&n(Ls),e&&n(ct),e&&n(Bs),e&&n(he),e&&n(Ks),e&&n(ht),e&&n(Hs),e&&n(xe),e&&n(Us),S(Ie,e),e&&n(Rs),e&&n(Ae),S(gt),e&&n(Gs),e&&n(qe),e&&n(Vs),e&&n(Yt),e&&n(Zs),S(Oe,e),e&&n(Js),e&&n(We),e&&n(Ys),S($t,e),e&&n(Qs),e&&n(Ne),e&&n(Xs),S(bt,e),e&&n(er),e&&n(Le),e&&n(tr),S(wt,e),e&&n(nr),e&&n(ge),e&&n(sr),S(vt,e),e&&n(rr),e&&n(Be),e&&n(ar),S(Et,e),e&&n(ir),e&&n($e),e&&n(lr),S(zt,e),e&&n(or),e&&n(Ke),e&&n(fr),S(St,e),e&&n(pr),e&&n(Qt),e&&n(ur),S(jt,e),e&&n(dr),e&&n(He),e&&n(mr),e&&n(Te),S(Mt),e&&n(cr),e&&n(ae),e&&n(hr),S(Pt,e),e&&n(gr),S(Re,e),e&&n($r),e&&n(_e),e&&n(_r),S(Ct,e),e&&n(br),e&&n(be),e&&n(wr),e&&n(ze),S(xt),e&&n(kr),S(It,e),e&&n(vr),e&&n(Q),e&&n(Er),e&&n(we),e&&n(Ar),e&&n(Se),S(Ft),e&&n(Tr),e&&n(de),e&&n(zr),S(qt,e),e&&n(Sr),e&&n(Ze),e&&n(jr),e&&n(rn),e&&n(Mr),S(Ot,e),e&&n(yr),e&&n(an),e&&n(Dr),e&&n(Je),e&&n(Pr),e&&n(Ye),e&&n(Cr),S(Qe,e),e&&n(xr),e&&n(Xe),e&&n(Ir),e&&n(je),S(Wt),e&&n(Fr),S(tt,e),e&&n(qr),S(nt,e),e&&n(Or),e&&n(X),e&&n(Wr),S(st,e),e&&n(Nr),e&&n(Me),S(Kt),e&&n(Lr),S(at,e),e&&n(Br),e&&n(mn),e&&n(Kr),S(it,e),e&&n(Hr),e&&n(ye),S(Ht),e&&n(Ur),e&&n(cn),e&&n(Rr),e&&n(ke),e&&n(Gr),S(Ut,e),e&&n(Vr),S(ot,e),e&&n(Zr),e&&n(ft),e&&n(Jr),e&&n(De),S(Rt),e&&n(Yr),e&&n(gn)}}}const lu={local:"schnellstart",sections:[{local:"pipeline",sections:[{local:"verwendung-der-pipeline",title:"Verwendung der Pipeline"},{local:"ein-anderes-modell-und-einen-anderen-tokenizer-in-der-pipeline-verwenden",title:"Ein anderes Modell und einen anderen Tokenizer in der Pipeline verwenden"}],title:"Pipeline"},{local:"autoclass",sections:[{local:"autotokenizer",title:"AutoTokenizer"},{local:"automodel",title:"AutoModel"},{local:"modell-speichern",title:"Modell speichern"}],title:"AutoClass"},{local:"custom-model-builds",title:"Custom model builds"},{local:"wie-geht-es-weiter",title:"Wie geht es weiter?"}],title:"Schnellstart"};function ou(j){return Ap(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class gu extends wp{constructor(s){super();kp(this,s,ou,iu,vp,{})}}export{gu as default,lu as metadata};
