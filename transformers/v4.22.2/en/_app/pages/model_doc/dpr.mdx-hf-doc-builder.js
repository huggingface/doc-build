import{S as pf,i as hf,s as uf,e as s,k as c,w,t as n,M as ff,c as a,d as t,m as p,a as i,x as k,h as r,b as l,G as e,g as f,y as P,q as $,o as E,B as y,v as mf,L as sr}from"../../chunks/vendor-hf-doc-builder.js";import{T as Vt}from"../../chunks/Tip-hf-doc-builder.js";import{D as U}from"../../chunks/Docstring-hf-doc-builder.js";import{C as ar}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Ee}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as rr}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function gf(F){let h,T,_,g,b;return g=new ar({props:{code:"[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>",highlighted:'[CLS] <span class="hljs-tag">&lt;<span class="hljs-name">question</span> <span class="hljs-attr">token</span> <span class="hljs-attr">ids</span>&gt;</span> [SEP] <span class="hljs-tag">&lt;<span class="hljs-name">titles</span> <span class="hljs-attr">ids</span>&gt;</span> [SEP] <span class="hljs-tag">&lt;<span class="hljs-name">texts</span> <span class="hljs-attr">ids</span>&gt;</span>'}}),{c(){h=s("p"),T=n("with the format:"),_=c(),w(g.$$.fragment)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"with the format:"),u.forEach(t),_=p(d),k(g.$$.fragment,d)},m(d,u){f(d,h,u),e(h,T),f(d,_,u),P(g,d,u),b=!0},p:sr,i(d){b||($(g.$$.fragment,d),b=!0)},o(d){E(g.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(_),y(g,d)}}}function _f(F){let h,T,_,g,b;return{c(){h=s("p"),T=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=s("code"),g=n("Module"),b=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=a(u,"CODE",{});var q=i(_);g=r(q,"Module"),q.forEach(t),b=r(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(d,u){f(d,h,u),e(h,T),e(h,_),e(_,g),e(h,b)},d(d){d&&t(h)}}}function vf(F){let h,T,_,g,b;return g=new ar({props:{code:`from transformers import DPRContextEncoder, DPRContextEncoderTokenizer

tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
model = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="pt")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRContextEncoder, DPRContextEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRContextEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRContextEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),{c(){h=s("p"),T=n("Examples:"),_=c(),w(g.$$.fragment)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Examples:"),u.forEach(t),_=p(d),k(g.$$.fragment,d)},m(d,u){f(d,h,u),e(h,T),f(d,_,u),P(g,d,u),b=!0},p:sr,i(d){b||($(g.$$.fragment,d),b=!0)},o(d){E(g.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(_),y(g,d)}}}function bf(F){let h,T,_,g,b;return{c(){h=s("p"),T=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=s("code"),g=n("Module"),b=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=a(u,"CODE",{});var q=i(_);g=r(q,"Module"),q.forEach(t),b=r(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(d,u){f(d,h,u),e(h,T),e(h,_),e(_,g),e(h,b)},d(d){d&&t(h)}}}function Tf(F){let h,T,_,g,b;return g=new ar({props:{code:`from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer

tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
model = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="pt")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRQuestionEncoder, DPRQuestionEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRQuestionEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),{c(){h=s("p"),T=n("Examples:"),_=c(),w(g.$$.fragment)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Examples:"),u.forEach(t),_=p(d),k(g.$$.fragment,d)},m(d,u){f(d,h,u),e(h,T),f(d,_,u),P(g,d,u),b=!0},p:sr,i(d){b||($(g.$$.fragment,d),b=!0)},o(d){E(g.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(_),y(g,d)}}}function wf(F){let h,T,_,g,b;return{c(){h=s("p"),T=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=s("code"),g=n("Module"),b=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=a(u,"CODE",{});var q=i(_);g=r(q,"Module"),q.forEach(t),b=r(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(d,u){f(d,h,u),e(h,T),e(h,_),e(_,g),e(h,b)},d(d){d&&t(h)}}}function kf(F){let h,T,_,g,b;return g=new ar({props:{code:`from transformers import DPRReader, DPRReaderTokenizer

tokenizer = DPRReaderTokenizer.from_pretrained("facebook/dpr-reader-single-nq-base")
model = DPRReader.from_pretrained("facebook/dpr-reader-single-nq-base")
encoded_inputs = tokenizer(
    questions=["What is love ?"],
    titles=["Haddaway"],
    texts=["'What Is Love' is a song recorded by the artist Haddaway"],
    return_tensors="pt",
)
outputs = model(**encoded_inputs)
start_logits = outputs.start_logits
end_logits = outputs.end_logits
relevance_logits = outputs.relevance_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRReader, DPRReaderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRReaderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRReader.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(
<span class="hljs-meta">... </span>    questions=[<span class="hljs-string">&quot;What is love ?&quot;</span>],
<span class="hljs-meta">... </span>    titles=[<span class="hljs-string">&quot;Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    texts=[<span class="hljs-string">&quot;&#x27;What Is Love&#x27; is a song recorded by the artist Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**encoded_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_logits = outputs.relevance_logits`}}),{c(){h=s("p"),T=n("Examples:"),_=c(),w(g.$$.fragment)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Examples:"),u.forEach(t),_=p(d),k(g.$$.fragment,d)},m(d,u){f(d,h,u),e(h,T),f(d,_,u),P(g,d,u),b=!0},p:sr,i(d){b||($(g.$$.fragment,d),b=!0)},o(d){E(g.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(_),y(g,d)}}}function Pf(F){let h,T,_,g,b,d,u,q,ye,be,L,te,oe,R,Re,H,De,Te,I,xe,se,V,ze,we,S,qe,ke,B,ue,Ce,ae,z,O,ie,Y,Fe,de,X,Ae,Pe,C,ne,K,le,Oe,W,je,$e,A,ce,G,re,J,j,Ne,N,Qe,Le;return{c(){h=s("p"),T=n("TensorFlow models and layers in "),_=s("code"),g=n("transformers"),b=n(" accept two formats as input:"),d=c(),u=s("ul"),q=s("li"),ye=n("having all inputs as keyword arguments (like PyTorch models), or"),be=c(),L=s("li"),te=n("having all inputs as a list, tuple or dict in the first positional argument."),oe=c(),R=s("p"),Re=n(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),H=s("code"),De=n("model.fit()"),Te=n(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),I=s("code"),xe=n("model.fit()"),se=n(` supports! If, however, you want to use the second
format outside of Keras methods like `),V=s("code"),ze=n("fit()"),we=n(" and "),S=s("code"),qe=n("predict()"),ke=n(`, such as when creating your own layers or models with
the Keras `),B=s("code"),ue=n("Functional"),Ce=n(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),ae=c(),z=s("ul"),O=s("li"),ie=n("a single Tensor with "),Y=s("code"),Fe=n("input_ids"),de=n(" only and nothing else: "),X=s("code"),Ae=n("model(input_ids)"),Pe=c(),C=s("li"),ne=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),K=s("code"),le=n("model([input_ids, attention_mask])"),Oe=n(" or "),W=s("code"),je=n("model([input_ids, attention_mask, token_type_ids])"),$e=c(),A=s("li"),ce=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),G=s("code"),re=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),J=c(),j=s("p"),Ne=n(`Note that when creating models and layers with
`),N=s("a"),Qe=n("subclassing"),Le=n(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(v){h=a(v,"P",{});var D=i(h);T=r(D,"TensorFlow models and layers in "),_=a(D,"CODE",{});var Xe=i(_);g=r(Xe,"transformers"),Xe.forEach(t),b=r(D," accept two formats as input:"),D.forEach(t),d=p(v),u=a(v,"UL",{});var Z=i(u);q=a(Z,"LI",{});var Ge=i(q);ye=r(Ge,"having all inputs as keyword arguments (like PyTorch models), or"),Ge.forEach(t),be=p(Z),L=a(Z,"LI",{});var Je=i(L);te=r(Je,"having all inputs as a list, tuple or dict in the first positional argument."),Je.forEach(t),Z.forEach(t),oe=p(v),R=a(v,"P",{});var x=i(R);Re=r(x,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),H=a(x,"CODE",{});var Ze=i(H);De=r(Ze,"model.fit()"),Ze.forEach(t),Te=r(x,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),I=a(x,"CODE",{});var et=i(I);xe=r(et,"model.fit()"),et.forEach(t),se=r(x,` supports! If, however, you want to use the second
format outside of Keras methods like `),V=a(x,"CODE",{});var Ie=i(V);ze=r(Ie,"fit()"),Ie.forEach(t),we=r(x," and "),S=a(x,"CODE",{});var tt=i(S);qe=r(tt,"predict()"),tt.forEach(t),ke=r(x,`, such as when creating your own layers or models with
the Keras `),B=a(x,"CODE",{});var ot=i(B);ue=r(ot,"Functional"),ot.forEach(t),Ce=r(x,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),x.forEach(t),ae=p(v),z=a(v,"UL",{});var Q=i(z);O=a(Q,"LI",{});var ee=i(O);ie=r(ee,"a single Tensor with "),Y=a(ee,"CODE",{});var nt=i(Y);Fe=r(nt,"input_ids"),nt.forEach(t),de=r(ee," only and nothing else: "),X=a(ee,"CODE",{});var rt=i(X);Ae=r(rt,"model(input_ids)"),rt.forEach(t),ee.forEach(t),Pe=p(Q),C=a(Q,"LI",{});var M=i(C);ne=r(M,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),K=a(M,"CODE",{});var st=i(K);le=r(st,"model([input_ids, attention_mask])"),st.forEach(t),Oe=r(M," or "),W=a(M,"CODE",{});var Me=i(W);je=r(Me,"model([input_ids, attention_mask, token_type_ids])"),Me.forEach(t),M.forEach(t),$e=p(Q),A=a(Q,"LI",{});var Se=i(A);ce=r(Se,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),G=a(Se,"CODE",{});var at=i(G);re=r(at,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),at.forEach(t),Se.forEach(t),Q.forEach(t),J=p(v),j=a(v,"P",{});var pe=i(j);Ne=r(pe,`Note that when creating models and layers with
`),N=a(pe,"A",{href:!0,rel:!0});var he=i(N);Qe=r(he,"subclassing"),he.forEach(t),Le=r(pe,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),pe.forEach(t),this.h()},h(){l(N,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),l(N,"rel","nofollow")},m(v,D){f(v,h,D),e(h,T),e(h,_),e(_,g),e(h,b),f(v,d,D),f(v,u,D),e(u,q),e(q,ye),e(u,be),e(u,L),e(L,te),f(v,oe,D),f(v,R,D),e(R,Re),e(R,H),e(H,De),e(R,Te),e(R,I),e(I,xe),e(R,se),e(R,V),e(V,ze),e(R,we),e(R,S),e(S,qe),e(R,ke),e(R,B),e(B,ue),e(R,Ce),f(v,ae,D),f(v,z,D),e(z,O),e(O,ie),e(O,Y),e(Y,Fe),e(O,de),e(O,X),e(X,Ae),e(z,Pe),e(z,C),e(C,ne),e(C,K),e(K,le),e(C,Oe),e(C,W),e(W,je),e(z,$e),e(z,A),e(A,ce),e(A,G),e(G,re),f(v,J,D),f(v,j,D),e(j,Ne),e(j,N),e(N,Qe),e(j,Le)},d(v){v&&t(h),v&&t(d),v&&t(u),v&&t(oe),v&&t(R),v&&t(ae),v&&t(z),v&&t(J),v&&t(j)}}}function $f(F){let h,T,_,g,b;return{c(){h=s("p"),T=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=s("code"),g=n("Module"),b=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=a(u,"CODE",{});var q=i(_);g=r(q,"Module"),q.forEach(t),b=r(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(d,u){f(d,h,u),e(h,T),e(h,_),e(_,g),e(h,b)},d(d){d&&t(h)}}}function Ef(F){let h,T,_,g,b;return g=new ar({props:{code:`from transformers import TFDPRContextEncoder, DPRContextEncoderTokenizer

tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
model = TFDPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base", from_pt=True)
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="tf")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRContextEncoder, DPRContextEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRContextEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRContextEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),{c(){h=s("p"),T=n("Examples:"),_=c(),w(g.$$.fragment)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Examples:"),u.forEach(t),_=p(d),k(g.$$.fragment,d)},m(d,u){f(d,h,u),e(h,T),f(d,_,u),P(g,d,u),b=!0},p:sr,i(d){b||($(g.$$.fragment,d),b=!0)},o(d){E(g.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(_),y(g,d)}}}function yf(F){let h,T,_,g,b,d,u,q,ye,be,L,te,oe,R,Re,H,De,Te,I,xe,se,V,ze,we,S,qe,ke,B,ue,Ce,ae,z,O,ie,Y,Fe,de,X,Ae,Pe,C,ne,K,le,Oe,W,je,$e,A,ce,G,re,J,j,Ne,N,Qe,Le;return{c(){h=s("p"),T=n("TensorFlow models and layers in "),_=s("code"),g=n("transformers"),b=n(" accept two formats as input:"),d=c(),u=s("ul"),q=s("li"),ye=n("having all inputs as keyword arguments (like PyTorch models), or"),be=c(),L=s("li"),te=n("having all inputs as a list, tuple or dict in the first positional argument."),oe=c(),R=s("p"),Re=n(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),H=s("code"),De=n("model.fit()"),Te=n(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),I=s("code"),xe=n("model.fit()"),se=n(` supports! If, however, you want to use the second
format outside of Keras methods like `),V=s("code"),ze=n("fit()"),we=n(" and "),S=s("code"),qe=n("predict()"),ke=n(`, such as when creating your own layers or models with
the Keras `),B=s("code"),ue=n("Functional"),Ce=n(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),ae=c(),z=s("ul"),O=s("li"),ie=n("a single Tensor with "),Y=s("code"),Fe=n("input_ids"),de=n(" only and nothing else: "),X=s("code"),Ae=n("model(input_ids)"),Pe=c(),C=s("li"),ne=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),K=s("code"),le=n("model([input_ids, attention_mask])"),Oe=n(" or "),W=s("code"),je=n("model([input_ids, attention_mask, token_type_ids])"),$e=c(),A=s("li"),ce=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),G=s("code"),re=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),J=c(),j=s("p"),Ne=n(`Note that when creating models and layers with
`),N=s("a"),Qe=n("subclassing"),Le=n(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(v){h=a(v,"P",{});var D=i(h);T=r(D,"TensorFlow models and layers in "),_=a(D,"CODE",{});var Xe=i(_);g=r(Xe,"transformers"),Xe.forEach(t),b=r(D," accept two formats as input:"),D.forEach(t),d=p(v),u=a(v,"UL",{});var Z=i(u);q=a(Z,"LI",{});var Ge=i(q);ye=r(Ge,"having all inputs as keyword arguments (like PyTorch models), or"),Ge.forEach(t),be=p(Z),L=a(Z,"LI",{});var Je=i(L);te=r(Je,"having all inputs as a list, tuple or dict in the first positional argument."),Je.forEach(t),Z.forEach(t),oe=p(v),R=a(v,"P",{});var x=i(R);Re=r(x,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),H=a(x,"CODE",{});var Ze=i(H);De=r(Ze,"model.fit()"),Ze.forEach(t),Te=r(x,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),I=a(x,"CODE",{});var et=i(I);xe=r(et,"model.fit()"),et.forEach(t),se=r(x,` supports! If, however, you want to use the second
format outside of Keras methods like `),V=a(x,"CODE",{});var Ie=i(V);ze=r(Ie,"fit()"),Ie.forEach(t),we=r(x," and "),S=a(x,"CODE",{});var tt=i(S);qe=r(tt,"predict()"),tt.forEach(t),ke=r(x,`, such as when creating your own layers or models with
the Keras `),B=a(x,"CODE",{});var ot=i(B);ue=r(ot,"Functional"),ot.forEach(t),Ce=r(x,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),x.forEach(t),ae=p(v),z=a(v,"UL",{});var Q=i(z);O=a(Q,"LI",{});var ee=i(O);ie=r(ee,"a single Tensor with "),Y=a(ee,"CODE",{});var nt=i(Y);Fe=r(nt,"input_ids"),nt.forEach(t),de=r(ee," only and nothing else: "),X=a(ee,"CODE",{});var rt=i(X);Ae=r(rt,"model(input_ids)"),rt.forEach(t),ee.forEach(t),Pe=p(Q),C=a(Q,"LI",{});var M=i(C);ne=r(M,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),K=a(M,"CODE",{});var st=i(K);le=r(st,"model([input_ids, attention_mask])"),st.forEach(t),Oe=r(M," or "),W=a(M,"CODE",{});var Me=i(W);je=r(Me,"model([input_ids, attention_mask, token_type_ids])"),Me.forEach(t),M.forEach(t),$e=p(Q),A=a(Q,"LI",{});var Se=i(A);ce=r(Se,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),G=a(Se,"CODE",{});var at=i(G);re=r(at,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),at.forEach(t),Se.forEach(t),Q.forEach(t),J=p(v),j=a(v,"P",{});var pe=i(j);Ne=r(pe,`Note that when creating models and layers with
`),N=a(pe,"A",{href:!0,rel:!0});var he=i(N);Qe=r(he,"subclassing"),he.forEach(t),Le=r(pe,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),pe.forEach(t),this.h()},h(){l(N,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),l(N,"rel","nofollow")},m(v,D){f(v,h,D),e(h,T),e(h,_),e(_,g),e(h,b),f(v,d,D),f(v,u,D),e(u,q),e(q,ye),e(u,be),e(u,L),e(L,te),f(v,oe,D),f(v,R,D),e(R,Re),e(R,H),e(H,De),e(R,Te),e(R,I),e(I,xe),e(R,se),e(R,V),e(V,ze),e(R,we),e(R,S),e(S,qe),e(R,ke),e(R,B),e(B,ue),e(R,Ce),f(v,ae,D),f(v,z,D),e(z,O),e(O,ie),e(O,Y),e(Y,Fe),e(O,de),e(O,X),e(X,Ae),e(z,Pe),e(z,C),e(C,ne),e(C,K),e(K,le),e(C,Oe),e(C,W),e(W,je),e(z,$e),e(z,A),e(A,ce),e(A,G),e(G,re),f(v,J,D),f(v,j,D),e(j,Ne),e(j,N),e(N,Qe),e(j,Le)},d(v){v&&t(h),v&&t(d),v&&t(u),v&&t(oe),v&&t(R),v&&t(ae),v&&t(z),v&&t(J),v&&t(j)}}}function Rf(F){let h,T,_,g,b;return{c(){h=s("p"),T=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=s("code"),g=n("Module"),b=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=a(u,"CODE",{});var q=i(_);g=r(q,"Module"),q.forEach(t),b=r(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(d,u){f(d,h,u),e(h,T),e(h,_),e(_,g),e(h,b)},d(d){d&&t(h)}}}function Df(F){let h,T,_,g,b;return g=new ar({props:{code:`from transformers import TFDPRQuestionEncoder, DPRQuestionEncoderTokenizer

tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
model = TFDPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base", from_pt=True)
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="tf")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRQuestionEncoder, DPRQuestionEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRQuestionEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),{c(){h=s("p"),T=n("Examples:"),_=c(),w(g.$$.fragment)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Examples:"),u.forEach(t),_=p(d),k(g.$$.fragment,d)},m(d,u){f(d,h,u),e(h,T),f(d,_,u),P(g,d,u),b=!0},p:sr,i(d){b||($(g.$$.fragment,d),b=!0)},o(d){E(g.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(_),y(g,d)}}}function xf(F){let h,T,_,g,b,d,u,q,ye,be,L,te,oe,R,Re,H,De,Te,I,xe,se,V,ze,we,S,qe,ke,B,ue,Ce,ae,z,O,ie,Y,Fe,de,X,Ae,Pe,C,ne,K,le,Oe,W,je,$e,A,ce,G,re,J,j,Ne,N,Qe,Le;return{c(){h=s("p"),T=n("TensorFlow models and layers in "),_=s("code"),g=n("transformers"),b=n(" accept two formats as input:"),d=c(),u=s("ul"),q=s("li"),ye=n("having all inputs as keyword arguments (like PyTorch models), or"),be=c(),L=s("li"),te=n("having all inputs as a list, tuple or dict in the first positional argument."),oe=c(),R=s("p"),Re=n(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),H=s("code"),De=n("model.fit()"),Te=n(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),I=s("code"),xe=n("model.fit()"),se=n(` supports! If, however, you want to use the second
format outside of Keras methods like `),V=s("code"),ze=n("fit()"),we=n(" and "),S=s("code"),qe=n("predict()"),ke=n(`, such as when creating your own layers or models with
the Keras `),B=s("code"),ue=n("Functional"),Ce=n(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),ae=c(),z=s("ul"),O=s("li"),ie=n("a single Tensor with "),Y=s("code"),Fe=n("input_ids"),de=n(" only and nothing else: "),X=s("code"),Ae=n("model(input_ids)"),Pe=c(),C=s("li"),ne=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),K=s("code"),le=n("model([input_ids, attention_mask])"),Oe=n(" or "),W=s("code"),je=n("model([input_ids, attention_mask, token_type_ids])"),$e=c(),A=s("li"),ce=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),G=s("code"),re=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),J=c(),j=s("p"),Ne=n(`Note that when creating models and layers with
`),N=s("a"),Qe=n("subclassing"),Le=n(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(v){h=a(v,"P",{});var D=i(h);T=r(D,"TensorFlow models and layers in "),_=a(D,"CODE",{});var Xe=i(_);g=r(Xe,"transformers"),Xe.forEach(t),b=r(D," accept two formats as input:"),D.forEach(t),d=p(v),u=a(v,"UL",{});var Z=i(u);q=a(Z,"LI",{});var Ge=i(q);ye=r(Ge,"having all inputs as keyword arguments (like PyTorch models), or"),Ge.forEach(t),be=p(Z),L=a(Z,"LI",{});var Je=i(L);te=r(Je,"having all inputs as a list, tuple or dict in the first positional argument."),Je.forEach(t),Z.forEach(t),oe=p(v),R=a(v,"P",{});var x=i(R);Re=r(x,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),H=a(x,"CODE",{});var Ze=i(H);De=r(Ze,"model.fit()"),Ze.forEach(t),Te=r(x,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),I=a(x,"CODE",{});var et=i(I);xe=r(et,"model.fit()"),et.forEach(t),se=r(x,` supports! If, however, you want to use the second
format outside of Keras methods like `),V=a(x,"CODE",{});var Ie=i(V);ze=r(Ie,"fit()"),Ie.forEach(t),we=r(x," and "),S=a(x,"CODE",{});var tt=i(S);qe=r(tt,"predict()"),tt.forEach(t),ke=r(x,`, such as when creating your own layers or models with
the Keras `),B=a(x,"CODE",{});var ot=i(B);ue=r(ot,"Functional"),ot.forEach(t),Ce=r(x,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),x.forEach(t),ae=p(v),z=a(v,"UL",{});var Q=i(z);O=a(Q,"LI",{});var ee=i(O);ie=r(ee,"a single Tensor with "),Y=a(ee,"CODE",{});var nt=i(Y);Fe=r(nt,"input_ids"),nt.forEach(t),de=r(ee," only and nothing else: "),X=a(ee,"CODE",{});var rt=i(X);Ae=r(rt,"model(input_ids)"),rt.forEach(t),ee.forEach(t),Pe=p(Q),C=a(Q,"LI",{});var M=i(C);ne=r(M,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),K=a(M,"CODE",{});var st=i(K);le=r(st,"model([input_ids, attention_mask])"),st.forEach(t),Oe=r(M," or "),W=a(M,"CODE",{});var Me=i(W);je=r(Me,"model([input_ids, attention_mask, token_type_ids])"),Me.forEach(t),M.forEach(t),$e=p(Q),A=a(Q,"LI",{});var Se=i(A);ce=r(Se,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),G=a(Se,"CODE",{});var at=i(G);re=r(at,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),at.forEach(t),Se.forEach(t),Q.forEach(t),J=p(v),j=a(v,"P",{});var pe=i(j);Ne=r(pe,`Note that when creating models and layers with
`),N=a(pe,"A",{href:!0,rel:!0});var he=i(N);Qe=r(he,"subclassing"),he.forEach(t),Le=r(pe,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),pe.forEach(t),this.h()},h(){l(N,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),l(N,"rel","nofollow")},m(v,D){f(v,h,D),e(h,T),e(h,_),e(_,g),e(h,b),f(v,d,D),f(v,u,D),e(u,q),e(q,ye),e(u,be),e(u,L),e(L,te),f(v,oe,D),f(v,R,D),e(R,Re),e(R,H),e(H,De),e(R,Te),e(R,I),e(I,xe),e(R,se),e(R,V),e(V,ze),e(R,we),e(R,S),e(S,qe),e(R,ke),e(R,B),e(B,ue),e(R,Ce),f(v,ae,D),f(v,z,D),e(z,O),e(O,ie),e(O,Y),e(Y,Fe),e(O,de),e(O,X),e(X,Ae),e(z,Pe),e(z,C),e(C,ne),e(C,K),e(K,le),e(C,Oe),e(C,W),e(W,je),e(z,$e),e(z,A),e(A,ce),e(A,G),e(G,re),f(v,J,D),f(v,j,D),e(j,Ne),e(j,N),e(N,Qe),e(j,Le)},d(v){v&&t(h),v&&t(d),v&&t(u),v&&t(oe),v&&t(R),v&&t(ae),v&&t(z),v&&t(J),v&&t(j)}}}function zf(F){let h,T,_,g,b;return{c(){h=s("p"),T=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=s("code"),g=n("Module"),b=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=a(u,"CODE",{});var q=i(_);g=r(q,"Module"),q.forEach(t),b=r(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(d,u){f(d,h,u),e(h,T),e(h,_),e(_,g),e(h,b)},d(d){d&&t(h)}}}function qf(F){let h,T,_,g,b;return g=new ar({props:{code:`from transformers import TFDPRReader, DPRReaderTokenizer

tokenizer = DPRReaderTokenizer.from_pretrained("facebook/dpr-reader-single-nq-base")
model = TFDPRReader.from_pretrained("facebook/dpr-reader-single-nq-base", from_pt=True)
encoded_inputs = tokenizer(
    questions=["What is love ?"],
    titles=["Haddaway"],
    texts=["'What Is Love' is a song recorded by the artist Haddaway"],
    return_tensors="tf",
)
outputs = model(encoded_inputs)
start_logits = outputs.start_logits
end_logits = outputs.end_logits
relevance_logits = outputs.relevance_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRReader, DPRReaderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRReaderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRReader.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(
<span class="hljs-meta">... </span>    questions=[<span class="hljs-string">&quot;What is love ?&quot;</span>],
<span class="hljs-meta">... </span>    titles=[<span class="hljs-string">&quot;Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    texts=[<span class="hljs-string">&quot;&#x27;What Is Love&#x27; is a song recorded by the artist Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(encoded_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_logits = outputs.relevance_logits`}}),{c(){h=s("p"),T=n("Examples:"),_=c(),w(g.$$.fragment)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Examples:"),u.forEach(t),_=p(d),k(g.$$.fragment,d)},m(d,u){f(d,h,u),e(h,T),f(d,_,u),P(g,d,u),b=!0},p:sr,i(d){b||($(g.$$.fragment,d),b=!0)},o(d){E(g.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(_),y(g,d)}}}function Cf(F){let h,T,_,g,b,d,u,q,ye,be,L,te,oe,R,Re,H,De,Te,I,xe,se,V,ze,we,S,qe,ke,B,ue,Ce,ae,z,O,ie,Y,Fe,de,X,Ae,Pe,C,ne,K,le,Oe,W,je,$e,A,ce,G,re,J,j,Ne,N,Qe,Le,v,D,Xe,Z,Ge,Je,x,Ze,et,Ie,tt,ot,Q,ee,nt,rt,M,st,Me,Se,at,pe,he,Yt,Vr,Mo,$i,Yr,Ei,da,We,So,yi,Xr,Ri,Di,Xt,ir,xi,zi,dr,qi,Ci,Fi,Bo,Ai,lr,Oi,ji,la,yt,Gt,Gr,Ho,Ni,Jr,Qi,ca,Ue,Ko,Li,Wo,Ii,Zr,Mi,Si,Bi,Jt,cr,Hi,Ki,pr,Wi,Ui,Vi,Uo,Yi,hr,Xi,Gi,pa,Rt,Zt,es,Vo,Ji,ts,Zi,ha,Ve,Yo,ed,os,td,od,eo,ur,nd,rd,fr,sd,ad,id,Xo,dd,mr,ld,cd,ua,Dt,to,ns,Go,pd,rs,hd,fa,Ye,Jo,ud,Zo,fd,ss,md,gd,_d,oo,gr,vd,bd,_r,Td,wd,kd,en,Pd,vr,$d,Ed,ma,xt,no,as,tn,yd,is,Rd,ga,fe,on,Dd,ds,xd,zd,bt,br,qd,Cd,Tr,Fd,Ad,wr,Od,jd,Nd,nn,Qd,kr,Ld,Id,Md,Tt,Sd,ls,Bd,Hd,cs,Kd,Wd,ps,Ud,Vd,ro,_a,zt,so,hs,rn,Yd,us,Xd,va,me,sn,Gd,an,Jd,fs,Zd,el,tl,wt,Pr,ol,nl,$r,rl,sl,Er,al,il,dl,dn,ll,yr,cl,pl,hl,vt,ul,ms,fl,ml,gs,gl,_l,_s,vl,bl,Tl,vs,wl,ba,qt,ao,bs,ln,kl,Ts,Pl,Ta,Ct,cn,$l,pn,El,Rr,yl,Rl,wa,Ft,hn,Dl,un,xl,Dr,zl,ql,ka,At,fn,Cl,mn,Fl,xr,Al,Ol,Pa,Ot,io,ws,gn,jl,ks,Nl,$a,Be,_n,Ql,Ps,Ll,Il,vn,Ml,zr,Sl,Bl,Hl,bn,Kl,Tn,Wl,Ul,Vl,it,wn,Yl,jt,Xl,qr,Gl,Jl,$s,Zl,ec,tc,lo,oc,co,Ea,Nt,po,Es,kn,nc,ys,rc,ya,He,Pn,sc,Rs,ac,ic,$n,dc,Cr,lc,cc,pc,En,hc,yn,uc,fc,mc,dt,Rn,gc,Qt,_c,Fr,vc,bc,Ds,Tc,wc,kc,ho,Pc,uo,Ra,Lt,fo,xs,Dn,$c,zs,Ec,Da,Ke,xn,yc,qs,Rc,Dc,zn,xc,Ar,zc,qc,Cc,qn,Fc,Cn,Ac,Oc,jc,lt,Fn,Nc,It,Qc,Or,Lc,Ic,Cs,Mc,Sc,Bc,mo,Hc,go,xa,Mt,_o,Fs,An,Kc,As,Wc,za,ge,On,Uc,Os,Vc,Yc,jn,Xc,jr,Gc,Jc,Zc,Nn,ep,Qn,tp,op,np,vo,rp,ct,Ln,sp,St,ap,Nr,ip,dp,js,lp,cp,pp,bo,hp,To,qa,Bt,wo,Ns,In,up,Qs,fp,Ca,_e,Mn,mp,Ls,gp,_p,Sn,vp,Qr,bp,Tp,wp,Bn,kp,Hn,Pp,$p,Ep,ko,yp,pt,Kn,Rp,Ht,Dp,Lr,xp,zp,Is,qp,Cp,Fp,Po,Ap,$o,Fa,Kt,Eo,Ms,Wn,Op,Ss,jp,Aa,ve,Un,Np,Bs,Qp,Lp,Vn,Ip,Ir,Mp,Sp,Bp,Yn,Hp,Xn,Kp,Wp,Up,yo,Vp,ht,Gn,Yp,Wt,Xp,Mr,Gp,Jp,Hs,Zp,eh,th,Ro,oh,Do,Oa;return d=new Ee({}),R=new Ee({}),le=new Ee({}),ce=new U({props:{name:"class transformers.DPRConfig",anchor:"transformers.DPRConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"projection_dim",val:": int = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the DPR model. Defines the different tokens that can be represented by the <em>inputs_ids</em>
passed to the forward method of <a href="/docs/transformers/v4.22.2/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"vocab_size"},{anchor:"transformers.DPRConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.DPRConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.DPRConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.DPRConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.DPRConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.DPRConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.DPRConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.DPRConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.DPRConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <em>token_type_ids</em> passed into <a href="/docs/transformers/v4.22.2/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.DPRConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DPRConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.DPRConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.DPRConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Dimension of the projection for the context and question encoders. If it is set to zero (default), then no
projection is done.`,name:"projection_dim"}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/configuration_dpr.py#L45"}}),Mo=new Ee({}),So=new U({props:{name:"class transformers.DPRContextEncoderTokenizer",anchor:"transformers.DPRContextEncoderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/tokenization_dpr.py#L113"}}),Ho=new Ee({}),Ko=new U({props:{name:"class transformers.DPRContextEncoderTokenizerFast",anchor:"transformers.DPRContextEncoderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/tokenization_dpr_fast.py#L114"}}),Vo=new Ee({}),Yo=new U({props:{name:"class transformers.DPRQuestionEncoderTokenizer",anchor:"transformers.DPRQuestionEncoderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/tokenization_dpr.py#L129"}}),Go=new Ee({}),Jo=new U({props:{name:"class transformers.DPRQuestionEncoderTokenizerFast",anchor:"transformers.DPRQuestionEncoderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/tokenization_dpr_fast.py#L131"}}),tn=new Ee({}),on=new U({props:{name:"class transformers.DPRReaderTokenizer",anchor:"transformers.DPRReaderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRReaderTokenizer.questions",description:`<strong>questions</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code>[questions] * n_passages</code>. Otherwise you have to specify as many questions as in
<code>titles</code> or <code>texts</code>.`,name:"questions"},{anchor:"transformers.DPRReaderTokenizer.titles",description:`<strong>titles</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages titles to be encoded. This can be a string or a list of strings if there are several passages.`,name:"titles"},{anchor:"transformers.DPRReaderTokenizer.texts",description:`<strong>texts</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages texts to be encoded. This can be a string or a list of strings if there are several passages.`,name:"texts"},{anchor:"transformers.DPRReaderTokenizer.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.22.2/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DPRReaderTokenizer.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.22.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to
the maximum acceptable input length for the model if that argument is not provided. This will truncate
token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch
of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the first
sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the
second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.DPRReaderTokenizer.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.DPRReaderTokenizer.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/v4.22.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DPRReaderTokenizer.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/tokenization_dpr.py#L396",returnDescription:`
<p>A dictionary with the following keys:</p>
<ul>
<li><code>input_ids</code>: List of token ids to be fed to a model.</li>
<li><code>attention_mask</code>: List of indices specifying which tokens should be attended to by the model.</li>
</ul>
`,returnType:`
<p><code>Dict[str, List[List[int]]]</code></p>
`}}),ro=new rr({props:{anchor:"transformers.DPRReaderTokenizer.example",$$slots:{default:[gf]},$$scope:{ctx:F}}}),rn=new Ee({}),sn=new U({props:{name:"class transformers.DPRReaderTokenizerFast",anchor:"transformers.DPRReaderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRReaderTokenizerFast.questions",description:`<strong>questions</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code>[questions] * n_passages</code>. Otherwise you have to specify as many questions as in
<code>titles</code> or <code>texts</code>.`,name:"questions"},{anchor:"transformers.DPRReaderTokenizerFast.titles",description:`<strong>titles</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages titles to be encoded. This can be a string or a list of strings if there are several passages.`,name:"titles"},{anchor:"transformers.DPRReaderTokenizerFast.texts",description:`<strong>texts</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages texts to be encoded. This can be a string or a list of strings if there are several passages.`,name:"texts"},{anchor:"transformers.DPRReaderTokenizerFast.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.22.2/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DPRReaderTokenizerFast.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.22.2/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to
the maximum acceptable input length for the model if that argument is not provided. This will truncate
token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch
of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the first
sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the
second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.DPRReaderTokenizerFast.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.DPRReaderTokenizerFast.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/v4.22.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DPRReaderTokenizerFast.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/tokenization_dpr_fast.py#L394",returnDescription:`
<p>A dictionary with the following keys:</p>
<ul>
<li><code>input_ids</code>: List of token ids to be fed to a model.</li>
<li><code>attention_mask</code>: List of indices specifying which tokens should be attended to by the model.</li>
</ul>
`,returnType:`
<p><code>Dict[str, List[List[int]]]</code></p>
`}}),ln=new Ee({}),cn=new U({props:{name:"class transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",parameters:[{name:"pooler_output",val:": FloatTensor"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) &#x2014;
The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.`,name:"pooler_output"},{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/modeling_dpr.py#L62"}}),hn=new U({props:{name:"class transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput",anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput",parameters:[{name:"pooler_output",val:": FloatTensor"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) &#x2014;
The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.`,name:"pooler_output"},{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/modeling_dpr.py#L90"}}),fn=new U({props:{name:"class transformers.DPRReaderOutput",anchor:"transformers.DPRReaderOutput",parameters:[{name:"start_logits",val:": FloatTensor"},{name:"end_logits",val:": FloatTensor = None"},{name:"relevance_logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.DPRReaderOutput.start_logits",description:`<strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) &#x2014;
Logits of the start index of the span for each passage.`,name:"start_logits"},{anchor:"transformers.DPRReaderOutput.end_logits",description:`<strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) &#x2014;
Logits of the end index of the span for each passage.`,name:"end_logits"},{anchor:"transformers.DPRReaderOutput.relevance_logits",description:`<strong>relevance_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, )</code>) &#x2014;
Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.`,name:"relevance_logits"},{anchor:"transformers.DPRReaderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.DPRReaderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/modeling_dpr.py#L118"}}),gn=new Ee({}),_n=new U({props:{name:"class transformers.DPRContextEncoder",anchor:"transformers.DPRContextEncoder",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRContextEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.22.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/modeling_dpr.py#L446"}}),wn=new U({props:{name:"forward",anchor:"transformers.DPRContextEncoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRContextEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/modeling_dpr.py#L454",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),lo=new Vt({props:{$$slots:{default:[_f]},$$scope:{ctx:F}}}),co=new rr({props:{anchor:"transformers.DPRContextEncoder.forward.example",$$slots:{default:[vf]},$$scope:{ctx:F}}}),kn=new Ee({}),Pn=new U({props:{name:"class transformers.DPRQuestionEncoder",anchor:"transformers.DPRQuestionEncoder",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRQuestionEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.22.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/modeling_dpr.py#L527"}}),Rn=new U({props:{name:"forward",anchor:"transformers.DPRQuestionEncoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRQuestionEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/modeling_dpr.py#L535",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ho=new Vt({props:{$$slots:{default:[bf]},$$scope:{ctx:F}}}),uo=new rr({props:{anchor:"transformers.DPRQuestionEncoder.forward.example",$$slots:{default:[Tf]},$$scope:{ctx:F}}}),Dn=new Ee({}),xn=new U({props:{name:"class transformers.DPRReader",anchor:"transformers.DPRReader",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.22.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/modeling_dpr.py#L608"}}),Fn=new U({props:{name:"forward",anchor:"transformers.DPRReader.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": bool = None"},{name:"output_hidden_states",val:": bool = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRReader.forward.input_ids",description:`<strong>input_ids</strong> (<code>Tuple[torch.LongTensor]</code> of shapes <code>(n_passages, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. It has to be a sequence triplet with 1) the question
and 2) the passages titles and 3) the passages texts To match pretraining, DPR <code>input_ids</code> sequence should
be formatted with [CLS] and [SEP] with the format:</p>
<p><code>[CLS] &lt;question token ids&gt; [SEP] &lt;titles ids&gt; [SEP] &lt;texts ids&gt;</code></p>
<p>DPR is a model with absolute position embeddings so it&#x2019;s usually advised to pad the inputs on the right
rather than the left.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRReaderTokenizer">DPRReaderTokenizer</a>. See this class documentation for more details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DPRReader.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DPRReader.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DPRReader.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DPRReader.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DPRReader.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.22.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/modeling_dpr.py#L616",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRReaderOutput"
>transformers.models.dpr.modeling_dpr.DPRReaderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the start index of the span for each passage.</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the end index of the span for each passage.</p>
</li>
<li>
<p><strong>relevance_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, )</code>) \u2014 Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRReaderOutput"
>transformers.models.dpr.modeling_dpr.DPRReaderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),mo=new Vt({props:{$$slots:{default:[wf]},$$scope:{ctx:F}}}),go=new rr({props:{anchor:"transformers.DPRReader.forward.example",$$slots:{default:[kf]},$$scope:{ctx:F}}}),An=new Ee({}),On=new U({props:{name:"class transformers.TFDPRContextEncoder",anchor:"transformers.TFDPRContextEncoder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRContextEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.22.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/modeling_tf_dpr.py#L540"}}),vo=new Vt({props:{$$slots:{default:[Pf]},$$scope:{ctx:F}}}),Ln=new U({props:{name:"call",anchor:"transformers.TFDPRContextEncoder.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRContextEncoder.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/modeling_tf_dpr.py#L552",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),bo=new Vt({props:{$$slots:{default:[$f]},$$scope:{ctx:F}}}),To=new rr({props:{anchor:"transformers.TFDPRContextEncoder.call.example",$$slots:{default:[Ef]},$$scope:{ctx:F}}}),In=new Ee({}),Mn=new U({props:{name:"class transformers.TFDPRQuestionEncoder",anchor:"transformers.TFDPRQuestionEncoder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRQuestionEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.22.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/modeling_tf_dpr.py#L627"}}),ko=new Vt({props:{$$slots:{default:[yf]},$$scope:{ctx:F}}}),Kn=new U({props:{name:"call",anchor:"transformers.TFDPRQuestionEncoder.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRQuestionEncoder.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/modeling_tf_dpr.py#L639",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),Po=new Vt({props:{$$slots:{default:[Rf]},$$scope:{ctx:F}}}),$o=new rr({props:{anchor:"transformers.TFDPRQuestionEncoder.call.example",$$slots:{default:[Df]},$$scope:{ctx:F}}}),Wn=new Ee({}),Un=new U({props:{name:"class transformers.TFDPRReader",anchor:"transformers.TFDPRReader",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.22.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/modeling_tf_dpr.py#L713"}}),yo=new Vt({props:{$$slots:{default:[xf]},$$scope:{ctx:F}}}),Gn=new U({props:{name:"call",anchor:"transformers.TFDPRReader.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:": bool = None"},{name:"output_hidden_states",val:": bool = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRReader.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shapes <code>(n_passages, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. It has to be a sequence triplet with 1) the question
and 2) the passages titles and 3) the passages texts To match pretraining, DPR <code>input_ids</code> sequence should
be formatted with [CLS] and [SEP] with the format:</p>
<p><code>[CLS] &lt;question token ids&gt; [SEP] &lt;titles ids&gt; [SEP] &lt;texts ids&gt;</code></p>
<p>DPR is a model with absolute position embeddings so it&#x2019;s usually advised to pad the inputs on the right
rather than the left.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRReaderTokenizer">DPRReaderTokenizer</a>. See this class documentation for more details.`,name:"input_ids"},{anchor:"transformers.TFDPRReader.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDPRReader.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(n_passages, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDPRReader.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDPRReader.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.22.2/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDPRReader.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],source:"https://github.com/huggingface/transformers/blob/v4.22.2/src/transformers/models/dpr/modeling_tf_dpr.py#L725",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the start index of the span for each passage.</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the end index of the span for each passage.</p>
</li>
<li>
<p><strong>relevance_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, )</code>) \u2014 Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),Ro=new Vt({props:{$$slots:{default:[zf]},$$scope:{ctx:F}}}),Do=new rr({props:{anchor:"transformers.TFDPRReader.call.example",$$slots:{default:[qf]},$$scope:{ctx:F}}}),{c(){h=s("meta"),T=c(),_=s("h1"),g=s("a"),b=s("span"),w(d.$$.fragment),u=c(),q=s("span"),ye=n("DPR"),be=c(),L=s("h2"),te=s("a"),oe=s("span"),w(R.$$.fragment),Re=c(),H=s("span"),De=n("Overview"),Te=c(),I=s("p"),xe=n(`Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. It was
introduced in `),se=s("a"),V=n("Dense Passage Retrieval for Open-Domain Question Answering"),ze=n(` by
Vladimir Karpukhin, Barlas O\u011Fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.`),we=c(),S=s("p"),qe=n("The abstract from the paper is the following:"),ke=c(),B=s("p"),ue=s("em"),Ce=n(`Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional
sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can
be practically implemented using dense representations alone, where embeddings are learned from a small number of
questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,
our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage
retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.`),ae=c(),z=s("p"),O=n("This model was contributed by "),ie=s("a"),Y=n("lhoestq"),Fe=n(". The original code can be found "),de=s("a"),X=n("here"),Ae=n("."),Pe=c(),C=s("h2"),ne=s("a"),K=s("span"),w(le.$$.fragment),Oe=c(),W=s("span"),je=n("DPRConfig"),$e=c(),A=s("div"),w(ce.$$.fragment),G=c(),re=s("p"),J=s("a"),j=n("DPRConfig"),Ne=n(" is the configuration class to store the configuration of a "),N=s("em"),Qe=n("DPRModel"),Le=n("."),v=c(),D=s("p"),Xe=n("This is the configuration class to store the configuration of a "),Z=s("a"),Ge=n("DPRContextEncoder"),Je=n(", "),x=s("a"),Ze=n("DPRQuestionEncoder"),et=n(`, or a
`),Ie=s("a"),tt=n("DPRReader"),ot=n(`. It is used to instantiate the components of the DPR model according to the specified arguments,
defining the model component architectures. Instantiating a configuration with the defaults will yield a similar
configuration to that of the DPRContextEncoder
`),Q=s("a"),ee=n("facebook/dpr-ctx_encoder-single-nq-base"),nt=n(`
architecture.`),rt=c(),M=s("p"),st=n("This class is a subclass of "),Me=s("a"),Se=n("BertConfig"),at=n(". Please check the superclass for the documentation of all kwargs."),pe=c(),he=s("h2"),Yt=s("a"),Vr=s("span"),w(Mo.$$.fragment),$i=c(),Yr=s("span"),Ei=n("DPRContextEncoderTokenizer"),da=c(),We=s("div"),w(So.$$.fragment),yi=c(),Xr=s("p"),Ri=n("Construct a DPRContextEncoder tokenizer."),Di=c(),Xt=s("p"),ir=s("a"),xi=n("DPRContextEncoderTokenizer"),zi=n(" is identical to "),dr=s("a"),qi=n("BertTokenizer"),Ci=n(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Fi=c(),Bo=s("p"),Ai=n("Refer to superclass "),lr=s("a"),Oi=n("BertTokenizer"),ji=n(" for usage examples and documentation concerning parameters."),la=c(),yt=s("h2"),Gt=s("a"),Gr=s("span"),w(Ho.$$.fragment),Ni=c(),Jr=s("span"),Qi=n("DPRContextEncoderTokenizerFast"),ca=c(),Ue=s("div"),w(Ko.$$.fragment),Li=c(),Wo=s("p"),Ii=n("Construct a \u201Cfast\u201D DPRContextEncoder tokenizer (backed by HuggingFace\u2019s "),Zr=s("em"),Mi=n("tokenizers"),Si=n(" library)."),Bi=c(),Jt=s("p"),cr=s("a"),Hi=n("DPRContextEncoderTokenizerFast"),Ki=n(" is identical to "),pr=s("a"),Wi=n("BertTokenizerFast"),Ui=n(` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Vi=c(),Uo=s("p"),Yi=n("Refer to superclass "),hr=s("a"),Xi=n("BertTokenizerFast"),Gi=n(" for usage examples and documentation concerning parameters."),pa=c(),Rt=s("h2"),Zt=s("a"),es=s("span"),w(Vo.$$.fragment),Ji=c(),ts=s("span"),Zi=n("DPRQuestionEncoderTokenizer"),ha=c(),Ve=s("div"),w(Yo.$$.fragment),ed=c(),os=s("p"),td=n("Constructs a DPRQuestionEncoder tokenizer."),od=c(),eo=s("p"),ur=s("a"),nd=n("DPRQuestionEncoderTokenizer"),rd=n(" is identical to "),fr=s("a"),sd=n("BertTokenizer"),ad=n(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),id=c(),Xo=s("p"),dd=n("Refer to superclass "),mr=s("a"),ld=n("BertTokenizer"),cd=n(" for usage examples and documentation concerning parameters."),ua=c(),Dt=s("h2"),to=s("a"),ns=s("span"),w(Go.$$.fragment),pd=c(),rs=s("span"),hd=n("DPRQuestionEncoderTokenizerFast"),fa=c(),Ye=s("div"),w(Jo.$$.fragment),ud=c(),Zo=s("p"),fd=n("Constructs a \u201Cfast\u201D DPRQuestionEncoder tokenizer (backed by HuggingFace\u2019s "),ss=s("em"),md=n("tokenizers"),gd=n(" library)."),_d=c(),oo=s("p"),gr=s("a"),vd=n("DPRQuestionEncoderTokenizerFast"),bd=n(" is identical to "),_r=s("a"),Td=n("BertTokenizerFast"),wd=n(` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),kd=c(),en=s("p"),Pd=n("Refer to superclass "),vr=s("a"),$d=n("BertTokenizerFast"),Ed=n(" for usage examples and documentation concerning parameters."),ma=c(),xt=s("h2"),no=s("a"),as=s("span"),w(tn.$$.fragment),yd=c(),is=s("span"),Rd=n("DPRReaderTokenizer"),ga=c(),fe=s("div"),w(on.$$.fragment),Dd=c(),ds=s("p"),xd=n("Construct a DPRReader tokenizer."),zd=c(),bt=s("p"),br=s("a"),qd=n("DPRReaderTokenizer"),Cd=n(" is almost identical to "),Tr=s("a"),Fd=n("BertTokenizer"),Ad=n(` and runs end-to-end tokenization: punctuation
splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts that are
combined to be fed to the `),wr=s("a"),Od=n("DPRReader"),jd=n(" model."),Nd=c(),nn=s("p"),Qd=n("Refer to superclass "),kr=s("a"),Ld=n("BertTokenizer"),Id=n(" for usage examples and documentation concerning parameters."),Md=c(),Tt=s("p"),Sd=n("Return a dictionary with the token ids of the input strings and other information to give to "),ls=s("code"),Bd=n(".decode_best_spans"),Hd=n(`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),cs=s("code"),Kd=n("input_ids"),Wd=n(" is a matrix of size "),ps=s("code"),Ud=n("(n_passages, sequence_length)"),Vd=c(),w(ro.$$.fragment),_a=c(),zt=s("h2"),so=s("a"),hs=s("span"),w(rn.$$.fragment),Yd=c(),us=s("span"),Xd=n("DPRReaderTokenizerFast"),va=c(),me=s("div"),w(sn.$$.fragment),Gd=c(),an=s("p"),Jd=n("Constructs a \u201Cfast\u201D DPRReader tokenizer (backed by HuggingFace\u2019s "),fs=s("em"),Zd=n("tokenizers"),el=n(" library)."),tl=c(),wt=s("p"),Pr=s("a"),ol=n("DPRReaderTokenizerFast"),nl=n(" is almost identical to "),$r=s("a"),rl=n("BertTokenizerFast"),sl=n(` and runs end-to-end tokenization:
punctuation splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts
that are combined to be fed to the `),Er=s("a"),al=n("DPRReader"),il=n(" model."),dl=c(),dn=s("p"),ll=n("Refer to superclass "),yr=s("a"),cl=n("BertTokenizerFast"),pl=n(" for usage examples and documentation concerning parameters."),hl=c(),vt=s("p"),ul=n("Return a dictionary with the token ids of the input strings and other information to give to "),ms=s("code"),fl=n(".decode_best_spans"),ml=n(`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),gs=s("code"),gl=n("input_ids"),_l=n(" is a matrix of size "),_s=s("code"),vl=n("(n_passages, sequence_length)"),bl=n(`
with the format:`),Tl=c(),vs=s("p"),wl=n("[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>"),ba=c(),qt=s("h2"),ao=s("a"),bs=s("span"),w(ln.$$.fragment),kl=c(),Ts=s("span"),Pl=n("DPR specific outputs"),Ta=c(),Ct=s("div"),w(cn.$$.fragment),$l=c(),pn=s("p"),El=n("Class for outputs of "),Rr=s("a"),yl=n("DPRQuestionEncoder"),Rl=n("."),wa=c(),Ft=s("div"),w(hn.$$.fragment),Dl=c(),un=s("p"),xl=n("Class for outputs of "),Dr=s("a"),zl=n("DPRQuestionEncoder"),ql=n("."),ka=c(),At=s("div"),w(fn.$$.fragment),Cl=c(),mn=s("p"),Fl=n("Class for outputs of "),xr=s("a"),Al=n("DPRQuestionEncoder"),Ol=n("."),Pa=c(),Ot=s("h2"),io=s("a"),ws=s("span"),w(gn.$$.fragment),jl=c(),ks=s("span"),Nl=n("DPRContextEncoder"),$a=c(),Be=s("div"),w(_n.$$.fragment),Ql=c(),Ps=s("p"),Ll=n("The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Il=c(),vn=s("p"),Ml=n("This model inherits from "),zr=s("a"),Sl=n("PreTrainedModel"),Bl=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Hl=c(),bn=s("p"),Kl=n("This model is also a PyTorch "),Tn=s("a"),Wl=n("torch.nn.Module"),Ul=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Vl=c(),it=s("div"),w(wn.$$.fragment),Yl=c(),jt=s("p"),Xl=n("The "),qr=s("a"),Gl=n("DPRContextEncoder"),Jl=n(" forward method, overrides the "),$s=s("code"),Zl=n("__call__"),ec=n(" special method."),tc=c(),w(lo.$$.fragment),oc=c(),w(co.$$.fragment),Ea=c(),Nt=s("h2"),po=s("a"),Es=s("span"),w(kn.$$.fragment),nc=c(),ys=s("span"),rc=n("DPRQuestionEncoder"),ya=c(),He=s("div"),w(Pn.$$.fragment),sc=c(),Rs=s("p"),ac=n("The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),ic=c(),$n=s("p"),dc=n("This model inherits from "),Cr=s("a"),lc=n("PreTrainedModel"),cc=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),pc=c(),En=s("p"),hc=n("This model is also a PyTorch "),yn=s("a"),uc=n("torch.nn.Module"),fc=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),mc=c(),dt=s("div"),w(Rn.$$.fragment),gc=c(),Qt=s("p"),_c=n("The "),Fr=s("a"),vc=n("DPRQuestionEncoder"),bc=n(" forward method, overrides the "),Ds=s("code"),Tc=n("__call__"),wc=n(" special method."),kc=c(),w(ho.$$.fragment),Pc=c(),w(uo.$$.fragment),Ra=c(),Lt=s("h2"),fo=s("a"),xs=s("span"),w(Dn.$$.fragment),$c=c(),zs=s("span"),Ec=n("DPRReader"),Da=c(),Ke=s("div"),w(xn.$$.fragment),yc=c(),qs=s("p"),Rc=n("The bare DPRReader transformer outputting span predictions."),Dc=c(),zn=s("p"),xc=n("This model inherits from "),Ar=s("a"),zc=n("PreTrainedModel"),qc=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Cc=c(),qn=s("p"),Fc=n("This model is also a PyTorch "),Cn=s("a"),Ac=n("torch.nn.Module"),Oc=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),jc=c(),lt=s("div"),w(Fn.$$.fragment),Nc=c(),It=s("p"),Qc=n("The "),Or=s("a"),Lc=n("DPRReader"),Ic=n(" forward method, overrides the "),Cs=s("code"),Mc=n("__call__"),Sc=n(" special method."),Bc=c(),w(mo.$$.fragment),Hc=c(),w(go.$$.fragment),xa=c(),Mt=s("h2"),_o=s("a"),Fs=s("span"),w(An.$$.fragment),Kc=c(),As=s("span"),Wc=n("TFDPRContextEncoder"),za=c(),ge=s("div"),w(On.$$.fragment),Uc=c(),Os=s("p"),Vc=n("The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Yc=c(),jn=s("p"),Xc=n("This model inherits from "),jr=s("a"),Gc=n("TFPreTrainedModel"),Jc=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Zc=c(),Nn=s("p"),ep=n("This model is also a Tensorflow "),Qn=s("a"),tp=n("tf.keras.Model"),op=n(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),np=c(),w(vo.$$.fragment),rp=c(),ct=s("div"),w(Ln.$$.fragment),sp=c(),St=s("p"),ap=n("The "),Nr=s("a"),ip=n("TFDPRContextEncoder"),dp=n(" forward method, overrides the "),js=s("code"),lp=n("__call__"),cp=n(" special method."),pp=c(),w(bo.$$.fragment),hp=c(),w(To.$$.fragment),qa=c(),Bt=s("h2"),wo=s("a"),Ns=s("span"),w(In.$$.fragment),up=c(),Qs=s("span"),fp=n("TFDPRQuestionEncoder"),Ca=c(),_e=s("div"),w(Mn.$$.fragment),mp=c(),Ls=s("p"),gp=n("The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),_p=c(),Sn=s("p"),vp=n("This model inherits from "),Qr=s("a"),bp=n("TFPreTrainedModel"),Tp=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),wp=c(),Bn=s("p"),kp=n("This model is also a Tensorflow "),Hn=s("a"),Pp=n("tf.keras.Model"),$p=n(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),Ep=c(),w(ko.$$.fragment),yp=c(),pt=s("div"),w(Kn.$$.fragment),Rp=c(),Ht=s("p"),Dp=n("The "),Lr=s("a"),xp=n("TFDPRQuestionEncoder"),zp=n(" forward method, overrides the "),Is=s("code"),qp=n("__call__"),Cp=n(" special method."),Fp=c(),w(Po.$$.fragment),Ap=c(),w($o.$$.fragment),Fa=c(),Kt=s("h2"),Eo=s("a"),Ms=s("span"),w(Wn.$$.fragment),Op=c(),Ss=s("span"),jp=n("TFDPRReader"),Aa=c(),ve=s("div"),w(Un.$$.fragment),Np=c(),Bs=s("p"),Qp=n("The bare DPRReader transformer outputting span predictions."),Lp=c(),Vn=s("p"),Ip=n("This model inherits from "),Ir=s("a"),Mp=n("TFPreTrainedModel"),Sp=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Bp=c(),Yn=s("p"),Hp=n("This model is also a Tensorflow "),Xn=s("a"),Kp=n("tf.keras.Model"),Wp=n(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),Up=c(),w(yo.$$.fragment),Vp=c(),ht=s("div"),w(Gn.$$.fragment),Yp=c(),Wt=s("p"),Xp=n("The "),Mr=s("a"),Gp=n("TFDPRReader"),Jp=n(" forward method, overrides the "),Hs=s("code"),Zp=n("__call__"),eh=n(" special method."),th=c(),w(Ro.$$.fragment),oh=c(),w(Do.$$.fragment),this.h()},l(o){const m=ff('[data-svelte="svelte-1phssyn"]',document.head);h=a(m,"META",{name:!0,content:!0}),m.forEach(t),T=p(o),_=a(o,"H1",{class:!0});var Jn=i(_);g=a(Jn,"A",{id:!0,class:!0,href:!0});var Ks=i(g);b=a(Ks,"SPAN",{});var Ws=i(b);k(d.$$.fragment,Ws),Ws.forEach(t),Ks.forEach(t),u=p(Jn),q=a(Jn,"SPAN",{});var Us=i(q);ye=r(Us,"DPR"),Us.forEach(t),Jn.forEach(t),be=p(o),L=a(o,"H2",{class:!0});var Zn=i(L);te=a(Zn,"A",{id:!0,class:!0,href:!0});var Vs=i(te);oe=a(Vs,"SPAN",{});var Ys=i(oe);k(R.$$.fragment,Ys),Ys.forEach(t),Vs.forEach(t),Re=p(Zn),H=a(Zn,"SPAN",{});var Xs=i(H);De=r(Xs,"Overview"),Xs.forEach(t),Zn.forEach(t),Te=p(o),I=a(o,"P",{});var er=i(I);xe=r(er,`Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. It was
introduced in `),se=a(er,"A",{href:!0,rel:!0});var Gs=i(se);V=r(Gs,"Dense Passage Retrieval for Open-Domain Question Answering"),Gs.forEach(t),ze=r(er,` by
Vladimir Karpukhin, Barlas O\u011Fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.`),er.forEach(t),we=p(o),S=a(o,"P",{});var Js=i(S);qe=r(Js,"The abstract from the paper is the following:"),Js.forEach(t),ke=p(o),B=a(o,"P",{});var Zs=i(B);ue=a(Zs,"EM",{});var ea=i(ue);Ce=r(ea,`Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional
sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can
be practically implemented using dense representations alone, where embeddings are learned from a small number of
questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,
our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage
retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.`),ea.forEach(t),Zs.forEach(t),ae=p(o),z=a(o,"P",{});var Ut=i(z);O=r(Ut,"This model was contributed by "),ie=a(Ut,"A",{href:!0,rel:!0});var ta=i(ie);Y=r(ta,"lhoestq"),ta.forEach(t),Fe=r(Ut,". The original code can be found "),de=a(Ut,"A",{href:!0,rel:!0});var oa=i(de);X=r(oa,"here"),oa.forEach(t),Ae=r(Ut,"."),Ut.forEach(t),Pe=p(o),C=a(o,"H2",{class:!0});var ja=i(C);ne=a(ja,"A",{id:!0,class:!0,href:!0});var nh=i(ne);K=a(nh,"SPAN",{});var rh=i(K);k(le.$$.fragment,rh),rh.forEach(t),nh.forEach(t),Oe=p(ja),W=a(ja,"SPAN",{});var sh=i(W);je=r(sh,"DPRConfig"),sh.forEach(t),ja.forEach(t),$e=p(o),A=a(o,"DIV",{class:!0});var xo=i(A);k(ce.$$.fragment,xo),G=p(xo),re=a(xo,"P",{});var na=i(re);J=a(na,"A",{href:!0});var ah=i(J);j=r(ah,"DPRConfig"),ah.forEach(t),Ne=r(na," is the configuration class to store the configuration of a "),N=a(na,"EM",{});var ih=i(N);Qe=r(ih,"DPRModel"),ih.forEach(t),Le=r(na,"."),na.forEach(t),v=p(xo),D=a(xo,"P",{});var kt=i(D);Xe=r(kt,"This is the configuration class to store the configuration of a "),Z=a(kt,"A",{href:!0});var dh=i(Z);Ge=r(dh,"DPRContextEncoder"),dh.forEach(t),Je=r(kt,", "),x=a(kt,"A",{href:!0});var lh=i(x);Ze=r(lh,"DPRQuestionEncoder"),lh.forEach(t),et=r(kt,`, or a
`),Ie=a(kt,"A",{href:!0});var ch=i(Ie);tt=r(ch,"DPRReader"),ch.forEach(t),ot=r(kt,`. It is used to instantiate the components of the DPR model according to the specified arguments,
defining the model component architectures. Instantiating a configuration with the defaults will yield a similar
configuration to that of the DPRContextEncoder
`),Q=a(kt,"A",{href:!0,rel:!0});var ph=i(Q);ee=r(ph,"facebook/dpr-ctx_encoder-single-nq-base"),ph.forEach(t),nt=r(kt,`
architecture.`),kt.forEach(t),rt=p(xo),M=a(xo,"P",{});var Na=i(M);st=r(Na,"This class is a subclass of "),Me=a(Na,"A",{href:!0});var hh=i(Me);Se=r(hh,"BertConfig"),hh.forEach(t),at=r(Na,". Please check the superclass for the documentation of all kwargs."),Na.forEach(t),xo.forEach(t),pe=p(o),he=a(o,"H2",{class:!0});var Qa=i(he);Yt=a(Qa,"A",{id:!0,class:!0,href:!0});var uh=i(Yt);Vr=a(uh,"SPAN",{});var fh=i(Vr);k(Mo.$$.fragment,fh),fh.forEach(t),uh.forEach(t),$i=p(Qa),Yr=a(Qa,"SPAN",{});var mh=i(Yr);Ei=r(mh,"DPRContextEncoderTokenizer"),mh.forEach(t),Qa.forEach(t),da=p(o),We=a(o,"DIV",{class:!0});var zo=i(We);k(So.$$.fragment,zo),yi=p(zo),Xr=a(zo,"P",{});var gh=i(Xr);Ri=r(gh,"Construct a DPRContextEncoder tokenizer."),gh.forEach(t),Di=p(zo),Xt=a(zo,"P",{});var ra=i(Xt);ir=a(ra,"A",{href:!0});var _h=i(ir);xi=r(_h,"DPRContextEncoderTokenizer"),_h.forEach(t),zi=r(ra," is identical to "),dr=a(ra,"A",{href:!0});var vh=i(dr);qi=r(vh,"BertTokenizer"),vh.forEach(t),Ci=r(ra,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),ra.forEach(t),Fi=p(zo),Bo=a(zo,"P",{});var La=i(Bo);Ai=r(La,"Refer to superclass "),lr=a(La,"A",{href:!0});var bh=i(lr);Oi=r(bh,"BertTokenizer"),bh.forEach(t),ji=r(La," for usage examples and documentation concerning parameters."),La.forEach(t),zo.forEach(t),la=p(o),yt=a(o,"H2",{class:!0});var Ia=i(yt);Gt=a(Ia,"A",{id:!0,class:!0,href:!0});var Th=i(Gt);Gr=a(Th,"SPAN",{});var wh=i(Gr);k(Ho.$$.fragment,wh),wh.forEach(t),Th.forEach(t),Ni=p(Ia),Jr=a(Ia,"SPAN",{});var kh=i(Jr);Qi=r(kh,"DPRContextEncoderTokenizerFast"),kh.forEach(t),Ia.forEach(t),ca=p(o),Ue=a(o,"DIV",{class:!0});var qo=i(Ue);k(Ko.$$.fragment,qo),Li=p(qo),Wo=a(qo,"P",{});var Ma=i(Wo);Ii=r(Ma,"Construct a \u201Cfast\u201D DPRContextEncoder tokenizer (backed by HuggingFace\u2019s "),Zr=a(Ma,"EM",{});var Ph=i(Zr);Mi=r(Ph,"tokenizers"),Ph.forEach(t),Si=r(Ma," library)."),Ma.forEach(t),Bi=p(qo),Jt=a(qo,"P",{});var sa=i(Jt);cr=a(sa,"A",{href:!0});var $h=i(cr);Hi=r($h,"DPRContextEncoderTokenizerFast"),$h.forEach(t),Ki=r(sa," is identical to "),pr=a(sa,"A",{href:!0});var Eh=i(pr);Wi=r(Eh,"BertTokenizerFast"),Eh.forEach(t),Ui=r(sa,` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),sa.forEach(t),Vi=p(qo),Uo=a(qo,"P",{});var Sa=i(Uo);Yi=r(Sa,"Refer to superclass "),hr=a(Sa,"A",{href:!0});var yh=i(hr);Xi=r(yh,"BertTokenizerFast"),yh.forEach(t),Gi=r(Sa," for usage examples and documentation concerning parameters."),Sa.forEach(t),qo.forEach(t),pa=p(o),Rt=a(o,"H2",{class:!0});var Ba=i(Rt);Zt=a(Ba,"A",{id:!0,class:!0,href:!0});var Rh=i(Zt);es=a(Rh,"SPAN",{});var Dh=i(es);k(Vo.$$.fragment,Dh),Dh.forEach(t),Rh.forEach(t),Ji=p(Ba),ts=a(Ba,"SPAN",{});var xh=i(ts);Zi=r(xh,"DPRQuestionEncoderTokenizer"),xh.forEach(t),Ba.forEach(t),ha=p(o),Ve=a(o,"DIV",{class:!0});var Co=i(Ve);k(Yo.$$.fragment,Co),ed=p(Co),os=a(Co,"P",{});var zh=i(os);td=r(zh,"Constructs a DPRQuestionEncoder tokenizer."),zh.forEach(t),od=p(Co),eo=a(Co,"P",{});var aa=i(eo);ur=a(aa,"A",{href:!0});var qh=i(ur);nd=r(qh,"DPRQuestionEncoderTokenizer"),qh.forEach(t),rd=r(aa," is identical to "),fr=a(aa,"A",{href:!0});var Ch=i(fr);sd=r(Ch,"BertTokenizer"),Ch.forEach(t),ad=r(aa,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),aa.forEach(t),id=p(Co),Xo=a(Co,"P",{});var Ha=i(Xo);dd=r(Ha,"Refer to superclass "),mr=a(Ha,"A",{href:!0});var Fh=i(mr);ld=r(Fh,"BertTokenizer"),Fh.forEach(t),cd=r(Ha," for usage examples and documentation concerning parameters."),Ha.forEach(t),Co.forEach(t),ua=p(o),Dt=a(o,"H2",{class:!0});var Ka=i(Dt);to=a(Ka,"A",{id:!0,class:!0,href:!0});var Ah=i(to);ns=a(Ah,"SPAN",{});var Oh=i(ns);k(Go.$$.fragment,Oh),Oh.forEach(t),Ah.forEach(t),pd=p(Ka),rs=a(Ka,"SPAN",{});var jh=i(rs);hd=r(jh,"DPRQuestionEncoderTokenizerFast"),jh.forEach(t),Ka.forEach(t),fa=p(o),Ye=a(o,"DIV",{class:!0});var Fo=i(Ye);k(Jo.$$.fragment,Fo),ud=p(Fo),Zo=a(Fo,"P",{});var Wa=i(Zo);fd=r(Wa,"Constructs a \u201Cfast\u201D DPRQuestionEncoder tokenizer (backed by HuggingFace\u2019s "),ss=a(Wa,"EM",{});var Nh=i(ss);md=r(Nh,"tokenizers"),Nh.forEach(t),gd=r(Wa," library)."),Wa.forEach(t),_d=p(Fo),oo=a(Fo,"P",{});var ia=i(oo);gr=a(ia,"A",{href:!0});var Qh=i(gr);vd=r(Qh,"DPRQuestionEncoderTokenizerFast"),Qh.forEach(t),bd=r(ia," is identical to "),_r=a(ia,"A",{href:!0});var Lh=i(_r);Td=r(Lh,"BertTokenizerFast"),Lh.forEach(t),wd=r(ia,` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),ia.forEach(t),kd=p(Fo),en=a(Fo,"P",{});var Ua=i(en);Pd=r(Ua,"Refer to superclass "),vr=a(Ua,"A",{href:!0});var Ih=i(vr);$d=r(Ih,"BertTokenizerFast"),Ih.forEach(t),Ed=r(Ua," for usage examples and documentation concerning parameters."),Ua.forEach(t),Fo.forEach(t),ma=p(o),xt=a(o,"H2",{class:!0});var Va=i(xt);no=a(Va,"A",{id:!0,class:!0,href:!0});var Mh=i(no);as=a(Mh,"SPAN",{});var Sh=i(as);k(tn.$$.fragment,Sh),Sh.forEach(t),Mh.forEach(t),yd=p(Va),is=a(Va,"SPAN",{});var Bh=i(is);Rd=r(Bh,"DPRReaderTokenizer"),Bh.forEach(t),Va.forEach(t),ga=p(o),fe=a(o,"DIV",{class:!0});var ut=i(fe);k(on.$$.fragment,ut),Dd=p(ut),ds=a(ut,"P",{});var Hh=i(ds);xd=r(Hh,"Construct a DPRReader tokenizer."),Hh.forEach(t),zd=p(ut),bt=a(ut,"P",{});var tr=i(bt);br=a(tr,"A",{href:!0});var Kh=i(br);qd=r(Kh,"DPRReaderTokenizer"),Kh.forEach(t),Cd=r(tr," is almost identical to "),Tr=a(tr,"A",{href:!0});var Wh=i(Tr);Fd=r(Wh,"BertTokenizer"),Wh.forEach(t),Ad=r(tr,` and runs end-to-end tokenization: punctuation
splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts that are
combined to be fed to the `),wr=a(tr,"A",{href:!0});var Uh=i(wr);Od=r(Uh,"DPRReader"),Uh.forEach(t),jd=r(tr," model."),tr.forEach(t),Nd=p(ut),nn=a(ut,"P",{});var Ya=i(nn);Qd=r(Ya,"Refer to superclass "),kr=a(Ya,"A",{href:!0});var Vh=i(kr);Ld=r(Vh,"BertTokenizer"),Vh.forEach(t),Id=r(Ya," for usage examples and documentation concerning parameters."),Ya.forEach(t),Md=p(ut),Tt=a(ut,"P",{});var or=i(Tt);Sd=r(or,"Return a dictionary with the token ids of the input strings and other information to give to "),ls=a(or,"CODE",{});var Yh=i(ls);Bd=r(Yh,".decode_best_spans"),Yh.forEach(t),Hd=r(or,`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),cs=a(or,"CODE",{});var Xh=i(cs);Kd=r(Xh,"input_ids"),Xh.forEach(t),Wd=r(or," is a matrix of size "),ps=a(or,"CODE",{});var Gh=i(ps);Ud=r(Gh,"(n_passages, sequence_length)"),Gh.forEach(t),or.forEach(t),Vd=p(ut),k(ro.$$.fragment,ut),ut.forEach(t),_a=p(o),zt=a(o,"H2",{class:!0});var Xa=i(zt);so=a(Xa,"A",{id:!0,class:!0,href:!0});var Jh=i(so);hs=a(Jh,"SPAN",{});var Zh=i(hs);k(rn.$$.fragment,Zh),Zh.forEach(t),Jh.forEach(t),Yd=p(Xa),us=a(Xa,"SPAN",{});var eu=i(us);Xd=r(eu,"DPRReaderTokenizerFast"),eu.forEach(t),Xa.forEach(t),va=p(o),me=a(o,"DIV",{class:!0});var ft=i(me);k(sn.$$.fragment,ft),Gd=p(ft),an=a(ft,"P",{});var Ga=i(an);Jd=r(Ga,"Constructs a \u201Cfast\u201D DPRReader tokenizer (backed by HuggingFace\u2019s "),fs=a(Ga,"EM",{});var tu=i(fs);Zd=r(tu,"tokenizers"),tu.forEach(t),el=r(Ga," library)."),Ga.forEach(t),tl=p(ft),wt=a(ft,"P",{});var nr=i(wt);Pr=a(nr,"A",{href:!0});var ou=i(Pr);ol=r(ou,"DPRReaderTokenizerFast"),ou.forEach(t),nl=r(nr," is almost identical to "),$r=a(nr,"A",{href:!0});var nu=i($r);rl=r(nu,"BertTokenizerFast"),nu.forEach(t),sl=r(nr,` and runs end-to-end tokenization:
punctuation splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts
that are combined to be fed to the `),Er=a(nr,"A",{href:!0});var ru=i(Er);al=r(ru,"DPRReader"),ru.forEach(t),il=r(nr," model."),nr.forEach(t),dl=p(ft),dn=a(ft,"P",{});var Ja=i(dn);ll=r(Ja,"Refer to superclass "),yr=a(Ja,"A",{href:!0});var su=i(yr);cl=r(su,"BertTokenizerFast"),su.forEach(t),pl=r(Ja," for usage examples and documentation concerning parameters."),Ja.forEach(t),hl=p(ft),vt=a(ft,"P",{});var Ao=i(vt);ul=r(Ao,"Return a dictionary with the token ids of the input strings and other information to give to "),ms=a(Ao,"CODE",{});var au=i(ms);fl=r(au,".decode_best_spans"),au.forEach(t),ml=r(Ao,`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),gs=a(Ao,"CODE",{});var iu=i(gs);gl=r(iu,"input_ids"),iu.forEach(t),_l=r(Ao," is a matrix of size "),_s=a(Ao,"CODE",{});var du=i(_s);vl=r(du,"(n_passages, sequence_length)"),du.forEach(t),bl=r(Ao,`
with the format:`),Ao.forEach(t),Tl=p(ft),vs=a(ft,"P",{});var lu=i(vs);wl=r(lu,"[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>"),lu.forEach(t),ft.forEach(t),ba=p(o),qt=a(o,"H2",{class:!0});var Za=i(qt);ao=a(Za,"A",{id:!0,class:!0,href:!0});var cu=i(ao);bs=a(cu,"SPAN",{});var pu=i(bs);k(ln.$$.fragment,pu),pu.forEach(t),cu.forEach(t),kl=p(Za),Ts=a(Za,"SPAN",{});var hu=i(Ts);Pl=r(hu,"DPR specific outputs"),hu.forEach(t),Za.forEach(t),Ta=p(o),Ct=a(o,"DIV",{class:!0});var ei=i(Ct);k(cn.$$.fragment,ei),$l=p(ei),pn=a(ei,"P",{});var ti=i(pn);El=r(ti,"Class for outputs of "),Rr=a(ti,"A",{href:!0});var uu=i(Rr);yl=r(uu,"DPRQuestionEncoder"),uu.forEach(t),Rl=r(ti,"."),ti.forEach(t),ei.forEach(t),wa=p(o),Ft=a(o,"DIV",{class:!0});var oi=i(Ft);k(hn.$$.fragment,oi),Dl=p(oi),un=a(oi,"P",{});var ni=i(un);xl=r(ni,"Class for outputs of "),Dr=a(ni,"A",{href:!0});var fu=i(Dr);zl=r(fu,"DPRQuestionEncoder"),fu.forEach(t),ql=r(ni,"."),ni.forEach(t),oi.forEach(t),ka=p(o),At=a(o,"DIV",{class:!0});var ri=i(At);k(fn.$$.fragment,ri),Cl=p(ri),mn=a(ri,"P",{});var si=i(mn);Fl=r(si,"Class for outputs of "),xr=a(si,"A",{href:!0});var mu=i(xr);Al=r(mu,"DPRQuestionEncoder"),mu.forEach(t),Ol=r(si,"."),si.forEach(t),ri.forEach(t),Pa=p(o),Ot=a(o,"H2",{class:!0});var ai=i(Ot);io=a(ai,"A",{id:!0,class:!0,href:!0});var gu=i(io);ws=a(gu,"SPAN",{});var _u=i(ws);k(gn.$$.fragment,_u),_u.forEach(t),gu.forEach(t),jl=p(ai),ks=a(ai,"SPAN",{});var vu=i(ks);Nl=r(vu,"DPRContextEncoder"),vu.forEach(t),ai.forEach(t),$a=p(o),Be=a(o,"DIV",{class:!0});var Pt=i(Be);k(_n.$$.fragment,Pt),Ql=p(Pt),Ps=a(Pt,"P",{});var bu=i(Ps);Ll=r(bu,"The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),bu.forEach(t),Il=p(Pt),vn=a(Pt,"P",{});var ii=i(vn);Ml=r(ii,"This model inherits from "),zr=a(ii,"A",{href:!0});var Tu=i(zr);Sl=r(Tu,"PreTrainedModel"),Tu.forEach(t),Bl=r(ii,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ii.forEach(t),Hl=p(Pt),bn=a(Pt,"P",{});var di=i(bn);Kl=r(di,"This model is also a PyTorch "),Tn=a(di,"A",{href:!0,rel:!0});var wu=i(Tn);Wl=r(wu,"torch.nn.Module"),wu.forEach(t),Ul=r(di,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),di.forEach(t),Vl=p(Pt),it=a(Pt,"DIV",{class:!0});var Oo=i(it);k(wn.$$.fragment,Oo),Yl=p(Oo),jt=a(Oo,"P",{});var Sr=i(jt);Xl=r(Sr,"The "),qr=a(Sr,"A",{href:!0});var ku=i(qr);Gl=r(ku,"DPRContextEncoder"),ku.forEach(t),Jl=r(Sr," forward method, overrides the "),$s=a(Sr,"CODE",{});var Pu=i($s);Zl=r(Pu,"__call__"),Pu.forEach(t),ec=r(Sr," special method."),Sr.forEach(t),tc=p(Oo),k(lo.$$.fragment,Oo),oc=p(Oo),k(co.$$.fragment,Oo),Oo.forEach(t),Pt.forEach(t),Ea=p(o),Nt=a(o,"H2",{class:!0});var li=i(Nt);po=a(li,"A",{id:!0,class:!0,href:!0});var $u=i(po);Es=a($u,"SPAN",{});var Eu=i(Es);k(kn.$$.fragment,Eu),Eu.forEach(t),$u.forEach(t),nc=p(li),ys=a(li,"SPAN",{});var yu=i(ys);rc=r(yu,"DPRQuestionEncoder"),yu.forEach(t),li.forEach(t),ya=p(o),He=a(o,"DIV",{class:!0});var $t=i(He);k(Pn.$$.fragment,$t),sc=p($t),Rs=a($t,"P",{});var Ru=i(Rs);ac=r(Ru,"The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),Ru.forEach(t),ic=p($t),$n=a($t,"P",{});var ci=i($n);dc=r(ci,"This model inherits from "),Cr=a(ci,"A",{href:!0});var Du=i(Cr);lc=r(Du,"PreTrainedModel"),Du.forEach(t),cc=r(ci,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ci.forEach(t),pc=p($t),En=a($t,"P",{});var pi=i(En);hc=r(pi,"This model is also a PyTorch "),yn=a(pi,"A",{href:!0,rel:!0});var xu=i(yn);uc=r(xu,"torch.nn.Module"),xu.forEach(t),fc=r(pi,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),pi.forEach(t),mc=p($t),dt=a($t,"DIV",{class:!0});var jo=i(dt);k(Rn.$$.fragment,jo),gc=p(jo),Qt=a(jo,"P",{});var Br=i(Qt);_c=r(Br,"The "),Fr=a(Br,"A",{href:!0});var zu=i(Fr);vc=r(zu,"DPRQuestionEncoder"),zu.forEach(t),bc=r(Br," forward method, overrides the "),Ds=a(Br,"CODE",{});var qu=i(Ds);Tc=r(qu,"__call__"),qu.forEach(t),wc=r(Br," special method."),Br.forEach(t),kc=p(jo),k(ho.$$.fragment,jo),Pc=p(jo),k(uo.$$.fragment,jo),jo.forEach(t),$t.forEach(t),Ra=p(o),Lt=a(o,"H2",{class:!0});var hi=i(Lt);fo=a(hi,"A",{id:!0,class:!0,href:!0});var Cu=i(fo);xs=a(Cu,"SPAN",{});var Fu=i(xs);k(Dn.$$.fragment,Fu),Fu.forEach(t),Cu.forEach(t),$c=p(hi),zs=a(hi,"SPAN",{});var Au=i(zs);Ec=r(Au,"DPRReader"),Au.forEach(t),hi.forEach(t),Da=p(o),Ke=a(o,"DIV",{class:!0});var Et=i(Ke);k(xn.$$.fragment,Et),yc=p(Et),qs=a(Et,"P",{});var Ou=i(qs);Rc=r(Ou,"The bare DPRReader transformer outputting span predictions."),Ou.forEach(t),Dc=p(Et),zn=a(Et,"P",{});var ui=i(zn);xc=r(ui,"This model inherits from "),Ar=a(ui,"A",{href:!0});var ju=i(Ar);zc=r(ju,"PreTrainedModel"),ju.forEach(t),qc=r(ui,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ui.forEach(t),Cc=p(Et),qn=a(Et,"P",{});var fi=i(qn);Fc=r(fi,"This model is also a PyTorch "),Cn=a(fi,"A",{href:!0,rel:!0});var Nu=i(Cn);Ac=r(Nu,"torch.nn.Module"),Nu.forEach(t),Oc=r(fi,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),fi.forEach(t),jc=p(Et),lt=a(Et,"DIV",{class:!0});var No=i(lt);k(Fn.$$.fragment,No),Nc=p(No),It=a(No,"P",{});var Hr=i(It);Qc=r(Hr,"The "),Or=a(Hr,"A",{href:!0});var Qu=i(Or);Lc=r(Qu,"DPRReader"),Qu.forEach(t),Ic=r(Hr," forward method, overrides the "),Cs=a(Hr,"CODE",{});var Lu=i(Cs);Mc=r(Lu,"__call__"),Lu.forEach(t),Sc=r(Hr," special method."),Hr.forEach(t),Bc=p(No),k(mo.$$.fragment,No),Hc=p(No),k(go.$$.fragment,No),No.forEach(t),Et.forEach(t),xa=p(o),Mt=a(o,"H2",{class:!0});var mi=i(Mt);_o=a(mi,"A",{id:!0,class:!0,href:!0});var Iu=i(_o);Fs=a(Iu,"SPAN",{});var Mu=i(Fs);k(An.$$.fragment,Mu),Mu.forEach(t),Iu.forEach(t),Kc=p(mi),As=a(mi,"SPAN",{});var Su=i(As);Wc=r(Su,"TFDPRContextEncoder"),Su.forEach(t),mi.forEach(t),za=p(o),ge=a(o,"DIV",{class:!0});var mt=i(ge);k(On.$$.fragment,mt),Uc=p(mt),Os=a(mt,"P",{});var Bu=i(Os);Vc=r(Bu,"The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Bu.forEach(t),Yc=p(mt),jn=a(mt,"P",{});var gi=i(jn);Xc=r(gi,"This model inherits from "),jr=a(gi,"A",{href:!0});var Hu=i(jr);Gc=r(Hu,"TFPreTrainedModel"),Hu.forEach(t),Jc=r(gi,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),gi.forEach(t),Zc=p(mt),Nn=a(mt,"P",{});var _i=i(Nn);ep=r(_i,"This model is also a Tensorflow "),Qn=a(_i,"A",{href:!0,rel:!0});var Ku=i(Qn);tp=r(Ku,"tf.keras.Model"),Ku.forEach(t),op=r(_i,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),_i.forEach(t),np=p(mt),k(vo.$$.fragment,mt),rp=p(mt),ct=a(mt,"DIV",{class:!0});var Qo=i(ct);k(Ln.$$.fragment,Qo),sp=p(Qo),St=a(Qo,"P",{});var Kr=i(St);ap=r(Kr,"The "),Nr=a(Kr,"A",{href:!0});var Wu=i(Nr);ip=r(Wu,"TFDPRContextEncoder"),Wu.forEach(t),dp=r(Kr," forward method, overrides the "),js=a(Kr,"CODE",{});var Uu=i(js);lp=r(Uu,"__call__"),Uu.forEach(t),cp=r(Kr," special method."),Kr.forEach(t),pp=p(Qo),k(bo.$$.fragment,Qo),hp=p(Qo),k(To.$$.fragment,Qo),Qo.forEach(t),mt.forEach(t),qa=p(o),Bt=a(o,"H2",{class:!0});var vi=i(Bt);wo=a(vi,"A",{id:!0,class:!0,href:!0});var Vu=i(wo);Ns=a(Vu,"SPAN",{});var Yu=i(Ns);k(In.$$.fragment,Yu),Yu.forEach(t),Vu.forEach(t),up=p(vi),Qs=a(vi,"SPAN",{});var Xu=i(Qs);fp=r(Xu,"TFDPRQuestionEncoder"),Xu.forEach(t),vi.forEach(t),Ca=p(o),_e=a(o,"DIV",{class:!0});var gt=i(_e);k(Mn.$$.fragment,gt),mp=p(gt),Ls=a(gt,"P",{});var Gu=i(Ls);gp=r(Gu,"The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),Gu.forEach(t),_p=p(gt),Sn=a(gt,"P",{});var bi=i(Sn);vp=r(bi,"This model inherits from "),Qr=a(bi,"A",{href:!0});var Ju=i(Qr);bp=r(Ju,"TFPreTrainedModel"),Ju.forEach(t),Tp=r(bi,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),bi.forEach(t),wp=p(gt),Bn=a(gt,"P",{});var Ti=i(Bn);kp=r(Ti,"This model is also a Tensorflow "),Hn=a(Ti,"A",{href:!0,rel:!0});var Zu=i(Hn);Pp=r(Zu,"tf.keras.Model"),Zu.forEach(t),$p=r(Ti,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),Ti.forEach(t),Ep=p(gt),k(ko.$$.fragment,gt),yp=p(gt),pt=a(gt,"DIV",{class:!0});var Lo=i(pt);k(Kn.$$.fragment,Lo),Rp=p(Lo),Ht=a(Lo,"P",{});var Wr=i(Ht);Dp=r(Wr,"The "),Lr=a(Wr,"A",{href:!0});var ef=i(Lr);xp=r(ef,"TFDPRQuestionEncoder"),ef.forEach(t),zp=r(Wr," forward method, overrides the "),Is=a(Wr,"CODE",{});var tf=i(Is);qp=r(tf,"__call__"),tf.forEach(t),Cp=r(Wr," special method."),Wr.forEach(t),Fp=p(Lo),k(Po.$$.fragment,Lo),Ap=p(Lo),k($o.$$.fragment,Lo),Lo.forEach(t),gt.forEach(t),Fa=p(o),Kt=a(o,"H2",{class:!0});var wi=i(Kt);Eo=a(wi,"A",{id:!0,class:!0,href:!0});var of=i(Eo);Ms=a(of,"SPAN",{});var nf=i(Ms);k(Wn.$$.fragment,nf),nf.forEach(t),of.forEach(t),Op=p(wi),Ss=a(wi,"SPAN",{});var rf=i(Ss);jp=r(rf,"TFDPRReader"),rf.forEach(t),wi.forEach(t),Aa=p(o),ve=a(o,"DIV",{class:!0});var _t=i(ve);k(Un.$$.fragment,_t),Np=p(_t),Bs=a(_t,"P",{});var sf=i(Bs);Qp=r(sf,"The bare DPRReader transformer outputting span predictions."),sf.forEach(t),Lp=p(_t),Vn=a(_t,"P",{});var ki=i(Vn);Ip=r(ki,"This model inherits from "),Ir=a(ki,"A",{href:!0});var af=i(Ir);Mp=r(af,"TFPreTrainedModel"),af.forEach(t),Sp=r(ki,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ki.forEach(t),Bp=p(_t),Yn=a(_t,"P",{});var Pi=i(Yn);Hp=r(Pi,"This model is also a Tensorflow "),Xn=a(Pi,"A",{href:!0,rel:!0});var df=i(Xn);Kp=r(df,"tf.keras.Model"),df.forEach(t),Wp=r(Pi,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),Pi.forEach(t),Up=p(_t),k(yo.$$.fragment,_t),Vp=p(_t),ht=a(_t,"DIV",{class:!0});var Io=i(ht);k(Gn.$$.fragment,Io),Yp=p(Io),Wt=a(Io,"P",{});var Ur=i(Wt);Xp=r(Ur,"The "),Mr=a(Ur,"A",{href:!0});var lf=i(Mr);Gp=r(lf,"TFDPRReader"),lf.forEach(t),Jp=r(Ur," forward method, overrides the "),Hs=a(Ur,"CODE",{});var cf=i(Hs);Zp=r(cf,"__call__"),cf.forEach(t),eh=r(Ur," special method."),Ur.forEach(t),th=p(Io),k(Ro.$$.fragment,Io),oh=p(Io),k(Do.$$.fragment,Io),Io.forEach(t),_t.forEach(t),this.h()},h(){l(h,"name","hf:doc:metadata"),l(h,"content",JSON.stringify(Ff)),l(g,"id","dpr"),l(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(g,"href","#dpr"),l(_,"class","relative group"),l(te,"id","overview"),l(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(te,"href","#overview"),l(L,"class","relative group"),l(se,"href","https://arxiv.org/abs/2004.04906"),l(se,"rel","nofollow"),l(ie,"href","https://huggingface.co/lhoestq"),l(ie,"rel","nofollow"),l(de,"href","https://github.com/facebookresearch/DPR"),l(de,"rel","nofollow"),l(ne,"id","transformers.DPRConfig"),l(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ne,"href","#transformers.DPRConfig"),l(C,"class","relative group"),l(J,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRConfig"),l(Z,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRContextEncoder"),l(x,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRQuestionEncoder"),l(Ie,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRReader"),l(Q,"href","https://huggingface.co/facebook/dpr-ctx_encoder-single-nq-base"),l(Q,"rel","nofollow"),l(Me,"href","/docs/transformers/v4.22.2/en/model_doc/bert#transformers.BertConfig"),l(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Yt,"id","transformers.DPRContextEncoderTokenizer"),l(Yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Yt,"href","#transformers.DPRContextEncoderTokenizer"),l(he,"class","relative group"),l(ir,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRContextEncoderTokenizer"),l(dr,"href","/docs/transformers/v4.22.2/en/model_doc/bert#transformers.BertTokenizer"),l(lr,"href","/docs/transformers/v4.22.2/en/model_doc/bert#transformers.BertTokenizer"),l(We,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Gt,"id","transformers.DPRContextEncoderTokenizerFast"),l(Gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Gt,"href","#transformers.DPRContextEncoderTokenizerFast"),l(yt,"class","relative group"),l(cr,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRContextEncoderTokenizerFast"),l(pr,"href","/docs/transformers/v4.22.2/en/model_doc/bert#transformers.BertTokenizerFast"),l(hr,"href","/docs/transformers/v4.22.2/en/model_doc/bert#transformers.BertTokenizerFast"),l(Ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Zt,"id","transformers.DPRQuestionEncoderTokenizer"),l(Zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Zt,"href","#transformers.DPRQuestionEncoderTokenizer"),l(Rt,"class","relative group"),l(ur,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),l(fr,"href","/docs/transformers/v4.22.2/en/model_doc/bert#transformers.BertTokenizer"),l(mr,"href","/docs/transformers/v4.22.2/en/model_doc/bert#transformers.BertTokenizer"),l(Ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(to,"id","transformers.DPRQuestionEncoderTokenizerFast"),l(to,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(to,"href","#transformers.DPRQuestionEncoderTokenizerFast"),l(Dt,"class","relative group"),l(gr,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),l(_r,"href","/docs/transformers/v4.22.2/en/model_doc/bert#transformers.BertTokenizerFast"),l(vr,"href","/docs/transformers/v4.22.2/en/model_doc/bert#transformers.BertTokenizerFast"),l(Ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(no,"id","transformers.DPRReaderTokenizer"),l(no,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(no,"href","#transformers.DPRReaderTokenizer"),l(xt,"class","relative group"),l(br,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRReaderTokenizer"),l(Tr,"href","/docs/transformers/v4.22.2/en/model_doc/bert#transformers.BertTokenizer"),l(wr,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRReader"),l(kr,"href","/docs/transformers/v4.22.2/en/model_doc/bert#transformers.BertTokenizer"),l(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(so,"id","transformers.DPRReaderTokenizerFast"),l(so,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(so,"href","#transformers.DPRReaderTokenizerFast"),l(zt,"class","relative group"),l(Pr,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRReaderTokenizerFast"),l($r,"href","/docs/transformers/v4.22.2/en/model_doc/bert#transformers.BertTokenizerFast"),l(Er,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRReader"),l(yr,"href","/docs/transformers/v4.22.2/en/model_doc/bert#transformers.BertTokenizerFast"),l(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ao,"id","transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"),l(ao,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ao,"href","#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"),l(qt,"class","relative group"),l(Rr,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRQuestionEncoder"),l(Ct,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Dr,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRQuestionEncoder"),l(Ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(xr,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRQuestionEncoder"),l(At,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(io,"id","transformers.DPRContextEncoder"),l(io,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(io,"href","#transformers.DPRContextEncoder"),l(Ot,"class","relative group"),l(zr,"href","/docs/transformers/v4.22.2/en/main_classes/model#transformers.PreTrainedModel"),l(Tn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Tn,"rel","nofollow"),l(qr,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRContextEncoder"),l(it,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(po,"id","transformers.DPRQuestionEncoder"),l(po,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(po,"href","#transformers.DPRQuestionEncoder"),l(Nt,"class","relative group"),l(Cr,"href","/docs/transformers/v4.22.2/en/main_classes/model#transformers.PreTrainedModel"),l(yn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(yn,"rel","nofollow"),l(Fr,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRQuestionEncoder"),l(dt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(He,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(fo,"id","transformers.DPRReader"),l(fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(fo,"href","#transformers.DPRReader"),l(Lt,"class","relative group"),l(Ar,"href","/docs/transformers/v4.22.2/en/main_classes/model#transformers.PreTrainedModel"),l(Cn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Cn,"rel","nofollow"),l(Or,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.DPRReader"),l(lt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(_o,"id","transformers.TFDPRContextEncoder"),l(_o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(_o,"href","#transformers.TFDPRContextEncoder"),l(Mt,"class","relative group"),l(jr,"href","/docs/transformers/v4.22.2/en/main_classes/model#transformers.TFPreTrainedModel"),l(Qn,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),l(Qn,"rel","nofollow"),l(Nr,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.TFDPRContextEncoder"),l(ct,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(wo,"id","transformers.TFDPRQuestionEncoder"),l(wo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(wo,"href","#transformers.TFDPRQuestionEncoder"),l(Bt,"class","relative group"),l(Qr,"href","/docs/transformers/v4.22.2/en/main_classes/model#transformers.TFPreTrainedModel"),l(Hn,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),l(Hn,"rel","nofollow"),l(Lr,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),l(pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(_e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(Eo,"id","transformers.TFDPRReader"),l(Eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Eo,"href","#transformers.TFDPRReader"),l(Kt,"class","relative group"),l(Ir,"href","/docs/transformers/v4.22.2/en/main_classes/model#transformers.TFPreTrainedModel"),l(Xn,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),l(Xn,"rel","nofollow"),l(Mr,"href","/docs/transformers/v4.22.2/en/model_doc/dpr#transformers.TFDPRReader"),l(ht,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,m){e(document.head,h),f(o,T,m),f(o,_,m),e(_,g),e(g,b),P(d,b,null),e(_,u),e(_,q),e(q,ye),f(o,be,m),f(o,L,m),e(L,te),e(te,oe),P(R,oe,null),e(L,Re),e(L,H),e(H,De),f(o,Te,m),f(o,I,m),e(I,xe),e(I,se),e(se,V),e(I,ze),f(o,we,m),f(o,S,m),e(S,qe),f(o,ke,m),f(o,B,m),e(B,ue),e(ue,Ce),f(o,ae,m),f(o,z,m),e(z,O),e(z,ie),e(ie,Y),e(z,Fe),e(z,de),e(de,X),e(z,Ae),f(o,Pe,m),f(o,C,m),e(C,ne),e(ne,K),P(le,K,null),e(C,Oe),e(C,W),e(W,je),f(o,$e,m),f(o,A,m),P(ce,A,null),e(A,G),e(A,re),e(re,J),e(J,j),e(re,Ne),e(re,N),e(N,Qe),e(re,Le),e(A,v),e(A,D),e(D,Xe),e(D,Z),e(Z,Ge),e(D,Je),e(D,x),e(x,Ze),e(D,et),e(D,Ie),e(Ie,tt),e(D,ot),e(D,Q),e(Q,ee),e(D,nt),e(A,rt),e(A,M),e(M,st),e(M,Me),e(Me,Se),e(M,at),f(o,pe,m),f(o,he,m),e(he,Yt),e(Yt,Vr),P(Mo,Vr,null),e(he,$i),e(he,Yr),e(Yr,Ei),f(o,da,m),f(o,We,m),P(So,We,null),e(We,yi),e(We,Xr),e(Xr,Ri),e(We,Di),e(We,Xt),e(Xt,ir),e(ir,xi),e(Xt,zi),e(Xt,dr),e(dr,qi),e(Xt,Ci),e(We,Fi),e(We,Bo),e(Bo,Ai),e(Bo,lr),e(lr,Oi),e(Bo,ji),f(o,la,m),f(o,yt,m),e(yt,Gt),e(Gt,Gr),P(Ho,Gr,null),e(yt,Ni),e(yt,Jr),e(Jr,Qi),f(o,ca,m),f(o,Ue,m),P(Ko,Ue,null),e(Ue,Li),e(Ue,Wo),e(Wo,Ii),e(Wo,Zr),e(Zr,Mi),e(Wo,Si),e(Ue,Bi),e(Ue,Jt),e(Jt,cr),e(cr,Hi),e(Jt,Ki),e(Jt,pr),e(pr,Wi),e(Jt,Ui),e(Ue,Vi),e(Ue,Uo),e(Uo,Yi),e(Uo,hr),e(hr,Xi),e(Uo,Gi),f(o,pa,m),f(o,Rt,m),e(Rt,Zt),e(Zt,es),P(Vo,es,null),e(Rt,Ji),e(Rt,ts),e(ts,Zi),f(o,ha,m),f(o,Ve,m),P(Yo,Ve,null),e(Ve,ed),e(Ve,os),e(os,td),e(Ve,od),e(Ve,eo),e(eo,ur),e(ur,nd),e(eo,rd),e(eo,fr),e(fr,sd),e(eo,ad),e(Ve,id),e(Ve,Xo),e(Xo,dd),e(Xo,mr),e(mr,ld),e(Xo,cd),f(o,ua,m),f(o,Dt,m),e(Dt,to),e(to,ns),P(Go,ns,null),e(Dt,pd),e(Dt,rs),e(rs,hd),f(o,fa,m),f(o,Ye,m),P(Jo,Ye,null),e(Ye,ud),e(Ye,Zo),e(Zo,fd),e(Zo,ss),e(ss,md),e(Zo,gd),e(Ye,_d),e(Ye,oo),e(oo,gr),e(gr,vd),e(oo,bd),e(oo,_r),e(_r,Td),e(oo,wd),e(Ye,kd),e(Ye,en),e(en,Pd),e(en,vr),e(vr,$d),e(en,Ed),f(o,ma,m),f(o,xt,m),e(xt,no),e(no,as),P(tn,as,null),e(xt,yd),e(xt,is),e(is,Rd),f(o,ga,m),f(o,fe,m),P(on,fe,null),e(fe,Dd),e(fe,ds),e(ds,xd),e(fe,zd),e(fe,bt),e(bt,br),e(br,qd),e(bt,Cd),e(bt,Tr),e(Tr,Fd),e(bt,Ad),e(bt,wr),e(wr,Od),e(bt,jd),e(fe,Nd),e(fe,nn),e(nn,Qd),e(nn,kr),e(kr,Ld),e(nn,Id),e(fe,Md),e(fe,Tt),e(Tt,Sd),e(Tt,ls),e(ls,Bd),e(Tt,Hd),e(Tt,cs),e(cs,Kd),e(Tt,Wd),e(Tt,ps),e(ps,Ud),e(fe,Vd),P(ro,fe,null),f(o,_a,m),f(o,zt,m),e(zt,so),e(so,hs),P(rn,hs,null),e(zt,Yd),e(zt,us),e(us,Xd),f(o,va,m),f(o,me,m),P(sn,me,null),e(me,Gd),e(me,an),e(an,Jd),e(an,fs),e(fs,Zd),e(an,el),e(me,tl),e(me,wt),e(wt,Pr),e(Pr,ol),e(wt,nl),e(wt,$r),e($r,rl),e(wt,sl),e(wt,Er),e(Er,al),e(wt,il),e(me,dl),e(me,dn),e(dn,ll),e(dn,yr),e(yr,cl),e(dn,pl),e(me,hl),e(me,vt),e(vt,ul),e(vt,ms),e(ms,fl),e(vt,ml),e(vt,gs),e(gs,gl),e(vt,_l),e(vt,_s),e(_s,vl),e(vt,bl),e(me,Tl),e(me,vs),e(vs,wl),f(o,ba,m),f(o,qt,m),e(qt,ao),e(ao,bs),P(ln,bs,null),e(qt,kl),e(qt,Ts),e(Ts,Pl),f(o,Ta,m),f(o,Ct,m),P(cn,Ct,null),e(Ct,$l),e(Ct,pn),e(pn,El),e(pn,Rr),e(Rr,yl),e(pn,Rl),f(o,wa,m),f(o,Ft,m),P(hn,Ft,null),e(Ft,Dl),e(Ft,un),e(un,xl),e(un,Dr),e(Dr,zl),e(un,ql),f(o,ka,m),f(o,At,m),P(fn,At,null),e(At,Cl),e(At,mn),e(mn,Fl),e(mn,xr),e(xr,Al),e(mn,Ol),f(o,Pa,m),f(o,Ot,m),e(Ot,io),e(io,ws),P(gn,ws,null),e(Ot,jl),e(Ot,ks),e(ks,Nl),f(o,$a,m),f(o,Be,m),P(_n,Be,null),e(Be,Ql),e(Be,Ps),e(Ps,Ll),e(Be,Il),e(Be,vn),e(vn,Ml),e(vn,zr),e(zr,Sl),e(vn,Bl),e(Be,Hl),e(Be,bn),e(bn,Kl),e(bn,Tn),e(Tn,Wl),e(bn,Ul),e(Be,Vl),e(Be,it),P(wn,it,null),e(it,Yl),e(it,jt),e(jt,Xl),e(jt,qr),e(qr,Gl),e(jt,Jl),e(jt,$s),e($s,Zl),e(jt,ec),e(it,tc),P(lo,it,null),e(it,oc),P(co,it,null),f(o,Ea,m),f(o,Nt,m),e(Nt,po),e(po,Es),P(kn,Es,null),e(Nt,nc),e(Nt,ys),e(ys,rc),f(o,ya,m),f(o,He,m),P(Pn,He,null),e(He,sc),e(He,Rs),e(Rs,ac),e(He,ic),e(He,$n),e($n,dc),e($n,Cr),e(Cr,lc),e($n,cc),e(He,pc),e(He,En),e(En,hc),e(En,yn),e(yn,uc),e(En,fc),e(He,mc),e(He,dt),P(Rn,dt,null),e(dt,gc),e(dt,Qt),e(Qt,_c),e(Qt,Fr),e(Fr,vc),e(Qt,bc),e(Qt,Ds),e(Ds,Tc),e(Qt,wc),e(dt,kc),P(ho,dt,null),e(dt,Pc),P(uo,dt,null),f(o,Ra,m),f(o,Lt,m),e(Lt,fo),e(fo,xs),P(Dn,xs,null),e(Lt,$c),e(Lt,zs),e(zs,Ec),f(o,Da,m),f(o,Ke,m),P(xn,Ke,null),e(Ke,yc),e(Ke,qs),e(qs,Rc),e(Ke,Dc),e(Ke,zn),e(zn,xc),e(zn,Ar),e(Ar,zc),e(zn,qc),e(Ke,Cc),e(Ke,qn),e(qn,Fc),e(qn,Cn),e(Cn,Ac),e(qn,Oc),e(Ke,jc),e(Ke,lt),P(Fn,lt,null),e(lt,Nc),e(lt,It),e(It,Qc),e(It,Or),e(Or,Lc),e(It,Ic),e(It,Cs),e(Cs,Mc),e(It,Sc),e(lt,Bc),P(mo,lt,null),e(lt,Hc),P(go,lt,null),f(o,xa,m),f(o,Mt,m),e(Mt,_o),e(_o,Fs),P(An,Fs,null),e(Mt,Kc),e(Mt,As),e(As,Wc),f(o,za,m),f(o,ge,m),P(On,ge,null),e(ge,Uc),e(ge,Os),e(Os,Vc),e(ge,Yc),e(ge,jn),e(jn,Xc),e(jn,jr),e(jr,Gc),e(jn,Jc),e(ge,Zc),e(ge,Nn),e(Nn,ep),e(Nn,Qn),e(Qn,tp),e(Nn,op),e(ge,np),P(vo,ge,null),e(ge,rp),e(ge,ct),P(Ln,ct,null),e(ct,sp),e(ct,St),e(St,ap),e(St,Nr),e(Nr,ip),e(St,dp),e(St,js),e(js,lp),e(St,cp),e(ct,pp),P(bo,ct,null),e(ct,hp),P(To,ct,null),f(o,qa,m),f(o,Bt,m),e(Bt,wo),e(wo,Ns),P(In,Ns,null),e(Bt,up),e(Bt,Qs),e(Qs,fp),f(o,Ca,m),f(o,_e,m),P(Mn,_e,null),e(_e,mp),e(_e,Ls),e(Ls,gp),e(_e,_p),e(_e,Sn),e(Sn,vp),e(Sn,Qr),e(Qr,bp),e(Sn,Tp),e(_e,wp),e(_e,Bn),e(Bn,kp),e(Bn,Hn),e(Hn,Pp),e(Bn,$p),e(_e,Ep),P(ko,_e,null),e(_e,yp),e(_e,pt),P(Kn,pt,null),e(pt,Rp),e(pt,Ht),e(Ht,Dp),e(Ht,Lr),e(Lr,xp),e(Ht,zp),e(Ht,Is),e(Is,qp),e(Ht,Cp),e(pt,Fp),P(Po,pt,null),e(pt,Ap),P($o,pt,null),f(o,Fa,m),f(o,Kt,m),e(Kt,Eo),e(Eo,Ms),P(Wn,Ms,null),e(Kt,Op),e(Kt,Ss),e(Ss,jp),f(o,Aa,m),f(o,ve,m),P(Un,ve,null),e(ve,Np),e(ve,Bs),e(Bs,Qp),e(ve,Lp),e(ve,Vn),e(Vn,Ip),e(Vn,Ir),e(Ir,Mp),e(Vn,Sp),e(ve,Bp),e(ve,Yn),e(Yn,Hp),e(Yn,Xn),e(Xn,Kp),e(Yn,Wp),e(ve,Up),P(yo,ve,null),e(ve,Vp),e(ve,ht),P(Gn,ht,null),e(ht,Yp),e(ht,Wt),e(Wt,Xp),e(Wt,Mr),e(Mr,Gp),e(Wt,Jp),e(Wt,Hs),e(Hs,Zp),e(Wt,eh),e(ht,th),P(Ro,ht,null),e(ht,oh),P(Do,ht,null),Oa=!0},p(o,[m]){const Jn={};m&2&&(Jn.$$scope={dirty:m,ctx:o}),ro.$set(Jn);const Ks={};m&2&&(Ks.$$scope={dirty:m,ctx:o}),lo.$set(Ks);const Ws={};m&2&&(Ws.$$scope={dirty:m,ctx:o}),co.$set(Ws);const Us={};m&2&&(Us.$$scope={dirty:m,ctx:o}),ho.$set(Us);const Zn={};m&2&&(Zn.$$scope={dirty:m,ctx:o}),uo.$set(Zn);const Vs={};m&2&&(Vs.$$scope={dirty:m,ctx:o}),mo.$set(Vs);const Ys={};m&2&&(Ys.$$scope={dirty:m,ctx:o}),go.$set(Ys);const Xs={};m&2&&(Xs.$$scope={dirty:m,ctx:o}),vo.$set(Xs);const er={};m&2&&(er.$$scope={dirty:m,ctx:o}),bo.$set(er);const Gs={};m&2&&(Gs.$$scope={dirty:m,ctx:o}),To.$set(Gs);const Js={};m&2&&(Js.$$scope={dirty:m,ctx:o}),ko.$set(Js);const Zs={};m&2&&(Zs.$$scope={dirty:m,ctx:o}),Po.$set(Zs);const ea={};m&2&&(ea.$$scope={dirty:m,ctx:o}),$o.$set(ea);const Ut={};m&2&&(Ut.$$scope={dirty:m,ctx:o}),yo.$set(Ut);const ta={};m&2&&(ta.$$scope={dirty:m,ctx:o}),Ro.$set(ta);const oa={};m&2&&(oa.$$scope={dirty:m,ctx:o}),Do.$set(oa)},i(o){Oa||($(d.$$.fragment,o),$(R.$$.fragment,o),$(le.$$.fragment,o),$(ce.$$.fragment,o),$(Mo.$$.fragment,o),$(So.$$.fragment,o),$(Ho.$$.fragment,o),$(Ko.$$.fragment,o),$(Vo.$$.fragment,o),$(Yo.$$.fragment,o),$(Go.$$.fragment,o),$(Jo.$$.fragment,o),$(tn.$$.fragment,o),$(on.$$.fragment,o),$(ro.$$.fragment,o),$(rn.$$.fragment,o),$(sn.$$.fragment,o),$(ln.$$.fragment,o),$(cn.$$.fragment,o),$(hn.$$.fragment,o),$(fn.$$.fragment,o),$(gn.$$.fragment,o),$(_n.$$.fragment,o),$(wn.$$.fragment,o),$(lo.$$.fragment,o),$(co.$$.fragment,o),$(kn.$$.fragment,o),$(Pn.$$.fragment,o),$(Rn.$$.fragment,o),$(ho.$$.fragment,o),$(uo.$$.fragment,o),$(Dn.$$.fragment,o),$(xn.$$.fragment,o),$(Fn.$$.fragment,o),$(mo.$$.fragment,o),$(go.$$.fragment,o),$(An.$$.fragment,o),$(On.$$.fragment,o),$(vo.$$.fragment,o),$(Ln.$$.fragment,o),$(bo.$$.fragment,o),$(To.$$.fragment,o),$(In.$$.fragment,o),$(Mn.$$.fragment,o),$(ko.$$.fragment,o),$(Kn.$$.fragment,o),$(Po.$$.fragment,o),$($o.$$.fragment,o),$(Wn.$$.fragment,o),$(Un.$$.fragment,o),$(yo.$$.fragment,o),$(Gn.$$.fragment,o),$(Ro.$$.fragment,o),$(Do.$$.fragment,o),Oa=!0)},o(o){E(d.$$.fragment,o),E(R.$$.fragment,o),E(le.$$.fragment,o),E(ce.$$.fragment,o),E(Mo.$$.fragment,o),E(So.$$.fragment,o),E(Ho.$$.fragment,o),E(Ko.$$.fragment,o),E(Vo.$$.fragment,o),E(Yo.$$.fragment,o),E(Go.$$.fragment,o),E(Jo.$$.fragment,o),E(tn.$$.fragment,o),E(on.$$.fragment,o),E(ro.$$.fragment,o),E(rn.$$.fragment,o),E(sn.$$.fragment,o),E(ln.$$.fragment,o),E(cn.$$.fragment,o),E(hn.$$.fragment,o),E(fn.$$.fragment,o),E(gn.$$.fragment,o),E(_n.$$.fragment,o),E(wn.$$.fragment,o),E(lo.$$.fragment,o),E(co.$$.fragment,o),E(kn.$$.fragment,o),E(Pn.$$.fragment,o),E(Rn.$$.fragment,o),E(ho.$$.fragment,o),E(uo.$$.fragment,o),E(Dn.$$.fragment,o),E(xn.$$.fragment,o),E(Fn.$$.fragment,o),E(mo.$$.fragment,o),E(go.$$.fragment,o),E(An.$$.fragment,o),E(On.$$.fragment,o),E(vo.$$.fragment,o),E(Ln.$$.fragment,o),E(bo.$$.fragment,o),E(To.$$.fragment,o),E(In.$$.fragment,o),E(Mn.$$.fragment,o),E(ko.$$.fragment,o),E(Kn.$$.fragment,o),E(Po.$$.fragment,o),E($o.$$.fragment,o),E(Wn.$$.fragment,o),E(Un.$$.fragment,o),E(yo.$$.fragment,o),E(Gn.$$.fragment,o),E(Ro.$$.fragment,o),E(Do.$$.fragment,o),Oa=!1},d(o){t(h),o&&t(T),o&&t(_),y(d),o&&t(be),o&&t(L),y(R),o&&t(Te),o&&t(I),o&&t(we),o&&t(S),o&&t(ke),o&&t(B),o&&t(ae),o&&t(z),o&&t(Pe),o&&t(C),y(le),o&&t($e),o&&t(A),y(ce),o&&t(pe),o&&t(he),y(Mo),o&&t(da),o&&t(We),y(So),o&&t(la),o&&t(yt),y(Ho),o&&t(ca),o&&t(Ue),y(Ko),o&&t(pa),o&&t(Rt),y(Vo),o&&t(ha),o&&t(Ve),y(Yo),o&&t(ua),o&&t(Dt),y(Go),o&&t(fa),o&&t(Ye),y(Jo),o&&t(ma),o&&t(xt),y(tn),o&&t(ga),o&&t(fe),y(on),y(ro),o&&t(_a),o&&t(zt),y(rn),o&&t(va),o&&t(me),y(sn),o&&t(ba),o&&t(qt),y(ln),o&&t(Ta),o&&t(Ct),y(cn),o&&t(wa),o&&t(Ft),y(hn),o&&t(ka),o&&t(At),y(fn),o&&t(Pa),o&&t(Ot),y(gn),o&&t($a),o&&t(Be),y(_n),y(wn),y(lo),y(co),o&&t(Ea),o&&t(Nt),y(kn),o&&t(ya),o&&t(He),y(Pn),y(Rn),y(ho),y(uo),o&&t(Ra),o&&t(Lt),y(Dn),o&&t(Da),o&&t(Ke),y(xn),y(Fn),y(mo),y(go),o&&t(xa),o&&t(Mt),y(An),o&&t(za),o&&t(ge),y(On),y(vo),y(Ln),y(bo),y(To),o&&t(qa),o&&t(Bt),y(In),o&&t(Ca),o&&t(_e),y(Mn),y(ko),y(Kn),y(Po),y($o),o&&t(Fa),o&&t(Kt),y(Wn),o&&t(Aa),o&&t(ve),y(Un),y(yo),y(Gn),y(Ro),y(Do)}}}const Ff={local:"dpr",sections:[{local:"overview",title:"Overview"},{local:"transformers.DPRConfig",title:"DPRConfig"},{local:"transformers.DPRContextEncoderTokenizer",title:"DPRContextEncoderTokenizer"},{local:"transformers.DPRContextEncoderTokenizerFast",title:"DPRContextEncoderTokenizerFast"},{local:"transformers.DPRQuestionEncoderTokenizer",title:"DPRQuestionEncoderTokenizer"},{local:"transformers.DPRQuestionEncoderTokenizerFast",title:"DPRQuestionEncoderTokenizerFast"},{local:"transformers.DPRReaderTokenizer",title:"DPRReaderTokenizer"},{local:"transformers.DPRReaderTokenizerFast",title:"DPRReaderTokenizerFast"},{local:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",title:"DPR specific outputs"},{local:"transformers.DPRContextEncoder",title:"DPRContextEncoder"},{local:"transformers.DPRQuestionEncoder",title:"DPRQuestionEncoder"},{local:"transformers.DPRReader",title:"DPRReader"},{local:"transformers.TFDPRContextEncoder",title:"TFDPRContextEncoder"},{local:"transformers.TFDPRQuestionEncoder",title:"TFDPRQuestionEncoder"},{local:"transformers.TFDPRReader",title:"TFDPRReader"}],title:"DPR"};function Af(F){return mf(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Mf extends pf{constructor(h){super();hf(this,h,Af,Cf,uf,{})}}export{Mf as default,Ff as metadata};
