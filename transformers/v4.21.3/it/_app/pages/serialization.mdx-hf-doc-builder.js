import{S as EE,i as $E,s as bE,e as i,k as c,w as _,t as l,M as wE,c as a,d as o,m as d,a as n,x as z,h as s,b as u,G as t,g as p,y as E,q as $,o as b,B as w,v as kE,L as _E}from"../chunks/vendor-hf-doc-builder.js";import{T as ia}from"../chunks/Tip-hf-doc-builder.js";import{I as M}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as S}from"../chunks/CodeBlock-hf-doc-builder.js";import{F as xE,M as zE}from"../chunks/Markdown-hf-doc-builder.js";import"../chunks/IconTensorflow-hf-doc-builder.js";function NE(L){let f,x,m,k,O,N,P,q,A,T,C,y,D;return f=new S({props:{code:`from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Load tokenizer and PyTorch weights form the Hub
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
pt_model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
# Save to disk
tokenizer.save_pretrained("local-pt-checkpoint")
pt_model.save_pretrained("local-pt-checkpoint")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load tokenizer and PyTorch weights form the Hub</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Save to disk</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(<span class="hljs-string">&quot;local-pt-checkpoint&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(<span class="hljs-string">&quot;local-pt-checkpoint&quot;</span>)`}}),y=new S({props:{code:"python -m transformers.onnx --model=local-pt-checkpoint onnx/",highlighted:"python -m transformers.onnx --model=local-pt-checkpoint onnx/"}}),{c(){_(f.$$.fragment),x=c(),m=i("p"),k=l("Una volta salvato il checkpoint, possiamo esportarlo su ONNX puntando l\u2019argomento "),O=i("code"),N=l("--model"),P=l(`
del pacchetto `),q=i("code"),A=l("transformers.onnx"),T=l(" nella directory desiderata:"),C=c(),_(y.$$.fragment)},l(g){z(f.$$.fragment,g),x=d(g),m=a(g,"P",{});var j=n(m);k=s(j,"Una volta salvato il checkpoint, possiamo esportarlo su ONNX puntando l\u2019argomento "),O=a(j,"CODE",{});var B=n(O);N=s(B,"--model"),B.forEach(o),P=s(j,`
del pacchetto `),q=a(j,"CODE",{});var W=n(q);A=s(W,"transformers.onnx"),W.forEach(o),T=s(j," nella directory desiderata:"),j.forEach(o),C=d(g),z(y.$$.fragment,g)},m(g,j){E(f,g,j),p(g,x,j),p(g,m,j),t(m,k),t(m,O),t(O,N),t(m,P),t(m,q),t(q,A),t(m,T),p(g,C,j),E(y,g,j),D=!0},p:_E,i(g){D||($(f.$$.fragment,g),$(y.$$.fragment,g),D=!0)},o(g){b(f.$$.fragment,g),b(y.$$.fragment,g),D=!1},d(g){w(f,g),g&&o(x),g&&o(m),g&&o(C),w(y,g)}}}function OE(L){let f,x;return f=new zE({props:{$$slots:{default:[NE]},$$scope:{ctx:L}}}),{c(){_(f.$$.fragment)},l(m){z(f.$$.fragment,m)},m(m,k){E(f,m,k),x=!0},p(m,k){const O={};k&2&&(O.$$scope={dirty:k,ctx:m}),f.$set(O)},i(m){x||($(f.$$.fragment,m),x=!0)},o(m){b(f.$$.fragment,m),x=!1},d(m){w(f,m)}}}function TE(L){let f,x,m,k,O,N,P,q,A,T,C,y,D;return f=new S({props:{code:`from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# Load tokenizer and TensorFlow weights from the Hub
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
tf_model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")
# Save to disk
tokenizer.save_pretrained("local-tf-checkpoint")
tf_model.save_pretrained("local-tf-checkpoint")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load tokenizer and TensorFlow weights from the Hub</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Save to disk</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(<span class="hljs-string">&quot;local-tf-checkpoint&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(<span class="hljs-string">&quot;local-tf-checkpoint&quot;</span>)`}}),y=new S({props:{code:"python -m transformers.onnx --model=local-tf-checkpoint onnx/",highlighted:"python -m transformers.onnx --model=local-tf-checkpoint onnx/"}}),{c(){_(f.$$.fragment),x=c(),m=i("p"),k=l("Once the checkpoint is saved, we can export it to ONNX by pointing the "),O=i("code"),N=l("--model"),P=l(`
argument of the `),q=i("code"),A=l("transformers.onnx"),T=l(" package to the desired directory:"),C=c(),_(y.$$.fragment)},l(g){z(f.$$.fragment,g),x=d(g),m=a(g,"P",{});var j=n(m);k=s(j,"Once the checkpoint is saved, we can export it to ONNX by pointing the "),O=a(j,"CODE",{});var B=n(O);N=s(B,"--model"),B.forEach(o),P=s(j,`
argument of the `),q=a(j,"CODE",{});var W=n(q);A=s(W,"transformers.onnx"),W.forEach(o),T=s(j," package to the desired directory:"),j.forEach(o),C=d(g),z(y.$$.fragment,g)},m(g,j){E(f,g,j),p(g,x,j),p(g,m,j),t(m,k),t(m,O),t(O,N),t(m,P),t(m,q),t(q,A),t(m,T),p(g,C,j),E(y,g,j),D=!0},p:_E,i(g){D||($(f.$$.fragment,g),$(y.$$.fragment,g),D=!0)},o(g){b(f.$$.fragment,g),b(y.$$.fragment,g),D=!1},d(g){w(f,g),g&&o(x),g&&o(m),g&&o(C),w(y,g)}}}function jE(L){let f,x;return f=new zE({props:{$$slots:{default:[TE]},$$scope:{ctx:L}}}),{c(){_(f.$$.fragment)},l(m){z(f.$$.fragment,m)},m(m,k){E(f,m,k),x=!0},p(m,k){const O={};k&2&&(O.$$scope={dirty:k,ctx:m}),f.$set(O)},i(m){x||($(f.$$.fragment,m),x=!0)},o(m){b(f.$$.fragment,m),x=!1},d(m){w(f,m)}}}function AE(L){let f,x,m,k,O,N,P,q;return{c(){f=i("p"),x=l("Le caratteristiche che hanno un suffisso "),m=i("code"),k=l("wtih-past"),O=l(" (ad es. "),N=i("code"),P=l("causal-lm-with-past"),q=l(`)
corrispondono a topologie di modello con stati nascosti precalcolati (chiave e valori
nei blocchi di attenzione) che possono essere utilizzati per la decodifica autoregressiva veloce.`)},l(A){f=a(A,"P",{});var T=n(f);x=s(T,"Le caratteristiche che hanno un suffisso "),m=a(T,"CODE",{});var C=n(m);k=s(C,"wtih-past"),C.forEach(o),O=s(T," (ad es. "),N=a(T,"CODE",{});var y=n(N);P=s(y,"causal-lm-with-past"),y.forEach(o),q=s(T,`)
corrispondono a topologie di modello con stati nascosti precalcolati (chiave e valori
nei blocchi di attenzione) che possono essere utilizzati per la decodifica autoregressiva veloce.`),T.forEach(o)},m(A,T){p(A,f,T),t(f,x),t(f,m),t(m,k),t(f,O),t(f,N),t(N,P),t(f,q)},d(A){A&&o(f)}}}function yE(L){let f,x,m,k,O;return{c(){f=i("p"),x=l(`Un buon modo per implementare una configurazione ONNX personalizzata \xE8 guardare l\u2019implementazione
esistente nel file `),m=i("code"),k=l("configuration_<model_name>.py"),O=l(" di un\u2019architettura simile.")},l(N){f=a(N,"P",{});var P=n(f);x=s(P,`Un buon modo per implementare una configurazione ONNX personalizzata \xE8 guardare l\u2019implementazione
esistente nel file `),m=a(P,"CODE",{});var q=n(m);k=s(q,"configuration_<model_name>.py"),q.forEach(o),O=s(P," di un\u2019architettura simile."),P.forEach(o)},m(N,P){p(N,f,P),t(f,x),t(f,m),t(m,k),t(f,O)},d(N){N&&o(f)}}}function qE(L){let f,x,m,k,O,N,P,q,A,T,C,y,D,g,j,B,W,ge,Me,K,Q,_e,ae;return{c(){f=i("p"),x=l("Puoi notare che la propriet\xE0 "),m=i("code"),k=l("inputs"),O=l(" per "),N=i("code"),P=l("DistilBertOnnxConfig"),q=l(` restituisce un
`),A=i("code"),T=l("OrdinatoDict"),C=l(`. Ci\xF2 garantisce che gli input corrispondano alla loro posizione
relativa all\u2019interno del metodo `),y=i("code"),D=l("PreTrainedModel.forward()"),g=l(` durante il tracciamento del grafico.
Raccomandiamo di usare un `),j=i("code"),B=l("OrderedDict"),W=l(" per le propriet\xE0 "),ge=i("code"),Me=l("inputs"),K=l(" e "),Q=i("code"),_e=l("outputs"),ae=l(`
quando si implementano configurazioni ONNX personalizzate.`)},l(ze){f=a(ze,"P",{});var I=n(f);x=s(I,"Puoi notare che la propriet\xE0 "),m=a(I,"CODE",{});var ti=n(m);k=s(ti,"inputs"),ti.forEach(o),O=s(I," per "),N=a(I,"CODE",{});var qt=n(N);P=s(qt,"DistilBertOnnxConfig"),qt.forEach(o),q=s(I,` restituisce un
`),A=a(I,"CODE",{});var G=n(A);T=s(G,"OrdinatoDict"),G.forEach(o),C=s(I,`. Ci\xF2 garantisce che gli input corrispondano alla loro posizione
relativa all\u2019interno del metodo `),y=a(I,"CODE",{});var oi=n(y);D=s(oi,"PreTrainedModel.forward()"),oi.forEach(o),g=s(I,` durante il tracciamento del grafico.
Raccomandiamo di usare un `),j=a(I,"CODE",{});var ne=n(j);B=s(ne,"OrderedDict"),ne.forEach(o),W=s(I," per le propriet\xE0 "),ge=a(I,"CODE",{});var ii=n(ge);Me=s(ii,"inputs"),ii.forEach(o),K=s(I," e "),Q=a(I,"CODE",{});var ai=n(Q);_e=s(ai,"outputs"),ai.forEach(o),ae=s(I,`
quando si implementano configurazioni ONNX personalizzate.`),I.forEach(o)},m(ze,I){p(ze,f,I),t(f,x),t(f,m),t(m,k),t(f,O),t(f,N),t(N,P),t(f,q),t(f,A),t(A,T),t(f,C),t(f,y),t(y,D),t(f,g),t(f,j),t(j,B),t(f,W),t(f,ge),t(ge,Me),t(f,K),t(f,Q),t(Q,_e),t(f,ae)},d(ze){ze&&o(f)}}}function PE(L){let f,x,m,k,O,N,P,q;return{c(){f=i("p"),x=l("Tutte le propriet\xE0 e i metodi di base associati a "),m=i("code"),k=l("OnnxConfig"),O=l(` e le
altre classi di configurazione possono essere sovrascritte se necessario. Guarda
`),N=i("code"),P=l("BartOnnxConfig"),q=l(" per un esempio avanzato.")},l(A){f=a(A,"P",{});var T=n(f);x=s(T,"Tutte le propriet\xE0 e i metodi di base associati a "),m=a(T,"CODE",{});var C=n(m);k=s(C,"OnnxConfig"),C.forEach(o),O=s(T,` e le
altre classi di configurazione possono essere sovrascritte se necessario. Guarda
`),N=a(T,"CODE",{});var y=n(N);P=s(y,"BartOnnxConfig"),y.forEach(o),q=s(T," per un esempio avanzato."),T.forEach(o)},m(A,T){p(A,f,T),t(f,x),t(f,m),t(m,k),t(f,O),t(f,N),t(N,P),t(f,q)},d(A){A&&o(f)}}}function CE(L){let f,x,m,k,O,N,P,q,A,T,C;return{c(){f=i("p"),x=l(`Se il tuo modello \xE8 pi\xF9 largo di 2 GB, vedrai che molti file aggiuntivi sono
creati durante l\u2019esportazione. Questo \xE8 `),m=i("em"),k=l("previsto"),O=l(" perch\xE9 ONNX utilizza "),N=i("a"),P=l(`Protocol
Buffer`),q=l(` per memorizzare il modello e
questi hanno un limite di dimensione 2 GB. Vedi la `),A=i("a"),T=l(`Documentazione
ONNX`),C=l(`
per istruzioni su come caricare modelli con dati esterni.`),this.h()},l(y){f=a(y,"P",{});var D=n(f);x=s(D,`Se il tuo modello \xE8 pi\xF9 largo di 2 GB, vedrai che molti file aggiuntivi sono
creati durante l\u2019esportazione. Questo \xE8 `),m=a(D,"EM",{});var g=n(m);k=s(g,"previsto"),g.forEach(o),O=s(D," perch\xE9 ONNX utilizza "),N=a(D,"A",{href:!0,rel:!0});var j=n(N);P=s(j,`Protocol
Buffer`),j.forEach(o),q=s(D,` per memorizzare il modello e
questi hanno un limite di dimensione 2 GB. Vedi la `),A=a(D,"A",{href:!0,rel:!0});var B=n(A);T=s(B,`Documentazione
ONNX`),B.forEach(o),C=s(D,`
per istruzioni su come caricare modelli con dati esterni.`),D.forEach(o),this.h()},h(){u(N,"href","https://developers.google.com/protocol-buffers/"),u(N,"rel","nofollow"),u(A,"href","https://github.com/onnx/onnx/blob/master/docs/ExternalData.md"),u(A,"rel","nofollow")},m(y,D){p(y,f,D),t(f,x),t(f,m),t(m,k),t(f,O),t(f,N),t(N,P),t(f,q),t(f,A),t(A,T),t(f,C)},d(y){y&&o(f)}}}function DE(L){let f,x;return{c(){f=i("p"),x=l(`Questo \xE8 l\u2019inizio dei nostri esperimenti con TorchScript e stiamo ancora esplorando le sue capacit\xE0 con
modelli con variable-input-size. \xC8 una nostra priorit\xE0 e approfondiremo le nostre analisi nelle prossime versioni,
con pi\xF9 esempi di codici, un\u2019implementazione pi\xF9 flessibile e benchmark che confrontano i codici basati su Python con quelli compilati con
TorchScript.`)},l(m){f=a(m,"P",{});var k=n(f);x=s(k,`Questo \xE8 l\u2019inizio dei nostri esperimenti con TorchScript e stiamo ancora esplorando le sue capacit\xE0 con
modelli con variable-input-size. \xC8 una nostra priorit\xE0 e approfondiremo le nostre analisi nelle prossime versioni,
con pi\xF9 esempi di codici, un\u2019implementazione pi\xF9 flessibile e benchmark che confrontano i codici basati su Python con quelli compilati con
TorchScript.`),k.forEach(o)},m(m,k){p(m,f,k),t(f,x)},d(m){m&&o(f)}}}function SE(L){let f,x,m,k,O,N,P,q,A,T,C,y,D,g,j,B,W,ge,Me,K,Q,_e,ae,ze,I,ti,qt,G,oi,ne,ii,ai,aa,cc,dc,rs,ni,uc,ps,Xe,fc,na,mc,hc,cs,li,vc,ds,h,la,gc,_c,sa,zc,Ec,ra,$c,bc,pa,wc,kc,ca,xc,Nc,da,Oc,Tc,ua,jc,Ac,fa,yc,qc,ma,Pc,Cc,ha,Dc,Sc,va,Ic,Lc,ga,Bc,Mc,_a,Xc,Rc,za,Fc,Qc,Ea,Hc,Uc,$a,Wc,Kc,ba,Gc,Vc,wa,Jc,Yc,ka,Zc,ed,xa,td,od,Na,id,ad,Oa,nd,ld,Ta,sd,rd,ja,pd,cd,Aa,dd,ud,ya,fd,md,qa,hd,vd,Pa,gd,_d,Ca,zd,Ed,Da,$d,bd,Sa,wd,kd,Ia,xd,Nd,La,Od,Td,Ba,jd,Ad,Ma,yd,us,si,qd,fs,Re,Pt,Pd,Xa,Cd,Dd,Sd,Ra,Id,ms,Ee,Fe,Fa,Ct,Ld,Qa,Bd,hs,ri,Md,vs,Dt,gs,Qe,Xd,Ha,Rd,Fd,_s,St,zs,pi,Qd,Es,It,$s,ci,Hd,bs,Lt,ws,le,Ud,Ua,Wd,Kd,Wa,Gd,Vd,ks,J,Jd,Ka,Yd,Zd,Bt,eu,tu,Mt,ou,iu,xs,Xt,Ns,He,au,Ga,nu,lu,Os,Rt,Ts,Ue,su,Ft,ru,pu,js,Qt,As,di,cu,ys,We,qs,$e,Ke,Va,Ht,du,Ja,uu,Ps,Ge,fu,Ya,mu,hu,Cs,Ve,Za,Ut,en,vu,gu,tn,_u,zu,X,Wt,Kt,on,Eu,$u,an,bu,wu,nn,ln,ku,xu,Gt,Vt,sn,Nu,Ou,rn,Tu,ju,pn,cn,Au,yu,Jt,dn,un,qu,Pu,fn,mn,Cu,Du,Yt,hn,vn,Su,Iu,gn,_n,Lu,Bu,Zt,eo,zn,Mu,Xu,En,Ru,Fu,$n,bn,Qu,Hu,to,wn,kn,Uu,Wu,xn,Nn,Ku,Gu,oo,On,Tn,Vu,Ju,jn,An,Yu,Ds,Je,Zu,yn,ef,tf,Ss,io,Is,se,of,qn,af,nf,Pn,lf,sf,Ls,ao,Bs,ui,rf,Ms,no,Xs,Y,pf,Cn,cf,df,Dn,uf,ff,Sn,mf,hf,Rs,Ye,Fs,be,Ze,In,lo,vf,Ln,gf,Qs,fi,_f,Hs,re,Bn,zf,Ef,Mn,$f,bf,Xn,wf,Us,mi,kf,Ws,we,et,Rn,so,xf,Fn,Nf,Ks,hi,Of,Gs,pe,vi,Tf,Qn,jf,Af,gi,yf,Hn,qf,Pf,_i,Cf,Un,Df,Vs,tt,Js,ot,Sf,Wn,If,Lf,Ys,ro,Zs,H,Bf,Kn,Mf,Xf,Gn,Rf,Ff,Vn,Qf,Hf,Jn,Uf,Wf,er,it,tr,zi,Kf,or,po,ir,Ei,Gf,ar,co,nr,$i,Vf,lr,uo,sr,Z,Jf,Yn,Yf,Zf,Zn,em,tm,el,om,im,rr,fo,pr,at,cr,ke,nt,tl,mo,am,ol,nm,dr,ce,lm,il,sm,rm,al,pm,cm,ur,ho,fr,R,dm,nl,um,fm,ll,mm,hm,sl,vm,gm,rl,_m,zm,pl,Em,$m,mr,vo,hr,lt,vr,xe,st,cl,go,bm,dl,wm,gr,de,km,ul,xm,Nm,fl,Om,Tm,_r,_o,zr,rt,jm,ml,Am,ym,Er,Ne,pt,hl,zo,qm,vl,Pm,$r,bi,Cm,br,ue,wi,Dm,gl,Sm,Im,ki,Lm,_l,Bm,Mm,xi,Xm,zl,Rm,wr,ct,Fm,Eo,Qm,Hm,kr,Oe,dt,El,$o,Um,$l,Wm,xr,ut,Nr,ft,Km,bo,Gm,Vm,Or,Ni,Jm,Tr,Oi,Ym,jr,mt,bl,Zm,eh,wo,th,wl,oh,ih,Ar,Ti,ah,yr,Te,ht,kl,ko,nh,xl,lh,qr,ji,sh,Pr,ee,rh,Nl,ph,ch,Ol,dh,uh,Tl,fh,mh,Cr,vt,hh,jl,vh,gh,Dr,je,gt,Al,xo,_h,yl,zh,Sr,Ai,Eh,Ir,yi,$h,Lr,qi,ql,bh,Br,Pi,wh,Mr,Ci,kh,Xr,Ae,_t,Pl,No,xh,Cl,Nh,Rr,Di,Oh,Fr,ye,zt,Dl,Oo,Th,Sl,jh,Qr,V,Ah,Il,yh,qh,Ll,Ph,Ch,Bl,Dh,Sh,Ml,Ih,Hr,To,Ur,qe,Et,Xl,jo,Lh,Rl,Bh,Wr,te,Mh,Fl,Xh,Rh,Ql,Fh,Qh,Hl,Hh,Uh,Kr,Ao,Gr,Pe,$t,Ul,yo,Wh,Wl,Kh,Vr,bt,Gh,Kl,Vh,Jh,Jr,qo,Yr,Si,Yh,Zr,fe,Zh,Po,ev,tv,Co,ov,iv,ep,me,Gl,av,nv,Ii,lv,Do,sv,rv,Ce,pv,So,cv,dv,Io,uv,fv,tp,De,wt,Vl,Lo,mv,Jl,hv,op,F,vv,Bo,gv,_v,Mo,zv,Ev,Xo,$v,bv,Ro,wv,kv,Fo,xv,Nv,ip,Se,kt,Yl,Qo,Ov,Zl,Tv,ap,Li,jv,np,Bi,Ie,Av,Ho,yv,qv,Uo,Pv,Cv,lp,Le,xt,es,Wo,Dv,ts,Sv,sp,he,Iv,Ko,Lv,Bv,os,Mv,Xv,rp,Go,pp,Mi,Rv,cp,Xi,Fv,dp,Vo,up,Ri,Qv,fp,Jo,mp,Fi,Hv,hp,Nt,Uv,Yo,Wv,Kv,vp;return N=new M({}),ae=new M({}),Ct=new M({}),Dt=new S({props:{code:"pip install transformers[onnx]",highlighted:"pip install transformers[onnx]"}}),St=new S({props:{code:`python -m transformers.onnx --help

usage: Hugging Face Transformers ONNX exporter [-h] -m MODEL [--feature {causal-lm, ...}] [--opset OPSET] [--atol ATOL] output

positional arguments:
  output                Path indicating where to store generated ONNX model.

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Model ID on huggingface.co or path on disk to load model from.
  --feature {causal-lm, ...}
                        The type of features to export the model with.
  --opset OPSET         ONNX opset version to export the model with.
  --atol ATOL           Absolute difference tolerence when validating the model.`,highlighted:`python -m transformers.onnx --<span class="hljs-built_in">help</span>

usage: Hugging Face Transformers ONNX exporter [-h] -m MODEL [--feature {causal-lm, ...}] [--opset OPSET] [--atol ATOL] output

positional arguments:
  output                Path indicating <span class="hljs-built_in">where</span> to store generated ONNX model.

optional arguments:
  -h, --<span class="hljs-built_in">help</span>            show this <span class="hljs-built_in">help</span> message and <span class="hljs-built_in">exit</span>
  -m MODEL, --model MODEL
                        Model ID on huggingface.co or path on disk to load model from.
  --feature {causal-lm, ...}
                        The <span class="hljs-built_in">type</span> of features to <span class="hljs-built_in">export</span> the model with.
  --opset OPSET         ONNX opset version to <span class="hljs-built_in">export</span> the model with.
  --atol ATOL           Absolute difference tolerence when validating the model.`}}),It=new S({props:{code:"python -m transformers.onnx --model=distilbert-base-uncased onnx/",highlighted:"python -m transformers.onnx --model=distilbert-base-uncased onnx/"}}),Lt=new S({props:{code:`Validating ONNX model...
        -[\u2713] ONNX model output names match reference model ({'last_hidden_state'})
        - Validating ONNX Model output "last_hidden_state":
                -[\u2713] (2, 8, 768) matches (2, 8, 768)
                -[\u2713] all values close (atol: 1e-05)
All good, model saved at: onnx/model.onnx`,highlighted:`Validating ONNX model...
        -[\u2713] ONNX model output names match reference model ({<span class="hljs-string">&#x27;last_hidden_state&#x27;</span>})
        - Validating ONNX Model output <span class="hljs-string">&quot;last_hidden_state&quot;</span>:
                -[\u2713] (2, 8, 768) matches (2, 8, 768)
                -[\u2713] all values close (atol: 1e-05)
All good, model saved at: onnx/model.onnx`}}),Xt=new S({props:{code:`from transformers import AutoTokenizer
from onnxruntime import InferenceSession

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
session = InferenceSession("onnx/model.onnx")
# ONNX Runtime expects NumPy arrays as input
inputs = tokenizer("Using DistilBERT with ONNX Runtime!", return_tensors="np")
outputs = session.run(output_names=["last_hidden_state"], input_feed=dict(inputs))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> onnxruntime <span class="hljs-keyword">import</span> InferenceSession

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>session = InferenceSession(<span class="hljs-string">&quot;onnx/model.onnx&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># ONNX Runtime expects NumPy arrays as input</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Using DistilBERT with ONNX Runtime!&quot;</span>, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = session.run(output_names=[<span class="hljs-string">&quot;last_hidden_state&quot;</span>], input_feed=<span class="hljs-built_in">dict</span>(inputs))`}}),Rt=new S({props:{code:`from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig

config = DistilBertConfig()
onnx_config = DistilBertOnnxConfig(config)
print(list(onnx_config.outputs.keys()))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.models.distilbert <span class="hljs-keyword">import</span> DistilBertConfig, DistilBertOnnxConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>config = DistilBertConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_config = DistilBertOnnxConfig(config)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(onnx_config.outputs.keys()))
[<span class="hljs-string">&quot;last_hidden_state&quot;</span>]`}}),Qt=new S({props:{code:"python -m transformers.onnx --model=keras-io/transformers-qa onnx/",highlighted:"python -m transformers.onnx --model=keras-io/transformers-qa onnx/"}}),We=new xE({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[jE],pytorch:[OE]},$$scope:{ctx:L}}}),Ht=new M({}),io=new S({props:{code:`from transformers.onnx.features import FeaturesManager

distilbert_features = list(FeaturesManager.get_supported_features_for_model_type("distilbert").keys())
print(distilbert_features)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.onnx.features <span class="hljs-keyword">import</span> FeaturesManager

<span class="hljs-meta">&gt;&gt;&gt; </span>distilbert_features = <span class="hljs-built_in">list</span>(FeaturesManager.get_supported_features_for_model_type(<span class="hljs-string">&quot;distilbert&quot;</span>).keys())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(distilbert_features)
[<span class="hljs-string">&quot;default&quot;</span>, <span class="hljs-string">&quot;masked-lm&quot;</span>, <span class="hljs-string">&quot;causal-lm&quot;</span>, <span class="hljs-string">&quot;sequence-classification&quot;</span>, <span class="hljs-string">&quot;token-classification&quot;</span>, <span class="hljs-string">&quot;question-answering&quot;</span>]`}}),ao=new S({props:{code:`python -m transformers.onnx --model=distilbert-base-uncased-finetuned-sst-2-english \\
                            --feature=sequence-classification onnx/`,highlighted:`python -m transformers.onnx --model=distilbert-base-uncased-finetuned-sst-2-english \\
                            --feature=sequence-classification onnx/`}}),no=new S({props:{code:`Validating ONNX model...
        -[\u2713] ONNX model output names match reference model ({'logits'})
        - Validating ONNX Model output "logits":
                -[\u2713] (2, 2) matches (2, 2)
                -[\u2713] all values close (atol: 1e-05)
All good, model saved at: onnx/model.onnx`,highlighted:`Validating ONNX model...
        -[\u2713] ONNX model output names match reference model ({<span class="hljs-string">&#x27;logits&#x27;</span>})
        - Validating ONNX Model output <span class="hljs-string">&quot;logits&quot;</span>:
                -[\u2713] (2, 2) matches (2, 2)
                -[\u2713] all values close (atol: 1e-05)
All good, model saved at: onnx/model.onnx`}}),Ye=new ia({props:{$$slots:{default:[AE]},$$scope:{ctx:L}}}),lo=new M({}),so=new M({}),tt=new ia({props:{$$slots:{default:[yE]},$$scope:{ctx:L}}}),ro=new S({props:{code:`from typing import Mapping, OrderedDict
from transformers.onnx import OnnxConfig


class DistilBertOnnxConfig(OnnxConfig):
    @property
    def inputs(self) -> Mapping[str, Mapping[int, str]]:
        return OrderedDict(
            [
                ("input_ids", {0: "batch", 1: "sequence"}),
                ("attention_mask", {0: "batch", 1: "sequence"}),
            ]
        )`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> Mapping, OrderedDict
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.onnx <span class="hljs-keyword">import</span> OnnxConfig


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">class</span> <span class="hljs-title class_">DistilBertOnnxConfig</span>(<span class="hljs-title class_ inherited__">OnnxConfig</span>):
<span class="hljs-meta">... </span>    @<span class="hljs-built_in">property</span>
<span class="hljs-meta">... </span>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">inputs</span>(<span class="hljs-params">self</span>) -&gt; Mapping[<span class="hljs-built_in">str</span>, Mapping[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">str</span>]]:
<span class="hljs-meta">... </span>        <span class="hljs-keyword">return</span> OrderedDict(
<span class="hljs-meta">... </span>            [
<span class="hljs-meta">... </span>                (<span class="hljs-string">&quot;input_ids&quot;</span>, {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;batch&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;sequence&quot;</span>}),
<span class="hljs-meta">... </span>                (<span class="hljs-string">&quot;attention_mask&quot;</span>, {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;batch&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;sequence&quot;</span>}),
<span class="hljs-meta">... </span>            ]
<span class="hljs-meta">... </span>        )`}}),it=new ia({props:{$$slots:{default:[qE]},$$scope:{ctx:L}}}),po=new S({props:{code:`from transformers import AutoConfig

config = AutoConfig.from_pretrained("distilbert-base-uncased")
onnx_config = DistilBertOnnxConfig(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_config = DistilBertOnnxConfig(config)`}}),co=new S({props:{code:"print(onnx_config.default_onnx_opset)",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(onnx_config.default_onnx_opset)
<span class="hljs-number">11</span>`}}),uo=new S({props:{code:"print(onnx_config.outputs)",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(onnx_config.outputs)
OrderedDict([(<span class="hljs-string">&quot;last_hidden_state&quot;</span>, {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;batch&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;sequence&quot;</span>})])`}}),fo=new S({props:{code:`from transformers import AutoConfig

config = AutoConfig.from_pretrained("distilbert-base-uncased")
onnx_config_for_seq_clf = DistilBertOnnxConfig(config, task="sequence-classification")
print(onnx_config_for_seq_clf.outputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_config_for_seq_clf = DistilBertOnnxConfig(config, task=<span class="hljs-string">&quot;sequence-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(onnx_config_for_seq_clf.outputs)
OrderedDict([(<span class="hljs-string">&#x27;logits&#x27;</span>, {<span class="hljs-number">0</span>: <span class="hljs-string">&#x27;batch&#x27;</span>})])`}}),at=new ia({props:{$$slots:{default:[PE]},$$scope:{ctx:L}}}),mo=new M({}),ho=new S({props:{code:`from pathlib import Path
from transformers.onnx import export
from transformers import AutoTokenizer, AutoModel

onnx_path = Path("model.onnx")
model_ckpt = "distilbert-base-uncased"
base_model = AutoModel.from_pretrained(model_ckpt)
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

onnx_inputs, onnx_outputs = export(tokenizer, base_model, onnx_config, onnx_config.default_onnx_opset, onnx_path)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.onnx <span class="hljs-keyword">import</span> export
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_path = Path(<span class="hljs-string">&quot;model.onnx&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model_ckpt = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>base_model = AutoModel.from_pretrained(model_ckpt)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_inputs, onnx_outputs = export(tokenizer, base_model, onnx_config, onnx_config.default_onnx_opset, onnx_path)`}}),vo=new S({props:{code:`import onnx

onnx_model = onnx.load("model.onnx")
onnx.checker.check_model(onnx_model)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> onnx

<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_model = onnx.load(<span class="hljs-string">&quot;model.onnx&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>onnx.checker.check_model(onnx_model)`}}),lt=new ia({props:{$$slots:{default:[CE]},$$scope:{ctx:L}}}),go=new M({}),_o=new S({props:{code:`from transformers.onnx import validate_model_outputs

validate_model_outputs(
    onnx_config, tokenizer, base_model, onnx_path, onnx_outputs, onnx_config.atol_for_validation
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.onnx <span class="hljs-keyword">import</span> validate_model_outputs

<span class="hljs-meta">&gt;&gt;&gt; </span>validate_model_outputs(
<span class="hljs-meta">... </span>    onnx_config, tokenizer, base_model, onnx_path, onnx_outputs, onnx_config.atol_for_validation
<span class="hljs-meta">... </span>)`}}),zo=new M({}),$o=new M({}),ut=new ia({props:{$$slots:{default:[DE]},$$scope:{ctx:L}}}),ko=new M({}),xo=new M({}),No=new M({}),Oo=new M({}),To=new S({props:{code:`from transformers import BertModel, BertTokenizer, BertConfig
import torch

enc = BertTokenizer.from_pretrained("bert-base-uncased")

# Tokenizing input text
text = "[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]"
tokenized_text = enc.tokenize(text)

# Masking one of the input tokens
masked_index = 8
tokenized_text[masked_index] = "[MASK]"
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

# Creating a dummy input
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

# Initializing the model with the torchscript flag
# Flag set to True even though it is not necessary as this model does not have an LM Head.
config = BertConfig(
    vocab_size_or_config_json_file=32000,
    hidden_size=768,
    num_hidden_layers=12,
    num_attention_heads=12,
    intermediate_size=3072,
    torchscript=True,
)

# Instantiating the model
model = BertModel(config)

# The model needs to be in evaluation mode
model.eval()

# If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag
model = BertModel.from_pretrained("bert-base-uncased", torchscript=True)

# Creating the trace
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, "traced_bert.pt")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertModel, BertTokenizer, BertConfig
<span class="hljs-keyword">import</span> torch

enc = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-comment"># Tokenizing input text</span>
text = <span class="hljs-string">&quot;[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]&quot;</span>
tokenized_text = enc.tokenize(text)

<span class="hljs-comment"># Masking one of the input tokens</span>
masked_index = <span class="hljs-number">8</span>
tokenized_text[masked_index] = <span class="hljs-string">&quot;[MASK]&quot;</span>
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]

<span class="hljs-comment"># Creating a dummy input</span>
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

<span class="hljs-comment"># Initializing the model with the torchscript flag</span>
<span class="hljs-comment"># Flag set to True even though it is not necessary as this model does not have an LM Head.</span>
config = BertConfig(
    vocab_size_or_config_json_file=<span class="hljs-number">32000</span>,
    hidden_size=<span class="hljs-number">768</span>,
    num_hidden_layers=<span class="hljs-number">12</span>,
    num_attention_heads=<span class="hljs-number">12</span>,
    intermediate_size=<span class="hljs-number">3072</span>,
    torchscript=<span class="hljs-literal">True</span>,
)

<span class="hljs-comment"># Instantiating the model</span>
model = BertModel(config)

<span class="hljs-comment"># The model needs to be in evaluation mode</span>
model.<span class="hljs-built_in">eval</span>()

<span class="hljs-comment"># If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag</span>
model = BertModel.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, torchscript=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># Creating the trace</span>
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, <span class="hljs-string">&quot;traced_bert.pt&quot;</span>)`}}),jo=new M({}),Ao=new S({props:{code:`loaded_model = torch.jit.load("traced_bert.pt")
loaded_model.eval()

all_encoder_layers, pooled_output = loaded_model(*dummy_input)`,highlighted:`loaded_model = torch.jit.load(<span class="hljs-string">&quot;traced_bert.pt&quot;</span>)
loaded_model.<span class="hljs-built_in">eval</span>()

all_encoder_layers, pooled_output = loaded_model(*dummy_input)`}}),yo=new M({}),qo=new S({props:{code:"traced_model(tokens_tensor, segments_tensors)",highlighted:"traced_model(tokens_tensor, segments_tensors)"}}),Lo=new M({}),Qo=new M({}),Wo=new M({}),Go=new S({props:{code:`from transformers import BertModel, BertTokenizer, BertConfig
import torch
import torch.neuron`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertModel, BertTokenizer, BertConfig
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.neuron`}}),Vo=new S({props:{code:"torch.jit.trace(model, [tokens_tensor, segments_tensors])",highlighted:"torch.jit.trace(model, [tokens_tensor, segments_tensors])"}}),Jo=new S({props:{code:"torch.neuron.trace(model, [token_tensor, segments_tensors])",highlighted:"torch.neuron.trace(model, [token_tensor, segments_tensors])"}}),{c(){f=i("meta"),x=c(),m=i("h1"),k=i("a"),O=i("span"),_(N.$$.fragment),P=c(),q=i("span"),A=l("Esporta modelli \u{1F917} Transformers"),T=c(),C=i("p"),y=l(`Se devi implementare \u{1F917} modelli Transformers in ambienti di produzione, noi
consigliamo di esportarli in un formato serializzato che pu\xF2 essere caricato ed eseguito
su runtime e hardware specializzati. In questa guida ti mostreremo come farlo
esporta \u{1F917} Modelli Transformers in due formati ampiamente utilizzati: ONNX e TorchScript.`),D=c(),g=i("p"),j=l(`Una volta esportato, un modello pu\xF2 essere ottimizato per l\u2019inferenza tramite tecniche come
la quantizzazione e soppressione. Se sei interessato a ottimizzare i tuoi modelli per l\u2019esecuzione
con la massima efficienza, dai un\u2019occhiata a `),B=i("a"),W=l(`\u{1F917} Optimum
library`),ge=l("."),Me=c(),K=i("h2"),Q=i("a"),_e=i("span"),_(ae.$$.fragment),ze=c(),I=i("span"),ti=l("ONNX"),qt=c(),G=i("p"),oi=l("Il progetto "),ne=i("a"),ii=l("ONNX (Open Neural Network eXchange)"),ai=l(` Il progetto onnx \xE8 un open
standard che definisce un insieme comune di operatori e un formato di file comune a
rappresentano modelli di deep learning in un\u2019ampia variet\xE0 di framework, tra cui
PyTorch e TensorFlow. Quando un modello viene esportato nel formato ONNX, questi
operatori sono usati per costruire un grafico computazionale (often called an
`),aa=i("em"),cc=l("intermediate representation"),dc=l(`) che rappresenta il flusso di dati attraverso la
rete neurale.`),rs=c(),ni=i("p"),uc=l(`Esponendo un grafico con operatori e tipi di dati standardizzati, ONNX rende
pi\xF9 facile passare da un framework all\u2019altro. Ad esempio, un modello allenato in PyTorch pu\xF2
essere esportato in formato ONNX e quindi importato in TensorFlow (e viceversa).`),ps=c(),Xe=i("p"),fc=l("\u{1F917} Transformers fornisce un pacchetto "),na=i("code"),mc=l("transformers.onnx"),hc=l(` che ti consente di
convertire i checkpoint del modello in un grafico ONNX sfruttando gli oggetti di configurazione.
Questi oggetti di configurazione sono gi\xE0 pronti per una serie di architetture di modelli,
e sono progettati per essere facilmente estensibili ad altre architetture.`),cs=c(),li=i("p"),vc=l("Le configurazioni pronte includono le seguenti architetture:"),ds=c(),h=i("ul"),la=i("li"),gc=l("ALBERT"),_c=c(),sa=i("li"),zc=l("BART"),Ec=c(),ra=i("li"),$c=l("BEiT"),bc=c(),pa=i("li"),wc=l("BERT"),kc=c(),ca=i("li"),xc=l("BigBird"),Nc=c(),da=i("li"),Oc=l("BigBird-Pegasus"),Tc=c(),ua=i("li"),jc=l("Blenderbot"),Ac=c(),fa=i("li"),yc=l("BlenderbotSmall"),qc=c(),ma=i("li"),Pc=l("CamemBERT"),Cc=c(),ha=i("li"),Dc=l("ConvBERT"),Sc=c(),va=i("li"),Ic=l("Data2VecText"),Lc=c(),ga=i("li"),Bc=l("Data2VecVision"),Mc=c(),_a=i("li"),Xc=l("DeiT"),Rc=c(),za=i("li"),Fc=l("DistilBERT"),Qc=c(),Ea=i("li"),Hc=l("ELECTRA"),Uc=c(),$a=i("li"),Wc=l("FlauBERT"),Kc=c(),ba=i("li"),Gc=l("GPT Neo"),Vc=c(),wa=i("li"),Jc=l("GPT-J"),Yc=c(),ka=i("li"),Zc=l("I-BERT"),ed=c(),xa=i("li"),td=l("LayoutLM"),od=c(),Na=i("li"),id=l("M2M100"),ad=c(),Oa=i("li"),nd=l("Marian"),ld=c(),Ta=i("li"),sd=l("mBART"),rd=c(),ja=i("li"),pd=l("MobileBERT"),cd=c(),Aa=i("li"),dd=l("OpenAI GPT-2"),ud=c(),ya=i("li"),fd=l("Perceiver"),md=c(),qa=i("li"),hd=l("PLBart"),vd=c(),Pa=i("li"),gd=l("RoBERTa"),_d=c(),Ca=i("li"),zd=l("RoFormer"),Ed=c(),Da=i("li"),$d=l("SqueezeBERT"),bd=c(),Sa=i("li"),wd=l("T5"),kd=c(),Ia=i("li"),xd=l("ViT"),Nd=c(),La=i("li"),Od=l("XLM"),Td=c(),Ba=i("li"),jd=l("XLM-RoBERTa"),Ad=c(),Ma=i("li"),yd=l("XLM-RoBERTa-XL"),us=c(),si=i("p"),qd=l("Nelle prossime due sezioni, ti mostreremo come:"),fs=c(),Re=i("ul"),Pt=i("li"),Pd=l("Esporta un modello supportato usando il pacchetto "),Xa=i("code"),Cd=l("transformers.onnx"),Dd=l("."),Sd=c(),Ra=i("li"),Id=l("Esporta un modello personalizzato per un\u2019architettura non supportata."),ms=c(),Ee=i("h3"),Fe=i("a"),Fa=i("span"),_(Ct.$$.fragment),Ld=c(),Qa=i("span"),Bd=l("Esportazione di un modello in ONNX"),hs=c(),ri=i("p"),Md=l(`Per esportare un modello \u{1F917} Transformers in ONNX, dovrai prima installarne alcune
dipendenze extra:`),vs=c(),_(Dt.$$.fragment),gs=c(),Qe=i("p"),Xd=l("Il pacchetto "),Ha=i("code"),Rd=l("transformers.onnx"),Fd=l(" pu\xF2 essere usato come modulo Python:"),_s=c(),_(St.$$.fragment),zs=c(),pi=i("p"),Qd=l("L\u2019esportazione di un checkpoint utilizzando una configurazione gi\xE0 pronta pu\xF2 essere eseguita come segue:"),Es=c(),_(It.$$.fragment),$s=c(),ci=i("p"),Hd=l("che dovrebbe mostrare i seguenti log:"),bs=c(),_(Lt.$$.fragment),ws=c(),le=i("p"),Ud=l("Questo esporta un grafico ONNX del checkpoint definito dall\u2019argomento "),Ua=i("code"),Wd=l("--model"),Kd=l(`.
In questo esempio \xE8 `),Wa=i("code"),Gd=l("distilbert-base-uncased"),Vd=l(`, ma pu\xF2 essere qualsiasi checkpoint
Hugging Face Hub o uno memorizzato localmente.`),ks=c(),J=i("p"),Jd=l("Il file risultante "),Ka=i("code"),Yd=l("model.onnx"),Zd=l(" pu\xF2 quindi essere eseguito su uno dei "),Bt=i("a"),eu=l(`tanti
acceleratori`),tu=l(` che supportano il
lo standard ONNX. Ad esempio, possiamo caricare ed eseguire il modello con `),Mt=i("a"),ou=l(`ONNX
Runtime`),iu=l(" come segue:"),xs=c(),_(Xt.$$.fragment),Ns=c(),He=i("p"),au=l("I nomi di output richiesti (cio\xE8 "),Ga=i("code"),nu=l('["last_hidden_state"]'),lu=l(`) possono essere ottenuti
dando un\u2019occhiata alla configurazione ONNX di ogni modello. Ad esempio, per
DistilBERT abbiamo:`),Os=c(),_(Rt.$$.fragment),Ts=c(),Ue=i("p"),su=l(`Il processo \xE8 identico per i checkpoint TensorFlow sull\u2019hub. Ad esempio, noi
possiamo esportare un checkpoint TensorFlow puro da `),Ft=i("a"),ru=l(`Keras
organizzazione`),pu=l(" come segue:"),js=c(),_(Qt.$$.fragment),As=c(),di=i("p"),cu=l(`Per esportare un modello memorizzato localmente, devi disporre dei pesi del modello
e file tokenizer memorizzati in una directory. Ad esempio, possiamo caricare e salvare un
checkpoint come segue:`),ys=c(),_(We.$$.fragment),qs=c(),$e=i("h3"),Ke=i("a"),Va=i("span"),_(Ht.$$.fragment),du=c(),Ja=i("span"),uu=l("Selezione delle caratteristiche per diverse topologie di modello"),Ps=c(),Ge=i("p"),fu=l("Ogni configurazione gi\xE0 pronta viene fornita con una serie di "),Ya=i("em"),mu=l("caratteristiche"),hu=l(` che ti consentono di
esportare modelli per diversi tipi di topologie o attivit\xE0. Come mostrato nella tabella
di seguito, ogni caratteristica \xE8 associata a una diversa Auto Class:`),Cs=c(),Ve=i("table"),Za=i("thead"),Ut=i("tr"),en=i("th"),vu=l("Caratteristica"),gu=c(),tn=i("th"),_u=l("Auto Class"),zu=c(),X=i("tbody"),Wt=i("tr"),Kt=i("td"),on=i("code"),Eu=l("causal-lm"),$u=l(", "),an=i("code"),bu=l("causal-lm-with-past"),wu=c(),nn=i("td"),ln=i("code"),ku=l("AutoModelForCausalLM"),xu=c(),Gt=i("tr"),Vt=i("td"),sn=i("code"),Nu=l("default"),Ou=l(", "),rn=i("code"),Tu=l("default-with-past"),ju=c(),pn=i("td"),cn=i("code"),Au=l("AutoModel"),yu=c(),Jt=i("tr"),dn=i("td"),un=i("code"),qu=l("masked-lm"),Pu=c(),fn=i("td"),mn=i("code"),Cu=l("AutoModelForMaskedLM"),Du=c(),Yt=i("tr"),hn=i("td"),vn=i("code"),Su=l("question-answering"),Iu=c(),gn=i("td"),_n=i("code"),Lu=l("AutoModelForQuestionAnswering"),Bu=c(),Zt=i("tr"),eo=i("td"),zn=i("code"),Mu=l("seq2seq-lm"),Xu=l(", "),En=i("code"),Ru=l("seq2seq-lm-with-past"),Fu=c(),$n=i("td"),bn=i("code"),Qu=l("AutoModelForSeq2SeqLM"),Hu=c(),to=i("tr"),wn=i("td"),kn=i("code"),Uu=l("sequence-classification"),Wu=c(),xn=i("td"),Nn=i("code"),Ku=l("AutoModelForSequenceClassification"),Gu=c(),oo=i("tr"),On=i("td"),Tn=i("code"),Vu=l("token-classification"),Ju=c(),jn=i("td"),An=i("code"),Yu=l("AutoModelForTokenClassification"),Ds=c(),Je=i("p"),Zu=l(`Per ciascuna configurazione, puoi trovare l\u2019elenco delle funzionalit\xE0 supportate tramite il
`),yn=i("code"),ef=l("FeaturesManager"),tf=l(". Ad esempio, per DistilBERT abbiamo:"),Ss=c(),_(io.$$.fragment),Is=c(),se=i("p"),of=l("Puoi quindi passare una di queste funzionalit\xE0 all\u2019argomento "),qn=i("code"),af=l("--feature"),nf=l(` nel
pacchetto `),Pn=i("code"),lf=l("transformers.onnx"),sf=l(`. Ad esempio, per esportare un modello di classificazione del testo
possiamo scegliere un modello ottimizzato dall\u2019Hub ed eseguire:`),Ls=c(),_(ao.$$.fragment),Bs=c(),ui=i("p"),rf=l("che visualizzer\xE0 i seguenti registri:"),Ms=c(),_(no.$$.fragment),Xs=c(),Y=i("p"),pf=l(`Puoi notare che in questo caso, i nomi di output del modello ottimizzato sono
`),Cn=i("code"),cf=l("logits"),df=l(" invece di "),Dn=i("code"),uf=l("last_hidden_state"),ff=l(` che abbiamo visto con il
checkpoint `),Sn=i("code"),mf=l("distilbert-base-uncased"),hf=l(` precedente. Questo \xE8 previsto dal
modello ottimizato visto che ha una testa di e.`),Rs=c(),_(Ye.$$.fragment),Fs=c(),be=i("h3"),Ze=i("a"),In=i("span"),_(lo.$$.fragment),vf=c(),Ln=i("span"),gf=l("Esportazione di un modello per un'architettura non supportata"),Qs=c(),fi=i("p"),_f=l(`Se desideri esportare un modello la cui architettura non \xE8 nativamente supportata dalla
libreria, ci sono tre passaggi principali da seguire:`),Hs=c(),re=i("ol"),Bn=i("li"),zf=l("Implementare una configurazione ONNX personalizzata."),Ef=c(),Mn=i("li"),$f=l("Esportare il modello in ONNX."),bf=c(),Xn=i("li"),wf=l("Convalidare gli output di PyTorch e dei modelli esportati."),Us=c(),mi=i("p"),kf=l(`In questa sezione, vedremo come DistilBERT \xE8 stato implementato per mostrare cosa \xE8
coinvolto in ogni passaggio.`),Ws=c(),we=i("h4"),et=i("a"),Rn=i("span"),_(so.$$.fragment),xf=c(),Fn=i("span"),Nf=l("Implementazione di una configurazione ONNX personalizzata"),Ks=c(),hi=i("p"),Of=l(`Iniziamo con l\u2019oggetto di configurazione ONNX. Forniamo tre classi
astratte da cui ereditare, a seconda del tipo di archittettura
del modello che desideri esportare:`),Gs=c(),pe=i("ul"),vi=i("li"),Tf=l("I modelli basati su encoder ereditano da "),Qn=i("code"),jf=l("OnnxConfig"),Af=c(),gi=i("li"),yf=l("I modelli basati su decoder ereditano da "),Hn=i("code"),qf=l("OnnxConfigWithPast"),Pf=c(),_i=i("li"),Cf=l("I modelli encoder-decoder ereditano da"),Un=i("code"),Df=l("OnnxSeq2SeqConfigWithPast"),Vs=c(),_(tt.$$.fragment),Js=c(),ot=i("p"),Sf=l(`Poich\xE9 DistilBERT \xE8 un modello basato su encoder, la sua configurazione eredita da
`),Wn=i("code"),If=l("OnnxConfig"),Lf=l(":"),Ys=c(),_(ro.$$.fragment),Zs=c(),H=i("p"),Bf=l("Ogni oggetto di configurazione deve implementare la propriet\xE0 "),Kn=i("code"),Mf=l("inputs"),Xf=l(` e restituire una
mappatura, dove ogni chiave corrisponde a un input previsto e ogni valore
indica l\u2019asse di quell\u2019input. Per DistilBERT, possiamo vedere che sono richiesti
due input: `),Gn=i("code"),Rf=l("input_ids"),Ff=l(" e "),Vn=i("code"),Qf=l("attention_mask"),Hf=l(`. Questi inputs hanno la stessa forma di
`),Jn=i("code"),Uf=l("(batch_size, sequence_length)"),Wf=l(` per questo motivo vediamo gli stessi assi usati nella
configurazione.`),er=c(),_(it.$$.fragment),tr=c(),zi=i("p"),Kf=l(`Dopo aver implementato una configurazione ONNX, \xE8 possibile istanziarla
fornendo alla configurazione del modello base come segue:`),or=c(),_(po.$$.fragment),ir=c(),Ei=i("p"),Gf=l(`L\u2019oggetto risultante ha diverse propriet\xE0 utili. Ad esempio \xE8 possibile visualizzare il
Set operatore ONNX che verr\xE0 utilizzato durante l\u2019esportazione:`),ar=c(),_(co.$$.fragment),nr=c(),$i=i("p"),Vf=l("\xC8 inoltre possibile visualizzare gli output associati al modello come segue:"),lr=c(),_(uo.$$.fragment),sr=c(),Z=i("p"),Jf=l(`Puoi notare che la propriet\xE0 degli output segue la stessa struttura degli input; esso
restituisce un `),Yn=i("code"),Yf=l("OrderedDict"),Zf=l(` di output con nome e le loro forme. La struttura di output
\xE8 legato alla scelta della funzione con cui viene inizializzata la configurazione.
Per impostazione predefinita, la configurazione ONNX viene inizializzata con la funzione \u2018predefinita\u2019
che corrisponde all\u2019esportazione di un modello caricato con la classe `),Zn=i("code"),em=l("AutoModel"),tm=l(`. Se tu
desideri esportare una topologia di modello diversa, \xE8 sufficiente fornire una funzionalit\xE0 diversa a
l\u2019argomento `),el=i("code"),om=l("task"),im=l(` quando inizializzi la configurazione ONNX. Ad esempio, se
volevamo esportare DistilBERT con una testa di classificazione per sequenze, potremmo
usare:`),rr=c(),_(fo.$$.fragment),pr=c(),_(at.$$.fragment),cr=c(),ke=i("h4"),nt=i("a"),tl=i("span"),_(mo.$$.fragment),am=c(),ol=i("span"),nm=l("Esportazione del modello"),dr=c(),ce=i("p"),lm=l(`Una volta implementata la configurazione ONNX, il passaggio successivo consiste nell\u2019esportare il
modello. Qui possiamo usare la funzione `),il=i("code"),sm=l("export()"),rm=l(` fornita dal
pacchetto `),al=i("code"),pm=l("transformers.onnx"),cm=l(`. Questa funzione prevede la configurazione ONNX, insieme
con il modello base e il tokenizer e il percorso per salvare il file esportato:`),ur=c(),_(ho.$$.fragment),fr=c(),R=i("p"),dm=l("Gli "),nl=i("code"),um=l("onnx_inputs"),fm=l(" e "),ll=i("code"),mm=l("onnx_outputs"),hm=l(" restituiti dalla funzione "),sl=i("code"),vm=l("export()"),gm=l(` sono
liste di chiavi definite nelle propriet\xE0 di `),rl=i("code"),_m=l("input"),zm=l(" e "),pl=i("code"),Em=l("output"),$m=l(` della
configurazione. Una volta esportato il modello, puoi verificare che il modello sia ben
formato come segue:`),mr=c(),_(vo.$$.fragment),hr=c(),_(lt.$$.fragment),vr=c(),xe=i("h4"),st=i("a"),cl=i("span"),_(go.$$.fragment),bm=c(),dl=i("span"),wm=l("Convalida degli output del modello"),gr=c(),de=i("p"),km=l(`Il passaggio finale consiste nel convalidare gli output dal modello di base e quello esportato
corrispondere entro una soglia di tolleranza assoluta. Qui possiamo usare la
Funzione `),ul=i("code"),xm=l("validate_model_outputs()"),Nm=l(" fornita dal pacchetto "),fl=i("code"),Om=l("transformers.onnx"),Tm=l(`
come segue:`),_r=c(),_(_o.$$.fragment),zr=c(),rt=i("p"),jm=l("Questa funzione usa il metodo "),ml=i("code"),Am=l("OnnxConfig.generate_dummy_inputs()"),ym=l(` per generare
input per il modello di base e quello esportato e la tolleranza assoluta pu\xF2 essere
definita nella configurazione. Generalmente troviamo una corrispondenza numerica nell\u2019intervallo da 1e-6
a 1e-4, anche se \xE8 probabile che qualsiasi cosa inferiore a 1e-3 vada bene.`),Er=c(),Ne=i("h3"),pt=i("a"),hl=i("span"),_(zo.$$.fragment),qm=c(),vl=i("span"),Pm=l("Contribuire con una nuova configurazione a \u{1F917} Transformers"),$r=c(),bi=i("p"),Cm=l(`Stiamo cercando di espandere l\u2019insieme di configurazioni gi\xE0 pronte e di accettare
contributi della community! Se vuoi contribuire con la tua aggiunta
nella libreria, dovrai:`),br=c(),ue=i("ul"),wi=i("li"),Dm=l("Implementare la configurazione ONNX nella corrispondente "),gl=i("code"),Sm=l("configuration file _<model_name>.py"),Im=c(),ki=i("li"),Lm=l("Includere l\u2019architettura del modello e le funzioni corrispondenti in "),_l=i("code"),Bm=l("FeatureManager"),Mm=c(),xi=i("li"),Xm=l("Aggiungere la tua architettura del modello ai test in "),zl=i("code"),Rm=l("test_onnx_v2.py"),wr=c(),ct=i("p"),Fm=l(`Scopri come stato contribuito la configurazione per [IBERT]
(`),Eo=i("a"),Qm=l("https://github.com/huggingface/transformers/pull/14868/files"),Hm=l(`) per
avere un\u2019idea di cosa \xE8 coinvolto.`),kr=c(),Oe=i("h2"),dt=i("a"),El=i("span"),_($o.$$.fragment),Um=c(),$l=i("span"),Wm=l("TorchScript"),xr=c(),_(ut.$$.fragment),Nr=c(),ft=i("p"),Km=l(`Secondo la documentazione di Pytorch: \u201CTorchScript \xE8 un modo per creare modelli serializzabili e ottimizzabili da codice
Pytorch\u201D. I due moduli di Pytorch `),bo=i("a"),Gm=l("JIT e TRACE"),Vm=l(` consentono allo sviluppatore di esportare
il loro modello da riutilizzare in altri programmi, come i programmi C++ orientati all\u2019efficienza.`),Or=c(),Ni=i("p"),Jm=l(`Abbiamo fornito un\u2019interfaccia che consente l\u2019esportazione di modelli \u{1F917} Transformers in TorchScript in modo che possano essere riutilizzati
in un ambiente diverso rispetto a un programma Python basato su Pytorch. Qui spieghiamo come esportare e utilizzare i nostri modelli utilizzando
TorchScript.`),Tr=c(),Oi=i("p"),Ym=l("Esportare un modello richiede due cose:"),jr=c(),mt=i("ul"),bl=i("li"),Zm=l("Un passaggio in avanti con input fittizzi."),eh=c(),wo=i("li"),th=l("Istanziazione del modello con flag "),wl=i("code"),oh=l("torchscript"),ih=l("."),Ar=c(),Ti=i("p"),ah=l("Queste necessit\xE0 implicano diverse cose a cui gli sviluppatori dovrebbero prestare attenzione. Questi dettagli mostrati sotto."),yr=c(),Te=i("h3"),ht=i("a"),kl=i("span"),_(ko.$$.fragment),nh=c(),xl=i("span"),lh=l("Flag TorchScript e pesi legati"),qr=c(),ji=i("p"),sh=l(`Questo flag \xE8 necessario perch\xE9 la maggior parte dei modelli linguistici in questo repository hanno pesi legati tra il loro
strato \u201CEmbedding\u201D e lo strato \u201CDecoding\u201D. TorchScript non consente l\u2019esportazione di modelli che hanno pesi
legati, quindi \xE8 necessario prima slegare e clonare i pesi.`),Pr=c(),ee=i("p"),rh=l("Ci\xF2 implica che i modelli istanziati con il flag "),Nl=i("code"),ph=l("torchscript"),ch=l(" hanno il loro strato "),Ol=i("code"),dh=l("Embedding"),uh=l(" e strato "),Tl=i("code"),fh=l("Decoding"),mh=l(`
separato, il che significa che non dovrebbero essere addestrati in futuro. L\u2019allenamento de-sincronizza i due
strati, portando a risultati inaspettati.`),Cr=c(),vt=i("p"),hh=l(`Questo non \xE8 il caso per i modelli che non hanno una testa del modello linguistico, poich\xE9 quelli non hanno pesi legati. Questi modelli
pu\xF2 essere esportato in sicurezza senza il flag `),jl=i("code"),vh=l("torchscript"),gh=l("."),Dr=c(),je=i("h3"),gt=i("a"),Al=i("span"),_(xo.$$.fragment),_h=c(),yl=i("span"),zh=l("Input fittizi e standard lengths"),Sr=c(),Ai=i("p"),Eh=l(`Gli input fittizzi sono usati per fare un modello passaggio in avanti . Mentre i valori degli input si propagano attraverso i strati,
Pytorch tiene traccia delle diverse operazioni eseguite su ciascun tensore. Queste operazioni registrate vengono quindi utilizzate per
creare la \u201Ctraccia\u201D del modello.`),Ir=c(),yi=i("p"),$h=l(`La traccia viene creata relativamente alle dimensioni degli input. \xC8 quindi vincolato dalle dimensioni dell\u2019input
fittizio e non funzioner\xE0 per altre lunghezze di sequenza o dimensioni batch. Quando si prover\xE0 con una dimensione diversa, ci sar\xE0 errore
come:`),Lr=c(),qi=i("p"),ql=i("code"),bh=l("La dimensione espansa del tensore (3) deve corrispondere alla dimensione esistente (7) nella dimensione non singleton 2"),Br=c(),Pi=i("p"),wh=l(`will be raised. Si consiglia pertanto di tracciare il modello con una dimensione di input fittizia grande almeno quanto il pi\xF9 grande
input che verr\xE0 fornito al modello durante l\u2019inferenza. \xC8 possibile eseguire il padding per riempire i valori mancanti. Il modello
sar\xE0 tracciato con una grande dimensione di input, tuttavia, anche le dimensioni della diverse matrici saranno grandi,
risultando in pi\xF9 calcoli.`),Mr=c(),Ci=i("p"),kh=l(`Si raccomanda di prestare attenzione al numero totale di operazioni eseguite su ciascun input e di seguire da vicino le prestazioni
durante l\u2019esportazione di modelli di sequenza-lunghezza variabili.`),Xr=c(),Ae=i("h3"),_t=i("a"),Pl=i("span"),_(No.$$.fragment),xh=c(),Cl=i("span"),Nh=l("Usare TorchSscript in Python"),Rr=c(),Di=i("p"),Oh=l("Di seguito \xE8 riportato un esempio, che mostra come salvare, caricare modelli e come utilizzare la traccia per l\u2019inferenza."),Fr=c(),ye=i("h4"),zt=i("a"),Dl=i("span"),_(Oo.$$.fragment),Th=c(),Sl=i("span"),jh=l("Salvare un modello"),Qr=c(),V=i("p"),Ah=l("Questo frammento di codice mostra come usare TorchScript per esportare un "),Il=i("code"),yh=l("BertModel"),qh=l(". Qui il "),Ll=i("code"),Ph=l("BertModel"),Ch=l(` \xE8 istanziato secondo
una classe `),Bl=i("code"),Dh=l("BertConfig"),Sh=l(" e quindi salvato su disco con il nome del file "),Ml=i("code"),Ih=l("traced_bert.pt"),Hr=c(),_(To.$$.fragment),Ur=c(),qe=i("h4"),Et=i("a"),Xl=i("span"),_(jo.$$.fragment),Lh=c(),Rl=i("span"),Bh=l("Caricare un modello"),Wr=c(),te=i("p"),Mh=l("Questo frammento di codice mostra come caricare il "),Fl=i("code"),Xh=l("BertModel"),Rh=l(" che era stato precedentemente salvato su disco con il nome "),Ql=i("code"),Fh=l("traced_bert.pt"),Qh=l(`.
Stiamo riutilizzando il `),Hl=i("code"),Hh=l("dummy_input"),Uh=l(" precedentemente inizializzato."),Kr=c(),_(Ao.$$.fragment),Gr=c(),Pe=i("h4"),$t=i("a"),Ul=i("span"),_(yo.$$.fragment),Wh=c(),Wl=i("span"),Kh=l("Utilizzare un modello tracciato per l'inferenza"),Vr=c(),bt=i("p"),Gh=l("Usare il modello tracciato per l\u2019inferenza \xE8 semplice come usare il suo metodo dunder "),Kl=i("code"),Vh=l("__call__"),Jh=l(":"),Jr=c(),_(qo.$$.fragment),Yr=c(),Si=i("p"),Yh=l("###Implementare modelli HuggingFace TorchScript su AWS utilizzando Neuron SDK"),Zr=c(),fe=i("p"),Zh=l("AWS ha introdotto "),Po=i("a"),ev=l("Amazon EC2 Inf1"),tv=l(`
famiglia di istanze per l\u2019inferenza di machine learning a basso costo e ad alte prestazioni nel cloud.
Le istanze Inf1 sono alimentate dal chip AWS Inferentia, un acceleratore hardware personalizzato,
specializzato in carichi di lavoro di inferenza di deep learning.
`),Co=i("a"),ov=l("AWS Neuron"),iv=l(`
\xE8 l\u2019SDK per Inferentia che supporta il tracciamento e l\u2019ottimizzazione dei modelli transformers per
distribuzione su Inf1. L\u2019SDK Neuron fornisce:`),ep=c(),me=i("ol"),Gl=i("li"),av=l("API di facile utilizzo con una riga di modifica del codice per tracciare e ottimizzare un modello TorchScript per l\u2019inferenza nel cloud."),nv=c(),Ii=i("li"),lv=l("Ottimizzazioni delle prestazioni pronte all\u2019uso per "),Do=i("a"),sv=l("miglioramento dei costi-prestazioni"),rv=c(),Ce=i("li"),pv=l("Supporto per i modelli di trasformatori HuggingFace costruiti con "),So=i("a"),cv=l("PyTorch"),dv=l(`
o `),Io=i("a"),uv=l("TensorFlow"),fv=l("."),tp=c(),De=i("h4"),wt=i("a"),Vl=i("span"),_(Lo.$$.fragment),mv=c(),Jl=i("span"),hv=l("Implicazioni"),op=c(),F=i("p"),vv=l("Modelli Transformers basati su architettura "),Bo=i("a"),gv=l("BERT (Bidirectional Encoder Representations from Transformers)"),_v=l(`,
o sue varianti come `),Mo=i("a"),zv=l("distilBERT"),Ev=l(`
e `),Xo=i("a"),$v=l("roBERTa"),bv=l(`
funzioneranno meglio su Inf1 per attivit\xE0 non generative come la question answering estrattive,
Classificazione della sequenza, Classificazione dei token. In alternativa, generazione di testo
le attivit\xE0 possono essere adattate per essere eseguite su Inf1, secondo questo `),Ro=i("a"),wv=l("tutorial AWS Neuron MarianMT"),kv=l(`.
Ulteriori informazioni sui modelli che possono essere convertiti fuori dagli schemi su Inferentia possono essere
trovati nella `),Fo=i("a"),xv=l("sezione Model Architecture Fit della documentazione Neuron"),Nv=l("."),ip=c(),Se=i("h4"),kt=i("a"),Yl=i("span"),_(Qo.$$.fragment),Ov=c(),Zl=i("span"),Tv=l("Dipendenze"),ap=c(),Li=i("p"),jv=l("L\u2019utilizzo di AWS Neuron per convertire i modelli richiede le seguenti dipendenze e l\u2019ambiente:"),np=c(),Bi=i("ul"),Ie=i("li"),Av=l("A "),Ho=i("a"),yv=l("Neuron SDK environment"),qv=l(`,
which comes pre-configured on `),Uo=i("a"),Pv=l("AWS Deep Learning AMI"),Cv=l("."),lp=c(),Le=i("h4"),xt=i("a"),es=i("span"),_(Wo.$$.fragment),Dv=c(),ts=i("span"),Sv=l("Convertire un modello per AWS Neuron"),sp=c(),he=i("p"),Iv=l("Usando lo stesso script come in "),Ko=i("a"),Lv=l("Usando TorchScipt in Python"),Bv=l(`
per tracciare un \u201CBertModel\u201D, importi l\u2019estensione del framework `),os=i("code"),Mv=l("torch.neuron"),Xv=l(` per accedere
i componenti di Neuron SDK tramite un\u2019API Python.`),rp=c(),_(Go.$$.fragment),pp=c(),Mi=i("p"),Rv=l("E modificare solo la riga di codice di traccia"),cp=c(),Xi=i("p"),Fv=l("Da:"),dp=c(),_(Vo.$$.fragment),up=c(),Ri=i("p"),Qv=l("A:"),fp=c(),_(Jo.$$.fragment),mp=c(),Fi=i("p"),Hv=l("Questa modifica consente a Neuron SDK di tracciare il modello e ottimizzarlo per l\u2019esecuzione nelle istanze Inf1."),hp=c(),Nt=i("p"),Uv=l(`Per ulteriori informazioni sulle funzionalit\xE0, gli strumenti, i tutorial di esempi e gli ultimi aggiornamenti di AWS Neuron SDK,
consultare la `),Yo=i("a"),Wv=l("documentazione AWS NeuronSDK"),Kv=l("."),this.h()},l(e){const r=wE('[data-svelte="svelte-1phssyn"]',document.head);f=a(r,"META",{name:!0,content:!0}),r.forEach(o),x=d(e),m=a(e,"H1",{class:!0});var Zo=n(m);k=a(Zo,"A",{id:!0,class:!0,href:!0});var is=n(k);O=a(is,"SPAN",{});var as=n(O);z(N.$$.fragment,as),as.forEach(o),is.forEach(o),P=d(Zo),q=a(Zo,"SPAN",{});var ns=n(q);A=s(ns,"Esporta modelli \u{1F917} Transformers"),ns.forEach(o),Zo.forEach(o),T=d(e),C=a(e,"P",{});var ls=n(C);y=s(ls,`Se devi implementare \u{1F917} modelli Transformers in ambienti di produzione, noi
consigliamo di esportarli in un formato serializzato che pu\xF2 essere caricato ed eseguito
su runtime e hardware specializzati. In questa guida ti mostreremo come farlo
esporta \u{1F917} Modelli Transformers in due formati ampiamente utilizzati: ONNX e TorchScript.`),ls.forEach(o),D=d(e),g=a(e,"P",{});var ei=n(g);j=s(ei,`Una volta esportato, un modello pu\xF2 essere ottimizato per l\u2019inferenza tramite tecniche come
la quantizzazione e soppressione. Se sei interessato a ottimizzare i tuoi modelli per l\u2019esecuzione
con la massima efficienza, dai un\u2019occhiata a `),B=a(ei,"A",{href:!0,rel:!0});var ss=n(B);W=s(ss,`\u{1F917} Optimum
library`),ss.forEach(o),ge=s(ei,"."),ei.forEach(o),Me=d(e),K=a(e,"H2",{class:!0});var gp=n(K);Q=a(gp,"A",{id:!0,class:!0,href:!0});var og=n(Q);_e=a(og,"SPAN",{});var ig=n(_e);z(ae.$$.fragment,ig),ig.forEach(o),og.forEach(o),ze=d(gp),I=a(gp,"SPAN",{});var ag=n(I);ti=s(ag,"ONNX"),ag.forEach(o),gp.forEach(o),qt=d(e),G=a(e,"P",{});var Qi=n(G);oi=s(Qi,"Il progetto "),ne=a(Qi,"A",{href:!0,rel:!0});var ng=n(ne);ii=s(ng,"ONNX (Open Neural Network eXchange)"),ng.forEach(o),ai=s(Qi,` Il progetto onnx \xE8 un open
standard che definisce un insieme comune di operatori e un formato di file comune a
rappresentano modelli di deep learning in un\u2019ampia variet\xE0 di framework, tra cui
PyTorch e TensorFlow. Quando un modello viene esportato nel formato ONNX, questi
operatori sono usati per costruire un grafico computazionale (often called an
`),aa=a(Qi,"EM",{});var lg=n(aa);cc=s(lg,"intermediate representation"),lg.forEach(o),dc=s(Qi,`) che rappresenta il flusso di dati attraverso la
rete neurale.`),Qi.forEach(o),rs=d(e),ni=a(e,"P",{});var sg=n(ni);uc=s(sg,`Esponendo un grafico con operatori e tipi di dati standardizzati, ONNX rende
pi\xF9 facile passare da un framework all\u2019altro. Ad esempio, un modello allenato in PyTorch pu\xF2
essere esportato in formato ONNX e quindi importato in TensorFlow (e viceversa).`),sg.forEach(o),ps=d(e),Xe=a(e,"P",{});var _p=n(Xe);fc=s(_p,"\u{1F917} Transformers fornisce un pacchetto "),na=a(_p,"CODE",{});var rg=n(na);mc=s(rg,"transformers.onnx"),rg.forEach(o),hc=s(_p,` che ti consente di
convertire i checkpoint del modello in un grafico ONNX sfruttando gli oggetti di configurazione.
Questi oggetti di configurazione sono gi\xE0 pronti per una serie di architetture di modelli,
e sono progettati per essere facilmente estensibili ad altre architetture.`),_p.forEach(o),cs=d(e),li=a(e,"P",{});var pg=n(li);vc=s(pg,"Le configurazioni pronte includono le seguenti architetture:"),pg.forEach(o),ds=d(e),h=a(e,"UL",{});var v=n(h);la=a(v,"LI",{});var cg=n(la);gc=s(cg,"ALBERT"),cg.forEach(o),_c=d(v),sa=a(v,"LI",{});var dg=n(sa);zc=s(dg,"BART"),dg.forEach(o),Ec=d(v),ra=a(v,"LI",{});var ug=n(ra);$c=s(ug,"BEiT"),ug.forEach(o),bc=d(v),pa=a(v,"LI",{});var fg=n(pa);wc=s(fg,"BERT"),fg.forEach(o),kc=d(v),ca=a(v,"LI",{});var mg=n(ca);xc=s(mg,"BigBird"),mg.forEach(o),Nc=d(v),da=a(v,"LI",{});var hg=n(da);Oc=s(hg,"BigBird-Pegasus"),hg.forEach(o),Tc=d(v),ua=a(v,"LI",{});var vg=n(ua);jc=s(vg,"Blenderbot"),vg.forEach(o),Ac=d(v),fa=a(v,"LI",{});var gg=n(fa);yc=s(gg,"BlenderbotSmall"),gg.forEach(o),qc=d(v),ma=a(v,"LI",{});var _g=n(ma);Pc=s(_g,"CamemBERT"),_g.forEach(o),Cc=d(v),ha=a(v,"LI",{});var zg=n(ha);Dc=s(zg,"ConvBERT"),zg.forEach(o),Sc=d(v),va=a(v,"LI",{});var Eg=n(va);Ic=s(Eg,"Data2VecText"),Eg.forEach(o),Lc=d(v),ga=a(v,"LI",{});var $g=n(ga);Bc=s($g,"Data2VecVision"),$g.forEach(o),Mc=d(v),_a=a(v,"LI",{});var bg=n(_a);Xc=s(bg,"DeiT"),bg.forEach(o),Rc=d(v),za=a(v,"LI",{});var wg=n(za);Fc=s(wg,"DistilBERT"),wg.forEach(o),Qc=d(v),Ea=a(v,"LI",{});var kg=n(Ea);Hc=s(kg,"ELECTRA"),kg.forEach(o),Uc=d(v),$a=a(v,"LI",{});var xg=n($a);Wc=s(xg,"FlauBERT"),xg.forEach(o),Kc=d(v),ba=a(v,"LI",{});var Ng=n(ba);Gc=s(Ng,"GPT Neo"),Ng.forEach(o),Vc=d(v),wa=a(v,"LI",{});var Og=n(wa);Jc=s(Og,"GPT-J"),Og.forEach(o),Yc=d(v),ka=a(v,"LI",{});var Tg=n(ka);Zc=s(Tg,"I-BERT"),Tg.forEach(o),ed=d(v),xa=a(v,"LI",{});var jg=n(xa);td=s(jg,"LayoutLM"),jg.forEach(o),od=d(v),Na=a(v,"LI",{});var Ag=n(Na);id=s(Ag,"M2M100"),Ag.forEach(o),ad=d(v),Oa=a(v,"LI",{});var yg=n(Oa);nd=s(yg,"Marian"),yg.forEach(o),ld=d(v),Ta=a(v,"LI",{});var qg=n(Ta);sd=s(qg,"mBART"),qg.forEach(o),rd=d(v),ja=a(v,"LI",{});var Pg=n(ja);pd=s(Pg,"MobileBERT"),Pg.forEach(o),cd=d(v),Aa=a(v,"LI",{});var Cg=n(Aa);dd=s(Cg,"OpenAI GPT-2"),Cg.forEach(o),ud=d(v),ya=a(v,"LI",{});var Dg=n(ya);fd=s(Dg,"Perceiver"),Dg.forEach(o),md=d(v),qa=a(v,"LI",{});var Sg=n(qa);hd=s(Sg,"PLBart"),Sg.forEach(o),vd=d(v),Pa=a(v,"LI",{});var Ig=n(Pa);gd=s(Ig,"RoBERTa"),Ig.forEach(o),_d=d(v),Ca=a(v,"LI",{});var Lg=n(Ca);zd=s(Lg,"RoFormer"),Lg.forEach(o),Ed=d(v),Da=a(v,"LI",{});var Bg=n(Da);$d=s(Bg,"SqueezeBERT"),Bg.forEach(o),bd=d(v),Sa=a(v,"LI",{});var Mg=n(Sa);wd=s(Mg,"T5"),Mg.forEach(o),kd=d(v),Ia=a(v,"LI",{});var Xg=n(Ia);xd=s(Xg,"ViT"),Xg.forEach(o),Nd=d(v),La=a(v,"LI",{});var Rg=n(La);Od=s(Rg,"XLM"),Rg.forEach(o),Td=d(v),Ba=a(v,"LI",{});var Fg=n(Ba);jd=s(Fg,"XLM-RoBERTa"),Fg.forEach(o),Ad=d(v),Ma=a(v,"LI",{});var Qg=n(Ma);yd=s(Qg,"XLM-RoBERTa-XL"),Qg.forEach(o),v.forEach(o),us=d(e),si=a(e,"P",{});var Hg=n(si);qd=s(Hg,"Nelle prossime due sezioni, ti mostreremo come:"),Hg.forEach(o),fs=d(e),Re=a(e,"UL",{});var zp=n(Re);Pt=a(zp,"LI",{});var Ep=n(Pt);Pd=s(Ep,"Esporta un modello supportato usando il pacchetto "),Xa=a(Ep,"CODE",{});var Ug=n(Xa);Cd=s(Ug,"transformers.onnx"),Ug.forEach(o),Dd=s(Ep,"."),Ep.forEach(o),Sd=d(zp),Ra=a(zp,"LI",{});var Wg=n(Ra);Id=s(Wg,"Esporta un modello personalizzato per un\u2019architettura non supportata."),Wg.forEach(o),zp.forEach(o),ms=d(e),Ee=a(e,"H3",{class:!0});var $p=n(Ee);Fe=a($p,"A",{id:!0,class:!0,href:!0});var Kg=n(Fe);Fa=a(Kg,"SPAN",{});var Gg=n(Fa);z(Ct.$$.fragment,Gg),Gg.forEach(o),Kg.forEach(o),Ld=d($p),Qa=a($p,"SPAN",{});var Vg=n(Qa);Bd=s(Vg,"Esportazione di un modello in ONNX"),Vg.forEach(o),$p.forEach(o),hs=d(e),ri=a(e,"P",{});var Jg=n(ri);Md=s(Jg,`Per esportare un modello \u{1F917} Transformers in ONNX, dovrai prima installarne alcune
dipendenze extra:`),Jg.forEach(o),vs=d(e),z(Dt.$$.fragment,e),gs=d(e),Qe=a(e,"P",{});var bp=n(Qe);Xd=s(bp,"Il pacchetto "),Ha=a(bp,"CODE",{});var Yg=n(Ha);Rd=s(Yg,"transformers.onnx"),Yg.forEach(o),Fd=s(bp," pu\xF2 essere usato come modulo Python:"),bp.forEach(o),_s=d(e),z(St.$$.fragment,e),zs=d(e),pi=a(e,"P",{});var Zg=n(pi);Qd=s(Zg,"L\u2019esportazione di un checkpoint utilizzando una configurazione gi\xE0 pronta pu\xF2 essere eseguita come segue:"),Zg.forEach(o),Es=d(e),z(It.$$.fragment,e),$s=d(e),ci=a(e,"P",{});var e_=n(ci);Hd=s(e_,"che dovrebbe mostrare i seguenti log:"),e_.forEach(o),bs=d(e),z(Lt.$$.fragment,e),ws=d(e),le=a(e,"P",{});var Hi=n(le);Ud=s(Hi,"Questo esporta un grafico ONNX del checkpoint definito dall\u2019argomento "),Ua=a(Hi,"CODE",{});var t_=n(Ua);Wd=s(t_,"--model"),t_.forEach(o),Kd=s(Hi,`.
In questo esempio \xE8 `),Wa=a(Hi,"CODE",{});var o_=n(Wa);Gd=s(o_,"distilbert-base-uncased"),o_.forEach(o),Vd=s(Hi,`, ma pu\xF2 essere qualsiasi checkpoint
Hugging Face Hub o uno memorizzato localmente.`),Hi.forEach(o),ks=d(e),J=a(e,"P",{});var Ot=n(J);Jd=s(Ot,"Il file risultante "),Ka=a(Ot,"CODE",{});var i_=n(Ka);Yd=s(i_,"model.onnx"),i_.forEach(o),Zd=s(Ot," pu\xF2 quindi essere eseguito su uno dei "),Bt=a(Ot,"A",{href:!0,rel:!0});var a_=n(Bt);eu=s(a_,`tanti
acceleratori`),a_.forEach(o),tu=s(Ot,` che supportano il
lo standard ONNX. Ad esempio, possiamo caricare ed eseguire il modello con `),Mt=a(Ot,"A",{href:!0,rel:!0});var n_=n(Mt);ou=s(n_,`ONNX
Runtime`),n_.forEach(o),iu=s(Ot," come segue:"),Ot.forEach(o),xs=d(e),z(Xt.$$.fragment,e),Ns=d(e),He=a(e,"P",{});var wp=n(He);au=s(wp,"I nomi di output richiesti (cio\xE8 "),Ga=a(wp,"CODE",{});var l_=n(Ga);nu=s(l_,'["last_hidden_state"]'),l_.forEach(o),lu=s(wp,`) possono essere ottenuti
dando un\u2019occhiata alla configurazione ONNX di ogni modello. Ad esempio, per
DistilBERT abbiamo:`),wp.forEach(o),Os=d(e),z(Rt.$$.fragment,e),Ts=d(e),Ue=a(e,"P",{});var kp=n(Ue);su=s(kp,`Il processo \xE8 identico per i checkpoint TensorFlow sull\u2019hub. Ad esempio, noi
possiamo esportare un checkpoint TensorFlow puro da `),Ft=a(kp,"A",{href:!0,rel:!0});var s_=n(Ft);ru=s(s_,`Keras
organizzazione`),s_.forEach(o),pu=s(kp," come segue:"),kp.forEach(o),js=d(e),z(Qt.$$.fragment,e),As=d(e),di=a(e,"P",{});var r_=n(di);cu=s(r_,`Per esportare un modello memorizzato localmente, devi disporre dei pesi del modello
e file tokenizer memorizzati in una directory. Ad esempio, possiamo caricare e salvare un
checkpoint come segue:`),r_.forEach(o),ys=d(e),z(We.$$.fragment,e),qs=d(e),$e=a(e,"H3",{class:!0});var xp=n($e);Ke=a(xp,"A",{id:!0,class:!0,href:!0});var p_=n(Ke);Va=a(p_,"SPAN",{});var c_=n(Va);z(Ht.$$.fragment,c_),c_.forEach(o),p_.forEach(o),du=d(xp),Ja=a(xp,"SPAN",{});var d_=n(Ja);uu=s(d_,"Selezione delle caratteristiche per diverse topologie di modello"),d_.forEach(o),xp.forEach(o),Ps=d(e),Ge=a(e,"P",{});var Np=n(Ge);fu=s(Np,"Ogni configurazione gi\xE0 pronta viene fornita con una serie di "),Ya=a(Np,"EM",{});var u_=n(Ya);mu=s(u_,"caratteristiche"),u_.forEach(o),hu=s(Np,` che ti consentono di
esportare modelli per diversi tipi di topologie o attivit\xE0. Come mostrato nella tabella
di seguito, ogni caratteristica \xE8 associata a una diversa Auto Class:`),Np.forEach(o),Cs=d(e),Ve=a(e,"TABLE",{});var Op=n(Ve);Za=a(Op,"THEAD",{});var f_=n(Za);Ut=a(f_,"TR",{});var Tp=n(Ut);en=a(Tp,"TH",{});var m_=n(en);vu=s(m_,"Caratteristica"),m_.forEach(o),gu=d(Tp),tn=a(Tp,"TH",{});var h_=n(tn);_u=s(h_,"Auto Class"),h_.forEach(o),Tp.forEach(o),f_.forEach(o),zu=d(Op),X=a(Op,"TBODY",{});var U=n(X);Wt=a(U,"TR",{});var jp=n(Wt);Kt=a(jp,"TD",{});var Ap=n(Kt);on=a(Ap,"CODE",{});var v_=n(on);Eu=s(v_,"causal-lm"),v_.forEach(o),$u=s(Ap,", "),an=a(Ap,"CODE",{});var g_=n(an);bu=s(g_,"causal-lm-with-past"),g_.forEach(o),Ap.forEach(o),wu=d(jp),nn=a(jp,"TD",{});var __=n(nn);ln=a(__,"CODE",{});var z_=n(ln);ku=s(z_,"AutoModelForCausalLM"),z_.forEach(o),__.forEach(o),jp.forEach(o),xu=d(U),Gt=a(U,"TR",{});var yp=n(Gt);Vt=a(yp,"TD",{});var qp=n(Vt);sn=a(qp,"CODE",{});var E_=n(sn);Nu=s(E_,"default"),E_.forEach(o),Ou=s(qp,", "),rn=a(qp,"CODE",{});var $_=n(rn);Tu=s($_,"default-with-past"),$_.forEach(o),qp.forEach(o),ju=d(yp),pn=a(yp,"TD",{});var b_=n(pn);cn=a(b_,"CODE",{});var w_=n(cn);Au=s(w_,"AutoModel"),w_.forEach(o),b_.forEach(o),yp.forEach(o),yu=d(U),Jt=a(U,"TR",{});var Pp=n(Jt);dn=a(Pp,"TD",{});var k_=n(dn);un=a(k_,"CODE",{});var x_=n(un);qu=s(x_,"masked-lm"),x_.forEach(o),k_.forEach(o),Pu=d(Pp),fn=a(Pp,"TD",{});var N_=n(fn);mn=a(N_,"CODE",{});var O_=n(mn);Cu=s(O_,"AutoModelForMaskedLM"),O_.forEach(o),N_.forEach(o),Pp.forEach(o),Du=d(U),Yt=a(U,"TR",{});var Cp=n(Yt);hn=a(Cp,"TD",{});var T_=n(hn);vn=a(T_,"CODE",{});var j_=n(vn);Su=s(j_,"question-answering"),j_.forEach(o),T_.forEach(o),Iu=d(Cp),gn=a(Cp,"TD",{});var A_=n(gn);_n=a(A_,"CODE",{});var y_=n(_n);Lu=s(y_,"AutoModelForQuestionAnswering"),y_.forEach(o),A_.forEach(o),Cp.forEach(o),Bu=d(U),Zt=a(U,"TR",{});var Dp=n(Zt);eo=a(Dp,"TD",{});var Sp=n(eo);zn=a(Sp,"CODE",{});var q_=n(zn);Mu=s(q_,"seq2seq-lm"),q_.forEach(o),Xu=s(Sp,", "),En=a(Sp,"CODE",{});var P_=n(En);Ru=s(P_,"seq2seq-lm-with-past"),P_.forEach(o),Sp.forEach(o),Fu=d(Dp),$n=a(Dp,"TD",{});var C_=n($n);bn=a(C_,"CODE",{});var D_=n(bn);Qu=s(D_,"AutoModelForSeq2SeqLM"),D_.forEach(o),C_.forEach(o),Dp.forEach(o),Hu=d(U),to=a(U,"TR",{});var Ip=n(to);wn=a(Ip,"TD",{});var S_=n(wn);kn=a(S_,"CODE",{});var I_=n(kn);Uu=s(I_,"sequence-classification"),I_.forEach(o),S_.forEach(o),Wu=d(Ip),xn=a(Ip,"TD",{});var L_=n(xn);Nn=a(L_,"CODE",{});var B_=n(Nn);Ku=s(B_,"AutoModelForSequenceClassification"),B_.forEach(o),L_.forEach(o),Ip.forEach(o),Gu=d(U),oo=a(U,"TR",{});var Lp=n(oo);On=a(Lp,"TD",{});var M_=n(On);Tn=a(M_,"CODE",{});var X_=n(Tn);Vu=s(X_,"token-classification"),X_.forEach(o),M_.forEach(o),Ju=d(Lp),jn=a(Lp,"TD",{});var R_=n(jn);An=a(R_,"CODE",{});var F_=n(An);Yu=s(F_,"AutoModelForTokenClassification"),F_.forEach(o),R_.forEach(o),Lp.forEach(o),U.forEach(o),Op.forEach(o),Ds=d(e),Je=a(e,"P",{});var Bp=n(Je);Zu=s(Bp,`Per ciascuna configurazione, puoi trovare l\u2019elenco delle funzionalit\xE0 supportate tramite il
`),yn=a(Bp,"CODE",{});var Q_=n(yn);ef=s(Q_,"FeaturesManager"),Q_.forEach(o),tf=s(Bp,". Ad esempio, per DistilBERT abbiamo:"),Bp.forEach(o),Ss=d(e),z(io.$$.fragment,e),Is=d(e),se=a(e,"P",{});var Ui=n(se);of=s(Ui,"Puoi quindi passare una di queste funzionalit\xE0 all\u2019argomento "),qn=a(Ui,"CODE",{});var H_=n(qn);af=s(H_,"--feature"),H_.forEach(o),nf=s(Ui,` nel
pacchetto `),Pn=a(Ui,"CODE",{});var U_=n(Pn);lf=s(U_,"transformers.onnx"),U_.forEach(o),sf=s(Ui,`. Ad esempio, per esportare un modello di classificazione del testo
possiamo scegliere un modello ottimizzato dall\u2019Hub ed eseguire:`),Ui.forEach(o),Ls=d(e),z(ao.$$.fragment,e),Bs=d(e),ui=a(e,"P",{});var W_=n(ui);rf=s(W_,"che visualizzer\xE0 i seguenti registri:"),W_.forEach(o),Ms=d(e),z(no.$$.fragment,e),Xs=d(e),Y=a(e,"P",{});var Tt=n(Y);pf=s(Tt,`Puoi notare che in questo caso, i nomi di output del modello ottimizzato sono
`),Cn=a(Tt,"CODE",{});var K_=n(Cn);cf=s(K_,"logits"),K_.forEach(o),df=s(Tt," invece di "),Dn=a(Tt,"CODE",{});var G_=n(Dn);uf=s(G_,"last_hidden_state"),G_.forEach(o),ff=s(Tt,` che abbiamo visto con il
checkpoint `),Sn=a(Tt,"CODE",{});var V_=n(Sn);mf=s(V_,"distilbert-base-uncased"),V_.forEach(o),hf=s(Tt,` precedente. Questo \xE8 previsto dal
modello ottimizato visto che ha una testa di e.`),Tt.forEach(o),Rs=d(e),z(Ye.$$.fragment,e),Fs=d(e),be=a(e,"H3",{class:!0});var Mp=n(be);Ze=a(Mp,"A",{id:!0,class:!0,href:!0});var J_=n(Ze);In=a(J_,"SPAN",{});var Y_=n(In);z(lo.$$.fragment,Y_),Y_.forEach(o),J_.forEach(o),vf=d(Mp),Ln=a(Mp,"SPAN",{});var Z_=n(Ln);gf=s(Z_,"Esportazione di un modello per un'architettura non supportata"),Z_.forEach(o),Mp.forEach(o),Qs=d(e),fi=a(e,"P",{});var ez=n(fi);_f=s(ez,`Se desideri esportare un modello la cui architettura non \xE8 nativamente supportata dalla
libreria, ci sono tre passaggi principali da seguire:`),ez.forEach(o),Hs=d(e),re=a(e,"OL",{});var Wi=n(re);Bn=a(Wi,"LI",{});var tz=n(Bn);zf=s(tz,"Implementare una configurazione ONNX personalizzata."),tz.forEach(o),Ef=d(Wi),Mn=a(Wi,"LI",{});var oz=n(Mn);$f=s(oz,"Esportare il modello in ONNX."),oz.forEach(o),bf=d(Wi),Xn=a(Wi,"LI",{});var iz=n(Xn);wf=s(iz,"Convalidare gli output di PyTorch e dei modelli esportati."),iz.forEach(o),Wi.forEach(o),Us=d(e),mi=a(e,"P",{});var az=n(mi);kf=s(az,`In questa sezione, vedremo come DistilBERT \xE8 stato implementato per mostrare cosa \xE8
coinvolto in ogni passaggio.`),az.forEach(o),Ws=d(e),we=a(e,"H4",{class:!0});var Xp=n(we);et=a(Xp,"A",{id:!0,class:!0,href:!0});var nz=n(et);Rn=a(nz,"SPAN",{});var lz=n(Rn);z(so.$$.fragment,lz),lz.forEach(o),nz.forEach(o),xf=d(Xp),Fn=a(Xp,"SPAN",{});var sz=n(Fn);Nf=s(sz,"Implementazione di una configurazione ONNX personalizzata"),sz.forEach(o),Xp.forEach(o),Ks=d(e),hi=a(e,"P",{});var rz=n(hi);Of=s(rz,`Iniziamo con l\u2019oggetto di configurazione ONNX. Forniamo tre classi
astratte da cui ereditare, a seconda del tipo di archittettura
del modello che desideri esportare:`),rz.forEach(o),Gs=d(e),pe=a(e,"UL",{});var Ki=n(pe);vi=a(Ki,"LI",{});var Gv=n(vi);Tf=s(Gv,"I modelli basati su encoder ereditano da "),Qn=a(Gv,"CODE",{});var pz=n(Qn);jf=s(pz,"OnnxConfig"),pz.forEach(o),Gv.forEach(o),Af=d(Ki),gi=a(Ki,"LI",{});var Vv=n(gi);yf=s(Vv,"I modelli basati su decoder ereditano da "),Hn=a(Vv,"CODE",{});var cz=n(Hn);qf=s(cz,"OnnxConfigWithPast"),cz.forEach(o),Vv.forEach(o),Pf=d(Ki),_i=a(Ki,"LI",{});var Jv=n(_i);Cf=s(Jv,"I modelli encoder-decoder ereditano da"),Un=a(Jv,"CODE",{});var dz=n(Un);Df=s(dz,"OnnxSeq2SeqConfigWithPast"),dz.forEach(o),Jv.forEach(o),Ki.forEach(o),Vs=d(e),z(tt.$$.fragment,e),Js=d(e),ot=a(e,"P",{});var Rp=n(ot);Sf=s(Rp,`Poich\xE9 DistilBERT \xE8 un modello basato su encoder, la sua configurazione eredita da
`),Wn=a(Rp,"CODE",{});var uz=n(Wn);If=s(uz,"OnnxConfig"),uz.forEach(o),Lf=s(Rp,":"),Rp.forEach(o),Ys=d(e),z(ro.$$.fragment,e),Zs=d(e),H=a(e,"P",{});var ve=n(H);Bf=s(ve,"Ogni oggetto di configurazione deve implementare la propriet\xE0 "),Kn=a(ve,"CODE",{});var fz=n(Kn);Mf=s(fz,"inputs"),fz.forEach(o),Xf=s(ve,` e restituire una
mappatura, dove ogni chiave corrisponde a un input previsto e ogni valore
indica l\u2019asse di quell\u2019input. Per DistilBERT, possiamo vedere che sono richiesti
due input: `),Gn=a(ve,"CODE",{});var mz=n(Gn);Rf=s(mz,"input_ids"),mz.forEach(o),Ff=s(ve," e "),Vn=a(ve,"CODE",{});var hz=n(Vn);Qf=s(hz,"attention_mask"),hz.forEach(o),Hf=s(ve,`. Questi inputs hanno la stessa forma di
`),Jn=a(ve,"CODE",{});var vz=n(Jn);Uf=s(vz,"(batch_size, sequence_length)"),vz.forEach(o),Wf=s(ve,` per questo motivo vediamo gli stessi assi usati nella
configurazione.`),ve.forEach(o),er=d(e),z(it.$$.fragment,e),tr=d(e),zi=a(e,"P",{});var gz=n(zi);Kf=s(gz,`Dopo aver implementato una configurazione ONNX, \xE8 possibile istanziarla
fornendo alla configurazione del modello base come segue:`),gz.forEach(o),or=d(e),z(po.$$.fragment,e),ir=d(e),Ei=a(e,"P",{});var _z=n(Ei);Gf=s(_z,`L\u2019oggetto risultante ha diverse propriet\xE0 utili. Ad esempio \xE8 possibile visualizzare il
Set operatore ONNX che verr\xE0 utilizzato durante l\u2019esportazione:`),_z.forEach(o),ar=d(e),z(co.$$.fragment,e),nr=d(e),$i=a(e,"P",{});var zz=n($i);Vf=s(zz,"\xC8 inoltre possibile visualizzare gli output associati al modello come segue:"),zz.forEach(o),lr=d(e),z(uo.$$.fragment,e),sr=d(e),Z=a(e,"P",{});var jt=n(Z);Jf=s(jt,`Puoi notare che la propriet\xE0 degli output segue la stessa struttura degli input; esso
restituisce un `),Yn=a(jt,"CODE",{});var Ez=n(Yn);Yf=s(Ez,"OrderedDict"),Ez.forEach(o),Zf=s(jt,` di output con nome e le loro forme. La struttura di output
\xE8 legato alla scelta della funzione con cui viene inizializzata la configurazione.
Per impostazione predefinita, la configurazione ONNX viene inizializzata con la funzione \u2018predefinita\u2019
che corrisponde all\u2019esportazione di un modello caricato con la classe `),Zn=a(jt,"CODE",{});var $z=n(Zn);em=s($z,"AutoModel"),$z.forEach(o),tm=s(jt,`. Se tu
desideri esportare una topologia di modello diversa, \xE8 sufficiente fornire una funzionalit\xE0 diversa a
l\u2019argomento `),el=a(jt,"CODE",{});var bz=n(el);om=s(bz,"task"),bz.forEach(o),im=s(jt,` quando inizializzi la configurazione ONNX. Ad esempio, se
volevamo esportare DistilBERT con una testa di classificazione per sequenze, potremmo
usare:`),jt.forEach(o),rr=d(e),z(fo.$$.fragment,e),pr=d(e),z(at.$$.fragment,e),cr=d(e),ke=a(e,"H4",{class:!0});var Fp=n(ke);nt=a(Fp,"A",{id:!0,class:!0,href:!0});var wz=n(nt);tl=a(wz,"SPAN",{});var kz=n(tl);z(mo.$$.fragment,kz),kz.forEach(o),wz.forEach(o),am=d(Fp),ol=a(Fp,"SPAN",{});var xz=n(ol);nm=s(xz,"Esportazione del modello"),xz.forEach(o),Fp.forEach(o),dr=d(e),ce=a(e,"P",{});var Gi=n(ce);lm=s(Gi,`Una volta implementata la configurazione ONNX, il passaggio successivo consiste nell\u2019esportare il
modello. Qui possiamo usare la funzione `),il=a(Gi,"CODE",{});var Nz=n(il);sm=s(Nz,"export()"),Nz.forEach(o),rm=s(Gi,` fornita dal
pacchetto `),al=a(Gi,"CODE",{});var Oz=n(al);pm=s(Oz,"transformers.onnx"),Oz.forEach(o),cm=s(Gi,`. Questa funzione prevede la configurazione ONNX, insieme
con il modello base e il tokenizer e il percorso per salvare il file esportato:`),Gi.forEach(o),ur=d(e),z(ho.$$.fragment,e),fr=d(e),R=a(e,"P",{});var oe=n(R);dm=s(oe,"Gli "),nl=a(oe,"CODE",{});var Tz=n(nl);um=s(Tz,"onnx_inputs"),Tz.forEach(o),fm=s(oe," e "),ll=a(oe,"CODE",{});var jz=n(ll);mm=s(jz,"onnx_outputs"),jz.forEach(o),hm=s(oe," restituiti dalla funzione "),sl=a(oe,"CODE",{});var Az=n(sl);vm=s(Az,"export()"),Az.forEach(o),gm=s(oe,` sono
liste di chiavi definite nelle propriet\xE0 di `),rl=a(oe,"CODE",{});var yz=n(rl);_m=s(yz,"input"),yz.forEach(o),zm=s(oe," e "),pl=a(oe,"CODE",{});var qz=n(pl);Em=s(qz,"output"),qz.forEach(o),$m=s(oe,` della
configurazione. Una volta esportato il modello, puoi verificare che il modello sia ben
formato come segue:`),oe.forEach(o),mr=d(e),z(vo.$$.fragment,e),hr=d(e),z(lt.$$.fragment,e),vr=d(e),xe=a(e,"H4",{class:!0});var Qp=n(xe);st=a(Qp,"A",{id:!0,class:!0,href:!0});var Pz=n(st);cl=a(Pz,"SPAN",{});var Cz=n(cl);z(go.$$.fragment,Cz),Cz.forEach(o),Pz.forEach(o),bm=d(Qp),dl=a(Qp,"SPAN",{});var Dz=n(dl);wm=s(Dz,"Convalida degli output del modello"),Dz.forEach(o),Qp.forEach(o),gr=d(e),de=a(e,"P",{});var Vi=n(de);km=s(Vi,`Il passaggio finale consiste nel convalidare gli output dal modello di base e quello esportato
corrispondere entro una soglia di tolleranza assoluta. Qui possiamo usare la
Funzione `),ul=a(Vi,"CODE",{});var Sz=n(ul);xm=s(Sz,"validate_model_outputs()"),Sz.forEach(o),Nm=s(Vi," fornita dal pacchetto "),fl=a(Vi,"CODE",{});var Iz=n(fl);Om=s(Iz,"transformers.onnx"),Iz.forEach(o),Tm=s(Vi,`
come segue:`),Vi.forEach(o),_r=d(e),z(_o.$$.fragment,e),zr=d(e),rt=a(e,"P",{});var Hp=n(rt);jm=s(Hp,"Questa funzione usa il metodo "),ml=a(Hp,"CODE",{});var Lz=n(ml);Am=s(Lz,"OnnxConfig.generate_dummy_inputs()"),Lz.forEach(o),ym=s(Hp,` per generare
input per il modello di base e quello esportato e la tolleranza assoluta pu\xF2 essere
definita nella configurazione. Generalmente troviamo una corrispondenza numerica nell\u2019intervallo da 1e-6
a 1e-4, anche se \xE8 probabile che qualsiasi cosa inferiore a 1e-3 vada bene.`),Hp.forEach(o),Er=d(e),Ne=a(e,"H3",{class:!0});var Up=n(Ne);pt=a(Up,"A",{id:!0,class:!0,href:!0});var Bz=n(pt);hl=a(Bz,"SPAN",{});var Mz=n(hl);z(zo.$$.fragment,Mz),Mz.forEach(o),Bz.forEach(o),qm=d(Up),vl=a(Up,"SPAN",{});var Xz=n(vl);Pm=s(Xz,"Contribuire con una nuova configurazione a \u{1F917} Transformers"),Xz.forEach(o),Up.forEach(o),$r=d(e),bi=a(e,"P",{});var Rz=n(bi);Cm=s(Rz,`Stiamo cercando di espandere l\u2019insieme di configurazioni gi\xE0 pronte e di accettare
contributi della community! Se vuoi contribuire con la tua aggiunta
nella libreria, dovrai:`),Rz.forEach(o),br=d(e),ue=a(e,"UL",{});var Ji=n(ue);wi=a(Ji,"LI",{});var Yv=n(wi);Dm=s(Yv,"Implementare la configurazione ONNX nella corrispondente "),gl=a(Yv,"CODE",{});var Fz=n(gl);Sm=s(Fz,"configuration file _<model_name>.py"),Fz.forEach(o),Yv.forEach(o),Im=d(Ji),ki=a(Ji,"LI",{});var Zv=n(ki);Lm=s(Zv,"Includere l\u2019architettura del modello e le funzioni corrispondenti in "),_l=a(Zv,"CODE",{});var Qz=n(_l);Bm=s(Qz,"FeatureManager"),Qz.forEach(o),Zv.forEach(o),Mm=d(Ji),xi=a(Ji,"LI",{});var eg=n(xi);Xm=s(eg,"Aggiungere la tua architettura del modello ai test in "),zl=a(eg,"CODE",{});var Hz=n(zl);Rm=s(Hz,"test_onnx_v2.py"),Hz.forEach(o),eg.forEach(o),Ji.forEach(o),wr=d(e),ct=a(e,"P",{});var Wp=n(ct);Fm=s(Wp,`Scopri come stato contribuito la configurazione per [IBERT]
(`),Eo=a(Wp,"A",{href:!0,rel:!0});var Uz=n(Eo);Qm=s(Uz,"https://github.com/huggingface/transformers/pull/14868/files"),Uz.forEach(o),Hm=s(Wp,`) per
avere un\u2019idea di cosa \xE8 coinvolto.`),Wp.forEach(o),kr=d(e),Oe=a(e,"H2",{class:!0});var Kp=n(Oe);dt=a(Kp,"A",{id:!0,class:!0,href:!0});var Wz=n(dt);El=a(Wz,"SPAN",{});var Kz=n(El);z($o.$$.fragment,Kz),Kz.forEach(o),Wz.forEach(o),Um=d(Kp),$l=a(Kp,"SPAN",{});var Gz=n($l);Wm=s(Gz,"TorchScript"),Gz.forEach(o),Kp.forEach(o),xr=d(e),z(ut.$$.fragment,e),Nr=d(e),ft=a(e,"P",{});var Gp=n(ft);Km=s(Gp,`Secondo la documentazione di Pytorch: \u201CTorchScript \xE8 un modo per creare modelli serializzabili e ottimizzabili da codice
Pytorch\u201D. I due moduli di Pytorch `),bo=a(Gp,"A",{href:!0,rel:!0});var Vz=n(bo);Gm=s(Vz,"JIT e TRACE"),Vz.forEach(o),Vm=s(Gp,` consentono allo sviluppatore di esportare
il loro modello da riutilizzare in altri programmi, come i programmi C++ orientati all\u2019efficienza.`),Gp.forEach(o),Or=d(e),Ni=a(e,"P",{});var Jz=n(Ni);Jm=s(Jz,`Abbiamo fornito un\u2019interfaccia che consente l\u2019esportazione di modelli \u{1F917} Transformers in TorchScript in modo che possano essere riutilizzati
in un ambiente diverso rispetto a un programma Python basato su Pytorch. Qui spieghiamo come esportare e utilizzare i nostri modelli utilizzando
TorchScript.`),Jz.forEach(o),Tr=d(e),Oi=a(e,"P",{});var Yz=n(Oi);Ym=s(Yz,"Esportare un modello richiede due cose:"),Yz.forEach(o),jr=d(e),mt=a(e,"UL",{});var Vp=n(mt);bl=a(Vp,"LI",{});var Zz=n(bl);Zm=s(Zz,"Un passaggio in avanti con input fittizzi."),Zz.forEach(o),eh=d(Vp),wo=a(Vp,"LI",{});var Jp=n(wo);th=s(Jp,"Istanziazione del modello con flag "),wl=a(Jp,"CODE",{});var e1=n(wl);oh=s(e1,"torchscript"),e1.forEach(o),ih=s(Jp,"."),Jp.forEach(o),Vp.forEach(o),Ar=d(e),Ti=a(e,"P",{});var t1=n(Ti);ah=s(t1,"Queste necessit\xE0 implicano diverse cose a cui gli sviluppatori dovrebbero prestare attenzione. Questi dettagli mostrati sotto."),t1.forEach(o),yr=d(e),Te=a(e,"H3",{class:!0});var Yp=n(Te);ht=a(Yp,"A",{id:!0,class:!0,href:!0});var o1=n(ht);kl=a(o1,"SPAN",{});var i1=n(kl);z(ko.$$.fragment,i1),i1.forEach(o),o1.forEach(o),nh=d(Yp),xl=a(Yp,"SPAN",{});var a1=n(xl);lh=s(a1,"Flag TorchScript e pesi legati"),a1.forEach(o),Yp.forEach(o),qr=d(e),ji=a(e,"P",{});var n1=n(ji);sh=s(n1,`Questo flag \xE8 necessario perch\xE9 la maggior parte dei modelli linguistici in questo repository hanno pesi legati tra il loro
strato \u201CEmbedding\u201D e lo strato \u201CDecoding\u201D. TorchScript non consente l\u2019esportazione di modelli che hanno pesi
legati, quindi \xE8 necessario prima slegare e clonare i pesi.`),n1.forEach(o),Pr=d(e),ee=a(e,"P",{});var At=n(ee);rh=s(At,"Ci\xF2 implica che i modelli istanziati con il flag "),Nl=a(At,"CODE",{});var l1=n(Nl);ph=s(l1,"torchscript"),l1.forEach(o),ch=s(At," hanno il loro strato "),Ol=a(At,"CODE",{});var s1=n(Ol);dh=s(s1,"Embedding"),s1.forEach(o),uh=s(At," e strato "),Tl=a(At,"CODE",{});var r1=n(Tl);fh=s(r1,"Decoding"),r1.forEach(o),mh=s(At,`
separato, il che significa che non dovrebbero essere addestrati in futuro. L\u2019allenamento de-sincronizza i due
strati, portando a risultati inaspettati.`),At.forEach(o),Cr=d(e),vt=a(e,"P",{});var Zp=n(vt);hh=s(Zp,`Questo non \xE8 il caso per i modelli che non hanno una testa del modello linguistico, poich\xE9 quelli non hanno pesi legati. Questi modelli
pu\xF2 essere esportato in sicurezza senza il flag `),jl=a(Zp,"CODE",{});var p1=n(jl);vh=s(p1,"torchscript"),p1.forEach(o),gh=s(Zp,"."),Zp.forEach(o),Dr=d(e),je=a(e,"H3",{class:!0});var ec=n(je);gt=a(ec,"A",{id:!0,class:!0,href:!0});var c1=n(gt);Al=a(c1,"SPAN",{});var d1=n(Al);z(xo.$$.fragment,d1),d1.forEach(o),c1.forEach(o),_h=d(ec),yl=a(ec,"SPAN",{});var u1=n(yl);zh=s(u1,"Input fittizi e standard lengths"),u1.forEach(o),ec.forEach(o),Sr=d(e),Ai=a(e,"P",{});var f1=n(Ai);Eh=s(f1,`Gli input fittizzi sono usati per fare un modello passaggio in avanti . Mentre i valori degli input si propagano attraverso i strati,
Pytorch tiene traccia delle diverse operazioni eseguite su ciascun tensore. Queste operazioni registrate vengono quindi utilizzate per
creare la \u201Ctraccia\u201D del modello.`),f1.forEach(o),Ir=d(e),yi=a(e,"P",{});var m1=n(yi);$h=s(m1,`La traccia viene creata relativamente alle dimensioni degli input. \xC8 quindi vincolato dalle dimensioni dell\u2019input
fittizio e non funzioner\xE0 per altre lunghezze di sequenza o dimensioni batch. Quando si prover\xE0 con una dimensione diversa, ci sar\xE0 errore
come:`),m1.forEach(o),Lr=d(e),qi=a(e,"P",{});var h1=n(qi);ql=a(h1,"CODE",{});var v1=n(ql);bh=s(v1,"La dimensione espansa del tensore (3) deve corrispondere alla dimensione esistente (7) nella dimensione non singleton 2"),v1.forEach(o),h1.forEach(o),Br=d(e),Pi=a(e,"P",{});var g1=n(Pi);wh=s(g1,`will be raised. Si consiglia pertanto di tracciare il modello con una dimensione di input fittizia grande almeno quanto il pi\xF9 grande
input che verr\xE0 fornito al modello durante l\u2019inferenza. \xC8 possibile eseguire il padding per riempire i valori mancanti. Il modello
sar\xE0 tracciato con una grande dimensione di input, tuttavia, anche le dimensioni della diverse matrici saranno grandi,
risultando in pi\xF9 calcoli.`),g1.forEach(o),Mr=d(e),Ci=a(e,"P",{});var _1=n(Ci);kh=s(_1,`Si raccomanda di prestare attenzione al numero totale di operazioni eseguite su ciascun input e di seguire da vicino le prestazioni
durante l\u2019esportazione di modelli di sequenza-lunghezza variabili.`),_1.forEach(o),Xr=d(e),Ae=a(e,"H3",{class:!0});var tc=n(Ae);_t=a(tc,"A",{id:!0,class:!0,href:!0});var z1=n(_t);Pl=a(z1,"SPAN",{});var E1=n(Pl);z(No.$$.fragment,E1),E1.forEach(o),z1.forEach(o),xh=d(tc),Cl=a(tc,"SPAN",{});var $1=n(Cl);Nh=s($1,"Usare TorchSscript in Python"),$1.forEach(o),tc.forEach(o),Rr=d(e),Di=a(e,"P",{});var b1=n(Di);Oh=s(b1,"Di seguito \xE8 riportato un esempio, che mostra come salvare, caricare modelli e come utilizzare la traccia per l\u2019inferenza."),b1.forEach(o),Fr=d(e),ye=a(e,"H4",{class:!0});var oc=n(ye);zt=a(oc,"A",{id:!0,class:!0,href:!0});var w1=n(zt);Dl=a(w1,"SPAN",{});var k1=n(Dl);z(Oo.$$.fragment,k1),k1.forEach(o),w1.forEach(o),Th=d(oc),Sl=a(oc,"SPAN",{});var x1=n(Sl);jh=s(x1,"Salvare un modello"),x1.forEach(o),oc.forEach(o),Qr=d(e),V=a(e,"P",{});var Be=n(V);Ah=s(Be,"Questo frammento di codice mostra come usare TorchScript per esportare un "),Il=a(Be,"CODE",{});var N1=n(Il);yh=s(N1,"BertModel"),N1.forEach(o),qh=s(Be,". Qui il "),Ll=a(Be,"CODE",{});var O1=n(Ll);Ph=s(O1,"BertModel"),O1.forEach(o),Ch=s(Be,` \xE8 istanziato secondo
una classe `),Bl=a(Be,"CODE",{});var T1=n(Bl);Dh=s(T1,"BertConfig"),T1.forEach(o),Sh=s(Be," e quindi salvato su disco con il nome del file "),Ml=a(Be,"CODE",{});var j1=n(Ml);Ih=s(j1,"traced_bert.pt"),j1.forEach(o),Be.forEach(o),Hr=d(e),z(To.$$.fragment,e),Ur=d(e),qe=a(e,"H4",{class:!0});var ic=n(qe);Et=a(ic,"A",{id:!0,class:!0,href:!0});var A1=n(Et);Xl=a(A1,"SPAN",{});var y1=n(Xl);z(jo.$$.fragment,y1),y1.forEach(o),A1.forEach(o),Lh=d(ic),Rl=a(ic,"SPAN",{});var q1=n(Rl);Bh=s(q1,"Caricare un modello"),q1.forEach(o),ic.forEach(o),Wr=d(e),te=a(e,"P",{});var yt=n(te);Mh=s(yt,"Questo frammento di codice mostra come caricare il "),Fl=a(yt,"CODE",{});var P1=n(Fl);Xh=s(P1,"BertModel"),P1.forEach(o),Rh=s(yt," che era stato precedentemente salvato su disco con il nome "),Ql=a(yt,"CODE",{});var C1=n(Ql);Fh=s(C1,"traced_bert.pt"),C1.forEach(o),Qh=s(yt,`.
Stiamo riutilizzando il `),Hl=a(yt,"CODE",{});var D1=n(Hl);Hh=s(D1,"dummy_input"),D1.forEach(o),Uh=s(yt," precedentemente inizializzato."),yt.forEach(o),Kr=d(e),z(Ao.$$.fragment,e),Gr=d(e),Pe=a(e,"H4",{class:!0});var ac=n(Pe);$t=a(ac,"A",{id:!0,class:!0,href:!0});var S1=n($t);Ul=a(S1,"SPAN",{});var I1=n(Ul);z(yo.$$.fragment,I1),I1.forEach(o),S1.forEach(o),Wh=d(ac),Wl=a(ac,"SPAN",{});var L1=n(Wl);Kh=s(L1,"Utilizzare un modello tracciato per l'inferenza"),L1.forEach(o),ac.forEach(o),Vr=d(e),bt=a(e,"P",{});var nc=n(bt);Gh=s(nc,"Usare il modello tracciato per l\u2019inferenza \xE8 semplice come usare il suo metodo dunder "),Kl=a(nc,"CODE",{});var B1=n(Kl);Vh=s(B1,"__call__"),B1.forEach(o),Jh=s(nc,":"),nc.forEach(o),Jr=d(e),z(qo.$$.fragment,e),Yr=d(e),Si=a(e,"P",{});var M1=n(Si);Yh=s(M1,"###Implementare modelli HuggingFace TorchScript su AWS utilizzando Neuron SDK"),M1.forEach(o),Zr=d(e),fe=a(e,"P",{});var Yi=n(fe);Zh=s(Yi,"AWS ha introdotto "),Po=a(Yi,"A",{href:!0,rel:!0});var X1=n(Po);ev=s(X1,"Amazon EC2 Inf1"),X1.forEach(o),tv=s(Yi,`
famiglia di istanze per l\u2019inferenza di machine learning a basso costo e ad alte prestazioni nel cloud.
Le istanze Inf1 sono alimentate dal chip AWS Inferentia, un acceleratore hardware personalizzato,
specializzato in carichi di lavoro di inferenza di deep learning.
`),Co=a(Yi,"A",{href:!0,rel:!0});var R1=n(Co);ov=s(R1,"AWS Neuron"),R1.forEach(o),iv=s(Yi,`
\xE8 l\u2019SDK per Inferentia che supporta il tracciamento e l\u2019ottimizzazione dei modelli transformers per
distribuzione su Inf1. L\u2019SDK Neuron fornisce:`),Yi.forEach(o),ep=d(e),me=a(e,"OL",{});var Zi=n(me);Gl=a(Zi,"LI",{});var F1=n(Gl);av=s(F1,"API di facile utilizzo con una riga di modifica del codice per tracciare e ottimizzare un modello TorchScript per l\u2019inferenza nel cloud."),F1.forEach(o),nv=d(Zi),Ii=a(Zi,"LI",{});var tg=n(Ii);lv=s(tg,"Ottimizzazioni delle prestazioni pronte all\u2019uso per "),Do=a(tg,"A",{href:!0,rel:!0});var Q1=n(Do);sv=s(Q1,"miglioramento dei costi-prestazioni"),Q1.forEach(o),tg.forEach(o),rv=d(Zi),Ce=a(Zi,"LI",{});var ea=n(Ce);pv=s(ea,"Supporto per i modelli di trasformatori HuggingFace costruiti con "),So=a(ea,"A",{href:!0,rel:!0});var H1=n(So);cv=s(H1,"PyTorch"),H1.forEach(o),dv=s(ea,`
o `),Io=a(ea,"A",{href:!0,rel:!0});var U1=n(Io);uv=s(U1,"TensorFlow"),U1.forEach(o),fv=s(ea,"."),ea.forEach(o),Zi.forEach(o),tp=d(e),De=a(e,"H4",{class:!0});var lc=n(De);wt=a(lc,"A",{id:!0,class:!0,href:!0});var W1=n(wt);Vl=a(W1,"SPAN",{});var K1=n(Vl);z(Lo.$$.fragment,K1),K1.forEach(o),W1.forEach(o),mv=d(lc),Jl=a(lc,"SPAN",{});var G1=n(Jl);hv=s(G1,"Implicazioni"),G1.forEach(o),lc.forEach(o),op=d(e),F=a(e,"P",{});var ie=n(F);vv=s(ie,"Modelli Transformers basati su architettura "),Bo=a(ie,"A",{href:!0,rel:!0});var V1=n(Bo);gv=s(V1,"BERT (Bidirectional Encoder Representations from Transformers)"),V1.forEach(o),_v=s(ie,`,
o sue varianti come `),Mo=a(ie,"A",{href:!0,rel:!0});var J1=n(Mo);zv=s(J1,"distilBERT"),J1.forEach(o),Ev=s(ie,`
e `),Xo=a(ie,"A",{href:!0,rel:!0});var Y1=n(Xo);$v=s(Y1,"roBERTa"),Y1.forEach(o),bv=s(ie,`
funzioneranno meglio su Inf1 per attivit\xE0 non generative come la question answering estrattive,
Classificazione della sequenza, Classificazione dei token. In alternativa, generazione di testo
le attivit\xE0 possono essere adattate per essere eseguite su Inf1, secondo questo `),Ro=a(ie,"A",{href:!0,rel:!0});var Z1=n(Ro);wv=s(Z1,"tutorial AWS Neuron MarianMT"),Z1.forEach(o),kv=s(ie,`.
Ulteriori informazioni sui modelli che possono essere convertiti fuori dagli schemi su Inferentia possono essere
trovati nella `),Fo=a(ie,"A",{href:!0,rel:!0});var eE=n(Fo);xv=s(eE,"sezione Model Architecture Fit della documentazione Neuron"),eE.forEach(o),Nv=s(ie,"."),ie.forEach(o),ip=d(e),Se=a(e,"H4",{class:!0});var sc=n(Se);kt=a(sc,"A",{id:!0,class:!0,href:!0});var tE=n(kt);Yl=a(tE,"SPAN",{});var oE=n(Yl);z(Qo.$$.fragment,oE),oE.forEach(o),tE.forEach(o),Ov=d(sc),Zl=a(sc,"SPAN",{});var iE=n(Zl);Tv=s(iE,"Dipendenze"),iE.forEach(o),sc.forEach(o),ap=d(e),Li=a(e,"P",{});var aE=n(Li);jv=s(aE,"L\u2019utilizzo di AWS Neuron per convertire i modelli richiede le seguenti dipendenze e l\u2019ambiente:"),aE.forEach(o),np=d(e),Bi=a(e,"UL",{});var nE=n(Bi);Ie=a(nE,"LI",{});var ta=n(Ie);Av=s(ta,"A "),Ho=a(ta,"A",{href:!0,rel:!0});var lE=n(Ho);yv=s(lE,"Neuron SDK environment"),lE.forEach(o),qv=s(ta,`,
which comes pre-configured on `),Uo=a(ta,"A",{href:!0,rel:!0});var sE=n(Uo);Pv=s(sE,"AWS Deep Learning AMI"),sE.forEach(o),Cv=s(ta,"."),ta.forEach(o),nE.forEach(o),lp=d(e),Le=a(e,"H4",{class:!0});var rc=n(Le);xt=a(rc,"A",{id:!0,class:!0,href:!0});var rE=n(xt);es=a(rE,"SPAN",{});var pE=n(es);z(Wo.$$.fragment,pE),pE.forEach(o),rE.forEach(o),Dv=d(rc),ts=a(rc,"SPAN",{});var cE=n(ts);Sv=s(cE,"Convertire un modello per AWS Neuron"),cE.forEach(o),rc.forEach(o),sp=d(e),he=a(e,"P",{});var oa=n(he);Iv=s(oa,"Usando lo stesso script come in "),Ko=a(oa,"A",{href:!0,rel:!0});var dE=n(Ko);Lv=s(dE,"Usando TorchScipt in Python"),dE.forEach(o),Bv=s(oa,`
per tracciare un \u201CBertModel\u201D, importi l\u2019estensione del framework `),os=a(oa,"CODE",{});var uE=n(os);Mv=s(uE,"torch.neuron"),uE.forEach(o),Xv=s(oa,` per accedere
i componenti di Neuron SDK tramite un\u2019API Python.`),oa.forEach(o),rp=d(e),z(Go.$$.fragment,e),pp=d(e),Mi=a(e,"P",{});var fE=n(Mi);Rv=s(fE,"E modificare solo la riga di codice di traccia"),fE.forEach(o),cp=d(e),Xi=a(e,"P",{});var mE=n(Xi);Fv=s(mE,"Da:"),mE.forEach(o),dp=d(e),z(Vo.$$.fragment,e),up=d(e),Ri=a(e,"P",{});var hE=n(Ri);Qv=s(hE,"A:"),hE.forEach(o),fp=d(e),z(Jo.$$.fragment,e),mp=d(e),Fi=a(e,"P",{});var vE=n(Fi);Hv=s(vE,"Questa modifica consente a Neuron SDK di tracciare il modello e ottimizzarlo per l\u2019esecuzione nelle istanze Inf1."),vE.forEach(o),hp=d(e),Nt=a(e,"P",{});var pc=n(Nt);Uv=s(pc,`Per ulteriori informazioni sulle funzionalit\xE0, gli strumenti, i tutorial di esempi e gli ultimi aggiornamenti di AWS Neuron SDK,
consultare la `),Yo=a(pc,"A",{href:!0,rel:!0});var gE=n(Yo);Wv=s(gE,"documentazione AWS NeuronSDK"),gE.forEach(o),Kv=s(pc,"."),pc.forEach(o),this.h()},h(){u(f,"name","hf:doc:metadata"),u(f,"content",JSON.stringify(IE)),u(k,"id","esporta-modelli-transformers"),u(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(k,"href","#esporta-modelli-transformers"),u(m,"class","relative group"),u(B,"href","https://github.com/huggingface/optimum"),u(B,"rel","nofollow"),u(Q,"id","onnx"),u(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Q,"href","#onnx"),u(K,"class","relative group"),u(ne,"href","http://onnx.ai"),u(ne,"rel","nofollow"),u(Fe,"id","esportazione-di-un-modello-in-onnx"),u(Fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Fe,"href","#esportazione-di-un-modello-in-onnx"),u(Ee,"class","relative group"),u(Bt,"href","https://onnx.ai/supported-tools.html#deployModel"),u(Bt,"rel","nofollow"),u(Mt,"href","https://onnxruntime.ai/"),u(Mt,"rel","nofollow"),u(Ft,"href","https://huggingface.co/keras-io"),u(Ft,"rel","nofollow"),u(Ke,"id","selezione-delle-caratteristiche-per-diverse-topologie-di-modello"),u(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Ke,"href","#selezione-delle-caratteristiche-per-diverse-topologie-di-modello"),u($e,"class","relative group"),u(Ze,"id","esportazione-di-un-modello-per-unarchitettura-non-supportata"),u(Ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Ze,"href","#esportazione-di-un-modello-per-unarchitettura-non-supportata"),u(be,"class","relative group"),u(et,"id","implementazione-di-una-configurazione-onnx-personalizzata"),u(et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(et,"href","#implementazione-di-una-configurazione-onnx-personalizzata"),u(we,"class","relative group"),u(nt,"id","esportazione-del-modello"),u(nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(nt,"href","#esportazione-del-modello"),u(ke,"class","relative group"),u(st,"id","convalida-degli-output-del-modello"),u(st,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(st,"href","#convalida-degli-output-del-modello"),u(xe,"class","relative group"),u(pt,"id","contribuire-con-una-nuova-configurazione-a-transformers"),u(pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(pt,"href","#contribuire-con-una-nuova-configurazione-a-transformers"),u(Ne,"class","relative group"),u(Eo,"href","https://github.com/huggingface/transformers/pull/14868/files"),u(Eo,"rel","nofollow"),u(dt,"id","torchscript"),u(dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(dt,"href","#torchscript"),u(Oe,"class","relative group"),u(bo,"href","https://pytorch.org/docs/stable/jit.html"),u(bo,"rel","nofollow"),u(ht,"id","flag-torchscript-e-pesi-legati"),u(ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ht,"href","#flag-torchscript-e-pesi-legati"),u(Te,"class","relative group"),u(gt,"id","input-fittizi-e-standard-lengths"),u(gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(gt,"href","#input-fittizi-e-standard-lengths"),u(je,"class","relative group"),u(_t,"id","usare-torchsscript-in-python"),u(_t,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(_t,"href","#usare-torchsscript-in-python"),u(Ae,"class","relative group"),u(zt,"id","salvare-un-modello"),u(zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(zt,"href","#salvare-un-modello"),u(ye,"class","relative group"),u(Et,"id","caricare-un-modello"),u(Et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Et,"href","#caricare-un-modello"),u(qe,"class","relative group"),u($t,"id","utilizzare-un-modello-tracciato-per-linferenza"),u($t,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u($t,"href","#utilizzare-un-modello-tracciato-per-linferenza"),u(Pe,"class","relative group"),u(Po,"href","https://aws.amazon.com/ec2/instance-types/inf1/"),u(Po,"rel","nofollow"),u(Co,"href","https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#"),u(Co,"rel","nofollow"),u(Do,"href","https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/%3E"),u(Do,"rel","nofollow"),u(So,"href","https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html"),u(So,"rel","nofollow"),u(Io,"href","https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html"),u(Io,"rel","nofollow"),u(wt,"id","implicazioni"),u(wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(wt,"href","#implicazioni"),u(De,"class","relative group"),u(Bo,"href","https://huggingface.co/docs/transformers/main/model_doc/bert"),u(Bo,"rel","nofollow"),u(Mo,"href","https://huggingface.co/docs/transformers/main/model_doc/distilbert"),u(Mo,"rel","nofollow"),u(Xo,"href","https://huggingface.co/docs/transformers/main/model_doc/roberta"),u(Xo,"rel","nofollow"),u(Ro,"href","https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html"),u(Ro,"rel","nofollow"),u(Fo,"href","https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia"),u(Fo,"rel","nofollow"),u(kt,"id","dipendenze"),u(kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(kt,"href","#dipendenze"),u(Se,"class","relative group"),u(Ho,"href","https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide"),u(Ho,"rel","nofollow"),u(Uo,"href","https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html"),u(Uo,"rel","nofollow"),u(xt,"id","convertire-un-modello-per-aws-neuron"),u(xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(xt,"href","#convertire-un-modello-per-aws-neuron"),u(Le,"class","relative group"),u(Ko,"href","https://huggingface.co/docs/transformers/main/en/serialization#using-torchscript-in-python"),u(Ko,"rel","nofollow"),u(Yo,"href","https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html"),u(Yo,"rel","nofollow")},m(e,r){t(document.head,f),p(e,x,r),p(e,m,r),t(m,k),t(k,O),E(N,O,null),t(m,P),t(m,q),t(q,A),p(e,T,r),p(e,C,r),t(C,y),p(e,D,r),p(e,g,r),t(g,j),t(g,B),t(B,W),t(g,ge),p(e,Me,r),p(e,K,r),t(K,Q),t(Q,_e),E(ae,_e,null),t(K,ze),t(K,I),t(I,ti),p(e,qt,r),p(e,G,r),t(G,oi),t(G,ne),t(ne,ii),t(G,ai),t(G,aa),t(aa,cc),t(G,dc),p(e,rs,r),p(e,ni,r),t(ni,uc),p(e,ps,r),p(e,Xe,r),t(Xe,fc),t(Xe,na),t(na,mc),t(Xe,hc),p(e,cs,r),p(e,li,r),t(li,vc),p(e,ds,r),p(e,h,r),t(h,la),t(la,gc),t(h,_c),t(h,sa),t(sa,zc),t(h,Ec),t(h,ra),t(ra,$c),t(h,bc),t(h,pa),t(pa,wc),t(h,kc),t(h,ca),t(ca,xc),t(h,Nc),t(h,da),t(da,Oc),t(h,Tc),t(h,ua),t(ua,jc),t(h,Ac),t(h,fa),t(fa,yc),t(h,qc),t(h,ma),t(ma,Pc),t(h,Cc),t(h,ha),t(ha,Dc),t(h,Sc),t(h,va),t(va,Ic),t(h,Lc),t(h,ga),t(ga,Bc),t(h,Mc),t(h,_a),t(_a,Xc),t(h,Rc),t(h,za),t(za,Fc),t(h,Qc),t(h,Ea),t(Ea,Hc),t(h,Uc),t(h,$a),t($a,Wc),t(h,Kc),t(h,ba),t(ba,Gc),t(h,Vc),t(h,wa),t(wa,Jc),t(h,Yc),t(h,ka),t(ka,Zc),t(h,ed),t(h,xa),t(xa,td),t(h,od),t(h,Na),t(Na,id),t(h,ad),t(h,Oa),t(Oa,nd),t(h,ld),t(h,Ta),t(Ta,sd),t(h,rd),t(h,ja),t(ja,pd),t(h,cd),t(h,Aa),t(Aa,dd),t(h,ud),t(h,ya),t(ya,fd),t(h,md),t(h,qa),t(qa,hd),t(h,vd),t(h,Pa),t(Pa,gd),t(h,_d),t(h,Ca),t(Ca,zd),t(h,Ed),t(h,Da),t(Da,$d),t(h,bd),t(h,Sa),t(Sa,wd),t(h,kd),t(h,Ia),t(Ia,xd),t(h,Nd),t(h,La),t(La,Od),t(h,Td),t(h,Ba),t(Ba,jd),t(h,Ad),t(h,Ma),t(Ma,yd),p(e,us,r),p(e,si,r),t(si,qd),p(e,fs,r),p(e,Re,r),t(Re,Pt),t(Pt,Pd),t(Pt,Xa),t(Xa,Cd),t(Pt,Dd),t(Re,Sd),t(Re,Ra),t(Ra,Id),p(e,ms,r),p(e,Ee,r),t(Ee,Fe),t(Fe,Fa),E(Ct,Fa,null),t(Ee,Ld),t(Ee,Qa),t(Qa,Bd),p(e,hs,r),p(e,ri,r),t(ri,Md),p(e,vs,r),E(Dt,e,r),p(e,gs,r),p(e,Qe,r),t(Qe,Xd),t(Qe,Ha),t(Ha,Rd),t(Qe,Fd),p(e,_s,r),E(St,e,r),p(e,zs,r),p(e,pi,r),t(pi,Qd),p(e,Es,r),E(It,e,r),p(e,$s,r),p(e,ci,r),t(ci,Hd),p(e,bs,r),E(Lt,e,r),p(e,ws,r),p(e,le,r),t(le,Ud),t(le,Ua),t(Ua,Wd),t(le,Kd),t(le,Wa),t(Wa,Gd),t(le,Vd),p(e,ks,r),p(e,J,r),t(J,Jd),t(J,Ka),t(Ka,Yd),t(J,Zd),t(J,Bt),t(Bt,eu),t(J,tu),t(J,Mt),t(Mt,ou),t(J,iu),p(e,xs,r),E(Xt,e,r),p(e,Ns,r),p(e,He,r),t(He,au),t(He,Ga),t(Ga,nu),t(He,lu),p(e,Os,r),E(Rt,e,r),p(e,Ts,r),p(e,Ue,r),t(Ue,su),t(Ue,Ft),t(Ft,ru),t(Ue,pu),p(e,js,r),E(Qt,e,r),p(e,As,r),p(e,di,r),t(di,cu),p(e,ys,r),E(We,e,r),p(e,qs,r),p(e,$e,r),t($e,Ke),t(Ke,Va),E(Ht,Va,null),t($e,du),t($e,Ja),t(Ja,uu),p(e,Ps,r),p(e,Ge,r),t(Ge,fu),t(Ge,Ya),t(Ya,mu),t(Ge,hu),p(e,Cs,r),p(e,Ve,r),t(Ve,Za),t(Za,Ut),t(Ut,en),t(en,vu),t(Ut,gu),t(Ut,tn),t(tn,_u),t(Ve,zu),t(Ve,X),t(X,Wt),t(Wt,Kt),t(Kt,on),t(on,Eu),t(Kt,$u),t(Kt,an),t(an,bu),t(Wt,wu),t(Wt,nn),t(nn,ln),t(ln,ku),t(X,xu),t(X,Gt),t(Gt,Vt),t(Vt,sn),t(sn,Nu),t(Vt,Ou),t(Vt,rn),t(rn,Tu),t(Gt,ju),t(Gt,pn),t(pn,cn),t(cn,Au),t(X,yu),t(X,Jt),t(Jt,dn),t(dn,un),t(un,qu),t(Jt,Pu),t(Jt,fn),t(fn,mn),t(mn,Cu),t(X,Du),t(X,Yt),t(Yt,hn),t(hn,vn),t(vn,Su),t(Yt,Iu),t(Yt,gn),t(gn,_n),t(_n,Lu),t(X,Bu),t(X,Zt),t(Zt,eo),t(eo,zn),t(zn,Mu),t(eo,Xu),t(eo,En),t(En,Ru),t(Zt,Fu),t(Zt,$n),t($n,bn),t(bn,Qu),t(X,Hu),t(X,to),t(to,wn),t(wn,kn),t(kn,Uu),t(to,Wu),t(to,xn),t(xn,Nn),t(Nn,Ku),t(X,Gu),t(X,oo),t(oo,On),t(On,Tn),t(Tn,Vu),t(oo,Ju),t(oo,jn),t(jn,An),t(An,Yu),p(e,Ds,r),p(e,Je,r),t(Je,Zu),t(Je,yn),t(yn,ef),t(Je,tf),p(e,Ss,r),E(io,e,r),p(e,Is,r),p(e,se,r),t(se,of),t(se,qn),t(qn,af),t(se,nf),t(se,Pn),t(Pn,lf),t(se,sf),p(e,Ls,r),E(ao,e,r),p(e,Bs,r),p(e,ui,r),t(ui,rf),p(e,Ms,r),E(no,e,r),p(e,Xs,r),p(e,Y,r),t(Y,pf),t(Y,Cn),t(Cn,cf),t(Y,df),t(Y,Dn),t(Dn,uf),t(Y,ff),t(Y,Sn),t(Sn,mf),t(Y,hf),p(e,Rs,r),E(Ye,e,r),p(e,Fs,r),p(e,be,r),t(be,Ze),t(Ze,In),E(lo,In,null),t(be,vf),t(be,Ln),t(Ln,gf),p(e,Qs,r),p(e,fi,r),t(fi,_f),p(e,Hs,r),p(e,re,r),t(re,Bn),t(Bn,zf),t(re,Ef),t(re,Mn),t(Mn,$f),t(re,bf),t(re,Xn),t(Xn,wf),p(e,Us,r),p(e,mi,r),t(mi,kf),p(e,Ws,r),p(e,we,r),t(we,et),t(et,Rn),E(so,Rn,null),t(we,xf),t(we,Fn),t(Fn,Nf),p(e,Ks,r),p(e,hi,r),t(hi,Of),p(e,Gs,r),p(e,pe,r),t(pe,vi),t(vi,Tf),t(vi,Qn),t(Qn,jf),t(pe,Af),t(pe,gi),t(gi,yf),t(gi,Hn),t(Hn,qf),t(pe,Pf),t(pe,_i),t(_i,Cf),t(_i,Un),t(Un,Df),p(e,Vs,r),E(tt,e,r),p(e,Js,r),p(e,ot,r),t(ot,Sf),t(ot,Wn),t(Wn,If),t(ot,Lf),p(e,Ys,r),E(ro,e,r),p(e,Zs,r),p(e,H,r),t(H,Bf),t(H,Kn),t(Kn,Mf),t(H,Xf),t(H,Gn),t(Gn,Rf),t(H,Ff),t(H,Vn),t(Vn,Qf),t(H,Hf),t(H,Jn),t(Jn,Uf),t(H,Wf),p(e,er,r),E(it,e,r),p(e,tr,r),p(e,zi,r),t(zi,Kf),p(e,or,r),E(po,e,r),p(e,ir,r),p(e,Ei,r),t(Ei,Gf),p(e,ar,r),E(co,e,r),p(e,nr,r),p(e,$i,r),t($i,Vf),p(e,lr,r),E(uo,e,r),p(e,sr,r),p(e,Z,r),t(Z,Jf),t(Z,Yn),t(Yn,Yf),t(Z,Zf),t(Z,Zn),t(Zn,em),t(Z,tm),t(Z,el),t(el,om),t(Z,im),p(e,rr,r),E(fo,e,r),p(e,pr,r),E(at,e,r),p(e,cr,r),p(e,ke,r),t(ke,nt),t(nt,tl),E(mo,tl,null),t(ke,am),t(ke,ol),t(ol,nm),p(e,dr,r),p(e,ce,r),t(ce,lm),t(ce,il),t(il,sm),t(ce,rm),t(ce,al),t(al,pm),t(ce,cm),p(e,ur,r),E(ho,e,r),p(e,fr,r),p(e,R,r),t(R,dm),t(R,nl),t(nl,um),t(R,fm),t(R,ll),t(ll,mm),t(R,hm),t(R,sl),t(sl,vm),t(R,gm),t(R,rl),t(rl,_m),t(R,zm),t(R,pl),t(pl,Em),t(R,$m),p(e,mr,r),E(vo,e,r),p(e,hr,r),E(lt,e,r),p(e,vr,r),p(e,xe,r),t(xe,st),t(st,cl),E(go,cl,null),t(xe,bm),t(xe,dl),t(dl,wm),p(e,gr,r),p(e,de,r),t(de,km),t(de,ul),t(ul,xm),t(de,Nm),t(de,fl),t(fl,Om),t(de,Tm),p(e,_r,r),E(_o,e,r),p(e,zr,r),p(e,rt,r),t(rt,jm),t(rt,ml),t(ml,Am),t(rt,ym),p(e,Er,r),p(e,Ne,r),t(Ne,pt),t(pt,hl),E(zo,hl,null),t(Ne,qm),t(Ne,vl),t(vl,Pm),p(e,$r,r),p(e,bi,r),t(bi,Cm),p(e,br,r),p(e,ue,r),t(ue,wi),t(wi,Dm),t(wi,gl),t(gl,Sm),t(ue,Im),t(ue,ki),t(ki,Lm),t(ki,_l),t(_l,Bm),t(ue,Mm),t(ue,xi),t(xi,Xm),t(xi,zl),t(zl,Rm),p(e,wr,r),p(e,ct,r),t(ct,Fm),t(ct,Eo),t(Eo,Qm),t(ct,Hm),p(e,kr,r),p(e,Oe,r),t(Oe,dt),t(dt,El),E($o,El,null),t(Oe,Um),t(Oe,$l),t($l,Wm),p(e,xr,r),E(ut,e,r),p(e,Nr,r),p(e,ft,r),t(ft,Km),t(ft,bo),t(bo,Gm),t(ft,Vm),p(e,Or,r),p(e,Ni,r),t(Ni,Jm),p(e,Tr,r),p(e,Oi,r),t(Oi,Ym),p(e,jr,r),p(e,mt,r),t(mt,bl),t(bl,Zm),t(mt,eh),t(mt,wo),t(wo,th),t(wo,wl),t(wl,oh),t(wo,ih),p(e,Ar,r),p(e,Ti,r),t(Ti,ah),p(e,yr,r),p(e,Te,r),t(Te,ht),t(ht,kl),E(ko,kl,null),t(Te,nh),t(Te,xl),t(xl,lh),p(e,qr,r),p(e,ji,r),t(ji,sh),p(e,Pr,r),p(e,ee,r),t(ee,rh),t(ee,Nl),t(Nl,ph),t(ee,ch),t(ee,Ol),t(Ol,dh),t(ee,uh),t(ee,Tl),t(Tl,fh),t(ee,mh),p(e,Cr,r),p(e,vt,r),t(vt,hh),t(vt,jl),t(jl,vh),t(vt,gh),p(e,Dr,r),p(e,je,r),t(je,gt),t(gt,Al),E(xo,Al,null),t(je,_h),t(je,yl),t(yl,zh),p(e,Sr,r),p(e,Ai,r),t(Ai,Eh),p(e,Ir,r),p(e,yi,r),t(yi,$h),p(e,Lr,r),p(e,qi,r),t(qi,ql),t(ql,bh),p(e,Br,r),p(e,Pi,r),t(Pi,wh),p(e,Mr,r),p(e,Ci,r),t(Ci,kh),p(e,Xr,r),p(e,Ae,r),t(Ae,_t),t(_t,Pl),E(No,Pl,null),t(Ae,xh),t(Ae,Cl),t(Cl,Nh),p(e,Rr,r),p(e,Di,r),t(Di,Oh),p(e,Fr,r),p(e,ye,r),t(ye,zt),t(zt,Dl),E(Oo,Dl,null),t(ye,Th),t(ye,Sl),t(Sl,jh),p(e,Qr,r),p(e,V,r),t(V,Ah),t(V,Il),t(Il,yh),t(V,qh),t(V,Ll),t(Ll,Ph),t(V,Ch),t(V,Bl),t(Bl,Dh),t(V,Sh),t(V,Ml),t(Ml,Ih),p(e,Hr,r),E(To,e,r),p(e,Ur,r),p(e,qe,r),t(qe,Et),t(Et,Xl),E(jo,Xl,null),t(qe,Lh),t(qe,Rl),t(Rl,Bh),p(e,Wr,r),p(e,te,r),t(te,Mh),t(te,Fl),t(Fl,Xh),t(te,Rh),t(te,Ql),t(Ql,Fh),t(te,Qh),t(te,Hl),t(Hl,Hh),t(te,Uh),p(e,Kr,r),E(Ao,e,r),p(e,Gr,r),p(e,Pe,r),t(Pe,$t),t($t,Ul),E(yo,Ul,null),t(Pe,Wh),t(Pe,Wl),t(Wl,Kh),p(e,Vr,r),p(e,bt,r),t(bt,Gh),t(bt,Kl),t(Kl,Vh),t(bt,Jh),p(e,Jr,r),E(qo,e,r),p(e,Yr,r),p(e,Si,r),t(Si,Yh),p(e,Zr,r),p(e,fe,r),t(fe,Zh),t(fe,Po),t(Po,ev),t(fe,tv),t(fe,Co),t(Co,ov),t(fe,iv),p(e,ep,r),p(e,me,r),t(me,Gl),t(Gl,av),t(me,nv),t(me,Ii),t(Ii,lv),t(Ii,Do),t(Do,sv),t(me,rv),t(me,Ce),t(Ce,pv),t(Ce,So),t(So,cv),t(Ce,dv),t(Ce,Io),t(Io,uv),t(Ce,fv),p(e,tp,r),p(e,De,r),t(De,wt),t(wt,Vl),E(Lo,Vl,null),t(De,mv),t(De,Jl),t(Jl,hv),p(e,op,r),p(e,F,r),t(F,vv),t(F,Bo),t(Bo,gv),t(F,_v),t(F,Mo),t(Mo,zv),t(F,Ev),t(F,Xo),t(Xo,$v),t(F,bv),t(F,Ro),t(Ro,wv),t(F,kv),t(F,Fo),t(Fo,xv),t(F,Nv),p(e,ip,r),p(e,Se,r),t(Se,kt),t(kt,Yl),E(Qo,Yl,null),t(Se,Ov),t(Se,Zl),t(Zl,Tv),p(e,ap,r),p(e,Li,r),t(Li,jv),p(e,np,r),p(e,Bi,r),t(Bi,Ie),t(Ie,Av),t(Ie,Ho),t(Ho,yv),t(Ie,qv),t(Ie,Uo),t(Uo,Pv),t(Ie,Cv),p(e,lp,r),p(e,Le,r),t(Le,xt),t(xt,es),E(Wo,es,null),t(Le,Dv),t(Le,ts),t(ts,Sv),p(e,sp,r),p(e,he,r),t(he,Iv),t(he,Ko),t(Ko,Lv),t(he,Bv),t(he,os),t(os,Mv),t(he,Xv),p(e,rp,r),E(Go,e,r),p(e,pp,r),p(e,Mi,r),t(Mi,Rv),p(e,cp,r),p(e,Xi,r),t(Xi,Fv),p(e,dp,r),E(Vo,e,r),p(e,up,r),p(e,Ri,r),t(Ri,Qv),p(e,fp,r),E(Jo,e,r),p(e,mp,r),p(e,Fi,r),t(Fi,Hv),p(e,hp,r),p(e,Nt,r),t(Nt,Uv),t(Nt,Yo),t(Yo,Wv),t(Nt,Kv),vp=!0},p(e,[r]){const Zo={};r&2&&(Zo.$$scope={dirty:r,ctx:e}),We.$set(Zo);const is={};r&2&&(is.$$scope={dirty:r,ctx:e}),Ye.$set(is);const as={};r&2&&(as.$$scope={dirty:r,ctx:e}),tt.$set(as);const ns={};r&2&&(ns.$$scope={dirty:r,ctx:e}),it.$set(ns);const ls={};r&2&&(ls.$$scope={dirty:r,ctx:e}),at.$set(ls);const ei={};r&2&&(ei.$$scope={dirty:r,ctx:e}),lt.$set(ei);const ss={};r&2&&(ss.$$scope={dirty:r,ctx:e}),ut.$set(ss)},i(e){vp||($(N.$$.fragment,e),$(ae.$$.fragment,e),$(Ct.$$.fragment,e),$(Dt.$$.fragment,e),$(St.$$.fragment,e),$(It.$$.fragment,e),$(Lt.$$.fragment,e),$(Xt.$$.fragment,e),$(Rt.$$.fragment,e),$(Qt.$$.fragment,e),$(We.$$.fragment,e),$(Ht.$$.fragment,e),$(io.$$.fragment,e),$(ao.$$.fragment,e),$(no.$$.fragment,e),$(Ye.$$.fragment,e),$(lo.$$.fragment,e),$(so.$$.fragment,e),$(tt.$$.fragment,e),$(ro.$$.fragment,e),$(it.$$.fragment,e),$(po.$$.fragment,e),$(co.$$.fragment,e),$(uo.$$.fragment,e),$(fo.$$.fragment,e),$(at.$$.fragment,e),$(mo.$$.fragment,e),$(ho.$$.fragment,e),$(vo.$$.fragment,e),$(lt.$$.fragment,e),$(go.$$.fragment,e),$(_o.$$.fragment,e),$(zo.$$.fragment,e),$($o.$$.fragment,e),$(ut.$$.fragment,e),$(ko.$$.fragment,e),$(xo.$$.fragment,e),$(No.$$.fragment,e),$(Oo.$$.fragment,e),$(To.$$.fragment,e),$(jo.$$.fragment,e),$(Ao.$$.fragment,e),$(yo.$$.fragment,e),$(qo.$$.fragment,e),$(Lo.$$.fragment,e),$(Qo.$$.fragment,e),$(Wo.$$.fragment,e),$(Go.$$.fragment,e),$(Vo.$$.fragment,e),$(Jo.$$.fragment,e),vp=!0)},o(e){b(N.$$.fragment,e),b(ae.$$.fragment,e),b(Ct.$$.fragment,e),b(Dt.$$.fragment,e),b(St.$$.fragment,e),b(It.$$.fragment,e),b(Lt.$$.fragment,e),b(Xt.$$.fragment,e),b(Rt.$$.fragment,e),b(Qt.$$.fragment,e),b(We.$$.fragment,e),b(Ht.$$.fragment,e),b(io.$$.fragment,e),b(ao.$$.fragment,e),b(no.$$.fragment,e),b(Ye.$$.fragment,e),b(lo.$$.fragment,e),b(so.$$.fragment,e),b(tt.$$.fragment,e),b(ro.$$.fragment,e),b(it.$$.fragment,e),b(po.$$.fragment,e),b(co.$$.fragment,e),b(uo.$$.fragment,e),b(fo.$$.fragment,e),b(at.$$.fragment,e),b(mo.$$.fragment,e),b(ho.$$.fragment,e),b(vo.$$.fragment,e),b(lt.$$.fragment,e),b(go.$$.fragment,e),b(_o.$$.fragment,e),b(zo.$$.fragment,e),b($o.$$.fragment,e),b(ut.$$.fragment,e),b(ko.$$.fragment,e),b(xo.$$.fragment,e),b(No.$$.fragment,e),b(Oo.$$.fragment,e),b(To.$$.fragment,e),b(jo.$$.fragment,e),b(Ao.$$.fragment,e),b(yo.$$.fragment,e),b(qo.$$.fragment,e),b(Lo.$$.fragment,e),b(Qo.$$.fragment,e),b(Wo.$$.fragment,e),b(Go.$$.fragment,e),b(Vo.$$.fragment,e),b(Jo.$$.fragment,e),vp=!1},d(e){o(f),e&&o(x),e&&o(m),w(N),e&&o(T),e&&o(C),e&&o(D),e&&o(g),e&&o(Me),e&&o(K),w(ae),e&&o(qt),e&&o(G),e&&o(rs),e&&o(ni),e&&o(ps),e&&o(Xe),e&&o(cs),e&&o(li),e&&o(ds),e&&o(h),e&&o(us),e&&o(si),e&&o(fs),e&&o(Re),e&&o(ms),e&&o(Ee),w(Ct),e&&o(hs),e&&o(ri),e&&o(vs),w(Dt,e),e&&o(gs),e&&o(Qe),e&&o(_s),w(St,e),e&&o(zs),e&&o(pi),e&&o(Es),w(It,e),e&&o($s),e&&o(ci),e&&o(bs),w(Lt,e),e&&o(ws),e&&o(le),e&&o(ks),e&&o(J),e&&o(xs),w(Xt,e),e&&o(Ns),e&&o(He),e&&o(Os),w(Rt,e),e&&o(Ts),e&&o(Ue),e&&o(js),w(Qt,e),e&&o(As),e&&o(di),e&&o(ys),w(We,e),e&&o(qs),e&&o($e),w(Ht),e&&o(Ps),e&&o(Ge),e&&o(Cs),e&&o(Ve),e&&o(Ds),e&&o(Je),e&&o(Ss),w(io,e),e&&o(Is),e&&o(se),e&&o(Ls),w(ao,e),e&&o(Bs),e&&o(ui),e&&o(Ms),w(no,e),e&&o(Xs),e&&o(Y),e&&o(Rs),w(Ye,e),e&&o(Fs),e&&o(be),w(lo),e&&o(Qs),e&&o(fi),e&&o(Hs),e&&o(re),e&&o(Us),e&&o(mi),e&&o(Ws),e&&o(we),w(so),e&&o(Ks),e&&o(hi),e&&o(Gs),e&&o(pe),e&&o(Vs),w(tt,e),e&&o(Js),e&&o(ot),e&&o(Ys),w(ro,e),e&&o(Zs),e&&o(H),e&&o(er),w(it,e),e&&o(tr),e&&o(zi),e&&o(or),w(po,e),e&&o(ir),e&&o(Ei),e&&o(ar),w(co,e),e&&o(nr),e&&o($i),e&&o(lr),w(uo,e),e&&o(sr),e&&o(Z),e&&o(rr),w(fo,e),e&&o(pr),w(at,e),e&&o(cr),e&&o(ke),w(mo),e&&o(dr),e&&o(ce),e&&o(ur),w(ho,e),e&&o(fr),e&&o(R),e&&o(mr),w(vo,e),e&&o(hr),w(lt,e),e&&o(vr),e&&o(xe),w(go),e&&o(gr),e&&o(de),e&&o(_r),w(_o,e),e&&o(zr),e&&o(rt),e&&o(Er),e&&o(Ne),w(zo),e&&o($r),e&&o(bi),e&&o(br),e&&o(ue),e&&o(wr),e&&o(ct),e&&o(kr),e&&o(Oe),w($o),e&&o(xr),w(ut,e),e&&o(Nr),e&&o(ft),e&&o(Or),e&&o(Ni),e&&o(Tr),e&&o(Oi),e&&o(jr),e&&o(mt),e&&o(Ar),e&&o(Ti),e&&o(yr),e&&o(Te),w(ko),e&&o(qr),e&&o(ji),e&&o(Pr),e&&o(ee),e&&o(Cr),e&&o(vt),e&&o(Dr),e&&o(je),w(xo),e&&o(Sr),e&&o(Ai),e&&o(Ir),e&&o(yi),e&&o(Lr),e&&o(qi),e&&o(Br),e&&o(Pi),e&&o(Mr),e&&o(Ci),e&&o(Xr),e&&o(Ae),w(No),e&&o(Rr),e&&o(Di),e&&o(Fr),e&&o(ye),w(Oo),e&&o(Qr),e&&o(V),e&&o(Hr),w(To,e),e&&o(Ur),e&&o(qe),w(jo),e&&o(Wr),e&&o(te),e&&o(Kr),w(Ao,e),e&&o(Gr),e&&o(Pe),w(yo),e&&o(Vr),e&&o(bt),e&&o(Jr),w(qo,e),e&&o(Yr),e&&o(Si),e&&o(Zr),e&&o(fe),e&&o(ep),e&&o(me),e&&o(tp),e&&o(De),w(Lo),e&&o(op),e&&o(F),e&&o(ip),e&&o(Se),w(Qo),e&&o(ap),e&&o(Li),e&&o(np),e&&o(Bi),e&&o(lp),e&&o(Le),w(Wo),e&&o(sp),e&&o(he),e&&o(rp),w(Go,e),e&&o(pp),e&&o(Mi),e&&o(cp),e&&o(Xi),e&&o(dp),w(Vo,e),e&&o(up),e&&o(Ri),e&&o(fp),w(Jo,e),e&&o(mp),e&&o(Fi),e&&o(hp),e&&o(Nt)}}}const IE={local:"esporta-modelli-transformers",sections:[{local:"onnx",sections:[{local:"esportazione-di-un-modello-in-onnx",title:"Esportazione di un modello in ONNX"},{local:"selezione-delle-caratteristiche-per-diverse-topologie-di-modello",title:"Selezione delle caratteristiche per diverse topologie di modello"},{local:"esportazione-di-un-modello-per-unarchitettura-non-supportata",sections:[{local:"implementazione-di-una-configurazione-onnx-personalizzata",title:"Implementazione di una configurazione ONNX personalizzata"},{local:"esportazione-del-modello",title:"Esportazione del modello"},{local:"convalida-degli-output-del-modello",title:"Convalida degli output del modello"}],title:"Esportazione di un modello per un'architettura non supportata"},{local:"contribuire-con-una-nuova-configurazione-a-transformers",title:"Contribuire con una nuova configurazione a \u{1F917} Transformers"}],title:"ONNX"},{local:"torchscript",sections:[{local:"flag-torchscript-e-pesi-legati",title:"Flag TorchScript e pesi legati"},{local:"input-fittizi-e-standard-lengths",title:"Input fittizi e standard lengths"},{local:"usare-torchsscript-in-python",sections:[{local:"salvare-un-modello",title:"Salvare un modello"},{local:"caricare-un-modello",title:"Caricare un modello"},{local:"utilizzare-un-modello-tracciato-per-linferenza",title:"Utilizzare un modello tracciato per l'inferenza"},{local:"implicazioni",title:"Implicazioni"},{local:"dipendenze",title:"Dipendenze"},{local:"convertire-un-modello-per-aws-neuron",title:"Convertire un modello per AWS Neuron"}],title:"Usare TorchSscript in Python"}],title:"TorchScript"}],title:"Esporta modelli \u{1F917} Transformers "};function LE(L){return kE(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class HE extends EE{constructor(f){super();$E(this,f,LE,SE,bE,{})}}export{HE as default,IE as metadata};
