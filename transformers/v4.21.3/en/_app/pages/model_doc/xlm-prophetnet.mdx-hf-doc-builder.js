import{S as $n,i as bn,s as Pn,e as o,k as d,w as v,t as l,M as wn,c as r,d as s,m as c,a as n,x as $,h as p,b as i,G as t,g as m,y as b,q as P,o as w,B as N,v as Nn,L as Rt}from"../../chunks/vendor-hf-doc-builder.js";import{D as C}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Ht}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as U}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Bt}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Ln(x){let f,L,_,u,k;return u=new Ht({props:{code:`from transformers import XLMProphetNetTokenizer, XLMProphetNetModel

tokenizer = XLMProphetNetTokenizer.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
model = XLMProphetNetModel.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")

input_ids = tokenizer(
    "Studies have been shown that owning a dog is good for you", return_tensors="pt"
).input_ids  # Batch size 1
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

last_hidden_states = outputs.last_hidden_state  # main stream hidden states
last_hidden_states_ngram = outputs.last_hidden_state_ngram  # predict hidden states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMProphetNetTokenizer, XLMProphetNetModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMProphetNetModel.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Studies have been shown that owning a dog is good for you&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(<span class="hljs-string">&quot;Studies show that&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state  <span class="hljs-comment"># main stream hidden states</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states_ngram = outputs.last_hidden_state_ngram  <span class="hljs-comment"># predict hidden states</span>`}}),{c(){f=o("p"),L=l("Example:"),_=d(),v(u.$$.fragment)},l(a){f=r(a,"P",{});var g=n(f);L=p(g,"Example:"),g.forEach(s),_=c(a),$(u.$$.fragment,a)},m(a,g){m(a,f,g),t(f,L),m(a,_,g),b(u,a,g),k=!0},p:Rt,i(a){k||(P(u.$$.fragment,a),k=!0)},o(a){w(u.$$.fragment,a),k=!1},d(a){a&&s(f),a&&s(_),N(u,a)}}}function Mn(x){let f,L,_,u,k;return u=new Ht({props:{code:`from transformers import XLMProphetNetTokenizer, XLMProphetNetEncoder
import torch

tokenizer = XLMProphetNetTokenizer.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
model = XLMProphetNetEncoder.from_pretrained("patrickvonplaten/xprophetnet-large-uncased-standalone")
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMProphetNetTokenizer, XLMProphetNetEncoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMProphetNetEncoder.from_pretrained(<span class="hljs-string">&quot;patrickvonplaten/xprophetnet-large-uncased-standalone&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> model.config.is_decoder, <span class="hljs-string">f&quot;<span class="hljs-subst">{model.__class__}</span> has to be configured as a decoder.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){f=o("p"),L=l("Example:"),_=d(),v(u.$$.fragment)},l(a){f=r(a,"P",{});var g=n(f);L=p(g,"Example:"),g.forEach(s),_=c(a),$(u.$$.fragment,a)},m(a,g){m(a,f,g),t(f,L),m(a,_,g),b(u,a,g),k=!0},p:Rt,i(a){k||(P(u.$$.fragment,a),k=!0)},o(a){w(u.$$.fragment,a),k=!1},d(a){a&&s(f),a&&s(_),N(u,a)}}}function yn(x){let f,L,_,u,k;return u=new Ht({props:{code:`from transformers import XLMProphetNetTokenizer, XLMProphetNetDecoder
import torch

tokenizer = XLMProphetNetTokenizer.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
model = XLMProphetNetDecoder.from_pretrained(
    "patrickvonplaten/xprophetnet-large-uncased-standalone", add_cross_attention=False
)
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMProphetNetTokenizer, XLMProphetNetDecoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMProphetNetDecoder.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;patrickvonplaten/xprophetnet-large-uncased-standalone&quot;</span>, add_cross_attention=<span class="hljs-literal">False</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> model.config.is_decoder, <span class="hljs-string">f&quot;<span class="hljs-subst">{model.__class__}</span> has to be configured as a decoder.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){f=o("p"),L=l("Example:"),_=d(),v(u.$$.fragment)},l(a){f=r(a,"P",{});var g=n(f);L=p(g,"Example:"),g.forEach(s),_=c(a),$(u.$$.fragment,a)},m(a,g){m(a,f,g),t(f,L),m(a,_,g),b(u,a,g),k=!0},p:Rt,i(a){k||(P(u.$$.fragment,a),k=!0)},o(a){w(u.$$.fragment,a),k=!1},d(a){a&&s(f),a&&s(_),N(u,a)}}}function xn(x){let f,L,_,u,k;return u=new Ht({props:{code:`from transformers import XLMProphetNetTokenizer, XLMProphetNetForConditionalGeneration

tokenizer = XLMProphetNetTokenizer.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
model = XLMProphetNetForConditionalGeneration.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")

input_ids = tokenizer(
    "Studies have been shown that owning a dog is good for you", return_tensors="pt"
).input_ids  # Batch size 1
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

logits_next_token = outputs.logits  # logits to predict next token as usual
logits_ngram_next_tokens = outputs.logits_ngram  # logits to predict 2nd, 3rd, ... next tokens`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMProphetNetTokenizer, XLMProphetNetForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMProphetNetForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Studies have been shown that owning a dog is good for you&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(<span class="hljs-string">&quot;Studies show that&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits_next_token = outputs.logits  <span class="hljs-comment"># logits to predict next token as usual</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_ngram_next_tokens = outputs.logits_ngram  <span class="hljs-comment"># logits to predict 2nd, 3rd, ... next tokens</span>`}}),{c(){f=o("p"),L=l("Example:"),_=d(),v(u.$$.fragment)},l(a){f=r(a,"P",{});var g=n(f);L=p(g,"Example:"),g.forEach(s),_=c(a),$(u.$$.fragment,a)},m(a,g){m(a,f,g),t(f,L),m(a,_,g),b(u,a,g),k=!0},p:Rt,i(a){k||(P(u.$$.fragment,a),k=!0)},o(a){w(u.$$.fragment,a),k=!1},d(a){a&&s(f),a&&s(_),N(u,a)}}}function En(x){let f,L,_,u,k;return u=new Ht({props:{code:`from transformers import XLMProphetNetTokenizer, XLMProphetNetForCausalLM
import torch

tokenizer = XLMProphetNetTokenizer.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
model = XLMProphetNetForCausalLM.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

logits = outputs.logits

# Model can also be used with EncoderDecoder framework
from transformers import EncoderDecoderModel, XLMProphetNetTokenizer, XLMRobertaTokenizer
import torch

tokenizer_enc = XLMRobertaTokenizer.from_pretrained("xlm-roberta-large")
tokenizer_dec = XLMProphetNetTokenizer.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
model = EncoderDecoderModel.from_encoder_decoder_pretrained(
    "xlm-roberta-large", "microsoft/xprophetnet-large-wiki100-cased"
)

ARTICLE = (
    "the us state department said wednesday it had received no "
    "formal word from bolivia that it was expelling the us ambassador there "
    "but said the charges made against him are \`\` baseless ."
)
input_ids = tokenizer_enc(ARTICLE, return_tensors="pt").input_ids
labels = tokenizer_dec("us rejects charges against its ambassador in bolivia", return_tensors="pt").input_ids
outputs = model(input_ids=input_ids, decoder_input_ids=labels[:, :-1], labels=labels[:, 1:])

loss = outputs.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMProphetNetTokenizer, XLMProphetNetForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMProphetNetForCausalLM.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> model.config.is_decoder, <span class="hljs-string">f&quot;<span class="hljs-subst">{model.__class__}</span> has to be configured as a decoder.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Model can also be used with EncoderDecoder framework</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> EncoderDecoderModel, XLMProphetNetTokenizer, XLMRobertaTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_enc = XLMRobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;xlm-roberta-large&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_dec = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel.from_encoder_decoder_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;xlm-roberta-large&quot;</span>, <span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>ARTICLE = (
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;the us state department said wednesday it had received no &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;formal word from bolivia that it was expelling the us ambassador there &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;but said the charges made against him are \`\` baseless .&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer_enc(ARTICLE, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer_dec(<span class="hljs-string">&quot;us rejects charges against its ambassador in bolivia&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=labels[:, :-<span class="hljs-number">1</span>], labels=labels[:, <span class="hljs-number">1</span>:])

<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),{c(){f=o("p"),L=l("Example:"),_=d(),v(u.$$.fragment)},l(a){f=r(a,"P",{});var g=n(f);L=p(g,"Example:"),g.forEach(s),_=c(a),$(u.$$.fragment,a)},m(a,g){m(a,f,g),t(f,L),m(a,_,g),b(u,a,g),k=!0},p:Rt,i(a){k||(P(u.$$.fragment,a),k=!0)},o(a){w(u.$$.fragment,a),k=!1},d(a){a&&s(f),a&&s(_),N(u,a)}}}function qn(x){let f,L,_,u,k,a,g,ft,Ds,Vt,A,gt,As,Ss,fe,Is,Os,Wt,S,Y,_t,ge,Fs,kt,Gs,Ut,J,Bs,_e,Rs,Hs,Yt,Ue,Vs,Jt,Ye,Ws,Qt,Je,vt,Us,Zt,Q,Ys,ke,Js,Qs,Kt,I,Z,$t,ve,Zs,bt,Ks,es,O,$e,eo,F,to,Qe,so,oo,be,ro,no,ts,G,K,Pt,Pe,ao,wt,io,ss,M,we,lo,E,po,Ze,co,ho,Ke,mo,uo,Ne,fo,go,_o,Le,ko,et,vo,$o,bo,D,Me,Po,Nt,wo,No,ye,tt,Lo,Lt,Mo,yo,st,xo,Mt,Eo,qo,ee,xe,Xo,yt,jo,zo,te,Ee,To,xt,Co,Do,se,qe,Ao,Xe,So,Et,Io,Oo,os,B,oe,qt,je,Fo,Xt,Go,rs,q,ze,Bo,Te,Ro,ot,Ho,Vo,Wo,re,ns,R,ne,jt,Ce,Uo,zt,Yo,as,X,De,Jo,Ae,Qo,rt,Zo,Ko,er,ae,is,H,ie,Tt,Se,tr,Ct,sr,ls,j,Ie,or,Oe,rr,nt,nr,ar,ir,le,ps,V,pe,Dt,Fe,lr,At,pr,ds,z,Ge,dr,Be,cr,at,hr,mr,ur,de,cs,W,ce,St,Re,fr,It,gr,hs,T,He,_r,Ve,kr,it,vr,$r,br,he,ms;return a=new U({}),ge=new U({}),ve=new U({}),$e=new C({props:{name:"class transformers.XLMProphetNetConfig",anchor:"transformers.XLMProphetNetConfig",parameters:[{name:"activation_dropout",val:": typing.Optional[float] = 0.1"},{name:"activation_function",val:": typing.Union[str, typing.Callable, NoneType] = 'gelu'"},{name:"vocab_size",val:": typing.Optional[int] = 30522"},{name:"hidden_size",val:": typing.Optional[int] = 1024"},{name:"encoder_ffn_dim",val:": typing.Optional[int] = 4096"},{name:"num_encoder_layers",val:": typing.Optional[int] = 12"},{name:"num_encoder_attention_heads",val:": typing.Optional[int] = 16"},{name:"decoder_ffn_dim",val:": typing.Optional[int] = 4096"},{name:"num_decoder_layers",val:": typing.Optional[int] = 12"},{name:"num_decoder_attention_heads",val:": typing.Optional[int] = 16"},{name:"attention_dropout",val:": typing.Optional[float] = 0.1"},{name:"dropout",val:": typing.Optional[float] = 0.1"},{name:"max_position_embeddings",val:": typing.Optional[int] = 512"},{name:"init_std",val:": typing.Optional[float] = 0.02"},{name:"is_encoder_decoder",val:": typing.Optional[bool] = True"},{name:"add_cross_attention",val:": typing.Optional[bool] = True"},{name:"decoder_start_token_id",val:": typing.Optional[int] = 0"},{name:"ngram",val:": typing.Optional[int] = 2"},{name:"num_buckets",val:": typing.Optional[int] = 32"},{name:"relative_max_distance",val:": typing.Optional[int] = 128"},{name:"disable_ngram_loss",val:": typing.Optional[bool] = False"},{name:"eps",val:": typing.Optional[float] = 0.0"},{name:"use_cache",val:": typing.Optional[bool] = True"},{name:"pad_token_id",val:": typing.Optional[int] = 0"},{name:"bos_token_id",val:": typing.Optional[int] = 1"},{name:"eos_token_id",val:": typing.Optional[int] = 2"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.21.3/src/transformers/models/xlm_prophetnet/configuration_xlm_prophetnet.py#L31"}}),Pe=new U({}),we=new C({props:{name:"class transformers.XLMProphetNetTokenizer",anchor:"transformers.XLMProphetNetTokenizer",parameters:[{name:"vocab_file",val:""},{name:"bos_token",val:" = '[SEP]'"},{name:"eos_token",val:" = '[SEP]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"unk_token",val:" = '[UNK]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.XLMProphetNetTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.XLMProphetNetTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.XLMProphetNetTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.XLMProphetNetTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.XLMProphetNetTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.XLMProphetNetTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.XLMProphetNetTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.XLMProphetNetTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.XLMProphetNetTokenizer.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>, defaults to <code>[&quot;&lt;s&gt;NOTUSED&quot;, &quot;&lt;/s&gt;NOTUSED&quot;]</code>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"},{anchor:"transformers.XLMProphetNetTokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"},{anchor:"transformers.XLMProphetNetTokenizer.sp_model",description:`<strong>sp_model</strong> (<code>SentencePieceProcessor</code>) &#x2014;
The <em>SentencePiece</em> processor that is used for every conversion (string, tokens and IDs).`,name:"sp_model"}],source:"https://github.com/huggingface/transformers/blob/v4.21.3/src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py#L59"}}),Me=new C({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.XLMProphetNetTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.XLMProphetNetTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added`,name:"token_ids_0"},{anchor:"transformers.XLMProphetNetTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.21.3/src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py#L316",returnDescription:`
<p>list of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),xe=new C({props:{name:"convert_tokens_to_string",anchor:"transformers.XLMProphetNetTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.21.3/src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py#L294"}}),Ee=new C({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.XLMProphetNetTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.XLMProphetNetTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.XLMProphetNetTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.21.3/src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py#L243",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),qe=new C({props:{name:"get_special_tokens_mask",anchor:"transformers.XLMProphetNetTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.XLMProphetNetTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.XLMProphetNetTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.XLMProphetNetTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.21.3/src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py#L215",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),je=new U({}),ze=new C({props:{name:"class transformers.XLMProphetNetModel",anchor:"transformers.XLMProphetNetModel",parameters:[{name:"config",val:": ProphetNetConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.21.3/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py#L86"}}),re=new Bt({props:{anchor:"transformers.XLMProphetNetModel.example",$$slots:{default:[Ln]},$$scope:{ctx:x}}}),Ce=new U({}),De=new C({props:{name:"class transformers.XLMProphetNetEncoder",anchor:"transformers.XLMProphetNetEncoder",parameters:[{name:"config",val:": ProphetNetConfig"},{name:"word_embeddings",val:": Embedding = None"}],source:"https://github.com/huggingface/transformers/blob/v4.21.3/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py#L38"}}),ae=new Bt({props:{anchor:"transformers.XLMProphetNetEncoder.example",$$slots:{default:[Mn]},$$scope:{ctx:x}}}),Se=new U({}),Ie=new C({props:{name:"class transformers.XLMProphetNetDecoder",anchor:"transformers.XLMProphetNetDecoder",parameters:[{name:"config",val:": ProphetNetConfig"},{name:"word_embeddings",val:": typing.Optional[torch.nn.modules.sparse.Embedding] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.21.3/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py#L61"}}),le=new Bt({props:{anchor:"transformers.XLMProphetNetDecoder.example",$$slots:{default:[yn]},$$scope:{ctx:x}}}),Fe=new U({}),Ge=new C({props:{name:"class transformers.XLMProphetNetForConditionalGeneration",anchor:"transformers.XLMProphetNetForConditionalGeneration",parameters:[{name:"config",val:": ProphetNetConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.21.3/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py#L112"}}),de=new Bt({props:{anchor:"transformers.XLMProphetNetForConditionalGeneration.example",$$slots:{default:[xn]},$$scope:{ctx:x}}}),Re=new U({}),He=new C({props:{name:"class transformers.XLMProphetNetForCausalLM",anchor:"transformers.XLMProphetNetForCausalLM",parameters:[{name:"config",val:": ProphetNetConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.21.3/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py#L138"}}),he=new Bt({props:{anchor:"transformers.XLMProphetNetForCausalLM.example",$$slots:{default:[En]},$$scope:{ctx:x}}}),{c(){f=o("meta"),L=d(),_=o("h1"),u=o("a"),k=o("span"),v(a.$$.fragment),g=d(),ft=o("span"),Ds=l("XLM-ProphetNet"),Vt=d(),A=o("p"),gt=o("strong"),As=l("DISCLAIMER:"),Ss=l(" If you see something strange, file a "),fe=o("a"),Is=l("Github Issue"),Os=l(` and assign
@patrickvonplaten`),Wt=d(),S=o("h2"),Y=o("a"),_t=o("span"),v(ge.$$.fragment),Fs=d(),kt=o("span"),Gs=l("Overview"),Ut=d(),J=o("p"),Bs=l("The XLM-ProphetNet model was proposed in "),_e=o("a"),Rs=l("ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,"),Hs=l(` by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei
Zhang, Ming Zhou on 13 Jan, 2020.`),Yt=d(),Ue=o("p"),Vs=l(`XLM-ProphetNet is an encoder-decoder model and can predict n-future tokens for \u201Cngram\u201D language modeling instead of
just the next token. Its architecture is identical to ProhpetNet, but the model was trained on the multi-lingual
\u201Cwiki100\u201D Wikipedia dump.`),Jt=d(),Ye=o("p"),Ws=l("The abstract from the paper is the following:"),Qt=d(),Je=o("p"),vt=o("em"),Us=l(`In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel
self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of
the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by
n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time
step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent
overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale
dataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for
abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new
state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.`),Zt=d(),Q=o("p"),Ys=l("The Authors\u2019 code can be found "),ke=o("a"),Js=l("here"),Qs=l("."),Kt=d(),I=o("h2"),Z=o("a"),$t=o("span"),v(ve.$$.fragment),Zs=d(),bt=o("span"),Ks=l("XLMProphetNetConfig"),es=d(),O=o("div"),v($e.$$.fragment),eo=d(),F=o("p"),to=l("This class overrides "),Qe=o("a"),so=l("ProphetNetConfig"),oo=l(`. Please check the superclass for the appropriate documentation alongside
usage examples. Instantiating a configuration with the defaults will yield a similar configuration to that of the
XLMProphetNet
`),be=o("a"),ro=l("microsoft/xprophetnet-large-wiki100-cased"),no=l(`
architecture.`),ts=d(),G=o("h2"),K=o("a"),Pt=o("span"),v(Pe.$$.fragment),ao=d(),wt=o("span"),io=l("XLMProphetNetTokenizer"),ss=d(),M=o("div"),v(we.$$.fragment),lo=d(),E=o("p"),po=l("Adapted from "),Ze=o("a"),co=l("RobertaTokenizer"),ho=l(" and "),Ke=o("a"),mo=l("XLNetTokenizer"),uo=l(`. Based on
`),Ne=o("a"),fo=l("SentencePiece"),go=l("."),_o=d(),Le=o("p"),ko=l("This tokenizer inherits from "),et=o("a"),vo=l("PreTrainedTokenizer"),$o=l(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),bo=d(),D=o("div"),v(Me.$$.fragment),Po=d(),Nt=o("p"),wo=l(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A XLMProphetNet sequence has the following format:`),No=d(),ye=o("ul"),tt=o("li"),Lo=l("single sequence: "),Lt=o("code"),Mo=l("X [SEP]"),yo=d(),st=o("li"),xo=l("pair of sequences: "),Mt=o("code"),Eo=l("A [SEP] B [SEP]"),qo=d(),ee=o("div"),v(xe.$$.fragment),Xo=d(),yt=o("p"),jo=l("Converts a sequence of tokens (strings for sub-words) in a single string."),zo=d(),te=o("div"),v(Ee.$$.fragment),To=d(),xt=o("p"),Co=l(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. XLMProphetNet
does not make use of token type ids, therefore a list of zeros is returned.`),Do=d(),se=o("div"),v(qe.$$.fragment),Ao=d(),Xe=o("p"),So=l(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Et=o("code"),Io=l("prepare_for_model"),Oo=l(" method."),os=d(),B=o("h2"),oe=o("a"),qt=o("span"),v(je.$$.fragment),Fo=d(),Xt=o("span"),Go=l("XLMProphetNetModel"),rs=d(),q=o("div"),v(ze.$$.fragment),Bo=d(),Te=o("p"),Ro=l("This class overrides "),ot=o("a"),Ho=l("ProphetNetModel"),Vo=l(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Wo=d(),v(re.$$.fragment),ns=d(),R=o("h2"),ne=o("a"),jt=o("span"),v(Ce.$$.fragment),Uo=d(),zt=o("span"),Yo=l("XLMProphetNetEncoder"),as=d(),X=o("div"),v(De.$$.fragment),Jo=d(),Ae=o("p"),Qo=l("This class overrides "),rt=o("a"),Zo=l("ProphetNetEncoder"),Ko=l(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),er=d(),v(ae.$$.fragment),is=d(),H=o("h2"),ie=o("a"),Tt=o("span"),v(Se.$$.fragment),tr=d(),Ct=o("span"),sr=l("XLMProphetNetDecoder"),ls=d(),j=o("div"),v(Ie.$$.fragment),or=d(),Oe=o("p"),rr=l("This class overrides "),nt=o("a"),nr=l("ProphetNetDecoder"),ar=l(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),ir=d(),v(le.$$.fragment),ps=d(),V=o("h2"),pe=o("a"),Dt=o("span"),v(Fe.$$.fragment),lr=d(),At=o("span"),pr=l("XLMProphetNetForConditionalGeneration"),ds=d(),z=o("div"),v(Ge.$$.fragment),dr=d(),Be=o("p"),cr=l("This class overrides "),at=o("a"),hr=l("ProphetNetForConditionalGeneration"),mr=l(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),ur=d(),v(de.$$.fragment),cs=d(),W=o("h2"),ce=o("a"),St=o("span"),v(Re.$$.fragment),fr=d(),It=o("span"),gr=l("XLMProphetNetForCausalLM"),hs=d(),T=o("div"),v(He.$$.fragment),_r=d(),Ve=o("p"),kr=l("This class overrides "),it=o("a"),vr=l("ProphetNetForCausalLM"),$r=l(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),br=d(),v(he.$$.fragment),this.h()},l(e){const h=wn('[data-svelte="svelte-1phssyn"]',document.head);f=r(h,"META",{name:!0,content:!0}),h.forEach(s),L=c(e),_=r(e,"H1",{class:!0});var We=n(_);u=r(We,"A",{id:!0,class:!0,href:!0});var Ot=n(u);k=r(Ot,"SPAN",{});var Ft=n(k);$(a.$$.fragment,Ft),Ft.forEach(s),Ot.forEach(s),g=c(We),ft=r(We,"SPAN",{});var Gt=n(ft);Ds=p(Gt,"XLM-ProphetNet"),Gt.forEach(s),We.forEach(s),Vt=c(e),A=r(e,"P",{});var me=n(A);gt=r(me,"STRONG",{});var Nr=n(gt);As=p(Nr,"DISCLAIMER:"),Nr.forEach(s),Ss=p(me," If you see something strange, file a "),fe=r(me,"A",{href:!0,rel:!0});var Lr=n(fe);Is=p(Lr,"Github Issue"),Lr.forEach(s),Os=p(me,` and assign
@patrickvonplaten`),me.forEach(s),Wt=c(e),S=r(e,"H2",{class:!0});var us=n(S);Y=r(us,"A",{id:!0,class:!0,href:!0});var Mr=n(Y);_t=r(Mr,"SPAN",{});var yr=n(_t);$(ge.$$.fragment,yr),yr.forEach(s),Mr.forEach(s),Fs=c(us),kt=r(us,"SPAN",{});var xr=n(kt);Gs=p(xr,"Overview"),xr.forEach(s),us.forEach(s),Ut=c(e),J=r(e,"P",{});var fs=n(J);Bs=p(fs,"The XLM-ProphetNet model was proposed in "),_e=r(fs,"A",{href:!0,rel:!0});var Er=n(_e);Rs=p(Er,"ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,"),Er.forEach(s),Hs=p(fs,` by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei
Zhang, Ming Zhou on 13 Jan, 2020.`),fs.forEach(s),Yt=c(e),Ue=r(e,"P",{});var qr=n(Ue);Vs=p(qr,`XLM-ProphetNet is an encoder-decoder model and can predict n-future tokens for \u201Cngram\u201D language modeling instead of
just the next token. Its architecture is identical to ProhpetNet, but the model was trained on the multi-lingual
\u201Cwiki100\u201D Wikipedia dump.`),qr.forEach(s),Jt=c(e),Ye=r(e,"P",{});var Xr=n(Ye);Ws=p(Xr,"The abstract from the paper is the following:"),Xr.forEach(s),Qt=c(e),Je=r(e,"P",{});var jr=n(Je);vt=r(jr,"EM",{});var zr=n(vt);Us=p(zr,`In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel
self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of
the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by
n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time
step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent
overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale
dataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for
abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new
state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.`),zr.forEach(s),jr.forEach(s),Zt=c(e),Q=r(e,"P",{});var gs=n(Q);Ys=p(gs,"The Authors\u2019 code can be found "),ke=r(gs,"A",{href:!0,rel:!0});var Tr=n(ke);Js=p(Tr,"here"),Tr.forEach(s),Qs=p(gs,"."),gs.forEach(s),Kt=c(e),I=r(e,"H2",{class:!0});var _s=n(I);Z=r(_s,"A",{id:!0,class:!0,href:!0});var Cr=n(Z);$t=r(Cr,"SPAN",{});var Dr=n($t);$(ve.$$.fragment,Dr),Dr.forEach(s),Cr.forEach(s),Zs=c(_s),bt=r(_s,"SPAN",{});var Ar=n(bt);Ks=p(Ar,"XLMProphetNetConfig"),Ar.forEach(s),_s.forEach(s),es=c(e),O=r(e,"DIV",{class:!0});var ks=n(O);$($e.$$.fragment,ks),eo=c(ks),F=r(ks,"P",{});var lt=n(F);to=p(lt,"This class overrides "),Qe=r(lt,"A",{href:!0});var Sr=n(Qe);so=p(Sr,"ProphetNetConfig"),Sr.forEach(s),oo=p(lt,`. Please check the superclass for the appropriate documentation alongside
usage examples. Instantiating a configuration with the defaults will yield a similar configuration to that of the
XLMProphetNet
`),be=r(lt,"A",{href:!0,rel:!0});var Ir=n(be);ro=p(Ir,"microsoft/xprophetnet-large-wiki100-cased"),Ir.forEach(s),no=p(lt,`
architecture.`),lt.forEach(s),ks.forEach(s),ts=c(e),G=r(e,"H2",{class:!0});var vs=n(G);K=r(vs,"A",{id:!0,class:!0,href:!0});var Or=n(K);Pt=r(Or,"SPAN",{});var Fr=n(Pt);$(Pe.$$.fragment,Fr),Fr.forEach(s),Or.forEach(s),ao=c(vs),wt=r(vs,"SPAN",{});var Gr=n(wt);io=p(Gr,"XLMProphetNetTokenizer"),Gr.forEach(s),vs.forEach(s),ss=c(e),M=r(e,"DIV",{class:!0});var y=n(M);$(we.$$.fragment,y),lo=c(y),E=r(y,"P",{});var ue=n(E);po=p(ue,"Adapted from "),Ze=r(ue,"A",{href:!0});var Br=n(Ze);co=p(Br,"RobertaTokenizer"),Br.forEach(s),ho=p(ue," and "),Ke=r(ue,"A",{href:!0});var Rr=n(Ke);mo=p(Rr,"XLNetTokenizer"),Rr.forEach(s),uo=p(ue,`. Based on
`),Ne=r(ue,"A",{href:!0,rel:!0});var Hr=n(Ne);fo=p(Hr,"SentencePiece"),Hr.forEach(s),go=p(ue,"."),ue.forEach(s),_o=c(y),Le=r(y,"P",{});var $s=n(Le);ko=p($s,"This tokenizer inherits from "),et=r($s,"A",{href:!0});var Vr=n(et);vo=p(Vr,"PreTrainedTokenizer"),Vr.forEach(s),$o=p($s,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),$s.forEach(s),bo=c(y),D=r(y,"DIV",{class:!0});var pt=n(D);$(Me.$$.fragment,pt),Po=c(pt),Nt=r(pt,"P",{});var Wr=n(Nt);wo=p(Wr,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A XLMProphetNet sequence has the following format:`),Wr.forEach(s),No=c(pt),ye=r(pt,"UL",{});var bs=n(ye);tt=r(bs,"LI",{});var Pr=n(tt);Lo=p(Pr,"single sequence: "),Lt=r(Pr,"CODE",{});var Ur=n(Lt);Mo=p(Ur,"X [SEP]"),Ur.forEach(s),Pr.forEach(s),yo=c(bs),st=r(bs,"LI",{});var wr=n(st);xo=p(wr,"pair of sequences: "),Mt=r(wr,"CODE",{});var Yr=n(Mt);Eo=p(Yr,"A [SEP] B [SEP]"),Yr.forEach(s),wr.forEach(s),bs.forEach(s),pt.forEach(s),qo=c(y),ee=r(y,"DIV",{class:!0});var Ps=n(ee);$(xe.$$.fragment,Ps),Xo=c(Ps),yt=r(Ps,"P",{});var Jr=n(yt);jo=p(Jr,"Converts a sequence of tokens (strings for sub-words) in a single string."),Jr.forEach(s),Ps.forEach(s),zo=c(y),te=r(y,"DIV",{class:!0});var ws=n(te);$(Ee.$$.fragment,ws),To=c(ws),xt=r(ws,"P",{});var Qr=n(xt);Co=p(Qr,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. XLMProphetNet
does not make use of token type ids, therefore a list of zeros is returned.`),Qr.forEach(s),ws.forEach(s),Do=c(y),se=r(y,"DIV",{class:!0});var Ns=n(se);$(qe.$$.fragment,Ns),Ao=c(Ns),Xe=r(Ns,"P",{});var Ls=n(Xe);So=p(Ls,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Et=r(Ls,"CODE",{});var Zr=n(Et);Io=p(Zr,"prepare_for_model"),Zr.forEach(s),Oo=p(Ls," method."),Ls.forEach(s),Ns.forEach(s),y.forEach(s),os=c(e),B=r(e,"H2",{class:!0});var Ms=n(B);oe=r(Ms,"A",{id:!0,class:!0,href:!0});var Kr=n(oe);qt=r(Kr,"SPAN",{});var en=n(qt);$(je.$$.fragment,en),en.forEach(s),Kr.forEach(s),Fo=c(Ms),Xt=r(Ms,"SPAN",{});var tn=n(Xt);Go=p(tn,"XLMProphetNetModel"),tn.forEach(s),Ms.forEach(s),rs=c(e),q=r(e,"DIV",{class:!0});var dt=n(q);$(ze.$$.fragment,dt),Bo=c(dt),Te=r(dt,"P",{});var ys=n(Te);Ro=p(ys,"This class overrides "),ot=r(ys,"A",{href:!0});var sn=n(ot);Ho=p(sn,"ProphetNetModel"),sn.forEach(s),Vo=p(ys,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),ys.forEach(s),Wo=c(dt),$(re.$$.fragment,dt),dt.forEach(s),ns=c(e),R=r(e,"H2",{class:!0});var xs=n(R);ne=r(xs,"A",{id:!0,class:!0,href:!0});var on=n(ne);jt=r(on,"SPAN",{});var rn=n(jt);$(Ce.$$.fragment,rn),rn.forEach(s),on.forEach(s),Uo=c(xs),zt=r(xs,"SPAN",{});var nn=n(zt);Yo=p(nn,"XLMProphetNetEncoder"),nn.forEach(s),xs.forEach(s),as=c(e),X=r(e,"DIV",{class:!0});var ct=n(X);$(De.$$.fragment,ct),Jo=c(ct),Ae=r(ct,"P",{});var Es=n(Ae);Qo=p(Es,"This class overrides "),rt=r(Es,"A",{href:!0});var an=n(rt);Zo=p(an,"ProphetNetEncoder"),an.forEach(s),Ko=p(Es,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Es.forEach(s),er=c(ct),$(ae.$$.fragment,ct),ct.forEach(s),is=c(e),H=r(e,"H2",{class:!0});var qs=n(H);ie=r(qs,"A",{id:!0,class:!0,href:!0});var ln=n(ie);Tt=r(ln,"SPAN",{});var pn=n(Tt);$(Se.$$.fragment,pn),pn.forEach(s),ln.forEach(s),tr=c(qs),Ct=r(qs,"SPAN",{});var dn=n(Ct);sr=p(dn,"XLMProphetNetDecoder"),dn.forEach(s),qs.forEach(s),ls=c(e),j=r(e,"DIV",{class:!0});var ht=n(j);$(Ie.$$.fragment,ht),or=c(ht),Oe=r(ht,"P",{});var Xs=n(Oe);rr=p(Xs,"This class overrides "),nt=r(Xs,"A",{href:!0});var cn=n(nt);nr=p(cn,"ProphetNetDecoder"),cn.forEach(s),ar=p(Xs,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Xs.forEach(s),ir=c(ht),$(le.$$.fragment,ht),ht.forEach(s),ps=c(e),V=r(e,"H2",{class:!0});var js=n(V);pe=r(js,"A",{id:!0,class:!0,href:!0});var hn=n(pe);Dt=r(hn,"SPAN",{});var mn=n(Dt);$(Fe.$$.fragment,mn),mn.forEach(s),hn.forEach(s),lr=c(js),At=r(js,"SPAN",{});var un=n(At);pr=p(un,"XLMProphetNetForConditionalGeneration"),un.forEach(s),js.forEach(s),ds=c(e),z=r(e,"DIV",{class:!0});var mt=n(z);$(Ge.$$.fragment,mt),dr=c(mt),Be=r(mt,"P",{});var zs=n(Be);cr=p(zs,"This class overrides "),at=r(zs,"A",{href:!0});var fn=n(at);hr=p(fn,"ProphetNetForConditionalGeneration"),fn.forEach(s),mr=p(zs,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),zs.forEach(s),ur=c(mt),$(de.$$.fragment,mt),mt.forEach(s),cs=c(e),W=r(e,"H2",{class:!0});var Ts=n(W);ce=r(Ts,"A",{id:!0,class:!0,href:!0});var gn=n(ce);St=r(gn,"SPAN",{});var _n=n(St);$(Re.$$.fragment,_n),_n.forEach(s),gn.forEach(s),fr=c(Ts),It=r(Ts,"SPAN",{});var kn=n(It);gr=p(kn,"XLMProphetNetForCausalLM"),kn.forEach(s),Ts.forEach(s),hs=c(e),T=r(e,"DIV",{class:!0});var ut=n(T);$(He.$$.fragment,ut),_r=c(ut),Ve=r(ut,"P",{});var Cs=n(Ve);kr=p(Cs,"This class overrides "),it=r(Cs,"A",{href:!0});var vn=n(it);vr=p(vn,"ProphetNetForCausalLM"),vn.forEach(s),$r=p(Cs,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Cs.forEach(s),br=c(ut),$(he.$$.fragment,ut),ut.forEach(s),this.h()},h(){i(f,"name","hf:doc:metadata"),i(f,"content",JSON.stringify(Xn)),i(u,"id","xlmprophetnet"),i(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(u,"href","#xlmprophetnet"),i(_,"class","relative group"),i(fe,"href","https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title"),i(fe,"rel","nofollow"),i(Y,"id","overview"),i(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Y,"href","#overview"),i(S,"class","relative group"),i(_e,"href","https://arxiv.org/abs/2001.04063"),i(_e,"rel","nofollow"),i(ke,"href","https://github.com/microsoft/ProphetNet"),i(ke,"rel","nofollow"),i(Z,"id","transformers.XLMProphetNetConfig"),i(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Z,"href","#transformers.XLMProphetNetConfig"),i(I,"class","relative group"),i(Qe,"href","/docs/transformers/v4.21.3/en/model_doc/prophetnet#transformers.ProphetNetConfig"),i(be,"href","https://huggingface.co/microsoft/xprophetnet-large-wiki100-cased"),i(be,"rel","nofollow"),i(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(K,"id","transformers.XLMProphetNetTokenizer"),i(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(K,"href","#transformers.XLMProphetNetTokenizer"),i(G,"class","relative group"),i(Ze,"href","/docs/transformers/v4.21.3/en/model_doc/roberta#transformers.RobertaTokenizer"),i(Ke,"href","/docs/transformers/v4.21.3/en/model_doc/xlnet#transformers.XLNetTokenizer"),i(Ne,"href","https://github.com/google/sentencepiece"),i(Ne,"rel","nofollow"),i(et,"href","/docs/transformers/v4.21.3/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),i(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(oe,"id","transformers.XLMProphetNetModel"),i(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(oe,"href","#transformers.XLMProphetNetModel"),i(B,"class","relative group"),i(ot,"href","/docs/transformers/v4.21.3/en/model_doc/prophetnet#transformers.ProphetNetModel"),i(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(ne,"id","transformers.XLMProphetNetEncoder"),i(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(ne,"href","#transformers.XLMProphetNetEncoder"),i(R,"class","relative group"),i(rt,"href","/docs/transformers/v4.21.3/en/model_doc/prophetnet#transformers.ProphetNetEncoder"),i(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(ie,"id","transformers.XLMProphetNetDecoder"),i(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(ie,"href","#transformers.XLMProphetNetDecoder"),i(H,"class","relative group"),i(nt,"href","/docs/transformers/v4.21.3/en/model_doc/prophetnet#transformers.ProphetNetDecoder"),i(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(pe,"id","transformers.XLMProphetNetForConditionalGeneration"),i(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(pe,"href","#transformers.XLMProphetNetForConditionalGeneration"),i(V,"class","relative group"),i(at,"href","/docs/transformers/v4.21.3/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),i(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(ce,"id","transformers.XLMProphetNetForCausalLM"),i(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(ce,"href","#transformers.XLMProphetNetForCausalLM"),i(W,"class","relative group"),i(it,"href","/docs/transformers/v4.21.3/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),i(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,h){t(document.head,f),m(e,L,h),m(e,_,h),t(_,u),t(u,k),b(a,k,null),t(_,g),t(_,ft),t(ft,Ds),m(e,Vt,h),m(e,A,h),t(A,gt),t(gt,As),t(A,Ss),t(A,fe),t(fe,Is),t(A,Os),m(e,Wt,h),m(e,S,h),t(S,Y),t(Y,_t),b(ge,_t,null),t(S,Fs),t(S,kt),t(kt,Gs),m(e,Ut,h),m(e,J,h),t(J,Bs),t(J,_e),t(_e,Rs),t(J,Hs),m(e,Yt,h),m(e,Ue,h),t(Ue,Vs),m(e,Jt,h),m(e,Ye,h),t(Ye,Ws),m(e,Qt,h),m(e,Je,h),t(Je,vt),t(vt,Us),m(e,Zt,h),m(e,Q,h),t(Q,Ys),t(Q,ke),t(ke,Js),t(Q,Qs),m(e,Kt,h),m(e,I,h),t(I,Z),t(Z,$t),b(ve,$t,null),t(I,Zs),t(I,bt),t(bt,Ks),m(e,es,h),m(e,O,h),b($e,O,null),t(O,eo),t(O,F),t(F,to),t(F,Qe),t(Qe,so),t(F,oo),t(F,be),t(be,ro),t(F,no),m(e,ts,h),m(e,G,h),t(G,K),t(K,Pt),b(Pe,Pt,null),t(G,ao),t(G,wt),t(wt,io),m(e,ss,h),m(e,M,h),b(we,M,null),t(M,lo),t(M,E),t(E,po),t(E,Ze),t(Ze,co),t(E,ho),t(E,Ke),t(Ke,mo),t(E,uo),t(E,Ne),t(Ne,fo),t(E,go),t(M,_o),t(M,Le),t(Le,ko),t(Le,et),t(et,vo),t(Le,$o),t(M,bo),t(M,D),b(Me,D,null),t(D,Po),t(D,Nt),t(Nt,wo),t(D,No),t(D,ye),t(ye,tt),t(tt,Lo),t(tt,Lt),t(Lt,Mo),t(ye,yo),t(ye,st),t(st,xo),t(st,Mt),t(Mt,Eo),t(M,qo),t(M,ee),b(xe,ee,null),t(ee,Xo),t(ee,yt),t(yt,jo),t(M,zo),t(M,te),b(Ee,te,null),t(te,To),t(te,xt),t(xt,Co),t(M,Do),t(M,se),b(qe,se,null),t(se,Ao),t(se,Xe),t(Xe,So),t(Xe,Et),t(Et,Io),t(Xe,Oo),m(e,os,h),m(e,B,h),t(B,oe),t(oe,qt),b(je,qt,null),t(B,Fo),t(B,Xt),t(Xt,Go),m(e,rs,h),m(e,q,h),b(ze,q,null),t(q,Bo),t(q,Te),t(Te,Ro),t(Te,ot),t(ot,Ho),t(Te,Vo),t(q,Wo),b(re,q,null),m(e,ns,h),m(e,R,h),t(R,ne),t(ne,jt),b(Ce,jt,null),t(R,Uo),t(R,zt),t(zt,Yo),m(e,as,h),m(e,X,h),b(De,X,null),t(X,Jo),t(X,Ae),t(Ae,Qo),t(Ae,rt),t(rt,Zo),t(Ae,Ko),t(X,er),b(ae,X,null),m(e,is,h),m(e,H,h),t(H,ie),t(ie,Tt),b(Se,Tt,null),t(H,tr),t(H,Ct),t(Ct,sr),m(e,ls,h),m(e,j,h),b(Ie,j,null),t(j,or),t(j,Oe),t(Oe,rr),t(Oe,nt),t(nt,nr),t(Oe,ar),t(j,ir),b(le,j,null),m(e,ps,h),m(e,V,h),t(V,pe),t(pe,Dt),b(Fe,Dt,null),t(V,lr),t(V,At),t(At,pr),m(e,ds,h),m(e,z,h),b(Ge,z,null),t(z,dr),t(z,Be),t(Be,cr),t(Be,at),t(at,hr),t(Be,mr),t(z,ur),b(de,z,null),m(e,cs,h),m(e,W,h),t(W,ce),t(ce,St),b(Re,St,null),t(W,fr),t(W,It),t(It,gr),m(e,hs,h),m(e,T,h),b(He,T,null),t(T,_r),t(T,Ve),t(Ve,kr),t(Ve,it),t(it,vr),t(Ve,$r),t(T,br),b(he,T,null),ms=!0},p(e,[h]){const We={};h&2&&(We.$$scope={dirty:h,ctx:e}),re.$set(We);const Ot={};h&2&&(Ot.$$scope={dirty:h,ctx:e}),ae.$set(Ot);const Ft={};h&2&&(Ft.$$scope={dirty:h,ctx:e}),le.$set(Ft);const Gt={};h&2&&(Gt.$$scope={dirty:h,ctx:e}),de.$set(Gt);const me={};h&2&&(me.$$scope={dirty:h,ctx:e}),he.$set(me)},i(e){ms||(P(a.$$.fragment,e),P(ge.$$.fragment,e),P(ve.$$.fragment,e),P($e.$$.fragment,e),P(Pe.$$.fragment,e),P(we.$$.fragment,e),P(Me.$$.fragment,e),P(xe.$$.fragment,e),P(Ee.$$.fragment,e),P(qe.$$.fragment,e),P(je.$$.fragment,e),P(ze.$$.fragment,e),P(re.$$.fragment,e),P(Ce.$$.fragment,e),P(De.$$.fragment,e),P(ae.$$.fragment,e),P(Se.$$.fragment,e),P(Ie.$$.fragment,e),P(le.$$.fragment,e),P(Fe.$$.fragment,e),P(Ge.$$.fragment,e),P(de.$$.fragment,e),P(Re.$$.fragment,e),P(He.$$.fragment,e),P(he.$$.fragment,e),ms=!0)},o(e){w(a.$$.fragment,e),w(ge.$$.fragment,e),w(ve.$$.fragment,e),w($e.$$.fragment,e),w(Pe.$$.fragment,e),w(we.$$.fragment,e),w(Me.$$.fragment,e),w(xe.$$.fragment,e),w(Ee.$$.fragment,e),w(qe.$$.fragment,e),w(je.$$.fragment,e),w(ze.$$.fragment,e),w(re.$$.fragment,e),w(Ce.$$.fragment,e),w(De.$$.fragment,e),w(ae.$$.fragment,e),w(Se.$$.fragment,e),w(Ie.$$.fragment,e),w(le.$$.fragment,e),w(Fe.$$.fragment,e),w(Ge.$$.fragment,e),w(de.$$.fragment,e),w(Re.$$.fragment,e),w(He.$$.fragment,e),w(he.$$.fragment,e),ms=!1},d(e){s(f),e&&s(L),e&&s(_),N(a),e&&s(Vt),e&&s(A),e&&s(Wt),e&&s(S),N(ge),e&&s(Ut),e&&s(J),e&&s(Yt),e&&s(Ue),e&&s(Jt),e&&s(Ye),e&&s(Qt),e&&s(Je),e&&s(Zt),e&&s(Q),e&&s(Kt),e&&s(I),N(ve),e&&s(es),e&&s(O),N($e),e&&s(ts),e&&s(G),N(Pe),e&&s(ss),e&&s(M),N(we),N(Me),N(xe),N(Ee),N(qe),e&&s(os),e&&s(B),N(je),e&&s(rs),e&&s(q),N(ze),N(re),e&&s(ns),e&&s(R),N(Ce),e&&s(as),e&&s(X),N(De),N(ae),e&&s(is),e&&s(H),N(Se),e&&s(ls),e&&s(j),N(Ie),N(le),e&&s(ps),e&&s(V),N(Fe),e&&s(ds),e&&s(z),N(Ge),N(de),e&&s(cs),e&&s(W),N(Re),e&&s(hs),e&&s(T),N(He),N(he)}}}const Xn={local:"xlmprophetnet",sections:[{local:"overview",title:"Overview"},{local:"transformers.XLMProphetNetConfig",title:"XLMProphetNetConfig"},{local:"transformers.XLMProphetNetTokenizer",title:"XLMProphetNetTokenizer"},{local:"transformers.XLMProphetNetModel",title:"XLMProphetNetModel"},{local:"transformers.XLMProphetNetEncoder",title:"XLMProphetNetEncoder"},{local:"transformers.XLMProphetNetDecoder",title:"XLMProphetNetDecoder"},{local:"transformers.XLMProphetNetForConditionalGeneration",title:"XLMProphetNetForConditionalGeneration"},{local:"transformers.XLMProphetNetForCausalLM",title:"XLMProphetNetForCausalLM"}],title:"XLM-ProphetNet"};function jn(x){return Nn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Sn extends $n{constructor(f){super();bn(this,f,jn,qn,Pn,{})}}export{Sn as default,Xn as metadata};
