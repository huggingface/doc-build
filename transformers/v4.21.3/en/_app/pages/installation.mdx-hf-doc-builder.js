import{S as qi,i as zi,s as Ui,e as r,k as p,w as u,t as n,M as Yi,c as l,d as o,m as h,a,x as v,h as s,b as m,N as Bi,G as t,g as f,y as _,q as $,o as w,B as g,v as Wi}from"../chunks/vendor-hf-doc-builder.js";import{T as Al}from"../chunks/Tip-hf-doc-builder.js";import{I as $e}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as T}from"../chunks/CodeBlock-hf-doc-builder.js";function Gi(N){let c,P,d,b,k;return{c(){c=r("p"),P=n("You must keep the "),d=r("code"),b=n("transformers"),k=n(" folder if you want to keep using the library.")},l(y){c=l(y,"P",{});var E=a(c);P=s(E,"You must keep the "),d=l(E,"CODE",{});var A=a(d);b=s(A,"transformers"),A.forEach(o),k=s(E," folder if you want to keep using the library."),E.forEach(o)},m(y,E){f(y,c,E),t(c,P),t(c,d),t(d,b),t(c,k)},d(y){y&&o(c)}}}function Vi(N){let c,P,d,b,k,y,E,A,O,C,x;return{c(){c=r("p"),P=n("\u{1F917} Transformers will use the shell environment variables "),d=r("code"),b=n("PYTORCH_TRANSFORMERS_CACHE"),k=n(" or "),y=r("code"),E=n("PYTORCH_PRETRAINED_BERT_CACHE"),A=n(" if you are coming from an earlier iteration of this library and have set those environment variables, unless you specify the shell environment variable "),O=r("code"),C=n("TRANSFORMERS_CACHE"),x=n(".")},l(j){c=l(j,"P",{});var F=a(c);P=s(F,"\u{1F917} Transformers will use the shell environment variables "),d=l(F,"CODE",{});var U=a(d);b=s(U,"PYTORCH_TRANSFORMERS_CACHE"),U.forEach(o),k=s(F," or "),y=l(F,"CODE",{});var bt=a(y);E=s(bt,"PYTORCH_PRETRAINED_BERT_CACHE"),bt.forEach(o),A=s(F," if you are coming from an earlier iteration of this library and have set those environment variables, unless you specify the shell environment variable "),O=l(F,"CODE",{});var we=a(O);C=s(we,"TRANSFORMERS_CACHE"),we.forEach(o),x=s(F,"."),F.forEach(o)},m(j,F){f(j,c,F),t(c,P),t(c,d),t(d,b),t(c,k),t(c,y),t(y,E),t(c,A),t(c,O),t(O,C),t(c,x)},d(j){j&&o(c)}}}function Xi(N){let c,P,d,b,k,y,E,A;return{c(){c=r("p"),P=n("Add "),d=r("a"),b=n("\u{1F917} Datasets"),k=n(" to your offline training workflow by setting the environment variable "),y=r("code"),E=n("HF_DATASETS_OFFLINE=1"),A=n("."),this.h()},l(O){c=l(O,"P",{});var C=a(c);P=s(C,"Add "),d=l(C,"A",{href:!0,rel:!0});var x=a(d);b=s(x,"\u{1F917} Datasets"),x.forEach(o),k=s(C," to your offline training workflow by setting the environment variable "),y=l(C,"CODE",{});var j=a(y);E=s(j,"HF_DATASETS_OFFLINE=1"),j.forEach(o),A=s(C,"."),C.forEach(o),this.h()},h(){m(d,"href","https://huggingface.co/docs/datasets/"),m(d,"rel","nofollow")},m(O,C){f(O,c,C),t(c,P),t(c,d),t(d,b),t(c,k),t(c,y),t(y,E),t(c,A)},d(O){O&&o(c)}}}function Ji(N){let c,P,d,b,k;return{c(){c=r("p"),P=n("See the "),d=r("a"),b=n("How to download files from the Hub"),k=n(" section for more details on downloading files stored on the Hub."),this.h()},l(y){c=l(y,"P",{});var E=a(c);P=s(E,"See the "),d=l(E,"A",{href:!0,rel:!0});var A=a(d);b=s(A,"How to download files from the Hub"),A.forEach(o),k=s(E," section for more details on downloading files stored on the Hub."),E.forEach(o),this.h()},h(){m(d,"href","https://huggingface.co/docs/hub/how-to-downstream"),m(d,"rel","nofollow")},m(y,E){f(y,c,E),t(c,P),t(c,d),t(d,b),t(c,k)},d(y){y&&o(c)}}}function Ki(N){let c,P,d,b,k,y,E,A,O,C,x,j,F,U,bt,we,H,Tt,ge,Sl,Cl,Ol,kt,ye,Fl,Il,xl,Pt,Ee,jl,Ml,Xo,Y,te,so,be,Rl,io,Nl,Jo,D,Hl,Te,Dl,Ll,ke,ql,zl,Ko,At,Ul,Qo,Pe,Zo,St,Yl,er,Ae,tr,Ct,Bl,or,Se,rr,Ot,Wl,lr,Ce,ar,Ft,Gl,nr,Oe,sr,It,Vl,ir,Fe,fr,xt,Xl,pr,Ie,hr,jt,Jl,mr,xe,cr,Mt,Kl,dr,je,ur,B,oe,fo,Me,Ql,po,Zl,vr,Rt,ea,_r,Re,$r,S,ta,ho,oa,ra,mo,la,aa,co,na,sa,uo,ia,fa,vo,pa,ha,Ne,ma,ca,wr,Nt,da,gr,He,yr,W,re,_o,De,ua,$o,va,Er,Ht,_a,br,le,Le,$a,wo,wa,ga,ya,go,Ea,Tr,Dt,ba,kr,qe,Pr,L,Ta,yo,ka,Pa,Eo,Aa,Sa,Ar,ae,Sr,Lt,Ca,Cr,ze,Or,ne,Oa,bo,Fa,Ia,Fr,G,se,To,Ue,xa,ko,ja,Ir,ie,Ma,Po,Ra,Na,xr,Ye,jr,V,fe,Ao,Be,Ha,So,Da,Mr,M,La,Co,qa,za,Oo,Ua,Ya,Fo,Ba,Wa,Rr,q,We,Ga,Io,Va,Xa,Ja,X,Ka,xo,Qa,Za,jo,en,tn,on,J,rn,Mo,ln,an,Ro,nn,sn,Nr,pe,Hr,K,he,No,Ge,fn,Ho,pn,Dr,me,hn,Do,mn,cn,Lr,ce,qr,qt,dn,zr,Ve,Ur,zt,un,Yr,Xe,Br,Ut,vn,Wr,Q,de,Lo,Je,_n,qo,$n,Gr,Yt,wn,Vr,z,Ke,Qe,gn,Ze,yn,En,bn,zo,Bt,cs,Tn,et,Z,kn,Wt,Pn,An,Gt,Sn,Cn,On,ee,tt,ot,Fn,Vt,In,xn,jn,rt,Mn,lt,at,Rn,Xt,Nn,Hn,Dn,nt,Ln,st,it,qn,Jt,zn,Un,Yn,ft,Bn,pt,ht,Wn,mt,Gn,Vn,Xn,ct,dt,ut,Jn,Uo,Kn,Qn,Zn,vt,es,_t,R,ts,$t,Yo,os,rs,Bo,ls,as,wt,ns,ss,is,gt,Xr,Kt,fs,Jr,yt,Kr,ue,Qr;return y=new $e({}),be=new $e({}),Pe=new T({props:{code:"python -m venv .env",highlighted:'python -m venv .<span class="hljs-built_in">env</span>'}}),Ae=new T({props:{code:"source .env/bin/activate",highlighted:'<span class="hljs-built_in">source</span> .<span class="hljs-built_in">env</span>/bin/activate'}}),Se=new T({props:{code:".env/Scripts/activate",highlighted:'.<span class="hljs-built_in">env</span>/Scripts/activate'}}),Ce=new T({props:{code:"pip install transformers",highlighted:"pip install transformers"}}),Oe=new T({props:{code:"pip install transformers[torch]",highlighted:"pip install transformers[torch]"}}),Fe=new T({props:{code:"pip install transformers[tf-cpu]",highlighted:"pip install transformers[tf-cpu]"}}),Ie=new T({props:{code:"pip install transformers[flax]",highlighted:"pip install transformers[flax]"}}),xe=new T({props:{code:`python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))"`,highlighted:'python -c <span class="hljs-string">&quot;from transformers import pipeline; print(pipeline(&#x27;sentiment-analysis&#x27;)(&#x27;we love you&#x27;))&quot;</span>'}}),je=new T({props:{code:"[{'label': 'POSITIVE', 'score': 0.9998704791069031}]",highlighted:'[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: 0.9998704791069031}]'}}),Me=new $e({}),Re=new T({props:{code:"pip install git+https://github.com/huggingface/transformers",highlighted:"pip install git+https://github.com/huggingface/transformers"}}),He=new T({props:{code:`python -c "from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))"`,highlighted:'python -c <span class="hljs-string">&quot;from transformers import pipeline; print(pipeline(&#x27;sentiment-analysis&#x27;)(&#x27;I love you&#x27;))&quot;</span>'}}),De=new $e({}),qe=new T({props:{code:`git clone https://github.com/huggingface/transformers.git
cd transformers
pip install -e .`,highlighted:`git <span class="hljs-built_in">clone</span> https://github.com/huggingface/transformers.git
<span class="hljs-built_in">cd</span> transformers
pip install -e .`}}),ae=new Al({props:{warning:!0,$$slots:{default:[Gi]},$$scope:{ctx:N}}}),ze=new T({props:{code:`cd ~/transformers/
git pull`,highlighted:`<span class="hljs-built_in">cd</span> ~/transformers/
git pull`}}),Ue=new $e({}),Ye=new T({props:{code:"conda install -c huggingface transformers",highlighted:"conda install -c huggingface transformers"}}),Be=new $e({}),pe=new Al({props:{$$slots:{default:[Vi]},$$scope:{ctx:N}}}),Ge=new $e({}),ce=new Al({props:{$$slots:{default:[Xi]},$$scope:{ctx:N}}}),Ve=new T({props:{code:"python examples/pytorch/translation/run_translation.py --model_name_or_path t5-small --dataset_name wmt16 --dataset_config ro-en ...",highlighted:"python examples/pytorch/translation/run_translation.py --model_name_or_path t5-small --dataset_name wmt16 --dataset_config ro-en ..."}}),Xe=new T({props:{code:`HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \\
python examples/pytorch/translation/run_translation.py --model_name_or_path t5-small --dataset_name wmt16 --dataset_config ro-en ...`,highlighted:`HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \\
python examples/pytorch/translation/run_translation.py --model_name_or_path t5-small --dataset_name wmt16 --dataset_config ro-en ...`}}),Je=new $e({}),rt=new T({props:{code:`from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("bigscience/T0_3B")
model = AutoModelForSeq2SeqLM.from_pretrained("bigscience/T0_3B")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bigscience/T0_3B&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;bigscience/T0_3B&quot;</span>)`}}),nt=new T({props:{code:`tokenizer.save_pretrained("./your/path/bigscience_t0")
model.save_pretrained("./your/path/bigscience_t0")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(<span class="hljs-string">&quot;./your/path/bigscience_t0&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;./your/path/bigscience_t0&quot;</span>)`}}),ft=new T({props:{code:`tokenizer = AutoTokenizer.from_pretrained("./your/path/bigscience_t0")
model = AutoModel.from_pretrained("./your/path/bigscience_t0")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./your/path/bigscience_t0&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;./your/path/bigscience_t0&quot;</span>)`}}),vt=new T({props:{code:"python -m pip install huggingface_hub",highlighted:"python -m pip install huggingface_hub"}}),gt=new T({props:{code:`from huggingface_hub import hf_hub_download

hf_hub_download(repo_id="bigscience/T0_3B", filename="config.json", cache_dir="./your/path/bigscience_t0")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> hf_hub_download

<span class="hljs-meta">&gt;&gt;&gt; </span>hf_hub_download(repo_id=<span class="hljs-string">&quot;bigscience/T0_3B&quot;</span>, filename=<span class="hljs-string">&quot;config.json&quot;</span>, cache_dir=<span class="hljs-string">&quot;./your/path/bigscience_t0&quot;</span>)`}}),yt=new T({props:{code:`from transformers import AutoConfig

config = AutoConfig.from_pretrained("./your/path/bigscience_t0/config.json")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./your/path/bigscience_t0/config.json&quot;</span>)`}}),ue=new Al({props:{$$slots:{default:[Ji]},$$scope:{ctx:N}}}),{c(){c=r("meta"),P=p(),d=r("h1"),b=r("a"),k=r("span"),u(y.$$.fragment),E=p(),A=r("span"),O=n("Installation"),C=p(),x=r("p"),j=n("Install \u{1F917} Transformers for whichever deep learning library you\u2019re working with, setup your cache, and optionally configure \u{1F917} Transformers to run offline."),F=p(),U=r("p"),bt=n("\u{1F917} Transformers is tested on Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+, and Flax. Follow the installation instructions below for the deep learning library you are using:"),we=p(),H=r("ul"),Tt=r("li"),ge=r("a"),Sl=n("PyTorch"),Cl=n(" installation instructions."),Ol=p(),kt=r("li"),ye=r("a"),Fl=n("TensorFlow 2.0"),Il=n(" installation instructions."),xl=p(),Pt=r("li"),Ee=r("a"),jl=n("Flax"),Ml=n(" installation instructions."),Xo=p(),Y=r("h2"),te=r("a"),so=r("span"),u(be.$$.fragment),Rl=p(),io=r("span"),Nl=n("Install with pip"),Jo=p(),D=r("p"),Hl=n("You should install \u{1F917} Transformers in a "),Te=r("a"),Dl=n("virtual environment"),Ll=n(". If you\u2019re unfamiliar with Python virtual environments, take a look at this "),ke=r("a"),ql=n("guide"),zl=n(". A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies."),Ko=p(),At=r("p"),Ul=n("Start by creating a virtual environment in your project directory:"),Qo=p(),u(Pe.$$.fragment),Zo=p(),St=r("p"),Yl=n("Activate the virtual environment. On Linux and MacOs:"),er=p(),u(Ae.$$.fragment),tr=p(),Ct=r("p"),Bl=n("Activate Virtual environment on Windows"),or=p(),u(Se.$$.fragment),rr=p(),Ot=r("p"),Wl=n("Now you\u2019re ready to install \u{1F917} Transformers with the following command:"),lr=p(),u(Ce.$$.fragment),ar=p(),Ft=r("p"),Gl=n("For CPU-support only, you can conveniently install \u{1F917} Transformers and a deep learning library in one line. For example, install \u{1F917} Transformers and PyTorch with:"),nr=p(),u(Oe.$$.fragment),sr=p(),It=r("p"),Vl=n("\u{1F917} Transformers and TensorFlow 2.0:"),ir=p(),u(Fe.$$.fragment),fr=p(),xt=r("p"),Xl=n("\u{1F917} Transformers and Flax:"),pr=p(),u(Ie.$$.fragment),hr=p(),jt=r("p"),Jl=n("Finally, check if \u{1F917} Transformers has been properly installed by running the following command. It will download a pretrained model:"),mr=p(),u(xe.$$.fragment),cr=p(),Mt=r("p"),Kl=n("Then print out the label and score:"),dr=p(),u(je.$$.fragment),ur=p(),B=r("h2"),oe=r("a"),fo=r("span"),u(Me.$$.fragment),Ql=p(),po=r("span"),Zl=n("Install from source"),vr=p(),Rt=r("p"),ea=n("Install \u{1F917} Transformers from source with the following command:"),_r=p(),u(Re.$$.fragment),$r=p(),S=r("p"),ta=n("This command installs the bleeding edge "),ho=r("code"),oa=n("main"),ra=n(" version rather than the latest "),mo=r("code"),la=n("stable"),aa=n(" version. The "),co=r("code"),na=n("main"),sa=n(" version is useful for staying up-to-date with the latest developments. For instance, if a bug has been fixed since the last official release but a new release hasn\u2019t been rolled out yet. However, this means the "),uo=r("code"),ia=n("main"),fa=n(" version may not always be stable. We strive to keep the "),vo=r("code"),pa=n("main"),ha=n(" version operational, and most issues are usually resolved within a few hours or a day. If you run into a problem, please open an "),Ne=r("a"),ma=n("Issue"),ca=n(" so we can fix it even sooner!"),wr=p(),Nt=r("p"),da=n("Check if \u{1F917} Transformers has been properly installed by running the following command:"),gr=p(),u(He.$$.fragment),yr=p(),W=r("h2"),re=r("a"),_o=r("span"),u(De.$$.fragment),ua=p(),$o=r("span"),va=n("Editable install"),Er=p(),Ht=r("p"),_a=n("You will need an editable install if you\u2019d like to:"),br=p(),le=r("ul"),Le=r("li"),$a=n("Use the "),wo=r("code"),wa=n("main"),ga=n(" version of the source code."),ya=p(),go=r("li"),Ea=n("Contribute to \u{1F917} Transformers and need to test changes in the code."),Tr=p(),Dt=r("p"),ba=n("Clone the repository and install \u{1F917} Transformers with the following commands:"),kr=p(),u(qe.$$.fragment),Pr=p(),L=r("p"),Ta=n("These commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in "),yo=r("code"),ka=n("~/anaconda3/envs/main/lib/python3.7/site-packages/"),Pa=n(", Python will also search the folder you cloned to: "),Eo=r("code"),Aa=n("~/transformers/"),Sa=n("."),Ar=p(),u(ae.$$.fragment),Sr=p(),Lt=r("p"),Ca=n("Now you can easily update your clone to the latest version of \u{1F917} Transformers with the following command:"),Cr=p(),u(ze.$$.fragment),Or=p(),ne=r("p"),Oa=n("Your Python environment will find the "),bo=r("code"),Fa=n("main"),Ia=n(" version of \u{1F917} Transformers on the next run."),Fr=p(),G=r("h2"),se=r("a"),To=r("span"),u(Ue.$$.fragment),xa=p(),ko=r("span"),ja=n("Install with conda"),Ir=p(),ie=r("p"),Ma=n("Install from the conda channel "),Po=r("code"),Ra=n("huggingface"),Na=n(":"),xr=p(),u(Ye.$$.fragment),jr=p(),V=r("h2"),fe=r("a"),Ao=r("span"),u(Be.$$.fragment),Ha=p(),So=r("span"),Da=n("Cache setup"),Mr=p(),M=r("p"),La=n("Pretrained models are downloaded and locally cached at: "),Co=r("code"),qa=n("~/.cache/huggingface/transformers/"),za=n(". This is the default directory given by the shell environment variable "),Oo=r("code"),Ua=n("TRANSFORMERS_CACHE"),Ya=n(". On Windows, the default directory is given by "),Fo=r("code"),Ba=n("C:\\Users\\username\\.cache\\huggingface\\transformers"),Wa=n(". You can change the shell environment variables shown below - in order of priority - to specify a different cache directory:"),Rr=p(),q=r("ol"),We=r("li"),Ga=n("Shell environment variable (default): "),Io=r("code"),Va=n("TRANSFORMERS_CACHE"),Xa=n("."),Ja=p(),X=r("li"),Ka=n("Shell environment variable: "),xo=r("code"),Qa=n("HF_HOME"),Za=n(" + "),jo=r("code"),en=n("transformers/"),tn=n("."),on=p(),J=r("li"),rn=n("Shell environment variable: "),Mo=r("code"),ln=n("XDG_CACHE_HOME"),an=n(" + "),Ro=r("code"),nn=n("/huggingface/transformers"),sn=n("."),Nr=p(),u(pe.$$.fragment),Hr=p(),K=r("h2"),he=r("a"),No=r("span"),u(Ge.$$.fragment),fn=p(),Ho=r("span"),pn=n("Offline mode"),Dr=p(),me=r("p"),hn=n("\u{1F917} Transformers is able to run in a firewalled or offline environment by only using local files. Set the environment variable "),Do=r("code"),mn=n("TRANSFORMERS_OFFLINE=1"),cn=n(" to enable this behavior."),Lr=p(),u(ce.$$.fragment),qr=p(),qt=r("p"),dn=n("For example, you would typically run a program on a normal network firewalled to external instances with the following command:"),zr=p(),u(Ve.$$.fragment),Ur=p(),zt=r("p"),un=n("Run this same program in an offline instance with:"),Yr=p(),u(Xe.$$.fragment),Br=p(),Ut=r("p"),vn=n("The script should now run without hanging or waiting to timeout because it knows it should only look for local files."),Wr=p(),Q=r("h3"),de=r("a"),Lo=r("span"),u(Je.$$.fragment),_n=p(),qo=r("span"),$n=n("Fetch models and tokenizers to use offline"),Gr=p(),Yt=r("p"),wn=n("Another option for using \u{1F917} Transformers offline is to download the files ahead of time, and then point to their local path when you need to use them offline. There are three ways to do this:"),Vr=p(),z=r("ul"),Ke=r("li"),Qe=r("p"),gn=n("Download a file through the user interface on the "),Ze=r("a"),yn=n("Model Hub"),En=n(" by clicking on the \u2193 icon."),bn=p(),zo=r("p"),Bt=r("img"),Tn=p(),et=r("li"),Z=r("p"),kn=n("Use the "),Wt=r("a"),Pn=n("PreTrainedModel.from_pretrained()"),An=n(" and "),Gt=r("a"),Sn=n("PreTrainedModel.save_pretrained()"),Cn=n(" workflow:"),On=p(),ee=r("ol"),tt=r("li"),ot=r("p"),Fn=n("Download your files ahead of time with "),Vt=r("a"),In=n("PreTrainedModel.from_pretrained()"),xn=n(":"),jn=p(),u(rt.$$.fragment),Mn=p(),lt=r("li"),at=r("p"),Rn=n("Save your files to a specified directory with "),Xt=r("a"),Nn=n("PreTrainedModel.save_pretrained()"),Hn=n(":"),Dn=p(),u(nt.$$.fragment),Ln=p(),st=r("li"),it=r("p"),qn=n("Now when you\u2019re offline, reload your files with "),Jt=r("a"),zn=n("PreTrainedModel.from_pretrained()"),Un=n(" from the specified directory:"),Yn=p(),u(ft.$$.fragment),Bn=p(),pt=r("li"),ht=r("p"),Wn=n("Programmatically download files with the "),mt=r("a"),Gn=n("huggingface_hub"),Vn=n(" library:"),Xn=p(),ct=r("ol"),dt=r("li"),ut=r("p"),Jn=n("Install the "),Uo=r("code"),Kn=n("huggingface_hub"),Qn=n(" library in your virtual environment:"),Zn=p(),u(vt.$$.fragment),es=p(),_t=r("li"),R=r("p"),ts=n("Use the "),$t=r("a"),Yo=r("code"),os=n("hf_hub_download"),rs=n(" function to download a file to a specific path. For example, the following command downloads the "),Bo=r("code"),ls=n("config.json"),as=n(" file from the "),wt=r("a"),ns=n("T0"),ss=n(" model to your desired path:"),is=p(),u(gt.$$.fragment),Xr=p(),Kt=r("p"),fs=n("Once your file is downloaded and locally cached, specify it\u2019s local path to load and use it:"),Jr=p(),u(yt.$$.fragment),Kr=p(),u(ue.$$.fragment),this.h()},l(e){const i=Yi('[data-svelte="svelte-1phssyn"]',document.head);c=l(i,"META",{name:!0,content:!0}),i.forEach(o),P=h(e),d=l(e,"H1",{class:!0});var Et=a(d);b=l(Et,"A",{id:!0,class:!0,href:!0});var Wo=a(b);k=l(Wo,"SPAN",{});var Go=a(k);v(y.$$.fragment,Go),Go.forEach(o),Wo.forEach(o),E=h(Et),A=l(Et,"SPAN",{});var Vo=a(A);O=s(Vo,"Installation"),Vo.forEach(o),Et.forEach(o),C=h(e),x=l(e,"P",{});var ds=a(x);j=s(ds,"Install \u{1F917} Transformers for whichever deep learning library you\u2019re working with, setup your cache, and optionally configure \u{1F917} Transformers to run offline."),ds.forEach(o),F=h(e),U=l(e,"P",{});var us=a(U);bt=s(us,"\u{1F917} Transformers is tested on Python 3.6+, PyTorch 1.1.0+, TensorFlow 2.0+, and Flax. Follow the installation instructions below for the deep learning library you are using:"),us.forEach(o),we=h(e),H=l(e,"UL",{});var Qt=a(H);Tt=l(Qt,"LI",{});var ps=a(Tt);ge=l(ps,"A",{href:!0,rel:!0});var vs=a(ge);Sl=s(vs,"PyTorch"),vs.forEach(o),Cl=s(ps," installation instructions."),ps.forEach(o),Ol=h(Qt),kt=l(Qt,"LI",{});var hs=a(kt);ye=l(hs,"A",{href:!0,rel:!0});var _s=a(ye);Fl=s(_s,"TensorFlow 2.0"),_s.forEach(o),Il=s(hs," installation instructions."),hs.forEach(o),xl=h(Qt),Pt=l(Qt,"LI",{});var ms=a(Pt);Ee=l(ms,"A",{href:!0,rel:!0});var $s=a(Ee);jl=s($s,"Flax"),$s.forEach(o),Ml=s(ms," installation instructions."),ms.forEach(o),Qt.forEach(o),Xo=h(e),Y=l(e,"H2",{class:!0});var Zr=a(Y);te=l(Zr,"A",{id:!0,class:!0,href:!0});var ws=a(te);so=l(ws,"SPAN",{});var gs=a(so);v(be.$$.fragment,gs),gs.forEach(o),ws.forEach(o),Rl=h(Zr),io=l(Zr,"SPAN",{});var ys=a(io);Nl=s(ys,"Install with pip"),ys.forEach(o),Zr.forEach(o),Jo=h(e),D=l(e,"P",{});var Zt=a(D);Hl=s(Zt,"You should install \u{1F917} Transformers in a "),Te=l(Zt,"A",{href:!0,rel:!0});var Es=a(Te);Dl=s(Es,"virtual environment"),Es.forEach(o),Ll=s(Zt,". If you\u2019re unfamiliar with Python virtual environments, take a look at this "),ke=l(Zt,"A",{href:!0,rel:!0});var bs=a(ke);ql=s(bs,"guide"),bs.forEach(o),zl=s(Zt,". A virtual environment makes it easier to manage different projects, and avoid compatibility issues between dependencies."),Zt.forEach(o),Ko=h(e),At=l(e,"P",{});var Ts=a(At);Ul=s(Ts,"Start by creating a virtual environment in your project directory:"),Ts.forEach(o),Qo=h(e),v(Pe.$$.fragment,e),Zo=h(e),St=l(e,"P",{});var ks=a(St);Yl=s(ks,"Activate the virtual environment. On Linux and MacOs:"),ks.forEach(o),er=h(e),v(Ae.$$.fragment,e),tr=h(e),Ct=l(e,"P",{});var Ps=a(Ct);Bl=s(Ps,"Activate Virtual environment on Windows"),Ps.forEach(o),or=h(e),v(Se.$$.fragment,e),rr=h(e),Ot=l(e,"P",{});var As=a(Ot);Wl=s(As,"Now you\u2019re ready to install \u{1F917} Transformers with the following command:"),As.forEach(o),lr=h(e),v(Ce.$$.fragment,e),ar=h(e),Ft=l(e,"P",{});var Ss=a(Ft);Gl=s(Ss,"For CPU-support only, you can conveniently install \u{1F917} Transformers and a deep learning library in one line. For example, install \u{1F917} Transformers and PyTorch with:"),Ss.forEach(o),nr=h(e),v(Oe.$$.fragment,e),sr=h(e),It=l(e,"P",{});var Cs=a(It);Vl=s(Cs,"\u{1F917} Transformers and TensorFlow 2.0:"),Cs.forEach(o),ir=h(e),v(Fe.$$.fragment,e),fr=h(e),xt=l(e,"P",{});var Os=a(xt);Xl=s(Os,"\u{1F917} Transformers and Flax:"),Os.forEach(o),pr=h(e),v(Ie.$$.fragment,e),hr=h(e),jt=l(e,"P",{});var Fs=a(jt);Jl=s(Fs,"Finally, check if \u{1F917} Transformers has been properly installed by running the following command. It will download a pretrained model:"),Fs.forEach(o),mr=h(e),v(xe.$$.fragment,e),cr=h(e),Mt=l(e,"P",{});var Is=a(Mt);Kl=s(Is,"Then print out the label and score:"),Is.forEach(o),dr=h(e),v(je.$$.fragment,e),ur=h(e),B=l(e,"H2",{class:!0});var el=a(B);oe=l(el,"A",{id:!0,class:!0,href:!0});var xs=a(oe);fo=l(xs,"SPAN",{});var js=a(fo);v(Me.$$.fragment,js),js.forEach(o),xs.forEach(o),Ql=h(el),po=l(el,"SPAN",{});var Ms=a(po);Zl=s(Ms,"Install from source"),Ms.forEach(o),el.forEach(o),vr=h(e),Rt=l(e,"P",{});var Rs=a(Rt);ea=s(Rs,"Install \u{1F917} Transformers from source with the following command:"),Rs.forEach(o),_r=h(e),v(Re.$$.fragment,e),$r=h(e),S=l(e,"P",{});var I=a(S);ta=s(I,"This command installs the bleeding edge "),ho=l(I,"CODE",{});var Ns=a(ho);oa=s(Ns,"main"),Ns.forEach(o),ra=s(I," version rather than the latest "),mo=l(I,"CODE",{});var Hs=a(mo);la=s(Hs,"stable"),Hs.forEach(o),aa=s(I," version. The "),co=l(I,"CODE",{});var Ds=a(co);na=s(Ds,"main"),Ds.forEach(o),sa=s(I," version is useful for staying up-to-date with the latest developments. For instance, if a bug has been fixed since the last official release but a new release hasn\u2019t been rolled out yet. However, this means the "),uo=l(I,"CODE",{});var Ls=a(uo);ia=s(Ls,"main"),Ls.forEach(o),fa=s(I," version may not always be stable. We strive to keep the "),vo=l(I,"CODE",{});var qs=a(vo);pa=s(qs,"main"),qs.forEach(o),ha=s(I," version operational, and most issues are usually resolved within a few hours or a day. If you run into a problem, please open an "),Ne=l(I,"A",{href:!0,rel:!0});var zs=a(Ne);ma=s(zs,"Issue"),zs.forEach(o),ca=s(I," so we can fix it even sooner!"),I.forEach(o),wr=h(e),Nt=l(e,"P",{});var Us=a(Nt);da=s(Us,"Check if \u{1F917} Transformers has been properly installed by running the following command:"),Us.forEach(o),gr=h(e),v(He.$$.fragment,e),yr=h(e),W=l(e,"H2",{class:!0});var tl=a(W);re=l(tl,"A",{id:!0,class:!0,href:!0});var Ys=a(re);_o=l(Ys,"SPAN",{});var Bs=a(_o);v(De.$$.fragment,Bs),Bs.forEach(o),Ys.forEach(o),ua=h(tl),$o=l(tl,"SPAN",{});var Ws=a($o);va=s(Ws,"Editable install"),Ws.forEach(o),tl.forEach(o),Er=h(e),Ht=l(e,"P",{});var Gs=a(Ht);_a=s(Gs,"You will need an editable install if you\u2019d like to:"),Gs.forEach(o),br=h(e),le=l(e,"UL",{});var ol=a(le);Le=l(ol,"LI",{});var rl=a(Le);$a=s(rl,"Use the "),wo=l(rl,"CODE",{});var Vs=a(wo);wa=s(Vs,"main"),Vs.forEach(o),ga=s(rl," version of the source code."),rl.forEach(o),ya=h(ol),go=l(ol,"LI",{});var Xs=a(go);Ea=s(Xs,"Contribute to \u{1F917} Transformers and need to test changes in the code."),Xs.forEach(o),ol.forEach(o),Tr=h(e),Dt=l(e,"P",{});var Js=a(Dt);ba=s(Js,"Clone the repository and install \u{1F917} Transformers with the following commands:"),Js.forEach(o),kr=h(e),v(qe.$$.fragment,e),Pr=h(e),L=l(e,"P",{});var eo=a(L);Ta=s(eo,"These commands will link the folder you cloned the repository to and your Python library paths. Python will now look inside the folder you cloned to in addition to the normal library paths. For example, if your Python packages are typically installed in "),yo=l(eo,"CODE",{});var Ks=a(yo);ka=s(Ks,"~/anaconda3/envs/main/lib/python3.7/site-packages/"),Ks.forEach(o),Pa=s(eo,", Python will also search the folder you cloned to: "),Eo=l(eo,"CODE",{});var Qs=a(Eo);Aa=s(Qs,"~/transformers/"),Qs.forEach(o),Sa=s(eo,"."),eo.forEach(o),Ar=h(e),v(ae.$$.fragment,e),Sr=h(e),Lt=l(e,"P",{});var Zs=a(Lt);Ca=s(Zs,"Now you can easily update your clone to the latest version of \u{1F917} Transformers with the following command:"),Zs.forEach(o),Cr=h(e),v(ze.$$.fragment,e),Or=h(e),ne=l(e,"P",{});var ll=a(ne);Oa=s(ll,"Your Python environment will find the "),bo=l(ll,"CODE",{});var ei=a(bo);Fa=s(ei,"main"),ei.forEach(o),Ia=s(ll," version of \u{1F917} Transformers on the next run."),ll.forEach(o),Fr=h(e),G=l(e,"H2",{class:!0});var al=a(G);se=l(al,"A",{id:!0,class:!0,href:!0});var ti=a(se);To=l(ti,"SPAN",{});var oi=a(To);v(Ue.$$.fragment,oi),oi.forEach(o),ti.forEach(o),xa=h(al),ko=l(al,"SPAN",{});var ri=a(ko);ja=s(ri,"Install with conda"),ri.forEach(o),al.forEach(o),Ir=h(e),ie=l(e,"P",{});var nl=a(ie);Ma=s(nl,"Install from the conda channel "),Po=l(nl,"CODE",{});var li=a(Po);Ra=s(li,"huggingface"),li.forEach(o),Na=s(nl,":"),nl.forEach(o),xr=h(e),v(Ye.$$.fragment,e),jr=h(e),V=l(e,"H2",{class:!0});var sl=a(V);fe=l(sl,"A",{id:!0,class:!0,href:!0});var ai=a(fe);Ao=l(ai,"SPAN",{});var ni=a(Ao);v(Be.$$.fragment,ni),ni.forEach(o),ai.forEach(o),Ha=h(sl),So=l(sl,"SPAN",{});var si=a(So);Da=s(si,"Cache setup"),si.forEach(o),sl.forEach(o),Mr=h(e),M=l(e,"P",{});var ve=a(M);La=s(ve,"Pretrained models are downloaded and locally cached at: "),Co=l(ve,"CODE",{});var ii=a(Co);qa=s(ii,"~/.cache/huggingface/transformers/"),ii.forEach(o),za=s(ve,". This is the default directory given by the shell environment variable "),Oo=l(ve,"CODE",{});var fi=a(Oo);Ua=s(fi,"TRANSFORMERS_CACHE"),fi.forEach(o),Ya=s(ve,". On Windows, the default directory is given by "),Fo=l(ve,"CODE",{});var pi=a(Fo);Ba=s(pi,"C:\\Users\\username\\.cache\\huggingface\\transformers"),pi.forEach(o),Wa=s(ve,". You can change the shell environment variables shown below - in order of priority - to specify a different cache directory:"),ve.forEach(o),Rr=h(e),q=l(e,"OL",{});var to=a(q);We=l(to,"LI",{});var il=a(We);Ga=s(il,"Shell environment variable (default): "),Io=l(il,"CODE",{});var hi=a(Io);Va=s(hi,"TRANSFORMERS_CACHE"),hi.forEach(o),Xa=s(il,"."),il.forEach(o),Ja=h(to),X=l(to,"LI",{});var oo=a(X);Ka=s(oo,"Shell environment variable: "),xo=l(oo,"CODE",{});var mi=a(xo);Qa=s(mi,"HF_HOME"),mi.forEach(o),Za=s(oo," + "),jo=l(oo,"CODE",{});var ci=a(jo);en=s(ci,"transformers/"),ci.forEach(o),tn=s(oo,"."),oo.forEach(o),on=h(to),J=l(to,"LI",{});var ro=a(J);rn=s(ro,"Shell environment variable: "),Mo=l(ro,"CODE",{});var di=a(Mo);ln=s(di,"XDG_CACHE_HOME"),di.forEach(o),an=s(ro," + "),Ro=l(ro,"CODE",{});var ui=a(Ro);nn=s(ui,"/huggingface/transformers"),ui.forEach(o),sn=s(ro,"."),ro.forEach(o),to.forEach(o),Nr=h(e),v(pe.$$.fragment,e),Hr=h(e),K=l(e,"H2",{class:!0});var fl=a(K);he=l(fl,"A",{id:!0,class:!0,href:!0});var vi=a(he);No=l(vi,"SPAN",{});var _i=a(No);v(Ge.$$.fragment,_i),_i.forEach(o),vi.forEach(o),fn=h(fl),Ho=l(fl,"SPAN",{});var $i=a(Ho);pn=s($i,"Offline mode"),$i.forEach(o),fl.forEach(o),Dr=h(e),me=l(e,"P",{});var pl=a(me);hn=s(pl,"\u{1F917} Transformers is able to run in a firewalled or offline environment by only using local files. Set the environment variable "),Do=l(pl,"CODE",{});var wi=a(Do);mn=s(wi,"TRANSFORMERS_OFFLINE=1"),wi.forEach(o),cn=s(pl," to enable this behavior."),pl.forEach(o),Lr=h(e),v(ce.$$.fragment,e),qr=h(e),qt=l(e,"P",{});var gi=a(qt);dn=s(gi,"For example, you would typically run a program on a normal network firewalled to external instances with the following command:"),gi.forEach(o),zr=h(e),v(Ve.$$.fragment,e),Ur=h(e),zt=l(e,"P",{});var yi=a(zt);un=s(yi,"Run this same program in an offline instance with:"),yi.forEach(o),Yr=h(e),v(Xe.$$.fragment,e),Br=h(e),Ut=l(e,"P",{});var Ei=a(Ut);vn=s(Ei,"The script should now run without hanging or waiting to timeout because it knows it should only look for local files."),Ei.forEach(o),Wr=h(e),Q=l(e,"H3",{class:!0});var hl=a(Q);de=l(hl,"A",{id:!0,class:!0,href:!0});var bi=a(de);Lo=l(bi,"SPAN",{});var Ti=a(Lo);v(Je.$$.fragment,Ti),Ti.forEach(o),bi.forEach(o),_n=h(hl),qo=l(hl,"SPAN",{});var ki=a(qo);$n=s(ki,"Fetch models and tokenizers to use offline"),ki.forEach(o),hl.forEach(o),Gr=h(e),Yt=l(e,"P",{});var Pi=a(Yt);wn=s(Pi,"Another option for using \u{1F917} Transformers offline is to download the files ahead of time, and then point to their local path when you need to use them offline. There are three ways to do this:"),Pi.forEach(o),Vr=h(e),z=l(e,"UL",{});var lo=a(z);Ke=l(lo,"LI",{});var ml=a(Ke);Qe=l(ml,"P",{});var cl=a(Qe);gn=s(cl,"Download a file through the user interface on the "),Ze=l(cl,"A",{href:!0,rel:!0});var Ai=a(Ze);yn=s(Ai,"Model Hub"),Ai.forEach(o),En=s(cl," by clicking on the \u2193 icon."),cl.forEach(o),bn=h(ml),zo=l(ml,"P",{});var Si=a(zo);Bt=l(Si,"IMG",{src:!0,alt:!0}),Si.forEach(o),ml.forEach(o),Tn=h(lo),et=l(lo,"LI",{});var dl=a(et);Z=l(dl,"P",{});var ao=a(Z);kn=s(ao,"Use the "),Wt=l(ao,"A",{href:!0});var Ci=a(Wt);Pn=s(Ci,"PreTrainedModel.from_pretrained()"),Ci.forEach(o),An=s(ao," and "),Gt=l(ao,"A",{href:!0});var Oi=a(Gt);Sn=s(Oi,"PreTrainedModel.save_pretrained()"),Oi.forEach(o),Cn=s(ao," workflow:"),ao.forEach(o),On=h(dl),ee=l(dl,"OL",{});var no=a(ee);tt=l(no,"LI",{});var ul=a(tt);ot=l(ul,"P",{});var vl=a(ot);Fn=s(vl,"Download your files ahead of time with "),Vt=l(vl,"A",{href:!0});var Fi=a(Vt);In=s(Fi,"PreTrainedModel.from_pretrained()"),Fi.forEach(o),xn=s(vl,":"),vl.forEach(o),jn=h(ul),v(rt.$$.fragment,ul),ul.forEach(o),Mn=h(no),lt=l(no,"LI",{});var _l=a(lt);at=l(_l,"P",{});var $l=a(at);Rn=s($l,"Save your files to a specified directory with "),Xt=l($l,"A",{href:!0});var Ii=a(Xt);Nn=s(Ii,"PreTrainedModel.save_pretrained()"),Ii.forEach(o),Hn=s($l,":"),$l.forEach(o),Dn=h(_l),v(nt.$$.fragment,_l),_l.forEach(o),Ln=h(no),st=l(no,"LI",{});var wl=a(st);it=l(wl,"P",{});var gl=a(it);qn=s(gl,"Now when you\u2019re offline, reload your files with "),Jt=l(gl,"A",{href:!0});var xi=a(Jt);zn=s(xi,"PreTrainedModel.from_pretrained()"),xi.forEach(o),Un=s(gl," from the specified directory:"),gl.forEach(o),Yn=h(wl),v(ft.$$.fragment,wl),wl.forEach(o),no.forEach(o),dl.forEach(o),Bn=h(lo),pt=l(lo,"LI",{});var yl=a(pt);ht=l(yl,"P",{});var El=a(ht);Wn=s(El,"Programmatically download files with the "),mt=l(El,"A",{href:!0,rel:!0});var ji=a(mt);Gn=s(ji,"huggingface_hub"),ji.forEach(o),Vn=s(El," library:"),El.forEach(o),Xn=h(yl),ct=l(yl,"OL",{});var bl=a(ct);dt=l(bl,"LI",{});var Tl=a(dt);ut=l(Tl,"P",{});var kl=a(ut);Jn=s(kl,"Install the "),Uo=l(kl,"CODE",{});var Mi=a(Uo);Kn=s(Mi,"huggingface_hub"),Mi.forEach(o),Qn=s(kl," library in your virtual environment:"),kl.forEach(o),Zn=h(Tl),v(vt.$$.fragment,Tl),Tl.forEach(o),es=h(bl),_t=l(bl,"LI",{});var Pl=a(_t);R=l(Pl,"P",{});var _e=a(R);ts=s(_e,"Use the "),$t=l(_e,"A",{href:!0,rel:!0});var Ri=a($t);Yo=l(Ri,"CODE",{});var Ni=a(Yo);os=s(Ni,"hf_hub_download"),Ni.forEach(o),Ri.forEach(o),rs=s(_e," function to download a file to a specific path. For example, the following command downloads the "),Bo=l(_e,"CODE",{});var Hi=a(Bo);ls=s(Hi,"config.json"),Hi.forEach(o),as=s(_e," file from the "),wt=l(_e,"A",{href:!0,rel:!0});var Di=a(wt);ns=s(Di,"T0"),Di.forEach(o),ss=s(_e," model to your desired path:"),_e.forEach(o),is=h(Pl),v(gt.$$.fragment,Pl),Pl.forEach(o),bl.forEach(o),yl.forEach(o),lo.forEach(o),Xr=h(e),Kt=l(e,"P",{});var Li=a(Kt);fs=s(Li,"Once your file is downloaded and locally cached, specify it\u2019s local path to load and use it:"),Li.forEach(o),Jr=h(e),v(yt.$$.fragment,e),Kr=h(e),v(ue.$$.fragment,e),this.h()},h(){m(c,"name","hf:doc:metadata"),m(c,"content",JSON.stringify(Qi)),m(b,"id","installation"),m(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(b,"href","#installation"),m(d,"class","relative group"),m(ge,"href","https://pytorch.org/get-started/locally/"),m(ge,"rel","nofollow"),m(ye,"href","https://www.tensorflow.org/install/pip"),m(ye,"rel","nofollow"),m(Ee,"href","https://flax.readthedocs.io/en/latest/"),m(Ee,"rel","nofollow"),m(te,"id","install-with-pip"),m(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(te,"href","#install-with-pip"),m(Y,"class","relative group"),m(Te,"href","https://docs.python.org/3/library/venv.html"),m(Te,"rel","nofollow"),m(ke,"href","https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/"),m(ke,"rel","nofollow"),m(oe,"id","install-from-source"),m(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(oe,"href","#install-from-source"),m(B,"class","relative group"),m(Ne,"href","https://github.com/huggingface/transformers/issues"),m(Ne,"rel","nofollow"),m(re,"id","editable-install"),m(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(re,"href","#editable-install"),m(W,"class","relative group"),m(se,"id","install-with-conda"),m(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(se,"href","#install-with-conda"),m(G,"class","relative group"),m(fe,"id","cache-setup"),m(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(fe,"href","#cache-setup"),m(V,"class","relative group"),m(he,"id","offline-mode"),m(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(he,"href","#offline-mode"),m(K,"class","relative group"),m(de,"id","fetch-models-and-tokenizers-to-use-offline"),m(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(de,"href","#fetch-models-and-tokenizers-to-use-offline"),m(Q,"class","relative group"),m(Ze,"href","https://huggingface.co/models"),m(Ze,"rel","nofollow"),Bi(Bt.src,cs="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/download-icon.png")||m(Bt,"src",cs),m(Bt,"alt","download-icon"),m(Wt,"href","/docs/transformers/v4.21.3/en/main_classes/model#transformers.PreTrainedModel.from_pretrained"),m(Gt,"href","/docs/transformers/v4.21.3/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),m(Vt,"href","/docs/transformers/v4.21.3/en/main_classes/model#transformers.PreTrainedModel.from_pretrained"),m(Xt,"href","/docs/transformers/v4.21.3/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),m(Jt,"href","/docs/transformers/v4.21.3/en/main_classes/model#transformers.PreTrainedModel.from_pretrained"),m(mt,"href","https://github.com/huggingface/huggingface_hub/tree/main/src/huggingface_hub"),m(mt,"rel","nofollow"),m($t,"href","https://huggingface.co/docs/hub/adding-a-library#download-files-from-the-hub"),m($t,"rel","nofollow"),m(wt,"href","https://huggingface.co/bigscience/T0_3B"),m(wt,"rel","nofollow")},m(e,i){t(document.head,c),f(e,P,i),f(e,d,i),t(d,b),t(b,k),_(y,k,null),t(d,E),t(d,A),t(A,O),f(e,C,i),f(e,x,i),t(x,j),f(e,F,i),f(e,U,i),t(U,bt),f(e,we,i),f(e,H,i),t(H,Tt),t(Tt,ge),t(ge,Sl),t(Tt,Cl),t(H,Ol),t(H,kt),t(kt,ye),t(ye,Fl),t(kt,Il),t(H,xl),t(H,Pt),t(Pt,Ee),t(Ee,jl),t(Pt,Ml),f(e,Xo,i),f(e,Y,i),t(Y,te),t(te,so),_(be,so,null),t(Y,Rl),t(Y,io),t(io,Nl),f(e,Jo,i),f(e,D,i),t(D,Hl),t(D,Te),t(Te,Dl),t(D,Ll),t(D,ke),t(ke,ql),t(D,zl),f(e,Ko,i),f(e,At,i),t(At,Ul),f(e,Qo,i),_(Pe,e,i),f(e,Zo,i),f(e,St,i),t(St,Yl),f(e,er,i),_(Ae,e,i),f(e,tr,i),f(e,Ct,i),t(Ct,Bl),f(e,or,i),_(Se,e,i),f(e,rr,i),f(e,Ot,i),t(Ot,Wl),f(e,lr,i),_(Ce,e,i),f(e,ar,i),f(e,Ft,i),t(Ft,Gl),f(e,nr,i),_(Oe,e,i),f(e,sr,i),f(e,It,i),t(It,Vl),f(e,ir,i),_(Fe,e,i),f(e,fr,i),f(e,xt,i),t(xt,Xl),f(e,pr,i),_(Ie,e,i),f(e,hr,i),f(e,jt,i),t(jt,Jl),f(e,mr,i),_(xe,e,i),f(e,cr,i),f(e,Mt,i),t(Mt,Kl),f(e,dr,i),_(je,e,i),f(e,ur,i),f(e,B,i),t(B,oe),t(oe,fo),_(Me,fo,null),t(B,Ql),t(B,po),t(po,Zl),f(e,vr,i),f(e,Rt,i),t(Rt,ea),f(e,_r,i),_(Re,e,i),f(e,$r,i),f(e,S,i),t(S,ta),t(S,ho),t(ho,oa),t(S,ra),t(S,mo),t(mo,la),t(S,aa),t(S,co),t(co,na),t(S,sa),t(S,uo),t(uo,ia),t(S,fa),t(S,vo),t(vo,pa),t(S,ha),t(S,Ne),t(Ne,ma),t(S,ca),f(e,wr,i),f(e,Nt,i),t(Nt,da),f(e,gr,i),_(He,e,i),f(e,yr,i),f(e,W,i),t(W,re),t(re,_o),_(De,_o,null),t(W,ua),t(W,$o),t($o,va),f(e,Er,i),f(e,Ht,i),t(Ht,_a),f(e,br,i),f(e,le,i),t(le,Le),t(Le,$a),t(Le,wo),t(wo,wa),t(Le,ga),t(le,ya),t(le,go),t(go,Ea),f(e,Tr,i),f(e,Dt,i),t(Dt,ba),f(e,kr,i),_(qe,e,i),f(e,Pr,i),f(e,L,i),t(L,Ta),t(L,yo),t(yo,ka),t(L,Pa),t(L,Eo),t(Eo,Aa),t(L,Sa),f(e,Ar,i),_(ae,e,i),f(e,Sr,i),f(e,Lt,i),t(Lt,Ca),f(e,Cr,i),_(ze,e,i),f(e,Or,i),f(e,ne,i),t(ne,Oa),t(ne,bo),t(bo,Fa),t(ne,Ia),f(e,Fr,i),f(e,G,i),t(G,se),t(se,To),_(Ue,To,null),t(G,xa),t(G,ko),t(ko,ja),f(e,Ir,i),f(e,ie,i),t(ie,Ma),t(ie,Po),t(Po,Ra),t(ie,Na),f(e,xr,i),_(Ye,e,i),f(e,jr,i),f(e,V,i),t(V,fe),t(fe,Ao),_(Be,Ao,null),t(V,Ha),t(V,So),t(So,Da),f(e,Mr,i),f(e,M,i),t(M,La),t(M,Co),t(Co,qa),t(M,za),t(M,Oo),t(Oo,Ua),t(M,Ya),t(M,Fo),t(Fo,Ba),t(M,Wa),f(e,Rr,i),f(e,q,i),t(q,We),t(We,Ga),t(We,Io),t(Io,Va),t(We,Xa),t(q,Ja),t(q,X),t(X,Ka),t(X,xo),t(xo,Qa),t(X,Za),t(X,jo),t(jo,en),t(X,tn),t(q,on),t(q,J),t(J,rn),t(J,Mo),t(Mo,ln),t(J,an),t(J,Ro),t(Ro,nn),t(J,sn),f(e,Nr,i),_(pe,e,i),f(e,Hr,i),f(e,K,i),t(K,he),t(he,No),_(Ge,No,null),t(K,fn),t(K,Ho),t(Ho,pn),f(e,Dr,i),f(e,me,i),t(me,hn),t(me,Do),t(Do,mn),t(me,cn),f(e,Lr,i),_(ce,e,i),f(e,qr,i),f(e,qt,i),t(qt,dn),f(e,zr,i),_(Ve,e,i),f(e,Ur,i),f(e,zt,i),t(zt,un),f(e,Yr,i),_(Xe,e,i),f(e,Br,i),f(e,Ut,i),t(Ut,vn),f(e,Wr,i),f(e,Q,i),t(Q,de),t(de,Lo),_(Je,Lo,null),t(Q,_n),t(Q,qo),t(qo,$n),f(e,Gr,i),f(e,Yt,i),t(Yt,wn),f(e,Vr,i),f(e,z,i),t(z,Ke),t(Ke,Qe),t(Qe,gn),t(Qe,Ze),t(Ze,yn),t(Qe,En),t(Ke,bn),t(Ke,zo),t(zo,Bt),t(z,Tn),t(z,et),t(et,Z),t(Z,kn),t(Z,Wt),t(Wt,Pn),t(Z,An),t(Z,Gt),t(Gt,Sn),t(Z,Cn),t(et,On),t(et,ee),t(ee,tt),t(tt,ot),t(ot,Fn),t(ot,Vt),t(Vt,In),t(ot,xn),t(tt,jn),_(rt,tt,null),t(ee,Mn),t(ee,lt),t(lt,at),t(at,Rn),t(at,Xt),t(Xt,Nn),t(at,Hn),t(lt,Dn),_(nt,lt,null),t(ee,Ln),t(ee,st),t(st,it),t(it,qn),t(it,Jt),t(Jt,zn),t(it,Un),t(st,Yn),_(ft,st,null),t(z,Bn),t(z,pt),t(pt,ht),t(ht,Wn),t(ht,mt),t(mt,Gn),t(ht,Vn),t(pt,Xn),t(pt,ct),t(ct,dt),t(dt,ut),t(ut,Jn),t(ut,Uo),t(Uo,Kn),t(ut,Qn),t(dt,Zn),_(vt,dt,null),t(ct,es),t(ct,_t),t(_t,R),t(R,ts),t(R,$t),t($t,Yo),t(Yo,os),t(R,rs),t(R,Bo),t(Bo,ls),t(R,as),t(R,wt),t(wt,ns),t(R,ss),t(_t,is),_(gt,_t,null),f(e,Xr,i),f(e,Kt,i),t(Kt,fs),f(e,Jr,i),_(yt,e,i),f(e,Kr,i),_(ue,e,i),Qr=!0},p(e,[i]){const Et={};i&2&&(Et.$$scope={dirty:i,ctx:e}),ae.$set(Et);const Wo={};i&2&&(Wo.$$scope={dirty:i,ctx:e}),pe.$set(Wo);const Go={};i&2&&(Go.$$scope={dirty:i,ctx:e}),ce.$set(Go);const Vo={};i&2&&(Vo.$$scope={dirty:i,ctx:e}),ue.$set(Vo)},i(e){Qr||($(y.$$.fragment,e),$(be.$$.fragment,e),$(Pe.$$.fragment,e),$(Ae.$$.fragment,e),$(Se.$$.fragment,e),$(Ce.$$.fragment,e),$(Oe.$$.fragment,e),$(Fe.$$.fragment,e),$(Ie.$$.fragment,e),$(xe.$$.fragment,e),$(je.$$.fragment,e),$(Me.$$.fragment,e),$(Re.$$.fragment,e),$(He.$$.fragment,e),$(De.$$.fragment,e),$(qe.$$.fragment,e),$(ae.$$.fragment,e),$(ze.$$.fragment,e),$(Ue.$$.fragment,e),$(Ye.$$.fragment,e),$(Be.$$.fragment,e),$(pe.$$.fragment,e),$(Ge.$$.fragment,e),$(ce.$$.fragment,e),$(Ve.$$.fragment,e),$(Xe.$$.fragment,e),$(Je.$$.fragment,e),$(rt.$$.fragment,e),$(nt.$$.fragment,e),$(ft.$$.fragment,e),$(vt.$$.fragment,e),$(gt.$$.fragment,e),$(yt.$$.fragment,e),$(ue.$$.fragment,e),Qr=!0)},o(e){w(y.$$.fragment,e),w(be.$$.fragment,e),w(Pe.$$.fragment,e),w(Ae.$$.fragment,e),w(Se.$$.fragment,e),w(Ce.$$.fragment,e),w(Oe.$$.fragment,e),w(Fe.$$.fragment,e),w(Ie.$$.fragment,e),w(xe.$$.fragment,e),w(je.$$.fragment,e),w(Me.$$.fragment,e),w(Re.$$.fragment,e),w(He.$$.fragment,e),w(De.$$.fragment,e),w(qe.$$.fragment,e),w(ae.$$.fragment,e),w(ze.$$.fragment,e),w(Ue.$$.fragment,e),w(Ye.$$.fragment,e),w(Be.$$.fragment,e),w(pe.$$.fragment,e),w(Ge.$$.fragment,e),w(ce.$$.fragment,e),w(Ve.$$.fragment,e),w(Xe.$$.fragment,e),w(Je.$$.fragment,e),w(rt.$$.fragment,e),w(nt.$$.fragment,e),w(ft.$$.fragment,e),w(vt.$$.fragment,e),w(gt.$$.fragment,e),w(yt.$$.fragment,e),w(ue.$$.fragment,e),Qr=!1},d(e){o(c),e&&o(P),e&&o(d),g(y),e&&o(C),e&&o(x),e&&o(F),e&&o(U),e&&o(we),e&&o(H),e&&o(Xo),e&&o(Y),g(be),e&&o(Jo),e&&o(D),e&&o(Ko),e&&o(At),e&&o(Qo),g(Pe,e),e&&o(Zo),e&&o(St),e&&o(er),g(Ae,e),e&&o(tr),e&&o(Ct),e&&o(or),g(Se,e),e&&o(rr),e&&o(Ot),e&&o(lr),g(Ce,e),e&&o(ar),e&&o(Ft),e&&o(nr),g(Oe,e),e&&o(sr),e&&o(It),e&&o(ir),g(Fe,e),e&&o(fr),e&&o(xt),e&&o(pr),g(Ie,e),e&&o(hr),e&&o(jt),e&&o(mr),g(xe,e),e&&o(cr),e&&o(Mt),e&&o(dr),g(je,e),e&&o(ur),e&&o(B),g(Me),e&&o(vr),e&&o(Rt),e&&o(_r),g(Re,e),e&&o($r),e&&o(S),e&&o(wr),e&&o(Nt),e&&o(gr),g(He,e),e&&o(yr),e&&o(W),g(De),e&&o(Er),e&&o(Ht),e&&o(br),e&&o(le),e&&o(Tr),e&&o(Dt),e&&o(kr),g(qe,e),e&&o(Pr),e&&o(L),e&&o(Ar),g(ae,e),e&&o(Sr),e&&o(Lt),e&&o(Cr),g(ze,e),e&&o(Or),e&&o(ne),e&&o(Fr),e&&o(G),g(Ue),e&&o(Ir),e&&o(ie),e&&o(xr),g(Ye,e),e&&o(jr),e&&o(V),g(Be),e&&o(Mr),e&&o(M),e&&o(Rr),e&&o(q),e&&o(Nr),g(pe,e),e&&o(Hr),e&&o(K),g(Ge),e&&o(Dr),e&&o(me),e&&o(Lr),g(ce,e),e&&o(qr),e&&o(qt),e&&o(zr),g(Ve,e),e&&o(Ur),e&&o(zt),e&&o(Yr),g(Xe,e),e&&o(Br),e&&o(Ut),e&&o(Wr),e&&o(Q),g(Je),e&&o(Gr),e&&o(Yt),e&&o(Vr),e&&o(z),g(rt),g(nt),g(ft),g(vt),g(gt),e&&o(Xr),e&&o(Kt),e&&o(Jr),g(yt,e),e&&o(Kr),g(ue,e)}}}const Qi={local:"installation",sections:[{local:"install-with-pip",title:"Install with pip"},{local:"install-from-source",title:"Install from source"},{local:"editable-install",title:"Editable install"},{local:"install-with-conda",title:"Install with conda"},{local:"cache-setup",title:"Cache setup"},{local:"offline-mode",sections:[{local:"fetch-models-and-tokenizers-to-use-offline",title:"Fetch models and tokenizers to use offline"}],title:"Offline mode"}],title:"Installation"};function Zi(N){return Wi(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class lf extends qi{constructor(c){super();zi(this,c,Zi,Ki,Ui,{})}}export{lf as default,Qi as metadata};
