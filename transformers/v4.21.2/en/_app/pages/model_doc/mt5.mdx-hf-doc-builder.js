import{S as Wd,i as Bd,s as Hd,e as r,k as p,w as k,t as l,M as Rd,c as n,d as s,m,a as o,x as T,h as i,b as d,G as t,g as f,y as $,q as b,o as w,B as y,v as Xd,L as me}from"../../chunks/vendor-hf-doc-builder.js";import{D as E}from"../../chunks/Docstring-hf-doc-builder.js";import{C as ce}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as P}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as pe}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Kd(M){let h,x,_,u,v;return u=new ce({props:{code:`from transformers import MT5Model, T5Tokenizer

model = MT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="pt")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="pt")

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=labels["input_ids"])
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),{c(){h=r("p"),x=l("Examples:"),_=p(),k(u.$$.fragment)},l(a){h=n(a,"P",{});var g=o(h);x=i(g,"Examples:"),g.forEach(s),_=m(a),T(u.$$.fragment,a)},m(a,g){f(a,h,g),t(h,x),f(a,_,g),$(u,a,g),v=!0},p:me,i(a){v||(b(u.$$.fragment,a),v=!0)},o(a){w(u.$$.fragment,a),v=!1},d(a){a&&s(h),a&&s(_),y(u,a)}}}function Jd(M){let h,x,_,u,v;return u=new ce({props:{code:`from transformers import MT5ForConditionalGeneration, T5Tokenizer

model = MT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="pt")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="pt")

outputs = model(**inputs, labels=labels["input_ids"])
loss = outputs.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),{c(){h=r("p"),x=l("Examples:"),_=p(),k(u.$$.fragment)},l(a){h=n(a,"P",{});var g=o(h);x=i(g,"Examples:"),g.forEach(s),_=m(a),T(u.$$.fragment,a)},m(a,g){f(a,h,g),t(h,x),f(a,_,g),$(u,a,g),v=!0},p:me,i(a){v||(b(u.$$.fragment,a),v=!0)},o(a){w(u.$$.fragment,a),v=!1},d(a){a&&s(h),a&&s(_),y(u,a)}}}function Qd(M){let h,x,_,u,v;return u=new ce({props:{code:`from transformers import MT5EncoderModel, T5Tokenizer

model = MT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="pt").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5EncoderModel, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_state = outputs.last_hidden_state`}}),{c(){h=r("p"),x=l("Examples:"),_=p(),k(u.$$.fragment)},l(a){h=n(a,"P",{});var g=o(h);x=i(g,"Examples:"),g.forEach(s),_=m(a),T(u.$$.fragment,a)},m(a,g){f(a,h,g),t(h,x),f(a,_,g),$(u,a,g),v=!0},p:me,i(a){v||(b(u.$$.fragment,a),v=!0)},o(a){w(u.$$.fragment,a),v=!1},d(a){a&&s(h),a&&s(_),y(u,a)}}}function Yd(M){let h,x,_,u,v;return u=new ce({props:{code:`from transformers import TFMT5Model, T5Tokenizer

model = TFMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="tf")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="tf")

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=labels["input_ids"])
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),{c(){h=r("p"),x=l("Examples:"),_=p(),k(u.$$.fragment)},l(a){h=n(a,"P",{});var g=o(h);x=i(g,"Examples:"),g.forEach(s),_=m(a),T(u.$$.fragment,a)},m(a,g){f(a,h,g),t(h,x),f(a,_,g),$(u,a,g),v=!0},p:me,i(a){v||(b(u.$$.fragment,a),v=!0)},o(a){w(u.$$.fragment,a),v=!1},d(a){a&&s(h),a&&s(_),y(u,a)}}}function Zd(M){let h,x,_,u,v;return u=new ce({props:{code:`from transformers import TFMT5ForConditionalGeneration, T5Tokenizer

model = TFMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="tf")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="tf")

outputs = model(**inputs, labels=labels["input_ids"])
loss = outputs.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),{c(){h=r("p"),x=l("Examples:"),_=p(),k(u.$$.fragment)},l(a){h=n(a,"P",{});var g=o(h);x=i(g,"Examples:"),g.forEach(s),_=m(a),T(u.$$.fragment,a)},m(a,g){f(a,h,g),t(h,x),f(a,_,g),$(u,a,g),v=!0},p:me,i(a){v||(b(u.$$.fragment,a),v=!0)},o(a){w(u.$$.fragment,a),v=!1},d(a){a&&s(h),a&&s(_),y(u,a)}}}function ep(M){let h,x,_,u,v;return u=new ce({props:{code:`from transformers import TFMT5EncoderModel, T5Tokenizer

model = TFMT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="tf").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5EncoderModel, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_state = outputs.last_hidden_state`}}),{c(){h=r("p"),x=l("Examples:"),_=p(),k(u.$$.fragment)},l(a){h=n(a,"P",{});var g=o(h);x=i(g,"Examples:"),g.forEach(s),_=m(a),T(u.$$.fragment,a)},m(a,g){f(a,h,g),t(h,x),f(a,_,g),$(u,a,g),v=!0},p:me,i(a){v||(b(u.$$.fragment,a),v=!0)},o(a){w(u.$$.fragment,a),v=!1},d(a){a&&s(h),a&&s(_),y(u,a)}}}function tp(M){let h,x,_,u,v;return u=new ce({props:{code:`from transformers import FlaxMT5Model, T5Tokenizer

model = FlaxMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

with tokenizer.as_target_tokenizer():
    decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=decoder_input_ids)
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxMT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxMT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    decoder_input_ids = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),{c(){h=r("p"),x=l("Examples:"),_=p(),k(u.$$.fragment)},l(a){h=n(a,"P",{});var g=o(h);x=i(g,"Examples:"),g.forEach(s),_=m(a),T(u.$$.fragment,a)},m(a,g){f(a,h,g),t(h,x),f(a,_,g),$(u,a,g),v=!0},p:me,i(a){v||(b(u.$$.fragment,a),v=!0)},o(a){w(u.$$.fragment,a),v=!1},d(a){a&&s(h),a&&s(_),y(u,a)}}}function sp(M){let h,x,_,u,v;return u=new ce({props:{code:`from transformers import FlaxMT5ForConditionalGeneration, T5Tokenizer

model = FlaxMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

with tokenizer.as_target_tokenizer():
    decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids

outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxMT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxMT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    decoder_input_ids = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){h=r("p"),x=l("Examples:"),_=p(),k(u.$$.fragment)},l(a){h=n(a,"P",{});var g=o(h);x=i(g,"Examples:"),g.forEach(s),_=m(a),T(u.$$.fragment,a)},m(a,g){f(a,h,g),t(h,x),f(a,_,g),$(u,a,g),v=!0},p:me,i(a){v||(b(u.$$.fragment,a),v=!0)},o(a){w(u.$$.fragment,a),v=!1},d(a){a&&s(h),a&&s(_),y(u,a)}}}function rp(M){let h,x,_,u,v;return u=new ce({props:{code:`from transformers import FlaxT5EncoderModel, T5Tokenizer

model = FlaxT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

with tokenizer.as_target_tokenizer():
    decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids

outputs = model(input_ids=inputs["input_ids"])
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxT5EncoderModel, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    decoder_input_ids = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),{c(){h=r("p"),x=l("Examples:"),_=p(),k(u.$$.fragment)},l(a){h=n(a,"P",{});var g=o(h);x=i(g,"Examples:"),g.forEach(s),_=m(a),T(u.$$.fragment,a)},m(a,g){f(a,h,g),t(h,x),f(a,_,g),$(u,a,g),v=!0},p:me,i(a){v||(b(u.$$.fragment,a),v=!0)},o(a){w(u.$$.fragment,a),v=!1},d(a){a&&s(h),a&&s(_),y(u,a)}}}function np(M){let h,x,_,u,v,a,g,Ns,Zn,Nr,K,fe,Ds,We,eo,Is,to,Dr,ue,so,Be,ro,no,Ir,Qt,oo,Gr,Yt,Gs,ao,Or,he,lo,He,io,po,Vr,Zt,mo,Ur,F,Os,Vs,Re,co,fo,Us,Ws,Xe,uo,ho,Bs,Hs,Ke,go,_o,Rs,Xs,Je,vo,ko,Ks,es,Qe,To,$o,Wr,W,bo,Ye,wo,yo,Ze,xo,Mo,Br,J,ge,Js,et,zo,Qs,Eo,Hr,C,tt,qo,A,Fo,ts,jo,Po,ss,Co,Ao,st,So,Lo,No,Q,Do,rs,Io,Go,ns,Oo,Vo,Rr,Y,_e,Ys,rt,Uo,Zs,Wo,Xr,z,nt,Bo,ot,Ho,at,Ro,Xo,Ko,lt,Jo,os,Qo,Yo,Zo,B,it,ea,er,ta,sa,dt,as,ra,tr,na,oa,ls,aa,sr,la,ia,ve,pt,da,rr,pa,ma,ke,mt,ca,nr,fa,ua,Te,ct,ha,ft,ga,or,_a,va,Kr,$e,ka,is,Ta,$a,Jr,Z,be,ar,ut,ba,lr,wa,Qr,q,ht,ya,ee,xa,ir,Ma,za,gt,Ea,qa,Fa,_t,ja,ds,Pa,Ca,Aa,H,vt,Sa,dr,La,Na,kt,ps,Da,pr,Ia,Ga,ms,Oa,mr,Va,Ua,we,Tt,Wa,cr,Ba,Yr,ye,Ha,cs,Ra,Xa,Zr,te,xe,fr,$t,Ka,ur,Ja,en,S,bt,Qa,wt,Ya,fs,Za,el,tl,Me,tn,se,ze,hr,yt,sl,gr,rl,sn,L,xt,nl,Mt,ol,us,al,ll,il,Ee,rn,re,qe,_r,zt,dl,vr,pl,nn,N,Et,ml,qt,cl,hs,fl,ul,hl,Fe,on,ne,je,kr,Ft,gl,Tr,_l,an,D,jt,vl,Pt,kl,gs,Tl,$l,bl,Pe,ln,oe,Ce,$r,Ct,wl,br,yl,dn,I,At,xl,St,Ml,_s,zl,El,ql,Ae,pn,ae,Se,wr,Lt,Fl,yr,jl,mn,G,Nt,Pl,Dt,Cl,vs,Al,Sl,Ll,Le,cn,le,Ne,xr,It,Nl,Mr,Dl,fn,O,Gt,Il,Ot,Gl,ks,Ol,Vl,Ul,De,un,ie,Ie,zr,Vt,Wl,Er,Bl,hn,V,Ut,Hl,Wt,Rl,Ts,Xl,Kl,Jl,Ge,gn,de,Oe,qr,Bt,Ql,Fr,Yl,_n,U,Ht,Zl,Rt,ei,$s,ti,si,ri,Ve,vn;return a=new P({}),We=new P({}),et=new P({}),tt=new E({props:{name:"class transformers.MT5Config",anchor:"transformers.MT5Config",parameters:[{name:"vocab_size",val:" = 250112"},{name:"d_model",val:" = 512"},{name:"d_kv",val:" = 64"},{name:"d_ff",val:" = 1024"},{name:"num_layers",val:" = 8"},{name:"num_decoder_layers",val:" = None"},{name:"num_heads",val:" = 6"},{name:"relative_attention_num_buckets",val:" = 32"},{name:"relative_attention_max_distance",val:" = 128"},{name:"dropout_rate",val:" = 0.1"},{name:"layer_norm_epsilon",val:" = 1e-06"},{name:"initializer_factor",val:" = 1.0"},{name:"feed_forward_proj",val:" = 'gated-gelu'"},{name:"is_encoder_decoder",val:" = True"},{name:"use_cache",val:" = True"},{name:"tokenizer_class",val:" = 'T5Tokenizer'"},{name:"tie_word_embeddings",val:" = False"},{name:"pad_token_id",val:" = 0"},{name:"eos_token_id",val:" = 1"},{name:"decoder_start_token_id",val:" = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MT5Config.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 250112) &#x2014;
Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.21.2/en/model_doc/t5#transformers.T5Model">T5Model</a> or <a href="/docs/transformers/v4.21.2/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a>.`,name:"vocab_size"},{anchor:"transformers.MT5Config.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Size of the encoder layers and the pooler layer.`,name:"d_model"},{anchor:"transformers.MT5Config.d_kv",description:`<strong>d_kv</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Size of the key, query, value projections per attention head. <code>d_kv</code> has to be equal to <code>d_model // num_heads</code>.`,name:"d_kv"},{anchor:"transformers.MT5Config.d_ff",description:`<strong>d_ff</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Size of the intermediate feed forward layer in each <code>T5Block</code>.`,name:"d_ff"},{anchor:"transformers.MT5Config.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_layers"},{anchor:"transformers.MT5Config.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Number of hidden layers in the Transformer decoder. Will use the same value as <code>num_layers</code> if not set.`,name:"num_decoder_layers"},{anchor:"transformers.MT5Config.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_heads"},{anchor:"transformers.MT5Config.relative_attention_num_buckets",description:`<strong>relative_attention_num_buckets</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of buckets to use for each attention layer.`,name:"relative_attention_num_buckets"},{anchor:"transformers.MT5Config.relative_attention_max_distance",description:`<strong>relative_attention_max_distance</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
The maximum distance of the longer sequences for the bucket separation.`,name:"relative_attention_max_distance"},{anchor:"transformers.MT5Config.dropout_rate",description:`<strong>dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The ratio for all dropout layers.`,name:"dropout_rate"},{anchor:"transformers.MT5Config.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-6) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.MT5Config.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.MT5Config.feed_forward_proj",description:`<strong>feed_forward_proj</strong> (<code>string</code>, <em>optional</em>, defaults to <code>&quot;gated-gelu&quot;</code>) &#x2014;
Type of feed forward layer to be used. Should be one of <code>&quot;relu&quot;</code> or <code>&quot;gated-gelu&quot;</code>.`,name:"feed_forward_proj"},{anchor:"transformers.MT5Config.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/mt5/configuration_mt5.py#L24"}}),rt=new P({}),nt=new E({props:{name:"class transformers.T5Tokenizer",anchor:"transformers.T5Tokenizer",parameters:[{name:"vocab_file",val:""},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 100"},{name:"additional_special_tokens",val:" = None"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.T5Tokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.T5Tokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.T5Tokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.T5Tokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.T5Tokenizer.extra_ids",description:`<strong>extra_ids</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
accessible as &#x201C;<extra<em>id{%d}&gt;&#x201D; where &#x201D;{%d}&#x201D; is a number between 0 and extra_ids-1. Extra tokens are
indexed from the end of the vocabulary up to beginning (&#x201C;<extra_id_0>&#x201D; is the last token in the vocabulary
like in T5 preprocessing see
<a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117" rel="nofollow">here</a>).</extra_id_0></extra<em>`,name:"extra_ids"},{anchor:"transformers.T5Tokenizer.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"},{anchor:"transformers.T5Tokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"},{anchor:"transformers.T5Tokenizer.sp_model",description:`<strong>sp_model</strong> (<code>SentencePieceProcessor</code>) &#x2014;
The <em>SentencePiece</em> processor that is used for every conversion (string, tokens and IDs).`,name:"sp_model"}],source:"https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/t5/tokenization_t5.py#L55"}}),it=new E({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/t5/tokenization_t5.py#L249",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),pt=new E({props:{name:"convert_tokens_to_string",anchor:"transformers.T5Tokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/t5/tokenization_t5.py#L310"}}),mt=new E({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/t5/tokenization_t5.py#L227",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ct=new E({props:{name:"get_special_tokens_mask",anchor:"transformers.T5Tokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/t5/tokenization_t5.py#L188",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ut=new P({}),ht=new E({props:{name:"class transformers.T5TokenizerFast",anchor:"transformers.T5TokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 100"},{name:"additional_special_tokens",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.T5TokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.T5TokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.T5TokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.T5TokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.T5TokenizerFast.extra_ids",description:`<strong>extra_ids</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
accessible as &#x201C;<extra<em>id{%d}&gt;&#x201D; where &#x201D;{%d}&#x201D; is a number between 0 and extra_ids-1. Extra tokens are
indexed from the end of the vocabulary up to beginning (&#x201C;<extra_id_0>&#x201D; is the last token in the vocabulary
like in T5 preprocessing see
<a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117" rel="nofollow">here</a>).</extra_id_0></extra<em>`,name:"extra_ids"},{anchor:"transformers.T5TokenizerFast.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/t5/tokenization_t5_fast.py#L65"}}),vt=new E({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/t5/tokenization_t5_fast.py#L191",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Tt=new E({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/t5/tokenization_t5_fast.py#L217",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),$t=new P({}),bt=new E({props:{name:"class transformers.MT5Model",anchor:"transformers.MT5Model",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/mt5/modeling_mt5.py#L28"}}),Me=new pe({props:{anchor:"transformers.MT5Model.example",$$slots:{default:[Kd]},$$scope:{ctx:M}}}),yt=new P({}),xt=new E({props:{name:"class transformers.MT5ForConditionalGeneration",anchor:"transformers.MT5ForConditionalGeneration",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/mt5/modeling_mt5.py#L62"}}),Ee=new pe({props:{anchor:"transformers.MT5ForConditionalGeneration.example",$$slots:{default:[Jd]},$$scope:{ctx:M}}}),zt=new P({}),Et=new E({props:{name:"class transformers.MT5EncoderModel",anchor:"transformers.MT5EncoderModel",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/mt5/modeling_mt5.py#L94"}}),Fe=new pe({props:{anchor:"transformers.MT5EncoderModel.example",$$slots:{default:[Qd]},$$scope:{ctx:M}}}),Ft=new P({}),jt=new E({props:{name:"class transformers.TFMT5Model",anchor:"transformers.TFMT5Model",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/mt5/modeling_tf_mt5.py#L28"}}),Pe=new pe({props:{anchor:"transformers.TFMT5Model.example",$$slots:{default:[Yd]},$$scope:{ctx:M}}}),Ct=new P({}),At=new E({props:{name:"class transformers.TFMT5ForConditionalGeneration",anchor:"transformers.TFMT5ForConditionalGeneration",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/mt5/modeling_tf_mt5.py#L53"}}),Ae=new pe({props:{anchor:"transformers.TFMT5ForConditionalGeneration.example",$$slots:{default:[Zd]},$$scope:{ctx:M}}}),Lt=new P({}),Nt=new E({props:{name:"class transformers.TFMT5EncoderModel",anchor:"transformers.TFMT5EncoderModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/mt5/modeling_tf_mt5.py#L79"}}),Le=new pe({props:{anchor:"transformers.TFMT5EncoderModel.example",$$slots:{default:[ep]},$$scope:{ctx:M}}}),It=new P({}),Gt=new E({props:{name:"class transformers.FlaxMT5Model",anchor:"transformers.FlaxMT5Model",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/mt5/modeling_flax_mt5.py#L43"}}),De=new pe({props:{anchor:"transformers.FlaxMT5Model.example",$$slots:{default:[tp]},$$scope:{ctx:M}}}),Vt=new P({}),Ut=new E({props:{name:"class transformers.FlaxMT5ForConditionalGeneration",anchor:"transformers.FlaxMT5ForConditionalGeneration",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/mt5/modeling_flax_mt5.py#L97"}}),Ge=new pe({props:{anchor:"transformers.FlaxMT5ForConditionalGeneration.example",$$slots:{default:[sp]},$$scope:{ctx:M}}}),Bt=new P({}),Ht=new E({props:{name:"class transformers.FlaxMT5EncoderModel",anchor:"transformers.FlaxMT5EncoderModel",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.21.2/src/transformers/models/mt5/modeling_flax_mt5.py#L70"}}),Ve=new pe({props:{anchor:"transformers.FlaxMT5EncoderModel.example",$$slots:{default:[rp]},$$scope:{ctx:M}}}),{c(){h=r("meta"),x=p(),_=r("h1"),u=r("a"),v=r("span"),k(a.$$.fragment),g=p(),Ns=r("span"),Zn=l("mT5"),Nr=p(),K=r("h2"),fe=r("a"),Ds=r("span"),k(We.$$.fragment),eo=p(),Is=r("span"),to=l("Overview"),Dr=p(),ue=r("p"),so=l("The mT5 model was presented in "),Be=r("a"),ro=l("mT5: A massively multilingual pre-trained text-to-text transformer"),no=l(` by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, Colin Raffel.`),Ir=p(),Qt=r("p"),oo=l("The abstract from the paper is the following:"),Gr=p(),Yt=r("p"),Gs=r("em"),ao=l(`The recent \u201CText-to-Text Transfer Transformer\u201D (T5) leveraged a unified text-to-text format and scale to attain
state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail
the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. We also describe a simple technique to prevent \u201Caccidental translation\u201D in the zero-shot setting, where a
generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model
checkpoints used in this work are publicly available.`),Or=p(),he=r("p"),lo=l("Note: mT5 was only pre-trained on "),He=r("a"),io=l("mC4"),po=l(` excluding any supervised training.
Therefore, this model has to be fine-tuned before it is useable on a downstream task, unlike the original T5 model.
Since mT5 was pre-trained unsupervisedly, there\u2019s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`),Vr=p(),Zt=r("p"),mo=l("Google has released the following variants:"),Ur=p(),F=r("ul"),Os=r("li"),Vs=r("p"),Re=r("a"),co=l("google/mt5-small"),fo=p(),Us=r("li"),Ws=r("p"),Xe=r("a"),uo=l("google/mt5-base"),ho=p(),Bs=r("li"),Hs=r("p"),Ke=r("a"),go=l("google/mt5-large"),_o=p(),Rs=r("li"),Xs=r("p"),Je=r("a"),vo=l("google/mt5-xl"),ko=p(),Ks=r("li"),es=r("p"),Qe=r("a"),To=l("google/mt5-xxl"),$o=l("."),Wr=p(),W=r("p"),bo=l("This model was contributed by "),Ye=r("a"),wo=l("patrickvonplaten"),yo=l(`. The original code can be
found `),Ze=r("a"),xo=l("here"),Mo=l("."),Br=p(),J=r("h2"),ge=r("a"),Js=r("span"),k(et.$$.fragment),zo=p(),Qs=r("span"),Eo=l("MT5Config"),Hr=p(),C=r("div"),k(tt.$$.fragment),qo=p(),A=r("p"),Fo=l("This is the configuration class to store the configuration of a "),ts=r("a"),jo=l("MT5Model"),Po=l(" or a "),ss=r("a"),Co=l("TFMT5Model"),Ao=l(`. It is used to
instantiate a mT5 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the mT5
`),st=r("a"),So=l("google/mt5-small"),Lo=l(" architecture."),No=p(),Q=r("p"),Do=l("Configuration objects inherit from "),rs=r("a"),Io=l("PretrainedConfig"),Go=l(` and can be used to control the model outputs. Read the
documentation from `),ns=r("a"),Oo=l("PretrainedConfig"),Vo=l(" for more information."),Rr=p(),Y=r("h2"),_e=r("a"),Ys=r("span"),k(rt.$$.fragment),Uo=p(),Zs=r("span"),Wo=l("MT5Tokenizer"),Xr=p(),z=r("div"),k(nt.$$.fragment),Bo=p(),ot=r("p"),Ho=l("Construct a T5 tokenizer. Based on "),at=r("a"),Ro=l("SentencePiece"),Xo=l("."),Ko=p(),lt=r("p"),Jo=l("This tokenizer inherits from "),os=r("a"),Qo=l("PreTrainedTokenizer"),Yo=l(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Zo=p(),B=r("div"),k(it.$$.fragment),ea=p(),er=r("p"),ta=l(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),sa=p(),dt=r("ul"),as=r("li"),ra=l("single sequence: "),tr=r("code"),na=l("X </s>"),oa=p(),ls=r("li"),aa=l("pair of sequences: "),sr=r("code"),la=l("A </s> B </s>"),ia=p(),ve=r("div"),k(pt.$$.fragment),da=p(),rr=r("p"),pa=l("Converts a sequence of tokens (string) in a single string."),ma=p(),ke=r("div"),k(mt.$$.fragment),ca=p(),nr=r("p"),fa=l(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),ua=p(),Te=r("div"),k(ct.$$.fragment),ha=p(),ft=r("p"),ga=l(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),or=r("code"),_a=l("prepare_for_model"),va=l(" method."),Kr=p(),$e=r("p"),ka=l("See "),is=r("a"),Ta=l("T5Tokenizer"),$a=l(" for all details."),Jr=p(),Z=r("h2"),be=r("a"),ar=r("span"),k(ut.$$.fragment),ba=p(),lr=r("span"),wa=l("MT5TokenizerFast"),Qr=p(),q=r("div"),k(ht.$$.fragment),ya=p(),ee=r("p"),xa=l("Construct a \u201Cfast\u201D T5 tokenizer (backed by HuggingFace\u2019s "),ir=r("em"),Ma=l("tokenizers"),za=l(` library). Based on
`),gt=r("a"),Ea=l("Unigram"),qa=l("."),Fa=p(),_t=r("p"),ja=l("This tokenizer inherits from "),ds=r("a"),Pa=l("PreTrainedTokenizerFast"),Ca=l(` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),Aa=p(),H=r("div"),k(vt.$$.fragment),Sa=p(),dr=r("p"),La=l(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),Na=p(),kt=r("ul"),ps=r("li"),Da=l("single sequence: "),pr=r("code"),Ia=l("X </s>"),Ga=p(),ms=r("li"),Oa=l("pair of sequences: "),mr=r("code"),Va=l("A </s> B </s>"),Ua=p(),we=r("div"),k(Tt.$$.fragment),Wa=p(),cr=r("p"),Ba=l(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),Yr=p(),ye=r("p"),Ha=l("See "),cs=r("a"),Ra=l("T5TokenizerFast"),Xa=l(" for all details."),Zr=p(),te=r("h2"),xe=r("a"),fr=r("span"),k($t.$$.fragment),Ka=p(),ur=r("span"),Ja=l("MT5Model"),en=p(),S=r("div"),k(bt.$$.fragment),Qa=p(),wt=r("p"),Ya=l("This class overrides "),fs=r("a"),Za=l("T5Model"),el=l(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),tl=p(),k(Me.$$.fragment),tn=p(),se=r("h2"),ze=r("a"),hr=r("span"),k(yt.$$.fragment),sl=p(),gr=r("span"),rl=l("MT5ForConditionalGeneration"),sn=p(),L=r("div"),k(xt.$$.fragment),nl=p(),Mt=r("p"),ol=l("This class overrides "),us=r("a"),al=l("T5ForConditionalGeneration"),ll=l(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),il=p(),k(Ee.$$.fragment),rn=p(),re=r("h2"),qe=r("a"),_r=r("span"),k(zt.$$.fragment),dl=p(),vr=r("span"),pl=l("MT5EncoderModel"),nn=p(),N=r("div"),k(Et.$$.fragment),ml=p(),qt=r("p"),cl=l("This class overrides "),hs=r("a"),fl=l("T5EncoderModel"),ul=l(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),hl=p(),k(Fe.$$.fragment),on=p(),ne=r("h2"),je=r("a"),kr=r("span"),k(Ft.$$.fragment),gl=p(),Tr=r("span"),_l=l("TFMT5Model"),an=p(),D=r("div"),k(jt.$$.fragment),vl=p(),Pt=r("p"),kl=l("This class overrides "),gs=r("a"),Tl=l("TFT5Model"),$l=l(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),bl=p(),k(Pe.$$.fragment),ln=p(),oe=r("h2"),Ce=r("a"),$r=r("span"),k(Ct.$$.fragment),wl=p(),br=r("span"),yl=l("TFMT5ForConditionalGeneration"),dn=p(),I=r("div"),k(At.$$.fragment),xl=p(),St=r("p"),Ml=l("This class overrides "),_s=r("a"),zl=l("TFT5ForConditionalGeneration"),El=l(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),ql=p(),k(Ae.$$.fragment),pn=p(),ae=r("h2"),Se=r("a"),wr=r("span"),k(Lt.$$.fragment),Fl=p(),yr=r("span"),jl=l("TFMT5EncoderModel"),mn=p(),G=r("div"),k(Nt.$$.fragment),Pl=p(),Dt=r("p"),Cl=l("This class overrides "),vs=r("a"),Al=l("TFT5EncoderModel"),Sl=l(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Ll=p(),k(Le.$$.fragment),cn=p(),le=r("h2"),Ne=r("a"),xr=r("span"),k(It.$$.fragment),Nl=p(),Mr=r("span"),Dl=l("FlaxMT5Model"),fn=p(),O=r("div"),k(Gt.$$.fragment),Il=p(),Ot=r("p"),Gl=l("This class overrides "),ks=r("a"),Ol=l("FlaxT5Model"),Vl=l(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Ul=p(),k(De.$$.fragment),un=p(),ie=r("h2"),Ie=r("a"),zr=r("span"),k(Vt.$$.fragment),Wl=p(),Er=r("span"),Bl=l("FlaxMT5ForConditionalGeneration"),hn=p(),V=r("div"),k(Ut.$$.fragment),Hl=p(),Wt=r("p"),Rl=l("This class overrides "),Ts=r("a"),Xl=l("FlaxT5ForConditionalGeneration"),Kl=l(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Jl=p(),k(Ge.$$.fragment),gn=p(),de=r("h2"),Oe=r("a"),qr=r("span"),k(Bt.$$.fragment),Ql=p(),Fr=r("span"),Yl=l("FlaxMT5EncoderModel"),_n=p(),U=r("div"),k(Ht.$$.fragment),Zl=p(),Rt=r("p"),ei=l("This class overrides "),$s=r("a"),ti=l("FlaxT5EncoderModel"),si=l(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),ri=p(),k(Ve.$$.fragment),this.h()},l(e){const c=Rd('[data-svelte="svelte-1phssyn"]',document.head);h=n(c,"META",{name:!0,content:!0}),c.forEach(s),x=m(e),_=n(e,"H1",{class:!0});var Xt=o(_);u=n(Xt,"A",{id:!0,class:!0,href:!0});var jr=o(u);v=n(jr,"SPAN",{});var Pr=o(v);T(a.$$.fragment,Pr),Pr.forEach(s),jr.forEach(s),g=m(Xt),Ns=n(Xt,"SPAN",{});var Cr=o(Ns);Zn=i(Cr,"mT5"),Cr.forEach(s),Xt.forEach(s),Nr=m(e),K=n(e,"H2",{class:!0});var Kt=o(K);fe=n(Kt,"A",{id:!0,class:!0,href:!0});var Ar=o(fe);Ds=n(Ar,"SPAN",{});var Sr=o(Ds);T(We.$$.fragment,Sr),Sr.forEach(s),Ar.forEach(s),eo=m(Kt),Is=n(Kt,"SPAN",{});var Lr=o(Is);to=i(Lr,"Overview"),Lr.forEach(s),Kt.forEach(s),Dr=m(e),ue=n(e,"P",{});var Jt=o(ue);so=i(Jt,"The mT5 model was presented in "),Be=n(Jt,"A",{href:!0,rel:!0});var di=o(Be);ro=i(di,"mT5: A massively multilingual pre-trained text-to-text transformer"),di.forEach(s),no=i(Jt,` by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, Colin Raffel.`),Jt.forEach(s),Ir=m(e),Qt=n(e,"P",{});var pi=o(Qt);oo=i(pi,"The abstract from the paper is the following:"),pi.forEach(s),Gr=m(e),Yt=n(e,"P",{});var mi=o(Yt);Gs=n(mi,"EM",{});var ci=o(Gs);ao=i(ci,`The recent \u201CText-to-Text Transfer Transformer\u201D (T5) leveraged a unified text-to-text format and scale to attain
state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail
the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. We also describe a simple technique to prevent \u201Caccidental translation\u201D in the zero-shot setting, where a
generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model
checkpoints used in this work are publicly available.`),ci.forEach(s),mi.forEach(s),Or=m(e),he=n(e,"P",{});var kn=o(he);lo=i(kn,"Note: mT5 was only pre-trained on "),He=n(kn,"A",{href:!0,rel:!0});var fi=o(He);io=i(fi,"mC4"),fi.forEach(s),po=i(kn,` excluding any supervised training.
Therefore, this model has to be fine-tuned before it is useable on a downstream task, unlike the original T5 model.
Since mT5 was pre-trained unsupervisedly, there\u2019s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`),kn.forEach(s),Vr=m(e),Zt=n(e,"P",{});var ui=o(Zt);mo=i(ui,"Google has released the following variants:"),ui.forEach(s),Ur=m(e),F=n(e,"UL",{});var R=o(F);Os=n(R,"LI",{});var hi=o(Os);Vs=n(hi,"P",{});var gi=o(Vs);Re=n(gi,"A",{href:!0,rel:!0});var _i=o(Re);co=i(_i,"google/mt5-small"),_i.forEach(s),gi.forEach(s),hi.forEach(s),fo=m(R),Us=n(R,"LI",{});var vi=o(Us);Ws=n(vi,"P",{});var ki=o(Ws);Xe=n(ki,"A",{href:!0,rel:!0});var Ti=o(Xe);uo=i(Ti,"google/mt5-base"),Ti.forEach(s),ki.forEach(s),vi.forEach(s),ho=m(R),Bs=n(R,"LI",{});var $i=o(Bs);Hs=n($i,"P",{});var bi=o(Hs);Ke=n(bi,"A",{href:!0,rel:!0});var wi=o(Ke);go=i(wi,"google/mt5-large"),wi.forEach(s),bi.forEach(s),$i.forEach(s),_o=m(R),Rs=n(R,"LI",{});var yi=o(Rs);Xs=n(yi,"P",{});var xi=o(Xs);Je=n(xi,"A",{href:!0,rel:!0});var Mi=o(Je);vo=i(Mi,"google/mt5-xl"),Mi.forEach(s),xi.forEach(s),yi.forEach(s),ko=m(R),Ks=n(R,"LI",{});var zi=o(Ks);es=n(zi,"P",{});var ni=o(es);Qe=n(ni,"A",{href:!0,rel:!0});var Ei=o(Qe);To=i(Ei,"google/mt5-xxl"),Ei.forEach(s),$o=i(ni,"."),ni.forEach(s),zi.forEach(s),R.forEach(s),Wr=m(e),W=n(e,"P",{});var bs=o(W);bo=i(bs,"This model was contributed by "),Ye=n(bs,"A",{href:!0,rel:!0});var qi=o(Ye);wo=i(qi,"patrickvonplaten"),qi.forEach(s),yo=i(bs,`. The original code can be
found `),Ze=n(bs,"A",{href:!0,rel:!0});var Fi=o(Ze);xo=i(Fi,"here"),Fi.forEach(s),Mo=i(bs,"."),bs.forEach(s),Br=m(e),J=n(e,"H2",{class:!0});var Tn=o(J);ge=n(Tn,"A",{id:!0,class:!0,href:!0});var ji=o(ge);Js=n(ji,"SPAN",{});var Pi=o(Js);T(et.$$.fragment,Pi),Pi.forEach(s),ji.forEach(s),zo=m(Tn),Qs=n(Tn,"SPAN",{});var Ci=o(Qs);Eo=i(Ci,"MT5Config"),Ci.forEach(s),Tn.forEach(s),Hr=m(e),C=n(e,"DIV",{class:!0});var ws=o(C);T(tt.$$.fragment,ws),qo=m(ws),A=n(ws,"P",{});var Ue=o(A);Fo=i(Ue,"This is the configuration class to store the configuration of a "),ts=n(Ue,"A",{href:!0});var Ai=o(ts);jo=i(Ai,"MT5Model"),Ai.forEach(s),Po=i(Ue," or a "),ss=n(Ue,"A",{href:!0});var Si=o(ss);Co=i(Si,"TFMT5Model"),Si.forEach(s),Ao=i(Ue,`. It is used to
instantiate a mT5 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the mT5
`),st=n(Ue,"A",{href:!0,rel:!0});var Li=o(st);So=i(Li,"google/mt5-small"),Li.forEach(s),Lo=i(Ue," architecture."),Ue.forEach(s),No=m(ws),Q=n(ws,"P",{});var ys=o(Q);Do=i(ys,"Configuration objects inherit from "),rs=n(ys,"A",{href:!0});var Ni=o(rs);Io=i(Ni,"PretrainedConfig"),Ni.forEach(s),Go=i(ys,` and can be used to control the model outputs. Read the
documentation from `),ns=n(ys,"A",{href:!0});var Di=o(ns);Oo=i(Di,"PretrainedConfig"),Di.forEach(s),Vo=i(ys," for more information."),ys.forEach(s),ws.forEach(s),Rr=m(e),Y=n(e,"H2",{class:!0});var $n=o(Y);_e=n($n,"A",{id:!0,class:!0,href:!0});var Ii=o(_e);Ys=n(Ii,"SPAN",{});var Gi=o(Ys);T(rt.$$.fragment,Gi),Gi.forEach(s),Ii.forEach(s),Uo=m($n),Zs=n($n,"SPAN",{});var Oi=o(Zs);Wo=i(Oi,"MT5Tokenizer"),Oi.forEach(s),$n.forEach(s),Xr=m(e),z=n(e,"DIV",{class:!0});var j=o(z);T(nt.$$.fragment,j),Bo=m(j),ot=n(j,"P",{});var bn=o(ot);Ho=i(bn,"Construct a T5 tokenizer. Based on "),at=n(bn,"A",{href:!0,rel:!0});var Vi=o(at);Ro=i(Vi,"SentencePiece"),Vi.forEach(s),Xo=i(bn,"."),bn.forEach(s),Ko=m(j),lt=n(j,"P",{});var wn=o(lt);Jo=i(wn,"This tokenizer inherits from "),os=n(wn,"A",{href:!0});var Ui=o(os);Qo=i(Ui,"PreTrainedTokenizer"),Ui.forEach(s),Yo=i(wn,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),wn.forEach(s),Zo=m(j),B=n(j,"DIV",{class:!0});var xs=o(B);T(it.$$.fragment,xs),ea=m(xs),er=n(xs,"P",{});var Wi=o(er);ta=i(Wi,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),Wi.forEach(s),sa=m(xs),dt=n(xs,"UL",{});var yn=o(dt);as=n(yn,"LI",{});var oi=o(as);ra=i(oi,"single sequence: "),tr=n(oi,"CODE",{});var Bi=o(tr);na=i(Bi,"X </s>"),Bi.forEach(s),oi.forEach(s),oa=m(yn),ls=n(yn,"LI",{});var ai=o(ls);aa=i(ai,"pair of sequences: "),sr=n(ai,"CODE",{});var Hi=o(sr);la=i(Hi,"A </s> B </s>"),Hi.forEach(s),ai.forEach(s),yn.forEach(s),xs.forEach(s),ia=m(j),ve=n(j,"DIV",{class:!0});var xn=o(ve);T(pt.$$.fragment,xn),da=m(xn),rr=n(xn,"P",{});var Ri=o(rr);pa=i(Ri,"Converts a sequence of tokens (string) in a single string."),Ri.forEach(s),xn.forEach(s),ma=m(j),ke=n(j,"DIV",{class:!0});var Mn=o(ke);T(mt.$$.fragment,Mn),ca=m(Mn),nr=n(Mn,"P",{});var Xi=o(nr);fa=i(Xi,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),Xi.forEach(s),Mn.forEach(s),ua=m(j),Te=n(j,"DIV",{class:!0});var zn=o(Te);T(ct.$$.fragment,zn),ha=m(zn),ft=n(zn,"P",{});var En=o(ft);ga=i(En,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),or=n(En,"CODE",{});var Ki=o(or);_a=i(Ki,"prepare_for_model"),Ki.forEach(s),va=i(En," method."),En.forEach(s),zn.forEach(s),j.forEach(s),Kr=m(e),$e=n(e,"P",{});var qn=o($e);ka=i(qn,"See "),is=n(qn,"A",{href:!0});var Ji=o(is);Ta=i(Ji,"T5Tokenizer"),Ji.forEach(s),$a=i(qn," for all details."),qn.forEach(s),Jr=m(e),Z=n(e,"H2",{class:!0});var Fn=o(Z);be=n(Fn,"A",{id:!0,class:!0,href:!0});var Qi=o(be);ar=n(Qi,"SPAN",{});var Yi=o(ar);T(ut.$$.fragment,Yi),Yi.forEach(s),Qi.forEach(s),ba=m(Fn),lr=n(Fn,"SPAN",{});var Zi=o(lr);wa=i(Zi,"MT5TokenizerFast"),Zi.forEach(s),Fn.forEach(s),Qr=m(e),q=n(e,"DIV",{class:!0});var X=o(q);T(ht.$$.fragment,X),ya=m(X),ee=n(X,"P",{});var Ms=o(ee);xa=i(Ms,"Construct a \u201Cfast\u201D T5 tokenizer (backed by HuggingFace\u2019s "),ir=n(Ms,"EM",{});var ed=o(ir);Ma=i(ed,"tokenizers"),ed.forEach(s),za=i(Ms,` library). Based on
`),gt=n(Ms,"A",{href:!0,rel:!0});var td=o(gt);Ea=i(td,"Unigram"),td.forEach(s),qa=i(Ms,"."),Ms.forEach(s),Fa=m(X),_t=n(X,"P",{});var jn=o(_t);ja=i(jn,"This tokenizer inherits from "),ds=n(jn,"A",{href:!0});var sd=o(ds);Pa=i(sd,"PreTrainedTokenizerFast"),sd.forEach(s),Ca=i(jn,` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),jn.forEach(s),Aa=m(X),H=n(X,"DIV",{class:!0});var zs=o(H);T(vt.$$.fragment,zs),Sa=m(zs),dr=n(zs,"P",{});var rd=o(dr);La=i(rd,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),rd.forEach(s),Na=m(zs),kt=n(zs,"UL",{});var Pn=o(kt);ps=n(Pn,"LI",{});var li=o(ps);Da=i(li,"single sequence: "),pr=n(li,"CODE",{});var nd=o(pr);Ia=i(nd,"X </s>"),nd.forEach(s),li.forEach(s),Ga=m(Pn),ms=n(Pn,"LI",{});var ii=o(ms);Oa=i(ii,"pair of sequences: "),mr=n(ii,"CODE",{});var od=o(mr);Va=i(od,"A </s> B </s>"),od.forEach(s),ii.forEach(s),Pn.forEach(s),zs.forEach(s),Ua=m(X),we=n(X,"DIV",{class:!0});var Cn=o(we);T(Tt.$$.fragment,Cn),Wa=m(Cn),cr=n(Cn,"P",{});var ad=o(cr);Ba=i(ad,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),ad.forEach(s),Cn.forEach(s),X.forEach(s),Yr=m(e),ye=n(e,"P",{});var An=o(ye);Ha=i(An,"See "),cs=n(An,"A",{href:!0});var ld=o(cs);Ra=i(ld,"T5TokenizerFast"),ld.forEach(s),Xa=i(An," for all details."),An.forEach(s),Zr=m(e),te=n(e,"H2",{class:!0});var Sn=o(te);xe=n(Sn,"A",{id:!0,class:!0,href:!0});var id=o(xe);fr=n(id,"SPAN",{});var dd=o(fr);T($t.$$.fragment,dd),dd.forEach(s),id.forEach(s),Ka=m(Sn),ur=n(Sn,"SPAN",{});var pd=o(ur);Ja=i(pd,"MT5Model"),pd.forEach(s),Sn.forEach(s),en=m(e),S=n(e,"DIV",{class:!0});var Es=o(S);T(bt.$$.fragment,Es),Qa=m(Es),wt=n(Es,"P",{});var Ln=o(wt);Ya=i(Ln,"This class overrides "),fs=n(Ln,"A",{href:!0});var md=o(fs);Za=i(md,"T5Model"),md.forEach(s),el=i(Ln,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Ln.forEach(s),tl=m(Es),T(Me.$$.fragment,Es),Es.forEach(s),tn=m(e),se=n(e,"H2",{class:!0});var Nn=o(se);ze=n(Nn,"A",{id:!0,class:!0,href:!0});var cd=o(ze);hr=n(cd,"SPAN",{});var fd=o(hr);T(yt.$$.fragment,fd),fd.forEach(s),cd.forEach(s),sl=m(Nn),gr=n(Nn,"SPAN",{});var ud=o(gr);rl=i(ud,"MT5ForConditionalGeneration"),ud.forEach(s),Nn.forEach(s),sn=m(e),L=n(e,"DIV",{class:!0});var qs=o(L);T(xt.$$.fragment,qs),nl=m(qs),Mt=n(qs,"P",{});var Dn=o(Mt);ol=i(Dn,"This class overrides "),us=n(Dn,"A",{href:!0});var hd=o(us);al=i(hd,"T5ForConditionalGeneration"),hd.forEach(s),ll=i(Dn,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Dn.forEach(s),il=m(qs),T(Ee.$$.fragment,qs),qs.forEach(s),rn=m(e),re=n(e,"H2",{class:!0});var In=o(re);qe=n(In,"A",{id:!0,class:!0,href:!0});var gd=o(qe);_r=n(gd,"SPAN",{});var _d=o(_r);T(zt.$$.fragment,_d),_d.forEach(s),gd.forEach(s),dl=m(In),vr=n(In,"SPAN",{});var vd=o(vr);pl=i(vd,"MT5EncoderModel"),vd.forEach(s),In.forEach(s),nn=m(e),N=n(e,"DIV",{class:!0});var Fs=o(N);T(Et.$$.fragment,Fs),ml=m(Fs),qt=n(Fs,"P",{});var Gn=o(qt);cl=i(Gn,"This class overrides "),hs=n(Gn,"A",{href:!0});var kd=o(hs);fl=i(kd,"T5EncoderModel"),kd.forEach(s),ul=i(Gn,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Gn.forEach(s),hl=m(Fs),T(Fe.$$.fragment,Fs),Fs.forEach(s),on=m(e),ne=n(e,"H2",{class:!0});var On=o(ne);je=n(On,"A",{id:!0,class:!0,href:!0});var Td=o(je);kr=n(Td,"SPAN",{});var $d=o(kr);T(Ft.$$.fragment,$d),$d.forEach(s),Td.forEach(s),gl=m(On),Tr=n(On,"SPAN",{});var bd=o(Tr);_l=i(bd,"TFMT5Model"),bd.forEach(s),On.forEach(s),an=m(e),D=n(e,"DIV",{class:!0});var js=o(D);T(jt.$$.fragment,js),vl=m(js),Pt=n(js,"P",{});var Vn=o(Pt);kl=i(Vn,"This class overrides "),gs=n(Vn,"A",{href:!0});var wd=o(gs);Tl=i(wd,"TFT5Model"),wd.forEach(s),$l=i(Vn,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Vn.forEach(s),bl=m(js),T(Pe.$$.fragment,js),js.forEach(s),ln=m(e),oe=n(e,"H2",{class:!0});var Un=o(oe);Ce=n(Un,"A",{id:!0,class:!0,href:!0});var yd=o(Ce);$r=n(yd,"SPAN",{});var xd=o($r);T(Ct.$$.fragment,xd),xd.forEach(s),yd.forEach(s),wl=m(Un),br=n(Un,"SPAN",{});var Md=o(br);yl=i(Md,"TFMT5ForConditionalGeneration"),Md.forEach(s),Un.forEach(s),dn=m(e),I=n(e,"DIV",{class:!0});var Ps=o(I);T(At.$$.fragment,Ps),xl=m(Ps),St=n(Ps,"P",{});var Wn=o(St);Ml=i(Wn,"This class overrides "),_s=n(Wn,"A",{href:!0});var zd=o(_s);zl=i(zd,"TFT5ForConditionalGeneration"),zd.forEach(s),El=i(Wn,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Wn.forEach(s),ql=m(Ps),T(Ae.$$.fragment,Ps),Ps.forEach(s),pn=m(e),ae=n(e,"H2",{class:!0});var Bn=o(ae);Se=n(Bn,"A",{id:!0,class:!0,href:!0});var Ed=o(Se);wr=n(Ed,"SPAN",{});var qd=o(wr);T(Lt.$$.fragment,qd),qd.forEach(s),Ed.forEach(s),Fl=m(Bn),yr=n(Bn,"SPAN",{});var Fd=o(yr);jl=i(Fd,"TFMT5EncoderModel"),Fd.forEach(s),Bn.forEach(s),mn=m(e),G=n(e,"DIV",{class:!0});var Cs=o(G);T(Nt.$$.fragment,Cs),Pl=m(Cs),Dt=n(Cs,"P",{});var Hn=o(Dt);Cl=i(Hn,"This class overrides "),vs=n(Hn,"A",{href:!0});var jd=o(vs);Al=i(jd,"TFT5EncoderModel"),jd.forEach(s),Sl=i(Hn,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Hn.forEach(s),Ll=m(Cs),T(Le.$$.fragment,Cs),Cs.forEach(s),cn=m(e),le=n(e,"H2",{class:!0});var Rn=o(le);Ne=n(Rn,"A",{id:!0,class:!0,href:!0});var Pd=o(Ne);xr=n(Pd,"SPAN",{});var Cd=o(xr);T(It.$$.fragment,Cd),Cd.forEach(s),Pd.forEach(s),Nl=m(Rn),Mr=n(Rn,"SPAN",{});var Ad=o(Mr);Dl=i(Ad,"FlaxMT5Model"),Ad.forEach(s),Rn.forEach(s),fn=m(e),O=n(e,"DIV",{class:!0});var As=o(O);T(Gt.$$.fragment,As),Il=m(As),Ot=n(As,"P",{});var Xn=o(Ot);Gl=i(Xn,"This class overrides "),ks=n(Xn,"A",{href:!0});var Sd=o(ks);Ol=i(Sd,"FlaxT5Model"),Sd.forEach(s),Vl=i(Xn,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Xn.forEach(s),Ul=m(As),T(De.$$.fragment,As),As.forEach(s),un=m(e),ie=n(e,"H2",{class:!0});var Kn=o(ie);Ie=n(Kn,"A",{id:!0,class:!0,href:!0});var Ld=o(Ie);zr=n(Ld,"SPAN",{});var Nd=o(zr);T(Vt.$$.fragment,Nd),Nd.forEach(s),Ld.forEach(s),Wl=m(Kn),Er=n(Kn,"SPAN",{});var Dd=o(Er);Bl=i(Dd,"FlaxMT5ForConditionalGeneration"),Dd.forEach(s),Kn.forEach(s),hn=m(e),V=n(e,"DIV",{class:!0});var Ss=o(V);T(Ut.$$.fragment,Ss),Hl=m(Ss),Wt=n(Ss,"P",{});var Jn=o(Wt);Rl=i(Jn,"This class overrides "),Ts=n(Jn,"A",{href:!0});var Id=o(Ts);Xl=i(Id,"FlaxT5ForConditionalGeneration"),Id.forEach(s),Kl=i(Jn,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Jn.forEach(s),Jl=m(Ss),T(Ge.$$.fragment,Ss),Ss.forEach(s),gn=m(e),de=n(e,"H2",{class:!0});var Qn=o(de);Oe=n(Qn,"A",{id:!0,class:!0,href:!0});var Gd=o(Oe);qr=n(Gd,"SPAN",{});var Od=o(qr);T(Bt.$$.fragment,Od),Od.forEach(s),Gd.forEach(s),Ql=m(Qn),Fr=n(Qn,"SPAN",{});var Vd=o(Fr);Yl=i(Vd,"FlaxMT5EncoderModel"),Vd.forEach(s),Qn.forEach(s),_n=m(e),U=n(e,"DIV",{class:!0});var Ls=o(U);T(Ht.$$.fragment,Ls),Zl=m(Ls),Rt=n(Ls,"P",{});var Yn=o(Rt);ei=i(Yn,"This class overrides "),$s=n(Yn,"A",{href:!0});var Ud=o($s);ti=i(Ud,"FlaxT5EncoderModel"),Ud.forEach(s),si=i(Yn,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Yn.forEach(s),ri=m(Ls),T(Ve.$$.fragment,Ls),Ls.forEach(s),this.h()},h(){d(h,"name","hf:doc:metadata"),d(h,"content",JSON.stringify(op)),d(u,"id","mt5"),d(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(u,"href","#mt5"),d(_,"class","relative group"),d(fe,"id","overview"),d(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(fe,"href","#overview"),d(K,"class","relative group"),d(Be,"href","https://arxiv.org/abs/2010.11934"),d(Be,"rel","nofollow"),d(He,"href","https://huggingface.co/datasets/mc4"),d(He,"rel","nofollow"),d(Re,"href","https://huggingface.co/google/mt5-small"),d(Re,"rel","nofollow"),d(Xe,"href","https://huggingface.co/google/mt5-base"),d(Xe,"rel","nofollow"),d(Ke,"href","https://huggingface.co/google/mt5-large"),d(Ke,"rel","nofollow"),d(Je,"href","https://huggingface.co/google/mt5-xl"),d(Je,"rel","nofollow"),d(Qe,"href","https://huggingface.co/google/mt5-xxl"),d(Qe,"rel","nofollow"),d(Ye,"href","https://huggingface.co/patrickvonplaten"),d(Ye,"rel","nofollow"),d(Ze,"href","https://github.com/google-research/multilingual-t5"),d(Ze,"rel","nofollow"),d(ge,"id","transformers.MT5Config"),d(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ge,"href","#transformers.MT5Config"),d(J,"class","relative group"),d(ts,"href","/docs/transformers/v4.21.2/en/model_doc/mt5#transformers.MT5Model"),d(ss,"href","/docs/transformers/v4.21.2/en/model_doc/mt5#transformers.TFMT5Model"),d(st,"href","https://huggingface.co/google/mt5-small"),d(st,"rel","nofollow"),d(rs,"href","/docs/transformers/v4.21.2/en/main_classes/configuration#transformers.PretrainedConfig"),d(ns,"href","/docs/transformers/v4.21.2/en/main_classes/configuration#transformers.PretrainedConfig"),d(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(_e,"id","transformers.T5Tokenizer"),d(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_e,"href","#transformers.T5Tokenizer"),d(Y,"class","relative group"),d(at,"href","https://github.com/google/sentencepiece"),d(at,"rel","nofollow"),d(os,"href","/docs/transformers/v4.21.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),d(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(is,"href","/docs/transformers/v4.21.2/en/model_doc/t5#transformers.T5Tokenizer"),d(be,"id","transformers.T5TokenizerFast"),d(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(be,"href","#transformers.T5TokenizerFast"),d(Z,"class","relative group"),d(gt,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models"),d(gt,"rel","nofollow"),d(ds,"href","/docs/transformers/v4.21.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),d(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(we,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(cs,"href","/docs/transformers/v4.21.2/en/model_doc/t5#transformers.T5TokenizerFast"),d(xe,"id","transformers.MT5Model"),d(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(xe,"href","#transformers.MT5Model"),d(te,"class","relative group"),d(fs,"href","/docs/transformers/v4.21.2/en/model_doc/t5#transformers.T5Model"),d(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ze,"id","transformers.MT5ForConditionalGeneration"),d(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ze,"href","#transformers.MT5ForConditionalGeneration"),d(se,"class","relative group"),d(us,"href","/docs/transformers/v4.21.2/en/model_doc/t5#transformers.T5ForConditionalGeneration"),d(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(qe,"id","transformers.MT5EncoderModel"),d(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(qe,"href","#transformers.MT5EncoderModel"),d(re,"class","relative group"),d(hs,"href","/docs/transformers/v4.21.2/en/model_doc/t5#transformers.T5EncoderModel"),d(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(je,"id","transformers.TFMT5Model"),d(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(je,"href","#transformers.TFMT5Model"),d(ne,"class","relative group"),d(gs,"href","/docs/transformers/v4.21.2/en/model_doc/t5#transformers.TFT5Model"),d(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ce,"id","transformers.TFMT5ForConditionalGeneration"),d(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ce,"href","#transformers.TFMT5ForConditionalGeneration"),d(oe,"class","relative group"),d(_s,"href","/docs/transformers/v4.21.2/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),d(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Se,"id","transformers.TFMT5EncoderModel"),d(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Se,"href","#transformers.TFMT5EncoderModel"),d(ae,"class","relative group"),d(vs,"href","/docs/transformers/v4.21.2/en/model_doc/t5#transformers.TFT5EncoderModel"),d(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ne,"id","transformers.FlaxMT5Model"),d(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ne,"href","#transformers.FlaxMT5Model"),d(le,"class","relative group"),d(ks,"href","/docs/transformers/v4.21.2/en/model_doc/t5#transformers.FlaxT5Model"),d(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ie,"id","transformers.FlaxMT5ForConditionalGeneration"),d(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ie,"href","#transformers.FlaxMT5ForConditionalGeneration"),d(ie,"class","relative group"),d(Ts,"href","/docs/transformers/v4.21.2/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),d(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Oe,"id","transformers.FlaxMT5EncoderModel"),d(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Oe,"href","#transformers.FlaxMT5EncoderModel"),d(de,"class","relative group"),d($s,"href","/docs/transformers/v4.21.2/en/model_doc/t5#transformers.FlaxT5EncoderModel"),d(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,c){t(document.head,h),f(e,x,c),f(e,_,c),t(_,u),t(u,v),$(a,v,null),t(_,g),t(_,Ns),t(Ns,Zn),f(e,Nr,c),f(e,K,c),t(K,fe),t(fe,Ds),$(We,Ds,null),t(K,eo),t(K,Is),t(Is,to),f(e,Dr,c),f(e,ue,c),t(ue,so),t(ue,Be),t(Be,ro),t(ue,no),f(e,Ir,c),f(e,Qt,c),t(Qt,oo),f(e,Gr,c),f(e,Yt,c),t(Yt,Gs),t(Gs,ao),f(e,Or,c),f(e,he,c),t(he,lo),t(he,He),t(He,io),t(he,po),f(e,Vr,c),f(e,Zt,c),t(Zt,mo),f(e,Ur,c),f(e,F,c),t(F,Os),t(Os,Vs),t(Vs,Re),t(Re,co),t(F,fo),t(F,Us),t(Us,Ws),t(Ws,Xe),t(Xe,uo),t(F,ho),t(F,Bs),t(Bs,Hs),t(Hs,Ke),t(Ke,go),t(F,_o),t(F,Rs),t(Rs,Xs),t(Xs,Je),t(Je,vo),t(F,ko),t(F,Ks),t(Ks,es),t(es,Qe),t(Qe,To),t(es,$o),f(e,Wr,c),f(e,W,c),t(W,bo),t(W,Ye),t(Ye,wo),t(W,yo),t(W,Ze),t(Ze,xo),t(W,Mo),f(e,Br,c),f(e,J,c),t(J,ge),t(ge,Js),$(et,Js,null),t(J,zo),t(J,Qs),t(Qs,Eo),f(e,Hr,c),f(e,C,c),$(tt,C,null),t(C,qo),t(C,A),t(A,Fo),t(A,ts),t(ts,jo),t(A,Po),t(A,ss),t(ss,Co),t(A,Ao),t(A,st),t(st,So),t(A,Lo),t(C,No),t(C,Q),t(Q,Do),t(Q,rs),t(rs,Io),t(Q,Go),t(Q,ns),t(ns,Oo),t(Q,Vo),f(e,Rr,c),f(e,Y,c),t(Y,_e),t(_e,Ys),$(rt,Ys,null),t(Y,Uo),t(Y,Zs),t(Zs,Wo),f(e,Xr,c),f(e,z,c),$(nt,z,null),t(z,Bo),t(z,ot),t(ot,Ho),t(ot,at),t(at,Ro),t(ot,Xo),t(z,Ko),t(z,lt),t(lt,Jo),t(lt,os),t(os,Qo),t(lt,Yo),t(z,Zo),t(z,B),$(it,B,null),t(B,ea),t(B,er),t(er,ta),t(B,sa),t(B,dt),t(dt,as),t(as,ra),t(as,tr),t(tr,na),t(dt,oa),t(dt,ls),t(ls,aa),t(ls,sr),t(sr,la),t(z,ia),t(z,ve),$(pt,ve,null),t(ve,da),t(ve,rr),t(rr,pa),t(z,ma),t(z,ke),$(mt,ke,null),t(ke,ca),t(ke,nr),t(nr,fa),t(z,ua),t(z,Te),$(ct,Te,null),t(Te,ha),t(Te,ft),t(ft,ga),t(ft,or),t(or,_a),t(ft,va),f(e,Kr,c),f(e,$e,c),t($e,ka),t($e,is),t(is,Ta),t($e,$a),f(e,Jr,c),f(e,Z,c),t(Z,be),t(be,ar),$(ut,ar,null),t(Z,ba),t(Z,lr),t(lr,wa),f(e,Qr,c),f(e,q,c),$(ht,q,null),t(q,ya),t(q,ee),t(ee,xa),t(ee,ir),t(ir,Ma),t(ee,za),t(ee,gt),t(gt,Ea),t(ee,qa),t(q,Fa),t(q,_t),t(_t,ja),t(_t,ds),t(ds,Pa),t(_t,Ca),t(q,Aa),t(q,H),$(vt,H,null),t(H,Sa),t(H,dr),t(dr,La),t(H,Na),t(H,kt),t(kt,ps),t(ps,Da),t(ps,pr),t(pr,Ia),t(kt,Ga),t(kt,ms),t(ms,Oa),t(ms,mr),t(mr,Va),t(q,Ua),t(q,we),$(Tt,we,null),t(we,Wa),t(we,cr),t(cr,Ba),f(e,Yr,c),f(e,ye,c),t(ye,Ha),t(ye,cs),t(cs,Ra),t(ye,Xa),f(e,Zr,c),f(e,te,c),t(te,xe),t(xe,fr),$($t,fr,null),t(te,Ka),t(te,ur),t(ur,Ja),f(e,en,c),f(e,S,c),$(bt,S,null),t(S,Qa),t(S,wt),t(wt,Ya),t(wt,fs),t(fs,Za),t(wt,el),t(S,tl),$(Me,S,null),f(e,tn,c),f(e,se,c),t(se,ze),t(ze,hr),$(yt,hr,null),t(se,sl),t(se,gr),t(gr,rl),f(e,sn,c),f(e,L,c),$(xt,L,null),t(L,nl),t(L,Mt),t(Mt,ol),t(Mt,us),t(us,al),t(Mt,ll),t(L,il),$(Ee,L,null),f(e,rn,c),f(e,re,c),t(re,qe),t(qe,_r),$(zt,_r,null),t(re,dl),t(re,vr),t(vr,pl),f(e,nn,c),f(e,N,c),$(Et,N,null),t(N,ml),t(N,qt),t(qt,cl),t(qt,hs),t(hs,fl),t(qt,ul),t(N,hl),$(Fe,N,null),f(e,on,c),f(e,ne,c),t(ne,je),t(je,kr),$(Ft,kr,null),t(ne,gl),t(ne,Tr),t(Tr,_l),f(e,an,c),f(e,D,c),$(jt,D,null),t(D,vl),t(D,Pt),t(Pt,kl),t(Pt,gs),t(gs,Tl),t(Pt,$l),t(D,bl),$(Pe,D,null),f(e,ln,c),f(e,oe,c),t(oe,Ce),t(Ce,$r),$(Ct,$r,null),t(oe,wl),t(oe,br),t(br,yl),f(e,dn,c),f(e,I,c),$(At,I,null),t(I,xl),t(I,St),t(St,Ml),t(St,_s),t(_s,zl),t(St,El),t(I,ql),$(Ae,I,null),f(e,pn,c),f(e,ae,c),t(ae,Se),t(Se,wr),$(Lt,wr,null),t(ae,Fl),t(ae,yr),t(yr,jl),f(e,mn,c),f(e,G,c),$(Nt,G,null),t(G,Pl),t(G,Dt),t(Dt,Cl),t(Dt,vs),t(vs,Al),t(Dt,Sl),t(G,Ll),$(Le,G,null),f(e,cn,c),f(e,le,c),t(le,Ne),t(Ne,xr),$(It,xr,null),t(le,Nl),t(le,Mr),t(Mr,Dl),f(e,fn,c),f(e,O,c),$(Gt,O,null),t(O,Il),t(O,Ot),t(Ot,Gl),t(Ot,ks),t(ks,Ol),t(Ot,Vl),t(O,Ul),$(De,O,null),f(e,un,c),f(e,ie,c),t(ie,Ie),t(Ie,zr),$(Vt,zr,null),t(ie,Wl),t(ie,Er),t(Er,Bl),f(e,hn,c),f(e,V,c),$(Ut,V,null),t(V,Hl),t(V,Wt),t(Wt,Rl),t(Wt,Ts),t(Ts,Xl),t(Wt,Kl),t(V,Jl),$(Ge,V,null),f(e,gn,c),f(e,de,c),t(de,Oe),t(Oe,qr),$(Bt,qr,null),t(de,Ql),t(de,Fr),t(Fr,Yl),f(e,_n,c),f(e,U,c),$(Ht,U,null),t(U,Zl),t(U,Rt),t(Rt,ei),t(Rt,$s),t($s,ti),t(Rt,si),t(U,ri),$(Ve,U,null),vn=!0},p(e,[c]){const Xt={};c&2&&(Xt.$$scope={dirty:c,ctx:e}),Me.$set(Xt);const jr={};c&2&&(jr.$$scope={dirty:c,ctx:e}),Ee.$set(jr);const Pr={};c&2&&(Pr.$$scope={dirty:c,ctx:e}),Fe.$set(Pr);const Cr={};c&2&&(Cr.$$scope={dirty:c,ctx:e}),Pe.$set(Cr);const Kt={};c&2&&(Kt.$$scope={dirty:c,ctx:e}),Ae.$set(Kt);const Ar={};c&2&&(Ar.$$scope={dirty:c,ctx:e}),Le.$set(Ar);const Sr={};c&2&&(Sr.$$scope={dirty:c,ctx:e}),De.$set(Sr);const Lr={};c&2&&(Lr.$$scope={dirty:c,ctx:e}),Ge.$set(Lr);const Jt={};c&2&&(Jt.$$scope={dirty:c,ctx:e}),Ve.$set(Jt)},i(e){vn||(b(a.$$.fragment,e),b(We.$$.fragment,e),b(et.$$.fragment,e),b(tt.$$.fragment,e),b(rt.$$.fragment,e),b(nt.$$.fragment,e),b(it.$$.fragment,e),b(pt.$$.fragment,e),b(mt.$$.fragment,e),b(ct.$$.fragment,e),b(ut.$$.fragment,e),b(ht.$$.fragment,e),b(vt.$$.fragment,e),b(Tt.$$.fragment,e),b($t.$$.fragment,e),b(bt.$$.fragment,e),b(Me.$$.fragment,e),b(yt.$$.fragment,e),b(xt.$$.fragment,e),b(Ee.$$.fragment,e),b(zt.$$.fragment,e),b(Et.$$.fragment,e),b(Fe.$$.fragment,e),b(Ft.$$.fragment,e),b(jt.$$.fragment,e),b(Pe.$$.fragment,e),b(Ct.$$.fragment,e),b(At.$$.fragment,e),b(Ae.$$.fragment,e),b(Lt.$$.fragment,e),b(Nt.$$.fragment,e),b(Le.$$.fragment,e),b(It.$$.fragment,e),b(Gt.$$.fragment,e),b(De.$$.fragment,e),b(Vt.$$.fragment,e),b(Ut.$$.fragment,e),b(Ge.$$.fragment,e),b(Bt.$$.fragment,e),b(Ht.$$.fragment,e),b(Ve.$$.fragment,e),vn=!0)},o(e){w(a.$$.fragment,e),w(We.$$.fragment,e),w(et.$$.fragment,e),w(tt.$$.fragment,e),w(rt.$$.fragment,e),w(nt.$$.fragment,e),w(it.$$.fragment,e),w(pt.$$.fragment,e),w(mt.$$.fragment,e),w(ct.$$.fragment,e),w(ut.$$.fragment,e),w(ht.$$.fragment,e),w(vt.$$.fragment,e),w(Tt.$$.fragment,e),w($t.$$.fragment,e),w(bt.$$.fragment,e),w(Me.$$.fragment,e),w(yt.$$.fragment,e),w(xt.$$.fragment,e),w(Ee.$$.fragment,e),w(zt.$$.fragment,e),w(Et.$$.fragment,e),w(Fe.$$.fragment,e),w(Ft.$$.fragment,e),w(jt.$$.fragment,e),w(Pe.$$.fragment,e),w(Ct.$$.fragment,e),w(At.$$.fragment,e),w(Ae.$$.fragment,e),w(Lt.$$.fragment,e),w(Nt.$$.fragment,e),w(Le.$$.fragment,e),w(It.$$.fragment,e),w(Gt.$$.fragment,e),w(De.$$.fragment,e),w(Vt.$$.fragment,e),w(Ut.$$.fragment,e),w(Ge.$$.fragment,e),w(Bt.$$.fragment,e),w(Ht.$$.fragment,e),w(Ve.$$.fragment,e),vn=!1},d(e){s(h),e&&s(x),e&&s(_),y(a),e&&s(Nr),e&&s(K),y(We),e&&s(Dr),e&&s(ue),e&&s(Ir),e&&s(Qt),e&&s(Gr),e&&s(Yt),e&&s(Or),e&&s(he),e&&s(Vr),e&&s(Zt),e&&s(Ur),e&&s(F),e&&s(Wr),e&&s(W),e&&s(Br),e&&s(J),y(et),e&&s(Hr),e&&s(C),y(tt),e&&s(Rr),e&&s(Y),y(rt),e&&s(Xr),e&&s(z),y(nt),y(it),y(pt),y(mt),y(ct),e&&s(Kr),e&&s($e),e&&s(Jr),e&&s(Z),y(ut),e&&s(Qr),e&&s(q),y(ht),y(vt),y(Tt),e&&s(Yr),e&&s(ye),e&&s(Zr),e&&s(te),y($t),e&&s(en),e&&s(S),y(bt),y(Me),e&&s(tn),e&&s(se),y(yt),e&&s(sn),e&&s(L),y(xt),y(Ee),e&&s(rn),e&&s(re),y(zt),e&&s(nn),e&&s(N),y(Et),y(Fe),e&&s(on),e&&s(ne),y(Ft),e&&s(an),e&&s(D),y(jt),y(Pe),e&&s(ln),e&&s(oe),y(Ct),e&&s(dn),e&&s(I),y(At),y(Ae),e&&s(pn),e&&s(ae),y(Lt),e&&s(mn),e&&s(G),y(Nt),y(Le),e&&s(cn),e&&s(le),y(It),e&&s(fn),e&&s(O),y(Gt),y(De),e&&s(un),e&&s(ie),y(Vt),e&&s(hn),e&&s(V),y(Ut),y(Ge),e&&s(gn),e&&s(de),y(Bt),e&&s(_n),e&&s(U),y(Ht),y(Ve)}}}const op={local:"mt5",sections:[{local:"overview",title:"Overview"},{local:"transformers.MT5Config",title:"MT5Config"},{local:"transformers.T5Tokenizer",title:"MT5Tokenizer"},{local:"transformers.T5TokenizerFast",title:"MT5TokenizerFast"},{local:"transformers.MT5Model",title:"MT5Model"},{local:"transformers.MT5ForConditionalGeneration",title:"MT5ForConditionalGeneration"},{local:"transformers.MT5EncoderModel",title:"MT5EncoderModel"},{local:"transformers.TFMT5Model",title:"TFMT5Model"},{local:"transformers.TFMT5ForConditionalGeneration",title:"TFMT5ForConditionalGeneration"},{local:"transformers.TFMT5EncoderModel",title:"TFMT5EncoderModel"},{local:"transformers.FlaxMT5Model",title:"FlaxMT5Model"},{local:"transformers.FlaxMT5ForConditionalGeneration",title:"FlaxMT5ForConditionalGeneration"},{local:"transformers.FlaxMT5EncoderModel",title:"FlaxMT5EncoderModel"}],title:"mT5"};function ap(M){return Xd(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class cp extends Wd{constructor(h){super();Bd(this,h,ap,np,Hd,{})}}export{cp as default,op as metadata};
