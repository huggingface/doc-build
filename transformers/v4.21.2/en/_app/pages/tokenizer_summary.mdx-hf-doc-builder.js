import{S as zm,i as _m,s as qm,e as o,k as c,w as f,t as a,Y as ko,M as jm,c as l,d as s,m as u,a as r,x as g,h as n,Z as Eo,b as h,G as t,g as p,y as b,L as $m,q as v,o as y,B as w,v as xm}from"../chunks/vendor-hf-doc-builder.js";import{Y as zo}from"../chunks/Youtube-hf-doc-builder.js";import{I as De}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as ne}from"../chunks/CodeBlock-hf-doc-builder.js";import{D as Pm}from"../chunks/DocNotebookDropdown-hf-doc-builder.js";function Tm(Ch){let oe,Ua,le,be,es,Se,_o,ts,qo,Ha,Ce,Ra,ht,jo,Ga,Be,Xa,D,$o,ct,xo,Po,ut,To,Ao,mt,Do,So,dt,Co,Bo,Fa,F,Oo,ft,Mo,No,gt,Wo,Io,Ka,re,ve,ss,Oe,Lo,as,Uo,Va,Me,Ho,ns,Ro,Ja,Ne,Ya,bt,Go,Za,We,Qa,S,Xo,os,Fo,Ko,ls,Vo,Jo,rs,Yo,Zo,is,Qo,el,en,Ie,tn,C,tl,ps,sl,al,hs,nl,ol,cs,ll,rl,us,il,pl,sn,W,Le,hl,cl,Ue,ul,ml,ms,dl,fl,ds,gl,bl,an,He,nn,K,vl,fs,yl,wl,vt,kl,El,on,yt,zl,ln,wt,_l,rn,Re,pn,B,ql,gs,jl,$l,bs,xl,Pl,vs,Tl,Al,ys,Dl,Sl,hn,ie,ye,ws,Ge,Cl,ks,Bl,cn,Xe,un,k,Ol,Es,Ml,Nl,zs,Wl,Il,_s,Ll,Ul,qs,Hl,Rl,js,Gl,Xl,$s,Fl,Kl,xs,Vl,Jl,Ps,Yl,Zl,mn,V,Ql,kt,er,tr,Ts,sr,ar,dn,Fe,fn,P,nr,As,or,lr,Ds,rr,ir,Ss,pr,hr,Cs,cr,ur,Bs,mr,dr,gn,we,fr,Et,gr,br,bn,Ke,vn,T,vr,Os,yr,wr,zt,kr,Er,Ms,zr,_r,Ns,qr,jr,Ws,$r,xr,yn,_t,Pr,wn,qt,kn,pe,ke,Is,Ve,Tr,Ls,Ar,En,q,Dr,Je,Sr,Cr,jt,Br,Or,$t,Mr,Nr,xt,Wr,Ir,Pt,Lr,Ur,Tt,Hr,Rr,zn,At,Gr,_n,Dt,Xr,qn,Ye,jn,Ee,Fr,Us,Kr,Vr,$n,Ze,xn,m,Jr,Hs,Yr,Zr,Rs,Qr,ei,Gs,ti,si,Xs,ai,ni,Fs,oi,li,Ks,ri,ii,Vs,pi,hi,Js,ci,ui,Ys,mi,di,Zs,fi,gi,Qs,bi,vi,Pn,Qe,Tn,E,yi,ea,wi,ki,ta,Ei,zi,sa,_i,qi,aa,ji,$i,na,xi,Pi,oa,Ti,Ai,la,Di,Si,ra,Ci,Bi,An,ze,Oi,ia,Mi,Ni,Dn,et,Sn,_,Wi,pa,Ii,Li,ha,Ui,Hi,ca,Ri,Gi,ua,Xi,Fi,ma,Ki,Vi,da,Ji,Yi,fa,Zi,Qi,Cn,J,ep,ga,tp,sp,St,ap,np,Bn,he,_e,ba,tt,op,va,lp,On,U,rp,ya,ip,pp,st,hp,cp,Ct,up,mp,Mn,Bt,Nn,ce,qe,wa,at,dp,ka,fp,Wn,O,gp,Ot,bp,vp,Mt,yp,wp,Nt,kp,Ep,nt,zp,_p,In,z,qp,Ea,jp,$p,za,xp,Pp,_a,Tp,Ap,qa,Dp,Sp,ja,Cp,Bp,$a,Op,Mp,xa,Np,Wp,Pa,Ip,Lp,Ln,Wt,Un,ue,je,Ta,ot,Up,Aa,Hp,Hn,Y,Rp,lt,Gp,Xp,It,Fp,Kp,Rn,$e,Vp,Da,Jp,Yp,Gn,Lt,Zp,Xn,rt,Fn,I,Sa,Qp,eh,Ca,th,sh,Ba,ah,nh,Oa,oh,lh,Kn,L,rh,Vn,ym='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>\u2026</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>N</mi></msub></mrow><annotation encoding="application/x-tex">x_{1}, \\dots, x_{N}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">\u2026</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',Jn,Yn,wm='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',Zn,Qn,km='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">S(x_{i})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>',eo,to,Em=`<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="script">L</mi><mo>=</mo><mo>\u2212</mo><munderover><mo>\u2211</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>log</mi><mo>\u2061</mo><mrow><mo fence="true">(</mo><munder><mo>\u2211</mo><mrow><mi>x</mi><mo>\u2208</mo><mi>S</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></munder><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\\mathcal{L} = -\\sum_{i=1}^{N} \\log \\left ( \\sum_{x \\in S(x_{i})} p(x) \\right )</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathcal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.6em;vertical-align:-1.55em;"></span><span class="mord">\u2212</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">\u2211</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-2.25em;"><span class="pstrut" style="height:3.155em;"></span><span class="delimsizinginner delim-size4"><span>\u239D</span></span></span><span style="top:-3.397em;"><span class="pstrut" style="height:3.155em;"></span><span style="height:0.016em;width:0.875em;"><svg xmlns="http://www.w3.org/2000/svg" width='0.875em' height='0.016em' style='width:0.875em' viewBox='0 0 875 16' preserveAspectRatio='xMinYMin'><path d='M291 0 H417 V16 H291z M291 0 H417 V16 H291z'/></svg></span></span><span style="top:-4.05em;"><span class="pstrut" style="height:3.155em;"></span><span class="delimsizinginner delim-size4"><span>\u239B</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.809em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mrel mtight">\u2208</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">\u2211</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:1.516em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-2.25em;"><span class="pstrut" style="height:3.155em;"></span><span class="delimsizinginner delim-size4"><span>\u23A0</span></span></span><span style="top:-3.397em;"><span class="pstrut" style="height:3.155em;"></span><span style="height:0.016em;width:0.875em;"><svg xmlns="http://www.w3.org/2000/svg" width='0.875em' height='0.016em' style='width:0.875em' viewBox='0 0 875 16' preserveAspectRatio='xMinYMin'><path d='M457 0 H583 V16 H457z M457 0 H583 V16 H457z'/></svg></span></span><span style="top:-4.05em;"><span class="pstrut" style="height:3.155em;"></span><span class="delimsizinginner delim-size4"><span>\u239E</span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span></span></span></span></span></span></span>`,so,Ut,ao,me,xe,Ma,it,ih,Na,ph,no,H,hh,Wa,ch,uh,Ht,mh,dh,pt,fh,gh,oo,R,bh,Rt,vh,yh,Ia,wh,kh,La,Eh,zh,lo,M,_h,Gt,qh,jh,Xt,$h,xh,Ft,Ph,Th,Kt,Ah,Dh,ro;return Se=new De({}),Ce=new Pm({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tokenizer_summary.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/tokenizer_summary.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/tokenizer_summary.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tokenizer_summary.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/tokenizer_summary.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/tokenizer_summary.ipynb"}]}}),Be=new zo({props:{id:"VFp38yj8h3A"}}),Oe=new De({}),Ne=new zo({props:{id:"nhJxYji1aho"}}),We=new ne({props:{code:`["Don't", "you", "love", "\u{1F917}", "Transformers?", "We", "sure", "do."]`,highlighted:'<span class="hljs-selector-attr">[<span class="hljs-string">&quot;Don&#x27;t&quot;</span>, <span class="hljs-string">&quot;you&quot;</span>, <span class="hljs-string">&quot;love&quot;</span>, <span class="hljs-string">&quot;\u{1F917}&quot;</span>, <span class="hljs-string">&quot;Transformers?&quot;</span>, <span class="hljs-string">&quot;We&quot;</span>, <span class="hljs-string">&quot;sure&quot;</span>, <span class="hljs-string">&quot;do.&quot;</span>]</span>'}}),Ie=new ne({props:{code:`["Don", "'", "t", "you", "love", "\u{1F917}", "Transformers", "?", "We", "sure", "do", "."]`,highlighted:'<span class="hljs-selector-attr">[<span class="hljs-string">&quot;Don&quot;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&quot;t&quot;</span>, <span class="hljs-string">&quot;you&quot;</span>, <span class="hljs-string">&quot;love&quot;</span>, <span class="hljs-string">&quot;\u{1F917}&quot;</span>, <span class="hljs-string">&quot;Transformers&quot;</span>, <span class="hljs-string">&quot;?&quot;</span>, <span class="hljs-string">&quot;We&quot;</span>, <span class="hljs-string">&quot;sure&quot;</span>, <span class="hljs-string">&quot;do&quot;</span>, <span class="hljs-string">&quot;.&quot;</span>]</span>'}}),He=new ne({props:{code:`["Do", "n't", "you", "love", "\u{1F917}", "Transformers", "?", "We", "sure", "do", "."]`,highlighted:'<span class="hljs-selector-attr">[<span class="hljs-string">&quot;Do&quot;</span>, <span class="hljs-string">&quot;n&#x27;t&quot;</span>, <span class="hljs-string">&quot;you&quot;</span>, <span class="hljs-string">&quot;love&quot;</span>, <span class="hljs-string">&quot;\u{1F917}&quot;</span>, <span class="hljs-string">&quot;Transformers&quot;</span>, <span class="hljs-string">&quot;?&quot;</span>, <span class="hljs-string">&quot;We&quot;</span>, <span class="hljs-string">&quot;sure&quot;</span>, <span class="hljs-string">&quot;do&quot;</span>, <span class="hljs-string">&quot;.&quot;</span>]</span>'}}),Re=new zo({props:{id:"ssLq_EK2jLE"}}),Ge=new De({}),Xe=new zo({props:{id:"zHvTiHr506c"}}),Fe=new ne({props:{code:`from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
tokenizer.tokenize("I have a new GPU!")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.tokenize(<span class="hljs-string">&quot;I have a new GPU!&quot;</span>)
[<span class="hljs-string">&quot;i&quot;</span>, <span class="hljs-string">&quot;have&quot;</span>, <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;new&quot;</span>, <span class="hljs-string">&quot;gp&quot;</span>, <span class="hljs-string">&quot;##u&quot;</span>, <span class="hljs-string">&quot;!&quot;</span>]`}}),Ke=new ne({props:{code:`from transformers import XLNetTokenizer

tokenizer = XLNetTokenizer.from_pretrained("xlnet-base-cased")
tokenizer.tokenize("Don't you love \u{1F917} Transformers? We sure do.")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLNetTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLNetTokenizer.from_pretrained(<span class="hljs-string">&quot;xlnet-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.tokenize(<span class="hljs-string">&quot;Don&#x27;t you love \u{1F917} Transformers? We sure do.&quot;</span>)
[<span class="hljs-string">&quot;\u2581Don&quot;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&quot;t&quot;</span>, <span class="hljs-string">&quot;\u2581you&quot;</span>, <span class="hljs-string">&quot;\u2581love&quot;</span>, <span class="hljs-string">&quot;\u2581&quot;</span>, <span class="hljs-string">&quot;\u{1F917}&quot;</span>, <span class="hljs-string">&quot;\u2581&quot;</span>, <span class="hljs-string">&quot;Transform&quot;</span>, <span class="hljs-string">&quot;ers&quot;</span>, <span class="hljs-string">&quot;?&quot;</span>, <span class="hljs-string">&quot;\u2581We&quot;</span>, <span class="hljs-string">&quot;\u2581sure&quot;</span>, <span class="hljs-string">&quot;\u2581do&quot;</span>, <span class="hljs-string">&quot;.&quot;</span>]`}}),Ve=new De({}),Ye=new ne({props:{code:'("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)',highlighted:'(<span class="hljs-string">&quot;hug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">10</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;pug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;pun&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">12</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;bun&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">4</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;hugs&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)'}}),Ze=new ne({props:{code:'("h" "u" "g", 10), ("p" "u" "g", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "u" "g" "s", 5)',highlighted:'(<span class="hljs-string">&quot;h&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;g&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">10</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;g&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;n&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">12</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;b&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;n&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">4</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;h&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;g&quot;</span> <span class="hljs-string">&quot;s&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)'}}),Qe=new ne({props:{code:'("h" "ug", 10), ("p" "ug", 5), ("p" "u" "n", 12), ("b" "u" "n", 4), ("h" "ug" "s", 5)',highlighted:'(<span class="hljs-string">&quot;h&quot;</span> <span class="hljs-string">&quot;ug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">10</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;ug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;n&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">12</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;b&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;n&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">4</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;h&quot;</span> <span class="hljs-string">&quot;ug&quot;</span> <span class="hljs-string">&quot;s&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)'}}),et=new ne({props:{code:'("hug", 10), ("p" "ug", 5), ("p" "un", 12), ("b" "un", 4), ("hug" "s", 5)',highlighted:'(<span class="hljs-string">&quot;hug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">10</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;ug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;un&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">12</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;b&quot;</span> <span class="hljs-string">&quot;un&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">4</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;hug&quot;</span> <span class="hljs-string">&quot;s&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)'}}),tt=new De({}),at=new De({}),ot=new De({}),rt=new ne({props:{code:'["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"],',highlighted:'<span class="hljs-selector-attr">[<span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;g&quot;</span>, <span class="hljs-string">&quot;h&quot;</span>, <span class="hljs-string">&quot;n&quot;</span>, <span class="hljs-string">&quot;p&quot;</span>, <span class="hljs-string">&quot;s&quot;</span>, <span class="hljs-string">&quot;u&quot;</span>, <span class="hljs-string">&quot;ug&quot;</span>, <span class="hljs-string">&quot;un&quot;</span>, <span class="hljs-string">&quot;hug&quot;</span>]</span>,'}}),it=new De({}),{c(){oe=o("meta"),Ua=c(),le=o("h1"),be=o("a"),es=o("span"),f(Se.$$.fragment),_o=c(),ts=o("span"),qo=a("Summary of the tokenizers"),Ha=c(),f(Ce.$$.fragment),Ra=c(),ht=o("p"),jo=a("On this page, we will have a closer look at tokenization."),Ga=c(),f(Be.$$.fragment),Xa=c(),D=o("p"),$o=a("As we saw in "),ct=o("a"),xo=a("the preprocessing tutorial"),Po=a(`, tokenizing a text is splitting it into words or
subwords, which then are converted to ids through a look-up table. Converting words or subwords to ids is
straightforward, so in this summary, we will focus on splitting a text into words or subwords (i.e. tokenizing a text).
More specifically, we will look at the three main types of tokenizers used in \u{1F917} Transformers: `),ut=o("a"),To=a(`Byte-Pair Encoding
(BPE)`),Ao=a(", "),mt=o("a"),Do=a("WordPiece"),So=a(", and "),dt=o("a"),Co=a("SentencePiece"),Bo=a(`, and show examples
of which tokenizer type is used by which model.`),Fa=c(),F=o("p"),Oo=a(`Note that on each model page, you can look at the documentation of the associated tokenizer to know which tokenizer
type was used by the pretrained model. For instance, if we look at `),ft=o("a"),Mo=a("BertTokenizer"),No=a(`, we can see
that the model uses `),gt=o("a"),Wo=a("WordPiece"),Io=a("."),Ka=c(),re=o("h2"),ve=o("a"),ss=o("span"),f(Oe.$$.fragment),Lo=c(),as=o("span"),Uo=a("Introduction"),Va=c(),Me=o("p"),Ho=a(`Splitting a text into smaller chunks is a task that is harder than it looks, and there are multiple ways of doing so.
For instance, let\u2019s look at the sentence `),ns=o("code"),Ro=a(`"Don't you love \u{1F917} Transformers? We sure do."`),Ja=c(),f(Ne.$$.fragment),Ya=c(),bt=o("p"),Go=a("A simple way of tokenizing this text is to split it by spaces, which would give:"),Za=c(),f(We.$$.fragment),Qa=c(),S=o("p"),Xo=a("This is a sensible first step, but if we look at the tokens "),os=o("code"),Fo=a('"Transformers?"'),Ko=a(" and "),ls=o("code"),Vo=a('"do."'),Jo=a(`, we notice that the
punctuation is attached to the words `),rs=o("code"),Yo=a('"Transformer"'),Zo=a(" and "),is=o("code"),Qo=a('"do"'),el=a(`, which is suboptimal. We should take the
punctuation into account so that a model does not have to learn a different representation of a word and every possible
punctuation symbol that could follow it, which would explode the number of representations the model has to learn.
Taking punctuation into account, tokenizing our exemplary text would give:`),en=c(),f(Ie.$$.fragment),tn=c(),C=o("p"),tl=a("Better. However, it is disadvantageous, how the tokenization dealt with the word "),ps=o("code"),sl=a(`"Don't"`),al=a(". "),hs=o("code"),nl=a(`"Don't"`),ol=a(` stands for
`),cs=o("code"),ll=a('"do not"'),rl=a(", so it would be better tokenized as "),us=o("code"),il=a(`["Do", "n't"]`),pl=a(`. This is where things start getting complicated, and
part of the reason each model has its own tokenizer type. Depending on the rules we apply for tokenizing a text, a
different tokenized output is generated for the same text. A pretrained model only performs properly if you feed it an
input that was tokenized with the same rules that were used to tokenize its training data.`),sn=c(),W=o("p"),Le=o("a"),hl=a("spaCy"),cl=a(" and "),Ue=o("a"),ul=a("Moses"),ml=a(` are two popular
rule-based tokenizers. Applying them on our example, `),ms=o("em"),dl=a("spaCy"),fl=a(" and "),ds=o("em"),gl=a("Moses"),bl=a(" would output something like:"),an=c(),f(He.$$.fragment),nn=c(),K=o("p"),vl=a(`As can be seen space and punctuation tokenization, as well as rule-based tokenization, is used here. Space and
punctuation tokenization and rule-based tokenization are both examples of word tokenization, which is loosely defined
as splitting sentences into words. While it\u2019s the most intuitive way to split texts into smaller chunks, this
tokenization method can lead to problems for massive text corpora. In this case, space and punctuation tokenization
usually generates a very big vocabulary (the set of all unique words and tokens used). `),fs=o("em"),yl=a("E.g."),wl=a(", "),vt=o("a"),kl=a("Transformer XL"),El=a(" uses space and punctuation tokenization, resulting in a vocabulary size of 267,735!"),on=c(),yt=o("p"),zl=a(`Such a big vocabulary size forces the model to have an enormous embedding matrix as the input and output layer, which
causes both an increased memory and time complexity. In general, transformers models rarely have a vocabulary size
greater than 50,000, especially if they are pretrained only on a single language.`),ln=c(),wt=o("p"),_l=a("So if simple space and punctuation tokenization is unsatisfactory, why not simply tokenize on characters?"),rn=c(),f(Re.$$.fragment),pn=c(),B=o("p"),ql=a(`While character tokenization is very simple and would greatly reduce memory and time complexity it makes it much harder
for the model to learn meaningful input representations. `),gs=o("em"),jl=a("E.g."),$l=a(` learning a meaningful context-independent
representation for the letter `),bs=o("code"),xl=a('"t"'),Pl=a(` is much harder than learning a context-independent representation for the word
`),vs=o("code"),Tl=a('"today"'),Al=a(`. Therefore, character tokenization is often accompanied by a loss of performance. So to get the best of
both worlds, transformers models use a hybrid between word-level and character-level tokenization called `),ys=o("strong"),Dl=a("subword"),Sl=a(`
tokenization.`),hn=c(),ie=o("h3"),ye=o("a"),ws=o("span"),f(Ge.$$.fragment),Cl=c(),ks=o("span"),Bl=a("Subword tokenization"),cn=c(),f(Xe.$$.fragment),un=c(),k=o("p"),Ol=a(`Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller
subwords, but rare words should be decomposed into meaningful subwords. For instance `),Es=o("code"),Ml=a('"annoyingly"'),Nl=a(` might be
considered a rare word and could be decomposed into `),zs=o("code"),Wl=a('"annoying"'),Il=a(" and "),_s=o("code"),Ll=a('"ly"'),Ul=a(". Both "),qs=o("code"),Hl=a('"annoying"'),Rl=a(" and "),js=o("code"),Gl=a('"ly"'),Xl=a(` as
stand-alone subwords would appear more frequently while at the same time the meaning of `),$s=o("code"),Fl=a('"annoyingly"'),Kl=a(` is kept by the
composite meaning of `),xs=o("code"),Vl=a('"annoying"'),Jl=a(" and "),Ps=o("code"),Yl=a('"ly"'),Zl=a(`. This is especially useful in agglutinative languages such as Turkish,
where you can form (almost) arbitrarily long complex words by stringing together subwords.`),mn=c(),V=o("p"),Ql=a(`Subword tokenization allows the model to have a reasonable vocabulary size while being able to learn meaningful
context-independent representations. In addition, subword tokenization enables the model to process words it has never
seen before, by decomposing them into known subwords. For instance, the `),kt=o("a"),er=a("BertTokenizer"),tr=a(` tokenizes
`),Ts=o("code"),sr=a('"I have a new GPU!"'),ar=a(" as follows:"),dn=c(),f(Fe.$$.fragment),fn=c(),P=o("p"),nr=a("Because we are considering the uncased model, the sentence was lowercased first. We can see that the words "),As=o("code"),or=a('["i", "have", "a", "new"]'),lr=a(" are present in the tokenizer\u2019s vocabulary, but the word "),Ds=o("code"),rr=a('"gpu"'),ir=a(` is not. Consequently, the
tokenizer splits `),Ss=o("code"),pr=a('"gpu"'),hr=a(" into known subwords: "),Cs=o("code"),cr=a('["gp" and "##u"]'),ur=a(". "),Bs=o("code"),mr=a('"##"'),dr=a(` means that the rest of the token should
be attached to the previous one, without space (for decoding or reversal of the tokenization).`),gn=c(),we=o("p"),fr=a("As another example, "),Et=o("a"),gr=a("XLNetTokenizer"),br=a(" tokenizes our previously exemplary text as follows:"),bn=c(),f(Ke.$$.fragment),vn=c(),T=o("p"),vr=a("We\u2019ll get back to the meaning of those "),Os=o("code"),yr=a('"\u2581"'),wr=a(" when we look at "),zt=o("a"),kr=a("SentencePiece"),Er=a(`. As one can see,
the rare word `),Ms=o("code"),zr=a('"Transformers"'),_r=a(" has been split into the more frequent subwords "),Ns=o("code"),qr=a('"Transform"'),jr=a(" and "),Ws=o("code"),$r=a('"ers"'),xr=a("."),yn=c(),_t=o("p"),Pr=a(`Let\u2019s now look at how the different subword tokenization algorithms work. Note that all of those tokenization
algorithms rely on some form of training which is usually done on the corpus the corresponding model will be trained
on.`),wn=c(),qt=o("a"),kn=c(),pe=o("h2"),ke=o("a"),Is=o("span"),f(Ve.$$.fragment),Tr=c(),Ls=o("span"),Ar=a("Byte-Pair Encoding (BPE)"),En=c(),q=o("p"),Dr=a("Byte-Pair Encoding (BPE) was introduced in "),Je=o("a"),Sr=a(`Neural Machine Translation of Rare Words with Subword Units (Sennrich et
al., 2015)`),Cr=a(`. BPE relies on a pre-tokenizer that splits the training data into
words. Pretokenization can be as simple as space tokenization, e.g. `),jt=o("a"),Br=a("GPT-2"),Or=a(", "),$t=o("a"),Mr=a("Roberta"),Nr=a(". More advanced pre-tokenization include rule-based tokenization, e.g. "),xt=o("a"),Wr=a("XLM"),Ir=a(`,
`),Pt=o("a"),Lr=a("FlauBERT"),Ur=a(" which uses Moses for most languages, or "),Tt=o("a"),Hr=a("GPT"),Rr=a(` which uses
Spacy and ftfy, to count the frequency of each word in the training corpus.`),zn=c(),At=o("p"),Gr=a(`After pre-tokenization, a set of unique words has been created and the frequency of each word it occurred in the
training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set
of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until
the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to
define before training the tokenizer.`),_n=c(),Dt=o("p"),Xr=a(`As an example, let\u2019s assume that after pre-tokenization, the following set of words including their frequency has been
determined:`),qn=c(),f(Ye.$$.fragment),jn=c(),Ee=o("p"),Fr=a("Consequently, the base vocabulary is "),Us=o("code"),Kr=a('["b", "g", "h", "n", "p", "s", "u"]'),Vr=a(`. Splitting all words into symbols of the
base vocabulary, we obtain:`),$n=c(),f(Ze.$$.fragment),xn=c(),m=o("p"),Jr=a(`BPE then counts the frequency of each possible symbol pair and picks the symbol pair that occurs most frequently. In
the example above `),Hs=o("code"),Yr=a('"h"'),Zr=a(" followed by "),Rs=o("code"),Qr=a('"u"'),ei=a(" is present "),Gs=o("em"),ti=a("10 + 5 = 15"),si=a(` times (10 times in the 10 occurrences of
`),Xs=o("code"),ai=a('"hug"'),ni=a(", 5 times in the 5 occurrences of "),Fs=o("code"),oi=a('"hugs"'),li=a("). However, the most frequent symbol pair is "),Ks=o("code"),ri=a('"u"'),ii=a(` followed by
`),Vs=o("code"),pi=a('"g"'),hi=a(", occurring "),Js=o("em"),ci=a("10 + 5 + 5 = 20"),ui=a(` times in total. Thus, the first merge rule the tokenizer learns is to group all
`),Ys=o("code"),mi=a('"u"'),di=a(" symbols followed by a "),Zs=o("code"),fi=a('"g"'),gi=a(" symbol together. Next, "),Qs=o("code"),bi=a('"ug"'),vi=a(` is added to the vocabulary. The set of words then
becomes`),Pn=c(),f(Qe.$$.fragment),Tn=c(),E=o("p"),yi=a("BPE then identifies the next most common symbol pair. It\u2019s "),ea=o("code"),wi=a('"u"'),ki=a(" followed by "),ta=o("code"),Ei=a('"n"'),zi=a(", which occurs 16 times. "),sa=o("code"),_i=a('"u"'),qi=a(`,
`),aa=o("code"),ji=a('"n"'),$i=a(" is merged to "),na=o("code"),xi=a('"un"'),Pi=a(" and added to the vocabulary. The next most frequent symbol pair is "),oa=o("code"),Ti=a('"h"'),Ai=a(` followed by
`),la=o("code"),Di=a('"ug"'),Si=a(", occurring 15 times. Again the pair is merged and "),ra=o("code"),Ci=a('"hug"'),Bi=a(" can be added to the vocabulary."),An=c(),ze=o("p"),Oi=a("At this stage, the vocabulary is "),ia=o("code"),Mi=a('["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]'),Ni=a(` and our set of unique words
is represented as`),Dn=c(),f(et.$$.fragment),Sn=c(),_=o("p"),Wi=a(`Assuming, that the Byte-Pair Encoding training would stop at this point, the learned merge rules would then be applied
to new words (as long as those new words do not include symbols that were not in the base vocabulary). For instance,
the word `),pa=o("code"),Ii=a('"bug"'),Li=a(" would be tokenized to "),ha=o("code"),Ui=a('["b", "ug"]'),Hi=a(" but "),ca=o("code"),Ri=a('"mug"'),Gi=a(" would be tokenized as "),ua=o("code"),Xi=a('["<unk>", "ug"]'),Fi=a(` since
the symbol `),ma=o("code"),Ki=a('"m"'),Vi=a(" is not in the base vocabulary. In general, single letters such as "),da=o("code"),Ji=a('"m"'),Yi=a(` are not replaced by the
`),fa=o("code"),Zi=a('"<unk>"'),Qi=a(` symbol because the training data usually includes at least one occurrence of each letter, but it is likely
to happen for very special characters like emojis.`),Cn=c(),J=o("p"),ep=a("As mentioned earlier, the vocabulary size, "),ga=o("em"),tp=a("i.e."),sp=a(` the base vocabulary size + the number of merges, is a hyperparameter
to choose. For instance `),St=o("a"),ap=a("GPT"),np=a(` has a vocabulary size of 40,478 since they have 478 base characters
and chose to stop training after 40,000 merges.`),Bn=c(),he=o("h3"),_e=o("a"),ba=o("span"),f(tt.$$.fragment),op=c(),va=o("span"),lp=a("Byte-level BPE"),On=c(),U=o("p"),rp=a("A base vocabulary that includes all possible base characters can be quite large if "),ya=o("em"),ip=a("e.g."),pp=a(` all unicode characters are
considered as base characters. To have a better base vocabulary, `),st=o("a"),hp=a("GPT-2"),cp=a(` uses bytes
as the base vocabulary, which is a clever trick to force the base vocabulary to be of size 256 while ensuring that
every base character is included in the vocabulary. With some additional rules to deal with punctuation, the GPT2\u2019s
tokenizer can tokenize every text without the need for the <unk> symbol. `),Ct=o("a"),up=a("GPT-2"),mp=a(` has a vocabulary
size of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned
with 50,000 merges.`),Mn=c(),Bt=o("a"),Nn=c(),ce=o("h4"),qe=o("a"),wa=o("span"),f(at.$$.fragment),dp=c(),ka=o("span"),fp=a("WordPiece"),Wn=c(),O=o("p"),gp=a("WordPiece is the subword tokenization algorithm used for "),Ot=o("a"),bp=a("BERT"),vp=a(", "),Mt=o("a"),yp=a("DistilBERT"),wp=a(", and "),Nt=o("a"),kp=a("Electra"),Ep=a(". The algorithm was outlined in "),nt=o("a"),zp=a(`Japanese and Korean
Voice Search (Schuster et al., 2012)`),_p=a(` and is very similar to
BPE. WordPiece first initializes the vocabulary to include every character present in the training data and
progressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent
symbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.`),In=c(),z=o("p"),qp=a(`So what does this mean exactly? Referring to the previous example, maximizing the likelihood of the training data is
equivalent to finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by
its second symbol is the greatest among all symbol pairs. `),Ea=o("em"),jp=a("E.g."),$p=c(),za=o("code"),xp=a('"u"'),Pp=a(", followed by "),_a=o("code"),Tp=a('"g"'),Ap=a(` would have only been
merged if the probability of `),qa=o("code"),Dp=a('"ug"'),Sp=a(" divided by "),ja=o("code"),Cp=a('"u"'),Bp=a(", "),$a=o("code"),Op=a('"g"'),Mp=a(` would have been greater than for any other symbol
pair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it `),xa=o("em"),Np=a("loses"),Wp=a(` by merging two symbols
to ensure it\u2019s `),Pa=o("em"),Ip=a("worth it"),Lp=a("."),Ln=c(),Wt=o("a"),Un=c(),ue=o("h4"),je=o("a"),Ta=o("span"),f(ot.$$.fragment),Up=c(),Aa=o("span"),Hp=a("Unigram"),Hn=c(),Y=o("p"),Rp=a("Unigram is a subword tokenization algorithm introduced in "),lt=o("a"),Gp=a(`Subword Regularization: Improving Neural Network Translation
Models with Multiple Subword Candidates (Kudo, 2018)`),Xp=a(`. In contrast to BPE or
WordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each
symbol to obtain a smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and
the most common substrings. Unigram is not used directly for any of the models in the transformers, but it\u2019s used in
conjunction with `),It=o("a"),Fp=a("SentencePiece"),Kp=a("."),Rn=c(),$e=o("p"),Vp=a(`At each training step, the Unigram algorithm defines a loss (often defined as the log-likelihood) over the training
data given the current vocabulary and a unigram language model. Then, for each symbol in the vocabulary, the algorithm
computes how much the overall loss would increase if the symbol was to be removed from the vocabulary. Unigram then
removes p (with p usually being 10% or 20%) percent of the symbols whose loss increase is the lowest, `),Da=o("em"),Jp=a("i.e."),Yp=a(` those
symbols that least affect the overall loss over the training data. This process is repeated until the vocabulary has
reached the desired size. The Unigram algorithm always keeps the base characters so that any word can be tokenized.`),Gn=c(),Lt=o("p"),Zp=a(`Because Unigram is not based on merge rules (in contrast to BPE and WordPiece), the algorithm has several ways of
tokenizing new text after training. As an example, if a trained Unigram tokenizer exhibits the vocabulary:`),Xn=c(),f(rt.$$.fragment),Fn=c(),I=o("p"),Sa=o("code"),Qp=a('"hugs"'),eh=a(" could be tokenized both as "),Ca=o("code"),th=a('["hug", "s"]'),sh=a(", "),Ba=o("code"),ah=a('["h", "ug", "s"]'),nh=a(" or "),Oa=o("code"),oh=a('["h", "u", "g", "s"]'),lh=a(`. So which one
to choose? Unigram saves the probability of each token in the training corpus on top of saving the vocabulary so that
the probability of each possible tokenization can be computed after training. The algorithm simply picks the most
likely tokenization in practice, but also offers the possibility to sample a possible tokenization according to their
probabilities.`),Kn=c(),L=o("p"),rh=a(`Those probabilities are defined by the loss the tokenizer is trained on. Assuming that the training data consists of
the words `),Vn=new ko,Jn=a(" and that the set of all possible tokenizations for a word "),Yn=new ko,Zn=a(` is
defined as `),Qn=new ko,eo=a(`, then the overall loss is defined as
`),to=new ko,so=c(),Ut=o("a"),ao=c(),me=o("h4"),xe=o("a"),Ma=o("span"),f(it.$$.fragment),ih=c(),Na=o("span"),ph=a("SentencePiece"),no=c(),H=o("p"),hh=a(`All tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to
separate words. However, not all languages use spaces to separate words. One possible solution is to use language
specific pre-tokenizers, `),Wa=o("em"),ch=a("e.g."),uh=c(),Ht=o("a"),mh=a("XLM"),dh=a(` uses a specific Chinese, Japanese, and Thai pre-tokenizer).
To solve this problem more generally, `),pt=o("a"),fh=a(`SentencePiece: A simple and language independent subword tokenizer and
detokenizer for Neural Text Processing (Kudo et al., 2018)`),gh=a(` treats the input
as a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram
algorithm to construct the appropriate vocabulary.`),oo=c(),R=o("p"),bh=a("The "),Rt=o("a"),vh=a("XLNetTokenizer"),yh=a(` uses SentencePiece for example, which is also why in the example earlier the
`),Ia=o("code"),wh=a('"\u2581"'),kh=a(` character was included in the vocabulary. Decoding with SentencePiece is very easy since all tokens can just be
concatenated and `),La=o("code"),Eh=a('"\u2581"'),zh=a(" is replaced by a space."),lo=c(),M=o("p"),_h=a(`All transformers models in the library that use SentencePiece use it in combination with unigram. Examples of models
using SentencePiece are `),Gt=o("a"),qh=a("ALBERT"),jh=a(", "),Xt=o("a"),$h=a("XLNet"),xh=a(", "),Ft=o("a"),Ph=a("Marian"),Th=a(", and "),Kt=o("a"),Ah=a("T5"),Dh=a("."),this.h()},l(e){const i=jm('[data-svelte="svelte-1phssyn"]',document.head);oe=l(i,"META",{name:!0,content:!0}),i.forEach(s),Ua=u(e),le=l(e,"H1",{class:!0});var io=r(le);be=l(io,"A",{id:!0,class:!0,href:!0});var Bh=r(be);es=l(Bh,"SPAN",{});var Oh=r(es);g(Se.$$.fragment,Oh),Oh.forEach(s),Bh.forEach(s),_o=u(io),ts=l(io,"SPAN",{});var Mh=r(ts);qo=n(Mh,"Summary of the tokenizers"),Mh.forEach(s),io.forEach(s),Ha=u(e),g(Ce.$$.fragment,e),Ra=u(e),ht=l(e,"P",{});var Nh=r(ht);jo=n(Nh,"On this page, we will have a closer look at tokenization."),Nh.forEach(s),Ga=u(e),g(Be.$$.fragment,e),Xa=u(e),D=l(e,"P",{});var Z=r(D);$o=n(Z,"As we saw in "),ct=l(Z,"A",{href:!0});var Wh=r(ct);xo=n(Wh,"the preprocessing tutorial"),Wh.forEach(s),Po=n(Z,`, tokenizing a text is splitting it into words or
subwords, which then are converted to ids through a look-up table. Converting words or subwords to ids is
straightforward, so in this summary, we will focus on splitting a text into words or subwords (i.e. tokenizing a text).
More specifically, we will look at the three main types of tokenizers used in \u{1F917} Transformers: `),ut=l(Z,"A",{href:!0});var Ih=r(ut);To=n(Ih,`Byte-Pair Encoding
(BPE)`),Ih.forEach(s),Ao=n(Z,", "),mt=l(Z,"A",{href:!0});var Lh=r(mt);Do=n(Lh,"WordPiece"),Lh.forEach(s),So=n(Z,", and "),dt=l(Z,"A",{href:!0});var Uh=r(dt);Co=n(Uh,"SentencePiece"),Uh.forEach(s),Bo=n(Z,`, and show examples
of which tokenizer type is used by which model.`),Z.forEach(s),Fa=u(e),F=l(e,"P",{});var Vt=r(F);Oo=n(Vt,`Note that on each model page, you can look at the documentation of the associated tokenizer to know which tokenizer
type was used by the pretrained model. For instance, if we look at `),ft=l(Vt,"A",{href:!0});var Hh=r(ft);Mo=n(Hh,"BertTokenizer"),Hh.forEach(s),No=n(Vt,`, we can see
that the model uses `),gt=l(Vt,"A",{href:!0});var Rh=r(gt);Wo=n(Rh,"WordPiece"),Rh.forEach(s),Io=n(Vt,"."),Vt.forEach(s),Ka=u(e),re=l(e,"H2",{class:!0});var po=r(re);ve=l(po,"A",{id:!0,class:!0,href:!0});var Gh=r(ve);ss=l(Gh,"SPAN",{});var Xh=r(ss);g(Oe.$$.fragment,Xh),Xh.forEach(s),Gh.forEach(s),Lo=u(po),as=l(po,"SPAN",{});var Fh=r(as);Uo=n(Fh,"Introduction"),Fh.forEach(s),po.forEach(s),Va=u(e),Me=l(e,"P",{});var Sh=r(Me);Ho=n(Sh,`Splitting a text into smaller chunks is a task that is harder than it looks, and there are multiple ways of doing so.
For instance, let\u2019s look at the sentence `),ns=l(Sh,"CODE",{});var Kh=r(ns);Ro=n(Kh,`"Don't you love \u{1F917} Transformers? We sure do."`),Kh.forEach(s),Sh.forEach(s),Ja=u(e),g(Ne.$$.fragment,e),Ya=u(e),bt=l(e,"P",{});var Vh=r(bt);Go=n(Vh,"A simple way of tokenizing this text is to split it by spaces, which would give:"),Vh.forEach(s),Za=u(e),g(We.$$.fragment,e),Qa=u(e),S=l(e,"P",{});var Q=r(S);Xo=n(Q,"This is a sensible first step, but if we look at the tokens "),os=l(Q,"CODE",{});var Jh=r(os);Fo=n(Jh,'"Transformers?"'),Jh.forEach(s),Ko=n(Q," and "),ls=l(Q,"CODE",{});var Yh=r(ls);Vo=n(Yh,'"do."'),Yh.forEach(s),Jo=n(Q,`, we notice that the
punctuation is attached to the words `),rs=l(Q,"CODE",{});var Zh=r(rs);Yo=n(Zh,'"Transformer"'),Zh.forEach(s),Zo=n(Q," and "),is=l(Q,"CODE",{});var Qh=r(is);Qo=n(Qh,'"do"'),Qh.forEach(s),el=n(Q,`, which is suboptimal. We should take the
punctuation into account so that a model does not have to learn a different representation of a word and every possible
punctuation symbol that could follow it, which would explode the number of representations the model has to learn.
Taking punctuation into account, tokenizing our exemplary text would give:`),Q.forEach(s),en=u(e),g(Ie.$$.fragment,e),tn=u(e),C=l(e,"P",{});var ee=r(C);tl=n(ee,"Better. However, it is disadvantageous, how the tokenization dealt with the word "),ps=l(ee,"CODE",{});var ec=r(ps);sl=n(ec,`"Don't"`),ec.forEach(s),al=n(ee,". "),hs=l(ee,"CODE",{});var tc=r(hs);nl=n(tc,`"Don't"`),tc.forEach(s),ol=n(ee,` stands for
`),cs=l(ee,"CODE",{});var sc=r(cs);ll=n(sc,'"do not"'),sc.forEach(s),rl=n(ee,", so it would be better tokenized as "),us=l(ee,"CODE",{});var ac=r(us);il=n(ac,`["Do", "n't"]`),ac.forEach(s),pl=n(ee,`. This is where things start getting complicated, and
part of the reason each model has its own tokenizer type. Depending on the rules we apply for tokenizing a text, a
different tokenized output is generated for the same text. A pretrained model only performs properly if you feed it an
input that was tokenized with the same rules that were used to tokenize its training data.`),ee.forEach(s),sn=u(e),W=l(e,"P",{});var de=r(W);Le=l(de,"A",{href:!0,rel:!0});var nc=r(Le);hl=n(nc,"spaCy"),nc.forEach(s),cl=n(de," and "),Ue=l(de,"A",{href:!0,rel:!0});var oc=r(Ue);ul=n(oc,"Moses"),oc.forEach(s),ml=n(de,` are two popular
rule-based tokenizers. Applying them on our example, `),ms=l(de,"EM",{});var lc=r(ms);dl=n(lc,"spaCy"),lc.forEach(s),fl=n(de," and "),ds=l(de,"EM",{});var rc=r(ds);gl=n(rc,"Moses"),rc.forEach(s),bl=n(de," would output something like:"),de.forEach(s),an=u(e),g(He.$$.fragment,e),nn=u(e),K=l(e,"P",{});var Jt=r(K);vl=n(Jt,`As can be seen space and punctuation tokenization, as well as rule-based tokenization, is used here. Space and
punctuation tokenization and rule-based tokenization are both examples of word tokenization, which is loosely defined
as splitting sentences into words. While it\u2019s the most intuitive way to split texts into smaller chunks, this
tokenization method can lead to problems for massive text corpora. In this case, space and punctuation tokenization
usually generates a very big vocabulary (the set of all unique words and tokens used). `),fs=l(Jt,"EM",{});var ic=r(fs);yl=n(ic,"E.g."),ic.forEach(s),wl=n(Jt,", "),vt=l(Jt,"A",{href:!0});var pc=r(vt);kl=n(pc,"Transformer XL"),pc.forEach(s),El=n(Jt," uses space and punctuation tokenization, resulting in a vocabulary size of 267,735!"),Jt.forEach(s),on=u(e),yt=l(e,"P",{});var hc=r(yt);zl=n(hc,`Such a big vocabulary size forces the model to have an enormous embedding matrix as the input and output layer, which
causes both an increased memory and time complexity. In general, transformers models rarely have a vocabulary size
greater than 50,000, especially if they are pretrained only on a single language.`),hc.forEach(s),ln=u(e),wt=l(e,"P",{});var cc=r(wt);_l=n(cc,"So if simple space and punctuation tokenization is unsatisfactory, why not simply tokenize on characters?"),cc.forEach(s),rn=u(e),g(Re.$$.fragment,e),pn=u(e),B=l(e,"P",{});var te=r(B);ql=n(te,`While character tokenization is very simple and would greatly reduce memory and time complexity it makes it much harder
for the model to learn meaningful input representations. `),gs=l(te,"EM",{});var uc=r(gs);jl=n(uc,"E.g."),uc.forEach(s),$l=n(te,` learning a meaningful context-independent
representation for the letter `),bs=l(te,"CODE",{});var mc=r(bs);xl=n(mc,'"t"'),mc.forEach(s),Pl=n(te,` is much harder than learning a context-independent representation for the word
`),vs=l(te,"CODE",{});var dc=r(vs);Tl=n(dc,'"today"'),dc.forEach(s),Al=n(te,`. Therefore, character tokenization is often accompanied by a loss of performance. So to get the best of
both worlds, transformers models use a hybrid between word-level and character-level tokenization called `),ys=l(te,"STRONG",{});var fc=r(ys);Dl=n(fc,"subword"),fc.forEach(s),Sl=n(te,`
tokenization.`),te.forEach(s),hn=u(e),ie=l(e,"H3",{class:!0});var ho=r(ie);ye=l(ho,"A",{id:!0,class:!0,href:!0});var gc=r(ye);ws=l(gc,"SPAN",{});var bc=r(ws);g(Ge.$$.fragment,bc),bc.forEach(s),gc.forEach(s),Cl=u(ho),ks=l(ho,"SPAN",{});var vc=r(ks);Bl=n(vc,"Subword tokenization"),vc.forEach(s),ho.forEach(s),cn=u(e),g(Xe.$$.fragment,e),un=u(e),k=l(e,"P",{});var j=r(k);Ol=n(j,`Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller
subwords, but rare words should be decomposed into meaningful subwords. For instance `),Es=l(j,"CODE",{});var yc=r(Es);Ml=n(yc,'"annoyingly"'),yc.forEach(s),Nl=n(j,` might be
considered a rare word and could be decomposed into `),zs=l(j,"CODE",{});var wc=r(zs);Wl=n(wc,'"annoying"'),wc.forEach(s),Il=n(j," and "),_s=l(j,"CODE",{});var kc=r(_s);Ll=n(kc,'"ly"'),kc.forEach(s),Ul=n(j,". Both "),qs=l(j,"CODE",{});var Ec=r(qs);Hl=n(Ec,'"annoying"'),Ec.forEach(s),Rl=n(j," and "),js=l(j,"CODE",{});var zc=r(js);Gl=n(zc,'"ly"'),zc.forEach(s),Xl=n(j,` as
stand-alone subwords would appear more frequently while at the same time the meaning of `),$s=l(j,"CODE",{});var _c=r($s);Fl=n(_c,'"annoyingly"'),_c.forEach(s),Kl=n(j,` is kept by the
composite meaning of `),xs=l(j,"CODE",{});var qc=r(xs);Vl=n(qc,'"annoying"'),qc.forEach(s),Jl=n(j," and "),Ps=l(j,"CODE",{});var jc=r(Ps);Yl=n(jc,'"ly"'),jc.forEach(s),Zl=n(j,`. This is especially useful in agglutinative languages such as Turkish,
where you can form (almost) arbitrarily long complex words by stringing together subwords.`),j.forEach(s),mn=u(e),V=l(e,"P",{});var Yt=r(V);Ql=n(Yt,`Subword tokenization allows the model to have a reasonable vocabulary size while being able to learn meaningful
context-independent representations. In addition, subword tokenization enables the model to process words it has never
seen before, by decomposing them into known subwords. For instance, the `),kt=l(Yt,"A",{href:!0});var $c=r(kt);er=n($c,"BertTokenizer"),$c.forEach(s),tr=n(Yt,` tokenizes
`),Ts=l(Yt,"CODE",{});var xc=r(Ts);sr=n(xc,'"I have a new GPU!"'),xc.forEach(s),ar=n(Yt," as follows:"),Yt.forEach(s),dn=u(e),g(Fe.$$.fragment,e),fn=u(e),P=l(e,"P",{});var G=r(P);nr=n(G,"Because we are considering the uncased model, the sentence was lowercased first. We can see that the words "),As=l(G,"CODE",{});var Pc=r(As);or=n(Pc,'["i", "have", "a", "new"]'),Pc.forEach(s),lr=n(G," are present in the tokenizer\u2019s vocabulary, but the word "),Ds=l(G,"CODE",{});var Tc=r(Ds);rr=n(Tc,'"gpu"'),Tc.forEach(s),ir=n(G,` is not. Consequently, the
tokenizer splits `),Ss=l(G,"CODE",{});var Ac=r(Ss);pr=n(Ac,'"gpu"'),Ac.forEach(s),hr=n(G," into known subwords: "),Cs=l(G,"CODE",{});var Dc=r(Cs);cr=n(Dc,'["gp" and "##u"]'),Dc.forEach(s),ur=n(G,". "),Bs=l(G,"CODE",{});var Sc=r(Bs);mr=n(Sc,'"##"'),Sc.forEach(s),dr=n(G,` means that the rest of the token should
be attached to the previous one, without space (for decoding or reversal of the tokenization).`),G.forEach(s),gn=u(e),we=l(e,"P",{});var co=r(we);fr=n(co,"As another example, "),Et=l(co,"A",{href:!0});var Cc=r(Et);gr=n(Cc,"XLNetTokenizer"),Cc.forEach(s),br=n(co," tokenizes our previously exemplary text as follows:"),co.forEach(s),bn=u(e),g(Ke.$$.fragment,e),vn=u(e),T=l(e,"P",{});var X=r(T);vr=n(X,"We\u2019ll get back to the meaning of those "),Os=l(X,"CODE",{});var Bc=r(Os);yr=n(Bc,'"\u2581"'),Bc.forEach(s),wr=n(X," when we look at "),zt=l(X,"A",{href:!0});var Oc=r(zt);kr=n(Oc,"SentencePiece"),Oc.forEach(s),Er=n(X,`. As one can see,
the rare word `),Ms=l(X,"CODE",{});var Mc=r(Ms);zr=n(Mc,'"Transformers"'),Mc.forEach(s),_r=n(X," has been split into the more frequent subwords "),Ns=l(X,"CODE",{});var Nc=r(Ns);qr=n(Nc,'"Transform"'),Nc.forEach(s),jr=n(X," and "),Ws=l(X,"CODE",{});var Wc=r(Ws);$r=n(Wc,'"ers"'),Wc.forEach(s),xr=n(X,"."),X.forEach(s),yn=u(e),_t=l(e,"P",{});var Ic=r(_t);Pr=n(Ic,`Let\u2019s now look at how the different subword tokenization algorithms work. Note that all of those tokenization
algorithms rely on some form of training which is usually done on the corpus the corresponding model will be trained
on.`),Ic.forEach(s),wn=u(e),qt=l(e,"A",{id:!0}),r(qt).forEach(s),kn=u(e),pe=l(e,"H2",{class:!0});var uo=r(pe);ke=l(uo,"A",{id:!0,class:!0,href:!0});var Lc=r(ke);Is=l(Lc,"SPAN",{});var Uc=r(Is);g(Ve.$$.fragment,Uc),Uc.forEach(s),Lc.forEach(s),Tr=u(uo),Ls=l(uo,"SPAN",{});var Hc=r(Ls);Ar=n(Hc,"Byte-Pair Encoding (BPE)"),Hc.forEach(s),uo.forEach(s),En=u(e),q=l(e,"P",{});var N=r(q);Dr=n(N,"Byte-Pair Encoding (BPE) was introduced in "),Je=l(N,"A",{href:!0,rel:!0});var Rc=r(Je);Sr=n(Rc,`Neural Machine Translation of Rare Words with Subword Units (Sennrich et
al., 2015)`),Rc.forEach(s),Cr=n(N,`. BPE relies on a pre-tokenizer that splits the training data into
words. Pretokenization can be as simple as space tokenization, e.g. `),jt=l(N,"A",{href:!0});var Gc=r(jt);Br=n(Gc,"GPT-2"),Gc.forEach(s),Or=n(N,", "),$t=l(N,"A",{href:!0});var Xc=r($t);Mr=n(Xc,"Roberta"),Xc.forEach(s),Nr=n(N,". More advanced pre-tokenization include rule-based tokenization, e.g. "),xt=l(N,"A",{href:!0});var Fc=r(xt);Wr=n(Fc,"XLM"),Fc.forEach(s),Ir=n(N,`,
`),Pt=l(N,"A",{href:!0});var Kc=r(Pt);Lr=n(Kc,"FlauBERT"),Kc.forEach(s),Ur=n(N," which uses Moses for most languages, or "),Tt=l(N,"A",{href:!0});var Vc=r(Tt);Hr=n(Vc,"GPT"),Vc.forEach(s),Rr=n(N,` which uses
Spacy and ftfy, to count the frequency of each word in the training corpus.`),N.forEach(s),zn=u(e),At=l(e,"P",{});var Jc=r(At);Gr=n(Jc,`After pre-tokenization, a set of unique words has been created and the frequency of each word it occurred in the
training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set
of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until
the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to
define before training the tokenizer.`),Jc.forEach(s),_n=u(e),Dt=l(e,"P",{});var Yc=r(Dt);Xr=n(Yc,`As an example, let\u2019s assume that after pre-tokenization, the following set of words including their frequency has been
determined:`),Yc.forEach(s),qn=u(e),g(Ye.$$.fragment,e),jn=u(e),Ee=l(e,"P",{});var mo=r(Ee);Fr=n(mo,"Consequently, the base vocabulary is "),Us=l(mo,"CODE",{});var Zc=r(Us);Kr=n(Zc,'["b", "g", "h", "n", "p", "s", "u"]'),Zc.forEach(s),Vr=n(mo,`. Splitting all words into symbols of the
base vocabulary, we obtain:`),mo.forEach(s),$n=u(e),g(Ze.$$.fragment,e),xn=u(e),m=l(e,"P",{});var d=r(m);Jr=n(d,`BPE then counts the frequency of each possible symbol pair and picks the symbol pair that occurs most frequently. In
the example above `),Hs=l(d,"CODE",{});var Qc=r(Hs);Yr=n(Qc,'"h"'),Qc.forEach(s),Zr=n(d," followed by "),Rs=l(d,"CODE",{});var eu=r(Rs);Qr=n(eu,'"u"'),eu.forEach(s),ei=n(d," is present "),Gs=l(d,"EM",{});var tu=r(Gs);ti=n(tu,"10 + 5 = 15"),tu.forEach(s),si=n(d,` times (10 times in the 10 occurrences of
`),Xs=l(d,"CODE",{});var su=r(Xs);ai=n(su,'"hug"'),su.forEach(s),ni=n(d,", 5 times in the 5 occurrences of "),Fs=l(d,"CODE",{});var au=r(Fs);oi=n(au,'"hugs"'),au.forEach(s),li=n(d,"). However, the most frequent symbol pair is "),Ks=l(d,"CODE",{});var nu=r(Ks);ri=n(nu,'"u"'),nu.forEach(s),ii=n(d,` followed by
`),Vs=l(d,"CODE",{});var ou=r(Vs);pi=n(ou,'"g"'),ou.forEach(s),hi=n(d,", occurring "),Js=l(d,"EM",{});var lu=r(Js);ci=n(lu,"10 + 5 + 5 = 20"),lu.forEach(s),ui=n(d,` times in total. Thus, the first merge rule the tokenizer learns is to group all
`),Ys=l(d,"CODE",{});var ru=r(Ys);mi=n(ru,'"u"'),ru.forEach(s),di=n(d," symbols followed by a "),Zs=l(d,"CODE",{});var iu=r(Zs);fi=n(iu,'"g"'),iu.forEach(s),gi=n(d," symbol together. Next, "),Qs=l(d,"CODE",{});var pu=r(Qs);bi=n(pu,'"ug"'),pu.forEach(s),vi=n(d,` is added to the vocabulary. The set of words then
becomes`),d.forEach(s),Pn=u(e),g(Qe.$$.fragment,e),Tn=u(e),E=l(e,"P",{});var $=r(E);yi=n($,"BPE then identifies the next most common symbol pair. It\u2019s "),ea=l($,"CODE",{});var hu=r(ea);wi=n(hu,'"u"'),hu.forEach(s),ki=n($," followed by "),ta=l($,"CODE",{});var cu=r(ta);Ei=n(cu,'"n"'),cu.forEach(s),zi=n($,", which occurs 16 times. "),sa=l($,"CODE",{});var uu=r(sa);_i=n(uu,'"u"'),uu.forEach(s),qi=n($,`,
`),aa=l($,"CODE",{});var mu=r(aa);ji=n(mu,'"n"'),mu.forEach(s),$i=n($," is merged to "),na=l($,"CODE",{});var du=r(na);xi=n(du,'"un"'),du.forEach(s),Pi=n($," and added to the vocabulary. The next most frequent symbol pair is "),oa=l($,"CODE",{});var fu=r(oa);Ti=n(fu,'"h"'),fu.forEach(s),Ai=n($,` followed by
`),la=l($,"CODE",{});var gu=r(la);Di=n(gu,'"ug"'),gu.forEach(s),Si=n($,", occurring 15 times. Again the pair is merged and "),ra=l($,"CODE",{});var bu=r(ra);Ci=n(bu,'"hug"'),bu.forEach(s),Bi=n($," can be added to the vocabulary."),$.forEach(s),An=u(e),ze=l(e,"P",{});var fo=r(ze);Oi=n(fo,"At this stage, the vocabulary is "),ia=l(fo,"CODE",{});var vu=r(ia);Mi=n(vu,'["b", "g", "h", "n", "p", "s", "u", "ug", "un", "hug"]'),vu.forEach(s),Ni=n(fo,` and our set of unique words
is represented as`),fo.forEach(s),Dn=u(e),g(et.$$.fragment,e),Sn=u(e),_=l(e,"P",{});var A=r(_);Wi=n(A,`Assuming, that the Byte-Pair Encoding training would stop at this point, the learned merge rules would then be applied
to new words (as long as those new words do not include symbols that were not in the base vocabulary). For instance,
the word `),pa=l(A,"CODE",{});var yu=r(pa);Ii=n(yu,'"bug"'),yu.forEach(s),Li=n(A," would be tokenized to "),ha=l(A,"CODE",{});var wu=r(ha);Ui=n(wu,'["b", "ug"]'),wu.forEach(s),Hi=n(A," but "),ca=l(A,"CODE",{});var ku=r(ca);Ri=n(ku,'"mug"'),ku.forEach(s),Gi=n(A," would be tokenized as "),ua=l(A,"CODE",{});var Eu=r(ua);Xi=n(Eu,'["<unk>", "ug"]'),Eu.forEach(s),Fi=n(A,` since
the symbol `),ma=l(A,"CODE",{});var zu=r(ma);Ki=n(zu,'"m"'),zu.forEach(s),Vi=n(A," is not in the base vocabulary. In general, single letters such as "),da=l(A,"CODE",{});var _u=r(da);Ji=n(_u,'"m"'),_u.forEach(s),Yi=n(A,` are not replaced by the
`),fa=l(A,"CODE",{});var qu=r(fa);Zi=n(qu,'"<unk>"'),qu.forEach(s),Qi=n(A,` symbol because the training data usually includes at least one occurrence of each letter, but it is likely
to happen for very special characters like emojis.`),A.forEach(s),Cn=u(e),J=l(e,"P",{});var Zt=r(J);ep=n(Zt,"As mentioned earlier, the vocabulary size, "),ga=l(Zt,"EM",{});var ju=r(ga);tp=n(ju,"i.e."),ju.forEach(s),sp=n(Zt,` the base vocabulary size + the number of merges, is a hyperparameter
to choose. For instance `),St=l(Zt,"A",{href:!0});var $u=r(St);ap=n($u,"GPT"),$u.forEach(s),np=n(Zt,` has a vocabulary size of 40,478 since they have 478 base characters
and chose to stop training after 40,000 merges.`),Zt.forEach(s),Bn=u(e),he=l(e,"H3",{class:!0});var go=r(he);_e=l(go,"A",{id:!0,class:!0,href:!0});var xu=r(_e);ba=l(xu,"SPAN",{});var Pu=r(ba);g(tt.$$.fragment,Pu),Pu.forEach(s),xu.forEach(s),op=u(go),va=l(go,"SPAN",{});var Tu=r(va);lp=n(Tu,"Byte-level BPE"),Tu.forEach(s),go.forEach(s),On=u(e),U=l(e,"P",{});var Pe=r(U);rp=n(Pe,"A base vocabulary that includes all possible base characters can be quite large if "),ya=l(Pe,"EM",{});var Au=r(ya);ip=n(Au,"e.g."),Au.forEach(s),pp=n(Pe,` all unicode characters are
considered as base characters. To have a better base vocabulary, `),st=l(Pe,"A",{href:!0,rel:!0});var Du=r(st);hp=n(Du,"GPT-2"),Du.forEach(s),cp=n(Pe,` uses bytes
as the base vocabulary, which is a clever trick to force the base vocabulary to be of size 256 while ensuring that
every base character is included in the vocabulary. With some additional rules to deal with punctuation, the GPT2\u2019s
tokenizer can tokenize every text without the need for the <unk> symbol. `),Ct=l(Pe,"A",{href:!0});var Su=r(Ct);up=n(Su,"GPT-2"),Su.forEach(s),mp=n(Pe,` has a vocabulary
size of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned
with 50,000 merges.`),Pe.forEach(s),Mn=u(e),Bt=l(e,"A",{id:!0}),r(Bt).forEach(s),Nn=u(e),ce=l(e,"H4",{class:!0});var bo=r(ce);qe=l(bo,"A",{id:!0,class:!0,href:!0});var Cu=r(qe);wa=l(Cu,"SPAN",{});var Bu=r(wa);g(at.$$.fragment,Bu),Bu.forEach(s),Cu.forEach(s),dp=u(bo),ka=l(bo,"SPAN",{});var Ou=r(ka);fp=n(Ou,"WordPiece"),Ou.forEach(s),bo.forEach(s),Wn=u(e),O=l(e,"P",{});var se=r(O);gp=n(se,"WordPiece is the subword tokenization algorithm used for "),Ot=l(se,"A",{href:!0});var Mu=r(Ot);bp=n(Mu,"BERT"),Mu.forEach(s),vp=n(se,", "),Mt=l(se,"A",{href:!0});var Nu=r(Mt);yp=n(Nu,"DistilBERT"),Nu.forEach(s),wp=n(se,", and "),Nt=l(se,"A",{href:!0});var Wu=r(Nt);kp=n(Wu,"Electra"),Wu.forEach(s),Ep=n(se,". The algorithm was outlined in "),nt=l(se,"A",{href:!0,rel:!0});var Iu=r(nt);zp=n(Iu,`Japanese and Korean
Voice Search (Schuster et al., 2012)`),Iu.forEach(s),_p=n(se,` and is very similar to
BPE. WordPiece first initializes the vocabulary to include every character present in the training data and
progressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent
symbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.`),se.forEach(s),In=u(e),z=l(e,"P",{});var x=r(z);qp=n(x,`So what does this mean exactly? Referring to the previous example, maximizing the likelihood of the training data is
equivalent to finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by
its second symbol is the greatest among all symbol pairs. `),Ea=l(x,"EM",{});var Lu=r(Ea);jp=n(Lu,"E.g."),Lu.forEach(s),$p=u(x),za=l(x,"CODE",{});var Uu=r(za);xp=n(Uu,'"u"'),Uu.forEach(s),Pp=n(x,", followed by "),_a=l(x,"CODE",{});var Hu=r(_a);Tp=n(Hu,'"g"'),Hu.forEach(s),Ap=n(x,` would have only been
merged if the probability of `),qa=l(x,"CODE",{});var Ru=r(qa);Dp=n(Ru,'"ug"'),Ru.forEach(s),Sp=n(x," divided by "),ja=l(x,"CODE",{});var Gu=r(ja);Cp=n(Gu,'"u"'),Gu.forEach(s),Bp=n(x,", "),$a=l(x,"CODE",{});var Xu=r($a);Op=n(Xu,'"g"'),Xu.forEach(s),Mp=n(x,` would have been greater than for any other symbol
pair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it `),xa=l(x,"EM",{});var Fu=r(xa);Np=n(Fu,"loses"),Fu.forEach(s),Wp=n(x,` by merging two symbols
to ensure it\u2019s `),Pa=l(x,"EM",{});var Ku=r(Pa);Ip=n(Ku,"worth it"),Ku.forEach(s),Lp=n(x,"."),x.forEach(s),Ln=u(e),Wt=l(e,"A",{id:!0}),r(Wt).forEach(s),Un=u(e),ue=l(e,"H4",{class:!0});var vo=r(ue);je=l(vo,"A",{id:!0,class:!0,href:!0});var Vu=r(je);Ta=l(Vu,"SPAN",{});var Ju=r(Ta);g(ot.$$.fragment,Ju),Ju.forEach(s),Vu.forEach(s),Up=u(vo),Aa=l(vo,"SPAN",{});var Yu=r(Aa);Hp=n(Yu,"Unigram"),Yu.forEach(s),vo.forEach(s),Hn=u(e),Y=l(e,"P",{});var Qt=r(Y);Rp=n(Qt,"Unigram is a subword tokenization algorithm introduced in "),lt=l(Qt,"A",{href:!0,rel:!0});var Zu=r(lt);Gp=n(Zu,`Subword Regularization: Improving Neural Network Translation
Models with Multiple Subword Candidates (Kudo, 2018)`),Zu.forEach(s),Xp=n(Qt,`. In contrast to BPE or
WordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each
symbol to obtain a smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and
the most common substrings. Unigram is not used directly for any of the models in the transformers, but it\u2019s used in
conjunction with `),It=l(Qt,"A",{href:!0});var Qu=r(It);Fp=n(Qu,"SentencePiece"),Qu.forEach(s),Kp=n(Qt,"."),Qt.forEach(s),Rn=u(e),$e=l(e,"P",{});var yo=r($e);Vp=n(yo,`At each training step, the Unigram algorithm defines a loss (often defined as the log-likelihood) over the training
data given the current vocabulary and a unigram language model. Then, for each symbol in the vocabulary, the algorithm
computes how much the overall loss would increase if the symbol was to be removed from the vocabulary. Unigram then
removes p (with p usually being 10% or 20%) percent of the symbols whose loss increase is the lowest, `),Da=l(yo,"EM",{});var em=r(Da);Jp=n(em,"i.e."),em.forEach(s),Yp=n(yo,` those
symbols that least affect the overall loss over the training data. This process is repeated until the vocabulary has
reached the desired size. The Unigram algorithm always keeps the base characters so that any word can be tokenized.`),yo.forEach(s),Gn=u(e),Lt=l(e,"P",{});var tm=r(Lt);Zp=n(tm,`Because Unigram is not based on merge rules (in contrast to BPE and WordPiece), the algorithm has several ways of
tokenizing new text after training. As an example, if a trained Unigram tokenizer exhibits the vocabulary:`),tm.forEach(s),Xn=u(e),g(rt.$$.fragment,e),Fn=u(e),I=l(e,"P",{});var fe=r(I);Sa=l(fe,"CODE",{});var sm=r(Sa);Qp=n(sm,'"hugs"'),sm.forEach(s),eh=n(fe," could be tokenized both as "),Ca=l(fe,"CODE",{});var am=r(Ca);th=n(am,'["hug", "s"]'),am.forEach(s),sh=n(fe,", "),Ba=l(fe,"CODE",{});var nm=r(Ba);ah=n(nm,'["h", "ug", "s"]'),nm.forEach(s),nh=n(fe," or "),Oa=l(fe,"CODE",{});var om=r(Oa);oh=n(om,'["h", "u", "g", "s"]'),om.forEach(s),lh=n(fe,`. So which one
to choose? Unigram saves the probability of each token in the training corpus on top of saving the vocabulary so that
the probability of each possible tokenization can be computed after training. The algorithm simply picks the most
likely tokenization in practice, but also offers the possibility to sample a possible tokenization according to their
probabilities.`),fe.forEach(s),Kn=u(e),L=l(e,"P",{});var ge=r(L);rh=n(ge,`Those probabilities are defined by the loss the tokenizer is trained on. Assuming that the training data consists of
the words `),Vn=Eo(ge),Jn=n(ge," and that the set of all possible tokenizations for a word "),Yn=Eo(ge),Zn=n(ge,` is
defined as `),Qn=Eo(ge),eo=n(ge,`, then the overall loss is defined as
`),to=Eo(ge),ge.forEach(s),so=u(e),Ut=l(e,"A",{id:!0}),r(Ut).forEach(s),ao=u(e),me=l(e,"H4",{class:!0});var wo=r(me);xe=l(wo,"A",{id:!0,class:!0,href:!0});var lm=r(xe);Ma=l(lm,"SPAN",{});var rm=r(Ma);g(it.$$.fragment,rm),rm.forEach(s),lm.forEach(s),ih=u(wo),Na=l(wo,"SPAN",{});var im=r(Na);ph=n(im,"SentencePiece"),im.forEach(s),wo.forEach(s),no=u(e),H=l(e,"P",{});var Te=r(H);hh=n(Te,`All tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to
separate words. However, not all languages use spaces to separate words. One possible solution is to use language
specific pre-tokenizers, `),Wa=l(Te,"EM",{});var pm=r(Wa);ch=n(pm,"e.g."),pm.forEach(s),uh=u(Te),Ht=l(Te,"A",{href:!0});var hm=r(Ht);mh=n(hm,"XLM"),hm.forEach(s),dh=n(Te,` uses a specific Chinese, Japanese, and Thai pre-tokenizer).
To solve this problem more generally, `),pt=l(Te,"A",{href:!0,rel:!0});var cm=r(pt);fh=n(cm,`SentencePiece: A simple and language independent subword tokenizer and
detokenizer for Neural Text Processing (Kudo et al., 2018)`),cm.forEach(s),gh=n(Te,` treats the input
as a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram
algorithm to construct the appropriate vocabulary.`),Te.forEach(s),oo=u(e),R=l(e,"P",{});var Ae=r(R);bh=n(Ae,"The "),Rt=l(Ae,"A",{href:!0});var um=r(Rt);vh=n(um,"XLNetTokenizer"),um.forEach(s),yh=n(Ae,` uses SentencePiece for example, which is also why in the example earlier the
`),Ia=l(Ae,"CODE",{});var mm=r(Ia);wh=n(mm,'"\u2581"'),mm.forEach(s),kh=n(Ae,` character was included in the vocabulary. Decoding with SentencePiece is very easy since all tokens can just be
concatenated and `),La=l(Ae,"CODE",{});var dm=r(La);Eh=n(dm,'"\u2581"'),dm.forEach(s),zh=n(Ae," is replaced by a space."),Ae.forEach(s),lo=u(e),M=l(e,"P",{});var ae=r(M);_h=n(ae,`All transformers models in the library that use SentencePiece use it in combination with unigram. Examples of models
using SentencePiece are `),Gt=l(ae,"A",{href:!0});var fm=r(Gt);qh=n(fm,"ALBERT"),fm.forEach(s),jh=n(ae,", "),Xt=l(ae,"A",{href:!0});var gm=r(Xt);$h=n(gm,"XLNet"),gm.forEach(s),xh=n(ae,", "),Ft=l(ae,"A",{href:!0});var bm=r(Ft);Ph=n(bm,"Marian"),bm.forEach(s),Th=n(ae,", and "),Kt=l(ae,"A",{href:!0});var vm=r(Kt);Ah=n(vm,"T5"),vm.forEach(s),Dh=n(ae,"."),ae.forEach(s),this.h()},h(){h(oe,"name","hf:doc:metadata"),h(oe,"content",JSON.stringify(Am)),h(be,"id","summary-of-the-tokenizers"),h(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(be,"href","#summary-of-the-tokenizers"),h(le,"class","relative group"),h(ct,"href","preprocessing"),h(ut,"href","#byte-pair-encoding"),h(mt,"href","#wordpiece"),h(dt,"href","#sentencepiece"),h(ft,"href","/docs/transformers/v4.21.2/en/model_doc/bert#transformers.BertTokenizer"),h(gt,"href","#wordpiece"),h(ve,"id","introduction"),h(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ve,"href","#introduction"),h(re,"class","relative group"),h(Le,"href","https://spacy.io/"),h(Le,"rel","nofollow"),h(Ue,"href","http://www.statmt.org/moses/?n=Development.GetStarted"),h(Ue,"rel","nofollow"),h(vt,"href","model_doc/transformerxl"),h(ye,"id","subword-tokenization"),h(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ye,"href","#subword-tokenization"),h(ie,"class","relative group"),h(kt,"href","/docs/transformers/v4.21.2/en/model_doc/bert#transformers.BertTokenizer"),h(Et,"href","/docs/transformers/v4.21.2/en/model_doc/xlnet#transformers.XLNetTokenizer"),h(zt,"href","#sentencepiece"),h(qt,"id","byte-pair-encoding"),h(ke,"id","bytepair-encoding-bpe"),h(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ke,"href","#bytepair-encoding-bpe"),h(pe,"class","relative group"),h(Je,"href","https://arxiv.org/abs/1508.07909"),h(Je,"rel","nofollow"),h(jt,"href","model_doc/gpt2"),h($t,"href","model_doc/roberta"),h(xt,"href","model_doc/xlm"),h(Pt,"href","model_doc/flaubert"),h(Tt,"href","model_doc/gpt"),h(St,"href","model_doc/gpt"),h(_e,"id","bytelevel-bpe"),h(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(_e,"href","#bytelevel-bpe"),h(he,"class","relative group"),h(st,"href","https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"),h(st,"rel","nofollow"),h(Ct,"href","model_doc/gpt"),h(Bt,"id","wordpiece"),h(qe,"id","wordpiece"),h(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(qe,"href","#wordpiece"),h(ce,"class","relative group"),h(Ot,"href","model_doc/bert"),h(Mt,"href","model_doc/distilbert"),h(Nt,"href","model_doc/electra"),h(nt,"href","https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf"),h(nt,"rel","nofollow"),h(Wt,"id","unigram"),h(je,"id","unigram"),h(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(je,"href","#unigram"),h(ue,"class","relative group"),h(lt,"href","https://arxiv.org/pdf/1804.10959.pdf"),h(lt,"rel","nofollow"),h(It,"href","#sentencepiece"),Vn.a=Jn,Yn.a=Zn,Qn.a=eo,to.a=null,h(Ut,"id","sentencepiece"),h(xe,"id","sentencepiece"),h(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(xe,"href","#sentencepiece"),h(me,"class","relative group"),h(Ht,"href","model_doc/xlm"),h(pt,"href","https://arxiv.org/pdf/1808.06226.pdf"),h(pt,"rel","nofollow"),h(Rt,"href","/docs/transformers/v4.21.2/en/model_doc/xlnet#transformers.XLNetTokenizer"),h(Gt,"href","model_doc/albert"),h(Xt,"href","model_doc/xlnet"),h(Ft,"href","model_doc/marian"),h(Kt,"href","model_doc/t5")},m(e,i){t(document.head,oe),p(e,Ua,i),p(e,le,i),t(le,be),t(be,es),b(Se,es,null),t(le,_o),t(le,ts),t(ts,qo),p(e,Ha,i),b(Ce,e,i),p(e,Ra,i),p(e,ht,i),t(ht,jo),p(e,Ga,i),b(Be,e,i),p(e,Xa,i),p(e,D,i),t(D,$o),t(D,ct),t(ct,xo),t(D,Po),t(D,ut),t(ut,To),t(D,Ao),t(D,mt),t(mt,Do),t(D,So),t(D,dt),t(dt,Co),t(D,Bo),p(e,Fa,i),p(e,F,i),t(F,Oo),t(F,ft),t(ft,Mo),t(F,No),t(F,gt),t(gt,Wo),t(F,Io),p(e,Ka,i),p(e,re,i),t(re,ve),t(ve,ss),b(Oe,ss,null),t(re,Lo),t(re,as),t(as,Uo),p(e,Va,i),p(e,Me,i),t(Me,Ho),t(Me,ns),t(ns,Ro),p(e,Ja,i),b(Ne,e,i),p(e,Ya,i),p(e,bt,i),t(bt,Go),p(e,Za,i),b(We,e,i),p(e,Qa,i),p(e,S,i),t(S,Xo),t(S,os),t(os,Fo),t(S,Ko),t(S,ls),t(ls,Vo),t(S,Jo),t(S,rs),t(rs,Yo),t(S,Zo),t(S,is),t(is,Qo),t(S,el),p(e,en,i),b(Ie,e,i),p(e,tn,i),p(e,C,i),t(C,tl),t(C,ps),t(ps,sl),t(C,al),t(C,hs),t(hs,nl),t(C,ol),t(C,cs),t(cs,ll),t(C,rl),t(C,us),t(us,il),t(C,pl),p(e,sn,i),p(e,W,i),t(W,Le),t(Le,hl),t(W,cl),t(W,Ue),t(Ue,ul),t(W,ml),t(W,ms),t(ms,dl),t(W,fl),t(W,ds),t(ds,gl),t(W,bl),p(e,an,i),b(He,e,i),p(e,nn,i),p(e,K,i),t(K,vl),t(K,fs),t(fs,yl),t(K,wl),t(K,vt),t(vt,kl),t(K,El),p(e,on,i),p(e,yt,i),t(yt,zl),p(e,ln,i),p(e,wt,i),t(wt,_l),p(e,rn,i),b(Re,e,i),p(e,pn,i),p(e,B,i),t(B,ql),t(B,gs),t(gs,jl),t(B,$l),t(B,bs),t(bs,xl),t(B,Pl),t(B,vs),t(vs,Tl),t(B,Al),t(B,ys),t(ys,Dl),t(B,Sl),p(e,hn,i),p(e,ie,i),t(ie,ye),t(ye,ws),b(Ge,ws,null),t(ie,Cl),t(ie,ks),t(ks,Bl),p(e,cn,i),b(Xe,e,i),p(e,un,i),p(e,k,i),t(k,Ol),t(k,Es),t(Es,Ml),t(k,Nl),t(k,zs),t(zs,Wl),t(k,Il),t(k,_s),t(_s,Ll),t(k,Ul),t(k,qs),t(qs,Hl),t(k,Rl),t(k,js),t(js,Gl),t(k,Xl),t(k,$s),t($s,Fl),t(k,Kl),t(k,xs),t(xs,Vl),t(k,Jl),t(k,Ps),t(Ps,Yl),t(k,Zl),p(e,mn,i),p(e,V,i),t(V,Ql),t(V,kt),t(kt,er),t(V,tr),t(V,Ts),t(Ts,sr),t(V,ar),p(e,dn,i),b(Fe,e,i),p(e,fn,i),p(e,P,i),t(P,nr),t(P,As),t(As,or),t(P,lr),t(P,Ds),t(Ds,rr),t(P,ir),t(P,Ss),t(Ss,pr),t(P,hr),t(P,Cs),t(Cs,cr),t(P,ur),t(P,Bs),t(Bs,mr),t(P,dr),p(e,gn,i),p(e,we,i),t(we,fr),t(we,Et),t(Et,gr),t(we,br),p(e,bn,i),b(Ke,e,i),p(e,vn,i),p(e,T,i),t(T,vr),t(T,Os),t(Os,yr),t(T,wr),t(T,zt),t(zt,kr),t(T,Er),t(T,Ms),t(Ms,zr),t(T,_r),t(T,Ns),t(Ns,qr),t(T,jr),t(T,Ws),t(Ws,$r),t(T,xr),p(e,yn,i),p(e,_t,i),t(_t,Pr),p(e,wn,i),p(e,qt,i),p(e,kn,i),p(e,pe,i),t(pe,ke),t(ke,Is),b(Ve,Is,null),t(pe,Tr),t(pe,Ls),t(Ls,Ar),p(e,En,i),p(e,q,i),t(q,Dr),t(q,Je),t(Je,Sr),t(q,Cr),t(q,jt),t(jt,Br),t(q,Or),t(q,$t),t($t,Mr),t(q,Nr),t(q,xt),t(xt,Wr),t(q,Ir),t(q,Pt),t(Pt,Lr),t(q,Ur),t(q,Tt),t(Tt,Hr),t(q,Rr),p(e,zn,i),p(e,At,i),t(At,Gr),p(e,_n,i),p(e,Dt,i),t(Dt,Xr),p(e,qn,i),b(Ye,e,i),p(e,jn,i),p(e,Ee,i),t(Ee,Fr),t(Ee,Us),t(Us,Kr),t(Ee,Vr),p(e,$n,i),b(Ze,e,i),p(e,xn,i),p(e,m,i),t(m,Jr),t(m,Hs),t(Hs,Yr),t(m,Zr),t(m,Rs),t(Rs,Qr),t(m,ei),t(m,Gs),t(Gs,ti),t(m,si),t(m,Xs),t(Xs,ai),t(m,ni),t(m,Fs),t(Fs,oi),t(m,li),t(m,Ks),t(Ks,ri),t(m,ii),t(m,Vs),t(Vs,pi),t(m,hi),t(m,Js),t(Js,ci),t(m,ui),t(m,Ys),t(Ys,mi),t(m,di),t(m,Zs),t(Zs,fi),t(m,gi),t(m,Qs),t(Qs,bi),t(m,vi),p(e,Pn,i),b(Qe,e,i),p(e,Tn,i),p(e,E,i),t(E,yi),t(E,ea),t(ea,wi),t(E,ki),t(E,ta),t(ta,Ei),t(E,zi),t(E,sa),t(sa,_i),t(E,qi),t(E,aa),t(aa,ji),t(E,$i),t(E,na),t(na,xi),t(E,Pi),t(E,oa),t(oa,Ti),t(E,Ai),t(E,la),t(la,Di),t(E,Si),t(E,ra),t(ra,Ci),t(E,Bi),p(e,An,i),p(e,ze,i),t(ze,Oi),t(ze,ia),t(ia,Mi),t(ze,Ni),p(e,Dn,i),b(et,e,i),p(e,Sn,i),p(e,_,i),t(_,Wi),t(_,pa),t(pa,Ii),t(_,Li),t(_,ha),t(ha,Ui),t(_,Hi),t(_,ca),t(ca,Ri),t(_,Gi),t(_,ua),t(ua,Xi),t(_,Fi),t(_,ma),t(ma,Ki),t(_,Vi),t(_,da),t(da,Ji),t(_,Yi),t(_,fa),t(fa,Zi),t(_,Qi),p(e,Cn,i),p(e,J,i),t(J,ep),t(J,ga),t(ga,tp),t(J,sp),t(J,St),t(St,ap),t(J,np),p(e,Bn,i),p(e,he,i),t(he,_e),t(_e,ba),b(tt,ba,null),t(he,op),t(he,va),t(va,lp),p(e,On,i),p(e,U,i),t(U,rp),t(U,ya),t(ya,ip),t(U,pp),t(U,st),t(st,hp),t(U,cp),t(U,Ct),t(Ct,up),t(U,mp),p(e,Mn,i),p(e,Bt,i),p(e,Nn,i),p(e,ce,i),t(ce,qe),t(qe,wa),b(at,wa,null),t(ce,dp),t(ce,ka),t(ka,fp),p(e,Wn,i),p(e,O,i),t(O,gp),t(O,Ot),t(Ot,bp),t(O,vp),t(O,Mt),t(Mt,yp),t(O,wp),t(O,Nt),t(Nt,kp),t(O,Ep),t(O,nt),t(nt,zp),t(O,_p),p(e,In,i),p(e,z,i),t(z,qp),t(z,Ea),t(Ea,jp),t(z,$p),t(z,za),t(za,xp),t(z,Pp),t(z,_a),t(_a,Tp),t(z,Ap),t(z,qa),t(qa,Dp),t(z,Sp),t(z,ja),t(ja,Cp),t(z,Bp),t(z,$a),t($a,Op),t(z,Mp),t(z,xa),t(xa,Np),t(z,Wp),t(z,Pa),t(Pa,Ip),t(z,Lp),p(e,Ln,i),p(e,Wt,i),p(e,Un,i),p(e,ue,i),t(ue,je),t(je,Ta),b(ot,Ta,null),t(ue,Up),t(ue,Aa),t(Aa,Hp),p(e,Hn,i),p(e,Y,i),t(Y,Rp),t(Y,lt),t(lt,Gp),t(Y,Xp),t(Y,It),t(It,Fp),t(Y,Kp),p(e,Rn,i),p(e,$e,i),t($e,Vp),t($e,Da),t(Da,Jp),t($e,Yp),p(e,Gn,i),p(e,Lt,i),t(Lt,Zp),p(e,Xn,i),b(rt,e,i),p(e,Fn,i),p(e,I,i),t(I,Sa),t(Sa,Qp),t(I,eh),t(I,Ca),t(Ca,th),t(I,sh),t(I,Ba),t(Ba,ah),t(I,nh),t(I,Oa),t(Oa,oh),t(I,lh),p(e,Kn,i),p(e,L,i),t(L,rh),Vn.m(ym,L),t(L,Jn),Yn.m(wm,L),t(L,Zn),Qn.m(km,L),t(L,eo),to.m(Em,L),p(e,so,i),p(e,Ut,i),p(e,ao,i),p(e,me,i),t(me,xe),t(xe,Ma),b(it,Ma,null),t(me,ih),t(me,Na),t(Na,ph),p(e,no,i),p(e,H,i),t(H,hh),t(H,Wa),t(Wa,ch),t(H,uh),t(H,Ht),t(Ht,mh),t(H,dh),t(H,pt),t(pt,fh),t(H,gh),p(e,oo,i),p(e,R,i),t(R,bh),t(R,Rt),t(Rt,vh),t(R,yh),t(R,Ia),t(Ia,wh),t(R,kh),t(R,La),t(La,Eh),t(R,zh),p(e,lo,i),p(e,M,i),t(M,_h),t(M,Gt),t(Gt,qh),t(M,jh),t(M,Xt),t(Xt,$h),t(M,xh),t(M,Ft),t(Ft,Ph),t(M,Th),t(M,Kt),t(Kt,Ah),t(M,Dh),ro=!0},p:$m,i(e){ro||(v(Se.$$.fragment,e),v(Ce.$$.fragment,e),v(Be.$$.fragment,e),v(Oe.$$.fragment,e),v(Ne.$$.fragment,e),v(We.$$.fragment,e),v(Ie.$$.fragment,e),v(He.$$.fragment,e),v(Re.$$.fragment,e),v(Ge.$$.fragment,e),v(Xe.$$.fragment,e),v(Fe.$$.fragment,e),v(Ke.$$.fragment,e),v(Ve.$$.fragment,e),v(Ye.$$.fragment,e),v(Ze.$$.fragment,e),v(Qe.$$.fragment,e),v(et.$$.fragment,e),v(tt.$$.fragment,e),v(at.$$.fragment,e),v(ot.$$.fragment,e),v(rt.$$.fragment,e),v(it.$$.fragment,e),ro=!0)},o(e){y(Se.$$.fragment,e),y(Ce.$$.fragment,e),y(Be.$$.fragment,e),y(Oe.$$.fragment,e),y(Ne.$$.fragment,e),y(We.$$.fragment,e),y(Ie.$$.fragment,e),y(He.$$.fragment,e),y(Re.$$.fragment,e),y(Ge.$$.fragment,e),y(Xe.$$.fragment,e),y(Fe.$$.fragment,e),y(Ke.$$.fragment,e),y(Ve.$$.fragment,e),y(Ye.$$.fragment,e),y(Ze.$$.fragment,e),y(Qe.$$.fragment,e),y(et.$$.fragment,e),y(tt.$$.fragment,e),y(at.$$.fragment,e),y(ot.$$.fragment,e),y(rt.$$.fragment,e),y(it.$$.fragment,e),ro=!1},d(e){s(oe),e&&s(Ua),e&&s(le),w(Se),e&&s(Ha),w(Ce,e),e&&s(Ra),e&&s(ht),e&&s(Ga),w(Be,e),e&&s(Xa),e&&s(D),e&&s(Fa),e&&s(F),e&&s(Ka),e&&s(re),w(Oe),e&&s(Va),e&&s(Me),e&&s(Ja),w(Ne,e),e&&s(Ya),e&&s(bt),e&&s(Za),w(We,e),e&&s(Qa),e&&s(S),e&&s(en),w(Ie,e),e&&s(tn),e&&s(C),e&&s(sn),e&&s(W),e&&s(an),w(He,e),e&&s(nn),e&&s(K),e&&s(on),e&&s(yt),e&&s(ln),e&&s(wt),e&&s(rn),w(Re,e),e&&s(pn),e&&s(B),e&&s(hn),e&&s(ie),w(Ge),e&&s(cn),w(Xe,e),e&&s(un),e&&s(k),e&&s(mn),e&&s(V),e&&s(dn),w(Fe,e),e&&s(fn),e&&s(P),e&&s(gn),e&&s(we),e&&s(bn),w(Ke,e),e&&s(vn),e&&s(T),e&&s(yn),e&&s(_t),e&&s(wn),e&&s(qt),e&&s(kn),e&&s(pe),w(Ve),e&&s(En),e&&s(q),e&&s(zn),e&&s(At),e&&s(_n),e&&s(Dt),e&&s(qn),w(Ye,e),e&&s(jn),e&&s(Ee),e&&s($n),w(Ze,e),e&&s(xn),e&&s(m),e&&s(Pn),w(Qe,e),e&&s(Tn),e&&s(E),e&&s(An),e&&s(ze),e&&s(Dn),w(et,e),e&&s(Sn),e&&s(_),e&&s(Cn),e&&s(J),e&&s(Bn),e&&s(he),w(tt),e&&s(On),e&&s(U),e&&s(Mn),e&&s(Bt),e&&s(Nn),e&&s(ce),w(at),e&&s(Wn),e&&s(O),e&&s(In),e&&s(z),e&&s(Ln),e&&s(Wt),e&&s(Un),e&&s(ue),w(ot),e&&s(Hn),e&&s(Y),e&&s(Rn),e&&s($e),e&&s(Gn),e&&s(Lt),e&&s(Xn),w(rt,e),e&&s(Fn),e&&s(I),e&&s(Kn),e&&s(L),e&&s(so),e&&s(Ut),e&&s(ao),e&&s(me),w(it),e&&s(no),e&&s(H),e&&s(oo),e&&s(R),e&&s(lo),e&&s(M)}}}const Am={local:"summary-of-the-tokenizers",sections:[{local:"introduction",sections:[{local:"subword-tokenization",title:"Subword tokenization"}],title:"Introduction"},{local:"bytepair-encoding-bpe",sections:[{local:"bytelevel-bpe",sections:[{local:"wordpiece",title:"WordPiece"},{local:"unigram",title:"Unigram"},{local:"sentencepiece",title:"SentencePiece"}],title:"Byte-level BPE"}],title:"Byte-Pair Encoding (BPE)"}],title:"Summary of the tokenizers"};function Dm(Ch){return xm(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Nm extends zm{constructor(oe){super();_m(this,oe,Dm,Tm,qm,{})}}export{Nm as default,Am as metadata};
