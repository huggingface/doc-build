<meta charset="utf-8" /><meta http-equiv="content-security-policy" content=""><meta name="hf:doc:metadata" content="{&quot;local&quot;:&quot;padding-and-truncation&quot;,&quot;title&quot;:&quot;Padding and truncation&quot;}" data-svelte="svelte-1phssyn">
	<link rel="modulepreload" href="/docs/transformers/v4.21.2/en/_app/assets/pages/__layout.svelte-hf-doc-builder.css">
	<link rel="modulepreload" href="/docs/transformers/v4.21.2/en/_app/start-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/transformers/v4.21.2/en/_app/chunks/vendor-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/transformers/v4.21.2/en/_app/chunks/paths-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/transformers/v4.21.2/en/_app/pages/__layout.svelte-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/transformers/v4.21.2/en/_app/pages/pad_truncation.mdx-hf-doc-builder.js">
	<link rel="modulepreload" href="/docs/transformers/v4.21.2/en/_app/chunks/IconCopyLink-hf-doc-builder.js"> 






<h1 class="relative group"><a id="padding-and-truncation" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#padding-and-truncation"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a>
	<span>Padding and truncation
	</span></h1>

<p>Batched inputs are often different lengths, so they canâ€™t be converted to fixed-size tensors. Padding and truncation are strategies for dealing with this problem, to create rectangular tensors from batches of varying lengths. Padding adds a special <strong>padding token</strong> to ensure shorter sequences will have the same length as either the longest sequence in a batch or the maximum length accepted by the model. Truncation works in the other direction by truncating long sequences.</p>
<p>In most cases, padding your batch to the length of the longest sequence and truncating to the maximum length a model can accept works pretty well. However, the API supports more strategies if you need them. The three arguments you need to are: <code>padding</code>, <code>truncation</code> and <code>max_length</code>.</p>
<p>The <code>padding</code> argument controls padding. It can be a boolean or a string:</p>
<ul><li><code>True</code> or <code>&#39;longest&#39;</code>: pad to the longest sequence in the batch (no padding is applied if you only provide
a single sequence).</li>
<li><code>&#39;max_length&#39;</code>: pad to a length specified by the <code>max_length</code> argument or the maximum length accepted
by the model if no <code>max_length</code> is provided (<code>max_length=None</code>). Padding will still be applied if you only provide a single sequence.</li>
<li><code>False</code> or <code>&#39;do_not_pad&#39;</code>: no padding is applied. This is the default behavior.</li></ul>
<p>The <code>truncation</code> argument controls truncation. It can be a boolean or a string:</p>
<ul><li><code>True</code> or <code>&#39;longest_first&#39;</code>: truncate to a maximum length specified by the <code>max_length</code> argument or
the maximum length accepted by the model if no <code>max_length</code> is provided (<code>max_length=None</code>). This will
truncate token by token, removing a token from the longest sequence in the pair until the proper length is
reached.</li>
<li><code>&#39;only_second&#39;</code>: truncate to a maximum length specified by the <code>max_length</code> argument or the maximum
length accepted by the model if no <code>max_length</code> is provided (<code>max_length=None</code>). This will only truncate
the second sentence of a pair if a pair of sequences (or a batch of pairs of sequences) is provided.</li>
<li><code>&#39;only_first&#39;</code>: truncate to a maximum length specified by the <code>max_length</code> argument or the maximum
length accepted by the model if no <code>max_length</code> is provided (<code>max_length=None</code>). This will only truncate
the first sentence of a pair if a pair of sequences (or a batch of pairs of sequences) is provided.</li>
<li><code>False</code> or <code>&#39;do_not_truncate&#39;</code>: no truncation is applied. This is the default behavior.</li></ul>
<p>The <code>max_length</code> argument controls the length of the padding and truncation. It can be an integer or <code>None</code>, in which case it will default to the maximum length the model can accept. If the model has no specific maximum input length, truncation or padding to <code>max_length</code> is deactivated.</p>
<p>The following table summarizes the recommended way to setup padding and truncation. If you use pairs of input sequences in any of the following examples, you can replace <code>truncation=True</code> by a <code>STRATEGY</code> selected in
<code>[&#39;only_first&#39;, &#39;only_second&#39;, &#39;longest_first&#39;]</code>, i.e. <code>truncation=&#39;only_second&#39;</code> or <code>truncation=&#39;longest_first&#39;</code> to control how both sequences in the pair are truncated as detailed before.</p>
<table><thead><tr><th>Truncation</th>
<th>Padding</th>
<th>Instruction</th></tr></thead>
<tbody><tr><td>no truncation</td>
<td>no padding</td>
<td><code>tokenizer(batch_sentences)</code></td></tr>
<tr><td></td>
<td>padding to max sequence in batch</td>
<td><code>tokenizer(batch_sentences, padding=True)</code> or</td></tr>
<tr><td></td>
<td></td>
<td><code>tokenizer(batch_sentences, padding=&#39;longest&#39;)</code></td></tr>
<tr><td></td>
<td>padding to max model input length</td>
<td><code>tokenizer(batch_sentences, padding=&#39;max_length&#39;)</code></td></tr>
<tr><td></td>
<td>padding to specific length</td>
<td><code>tokenizer(batch_sentences, padding=&#39;max_length&#39;, max_length=42)</code></td></tr>
<tr><td>truncation to max model input length</td>
<td>no padding</td>
<td><code>tokenizer(batch_sentences, truncation=True)</code> or</td></tr>
<tr><td></td>
<td></td>
<td><code>tokenizer(batch_sentences, truncation=STRATEGY)</code></td></tr>
<tr><td></td>
<td>padding to max sequence in batch</td>
<td><code>tokenizer(batch_sentences, padding=True, truncation=True)</code> or</td></tr>
<tr><td></td>
<td></td>
<td><code>tokenizer(batch_sentences, padding=True, truncation=STRATEGY)</code></td></tr>
<tr><td></td>
<td>padding to max model input length</td>
<td><code>tokenizer(batch_sentences, padding=&#39;max_length&#39;, truncation=True)</code> or</td></tr>
<tr><td></td>
<td></td>
<td><code>tokenizer(batch_sentences, padding=&#39;max_length&#39;, truncation=STRATEGY)</code></td></tr>
<tr><td></td>
<td>padding to specific length</td>
<td>Not possible</td></tr>
<tr><td>truncation to specific length</td>
<td>no padding</td>
<td><code>tokenizer(batch_sentences, truncation=True, max_length=42)</code> or</td></tr>
<tr><td></td>
<td></td>
<td><code>tokenizer(batch_sentences, truncation=STRATEGY, max_length=42)</code></td></tr>
<tr><td></td>
<td>padding to max sequence in batch</td>
<td><code>tokenizer(batch_sentences, padding=True, truncation=True, max_length=42)</code> or</td></tr>
<tr><td></td>
<td></td>
<td><code>tokenizer(batch_sentences, padding=True, truncation=STRATEGY, max_length=42)</code></td></tr>
<tr><td></td>
<td>padding to max model input length</td>
<td>Not possible</td></tr>
<tr><td></td>
<td>padding to specific length</td>
<td><code>tokenizer(batch_sentences, padding=&#39;max_length&#39;, truncation=True, max_length=42)</code> or</td></tr>
<tr><td></td>
<td></td>
<td><code>tokenizer(batch_sentences, padding=&#39;max_length&#39;, truncation=STRATEGY, max_length=42)</code></td></tr></tbody></table>


		<script type="module" data-hydrate="1kt8nqr">
		import { start } from "/docs/transformers/v4.21.2/en/_app/start-hf-doc-builder.js";
		start({
			target: document.querySelector('[data-hydrate="1kt8nqr"]').parentNode,
			paths: {"base":"/docs/transformers/v4.21.2/en","assets":"/docs/transformers/v4.21.2/en"},
			session: {},
			route: false,
			spa: false,
			trailing_slash: "never",
			hydrate: {
				status: 200,
				error: null,
				nodes: [
					import("/docs/transformers/v4.21.2/en/_app/pages/__layout.svelte-hf-doc-builder.js"),
						import("/docs/transformers/v4.21.2/en/_app/pages/pad_truncation.mdx-hf-doc-builder.js")
				],
				params: {}
			}
		});
	</script>
