import{S as $p,i as Tp,s as Ep,e as r,k as h,w as f,t as n,L as zp,c as i,d as s,m,a as p,x as c,h as l,b as u,J as t,g as o,y as d,q as g,o as b,B as _}from"../../chunks/vendor-b1433968.js";import{T as Lr}from"../../chunks/Tip-c3840994.js";import{Y as vp}from"../../chunks/Youtube-17e7948a.js";import{I as os}from"../../chunks/IconCopyLink-7029626d.js";import{C as ot}from"../../chunks/CodeBlock-a320dbd7.js";import{C as T}from"../../chunks/CodeBlockFw-e3b92d56.js";import{C as Ap}from"../../chunks/ColabDropdown-727dc22f.js";import"../../chunks/CopyButton-f65cb278.js";function qp(H){let w,$;return{c(){w=r("p"),$=n(`All code examples presented in the documentation have a switch on the top left for Pytorch versus TensorFlow. If
not, the code is expected to work for both backends without any change needed.`)},l(j){w=i(j,"P",{});var y=p(w);$=l(y,`All code examples presented in the documentation have a switch on the top left for Pytorch versus TensorFlow. If
not, the code is expected to work for both backends without any change needed.`),y.forEach(s)},m(j,y){o(j,w,y),t(w,$)},d(j){j&&s(w)}}}function Fp(H){let w,$,j,y,C;return{c(){w=r("p"),$=n("All \u{1F917} Transformers models (PyTorch or TensorFlow) return the activations of the model "),j=r("em"),y=n("before"),C=n(` the final activation
function (like SoftMax) since this final activation function is often fused with the loss.`)},l(v){w=i(v,"P",{});var E=p(w);$=l(E,"All \u{1F917} Transformers models (PyTorch or TensorFlow) return the activations of the model "),j=i(E,"EM",{});var D=p(j);y=l(D,"before"),D.forEach(s),C=l(E,` the final activation
function (like SoftMax) since this final activation function is often fused with the loss.`),E.forEach(s)},m(v,E){o(v,w,E),t(w,$),t(w,j),t(j,y),t(w,C)},d(v){v&&s(w)}}}function xp(H){let w,$,j,y,C;return{c(){w=r("p"),$=n(`Pytorch model outputs are special dataclasses so that you can get autocompletion for their attributes in an IDE.
They also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which
case the attributes not set (that have `),j=r("code"),y=n("None"),C=n(" values) are ignored.")},l(v){w=i(v,"P",{});var E=p(w);$=l(E,`Pytorch model outputs are special dataclasses so that you can get autocompletion for their attributes in an IDE.
They also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which
case the attributes not set (that have `),j=i(E,"CODE",{});var D=p(j);y=l(D,"None"),D.forEach(s),C=l(E," values) are ignored."),E.forEach(s)},m(v,E){o(v,w,E),t(w,$),t(w,j),t(j,y),t(w,C)},d(v){v&&s(w)}}}function Cp(H){let w,$,j,y,C,v,E,D,In,Yt,je,Ut,rs,Nn,Ht,is,Ln,Gt,Z,Rt,G,X,rt,ye,Wn,it,On,Qt,ee,Yn,ps,Un,Hn,Kt,ke,Vt,hs,Gn,Jt,k,pt,Rn,Qn,ht,Kn,Vn,mt,Jn,Zn,ut,Xn,el,ve,sl,ft,tl,al,nl,ct,ll,ol,dt,rl,il,gt,pl,Zt,se,hl,ms,ml,ul,Xt,us,fl,ea,$e,sa,Te,ta,fs,cl,aa,Ee,na,cs,dl,la,ze,oa,Ae,gl,ds,bl,ra,gs,_l,ia,B,wl,qe,jl,yl,bs,kl,vl,pa,te,$l,Fe,Tl,El,ha,ae,zl,_s,Al,ql,ma,xe,ua,ws,Fl,fa,q,xl,js,Cl,Sl,ys,Dl,Bl,ks,Pl,Ml,vs,Il,Nl,ca,Ce,da,P,Ll,bt,Wl,Ol,_t,Yl,Ul,ga,Se,ba,M,Hl,$s,Gl,Rl,Ts,Ql,Kl,_a,Es,wa,R,ne,wt,De,Vl,jt,Jl,ja,zs,Zl,ya,Be,ka,le,Xl,yt,eo,so,va,Pe,$a,Q,oe,kt,Me,to,vt,ao,Ta,I,no,$t,lo,oo,As,ro,io,Ea,F,po,Tt,ho,mo,Et,uo,fo,zt,co,go,At,bo,_o,za,qs,wo,Aa,Ie,qa,N,jo,Fs,yo,ko,xs,vo,$o,Fa,Ne,xa,Cs,To,Ca,Le,Sa,Ss,Eo,Da,We,Ba,re,zo,Ds,Ao,qo,Pa,K,ie,qt,Oe,Fo,Ft,xo,Ma,pe,Co,xt,So,Do,Ia,Ye,Na,he,Bo,Bs,Po,Mo,La,Ue,Wa,me,Io,Ct,No,Lo,Oa,ue,Ya,Ps,Wo,Ua,He,Ha,Ms,Oo,Ga,Ge,Ra,fe,Yo,St,Uo,Ho,Qa,Re,Ka,z,Go,Qe,Ro,Qo,Ke,Ko,Vo,Is,Jo,Zo,Dt,Xo,er,Ns,sr,tr,Va,ce,Ja,Ls,ar,Za,Ve,Xa,de,nr,Bt,lr,or,en,Ws,rr,sn,Je,tn,Os,ir,an,Ze,nn,Ys,pr,ln,Xe,on,V,ge,Pt,es,hr,Mt,mr,rn,L,ur,Us,fr,cr,Hs,dr,gr,pn,x,br,Gs,_r,wr,Rs,jr,yr,Qs,kr,vr,Ks,$r,Tr,hn,ss,mn,J,be,It,ts,Er,Nt,zr,un,_e,Ar,Vs,qr,Fr,fn,W,xr,Js,Cr,Sr,Zs,Dr,Br,cn,as,dn,we,Pr,Lt,Mr,Ir,gn,ns,bn;return v=new os({}),je=new Ap({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/tensorflow/quicktour.ipynb"}]}}),Z=new Lr({props:{$$slots:{default:[qp]},$$scope:{ctx:H}}}),ye=new os({}),ke=new vp({props:{id:"tiZFewofSLM"}}),$e=new T({props:{pt:{code:"pip install torch",highlighted:"pip install torch"},tf:{code:"pip install tensorflow",highlighted:"pip install tensorflow"}}}),Te=new ot({props:{code:`from transformers import pipeline

classifier = pipeline("sentiment-analysis"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),Ee=new ot({props:{code:'classifier("We are very happy to show you the \u{1F917} Transformers library."),',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9998</span>}]`}}),ze=new ot({props:{code:`results = classifier(["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">{result[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, with score: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>`}}),xe=new ot({props:{code:'classifier = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment"),',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=<span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>)'}}),Ce=new T({props:{pt:{code:"from transformers import AutoTokenizer, AutoModelForSequenceClassification",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification'},tf:{code:"from transformers import AutoTokenizer, TFAutoModelForSequenceClassification",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification'}}}),Se=new T({props:{pt:{code:`model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)`},tf:{code:`model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
# This model only exists in PyTorch, so we use the _from_pt_ flag to import that model in TensorFlow.
model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True)
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># This model only exists in PyTorch, so we use the _from_pt_ flag to import that model in TensorFlow.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)`}}}),De=new os({}),Be=new vp({props:{id:"AhChOFRegn4"}}),Pe=new T({props:{pt:{code:`from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_name = "distilbert-base-uncased-finetuned-sst-2-english"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`},tf:{code:`from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}}),Me=new os({}),Ie=new ot({props:{code:'inputs = tokenizer("We are very happy to show you the \u{1F917} Transformers library."),',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)'}}),Ne=new ot({props:{code:"print(inputs),",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(inputs)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">2057</span>, <span class="hljs-number">2024</span>, <span class="hljs-number">2200</span>, <span class="hljs-number">3407</span>, <span class="hljs-number">2000</span>, <span class="hljs-number">2265</span>, <span class="hljs-number">2017</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">100</span>, <span class="hljs-number">19081</span>, <span class="hljs-number">3075</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Le=new T({props:{pt:{code:`pt_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`},tf:{code:`tf_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`}}}),We=new T({props:{pt:{code:`for key, value in pt_batch.items():
    print(f"{key}: {value.numpy().tolist()}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> pt_batch.items():
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{key}</span>: <span class="hljs-subst">{value.numpy().tolist()}</span>&quot;</span>)
input_ids: [[<span class="hljs-number">101</span>, <span class="hljs-number">2057</span>, <span class="hljs-number">2024</span>, <span class="hljs-number">2200</span>, <span class="hljs-number">3407</span>, <span class="hljs-number">2000</span>, <span class="hljs-number">2265</span>, <span class="hljs-number">2017</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">100</span>, <span class="hljs-number">19081</span>, <span class="hljs-number">3075</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>], [<span class="hljs-number">101</span>, <span class="hljs-number">2057</span>, <span class="hljs-number">3246</span>, <span class="hljs-number">2017</span>, <span class="hljs-number">2123</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1056</span>, <span class="hljs-number">5223</span>, <span class="hljs-number">2009</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]
attention_mask: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]`},tf:{code:`for key, value in tf_batch.items():
    print(f"{key}: {value.numpy().tolist()}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> tf_batch.items():
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">{key}</span>: <span class="hljs-subst">{value.numpy().tolist()}</span>&quot;</span>)
input_ids: [[<span class="hljs-number">101</span>, <span class="hljs-number">2057</span>, <span class="hljs-number">2024</span>, <span class="hljs-number">2200</span>, <span class="hljs-number">3407</span>, <span class="hljs-number">2000</span>, <span class="hljs-number">2265</span>, <span class="hljs-number">2017</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">100</span>, <span class="hljs-number">19081</span>, <span class="hljs-number">3075</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>], [<span class="hljs-number">101</span>, <span class="hljs-number">2057</span>, <span class="hljs-number">3246</span>, <span class="hljs-number">2017</span>, <span class="hljs-number">2123</span>, <span class="hljs-number">1005</span>, <span class="hljs-number">1056</span>, <span class="hljs-number">5223</span>, <span class="hljs-number">2009</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]
attention_mask: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]`}}}),Oe=new os({}),Ye=new T({props:{pt:{code:"pt_outputs = pt_model(**pt_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)'},tf:{code:"tf_outputs = tf_model(tf_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)'}}}),Ue=new T({props:{pt:{code:"print(pt_outputs)",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_outputs)
SequenceClassifierOutput(loss=<span class="hljs-literal">None</span>, logits=tensor([[-<span class="hljs-number">4.0833</span>,  <span class="hljs-number">4.3364</span>],
        [ <span class="hljs-number">0.0818</span>, -<span class="hljs-number">0.0418</span>]], grad_fn=&lt;AddmmBackward&gt;), hidden_states=<span class="hljs-literal">None</span>, attentions=<span class="hljs-literal">None</span>)`},tf:{code:"print(tf_outputs)",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tf_outputs)
TFSequenceClassifierOutput(loss=<span class="hljs-literal">None</span>, logits=&lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32, numpy=
array([[-<span class="hljs-number">4.0833</span> ,  <span class="hljs-number">4.3364</span>  ],
       [ <span class="hljs-number">0.0818</span>, -<span class="hljs-number">0.0418</span>]], dtype=float32)&gt;, hidden_states=<span class="hljs-literal">None</span>, attentions=<span class="hljs-literal">None</span>)`}}}),ue=new Lr({props:{$$slots:{default:[Fp]},$$scope:{ctx:H}}}),He=new T({props:{pt:{code:`from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)`},tf:{code:`import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)`}}}),Ge=new T({props:{pt:{code:"print(pt_predictions)",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">2.2043e-04</span>, <span class="hljs-number">9.9978e-01</span>],
        [<span class="hljs-number">5.3086e-01</span>, <span class="hljs-number">4.6914e-01</span>]], grad_fn=&lt;SoftmaxBackward&gt;)`},tf:{code:"print(tf_predictions)",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tf_predictions)
tf.Tensor(
[[<span class="hljs-number">2.2043e-04</span> <span class="hljs-number">9.9978e-01</span>]
 [<span class="hljs-number">5.3086e-01</span> <span class="hljs-number">4.6914e-01</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32)`}}}),Re=new T({props:{pt:{code:`import torch

pt_outputs = pt_model(**pt_batch, labels=torch.tensor([1, 0]))
print(pt_outputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch, labels=torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_outputs)
SequenceClassifierOutput(loss=tensor(<span class="hljs-number">0.3167</span>, grad_fn=&lt;NllLossBackward&gt;), logits=tensor([[-<span class="hljs-number">4.0833</span>,  <span class="hljs-number">4.3364</span>],
        [ <span class="hljs-number">0.0818</span>, -<span class="hljs-number">0.0418</span>]], grad_fn=&lt;AddmmBackward&gt;), hidden_states=<span class="hljs-literal">None</span>, attentions=<span class="hljs-literal">None</span>)`},tf:{code:`import tensorflow as tf

tf_outputs = tf_model(tf_batch, labels=tf.constant([1, 0]))
print(tf_outputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch, labels=tf.constant([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tf_outputs)
TFSequenceClassifierOutput(loss=&lt;tf.Tensor: shape=(<span class="hljs-number">2</span>,), dtype=float32, numpy=array([<span class="hljs-number">2.2051e-04</span>, <span class="hljs-number">6.3326e-01</span>], dtype=float32)&gt;, logits=&lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32, numpy=
array([[-<span class="hljs-number">4.0833</span> ,  <span class="hljs-number">4.3364</span>  ],
       [ <span class="hljs-number">0.0818</span>, -<span class="hljs-number">0.0418</span>]], dtype=float32)&gt;, hidden_states=<span class="hljs-literal">None</span>, attentions=<span class="hljs-literal">None</span>)`}}}),ce=new Lr({props:{$$slots:{default:[xp]},$$scope:{ctx:H}}}),Ve=new T({props:{pt:{code:`pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)`},tf:{code:`tf_save_directory = "./tf_save_pretrained"
tokenizer.save_pretrained(tf_save_directory)
tf_model.save_pretrained(tf_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`}}}),Je=new T({props:{pt:{code:"pip install torch",highlighted:"pip install torch"},tf:{code:"pip install tensorflow",highlighted:"pip install tensorflow"}}}),Ze=new T({props:{pt:{code:`from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
pt_model = AutoModel.from_pretrained(tf_save_directory, from_tf=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModel.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`},tf:{code:`from transformers import TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
tf_model = TFAutoModel.from_pretrained(pt_save_directory, from_pt=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModel.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`}}}),Xe=new T({props:{pt:{code:`pt_outputs = pt_model(**pt_batch, output_hidden_states=True, output_attentions=True)
all_hidden_states = pt_outputs.hidden_states
all_attentions = pt_outputs.attentions`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch, output_hidden_states=<span class="hljs-literal">True</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>all_hidden_states = pt_outputs.hidden_states
<span class="hljs-meta">&gt;&gt;&gt; </span>all_attentions = pt_outputs.attentions`},tf:{code:`tf_outputs = tf_model(tf_batch, output_hidden_states=True, output_attentions=True)
all_hidden_states = tf_outputs.hidden_states
all_attentions = tf_outputs.attentions`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch, output_hidden_states=<span class="hljs-literal">True</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>all_hidden_states = tf_outputs.hidden_states
<span class="hljs-meta">&gt;&gt;&gt; </span>all_attentions = tf_outputs.attentions`}}}),es=new os({}),ss=new T({props:{pt:{code:`from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

model_name = "distilbert-base-uncased-finetuned-sst-2-english"
model = DistilBertForSequenceClassification.from_pretrained(model_name)
tokenizer = DistilBertTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(model_name)`},tf:{code:`from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification

model_name = "distilbert-base-uncased-finetuned-sst-2-english"
model = TFDistilBertForSequenceClassification.from_pretrained(model_name)
tokenizer = DistilBertTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(model_name)`}}}),ts=new os({}),as=new T({props:{pt:{code:`from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification

config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4 * 512)
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>config = DistilBertConfig(n_heads=<span class="hljs-number">8</span>, dim=<span class="hljs-number">512</span>, hidden_dim=<span class="hljs-number">4</span> * <span class="hljs-number">512</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification(config)`},tf:{code:`from transformers import DistilBertConfig, DistilBertTokenizer, TFDistilBertForSequenceClassification

config = DistilBertConfig(n_heads=8, dim=512, hidden_dim=4 * 512)
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForSequenceClassification(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertConfig, DistilBertTokenizer, TFDistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>config = DistilBertConfig(n_heads=<span class="hljs-number">8</span>, dim=<span class="hljs-number">512</span>, hidden_dim=<span class="hljs-number">4</span> * <span class="hljs-number">512</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForSequenceClassification(config)`}}}),ns=new T({props:{pt:{code:`from transformers import DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification

model_name = "distilbert-base-uncased"
model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10)
tokenizer = DistilBertTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertConfig, DistilBertTokenizer, DistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=<span class="hljs-number">10</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(model_name)`},tf:{code:`from transformers import DistilBertConfig, DistilBertTokenizer, TFDistilBertForSequenceClassification

model_name = "distilbert-base-uncased"
model = TFDistilBertForSequenceClassification.from_pretrained(model_name, num_labels=10)
tokenizer = DistilBertTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertConfig, DistilBertTokenizer, TFDistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForSequenceClassification.from_pretrained(model_name, num_labels=<span class="hljs-number">10</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(model_name)`}}}),{c(){w=r("meta"),$=h(),j=r("h1"),y=r("a"),C=r("span"),f(v.$$.fragment),E=h(),D=r("span"),In=n("Quick tour"),Yt=h(),f(je.$$.fragment),Ut=h(),rs=r("p"),Nn=n(`Let\u2019s have a quick look at the \u{1F917} Transformers library features. The library downloads pretrained models for Natural
Language Understanding (NLU) tasks, such as analyzing the sentiment of a text, and Natural Language Generation (NLG),
such as completing a prompt with new text or translating in another language.`),Ht=h(),is=r("p"),Ln=n(`First we will see how to easily leverage the pipeline API to quickly use those pretrained models at inference. Then, we
will dig a little bit more and see how the library gives you access to those models and helps you preprocess your data.`),Gt=h(),f(Z.$$.fragment),Rt=h(),G=r("h2"),X=r("a"),rt=r("span"),f(ye.$$.fragment),Wn=h(),it=r("span"),On=n("Getting started on a task with a pipeline"),Qt=h(),ee=r("p"),Yn=n("The easiest way to use a pretrained model on a given task is to use "),ps=r("a"),Un=n("pipeline()"),Hn=n("."),Kt=h(),f(ke.$$.fragment),Vt=h(),hs=r("p"),Gn=n("\u{1F917} Transformers provides the following tasks out of the box:"),Jt=h(),k=r("ul"),pt=r("li"),Rn=n("Sentiment analysis: is a text positive or negative?"),Qn=h(),ht=r("li"),Kn=n("Text generation (in English): provide a prompt and the model will generate what follows."),Vn=h(),mt=r("li"),Jn=n("Name entity recognition (NER): in an input sentence, label each word with the entity it represents (person, place, etc.)"),Zn=h(),ut=r("li"),Xn=n("Question answering: provide the model with some context and a question, extract the answer from the context."),el=h(),ve=r("li"),sl=n("Filling masked text: given a text with masked words (e.g., replaced by "),ft=r("code"),tl=n("[MASK]"),al=n("), fill the blanks."),nl=h(),ct=r("li"),ll=n("Summarization: generate a summary of a long text."),ol=h(),dt=r("li"),rl=n("Translation: translate a text in another language."),il=h(),gt=r("li"),pl=n("Feature extraction: return a tensor representation of the text."),Zt=h(),se=r("p"),hl=n("Let\u2019s see how this work for sentiment analysis (the other tasks are all covered in the "),ms=r("a"),ml=n("task summary"),ul=n("):"),Xt=h(),us=r("p"),fl=n("Install the following dependencies (if not already installed):"),ea=h(),f($e.$$.fragment),sa=h(),f(Te.$$.fragment),ta=h(),fs=r("p"),cl=n(`When typing this command for the first time, a pretrained model and its tokenizer are downloaded and cached. We will
look at both later on, but as an introduction the tokenizer\u2019s job is to preprocess the text for the model, which is
then responsible for making predictions. The pipeline groups all of that together, and post-process the predictions to
make them readable. For instance:`),aa=h(),f(Ee.$$.fragment),na=h(),cs=r("p"),dl=n(`That\u2019s encouraging! You can use it on a list of sentences, which will be preprocessed then fed to the model, returning
a list of dictionaries like this one:`),la=h(),f(ze.$$.fragment),oa=h(),Ae=r("p"),gl=n("To use with a large dataset, look at "),ds=r("a"),bl=n("iterating over a pipeline"),ra=h(),gs=r("p"),_l=n(`You can see the second sentence has been classified as negative (it needs to be positive or negative) but its score is
fairly neutral.`),ia=h(),B=r("p"),wl=n(`By default, the model downloaded for this pipeline is called \u201Cdistilbert-base-uncased-finetuned-sst-2-english\u201D. We can
look at its `),qe=r("a"),jl=n("model page"),yl=n(` to get more
information about it. It uses the `),bs=r("a"),kl=n("DistilBERT architecture"),vl=n(` and has been fine-tuned on a
dataset called SST-2 for the sentiment analysis task.`),pa=h(),te=r("p"),$l=n(`Let\u2019s say we want to use another model; for instance, one that has been trained on French data. We can search through
the `),Fe=r("a"),Tl=n("model hub"),El=n(` that gathers models pretrained on a lot of data by research labs, but
also community models (usually fine-tuned versions of those big models on a specific dataset). Applying the tags
\u201CFrench\u201D and \u201Ctext-classification\u201D gives back a suggestion \u201Cnlptown/bert-base-multilingual-uncased-sentiment\u201D. Let\u2019s
see how we can use it.`),ha=h(),ae=r("p"),zl=n("You can directly pass the name of the model to use to "),_s=r("a"),Al=n("pipeline()"),ql=n(":"),ma=h(),f(xe.$$.fragment),ua=h(),ws=r("p"),Fl=n(`This classifier can now deal with texts in English, French, but also Dutch, German, Italian and Spanish! You can also
replace that name by a local folder where you have saved a pretrained model (see below). You can also pass a model
object and its associated tokenizer.`),fa=h(),q=r("p"),xl=n("We will need two classes for this. The first is "),js=r("a"),Cl=n("AutoTokenizer"),Sl=n(", which we will use to download the tokenizer associated to the model we picked and instantiate it. The second is "),ys=r("a"),Dl=n("AutoModelForSequenceClassification"),Bl=n(" (or "),ks=r("a"),Pl=n("TFAutoModelForSequenceClassification"),Ml=n(" if you are using TensorFlow), which we will use to download the model itself. Note that if we were using the library on an other task, the class of the model would change. The "),vs=r("a"),Il=n("task summary"),Nl=n(" tutorial summarizes which class is used for which task."),ca=h(),f(Ce.$$.fragment),da=h(),P=r("p"),Ll=n(`Now, to download the models and tokenizer we found previously, we just have to use the
`),bt=r("code"),Wl=n("from_pretrained()"),Ol=n("method (feel free to replace "),_t=r("code"),Yl=n("model_name"),Ul=n(` by
any other model from the model hub):`),ga=h(),f(Se.$$.fragment),ba=h(),M=r("p"),Hl=n(`If you don\u2019t find a model that has been pretrained on some data similar to yours, you will need to fine-tune a
pretrained model on your data. We provide `),$s=r("a"),Gl=n("example scripts"),Rl=n(` to do so. Once you\u2019re done, don\u2019t forget
to share your fine-tuned model on the hub with the community, using `),Ts=r("a"),Ql=n("this tutorial"),Kl=n("."),_a=h(),Es=r("a"),wa=h(),R=r("h2"),ne=r("a"),wt=r("span"),f(De.$$.fragment),Vl=h(),jt=r("span"),Jl=n("Under the hood: pretrained models"),ja=h(),zs=r("p"),Zl=n("Let\u2019s now see what happens beneath the hood when using those pipelines."),ya=h(),f(Be.$$.fragment),ka=h(),le=r("p"),Xl=n("As we saw, the model and tokenizer are created using the "),yt=r("code"),eo=n("from_pretrained"),so=n(" method:"),va=h(),f(Pe.$$.fragment),$a=h(),Q=r("h3"),oe=r("a"),kt=r("span"),f(Me.$$.fragment),to=h(),vt=r("span"),ao=n("Using the tokenizer"),Ta=h(),I=r("p"),no=n(`We mentioned the tokenizer is responsible for the preprocessing of your texts. First, it will split a given text in
words (or part of words, punctuation symbols, etc.) usually called `),$t=r("em"),lo=n("tokens"),oo=n(`. There are multiple rules that can govern
that process (you can learn more about them in the `),As=r("a"),ro=n("tokenizer summary"),io=n(`), which is why we need
to instantiate the tokenizer using the name of the model, to make sure we use the same rules as when the model was
pretrained.`),Ea=h(),F=r("p"),po=n("The second step is to convert those "),Tt=r("em"),ho=n("tokens"),mo=n(` into numbers, to be able to build a tensor out of them and feed them to
the model. To do this, the tokenizer has a `),Et=r("em"),uo=n("vocab"),fo=n(`, which is the part we download when we instantiate it with the
`),zt=r("code"),co=n("from_pretrained"),go=n(" method, since we need to use the same "),At=r("em"),bo=n("vocab"),_o=n(" as when the model was pretrained."),za=h(),qs=r("p"),wo=n("To apply these steps on a given text, we can just feed it to our tokenizer:"),Aa=h(),f(Ie.$$.fragment),qa=h(),N=r("p"),jo=n("This returns a dictionary string to list of ints. It contains the "),Fs=r("a"),yo=n("ids of the tokens"),ko=n(`, as
mentioned before, but also additional arguments that will be useful to the model. Here for instance, we also have an
`),xs=r("a"),vo=n("attention mask"),$o=n(" that the model will use to have a better understanding of the sequence:"),Fa=h(),f(Ne.$$.fragment),xa=h(),Cs=r("p"),To=n(`You can pass a list of sentences directly to your tokenizer. If your goal is to send them through your model as a
batch, you probably want to pad them all to the same length, truncate them to the maximum length the model can accept
and get tensors back. You can specify all of that to the tokenizer:`),Ca=h(),f(Le.$$.fragment),Sa=h(),Ss=r("p"),Eo=n(`The padding is automatically applied on the side expected by the model (in this case, on the right), with the padding
token the model was pretrained with. The attention mask is also adapted to take the padding into account:`),Da=h(),f(We.$$.fragment),Ba=h(),re=r("p"),zo=n("You can learn more about tokenizers "),Ds=r("a"),Ao=n("here"),qo=n("."),Pa=h(),K=r("h3"),ie=r("a"),qt=r("span"),f(Oe.$$.fragment),Fo=h(),Ft=r("span"),xo=n("Using the model"),Ma=h(),pe=r("p"),Co=n(`Once your input has been preprocessed by the tokenizer, you can send it directly to the model. As we mentioned, it will
contain all the relevant information the model needs. If you\u2019re using a TensorFlow model, you can pass the dictionary
keys directly to tensors, for a PyTorch model, you need to unpack the dictionary by adding `),xt=r("code"),So=n("**"),Do=n("."),Ia=h(),f(Ye.$$.fragment),Na=h(),he=r("p"),Bo=n(`In \u{1F917} Transformers, all outputs are objects that contain the model\u2019s final activations along with other metadata. These
objects are described in greater detail `),Bs=r("a"),Po=n("here"),Mo=n(". For now, let\u2019s inspect the output ourselves:"),La=h(),f(Ue.$$.fragment),Wa=h(),me=r("p"),Io=n("Notice how the output object has a "),Ct=r("code"),No=n("logits"),Lo=n(" attribute. You can use this to access the model\u2019s final activations."),Oa=h(),f(ue.$$.fragment),Ya=h(),Ps=r("p"),Wo=n("Let\u2019s apply the SoftMax activation to get predictions."),Ua=h(),f(He.$$.fragment),Ha=h(),Ms=r("p"),Oo=n("We can see we get the numbers from before:"),Ga=h(),f(Ge.$$.fragment),Ra=h(),fe=r("p"),Yo=n("If you provide the model with labels in addition to inputs, the model output object will also contain a "),St=r("code"),Uo=n("loss"),Ho=n(`
attribute:`),Qa=h(),f(Re.$$.fragment),Ka=h(),z=r("p"),Go=n("Models are standard "),Qe=r("a"),Ro=n("torch.nn.Module"),Qo=n(" or "),Ke=r("a"),Ko=n("tf.keras.Model"),Vo=n(" so you can use them in your usual training loop. \u{1F917} Transformers also provides a "),Is=r("a"),Jo=n("Trainer"),Zo=n(" class to help with your training in PyTorch (taking care of things such as distributed training, mixed precision, etc.) whereas you can leverage the "),Dt=r("code"),Xo=n("fit()"),er=n(" method in Keras. See the "),Ns=r("a"),sr=n("training tutorial"),tr=n(" for more details."),Va=h(),f(ce.$$.fragment),Ja=h(),Ls=r("p"),ar=n("Once your model is fine-tuned, you can save it with its tokenizer in the following way:"),Za=h(),f(Ve.$$.fragment),Xa=h(),de=r("p"),nr=n("You can then load this model back using the "),Bt=r("code"),lr=n("AutoModel.from_pretrained()"),or=n(`method by passing the
directory name instead of the model name. One cool feature of \u{1F917} Transformers is that you can easily switch between
PyTorch and TensorFlow: any model saved as before can be loaded back either in PyTorch or TensorFlow.`),en=h(),Ws=r("p"),rr=n("If you would like to load your saved model in the other framework, first make sure it is installed:"),sn=h(),f(Je.$$.fragment),tn=h(),Os=r("p"),ir=n("Then, use the corresponding Auto class to load it like this:"),an=h(),f(Ze.$$.fragment),nn=h(),Ys=r("p"),pr=n("Lastly, you can also ask the model to return all hidden states and all attention weights if you need them:"),ln=h(),f(Xe.$$.fragment),on=h(),V=r("h3"),ge=r("a"),Pt=r("span"),f(es.$$.fragment),hr=h(),Mt=r("span"),mr=n("Accessing the code"),rn=h(),L=r("p"),ur=n("The "),Us=r("a"),fr=n("AutoModel"),cr=n(" and "),Hs=r("a"),dr=n("AutoTokenizer"),gr=n(` classes are just shortcuts that will automatically work with any
pretrained model. Behind the scenes, the library has one model class per combination of architecture plus class, so the
code is easy to access and tweak if you need to.`),pn=h(),x=r("p"),br=n(`In our previous example, the model was called \u201Cdistilbert-base-uncased-finetuned-sst-2-english\u201D, which means it\u2019s using
the `),Gs=r("a"),_r=n("DistilBERT"),wr=n(" architecture. As "),Rs=r("a"),jr=n("AutoModelForSequenceClassification"),yr=n(" (or "),Qs=r("a"),kr=n("TFAutoModelForSequenceClassification"),vr=n(" if you are using TensorFlow) was used, the model automatically created is then a "),Ks=r("a"),$r=n("DistilBertForSequenceClassification"),Tr=n(`. You can look at its documentation for all details relevant to that specific model, or browse the source code. This is how you would
directly instantiate model and tokenizer without the auto magic:`),hn=h(),f(ss.$$.fragment),mn=h(),J=r("h3"),be=r("a"),It=r("span"),f(ts.$$.fragment),Er=h(),Nt=r("span"),zr=n("Customizing the model"),un=h(),_e=r("p"),Ar=n(`If you want to change how the model itself is built, you can define a custom configuration class. Each architecture
comes with its own relevant configuration. For example, `),Vs=r("a"),qr=n("DistilBertConfig"),Fr=n(` allows you to specify
parameters such as the hidden dimension, dropout rate, etc for DistilBERT. If you do core modifications, like changing
the hidden size, you won\u2019t be able to use a pretrained model anymore and will need to train from scratch. You would
then instantiate the model directly from this configuration.`),fn=h(),W=r("p"),xr=n(`Below, we load a predefined vocabulary for a tokenizer with the
`),Js=r("a"),Cr=n("from_pretrained()"),Sr=n(` method. However, unlike the tokenizer, we wish to initialize
the model from scratch. Therefore, we instantiate the model from a configuration instead of using the
`),Zs=r("a"),Dr=n("DistilBertForSequenceClassification.from_pretrained()"),Br=n(" method."),cn=h(),f(as.$$.fragment),dn=h(),we=r("p"),Pr=n(`For something that only changes the head of the model (for instance, the number of labels), you can still use a
pretrained model for the body. For instance, let\u2019s define a classifier for 10 different labels using a pretrained body.
Instead of creating a new configuration with all the default values just to change the number of labels, we can instead
pass any argument a configuration would take to the `),Lt=r("code"),Mr=n("from_pretrained"),Ir=n(` method and it will update the default
configuration appropriately:`),gn=h(),f(ns.$$.fragment),this.h()},l(e){const a=zp('[data-svelte="svelte-1phssyn"]',document.head);w=i(a,"META",{name:!0,content:!0}),a.forEach(s),$=m(e),j=i(e,"H1",{class:!0});var ls=p(j);y=i(ls,"A",{id:!0,class:!0,href:!0});var Wt=p(y);C=i(Wt,"SPAN",{});var Ot=p(C);c(v.$$.fragment,Ot),Ot.forEach(s),Wt.forEach(s),E=m(ls),D=i(ls,"SPAN",{});var Wr=p(D);In=l(Wr,"Quick tour"),Wr.forEach(s),ls.forEach(s),Yt=m(e),c(je.$$.fragment,e),Ut=m(e),rs=i(e,"P",{});var Or=p(rs);Nn=l(Or,`Let\u2019s have a quick look at the \u{1F917} Transformers library features. The library downloads pretrained models for Natural
Language Understanding (NLU) tasks, such as analyzing the sentiment of a text, and Natural Language Generation (NLG),
such as completing a prompt with new text or translating in another language.`),Or.forEach(s),Ht=m(e),is=i(e,"P",{});var Yr=p(is);Ln=l(Yr,`First we will see how to easily leverage the pipeline API to quickly use those pretrained models at inference. Then, we
will dig a little bit more and see how the library gives you access to those models and helps you preprocess your data.`),Yr.forEach(s),Gt=m(e),c(Z.$$.fragment,e),Rt=m(e),G=i(e,"H2",{class:!0});var _n=p(G);X=i(_n,"A",{id:!0,class:!0,href:!0});var Ur=p(X);rt=i(Ur,"SPAN",{});var Hr=p(rt);c(ye.$$.fragment,Hr),Hr.forEach(s),Ur.forEach(s),Wn=m(_n),it=i(_n,"SPAN",{});var Gr=p(it);On=l(Gr,"Getting started on a task with a pipeline"),Gr.forEach(s),_n.forEach(s),Qt=m(e),ee=i(e,"P",{});var wn=p(ee);Yn=l(wn,"The easiest way to use a pretrained model on a given task is to use "),ps=i(wn,"A",{href:!0});var Rr=p(ps);Un=l(Rr,"pipeline()"),Rr.forEach(s),Hn=l(wn,"."),wn.forEach(s),Kt=m(e),c(ke.$$.fragment,e),Vt=m(e),hs=i(e,"P",{});var Qr=p(hs);Gn=l(Qr,"\u{1F917} Transformers provides the following tasks out of the box:"),Qr.forEach(s),Jt=m(e),k=i(e,"UL",{});var A=p(k);pt=i(A,"LI",{});var Kr=p(pt);Rn=l(Kr,"Sentiment analysis: is a text positive or negative?"),Kr.forEach(s),Qn=m(A),ht=i(A,"LI",{});var Vr=p(ht);Kn=l(Vr,"Text generation (in English): provide a prompt and the model will generate what follows."),Vr.forEach(s),Vn=m(A),mt=i(A,"LI",{});var Jr=p(mt);Jn=l(Jr,"Name entity recognition (NER): in an input sentence, label each word with the entity it represents (person, place, etc.)"),Jr.forEach(s),Zn=m(A),ut=i(A,"LI",{});var Zr=p(ut);Xn=l(Zr,"Question answering: provide the model with some context and a question, extract the answer from the context."),Zr.forEach(s),el=m(A),ve=i(A,"LI",{});var jn=p(ve);sl=l(jn,"Filling masked text: given a text with masked words (e.g., replaced by "),ft=i(jn,"CODE",{});var Xr=p(ft);tl=l(Xr,"[MASK]"),Xr.forEach(s),al=l(jn,"), fill the blanks."),jn.forEach(s),nl=m(A),ct=i(A,"LI",{});var ei=p(ct);ll=l(ei,"Summarization: generate a summary of a long text."),ei.forEach(s),ol=m(A),dt=i(A,"LI",{});var si=p(dt);rl=l(si,"Translation: translate a text in another language."),si.forEach(s),il=m(A),gt=i(A,"LI",{});var ti=p(gt);pl=l(ti,"Feature extraction: return a tensor representation of the text."),ti.forEach(s),A.forEach(s),Zt=m(e),se=i(e,"P",{});var yn=p(se);hl=l(yn,"Let\u2019s see how this work for sentiment analysis (the other tasks are all covered in the "),ms=i(yn,"A",{href:!0});var ai=p(ms);ml=l(ai,"task summary"),ai.forEach(s),ul=l(yn,"):"),yn.forEach(s),Xt=m(e),us=i(e,"P",{});var ni=p(us);fl=l(ni,"Install the following dependencies (if not already installed):"),ni.forEach(s),ea=m(e),c($e.$$.fragment,e),sa=m(e),c(Te.$$.fragment,e),ta=m(e),fs=i(e,"P",{});var li=p(fs);cl=l(li,`When typing this command for the first time, a pretrained model and its tokenizer are downloaded and cached. We will
look at both later on, but as an introduction the tokenizer\u2019s job is to preprocess the text for the model, which is
then responsible for making predictions. The pipeline groups all of that together, and post-process the predictions to
make them readable. For instance:`),li.forEach(s),aa=m(e),c(Ee.$$.fragment,e),na=m(e),cs=i(e,"P",{});var oi=p(cs);dl=l(oi,`That\u2019s encouraging! You can use it on a list of sentences, which will be preprocessed then fed to the model, returning
a list of dictionaries like this one:`),oi.forEach(s),la=m(e),c(ze.$$.fragment,e),oa=m(e),Ae=i(e,"P",{});var Nr=p(Ae);gl=l(Nr,"To use with a large dataset, look at "),ds=i(Nr,"A",{href:!0});var ri=p(ds);bl=l(ri,"iterating over a pipeline"),ri.forEach(s),Nr.forEach(s),ra=m(e),gs=i(e,"P",{});var ii=p(gs);_l=l(ii,`You can see the second sentence has been classified as negative (it needs to be positive or negative) but its score is
fairly neutral.`),ii.forEach(s),ia=m(e),B=i(e,"P",{});var Xs=p(B);wl=l(Xs,`By default, the model downloaded for this pipeline is called \u201Cdistilbert-base-uncased-finetuned-sst-2-english\u201D. We can
look at its `),qe=i(Xs,"A",{href:!0,rel:!0});var pi=p(qe);jl=l(pi,"model page"),pi.forEach(s),yl=l(Xs,` to get more
information about it. It uses the `),bs=i(Xs,"A",{href:!0});var hi=p(bs);kl=l(hi,"DistilBERT architecture"),hi.forEach(s),vl=l(Xs,` and has been fine-tuned on a
dataset called SST-2 for the sentiment analysis task.`),Xs.forEach(s),pa=m(e),te=i(e,"P",{});var kn=p(te);$l=l(kn,`Let\u2019s say we want to use another model; for instance, one that has been trained on French data. We can search through
the `),Fe=i(kn,"A",{href:!0,rel:!0});var mi=p(Fe);Tl=l(mi,"model hub"),mi.forEach(s),El=l(kn,` that gathers models pretrained on a lot of data by research labs, but
also community models (usually fine-tuned versions of those big models on a specific dataset). Applying the tags
\u201CFrench\u201D and \u201Ctext-classification\u201D gives back a suggestion \u201Cnlptown/bert-base-multilingual-uncased-sentiment\u201D. Let\u2019s
see how we can use it.`),kn.forEach(s),ha=m(e),ae=i(e,"P",{});var vn=p(ae);zl=l(vn,"You can directly pass the name of the model to use to "),_s=i(vn,"A",{href:!0});var ui=p(_s);Al=l(ui,"pipeline()"),ui.forEach(s),ql=l(vn,":"),vn.forEach(s),ma=m(e),c(xe.$$.fragment,e),ua=m(e),ws=i(e,"P",{});var fi=p(ws);Fl=l(fi,`This classifier can now deal with texts in English, French, but also Dutch, German, Italian and Spanish! You can also
replace that name by a local folder where you have saved a pretrained model (see below). You can also pass a model
object and its associated tokenizer.`),fi.forEach(s),fa=m(e),q=i(e,"P",{});var O=p(q);xl=l(O,"We will need two classes for this. The first is "),js=i(O,"A",{href:!0});var ci=p(js);Cl=l(ci,"AutoTokenizer"),ci.forEach(s),Sl=l(O,", which we will use to download the tokenizer associated to the model we picked and instantiate it. The second is "),ys=i(O,"A",{href:!0});var di=p(ys);Dl=l(di,"AutoModelForSequenceClassification"),di.forEach(s),Bl=l(O," (or "),ks=i(O,"A",{href:!0});var gi=p(ks);Pl=l(gi,"TFAutoModelForSequenceClassification"),gi.forEach(s),Ml=l(O," if you are using TensorFlow), which we will use to download the model itself. Note that if we were using the library on an other task, the class of the model would change. The "),vs=i(O,"A",{href:!0});var bi=p(vs);Il=l(bi,"task summary"),bi.forEach(s),Nl=l(O," tutorial summarizes which class is used for which task."),O.forEach(s),ca=m(e),c(Ce.$$.fragment,e),da=m(e),P=i(e,"P",{});var et=p(P);Ll=l(et,`Now, to download the models and tokenizer we found previously, we just have to use the
`),bt=i(et,"CODE",{});var _i=p(bt);Wl=l(_i,"from_pretrained()"),_i.forEach(s),Ol=l(et,"method (feel free to replace "),_t=i(et,"CODE",{});var wi=p(_t);Yl=l(wi,"model_name"),wi.forEach(s),Ul=l(et,` by
any other model from the model hub):`),et.forEach(s),ga=m(e),c(Se.$$.fragment,e),ba=m(e),M=i(e,"P",{});var st=p(M);Hl=l(st,`If you don\u2019t find a model that has been pretrained on some data similar to yours, you will need to fine-tune a
pretrained model on your data. We provide `),$s=i(st,"A",{href:!0});var ji=p($s);Gl=l(ji,"example scripts"),ji.forEach(s),Rl=l(st,` to do so. Once you\u2019re done, don\u2019t forget
to share your fine-tuned model on the hub with the community, using `),Ts=i(st,"A",{href:!0});var yi=p(Ts);Ql=l(yi,"this tutorial"),yi.forEach(s),Kl=l(st,"."),st.forEach(s),_a=m(e),Es=i(e,"A",{id:!0}),p(Es).forEach(s),wa=m(e),R=i(e,"H2",{class:!0});var $n=p(R);ne=i($n,"A",{id:!0,class:!0,href:!0});var ki=p(ne);wt=i(ki,"SPAN",{});var vi=p(wt);c(De.$$.fragment,vi),vi.forEach(s),ki.forEach(s),Vl=m($n),jt=i($n,"SPAN",{});var $i=p(jt);Jl=l($i,"Under the hood: pretrained models"),$i.forEach(s),$n.forEach(s),ja=m(e),zs=i(e,"P",{});var Ti=p(zs);Zl=l(Ti,"Let\u2019s now see what happens beneath the hood when using those pipelines."),Ti.forEach(s),ya=m(e),c(Be.$$.fragment,e),ka=m(e),le=i(e,"P",{});var Tn=p(le);Xl=l(Tn,"As we saw, the model and tokenizer are created using the "),yt=i(Tn,"CODE",{});var Ei=p(yt);eo=l(Ei,"from_pretrained"),Ei.forEach(s),so=l(Tn," method:"),Tn.forEach(s),va=m(e),c(Pe.$$.fragment,e),$a=m(e),Q=i(e,"H3",{class:!0});var En=p(Q);oe=i(En,"A",{id:!0,class:!0,href:!0});var zi=p(oe);kt=i(zi,"SPAN",{});var Ai=p(kt);c(Me.$$.fragment,Ai),Ai.forEach(s),zi.forEach(s),to=m(En),vt=i(En,"SPAN",{});var qi=p(vt);ao=l(qi,"Using the tokenizer"),qi.forEach(s),En.forEach(s),Ta=m(e),I=i(e,"P",{});var tt=p(I);no=l(tt,`We mentioned the tokenizer is responsible for the preprocessing of your texts. First, it will split a given text in
words (or part of words, punctuation symbols, etc.) usually called `),$t=i(tt,"EM",{});var Fi=p($t);lo=l(Fi,"tokens"),Fi.forEach(s),oo=l(tt,`. There are multiple rules that can govern
that process (you can learn more about them in the `),As=i(tt,"A",{href:!0});var xi=p(As);ro=l(xi,"tokenizer summary"),xi.forEach(s),io=l(tt,`), which is why we need
to instantiate the tokenizer using the name of the model, to make sure we use the same rules as when the model was
pretrained.`),tt.forEach(s),Ea=m(e),F=i(e,"P",{});var Y=p(F);po=l(Y,"The second step is to convert those "),Tt=i(Y,"EM",{});var Ci=p(Tt);ho=l(Ci,"tokens"),Ci.forEach(s),mo=l(Y,` into numbers, to be able to build a tensor out of them and feed them to
the model. To do this, the tokenizer has a `),Et=i(Y,"EM",{});var Si=p(Et);uo=l(Si,"vocab"),Si.forEach(s),fo=l(Y,`, which is the part we download when we instantiate it with the
`),zt=i(Y,"CODE",{});var Di=p(zt);co=l(Di,"from_pretrained"),Di.forEach(s),go=l(Y," method, since we need to use the same "),At=i(Y,"EM",{});var Bi=p(At);bo=l(Bi,"vocab"),Bi.forEach(s),_o=l(Y," as when the model was pretrained."),Y.forEach(s),za=m(e),qs=i(e,"P",{});var Pi=p(qs);wo=l(Pi,"To apply these steps on a given text, we can just feed it to our tokenizer:"),Pi.forEach(s),Aa=m(e),c(Ie.$$.fragment,e),qa=m(e),N=i(e,"P",{});var at=p(N);jo=l(at,"This returns a dictionary string to list of ints. It contains the "),Fs=i(at,"A",{href:!0});var Mi=p(Fs);yo=l(Mi,"ids of the tokens"),Mi.forEach(s),ko=l(at,`, as
mentioned before, but also additional arguments that will be useful to the model. Here for instance, we also have an
`),xs=i(at,"A",{href:!0});var Ii=p(xs);vo=l(Ii,"attention mask"),Ii.forEach(s),$o=l(at," that the model will use to have a better understanding of the sequence:"),at.forEach(s),Fa=m(e),c(Ne.$$.fragment,e),xa=m(e),Cs=i(e,"P",{});var Ni=p(Cs);To=l(Ni,`You can pass a list of sentences directly to your tokenizer. If your goal is to send them through your model as a
batch, you probably want to pad them all to the same length, truncate them to the maximum length the model can accept
and get tensors back. You can specify all of that to the tokenizer:`),Ni.forEach(s),Ca=m(e),c(Le.$$.fragment,e),Sa=m(e),Ss=i(e,"P",{});var Li=p(Ss);Eo=l(Li,`The padding is automatically applied on the side expected by the model (in this case, on the right), with the padding
token the model was pretrained with. The attention mask is also adapted to take the padding into account:`),Li.forEach(s),Da=m(e),c(We.$$.fragment,e),Ba=m(e),re=i(e,"P",{});var zn=p(re);zo=l(zn,"You can learn more about tokenizers "),Ds=i(zn,"A",{href:!0});var Wi=p(Ds);Ao=l(Wi,"here"),Wi.forEach(s),qo=l(zn,"."),zn.forEach(s),Pa=m(e),K=i(e,"H3",{class:!0});var An=p(K);ie=i(An,"A",{id:!0,class:!0,href:!0});var Oi=p(ie);qt=i(Oi,"SPAN",{});var Yi=p(qt);c(Oe.$$.fragment,Yi),Yi.forEach(s),Oi.forEach(s),Fo=m(An),Ft=i(An,"SPAN",{});var Ui=p(Ft);xo=l(Ui,"Using the model"),Ui.forEach(s),An.forEach(s),Ma=m(e),pe=i(e,"P",{});var qn=p(pe);Co=l(qn,`Once your input has been preprocessed by the tokenizer, you can send it directly to the model. As we mentioned, it will
contain all the relevant information the model needs. If you\u2019re using a TensorFlow model, you can pass the dictionary
keys directly to tensors, for a PyTorch model, you need to unpack the dictionary by adding `),xt=i(qn,"CODE",{});var Hi=p(xt);So=l(Hi,"**"),Hi.forEach(s),Do=l(qn,"."),qn.forEach(s),Ia=m(e),c(Ye.$$.fragment,e),Na=m(e),he=i(e,"P",{});var Fn=p(he);Bo=l(Fn,`In \u{1F917} Transformers, all outputs are objects that contain the model\u2019s final activations along with other metadata. These
objects are described in greater detail `),Bs=i(Fn,"A",{href:!0});var Gi=p(Bs);Po=l(Gi,"here"),Gi.forEach(s),Mo=l(Fn,". For now, let\u2019s inspect the output ourselves:"),Fn.forEach(s),La=m(e),c(Ue.$$.fragment,e),Wa=m(e),me=i(e,"P",{});var xn=p(me);Io=l(xn,"Notice how the output object has a "),Ct=i(xn,"CODE",{});var Ri=p(Ct);No=l(Ri,"logits"),Ri.forEach(s),Lo=l(xn," attribute. You can use this to access the model\u2019s final activations."),xn.forEach(s),Oa=m(e),c(ue.$$.fragment,e),Ya=m(e),Ps=i(e,"P",{});var Qi=p(Ps);Wo=l(Qi,"Let\u2019s apply the SoftMax activation to get predictions."),Qi.forEach(s),Ua=m(e),c(He.$$.fragment,e),Ha=m(e),Ms=i(e,"P",{});var Ki=p(Ms);Oo=l(Ki,"We can see we get the numbers from before:"),Ki.forEach(s),Ga=m(e),c(Ge.$$.fragment,e),Ra=m(e),fe=i(e,"P",{});var Cn=p(fe);Yo=l(Cn,"If you provide the model with labels in addition to inputs, the model output object will also contain a "),St=i(Cn,"CODE",{});var Vi=p(St);Uo=l(Vi,"loss"),Vi.forEach(s),Ho=l(Cn,`
attribute:`),Cn.forEach(s),Qa=m(e),c(Re.$$.fragment,e),Ka=m(e),z=i(e,"P",{});var S=p(z);Go=l(S,"Models are standard "),Qe=i(S,"A",{href:!0,rel:!0});var Ji=p(Qe);Ro=l(Ji,"torch.nn.Module"),Ji.forEach(s),Qo=l(S," or "),Ke=i(S,"A",{href:!0,rel:!0});var Zi=p(Ke);Ko=l(Zi,"tf.keras.Model"),Zi.forEach(s),Vo=l(S," so you can use them in your usual training loop. \u{1F917} Transformers also provides a "),Is=i(S,"A",{href:!0});var Xi=p(Is);Jo=l(Xi,"Trainer"),Xi.forEach(s),Zo=l(S," class to help with your training in PyTorch (taking care of things such as distributed training, mixed precision, etc.) whereas you can leverage the "),Dt=i(S,"CODE",{});var ep=p(Dt);Xo=l(ep,"fit()"),ep.forEach(s),er=l(S," method in Keras. See the "),Ns=i(S,"A",{href:!0});var sp=p(Ns);sr=l(sp,"training tutorial"),sp.forEach(s),tr=l(S," for more details."),S.forEach(s),Va=m(e),c(ce.$$.fragment,e),Ja=m(e),Ls=i(e,"P",{});var tp=p(Ls);ar=l(tp,"Once your model is fine-tuned, you can save it with its tokenizer in the following way:"),tp.forEach(s),Za=m(e),c(Ve.$$.fragment,e),Xa=m(e),de=i(e,"P",{});var Sn=p(de);nr=l(Sn,"You can then load this model back using the "),Bt=i(Sn,"CODE",{});var ap=p(Bt);lr=l(ap,"AutoModel.from_pretrained()"),ap.forEach(s),or=l(Sn,`method by passing the
directory name instead of the model name. One cool feature of \u{1F917} Transformers is that you can easily switch between
PyTorch and TensorFlow: any model saved as before can be loaded back either in PyTorch or TensorFlow.`),Sn.forEach(s),en=m(e),Ws=i(e,"P",{});var np=p(Ws);rr=l(np,"If you would like to load your saved model in the other framework, first make sure it is installed:"),np.forEach(s),sn=m(e),c(Je.$$.fragment,e),tn=m(e),Os=i(e,"P",{});var lp=p(Os);ir=l(lp,"Then, use the corresponding Auto class to load it like this:"),lp.forEach(s),an=m(e),c(Ze.$$.fragment,e),nn=m(e),Ys=i(e,"P",{});var op=p(Ys);pr=l(op,"Lastly, you can also ask the model to return all hidden states and all attention weights if you need them:"),op.forEach(s),ln=m(e),c(Xe.$$.fragment,e),on=m(e),V=i(e,"H3",{class:!0});var Dn=p(V);ge=i(Dn,"A",{id:!0,class:!0,href:!0});var rp=p(ge);Pt=i(rp,"SPAN",{});var ip=p(Pt);c(es.$$.fragment,ip),ip.forEach(s),rp.forEach(s),hr=m(Dn),Mt=i(Dn,"SPAN",{});var pp=p(Mt);mr=l(pp,"Accessing the code"),pp.forEach(s),Dn.forEach(s),rn=m(e),L=i(e,"P",{});var nt=p(L);ur=l(nt,"The "),Us=i(nt,"A",{href:!0});var hp=p(Us);fr=l(hp,"AutoModel"),hp.forEach(s),cr=l(nt," and "),Hs=i(nt,"A",{href:!0});var mp=p(Hs);dr=l(mp,"AutoTokenizer"),mp.forEach(s),gr=l(nt,` classes are just shortcuts that will automatically work with any
pretrained model. Behind the scenes, the library has one model class per combination of architecture plus class, so the
code is easy to access and tweak if you need to.`),nt.forEach(s),pn=m(e),x=i(e,"P",{});var U=p(x);br=l(U,`In our previous example, the model was called \u201Cdistilbert-base-uncased-finetuned-sst-2-english\u201D, which means it\u2019s using
the `),Gs=i(U,"A",{href:!0});var up=p(Gs);_r=l(up,"DistilBERT"),up.forEach(s),wr=l(U," architecture. As "),Rs=i(U,"A",{href:!0});var fp=p(Rs);jr=l(fp,"AutoModelForSequenceClassification"),fp.forEach(s),yr=l(U," (or "),Qs=i(U,"A",{href:!0});var cp=p(Qs);kr=l(cp,"TFAutoModelForSequenceClassification"),cp.forEach(s),vr=l(U," if you are using TensorFlow) was used, the model automatically created is then a "),Ks=i(U,"A",{href:!0});var dp=p(Ks);$r=l(dp,"DistilBertForSequenceClassification"),dp.forEach(s),Tr=l(U,`. You can look at its documentation for all details relevant to that specific model, or browse the source code. This is how you would
directly instantiate model and tokenizer without the auto magic:`),U.forEach(s),hn=m(e),c(ss.$$.fragment,e),mn=m(e),J=i(e,"H3",{class:!0});var Bn=p(J);be=i(Bn,"A",{id:!0,class:!0,href:!0});var gp=p(be);It=i(gp,"SPAN",{});var bp=p(It);c(ts.$$.fragment,bp),bp.forEach(s),gp.forEach(s),Er=m(Bn),Nt=i(Bn,"SPAN",{});var _p=p(Nt);zr=l(_p,"Customizing the model"),_p.forEach(s),Bn.forEach(s),un=m(e),_e=i(e,"P",{});var Pn=p(_e);Ar=l(Pn,`If you want to change how the model itself is built, you can define a custom configuration class. Each architecture
comes with its own relevant configuration. For example, `),Vs=i(Pn,"A",{href:!0});var wp=p(Vs);qr=l(wp,"DistilBertConfig"),wp.forEach(s),Fr=l(Pn,` allows you to specify
parameters such as the hidden dimension, dropout rate, etc for DistilBERT. If you do core modifications, like changing
the hidden size, you won\u2019t be able to use a pretrained model anymore and will need to train from scratch. You would
then instantiate the model directly from this configuration.`),Pn.forEach(s),fn=m(e),W=i(e,"P",{});var lt=p(W);xr=l(lt,`Below, we load a predefined vocabulary for a tokenizer with the
`),Js=i(lt,"A",{href:!0});var jp=p(Js);Cr=l(jp,"from_pretrained()"),jp.forEach(s),Sr=l(lt,` method. However, unlike the tokenizer, we wish to initialize
the model from scratch. Therefore, we instantiate the model from a configuration instead of using the
`),Zs=i(lt,"A",{href:!0});var yp=p(Zs);Dr=l(yp,"DistilBertForSequenceClassification.from_pretrained()"),yp.forEach(s),Br=l(lt," method."),lt.forEach(s),cn=m(e),c(as.$$.fragment,e),dn=m(e),we=i(e,"P",{});var Mn=p(we);Pr=l(Mn,`For something that only changes the head of the model (for instance, the number of labels), you can still use a
pretrained model for the body. For instance, let\u2019s define a classifier for 10 different labels using a pretrained body.
Instead of creating a new configuration with all the default values just to change the number of labels, we can instead
pass any argument a configuration would take to the `),Lt=i(Mn,"CODE",{});var kp=p(Lt);Mr=l(kp,"from_pretrained"),kp.forEach(s),Ir=l(Mn,` method and it will update the default
configuration appropriately:`),Mn.forEach(s),gn=m(e),c(ns.$$.fragment,e),this.h()},h(){u(w,"name","hf:doc:metadata"),u(w,"content",JSON.stringify(Sp)),u(y,"id","quick-tour"),u(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(y,"href","#quick-tour"),u(j,"class","relative group"),u(X,"id","getting-started-on-a-task-with-a-pipeline"),u(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(X,"href","#getting-started-on-a-task-with-a-pipeline"),u(G,"class","relative group"),u(ps,"href","/docs/transformers/v4.16.2/en/main_classes/pipelines#transformers.pipeline"),u(ms,"href","task_summary"),u(ds,"href","main_classes/pipelines"),u(qe,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),u(qe,"rel","nofollow"),u(bs,"href","model_doc/distilbert"),u(Fe,"href","https://huggingface.co/models"),u(Fe,"rel","nofollow"),u(_s,"href","/docs/transformers/v4.16.2/en/main_classes/pipelines#transformers.pipeline"),u(js,"href","/docs/transformers/v4.16.2/en/model_doc/auto#transformers.AutoTokenizer"),u(ys,"href","/docs/transformers/v4.16.2/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),u(ks,"href","/docs/transformers/v4.16.2/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification"),u(vs,"href","task_summary"),u($s,"href","examples"),u(Ts,"href","model_sharing"),u(Es,"id","pretrained-model"),u(ne,"id","under-the-hood-pretrained-models"),u(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ne,"href","#under-the-hood-pretrained-models"),u(R,"class","relative group"),u(oe,"id","using-the-tokenizer"),u(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(oe,"href","#using-the-tokenizer"),u(Q,"class","relative group"),u(As,"href","tokenizer_summary"),u(Fs,"href","glossary#input-ids"),u(xs,"href","glossary#attention-mask"),u(Ds,"href","preprocessing"),u(ie,"id","using-the-model"),u(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ie,"href","#using-the-model"),u(K,"class","relative group"),u(Bs,"href","main_classes/output"),u(Qe,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),u(Qe,"rel","nofollow"),u(Ke,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),u(Ke,"rel","nofollow"),u(Is,"href","/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.Trainer"),u(Ns,"href","training"),u(ge,"id","accessing-the-code"),u(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ge,"href","#accessing-the-code"),u(V,"class","relative group"),u(Us,"href","/docs/transformers/v4.16.2/en/model_doc/auto#transformers.AutoModel"),u(Hs,"href","/docs/transformers/v4.16.2/en/model_doc/auto#transformers.AutoTokenizer"),u(Gs,"href","model_doc/distilbert"),u(Rs,"href","/docs/transformers/v4.16.2/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),u(Qs,"href","/docs/transformers/v4.16.2/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification"),u(Ks,"href","/docs/transformers/v4.16.2/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),u(be,"id","customizing-the-model"),u(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(be,"href","#customizing-the-model"),u(J,"class","relative group"),u(Vs,"href","/docs/transformers/v4.16.2/en/model_doc/distilbert#transformers.DistilBertConfig"),u(Js,"href","/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained"),u(Zs,"href","/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained")},m(e,a){t(document.head,w),o(e,$,a),o(e,j,a),t(j,y),t(y,C),d(v,C,null),t(j,E),t(j,D),t(D,In),o(e,Yt,a),d(je,e,a),o(e,Ut,a),o(e,rs,a),t(rs,Nn),o(e,Ht,a),o(e,is,a),t(is,Ln),o(e,Gt,a),d(Z,e,a),o(e,Rt,a),o(e,G,a),t(G,X),t(X,rt),d(ye,rt,null),t(G,Wn),t(G,it),t(it,On),o(e,Qt,a),o(e,ee,a),t(ee,Yn),t(ee,ps),t(ps,Un),t(ee,Hn),o(e,Kt,a),d(ke,e,a),o(e,Vt,a),o(e,hs,a),t(hs,Gn),o(e,Jt,a),o(e,k,a),t(k,pt),t(pt,Rn),t(k,Qn),t(k,ht),t(ht,Kn),t(k,Vn),t(k,mt),t(mt,Jn),t(k,Zn),t(k,ut),t(ut,Xn),t(k,el),t(k,ve),t(ve,sl),t(ve,ft),t(ft,tl),t(ve,al),t(k,nl),t(k,ct),t(ct,ll),t(k,ol),t(k,dt),t(dt,rl),t(k,il),t(k,gt),t(gt,pl),o(e,Zt,a),o(e,se,a),t(se,hl),t(se,ms),t(ms,ml),t(se,ul),o(e,Xt,a),o(e,us,a),t(us,fl),o(e,ea,a),d($e,e,a),o(e,sa,a),d(Te,e,a),o(e,ta,a),o(e,fs,a),t(fs,cl),o(e,aa,a),d(Ee,e,a),o(e,na,a),o(e,cs,a),t(cs,dl),o(e,la,a),d(ze,e,a),o(e,oa,a),o(e,Ae,a),t(Ae,gl),t(Ae,ds),t(ds,bl),o(e,ra,a),o(e,gs,a),t(gs,_l),o(e,ia,a),o(e,B,a),t(B,wl),t(B,qe),t(qe,jl),t(B,yl),t(B,bs),t(bs,kl),t(B,vl),o(e,pa,a),o(e,te,a),t(te,$l),t(te,Fe),t(Fe,Tl),t(te,El),o(e,ha,a),o(e,ae,a),t(ae,zl),t(ae,_s),t(_s,Al),t(ae,ql),o(e,ma,a),d(xe,e,a),o(e,ua,a),o(e,ws,a),t(ws,Fl),o(e,fa,a),o(e,q,a),t(q,xl),t(q,js),t(js,Cl),t(q,Sl),t(q,ys),t(ys,Dl),t(q,Bl),t(q,ks),t(ks,Pl),t(q,Ml),t(q,vs),t(vs,Il),t(q,Nl),o(e,ca,a),d(Ce,e,a),o(e,da,a),o(e,P,a),t(P,Ll),t(P,bt),t(bt,Wl),t(P,Ol),t(P,_t),t(_t,Yl),t(P,Ul),o(e,ga,a),d(Se,e,a),o(e,ba,a),o(e,M,a),t(M,Hl),t(M,$s),t($s,Gl),t(M,Rl),t(M,Ts),t(Ts,Ql),t(M,Kl),o(e,_a,a),o(e,Es,a),o(e,wa,a),o(e,R,a),t(R,ne),t(ne,wt),d(De,wt,null),t(R,Vl),t(R,jt),t(jt,Jl),o(e,ja,a),o(e,zs,a),t(zs,Zl),o(e,ya,a),d(Be,e,a),o(e,ka,a),o(e,le,a),t(le,Xl),t(le,yt),t(yt,eo),t(le,so),o(e,va,a),d(Pe,e,a),o(e,$a,a),o(e,Q,a),t(Q,oe),t(oe,kt),d(Me,kt,null),t(Q,to),t(Q,vt),t(vt,ao),o(e,Ta,a),o(e,I,a),t(I,no),t(I,$t),t($t,lo),t(I,oo),t(I,As),t(As,ro),t(I,io),o(e,Ea,a),o(e,F,a),t(F,po),t(F,Tt),t(Tt,ho),t(F,mo),t(F,Et),t(Et,uo),t(F,fo),t(F,zt),t(zt,co),t(F,go),t(F,At),t(At,bo),t(F,_o),o(e,za,a),o(e,qs,a),t(qs,wo),o(e,Aa,a),d(Ie,e,a),o(e,qa,a),o(e,N,a),t(N,jo),t(N,Fs),t(Fs,yo),t(N,ko),t(N,xs),t(xs,vo),t(N,$o),o(e,Fa,a),d(Ne,e,a),o(e,xa,a),o(e,Cs,a),t(Cs,To),o(e,Ca,a),d(Le,e,a),o(e,Sa,a),o(e,Ss,a),t(Ss,Eo),o(e,Da,a),d(We,e,a),o(e,Ba,a),o(e,re,a),t(re,zo),t(re,Ds),t(Ds,Ao),t(re,qo),o(e,Pa,a),o(e,K,a),t(K,ie),t(ie,qt),d(Oe,qt,null),t(K,Fo),t(K,Ft),t(Ft,xo),o(e,Ma,a),o(e,pe,a),t(pe,Co),t(pe,xt),t(xt,So),t(pe,Do),o(e,Ia,a),d(Ye,e,a),o(e,Na,a),o(e,he,a),t(he,Bo),t(he,Bs),t(Bs,Po),t(he,Mo),o(e,La,a),d(Ue,e,a),o(e,Wa,a),o(e,me,a),t(me,Io),t(me,Ct),t(Ct,No),t(me,Lo),o(e,Oa,a),d(ue,e,a),o(e,Ya,a),o(e,Ps,a),t(Ps,Wo),o(e,Ua,a),d(He,e,a),o(e,Ha,a),o(e,Ms,a),t(Ms,Oo),o(e,Ga,a),d(Ge,e,a),o(e,Ra,a),o(e,fe,a),t(fe,Yo),t(fe,St),t(St,Uo),t(fe,Ho),o(e,Qa,a),d(Re,e,a),o(e,Ka,a),o(e,z,a),t(z,Go),t(z,Qe),t(Qe,Ro),t(z,Qo),t(z,Ke),t(Ke,Ko),t(z,Vo),t(z,Is),t(Is,Jo),t(z,Zo),t(z,Dt),t(Dt,Xo),t(z,er),t(z,Ns),t(Ns,sr),t(z,tr),o(e,Va,a),d(ce,e,a),o(e,Ja,a),o(e,Ls,a),t(Ls,ar),o(e,Za,a),d(Ve,e,a),o(e,Xa,a),o(e,de,a),t(de,nr),t(de,Bt),t(Bt,lr),t(de,or),o(e,en,a),o(e,Ws,a),t(Ws,rr),o(e,sn,a),d(Je,e,a),o(e,tn,a),o(e,Os,a),t(Os,ir),o(e,an,a),d(Ze,e,a),o(e,nn,a),o(e,Ys,a),t(Ys,pr),o(e,ln,a),d(Xe,e,a),o(e,on,a),o(e,V,a),t(V,ge),t(ge,Pt),d(es,Pt,null),t(V,hr),t(V,Mt),t(Mt,mr),o(e,rn,a),o(e,L,a),t(L,ur),t(L,Us),t(Us,fr),t(L,cr),t(L,Hs),t(Hs,dr),t(L,gr),o(e,pn,a),o(e,x,a),t(x,br),t(x,Gs),t(Gs,_r),t(x,wr),t(x,Rs),t(Rs,jr),t(x,yr),t(x,Qs),t(Qs,kr),t(x,vr),t(x,Ks),t(Ks,$r),t(x,Tr),o(e,hn,a),d(ss,e,a),o(e,mn,a),o(e,J,a),t(J,be),t(be,It),d(ts,It,null),t(J,Er),t(J,Nt),t(Nt,zr),o(e,un,a),o(e,_e,a),t(_e,Ar),t(_e,Vs),t(Vs,qr),t(_e,Fr),o(e,fn,a),o(e,W,a),t(W,xr),t(W,Js),t(Js,Cr),t(W,Sr),t(W,Zs),t(Zs,Dr),t(W,Br),o(e,cn,a),d(as,e,a),o(e,dn,a),o(e,we,a),t(we,Pr),t(we,Lt),t(Lt,Mr),t(we,Ir),o(e,gn,a),d(ns,e,a),bn=!0},p(e,[a]){const ls={};a&2&&(ls.$$scope={dirty:a,ctx:e}),Z.$set(ls);const Wt={};a&2&&(Wt.$$scope={dirty:a,ctx:e}),ue.$set(Wt);const Ot={};a&2&&(Ot.$$scope={dirty:a,ctx:e}),ce.$set(Ot)},i(e){bn||(g(v.$$.fragment,e),g(je.$$.fragment,e),g(Z.$$.fragment,e),g(ye.$$.fragment,e),g(ke.$$.fragment,e),g($e.$$.fragment,e),g(Te.$$.fragment,e),g(Ee.$$.fragment,e),g(ze.$$.fragment,e),g(xe.$$.fragment,e),g(Ce.$$.fragment,e),g(Se.$$.fragment,e),g(De.$$.fragment,e),g(Be.$$.fragment,e),g(Pe.$$.fragment,e),g(Me.$$.fragment,e),g(Ie.$$.fragment,e),g(Ne.$$.fragment,e),g(Le.$$.fragment,e),g(We.$$.fragment,e),g(Oe.$$.fragment,e),g(Ye.$$.fragment,e),g(Ue.$$.fragment,e),g(ue.$$.fragment,e),g(He.$$.fragment,e),g(Ge.$$.fragment,e),g(Re.$$.fragment,e),g(ce.$$.fragment,e),g(Ve.$$.fragment,e),g(Je.$$.fragment,e),g(Ze.$$.fragment,e),g(Xe.$$.fragment,e),g(es.$$.fragment,e),g(ss.$$.fragment,e),g(ts.$$.fragment,e),g(as.$$.fragment,e),g(ns.$$.fragment,e),bn=!0)},o(e){b(v.$$.fragment,e),b(je.$$.fragment,e),b(Z.$$.fragment,e),b(ye.$$.fragment,e),b(ke.$$.fragment,e),b($e.$$.fragment,e),b(Te.$$.fragment,e),b(Ee.$$.fragment,e),b(ze.$$.fragment,e),b(xe.$$.fragment,e),b(Ce.$$.fragment,e),b(Se.$$.fragment,e),b(De.$$.fragment,e),b(Be.$$.fragment,e),b(Pe.$$.fragment,e),b(Me.$$.fragment,e),b(Ie.$$.fragment,e),b(Ne.$$.fragment,e),b(Le.$$.fragment,e),b(We.$$.fragment,e),b(Oe.$$.fragment,e),b(Ye.$$.fragment,e),b(Ue.$$.fragment,e),b(ue.$$.fragment,e),b(He.$$.fragment,e),b(Ge.$$.fragment,e),b(Re.$$.fragment,e),b(ce.$$.fragment,e),b(Ve.$$.fragment,e),b(Je.$$.fragment,e),b(Ze.$$.fragment,e),b(Xe.$$.fragment,e),b(es.$$.fragment,e),b(ss.$$.fragment,e),b(ts.$$.fragment,e),b(as.$$.fragment,e),b(ns.$$.fragment,e),bn=!1},d(e){s(w),e&&s($),e&&s(j),_(v),e&&s(Yt),_(je,e),e&&s(Ut),e&&s(rs),e&&s(Ht),e&&s(is),e&&s(Gt),_(Z,e),e&&s(Rt),e&&s(G),_(ye),e&&s(Qt),e&&s(ee),e&&s(Kt),_(ke,e),e&&s(Vt),e&&s(hs),e&&s(Jt),e&&s(k),e&&s(Zt),e&&s(se),e&&s(Xt),e&&s(us),e&&s(ea),_($e,e),e&&s(sa),_(Te,e),e&&s(ta),e&&s(fs),e&&s(aa),_(Ee,e),e&&s(na),e&&s(cs),e&&s(la),_(ze,e),e&&s(oa),e&&s(Ae),e&&s(ra),e&&s(gs),e&&s(ia),e&&s(B),e&&s(pa),e&&s(te),e&&s(ha),e&&s(ae),e&&s(ma),_(xe,e),e&&s(ua),e&&s(ws),e&&s(fa),e&&s(q),e&&s(ca),_(Ce,e),e&&s(da),e&&s(P),e&&s(ga),_(Se,e),e&&s(ba),e&&s(M),e&&s(_a),e&&s(Es),e&&s(wa),e&&s(R),_(De),e&&s(ja),e&&s(zs),e&&s(ya),_(Be,e),e&&s(ka),e&&s(le),e&&s(va),_(Pe,e),e&&s($a),e&&s(Q),_(Me),e&&s(Ta),e&&s(I),e&&s(Ea),e&&s(F),e&&s(za),e&&s(qs),e&&s(Aa),_(Ie,e),e&&s(qa),e&&s(N),e&&s(Fa),_(Ne,e),e&&s(xa),e&&s(Cs),e&&s(Ca),_(Le,e),e&&s(Sa),e&&s(Ss),e&&s(Da),_(We,e),e&&s(Ba),e&&s(re),e&&s(Pa),e&&s(K),_(Oe),e&&s(Ma),e&&s(pe),e&&s(Ia),_(Ye,e),e&&s(Na),e&&s(he),e&&s(La),_(Ue,e),e&&s(Wa),e&&s(me),e&&s(Oa),_(ue,e),e&&s(Ya),e&&s(Ps),e&&s(Ua),_(He,e),e&&s(Ha),e&&s(Ms),e&&s(Ga),_(Ge,e),e&&s(Ra),e&&s(fe),e&&s(Qa),_(Re,e),e&&s(Ka),e&&s(z),e&&s(Va),_(ce,e),e&&s(Ja),e&&s(Ls),e&&s(Za),_(Ve,e),e&&s(Xa),e&&s(de),e&&s(en),e&&s(Ws),e&&s(sn),_(Je,e),e&&s(tn),e&&s(Os),e&&s(an),_(Ze,e),e&&s(nn),e&&s(Ys),e&&s(ln),_(Xe,e),e&&s(on),e&&s(V),_(es),e&&s(rn),e&&s(L),e&&s(pn),e&&s(x),e&&s(hn),_(ss,e),e&&s(mn),e&&s(J),_(ts),e&&s(un),e&&s(_e),e&&s(fn),e&&s(W),e&&s(cn),_(as,e),e&&s(dn),e&&s(we),e&&s(gn),_(ns,e)}}}const Sp={local:"quick-tour",sections:[{local:"getting-started-on-a-task-with-a-pipeline",title:"Getting started on a task with a pipeline"},{local:"under-the-hood-pretrained-models",sections:[{local:"using-the-tokenizer",title:"Using the tokenizer"},{local:"using-the-model",title:"Using the model"},{local:"accessing-the-code",title:"Accessing the code"},{local:"customizing-the-model",title:"Customizing the model"}],title:"Under the hood: pretrained models"}],title:"Quick tour"};function Dp(H,w,$){let{fw:j}=w;return H.$$set=y=>{"fw"in y&&$(0,j=y.fw)},[j]}class Yp extends $p{constructor(w){super();Tp(this,w,Dp,Cp,Ep,{fw:0})}}export{Yp as default,Sp as metadata};
