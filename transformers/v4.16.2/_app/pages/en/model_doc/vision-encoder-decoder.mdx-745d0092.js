import{S as Yi,i as Ki,s as Qi,e as r,k as c,w as f,t,L as Xi,c as a,d as o,m as l,a as d,x as u,h as n,b as i,J as e,g as T,y as g,q as _,o as v,B as b}from"../../../chunks/vendor-b1433968.js";import{T as Ed}from"../../../chunks/Tip-c3840994.js";import{D as B}from"../../../chunks/Docstring-ff504c58.js";import{C as wo}from"../../../chunks/CodeBlock-a320dbd7.js";import{I as Ht}from"../../../chunks/IconCopyLink-7029626d.js";import"../../../chunks/CopyButton-f65cb278.js";function ec(Q){let p,P,m,D,F;return{c(){p=r("p"),P=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),D=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(y){p=a(y,"P",{});var x=d(p);P=n(x,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(x,"CODE",{});var C=d(m);D=n(C,"Module"),C.forEach(o),F=n(x,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),x.forEach(o)},m(y,x){T(y,p,x),e(p,P),e(p,m),e(m,D),e(p,F)},d(y){y&&o(p)}}}function oc(Q){let p,P,m,D,F;return{c(){p=r("p"),P=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),D=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(y){p=a(y,"P",{});var x=d(p);P=n(x,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(x,"CODE",{});var C=d(m);D=n(C,"Module"),C.forEach(o),F=n(x,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),x.forEach(o)},m(y,x){T(y,p,x),e(p,P),e(p,m),e(m,D),e(p,F)},d(y){y&&o(p)}}}function tc(Q){let p,P,m,D,F;return{c(){p=r("p"),P=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),D=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(y){p=a(y,"P",{});var x=d(p);P=n(x,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(x,"CODE",{});var C=d(m);D=n(C,"Module"),C.forEach(o),F=n(x,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),x.forEach(o)},m(y,x){T(y,p,x),e(p,P),e(p,m),e(m,D),e(p,F)},d(y){y&&o(p)}}}function nc(Q){let p,P,m,D,F,y,x,C,Dn,Zt,j,Mn,Eo,Vn,$n,tt,qn,zn,ko,Pn,Fn,xo,Cn,An,jo,Sn,In,nt,Ln,Nn,Do,On,Rn,Mo,Bn,Gn,Vo,Wn,Un,Jt,he,Hn,xe,Zn,Jn,Yt,G,Yn,$o,Kn,Qn,qo,Xn,er,Kt,X,me,rt,je,or,at,tr,Qt,z,De,nr,fe,zo,rr,ar,Po,sr,dr,ir,ee,cr,Fo,lr,pr,Co,hr,mr,fr,st,ur,gr,Me,_r,ue,Ve,vr,$e,br,Ao,Tr,yr,wr,ge,qe,Er,oe,kr,dt,xr,jr,it,Dr,Mr,Xt,te,_e,ct,ze,Vr,lt,$r,en,w,Pe,qr,ne,zr,pt,Pr,Fr,ht,Cr,Ar,Sr,Fe,Ir,Ce,Lr,Nr,Or,Ae,Rr,Se,Br,Gr,Wr,mt,Ur,Hr,Ie,Zr,So,Jr,Yr,Kr,Le,Qr,Ne,Xr,ea,oa,W,Io,ta,na,ft,ra,aa,ut,sa,da,ia,A,Oe,ca,re,la,Lo,pa,ha,gt,ma,fa,ua,ve,ga,_t,_a,va,Re,ba,S,Be,Ta,vt,ya,wa,ae,Ea,bt,ka,xa,Tt,ja,Da,Ma,yt,Va,$a,Ge,on,se,be,wt,We,qa,Et,za,tn,E,Ue,Pa,de,Fa,kt,Ca,Aa,xt,Sa,Ia,La,He,Na,Ze,Oa,Ra,Ba,Je,Ga,Ye,Wa,Ua,Ha,jt,Za,Ja,Ke,Ya,No,Ka,Qa,Xa,Qe,es,Xe,os,ts,ns,U,Oo,rs,as,Dt,ss,ds,Mt,is,cs,ls,I,eo,ps,ie,hs,Ro,ms,fs,Vt,us,gs,_s,Te,vs,$t,bs,Ts,oo,ys,O,to,ws,qt,Es,ks,zt,xs,js,no,nn,ce,ye,Pt,ro,Ds,Ft,Ms,rn,k,ao,Vs,le,$s,Ct,qs,zs,At,Ps,Fs,Cs,so,As,io,Ss,Is,Ls,co,Ns,lo,Os,Rs,Bs,St,Gs,Ws,po,Us,Bo,Hs,Zs,Js,ho,Ys,mo,Ks,Qs,Xs,H,Go,ed,od,It,td,nd,Lt,rd,ad,sd,L,fo,dd,pe,id,Wo,cd,ld,Nt,pd,hd,md,we,fd,Ot,ud,gd,uo,_d,R,go,vd,Rt,bd,Td,Bt,yd,wd,_o,an;return y=new Ht({}),je=new Ht({}),De=new B({props:{name:"class transformers.VisionEncoderDecoderConfig",anchor:"transformers.VisionEncoderDecoderConfig",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py#L27",parametersDescription:[{anchor:"transformers.VisionEncoderDecoderConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments. Notably:</p>
<ul>
<li><strong>encoder</strong> (<a href="/docs/transformers/v4.16.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014; An instance of a configuration object that defines
the encoder config.</li>
<li><strong>decoder</strong> (<a href="/docs/transformers/v4.16.2/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014; An instance of a configuration object that defines
the decoder config.</li>
</ul>`,name:"kwargs"}]}}),Me=new wo({props:{code:`from transformers import BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel

# Initializing a ViT & BERT style configuration
config_encoder = ViTConfig()
config_decoder = BertConfig()

config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

# Initializing a ViTBert model from a ViT & bert-base-uncased style configurations
model = VisionEncoderDecoderModel(config=config)

# Accessing the model configuration
config_encoder = model.config.encoder
config_decoder = model.config.decoder
# set decoder config to causal lm
config_decoder.is_decoder = True
config_decoder.add_cross_attention = True

# Saving the model, including its configuration
model.save_pretrained("my-model")

# loading model and config from pretrained folder
encoder_decoder_config = VisionEncoderDecoderConfig.from_pretrained("my-model")
model = VisionEncoderDecoderModel.from_pretrained("my-model", config=encoder_decoder_config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a ViT &amp; BERT style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_encoder = ViTConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder = BertConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a ViTBert model from a ViT &amp; bert-base-uncased style configurations</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionEncoderDecoderModel(config=config)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_encoder = model.config.encoder
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder = model.config.decoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># set decoder config to causal lm</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder.is_decoder = <span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder.add_cross_attention = <span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Saving the model, including its configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;my-model&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># loading model and config from pretrained folder</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_decoder_config = VisionEncoderDecoderConfig.from_pretrained(<span class="hljs-string">&quot;my-model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;my-model&quot;</span>, config=encoder_decoder_config)`}}),Ve=new B({props:{name:"from_encoder_decoder_configs",anchor:"transformers.VisionEncoderDecoderConfig.from_encoder_decoder_configs",parameters:[{name:"encoder_config",val:": PretrainedConfig"},{name:"decoder_config",val:": PretrainedConfig"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py#L93",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/v4.16.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"
>VisionEncoderDecoderConfig</a></p>
`}}),qe=new B({props:{name:"to_dict",anchor:"transformers.VisionEncoderDecoderConfig.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py#L110",returnDescription:`
<p>Dictionary of all the attributes that make up this configuration instance,</p>
`,returnType:`
<p><code>Dict[str, any]</code></p>
`}}),ze=new Ht({}),Pe=new B({props:{name:"class transformers.VisionEncoderDecoderModel",anchor:"transformers.VisionEncoderDecoderModel",parameters:[{name:"config",val:": typing.Optional[transformers.configuration_utils.PretrainedConfig] = None"},{name:"encoder",val:": typing.Optional[transformers.modeling_utils.PreTrainedModel] = None"},{name:"decoder",val:": typing.Optional[transformers.modeling_utils.PreTrainedModel] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py#L149",parametersDescription:[{anchor:"transformers.VisionEncoderDecoderModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Oe=new B({props:{name:"forward",anchor:"transformers.VisionEncoderDecoderModel.forward",parameters:[{name:"pixel_values",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"encoder_outputs",val:" = None"},{name:"past_key_values",val:" = None"},{name:"decoder_inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py#L401",parametersDescription:[{anchor:"transformers.VisionEncoderDecoderModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using a feature extractor (e.g. if you use ViT as the encoder,
you should use <a href="/docs/transformers/v4.16.2/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a>). See <a href="/docs/transformers/v4.16.2/en/model_doc/vit#transformers.ViTFeatureExtractor.__call__">ViTFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.VisionEncoderDecoderModel.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.16.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See <a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>For training, <code>decoder_input_ids</code> are automatically created by the model by shifting the <code>labels</code> to the
right, replacing -100 by the <code>pad_token_id</code> and prepending them with the <code>decoder_start_token_id</code>.`,name:"decoder_input_ids"},{anchor:"transformers.VisionEncoderDecoderModel.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.VisionEncoderDecoderModel.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
This tuple must consist of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) is a tensor
of hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the
decoder.`,name:"encoder_outputs"},{anchor:"transformers.VisionEncoderDecoderModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.VisionEncoderDecoderModel.forward.decoder_inputs_embeds",description:`<strong>decoder_inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, target_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>decoder_input_ids</code> you can choose to directly pass an embedded
representation. This is useful if you want more control over how to convert <code>decoder_input_ids</code> indices
into associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"decoder_inputs_embeds"},{anchor:"transformers.VisionEncoderDecoderModel.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss for the decoder. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.VisionEncoderDecoderModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.VisionEncoderDecoderModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.VisionEncoderDecoderModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.VisionEncoderDecoderModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the model will return a <code>Seq2SeqLMOutput</code> instead of a plain tuple.
kwargs &#x2014; (<em>optional</em>) Remaining dictionary of keyword arguments. Keyword arguments come in two flavors:</p>
<ul>
<li>Without a prefix which will be input as <code>**encoder_kwargs</code> for the encoder forward function.</li>
<li>With a <em>decoder_</em> prefix which will be input as <code>**decoder_kwargs</code> for the decoder forward function.</li>
</ul>`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.16.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>transformers.modeling_outputs.Seq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.16.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"
>VisionEncoderDecoderConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.16.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>transformers.modeling_outputs.Seq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ve=new Ed({props:{$$slots:{default:[ec]},$$scope:{ctx:Q}}}),Re=new wo({props:{code:`from transformers import TrOCRProcessor, VisionEncoderDecoderModel
import requests
from PIL import Image
import torch

processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-handwritten")
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-handwritten")

# load image from the IAM dataset
url = "https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg"
image = Image.open(requests.get(url, stream=True).raw).convert("RGB")

# training
model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
model.config.pad_token_id = processor.tokenizer.pad_token_id
model.config.vocab_size = model.config.decoder.vocab_size

pixel_values = processor(image, return_tensors="pt").pixel_values
text = "hello world"
labels = processor.tokenizer(text, return_tensors="pt").input_ids
outputs = model(pixel_values=pixel_values, labels=labels)
loss = outputs.loss

# inference (generation)
generated_ids = model.generate(pixel_values)
generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0],`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrOCRProcessor, VisionEncoderDecoderModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = TrOCRProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/trocr-base-handwritten&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;microsoft/trocr-base-handwritten&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load image from the IAM dataset</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw).convert(<span class="hljs-string">&quot;RGB&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># training</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = processor.tokenizer.pad_token_id
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.vocab_size = model.config.decoder.vocab_size

<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values
<span class="hljs-meta">&gt;&gt;&gt; </span>text = <span class="hljs-string">&quot;hello world&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = processor.tokenizer(text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(pixel_values=pixel_values, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># inference (generation)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(pixel_values)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]`}}),Be=new B({props:{name:"from_encoder_decoder_pretrained",anchor:"transformers.VisionEncoderDecoderModel.from_encoder_decoder_pretrained",parameters:[{name:"encoder_pretrained_model_name_or_path",val:": str = None"},{name:"decoder_pretrained_model_name_or_path",val:": str = None"},{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py#L246",parametersDescription:[{anchor:"transformers.VisionEncoderDecoderModel.from_encoder_decoder_pretrained.encoder_pretrained_model_name_or_path",description:`<strong>encoder_pretrained_model_name_or_path</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Information necessary to initiate the image encoder. Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co. An
example is <code>google/vit-base-patch16-224-in21k</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"encoder_pretrained_model_name_or_path"},{anchor:"transformers.VisionEncoderDecoderModel.from_encoder_decoder_pretrained.decoder_pretrained_model_name_or_path",description:`<strong>decoder_pretrained_model_name_or_path</strong> (<code>str</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Information necessary to initiate the text decoder. Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"decoder_pretrained_model_name_or_path"},{anchor:"transformers.VisionEncoderDecoderModel.from_encoder_decoder_pretrained.model_args",description:`<strong>model_args</strong> (remaining positional arguments, <em>optional</em>) &#x2014;
All remaning positional arguments will be passed to the underlying model&#x2019;s <code>__init__</code> method.`,name:"model_args"},{anchor:"transformers.VisionEncoderDecoderModel.from_encoder_decoder_pretrained.kwargs",description:`<strong>kwargs</strong> (remaining dictionary of keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>).</p>
<ul>
<li>To update the encoder configuration, use the prefix <em>encoder_</em> for each configuration parameter.</li>
<li>To update the decoder configuration, use the prefix <em>decoder_</em> for each configuration parameter.</li>
<li>To update the parent model configuration, do not use a prefix for each configuration parameter.</li>
</ul>
<p>Behaves differently depending on whether a <code>config</code> is provided or automatically loaded.`,name:"kwargs"}]}}),Ge=new wo({props:{code:`from transformers import VisionEncoderDecoderModel

# initialize a vit-bert from a pretrained ViT and a pretrained BERT model. Note that the cross-attention layers will be randomly initialized
model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(
    "google/vit-base-patch16-224-in21k", "bert-base-uncased"
)
# saving model after fine-tuning
model.save_pretrained("./vit-bert")
# load fine-tuned model
model = VisionEncoderDecoderModel.from_pretrained("./vit-bert"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> VisionEncoderDecoderModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a vit-bert from a pretrained ViT and a pretrained BERT model. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>, <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saving model after fine-tuning</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;./vit-bert&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load fine-tuned model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;./vit-bert&quot;</span>)`}}),We=new Ht({}),Ue=new B({props:{name:"class transformers.TFVisionEncoderDecoderModel",anchor:"transformers.TFVisionEncoderDecoderModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py#L138",parametersDescription:[{anchor:"transformers.TFVisionEncoderDecoderModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),eo=new B({props:{name:"call",anchor:"transformers.TFVisionEncoderDecoderModel.call",parameters:[{name:"pixel_values",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"encoder_outputs",val:" = None"},{name:"past_key_values",val:" = None"},{name:"decoder_inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py#L475",parametersDescription:[{anchor:"transformers.TFVisionEncoderDecoderModel.call.pixel_values",description:`<strong>pixel_values</strong> (<code>np.ndarray</code>, <code>tf.Tensor</code>, <code>List[tf.Tensor]</code> \`<code>Dict[str, tf.Tensor]</code> or <code>Dict[str, np.ndarray]</code> and each example must have the shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using the vision&#x2019;s model&#x2019;s feature extractor. For example, using
<a href="/docs/transformers/v4.16.2/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a>. See <a href="/docs/transformers/v4.16.2/en/model_doc/vit#transformers.ViTFeatureExtractor.__call__">ViTFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.16.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See <a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>Provide for sequence to sequence training to the decoder. Indices can be obtained using
<a href="/docs/transformers/v4.16.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See <a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for
details.`,name:"decoder_input_ids"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(tuple(tf.Tensor)</code>, <em>optional</em>) &#x2014;
This tuple must consist of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) is a tensor of hidden-states at the output
of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(tf.Tensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.decoder_inputs_embeds",description:`<strong>decoder_inputs_embeds</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, target_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>decoder_input_ids</code> you can choose to directly pass an embedded
representation. This is useful if you want more control over how to convert <code>decoder_input_ids</code> indices
into associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"decoder_inputs_embeds"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.labels",description:`<strong>labels</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss for the decoder. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the model will return a <code>Seq2SeqLMOutput</code> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.TFVisionEncoderDecoderModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).
kwargs &#x2014; (<em>optional</em>) Remaining dictionary of keyword arguments. Keyword arguments come in two flavors:</p>
<ul>
<li>Without a prefix which will be input as <code>**encoder_kwargs</code> for the encoder forward function.</li>
<li>With a <em>decoder_</em> prefix which will be input as <code>**decoder_kwargs</code> for the decoder forward function.</li>
</ul>`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.16.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput"
>transformers.modeling_tf_outputs.TFSeq2SeqLMOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/v4.16.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"
>VisionEncoderDecoderConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.16.2/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput"
>transformers.modeling_tf_outputs.TFSeq2SeqLMOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Te=new Ed({props:{$$slots:{default:[oc]},$$scope:{ctx:Q}}}),oo=new wo({props:{code:`from transformers import AutoFeatureExtractor, AutoTokenizer, TFVisionEncoderDecoderModel
from PIL import Image
import requests

feature_extractor = AutoFeatureExtractor.from_pretrained("google/vit-base-patch16-224-in21k")
decoder_tokenizer = AutoTokenizer.from_pretrained("gpt2")

# initialize a bert2gpt2 from a pretrained BERT and GPT2 models. Note that the cross-attention layers will be randomly initialized
model = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
    "google/vit-base-patch16-224-in21k", "gpt2"
)

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
img = Image.open(requests.get(url, stream=True).raw)

# forward
pixel_values = feature_extractor(images=img, return_tensors="tf").pixel_values  # Batch size 1
decoder_input_ids = decoder_tokenizer("Linda Davis", return_tensors="tf").input_ids  # Batch size 1
outputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)

# training
outputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids, labels=decoder_input_ids)
loss, logits = outputs.loss, outputs.logits

# save and load from pretrained
model.save_pretrained("vit-gpt2")
model = TFVisionEncoderDecoderModel.from_pretrained("vit-gpt2")

# generation
generated = model.generate(pixel_values, decoder_start_token_id=model.config.decoder.bos_token_id),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor, AutoTokenizer, TFVisionEncoderDecoderModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a bert2gpt2 from a pretrained BERT and GPT2 models. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>, <span class="hljs-string">&quot;gpt2&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>img = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># forward</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = feature_extractor(images=img, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>).pixel_values  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = decoder_tokenizer(<span class="hljs-string">&quot;Linda Davis&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># training</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids, labels=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss, logits = outputs.loss, outputs.logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># save and load from pretrained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;vit-gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFVisionEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;vit-gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># generation</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generated = model.generate(pixel_values, decoder_start_token_id=model.config.decoder.bos_token_id)`}}),to=new B({props:{name:"from_encoder_decoder_pretrained",anchor:"transformers.TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained",parameters:[{name:"encoder_pretrained_model_name_or_path",val:": str = None"},{name:"decoder_pretrained_model_name_or_path",val:": str = None"},{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py#L310",parametersDescription:[{anchor:"transformers.TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained.encoder_pretrained_model_name_or_path",description:`<strong>encoder_pretrained_model_name_or_path</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Information necessary to initiate the encoder. Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co. An
example is <code>google/vit-base-patch16-224-in21k</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>pytorch index checkpoint file</em> (e.g, <code>./pt_model/</code>). In this case,
<code>encoder_from_pt</code> should be set to <code>True</code>.</li>
</ul>`,name:"encoder_pretrained_model_name_or_path"},{anchor:"transformers.TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained.decoder_pretrained_model_name_or_path",description:`<strong>decoder_pretrained_model_name_or_path</strong> (<code>str</code>, <em>optional</em>, defaults to <em>None</em>) &#x2014;
Information necessary to initiate the decoder. Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>pytorch checkpoint file</em> (e.g, <code>./pt_model/</code>). In this case,
<code>decoder_from_pt</code> should be set to <code>True</code>.</li>
</ul>`,name:"decoder_pretrained_model_name_or_path"},{anchor:"transformers.TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained.model_args",description:`<strong>model_args</strong> (remaining positional arguments, <em>optional</em>) &#x2014;
All remaning positional arguments will be passed to the underlying model&#x2019;s <code>__init__</code> method.`,name:"model_args"},{anchor:"transformers.TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained.kwargs",description:`<strong>kwargs</strong> (remaining dictionary of keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>).</p>
<ul>
<li>To update the encoder configuration, use the prefix <em>encoder_</em> for each configuration parameter.</li>
<li>To update the decoder configuration, use the prefix <em>decoder_</em> for each configuration parameter.</li>
<li>To update the parent model configuration, do not use a prefix for each configuration parameter.</li>
</ul>
<p>Behaves differently depending on whether a <code>config</code> is provided or automatically loaded.`,name:"kwargs"}]}}),no=new wo({props:{code:`from transformers import TFVisionEncoderDecoderModel

# initialize a vit-bert from a pretrained ViT and a pretrained BERT model. Note that the cross-attention layers will be randomly initialized
model = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
    "google/vit-base-patch16-224-in21k", "bert-base-uncased"
)
# saving model after fine-tuning
model.save_pretrained("./vit-bert")
# load fine-tuned model
model = TFVisionEncoderDecoderModel.from_pretrained("./vit-bert"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFVisionEncoderDecoderModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a vit-bert from a pretrained ViT and a pretrained BERT model. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>, <span class="hljs-string">&quot;bert-base-uncased&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saving model after fine-tuning</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;./vit-bert&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load fine-tuned model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFVisionEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;./vit-bert&quot;</span>)`}}),ro=new Ht({}),ao=new B({props:{name:"class transformers.FlaxVisionEncoderDecoderModel",anchor:"transformers.FlaxVisionEncoderDecoderModel",parameters:[{name:"config",val:": VisionEncoderDecoderConfig"},{name:"input_shape",val:": typing.Optional[typing.Tuple] = None"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py#L268",parametersDescription:[{anchor:"transformers.FlaxVisionEncoderDecoderModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on GPUs) and
<code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and
<a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}]}}),fo=new B({props:{name:"__call__",anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__",parameters:[{name:"pixel_values",val:": ndarray"},{name:"decoder_input_ids",val:": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"},{name:"decoder_attention_mask",val:": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"},{name:"decoder_position_ids",val:": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"train",val:": bool = False"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py#L579",parametersDescription:[{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.pixel_values",description:`<strong>pixel_values</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using the vision model&#x2019;s feature extractor. For example, using
<a href="/docs/transformers/v4.16.2/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a>. See <a href="/docs/transformers/v4.16.2/en/model_doc/vit#transformers.ViTFeatureExtractor.__call__">ViTFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.16.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See <a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a>`,name:"decoder_input_ids"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.decoder_position_ids",description:`<strong>decoder_position_ids</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the
range <code>[0, config.decoder.max_position_embeddings - 1]</code>.`,name:"decoder_position_ids"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the model will return a <code>FlaxSeq2SeqLMOutput</code> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.16.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput"
>transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.16.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"
>VisionEncoderDecoderConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(jnp.ndarray)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.16.2/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput"
>transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),we=new Ed({props:{$$slots:{default:[tc]},$$scope:{ctx:Q}}}),uo=new wo({props:{code:`from transformers import FlaxVisionEncoderDecoderModel, ViTFeatureExtractor, GPT2Tokenizer
from PIL import Image
import requests

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

feature_extractor = ViTFeatureExtractor.from_pretrained("google/vit-base-patch16-224-in21k")

# load output tokenizer
tokenizer_output = GPT2Tokenizer.from_pretrained("gpt2")

# initialize a vit-gpt2 from pretrained ViT and GPT2 models. Note that the cross-attention layers will be randomly initialized
model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained("vit", "gpt2")

pixel_values = feature_extractor(images=image, return_tensors="np").pixel_values

# use GPT2's eos_token as the pad as well as eos token
model.config.eos_token_id = model.config.decoder.eos_token_id
model.config.pad_token_id = model.config.eos_token_id

# generation
sequences = model.generate(pixel_values, num_beams=4, max_length=12).sequences

captions = tokenizer_output.batch_decode(sequences, skip_special_tokens=True),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxVisionEncoderDecoderModel, ViTFeatureExtractor, GPT2Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = ViTFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load output tokenizer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_output = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a vit-gpt2 from pretrained ViT and GPT2 models. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained(<span class="hljs-string">&quot;vit&quot;</span>, <span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = feature_extractor(images=image, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).pixel_values

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># use GPT2&#x27;s eos_token as the pad as well as eos token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.eos_token_id = model.config.decoder.eos_token_id
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = model.config.eos_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># generation</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>sequences = model.generate(pixel_values, num_beams=<span class="hljs-number">4</span>, max_length=<span class="hljs-number">12</span>).sequences

<span class="hljs-meta">&gt;&gt;&gt; </span>captions = tokenizer_output.batch_decode(sequences, skip_special_tokens=<span class="hljs-literal">True</span>)`}}),go=new B({props:{name:"from_encoder_decoder_pretrained",anchor:"transformers.FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained",parameters:[{name:"encoder_pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike, NoneType] = None"},{name:"decoder_pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike, NoneType] = None"},{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py#L702",parametersDescription:[{anchor:"transformers.FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained.encoder_pretrained_model_name_or_path",description:`<strong>encoder_pretrained_model_name_or_path</strong> (<code>Union[str, os.PathLike]</code>, <em>optional</em>) &#x2014;
Information necessary to initiate the encoder. Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co. An
example is <code>google/vit-base-patch16-224-in21k</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"encoder_pretrained_model_name_or_path"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained.decoder_pretrained_model_name_or_path",description:`<strong>decoder_pretrained_model_name_or_path</strong> (<code>Union[str, os.PathLike]</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
Information necessary to initiate the decoder. Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"decoder_pretrained_model_name_or_path"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained.model_args",description:`<strong>model_args</strong> (remaining positional arguments, <em>optional</em>) &#x2014;
All remaning positional arguments will be passed to the underlying model&#x2019;s <code>__init__</code> method.`,name:"model_args"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained.kwargs",description:`<strong>kwargs</strong> (remaining dictionary of keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>).</p>
<ul>
<li>To update the encoder configuration, use the prefix <em>encoder_</em> for each configuration parameter.</li>
<li>To update the decoder configuration, use the prefix <em>decoder_</em> for each configuration parameter.</li>
<li>To update the parent model configuration, do not use a prefix for each configuration parameter.</li>
</ul>
<p>Behaves differently depending on whether a <code>config</code> is provided or automatically loaded.`,name:"kwargs"}]}}),_o=new wo({props:{code:`from transformers import FlaxVisionEncoderDecoderModel

# initialize a vit-gpt2 from a pretrained ViT and a pretrained GPT2 model. Note that the cross-attention layers will be randomly initialized
model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
    "google/vit-base-patch16-224-in21k", "gpt2"
)
# saving model after fine-tuning
model.save_pretrained("./vit-gpt2")
# load fine-tuned model
model = FlaxVisionEncoderDecoderModel.from_pretrained("./vit-gpt2"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxVisionEncoderDecoderModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a vit-gpt2 from a pretrained ViT and a pretrained GPT2 model. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>, <span class="hljs-string">&quot;gpt2&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saving model after fine-tuning</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;./vit-gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load fine-tuned model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxVisionEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;./vit-gpt2&quot;</span>)`}}),{c(){p=r("meta"),P=c(),m=r("h1"),D=r("a"),F=r("span"),f(y.$$.fragment),x=c(),C=r("span"),Dn=t("Vision Encoder Decoder Models"),Zt=c(),j=r("p"),Mn=t("The "),Eo=r("a"),Vn=t("VisionEncoderDecoderModel"),$n=t(` can be used to initialize an image-to-text-sequence model with any
pretrained vision autoencoding model as the encoder (`),tt=r("em"),qn=t("e.g."),zn=c(),ko=r("a"),Pn=t("ViT"),Fn=t(", "),xo=r("a"),Cn=t("BEiT"),An=t(", "),jo=r("a"),Sn=t("DeiT"),In=t(`)
and any pretrained language model as the decoder (`),nt=r("em"),Ln=t("e.g."),Nn=c(),Do=r("a"),On=t("RoBERTa"),Rn=t(", "),Mo=r("a"),Bn=t("GPT2"),Gn=t(", "),Vo=r("a"),Wn=t("BERT"),Un=t(")."),Jt=c(),he=r("p"),Hn=t(`The effectiveness of initializing image-to-text-sequence models with pretrained checkpoints has been shown in (for
example) `),xe=r("a"),Zn=t("TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models"),Jn=t(` by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang,
Zhoujun Li, Furu Wei.`),Yt=c(),G=r("p"),Yn=t("An example of how to use a "),$o=r("a"),Kn=t("VisionEncoderDecoderModel"),Qn=t(" for inference can be seen in "),qo=r("a"),Xn=t("TrOCR"),er=t("."),Kt=c(),X=r("h2"),me=r("a"),rt=r("span"),f(je.$$.fragment),or=c(),at=r("span"),tr=t("VisionEncoderDecoderConfig"),Qt=c(),z=r("div"),f(De.$$.fragment),nr=c(),fe=r("p"),zo=r("a"),rr=t("VisionEncoderDecoderConfig"),ar=t(` is the configuration class to store the configuration of a
`),Po=r("a"),sr=t("VisionEncoderDecoderModel"),dr=t(`. It is used to instantiate a Vision-Encoder-Text-Decoder model according to the
specified arguments, defining the encoder and decoder configs.`),ir=c(),ee=r("p"),cr=t("Configuration objects inherit from "),Fo=r("a"),lr=t("PretrainedConfig"),pr=t(` and can be used to control the model outputs. Read the
documentation from `),Co=r("a"),hr=t("PretrainedConfig"),mr=t(" for more information."),fr=c(),st=r("p"),ur=t("Examples:"),gr=c(),f(Me.$$.fragment),_r=c(),ue=r("div"),f(Ve.$$.fragment),vr=c(),$e=r("p"),br=t("Instantiate a "),Ao=r("a"),Tr=t("VisionEncoderDecoderConfig"),yr=t(` (or a derived class) from a pre-trained encoder model
configuration and decoder model configuration.`),wr=c(),ge=r("div"),f(qe.$$.fragment),Er=c(),oe=r("p"),kr=t("Serializes this instance to a Python dictionary. Override the default "),dt=r("em"),xr=t("to_dict()"),jr=t(" from "),it=r("em"),Dr=t("PretrainedConfig"),Mr=t("."),Xt=c(),te=r("h2"),_e=r("a"),ct=r("span"),f(ze.$$.fragment),Vr=c(),lt=r("span"),$r=t("VisionEncoderDecoderModel"),en=c(),w=r("div"),f(Pe.$$.fragment),qr=c(),ne=r("p"),zr=t(`This class can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model
as the encoder and any pretrained text autoregressive model as the decoder. The encoder is loaded via
`),pt=r("code"),Pr=t("from_pretrained()"),Fr=t("function and the decoder is loaded via "),ht=r("code"),Cr=t("from_pretrained()"),Ar=t(`function. Cross-attention layers are automatically added to the decoder and should be fine-tuned on a downstream
generative task, like image captioning.`),Sr=c(),Fe=r("p"),Ir=t(`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),Ce=r("a"),Lr=t(`Leveraging Pre-trained Checkpoints for Sequence Generation
Tasks`),Nr=t(` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),Or=c(),Ae=r("p"),Rr=t("Additionally, in "),Se=r("a"),Br=t(`TrOCR: Transformer-based Optical Character Recognition with Pre-trained
Models`),Gr=t(` it is shown how leveraging large pretrained vision models for optical
character recognition (OCR) yields a significant performance improvement.`),Wr=c(),mt=r("p"),Ur=t(`After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned, it can be saved/loaded just like any
other models (see the examples for more information).`),Hr=c(),Ie=r("p"),Zr=t("This model inherits from "),So=r("a"),Jr=t("PreTrainedModel"),Yr=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Kr=c(),Le=r("p"),Qr=t("This model is also a PyTorch "),Ne=r("a"),Xr=t("torch.nn.Module"),ea=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),oa=c(),W=r("p"),Io=r("a"),ta=t("VisionEncoderDecoderModel"),na=t(` is a generic model class that will be instantiated as a transformer architecture with
one of the base vision model classes of the library as encoder and another one as decoder when created with the
:meth`),ft=r("em"),ra=t("~transformers.AutoModel.from_pretrained"),aa=t(` class method for the encoder and
:meth`),ut=r("em"),sa=t("~transformers.AutoModelForCausalLM.from_pretrained"),da=t(" class method for the decoder."),ia=c(),A=r("div"),f(Oe.$$.fragment),ca=c(),re=r("p"),la=t("The "),Lo=r("a"),pa=t("VisionEncoderDecoderModel"),ha=t(" forward method, overrides the "),gt=r("code"),ma=t("__call__"),fa=t(" special method."),ua=c(),f(ve.$$.fragment),ga=c(),_t=r("p"),_a=t("Examples:"),va=c(),f(Re.$$.fragment),ba=c(),S=r("div"),f(Be.$$.fragment),Ta=c(),vt=r("p"),ya=t(`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),wa=c(),ae=r("p"),Ea=t("The model is set in evaluation mode by default using "),bt=r("code"),ka=t("model.eval()"),xa=t(` (Dropout modules are deactivated). To train
the model, you need to first set it back in training mode with `),Tt=r("code"),ja=t("model.train()"),Da=t("."),Ma=c(),yt=r("p"),Va=t("Example:"),$a=c(),f(Ge.$$.fragment),on=c(),se=r("h2"),be=r("a"),wt=r("span"),f(We.$$.fragment),qa=c(),Et=r("span"),za=t("TFVisionEncoderDecoderModel"),tn=c(),E=r("div"),f(Ue.$$.fragment),Pa=c(),de=r("p"),Fa=t(`This class can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model
as the encoder and any pretrained text autoregressive model as the decoder. The encoder is loaded via
`),kt=r("code"),Ca=t("from_pretrained()"),Aa=t("function and the decoder is loaded via "),xt=r("code"),Sa=t("from_pretrained()"),Ia=t(`function. Cross-attention layers are automatically added to the decoder and should be fine-tuned on a downstream
generative task, like image captioning.`),La=c(),He=r("p"),Na=t(`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),Ze=r("a"),Oa=t(`Leveraging Pre-trained Checkpoints for Sequence Generation
Tasks`),Ra=t(` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),Ba=c(),Je=r("p"),Ga=t("Additionally, in "),Ye=r("a"),Wa=t(`TrOCR: Transformer-based Optical Character Recognition with Pre-trained
Models`),Ua=t(` it is shown how leveraging large pretrained vision models for optical
character recognition (OCR) yields a significant performance improvement.`),Ha=c(),jt=r("p"),Za=t(`After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned, it can be saved/loaded just like any
other models (see the examples for more information).`),Ja=c(),Ke=r("p"),Ya=t("This model inherits from "),No=r("a"),Ka=t("TFPreTrainedModel"),Qa=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Xa=c(),Qe=r("p"),es=t("This model is also a "),Xe=r("a"),os=t("tf.keras.Model"),ts=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),ns=c(),U=r("p"),Oo=r("a"),rs=t("TFVisionEncoderDecoderModel"),as=t(` is a generic model class that will be instantiated as a transformer architecture
with one of the base vision model classes of the library as encoder and another one of the base model classes as
decoder when created with the `),Dt=r("code"),ss=t("from_pretrained()"),ds=t(`class method for the encoder and
`),Mt=r("code"),is=t("from_pretrained()"),cs=t("class method for the decoder."),ls=c(),I=r("div"),f(eo.$$.fragment),ps=c(),ie=r("p"),hs=t("The "),Ro=r("a"),ms=t("TFVisionEncoderDecoderModel"),fs=t(" forward method, overrides the "),Vt=r("code"),us=t("__call__"),gs=t(" special method."),_s=c(),f(Te.$$.fragment),vs=c(),$t=r("p"),bs=t("Examples:"),Ts=c(),f(oo.$$.fragment),ys=c(),O=r("div"),f(to.$$.fragment),ws=c(),qt=r("p"),Es=t(`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),ks=c(),zt=r("p"),xs=t("Example:"),js=c(),f(no.$$.fragment),nn=c(),ce=r("h2"),ye=r("a"),Pt=r("span"),f(ro.$$.fragment),Ds=c(),Ft=r("span"),Ms=t("FlaxVisionEncoderDecoderModel"),rn=c(),k=r("div"),f(ao.$$.fragment),Vs=c(),le=r("p"),$s=t(`This class can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model
as the encoder and any pretrained text autoregressive model as the decoder. The encoder is loaded via
`),Ct=r("code"),qs=t("from_pretrained()"),zs=t("function and the decoder is loaded via "),At=r("code"),Ps=t("from_pretrained()"),Fs=t(`function. Cross-attention layers are automatically added to the decoder and should be fine-tuned on a downstream
generative task, like image captioning.`),Cs=c(),so=r("p"),As=t(`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),io=r("a"),Ss=t(`Leveraging Pre-trained Checkpoints for Sequence Generation
Tasks`),Is=t(` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),Ls=c(),co=r("p"),Ns=t("Additionally, in "),lo=r("a"),Os=t(`TrOCR: Transformer-based Optical Character Recognition with Pre-trained
Models`),Rs=t(` it is shown how leveraging large pretrained vision models for optical
character recognition (OCR) yields a significant performance improvement.`),Bs=c(),St=r("p"),Gs=t(`After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned, it can be saved/loaded just like any
other models (see the examples for more information).`),Ws=c(),po=r("p"),Us=t("This model inherits from "),Bo=r("a"),Hs=t("FlaxPreTrainedModel"),Zs=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Js=c(),ho=r("p"),Ys=t(`This model is also a Flax Linen
`),mo=r("a"),Ks=t("flax.nn.Module"),Qs=t(` subclass. Use it as a
regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.`),Xs=c(),H=r("p"),Go=r("a"),ed=t("FlaxVisionEncoderDecoderModel"),od=t(` is a generic model class that will be instantiated as a transformer architecture
with the module (flax.nn.Module) of one of the base vision model classes of the library as encoder module and
another one as decoder module when created with the :meth`),It=r("em"),td=t("~transformers.FlaxAutoModel.from_pretrained"),nd=t(` class method
for the encoder and :meth`),Lt=r("em"),rd=t("~transformers.FlaxAutoModelForCausalLM.from_pretrained"),ad=t(" class method for the decoder."),sd=c(),L=r("div"),f(fo.$$.fragment),dd=c(),pe=r("p"),id=t("The "),Wo=r("a"),cd=t("FlaxVisionEncoderDecoderModel"),ld=t(" forward method, overrides the "),Nt=r("code"),pd=t("__call__"),hd=t(" special method."),md=c(),f(we.$$.fragment),fd=c(),Ot=r("p"),ud=t("Examples:"),gd=c(),f(uo.$$.fragment),_d=c(),R=r("div"),f(go.$$.fragment),vd=c(),Rt=r("p"),bd=t(`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),Td=c(),Bt=r("p"),yd=t("Example:"),wd=c(),f(_o.$$.fragment),this.h()},l(s){const h=Xi('[data-svelte="svelte-1phssyn"]',document.head);p=a(h,"META",{name:!0,content:!0}),h.forEach(o),P=l(s),m=a(s,"H1",{class:!0});var vo=d(m);D=a(vo,"A",{id:!0,class:!0,href:!0});var Gt=d(D);F=a(Gt,"SPAN",{});var Wt=d(F);u(y.$$.fragment,Wt),Wt.forEach(o),Gt.forEach(o),x=l(vo),C=a(vo,"SPAN",{});var kd=d(C);Dn=n(kd,"Vision Encoder Decoder Models"),kd.forEach(o),vo.forEach(o),Zt=l(s),j=a(s,"P",{});var M=d(j);Mn=n(M,"The "),Eo=a(M,"A",{href:!0});var xd=d(Eo);Vn=n(xd,"VisionEncoderDecoderModel"),xd.forEach(o),$n=n(M,` can be used to initialize an image-to-text-sequence model with any
pretrained vision autoencoding model as the encoder (`),tt=a(M,"EM",{});var jd=d(tt);qn=n(jd,"e.g."),jd.forEach(o),zn=l(M),ko=a(M,"A",{href:!0});var Dd=d(ko);Pn=n(Dd,"ViT"),Dd.forEach(o),Fn=n(M,", "),xo=a(M,"A",{href:!0});var Md=d(xo);Cn=n(Md,"BEiT"),Md.forEach(o),An=n(M,", "),jo=a(M,"A",{href:!0});var Vd=d(jo);Sn=n(Vd,"DeiT"),Vd.forEach(o),In=n(M,`)
and any pretrained language model as the decoder (`),nt=a(M,"EM",{});var $d=d(nt);Ln=n($d,"e.g."),$d.forEach(o),Nn=l(M),Do=a(M,"A",{href:!0});var qd=d(Do);On=n(qd,"RoBERTa"),qd.forEach(o),Rn=n(M,", "),Mo=a(M,"A",{href:!0});var zd=d(Mo);Bn=n(zd,"GPT2"),zd.forEach(o),Gn=n(M,", "),Vo=a(M,"A",{href:!0});var Pd=d(Vo);Wn=n(Pd,"BERT"),Pd.forEach(o),Un=n(M,")."),M.forEach(o),Jt=l(s),he=a(s,"P",{});var sn=d(he);Hn=n(sn,`The effectiveness of initializing image-to-text-sequence models with pretrained checkpoints has been shown in (for
example) `),xe=a(sn,"A",{href:!0,rel:!0});var Fd=d(xe);Zn=n(Fd,"TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models"),Fd.forEach(o),Jn=n(sn,` by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang,
Zhoujun Li, Furu Wei.`),sn.forEach(o),Yt=l(s),G=a(s,"P",{});var Uo=d(G);Yn=n(Uo,"An example of how to use a "),$o=a(Uo,"A",{href:!0});var Cd=d($o);Kn=n(Cd,"VisionEncoderDecoderModel"),Cd.forEach(o),Qn=n(Uo," for inference can be seen in "),qo=a(Uo,"A",{href:!0});var Ad=d(qo);Xn=n(Ad,"TrOCR"),Ad.forEach(o),er=n(Uo,"."),Uo.forEach(o),Kt=l(s),X=a(s,"H2",{class:!0});var dn=d(X);me=a(dn,"A",{id:!0,class:!0,href:!0});var Sd=d(me);rt=a(Sd,"SPAN",{});var Id=d(rt);u(je.$$.fragment,Id),Id.forEach(o),Sd.forEach(o),or=l(dn),at=a(dn,"SPAN",{});var Ld=d(at);tr=n(Ld,"VisionEncoderDecoderConfig"),Ld.forEach(o),dn.forEach(o),Qt=l(s),z=a(s,"DIV",{class:!0});var N=d(z);u(De.$$.fragment,N),nr=l(N),fe=a(N,"P",{});var Ut=d(fe);zo=a(Ut,"A",{href:!0});var Nd=d(zo);rr=n(Nd,"VisionEncoderDecoderConfig"),Nd.forEach(o),ar=n(Ut,` is the configuration class to store the configuration of a
`),Po=a(Ut,"A",{href:!0});var Od=d(Po);sr=n(Od,"VisionEncoderDecoderModel"),Od.forEach(o),dr=n(Ut,`. It is used to instantiate a Vision-Encoder-Text-Decoder model according to the
specified arguments, defining the encoder and decoder configs.`),Ut.forEach(o),ir=l(N),ee=a(N,"P",{});var Ho=d(ee);cr=n(Ho,"Configuration objects inherit from "),Fo=a(Ho,"A",{href:!0});var Rd=d(Fo);lr=n(Rd,"PretrainedConfig"),Rd.forEach(o),pr=n(Ho,` and can be used to control the model outputs. Read the
documentation from `),Co=a(Ho,"A",{href:!0});var Bd=d(Co);hr=n(Bd,"PretrainedConfig"),Bd.forEach(o),mr=n(Ho," for more information."),Ho.forEach(o),fr=l(N),st=a(N,"P",{});var Gd=d(st);ur=n(Gd,"Examples:"),Gd.forEach(o),gr=l(N),u(Me.$$.fragment,N),_r=l(N),ue=a(N,"DIV",{class:!0});var cn=d(ue);u(Ve.$$.fragment,cn),vr=l(cn),$e=a(cn,"P",{});var ln=d($e);br=n(ln,"Instantiate a "),Ao=a(ln,"A",{href:!0});var Wd=d(Ao);Tr=n(Wd,"VisionEncoderDecoderConfig"),Wd.forEach(o),yr=n(ln,` (or a derived class) from a pre-trained encoder model
configuration and decoder model configuration.`),ln.forEach(o),cn.forEach(o),wr=l(N),ge=a(N,"DIV",{class:!0});var pn=d(ge);u(qe.$$.fragment,pn),Er=l(pn),oe=a(pn,"P",{});var Zo=d(oe);kr=n(Zo,"Serializes this instance to a Python dictionary. Override the default "),dt=a(Zo,"EM",{});var Ud=d(dt);xr=n(Ud,"to_dict()"),Ud.forEach(o),jr=n(Zo," from "),it=a(Zo,"EM",{});var Hd=d(it);Dr=n(Hd,"PretrainedConfig"),Hd.forEach(o),Mr=n(Zo,"."),Zo.forEach(o),pn.forEach(o),N.forEach(o),Xt=l(s),te=a(s,"H2",{class:!0});var hn=d(te);_e=a(hn,"A",{id:!0,class:!0,href:!0});var Zd=d(_e);ct=a(Zd,"SPAN",{});var Jd=d(ct);u(ze.$$.fragment,Jd),Jd.forEach(o),Zd.forEach(o),Vr=l(hn),lt=a(hn,"SPAN",{});var Yd=d(lt);$r=n(Yd,"VisionEncoderDecoderModel"),Yd.forEach(o),hn.forEach(o),en=l(s),w=a(s,"DIV",{class:!0});var V=d(w);u(Pe.$$.fragment,V),qr=l(V),ne=a(V,"P",{});var Jo=d(ne);zr=n(Jo,`This class can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model
as the encoder and any pretrained text autoregressive model as the decoder. The encoder is loaded via
`),pt=a(Jo,"CODE",{});var Kd=d(pt);Pr=n(Kd,"from_pretrained()"),Kd.forEach(o),Fr=n(Jo,"function and the decoder is loaded via "),ht=a(Jo,"CODE",{});var Qd=d(ht);Cr=n(Qd,"from_pretrained()"),Qd.forEach(o),Ar=n(Jo,`function. Cross-attention layers are automatically added to the decoder and should be fine-tuned on a downstream
generative task, like image captioning.`),Jo.forEach(o),Sr=l(V),Fe=a(V,"P",{});var mn=d(Fe);Ir=n(mn,`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),Ce=a(mn,"A",{href:!0,rel:!0});var Xd=d(Ce);Lr=n(Xd,`Leveraging Pre-trained Checkpoints for Sequence Generation
Tasks`),Xd.forEach(o),Nr=n(mn,` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),mn.forEach(o),Or=l(V),Ae=a(V,"P",{});var fn=d(Ae);Rr=n(fn,"Additionally, in "),Se=a(fn,"A",{href:!0,rel:!0});var ei=d(Se);Br=n(ei,`TrOCR: Transformer-based Optical Character Recognition with Pre-trained
Models`),ei.forEach(o),Gr=n(fn,` it is shown how leveraging large pretrained vision models for optical
character recognition (OCR) yields a significant performance improvement.`),fn.forEach(o),Wr=l(V),mt=a(V,"P",{});var oi=d(mt);Ur=n(oi,`After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned, it can be saved/loaded just like any
other models (see the examples for more information).`),oi.forEach(o),Hr=l(V),Ie=a(V,"P",{});var un=d(Ie);Zr=n(un,"This model inherits from "),So=a(un,"A",{href:!0});var ti=d(So);Jr=n(ti,"PreTrainedModel"),ti.forEach(o),Yr=n(un,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),un.forEach(o),Kr=l(V),Le=a(V,"P",{});var gn=d(Le);Qr=n(gn,"This model is also a PyTorch "),Ne=a(gn,"A",{href:!0,rel:!0});var ni=d(Ne);Xr=n(ni,"torch.nn.Module"),ni.forEach(o),ea=n(gn,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),gn.forEach(o),oa=l(V),W=a(V,"P",{});var bo=d(W);Io=a(bo,"A",{href:!0});var ri=d(Io);ta=n(ri,"VisionEncoderDecoderModel"),ri.forEach(o),na=n(bo,` is a generic model class that will be instantiated as a transformer architecture with
one of the base vision model classes of the library as encoder and another one as decoder when created with the
:meth`),ft=a(bo,"EM",{});var ai=d(ft);ra=n(ai,"~transformers.AutoModel.from_pretrained"),ai.forEach(o),aa=n(bo,` class method for the encoder and
:meth`),ut=a(bo,"EM",{});var si=d(ut);sa=n(si,"~transformers.AutoModelForCausalLM.from_pretrained"),si.forEach(o),da=n(bo," class method for the decoder."),bo.forEach(o),ia=l(V),A=a(V,"DIV",{class:!0});var Z=d(A);u(Oe.$$.fragment,Z),ca=l(Z),re=a(Z,"P",{});var Yo=d(re);la=n(Yo,"The "),Lo=a(Yo,"A",{href:!0});var di=d(Lo);pa=n(di,"VisionEncoderDecoderModel"),di.forEach(o),ha=n(Yo," forward method, overrides the "),gt=a(Yo,"CODE",{});var ii=d(gt);ma=n(ii,"__call__"),ii.forEach(o),fa=n(Yo," special method."),Yo.forEach(o),ua=l(Z),u(ve.$$.fragment,Z),ga=l(Z),_t=a(Z,"P",{});var ci=d(_t);_a=n(ci,"Examples:"),ci.forEach(o),va=l(Z),u(Re.$$.fragment,Z),Z.forEach(o),ba=l(V),S=a(V,"DIV",{class:!0});var J=d(S);u(Be.$$.fragment,J),Ta=l(J),vt=a(J,"P",{});var li=d(vt);ya=n(li,`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),li.forEach(o),wa=l(J),ae=a(J,"P",{});var Ko=d(ae);Ea=n(Ko,"The model is set in evaluation mode by default using "),bt=a(Ko,"CODE",{});var pi=d(bt);ka=n(pi,"model.eval()"),pi.forEach(o),xa=n(Ko,` (Dropout modules are deactivated). To train
the model, you need to first set it back in training mode with `),Tt=a(Ko,"CODE",{});var hi=d(Tt);ja=n(hi,"model.train()"),hi.forEach(o),Da=n(Ko,"."),Ko.forEach(o),Ma=l(J),yt=a(J,"P",{});var mi=d(yt);Va=n(mi,"Example:"),mi.forEach(o),$a=l(J),u(Ge.$$.fragment,J),J.forEach(o),V.forEach(o),on=l(s),se=a(s,"H2",{class:!0});var _n=d(se);be=a(_n,"A",{id:!0,class:!0,href:!0});var fi=d(be);wt=a(fi,"SPAN",{});var ui=d(wt);u(We.$$.fragment,ui),ui.forEach(o),fi.forEach(o),qa=l(_n),Et=a(_n,"SPAN",{});var gi=d(Et);za=n(gi,"TFVisionEncoderDecoderModel"),gi.forEach(o),_n.forEach(o),tn=l(s),E=a(s,"DIV",{class:!0});var $=d(E);u(Ue.$$.fragment,$),Pa=l($),de=a($,"P",{});var Qo=d(de);Fa=n(Qo,`This class can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model
as the encoder and any pretrained text autoregressive model as the decoder. The encoder is loaded via
`),kt=a(Qo,"CODE",{});var _i=d(kt);Ca=n(_i,"from_pretrained()"),_i.forEach(o),Aa=n(Qo,"function and the decoder is loaded via "),xt=a(Qo,"CODE",{});var vi=d(xt);Sa=n(vi,"from_pretrained()"),vi.forEach(o),Ia=n(Qo,`function. Cross-attention layers are automatically added to the decoder and should be fine-tuned on a downstream
generative task, like image captioning.`),Qo.forEach(o),La=l($),He=a($,"P",{});var vn=d(He);Na=n(vn,`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),Ze=a(vn,"A",{href:!0,rel:!0});var bi=d(Ze);Oa=n(bi,`Leveraging Pre-trained Checkpoints for Sequence Generation
Tasks`),bi.forEach(o),Ra=n(vn,` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),vn.forEach(o),Ba=l($),Je=a($,"P",{});var bn=d(Je);Ga=n(bn,"Additionally, in "),Ye=a(bn,"A",{href:!0,rel:!0});var Ti=d(Ye);Wa=n(Ti,`TrOCR: Transformer-based Optical Character Recognition with Pre-trained
Models`),Ti.forEach(o),Ua=n(bn,` it is shown how leveraging large pretrained vision models for optical
character recognition (OCR) yields a significant performance improvement.`),bn.forEach(o),Ha=l($),jt=a($,"P",{});var yi=d(jt);Za=n(yi,`After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned, it can be saved/loaded just like any
other models (see the examples for more information).`),yi.forEach(o),Ja=l($),Ke=a($,"P",{});var Tn=d(Ke);Ya=n(Tn,"This model inherits from "),No=a(Tn,"A",{href:!0});var wi=d(No);Ka=n(wi,"TFPreTrainedModel"),wi.forEach(o),Qa=n(Tn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Tn.forEach(o),Xa=l($),Qe=a($,"P",{});var yn=d(Qe);es=n(yn,"This model is also a "),Xe=a(yn,"A",{href:!0,rel:!0});var Ei=d(Xe);os=n(Ei,"tf.keras.Model"),Ei.forEach(o),ts=n(yn,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),yn.forEach(o),ns=l($),U=a($,"P",{});var To=d(U);Oo=a(To,"A",{href:!0});var ki=d(Oo);rs=n(ki,"TFVisionEncoderDecoderModel"),ki.forEach(o),as=n(To,` is a generic model class that will be instantiated as a transformer architecture
with one of the base vision model classes of the library as encoder and another one of the base model classes as
decoder when created with the `),Dt=a(To,"CODE",{});var xi=d(Dt);ss=n(xi,"from_pretrained()"),xi.forEach(o),ds=n(To,`class method for the encoder and
`),Mt=a(To,"CODE",{});var ji=d(Mt);is=n(ji,"from_pretrained()"),ji.forEach(o),cs=n(To,"class method for the decoder."),To.forEach(o),ls=l($),I=a($,"DIV",{class:!0});var Y=d(I);u(eo.$$.fragment,Y),ps=l(Y),ie=a(Y,"P",{});var Xo=d(ie);hs=n(Xo,"The "),Ro=a(Xo,"A",{href:!0});var Di=d(Ro);ms=n(Di,"TFVisionEncoderDecoderModel"),Di.forEach(o),fs=n(Xo," forward method, overrides the "),Vt=a(Xo,"CODE",{});var Mi=d(Vt);us=n(Mi,"__call__"),Mi.forEach(o),gs=n(Xo," special method."),Xo.forEach(o),_s=l(Y),u(Te.$$.fragment,Y),vs=l(Y),$t=a(Y,"P",{});var Vi=d($t);bs=n(Vi,"Examples:"),Vi.forEach(o),Ts=l(Y),u(oo.$$.fragment,Y),Y.forEach(o),ys=l($),O=a($,"DIV",{class:!0});var Ee=d(O);u(to.$$.fragment,Ee),ws=l(Ee),qt=a(Ee,"P",{});var $i=d(qt);Es=n($i,`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),$i.forEach(o),ks=l(Ee),zt=a(Ee,"P",{});var qi=d(zt);xs=n(qi,"Example:"),qi.forEach(o),js=l(Ee),u(no.$$.fragment,Ee),Ee.forEach(o),$.forEach(o),nn=l(s),ce=a(s,"H2",{class:!0});var wn=d(ce);ye=a(wn,"A",{id:!0,class:!0,href:!0});var zi=d(ye);Pt=a(zi,"SPAN",{});var Pi=d(Pt);u(ro.$$.fragment,Pi),Pi.forEach(o),zi.forEach(o),Ds=l(wn),Ft=a(wn,"SPAN",{});var Fi=d(Ft);Ms=n(Fi,"FlaxVisionEncoderDecoderModel"),Fi.forEach(o),wn.forEach(o),rn=l(s),k=a(s,"DIV",{class:!0});var q=d(k);u(ao.$$.fragment,q),Vs=l(q),le=a(q,"P",{});var et=d(le);$s=n(et,`This class can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model
as the encoder and any pretrained text autoregressive model as the decoder. The encoder is loaded via
`),Ct=a(et,"CODE",{});var Ci=d(Ct);qs=n(Ci,"from_pretrained()"),Ci.forEach(o),zs=n(et,"function and the decoder is loaded via "),At=a(et,"CODE",{});var Ai=d(At);Ps=n(Ai,"from_pretrained()"),Ai.forEach(o),Fs=n(et,`function. Cross-attention layers are automatically added to the decoder and should be fine-tuned on a downstream
generative task, like image captioning.`),et.forEach(o),Cs=l(q),so=a(q,"P",{});var En=d(so);As=n(En,`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),io=a(En,"A",{href:!0,rel:!0});var Si=d(io);Ss=n(Si,`Leveraging Pre-trained Checkpoints for Sequence Generation
Tasks`),Si.forEach(o),Is=n(En,` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),En.forEach(o),Ls=l(q),co=a(q,"P",{});var kn=d(co);Ns=n(kn,"Additionally, in "),lo=a(kn,"A",{href:!0,rel:!0});var Ii=d(lo);Os=n(Ii,`TrOCR: Transformer-based Optical Character Recognition with Pre-trained
Models`),Ii.forEach(o),Rs=n(kn,` it is shown how leveraging large pretrained vision models for optical
character recognition (OCR) yields a significant performance improvement.`),kn.forEach(o),Bs=l(q),St=a(q,"P",{});var Li=d(St);Gs=n(Li,`After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned, it can be saved/loaded just like any
other models (see the examples for more information).`),Li.forEach(o),Ws=l(q),po=a(q,"P",{});var xn=d(po);Us=n(xn,"This model inherits from "),Bo=a(xn,"A",{href:!0});var Ni=d(Bo);Hs=n(Ni,"FlaxPreTrainedModel"),Ni.forEach(o),Zs=n(xn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),xn.forEach(o),Js=l(q),ho=a(q,"P",{});var jn=d(ho);Ys=n(jn,`This model is also a Flax Linen
`),mo=a(jn,"A",{href:!0,rel:!0});var Oi=d(mo);Ks=n(Oi,"flax.nn.Module"),Oi.forEach(o),Qs=n(jn,` subclass. Use it as a
regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.`),jn.forEach(o),Xs=l(q),H=a(q,"P",{});var yo=d(H);Go=a(yo,"A",{href:!0});var Ri=d(Go);ed=n(Ri,"FlaxVisionEncoderDecoderModel"),Ri.forEach(o),od=n(yo,` is a generic model class that will be instantiated as a transformer architecture
with the module (flax.nn.Module) of one of the base vision model classes of the library as encoder module and
another one as decoder module when created with the :meth`),It=a(yo,"EM",{});var Bi=d(It);td=n(Bi,"~transformers.FlaxAutoModel.from_pretrained"),Bi.forEach(o),nd=n(yo,` class method
for the encoder and :meth`),Lt=a(yo,"EM",{});var Gi=d(Lt);rd=n(Gi,"~transformers.FlaxAutoModelForCausalLM.from_pretrained"),Gi.forEach(o),ad=n(yo," class method for the decoder."),yo.forEach(o),sd=l(q),L=a(q,"DIV",{class:!0});var K=d(L);u(fo.$$.fragment,K),dd=l(K),pe=a(K,"P",{});var ot=d(pe);id=n(ot,"The "),Wo=a(ot,"A",{href:!0});var Wi=d(Wo);cd=n(Wi,"FlaxVisionEncoderDecoderModel"),Wi.forEach(o),ld=n(ot," forward method, overrides the "),Nt=a(ot,"CODE",{});var Ui=d(Nt);pd=n(Ui,"__call__"),Ui.forEach(o),hd=n(ot," special method."),ot.forEach(o),md=l(K),u(we.$$.fragment,K),fd=l(K),Ot=a(K,"P",{});var Hi=d(Ot);ud=n(Hi,"Examples:"),Hi.forEach(o),gd=l(K),u(uo.$$.fragment,K),K.forEach(o),_d=l(q),R=a(q,"DIV",{class:!0});var ke=d(R);u(go.$$.fragment,ke),vd=l(ke),Rt=a(ke,"P",{});var Zi=d(Rt);bd=n(Zi,`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),Zi.forEach(o),Td=l(ke),Bt=a(ke,"P",{});var Ji=d(Bt);yd=n(Ji,"Example:"),Ji.forEach(o),wd=l(ke),u(_o.$$.fragment,ke),ke.forEach(o),q.forEach(o),this.h()},h(){i(p,"name","hf:doc:metadata"),i(p,"content",JSON.stringify(rc)),i(D,"id","vision-encoder-decoder-models"),i(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(D,"href","#vision-encoder-decoder-models"),i(m,"class","relative group"),i(Eo,"href","/docs/transformers/v4.16.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),i(ko,"href","vit"),i(xo,"href","beit"),i(jo,"href","deit"),i(Do,"href","roberta"),i(Mo,"href","gpt2"),i(Vo,"href","bert"),i(xe,"href","https://arxiv.org/abs/2109.10282"),i(xe,"rel","nofollow"),i($o,"href","/docs/transformers/v4.16.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),i(qo,"href","trocr"),i(me,"id","transformers.VisionEncoderDecoderConfig"),i(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(me,"href","#transformers.VisionEncoderDecoderConfig"),i(X,"class","relative group"),i(zo,"href","/docs/transformers/v4.16.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"),i(Po,"href","/docs/transformers/v4.16.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),i(Fo,"href","/docs/transformers/v4.16.2/en/main_classes/configuration#transformers.PretrainedConfig"),i(Co,"href","/docs/transformers/v4.16.2/en/main_classes/configuration#transformers.PretrainedConfig"),i(Ao,"href","/docs/transformers/v4.16.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"),i(ue,"class","docstring"),i(ge,"class","docstring"),i(z,"class","docstring"),i(_e,"id","transformers.VisionEncoderDecoderModel"),i(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(_e,"href","#transformers.VisionEncoderDecoderModel"),i(te,"class","relative group"),i(Ce,"href","https://arxiv.org/abs/1907.12461"),i(Ce,"rel","nofollow"),i(Se,"href","https://arxiv.org/abs/2109.10282"),i(Se,"rel","nofollow"),i(So,"href","/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel"),i(Ne,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(Ne,"rel","nofollow"),i(Io,"href","/docs/transformers/v4.16.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),i(Lo,"href","/docs/transformers/v4.16.2/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),i(A,"class","docstring"),i(S,"class","docstring"),i(w,"class","docstring"),i(be,"id","transformers.TFVisionEncoderDecoderModel"),i(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(be,"href","#transformers.TFVisionEncoderDecoderModel"),i(se,"class","relative group"),i(Ze,"href","https://arxiv.org/abs/1907.12461"),i(Ze,"rel","nofollow"),i(Ye,"href","https://arxiv.org/abs/2109.10282"),i(Ye,"rel","nofollow"),i(No,"href","/docs/transformers/v4.16.2/en/main_classes/model#transformers.TFPreTrainedModel"),i(Xe,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),i(Xe,"rel","nofollow"),i(Oo,"href","/docs/transformers/v4.16.2/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel"),i(Ro,"href","/docs/transformers/v4.16.2/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel"),i(I,"class","docstring"),i(O,"class","docstring"),i(E,"class","docstring"),i(ye,"id","transformers.FlaxVisionEncoderDecoderModel"),i(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(ye,"href","#transformers.FlaxVisionEncoderDecoderModel"),i(ce,"class","relative group"),i(io,"href","https://arxiv.org/abs/1907.12461"),i(io,"rel","nofollow"),i(lo,"href","https://arxiv.org/abs/2109.10282"),i(lo,"rel","nofollow"),i(Bo,"href","/docs/transformers/v4.16.2/en/main_classes/model#transformers.FlaxPreTrainedModel"),i(mo,"href","https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html"),i(mo,"rel","nofollow"),i(Go,"href","/docs/transformers/v4.16.2/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel"),i(Wo,"href","/docs/transformers/v4.16.2/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel"),i(L,"class","docstring"),i(R,"class","docstring"),i(k,"class","docstring")},m(s,h){e(document.head,p),T(s,P,h),T(s,m,h),e(m,D),e(D,F),g(y,F,null),e(m,x),e(m,C),e(C,Dn),T(s,Zt,h),T(s,j,h),e(j,Mn),e(j,Eo),e(Eo,Vn),e(j,$n),e(j,tt),e(tt,qn),e(j,zn),e(j,ko),e(ko,Pn),e(j,Fn),e(j,xo),e(xo,Cn),e(j,An),e(j,jo),e(jo,Sn),e(j,In),e(j,nt),e(nt,Ln),e(j,Nn),e(j,Do),e(Do,On),e(j,Rn),e(j,Mo),e(Mo,Bn),e(j,Gn),e(j,Vo),e(Vo,Wn),e(j,Un),T(s,Jt,h),T(s,he,h),e(he,Hn),e(he,xe),e(xe,Zn),e(he,Jn),T(s,Yt,h),T(s,G,h),e(G,Yn),e(G,$o),e($o,Kn),e(G,Qn),e(G,qo),e(qo,Xn),e(G,er),T(s,Kt,h),T(s,X,h),e(X,me),e(me,rt),g(je,rt,null),e(X,or),e(X,at),e(at,tr),T(s,Qt,h),T(s,z,h),g(De,z,null),e(z,nr),e(z,fe),e(fe,zo),e(zo,rr),e(fe,ar),e(fe,Po),e(Po,sr),e(fe,dr),e(z,ir),e(z,ee),e(ee,cr),e(ee,Fo),e(Fo,lr),e(ee,pr),e(ee,Co),e(Co,hr),e(ee,mr),e(z,fr),e(z,st),e(st,ur),e(z,gr),g(Me,z,null),e(z,_r),e(z,ue),g(Ve,ue,null),e(ue,vr),e(ue,$e),e($e,br),e($e,Ao),e(Ao,Tr),e($e,yr),e(z,wr),e(z,ge),g(qe,ge,null),e(ge,Er),e(ge,oe),e(oe,kr),e(oe,dt),e(dt,xr),e(oe,jr),e(oe,it),e(it,Dr),e(oe,Mr),T(s,Xt,h),T(s,te,h),e(te,_e),e(_e,ct),g(ze,ct,null),e(te,Vr),e(te,lt),e(lt,$r),T(s,en,h),T(s,w,h),g(Pe,w,null),e(w,qr),e(w,ne),e(ne,zr),e(ne,pt),e(pt,Pr),e(ne,Fr),e(ne,ht),e(ht,Cr),e(ne,Ar),e(w,Sr),e(w,Fe),e(Fe,Ir),e(Fe,Ce),e(Ce,Lr),e(Fe,Nr),e(w,Or),e(w,Ae),e(Ae,Rr),e(Ae,Se),e(Se,Br),e(Ae,Gr),e(w,Wr),e(w,mt),e(mt,Ur),e(w,Hr),e(w,Ie),e(Ie,Zr),e(Ie,So),e(So,Jr),e(Ie,Yr),e(w,Kr),e(w,Le),e(Le,Qr),e(Le,Ne),e(Ne,Xr),e(Le,ea),e(w,oa),e(w,W),e(W,Io),e(Io,ta),e(W,na),e(W,ft),e(ft,ra),e(W,aa),e(W,ut),e(ut,sa),e(W,da),e(w,ia),e(w,A),g(Oe,A,null),e(A,ca),e(A,re),e(re,la),e(re,Lo),e(Lo,pa),e(re,ha),e(re,gt),e(gt,ma),e(re,fa),e(A,ua),g(ve,A,null),e(A,ga),e(A,_t),e(_t,_a),e(A,va),g(Re,A,null),e(w,ba),e(w,S),g(Be,S,null),e(S,Ta),e(S,vt),e(vt,ya),e(S,wa),e(S,ae),e(ae,Ea),e(ae,bt),e(bt,ka),e(ae,xa),e(ae,Tt),e(Tt,ja),e(ae,Da),e(S,Ma),e(S,yt),e(yt,Va),e(S,$a),g(Ge,S,null),T(s,on,h),T(s,se,h),e(se,be),e(be,wt),g(We,wt,null),e(se,qa),e(se,Et),e(Et,za),T(s,tn,h),T(s,E,h),g(Ue,E,null),e(E,Pa),e(E,de),e(de,Fa),e(de,kt),e(kt,Ca),e(de,Aa),e(de,xt),e(xt,Sa),e(de,Ia),e(E,La),e(E,He),e(He,Na),e(He,Ze),e(Ze,Oa),e(He,Ra),e(E,Ba),e(E,Je),e(Je,Ga),e(Je,Ye),e(Ye,Wa),e(Je,Ua),e(E,Ha),e(E,jt),e(jt,Za),e(E,Ja),e(E,Ke),e(Ke,Ya),e(Ke,No),e(No,Ka),e(Ke,Qa),e(E,Xa),e(E,Qe),e(Qe,es),e(Qe,Xe),e(Xe,os),e(Qe,ts),e(E,ns),e(E,U),e(U,Oo),e(Oo,rs),e(U,as),e(U,Dt),e(Dt,ss),e(U,ds),e(U,Mt),e(Mt,is),e(U,cs),e(E,ls),e(E,I),g(eo,I,null),e(I,ps),e(I,ie),e(ie,hs),e(ie,Ro),e(Ro,ms),e(ie,fs),e(ie,Vt),e(Vt,us),e(ie,gs),e(I,_s),g(Te,I,null),e(I,vs),e(I,$t),e($t,bs),e(I,Ts),g(oo,I,null),e(E,ys),e(E,O),g(to,O,null),e(O,ws),e(O,qt),e(qt,Es),e(O,ks),e(O,zt),e(zt,xs),e(O,js),g(no,O,null),T(s,nn,h),T(s,ce,h),e(ce,ye),e(ye,Pt),g(ro,Pt,null),e(ce,Ds),e(ce,Ft),e(Ft,Ms),T(s,rn,h),T(s,k,h),g(ao,k,null),e(k,Vs),e(k,le),e(le,$s),e(le,Ct),e(Ct,qs),e(le,zs),e(le,At),e(At,Ps),e(le,Fs),e(k,Cs),e(k,so),e(so,As),e(so,io),e(io,Ss),e(so,Is),e(k,Ls),e(k,co),e(co,Ns),e(co,lo),e(lo,Os),e(co,Rs),e(k,Bs),e(k,St),e(St,Gs),e(k,Ws),e(k,po),e(po,Us),e(po,Bo),e(Bo,Hs),e(po,Zs),e(k,Js),e(k,ho),e(ho,Ys),e(ho,mo),e(mo,Ks),e(ho,Qs),e(k,Xs),e(k,H),e(H,Go),e(Go,ed),e(H,od),e(H,It),e(It,td),e(H,nd),e(H,Lt),e(Lt,rd),e(H,ad),e(k,sd),e(k,L),g(fo,L,null),e(L,dd),e(L,pe),e(pe,id),e(pe,Wo),e(Wo,cd),e(pe,ld),e(pe,Nt),e(Nt,pd),e(pe,hd),e(L,md),g(we,L,null),e(L,fd),e(L,Ot),e(Ot,ud),e(L,gd),g(uo,L,null),e(k,_d),e(k,R),g(go,R,null),e(R,vd),e(R,Rt),e(Rt,bd),e(R,Td),e(R,Bt),e(Bt,yd),e(R,wd),g(_o,R,null),an=!0},p(s,[h]){const vo={};h&2&&(vo.$$scope={dirty:h,ctx:s}),ve.$set(vo);const Gt={};h&2&&(Gt.$$scope={dirty:h,ctx:s}),Te.$set(Gt);const Wt={};h&2&&(Wt.$$scope={dirty:h,ctx:s}),we.$set(Wt)},i(s){an||(_(y.$$.fragment,s),_(je.$$.fragment,s),_(De.$$.fragment,s),_(Me.$$.fragment,s),_(Ve.$$.fragment,s),_(qe.$$.fragment,s),_(ze.$$.fragment,s),_(Pe.$$.fragment,s),_(Oe.$$.fragment,s),_(ve.$$.fragment,s),_(Re.$$.fragment,s),_(Be.$$.fragment,s),_(Ge.$$.fragment,s),_(We.$$.fragment,s),_(Ue.$$.fragment,s),_(eo.$$.fragment,s),_(Te.$$.fragment,s),_(oo.$$.fragment,s),_(to.$$.fragment,s),_(no.$$.fragment,s),_(ro.$$.fragment,s),_(ao.$$.fragment,s),_(fo.$$.fragment,s),_(we.$$.fragment,s),_(uo.$$.fragment,s),_(go.$$.fragment,s),_(_o.$$.fragment,s),an=!0)},o(s){v(y.$$.fragment,s),v(je.$$.fragment,s),v(De.$$.fragment,s),v(Me.$$.fragment,s),v(Ve.$$.fragment,s),v(qe.$$.fragment,s),v(ze.$$.fragment,s),v(Pe.$$.fragment,s),v(Oe.$$.fragment,s),v(ve.$$.fragment,s),v(Re.$$.fragment,s),v(Be.$$.fragment,s),v(Ge.$$.fragment,s),v(We.$$.fragment,s),v(Ue.$$.fragment,s),v(eo.$$.fragment,s),v(Te.$$.fragment,s),v(oo.$$.fragment,s),v(to.$$.fragment,s),v(no.$$.fragment,s),v(ro.$$.fragment,s),v(ao.$$.fragment,s),v(fo.$$.fragment,s),v(we.$$.fragment,s),v(uo.$$.fragment,s),v(go.$$.fragment,s),v(_o.$$.fragment,s),an=!1},d(s){o(p),s&&o(P),s&&o(m),b(y),s&&o(Zt),s&&o(j),s&&o(Jt),s&&o(he),s&&o(Yt),s&&o(G),s&&o(Kt),s&&o(X),b(je),s&&o(Qt),s&&o(z),b(De),b(Me),b(Ve),b(qe),s&&o(Xt),s&&o(te),b(ze),s&&o(en),s&&o(w),b(Pe),b(Oe),b(ve),b(Re),b(Be),b(Ge),s&&o(on),s&&o(se),b(We),s&&o(tn),s&&o(E),b(Ue),b(eo),b(Te),b(oo),b(to),b(no),s&&o(nn),s&&o(ce),b(ro),s&&o(rn),s&&o(k),b(ao),b(fo),b(we),b(uo),b(go),b(_o)}}}const rc={local:"vision-encoder-decoder-models",sections:[{local:"transformers.VisionEncoderDecoderConfig",title:"VisionEncoderDecoderConfig"},{local:"transformers.VisionEncoderDecoderModel",title:"VisionEncoderDecoderModel"},{local:"transformers.TFVisionEncoderDecoderModel",title:"TFVisionEncoderDecoderModel"},{local:"transformers.FlaxVisionEncoderDecoderModel",title:"FlaxVisionEncoderDecoderModel"}],title:"Vision Encoder Decoder Models"};function ac(Q,p,P){let{fw:m}=p;return Q.$$set=D=>{"fw"in D&&P(0,m=D.fw)},[m]}class hc extends Yi{constructor(p){super();Ki(this,p,ac,nc,Qi,{fw:0})}}export{hc as default,rc as metadata};
