import{S as Ja,i as Ka,s as Qa,e as o,k as h,w as u,t as c,M as Ra,c as l,d as t,m as f,a as i,x as d,h as p,b as n,N as Va,F as a,g as s,y as m,L as Xa,q as v,o as g,B as w}from"../chunks/vendor-ab4e3193.js";import{I as le}from"../chunks/IconCopyLink-d992940d.js";import{C as ie}from"../chunks/CodeBlock-516df0c5.js";import"../chunks/CopyButton-204b56db.js";function Za(yt){let _,se,$,y,ge,B,_t,we,bt,Ge,x,kt,F,Et,At,Ie,E,z,$e,q,Pt,ye,jt,Ue,ne,Tt,Be,L,Fe,b,St,M,_e,xt,zt,be,Nt,Ct,qe,W,Le,A,N,ke,Y,Dt,Ee,Ht,Me,C,Ot,J,Ae,Gt,It,We,K,Ye,P,D,Pe,Q,Ut,je,Bt,Je,k,Ft,Te,qt,Lt,R,Se,Mt,Wt,Ke,V,Qe,ce,Yt,Re,pe,he,na,Ve,j,H,xe,X,Jt,ze,Kt,Xe,fe,Qt,Ze,T,O,Ne,Z,Rt,Ce,Vt,et,ue,Xt,tt,ee,at,de,Zt,rt,te,ot,S,G,De,ae,ea,He,ta,lt,I,aa,Oe,ra,oa,it,re,st,U,la,oe,ia,sa,nt;return B=new le({}),q=new le({}),L=new ie({props:{code:"pip install accelerate,",highlighted:"pip install accelerate"}}),W=new ie({props:{code:`from accelerate import Accelerator

accelerator = Accelerator(),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator

<span class="hljs-meta">&gt;&gt;&gt; </span>accelerator = Accelerator()`}}),Y=new le({}),K=new ie({props:{code:`train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
<span class="hljs-meta">... </span>    train_dataloader, eval_dataloader, model, optimizer
<span class="hljs-meta">... </span>)`}}),Q=new le({}),V=new ie({props:{code:`for epoch in range(num_epochs):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:
<span class="hljs-meta">... </span>        outputs = model(**batch)
<span class="hljs-meta">... </span>        loss = outputs.loss
<span class="hljs-meta">... </span>        accelerator.backward(loss)

<span class="hljs-meta">... </span>        optimizer.step()
<span class="hljs-meta">... </span>        lr_scheduler.step()
<span class="hljs-meta">... </span>        optimizer.zero_grad()
<span class="hljs-meta">... </span>        progress_bar.update(<span class="hljs-number">1</span>)`}}),X=new le({}),Z=new le({}),ee=new ie({props:{code:"accelerate config,",highlighted:"accelerate config"}}),te=new ie({props:{code:"accelerate launch train.py,",highlighted:"accelerate launch train.py"}}),ae=new le({}),re=new ie({props:{code:`from accelerate import notebook_launcher

notebook_launcher(training_function),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> notebook_launcher

<span class="hljs-meta">&gt;&gt;&gt; </span>notebook_launcher(training_function)`}}),{c(){_=o("meta"),se=h(),$=o("h1"),y=o("a"),ge=o("span"),u(B.$$.fragment),_t=h(),we=o("span"),bt=c("Distributed training with \u{1F917} Accelerate"),Ge=h(),x=o("p"),kt=c("As models get bigger, parallelism has emerged as a strategy for training larger models on limited hardware and accelerating training speed by several orders of magnitude. At Hugging Face, we created the "),F=o("a"),Et=c("\u{1F917} Accelerate"),At=c(" library to help users easily train a \u{1F917} Transformers model on any type of distributed setup, whether it is multiple GPU\u2019s on one machine or multiple GPU\u2019s across several machines. In this tutorial, learn how to customize your native PyTorch training loop to enable training in a distributed environment."),Ie=h(),E=o("h2"),z=o("a"),$e=o("span"),u(q.$$.fragment),Pt=h(),ye=o("span"),jt=c("Setup"),Ue=h(),ne=o("p"),Tt=c("Get started by installing \u{1F917} Accelerate:"),Be=h(),u(L.$$.fragment),Fe=h(),b=o("p"),St=c("Then import and create an "),M=o("a"),_e=o("code"),xt=c("Accelerator"),zt=c(" object. "),be=o("code"),Nt=c("Accelerator"),Ct=c(" will automatically detect your type of distributed setup and initialize all the necessary components for training. You don\u2019t need to explicitly place your model on a device."),qe=h(),u(W.$$.fragment),Le=h(),A=o("h2"),N=o("a"),ke=o("span"),u(Y.$$.fragment),Dt=h(),Ee=o("span"),Ht=c("Prepare to accelerate"),Me=h(),C=o("p"),Ot=c("The next step is to pass all the relevant training objects to "),J=o("a"),Ae=o("code"),Gt=c("prepare"),It=c(". This includes your training and evaluation DataLoaders, a model and an optimizer:"),We=h(),u(K.$$.fragment),Ye=h(),P=o("h2"),D=o("a"),Pe=o("span"),u(Q.$$.fragment),Ut=h(),je=o("span"),Bt=c("Backward"),Je=h(),k=o("p"),Ft=c("The last addition is to replace the typical "),Te=o("code"),qt=c("loss.backward()"),Lt=c(" in your training loop with \u{1F917} Accelerate\u2019s "),R=o("a"),Se=o("code"),Mt=c("backward"),Wt=c(":"),Ke=h(),u(V.$$.fragment),Qe=h(),ce=o("p"),Yt=c("As you can see in the following image, you only need to add four additional lines of code to your training loop to enable distributed training!"),Re=h(),pe=o("p"),he=o("img"),Ve=h(),j=o("h2"),H=o("a"),xe=o("span"),u(X.$$.fragment),Jt=h(),ze=o("span"),Kt=c("Train"),Xe=h(),fe=o("p"),Qt=c("Once you\u2019ve added the relevant lines of code, launch your training in a script or a notebook like Colaboratory."),Ze=h(),T=o("h3"),O=o("a"),Ne=o("span"),u(Z.$$.fragment),Rt=h(),Ce=o("span"),Vt=c("Train with a script"),et=h(),ue=o("p"),Xt=c("If you are running your training from a script, run the following command to create and save a configuration file:"),tt=h(),u(ee.$$.fragment),at=h(),de=o("p"),Zt=c("Then launch your training with:"),rt=h(),u(te.$$.fragment),ot=h(),S=o("h3"),G=o("a"),De=o("span"),u(ae.$$.fragment),ea=h(),He=o("span"),ta=c("Train with a notebook"),lt=h(),I=o("p"),aa=c("\u{1F917} Accelerate can also run in a notebook if you\u2019re planning on using Colaboratory\u2019s TPUs. Wrap all the code responsible for training in a function, and pass it to "),Oe=o("code"),ra=c("notebook_launcher"),oa=c(":"),it=h(),u(re.$$.fragment),st=h(),U=o("p"),la=c("For more information about \u{1F917} Accelerate and it\u2019s rich features, refer to the "),oe=o("a"),ia=c("documentation"),sa=c("."),this.h()},l(e){const r=Ra('[data-svelte="svelte-1phssyn"]',document.head);_=l(r,"META",{name:!0,content:!0}),r.forEach(t),se=f(e),$=l(e,"H1",{class:!0});var ct=i($);y=l(ct,"A",{id:!0,class:!0,href:!0});var ca=i(y);ge=l(ca,"SPAN",{});var pa=i(ge);d(B.$$.fragment,pa),pa.forEach(t),ca.forEach(t),_t=f(ct),we=l(ct,"SPAN",{});var ha=i(we);bt=p(ha,"Distributed training with \u{1F917} Accelerate"),ha.forEach(t),ct.forEach(t),Ge=f(e),x=l(e,"P",{});var pt=i(x);kt=p(pt,"As models get bigger, parallelism has emerged as a strategy for training larger models on limited hardware and accelerating training speed by several orders of magnitude. At Hugging Face, we created the "),F=l(pt,"A",{href:!0,rel:!0});var fa=i(F);Et=p(fa,"\u{1F917} Accelerate"),fa.forEach(t),At=p(pt," library to help users easily train a \u{1F917} Transformers model on any type of distributed setup, whether it is multiple GPU\u2019s on one machine or multiple GPU\u2019s across several machines. In this tutorial, learn how to customize your native PyTorch training loop to enable training in a distributed environment."),pt.forEach(t),Ie=f(e),E=l(e,"H2",{class:!0});var ht=i(E);z=l(ht,"A",{id:!0,class:!0,href:!0});var ua=i(z);$e=l(ua,"SPAN",{});var da=i($e);d(q.$$.fragment,da),da.forEach(t),ua.forEach(t),Pt=f(ht),ye=l(ht,"SPAN",{});var ma=i(ye);jt=p(ma,"Setup"),ma.forEach(t),ht.forEach(t),Ue=f(e),ne=l(e,"P",{});var va=i(ne);Tt=p(va,"Get started by installing \u{1F917} Accelerate:"),va.forEach(t),Be=f(e),d(L.$$.fragment,e),Fe=f(e),b=l(e,"P",{});var me=i(b);St=p(me,"Then import and create an "),M=l(me,"A",{href:!0,rel:!0});var ga=i(M);_e=l(ga,"CODE",{});var wa=i(_e);xt=p(wa,"Accelerator"),wa.forEach(t),ga.forEach(t),zt=p(me," object. "),be=l(me,"CODE",{});var $a=i(be);Nt=p($a,"Accelerator"),$a.forEach(t),Ct=p(me," will automatically detect your type of distributed setup and initialize all the necessary components for training. You don\u2019t need to explicitly place your model on a device."),me.forEach(t),qe=f(e),d(W.$$.fragment,e),Le=f(e),A=l(e,"H2",{class:!0});var ft=i(A);N=l(ft,"A",{id:!0,class:!0,href:!0});var ya=i(N);ke=l(ya,"SPAN",{});var _a=i(ke);d(Y.$$.fragment,_a),_a.forEach(t),ya.forEach(t),Dt=f(ft),Ee=l(ft,"SPAN",{});var ba=i(Ee);Ht=p(ba,"Prepare to accelerate"),ba.forEach(t),ft.forEach(t),Me=f(e),C=l(e,"P",{});var ut=i(C);Ot=p(ut,"The next step is to pass all the relevant training objects to "),J=l(ut,"A",{href:!0,rel:!0});var ka=i(J);Ae=l(ka,"CODE",{});var Ea=i(Ae);Gt=p(Ea,"prepare"),Ea.forEach(t),ka.forEach(t),It=p(ut,". This includes your training and evaluation DataLoaders, a model and an optimizer:"),ut.forEach(t),We=f(e),d(K.$$.fragment,e),Ye=f(e),P=l(e,"H2",{class:!0});var dt=i(P);D=l(dt,"A",{id:!0,class:!0,href:!0});var Aa=i(D);Pe=l(Aa,"SPAN",{});var Pa=i(Pe);d(Q.$$.fragment,Pa),Pa.forEach(t),Aa.forEach(t),Ut=f(dt),je=l(dt,"SPAN",{});var ja=i(je);Bt=p(ja,"Backward"),ja.forEach(t),dt.forEach(t),Je=f(e),k=l(e,"P",{});var ve=i(k);Ft=p(ve,"The last addition is to replace the typical "),Te=l(ve,"CODE",{});var Ta=i(Te);qt=p(Ta,"loss.backward()"),Ta.forEach(t),Lt=p(ve," in your training loop with \u{1F917} Accelerate\u2019s "),R=l(ve,"A",{href:!0,rel:!0});var Sa=i(R);Se=l(Sa,"CODE",{});var xa=i(Se);Mt=p(xa,"backward"),xa.forEach(t),Sa.forEach(t),Wt=p(ve,":"),ve.forEach(t),Ke=f(e),d(V.$$.fragment,e),Qe=f(e),ce=l(e,"P",{});var za=i(ce);Yt=p(za,"As you can see in the following image, you only need to add four additional lines of code to your training loop to enable distributed training!"),za.forEach(t),Re=f(e),pe=l(e,"P",{});var Na=i(pe);he=l(Na,"IMG",{src:!0,alt:!0}),Na.forEach(t),Ve=f(e),j=l(e,"H2",{class:!0});var mt=i(j);H=l(mt,"A",{id:!0,class:!0,href:!0});var Ca=i(H);xe=l(Ca,"SPAN",{});var Da=i(xe);d(X.$$.fragment,Da),Da.forEach(t),Ca.forEach(t),Jt=f(mt),ze=l(mt,"SPAN",{});var Ha=i(ze);Kt=p(Ha,"Train"),Ha.forEach(t),mt.forEach(t),Xe=f(e),fe=l(e,"P",{});var Oa=i(fe);Qt=p(Oa,"Once you\u2019ve added the relevant lines of code, launch your training in a script or a notebook like Colaboratory."),Oa.forEach(t),Ze=f(e),T=l(e,"H3",{class:!0});var vt=i(T);O=l(vt,"A",{id:!0,class:!0,href:!0});var Ga=i(O);Ne=l(Ga,"SPAN",{});var Ia=i(Ne);d(Z.$$.fragment,Ia),Ia.forEach(t),Ga.forEach(t),Rt=f(vt),Ce=l(vt,"SPAN",{});var Ua=i(Ce);Vt=p(Ua,"Train with a script"),Ua.forEach(t),vt.forEach(t),et=f(e),ue=l(e,"P",{});var Ba=i(ue);Xt=p(Ba,"If you are running your training from a script, run the following command to create and save a configuration file:"),Ba.forEach(t),tt=f(e),d(ee.$$.fragment,e),at=f(e),de=l(e,"P",{});var Fa=i(de);Zt=p(Fa,"Then launch your training with:"),Fa.forEach(t),rt=f(e),d(te.$$.fragment,e),ot=f(e),S=l(e,"H3",{class:!0});var gt=i(S);G=l(gt,"A",{id:!0,class:!0,href:!0});var qa=i(G);De=l(qa,"SPAN",{});var La=i(De);d(ae.$$.fragment,La),La.forEach(t),qa.forEach(t),ea=f(gt),He=l(gt,"SPAN",{});var Ma=i(He);ta=p(Ma,"Train with a notebook"),Ma.forEach(t),gt.forEach(t),lt=f(e),I=l(e,"P",{});var wt=i(I);aa=p(wt,"\u{1F917} Accelerate can also run in a notebook if you\u2019re planning on using Colaboratory\u2019s TPUs. Wrap all the code responsible for training in a function, and pass it to "),Oe=l(wt,"CODE",{});var Wa=i(Oe);ra=p(Wa,"notebook_launcher"),Wa.forEach(t),oa=p(wt,":"),wt.forEach(t),it=f(e),d(re.$$.fragment,e),st=f(e),U=l(e,"P",{});var $t=i(U);la=p($t,"For more information about \u{1F917} Accelerate and it\u2019s rich features, refer to the "),oe=l($t,"A",{href:!0,rel:!0});var Ya=i(oe);ia=p(Ya,"documentation"),Ya.forEach(t),sa=p($t,"."),$t.forEach(t),this.h()},h(){n(_,"name","hf:doc:metadata"),n(_,"content",JSON.stringify(er)),n(y,"id","distributed-training-with-accelerate"),n(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(y,"href","#distributed-training-with-accelerate"),n($,"class","relative group"),n(F,"href","https://huggingface.co/docs/accelerate/index.html"),n(F,"rel","nofollow"),n(z,"id","setup"),n(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(z,"href","#setup"),n(E,"class","relative group"),n(M,"href","https://huggingface.co/docs/accelerate/accelerator.html#accelerate.Accelerator"),n(M,"rel","nofollow"),n(N,"id","prepare-to-accelerate"),n(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(N,"href","#prepare-to-accelerate"),n(A,"class","relative group"),n(J,"href","https://huggingface.co/docs/accelerate/accelerator.html#accelerate.Accelerator.prepare"),n(J,"rel","nofollow"),n(D,"id","backward"),n(D,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(D,"href","#backward"),n(P,"class","relative group"),n(R,"href","https://huggingface.co/docs/accelerate/accelerator.html#accelerate.Accelerator.backward"),n(R,"rel","nofollow"),Va(he.src,na="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/accelerate.png")||n(he,"src",na),n(he,"alt","accelerate"),n(H,"id","train"),n(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(H,"href","#train"),n(j,"class","relative group"),n(O,"id","train-with-a-script"),n(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(O,"href","#train-with-a-script"),n(T,"class","relative group"),n(G,"id","train-with-a-notebook"),n(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(G,"href","#train-with-a-notebook"),n(S,"class","relative group"),n(oe,"href","https://huggingface.co/docs/accelerate/index.html"),n(oe,"rel","nofollow")},m(e,r){a(document.head,_),s(e,se,r),s(e,$,r),a($,y),a(y,ge),m(B,ge,null),a($,_t),a($,we),a(we,bt),s(e,Ge,r),s(e,x,r),a(x,kt),a(x,F),a(F,Et),a(x,At),s(e,Ie,r),s(e,E,r),a(E,z),a(z,$e),m(q,$e,null),a(E,Pt),a(E,ye),a(ye,jt),s(e,Ue,r),s(e,ne,r),a(ne,Tt),s(e,Be,r),m(L,e,r),s(e,Fe,r),s(e,b,r),a(b,St),a(b,M),a(M,_e),a(_e,xt),a(b,zt),a(b,be),a(be,Nt),a(b,Ct),s(e,qe,r),m(W,e,r),s(e,Le,r),s(e,A,r),a(A,N),a(N,ke),m(Y,ke,null),a(A,Dt),a(A,Ee),a(Ee,Ht),s(e,Me,r),s(e,C,r),a(C,Ot),a(C,J),a(J,Ae),a(Ae,Gt),a(C,It),s(e,We,r),m(K,e,r),s(e,Ye,r),s(e,P,r),a(P,D),a(D,Pe),m(Q,Pe,null),a(P,Ut),a(P,je),a(je,Bt),s(e,Je,r),s(e,k,r),a(k,Ft),a(k,Te),a(Te,qt),a(k,Lt),a(k,R),a(R,Se),a(Se,Mt),a(k,Wt),s(e,Ke,r),m(V,e,r),s(e,Qe,r),s(e,ce,r),a(ce,Yt),s(e,Re,r),s(e,pe,r),a(pe,he),s(e,Ve,r),s(e,j,r),a(j,H),a(H,xe),m(X,xe,null),a(j,Jt),a(j,ze),a(ze,Kt),s(e,Xe,r),s(e,fe,r),a(fe,Qt),s(e,Ze,r),s(e,T,r),a(T,O),a(O,Ne),m(Z,Ne,null),a(T,Rt),a(T,Ce),a(Ce,Vt),s(e,et,r),s(e,ue,r),a(ue,Xt),s(e,tt,r),m(ee,e,r),s(e,at,r),s(e,de,r),a(de,Zt),s(e,rt,r),m(te,e,r),s(e,ot,r),s(e,S,r),a(S,G),a(G,De),m(ae,De,null),a(S,ea),a(S,He),a(He,ta),s(e,lt,r),s(e,I,r),a(I,aa),a(I,Oe),a(Oe,ra),a(I,oa),s(e,it,r),m(re,e,r),s(e,st,r),s(e,U,r),a(U,la),a(U,oe),a(oe,ia),a(U,sa),nt=!0},p:Xa,i(e){nt||(v(B.$$.fragment,e),v(q.$$.fragment,e),v(L.$$.fragment,e),v(W.$$.fragment,e),v(Y.$$.fragment,e),v(K.$$.fragment,e),v(Q.$$.fragment,e),v(V.$$.fragment,e),v(X.$$.fragment,e),v(Z.$$.fragment,e),v(ee.$$.fragment,e),v(te.$$.fragment,e),v(ae.$$.fragment,e),v(re.$$.fragment,e),nt=!0)},o(e){g(B.$$.fragment,e),g(q.$$.fragment,e),g(L.$$.fragment,e),g(W.$$.fragment,e),g(Y.$$.fragment,e),g(K.$$.fragment,e),g(Q.$$.fragment,e),g(V.$$.fragment,e),g(X.$$.fragment,e),g(Z.$$.fragment,e),g(ee.$$.fragment,e),g(te.$$.fragment,e),g(ae.$$.fragment,e),g(re.$$.fragment,e),nt=!1},d(e){t(_),e&&t(se),e&&t($),w(B),e&&t(Ge),e&&t(x),e&&t(Ie),e&&t(E),w(q),e&&t(Ue),e&&t(ne),e&&t(Be),w(L,e),e&&t(Fe),e&&t(b),e&&t(qe),w(W,e),e&&t(Le),e&&t(A),w(Y),e&&t(Me),e&&t(C),e&&t(We),w(K,e),e&&t(Ye),e&&t(P),w(Q),e&&t(Je),e&&t(k),e&&t(Ke),w(V,e),e&&t(Qe),e&&t(ce),e&&t(Re),e&&t(pe),e&&t(Ve),e&&t(j),w(X),e&&t(Xe),e&&t(fe),e&&t(Ze),e&&t(T),w(Z),e&&t(et),e&&t(ue),e&&t(tt),w(ee,e),e&&t(at),e&&t(de),e&&t(rt),w(te,e),e&&t(ot),e&&t(S),w(ae),e&&t(lt),e&&t(I),e&&t(it),w(re,e),e&&t(st),e&&t(U)}}}const er={local:"distributed-training-with-accelerate",sections:[{local:"setup",title:"Setup"},{local:"prepare-to-accelerate",title:"Prepare to accelerate"},{local:"backward",title:"Backward"},{local:"train",sections:[{local:"train-with-a-script",title:"Train with a script"},{local:"train-with-a-notebook",title:"Train with a notebook"}],title:"Train"}],title:"Distributed training with \u{1F917} Accelerate"};function tr(yt,_,se){let{fw:$}=_;return yt.$$set=y=>{"fw"in y&&se(0,$=y.fw)},[$]}class ir extends Ja{constructor(_){super();Ka(this,_,tr,Za,Qa,{fw:0})}}export{ir as default,er as metadata};
