import{S as Cm,i as Im,s as Dm,e as n,k as d,w as f,t as s,M as Nm,c as r,d as t,m as l,a,x as u,h as i,b as c,F as e,g as p,y as g,q as _,o as v,B as k}from"../../chunks/vendor-ab4e3193.js";import{T as _r}from"../../chunks/Tip-b5c6375a.js";import{D as q}from"../../chunks/Docstring-b69c0bd4.js";import{C as Ve}from"../../chunks/CodeBlock-516df0c5.js";import{I as J}from"../../chunks/IconCopyLink-d992940d.js";import"../../chunks/CopyButton-204b56db.js";function Om(N){let h,y,w,T,$;return{c(){h=n("p"),y=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),w=n("code"),T=s("Module"),$=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var b=a(h);y=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),w=r(b,"CODE",{});var z=a(w);T=i(z,"Module"),z.forEach(t),$=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(t)},m(R,b){p(R,h,b),e(h,y),e(h,w),e(w,T),e(h,$)},d(R){R&&t(h)}}}function Wm(N){let h,y,w,T,$;return{c(){h=n("p"),y=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),w=n("code"),T=s("Module"),$=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var b=a(h);y=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),w=r(b,"CODE",{});var z=a(w);T=i(z,"Module"),z.forEach(t),$=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(t)},m(R,b){p(R,h,b),e(h,y),e(h,w),e(w,T),e(h,$)},d(R){R&&t(h)}}}function Km(N){let h,y,w,T,$;return{c(){h=n("p"),y=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),w=n("code"),T=s("Module"),$=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var b=a(h);y=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),w=r(b,"CODE",{});var z=a(w);T=i(z,"Module"),z.forEach(t),$=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(t)},m(R,b){p(R,h,b),e(h,y),e(h,w),e(w,T),e(h,$)},d(R){R&&t(h)}}}function Bm(N){let h,y,w,T,$;return{c(){h=n("p"),y=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),w=n("code"),T=s("Module"),$=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var b=a(h);y=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),w=r(b,"CODE",{});var z=a(w);T=i(z,"Module"),z.forEach(t),$=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(t)},m(R,b){p(R,h,b),e(h,y),e(h,w),e(w,T),e(h,$)},d(R){R&&t(h)}}}function Qm(N){let h,y,w,T,$;return{c(){h=n("p"),y=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),w=n("code"),T=s("Module"),$=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var b=a(h);y=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),w=r(b,"CODE",{});var z=a(w);T=i(z,"Module"),z.forEach(t),$=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(t)},m(R,b){p(R,h,b),e(h,y),e(h,w),e(w,T),e(h,$)},d(R){R&&t(h)}}}function Hm(N){let h,y,w,T,$,R,b,z,va,vr,se,$e,Yo,Ue,ka,en,wa,kr,Ee,ba,Xe,Ra,Ta,wr,io,ya,br,lo,tn,$a,Rr,Z,Ea,Ge,za,qa,Je,xa,Aa,Tr,ie,ze,on,Ze,Pa,nn,ja,yr,x,Ye,La,rn,Ma,Fa,L,an,co,Sa,Ca,sn,mo,Ia,Da,dn,ho,Na,Oa,ln,po,Wa,Ka,cn,fo,Ba,Qa,mn,uo,Ha,Va,et,Ua,tt,Xa,Ga,Ja,de,Za,go,Ya,es,_o,ts,os,ns,hn,rs,as,ot,$r,le,qe,pn,nt,ss,fn,is,Er,E,rt,ds,un,ls,cs,xe,vo,ms,hs,ko,ps,fs,us,at,gs,wo,_s,vs,ks,Y,st,ws,gn,bs,Rs,it,bo,Ts,_n,ys,$s,Ro,Es,vn,zs,qs,Ae,dt,xs,lt,As,kn,Ps,js,Ls,O,ct,Ms,wn,Fs,Ss,mt,Cs,ce,Is,bn,Ds,Ns,Rn,Os,Ws,Ks,Tn,Bs,P,ht,Qs,pt,Hs,yn,Vs,Us,Xs,me,$n,Gs,Js,ft,Zs,En,Ys,ei,ti,ut,oi,zn,ni,ri,ai,gt,To,si,qn,ii,di,yo,li,xn,ci,mi,An,hi,pi,_t,zr,he,Pe,Pn,vt,fi,jn,ui,qr,M,kt,gi,wt,_i,Ln,vi,ki,wi,je,$o,bi,Ri,Eo,Ti,yi,$i,bt,Ei,zo,zi,qi,xi,j,Rt,Ai,Tt,Pi,Mn,ji,Li,Mi,pe,Fn,Fi,Si,yt,Ci,Sn,Ii,Di,Ni,$t,Oi,Cn,Wi,Ki,Bi,Et,qo,Qi,In,Hi,Vi,xo,Ui,Dn,Xi,Gi,Nn,Ji,Zi,zt,xr,fe,Le,On,qt,Yi,Wn,ed,Ar,Q,xt,td,Kn,od,nd,Me,At,rd,Bn,ad,Pr,ue,Fe,Qn,Pt,sd,Hn,id,jr,H,jt,dd,Lt,ld,Mt,cd,md,hd,F,Ft,pd,ge,fd,Ao,ud,gd,Vn,_d,vd,kd,Se,wd,Un,bd,Rd,St,Lr,_e,Ce,Xn,Ct,Td,Gn,yd,Mr,V,It,$d,Dt,Ed,Nt,zd,qd,xd,S,Ot,Ad,ve,Pd,Po,jd,Ld,Jn,Md,Fd,Sd,Ie,Cd,Zn,Id,Dd,Wt,Fr,ke,De,Yn,Kt,Nd,er,Od,Sr,U,Bt,Wd,Qt,Kd,Ht,Bd,Qd,Hd,C,Vt,Vd,we,Ud,jo,Xd,Gd,tr,Jd,Zd,Yd,Ne,el,or,tl,ol,Ut,Cr,be,Oe,nr,Xt,nl,rr,rl,Ir,X,Gt,al,Jt,sl,Zt,il,dl,ll,ee,Yt,cl,Re,ml,Lo,hl,pl,ar,fl,ul,gl,We,Dr,Te,Ke,sr,eo,_l,ir,vl,Nr,G,to,kl,Be,dr,wl,bl,oo,Rl,Tl,yl,I,no,$l,ye,El,Mo,zl,ql,lr,xl,Al,Pl,Qe,jl,cr,Ll,Ml,ro,Or;return R=new J({}),Ue=new J({}),Ze=new J({}),Ye=new q({props:{name:"class transformers.RealmConfig",anchor:"transformers.RealmConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"retriever_proj_size",val:" = 128"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"num_candidates",val:" = 8"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu_new'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"span_hidden_size",val:" = 256"},{name:"max_span_width",val:" = 10"},{name:"reader_layer_norm_eps",val:" = 0.001"},{name:"reader_beam_size",val:" = 5"},{name:"reader_seq_len",val:" = 320"},{name:"num_block_records",val:" = 13353718"},{name:"searcher_beam_size",val:" = 5000"},{name:"searcher_seq_len",val:" = 64"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/realm/configuration_realm.py#L36",parametersDescription:[{anchor:"transformers.RealmConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the REALM model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmEmbedder">RealmEmbedder</a>, <a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmScorer">RealmScorer</a>, <a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmKnowledgeAugEncoder">RealmKnowledgeAugEncoder</a>, or
<a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmReader">RealmReader</a>.`,name:"vocab_size"},{anchor:"transformers.RealmConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimension of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.RealmConfig.retriever_proj_size",description:`<strong>retriever_proj_size</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Dimension of the retriever(embedder) projection.`,name:"retriever_proj_size"},{anchor:"transformers.RealmConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.RealmConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.RealmConfig.num_candidates",description:`<strong>num_candidates</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of candidates inputted to the RealmScorer or RealmKnowledgeAugEncoder.`,name:"num_candidates"},{anchor:"transformers.RealmConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimension of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.RealmConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu_new&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.RealmConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.RealmConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.RealmConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.RealmConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmEmbedder">RealmEmbedder</a>, <a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmScorer">RealmScorer</a>,
<a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmKnowledgeAugEncoder">RealmKnowledgeAugEncoder</a>, or <a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmReader">RealmReader</a>.`,name:"type_vocab_size"},{anchor:"transformers.RealmConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.RealmConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.RealmConfig.span_hidden_size",description:`<strong>span_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimension of the reader&#x2019;s spans.`,name:"span_hidden_size"},{anchor:"transformers.RealmConfig.max_span_width",description:`<strong>max_span_width</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
Max span width of the reader.`,name:"max_span_width"},{anchor:"transformers.RealmConfig.reader_layer_norm_eps",description:`<strong>reader_layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-3) &#x2014;
The epsilon used by the reader&#x2019;s layer normalization layers.`,name:"reader_layer_norm_eps"},{anchor:"transformers.RealmConfig.reader_beam_size",description:`<strong>reader_beam_size</strong> (<code>int</code>, <em>optional</em>, defaults to 5) &#x2014;
Beam size of the reader.`,name:"reader_beam_size"},{anchor:"transformers.RealmConfig.reader_seq_len",description:`<strong>reader_seq_len</strong> (<code>int</code>, <em>optional</em>, defaults to 288+32) &#x2014;
Maximum sequence length of the reader.`,name:"reader_seq_len"},{anchor:"transformers.RealmConfig.num_block_records",description:`<strong>num_block_records</strong> (<code>int</code>, <em>optional</em>, defaults to 13353718) &#x2014;
Number of block records.`,name:"num_block_records"},{anchor:"transformers.RealmConfig.searcher_beam_size",description:`<strong>searcher_beam_size</strong> (<code>int</code>, <em>optional</em>, defaults to 5000) &#x2014;
Beam size of the searcher. Note that when eval mode is enabled, <em>searcher_beam_size</em> will be the same as
<em>reader_beam_size</em>.`,name:"searcher_beam_size"},{anchor:"transformers.RealmConfig.searcher_seq_len",description:`<strong>searcher_seq_len</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Maximum sequence length of the searcher.`,name:"searcher_seq_len"}]}}),ot=new Ve({props:{code:`from transformers import RealmEmbedder, RealmConfig

# Initializing a REALM realm-cc-news-pretrained-* style configuration
configuration = RealmConfig()

# Initializing a model from the google/realm-cc-news-pretrained-embedder style configuration
model = RealmEmbedder(configuration)

# Accessing the model configuration
configuration = model.config,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmEmbedder, RealmConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a REALM realm-cc-news-pretrained-* style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = RealmConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the google/realm-cc-news-pretrained-embedder style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmEmbedder(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),nt=new J({}),rt=new q({props:{name:"class transformers.RealmTokenizer",anchor:"transformers.RealmTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/realm/tokenization_realm.py#L88",parametersDescription:[{anchor:"transformers.RealmTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.RealmTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.RealmTokenizer.do_basic_tokenize",description:`<strong>do_basic_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to do basic tokenization before WordPiece.`,name:"do_basic_tokenize"},{anchor:"transformers.RealmTokenizer.never_split",description:`<strong>never_split</strong> (<code>Iterable</code>, <em>optional</em>) &#x2014;
Collection of tokens which will never be split during tokenization. Only has an effect when
<code>do_basic_tokenize=True</code>`,name:"never_split"},{anchor:"transformers.RealmTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.RealmTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.RealmTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.RealmTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.RealmTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.RealmTokenizer.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters.</p>
<p>This should likely be deactivated for Japanese (see this
<a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">issue</a>).
strip_accents &#x2014; (<code>bool</code>, <em>optional</em>):
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"tokenize_chinese_chars"}]}}),st=new q({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.RealmTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/realm/tokenization_realm.py#L295",parametersDescription:[{anchor:"transformers.RealmTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.RealmTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),dt=new q({props:{name:"get_special_tokens_mask",anchor:"transformers.RealmTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/realm/tokenization_realm.py#L320",parametersDescription:[{anchor:"transformers.RealmTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.RealmTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.RealmTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ct=new q({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.RealmTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/realm/tokenization_realm.py#L348",parametersDescription:[{anchor:"transformers.RealmTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.RealmTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),mt=new Ve({props:{code:`0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |,`,highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),ht=new q({props:{name:"batch_encode_candidates",anchor:"transformers.RealmTokenizer.batch_encode_candidates",parameters:[{name:"text",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/realm/tokenization_realm.py#L222",parametersDescription:[{anchor:"transformers.RealmTokenizer.batch_encode_candidates.text",description:`<strong>text</strong> (<code>List[List[str]]</code>) &#x2014;
The batch of sequences to be encoded. Each sequence must be in this format: (batch_size,
num_candidates, text).`,name:"text"},{anchor:"transformers.RealmTokenizer.batch_encode_candidates.text_pair",description:`<strong>text_pair</strong> (<code>List[List[str]]</code>, <em>optional</em>) &#x2014;
The batch of sequences to be encoded. Each sequence must be in this format: (batch_size,
num_candidates, text).
**kwargs &#x2014;
Keyword arguments of the <strong>call</strong> method.`,name:"text_pair"}],returnDescription:`
<p>Encoded text or text pair.</p>
`,returnType:`
<p><a
  href="/docs/transformers/v4.16.2/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a></p>
`}}),_t=new Ve({props:{code:`from transformers import RealmTokenizer

# batch_size = 2, num_candidates = 2
text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-encoder")
tokenized_text = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># batch_size = 2, num_candidates = 2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = [[<span class="hljs-string">&quot;Hello world!&quot;</span>, <span class="hljs-string">&quot;Nice to meet you!&quot;</span>], [<span class="hljs-string">&quot;The cute cat.&quot;</span>, <span class="hljs-string">&quot;The adorable dog.&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-encoder&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_text = tokenizer.batch_encode_candidates(text, max_length=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`}}),vt=new J({}),kt=new q({props:{name:"class transformers.RealmTokenizerFast",anchor:"transformers.RealmTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/realm/tokenization_realm_fast.py#L79",parametersDescription:[{anchor:"transformers.RealmTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.RealmTokenizerFast.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.RealmTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.RealmTokenizerFast.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.RealmTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.RealmTokenizerFast.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.RealmTokenizerFast.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.RealmTokenizerFast.clean_text",description:`<strong>clean_text</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to clean the text before tokenization by removing any control characters and replacing all
whitespaces by the classic one.`,name:"clean_text"},{anchor:"transformers.RealmTokenizerFast.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see <a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">this
issue</a>).`,name:"tokenize_chinese_chars"},{anchor:"transformers.RealmTokenizerFast.strip_accents",description:`<strong>strip_accents</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"strip_accents"},{anchor:"transformers.RealmTokenizerFast.wordpieces_prefix",description:`<strong>wordpieces_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;##&quot;</code>) &#x2014;
The prefix for subwords.`,name:"wordpieces_prefix"}]}}),Rt=new q({props:{name:"batch_encode_candidates",anchor:"transformers.RealmTokenizerFast.batch_encode_candidates",parameters:[{name:"text",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/realm/tokenization_realm_fast.py#L170",parametersDescription:[{anchor:"transformers.RealmTokenizerFast.batch_encode_candidates.text",description:`<strong>text</strong> (<code>List[List[str]]</code>) &#x2014;
The batch of sequences to be encoded. Each sequence must be in this format: (batch_size,
num_candidates, text).`,name:"text"},{anchor:"transformers.RealmTokenizerFast.batch_encode_candidates.text_pair",description:`<strong>text_pair</strong> (<code>List[List[str]]</code>, <em>optional</em>) &#x2014;
The batch of sequences to be encoded. Each sequence must be in this format: (batch_size,
num_candidates, text).
**kwargs &#x2014;
Keyword arguments of the <strong>call</strong> method.`,name:"text_pair"}],returnDescription:`
<p>Encoded text or text pair.</p>
`,returnType:`
<p><a
  href="/docs/transformers/v4.16.2/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a></p>
`}}),zt=new Ve({props:{code:`from transformers import RealmTokenizerFast

# batch_size = 2, num_candidates = 2
text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

tokenizer = RealmTokenizerFast.from_pretrained("google/realm-cc-news-pretrained-encoder")
tokenized_text = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># batch_size = 2, num_candidates = 2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = [[<span class="hljs-string">&quot;Hello world!&quot;</span>, <span class="hljs-string">&quot;Nice to meet you!&quot;</span>], [<span class="hljs-string">&quot;The cute cat.&quot;</span>, <span class="hljs-string">&quot;The adorable dog.&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizerFast.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-encoder&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_text = tokenizer.batch_encode_candidates(text, max_length=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`}}),qt=new J({}),xt=new q({props:{name:"class transformers.RealmRetriever",anchor:"transformers.RealmRetriever",parameters:[{name:"block_records",val:""},{name:"tokenizer",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/realm/retrieval_realm.py#L73",parametersDescription:[{anchor:"transformers.RealmRetriever.block_records",description:`<strong>block_records</strong> (<code>np.ndarray</code>) &#x2014;
A numpy array which cantains evidence texts.`,name:"block_records"},{anchor:"transformers.RealmRetriever.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>) &#x2014;
The tokenizer to encode retrieved texts.`,name:"tokenizer"}]}}),At=new q({props:{name:"block_has_answer",anchor:"transformers.RealmRetriever.block_has_answer",parameters:[{name:"concat_inputs",val:""},{name:"answer_ids",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/realm/retrieval_realm.py#L128"}}),Pt=new J({}),jt=new q({props:{name:"class transformers.RealmEmbedder",anchor:"transformers.RealmEmbedder",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/realm/modeling_realm.py#L1146",parametersDescription:[{anchor:"transformers.RealmEmbedder.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ft=new q({props:{name:"forward",anchor:"transformers.RealmEmbedder.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/realm/modeling_realm.py#L1160",parametersDescription:[{anchor:"transformers.RealmEmbedder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmEmbedder.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmEmbedder.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmEmbedder.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RealmEmbedder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RealmEmbedder.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RealmEmbedder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RealmEmbedder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RealmEmbedder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.16.2/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <code>transformers.models.realm.modeling_realm.RealmEmbedderOutput</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>projected_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.retriever_proj_size)</code>) \u2014 Projected score.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.realm.modeling_realm.RealmEmbedderOutput</code>or <code>tuple(torch.FloatTensor)</code></p>
`}}),Se=new _r({props:{$$slots:{default:[Om]},$$scope:{ctx:N}}}),St=new Ve({props:{code:`from transformers import RealmTokenizer, RealmEmbedder
import torch

tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-embedder")
model = RealmEmbedder.from_pretrained("google/realm-cc-news-pretrained-embedder")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

projected_score = outputs.projected_score,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizer, RealmEmbedder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-embedder&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmEmbedder.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-embedder&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>projected_score = outputs.projected_score`}}),Ct=new J({}),It=new q({props:{name:"class transformers.RealmScorer",anchor:"transformers.RealmScorer",parameters:[{name:"config",val:""},{name:"query_embedder",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/realm/modeling_realm.py#L1226",parametersDescription:[{anchor:"transformers.RealmScorer.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.RealmScorer.query_embedder",description:`<strong>query_embedder</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmEmbedder">RealmEmbedder</a>) &#x2014;
Embedder for input sequences. If not specified, it will use the same embedder as candidate sequences.`,name:"query_embedder"}]}}),Ot=new q({props:{name:"forward",anchor:"transformers.RealmScorer.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"candidate_input_ids",val:" = None"},{name:"candidate_attention_mask",val:" = None"},{name:"candidate_token_type_ids",val:" = None"},{name:"candidate_inputs_embeds",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/realm/modeling_realm.py#L1242",parametersDescription:[{anchor:"transformers.RealmScorer.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmScorer.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmScorer.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmScorer.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RealmScorer.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RealmScorer.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RealmScorer.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RealmScorer.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RealmScorer.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.16.2/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RealmScorer.forward.candidate_input_ids",description:`<strong>candidate_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>) &#x2014;
Indices of candidate input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"candidate_input_ids"},{anchor:"transformers.RealmScorer.forward.candidate_attention_mask",description:`<strong>candidate_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"candidate_attention_mask"},{anchor:"transformers.RealmScorer.forward.candidate_token_type_ids",description:`<strong>candidate_token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"candidate_token_type_ids"},{anchor:"transformers.RealmScorer.forward.candidate_inputs_embeds",description:`<strong>candidate_inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size * num_candidates, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>candidate_input_ids</code> you can choose to directly pass an embedded
representation. This is useful if you want more control over how to convert <em>candidate_input_ids</em> indices
into associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"candidate_inputs_embeds"}],returnDescription:`
<p>A <code>transformers.models.realm.modeling_realm.RealmScorerOutput</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li><strong>relevance_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_candidates)</code>) \u2014 The relevance score of document candidates (before softmax).</li>
<li><strong>query_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.retriever_proj_size)</code>) \u2014 Query score derived from the query embedder.</li>
<li><strong>candidate_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_candidates, config.retriever_proj_size)</code>) \u2014 Candidate score derived from the embedder.</li>
</ul>
`,returnType:`
<p><code>transformers.models.realm.modeling_realm.RealmScorerOutput</code>or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ie=new _r({props:{$$slots:{default:[Wm]},$$scope:{ctx:N}}}),Wt=new Ve({props:{code:`import torch
from transformers import RealmTokenizer, RealmScorer

tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-scorer")
model = RealmScorer.from_pretrained("google/realm-cc-news-pretrained-scorer", num_candidates=2)

# batch_size = 2, num_candidates = 2
input_texts = ["How are you?", "What is the item in the picture?"]
candidates_texts = [["Hello world!", "Nice to meet you!"], ["A cute cat.", "An adorable dog."]]

inputs = tokenizer(input_texts, return_tensors="pt")
candidates_inputs = tokenizer.batch_encode_candidates(candidates_texts, max_length=10, return_tensors="pt")

outputs = model(
    **inputs,
    candidate_input_ids=candidates_inputs.input_ids,
    candidate_attention_mask=candidates_inputs.attention_mask,
    candidate_token_type_ids=candidates_inputs.token_type_ids,
)
relevance_score = outputs.relevance_score,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizer, RealmScorer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-scorer&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmScorer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-scorer&quot;</span>, num_candidates=<span class="hljs-number">2</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># batch_size = 2, num_candidates = 2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_texts = [<span class="hljs-string">&quot;How are you?&quot;</span>, <span class="hljs-string">&quot;What is the item in the picture?&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>candidates_texts = [[<span class="hljs-string">&quot;Hello world!&quot;</span>, <span class="hljs-string">&quot;Nice to meet you!&quot;</span>], [<span class="hljs-string">&quot;A cute cat.&quot;</span>, <span class="hljs-string">&quot;An adorable dog.&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(input_texts, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>candidates_inputs = tokenizer.batch_encode_candidates(candidates_texts, max_length=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(
<span class="hljs-meta">... </span>    **inputs,
<span class="hljs-meta">... </span>    candidate_input_ids=candidates_inputs.input_ids,
<span class="hljs-meta">... </span>    candidate_attention_mask=candidates_inputs.attention_mask,
<span class="hljs-meta">... </span>    candidate_token_type_ids=candidates_inputs.token_type_ids,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_score = outputs.relevance_score`}}),Kt=new J({}),Bt=new q({props:{name:"class transformers.RealmKnowledgeAugEncoder",anchor:"transformers.RealmKnowledgeAugEncoder",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/realm/modeling_realm.py#L1373",parametersDescription:[{anchor:"transformers.RealmKnowledgeAugEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Vt=new q({props:{name:"forward",anchor:"transformers.RealmKnowledgeAugEncoder.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"relevance_score",val:" = None"},{name:"labels",val:" = None"},{name:"mlm_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/realm/modeling_realm.py#L1392",parametersDescription:[{anchor:"transformers.RealmKnowledgeAugEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_candidates, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.16.2/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.relevance_score",description:`<strong>relevance_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_candidates)</code>, <em>optional</em>) &#x2014;
Relevance score derived from RealmScorer, must be specified if you want to compute the masked language
modeling loss.`,name:"relevance_score"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.mlm_mask",description:`<strong>mlm_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid calculating joint loss on certain positions. If not specified, the loss will not be masked.
Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"mlm_mask"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.16.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.16.2/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ne=new _r({props:{$$slots:{default:[Km]},$$scope:{ctx:N}}}),Ut=new Ve({props:{code:`import torch
from transformers import RealmTokenizer, RealmKnowledgeAugEncoder

tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-encoder")
model = RealmKnowledgeAugEncoder.from_pretrained(
    "google/realm-cc-news-pretrained-encoder", num_candidates=2
)

# batch_size = 2, num_candidates = 2
text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

inputs = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizer, RealmKnowledgeAugEncoder

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-encoder&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmKnowledgeAugEncoder.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google/realm-cc-news-pretrained-encoder&quot;</span>, num_candidates=<span class="hljs-number">2</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># batch_size = 2, num_candidates = 2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = [[<span class="hljs-string">&quot;Hello world!&quot;</span>, <span class="hljs-string">&quot;Nice to meet you!&quot;</span>], [<span class="hljs-string">&quot;The cute cat.&quot;</span>, <span class="hljs-string">&quot;The adorable dog.&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer.batch_encode_candidates(text, max_length=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Xt=new J({}),Gt=new q({props:{name:"class transformers.RealmReader",anchor:"transformers.RealmReader",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/realm/modeling_realm.py#L1521",parametersDescription:[{anchor:"transformers.RealmReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Yt=new q({props:{name:"forward",anchor:"transformers.RealmReader.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"relevance_score",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"has_answers",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/realm/modeling_realm.py#L1535",parametersDescription:[{anchor:"transformers.RealmReader.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(reader_beam_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmReader.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(reader_beam_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmReader.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(reader_beam_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmReader.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(reader_beam_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RealmReader.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RealmReader.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(reader_beam_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RealmReader.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RealmReader.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RealmReader.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.16.2/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RealmReader.forward.relevance_score",description:`<strong>relevance_score</strong> (<code>torch.FloatTensor</code> of shape <code>(searcher_beam_size,)</code>, <em>optional</em>) &#x2014;
Relevance score, which must be specified if you want to compute the marginal log loss.`,name:"relevance_score"},{anchor:"transformers.RealmReader.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.RealmReader.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"},{anchor:"transformers.RealmReader.forward.has_answers",description:`<strong>has_answers</strong> (<code>torch.BoolTensor</code> of shape <code>(searcher_beam_size,)</code>, <em>optional</em>) &#x2014;
Whether or not the evidence block has answer(s).`,name:"has_answers"}],returnDescription:`
<p>A <code>transformers.models.realm.modeling_realm.RealmReaderOutput</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>start_positions</code>, <code>end_positions</code>, <code>has_answers</code> are provided) \u2014 Total loss.</p>
</li>
<li>
<p><strong>retriever_loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>start_positions</code>, <code>end_positions</code>, <code>has_answers</code> are provided) \u2014 Retriever loss.</p>
</li>
<li>
<p><strong>reader_loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>start_positions</code>, <code>end_positions</code>, <code>has_answers</code> are provided) \u2014 Reader loss.</p>
</li>
<li>
<p><strong>retriever_correct</strong> (<code>torch.BoolTensor</code> of shape <code>(config.searcher_beam_size,)</code>, <em>optional</em>) \u2014 Whether or not an evidence block contains answer.</p>
</li>
<li>
<p><strong>reader_correct</strong> (<code>torch.BoolTensor</code> of shape <code>(config.reader_beam_size, num_candidates)</code>, <em>optional</em>) \u2014 Whether or not a span candidate contains answer.</p>
</li>
<li>
<p><strong>block_idx</strong> (<code>torch.LongTensor</code> of shape <code>()</code>) \u2014 The index of the retrieved evidence block in which the predicted answer is most likely.</p>
</li>
<li>
<p><strong>candidate</strong> (<code>torch.LongTensor</code> of shape <code>()</code>) \u2014 The index of the retrieved span candidates in which the predicted answer is most likely.</p>
</li>
<li>
<p><strong>start_pos</strong> (<code>torch.IntTensor</code> of shape <code>()</code>) \u2014 Predicted answer starting position in <em>RealmReader</em>\u2019s inputs.</p>
</li>
<li>
<p><strong>end_pos:</strong> (<code>torch.IntTensor</code> of shape <code>()</code>) \u2014 Predicted answer ending position in <em>RealmReader</em>\u2019s inputs.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.realm.modeling_realm.RealmReaderOutput</code>or <code>tuple(torch.FloatTensor)</code></p>
`}}),We=new _r({props:{$$slots:{default:[Bm]},$$scope:{ctx:N}}}),eo=new J({}),to=new q({props:{name:"class transformers.RealmForOpenQA",anchor:"transformers.RealmForOpenQA",parameters:[{name:"config",val:""},{name:"retriever",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/realm/modeling_realm.py#L1722",parametersDescription:[{anchor:"transformers.RealmForOpenQA.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),no=new q({props:{name:"forward",anchor:"transformers.RealmForOpenQA.forward",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"answer_ids",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/realm/modeling_realm.py#L1745",parametersDescription:[{anchor:"transformers.RealmForOpenQA.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(1, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmForOpenQA.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(1, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmForOpenQA.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(1, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token (should not be used in this model by design).</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmForOpenQA.forward.answer_ids",description:`<strong>answer_ids</strong> (<code>list</code> of shape <code>(num_answers, answer_length)</code>, <em>optional</em>) &#x2014;
Answer ids for computing the marginal log-likelihood loss. Indices should be in <code>[-1, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-1</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"answer_ids"},{anchor:"transformers.RealmForOpenQA.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.16.2/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <code>transformers.models.realm.modeling_realm.RealmForOpenQAOutput</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li><strong>reader_output</strong> (<code>dict</code>) \u2014 Reader output.</li>
<li><strong>predicted_answer_ids</strong> (<code>torch.LongTensor</code> of shape <code>(answer_sequence_length)</code>) \u2014 Predicted answer ids.</li>
</ul>
`,returnType:`
<p><code>transformers.models.realm.modeling_realm.RealmForOpenQAOutput</code>or <code>tuple(torch.FloatTensor)</code></p>
`}}),Qe=new _r({props:{$$slots:{default:[Qm]},$$scope:{ctx:N}}}),ro=new Ve({props:{code:`import torch
from transformers import RealmForOpenQA, RealmRetriever, RealmTokenizer

retriever = RealmRetriever.from_pretrained("google/realm-orqa-nq-openqa")
tokenizer = RealmTokenizer.from_pretrained("google/realm-orqa-nq-openqa")
model = RealmForOpenQA.from_pretrained("google/realm-orqa-nq-openqa", retriever=retriever)

question = "Who is the pioneer in modern computer science?"
question_ids = tokenizer([question], return_tensors="pt")
answer_ids = tokenizer(
    ["alan mathison turing"],
    add_special_tokens=False,
    return_token_type_ids=False,
    return_attention_mask=False,
).input_ids

reader_output, predicted_answer_ids = model(**question_ids, answer_ids=answer_ids, return_dict=False)
predicted_answer = tokenizer.decode(predicted_answer_ids)
loss = reader_output.loss,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmForOpenQA, RealmRetriever, RealmTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>retriever = RealmRetriever.from_pretrained(<span class="hljs-string">&quot;google/realm-orqa-nq-openqa&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-orqa-nq-openqa&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmForOpenQA.from_pretrained(<span class="hljs-string">&quot;google/realm-orqa-nq-openqa&quot;</span>, retriever=retriever)

<span class="hljs-meta">&gt;&gt;&gt; </span>question = <span class="hljs-string">&quot;Who is the pioneer in modern computer science?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>question_ids = tokenizer([question], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>answer_ids = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;alan mathison turing&quot;</span>],
<span class="hljs-meta">... </span>    add_special_tokens=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    return_token_type_ids=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    return_attention_mask=<span class="hljs-literal">False</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>reader_output, predicted_answer_ids = model(**question_ids, answer_ids=answer_ids, return_dict=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_answer = tokenizer.decode(predicted_answer_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = reader_output.loss`}}),{c(){h=n("meta"),y=d(),w=n("h1"),T=n("a"),$=n("span"),f(R.$$.fragment),b=d(),z=n("span"),va=s("REALM"),vr=d(),se=n("h2"),$e=n("a"),Yo=n("span"),f(Ue.$$.fragment),ka=d(),en=n("span"),wa=s("Overview"),kr=d(),Ee=n("p"),ba=s("The REALM model was proposed in "),Xe=n("a"),Ra=s("REALM: Retrieval-Augmented Language Model Pre-Training"),Ta=s(` by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang. It\u2019s a
retrieval-augmented language model that firstly retrieves documents from a textual knowledge corpus and then
utilizes retrieved documents to process question answering tasks.`),wr=d(),io=n("p"),ya=s("The abstract from the paper is the following:"),br=d(),lo=n("p"),tn=n("em"),$a=s(`Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks
such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network,
requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we
augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend
over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the
first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language
modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We
demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the
challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both
explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous
methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as
interpretability and modularity.`),Rr=d(),Z=n("p"),Ea=s("This model was contributed by "),Ge=n("a"),za=s("qqaatw"),qa=s(`. The original code can be found
`),Je=n("a"),xa=s("here"),Aa=s("."),Tr=d(),ie=n("h2"),ze=n("a"),on=n("span"),f(Ze.$$.fragment),Pa=d(),nn=n("span"),ja=s("RealmConfig"),yr=d(),x=n("div"),f(Ye.$$.fragment),La=d(),rn=n("p"),Ma=s("This is the configuration class to store the configuration of"),Fa=d(),L=n("ol"),an=n("li"),co=n("a"),Sa=s("RealmEmbedder"),Ca=d(),sn=n("li"),mo=n("a"),Ia=s("RealmScorer"),Da=d(),dn=n("li"),ho=n("a"),Na=s("RealmKnowledgeAugEncoder"),Oa=d(),ln=n("li"),po=n("a"),Wa=s("RealmRetriever"),Ka=d(),cn=n("li"),fo=n("a"),Ba=s("RealmReader"),Qa=d(),mn=n("li"),uo=n("a"),Ha=s("RealmForOpenQA"),Va=d(),et=n("p"),Ua=s(`It is used to instantiate an REALM model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the REALM
`),tt=n("a"),Xa=s("realm-cc-news-pretrained"),Ga=s(" architecture."),Ja=d(),de=n("p"),Za=s("Configuration objects inherit from "),go=n("a"),Ya=s("PretrainedConfig"),es=s(` and can be used to control the model outputs. Read the
documentation from `),_o=n("a"),ts=s("PretrainedConfig"),os=s(" for more information."),ns=d(),hn=n("p"),rs=s("Example:"),as=d(),f(ot.$$.fragment),$r=d(),le=n("h2"),qe=n("a"),pn=n("span"),f(nt.$$.fragment),ss=d(),fn=n("span"),is=s("RealmTokenizer"),Er=d(),E=n("div"),f(rt.$$.fragment),ds=d(),un=n("p"),ls=s("Construct a REALM tokenizer."),cs=d(),xe=n("p"),vo=n("a"),ms=s("RealmTokenizer"),hs=s(" is identical to "),ko=n("a"),ps=s("BertTokenizer"),fs=s(` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),us=d(),at=n("p"),gs=s("This tokenizer inherits from "),wo=n("a"),_s=s("PreTrainedTokenizer"),vs=s(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),ks=d(),Y=n("div"),f(st.$$.fragment),ws=d(),gn=n("p"),bs=s(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A REALM sequence has the following format:`),Rs=d(),it=n("ul"),bo=n("li"),Ts=s("single sequence: "),_n=n("code"),ys=s("[CLS] X [SEP]"),$s=d(),Ro=n("li"),Es=s("pair of sequences: "),vn=n("code"),zs=s("[CLS] A [SEP] B [SEP]"),qs=d(),Ae=n("div"),f(dt.$$.fragment),xs=d(),lt=n("p"),As=s(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),kn=n("code"),Ps=s("prepare_for_model"),js=s(" method."),Ls=d(),O=n("div"),f(ct.$$.fragment),Ms=d(),wn=n("p"),Fs=s(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A REALM sequence
pair mask has the following format:`),Ss=d(),f(mt.$$.fragment),Cs=d(),ce=n("p"),Is=s("If "),bn=n("code"),Ds=s("token_ids_1"),Ns=s(" is "),Rn=n("code"),Os=s("None"),Ws=s(", this method only returns the first portion of the mask (0s)."),Ks=d(),Tn=n("div"),Bs=d(),P=n("div"),f(ht.$$.fragment),Qs=d(),pt=n("p"),Hs=s("Encode a batch of text or text pair. This method is similar to regular "),yn=n("strong"),Vs=s("call"),Us=s(` method but has the following
differences:`),Xs=d(),me=n("ol"),$n=n("li"),Gs=s("Handle additional num_candidate axis. (batch_size, num_candidates, text)"),Js=d(),ft=n("li"),Zs=s("Always pad the sequences to "),En=n("em"),Ys=s("max_length"),ei=s("."),ti=d(),ut=n("li"),oi=s("Must specify "),zn=n("em"),ni=s("max_length"),ri=s(" in order to stack packs of candidates into a batch."),ai=d(),gt=n("ul"),To=n("li"),si=s("single sequence: "),qn=n("code"),ii=s("[CLS] X [SEP]"),di=d(),yo=n("li"),li=s("pair of sequences: "),xn=n("code"),ci=s("[CLS] A [SEP] B [SEP]"),mi=d(),An=n("p"),hi=s("Example:"),pi=d(),f(_t.$$.fragment),zr=d(),he=n("h2"),Pe=n("a"),Pn=n("span"),f(vt.$$.fragment),fi=d(),jn=n("span"),ui=s("RealmTokenizerFast"),qr=d(),M=n("div"),f(kt.$$.fragment),gi=d(),wt=n("p"),_i=s("Construct a \u201Cfast\u201D REALM tokenizer (backed by HuggingFace\u2019s "),Ln=n("em"),vi=s("tokenizers"),ki=s(" library). Based on WordPiece."),wi=d(),je=n("p"),$o=n("a"),bi=s("RealmTokenizerFast"),Ri=s(" is identical to "),Eo=n("a"),Ti=s("BertTokenizerFast"),yi=s(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),$i=d(),bt=n("p"),Ei=s("This tokenizer inherits from "),zo=n("a"),zi=s("PreTrainedTokenizerFast"),qi=s(` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),xi=d(),j=n("div"),f(Rt.$$.fragment),Ai=d(),Tt=n("p"),Pi=s("Encode a batch of text or text pair. This method is similar to regular "),Mn=n("strong"),ji=s("call"),Li=s(` method but has the following
differences:`),Mi=d(),pe=n("ol"),Fn=n("li"),Fi=s("Handle additional num_candidate axis. (batch_size, num_candidates, text)"),Si=d(),yt=n("li"),Ci=s("Always pad the sequences to "),Sn=n("em"),Ii=s("max_length"),Di=s("."),Ni=d(),$t=n("li"),Oi=s("Must specify "),Cn=n("em"),Wi=s("max_length"),Ki=s(" in order to stack packs of candidates into a batch."),Bi=d(),Et=n("ul"),qo=n("li"),Qi=s("single sequence: "),In=n("code"),Hi=s("[CLS] X [SEP]"),Vi=d(),xo=n("li"),Ui=s("pair of sequences: "),Dn=n("code"),Xi=s("[CLS] A [SEP] B [SEP]"),Gi=d(),Nn=n("p"),Ji=s("Example:"),Zi=d(),f(zt.$$.fragment),xr=d(),fe=n("h2"),Le=n("a"),On=n("span"),f(qt.$$.fragment),Yi=d(),Wn=n("span"),ed=s("RealmRetriever"),Ar=d(),Q=n("div"),f(xt.$$.fragment),td=d(),Kn=n("p"),od=s(`The retriever of REALM outputting the retrieved evidence block and whether the block has answers as well as answer
positions.\u201D`),nd=d(),Me=n("div"),f(At.$$.fragment),rd=d(),Bn=n("p"),ad=s("check if retrieved_blocks has answers."),Pr=d(),ue=n("h2"),Fe=n("a"),Qn=n("span"),f(Pt.$$.fragment),sd=d(),Hn=n("span"),id=s("RealmEmbedder"),jr=d(),H=n("div"),f(jt.$$.fragment),dd=d(),Lt=n("p"),ld=s(`The embedder of REALM outputting projected score that will be used to calculate relevance score.
This model is a PyTorch `),Mt=n("a"),cd=s("torch.nn.Module"),md=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),hd=d(),F=n("div"),f(Ft.$$.fragment),pd=d(),ge=n("p"),fd=s("The "),Ao=n("a"),ud=s("RealmEmbedder"),gd=s(" forward method, overrides the "),Vn=n("code"),_d=s("__call__"),vd=s(" special method."),kd=d(),f(Se.$$.fragment),wd=d(),Un=n("p"),bd=s("Example:"),Rd=d(),f(St.$$.fragment),Lr=d(),_e=n("h2"),Ce=n("a"),Xn=n("span"),f(Ct.$$.fragment),Td=d(),Gn=n("span"),yd=s("RealmScorer"),Mr=d(),V=n("div"),f(It.$$.fragment),$d=d(),Dt=n("p"),Ed=s(`The scorer of REALM outputting relevance scores representing the score of document candidates (before softmax).
This model is a PyTorch `),Nt=n("a"),zd=s("torch.nn.Module"),qd=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),xd=d(),S=n("div"),f(Ot.$$.fragment),Ad=d(),ve=n("p"),Pd=s("The "),Po=n("a"),jd=s("RealmScorer"),Ld=s(" forward method, overrides the "),Jn=n("code"),Md=s("__call__"),Fd=s(" special method."),Sd=d(),f(Ie.$$.fragment),Cd=d(),Zn=n("p"),Id=s("Example:"),Dd=d(),f(Wt.$$.fragment),Fr=d(),ke=n("h2"),De=n("a"),Yn=n("span"),f(Kt.$$.fragment),Nd=d(),er=n("span"),Od=s("RealmKnowledgeAugEncoder"),Sr=d(),U=n("div"),f(Bt.$$.fragment),Wd=d(),Qt=n("p"),Kd=s(`The knowledge-augmented encoder of REALM outputting masked language model logits and marginal log-likelihood loss.
This model is a PyTorch `),Ht=n("a"),Bd=s("torch.nn.Module"),Qd=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Hd=d(),C=n("div"),f(Vt.$$.fragment),Vd=d(),we=n("p"),Ud=s("The "),jo=n("a"),Xd=s("RealmKnowledgeAugEncoder"),Gd=s(" forward method, overrides the "),tr=n("code"),Jd=s("__call__"),Zd=s(" special method."),Yd=d(),f(Ne.$$.fragment),el=d(),or=n("p"),tl=s("Example:"),ol=d(),f(Ut.$$.fragment),Cr=d(),be=n("h2"),Oe=n("a"),nr=n("span"),f(Xt.$$.fragment),nl=d(),rr=n("span"),rl=s("RealmReader"),Ir=d(),X=n("div"),f(Gt.$$.fragment),al=d(),Jt=n("p"),sl=s(`The reader of REALM.
This model is a PyTorch `),Zt=n("a"),il=s("torch.nn.Module"),dl=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ll=d(),ee=n("div"),f(Yt.$$.fragment),cl=d(),Re=n("p"),ml=s("The "),Lo=n("a"),hl=s("RealmReader"),pl=s(" forward method, overrides the "),ar=n("code"),fl=s("__call__"),ul=s(" special method."),gl=d(),f(We.$$.fragment),Dr=d(),Te=n("h2"),Ke=n("a"),sr=n("span"),f(eo.$$.fragment),_l=d(),ir=n("span"),vl=s("RealmForOpenQA"),Nr=d(),G=n("div"),f(to.$$.fragment),kl=d(),Be=n("p"),dr=n("code"),wl=s("RealmForOpenQA"),bl=s(` for end-to-end open domain question answering.
This model is a PyTorch `),oo=n("a"),Rl=s("torch.nn.Module"),Tl=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),yl=d(),I=n("div"),f(no.$$.fragment),$l=d(),ye=n("p"),El=s("The "),Mo=n("a"),zl=s("RealmForOpenQA"),ql=s(" forward method, overrides the "),lr=n("code"),xl=s("__call__"),Al=s(" special method."),Pl=d(),f(Qe.$$.fragment),jl=d(),cr=n("p"),Ll=s("Example:"),Ml=d(),f(ro.$$.fragment),this.h()},l(o){const m=Nm('[data-svelte="svelte-1phssyn"]',document.head);h=r(m,"META",{name:!0,content:!0}),m.forEach(t),y=l(o),w=r(o,"H1",{class:!0});var ao=a(w);T=r(ao,"A",{id:!0,class:!0,href:!0});var mr=a(T);$=r(mr,"SPAN",{});var hr=a($);u(R.$$.fragment,hr),hr.forEach(t),mr.forEach(t),b=l(ao),z=r(ao,"SPAN",{});var pr=a(z);va=i(pr,"REALM"),pr.forEach(t),ao.forEach(t),vr=l(o),se=r(o,"H2",{class:!0});var so=a(se);$e=r(so,"A",{id:!0,class:!0,href:!0});var Ol=a($e);Yo=r(Ol,"SPAN",{});var Wl=a(Yo);u(Ue.$$.fragment,Wl),Wl.forEach(t),Ol.forEach(t),ka=l(so),en=r(so,"SPAN",{});var Kl=a(en);wa=i(Kl,"Overview"),Kl.forEach(t),so.forEach(t),kr=l(o),Ee=r(o,"P",{});var Wr=a(Ee);ba=i(Wr,"The REALM model was proposed in "),Xe=r(Wr,"A",{href:!0,rel:!0});var Bl=a(Xe);Ra=i(Bl,"REALM: Retrieval-Augmented Language Model Pre-Training"),Bl.forEach(t),Ta=i(Wr,` by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang. It\u2019s a
retrieval-augmented language model that firstly retrieves documents from a textual knowledge corpus and then
utilizes retrieved documents to process question answering tasks.`),Wr.forEach(t),wr=l(o),io=r(o,"P",{});var Ql=a(io);ya=i(Ql,"The abstract from the paper is the following:"),Ql.forEach(t),br=l(o),lo=r(o,"P",{});var Hl=a(lo);tn=r(Hl,"EM",{});var Vl=a(tn);$a=i(Vl,`Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks
such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network,
requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we
augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend
over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the
first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language
modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We
demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the
challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both
explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous
methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as
interpretability and modularity.`),Vl.forEach(t),Hl.forEach(t),Rr=l(o),Z=r(o,"P",{});var Fo=a(Z);Ea=i(Fo,"This model was contributed by "),Ge=r(Fo,"A",{href:!0,rel:!0});var Ul=a(Ge);za=i(Ul,"qqaatw"),Ul.forEach(t),qa=i(Fo,`. The original code can be found
`),Je=r(Fo,"A",{href:!0,rel:!0});var Xl=a(Je);xa=i(Xl,"here"),Xl.forEach(t),Aa=i(Fo,"."),Fo.forEach(t),Tr=l(o),ie=r(o,"H2",{class:!0});var Kr=a(ie);ze=r(Kr,"A",{id:!0,class:!0,href:!0});var Gl=a(ze);on=r(Gl,"SPAN",{});var Jl=a(on);u(Ze.$$.fragment,Jl),Jl.forEach(t),Gl.forEach(t),Pa=l(Kr),nn=r(Kr,"SPAN",{});var Zl=a(nn);ja=i(Zl,"RealmConfig"),Zl.forEach(t),Kr.forEach(t),yr=l(o),x=r(o,"DIV",{class:!0});var D=a(x);u(Ye.$$.fragment,D),La=l(D),rn=r(D,"P",{});var Yl=a(rn);Ma=i(Yl,"This is the configuration class to store the configuration of"),Yl.forEach(t),Fa=l(D),L=r(D,"OL",{});var W=a(L);an=r(W,"LI",{});var ec=a(an);co=r(ec,"A",{href:!0});var tc=a(co);Sa=i(tc,"RealmEmbedder"),tc.forEach(t),ec.forEach(t),Ca=l(W),sn=r(W,"LI",{});var oc=a(sn);mo=r(oc,"A",{href:!0});var nc=a(mo);Ia=i(nc,"RealmScorer"),nc.forEach(t),oc.forEach(t),Da=l(W),dn=r(W,"LI",{});var rc=a(dn);ho=r(rc,"A",{href:!0});var ac=a(ho);Na=i(ac,"RealmKnowledgeAugEncoder"),ac.forEach(t),rc.forEach(t),Oa=l(W),ln=r(W,"LI",{});var sc=a(ln);po=r(sc,"A",{href:!0});var ic=a(po);Wa=i(ic,"RealmRetriever"),ic.forEach(t),sc.forEach(t),Ka=l(W),cn=r(W,"LI",{});var dc=a(cn);fo=r(dc,"A",{href:!0});var lc=a(fo);Ba=i(lc,"RealmReader"),lc.forEach(t),dc.forEach(t),Qa=l(W),mn=r(W,"LI",{});var cc=a(mn);uo=r(cc,"A",{href:!0});var mc=a(uo);Ha=i(mc,"RealmForOpenQA"),mc.forEach(t),cc.forEach(t),W.forEach(t),Va=l(D),et=r(D,"P",{});var Br=a(et);Ua=i(Br,`It is used to instantiate an REALM model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the REALM
`),tt=r(Br,"A",{href:!0,rel:!0});var hc=a(tt);Xa=i(hc,"realm-cc-news-pretrained"),hc.forEach(t),Ga=i(Br," architecture."),Br.forEach(t),Ja=l(D),de=r(D,"P",{});var So=a(de);Za=i(So,"Configuration objects inherit from "),go=r(So,"A",{href:!0});var pc=a(go);Ya=i(pc,"PretrainedConfig"),pc.forEach(t),es=i(So,` and can be used to control the model outputs. Read the
documentation from `),_o=r(So,"A",{href:!0});var fc=a(_o);ts=i(fc,"PretrainedConfig"),fc.forEach(t),os=i(So," for more information."),So.forEach(t),ns=l(D),hn=r(D,"P",{});var uc=a(hn);rs=i(uc,"Example:"),uc.forEach(t),as=l(D),u(ot.$$.fragment,D),D.forEach(t),$r=l(o),le=r(o,"H2",{class:!0});var Qr=a(le);qe=r(Qr,"A",{id:!0,class:!0,href:!0});var gc=a(qe);pn=r(gc,"SPAN",{});var _c=a(pn);u(nt.$$.fragment,_c),_c.forEach(t),gc.forEach(t),ss=l(Qr),fn=r(Qr,"SPAN",{});var vc=a(fn);is=i(vc,"RealmTokenizer"),vc.forEach(t),Qr.forEach(t),Er=l(o),E=r(o,"DIV",{class:!0});var A=a(E);u(rt.$$.fragment,A),ds=l(A),un=r(A,"P",{});var kc=a(un);ls=i(kc,"Construct a REALM tokenizer."),kc.forEach(t),cs=l(A),xe=r(A,"P",{});var fr=a(xe);vo=r(fr,"A",{href:!0});var wc=a(vo);ms=i(wc,"RealmTokenizer"),wc.forEach(t),hs=i(fr," is identical to "),ko=r(fr,"A",{href:!0});var bc=a(ko);ps=i(bc,"BertTokenizer"),bc.forEach(t),fs=i(fr,` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),fr.forEach(t),us=l(A),at=r(A,"P",{});var Hr=a(at);gs=i(Hr,"This tokenizer inherits from "),wo=r(Hr,"A",{href:!0});var Rc=a(wo);_s=i(Rc,"PreTrainedTokenizer"),Rc.forEach(t),vs=i(Hr,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Hr.forEach(t),ks=l(A),Y=r(A,"DIV",{class:!0});var Co=a(Y);u(st.$$.fragment,Co),ws=l(Co),gn=r(Co,"P",{});var Tc=a(gn);bs=i(Tc,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A REALM sequence has the following format:`),Tc.forEach(t),Rs=l(Co),it=r(Co,"UL",{});var Vr=a(it);bo=r(Vr,"LI",{});var Fl=a(bo);Ts=i(Fl,"single sequence: "),_n=r(Fl,"CODE",{});var yc=a(_n);ys=i(yc,"[CLS] X [SEP]"),yc.forEach(t),Fl.forEach(t),$s=l(Vr),Ro=r(Vr,"LI",{});var Sl=a(Ro);Es=i(Sl,"pair of sequences: "),vn=r(Sl,"CODE",{});var $c=a(vn);zs=i($c,"[CLS] A [SEP] B [SEP]"),$c.forEach(t),Sl.forEach(t),Vr.forEach(t),Co.forEach(t),qs=l(A),Ae=r(A,"DIV",{class:!0});var Ur=a(Ae);u(dt.$$.fragment,Ur),xs=l(Ur),lt=r(Ur,"P",{});var Xr=a(lt);As=i(Xr,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),kn=r(Xr,"CODE",{});var Ec=a(kn);Ps=i(Ec,"prepare_for_model"),Ec.forEach(t),js=i(Xr," method."),Xr.forEach(t),Ur.forEach(t),Ls=l(A),O=r(A,"DIV",{class:!0});var He=a(O);u(ct.$$.fragment,He),Ms=l(He),wn=r(He,"P",{});var zc=a(wn);Fs=i(zc,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A REALM sequence
pair mask has the following format:`),zc.forEach(t),Ss=l(He),u(mt.$$.fragment,He),Cs=l(He),ce=r(He,"P",{});var Io=a(ce);Is=i(Io,"If "),bn=r(Io,"CODE",{});var qc=a(bn);Ds=i(qc,"token_ids_1"),qc.forEach(t),Ns=i(Io," is "),Rn=r(Io,"CODE",{});var xc=a(Rn);Os=i(xc,"None"),xc.forEach(t),Ws=i(Io,", this method only returns the first portion of the mask (0s)."),Io.forEach(t),He.forEach(t),Ks=l(A),Tn=r(A,"DIV",{class:!0}),a(Tn).forEach(t),Bs=l(A),P=r(A,"DIV",{class:!0});var K=a(P);u(ht.$$.fragment,K),Qs=l(K),pt=r(K,"P",{});var Gr=a(pt);Hs=i(Gr,"Encode a batch of text or text pair. This method is similar to regular "),yn=r(Gr,"STRONG",{});var Ac=a(yn);Vs=i(Ac,"call"),Ac.forEach(t),Us=i(Gr,` method but has the following
differences:`),Gr.forEach(t),Xs=l(K),me=r(K,"OL",{});var Do=a(me);$n=r(Do,"LI",{});var Pc=a($n);Gs=i(Pc,"Handle additional num_candidate axis. (batch_size, num_candidates, text)"),Pc.forEach(t),Js=l(Do),ft=r(Do,"LI",{});var Jr=a(ft);Zs=i(Jr,"Always pad the sequences to "),En=r(Jr,"EM",{});var jc=a(En);Ys=i(jc,"max_length"),jc.forEach(t),ei=i(Jr,"."),Jr.forEach(t),ti=l(Do),ut=r(Do,"LI",{});var Zr=a(ut);oi=i(Zr,"Must specify "),zn=r(Zr,"EM",{});var Lc=a(zn);ni=i(Lc,"max_length"),Lc.forEach(t),ri=i(Zr," in order to stack packs of candidates into a batch."),Zr.forEach(t),Do.forEach(t),ai=l(K),gt=r(K,"UL",{});var Yr=a(gt);To=r(Yr,"LI",{});var Cl=a(To);si=i(Cl,"single sequence: "),qn=r(Cl,"CODE",{});var Mc=a(qn);ii=i(Mc,"[CLS] X [SEP]"),Mc.forEach(t),Cl.forEach(t),di=l(Yr),yo=r(Yr,"LI",{});var Il=a(yo);li=i(Il,"pair of sequences: "),xn=r(Il,"CODE",{});var Fc=a(xn);ci=i(Fc,"[CLS] A [SEP] B [SEP]"),Fc.forEach(t),Il.forEach(t),Yr.forEach(t),mi=l(K),An=r(K,"P",{});var Sc=a(An);hi=i(Sc,"Example:"),Sc.forEach(t),pi=l(K),u(_t.$$.fragment,K),K.forEach(t),A.forEach(t),zr=l(o),he=r(o,"H2",{class:!0});var ea=a(he);Pe=r(ea,"A",{id:!0,class:!0,href:!0});var Cc=a(Pe);Pn=r(Cc,"SPAN",{});var Ic=a(Pn);u(vt.$$.fragment,Ic),Ic.forEach(t),Cc.forEach(t),fi=l(ea),jn=r(ea,"SPAN",{});var Dc=a(jn);ui=i(Dc,"RealmTokenizerFast"),Dc.forEach(t),ea.forEach(t),qr=l(o),M=r(o,"DIV",{class:!0});var te=a(M);u(kt.$$.fragment,te),gi=l(te),wt=r(te,"P",{});var ta=a(wt);_i=i(ta,"Construct a \u201Cfast\u201D REALM tokenizer (backed by HuggingFace\u2019s "),Ln=r(ta,"EM",{});var Nc=a(Ln);vi=i(Nc,"tokenizers"),Nc.forEach(t),ki=i(ta," library). Based on WordPiece."),ta.forEach(t),wi=l(te),je=r(te,"P",{});var ur=a(je);$o=r(ur,"A",{href:!0});var Oc=a($o);bi=i(Oc,"RealmTokenizerFast"),Oc.forEach(t),Ri=i(ur," is identical to "),Eo=r(ur,"A",{href:!0});var Wc=a(Eo);Ti=i(Wc,"BertTokenizerFast"),Wc.forEach(t),yi=i(ur,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),ur.forEach(t),$i=l(te),bt=r(te,"P",{});var oa=a(bt);Ei=i(oa,"This tokenizer inherits from "),zo=r(oa,"A",{href:!0});var Kc=a(zo);zi=i(Kc,"PreTrainedTokenizerFast"),Kc.forEach(t),qi=i(oa,` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),oa.forEach(t),xi=l(te),j=r(te,"DIV",{class:!0});var B=a(j);u(Rt.$$.fragment,B),Ai=l(B),Tt=r(B,"P",{});var na=a(Tt);Pi=i(na,"Encode a batch of text or text pair. This method is similar to regular "),Mn=r(na,"STRONG",{});var Bc=a(Mn);ji=i(Bc,"call"),Bc.forEach(t),Li=i(na,` method but has the following
differences:`),na.forEach(t),Mi=l(B),pe=r(B,"OL",{});var No=a(pe);Fn=r(No,"LI",{});var Qc=a(Fn);Fi=i(Qc,"Handle additional num_candidate axis. (batch_size, num_candidates, text)"),Qc.forEach(t),Si=l(No),yt=r(No,"LI",{});var ra=a(yt);Ci=i(ra,"Always pad the sequences to "),Sn=r(ra,"EM",{});var Hc=a(Sn);Ii=i(Hc,"max_length"),Hc.forEach(t),Di=i(ra,"."),ra.forEach(t),Ni=l(No),$t=r(No,"LI",{});var aa=a($t);Oi=i(aa,"Must specify "),Cn=r(aa,"EM",{});var Vc=a(Cn);Wi=i(Vc,"max_length"),Vc.forEach(t),Ki=i(aa," in order to stack packs of candidates into a batch."),aa.forEach(t),No.forEach(t),Bi=l(B),Et=r(B,"UL",{});var sa=a(Et);qo=r(sa,"LI",{});var Dl=a(qo);Qi=i(Dl,"single sequence: "),In=r(Dl,"CODE",{});var Uc=a(In);Hi=i(Uc,"[CLS] X [SEP]"),Uc.forEach(t),Dl.forEach(t),Vi=l(sa),xo=r(sa,"LI",{});var Nl=a(xo);Ui=i(Nl,"pair of sequences: "),Dn=r(Nl,"CODE",{});var Xc=a(Dn);Xi=i(Xc,"[CLS] A [SEP] B [SEP]"),Xc.forEach(t),Nl.forEach(t),sa.forEach(t),Gi=l(B),Nn=r(B,"P",{});var Gc=a(Nn);Ji=i(Gc,"Example:"),Gc.forEach(t),Zi=l(B),u(zt.$$.fragment,B),B.forEach(t),te.forEach(t),xr=l(o),fe=r(o,"H2",{class:!0});var ia=a(fe);Le=r(ia,"A",{id:!0,class:!0,href:!0});var Jc=a(Le);On=r(Jc,"SPAN",{});var Zc=a(On);u(qt.$$.fragment,Zc),Zc.forEach(t),Jc.forEach(t),Yi=l(ia),Wn=r(ia,"SPAN",{});var Yc=a(Wn);ed=i(Yc,"RealmRetriever"),Yc.forEach(t),ia.forEach(t),Ar=l(o),Q=r(o,"DIV",{class:!0});var Oo=a(Q);u(xt.$$.fragment,Oo),td=l(Oo),Kn=r(Oo,"P",{});var em=a(Kn);od=i(em,`The retriever of REALM outputting the retrieved evidence block and whether the block has answers as well as answer
positions.\u201D`),em.forEach(t),nd=l(Oo),Me=r(Oo,"DIV",{class:!0});var da=a(Me);u(At.$$.fragment,da),rd=l(da),Bn=r(da,"P",{});var tm=a(Bn);ad=i(tm,"check if retrieved_blocks has answers."),tm.forEach(t),da.forEach(t),Oo.forEach(t),Pr=l(o),ue=r(o,"H2",{class:!0});var la=a(ue);Fe=r(la,"A",{id:!0,class:!0,href:!0});var om=a(Fe);Qn=r(om,"SPAN",{});var nm=a(Qn);u(Pt.$$.fragment,nm),nm.forEach(t),om.forEach(t),sd=l(la),Hn=r(la,"SPAN",{});var rm=a(Hn);id=i(rm,"RealmEmbedder"),rm.forEach(t),la.forEach(t),jr=l(o),H=r(o,"DIV",{class:!0});var Wo=a(H);u(jt.$$.fragment,Wo),dd=l(Wo),Lt=r(Wo,"P",{});var ca=a(Lt);ld=i(ca,`The embedder of REALM outputting projected score that will be used to calculate relevance score.
This model is a PyTorch `),Mt=r(ca,"A",{href:!0,rel:!0});var am=a(Mt);cd=i(am,"torch.nn.Module"),am.forEach(t),md=i(ca,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ca.forEach(t),hd=l(Wo),F=r(Wo,"DIV",{class:!0});var oe=a(F);u(Ft.$$.fragment,oe),pd=l(oe),ge=r(oe,"P",{});var Ko=a(ge);fd=i(Ko,"The "),Ao=r(Ko,"A",{href:!0});var sm=a(Ao);ud=i(sm,"RealmEmbedder"),sm.forEach(t),gd=i(Ko," forward method, overrides the "),Vn=r(Ko,"CODE",{});var im=a(Vn);_d=i(im,"__call__"),im.forEach(t),vd=i(Ko," special method."),Ko.forEach(t),kd=l(oe),u(Se.$$.fragment,oe),wd=l(oe),Un=r(oe,"P",{});var dm=a(Un);bd=i(dm,"Example:"),dm.forEach(t),Rd=l(oe),u(St.$$.fragment,oe),oe.forEach(t),Wo.forEach(t),Lr=l(o),_e=r(o,"H2",{class:!0});var ma=a(_e);Ce=r(ma,"A",{id:!0,class:!0,href:!0});var lm=a(Ce);Xn=r(lm,"SPAN",{});var cm=a(Xn);u(Ct.$$.fragment,cm),cm.forEach(t),lm.forEach(t),Td=l(ma),Gn=r(ma,"SPAN",{});var mm=a(Gn);yd=i(mm,"RealmScorer"),mm.forEach(t),ma.forEach(t),Mr=l(o),V=r(o,"DIV",{class:!0});var Bo=a(V);u(It.$$.fragment,Bo),$d=l(Bo),Dt=r(Bo,"P",{});var ha=a(Dt);Ed=i(ha,`The scorer of REALM outputting relevance scores representing the score of document candidates (before softmax).
This model is a PyTorch `),Nt=r(ha,"A",{href:!0,rel:!0});var hm=a(Nt);zd=i(hm,"torch.nn.Module"),hm.forEach(t),qd=i(ha,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ha.forEach(t),xd=l(Bo),S=r(Bo,"DIV",{class:!0});var ne=a(S);u(Ot.$$.fragment,ne),Ad=l(ne),ve=r(ne,"P",{});var Qo=a(ve);Pd=i(Qo,"The "),Po=r(Qo,"A",{href:!0});var pm=a(Po);jd=i(pm,"RealmScorer"),pm.forEach(t),Ld=i(Qo," forward method, overrides the "),Jn=r(Qo,"CODE",{});var fm=a(Jn);Md=i(fm,"__call__"),fm.forEach(t),Fd=i(Qo," special method."),Qo.forEach(t),Sd=l(ne),u(Ie.$$.fragment,ne),Cd=l(ne),Zn=r(ne,"P",{});var um=a(Zn);Id=i(um,"Example:"),um.forEach(t),Dd=l(ne),u(Wt.$$.fragment,ne),ne.forEach(t),Bo.forEach(t),Fr=l(o),ke=r(o,"H2",{class:!0});var pa=a(ke);De=r(pa,"A",{id:!0,class:!0,href:!0});var gm=a(De);Yn=r(gm,"SPAN",{});var _m=a(Yn);u(Kt.$$.fragment,_m),_m.forEach(t),gm.forEach(t),Nd=l(pa),er=r(pa,"SPAN",{});var vm=a(er);Od=i(vm,"RealmKnowledgeAugEncoder"),vm.forEach(t),pa.forEach(t),Sr=l(o),U=r(o,"DIV",{class:!0});var Ho=a(U);u(Bt.$$.fragment,Ho),Wd=l(Ho),Qt=r(Ho,"P",{});var fa=a(Qt);Kd=i(fa,`The knowledge-augmented encoder of REALM outputting masked language model logits and marginal log-likelihood loss.
This model is a PyTorch `),Ht=r(fa,"A",{href:!0,rel:!0});var km=a(Ht);Bd=i(km,"torch.nn.Module"),km.forEach(t),Qd=i(fa,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),fa.forEach(t),Hd=l(Ho),C=r(Ho,"DIV",{class:!0});var re=a(C);u(Vt.$$.fragment,re),Vd=l(re),we=r(re,"P",{});var Vo=a(we);Ud=i(Vo,"The "),jo=r(Vo,"A",{href:!0});var wm=a(jo);Xd=i(wm,"RealmKnowledgeAugEncoder"),wm.forEach(t),Gd=i(Vo," forward method, overrides the "),tr=r(Vo,"CODE",{});var bm=a(tr);Jd=i(bm,"__call__"),bm.forEach(t),Zd=i(Vo," special method."),Vo.forEach(t),Yd=l(re),u(Ne.$$.fragment,re),el=l(re),or=r(re,"P",{});var Rm=a(or);tl=i(Rm,"Example:"),Rm.forEach(t),ol=l(re),u(Ut.$$.fragment,re),re.forEach(t),Ho.forEach(t),Cr=l(o),be=r(o,"H2",{class:!0});var ua=a(be);Oe=r(ua,"A",{id:!0,class:!0,href:!0});var Tm=a(Oe);nr=r(Tm,"SPAN",{});var ym=a(nr);u(Xt.$$.fragment,ym),ym.forEach(t),Tm.forEach(t),nl=l(ua),rr=r(ua,"SPAN",{});var $m=a(rr);rl=i($m,"RealmReader"),$m.forEach(t),ua.forEach(t),Ir=l(o),X=r(o,"DIV",{class:!0});var Uo=a(X);u(Gt.$$.fragment,Uo),al=l(Uo),Jt=r(Uo,"P",{});var ga=a(Jt);sl=i(ga,`The reader of REALM.
This model is a PyTorch `),Zt=r(ga,"A",{href:!0,rel:!0});var Em=a(Zt);il=i(Em,"torch.nn.Module"),Em.forEach(t),dl=i(ga,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ga.forEach(t),ll=l(Uo),ee=r(Uo,"DIV",{class:!0});var Xo=a(ee);u(Yt.$$.fragment,Xo),cl=l(Xo),Re=r(Xo,"P",{});var Go=a(Re);ml=i(Go,"The "),Lo=r(Go,"A",{href:!0});var zm=a(Lo);hl=i(zm,"RealmReader"),zm.forEach(t),pl=i(Go," forward method, overrides the "),ar=r(Go,"CODE",{});var qm=a(ar);fl=i(qm,"__call__"),qm.forEach(t),ul=i(Go," special method."),Go.forEach(t),gl=l(Xo),u(We.$$.fragment,Xo),Xo.forEach(t),Uo.forEach(t),Dr=l(o),Te=r(o,"H2",{class:!0});var _a=a(Te);Ke=r(_a,"A",{id:!0,class:!0,href:!0});var xm=a(Ke);sr=r(xm,"SPAN",{});var Am=a(sr);u(eo.$$.fragment,Am),Am.forEach(t),xm.forEach(t),_l=l(_a),ir=r(_a,"SPAN",{});var Pm=a(ir);vl=i(Pm,"RealmForOpenQA"),Pm.forEach(t),_a.forEach(t),Nr=l(o),G=r(o,"DIV",{class:!0});var Jo=a(G);u(to.$$.fragment,Jo),kl=l(Jo),Be=r(Jo,"P",{});var gr=a(Be);dr=r(gr,"CODE",{});var jm=a(dr);wl=i(jm,"RealmForOpenQA"),jm.forEach(t),bl=i(gr,` for end-to-end open domain question answering.
This model is a PyTorch `),oo=r(gr,"A",{href:!0,rel:!0});var Lm=a(oo);Rl=i(Lm,"torch.nn.Module"),Lm.forEach(t),Tl=i(gr,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),gr.forEach(t),yl=l(Jo),I=r(Jo,"DIV",{class:!0});var ae=a(I);u(no.$$.fragment,ae),$l=l(ae),ye=r(ae,"P",{});var Zo=a(ye);El=i(Zo,"The "),Mo=r(Zo,"A",{href:!0});var Mm=a(Mo);zl=i(Mm,"RealmForOpenQA"),Mm.forEach(t),ql=i(Zo," forward method, overrides the "),lr=r(Zo,"CODE",{});var Fm=a(lr);xl=i(Fm,"__call__"),Fm.forEach(t),Al=i(Zo," special method."),Zo.forEach(t),Pl=l(ae),u(Qe.$$.fragment,ae),jl=l(ae),cr=r(ae,"P",{});var Sm=a(cr);Ll=i(Sm,"Example:"),Sm.forEach(t),Ml=l(ae),u(ro.$$.fragment,ae),ae.forEach(t),Jo.forEach(t),this.h()},h(){c(h,"name","hf:doc:metadata"),c(h,"content",JSON.stringify(Vm)),c(T,"id","realm"),c(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(T,"href","#realm"),c(w,"class","relative group"),c($e,"id","overview"),c($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($e,"href","#overview"),c(se,"class","relative group"),c(Xe,"href","https://arxiv.org/abs/2002.08909"),c(Xe,"rel","nofollow"),c(Ge,"href","https://huggingface.co/qqaatw"),c(Ge,"rel","nofollow"),c(Je,"href","https://github.com/google-research/language/tree/master/language/realm"),c(Je,"rel","nofollow"),c(ze,"id","transformers.RealmConfig"),c(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ze,"href","#transformers.RealmConfig"),c(ie,"class","relative group"),c(co,"href","/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmEmbedder"),c(mo,"href","/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmScorer"),c(ho,"href","/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmKnowledgeAugEncoder"),c(po,"href","/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmRetriever"),c(fo,"href","/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmReader"),c(uo,"href","/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmForOpenQA"),c(tt,"href","https://huggingface.co/google/realm-cc-news-pretrained-embedder"),c(tt,"rel","nofollow"),c(go,"href","/docs/transformers/v4.16.2/en/main_classes/configuration#transformers.PretrainedConfig"),c(_o,"href","/docs/transformers/v4.16.2/en/main_classes/configuration#transformers.PretrainedConfig"),c(x,"class","docstring"),c(qe,"id","transformers.RealmTokenizer"),c(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qe,"href","#transformers.RealmTokenizer"),c(le,"class","relative group"),c(vo,"href","/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmTokenizer"),c(ko,"href","/docs/transformers/v4.16.2/en/model_doc/bert#transformers.BertTokenizer"),c(wo,"href","/docs/transformers/v4.16.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(Y,"class","docstring"),c(Ae,"class","docstring"),c(O,"class","docstring"),c(Tn,"class","docstring"),c(P,"class","docstring"),c(E,"class","docstring"),c(Pe,"id","transformers.RealmTokenizerFast"),c(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Pe,"href","#transformers.RealmTokenizerFast"),c(he,"class","relative group"),c($o,"href","/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmTokenizerFast"),c(Eo,"href","/docs/transformers/v4.16.2/en/model_doc/bert#transformers.BertTokenizerFast"),c(zo,"href","/docs/transformers/v4.16.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),c(j,"class","docstring"),c(M,"class","docstring"),c(Le,"id","transformers.RealmRetriever"),c(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Le,"href","#transformers.RealmRetriever"),c(fe,"class","relative group"),c(Me,"class","docstring"),c(Q,"class","docstring"),c(Fe,"id","transformers.RealmEmbedder"),c(Fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fe,"href","#transformers.RealmEmbedder"),c(ue,"class","relative group"),c(Mt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Mt,"rel","nofollow"),c(Ao,"href","/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmEmbedder"),c(F,"class","docstring"),c(H,"class","docstring"),c(Ce,"id","transformers.RealmScorer"),c(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ce,"href","#transformers.RealmScorer"),c(_e,"class","relative group"),c(Nt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Nt,"rel","nofollow"),c(Po,"href","/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmScorer"),c(S,"class","docstring"),c(V,"class","docstring"),c(De,"id","transformers.RealmKnowledgeAugEncoder"),c(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(De,"href","#transformers.RealmKnowledgeAugEncoder"),c(ke,"class","relative group"),c(Ht,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ht,"rel","nofollow"),c(jo,"href","/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmKnowledgeAugEncoder"),c(C,"class","docstring"),c(U,"class","docstring"),c(Oe,"id","transformers.RealmReader"),c(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Oe,"href","#transformers.RealmReader"),c(be,"class","relative group"),c(Zt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Zt,"rel","nofollow"),c(Lo,"href","/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmReader"),c(ee,"class","docstring"),c(X,"class","docstring"),c(Ke,"id","transformers.RealmForOpenQA"),c(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ke,"href","#transformers.RealmForOpenQA"),c(Te,"class","relative group"),c(oo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(oo,"rel","nofollow"),c(Mo,"href","/docs/transformers/v4.16.2/en/model_doc/realm#transformers.RealmForOpenQA"),c(I,"class","docstring"),c(G,"class","docstring")},m(o,m){e(document.head,h),p(o,y,m),p(o,w,m),e(w,T),e(T,$),g(R,$,null),e(w,b),e(w,z),e(z,va),p(o,vr,m),p(o,se,m),e(se,$e),e($e,Yo),g(Ue,Yo,null),e(se,ka),e(se,en),e(en,wa),p(o,kr,m),p(o,Ee,m),e(Ee,ba),e(Ee,Xe),e(Xe,Ra),e(Ee,Ta),p(o,wr,m),p(o,io,m),e(io,ya),p(o,br,m),p(o,lo,m),e(lo,tn),e(tn,$a),p(o,Rr,m),p(o,Z,m),e(Z,Ea),e(Z,Ge),e(Ge,za),e(Z,qa),e(Z,Je),e(Je,xa),e(Z,Aa),p(o,Tr,m),p(o,ie,m),e(ie,ze),e(ze,on),g(Ze,on,null),e(ie,Pa),e(ie,nn),e(nn,ja),p(o,yr,m),p(o,x,m),g(Ye,x,null),e(x,La),e(x,rn),e(rn,Ma),e(x,Fa),e(x,L),e(L,an),e(an,co),e(co,Sa),e(L,Ca),e(L,sn),e(sn,mo),e(mo,Ia),e(L,Da),e(L,dn),e(dn,ho),e(ho,Na),e(L,Oa),e(L,ln),e(ln,po),e(po,Wa),e(L,Ka),e(L,cn),e(cn,fo),e(fo,Ba),e(L,Qa),e(L,mn),e(mn,uo),e(uo,Ha),e(x,Va),e(x,et),e(et,Ua),e(et,tt),e(tt,Xa),e(et,Ga),e(x,Ja),e(x,de),e(de,Za),e(de,go),e(go,Ya),e(de,es),e(de,_o),e(_o,ts),e(de,os),e(x,ns),e(x,hn),e(hn,rs),e(x,as),g(ot,x,null),p(o,$r,m),p(o,le,m),e(le,qe),e(qe,pn),g(nt,pn,null),e(le,ss),e(le,fn),e(fn,is),p(o,Er,m),p(o,E,m),g(rt,E,null),e(E,ds),e(E,un),e(un,ls),e(E,cs),e(E,xe),e(xe,vo),e(vo,ms),e(xe,hs),e(xe,ko),e(ko,ps),e(xe,fs),e(E,us),e(E,at),e(at,gs),e(at,wo),e(wo,_s),e(at,vs),e(E,ks),e(E,Y),g(st,Y,null),e(Y,ws),e(Y,gn),e(gn,bs),e(Y,Rs),e(Y,it),e(it,bo),e(bo,Ts),e(bo,_n),e(_n,ys),e(it,$s),e(it,Ro),e(Ro,Es),e(Ro,vn),e(vn,zs),e(E,qs),e(E,Ae),g(dt,Ae,null),e(Ae,xs),e(Ae,lt),e(lt,As),e(lt,kn),e(kn,Ps),e(lt,js),e(E,Ls),e(E,O),g(ct,O,null),e(O,Ms),e(O,wn),e(wn,Fs),e(O,Ss),g(mt,O,null),e(O,Cs),e(O,ce),e(ce,Is),e(ce,bn),e(bn,Ds),e(ce,Ns),e(ce,Rn),e(Rn,Os),e(ce,Ws),e(E,Ks),e(E,Tn),e(E,Bs),e(E,P),g(ht,P,null),e(P,Qs),e(P,pt),e(pt,Hs),e(pt,yn),e(yn,Vs),e(pt,Us),e(P,Xs),e(P,me),e(me,$n),e($n,Gs),e(me,Js),e(me,ft),e(ft,Zs),e(ft,En),e(En,Ys),e(ft,ei),e(me,ti),e(me,ut),e(ut,oi),e(ut,zn),e(zn,ni),e(ut,ri),e(P,ai),e(P,gt),e(gt,To),e(To,si),e(To,qn),e(qn,ii),e(gt,di),e(gt,yo),e(yo,li),e(yo,xn),e(xn,ci),e(P,mi),e(P,An),e(An,hi),e(P,pi),g(_t,P,null),p(o,zr,m),p(o,he,m),e(he,Pe),e(Pe,Pn),g(vt,Pn,null),e(he,fi),e(he,jn),e(jn,ui),p(o,qr,m),p(o,M,m),g(kt,M,null),e(M,gi),e(M,wt),e(wt,_i),e(wt,Ln),e(Ln,vi),e(wt,ki),e(M,wi),e(M,je),e(je,$o),e($o,bi),e(je,Ri),e(je,Eo),e(Eo,Ti),e(je,yi),e(M,$i),e(M,bt),e(bt,Ei),e(bt,zo),e(zo,zi),e(bt,qi),e(M,xi),e(M,j),g(Rt,j,null),e(j,Ai),e(j,Tt),e(Tt,Pi),e(Tt,Mn),e(Mn,ji),e(Tt,Li),e(j,Mi),e(j,pe),e(pe,Fn),e(Fn,Fi),e(pe,Si),e(pe,yt),e(yt,Ci),e(yt,Sn),e(Sn,Ii),e(yt,Di),e(pe,Ni),e(pe,$t),e($t,Oi),e($t,Cn),e(Cn,Wi),e($t,Ki),e(j,Bi),e(j,Et),e(Et,qo),e(qo,Qi),e(qo,In),e(In,Hi),e(Et,Vi),e(Et,xo),e(xo,Ui),e(xo,Dn),e(Dn,Xi),e(j,Gi),e(j,Nn),e(Nn,Ji),e(j,Zi),g(zt,j,null),p(o,xr,m),p(o,fe,m),e(fe,Le),e(Le,On),g(qt,On,null),e(fe,Yi),e(fe,Wn),e(Wn,ed),p(o,Ar,m),p(o,Q,m),g(xt,Q,null),e(Q,td),e(Q,Kn),e(Kn,od),e(Q,nd),e(Q,Me),g(At,Me,null),e(Me,rd),e(Me,Bn),e(Bn,ad),p(o,Pr,m),p(o,ue,m),e(ue,Fe),e(Fe,Qn),g(Pt,Qn,null),e(ue,sd),e(ue,Hn),e(Hn,id),p(o,jr,m),p(o,H,m),g(jt,H,null),e(H,dd),e(H,Lt),e(Lt,ld),e(Lt,Mt),e(Mt,cd),e(Lt,md),e(H,hd),e(H,F),g(Ft,F,null),e(F,pd),e(F,ge),e(ge,fd),e(ge,Ao),e(Ao,ud),e(ge,gd),e(ge,Vn),e(Vn,_d),e(ge,vd),e(F,kd),g(Se,F,null),e(F,wd),e(F,Un),e(Un,bd),e(F,Rd),g(St,F,null),p(o,Lr,m),p(o,_e,m),e(_e,Ce),e(Ce,Xn),g(Ct,Xn,null),e(_e,Td),e(_e,Gn),e(Gn,yd),p(o,Mr,m),p(o,V,m),g(It,V,null),e(V,$d),e(V,Dt),e(Dt,Ed),e(Dt,Nt),e(Nt,zd),e(Dt,qd),e(V,xd),e(V,S),g(Ot,S,null),e(S,Ad),e(S,ve),e(ve,Pd),e(ve,Po),e(Po,jd),e(ve,Ld),e(ve,Jn),e(Jn,Md),e(ve,Fd),e(S,Sd),g(Ie,S,null),e(S,Cd),e(S,Zn),e(Zn,Id),e(S,Dd),g(Wt,S,null),p(o,Fr,m),p(o,ke,m),e(ke,De),e(De,Yn),g(Kt,Yn,null),e(ke,Nd),e(ke,er),e(er,Od),p(o,Sr,m),p(o,U,m),g(Bt,U,null),e(U,Wd),e(U,Qt),e(Qt,Kd),e(Qt,Ht),e(Ht,Bd),e(Qt,Qd),e(U,Hd),e(U,C),g(Vt,C,null),e(C,Vd),e(C,we),e(we,Ud),e(we,jo),e(jo,Xd),e(we,Gd),e(we,tr),e(tr,Jd),e(we,Zd),e(C,Yd),g(Ne,C,null),e(C,el),e(C,or),e(or,tl),e(C,ol),g(Ut,C,null),p(o,Cr,m),p(o,be,m),e(be,Oe),e(Oe,nr),g(Xt,nr,null),e(be,nl),e(be,rr),e(rr,rl),p(o,Ir,m),p(o,X,m),g(Gt,X,null),e(X,al),e(X,Jt),e(Jt,sl),e(Jt,Zt),e(Zt,il),e(Jt,dl),e(X,ll),e(X,ee),g(Yt,ee,null),e(ee,cl),e(ee,Re),e(Re,ml),e(Re,Lo),e(Lo,hl),e(Re,pl),e(Re,ar),e(ar,fl),e(Re,ul),e(ee,gl),g(We,ee,null),p(o,Dr,m),p(o,Te,m),e(Te,Ke),e(Ke,sr),g(eo,sr,null),e(Te,_l),e(Te,ir),e(ir,vl),p(o,Nr,m),p(o,G,m),g(to,G,null),e(G,kl),e(G,Be),e(Be,dr),e(dr,wl),e(Be,bl),e(Be,oo),e(oo,Rl),e(Be,Tl),e(G,yl),e(G,I),g(no,I,null),e(I,$l),e(I,ye),e(ye,El),e(ye,Mo),e(Mo,zl),e(ye,ql),e(ye,lr),e(lr,xl),e(ye,Al),e(I,Pl),g(Qe,I,null),e(I,jl),e(I,cr),e(cr,Ll),e(I,Ml),g(ro,I,null),Or=!0},p(o,[m]){const ao={};m&2&&(ao.$$scope={dirty:m,ctx:o}),Se.$set(ao);const mr={};m&2&&(mr.$$scope={dirty:m,ctx:o}),Ie.$set(mr);const hr={};m&2&&(hr.$$scope={dirty:m,ctx:o}),Ne.$set(hr);const pr={};m&2&&(pr.$$scope={dirty:m,ctx:o}),We.$set(pr);const so={};m&2&&(so.$$scope={dirty:m,ctx:o}),Qe.$set(so)},i(o){Or||(_(R.$$.fragment,o),_(Ue.$$.fragment,o),_(Ze.$$.fragment,o),_(Ye.$$.fragment,o),_(ot.$$.fragment,o),_(nt.$$.fragment,o),_(rt.$$.fragment,o),_(st.$$.fragment,o),_(dt.$$.fragment,o),_(ct.$$.fragment,o),_(mt.$$.fragment,o),_(ht.$$.fragment,o),_(_t.$$.fragment,o),_(vt.$$.fragment,o),_(kt.$$.fragment,o),_(Rt.$$.fragment,o),_(zt.$$.fragment,o),_(qt.$$.fragment,o),_(xt.$$.fragment,o),_(At.$$.fragment,o),_(Pt.$$.fragment,o),_(jt.$$.fragment,o),_(Ft.$$.fragment,o),_(Se.$$.fragment,o),_(St.$$.fragment,o),_(Ct.$$.fragment,o),_(It.$$.fragment,o),_(Ot.$$.fragment,o),_(Ie.$$.fragment,o),_(Wt.$$.fragment,o),_(Kt.$$.fragment,o),_(Bt.$$.fragment,o),_(Vt.$$.fragment,o),_(Ne.$$.fragment,o),_(Ut.$$.fragment,o),_(Xt.$$.fragment,o),_(Gt.$$.fragment,o),_(Yt.$$.fragment,o),_(We.$$.fragment,o),_(eo.$$.fragment,o),_(to.$$.fragment,o),_(no.$$.fragment,o),_(Qe.$$.fragment,o),_(ro.$$.fragment,o),Or=!0)},o(o){v(R.$$.fragment,o),v(Ue.$$.fragment,o),v(Ze.$$.fragment,o),v(Ye.$$.fragment,o),v(ot.$$.fragment,o),v(nt.$$.fragment,o),v(rt.$$.fragment,o),v(st.$$.fragment,o),v(dt.$$.fragment,o),v(ct.$$.fragment,o),v(mt.$$.fragment,o),v(ht.$$.fragment,o),v(_t.$$.fragment,o),v(vt.$$.fragment,o),v(kt.$$.fragment,o),v(Rt.$$.fragment,o),v(zt.$$.fragment,o),v(qt.$$.fragment,o),v(xt.$$.fragment,o),v(At.$$.fragment,o),v(Pt.$$.fragment,o),v(jt.$$.fragment,o),v(Ft.$$.fragment,o),v(Se.$$.fragment,o),v(St.$$.fragment,o),v(Ct.$$.fragment,o),v(It.$$.fragment,o),v(Ot.$$.fragment,o),v(Ie.$$.fragment,o),v(Wt.$$.fragment,o),v(Kt.$$.fragment,o),v(Bt.$$.fragment,o),v(Vt.$$.fragment,o),v(Ne.$$.fragment,o),v(Ut.$$.fragment,o),v(Xt.$$.fragment,o),v(Gt.$$.fragment,o),v(Yt.$$.fragment,o),v(We.$$.fragment,o),v(eo.$$.fragment,o),v(to.$$.fragment,o),v(no.$$.fragment,o),v(Qe.$$.fragment,o),v(ro.$$.fragment,o),Or=!1},d(o){t(h),o&&t(y),o&&t(w),k(R),o&&t(vr),o&&t(se),k(Ue),o&&t(kr),o&&t(Ee),o&&t(wr),o&&t(io),o&&t(br),o&&t(lo),o&&t(Rr),o&&t(Z),o&&t(Tr),o&&t(ie),k(Ze),o&&t(yr),o&&t(x),k(Ye),k(ot),o&&t($r),o&&t(le),k(nt),o&&t(Er),o&&t(E),k(rt),k(st),k(dt),k(ct),k(mt),k(ht),k(_t),o&&t(zr),o&&t(he),k(vt),o&&t(qr),o&&t(M),k(kt),k(Rt),k(zt),o&&t(xr),o&&t(fe),k(qt),o&&t(Ar),o&&t(Q),k(xt),k(At),o&&t(Pr),o&&t(ue),k(Pt),o&&t(jr),o&&t(H),k(jt),k(Ft),k(Se),k(St),o&&t(Lr),o&&t(_e),k(Ct),o&&t(Mr),o&&t(V),k(It),k(Ot),k(Ie),k(Wt),o&&t(Fr),o&&t(ke),k(Kt),o&&t(Sr),o&&t(U),k(Bt),k(Vt),k(Ne),k(Ut),o&&t(Cr),o&&t(be),k(Xt),o&&t(Ir),o&&t(X),k(Gt),k(Yt),k(We),o&&t(Dr),o&&t(Te),k(eo),o&&t(Nr),o&&t(G),k(to),k(no),k(Qe),k(ro)}}}const Vm={local:"realm",sections:[{local:"overview",title:"Overview"},{local:"transformers.RealmConfig",title:"RealmConfig"},{local:"transformers.RealmTokenizer",title:"RealmTokenizer"},{local:"transformers.RealmTokenizerFast",title:"RealmTokenizerFast"},{local:"transformers.RealmRetriever",title:"RealmRetriever"},{local:"transformers.RealmEmbedder",title:"RealmEmbedder"},{local:"transformers.RealmScorer",title:"RealmScorer"},{local:"transformers.RealmKnowledgeAugEncoder",title:"RealmKnowledgeAugEncoder"},{local:"transformers.RealmReader",title:"RealmReader"},{local:"transformers.RealmForOpenQA",title:"RealmForOpenQA"}],title:"REALM"};function Um(N,h,y){let{fw:w}=h;return N.$$set=T=>{"fw"in T&&y(0,w=T.fw)},[w]}class th extends Cm{constructor(h){super();Im(this,h,Um,Hm,Dm,{fw:0})}}export{th as default,Vm as metadata};
