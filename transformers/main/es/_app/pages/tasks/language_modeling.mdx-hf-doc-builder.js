import{S as fo,i as ho,s as go,e as i,k as f,w as D,t as l,M as _o,c as p,d as t,m as h,a as m,x as T,h as o,b as E,G as a,g as c,y as A,q as C,o as P,B as M,v as $o,L as uo}from"../../chunks/vendor-hf-doc-builder.js";import{T as Ta}from"../../chunks/Tip-hf-doc-builder.js";import{Y as es}from"../../chunks/Youtube-hf-doc-builder.js";import{I as wa}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as se}from"../../chunks/CodeBlock-hf-doc-builder.js";import{F as ul,M as Aa}from"../../chunks/Markdown-hf-doc-builder.js";import"../../chunks/IconTensorflow-hf-doc-builder.js";function jo(J){let s,j,r,g,w,$,b,q,v,y,F,N,G,z,R,S,H,I,U,_;return{c(){s=i("p"),j=l("Puedes realizar fine-tuning a otras arquitecturas para modelos de lenguaje como "),r=i("a"),g=l("GPT-Neo"),w=l(", "),$=i("a"),b=l("GPT-J"),q=l(" y "),v=i("a"),y=l("BERT"),F=l(" siguiendo los mismos pasos presentados en esta gu\xEDa!"),N=f(),G=i("p"),z=l("Mira la "),R=i("a"),S=l("p\xE1gina de tarea"),H=l(" para generaci\xF3n de texto y la "),I=i("a"),U=l("p\xE1gina de tarea"),_=l(" para modelos de lenguajes por enmascaramiento para obtener m\xE1s informaci\xF3n sobre los modelos, datasets, y m\xE9tricas asociadas."),this.h()},l(k){s=p(k,"P",{});var B=m(s);j=o(B,"Puedes realizar fine-tuning a otras arquitecturas para modelos de lenguaje como "),r=p(B,"A",{href:!0,rel:!0});var L=m(r);g=o(L,"GPT-Neo"),L.forEach(t),w=o(B,", "),$=p(B,"A",{href:!0,rel:!0});var W=m($);b=o(W,"GPT-J"),W.forEach(t),q=o(B," y "),v=p(B,"A",{href:!0,rel:!0});var V=m(v);y=o(V,"BERT"),V.forEach(t),F=o(B," siguiendo los mismos pasos presentados en esta gu\xEDa!"),B.forEach(t),N=h(k),G=p(k,"P",{});var ee=m(G);z=o(ee,"Mira la "),R=p(ee,"A",{href:!0,rel:!0});var ae=m(R);S=o(ae,"p\xE1gina de tarea"),ae.forEach(t),H=o(ee," para generaci\xF3n de texto y la "),I=p(ee,"A",{href:!0,rel:!0});var pe=m(I);U=o(pe,"p\xE1gina de tarea"),pe.forEach(t),_=o(ee," para modelos de lenguajes por enmascaramiento para obtener m\xE1s informaci\xF3n sobre los modelos, datasets, y m\xE9tricas asociadas."),ee.forEach(t),this.h()},h(){E(r,"href","https://huggingface.co/EleutherAI/gpt-neo-125M"),E(r,"rel","nofollow"),E($,"href","https://huggingface.co/EleutherAI/gpt-j-6B"),E($,"rel","nofollow"),E(v,"href","https://huggingface.co/bert-base-uncased"),E(v,"rel","nofollow"),E(R,"href","https://huggingface.co/tasks/text-generation"),E(R,"rel","nofollow"),E(I,"href","https://huggingface.co/tasks/fill-mask"),E(I,"rel","nofollow")},m(k,B){c(k,s,B),a(s,j),a(s,r),a(r,g),a(s,w),a(s,$),a($,b),a(s,q),a(s,v),a(v,y),a(s,F),c(k,N,B),c(k,G,B),a(G,z),a(G,R),a(R,S),a(G,H),a(G,I),a(I,U),a(G,_)},d(k){k&&t(s),k&&t(N),k&&t(G)}}}function bo(J){let s,j,r,g,w,$,b,q,v,y,F,N,G,z,R,S,H,I,U;return b=new se({props:{code:`from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.pad_token = tokenizer.eos_token
<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=<span class="hljs-literal">False</span>)`}}),I=new se({props:{code:`from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.pad_token = tokenizer.eos_token
<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=<span class="hljs-number">0.15</span>)`}}),{c(){s=i("p"),j=l("Puedes usar el token de final de secuencia como el token de relleno y asignar "),r=i("code"),g=l("mlm=False"),w=l(". Esto usar\xE1 los inputs como etiquetas movidas un elemento hacia la derecha:"),$=f(),D(b.$$.fragment),q=f(),v=i("p"),y=l("Para modelados de lenguaje por enmascaramiento usa el mismo "),F=i("code"),N=l("DataCollatorForLanguageModeling"),G=l(" excepto que deber\xE1s especificar "),z=i("code"),R=l("mlm_probability"),S=l(" para enmascarar tokens aleatoriamente cada vez que iteras sobre los datos."),H=f(),D(I.$$.fragment)},l(_){s=p(_,"P",{});var k=m(s);j=o(k,"Puedes usar el token de final de secuencia como el token de relleno y asignar "),r=p(k,"CODE",{});var B=m(r);g=o(B,"mlm=False"),B.forEach(t),w=o(k,". Esto usar\xE1 los inputs como etiquetas movidas un elemento hacia la derecha:"),k.forEach(t),$=h(_),T(b.$$.fragment,_),q=h(_),v=p(_,"P",{});var L=m(v);y=o(L,"Para modelados de lenguaje por enmascaramiento usa el mismo "),F=p(L,"CODE",{});var W=m(F);N=o(W,"DataCollatorForLanguageModeling"),W.forEach(t),G=o(L," excepto que deber\xE1s especificar "),z=p(L,"CODE",{});var V=m(z);R=o(V,"mlm_probability"),V.forEach(t),S=o(L," para enmascarar tokens aleatoriamente cada vez que iteras sobre los datos."),L.forEach(t),H=h(_),T(I.$$.fragment,_)},m(_,k){c(_,s,k),a(s,j),a(s,r),a(r,g),a(s,w),c(_,$,k),A(b,_,k),c(_,q,k),c(_,v,k),a(v,y),a(v,F),a(F,N),a(v,G),a(v,z),a(z,R),a(v,S),c(_,H,k),A(I,_,k),U=!0},p:uo,i(_){U||(C(b.$$.fragment,_),C(I.$$.fragment,_),U=!0)},o(_){P(b.$$.fragment,_),P(I.$$.fragment,_),U=!1},d(_){_&&t(s),_&&t($),M(b,_),_&&t(q),_&&t(v),_&&t(H),M(I,_)}}}function vo(J){let s,j;return s=new Aa({props:{$$slots:{default:[bo]},$$scope:{ctx:J}}}),{c(){D(s.$$.fragment)},l(r){T(s.$$.fragment,r)},m(r,g){A(s,r,g),j=!0},p(r,g){const w={};g&2&&(w.$$scope={dirty:g,ctx:r}),s.$set(w)},i(r){j||(C(s.$$.fragment,r),j=!0)},o(r){P(s.$$.fragment,r),j=!1},d(r){M(s,r)}}}function ko(J){let s,j,r,g,w,$,b,q,v,y,F,N,G,z,R,S,H,I,U;return b=new se({props:{code:`from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors="tf")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),I=new se({props:{code:`from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors="tf")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){s=i("p"),j=l("Puedes usar el token de final de secuencia como el token de relleno y asignar "),r=i("code"),g=l("mlm=False"),w=l(". Esto usar\xE1 los inputs como etiquetas movidas un elemento hacia la derecha:"),$=f(),D(b.$$.fragment),q=f(),v=i("p"),y=l("Para modelados de lenguajes por enmascaramiento usa el mismo "),F=i("code"),N=l("DataCollatorForLanguageModeling"),G=l(" excepto que deber\xE1s especificar "),z=i("code"),R=l("mlm_probability"),S=l(" para enmascarar tokens aleatoriamente cada vez que iteras sobre los datos."),H=f(),D(I.$$.fragment)},l(_){s=p(_,"P",{});var k=m(s);j=o(k,"Puedes usar el token de final de secuencia como el token de relleno y asignar "),r=p(k,"CODE",{});var B=m(r);g=o(B,"mlm=False"),B.forEach(t),w=o(k,". Esto usar\xE1 los inputs como etiquetas movidas un elemento hacia la derecha:"),k.forEach(t),$=h(_),T(b.$$.fragment,_),q=h(_),v=p(_,"P",{});var L=m(v);y=o(L,"Para modelados de lenguajes por enmascaramiento usa el mismo "),F=p(L,"CODE",{});var W=m(F);N=o(W,"DataCollatorForLanguageModeling"),W.forEach(t),G=o(L," excepto que deber\xE1s especificar "),z=p(L,"CODE",{});var V=m(z);R=o(V,"mlm_probability"),V.forEach(t),S=o(L," para enmascarar tokens aleatoriamente cada vez que iteras sobre los datos."),L.forEach(t),H=h(_),T(I.$$.fragment,_)},m(_,k){c(_,s,k),a(s,j),a(s,r),a(r,g),a(s,w),c(_,$,k),A(b,_,k),c(_,q,k),c(_,v,k),a(v,y),a(v,F),a(F,N),a(v,G),a(v,z),a(z,R),a(v,S),c(_,H,k),A(I,_,k),U=!0},p:uo,i(_){U||(C(b.$$.fragment,_),C(I.$$.fragment,_),U=!0)},o(_){P(b.$$.fragment,_),P(I.$$.fragment,_),U=!1},d(_){_&&t(s),_&&t($),M(b,_),_&&t(q),_&&t(v),_&&t(H),M(I,_)}}}function Eo(J){let s,j;return s=new Aa({props:{$$slots:{default:[ko]},$$scope:{ctx:J}}}),{c(){D(s.$$.fragment)},l(r){T(s.$$.fragment,r)},m(r,g){A(s,r,g),j=!0},p(r,g){const w={};g&2&&(w.$$scope={dirty:g,ctx:r}),s.$set(w)},i(r){j||(C(s.$$.fragment,r),j=!0)},o(r){P(s.$$.fragment,r),j=!1},d(r){M(s,r)}}}function wo(J){let s,j,r,g,w,$,b,q;return{c(){s=i("p"),j=l("Si no est\xE1s familiarizado con el proceso de realizar fine-tuning sobre un modelo con "),r=i("code"),g=l("Trainer"),w=l(", considera el tutorial b\xE1sico "),$=i("a"),b=l("aqu\xED"),q=l("!"),this.h()},l(v){s=p(v,"P",{});var y=m(s);j=o(y,"Si no est\xE1s familiarizado con el proceso de realizar fine-tuning sobre un modelo con "),r=p(y,"CODE",{});var F=m(r);g=o(F,"Trainer"),F.forEach(t),w=o(y,", considera el tutorial b\xE1sico "),$=p(y,"A",{href:!0});var N=m($);b=o(N,"aqu\xED"),N.forEach(t),q=o(y,"!"),y.forEach(t),this.h()},h(){E($,"href","../training#finetune-with-trainer")},m(v,y){c(v,s,y),a(s,j),a(s,r),a(r,g),a(s,w),a(s,$),a($,b),a(s,q)},d(v){v&&t(s)}}}function xo(J){let s,j,r,g,w,$,b,q,v,y,F,N,G,z,R,S,H,I,U,_,k,B,L,W,V,ee,ae,pe,K,ue,te,oe,Y,ne;return b=new se({props:{code:`from transformers import AutoModelForCausalLM, TrainingArguments, Trainer

model = AutoModelForCausalLM.from_pretrained("distilgpt2")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)`}}),v=new Ta({props:{$$slots:{default:[wo]},$$scope:{ctx:J}}}),Y=new se({props:{code:`training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_dataset["train"],
    eval_dataset=lm_dataset["test"],
    data_collator=data_collator,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=lm_dataset[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=lm_dataset[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),{c(){s=i("p"),j=l("Carga DistilGPT2 con "),r=i("code"),g=l("AutoModelForCausalLM"),w=l(":"),$=f(),D(b.$$.fragment),q=f(),D(v.$$.fragment),y=f(),F=i("p"),N=l("A este punto, solo faltan tres pasos:"),G=f(),z=i("ol"),R=i("li"),S=l("Definir tus hiperpar\xE1metros de entrenamiento en "),H=i("code"),I=l("TrainingArguments"),U=l("."),_=f(),k=i("li"),B=l("Pasarle los argumentos de entrenamiento a "),L=i("code"),W=l("Trainer"),V=l(" junto con el modelo, dataset, y el data collator."),ee=f(),ae=i("li"),pe=l("Realiza la llamada "),K=i("code"),ue=l("train()"),te=l(" para realizar el fine-tuning sobre tu modelo."),oe=f(),D(Y.$$.fragment)},l(u){s=p(u,"P",{});var O=m(s);j=o(O,"Carga DistilGPT2 con "),r=p(O,"CODE",{});var re=m(r);g=o(re,"AutoModelForCausalLM"),re.forEach(t),w=o(O,":"),O.forEach(t),$=h(u),T(b.$$.fragment,u),q=h(u),T(v.$$.fragment,u),y=h(u),F=p(u,"P",{});var le=m(F);N=o(le,"A este punto, solo faltan tres pasos:"),le.forEach(t),G=h(u),z=p(u,"OL",{});var X=m(z);R=p(X,"LI",{});var Z=m(R);S=o(Z,"Definir tus hiperpar\xE1metros de entrenamiento en "),H=p(Z,"CODE",{});var me=m(H);I=o(me,"TrainingArguments"),me.forEach(t),U=o(Z,"."),Z.forEach(t),_=h(X),k=p(X,"LI",{});var Q=m(k);B=o(Q,"Pasarle los argumentos de entrenamiento a "),L=p(Q,"CODE",{});var ce=m(L);W=o(ce,"Trainer"),ce.forEach(t),V=o(Q," junto con el modelo, dataset, y el data collator."),Q.forEach(t),ee=h(X),ae=p(X,"LI",{});var ie=m(ae);pe=o(ie,"Realiza la llamada "),K=p(ie,"CODE",{});var fe=m(K);ue=o(fe,"train()"),fe.forEach(t),te=o(ie," para realizar el fine-tuning sobre tu modelo."),ie.forEach(t),X.forEach(t),oe=h(u),T(Y.$$.fragment,u)},m(u,O){c(u,s,O),a(s,j),a(s,r),a(r,g),a(s,w),c(u,$,O),A(b,u,O),c(u,q,O),A(v,u,O),c(u,y,O),c(u,F,O),a(F,N),c(u,G,O),c(u,z,O),a(z,R),a(R,S),a(R,H),a(H,I),a(R,U),a(z,_),a(z,k),a(k,B),a(k,L),a(L,W),a(k,V),a(z,ee),a(z,ae),a(ae,pe),a(ae,K),a(K,ue),a(ae,te),c(u,oe,O),A(Y,u,O),ne=!0},p(u,O){const re={};O&2&&(re.$$scope={dirty:O,ctx:u}),v.$set(re)},i(u){ne||(C(b.$$.fragment,u),C(v.$$.fragment,u),C(Y.$$.fragment,u),ne=!0)},o(u){P(b.$$.fragment,u),P(v.$$.fragment,u),P(Y.$$.fragment,u),ne=!1},d(u){u&&t(s),u&&t($),M(b,u),u&&t(q),M(v,u),u&&t(y),u&&t(F),u&&t(G),u&&t(z),u&&t(oe),M(Y,u)}}}function yo(J){let s,j;return s=new Aa({props:{$$slots:{default:[xo]},$$scope:{ctx:J}}}),{c(){D(s.$$.fragment)},l(r){T(s.$$.fragment,r)},m(r,g){A(s,r,g),j=!0},p(r,g){const w={};g&2&&(w.$$scope={dirty:g,ctx:r}),s.$set(w)},i(r){j||(C(s.$$.fragment,r),j=!0)},o(r){P(s.$$.fragment,r),j=!1},d(r){M(s,r)}}}function zo(J){let s,j,r,g,w;return{c(){s=i("p"),j=l("Si no est\xE1s familiarizado con realizar fine-tuning de tus modelos con Keras, considera el tutorial b\xE1sico "),r=i("a"),g=l("aqu\xED"),w=l("!"),this.h()},l($){s=p($,"P",{});var b=m(s);j=o(b,"Si no est\xE1s familiarizado con realizar fine-tuning de tus modelos con Keras, considera el tutorial b\xE1sico "),r=p(b,"A",{href:!0});var q=m(r);g=o(q,"aqu\xED"),q.forEach(t),w=o(b,"!"),b.forEach(t),this.h()},h(){E(r,"href","training#finetune-with-keras")},m($,b){c($,s,b),a(s,j),a(s,r),a(r,g),a(s,w)},d($){$&&t(s)}}}function qo(J){let s,j,r,g,w,$,b,q,v,y,F,N,G,z,R,S,H,I,U,_,k,B,L,W,V,ee,ae,pe,K,ue,te,oe,Y,ne,u,O,re,le,X,Z,me,Q,ce,ie,fe,ge,de,_e;return z=new se({props:{code:`tf_train_set = lm_dataset["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    dummy_labels=True,
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_test_set = lm_dataset["test"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    dummy_labels=True,
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = lm_dataset[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    dummy_labels=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_test_set = lm_dataset[<span class="hljs-string">&quot;test&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    dummy_labels=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`}}),S=new Ta({props:{$$slots:{default:[zo]},$$scope:{ctx:J}}}),k=new se({props:{code:`from transformers import create_optimizer, AdamWeightDecay

optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer, AdamWeightDecay

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = AdamWeightDecay(learning_rate=<span class="hljs-number">2e-5</span>, weight_decay_rate=<span class="hljs-number">0.01</span>)`}}),K=new se({props:{code:`from transformers import TFAutoModelForCausalLM

model = TFAutoModelForCausalLM.from_pretrained("distilgpt2")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)`}}),le=new se({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),de=new se({props:{code:"model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=<span class="hljs-number">3</span>)'}}),{c(){s=i("p"),j=l("Para realizar el fine-tuning de un modelo en TensorFlow, comienza por convertir tus datasets al formato "),r=i("code"),g=l("tf.data.Dataset"),w=l(" con "),$=i("a"),b=i("code"),q=l("to_tf_dataset"),v=l(". Especifica los inputs y etiquetas en "),y=i("code"),F=l("columns"),N=l(", ya sea para mezclar el dataset, tama\xF1o de lote, y el data collator:"),G=f(),D(z.$$.fragment),R=f(),D(S.$$.fragment),H=f(),I=i("p"),U=l("Crea la funci\xF3n optimizadora, la tasa de aprendizaje, y algunos hiperpar\xE1metros de entrenamiento:"),_=f(),D(k.$$.fragment),B=f(),L=i("p"),W=l("Carga DistilGPT2 con "),V=i("code"),ee=l("TFAutoModelForCausalLM"),ae=l(":"),pe=f(),D(K.$$.fragment),ue=f(),te=i("p"),oe=l("Configura el modelo para entrenamiento con "),Y=i("a"),ne=i("code"),u=l("compile"),O=l(":"),re=f(),D(le.$$.fragment),X=f(),Z=i("p"),me=l("Llama a "),Q=i("a"),ce=i("code"),ie=l("fit"),fe=l(" para realizar el fine-tuning del modelo:"),ge=f(),D(de.$$.fragment),this.h()},l(n){s=p(n,"P",{});var x=m(s);j=o(x,"Para realizar el fine-tuning de un modelo en TensorFlow, comienza por convertir tus datasets al formato "),r=p(x,"CODE",{});var he=m(r);g=o(he,"tf.data.Dataset"),he.forEach(t),w=o(x," con "),$=p(x,"A",{href:!0,rel:!0});var Ee=m($);b=p(Ee,"CODE",{});var De=m(b);q=o(De,"to_tf_dataset"),De.forEach(t),Ee.forEach(t),v=o(x,". Especifica los inputs y etiquetas en "),y=p(x,"CODE",{});var ye=m(y);F=o(ye,"columns"),ye.forEach(t),N=o(x,", ya sea para mezclar el dataset, tama\xF1o de lote, y el data collator:"),x.forEach(t),G=h(n),T(z.$$.fragment,n),R=h(n),T(S.$$.fragment,n),H=h(n),I=p(n,"P",{});var Le=m(I);U=o(Le,"Crea la funci\xF3n optimizadora, la tasa de aprendizaje, y algunos hiperpar\xE1metros de entrenamiento:"),Le.forEach(t),_=h(n),T(k.$$.fragment,n),B=h(n),L=p(n,"P",{});var $e=m(L);W=o($e,"Carga DistilGPT2 con "),V=p($e,"CODE",{});var we=m(V);ee=o(we,"TFAutoModelForCausalLM"),we.forEach(t),ae=o($e,":"),$e.forEach(t),pe=h(n),T(K.$$.fragment,n),ue=h(n),te=p(n,"P",{});var je=m(te);oe=o(je,"Configura el modelo para entrenamiento con "),Y=p(je,"A",{href:!0,rel:!0});var ze=m(Y);ne=p(ze,"CODE",{});var Oe=m(ne);u=o(Oe,"compile"),Oe.forEach(t),ze.forEach(t),O=o(je,":"),je.forEach(t),re=h(n),T(le.$$.fragment,n),X=h(n),Z=p(n,"P",{});var be=m(Z);me=o(be,"Llama a "),Q=p(be,"A",{href:!0,rel:!0});var xe=m(Q);ce=p(xe,"CODE",{});var Te=m(ce);ie=o(Te,"fit"),Te.forEach(t),xe.forEach(t),fe=o(be," para realizar el fine-tuning del modelo:"),be.forEach(t),ge=h(n),T(de.$$.fragment,n),this.h()},h(){E($,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.to_tf_dataset"),E($,"rel","nofollow"),E(Y,"href","https://keras.io/api/models/model_training_apis/#compile-method"),E(Y,"rel","nofollow"),E(Q,"href","https://keras.io/api/models/model_training_apis/#fit-method"),E(Q,"rel","nofollow")},m(n,x){c(n,s,x),a(s,j),a(s,r),a(r,g),a(s,w),a(s,$),a($,b),a(b,q),a(s,v),a(s,y),a(y,F),a(s,N),c(n,G,x),A(z,n,x),c(n,R,x),A(S,n,x),c(n,H,x),c(n,I,x),a(I,U),c(n,_,x),A(k,n,x),c(n,B,x),c(n,L,x),a(L,W),a(L,V),a(V,ee),a(L,ae),c(n,pe,x),A(K,n,x),c(n,ue,x),c(n,te,x),a(te,oe),a(te,Y),a(Y,ne),a(ne,u),a(te,O),c(n,re,x),A(le,n,x),c(n,X,x),c(n,Z,x),a(Z,me),a(Z,Q),a(Q,ce),a(ce,ie),a(Z,fe),c(n,ge,x),A(de,n,x),_e=!0},p(n,x){const he={};x&2&&(he.$$scope={dirty:x,ctx:n}),S.$set(he)},i(n){_e||(C(z.$$.fragment,n),C(S.$$.fragment,n),C(k.$$.fragment,n),C(K.$$.fragment,n),C(le.$$.fragment,n),C(de.$$.fragment,n),_e=!0)},o(n){P(z.$$.fragment,n),P(S.$$.fragment,n),P(k.$$.fragment,n),P(K.$$.fragment,n),P(le.$$.fragment,n),P(de.$$.fragment,n),_e=!1},d(n){n&&t(s),n&&t(G),M(z,n),n&&t(R),M(S,n),n&&t(H),n&&t(I),n&&t(_),M(k,n),n&&t(B),n&&t(L),n&&t(pe),M(K,n),n&&t(ue),n&&t(te),n&&t(re),M(le,n),n&&t(X),n&&t(Z),n&&t(ge),M(de,n)}}}function Do(J){let s,j;return s=new Aa({props:{$$slots:{default:[qo]},$$scope:{ctx:J}}}),{c(){D(s.$$.fragment)},l(r){T(s.$$.fragment,r)},m(r,g){A(s,r,g),j=!0},p(r,g){const w={};g&2&&(w.$$scope={dirty:g,ctx:r}),s.$set(w)},i(r){j||(C(s.$$.fragment,r),j=!0)},o(r){P(s.$$.fragment,r),j=!1},d(r){M(s,r)}}}function To(J){let s,j,r,g,w,$,b,q;return{c(){s=i("p"),j=l("Si no est\xE1s familiarizado con el proceso de realizar fine-tuning sobre un modelo con "),r=i("code"),g=l("Trainer"),w=l(", considera el tutorial b\xE1sico "),$=i("a"),b=l("aqu\xED"),q=l("!"),this.h()},l(v){s=p(v,"P",{});var y=m(s);j=o(y,"Si no est\xE1s familiarizado con el proceso de realizar fine-tuning sobre un modelo con "),r=p(y,"CODE",{});var F=m(r);g=o(F,"Trainer"),F.forEach(t),w=o(y,", considera el tutorial b\xE1sico "),$=p(y,"A",{href:!0});var N=m($);b=o(N,"aqu\xED"),N.forEach(t),q=o(y,"!"),y.forEach(t),this.h()},h(){E($,"href","../training#finetune-with-trainer")},m(v,y){c(v,s,y),a(s,j),a(s,r),a(r,g),a(s,w),a(s,$),a($,b),a(s,q)},d(v){v&&t(s)}}}function Ao(J){let s,j,r,g,w,$,b,q,v,y,F,N,G,z,R,S,H,I,U,_,k,B,L,W,V,ee,ae,pe,K,ue,te,oe,Y,ne;return b=new se({props:{code:`from transformers import AutoModelForMaskedLM

model = AutoModelForMaskedLM.from_pretrained("distilroberta-base")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilroberta-base&quot;</span>)`}}),v=new Ta({props:{$$slots:{default:[To]},$$scope:{ctx:J}}}),Y=new se({props:{code:`training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=3,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_dataset["train"],
    eval_dataset=lm_dataset["test"],
    data_collator=data_collator,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=lm_dataset[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=lm_dataset[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),{c(){s=i("p"),j=l("Carga DistilRoBERTa con "),r=i("code"),g=l("AutoModelForMaskedlM"),w=l(":"),$=f(),D(b.$$.fragment),q=f(),D(v.$$.fragment),y=f(),F=i("p"),N=l("A este punto, solo faltan tres pasos:"),G=f(),z=i("ol"),R=i("li"),S=l("Definir tus hiperpar\xE1metros de entrenamiento en "),H=i("code"),I=l("TrainingArguments"),U=l("."),_=f(),k=i("li"),B=l("Pasarle los argumentos de entrenamiento a "),L=i("code"),W=l("Trainer"),V=l(" junto con el modelo, dataset, y el data collator."),ee=f(),ae=i("li"),pe=l("Realiza la llamada "),K=i("code"),ue=l("train()"),te=l(" para realizar el fine-tuning de tu modelo."),oe=f(),D(Y.$$.fragment)},l(u){s=p(u,"P",{});var O=m(s);j=o(O,"Carga DistilRoBERTa con "),r=p(O,"CODE",{});var re=m(r);g=o(re,"AutoModelForMaskedlM"),re.forEach(t),w=o(O,":"),O.forEach(t),$=h(u),T(b.$$.fragment,u),q=h(u),T(v.$$.fragment,u),y=h(u),F=p(u,"P",{});var le=m(F);N=o(le,"A este punto, solo faltan tres pasos:"),le.forEach(t),G=h(u),z=p(u,"OL",{});var X=m(z);R=p(X,"LI",{});var Z=m(R);S=o(Z,"Definir tus hiperpar\xE1metros de entrenamiento en "),H=p(Z,"CODE",{});var me=m(H);I=o(me,"TrainingArguments"),me.forEach(t),U=o(Z,"."),Z.forEach(t),_=h(X),k=p(X,"LI",{});var Q=m(k);B=o(Q,"Pasarle los argumentos de entrenamiento a "),L=p(Q,"CODE",{});var ce=m(L);W=o(ce,"Trainer"),ce.forEach(t),V=o(Q," junto con el modelo, dataset, y el data collator."),Q.forEach(t),ee=h(X),ae=p(X,"LI",{});var ie=m(ae);pe=o(ie,"Realiza la llamada "),K=p(ie,"CODE",{});var fe=m(K);ue=o(fe,"train()"),fe.forEach(t),te=o(ie," para realizar el fine-tuning de tu modelo."),ie.forEach(t),X.forEach(t),oe=h(u),T(Y.$$.fragment,u)},m(u,O){c(u,s,O),a(s,j),a(s,r),a(r,g),a(s,w),c(u,$,O),A(b,u,O),c(u,q,O),A(v,u,O),c(u,y,O),c(u,F,O),a(F,N),c(u,G,O),c(u,z,O),a(z,R),a(R,S),a(R,H),a(H,I),a(R,U),a(z,_),a(z,k),a(k,B),a(k,L),a(L,W),a(k,V),a(z,ee),a(z,ae),a(ae,pe),a(ae,K),a(K,ue),a(ae,te),c(u,oe,O),A(Y,u,O),ne=!0},p(u,O){const re={};O&2&&(re.$$scope={dirty:O,ctx:u}),v.$set(re)},i(u){ne||(C(b.$$.fragment,u),C(v.$$.fragment,u),C(Y.$$.fragment,u),ne=!0)},o(u){P(b.$$.fragment,u),P(v.$$.fragment,u),P(Y.$$.fragment,u),ne=!1},d(u){u&&t(s),u&&t($),M(b,u),u&&t(q),M(v,u),u&&t(y),u&&t(F),u&&t(G),u&&t(z),u&&t(oe),M(Y,u)}}}function Co(J){let s,j;return s=new Aa({props:{$$slots:{default:[Ao]},$$scope:{ctx:J}}}),{c(){D(s.$$.fragment)},l(r){T(s.$$.fragment,r)},m(r,g){A(s,r,g),j=!0},p(r,g){const w={};g&2&&(w.$$scope={dirty:g,ctx:r}),s.$set(w)},i(r){j||(C(s.$$.fragment,r),j=!0)},o(r){P(s.$$.fragment,r),j=!1},d(r){M(s,r)}}}function Po(J){let s,j,r,g,w;return{c(){s=i("p"),j=l("Si no est\xE1s familiarizado con realizar fine-tuning de tus modelos con Keras, considera el tutorial b\xE1sico "),r=i("a"),g=l("aqu\xED"),w=l("!"),this.h()},l($){s=p($,"P",{});var b=m(s);j=o(b,"Si no est\xE1s familiarizado con realizar fine-tuning de tus modelos con Keras, considera el tutorial b\xE1sico "),r=p(b,"A",{href:!0});var q=m(r);g=o(q,"aqu\xED"),q.forEach(t),w=o(b,"!"),b.forEach(t),this.h()},h(){E(r,"href","training#finetune-with-keras")},m($,b){c($,s,b),a(s,j),a(s,r),a(r,g),a(s,w)},d($){$&&t(s)}}}function Mo(J){let s,j,r,g,w,$,b,q,v,y,F,N,G,z,R,S,H,I,U,_,k,B,L,W,V,ee,ae,pe,K,ue,te,oe,Y,ne,u,O,re,le,X,Z,me,Q,ce,ie,fe,ge,de,_e;return z=new se({props:{code:`tf_train_set = lm_dataset["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    dummy_labels=True,
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_test_set = lm_dataset["test"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    dummy_labels=True,
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = lm_dataset[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    dummy_labels=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_test_set = lm_dataset[<span class="hljs-string">&quot;test&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    dummy_labels=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`}}),S=new Ta({props:{$$slots:{default:[Po]},$$scope:{ctx:J}}}),k=new se({props:{code:`from transformers import create_optimizer, AdamWeightDecay

optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer, AdamWeightDecay

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = AdamWeightDecay(learning_rate=<span class="hljs-number">2e-5</span>, weight_decay_rate=<span class="hljs-number">0.01</span>)`}}),K=new se({props:{code:`from transformers import TFAutoModelForMaskedLM

model = TFAutoModelForCausalLM.from_pretrained("distilroberta-base")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;distilroberta-base&quot;</span>)`}}),le=new se({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),de=new se({props:{code:"model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=<span class="hljs-number">3</span>)'}}),{c(){s=i("p"),j=l("Para realizar el fine-tuning de un modelo en TensorFlow, comienza por convertir tus datasets al formato "),r=i("code"),g=l("tf.data.Dataset"),w=l(" con "),$=i("a"),b=i("code"),q=l("to_tf_dataset"),v=l(". Especifica los inputs y etiquetas en "),y=i("code"),F=l("columns"),N=l(", ya sea para mezclar el dataset, tama\xF1o de lote, y el data collator:"),G=f(),D(z.$$.fragment),R=f(),D(S.$$.fragment),H=f(),I=i("p"),U=l("Crea la funci\xF3n optimizadora, la tasa de aprendizaje, y algunos hiperpar\xE1metros de entrenamiento:"),_=f(),D(k.$$.fragment),B=f(),L=i("p"),W=l("Carga DistilRoBERTa con "),V=i("code"),ee=l("TFAutoModelForMaskedLM"),ae=l(":"),pe=f(),D(K.$$.fragment),ue=f(),te=i("p"),oe=l("Configura el modelo para entrenamiento con "),Y=i("a"),ne=i("code"),u=l("compile"),O=l(":"),re=f(),D(le.$$.fragment),X=f(),Z=i("p"),me=l("Llama a "),Q=i("a"),ce=i("code"),ie=l("fit"),fe=l(" para realizar el fine-tuning del modelo:"),ge=f(),D(de.$$.fragment),this.h()},l(n){s=p(n,"P",{});var x=m(s);j=o(x,"Para realizar el fine-tuning de un modelo en TensorFlow, comienza por convertir tus datasets al formato "),r=p(x,"CODE",{});var he=m(r);g=o(he,"tf.data.Dataset"),he.forEach(t),w=o(x," con "),$=p(x,"A",{href:!0,rel:!0});var Ee=m($);b=p(Ee,"CODE",{});var De=m(b);q=o(De,"to_tf_dataset"),De.forEach(t),Ee.forEach(t),v=o(x,". Especifica los inputs y etiquetas en "),y=p(x,"CODE",{});var ye=m(y);F=o(ye,"columns"),ye.forEach(t),N=o(x,", ya sea para mezclar el dataset, tama\xF1o de lote, y el data collator:"),x.forEach(t),G=h(n),T(z.$$.fragment,n),R=h(n),T(S.$$.fragment,n),H=h(n),I=p(n,"P",{});var Le=m(I);U=o(Le,"Crea la funci\xF3n optimizadora, la tasa de aprendizaje, y algunos hiperpar\xE1metros de entrenamiento:"),Le.forEach(t),_=h(n),T(k.$$.fragment,n),B=h(n),L=p(n,"P",{});var $e=m(L);W=o($e,"Carga DistilRoBERTa con "),V=p($e,"CODE",{});var we=m(V);ee=o(we,"TFAutoModelForMaskedLM"),we.forEach(t),ae=o($e,":"),$e.forEach(t),pe=h(n),T(K.$$.fragment,n),ue=h(n),te=p(n,"P",{});var je=m(te);oe=o(je,"Configura el modelo para entrenamiento con "),Y=p(je,"A",{href:!0,rel:!0});var ze=m(Y);ne=p(ze,"CODE",{});var Oe=m(ne);u=o(Oe,"compile"),Oe.forEach(t),ze.forEach(t),O=o(je,":"),je.forEach(t),re=h(n),T(le.$$.fragment,n),X=h(n),Z=p(n,"P",{});var be=m(Z);me=o(be,"Llama a "),Q=p(be,"A",{href:!0,rel:!0});var xe=m(Q);ce=p(xe,"CODE",{});var Te=m(ce);ie=o(Te,"fit"),Te.forEach(t),xe.forEach(t),fe=o(be," para realizar el fine-tuning del modelo:"),be.forEach(t),ge=h(n),T(de.$$.fragment,n),this.h()},h(){E($,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.to_tf_dataset"),E($,"rel","nofollow"),E(Y,"href","https://keras.io/api/models/model_training_apis/#compile-method"),E(Y,"rel","nofollow"),E(Q,"href","https://keras.io/api/models/model_training_apis/#fit-method"),E(Q,"rel","nofollow")},m(n,x){c(n,s,x),a(s,j),a(s,r),a(r,g),a(s,w),a(s,$),a($,b),a(b,q),a(s,v),a(s,y),a(y,F),a(s,N),c(n,G,x),A(z,n,x),c(n,R,x),A(S,n,x),c(n,H,x),c(n,I,x),a(I,U),c(n,_,x),A(k,n,x),c(n,B,x),c(n,L,x),a(L,W),a(L,V),a(V,ee),a(L,ae),c(n,pe,x),A(K,n,x),c(n,ue,x),c(n,te,x),a(te,oe),a(te,Y),a(Y,ne),a(ne,u),a(te,O),c(n,re,x),A(le,n,x),c(n,X,x),c(n,Z,x),a(Z,me),a(Z,Q),a(Q,ce),a(ce,ie),a(Z,fe),c(n,ge,x),A(de,n,x),_e=!0},p(n,x){const he={};x&2&&(he.$$scope={dirty:x,ctx:n}),S.$set(he)},i(n){_e||(C(z.$$.fragment,n),C(S.$$.fragment,n),C(k.$$.fragment,n),C(K.$$.fragment,n),C(le.$$.fragment,n),C(de.$$.fragment,n),_e=!0)},o(n){P(z.$$.fragment,n),P(S.$$.fragment,n),P(k.$$.fragment,n),P(K.$$.fragment,n),P(le.$$.fragment,n),P(de.$$.fragment,n),_e=!1},d(n){n&&t(s),n&&t(G),M(z,n),n&&t(R),M(S,n),n&&t(H),n&&t(I),n&&t(_),M(k,n),n&&t(B),n&&t(L),n&&t(pe),M(K,n),n&&t(ue),n&&t(te),n&&t(re),M(le,n),n&&t(X),n&&t(Z),n&&t(ge),M(de,n)}}}function Fo(J){let s,j;return s=new Aa({props:{$$slots:{default:[Mo]},$$scope:{ctx:J}}}),{c(){D(s.$$.fragment)},l(r){T(s.$$.fragment,r)},m(r,g){A(s,r,g),j=!0},p(r,g){const w={};g&2&&(w.$$scope={dirty:g,ctx:r}),s.$set(w)},i(r){j||(C(s.$$.fragment,r),j=!0)},o(r){P(s.$$.fragment,r),j=!1},d(r){M(s,r)}}}function Lo(J){let s,j,r,g,w,$,b,q;return{c(){s=i("p"),j=l(`Para un ejemplo m\xE1s profundo sobre c\xF3mo realizar el fine-tuning sobre un modelo de lenguaje causal, considera
`),r=i("a"),g=l("PyTorch notebook"),w=l(`
o `),$=i("a"),b=l("TensorFlow notebook"),q=l("."),this.h()},l(v){s=p(v,"P",{});var y=m(s);j=o(y,`Para un ejemplo m\xE1s profundo sobre c\xF3mo realizar el fine-tuning sobre un modelo de lenguaje causal, considera
`),r=p(y,"A",{href:!0,rel:!0});var F=m(r);g=o(F,"PyTorch notebook"),F.forEach(t),w=o(y,`
o `),$=p(y,"A",{href:!0,rel:!0});var N=m($);b=o(N,"TensorFlow notebook"),N.forEach(t),q=o(y,"."),y.forEach(t),this.h()},h(){E(r,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb"),E(r,"rel","nofollow"),E($,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb"),E($,"rel","nofollow")},m(v,y){c(v,s,y),a(s,j),a(s,r),a(r,g),a(s,w),a(s,$),a($,b),a(s,q)},d(v){v&&t(s)}}}function Oo(J){let s,j,r,g,w,$,b,q,v,y,F,N,G,z,R,S,H,I,U,_,k,B,L,W,V,ee,ae,pe,K,ue,te,oe,Y,ne,u,O,re,le,X,Z,me,Q,ce,ie,fe,ge,de,_e,n,x,he,Ee,De,ye,Le,$e,we,je,ze,Oe,be,xe,Te,qe,as,Ca,ts,ss,Pa,ls,os,Ma,ns,rs,ct,Ie,Be,Fa,la,is,La,ps,ut,oa,dt,He,ms,Oa,cs,us,ft,na,ht,ra,gt,xa,ds,_t,ia,$t,Ae,fs,Ia,hs,gs,pa,Ra,_s,$s,jt,ma,bt,Ce,js,Sa,bs,vs,Ga,ks,Es,vt,ya,ws,kt,ca,Et,ve,xs,ua,Na,ys,zs,Ba,qs,Ds,Ha,Ts,As,Wa,Cs,Ps,wt,da,xt,za,Ms,yt,We,Ua,Fs,Ls,fa,Os,Ja,Is,Rs,zt,ha,qt,Ue,Ss,Ka,Gs,Ns,Dt,ga,Tt,ke,Bs,Ya,Hs,Ws,Qa,Us,Js,Va,Ks,Ys,Xa,Qs,Vs,At,Je,Ct,Re,Ke,Za,_a,Xs,et,Zs,Pt,Ye,el,$a,al,tl,Mt,Se,Qe,at,ja,sl,tt,ll,Ft,Ve,Lt,Ge,Xe,st,ba,ol,lt,nl,Ot,Ze,rl,va,il,pl,It,Ne,ea,ot,ka,ml,nt,cl,Rt,aa,St,ta,Gt;return $=new wa({}),z=new es({props:{id:"Vpjb1lu0MDk"}}),U=new es({props:{id:"mqElG5QJWUg"}}),X=new Ta({props:{$$slots:{default:[jo]},$$scope:{ctx:J}}}),ie=new wa({}),Ee=new se({props:{code:`from datasets import load_dataset

eli5 = load_dataset("eli5", split="train_asks[:5000]")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>eli5 = load_dataset(<span class="hljs-string">&quot;eli5&quot;</span>, split=<span class="hljs-string">&quot;train_asks[:5000]&quot;</span>)`}}),we=new se({props:{code:"eli5 = eli5.train_test_split(test_size=0.2)",highlighted:'eli5 = eli5.train_test_split(test_size=<span class="hljs-number">0.2</span>)'}}),xe=new se({props:{code:'eli5["train"][0]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>eli5[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;answers&#x27;</span>: {<span class="hljs-string">&#x27;a_id&#x27;</span>: [<span class="hljs-string">&#x27;c3d1aib&#x27;</span>, <span class="hljs-string">&#x27;c3d4lya&#x27;</span>],
  <span class="hljs-string">&#x27;score&#x27;</span>: [<span class="hljs-number">6</span>, <span class="hljs-number">3</span>],
  <span class="hljs-string">&#x27;text&#x27;</span>: [<span class="hljs-string">&quot;The velocity needed to remain in orbit is equal to the square root of Newton&#x27;s constant times the mass of earth divided by the distance from the center of the earth. I don&#x27;t know the altitude of that specific mission, but they&#x27;re usually around 300 km. That means he&#x27;s going 7-8 km/s.\\n\\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.&quot;</span>,
   <span class="hljs-string">&quot;Hope you don&#x27;t mind me asking another question, but why aren&#x27;t there any stars visible in this photo?&quot;</span>]},
 <span class="hljs-string">&#x27;answers_urls&#x27;</span>: {<span class="hljs-string">&#x27;url&#x27;</span>: []},
 <span class="hljs-string">&#x27;document&#x27;</span>: <span class="hljs-string">&#x27;&#x27;</span>,
 <span class="hljs-string">&#x27;q_id&#x27;</span>: <span class="hljs-string">&#x27;nyxfp&#x27;</span>,
 <span class="hljs-string">&#x27;selftext&#x27;</span>: <span class="hljs-string">&#x27;_URL_0_\\n\\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?&#x27;</span>,
 <span class="hljs-string">&#x27;selftext_urls&#x27;</span>: {<span class="hljs-string">&#x27;url&#x27;</span>: [<span class="hljs-string">&#x27;http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg&#x27;</span>]},
 <span class="hljs-string">&#x27;subreddit&#x27;</span>: <span class="hljs-string">&#x27;askscience&#x27;</span>,
 <span class="hljs-string">&#x27;title&#x27;</span>: <span class="hljs-string">&#x27;Few questions about this space walk photograph.&#x27;</span>,
 <span class="hljs-string">&#x27;title_urls&#x27;</span>: {<span class="hljs-string">&#x27;url&#x27;</span>: []}}`}}),la=new wa({}),oa=new es({props:{id:"ma1TrR7gE7I"}}),na=new se({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilgpt2")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)`}}),ra=new es({props:{id:"8PmhEIXhBvI"}}),ia=new se({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilroberta-base")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilroberta-base&quot;</span>)`}}),ma=new se({props:{code:`eli5 = eli5.flatten()
eli5["train"][0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>eli5 = eli5.flatten()
<span class="hljs-meta">&gt;&gt;&gt; </span>eli5[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;answers.a_id&#x27;</span>: [<span class="hljs-string">&#x27;c3d1aib&#x27;</span>, <span class="hljs-string">&#x27;c3d4lya&#x27;</span>],
 <span class="hljs-string">&#x27;answers.score&#x27;</span>: [<span class="hljs-number">6</span>, <span class="hljs-number">3</span>],
 <span class="hljs-string">&#x27;answers.text&#x27;</span>: [<span class="hljs-string">&quot;The velocity needed to remain in orbit is equal to the square root of Newton&#x27;s constant times the mass of earth divided by the distance from the center of the earth. I don&#x27;t know the altitude of that specific mission, but they&#x27;re usually around 300 km. That means he&#x27;s going 7-8 km/s.\\n\\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.&quot;</span>,
  <span class="hljs-string">&quot;Hope you don&#x27;t mind me asking another question, but why aren&#x27;t there any stars visible in this photo?&quot;</span>],
 <span class="hljs-string">&#x27;answers_urls.url&#x27;</span>: [],
 <span class="hljs-string">&#x27;document&#x27;</span>: <span class="hljs-string">&#x27;&#x27;</span>,
 <span class="hljs-string">&#x27;q_id&#x27;</span>: <span class="hljs-string">&#x27;nyxfp&#x27;</span>,
 <span class="hljs-string">&#x27;selftext&#x27;</span>: <span class="hljs-string">&#x27;_URL_0_\\n\\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?&#x27;</span>,
 <span class="hljs-string">&#x27;selftext_urls.url&#x27;</span>: [<span class="hljs-string">&#x27;http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg&#x27;</span>],
 <span class="hljs-string">&#x27;subreddit&#x27;</span>: <span class="hljs-string">&#x27;askscience&#x27;</span>,
 <span class="hljs-string">&#x27;title&#x27;</span>: <span class="hljs-string">&#x27;Few questions about this space walk photograph.&#x27;</span>,
 <span class="hljs-string">&#x27;title_urls.url&#x27;</span>: []}`}}),ca=new se({props:{code:`def preprocess_function(examples):
    return tokenizer([" ".join(x) for x in examples["answers.text"]], truncation=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer([<span class="hljs-string">&quot; &quot;</span>.join(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;answers.text&quot;</span>]], truncation=<span class="hljs-literal">True</span>)`}}),da=new se({props:{code:`tokenized_eli5 = eli5.map(
    preprocess_function,
    batched=True,
    num_proc=4,
    remove_columns=eli5["train"].column_names,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_eli5 = eli5.<span class="hljs-built_in">map</span>(
<span class="hljs-meta">... </span>    preprocess_function,
<span class="hljs-meta">... </span>    batched=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    num_proc=<span class="hljs-number">4</span>,
<span class="hljs-meta">... </span>    remove_columns=eli5[<span class="hljs-string">&quot;train&quot;</span>].column_names,
<span class="hljs-meta">... </span>)`}}),ha=new se({props:{code:`block_size = 128


def group_texts(examples):
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>block_size = <span class="hljs-number">128</span>


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">group_texts</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    concatenated_examples = {k: <span class="hljs-built_in">sum</span>(examples[k], []) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> examples.keys()}
<span class="hljs-meta">... </span>    total_length = <span class="hljs-built_in">len</span>(concatenated_examples[<span class="hljs-built_in">list</span>(examples.keys())[<span class="hljs-number">0</span>]])
<span class="hljs-meta">... </span>    result = {
<span class="hljs-meta">... </span>        k: [t[i : i + block_size] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, total_length, block_size)]
<span class="hljs-meta">... </span>        <span class="hljs-keyword">for</span> k, t <span class="hljs-keyword">in</span> concatenated_examples.items()
<span class="hljs-meta">... </span>    }
<span class="hljs-meta">... </span>    result[<span class="hljs-string">&quot;labels&quot;</span>] = result[<span class="hljs-string">&quot;input_ids&quot;</span>].copy()
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> result`}}),ga=new se({props:{code:"lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>lm_dataset = tokenized_eli5.<span class="hljs-built_in">map</span>(group_texts, batched=<span class="hljs-literal">True</span>, num_proc=<span class="hljs-number">4</span>)'}}),Je=new ul({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Eo],pytorch:[vo]},$$scope:{ctx:J}}}),_a=new wa({}),ja=new wa({}),Ve=new ul({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Do],pytorch:[yo]},$$scope:{ctx:J}}}),ba=new wa({}),ka=new wa({}),aa=new ul({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Fo],pytorch:[Co]},$$scope:{ctx:J}}}),ta=new Ta({props:{$$slots:{default:[Lo]},$$scope:{ctx:J}}}),{c(){s=i("meta"),j=f(),r=i("h1"),g=i("a"),w=i("span"),D($.$$.fragment),b=f(),q=i("span"),v=l("Modelado de lenguaje"),y=f(),F=i("p"),N=l("El modelado de lenguaje predice palabras en un enunciado. Hay dos formas de modelado de lenguaje."),G=f(),D(z.$$.fragment),R=f(),S=i("p"),H=l("El modelado de lenguaje causal predice el siguiente token en una secuencia de tokens, y el modelo solo puede considerar los tokens a la izquierda."),I=f(),D(U.$$.fragment),_=f(),k=i("p"),B=l("El modelado de lenguaje por enmascaramiento predice un token enmascarado en una secuencia, y el modelo puede considerar los tokens bidireccionalmente."),L=f(),W=i("p"),V=l("Esta gu\xEDa te mostrar\xE1 c\xF3mo realizar fine-tuning "),ee=i("a"),ae=l("DistilGPT2"),pe=l(" para modelos de lenguaje causales y "),K=i("a"),ue=l("DistilRoBERTa"),te=l(" para modelos de lenguaje por enmascaramiento en el "),oe=i("a"),Y=l("r/askscience"),ne=l(" subdataset "),u=i("a"),O=l("ELI5"),re=l("."),le=f(),D(X.$$.fragment),Z=f(),me=i("h2"),Q=i("a"),ce=i("span"),D(ie.$$.fragment),fe=f(),ge=i("span"),de=l("Carga el dataset ELI5"),_e=f(),n=i("p"),x=l("Carga solo los primeros 5000 registros desde la biblioteca \u{1F917} Datasets, dado que es bastante grande:"),he=f(),D(Ee.$$.fragment),De=f(),ye=i("p"),Le=l("Divide este dataset en subdatasets para el entrenamiento y el test:"),$e=f(),D(we.$$.fragment),je=f(),ze=i("p"),Oe=l("Luego observa un ejemplo:"),be=f(),D(xe.$$.fragment),Te=f(),qe=i("p"),as=l("Observa que "),Ca=i("code"),ts=l("text"),ss=l(" es un subcampo anidado dentro del diccionario "),Pa=i("code"),ls=l("answers"),os=l(". Cuando preproceses el dataset, deber\xE1s extraer el subcampo "),Ma=i("code"),ns=l("text"),rs=l(" en una columna aparte."),ct=f(),Ie=i("h2"),Be=i("a"),Fa=i("span"),D(la.$$.fragment),is=f(),La=i("span"),ps=l("Preprocesamiento"),ut=f(),D(oa.$$.fragment),dt=f(),He=i("p"),ms=l("Para modelados de lenguaje causales carga el tokenizador DistilGPT2 para procesar el subcampo "),Oa=i("code"),cs=l("text"),us=l(":"),ft=f(),D(na.$$.fragment),ht=f(),D(ra.$$.fragment),gt=f(),xa=i("p"),ds=l("Para modelados de lenguaje por enmascaramiento carga el tokenizador DistilRoBERTa, en lugar de DistilGPT2:"),_t=f(),D(ia.$$.fragment),$t=f(),Ae=i("p"),fs=l("Extrae el subcampo "),Ia=i("code"),hs=l("text"),gs=l(" desde su estructura anidado con el m\xE9todo "),pa=i("a"),Ra=i("code"),_s=l("flatten"),$s=l(":"),jt=f(),D(ma.$$.fragment),bt=f(),Ce=i("p"),js=l("Cada subcampo es ahora una columna separada, como lo indica el prefijo "),Sa=i("code"),bs=l("answers"),vs=l(". Observa que "),Ga=i("code"),ks=l("answers.text"),Es=l(" es una lista. En lugar de tokenizar cada enunciado por separado, convierte la lista en un string para tokenizarlos conjuntamente."),vt=f(),ya=i("p"),ws=l("As\xED es como puedes crear una funci\xF3n de preprocesamiento para convertir la lista en una cadena y truncar las secuencias para que no superen la longitud m\xE1xima de input de DistilGPT2:"),kt=f(),D(ca.$$.fragment),Et=f(),ve=i("p"),xs=l("Usa de \u{1F917} Datasets la funci\xF3n "),ua=i("a"),Na=i("code"),ys=l("map"),zs=l(" para aplicar la funci\xF3n de preprocesamiento sobre el dataset en su totalidad. Puedes acelerar la funci\xF3n "),Ba=i("code"),qs=l("map"),Ds=l(" configurando el argumento "),Ha=i("code"),Ts=l("batched=True"),As=l(" para procesar m\xFAltiples elementos del dataset a la vez y aumentar la cantidad de procesos con "),Wa=i("code"),Cs=l("num_proc"),Ps=l(". Elimina las columnas que no necesitas:"),wt=f(),D(da.$$.fragment),xt=f(),za=i("p"),Ms=l("Ahora necesitas una segunda funci\xF3n de preprocesamiento para capturar el texto truncado de cualquier ejemplo demasiado largo para evitar cualquier p\xE9rdida de informaci\xF3n. Esta funci\xF3n de preprocesamiento deber\xEDa:"),yt=f(),We=i("ul"),Ua=i("li"),Fs=l("Concatenar todo el texto."),Ls=f(),fa=i("li"),Os=l("Dividir el texto concatenado en trozos m\xE1s peque\xF1os definidos por un "),Ja=i("code"),Is=l("block_size"),Rs=l("."),zt=f(),D(ha.$$.fragment),qt=f(),Ue=i("p"),Ss=l("Aplica la funci\xF3n "),Ka=i("code"),Gs=l("group_texts"),Ns=l(" sobre todo el dataset:"),Dt=f(),D(ga.$$.fragment),Tt=f(),ke=i("p"),Bs=l("Para modelados de lenguaje causales, usa "),Ya=i("code"),Hs=l("DataCollatorForLanguageModeling"),Ws=l(" para crear un lote de ejemplos. Esto tambi\xE9n "),Qa=i("em"),Us=l("rellenar\xE1 din\xE1micamente"),Js=l(" tu texto a la dimensi\xF3n del elemento m\xE1s largo del lote para que de esta manera tengan largo uniforme. Si bien es posible rellenar tu texto en la funci\xF3n "),Va=i("code"),Ks=l("tokenizer"),Ys=l(" mediante el argumento "),Xa=i("code"),Qs=l("padding=True"),Vs=l(", el rellenado din\xE1mico es m\xE1s eficiente."),At=f(),D(Je.$$.fragment),Ct=f(),Re=i("h2"),Ke=i("a"),Za=i("span"),D(_a.$$.fragment),Xs=f(),et=i("span"),Zs=l("Modelado de lenguaje causal"),Pt=f(),Ye=i("p"),el=l("El modelado de lenguaje causal es frecuentemente utilizado para generaci\xF3n de texto. Esta secci\xF3n te muestra c\xF3mo realizar fine-tuning a "),$a=i("a"),al=l("DistilGPT2"),tl=l(" para generar nuevo texto."),Mt=f(),Se=i("h3"),Qe=i("a"),at=i("span"),D(ja.$$.fragment),sl=f(),tt=i("span"),ll=l("Entrenamiento"),Ft=f(),D(Ve.$$.fragment),Lt=f(),Ge=i("h2"),Xe=i("a"),st=i("span"),D(ba.$$.fragment),ol=f(),lt=i("span"),nl=l("Modelado de lenguaje por enmascaramiento"),Ot=f(),Ze=i("p"),rl=l("El modelado de lenguaje por enmascaramiento es tambi\xE9n conocido como una tarea de rellenar la m\xE1scara, pues predice un token enmascarado dada una secuencia. Los modelos de lenguaje por enmascaramiento requieren una buena comprensi\xF3n del contexto de una secuencia entera, en lugar de solo el contexto a la izquierda. Esta secci\xF3n te ense\xF1a como realizar el fine-tuning de "),va=i("a"),il=l("DistilRoBERTa"),pl=l(" para predecir una palabra enmascarada."),It=f(),Ne=i("h3"),ea=i("a"),ot=i("span"),D(ka.$$.fragment),ml=f(),nt=i("span"),cl=l("Entrenamiento"),Rt=f(),D(aa.$$.fragment),St=f(),D(ta.$$.fragment),this.h()},l(e){const d=_o('[data-svelte="svelte-1phssyn"]',document.head);s=p(d,"META",{name:!0,content:!0}),d.forEach(t),j=h(e),r=p(e,"H1",{class:!0});var Ea=m(r);g=p(Ea,"A",{id:!0,class:!0,href:!0});var rt=m(g);w=p(rt,"SPAN",{});var it=m(w);T($.$$.fragment,it),it.forEach(t),rt.forEach(t),b=h(Ea),q=p(Ea,"SPAN",{});var pt=m(q);v=o(pt,"Modelado de lenguaje"),pt.forEach(t),Ea.forEach(t),y=h(e),F=p(e,"P",{});var mt=m(F);N=o(mt,"El modelado de lenguaje predice palabras en un enunciado. Hay dos formas de modelado de lenguaje."),mt.forEach(t),G=h(e),T(z.$$.fragment,e),R=h(e),S=p(e,"P",{});var dl=m(S);H=o(dl,"El modelado de lenguaje causal predice el siguiente token en una secuencia de tokens, y el modelo solo puede considerar los tokens a la izquierda."),dl.forEach(t),I=h(e),T(U.$$.fragment,e),_=h(e),k=p(e,"P",{});var fl=m(k);B=o(fl,"El modelado de lenguaje por enmascaramiento predice un token enmascarado en una secuencia, y el modelo puede considerar los tokens bidireccionalmente."),fl.forEach(t),L=h(e),W=p(e,"P",{});var Pe=m(W);V=o(Pe,"Esta gu\xEDa te mostrar\xE1 c\xF3mo realizar fine-tuning "),ee=p(Pe,"A",{href:!0,rel:!0});var hl=m(ee);ae=o(hl,"DistilGPT2"),hl.forEach(t),pe=o(Pe," para modelos de lenguaje causales y "),K=p(Pe,"A",{href:!0,rel:!0});var gl=m(K);ue=o(gl,"DistilRoBERTa"),gl.forEach(t),te=o(Pe," para modelos de lenguaje por enmascaramiento en el "),oe=p(Pe,"A",{href:!0,rel:!0});var _l=m(oe);Y=o(_l,"r/askscience"),_l.forEach(t),ne=o(Pe," subdataset "),u=p(Pe,"A",{href:!0,rel:!0});var $l=m(u);O=o($l,"ELI5"),$l.forEach(t),re=o(Pe,"."),Pe.forEach(t),le=h(e),T(X.$$.fragment,e),Z=h(e),me=p(e,"H2",{class:!0});var Nt=m(me);Q=p(Nt,"A",{id:!0,class:!0,href:!0});var jl=m(Q);ce=p(jl,"SPAN",{});var bl=m(ce);T(ie.$$.fragment,bl),bl.forEach(t),jl.forEach(t),fe=h(Nt),ge=p(Nt,"SPAN",{});var vl=m(ge);de=o(vl,"Carga el dataset ELI5"),vl.forEach(t),Nt.forEach(t),_e=h(e),n=p(e,"P",{});var kl=m(n);x=o(kl,"Carga solo los primeros 5000 registros desde la biblioteca \u{1F917} Datasets, dado que es bastante grande:"),kl.forEach(t),he=h(e),T(Ee.$$.fragment,e),De=h(e),ye=p(e,"P",{});var El=m(ye);Le=o(El,"Divide este dataset en subdatasets para el entrenamiento y el test:"),El.forEach(t),$e=h(e),T(we.$$.fragment,e),je=h(e),ze=p(e,"P",{});var wl=m(ze);Oe=o(wl,"Luego observa un ejemplo:"),wl.forEach(t),be=h(e),T(xe.$$.fragment,e),Te=h(e),qe=p(e,"P",{});var sa=m(qe);as=o(sa,"Observa que "),Ca=p(sa,"CODE",{});var xl=m(Ca);ts=o(xl,"text"),xl.forEach(t),ss=o(sa," es un subcampo anidado dentro del diccionario "),Pa=p(sa,"CODE",{});var yl=m(Pa);ls=o(yl,"answers"),yl.forEach(t),os=o(sa,". Cuando preproceses el dataset, deber\xE1s extraer el subcampo "),Ma=p(sa,"CODE",{});var zl=m(Ma);ns=o(zl,"text"),zl.forEach(t),rs=o(sa," en una columna aparte."),sa.forEach(t),ct=h(e),Ie=p(e,"H2",{class:!0});var Bt=m(Ie);Be=p(Bt,"A",{id:!0,class:!0,href:!0});var ql=m(Be);Fa=p(ql,"SPAN",{});var Dl=m(Fa);T(la.$$.fragment,Dl),Dl.forEach(t),ql.forEach(t),is=h(Bt),La=p(Bt,"SPAN",{});var Tl=m(La);ps=o(Tl,"Preprocesamiento"),Tl.forEach(t),Bt.forEach(t),ut=h(e),T(oa.$$.fragment,e),dt=h(e),He=p(e,"P",{});var Ht=m(He);ms=o(Ht,"Para modelados de lenguaje causales carga el tokenizador DistilGPT2 para procesar el subcampo "),Oa=p(Ht,"CODE",{});var Al=m(Oa);cs=o(Al,"text"),Al.forEach(t),us=o(Ht,":"),Ht.forEach(t),ft=h(e),T(na.$$.fragment,e),ht=h(e),T(ra.$$.fragment,e),gt=h(e),xa=p(e,"P",{});var Cl=m(xa);ds=o(Cl,"Para modelados de lenguaje por enmascaramiento carga el tokenizador DistilRoBERTa, en lugar de DistilGPT2:"),Cl.forEach(t),_t=h(e),T(ia.$$.fragment,e),$t=h(e),Ae=p(e,"P",{});var qa=m(Ae);fs=o(qa,"Extrae el subcampo "),Ia=p(qa,"CODE",{});var Pl=m(Ia);hs=o(Pl,"text"),Pl.forEach(t),gs=o(qa," desde su estructura anidado con el m\xE9todo "),pa=p(qa,"A",{href:!0,rel:!0});var Ml=m(pa);Ra=p(Ml,"CODE",{});var Fl=m(Ra);_s=o(Fl,"flatten"),Fl.forEach(t),Ml.forEach(t),$s=o(qa,":"),qa.forEach(t),jt=h(e),T(ma.$$.fragment,e),bt=h(e),Ce=p(e,"P",{});var Da=m(Ce);js=o(Da,"Cada subcampo es ahora una columna separada, como lo indica el prefijo "),Sa=p(Da,"CODE",{});var Ll=m(Sa);bs=o(Ll,"answers"),Ll.forEach(t),vs=o(Da,". Observa que "),Ga=p(Da,"CODE",{});var Ol=m(Ga);ks=o(Ol,"answers.text"),Ol.forEach(t),Es=o(Da," es una lista. En lugar de tokenizar cada enunciado por separado, convierte la lista en un string para tokenizarlos conjuntamente."),Da.forEach(t),vt=h(e),ya=p(e,"P",{});var Il=m(ya);ws=o(Il,"As\xED es como puedes crear una funci\xF3n de preprocesamiento para convertir la lista en una cadena y truncar las secuencias para que no superen la longitud m\xE1xima de input de DistilGPT2:"),Il.forEach(t),kt=h(e),T(ca.$$.fragment,e),Et=h(e),ve=p(e,"P",{});var Me=m(ve);xs=o(Me,"Usa de \u{1F917} Datasets la funci\xF3n "),ua=p(Me,"A",{href:!0,rel:!0});var Rl=m(ua);Na=p(Rl,"CODE",{});var Sl=m(Na);ys=o(Sl,"map"),Sl.forEach(t),Rl.forEach(t),zs=o(Me," para aplicar la funci\xF3n de preprocesamiento sobre el dataset en su totalidad. Puedes acelerar la funci\xF3n "),Ba=p(Me,"CODE",{});var Gl=m(Ba);qs=o(Gl,"map"),Gl.forEach(t),Ds=o(Me," configurando el argumento "),Ha=p(Me,"CODE",{});var Nl=m(Ha);Ts=o(Nl,"batched=True"),Nl.forEach(t),As=o(Me," para procesar m\xFAltiples elementos del dataset a la vez y aumentar la cantidad de procesos con "),Wa=p(Me,"CODE",{});var Bl=m(Wa);Cs=o(Bl,"num_proc"),Bl.forEach(t),Ps=o(Me,". Elimina las columnas que no necesitas:"),Me.forEach(t),wt=h(e),T(da.$$.fragment,e),xt=h(e),za=p(e,"P",{});var Hl=m(za);Ms=o(Hl,"Ahora necesitas una segunda funci\xF3n de preprocesamiento para capturar el texto truncado de cualquier ejemplo demasiado largo para evitar cualquier p\xE9rdida de informaci\xF3n. Esta funci\xF3n de preprocesamiento deber\xEDa:"),Hl.forEach(t),yt=h(e),We=p(e,"UL",{});var Wt=m(We);Ua=p(Wt,"LI",{});var Wl=m(Ua);Fs=o(Wl,"Concatenar todo el texto."),Wl.forEach(t),Ls=h(Wt),fa=p(Wt,"LI",{});var Ut=m(fa);Os=o(Ut,"Dividir el texto concatenado en trozos m\xE1s peque\xF1os definidos por un "),Ja=p(Ut,"CODE",{});var Ul=m(Ja);Is=o(Ul,"block_size"),Ul.forEach(t),Rs=o(Ut,"."),Ut.forEach(t),Wt.forEach(t),zt=h(e),T(ha.$$.fragment,e),qt=h(e),Ue=p(e,"P",{});var Jt=m(Ue);Ss=o(Jt,"Aplica la funci\xF3n "),Ka=p(Jt,"CODE",{});var Jl=m(Ka);Gs=o(Jl,"group_texts"),Jl.forEach(t),Ns=o(Jt," sobre todo el dataset:"),Jt.forEach(t),Dt=h(e),T(ga.$$.fragment,e),Tt=h(e),ke=p(e,"P",{});var Fe=m(ke);Bs=o(Fe,"Para modelados de lenguaje causales, usa "),Ya=p(Fe,"CODE",{});var Kl=m(Ya);Hs=o(Kl,"DataCollatorForLanguageModeling"),Kl.forEach(t),Ws=o(Fe," para crear un lote de ejemplos. Esto tambi\xE9n "),Qa=p(Fe,"EM",{});var Yl=m(Qa);Us=o(Yl,"rellenar\xE1 din\xE1micamente"),Yl.forEach(t),Js=o(Fe," tu texto a la dimensi\xF3n del elemento m\xE1s largo del lote para que de esta manera tengan largo uniforme. Si bien es posible rellenar tu texto en la funci\xF3n "),Va=p(Fe,"CODE",{});var Ql=m(Va);Ks=o(Ql,"tokenizer"),Ql.forEach(t),Ys=o(Fe," mediante el argumento "),Xa=p(Fe,"CODE",{});var Vl=m(Xa);Qs=o(Vl,"padding=True"),Vl.forEach(t),Vs=o(Fe,", el rellenado din\xE1mico es m\xE1s eficiente."),Fe.forEach(t),At=h(e),T(Je.$$.fragment,e),Ct=h(e),Re=p(e,"H2",{class:!0});var Kt=m(Re);Ke=p(Kt,"A",{id:!0,class:!0,href:!0});var Xl=m(Ke);Za=p(Xl,"SPAN",{});var Zl=m(Za);T(_a.$$.fragment,Zl),Zl.forEach(t),Xl.forEach(t),Xs=h(Kt),et=p(Kt,"SPAN",{});var eo=m(et);Zs=o(eo,"Modelado de lenguaje causal"),eo.forEach(t),Kt.forEach(t),Pt=h(e),Ye=p(e,"P",{});var Yt=m(Ye);el=o(Yt,"El modelado de lenguaje causal es frecuentemente utilizado para generaci\xF3n de texto. Esta secci\xF3n te muestra c\xF3mo realizar fine-tuning a "),$a=p(Yt,"A",{href:!0,rel:!0});var ao=m($a);al=o(ao,"DistilGPT2"),ao.forEach(t),tl=o(Yt," para generar nuevo texto."),Yt.forEach(t),Mt=h(e),Se=p(e,"H3",{class:!0});var Qt=m(Se);Qe=p(Qt,"A",{id:!0,class:!0,href:!0});var to=m(Qe);at=p(to,"SPAN",{});var so=m(at);T(ja.$$.fragment,so),so.forEach(t),to.forEach(t),sl=h(Qt),tt=p(Qt,"SPAN",{});var lo=m(tt);ll=o(lo,"Entrenamiento"),lo.forEach(t),Qt.forEach(t),Ft=h(e),T(Ve.$$.fragment,e),Lt=h(e),Ge=p(e,"H2",{class:!0});var Vt=m(Ge);Xe=p(Vt,"A",{id:!0,class:!0,href:!0});var oo=m(Xe);st=p(oo,"SPAN",{});var no=m(st);T(ba.$$.fragment,no),no.forEach(t),oo.forEach(t),ol=h(Vt),lt=p(Vt,"SPAN",{});var ro=m(lt);nl=o(ro,"Modelado de lenguaje por enmascaramiento"),ro.forEach(t),Vt.forEach(t),Ot=h(e),Ze=p(e,"P",{});var Xt=m(Ze);rl=o(Xt,"El modelado de lenguaje por enmascaramiento es tambi\xE9n conocido como una tarea de rellenar la m\xE1scara, pues predice un token enmascarado dada una secuencia. Los modelos de lenguaje por enmascaramiento requieren una buena comprensi\xF3n del contexto de una secuencia entera, en lugar de solo el contexto a la izquierda. Esta secci\xF3n te ense\xF1a como realizar el fine-tuning de "),va=p(Xt,"A",{href:!0,rel:!0});var io=m(va);il=o(io,"DistilRoBERTa"),io.forEach(t),pl=o(Xt," para predecir una palabra enmascarada."),Xt.forEach(t),It=h(e),Ne=p(e,"H3",{class:!0});var Zt=m(Ne);ea=p(Zt,"A",{id:!0,class:!0,href:!0});var po=m(ea);ot=p(po,"SPAN",{});var mo=m(ot);T(ka.$$.fragment,mo),mo.forEach(t),po.forEach(t),ml=h(Zt),nt=p(Zt,"SPAN",{});var co=m(nt);cl=o(co,"Entrenamiento"),co.forEach(t),Zt.forEach(t),Rt=h(e),T(aa.$$.fragment,e),St=h(e),T(ta.$$.fragment,e),this.h()},h(){E(s,"name","hf:doc:metadata"),E(s,"content",JSON.stringify(Io)),E(g,"id","modelado-de-lenguaje"),E(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(g,"href","#modelado-de-lenguaje"),E(r,"class","relative group"),E(ee,"href","https://huggingface.co/distilgpt2"),E(ee,"rel","nofollow"),E(K,"href","https://huggingface.co/distilroberta-base"),E(K,"rel","nofollow"),E(oe,"href","https://www.reddit.com/r/askscience/"),E(oe,"rel","nofollow"),E(u,"href","https://huggingface.co/datasets/eli5"),E(u,"rel","nofollow"),E(Q,"id","carga-el-dataset-eli5"),E(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Q,"href","#carga-el-dataset-eli5"),E(me,"class","relative group"),E(Be,"id","preprocesamiento"),E(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Be,"href","#preprocesamiento"),E(Ie,"class","relative group"),E(pa,"href","https://huggingface.co/docs/datasets/process.html#flatten"),E(pa,"rel","nofollow"),E(ua,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map"),E(ua,"rel","nofollow"),E(Ke,"id","modelado-de-lenguaje-causal"),E(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Ke,"href","#modelado-de-lenguaje-causal"),E(Re,"class","relative group"),E($a,"href","https://huggingface.co/distilgpt2"),E($a,"rel","nofollow"),E(Qe,"id","entrenamiento"),E(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Qe,"href","#entrenamiento"),E(Se,"class","relative group"),E(Xe,"id","modelado-de-lenguaje-por-enmascaramiento"),E(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(Xe,"href","#modelado-de-lenguaje-por-enmascaramiento"),E(Ge,"class","relative group"),E(va,"href","https://huggingface.co/distilroberta-base"),E(va,"rel","nofollow"),E(ea,"id","entrenamiento"),E(ea,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(ea,"href","#entrenamiento"),E(Ne,"class","relative group")},m(e,d){a(document.head,s),c(e,j,d),c(e,r,d),a(r,g),a(g,w),A($,w,null),a(r,b),a(r,q),a(q,v),c(e,y,d),c(e,F,d),a(F,N),c(e,G,d),A(z,e,d),c(e,R,d),c(e,S,d),a(S,H),c(e,I,d),A(U,e,d),c(e,_,d),c(e,k,d),a(k,B),c(e,L,d),c(e,W,d),a(W,V),a(W,ee),a(ee,ae),a(W,pe),a(W,K),a(K,ue),a(W,te),a(W,oe),a(oe,Y),a(W,ne),a(W,u),a(u,O),a(W,re),c(e,le,d),A(X,e,d),c(e,Z,d),c(e,me,d),a(me,Q),a(Q,ce),A(ie,ce,null),a(me,fe),a(me,ge),a(ge,de),c(e,_e,d),c(e,n,d),a(n,x),c(e,he,d),A(Ee,e,d),c(e,De,d),c(e,ye,d),a(ye,Le),c(e,$e,d),A(we,e,d),c(e,je,d),c(e,ze,d),a(ze,Oe),c(e,be,d),A(xe,e,d),c(e,Te,d),c(e,qe,d),a(qe,as),a(qe,Ca),a(Ca,ts),a(qe,ss),a(qe,Pa),a(Pa,ls),a(qe,os),a(qe,Ma),a(Ma,ns),a(qe,rs),c(e,ct,d),c(e,Ie,d),a(Ie,Be),a(Be,Fa),A(la,Fa,null),a(Ie,is),a(Ie,La),a(La,ps),c(e,ut,d),A(oa,e,d),c(e,dt,d),c(e,He,d),a(He,ms),a(He,Oa),a(Oa,cs),a(He,us),c(e,ft,d),A(na,e,d),c(e,ht,d),A(ra,e,d),c(e,gt,d),c(e,xa,d),a(xa,ds),c(e,_t,d),A(ia,e,d),c(e,$t,d),c(e,Ae,d),a(Ae,fs),a(Ae,Ia),a(Ia,hs),a(Ae,gs),a(Ae,pa),a(pa,Ra),a(Ra,_s),a(Ae,$s),c(e,jt,d),A(ma,e,d),c(e,bt,d),c(e,Ce,d),a(Ce,js),a(Ce,Sa),a(Sa,bs),a(Ce,vs),a(Ce,Ga),a(Ga,ks),a(Ce,Es),c(e,vt,d),c(e,ya,d),a(ya,ws),c(e,kt,d),A(ca,e,d),c(e,Et,d),c(e,ve,d),a(ve,xs),a(ve,ua),a(ua,Na),a(Na,ys),a(ve,zs),a(ve,Ba),a(Ba,qs),a(ve,Ds),a(ve,Ha),a(Ha,Ts),a(ve,As),a(ve,Wa),a(Wa,Cs),a(ve,Ps),c(e,wt,d),A(da,e,d),c(e,xt,d),c(e,za,d),a(za,Ms),c(e,yt,d),c(e,We,d),a(We,Ua),a(Ua,Fs),a(We,Ls),a(We,fa),a(fa,Os),a(fa,Ja),a(Ja,Is),a(fa,Rs),c(e,zt,d),A(ha,e,d),c(e,qt,d),c(e,Ue,d),a(Ue,Ss),a(Ue,Ka),a(Ka,Gs),a(Ue,Ns),c(e,Dt,d),A(ga,e,d),c(e,Tt,d),c(e,ke,d),a(ke,Bs),a(ke,Ya),a(Ya,Hs),a(ke,Ws),a(ke,Qa),a(Qa,Us),a(ke,Js),a(ke,Va),a(Va,Ks),a(ke,Ys),a(ke,Xa),a(Xa,Qs),a(ke,Vs),c(e,At,d),A(Je,e,d),c(e,Ct,d),c(e,Re,d),a(Re,Ke),a(Ke,Za),A(_a,Za,null),a(Re,Xs),a(Re,et),a(et,Zs),c(e,Pt,d),c(e,Ye,d),a(Ye,el),a(Ye,$a),a($a,al),a(Ye,tl),c(e,Mt,d),c(e,Se,d),a(Se,Qe),a(Qe,at),A(ja,at,null),a(Se,sl),a(Se,tt),a(tt,ll),c(e,Ft,d),A(Ve,e,d),c(e,Lt,d),c(e,Ge,d),a(Ge,Xe),a(Xe,st),A(ba,st,null),a(Ge,ol),a(Ge,lt),a(lt,nl),c(e,Ot,d),c(e,Ze,d),a(Ze,rl),a(Ze,va),a(va,il),a(Ze,pl),c(e,It,d),c(e,Ne,d),a(Ne,ea),a(ea,ot),A(ka,ot,null),a(Ne,ml),a(Ne,nt),a(nt,cl),c(e,Rt,d),A(aa,e,d),c(e,St,d),A(ta,e,d),Gt=!0},p(e,[d]){const Ea={};d&2&&(Ea.$$scope={dirty:d,ctx:e}),X.$set(Ea);const rt={};d&2&&(rt.$$scope={dirty:d,ctx:e}),Je.$set(rt);const it={};d&2&&(it.$$scope={dirty:d,ctx:e}),Ve.$set(it);const pt={};d&2&&(pt.$$scope={dirty:d,ctx:e}),aa.$set(pt);const mt={};d&2&&(mt.$$scope={dirty:d,ctx:e}),ta.$set(mt)},i(e){Gt||(C($.$$.fragment,e),C(z.$$.fragment,e),C(U.$$.fragment,e),C(X.$$.fragment,e),C(ie.$$.fragment,e),C(Ee.$$.fragment,e),C(we.$$.fragment,e),C(xe.$$.fragment,e),C(la.$$.fragment,e),C(oa.$$.fragment,e),C(na.$$.fragment,e),C(ra.$$.fragment,e),C(ia.$$.fragment,e),C(ma.$$.fragment,e),C(ca.$$.fragment,e),C(da.$$.fragment,e),C(ha.$$.fragment,e),C(ga.$$.fragment,e),C(Je.$$.fragment,e),C(_a.$$.fragment,e),C(ja.$$.fragment,e),C(Ve.$$.fragment,e),C(ba.$$.fragment,e),C(ka.$$.fragment,e),C(aa.$$.fragment,e),C(ta.$$.fragment,e),Gt=!0)},o(e){P($.$$.fragment,e),P(z.$$.fragment,e),P(U.$$.fragment,e),P(X.$$.fragment,e),P(ie.$$.fragment,e),P(Ee.$$.fragment,e),P(we.$$.fragment,e),P(xe.$$.fragment,e),P(la.$$.fragment,e),P(oa.$$.fragment,e),P(na.$$.fragment,e),P(ra.$$.fragment,e),P(ia.$$.fragment,e),P(ma.$$.fragment,e),P(ca.$$.fragment,e),P(da.$$.fragment,e),P(ha.$$.fragment,e),P(ga.$$.fragment,e),P(Je.$$.fragment,e),P(_a.$$.fragment,e),P(ja.$$.fragment,e),P(Ve.$$.fragment,e),P(ba.$$.fragment,e),P(ka.$$.fragment,e),P(aa.$$.fragment,e),P(ta.$$.fragment,e),Gt=!1},d(e){t(s),e&&t(j),e&&t(r),M($),e&&t(y),e&&t(F),e&&t(G),M(z,e),e&&t(R),e&&t(S),e&&t(I),M(U,e),e&&t(_),e&&t(k),e&&t(L),e&&t(W),e&&t(le),M(X,e),e&&t(Z),e&&t(me),M(ie),e&&t(_e),e&&t(n),e&&t(he),M(Ee,e),e&&t(De),e&&t(ye),e&&t($e),M(we,e),e&&t(je),e&&t(ze),e&&t(be),M(xe,e),e&&t(Te),e&&t(qe),e&&t(ct),e&&t(Ie),M(la),e&&t(ut),M(oa,e),e&&t(dt),e&&t(He),e&&t(ft),M(na,e),e&&t(ht),M(ra,e),e&&t(gt),e&&t(xa),e&&t(_t),M(ia,e),e&&t($t),e&&t(Ae),e&&t(jt),M(ma,e),e&&t(bt),e&&t(Ce),e&&t(vt),e&&t(ya),e&&t(kt),M(ca,e),e&&t(Et),e&&t(ve),e&&t(wt),M(da,e),e&&t(xt),e&&t(za),e&&t(yt),e&&t(We),e&&t(zt),M(ha,e),e&&t(qt),e&&t(Ue),e&&t(Dt),M(ga,e),e&&t(Tt),e&&t(ke),e&&t(At),M(Je,e),e&&t(Ct),e&&t(Re),M(_a),e&&t(Pt),e&&t(Ye),e&&t(Mt),e&&t(Se),M(ja),e&&t(Ft),M(Ve,e),e&&t(Lt),e&&t(Ge),M(ba),e&&t(Ot),e&&t(Ze),e&&t(It),e&&t(Ne),M(ka),e&&t(Rt),M(aa,e),e&&t(St),M(ta,e)}}}const Io={local:"modelado-de-lenguaje",sections:[{local:"carga-el-dataset-eli5",title:"Carga el dataset ELI5"},{local:"preprocesamiento",title:"Preprocesamiento"},{local:"modelado-de-lenguaje-causal",sections:[{local:"entrenamiento",title:"Entrenamiento"}],title:"Modelado de lenguaje causal"},{local:"modelado-de-lenguaje-por-enmascaramiento",sections:[{local:"entrenamiento",title:"Entrenamiento"}],title:"Modelado de lenguaje por enmascaramiento"}],title:"Modelado de lenguaje"};function Ro(J){return $o(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Jo extends fo{constructor(s){super();ho(this,s,Ro,Oo,go,{})}}export{Jo as default,Io as metadata};
