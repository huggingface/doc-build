import{S as qn,i as Nn,s as Sn,e as a,k as c,w as h,t as s,M as Bn,c as o,d as r,m as d,a as n,x as g,h as i,b as l,G as e,g as E,y as _,q as v,o as x,B as y,v as Cn,L as An}from"../../chunks/vendor-hf-doc-builder.js";import{T as Vn}from"../../chunks/Tip-hf-doc-builder.js";import{D as w}from"../../chunks/Docstring-hf-doc-builder.js";import{C as On}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Jt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as jn}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Wn(Ke){let u,L,$,F,T;return F=new On({props:{code:`# We can't instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let's show the examples on a
# derived class: *Wav2Vec2FeatureExtractor*
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h"
)  # Download feature_extraction_config from huggingface.co and cache.
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "./test/saved_model/"
)  # E.g. feature_extractor (or model) was saved using *save_pretrained('./test/saved_model/')*
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("./test/saved_model/preprocessor_config.json")
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h", return_attention_mask=False, foo=False
)
assert feature_extractor.return_attention_mask is False
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h", return_attention_mask=False, foo=False, return_unused_kwargs=True
)
assert feature_extractor.return_attention_mask is False
assert unused_kwargs == {"foo": False}`,highlighted:`<span class="hljs-comment"># We can&#x27;t instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let&#x27;s show the examples on a</span>
<span class="hljs-comment"># derived class: *Wav2Vec2FeatureExtractor*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>
)  <span class="hljs-comment"># Download feature_extraction_config from huggingface.co and cache.</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;./test/saved_model/&quot;</span>
)  <span class="hljs-comment"># E.g. feature_extractor (or model) was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/preprocessor_config.json&quot;</span>)
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
<span class="hljs-keyword">assert</span> unused_kwargs == {<span class="hljs-string">&quot;foo&quot;</span>: <span class="hljs-literal">False</span>}`}}),{c(){u=a("p"),L=s("Examples:"),$=c(),h(F.$$.fragment)},l(m){u=o(m,"P",{});var P=n(u);L=i(P,"Examples:"),P.forEach(r),$=d(m),g(F.$$.fragment,m)},m(m,P){E(m,u,P),e(u,L),E(m,$,P),_(F,m,P),T=!0},p:An,i(m){T||(v(F.$$.fragment,m),T=!0)},o(m){x(F.$$.fragment,m),T=!1},d(m){m&&r(u),m&&r($),y(F,m)}}}function Rn(Ke){let u,L,$,F,T,m,P,V;return{c(){u=a("p"),L=s("If the "),$=a("code"),F=s("processed_features"),T=s(` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the
result will use the same type unless you provide a different tensor type with `),m=a("code"),P=s("return_tensors"),V=s(`. In the case of
PyTorch tensors, you will lose the specific device of your tensors however.`)},l(O){u=o(O,"P",{});var k=n(u);L=i(k,"If the "),$=o(k,"CODE",{});var z=n($);F=i(z,"processed_features"),z.forEach(r),T=i(k,` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the
result will use the same type unless you provide a different tensor type with `),m=o(k,"CODE",{});var Qe=n(m);P=i(Qe,"return_tensors"),Qe.forEach(r),V=i(k,`. In the case of
PyTorch tensors, you will lose the specific device of your tensors however.`),k.forEach(r)},m(O,k){E(O,u,k),e(u,L),e(u,$),e($,F),e(u,T),e(u,m),e(m,P),e(u,V)},d(O){O&&r(u)}}}function Un(Ke){let u,L,$,F,T,m,P,V,O,k,z,Qe,lt,Pr,kr,mt,zr,Dr,Yt,j,Q,pt,xe,Mr,ft,Lr,Kt,D,ye,qr,ut,Nr,Sr,B,be,Br,N,Cr,Xe,Ar,Vr,ht,Or,jr,Ze,Wr,Rr,Ur,X,Hr,Z,Ee,Gr,W,Jr,gt,Yr,Kr,et,Qr,Xr,Qt,R,ee,_t,$e,Zr,vt,ea,Xt,S,Fe,ta,xt,ra,aa,q,we,oa,yt,na,sa,U,ia,bt,ca,da,Et,la,ma,pa,te,Zt,H,re,$t,Ie,fa,Ft,ua,er,I,Te,ha,G,ga,tt,_a,va,wt,xa,ya,ba,It,Ea,$a,ae,Pe,Fa,Tt,wa,Ia,oe,ke,Ta,ze,Pa,Pt,ka,za,tr,J,ne,kt,De,Da,zt,Ma,rr,p,Me,La,Dt,qa,Na,se,Le,Sa,qe,Ba,Mt,Ca,Aa,Va,ie,Ne,Oa,Se,ja,Lt,Wa,Ra,Ua,ce,Be,Ha,Ce,Ga,qt,Ja,Ya,Ka,de,Ae,Qa,Y,Xa,Nt,Za,eo,St,to,ro,ao,le,Ve,oo,M,no,Bt,so,io,Ct,co,lo,At,mo,po,Vt,fo,uo,ho,me,Oe,go,Ot,_o,vo,pe,je,xo,We,yo,jt,bo,Eo,$o,fe,Re,Fo,K,wo,Wt,Io,To,Rt,Po,ko,zo,ue,Ue,Do,He,Mo,Ut,Lo,qo,No,he,Ge,So,Je,Bo,Ht,Co,Ao,ar;return m=new Jt({}),xe=new Jt({}),ye=new w({props:{name:"class transformers.FeatureExtractionMixin",anchor:"transformers.FeatureExtractionMixin",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L200"}}),be=new w({props:{name:"from_pretrained",anchor:"transformers.FeatureExtractionMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, or not specified, will use
the token generated when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L224",returnDescription:`
<p>A feature extractor of type <a
  href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"
>FeatureExtractionMixin</a>.</p>
`}}),X=new jn({props:{anchor:"transformers.FeatureExtractionMixin.from_pretrained.example",$$slots:{default:[Wn]},$$scope:{ctx:Ke}}}),Ee=new w({props:{name:"save_pretrained",anchor:"transformers.FeatureExtractionMixin.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike]"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the feature extractor JSON file will be saved (will be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.FeatureExtractionMixin.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).
kwargs &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/main/en/internal/image_processing_utils#transformers.ImageProcessingMixin.push_to_hub">push_to_hub()</a> method.`,name:"push_to_hub"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L310"}}),$e=new Jt({}),Fe=new w({props:{name:"class transformers.SequenceFeatureExtractor",anchor:"transformers.SequenceFeatureExtractor",parameters:[{name:"feature_size",val:": int"},{name:"sampling_rate",val:": int"},{name:"padding_value",val:": float"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SequenceFeatureExtractor.feature_size",description:`<strong>feature_size</strong> (<code>int</code>) &#x2014;
The feature dimension of the extracted features.`,name:"feature_size"},{anchor:"transformers.SequenceFeatureExtractor.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>) &#x2014;
The sampling rate at which the audio files should be digitalized expressed in Hertz per second (Hz).`,name:"sampling_rate"},{anchor:"transformers.SequenceFeatureExtractor.padding_value",description:`<strong>padding_value</strong> (<code>float</code>) &#x2014;
The value that is used to fill the padding values / vectors.`,name:"padding_value"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_sequence_utils.py#L29"}}),we=new w({props:{name:"pad",anchor:"transformers.SequenceFeatureExtractor.pad",parameters:[{name:"processed_features",val:": typing.Union[transformers.feature_extraction_utils.BatchFeature, typing.List[transformers.feature_extraction_utils.BatchFeature], typing.Dict[str, transformers.feature_extraction_utils.BatchFeature], typing.Dict[str, typing.List[transformers.feature_extraction_utils.BatchFeature]], typing.List[typing.Dict[str, transformers.feature_extraction_utils.BatchFeature]]]"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = True"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"truncation",val:": bool = False"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"return_attention_mask",val:": typing.Optional[bool] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"}],parametersDescription:[{anchor:"transformers.SequenceFeatureExtractor.pad.processed_features",description:`<strong>processed_features</strong> (<a href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a>, list of <a href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a>, <code>Dict[str, List[float]]</code>, <code>Dict[str, List[List[float]]</code> or <code>List[Dict[str, List[float]]]</code>) &#x2014;
Processed inputs. Can represent one input (<a href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a> or <code>Dict[str, List[float]]</code>) or a batch of
input values / vectors (list of <a href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a>, <em>Dict[str, List[List[float]]]</em> or <em>List[Dict[str,
List[float]]]</em>) so you can use this method during preprocessing as well as in a PyTorch Dataloader
collate function.</p>
<p>Instead of <code>List[float]</code> you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),
see the note above for the return type.`,name:"processed_features"},{anchor:"transformers.SequenceFeatureExtractor.pad.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Select a strategy to pad the returned sequences (according to the model&#x2019;s padding side and padding
index) among:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.SequenceFeatureExtractor.pad.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum length of the returned list and optionally padding length (see above).`,name:"max_length"},{anchor:"transformers.SequenceFeatureExtractor.pad.truncation",description:`<strong>truncation</strong> (<code>bool</code>) &#x2014;
Activates truncation to cut input sequences longer than <code>max_length</code> to <code>max_length</code>.`,name:"truncation"},{anchor:"transformers.SequenceFeatureExtractor.pad.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability</p>
<blockquote>
<p>= 7.5 (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.</p>
</blockquote>`,name:"pad_to_multiple_of"},{anchor:"transformers.SequenceFeatureExtractor.pad.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific feature_extractor&#x2019;s default.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"},{anchor:"transformers.SequenceFeatureExtractor.pad.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_sequence_utils.py#L52"}}),te=new Vn({props:{$$slots:{default:[Rn]},$$scope:{ctx:Ke}}}),Ie=new Jt({}),Te=new w({props:{name:"class transformers.BatchFeature",anchor:"transformers.BatchFeature",parameters:[{name:"data",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"tensor_type",val:": typing.Union[NoneType, str, transformers.utils.generic.TensorType] = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.data",description:`<strong>data</strong> (<code>dict</code>) &#x2014;
Dictionary of lists/arrays/tensors returned by the <strong>call</strong>/pad methods (&#x2018;input_values&#x2019;, &#x2018;attention_mask&#x2019;,
etc.).`,name:"data"},{anchor:"transformers.BatchFeature.tensor_type",description:`<strong>tensor_type</strong> (<code>Union[None, str, TensorType]</code>, <em>optional</em>) &#x2014;
You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.`,name:"tensor_type"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L58"}}),Pe=new w({props:{name:"convert_to_tensors",anchor:"transformers.BatchFeature.convert_to_tensors",parameters:[{name:"tensor_type",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.convert_to_tensors.tensor_type",description:`<strong>tensor_type</strong> (<code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
The type of tensors to use. If <code>str</code>, should be one of the values of the enum <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>. If
<code>None</code>, no modification is done.`,name:"tensor_type"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L112"}}),ke=new w({props:{name:"to",anchor:"transformers.BatchFeature.to",parameters:[{name:"device",val:": typing.Union[str, ForwardRef('torch.device')]"}],parametersDescription:[{anchor:"transformers.BatchFeature.to.device",description:"<strong>device</strong> (<code>str</code> or <code>torch.device</code>) &#x2014; The device to put the tensors on.",name:"device"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L177",returnDescription:`
<p>The same instance after modification.</p>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),De=new Jt({}),Me=new w({props:{name:"class transformers.ImageFeatureExtractionMixin",anchor:"transformers.ImageFeatureExtractionMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L200"}}),Le=new w({props:{name:"center_crop",anchor:"transformers.ImageFeatureExtractionMixin.center_crop",parameters:[{name:"image",val:""},{name:"size",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.center_crop.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code> of shape (n_channels, height, width) or (height, width, n_channels)) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.center_crop.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple[int, int]</code>) &#x2014;
The size to which crop the image.`,name:"size"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L428",returnDescription:`
<p>A center cropped <code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code> of shape: (n_channels,
height, width).</p>
`,returnType:`
<p>new_image</p>
`}}),Ne=new w({props:{name:"convert_rgb",anchor:"transformers.ImageFeatureExtractionMixin.convert_rgb",parameters:[{name:"image",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.convert_rgb.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code>) &#x2014;
The image to convert.`,name:"image"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L242"}}),Be=new w({props:{name:"expand_dims",anchor:"transformers.ImageFeatureExtractionMixin.expand_dims",parameters:[{name:"image",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.expand_dims.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to expand.`,name:"image"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L295"}}),Ae=new w({props:{name:"flip_channel_order",anchor:"transformers.ImageFeatureExtractionMixin.flip_channel_order",parameters:[{name:"image",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.flip_channel_order.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image whose color channels to flip. If <code>np.ndarray</code> or <code>torch.Tensor</code>, the channel dimension should
be first.`,name:"image"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L503"}}),Ve=new w({props:{name:"normalize",anchor:"transformers.ImageFeatureExtractionMixin.normalize",parameters:[{name:"image",val:""},{name:"mean",val:""},{name:"std",val:""},{name:"rescale",val:" = False"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.normalize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to normalize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.normalize.mean",description:`<strong>mean</strong> (<code>List[float]</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The mean (per channel) to use for normalization.`,name:"mean"},{anchor:"transformers.ImageFeatureExtractionMixin.normalize.std",description:`<strong>std</strong> (<code>List[float]</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The standard deviation (per channel) to use for normalization.`,name:"std"},{anchor:"transformers.ImageFeatureExtractionMixin.normalize.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to rescale the image to be between 0 and 1. If a PIL image is provided, scaling will
happen automatically.`,name:"rescale"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L315"}}),Oe=new w({props:{name:"rescale",anchor:"transformers.ImageFeatureExtractionMixin.rescale",parameters:[{name:"image",val:": ndarray"},{name:"scale",val:": typing.Union[float, int]"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L256"}}),je=new w({props:{name:"resize",anchor:"transformers.ImageFeatureExtractionMixin.resize",parameters:[{name:"image",val:""},{name:"size",val:""},{name:"resample",val:" = None"},{name:"default_to_square",val:" = True"},{name:"max_size",val:" = None"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.resize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple[int, int]</code>) &#x2014;
The size to use for resizing the image. If <code>size</code> is a sequence like (h, w), output size will be
matched to this.</p>
<p>If <code>size</code> is an int and <code>default_to_square</code> is <code>True</code>, then image will be resized to (size, size). If
<code>size</code> is an int and <code>default_to_square</code> is <code>False</code>, then smaller edge of the image will be matched to
this number. i.e, if height &gt; width, then image will be rescaled to (size * height / width, size).`,name:"size"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PILImageResampling.BILINEAR</code>) &#x2014;
The filter to user for resampling.`,name:"resample"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.default_to_square",description:`<strong>default_to_square</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
How to convert <code>size</code> when it is a single int. If set to <code>True</code>, the <code>size</code> will be converted to a
square (<code>size</code>,<code>size</code>). If set to <code>False</code>, will replicate
<a href="https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Resize" rel="nofollow"><code>torchvision.transforms.Resize</code></a>
with support for resizing only the smallest edge and providing an optional <code>max_size</code>.`,name:"default_to_square"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.max_size",description:`<strong>max_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The maximum allowed for the longer edge of the resized image: if the longer edge of the image is
greater than <code>max_size</code> after being resized according to <code>size</code>, then the image is resized again so
that the longer edge is equal to <code>max_size</code>. As a result, <code>size</code> might be overruled, i.e the smaller
edge may be shorter than <code>size</code>. Only used if <code>default_to_square</code> is <code>False</code>.`,name:"max_size"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L361",returnDescription:`
<p>A resized <code>PIL.Image.Image</code>.</p>
`,returnType:`
<p>image</p>
`}}),Re=new w({props:{name:"rotate",anchor:"transformers.ImageFeatureExtractionMixin.rotate",parameters:[{name:"image",val:""},{name:"angle",val:""},{name:"resample",val:" = None"},{name:"expand",val:" = 0"},{name:"center",val:" = None"},{name:"translate",val:" = None"},{name:"fillcolor",val:" = None"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.rotate.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to rotate. If <code>np.ndarray</code> or <code>torch.Tensor</code>, will be converted to <code>PIL.Image.Image</code> before
rotating.`,name:"image"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L520",returnDescription:`
<p>A rotated <code>PIL.Image.Image</code>.</p>
`,returnType:`
<p>image</p>
`}}),Ue=new w({props:{name:"to_numpy_array",anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array",parameters:[{name:"image",val:""},{name:"rescale",val:" = None"},{name:"channel_first",val:" = True"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to convert to a NumPy array.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values floats between 0. and 1.). Will
default to <code>True</code> if the image is a PIL Image or an array/tensor of integers, <code>False</code> otherwise.`,name:"rescale"},{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.channel_first",description:`<strong>channel_first</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to permute the dimensions of the image to put the channel dimension first.`,name:"channel_first"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L263"}}),Ge=new w({props:{name:"to_pil_image",anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image",parameters:[{name:"image",val:""},{name:"rescale",val:" = None"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>numpy.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to convert to the PIL Image format.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values integers between 0 and 255). Will
default to <code>True</code> if the image type is a floating type, <code>False</code> otherwise.`,name:"rescale"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L212"}}),{c(){u=a("meta"),L=c(),$=a("h1"),F=a("a"),T=a("span"),h(m.$$.fragment),P=c(),V=a("span"),O=s("Feature Extractor"),k=c(),z=a("p"),Qe=s(`A feature extractor is in charge of preparing input features for audio or vision models. This includes feature extraction
from sequences, `),lt=a("em"),Pr=s("e.g."),kr=s(`, pre-processing audio files to Log-Mel Spectrogram features, feature extraction from images
`),mt=a("em"),zr=s("e.g."),Dr=s(` cropping image image files, but also padding, normalization, and conversion to Numpy, PyTorch, and TensorFlow
tensors.`),Yt=c(),j=a("h2"),Q=a("a"),pt=a("span"),h(xe.$$.fragment),Mr=c(),ft=a("span"),Lr=s("FeatureExtractionMixin"),Kt=c(),D=a("div"),h(ye.$$.fragment),qr=c(),ut=a("p"),Nr=s(`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),Sr=c(),B=a("div"),h(be.$$.fragment),Br=c(),N=a("p"),Cr=s("Instantiate a type of "),Xe=a("a"),Ar=s("FeatureExtractionMixin"),Vr=s(" from a feature extractor, "),ht=a("em"),Or=s("e.g."),jr=s(` a
derived class of `),Ze=a("a"),Wr=s("SequenceFeatureExtractor"),Rr=s("."),Ur=c(),h(X.$$.fragment),Hr=c(),Z=a("div"),h(Ee.$$.fragment),Gr=c(),W=a("p"),Jr=s("Save a feature_extractor object to the directory "),gt=a("code"),Yr=s("save_directory"),Kr=s(`, so that it can be re-loaded using the
`),et=a("a"),Qr=s("from_pretrained()"),Xr=s(" class method."),Qt=c(),R=a("h2"),ee=a("a"),_t=a("span"),h($e.$$.fragment),Zr=c(),vt=a("span"),ea=s("SequenceFeatureExtractor"),Xt=c(),S=a("div"),h(Fe.$$.fragment),ta=c(),xt=a("p"),ra=s("This is a general feature extraction class for speech recognition."),aa=c(),q=a("div"),h(we.$$.fragment),oa=c(),yt=a("p"),na=s(`Pad input values / input vectors or a batch of input values / input vectors up to predefined length or to the
max sequence length in the batch.`),sa=c(),U=a("p"),ia=s("Padding side (left/right) padding values are defined at the feature extractor level (with "),bt=a("code"),ca=s("self.padding_side"),da=s(`,
`),Et=a("code"),la=s("self.padding_value"),ma=s(")"),pa=c(),h(te.$$.fragment),Zt=c(),H=a("h2"),re=a("a"),$t=a("span"),h(Ie.$$.fragment),fa=c(),Ft=a("span"),ua=s("BatchFeature"),er=c(),I=a("div"),h(Te.$$.fragment),ha=c(),G=a("p"),ga=s("Holds the output of the "),tt=a("a"),_a=s("pad()"),va=s(" and feature extractor specific "),wt=a("code"),xa=s("__call__"),ya=s(" methods."),ba=c(),It=a("p"),Ea=s("This class is derived from a python dictionary and can be used as a dictionary."),$a=c(),ae=a("div"),h(Pe.$$.fragment),Fa=c(),Tt=a("p"),wa=s("Convert the inner content to tensors."),Ia=c(),oe=a("div"),h(ke.$$.fragment),Ta=c(),ze=a("p"),Pa=s("Send all values to device by calling "),Pt=a("code"),ka=s("v.to(device)"),za=s(" (PyTorch only)."),tr=c(),J=a("h2"),ne=a("a"),kt=a("span"),h(De.$$.fragment),Da=c(),zt=a("span"),Ma=s("ImageFeatureExtractionMixin"),rr=c(),p=a("div"),h(Me.$$.fragment),La=c(),Dt=a("p"),qa=s("Mixin that contain utilities for preparing image features."),Na=c(),se=a("div"),h(Le.$$.fragment),Sa=c(),qe=a("p"),Ba=s("Crops "),Mt=a("code"),Ca=s("image"),Aa=s(` to the given size using a center crop. Note that if the image is too small to be cropped to the
size given, it will be padded (so the returned result has the size asked).`),Va=c(),ie=a("div"),h(Ne.$$.fragment),Oa=c(),Se=a("p"),ja=s("Converts "),Lt=a("code"),Wa=s("PIL.Image.Image"),Ra=s(" to RGB format."),Ua=c(),ce=a("div"),h(Be.$$.fragment),Ha=c(),Ce=a("p"),Ga=s("Expands 2-dimensional "),qt=a("code"),Ja=s("image"),Ya=s(" to 3 dimensions."),Ka=c(),de=a("div"),h(Ae.$$.fragment),Qa=c(),Y=a("p"),Xa=s("Flips the channel order of "),Nt=a("code"),Za=s("image"),eo=s(` from RGB to BGR, or vice versa. Note that this will trigger a conversion of
`),St=a("code"),to=s("image"),ro=s(" to a NumPy array if it\u2019s a PIL Image."),ao=c(),le=a("div"),h(Ve.$$.fragment),oo=c(),M=a("p"),no=s("Normalizes "),Bt=a("code"),so=s("image"),io=s(" with "),Ct=a("code"),co=s("mean"),lo=s(" and "),At=a("code"),mo=s("std"),po=s(". Note that this will trigger a conversion of "),Vt=a("code"),fo=s("image"),uo=s(` to a NumPy array
if it\u2019s a PIL Image.`),ho=c(),me=a("div"),h(Oe.$$.fragment),go=c(),Ot=a("p"),_o=s("Rescale a numpy image by scale amount"),vo=c(),pe=a("div"),h(je.$$.fragment),xo=c(),We=a("p"),yo=s("Resizes "),jt=a("code"),bo=s("image"),Eo=s(". Enforces conversion of input to PIL.Image."),$o=c(),fe=a("div"),h(Re.$$.fragment),Fo=c(),K=a("p"),wo=s("Returns a rotated copy of "),Wt=a("code"),Io=s("image"),To=s(". This method returns a copy of "),Rt=a("code"),Po=s("image"),ko=s(`, rotated the given number of degrees
counter clockwise around its centre.`),zo=c(),ue=a("div"),h(Ue.$$.fragment),Do=c(),He=a("p"),Mo=s("Converts "),Ut=a("code"),Lo=s("image"),qo=s(` to a numpy array. Optionally rescales it and puts the channel dimension as the first
dimension.`),No=c(),he=a("div"),h(Ge.$$.fragment),So=c(),Je=a("p"),Bo=s("Converts "),Ht=a("code"),Co=s("image"),Ao=s(` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),this.h()},l(t){const f=Bn('[data-svelte="svelte-1phssyn"]',document.head);u=o(f,"META",{name:!0,content:!0}),f.forEach(r),L=d(t),$=o(t,"H1",{class:!0});var Ye=n($);F=o(Ye,"A",{id:!0,class:!0,href:!0});var Gt=n(F);T=o(Gt,"SPAN",{});var Vo=n(T);g(m.$$.fragment,Vo),Vo.forEach(r),Gt.forEach(r),P=d(Ye),V=o(Ye,"SPAN",{});var Oo=n(V);O=i(Oo,"Feature Extractor"),Oo.forEach(r),Ye.forEach(r),k=d(t),z=o(t,"P",{});var rt=n(z);Qe=i(rt,`A feature extractor is in charge of preparing input features for audio or vision models. This includes feature extraction
from sequences, `),lt=o(rt,"EM",{});var jo=n(lt);Pr=i(jo,"e.g."),jo.forEach(r),kr=i(rt,`, pre-processing audio files to Log-Mel Spectrogram features, feature extraction from images
`),mt=o(rt,"EM",{});var Wo=n(mt);zr=i(Wo,"e.g."),Wo.forEach(r),Dr=i(rt,` cropping image image files, but also padding, normalization, and conversion to Numpy, PyTorch, and TensorFlow
tensors.`),rt.forEach(r),Yt=d(t),j=o(t,"H2",{class:!0});var or=n(j);Q=o(or,"A",{id:!0,class:!0,href:!0});var Ro=n(Q);pt=o(Ro,"SPAN",{});var Uo=n(pt);g(xe.$$.fragment,Uo),Uo.forEach(r),Ro.forEach(r),Mr=d(or),ft=o(or,"SPAN",{});var Ho=n(ft);Lr=i(Ho,"FeatureExtractionMixin"),Ho.forEach(r),or.forEach(r),Kt=d(t),D=o(t,"DIV",{class:!0});var ge=n(D);g(ye.$$.fragment,ge),qr=d(ge),ut=o(ge,"P",{});var Go=n(ut);Nr=i(Go,`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),Go.forEach(r),Sr=d(ge),B=o(ge,"DIV",{class:!0});var at=n(B);g(be.$$.fragment,at),Br=d(at),N=o(at,"P",{});var _e=n(N);Cr=i(_e,"Instantiate a type of "),Xe=o(_e,"A",{href:!0});var Jo=n(Xe);Ar=i(Jo,"FeatureExtractionMixin"),Jo.forEach(r),Vr=i(_e," from a feature extractor, "),ht=o(_e,"EM",{});var Yo=n(ht);Or=i(Yo,"e.g."),Yo.forEach(r),jr=i(_e,` a
derived class of `),Ze=o(_e,"A",{href:!0});var Ko=n(Ze);Wr=i(Ko,"SequenceFeatureExtractor"),Ko.forEach(r),Rr=i(_e,"."),_e.forEach(r),Ur=d(at),g(X.$$.fragment,at),at.forEach(r),Hr=d(ge),Z=o(ge,"DIV",{class:!0});var nr=n(Z);g(Ee.$$.fragment,nr),Gr=d(nr),W=o(nr,"P",{});var ot=n(W);Jr=i(ot,"Save a feature_extractor object to the directory "),gt=o(ot,"CODE",{});var Qo=n(gt);Yr=i(Qo,"save_directory"),Qo.forEach(r),Kr=i(ot,`, so that it can be re-loaded using the
`),et=o(ot,"A",{href:!0});var Xo=n(et);Qr=i(Xo,"from_pretrained()"),Xo.forEach(r),Xr=i(ot," class method."),ot.forEach(r),nr.forEach(r),ge.forEach(r),Qt=d(t),R=o(t,"H2",{class:!0});var sr=n(R);ee=o(sr,"A",{id:!0,class:!0,href:!0});var Zo=n(ee);_t=o(Zo,"SPAN",{});var en=n(_t);g($e.$$.fragment,en),en.forEach(r),Zo.forEach(r),Zr=d(sr),vt=o(sr,"SPAN",{});var tn=n(vt);ea=i(tn,"SequenceFeatureExtractor"),tn.forEach(r),sr.forEach(r),Xt=d(t),S=o(t,"DIV",{class:!0});var nt=n(S);g(Fe.$$.fragment,nt),ta=d(nt),xt=o(nt,"P",{});var rn=n(xt);ra=i(rn,"This is a general feature extraction class for speech recognition."),rn.forEach(r),aa=d(nt),q=o(nt,"DIV",{class:!0});var ve=n(q);g(we.$$.fragment,ve),oa=d(ve),yt=o(ve,"P",{});var an=n(yt);na=i(an,`Pad input values / input vectors or a batch of input values / input vectors up to predefined length or to the
max sequence length in the batch.`),an.forEach(r),sa=d(ve),U=o(ve,"P",{});var st=n(U);ia=i(st,"Padding side (left/right) padding values are defined at the feature extractor level (with "),bt=o(st,"CODE",{});var on=n(bt);ca=i(on,"self.padding_side"),on.forEach(r),da=i(st,`,
`),Et=o(st,"CODE",{});var nn=n(Et);la=i(nn,"self.padding_value"),nn.forEach(r),ma=i(st,")"),st.forEach(r),pa=d(ve),g(te.$$.fragment,ve),ve.forEach(r),nt.forEach(r),Zt=d(t),H=o(t,"H2",{class:!0});var ir=n(H);re=o(ir,"A",{id:!0,class:!0,href:!0});var sn=n(re);$t=o(sn,"SPAN",{});var cn=n($t);g(Ie.$$.fragment,cn),cn.forEach(r),sn.forEach(r),fa=d(ir),Ft=o(ir,"SPAN",{});var dn=n(Ft);ua=i(dn,"BatchFeature"),dn.forEach(r),ir.forEach(r),er=d(t),I=o(t,"DIV",{class:!0});var C=n(I);g(Te.$$.fragment,C),ha=d(C),G=o(C,"P",{});var it=n(G);ga=i(it,"Holds the output of the "),tt=o(it,"A",{href:!0});var ln=n(tt);_a=i(ln,"pad()"),ln.forEach(r),va=i(it," and feature extractor specific "),wt=o(it,"CODE",{});var mn=n(wt);xa=i(mn,"__call__"),mn.forEach(r),ya=i(it," methods."),it.forEach(r),ba=d(C),It=o(C,"P",{});var pn=n(It);Ea=i(pn,"This class is derived from a python dictionary and can be used as a dictionary."),pn.forEach(r),$a=d(C),ae=o(C,"DIV",{class:!0});var cr=n(ae);g(Pe.$$.fragment,cr),Fa=d(cr),Tt=o(cr,"P",{});var fn=n(Tt);wa=i(fn,"Convert the inner content to tensors."),fn.forEach(r),cr.forEach(r),Ia=d(C),oe=o(C,"DIV",{class:!0});var dr=n(oe);g(ke.$$.fragment,dr),Ta=d(dr),ze=o(dr,"P",{});var lr=n(ze);Pa=i(lr,"Send all values to device by calling "),Pt=o(lr,"CODE",{});var un=n(Pt);ka=i(un,"v.to(device)"),un.forEach(r),za=i(lr," (PyTorch only)."),lr.forEach(r),dr.forEach(r),C.forEach(r),tr=d(t),J=o(t,"H2",{class:!0});var mr=n(J);ne=o(mr,"A",{id:!0,class:!0,href:!0});var hn=n(ne);kt=o(hn,"SPAN",{});var gn=n(kt);g(De.$$.fragment,gn),gn.forEach(r),hn.forEach(r),Da=d(mr),zt=o(mr,"SPAN",{});var _n=n(zt);Ma=i(_n,"ImageFeatureExtractionMixin"),_n.forEach(r),mr.forEach(r),rr=d(t),p=o(t,"DIV",{class:!0});var b=n(p);g(Me.$$.fragment,b),La=d(b),Dt=o(b,"P",{});var vn=n(Dt);qa=i(vn,"Mixin that contain utilities for preparing image features."),vn.forEach(r),Na=d(b),se=o(b,"DIV",{class:!0});var pr=n(se);g(Le.$$.fragment,pr),Sa=d(pr),qe=o(pr,"P",{});var fr=n(qe);Ba=i(fr,"Crops "),Mt=o(fr,"CODE",{});var xn=n(Mt);Ca=i(xn,"image"),xn.forEach(r),Aa=i(fr,` to the given size using a center crop. Note that if the image is too small to be cropped to the
size given, it will be padded (so the returned result has the size asked).`),fr.forEach(r),pr.forEach(r),Va=d(b),ie=o(b,"DIV",{class:!0});var ur=n(ie);g(Ne.$$.fragment,ur),Oa=d(ur),Se=o(ur,"P",{});var hr=n(Se);ja=i(hr,"Converts "),Lt=o(hr,"CODE",{});var yn=n(Lt);Wa=i(yn,"PIL.Image.Image"),yn.forEach(r),Ra=i(hr," to RGB format."),hr.forEach(r),ur.forEach(r),Ua=d(b),ce=o(b,"DIV",{class:!0});var gr=n(ce);g(Be.$$.fragment,gr),Ha=d(gr),Ce=o(gr,"P",{});var _r=n(Ce);Ga=i(_r,"Expands 2-dimensional "),qt=o(_r,"CODE",{});var bn=n(qt);Ja=i(bn,"image"),bn.forEach(r),Ya=i(_r," to 3 dimensions."),_r.forEach(r),gr.forEach(r),Ka=d(b),de=o(b,"DIV",{class:!0});var vr=n(de);g(Ae.$$.fragment,vr),Qa=d(vr),Y=o(vr,"P",{});var ct=n(Y);Xa=i(ct,"Flips the channel order of "),Nt=o(ct,"CODE",{});var En=n(Nt);Za=i(En,"image"),En.forEach(r),eo=i(ct,` from RGB to BGR, or vice versa. Note that this will trigger a conversion of
`),St=o(ct,"CODE",{});var $n=n(St);to=i($n,"image"),$n.forEach(r),ro=i(ct," to a NumPy array if it\u2019s a PIL Image."),ct.forEach(r),vr.forEach(r),ao=d(b),le=o(b,"DIV",{class:!0});var xr=n(le);g(Ve.$$.fragment,xr),oo=d(xr),M=o(xr,"P",{});var A=n(M);no=i(A,"Normalizes "),Bt=o(A,"CODE",{});var Fn=n(Bt);so=i(Fn,"image"),Fn.forEach(r),io=i(A," with "),Ct=o(A,"CODE",{});var wn=n(Ct);co=i(wn,"mean"),wn.forEach(r),lo=i(A," and "),At=o(A,"CODE",{});var In=n(At);mo=i(In,"std"),In.forEach(r),po=i(A,". Note that this will trigger a conversion of "),Vt=o(A,"CODE",{});var Tn=n(Vt);fo=i(Tn,"image"),Tn.forEach(r),uo=i(A,` to a NumPy array
if it\u2019s a PIL Image.`),A.forEach(r),xr.forEach(r),ho=d(b),me=o(b,"DIV",{class:!0});var yr=n(me);g(Oe.$$.fragment,yr),go=d(yr),Ot=o(yr,"P",{});var Pn=n(Ot);_o=i(Pn,"Rescale a numpy image by scale amount"),Pn.forEach(r),yr.forEach(r),vo=d(b),pe=o(b,"DIV",{class:!0});var br=n(pe);g(je.$$.fragment,br),xo=d(br),We=o(br,"P",{});var Er=n(We);yo=i(Er,"Resizes "),jt=o(Er,"CODE",{});var kn=n(jt);bo=i(kn,"image"),kn.forEach(r),Eo=i(Er,". Enforces conversion of input to PIL.Image."),Er.forEach(r),br.forEach(r),$o=d(b),fe=o(b,"DIV",{class:!0});var $r=n(fe);g(Re.$$.fragment,$r),Fo=d($r),K=o($r,"P",{});var dt=n(K);wo=i(dt,"Returns a rotated copy of "),Wt=o(dt,"CODE",{});var zn=n(Wt);Io=i(zn,"image"),zn.forEach(r),To=i(dt,". This method returns a copy of "),Rt=o(dt,"CODE",{});var Dn=n(Rt);Po=i(Dn,"image"),Dn.forEach(r),ko=i(dt,`, rotated the given number of degrees
counter clockwise around its centre.`),dt.forEach(r),$r.forEach(r),zo=d(b),ue=o(b,"DIV",{class:!0});var Fr=n(ue);g(Ue.$$.fragment,Fr),Do=d(Fr),He=o(Fr,"P",{});var wr=n(He);Mo=i(wr,"Converts "),Ut=o(wr,"CODE",{});var Mn=n(Ut);Lo=i(Mn,"image"),Mn.forEach(r),qo=i(wr,` to a numpy array. Optionally rescales it and puts the channel dimension as the first
dimension.`),wr.forEach(r),Fr.forEach(r),No=d(b),he=o(b,"DIV",{class:!0});var Ir=n(he);g(Ge.$$.fragment,Ir),So=d(Ir),Je=o(Ir,"P",{});var Tr=n(Je);Bo=i(Tr,"Converts "),Ht=o(Tr,"CODE",{});var Ln=n(Ht);Co=i(Ln,"image"),Ln.forEach(r),Ao=i(Tr,` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),Tr.forEach(r),Ir.forEach(r),b.forEach(r),this.h()},h(){l(u,"name","hf:doc:metadata"),l(u,"content",JSON.stringify(Hn)),l(F,"id","feature-extractor"),l(F,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(F,"href","#feature-extractor"),l($,"class","relative group"),l(Q,"id","transformers.FeatureExtractionMixin"),l(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Q,"href","#transformers.FeatureExtractionMixin"),l(j,"class","relative group"),l(Xe,"href","/docs/transformers/main/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),l(Ze,"href","/docs/transformers/main/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor"),l(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(et,"href","/docs/transformers/main/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained"),l(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ee,"id","transformers.SequenceFeatureExtractor"),l(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ee,"href","#transformers.SequenceFeatureExtractor"),l(R,"class","relative group"),l(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(re,"id","transformers.BatchFeature"),l(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(re,"href","#transformers.BatchFeature"),l(H,"class","relative group"),l(tt,"href","/docs/transformers/main/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad"),l(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ne,"id","transformers.ImageFeatureExtractionMixin"),l(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ne,"href","#transformers.ImageFeatureExtractionMixin"),l(J,"class","relative group"),l(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(p,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,f){e(document.head,u),E(t,L,f),E(t,$,f),e($,F),e(F,T),_(m,T,null),e($,P),e($,V),e(V,O),E(t,k,f),E(t,z,f),e(z,Qe),e(z,lt),e(lt,Pr),e(z,kr),e(z,mt),e(mt,zr),e(z,Dr),E(t,Yt,f),E(t,j,f),e(j,Q),e(Q,pt),_(xe,pt,null),e(j,Mr),e(j,ft),e(ft,Lr),E(t,Kt,f),E(t,D,f),_(ye,D,null),e(D,qr),e(D,ut),e(ut,Nr),e(D,Sr),e(D,B),_(be,B,null),e(B,Br),e(B,N),e(N,Cr),e(N,Xe),e(Xe,Ar),e(N,Vr),e(N,ht),e(ht,Or),e(N,jr),e(N,Ze),e(Ze,Wr),e(N,Rr),e(B,Ur),_(X,B,null),e(D,Hr),e(D,Z),_(Ee,Z,null),e(Z,Gr),e(Z,W),e(W,Jr),e(W,gt),e(gt,Yr),e(W,Kr),e(W,et),e(et,Qr),e(W,Xr),E(t,Qt,f),E(t,R,f),e(R,ee),e(ee,_t),_($e,_t,null),e(R,Zr),e(R,vt),e(vt,ea),E(t,Xt,f),E(t,S,f),_(Fe,S,null),e(S,ta),e(S,xt),e(xt,ra),e(S,aa),e(S,q),_(we,q,null),e(q,oa),e(q,yt),e(yt,na),e(q,sa),e(q,U),e(U,ia),e(U,bt),e(bt,ca),e(U,da),e(U,Et),e(Et,la),e(U,ma),e(q,pa),_(te,q,null),E(t,Zt,f),E(t,H,f),e(H,re),e(re,$t),_(Ie,$t,null),e(H,fa),e(H,Ft),e(Ft,ua),E(t,er,f),E(t,I,f),_(Te,I,null),e(I,ha),e(I,G),e(G,ga),e(G,tt),e(tt,_a),e(G,va),e(G,wt),e(wt,xa),e(G,ya),e(I,ba),e(I,It),e(It,Ea),e(I,$a),e(I,ae),_(Pe,ae,null),e(ae,Fa),e(ae,Tt),e(Tt,wa),e(I,Ia),e(I,oe),_(ke,oe,null),e(oe,Ta),e(oe,ze),e(ze,Pa),e(ze,Pt),e(Pt,ka),e(ze,za),E(t,tr,f),E(t,J,f),e(J,ne),e(ne,kt),_(De,kt,null),e(J,Da),e(J,zt),e(zt,Ma),E(t,rr,f),E(t,p,f),_(Me,p,null),e(p,La),e(p,Dt),e(Dt,qa),e(p,Na),e(p,se),_(Le,se,null),e(se,Sa),e(se,qe),e(qe,Ba),e(qe,Mt),e(Mt,Ca),e(qe,Aa),e(p,Va),e(p,ie),_(Ne,ie,null),e(ie,Oa),e(ie,Se),e(Se,ja),e(Se,Lt),e(Lt,Wa),e(Se,Ra),e(p,Ua),e(p,ce),_(Be,ce,null),e(ce,Ha),e(ce,Ce),e(Ce,Ga),e(Ce,qt),e(qt,Ja),e(Ce,Ya),e(p,Ka),e(p,de),_(Ae,de,null),e(de,Qa),e(de,Y),e(Y,Xa),e(Y,Nt),e(Nt,Za),e(Y,eo),e(Y,St),e(St,to),e(Y,ro),e(p,ao),e(p,le),_(Ve,le,null),e(le,oo),e(le,M),e(M,no),e(M,Bt),e(Bt,so),e(M,io),e(M,Ct),e(Ct,co),e(M,lo),e(M,At),e(At,mo),e(M,po),e(M,Vt),e(Vt,fo),e(M,uo),e(p,ho),e(p,me),_(Oe,me,null),e(me,go),e(me,Ot),e(Ot,_o),e(p,vo),e(p,pe),_(je,pe,null),e(pe,xo),e(pe,We),e(We,yo),e(We,jt),e(jt,bo),e(We,Eo),e(p,$o),e(p,fe),_(Re,fe,null),e(fe,Fo),e(fe,K),e(K,wo),e(K,Wt),e(Wt,Io),e(K,To),e(K,Rt),e(Rt,Po),e(K,ko),e(p,zo),e(p,ue),_(Ue,ue,null),e(ue,Do),e(ue,He),e(He,Mo),e(He,Ut),e(Ut,Lo),e(He,qo),e(p,No),e(p,he),_(Ge,he,null),e(he,So),e(he,Je),e(Je,Bo),e(Je,Ht),e(Ht,Co),e(Je,Ao),ar=!0},p(t,[f]){const Ye={};f&2&&(Ye.$$scope={dirty:f,ctx:t}),X.$set(Ye);const Gt={};f&2&&(Gt.$$scope={dirty:f,ctx:t}),te.$set(Gt)},i(t){ar||(v(m.$$.fragment,t),v(xe.$$.fragment,t),v(ye.$$.fragment,t),v(be.$$.fragment,t),v(X.$$.fragment,t),v(Ee.$$.fragment,t),v($e.$$.fragment,t),v(Fe.$$.fragment,t),v(we.$$.fragment,t),v(te.$$.fragment,t),v(Ie.$$.fragment,t),v(Te.$$.fragment,t),v(Pe.$$.fragment,t),v(ke.$$.fragment,t),v(De.$$.fragment,t),v(Me.$$.fragment,t),v(Le.$$.fragment,t),v(Ne.$$.fragment,t),v(Be.$$.fragment,t),v(Ae.$$.fragment,t),v(Ve.$$.fragment,t),v(Oe.$$.fragment,t),v(je.$$.fragment,t),v(Re.$$.fragment,t),v(Ue.$$.fragment,t),v(Ge.$$.fragment,t),ar=!0)},o(t){x(m.$$.fragment,t),x(xe.$$.fragment,t),x(ye.$$.fragment,t),x(be.$$.fragment,t),x(X.$$.fragment,t),x(Ee.$$.fragment,t),x($e.$$.fragment,t),x(Fe.$$.fragment,t),x(we.$$.fragment,t),x(te.$$.fragment,t),x(Ie.$$.fragment,t),x(Te.$$.fragment,t),x(Pe.$$.fragment,t),x(ke.$$.fragment,t),x(De.$$.fragment,t),x(Me.$$.fragment,t),x(Le.$$.fragment,t),x(Ne.$$.fragment,t),x(Be.$$.fragment,t),x(Ae.$$.fragment,t),x(Ve.$$.fragment,t),x(Oe.$$.fragment,t),x(je.$$.fragment,t),x(Re.$$.fragment,t),x(Ue.$$.fragment,t),x(Ge.$$.fragment,t),ar=!1},d(t){r(u),t&&r(L),t&&r($),y(m),t&&r(k),t&&r(z),t&&r(Yt),t&&r(j),y(xe),t&&r(Kt),t&&r(D),y(ye),y(be),y(X),y(Ee),t&&r(Qt),t&&r(R),y($e),t&&r(Xt),t&&r(S),y(Fe),y(we),y(te),t&&r(Zt),t&&r(H),y(Ie),t&&r(er),t&&r(I),y(Te),y(Pe),y(ke),t&&r(tr),t&&r(J),y(De),t&&r(rr),t&&r(p),y(Me),y(Le),y(Ne),y(Be),y(Ae),y(Ve),y(Oe),y(je),y(Re),y(Ue),y(Ge)}}}const Hn={local:"feature-extractor",sections:[{local:"transformers.FeatureExtractionMixin",title:"FeatureExtractionMixin"},{local:"transformers.SequenceFeatureExtractor",title:"SequenceFeatureExtractor"},{local:"transformers.BatchFeature",title:"BatchFeature"},{local:"transformers.ImageFeatureExtractionMixin",title:"ImageFeatureExtractionMixin"}],title:"Feature Extractor"};function Gn(Ke){return Cn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class es extends qn{constructor(u){super();Nn(this,u,Gn,Un,Sn,{})}}export{es as default,Hn as metadata};
