import{S as Bn,i as Cn,s as An,e as a,k as c,w as h,t as s,M as Vn,c as o,d as r,m as d,a as n,x as g,h as i,b as l,G as e,g as w,y as _,q as v,o as x,B as y,v as jn,L as On}from"../../chunks/vendor-hf-doc-builder.js";import{T as Sn}from"../../chunks/Tip-hf-doc-builder.js";import{D as I}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Wn}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Kt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Rn}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Un(Q){let m,k,b,$,T;return{c(){m=a("p"),k=s("Passing "),b=a("code"),$=s("use_auth_token=True"),T=s(" is required when you want to use a private model.")},l(p){m=o(p,"P",{});var F=n(m);k=i(F,"Passing "),b=o(F,"CODE",{});var q=n(b);$=i(q,"use_auth_token=True"),q.forEach(r),T=i(F," is required when you want to use a private model."),F.forEach(r)},m(p,F){w(p,m,F),e(m,k),e(m,b),e(b,$),e(m,T)},d(p){p&&r(m)}}}function Hn(Q){let m,k,b,$,T;return $=new Wn({props:{code:`# We can't instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let's show the examples on a
# derived class: *Wav2Vec2FeatureExtractor*
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h"
)  # Download feature_extraction_config from huggingface.co and cache.
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "./test/saved_model/"
)  # E.g. feature_extractor (or model) was saved using *save_pretrained('./test/saved_model/')*
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("./test/saved_model/preprocessor_config.json")
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h", return_attention_mask=False, foo=False
)
assert feature_extractor.return_attention_mask is False
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h", return_attention_mask=False, foo=False, return_unused_kwargs=True
)
assert feature_extractor.return_attention_mask is False
assert unused_kwargs == {"foo": False}`,highlighted:`<span class="hljs-comment"># We can&#x27;t instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let&#x27;s show the examples on a</span>
<span class="hljs-comment"># derived class: *Wav2Vec2FeatureExtractor*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>
)  <span class="hljs-comment"># Download feature_extraction_config from huggingface.co and cache.</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;./test/saved_model/&quot;</span>
)  <span class="hljs-comment"># E.g. feature_extractor (or model) was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/preprocessor_config.json&quot;</span>)
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
<span class="hljs-keyword">assert</span> unused_kwargs == {<span class="hljs-string">&quot;foo&quot;</span>: <span class="hljs-literal">False</span>}`}}),{c(){m=a("p"),k=s("Examples:"),b=c(),h($.$$.fragment)},l(p){m=o(p,"P",{});var F=n(m);k=i(F,"Examples:"),F.forEach(r),b=d(p),g($.$$.fragment,p)},m(p,F){w(p,m,F),e(m,k),w(p,b,F),_($,p,F),T=!0},p:On,i(p){T||(v($.$$.fragment,p),T=!0)},o(p){x($.$$.fragment,p),T=!1},d(p){p&&r(m),p&&r(b),y($,p)}}}function Gn(Q){let m,k,b,$,T,p,F,q;return{c(){m=a("p"),k=s("If the "),b=a("code"),$=s("processed_features"),T=s(` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the
result will use the same type unless you provide a different tensor type with `),p=a("code"),F=s("return_tensors"),q=s(`. In the case of
PyTorch tensors, you will lose the specific device of your tensors however.`)},l(j){m=o(j,"P",{});var z=n(m);k=i(z,"If the "),b=o(z,"CODE",{});var D=n(b);$=i(D,"processed_features"),D.forEach(r),T=i(z,` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the
result will use the same type unless you provide a different tensor type with `),p=o(z,"CODE",{});var Ze=n(p);F=i(Ze,"return_tensors"),Ze.forEach(r),q=i(z,`. In the case of
PyTorch tensors, you will lose the specific device of your tensors however.`),z.forEach(r)},m(j,z){w(j,m,z),e(m,k),e(m,b),e(b,$),e(m,T),e(m,p),e(p,F),e(m,q)},d(j){j&&r(m)}}}function Jn(Q){let m,k,b,$,T,p,F,q,j,z,D,Ze,mt,zr,Dr,pt,Mr,Lr,Qt,O,X,ut,$e,qr,ft,Nr,Xt,M,Ee,Sr,ht,Br,Cr,N,we,Ar,B,Vr,et,jr,Or,gt,Wr,Rr,tt,Ur,Hr,Gr,Z,Jr,ee,Yr,te,Fe,Kr,W,Qr,_t,Xr,Zr,rt,ea,ta,Zt,R,re,vt,Ie,ra,xt,aa,er,C,Te,oa,yt,na,sa,S,ke,ia,bt,ca,da,U,la,$t,ma,pa,Et,ua,fa,ha,ae,tr,H,oe,wt,Pe,ga,Ft,_a,rr,P,ze,va,G,xa,at,ya,ba,It,$a,Ea,wa,Tt,Fa,Ia,ne,De,Ta,kt,ka,Pa,se,Me,za,Le,Da,Pt,Ma,La,ar,J,ie,zt,qe,qa,Dt,Na,or,f,Ne,Sa,Mt,Ba,Ca,ce,Se,Aa,Be,Va,Lt,ja,Oa,Wa,de,Ce,Ra,Ae,Ua,qt,Ha,Ga,Ja,le,Ve,Ya,je,Ka,Nt,Qa,Xa,Za,me,Oe,eo,Y,to,St,ro,ao,Bt,oo,no,so,pe,We,io,L,co,Ct,lo,mo,At,po,uo,Vt,fo,ho,jt,go,_o,vo,ue,Re,xo,Ot,yo,bo,fe,Ue,$o,He,Eo,Wt,wo,Fo,Io,he,Ge,To,K,ko,Rt,Po,zo,Ut,Do,Mo,Lo,ge,Je,qo,Ye,No,Ht,So,Bo,Co,_e,Ke,Ao,Qe,Vo,Gt,jo,Oo,nr;return p=new Kt({}),$e=new Kt({}),Ee=new I({props:{name:"class transformers.FeatureExtractionMixin",anchor:"transformers.FeatureExtractionMixin",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L200"}}),we=new I({props:{name:"from_pretrained",anchor:"transformers.FeatureExtractionMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L224",returnDescription:`
<p>A feature extractor of type <a
  href="/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"
>FeatureExtractionMixin</a>.</p>
`}}),Z=new Sn({props:{$$slots:{default:[Un]},$$scope:{ctx:Q}}}),ee=new Rn({props:{anchor:"transformers.FeatureExtractionMixin.from_pretrained.example",$$slots:{default:[Hn]},$$scope:{ctx:Q}}}),Fe=new I({props:{name:"save_pretrained",anchor:"transformers.FeatureExtractionMixin.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike]"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the feature extractor JSON file will be saved (will be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.FeatureExtractionMixin.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).
kwargs &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin.push_to_hub">push_to_hub()</a> method.`,name:"push_to_hub"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L308"}}),Ie=new Kt({}),Te=new I({props:{name:"class transformers.SequenceFeatureExtractor",anchor:"transformers.SequenceFeatureExtractor",parameters:[{name:"feature_size",val:": int"},{name:"sampling_rate",val:": int"},{name:"padding_value",val:": float"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SequenceFeatureExtractor.feature_size",description:`<strong>feature_size</strong> (<code>int</code>) &#x2014;
The feature dimension of the extracted features.`,name:"feature_size"},{anchor:"transformers.SequenceFeatureExtractor.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>) &#x2014;
The sampling rate at which the audio files should be digitalized expressed in Hertz per second (Hz).`,name:"sampling_rate"},{anchor:"transformers.SequenceFeatureExtractor.padding_value",description:`<strong>padding_value</strong> (<code>float</code>) &#x2014;
The value that is used to fill the padding values / vectors.`,name:"padding_value"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_sequence_utils.py#L29"}}),ke=new I({props:{name:"pad",anchor:"transformers.SequenceFeatureExtractor.pad",parameters:[{name:"processed_features",val:": typing.Union[transformers.feature_extraction_utils.BatchFeature, typing.List[transformers.feature_extraction_utils.BatchFeature], typing.Dict[str, transformers.feature_extraction_utils.BatchFeature], typing.Dict[str, typing.List[transformers.feature_extraction_utils.BatchFeature]], typing.List[typing.Dict[str, transformers.feature_extraction_utils.BatchFeature]]]"},{name:"padding",val:": typing.Union[bool, str, transformers.utils.generic.PaddingStrategy] = True"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"truncation",val:": bool = False"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"return_attention_mask",val:": typing.Optional[bool] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"}],parametersDescription:[{anchor:"transformers.SequenceFeatureExtractor.pad.processed_features",description:`<strong>processed_features</strong> (<a href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a>, list of <a href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a>, <code>Dict[str, List[float]]</code>, <code>Dict[str, List[List[float]]</code> or <code>List[Dict[str, List[float]]]</code>) &#x2014;
Processed inputs. Can represent one input (<a href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a> or <code>Dict[str, List[float]]</code>) or a batch of
input values / vectors (list of <a href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a>, <em>Dict[str, List[List[float]]]</em> or <em>List[Dict[str,
List[float]]]</em>) so you can use this method during preprocessing as well as in a PyTorch Dataloader
collate function.</p>
<p>Instead of <code>List[float]</code> you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),
see the note above for the return type.`,name:"processed_features"},{anchor:"transformers.SequenceFeatureExtractor.pad.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Select a strategy to pad the returned sequences (according to the model&#x2019;s padding side and padding
index) among:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.SequenceFeatureExtractor.pad.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum length of the returned list and optionally padding length (see above).`,name:"max_length"},{anchor:"transformers.SequenceFeatureExtractor.pad.truncation",description:`<strong>truncation</strong> (<code>bool</code>) &#x2014;
Activates truncation to cut input sequences longer than <code>max_length</code> to <code>max_length</code>.`,name:"truncation"},{anchor:"transformers.SequenceFeatureExtractor.pad.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability</p>
<blockquote>
<p>= 7.5 (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.</p>
</blockquote>`,name:"pad_to_multiple_of"},{anchor:"transformers.SequenceFeatureExtractor.pad.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific feature_extractor&#x2019;s default.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"},{anchor:"transformers.SequenceFeatureExtractor.pad.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_sequence_utils.py#L52"}}),ae=new Sn({props:{$$slots:{default:[Gn]},$$scope:{ctx:Q}}}),Pe=new Kt({}),ze=new I({props:{name:"class transformers.BatchFeature",anchor:"transformers.BatchFeature",parameters:[{name:"data",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"tensor_type",val:": typing.Union[NoneType, str, transformers.utils.generic.TensorType] = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.data",description:`<strong>data</strong> (<code>dict</code>) &#x2014;
Dictionary of lists/arrays/tensors returned by the <strong>call</strong>/pad methods (&#x2018;input_values&#x2019;, &#x2018;attention_mask&#x2019;,
etc.).`,name:"data"},{anchor:"transformers.BatchFeature.tensor_type",description:`<strong>tensor_type</strong> (<code>Union[None, str, TensorType]</code>, <em>optional</em>) &#x2014;
You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.`,name:"tensor_type"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L58"}}),De=new I({props:{name:"convert_to_tensors",anchor:"transformers.BatchFeature.convert_to_tensors",parameters:[{name:"tensor_type",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.convert_to_tensors.tensor_type",description:`<strong>tensor_type</strong> (<code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
The type of tensors to use. If <code>str</code>, should be one of the values of the enum <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>. If
<code>None</code>, no modification is done.`,name:"tensor_type"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L112"}}),Me=new I({props:{name:"to",anchor:"transformers.BatchFeature.to",parameters:[{name:"device",val:": typing.Union[str, ForwardRef('torch.device')]"}],parametersDescription:[{anchor:"transformers.BatchFeature.to.device",description:"<strong>device</strong> (<code>str</code> or <code>torch.device</code>) &#x2014; The device to put the tensors on.",name:"device"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L177",returnDescription:`
<p>The same instance after modification.</p>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),qe=new Kt({}),Ne=new I({props:{name:"class transformers.ImageFeatureExtractionMixin",anchor:"transformers.ImageFeatureExtractionMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L192"}}),Se=new I({props:{name:"center_crop",anchor:"transformers.ImageFeatureExtractionMixin.center_crop",parameters:[{name:"image",val:""},{name:"size",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.center_crop.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code> of shape (n_channels, height, width) or (height, width, n_channels)) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.center_crop.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple[int, int]</code>) &#x2014;
The size to which crop the image.`,name:"size"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L420",returnDescription:`
<p>A center cropped <code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code> of shape: (n_channels,
height, width).</p>
`,returnType:`
<p>new_image</p>
`}}),Ce=new I({props:{name:"convert_rgb",anchor:"transformers.ImageFeatureExtractionMixin.convert_rgb",parameters:[{name:"image",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.convert_rgb.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code>) &#x2014;
The image to convert.`,name:"image"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L234"}}),Ve=new I({props:{name:"expand_dims",anchor:"transformers.ImageFeatureExtractionMixin.expand_dims",parameters:[{name:"image",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.expand_dims.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to expand.`,name:"image"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L287"}}),Oe=new I({props:{name:"flip_channel_order",anchor:"transformers.ImageFeatureExtractionMixin.flip_channel_order",parameters:[{name:"image",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.flip_channel_order.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image whose color channels to flip. If <code>np.ndarray</code> or <code>torch.Tensor</code>, the channel dimension should
be first.`,name:"image"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L495"}}),We=new I({props:{name:"normalize",anchor:"transformers.ImageFeatureExtractionMixin.normalize",parameters:[{name:"image",val:""},{name:"mean",val:""},{name:"std",val:""},{name:"rescale",val:" = False"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.normalize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to normalize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.normalize.mean",description:`<strong>mean</strong> (<code>List[float]</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The mean (per channel) to use for normalization.`,name:"mean"},{anchor:"transformers.ImageFeatureExtractionMixin.normalize.std",description:`<strong>std</strong> (<code>List[float]</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The standard deviation (per channel) to use for normalization.`,name:"std"},{anchor:"transformers.ImageFeatureExtractionMixin.normalize.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to rescale the image to be between 0 and 1. If a PIL image is provided, scaling will
happen automatically.`,name:"rescale"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L307"}}),Re=new I({props:{name:"rescale",anchor:"transformers.ImageFeatureExtractionMixin.rescale",parameters:[{name:"image",val:": ndarray"},{name:"scale",val:": typing.Union[float, int]"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L248"}}),Ue=new I({props:{name:"resize",anchor:"transformers.ImageFeatureExtractionMixin.resize",parameters:[{name:"image",val:""},{name:"size",val:""},{name:"resample",val:" = None"},{name:"default_to_square",val:" = True"},{name:"max_size",val:" = None"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.resize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple[int, int]</code>) &#x2014;
The size to use for resizing the image. If <code>size</code> is a sequence like (h, w), output size will be
matched to this.</p>
<p>If <code>size</code> is an int and <code>default_to_square</code> is <code>True</code>, then image will be resized to (size, size). If
<code>size</code> is an int and <code>default_to_square</code> is <code>False</code>, then smaller edge of the image will be matched to
this number. i.e, if height &gt; width, then image will be rescaled to (size * height / width, size).`,name:"size"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.Resampling.BILINEAR</code>) &#x2014;
The filter to user for resampling.`,name:"resample"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.default_to_square",description:`<strong>default_to_square</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
How to convert <code>size</code> when it is a single int. If set to <code>True</code>, the <code>size</code> will be converted to a
square (<code>size</code>,<code>size</code>). If set to <code>False</code>, will replicate
<a href="https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Resize" rel="nofollow"><code>torchvision.transforms.Resize</code></a>
with support for resizing only the smallest edge and providing an optional <code>max_size</code>.`,name:"default_to_square"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.max_size",description:`<strong>max_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The maximum allowed for the longer edge of the resized image: if the longer edge of the image is
greater than <code>max_size</code> after being resized according to <code>size</code>, then the image is resized again so
that the longer edge is equal to <code>max_size</code>. As a result, <code>size</code> might be overruled, i.e the smaller
edge may be shorter than <code>size</code>. Only used if <code>default_to_square</code> is <code>False</code>.`,name:"max_size"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L353",returnDescription:`
<p>A resized <code>PIL.Image.Image</code>.</p>
`,returnType:`
<p>image</p>
`}}),Ge=new I({props:{name:"rotate",anchor:"transformers.ImageFeatureExtractionMixin.rotate",parameters:[{name:"image",val:""},{name:"angle",val:""},{name:"resample",val:" = None"},{name:"expand",val:" = 0"},{name:"center",val:" = None"},{name:"translate",val:" = None"},{name:"fillcolor",val:" = None"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.rotate.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to rotate. If <code>np.ndarray</code> or <code>torch.Tensor</code>, will be converted to <code>PIL.Image.Image</code> before
rotating.`,name:"image"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L512",returnDescription:`
<p>A rotated <code>PIL.Image.Image</code>.</p>
`,returnType:`
<p>image</p>
`}}),Je=new I({props:{name:"to_numpy_array",anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array",parameters:[{name:"image",val:""},{name:"rescale",val:" = None"},{name:"channel_first",val:" = True"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to convert to a NumPy array.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values floats between 0. and 1.). Will
default to <code>True</code> if the image is a PIL Image or an array/tensor of integers, <code>False</code> otherwise.`,name:"rescale"},{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.channel_first",description:`<strong>channel_first</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to permute the dimensions of the image to put the channel dimension first.`,name:"channel_first"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L255"}}),Ke=new I({props:{name:"to_pil_image",anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image",parameters:[{name:"image",val:""},{name:"rescale",val:" = None"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>numpy.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to convert to the PIL Image format.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values integers between 0 and 255). Will
default to <code>True</code> if the image type is a floating type, <code>False</code> otherwise.`,name:"rescale"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_utils.py#L204"}}),{c(){m=a("meta"),k=c(),b=a("h1"),$=a("a"),T=a("span"),h(p.$$.fragment),F=c(),q=a("span"),j=s("Feature Extractor"),z=c(),D=a("p"),Ze=s(`A feature extractor is in charge of preparing input features for audio or vision models. This includes feature extraction
from sequences, `),mt=a("em"),zr=s("e.g."),Dr=s(`, pre-processing audio files to Log-Mel Spectrogram features, feature extraction from images
`),pt=a("em"),Mr=s("e.g."),Lr=s(` cropping image image files, but also padding, normalization, and conversion to Numpy, PyTorch, and TensorFlow
tensors.`),Qt=c(),O=a("h2"),X=a("a"),ut=a("span"),h($e.$$.fragment),qr=c(),ft=a("span"),Nr=s("FeatureExtractionMixin"),Xt=c(),M=a("div"),h(Ee.$$.fragment),Sr=c(),ht=a("p"),Br=s(`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),Cr=c(),N=a("div"),h(we.$$.fragment),Ar=c(),B=a("p"),Vr=s("Instantiate a type of "),et=a("a"),jr=s("FeatureExtractionMixin"),Or=s(" from a feature extractor, "),gt=a("em"),Wr=s("e.g."),Rr=s(` a
derived class of `),tt=a("a"),Ur=s("SequenceFeatureExtractor"),Hr=s("."),Gr=c(),h(Z.$$.fragment),Jr=c(),h(ee.$$.fragment),Yr=c(),te=a("div"),h(Fe.$$.fragment),Kr=c(),W=a("p"),Qr=s("Save a feature_extractor object to the directory "),_t=a("code"),Xr=s("save_directory"),Zr=s(`, so that it can be re-loaded using the
`),rt=a("a"),ea=s("from_pretrained()"),ta=s(" class method."),Zt=c(),R=a("h2"),re=a("a"),vt=a("span"),h(Ie.$$.fragment),ra=c(),xt=a("span"),aa=s("SequenceFeatureExtractor"),er=c(),C=a("div"),h(Te.$$.fragment),oa=c(),yt=a("p"),na=s("This is a general feature extraction class for speech recognition."),sa=c(),S=a("div"),h(ke.$$.fragment),ia=c(),bt=a("p"),ca=s(`Pad input values / input vectors or a batch of input values / input vectors up to predefined length or to the
max sequence length in the batch.`),da=c(),U=a("p"),la=s("Padding side (left/right) padding values are defined at the feature extractor level (with "),$t=a("code"),ma=s("self.padding_side"),pa=s(`,
`),Et=a("code"),ua=s("self.padding_value"),fa=s(")"),ha=c(),h(ae.$$.fragment),tr=c(),H=a("h2"),oe=a("a"),wt=a("span"),h(Pe.$$.fragment),ga=c(),Ft=a("span"),_a=s("BatchFeature"),rr=c(),P=a("div"),h(ze.$$.fragment),va=c(),G=a("p"),xa=s("Holds the output of the "),at=a("a"),ya=s("pad()"),ba=s(" and feature extractor specific "),It=a("code"),$a=s("__call__"),Ea=s(" methods."),wa=c(),Tt=a("p"),Fa=s("This class is derived from a python dictionary and can be used as a dictionary."),Ia=c(),ne=a("div"),h(De.$$.fragment),Ta=c(),kt=a("p"),ka=s("Convert the inner content to tensors."),Pa=c(),se=a("div"),h(Me.$$.fragment),za=c(),Le=a("p"),Da=s("Send all values to device by calling "),Pt=a("code"),Ma=s("v.to(device)"),La=s(" (PyTorch only)."),ar=c(),J=a("h2"),ie=a("a"),zt=a("span"),h(qe.$$.fragment),qa=c(),Dt=a("span"),Na=s("ImageFeatureExtractionMixin"),or=c(),f=a("div"),h(Ne.$$.fragment),Sa=c(),Mt=a("p"),Ba=s("Mixin that contain utilities for preparing image features."),Ca=c(),ce=a("div"),h(Se.$$.fragment),Aa=c(),Be=a("p"),Va=s("Crops "),Lt=a("code"),ja=s("image"),Oa=s(` to the given size using a center crop. Note that if the image is too small to be cropped to the
size given, it will be padded (so the returned result has the size asked).`),Wa=c(),de=a("div"),h(Ce.$$.fragment),Ra=c(),Ae=a("p"),Ua=s("Converts "),qt=a("code"),Ha=s("PIL.Image.Image"),Ga=s(" to RGB format."),Ja=c(),le=a("div"),h(Ve.$$.fragment),Ya=c(),je=a("p"),Ka=s("Expands 2-dimensional "),Nt=a("code"),Qa=s("image"),Xa=s(" to 3 dimensions."),Za=c(),me=a("div"),h(Oe.$$.fragment),eo=c(),Y=a("p"),to=s("Flips the channel order of "),St=a("code"),ro=s("image"),ao=s(` from RGB to BGR, or vice versa. Note that this will trigger a conversion of
`),Bt=a("code"),oo=s("image"),no=s(" to a NumPy array if it\u2019s a PIL Image."),so=c(),pe=a("div"),h(We.$$.fragment),io=c(),L=a("p"),co=s("Normalizes "),Ct=a("code"),lo=s("image"),mo=s(" with "),At=a("code"),po=s("mean"),uo=s(" and "),Vt=a("code"),fo=s("std"),ho=s(". Note that this will trigger a conversion of "),jt=a("code"),go=s("image"),_o=s(` to a NumPy array
if it\u2019s a PIL Image.`),vo=c(),ue=a("div"),h(Re.$$.fragment),xo=c(),Ot=a("p"),yo=s("Rescale a numpy image by scale amount"),bo=c(),fe=a("div"),h(Ue.$$.fragment),$o=c(),He=a("p"),Eo=s("Resizes "),Wt=a("code"),wo=s("image"),Fo=s(". Enforces conversion of input to PIL.Image."),Io=c(),he=a("div"),h(Ge.$$.fragment),To=c(),K=a("p"),ko=s("Returns a rotated copy of "),Rt=a("code"),Po=s("image"),zo=s(". This method returns a copy of "),Ut=a("code"),Do=s("image"),Mo=s(`, rotated the given number of degrees
counter clockwise around its centre.`),Lo=c(),ge=a("div"),h(Je.$$.fragment),qo=c(),Ye=a("p"),No=s("Converts "),Ht=a("code"),So=s("image"),Bo=s(` to a numpy array. Optionally rescales it and puts the channel dimension as the first
dimension.`),Co=c(),_e=a("div"),h(Ke.$$.fragment),Ao=c(),Qe=a("p"),Vo=s("Converts "),Gt=a("code"),jo=s("image"),Oo=s(` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),this.h()},l(t){const u=Vn('[data-svelte="svelte-1phssyn"]',document.head);m=o(u,"META",{name:!0,content:!0}),u.forEach(r),k=d(t),b=o(t,"H1",{class:!0});var Xe=n(b);$=o(Xe,"A",{id:!0,class:!0,href:!0});var Jt=n($);T=o(Jt,"SPAN",{});var Yt=n(T);g(p.$$.fragment,Yt),Yt.forEach(r),Jt.forEach(r),F=d(Xe),q=o(Xe,"SPAN",{});var Wo=n(q);j=i(Wo,"Feature Extractor"),Wo.forEach(r),Xe.forEach(r),z=d(t),D=o(t,"P",{});var ot=n(D);Ze=i(ot,`A feature extractor is in charge of preparing input features for audio or vision models. This includes feature extraction
from sequences, `),mt=o(ot,"EM",{});var Ro=n(mt);zr=i(Ro,"e.g."),Ro.forEach(r),Dr=i(ot,`, pre-processing audio files to Log-Mel Spectrogram features, feature extraction from images
`),pt=o(ot,"EM",{});var Uo=n(pt);Mr=i(Uo,"e.g."),Uo.forEach(r),Lr=i(ot,` cropping image image files, but also padding, normalization, and conversion to Numpy, PyTorch, and TensorFlow
tensors.`),ot.forEach(r),Qt=d(t),O=o(t,"H2",{class:!0});var sr=n(O);X=o(sr,"A",{id:!0,class:!0,href:!0});var Ho=n(X);ut=o(Ho,"SPAN",{});var Go=n(ut);g($e.$$.fragment,Go),Go.forEach(r),Ho.forEach(r),qr=d(sr),ft=o(sr,"SPAN",{});var Jo=n(ft);Nr=i(Jo,"FeatureExtractionMixin"),Jo.forEach(r),sr.forEach(r),Xt=d(t),M=o(t,"DIV",{class:!0});var ve=n(M);g(Ee.$$.fragment,ve),Sr=d(ve),ht=o(ve,"P",{});var Yo=n(ht);Br=i(Yo,`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),Yo.forEach(r),Cr=d(ve),N=o(ve,"DIV",{class:!0});var xe=n(N);g(we.$$.fragment,xe),Ar=d(xe),B=o(xe,"P",{});var ye=n(B);Vr=i(ye,"Instantiate a type of "),et=o(ye,"A",{href:!0});var Ko=n(et);jr=i(Ko,"FeatureExtractionMixin"),Ko.forEach(r),Or=i(ye," from a feature extractor, "),gt=o(ye,"EM",{});var Qo=n(gt);Wr=i(Qo,"e.g."),Qo.forEach(r),Rr=i(ye,` a
derived class of `),tt=o(ye,"A",{href:!0});var Xo=n(tt);Ur=i(Xo,"SequenceFeatureExtractor"),Xo.forEach(r),Hr=i(ye,"."),ye.forEach(r),Gr=d(xe),g(Z.$$.fragment,xe),Jr=d(xe),g(ee.$$.fragment,xe),xe.forEach(r),Yr=d(ve),te=o(ve,"DIV",{class:!0});var ir=n(te);g(Fe.$$.fragment,ir),Kr=d(ir),W=o(ir,"P",{});var nt=n(W);Qr=i(nt,"Save a feature_extractor object to the directory "),_t=o(nt,"CODE",{});var Zo=n(_t);Xr=i(Zo,"save_directory"),Zo.forEach(r),Zr=i(nt,`, so that it can be re-loaded using the
`),rt=o(nt,"A",{href:!0});var en=n(rt);ea=i(en,"from_pretrained()"),en.forEach(r),ta=i(nt," class method."),nt.forEach(r),ir.forEach(r),ve.forEach(r),Zt=d(t),R=o(t,"H2",{class:!0});var cr=n(R);re=o(cr,"A",{id:!0,class:!0,href:!0});var tn=n(re);vt=o(tn,"SPAN",{});var rn=n(vt);g(Ie.$$.fragment,rn),rn.forEach(r),tn.forEach(r),ra=d(cr),xt=o(cr,"SPAN",{});var an=n(xt);aa=i(an,"SequenceFeatureExtractor"),an.forEach(r),cr.forEach(r),er=d(t),C=o(t,"DIV",{class:!0});var st=n(C);g(Te.$$.fragment,st),oa=d(st),yt=o(st,"P",{});var on=n(yt);na=i(on,"This is a general feature extraction class for speech recognition."),on.forEach(r),sa=d(st),S=o(st,"DIV",{class:!0});var be=n(S);g(ke.$$.fragment,be),ia=d(be),bt=o(be,"P",{});var nn=n(bt);ca=i(nn,`Pad input values / input vectors or a batch of input values / input vectors up to predefined length or to the
max sequence length in the batch.`),nn.forEach(r),da=d(be),U=o(be,"P",{});var it=n(U);la=i(it,"Padding side (left/right) padding values are defined at the feature extractor level (with "),$t=o(it,"CODE",{});var sn=n($t);ma=i(sn,"self.padding_side"),sn.forEach(r),pa=i(it,`,
`),Et=o(it,"CODE",{});var cn=n(Et);ua=i(cn,"self.padding_value"),cn.forEach(r),fa=i(it,")"),it.forEach(r),ha=d(be),g(ae.$$.fragment,be),be.forEach(r),st.forEach(r),tr=d(t),H=o(t,"H2",{class:!0});var dr=n(H);oe=o(dr,"A",{id:!0,class:!0,href:!0});var dn=n(oe);wt=o(dn,"SPAN",{});var ln=n(wt);g(Pe.$$.fragment,ln),ln.forEach(r),dn.forEach(r),ga=d(dr),Ft=o(dr,"SPAN",{});var mn=n(Ft);_a=i(mn,"BatchFeature"),mn.forEach(r),dr.forEach(r),rr=d(t),P=o(t,"DIV",{class:!0});var A=n(P);g(ze.$$.fragment,A),va=d(A),G=o(A,"P",{});var ct=n(G);xa=i(ct,"Holds the output of the "),at=o(ct,"A",{href:!0});var pn=n(at);ya=i(pn,"pad()"),pn.forEach(r),ba=i(ct," and feature extractor specific "),It=o(ct,"CODE",{});var un=n(It);$a=i(un,"__call__"),un.forEach(r),Ea=i(ct," methods."),ct.forEach(r),wa=d(A),Tt=o(A,"P",{});var fn=n(Tt);Fa=i(fn,"This class is derived from a python dictionary and can be used as a dictionary."),fn.forEach(r),Ia=d(A),ne=o(A,"DIV",{class:!0});var lr=n(ne);g(De.$$.fragment,lr),Ta=d(lr),kt=o(lr,"P",{});var hn=n(kt);ka=i(hn,"Convert the inner content to tensors."),hn.forEach(r),lr.forEach(r),Pa=d(A),se=o(A,"DIV",{class:!0});var mr=n(se);g(Me.$$.fragment,mr),za=d(mr),Le=o(mr,"P",{});var pr=n(Le);Da=i(pr,"Send all values to device by calling "),Pt=o(pr,"CODE",{});var gn=n(Pt);Ma=i(gn,"v.to(device)"),gn.forEach(r),La=i(pr," (PyTorch only)."),pr.forEach(r),mr.forEach(r),A.forEach(r),ar=d(t),J=o(t,"H2",{class:!0});var ur=n(J);ie=o(ur,"A",{id:!0,class:!0,href:!0});var _n=n(ie);zt=o(_n,"SPAN",{});var vn=n(zt);g(qe.$$.fragment,vn),vn.forEach(r),_n.forEach(r),qa=d(ur),Dt=o(ur,"SPAN",{});var xn=n(Dt);Na=i(xn,"ImageFeatureExtractionMixin"),xn.forEach(r),ur.forEach(r),or=d(t),f=o(t,"DIV",{class:!0});var E=n(f);g(Ne.$$.fragment,E),Sa=d(E),Mt=o(E,"P",{});var yn=n(Mt);Ba=i(yn,"Mixin that contain utilities for preparing image features."),yn.forEach(r),Ca=d(E),ce=o(E,"DIV",{class:!0});var fr=n(ce);g(Se.$$.fragment,fr),Aa=d(fr),Be=o(fr,"P",{});var hr=n(Be);Va=i(hr,"Crops "),Lt=o(hr,"CODE",{});var bn=n(Lt);ja=i(bn,"image"),bn.forEach(r),Oa=i(hr,` to the given size using a center crop. Note that if the image is too small to be cropped to the
size given, it will be padded (so the returned result has the size asked).`),hr.forEach(r),fr.forEach(r),Wa=d(E),de=o(E,"DIV",{class:!0});var gr=n(de);g(Ce.$$.fragment,gr),Ra=d(gr),Ae=o(gr,"P",{});var _r=n(Ae);Ua=i(_r,"Converts "),qt=o(_r,"CODE",{});var $n=n(qt);Ha=i($n,"PIL.Image.Image"),$n.forEach(r),Ga=i(_r," to RGB format."),_r.forEach(r),gr.forEach(r),Ja=d(E),le=o(E,"DIV",{class:!0});var vr=n(le);g(Ve.$$.fragment,vr),Ya=d(vr),je=o(vr,"P",{});var xr=n(je);Ka=i(xr,"Expands 2-dimensional "),Nt=o(xr,"CODE",{});var En=n(Nt);Qa=i(En,"image"),En.forEach(r),Xa=i(xr," to 3 dimensions."),xr.forEach(r),vr.forEach(r),Za=d(E),me=o(E,"DIV",{class:!0});var yr=n(me);g(Oe.$$.fragment,yr),eo=d(yr),Y=o(yr,"P",{});var dt=n(Y);to=i(dt,"Flips the channel order of "),St=o(dt,"CODE",{});var wn=n(St);ro=i(wn,"image"),wn.forEach(r),ao=i(dt,` from RGB to BGR, or vice versa. Note that this will trigger a conversion of
`),Bt=o(dt,"CODE",{});var Fn=n(Bt);oo=i(Fn,"image"),Fn.forEach(r),no=i(dt," to a NumPy array if it\u2019s a PIL Image."),dt.forEach(r),yr.forEach(r),so=d(E),pe=o(E,"DIV",{class:!0});var br=n(pe);g(We.$$.fragment,br),io=d(br),L=o(br,"P",{});var V=n(L);co=i(V,"Normalizes "),Ct=o(V,"CODE",{});var In=n(Ct);lo=i(In,"image"),In.forEach(r),mo=i(V," with "),At=o(V,"CODE",{});var Tn=n(At);po=i(Tn,"mean"),Tn.forEach(r),uo=i(V," and "),Vt=o(V,"CODE",{});var kn=n(Vt);fo=i(kn,"std"),kn.forEach(r),ho=i(V,". Note that this will trigger a conversion of "),jt=o(V,"CODE",{});var Pn=n(jt);go=i(Pn,"image"),Pn.forEach(r),_o=i(V,` to a NumPy array
if it\u2019s a PIL Image.`),V.forEach(r),br.forEach(r),vo=d(E),ue=o(E,"DIV",{class:!0});var $r=n(ue);g(Re.$$.fragment,$r),xo=d($r),Ot=o($r,"P",{});var zn=n(Ot);yo=i(zn,"Rescale a numpy image by scale amount"),zn.forEach(r),$r.forEach(r),bo=d(E),fe=o(E,"DIV",{class:!0});var Er=n(fe);g(Ue.$$.fragment,Er),$o=d(Er),He=o(Er,"P",{});var wr=n(He);Eo=i(wr,"Resizes "),Wt=o(wr,"CODE",{});var Dn=n(Wt);wo=i(Dn,"image"),Dn.forEach(r),Fo=i(wr,". Enforces conversion of input to PIL.Image."),wr.forEach(r),Er.forEach(r),Io=d(E),he=o(E,"DIV",{class:!0});var Fr=n(he);g(Ge.$$.fragment,Fr),To=d(Fr),K=o(Fr,"P",{});var lt=n(K);ko=i(lt,"Returns a rotated copy of "),Rt=o(lt,"CODE",{});var Mn=n(Rt);Po=i(Mn,"image"),Mn.forEach(r),zo=i(lt,". This method returns a copy of "),Ut=o(lt,"CODE",{});var Ln=n(Ut);Do=i(Ln,"image"),Ln.forEach(r),Mo=i(lt,`, rotated the given number of degrees
counter clockwise around its centre.`),lt.forEach(r),Fr.forEach(r),Lo=d(E),ge=o(E,"DIV",{class:!0});var Ir=n(ge);g(Je.$$.fragment,Ir),qo=d(Ir),Ye=o(Ir,"P",{});var Tr=n(Ye);No=i(Tr,"Converts "),Ht=o(Tr,"CODE",{});var qn=n(Ht);So=i(qn,"image"),qn.forEach(r),Bo=i(Tr,` to a numpy array. Optionally rescales it and puts the channel dimension as the first
dimension.`),Tr.forEach(r),Ir.forEach(r),Co=d(E),_e=o(E,"DIV",{class:!0});var kr=n(_e);g(Ke.$$.fragment,kr),Ao=d(kr),Qe=o(kr,"P",{});var Pr=n(Qe);Vo=i(Pr,"Converts "),Gt=o(Pr,"CODE",{});var Nn=n(Gt);jo=i(Nn,"image"),Nn.forEach(r),Oo=i(Pr,` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),Pr.forEach(r),kr.forEach(r),E.forEach(r),this.h()},h(){l(m,"name","hf:doc:metadata"),l(m,"content",JSON.stringify(Yn)),l($,"id","feature-extractor"),l($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l($,"href","#feature-extractor"),l(b,"class","relative group"),l(X,"id","transformers.FeatureExtractionMixin"),l(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(X,"href","#transformers.FeatureExtractionMixin"),l(O,"class","relative group"),l(et,"href","/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"),l(tt,"href","/docs/transformers/main/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor"),l(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(rt,"href","/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin.from_pretrained"),l(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(re,"id","transformers.SequenceFeatureExtractor"),l(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(re,"href","#transformers.SequenceFeatureExtractor"),l(R,"class","relative group"),l(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(oe,"id","transformers.BatchFeature"),l(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(oe,"href","#transformers.BatchFeature"),l(H,"class","relative group"),l(at,"href","/docs/transformers/main/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad"),l(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ie,"id","transformers.ImageFeatureExtractionMixin"),l(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ie,"href","#transformers.ImageFeatureExtractionMixin"),l(J,"class","relative group"),l(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(_e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),l(f,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,u){e(document.head,m),w(t,k,u),w(t,b,u),e(b,$),e($,T),_(p,T,null),e(b,F),e(b,q),e(q,j),w(t,z,u),w(t,D,u),e(D,Ze),e(D,mt),e(mt,zr),e(D,Dr),e(D,pt),e(pt,Mr),e(D,Lr),w(t,Qt,u),w(t,O,u),e(O,X),e(X,ut),_($e,ut,null),e(O,qr),e(O,ft),e(ft,Nr),w(t,Xt,u),w(t,M,u),_(Ee,M,null),e(M,Sr),e(M,ht),e(ht,Br),e(M,Cr),e(M,N),_(we,N,null),e(N,Ar),e(N,B),e(B,Vr),e(B,et),e(et,jr),e(B,Or),e(B,gt),e(gt,Wr),e(B,Rr),e(B,tt),e(tt,Ur),e(B,Hr),e(N,Gr),_(Z,N,null),e(N,Jr),_(ee,N,null),e(M,Yr),e(M,te),_(Fe,te,null),e(te,Kr),e(te,W),e(W,Qr),e(W,_t),e(_t,Xr),e(W,Zr),e(W,rt),e(rt,ea),e(W,ta),w(t,Zt,u),w(t,R,u),e(R,re),e(re,vt),_(Ie,vt,null),e(R,ra),e(R,xt),e(xt,aa),w(t,er,u),w(t,C,u),_(Te,C,null),e(C,oa),e(C,yt),e(yt,na),e(C,sa),e(C,S),_(ke,S,null),e(S,ia),e(S,bt),e(bt,ca),e(S,da),e(S,U),e(U,la),e(U,$t),e($t,ma),e(U,pa),e(U,Et),e(Et,ua),e(U,fa),e(S,ha),_(ae,S,null),w(t,tr,u),w(t,H,u),e(H,oe),e(oe,wt),_(Pe,wt,null),e(H,ga),e(H,Ft),e(Ft,_a),w(t,rr,u),w(t,P,u),_(ze,P,null),e(P,va),e(P,G),e(G,xa),e(G,at),e(at,ya),e(G,ba),e(G,It),e(It,$a),e(G,Ea),e(P,wa),e(P,Tt),e(Tt,Fa),e(P,Ia),e(P,ne),_(De,ne,null),e(ne,Ta),e(ne,kt),e(kt,ka),e(P,Pa),e(P,se),_(Me,se,null),e(se,za),e(se,Le),e(Le,Da),e(Le,Pt),e(Pt,Ma),e(Le,La),w(t,ar,u),w(t,J,u),e(J,ie),e(ie,zt),_(qe,zt,null),e(J,qa),e(J,Dt),e(Dt,Na),w(t,or,u),w(t,f,u),_(Ne,f,null),e(f,Sa),e(f,Mt),e(Mt,Ba),e(f,Ca),e(f,ce),_(Se,ce,null),e(ce,Aa),e(ce,Be),e(Be,Va),e(Be,Lt),e(Lt,ja),e(Be,Oa),e(f,Wa),e(f,de),_(Ce,de,null),e(de,Ra),e(de,Ae),e(Ae,Ua),e(Ae,qt),e(qt,Ha),e(Ae,Ga),e(f,Ja),e(f,le),_(Ve,le,null),e(le,Ya),e(le,je),e(je,Ka),e(je,Nt),e(Nt,Qa),e(je,Xa),e(f,Za),e(f,me),_(Oe,me,null),e(me,eo),e(me,Y),e(Y,to),e(Y,St),e(St,ro),e(Y,ao),e(Y,Bt),e(Bt,oo),e(Y,no),e(f,so),e(f,pe),_(We,pe,null),e(pe,io),e(pe,L),e(L,co),e(L,Ct),e(Ct,lo),e(L,mo),e(L,At),e(At,po),e(L,uo),e(L,Vt),e(Vt,fo),e(L,ho),e(L,jt),e(jt,go),e(L,_o),e(f,vo),e(f,ue),_(Re,ue,null),e(ue,xo),e(ue,Ot),e(Ot,yo),e(f,bo),e(f,fe),_(Ue,fe,null),e(fe,$o),e(fe,He),e(He,Eo),e(He,Wt),e(Wt,wo),e(He,Fo),e(f,Io),e(f,he),_(Ge,he,null),e(he,To),e(he,K),e(K,ko),e(K,Rt),e(Rt,Po),e(K,zo),e(K,Ut),e(Ut,Do),e(K,Mo),e(f,Lo),e(f,ge),_(Je,ge,null),e(ge,qo),e(ge,Ye),e(Ye,No),e(Ye,Ht),e(Ht,So),e(Ye,Bo),e(f,Co),e(f,_e),_(Ke,_e,null),e(_e,Ao),e(_e,Qe),e(Qe,Vo),e(Qe,Gt),e(Gt,jo),e(Qe,Oo),nr=!0},p(t,[u]){const Xe={};u&2&&(Xe.$$scope={dirty:u,ctx:t}),Z.$set(Xe);const Jt={};u&2&&(Jt.$$scope={dirty:u,ctx:t}),ee.$set(Jt);const Yt={};u&2&&(Yt.$$scope={dirty:u,ctx:t}),ae.$set(Yt)},i(t){nr||(v(p.$$.fragment,t),v($e.$$.fragment,t),v(Ee.$$.fragment,t),v(we.$$.fragment,t),v(Z.$$.fragment,t),v(ee.$$.fragment,t),v(Fe.$$.fragment,t),v(Ie.$$.fragment,t),v(Te.$$.fragment,t),v(ke.$$.fragment,t),v(ae.$$.fragment,t),v(Pe.$$.fragment,t),v(ze.$$.fragment,t),v(De.$$.fragment,t),v(Me.$$.fragment,t),v(qe.$$.fragment,t),v(Ne.$$.fragment,t),v(Se.$$.fragment,t),v(Ce.$$.fragment,t),v(Ve.$$.fragment,t),v(Oe.$$.fragment,t),v(We.$$.fragment,t),v(Re.$$.fragment,t),v(Ue.$$.fragment,t),v(Ge.$$.fragment,t),v(Je.$$.fragment,t),v(Ke.$$.fragment,t),nr=!0)},o(t){x(p.$$.fragment,t),x($e.$$.fragment,t),x(Ee.$$.fragment,t),x(we.$$.fragment,t),x(Z.$$.fragment,t),x(ee.$$.fragment,t),x(Fe.$$.fragment,t),x(Ie.$$.fragment,t),x(Te.$$.fragment,t),x(ke.$$.fragment,t),x(ae.$$.fragment,t),x(Pe.$$.fragment,t),x(ze.$$.fragment,t),x(De.$$.fragment,t),x(Me.$$.fragment,t),x(qe.$$.fragment,t),x(Ne.$$.fragment,t),x(Se.$$.fragment,t),x(Ce.$$.fragment,t),x(Ve.$$.fragment,t),x(Oe.$$.fragment,t),x(We.$$.fragment,t),x(Re.$$.fragment,t),x(Ue.$$.fragment,t),x(Ge.$$.fragment,t),x(Je.$$.fragment,t),x(Ke.$$.fragment,t),nr=!1},d(t){r(m),t&&r(k),t&&r(b),y(p),t&&r(z),t&&r(D),t&&r(Qt),t&&r(O),y($e),t&&r(Xt),t&&r(M),y(Ee),y(we),y(Z),y(ee),y(Fe),t&&r(Zt),t&&r(R),y(Ie),t&&r(er),t&&r(C),y(Te),y(ke),y(ae),t&&r(tr),t&&r(H),y(Pe),t&&r(rr),t&&r(P),y(ze),y(De),y(Me),t&&r(ar),t&&r(J),y(qe),t&&r(or),t&&r(f),y(Ne),y(Se),y(Ce),y(Ve),y(Oe),y(We),y(Re),y(Ue),y(Ge),y(Je),y(Ke)}}}const Yn={local:"feature-extractor",sections:[{local:"transformers.FeatureExtractionMixin",title:"FeatureExtractionMixin"},{local:"transformers.SequenceFeatureExtractor",title:"SequenceFeatureExtractor"},{local:"transformers.BatchFeature",title:"BatchFeature"},{local:"transformers.ImageFeatureExtractionMixin",title:"ImageFeatureExtractionMixin"}],title:"Feature Extractor"};function Kn(Q){return jn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class as extends Bn{constructor(m){super();Cn(this,m,Kn,Jn,An,{})}}export{as as default,Yn as metadata};
