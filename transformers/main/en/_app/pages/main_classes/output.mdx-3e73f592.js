import{S as C$,i as N$,s as z$,e as n,k as i,w as h,t as c,M as E$,c as s,d as t,m as d,a,x as f,h as p,b as r,F as o,g as l,y as _,q as m,o as g,B as v,v as P$}from"../../chunks/vendor-6b77c823.js";import{T as B$}from"../../chunks/Tip-39098574.js";import{D as y}from"../../chunks/Docstring-abef54e3.js";import{C as A$}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as T}from"../../chunks/IconCopyLink-7a11ce68.js";function L$(Fu){let $,Bt,q,A,K,x,nn,X;return{c(){$=n("p"),Bt=c("You can\u2019t unpack a "),q=n("code"),A=c("ModelOutput"),K=c(" directly. Use the "),x=n("a"),nn=c("to_tuple()"),X=c(` method to convert it to a
tuple before.`),this.h()},l(Z){$=s(Z,"P",{});var M=a($);Bt=p(M,"You can\u2019t unpack a "),q=s(M,"CODE",{});var C=a(q);A=p(C,"ModelOutput"),C.forEach(t),K=p(M," directly. Use the "),x=s(M,"A",{href:!0});var $a=a(x);nn=p($a,"to_tuple()"),$a.forEach(t),X=p(M,` method to convert it to a
tuple before.`),M.forEach(t),this.h()},h(){r(x,"href","/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput.to_tuple")},m(Z,M){l(Z,$,M),o($,Bt),o($,q),o(q,A),o($,K),o($,x),o(x,nn),o($,X)},d(Z){Z&&t($)}}}function j$(Fu){let $,Bt,q,A,K,x,nn,X,Z,M,C,$a,qa,Lh,jh,Mu,xa,Wh,ku,sn,Au,w,Dh,Ia,Hh,Qh,Oa,Ih,Vh,Va,Rh,Uh,Ra,Yh,Jh,Ua,Gh,Kh,Ya,Xh,Zh,Ja,ef,tf,Ga,of,nf,Ka,sf,af,Xa,rf,df,Za,uf,lf,er,cf,pf,Cu,O,hf,tr,ff,_f,or,mf,gf,nr,vf,yf,sr,Tf,wf,Nu,S,bf,ar,$f,qf,rr,xf,Of,ir,Sf,Ff,dr,Mf,kf,zu,an,Eu,Lt,Af,ur,Cf,Nf,Pu,F,zf,lr,Ef,Pf,cr,Bf,Lf,pr,jf,Wf,hr,Df,Hf,Bu,Sa,Qf,Lu,ee,jt,fr,rn,If,_r,Vf,ju,k,dn,Rf,te,Uf,mr,Yf,Jf,gr,Gf,Kf,Xf,Wt,Zf,Dt,un,e_,ln,t_,vr,o_,n_,Wu,oe,Ht,yr,cn,s_,Tr,a_,Du,ne,pn,r_,wr,i_,Hu,se,Qt,br,hn,d_,$r,u_,Qu,ae,fn,l_,qr,c_,Iu,re,It,xr,_n,p_,Or,h_,Vu,ie,mn,f_,Sr,__,Ru,de,Vt,Fr,gn,m_,Mr,g_,Uu,ue,vn,v_,kr,y_,Yu,le,Rt,Ar,yn,T_,Cr,w_,Ju,ce,Tn,b_,Nr,$_,Gu,pe,Ut,zr,wn,q_,Er,x_,Ku,he,bn,O_,Pr,S_,Xu,fe,Yt,Br,$n,F_,Lr,M_,Zu,_e,qn,k_,jr,A_,el,me,Jt,Wr,xn,C_,Dr,N_,tl,ge,On,z_,Hr,E_,ol,ve,Gt,Qr,Sn,P_,Ir,B_,nl,ye,Fn,L_,Vr,j_,sl,Te,Kt,Rr,Mn,W_,Ur,D_,al,we,kn,H_,Yr,Q_,rl,be,Xt,Jr,An,I_,Gr,V_,il,$e,Cn,R_,Kr,U_,dl,qe,Zt,Xr,Nn,Y_,Zr,J_,ul,xe,zn,G_,ei,K_,ll,Oe,eo,ti,En,X_,oi,Z_,cl,Se,Pn,em,ni,tm,pl,Fe,to,si,Bn,om,ai,nm,hl,Me,Ln,sm,ri,am,fl,ke,oo,ii,jn,rm,di,im,_l,Ae,Wn,dm,ui,um,ml,Ce,no,li,Dn,lm,ci,cm,gl,Ne,Hn,pm,pi,hm,vl,ze,so,hi,Qn,fm,fi,_m,yl,Ee,In,mm,_i,gm,Tl,Pe,ao,mi,Vn,vm,gi,ym,wl,Be,Rn,Tm,vi,wm,bl,Le,ro,yi,Un,bm,Ti,$m,$l,je,Yn,qm,wi,xm,ql,We,io,bi,Jn,Om,$i,Sm,xl,De,Gn,Fm,qi,Mm,Ol,He,uo,xi,Kn,km,Oi,Am,Sl,Qe,Xn,Cm,Si,Nm,Fl,Ie,lo,Fi,Zn,zm,Mi,Em,Ml,Ve,es,Pm,ki,Bm,kl,Re,co,Ai,ts,Lm,Ci,jm,Al,Ue,os,Wm,Ni,Dm,Cl,Ye,po,zi,ns,Hm,Ei,Qm,Nl,Je,ss,Im,Pi,Vm,zl,Ge,ho,Bi,as,Rm,Li,Um,El,Ke,rs,Ym,ji,Jm,Pl,Xe,fo,Wi,is,Gm,Di,Km,Bl,Ze,ds,Xm,Hi,Zm,Ll,et,_o,Qi,us,eg,Ii,tg,jl,tt,ls,og,Vi,ng,Wl,ot,mo,Ri,cs,sg,Ui,ag,Dl,nt,ps,rg,Yi,ig,Hl,st,go,Ji,hs,dg,Gi,ug,Ql,at,fs,lg,Ki,cg,Il,rt,vo,Xi,_s,pg,Zi,hg,Vl,it,ms,fg,ed,_g,Rl,dt,yo,td,gs,mg,od,gg,Ul,ut,vs,vg,nd,yg,Yl,lt,To,sd,ys,Tg,ad,wg,Jl,ct,Ts,bg,rd,$g,Gl,pt,wo,id,ws,qg,dd,xg,Kl,ht,bs,Og,ud,Sg,Xl,ft,bo,ld,$s,Fg,cd,Mg,Zl,_t,qs,kg,pd,Ag,ec,mt,$o,hd,xs,Cg,fd,Ng,tc,gt,Os,zg,_d,Eg,oc,vt,qo,md,Ss,Pg,gd,Bg,nc,yt,Fs,Lg,vd,jg,sc,Tt,xo,yd,Ms,Wg,Td,Dg,ac,wt,ks,Hg,wd,Qg,rc,bt,Oo,bd,As,Ig,$d,Vg,ic,N,Cs,Rg,qd,Ug,Yg,So,Ns,Jg,xd,Gg,dc,$t,Fo,Od,zs,Kg,Sd,Xg,uc,z,Es,Zg,Fd,ev,tv,Mo,Ps,ov,Md,nv,lc,qt,ko,kd,Bs,sv,Ad,av,cc,E,Ls,rv,Cd,iv,dv,Ao,js,uv,Nd,lv,pc,xt,Co,zd,Ws,cv,Ed,pv,hc,P,Ds,hv,Pd,fv,_v,No,Hs,mv,Bd,gv,fc,Ot,zo,Ld,Qs,vv,jd,yv,_c,B,Is,Tv,Wd,wv,bv,Eo,Vs,$v,Dd,qv,mc,St,Po,Hd,Rs,xv,Qd,Ov,gc,L,Us,Sv,Id,Fv,Mv,Bo,Ys,kv,Vd,Av,vc,Ft,Lo,Rd,Js,Cv,Ud,Nv,yc,j,Gs,zv,Yd,Ev,Pv,jo,Ks,Bv,Jd,Lv,Tc,Mt,Wo,Gd,Xs,jv,Kd,Wv,wc,W,Zs,Dv,Xd,Hv,Qv,Do,ea,Iv,Zd,Vv,bc,kt,Ho,eu,ta,Rv,tu,Uv,$c,D,oa,Yv,ou,Jv,Gv,Qo,na,Kv,nu,Xv,qc,At,Io,su,sa,Zv,au,ey,xc,H,aa,ty,ru,oy,ny,Vo,ra,sy,iu,ay,Oc,Ct,Ro,du,ia,ry,uu,iy,Sc,Q,da,dy,lu,uy,ly,Uo,ua,cy,cu,py,Fc,Nt,Yo,pu,la,hy,hu,fy,Mc,I,ca,_y,fu,my,gy,Jo,pa,vy,_u,yy,kc,zt,Go,mu,ha,Ty,gu,wy,Ac,V,fa,by,vu,$y,qy,Ko,_a,xy,yu,Oy,Cc,Et,Xo,Tu,ma,Sy,wu,Fy,Nc,R,ga,My,bu,ky,Ay,Zo,va,Cy,$u,Ny,zc,Pt,en,qu,ya,zy,xu,Ey,Ec,U,Ta,Py,Ou,By,Ly,tn,wa,jy,Su,Wy,Pc;return x=new T({}),sn=new A$({props:{code:`from transformers import BertTokenizer, BertForSequenceClassification
import torch

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertForSequenceClassification.from_pretrained("bert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, BertForSequenceClassification
<span class="hljs-keyword">import</span> torch

tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)
model = BertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
labels = torch.tensor([<span class="hljs-number">1</span>]).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>
outputs = model(**inputs, labels=labels)`}}),an=new A$({props:{code:"outputs[:2]",highlighted:'outputs[:<span class="hljs-number">2</span>]'}}),rn=new T({}),dn=new y({props:{name:"class transformers.utils.ModelOutput",anchor:"transformers.utils.ModelOutput",parameters:"",source:"https://github.com/huggingface/transformers/blob/main/src/transformers/utils/generic.py#L145"}}),Wt=new B$({props:{warning:!0,$$slots:{default:[L$]},$$scope:{ctx:Fu}}}),un=new y({props:{name:"to_tuple",anchor:"transformers.utils.ModelOutput.to_tuple",parameters:[],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/utils/generic.py#L234"}}),cn=new T({}),pn=new y({props:{name:"class transformers.modeling_outputs.BaseModelOutput",anchor:"transformers.modeling_outputs.BaseModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#L24",parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),hn=new T({}),fn=new y({props:{name:"class transformers.modeling_outputs.BaseModelOutputWithPooling",anchor:"transformers.modeling_outputs.BaseModelOutputWithPooling",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"pooler_output",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#L50",parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutputWithPooling.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPooling.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.`,name:"pooler_output"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPooling.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPooling.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),_n=new T({}),mn=new y({props:{name:"class transformers.modeling_outputs.BaseModelOutputWithCrossAttentions",anchor:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#L121",parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"}]}}),gn=new T({}),vn=new y({props:{name:"class transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions",anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"pooler_output",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#L154",parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.`,name:"pooler_output"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.`,name:"past_key_values"}]}}),yn=new T({}),Tn=new y({props:{name:"class transformers.modeling_outputs.BaseModelOutputWithPast",anchor:"transformers.modeling_outputs.BaseModelOutputWithPast",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#L82",parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutputWithPast.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPast.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPast.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPast.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),wn=new T({}),bn=new y({props:{name:"class transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions",anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#L203",parametersDescription:[{anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"}]}}),$n=new T({}),qn=new y({props:{name:"class transformers.modeling_outputs.Seq2SeqModelOutput",anchor:"transformers.modeling_outputs.Seq2SeqModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#L249",parametersDescription:[{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}]}}),xn=new T({}),On=new y({props:{name:"class transformers.modeling_outputs.CausalLMOutput",anchor:"transformers.modeling_outputs.CausalLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#L310",parametersDescription:[{anchor:"transformers.modeling_outputs.CausalLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_outputs.CausalLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.CausalLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.CausalLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Sn=new T({}),Fn=new y({props:{name:"class transformers.modeling_outputs.CausalLMOutputWithCrossAttentions",anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#L375",parametersDescription:[{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Cross attentions weights after the attention softmax, used to compute the weighted average in the
cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> tuples of length <code>config.n_layers</code>, with each tuple containing the cached key,
value states of the self-attention and the cross-attention layers if model is used in encoder-decoder
setting. Only relevant if <code>config.is_decoder = True</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"}]}}),Mn=new T({}),kn=new y({props:{name:"class transformers.modeling_outputs.CausalLMOutputWithPast",anchor:"transformers.modeling_outputs.CausalLMOutputWithPast",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#L339",parametersDescription:[{anchor:"transformers.modeling_outputs.CausalLMOutputWithPast.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithPast.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithPast.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>)</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithPast.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.CausalLMOutputWithPast.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),An=new T({}),Cn=new y({props:{name:"class transformers.modeling_outputs.MaskedLMOutput",anchor:"transformers.modeling_outputs.MaskedLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#L455",parametersDescription:[{anchor:"transformers.modeling_outputs.MaskedLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Masked language modeling (MLM) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.MaskedLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.MaskedLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.MaskedLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Nn=new T({}),zn=new y({props:{name:"class transformers.modeling_outputs.Seq2SeqLMOutput",anchor:"transformers.modeling_outputs.Seq2SeqLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#L484",parametersDescription:[{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqLMOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}]}}),En=new T({}),Pn=new y({props:{name:"class transformers.modeling_outputs.NextSentencePredictorOutput",anchor:"transformers.modeling_outputs.NextSentencePredictorOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#L544",parametersDescription:[{anchor:"transformers.modeling_outputs.NextSentencePredictorOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>next_sentence_label</code> is provided) &#x2014;
Next sequence prediction (classification) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.NextSentencePredictorOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, 2)</code>) &#x2014;
Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation
before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.NextSentencePredictorOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.NextSentencePredictorOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Bn=new T({}),Ln=new y({props:{name:"class transformers.modeling_outputs.SequenceClassifierOutput",anchor:"transformers.modeling_outputs.SequenceClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#L574",parametersDescription:[{anchor:"transformers.modeling_outputs.SequenceClassifierOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.SequenceClassifierOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.SequenceClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.SequenceClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),jn=new T({}),Wn=new y({props:{name:"class transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput",anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#L603",parametersDescription:[{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>label</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}]}}),Dn=new T({}),Hn=new y({props:{name:"class transformers.modeling_outputs.MultipleChoiceModelOutput",anchor:"transformers.modeling_outputs.MultipleChoiceModelOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#L663",parametersDescription:[{anchor:"transformers.modeling_outputs.MultipleChoiceModelOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.MultipleChoiceModelOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
<em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.MultipleChoiceModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.MultipleChoiceModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Qn=new T({}),In=new y({props:{name:"class transformers.modeling_outputs.TokenClassifierOutput",anchor:"transformers.modeling_outputs.TokenClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#L694",parametersDescription:[{anchor:"transformers.modeling_outputs.TokenClassifierOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  &#x2014;
Classification loss.`,name:"loss"},{anchor:"transformers.modeling_outputs.TokenClassifierOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) &#x2014;
Classification scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_outputs.TokenClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.TokenClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Vn=new T({}),Rn=new y({props:{name:"class transformers.modeling_outputs.QuestionAnsweringModelOutput",anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"start_logits",val:": FloatTensor = None"},{name:"end_logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#L723",parametersDescription:[{anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.`,name:"loss"},{anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput.start_logits",description:`<strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-start scores (before SoftMax).`,name:"start_logits"},{anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput.end_logits",description:`<strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-end scores (before SoftMax).`,name:"end_logits"},{anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_outputs.QuestionAnsweringModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Un=new T({}),Yn=new y({props:{name:"class transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput",anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"start_logits",val:": FloatTensor = None"},{name:"end_logits",val:": FloatTensor = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_outputs.py#L755",parametersDescription:[{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.`,name:"loss"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.start_logits",description:`<strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-start scores (before SoftMax).`,name:"start_logits"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.end_logits",description:`<strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-end scores (before SoftMax).`,name:"end_logits"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}]}}),Jn=new T({}),Gn=new y({props:{name:"class transformers.modeling_tf_outputs.TFBaseModelOutput",anchor:"transformers.modeling_tf_outputs.TFBaseModelOutput",parameters:[{name:"last_hidden_state",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_tf_outputs.py#L24",parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Kn=new T({}),Xn=new y({props:{name:"class transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling",anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling",parameters:[{name:"last_hidden_state",val:": Tensor = None"},{name:"pooler_output",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_tf_outputs.py#L50",parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling.pooler_output",description:`<strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Last layer hidden-state of the first token of the sequence (classification token) further processed by a
Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
prediction (classification) objective during pretraining.</p>
<p>This output is usually <em>not</em> a good summary of the semantic content of the input, you&#x2019;re often better with
averaging or pooling the sequence of hidden-states for the whole input sequence.`,name:"pooler_output"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Zn=new T({}),es=new y({props:{name:"class transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions",anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions",parameters:[{name:"last_hidden_state",val:": Tensor = None"},{name:"pooler_output",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_tf_outputs.py#L84",parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions.pooler_output",description:`<strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Last layer hidden-state of the first token of the sequence (classification token) further processed by a
Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
prediction (classification) objective during pretraining.</p>
<p>This output is usually <em>not</em> a good summary of the semantic content of the input, you&#x2019;re often better with
averaging or pooling the sequence of hidden-states for the whole input sequence.`,name:"pooler_output"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"}]}}),ts=new T({}),os=new y({props:{name:"class transformers.modeling_tf_outputs.TFBaseModelOutputWithPast",anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPast",parameters:[{name:"last_hidden_state",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_tf_outputs.py#L132",parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPast.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPast.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPast.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPast.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),ns=new T({}),ss=new y({props:{name:"class transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions",anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions",parameters:[{name:"last_hidden_state",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_tf_outputs.py#L201",parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"}]}}),as=new T({}),rs=new y({props:{name:"class transformers.modeling_tf_outputs.TFSeq2SeqModelOutput",anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput",parameters:[{name:"last_hidden_state",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_tf_outputs.py#L244",parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}]}}),is=new T({}),ds=new y({props:{name:"class transformers.modeling_tf_outputs.TFCausalLMOutput",anchor:"transformers.modeling_tf_outputs.TFCausalLMOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_tf_outputs.py#L304",parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),us=new T({}),ls=new y({props:{name:"class transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions",anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_tf_outputs.py#L369",parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"}]}}),cs=new T({}),ps=new y({props:{name:"class transformers.modeling_tf_outputs.TFCausalLMOutputWithPast",anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_tf_outputs.py#L333",parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss (for next-token prediction).`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),hs=new T({}),fs=new y({props:{name:"class transformers.modeling_tf_outputs.TFMaskedLMOutput",anchor:"transformers.modeling_tf_outputs.TFMaskedLMOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_tf_outputs.py#L412",parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFMaskedLMOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) &#x2014;
Masked language modeling (MLM) loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFMaskedLMOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFMaskedLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFMaskedLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),_s=new T({}),ms=new y({props:{name:"class transformers.modeling_tf_outputs.TFSeq2SeqLMOutput",anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_tf_outputs.py#L441",parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}]}}),gs=new T({}),vs=new y({props:{name:"class transformers.modeling_tf_outputs.TFNextSentencePredictorOutput",anchor:"transformers.modeling_tf_outputs.TFNextSentencePredictorOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_tf_outputs.py#L500",parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFNextSentencePredictorOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>next_sentence_label</code> is provided) &#x2014;
Next sentence prediction loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFNextSentencePredictorOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, 2)</code>) &#x2014;
Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation
before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFNextSentencePredictorOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFNextSentencePredictorOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),ys=new T({}),Ts=new y({props:{name:"class transformers.modeling_tf_outputs.TFSequenceClassifierOutput",anchor:"transformers.modeling_tf_outputs.TFSequenceClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_tf_outputs.py#L530",parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFSequenceClassifierOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFSequenceClassifierOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFSequenceClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSequenceClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),ws=new T({}),bs=new y({props:{name:"class transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput",anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_tf_outputs.py#L559",parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>label</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}]}}),$s=new T({}),qs=new y({props:{name:"class transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput",anchor:"transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_tf_outputs.py#L611",parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <em>(batch_size, )</em>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
<em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),xs=new T({}),Os=new y({props:{name:"class transformers.modeling_tf_outputs.TFTokenClassifierOutput",anchor:"transformers.modeling_tf_outputs.TFTokenClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_tf_outputs.py#L642",parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFTokenClassifierOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of unmasked labels, returned when <code>labels</code> is provided)  &#x2014;
Classification loss.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFTokenClassifierOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) &#x2014;
Classification scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_tf_outputs.TFTokenClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFTokenClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Ss=new T({}),Fs=new y({props:{name:"class transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput",anchor:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"start_logits",val:": Tensor = None"},{name:"end_logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_tf_outputs.py#L671",parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>start_positions</code> and <code>end_positions</code> are provided) &#x2014;
Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput.start_logits",description:`<strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-start scores (before SoftMax).`,name:"start_logits"},{anchor:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput.end_logits",description:`<strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-end scores (before SoftMax).`,name:"end_logits"},{anchor:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Ms=new T({}),ks=new y({props:{name:"class transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput",anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput",parameters:[{name:"loss",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"start_logits",val:": Tensor = None"},{name:"end_logits",val:": Tensor = None"},{name:"past_key_values",val:": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_tf_outputs.py#L703",parametersDescription:[{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.loss",description:`<strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.`,name:"loss"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.start_logits",description:`<strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-start scores (before SoftMax).`,name:"start_logits"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.end_logits",description:`<strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-end scores (before SoftMax).`,name:"end_logits"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}]}}),As=new T({}),Cs=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxBaseModelOutput",anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutput",parameters:[{name:"last_hidden_state",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_flax_outputs.py#L23",parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Ns=new y({props:{name:"replace",anchor:"None",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/flax/struct.py#L120"}}),zs=new T({}),Es=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast",anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast",parameters:[{name:"last_hidden_state",val:": ndarray = None"},{name:"past_key_values",val:": typing.Union[typing.Dict[str, jax._src.numpy.ndarray.ndarray], NoneType] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_flax_outputs.py#L49",parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast.past_key_values",description:`<strong>past_key_values</strong> (<code>Dict[str, jnp.ndarray]</code>) &#x2014;
Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
auto-regressive decoding. Pre-computed key and value hidden-states are of shape <em>[batch_size, max_length]</em>.`,name:"past_key_values"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Ps=new y({props:{name:"replace",anchor:"None",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/flax/struct.py#L120"}}),Bs=new T({}),Ls=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling",anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling",parameters:[{name:"last_hidden_state",val:": ndarray = None"},{name:"pooler_output",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_flax_outputs.py#L79",parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling.pooler_output",description:`<strong>pooler_output</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Last layer hidden-state of the first token of the sequence (classification token) further processed by a
Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
prediction (classification) objective during pretraining.`,name:"pooler_output"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),js=new y({props:{name:"replace",anchor:"None",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/flax/struct.py#L120"}}),Ws=new T({}),Ds=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions",anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions",parameters:[{name:"last_hidden_state",val:": ndarray = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.ndarray.ndarray]]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_flax_outputs.py#L110",parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(jnp.ndarray)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"}]}}),Hs=new y({props:{name:"replace",anchor:"None",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/flax/struct.py#L120"}}),Qs=new T({}),Is=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput",anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput",parameters:[{name:"last_hidden_state",val:": ndarray = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.ndarray.ndarray]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_flax_outputs.py#L156",parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(jnp.ndarray)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}]}}),Vs=new y({props:{name:"replace",anchor:"None",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/flax/struct.py#L120"}}),Rs=new T({}),Us=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions",anchor:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions",parameters:[{name:"logits",val:": ndarray = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.ndarray.ndarray]]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_flax_outputs.py#L217",parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Cross attentions weights after the attention softmax, used to compute the weighted average in the
cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> tuples of length <code>config.n_layers</code>, with each tuple containing the cached key, value
states of the self-attention and the cross-attention layers if model is used in encoder-decoder setting.
Only relevant if <code>config.is_decoder = True</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"}]}}),Ys=new y({props:{name:"replace",anchor:"None",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/flax/struct.py#L120"}}),Js=new T({}),Gs=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxMaskedLMOutput",anchor:"transformers.modeling_flax_outputs.FlaxMaskedLMOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_flax_outputs.py#L258",parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxMaskedLMOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxMaskedLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxMaskedLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Ks=new y({props:{name:"replace",anchor:"None",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/flax/struct.py#L120"}}),Xs=new T({}),Zs=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput",anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.ndarray.ndarray]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_flax_outputs.py#L287",parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(jnp.ndarray)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}]}}),ea=new y({props:{name:"replace",anchor:"None",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/flax/struct.py#L120"}}),ta=new T({}),oa=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput",anchor:"transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_flax_outputs.py#L344",parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, 2)</code>) &#x2014;
Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation
before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),na=new y({props:{name:"replace",anchor:"None",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/flax/struct.py#L120"}}),sa=new T({}),aa=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput",anchor:"transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_flax_outputs.py#L371",parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),ra=new y({props:{name:"replace",anchor:"None",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/flax/struct.py#L120"}}),ia=new T({}),da=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput",anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.ndarray.ndarray]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_flax_outputs.py#L397",parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(jnp.ndarray)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}]}}),ua=new y({props:{name:"replace",anchor:"None",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/flax/struct.py#L120"}}),la=new T({}),ca=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput",anchor:"transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_flax_outputs.py#L454",parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
<em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),pa=new y({props:{name:"replace",anchor:"None",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/flax/struct.py#L120"}}),ha=new T({}),fa=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxTokenClassifierOutput",anchor:"transformers.modeling_flax_outputs.FlaxTokenClassifierOutput",parameters:[{name:"logits",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_flax_outputs.py#L482",parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxTokenClassifierOutput.logits",description:`<strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) &#x2014;
Classification scores (before SoftMax).`,name:"logits"},{anchor:"transformers.modeling_flax_outputs.FlaxTokenClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxTokenClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),_a=new y({props:{name:"replace",anchor:"None",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/flax/struct.py#L120"}}),ma=new T({}),ga=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput",anchor:"transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput",parameters:[{name:"start_logits",val:": ndarray = None"},{name:"end_logits",val:": ndarray = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_flax_outputs.py#L508",parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput.start_logits",description:`<strong>start_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-start scores (before SoftMax).`,name:"start_logits"},{anchor:"transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput.end_logits",description:`<strong>end_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-end scores (before SoftMax).`,name:"end_logits"},{anchor:"transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),va=new y({props:{name:"replace",anchor:"None",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/flax/struct.py#L120"}}),ya=new T({}),Ta=new y({props:{name:"class transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput",anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput",parameters:[{name:"start_logits",val:": ndarray = None"},{name:"end_logits",val:": ndarray = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.ndarray.ndarray]]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[jax._src.numpy.ndarray.ndarray]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_flax_outputs.py#L537",parametersDescription:[{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.start_logits",description:`<strong>start_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-start scores (before SoftMax).`,name:"start_logits"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.end_logits",description:`<strong>end_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Span-end scores (before SoftMax).`,name:"end_logits"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(jnp.ndarray)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder&#x2019;s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.`,name:"cross_attentions"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}]}}),wa=new y({props:{name:"replace",anchor:"None",parameters:[{name:"**updates",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/flax/struct.py#L120"}}),{c(){$=n("meta"),Bt=i(),q=n("h1"),A=n("a"),K=n("span"),h(x.$$.fragment),nn=i(),X=n("span"),Z=c("Model outputs"),M=i(),C=n("p"),$a=c("All models have outputs that are instances of subclasses of "),qa=n("a"),Lh=c("ModelOutput"),jh=c(`. Those are
data structures containing all the information returned by the model, but that can also be used as tuples or
dictionaries.`),Mu=i(),xa=n("p"),Wh=c("Let\u2019s see of this looks on an example:"),ku=i(),h(sn.$$.fragment),Au=i(),w=n("p"),Dh=c("The "),Ia=n("code"),Hh=c("outputs"),Qh=c(" object is a "),Oa=n("a"),Ih=c("SequenceClassifierOutput"),Vh=c(`, as we can see in the
documentation of that class below, it means it has an optional `),Va=n("code"),Rh=c("loss"),Uh=c(", a "),Ra=n("code"),Yh=c("logits"),Jh=c(" an optional "),Ua=n("code"),Gh=c("hidden_states"),Kh=c(` and
an optional `),Ya=n("code"),Xh=c("attentions"),Zh=c(" attribute. Here we have the "),Ja=n("code"),ef=c("loss"),tf=c(" since we passed along "),Ga=n("code"),of=c("labels"),nf=c(`, but we don\u2019t have
`),Ka=n("code"),sf=c("hidden_states"),af=c(" and "),Xa=n("code"),rf=c("attentions"),df=c(" because we didn\u2019t pass "),Za=n("code"),uf=c("output_hidden_states=True"),lf=c(` or
`),er=n("code"),cf=c("output_attentions=True"),pf=c("."),Cu=i(),O=n("p"),hf=c(`You can access each attribute as you would usually do, and if that attribute has not been returned by the model, you
will get `),tr=n("code"),ff=c("None"),_f=c(". Here for instance "),or=n("code"),mf=c("outputs.loss"),gf=c(" is the loss computed by the model, and "),nr=n("code"),vf=c("outputs.attentions"),yf=c(` is
`),sr=n("code"),Tf=c("None"),wf=c("."),Nu=i(),S=n("p"),bf=c("When considering our "),ar=n("code"),$f=c("outputs"),qf=c(" object as tuple, it only considers the attributes that don\u2019t have "),rr=n("code"),xf=c("None"),Of=c(` values.
Here for instance, it has two elements, `),ir=n("code"),Sf=c("loss"),Ff=c(" then "),dr=n("code"),Mf=c("logits"),kf=c(", so"),zu=i(),h(an.$$.fragment),Eu=i(),Lt=n("p"),Af=c("will return the tuple "),ur=n("code"),Cf=c("(outputs.loss, outputs.logits)"),Nf=c(" for instance."),Pu=i(),F=n("p"),zf=c("When considering our "),lr=n("code"),Ef=c("outputs"),Pf=c(" object as dictionary, it only considers the attributes that don\u2019t have "),cr=n("code"),Bf=c("None"),Lf=c(`
values. Here for instance, it has two keys that are `),pr=n("code"),jf=c("loss"),Wf=c(" and "),hr=n("code"),Df=c("logits"),Hf=c("."),Bu=i(),Sa=n("p"),Qf=c(`We document here the generic model outputs that are used by more than one model type. Specific output types are
documented on their corresponding model page.`),Lu=i(),ee=n("h2"),jt=n("a"),fr=n("span"),h(rn.$$.fragment),If=i(),_r=n("span"),Vf=c("ModelOutput"),ju=i(),k=n("div"),h(dn.$$.fragment),Rf=i(),te=n("p"),Uf=c("Base class for all model outputs as dataclass. Has a "),mr=n("code"),Yf=c("__getitem__"),Jf=c(` that allows indexing by integer or slice (like a
tuple) or strings (like a dictionary) that will ignore the `),gr=n("code"),Gf=c("None"),Kf=c(` attributes. Otherwise behaves like a regular
python dictionary.`),Xf=i(),h(Wt.$$.fragment),Zf=i(),Dt=n("div"),h(un.$$.fragment),e_=i(),ln=n("p"),t_=c("Convert self to a tuple containing all the attributes/keys that are not "),vr=n("code"),o_=c("None"),n_=c("."),Wu=i(),oe=n("h2"),Ht=n("a"),yr=n("span"),h(cn.$$.fragment),s_=i(),Tr=n("span"),a_=c("BaseModelOutput"),Du=i(),ne=n("div"),h(pn.$$.fragment),r_=i(),wr=n("p"),i_=c("Base class for model\u2019s outputs, with potential hidden states and attentions."),Hu=i(),se=n("h2"),Qt=n("a"),br=n("span"),h(hn.$$.fragment),d_=i(),$r=n("span"),u_=c("BaseModelOutputWithPooling"),Qu=i(),ae=n("div"),h(fn.$$.fragment),l_=i(),qr=n("p"),c_=c("Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),Iu=i(),re=n("h2"),It=n("a"),xr=n("span"),h(_n.$$.fragment),p_=i(),Or=n("span"),h_=c("BaseModelOutputWithCrossAttentions"),Vu=i(),ie=n("div"),h(mn.$$.fragment),f_=i(),Sr=n("p"),__=c("Base class for model\u2019s outputs, with potential hidden states and attentions."),Ru=i(),de=n("h2"),Vt=n("a"),Fr=n("span"),h(gn.$$.fragment),m_=i(),Mr=n("span"),g_=c("BaseModelOutputWithPoolingAndCrossAttentions"),Uu=i(),ue=n("div"),h(vn.$$.fragment),v_=i(),kr=n("p"),y_=c("Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),Yu=i(),le=n("h2"),Rt=n("a"),Ar=n("span"),h(yn.$$.fragment),T_=i(),Cr=n("span"),w_=c("BaseModelOutputWithPast"),Ju=i(),ce=n("div"),h(Tn.$$.fragment),b_=i(),Nr=n("p"),$_=c("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Gu=i(),pe=n("h2"),Ut=n("a"),zr=n("span"),h(wn.$$.fragment),q_=i(),Er=n("span"),x_=c("BaseModelOutputWithPastAndCrossAttentions"),Ku=i(),he=n("div"),h(bn.$$.fragment),O_=i(),Pr=n("p"),S_=c("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Xu=i(),fe=n("h2"),Yt=n("a"),Br=n("span"),h($n.$$.fragment),F_=i(),Lr=n("span"),M_=c("Seq2SeqModelOutput"),Zu=i(),_e=n("div"),h(qn.$$.fragment),k_=i(),jr=n("p"),A_=c(`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),el=i(),me=n("h2"),Jt=n("a"),Wr=n("span"),h(xn.$$.fragment),C_=i(),Dr=n("span"),N_=c("CausalLMOutput"),tl=i(),ge=n("div"),h(On.$$.fragment),z_=i(),Hr=n("p"),E_=c("Base class for causal language model (or autoregressive) outputs."),ol=i(),ve=n("h2"),Gt=n("a"),Qr=n("span"),h(Sn.$$.fragment),P_=i(),Ir=n("span"),B_=c("CausalLMOutputWithCrossAttentions"),nl=i(),ye=n("div"),h(Fn.$$.fragment),L_=i(),Vr=n("p"),j_=c("Base class for causal language model (or autoregressive) outputs."),sl=i(),Te=n("h2"),Kt=n("a"),Rr=n("span"),h(Mn.$$.fragment),W_=i(),Ur=n("span"),D_=c("CausalLMOutputWithPast"),al=i(),we=n("div"),h(kn.$$.fragment),H_=i(),Yr=n("p"),Q_=c("Base class for causal language model (or autoregressive) outputs."),rl=i(),be=n("h2"),Xt=n("a"),Jr=n("span"),h(An.$$.fragment),I_=i(),Gr=n("span"),V_=c("MaskedLMOutput"),il=i(),$e=n("div"),h(Cn.$$.fragment),R_=i(),Kr=n("p"),U_=c("Base class for masked language models outputs."),dl=i(),qe=n("h2"),Zt=n("a"),Xr=n("span"),h(Nn.$$.fragment),Y_=i(),Zr=n("span"),J_=c("Seq2SeqLMOutput"),ul=i(),xe=n("div"),h(zn.$$.fragment),G_=i(),ei=n("p"),K_=c("Base class for sequence-to-sequence language models outputs."),ll=i(),Oe=n("h2"),eo=n("a"),ti=n("span"),h(En.$$.fragment),X_=i(),oi=n("span"),Z_=c("NextSentencePredictorOutput"),cl=i(),Se=n("div"),h(Pn.$$.fragment),em=i(),ni=n("p"),tm=c("Base class for outputs of models predicting if two sentences are consecutive or not."),pl=i(),Fe=n("h2"),to=n("a"),si=n("span"),h(Bn.$$.fragment),om=i(),ai=n("span"),nm=c("SequenceClassifierOutput"),hl=i(),Me=n("div"),h(Ln.$$.fragment),sm=i(),ri=n("p"),am=c("Base class for outputs of sentence classification models."),fl=i(),ke=n("h2"),oo=n("a"),ii=n("span"),h(jn.$$.fragment),rm=i(),di=n("span"),im=c("Seq2SeqSequenceClassifierOutput"),_l=i(),Ae=n("div"),h(Wn.$$.fragment),dm=i(),ui=n("p"),um=c("Base class for outputs of sequence-to-sequence sentence classification models."),ml=i(),Ce=n("h2"),no=n("a"),li=n("span"),h(Dn.$$.fragment),lm=i(),ci=n("span"),cm=c("MultipleChoiceModelOutput"),gl=i(),Ne=n("div"),h(Hn.$$.fragment),pm=i(),pi=n("p"),hm=c("Base class for outputs of multiple choice models."),vl=i(),ze=n("h2"),so=n("a"),hi=n("span"),h(Qn.$$.fragment),fm=i(),fi=n("span"),_m=c("TokenClassifierOutput"),yl=i(),Ee=n("div"),h(In.$$.fragment),mm=i(),_i=n("p"),gm=c("Base class for outputs of token classification models."),Tl=i(),Pe=n("h2"),ao=n("a"),mi=n("span"),h(Vn.$$.fragment),vm=i(),gi=n("span"),ym=c("QuestionAnsweringModelOutput"),wl=i(),Be=n("div"),h(Rn.$$.fragment),Tm=i(),vi=n("p"),wm=c("Base class for outputs of question answering models."),bl=i(),Le=n("h2"),ro=n("a"),yi=n("span"),h(Un.$$.fragment),bm=i(),Ti=n("span"),$m=c("Seq2SeqQuestionAnsweringModelOutput"),$l=i(),je=n("div"),h(Yn.$$.fragment),qm=i(),wi=n("p"),xm=c("Base class for outputs of sequence-to-sequence question answering models."),ql=i(),We=n("h2"),io=n("a"),bi=n("span"),h(Jn.$$.fragment),Om=i(),$i=n("span"),Sm=c("TFBaseModelOutput"),xl=i(),De=n("div"),h(Gn.$$.fragment),Fm=i(),qi=n("p"),Mm=c("Base class for model\u2019s outputs, with potential hidden states and attentions."),Ol=i(),He=n("h2"),uo=n("a"),xi=n("span"),h(Kn.$$.fragment),km=i(),Oi=n("span"),Am=c("TFBaseModelOutputWithPooling"),Sl=i(),Qe=n("div"),h(Xn.$$.fragment),Cm=i(),Si=n("p"),Nm=c("Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),Fl=i(),Ie=n("h2"),lo=n("a"),Fi=n("span"),h(Zn.$$.fragment),zm=i(),Mi=n("span"),Em=c("TFBaseModelOutputWithPoolingAndCrossAttentions"),Ml=i(),Ve=n("div"),h(es.$$.fragment),Pm=i(),ki=n("p"),Bm=c("Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),kl=i(),Re=n("h2"),co=n("a"),Ai=n("span"),h(ts.$$.fragment),Lm=i(),Ci=n("span"),jm=c("TFBaseModelOutputWithPast"),Al=i(),Ue=n("div"),h(os.$$.fragment),Wm=i(),Ni=n("p"),Dm=c("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Cl=i(),Ye=n("h2"),po=n("a"),zi=n("span"),h(ns.$$.fragment),Hm=i(),Ei=n("span"),Qm=c("TFBaseModelOutputWithPastAndCrossAttentions"),Nl=i(),Je=n("div"),h(ss.$$.fragment),Im=i(),Pi=n("p"),Vm=c("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),zl=i(),Ge=n("h2"),ho=n("a"),Bi=n("span"),h(as.$$.fragment),Rm=i(),Li=n("span"),Um=c("TFSeq2SeqModelOutput"),El=i(),Ke=n("div"),h(rs.$$.fragment),Ym=i(),ji=n("p"),Jm=c(`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),Pl=i(),Xe=n("h2"),fo=n("a"),Wi=n("span"),h(is.$$.fragment),Gm=i(),Di=n("span"),Km=c("TFCausalLMOutput"),Bl=i(),Ze=n("div"),h(ds.$$.fragment),Xm=i(),Hi=n("p"),Zm=c("Base class for causal language model (or autoregressive) outputs."),Ll=i(),et=n("h2"),_o=n("a"),Qi=n("span"),h(us.$$.fragment),eg=i(),Ii=n("span"),tg=c("TFCausalLMOutputWithCrossAttentions"),jl=i(),tt=n("div"),h(ls.$$.fragment),og=i(),Vi=n("p"),ng=c("Base class for causal language model (or autoregressive) outputs."),Wl=i(),ot=n("h2"),mo=n("a"),Ri=n("span"),h(cs.$$.fragment),sg=i(),Ui=n("span"),ag=c("TFCausalLMOutputWithPast"),Dl=i(),nt=n("div"),h(ps.$$.fragment),rg=i(),Yi=n("p"),ig=c("Base class for causal language model (or autoregressive) outputs."),Hl=i(),st=n("h2"),go=n("a"),Ji=n("span"),h(hs.$$.fragment),dg=i(),Gi=n("span"),ug=c("TFMaskedLMOutput"),Ql=i(),at=n("div"),h(fs.$$.fragment),lg=i(),Ki=n("p"),cg=c("Base class for masked language models outputs."),Il=i(),rt=n("h2"),vo=n("a"),Xi=n("span"),h(_s.$$.fragment),pg=i(),Zi=n("span"),hg=c("TFSeq2SeqLMOutput"),Vl=i(),it=n("div"),h(ms.$$.fragment),fg=i(),ed=n("p"),_g=c("Base class for sequence-to-sequence language models outputs."),Rl=i(),dt=n("h2"),yo=n("a"),td=n("span"),h(gs.$$.fragment),mg=i(),od=n("span"),gg=c("TFNextSentencePredictorOutput"),Ul=i(),ut=n("div"),h(vs.$$.fragment),vg=i(),nd=n("p"),yg=c("Base class for outputs of models predicting if two sentences are consecutive or not."),Yl=i(),lt=n("h2"),To=n("a"),sd=n("span"),h(ys.$$.fragment),Tg=i(),ad=n("span"),wg=c("TFSequenceClassifierOutput"),Jl=i(),ct=n("div"),h(Ts.$$.fragment),bg=i(),rd=n("p"),$g=c("Base class for outputs of sentence classification models."),Gl=i(),pt=n("h2"),wo=n("a"),id=n("span"),h(ws.$$.fragment),qg=i(),dd=n("span"),xg=c("TFSeq2SeqSequenceClassifierOutput"),Kl=i(),ht=n("div"),h(bs.$$.fragment),Og=i(),ud=n("p"),Sg=c("Base class for outputs of sequence-to-sequence sentence classification models."),Xl=i(),ft=n("h2"),bo=n("a"),ld=n("span"),h($s.$$.fragment),Fg=i(),cd=n("span"),Mg=c("TFMultipleChoiceModelOutput"),Zl=i(),_t=n("div"),h(qs.$$.fragment),kg=i(),pd=n("p"),Ag=c("Base class for outputs of multiple choice models."),ec=i(),mt=n("h2"),$o=n("a"),hd=n("span"),h(xs.$$.fragment),Cg=i(),fd=n("span"),Ng=c("TFTokenClassifierOutput"),tc=i(),gt=n("div"),h(Os.$$.fragment),zg=i(),_d=n("p"),Eg=c("Base class for outputs of token classification models."),oc=i(),vt=n("h2"),qo=n("a"),md=n("span"),h(Ss.$$.fragment),Pg=i(),gd=n("span"),Bg=c("TFQuestionAnsweringModelOutput"),nc=i(),yt=n("div"),h(Fs.$$.fragment),Lg=i(),vd=n("p"),jg=c("Base class for outputs of question answering models."),sc=i(),Tt=n("h2"),xo=n("a"),yd=n("span"),h(Ms.$$.fragment),Wg=i(),Td=n("span"),Dg=c("TFSeq2SeqQuestionAnsweringModelOutput"),ac=i(),wt=n("div"),h(ks.$$.fragment),Hg=i(),wd=n("p"),Qg=c("Base class for outputs of sequence-to-sequence question answering models."),rc=i(),bt=n("h2"),Oo=n("a"),bd=n("span"),h(As.$$.fragment),Ig=i(),$d=n("span"),Vg=c("FlaxBaseModelOutput"),ic=i(),N=n("div"),h(Cs.$$.fragment),Rg=i(),qd=n("p"),Ug=c("Base class for model\u2019s outputs, with potential hidden states and attentions."),Yg=i(),So=n("div"),h(Ns.$$.fragment),Jg=i(),xd=n("p"),Gg=c("\u201CReturns a new object replacing the specified fields with new values."),dc=i(),$t=n("h2"),Fo=n("a"),Od=n("span"),h(zs.$$.fragment),Kg=i(),Sd=n("span"),Xg=c("FlaxBaseModelOutputWithPast"),uc=i(),z=n("div"),h(Es.$$.fragment),Zg=i(),Fd=n("p"),ev=c("Base class for model\u2019s outputs, with potential hidden states and attentions."),tv=i(),Mo=n("div"),h(Ps.$$.fragment),ov=i(),Md=n("p"),nv=c("\u201CReturns a new object replacing the specified fields with new values."),lc=i(),qt=n("h2"),ko=n("a"),kd=n("span"),h(Bs.$$.fragment),sv=i(),Ad=n("span"),av=c("FlaxBaseModelOutputWithPooling"),cc=i(),E=n("div"),h(Ls.$$.fragment),rv=i(),Cd=n("p"),iv=c("Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),dv=i(),Ao=n("div"),h(js.$$.fragment),uv=i(),Nd=n("p"),lv=c("\u201CReturns a new object replacing the specified fields with new values."),pc=i(),xt=n("h2"),Co=n("a"),zd=n("span"),h(Ws.$$.fragment),cv=i(),Ed=n("span"),pv=c("FlaxBaseModelOutputWithPastAndCrossAttentions"),hc=i(),P=n("div"),h(Ds.$$.fragment),hv=i(),Pd=n("p"),fv=c("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),_v=i(),No=n("div"),h(Hs.$$.fragment),mv=i(),Bd=n("p"),gv=c("\u201CReturns a new object replacing the specified fields with new values."),fc=i(),Ot=n("h2"),zo=n("a"),Ld=n("span"),h(Qs.$$.fragment),vv=i(),jd=n("span"),yv=c("FlaxSeq2SeqModelOutput"),_c=i(),B=n("div"),h(Is.$$.fragment),Tv=i(),Wd=n("p"),wv=c(`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),bv=i(),Eo=n("div"),h(Vs.$$.fragment),$v=i(),Dd=n("p"),qv=c("\u201CReturns a new object replacing the specified fields with new values."),mc=i(),St=n("h2"),Po=n("a"),Hd=n("span"),h(Rs.$$.fragment),xv=i(),Qd=n("span"),Ov=c("FlaxCausalLMOutputWithCrossAttentions"),gc=i(),L=n("div"),h(Us.$$.fragment),Sv=i(),Id=n("p"),Fv=c("Base class for causal language model (or autoregressive) outputs."),Mv=i(),Bo=n("div"),h(Ys.$$.fragment),kv=i(),Vd=n("p"),Av=c("\u201CReturns a new object replacing the specified fields with new values."),vc=i(),Ft=n("h2"),Lo=n("a"),Rd=n("span"),h(Js.$$.fragment),Cv=i(),Ud=n("span"),Nv=c("FlaxMaskedLMOutput"),yc=i(),j=n("div"),h(Gs.$$.fragment),zv=i(),Yd=n("p"),Ev=c("Base class for masked language models outputs."),Pv=i(),jo=n("div"),h(Ks.$$.fragment),Bv=i(),Jd=n("p"),Lv=c("\u201CReturns a new object replacing the specified fields with new values."),Tc=i(),Mt=n("h2"),Wo=n("a"),Gd=n("span"),h(Xs.$$.fragment),jv=i(),Kd=n("span"),Wv=c("FlaxSeq2SeqLMOutput"),wc=i(),W=n("div"),h(Zs.$$.fragment),Dv=i(),Xd=n("p"),Hv=c("Base class for sequence-to-sequence language models outputs."),Qv=i(),Do=n("div"),h(ea.$$.fragment),Iv=i(),Zd=n("p"),Vv=c("\u201CReturns a new object replacing the specified fields with new values."),bc=i(),kt=n("h2"),Ho=n("a"),eu=n("span"),h(ta.$$.fragment),Rv=i(),tu=n("span"),Uv=c("FlaxNextSentencePredictorOutput"),$c=i(),D=n("div"),h(oa.$$.fragment),Yv=i(),ou=n("p"),Jv=c("Base class for outputs of models predicting if two sentences are consecutive or not."),Gv=i(),Qo=n("div"),h(na.$$.fragment),Kv=i(),nu=n("p"),Xv=c("\u201CReturns a new object replacing the specified fields with new values."),qc=i(),At=n("h2"),Io=n("a"),su=n("span"),h(sa.$$.fragment),Zv=i(),au=n("span"),ey=c("FlaxSequenceClassifierOutput"),xc=i(),H=n("div"),h(aa.$$.fragment),ty=i(),ru=n("p"),oy=c("Base class for outputs of sentence classification models."),ny=i(),Vo=n("div"),h(ra.$$.fragment),sy=i(),iu=n("p"),ay=c("\u201CReturns a new object replacing the specified fields with new values."),Oc=i(),Ct=n("h2"),Ro=n("a"),du=n("span"),h(ia.$$.fragment),ry=i(),uu=n("span"),iy=c("FlaxSeq2SeqSequenceClassifierOutput"),Sc=i(),Q=n("div"),h(da.$$.fragment),dy=i(),lu=n("p"),uy=c("Base class for outputs of sequence-to-sequence sentence classification models."),ly=i(),Uo=n("div"),h(ua.$$.fragment),cy=i(),cu=n("p"),py=c("\u201CReturns a new object replacing the specified fields with new values."),Fc=i(),Nt=n("h2"),Yo=n("a"),pu=n("span"),h(la.$$.fragment),hy=i(),hu=n("span"),fy=c("FlaxMultipleChoiceModelOutput"),Mc=i(),I=n("div"),h(ca.$$.fragment),_y=i(),fu=n("p"),my=c("Base class for outputs of multiple choice models."),gy=i(),Jo=n("div"),h(pa.$$.fragment),vy=i(),_u=n("p"),yy=c("\u201CReturns a new object replacing the specified fields with new values."),kc=i(),zt=n("h2"),Go=n("a"),mu=n("span"),h(ha.$$.fragment),Ty=i(),gu=n("span"),wy=c("FlaxTokenClassifierOutput"),Ac=i(),V=n("div"),h(fa.$$.fragment),by=i(),vu=n("p"),$y=c("Base class for outputs of token classification models."),qy=i(),Ko=n("div"),h(_a.$$.fragment),xy=i(),yu=n("p"),Oy=c("\u201CReturns a new object replacing the specified fields with new values."),Cc=i(),Et=n("h2"),Xo=n("a"),Tu=n("span"),h(ma.$$.fragment),Sy=i(),wu=n("span"),Fy=c("FlaxQuestionAnsweringModelOutput"),Nc=i(),R=n("div"),h(ga.$$.fragment),My=i(),bu=n("p"),ky=c("Base class for outputs of question answering models."),Ay=i(),Zo=n("div"),h(va.$$.fragment),Cy=i(),$u=n("p"),Ny=c("\u201CReturns a new object replacing the specified fields with new values."),zc=i(),Pt=n("h2"),en=n("a"),qu=n("span"),h(ya.$$.fragment),zy=i(),xu=n("span"),Ey=c("FlaxSeq2SeqQuestionAnsweringModelOutput"),Ec=i(),U=n("div"),h(Ta.$$.fragment),Py=i(),Ou=n("p"),By=c("Base class for outputs of sequence-to-sequence question answering models."),Ly=i(),tn=n("div"),h(wa.$$.fragment),jy=i(),Su=n("p"),Wy=c("\u201CReturns a new object replacing the specified fields with new values."),this.h()},l(e){const u=E$('[data-svelte="svelte-1phssyn"]',document.head);$=s(u,"META",{name:!0,content:!0}),u.forEach(t),Bt=d(e),q=s(e,"H1",{class:!0});var ba=a(q);A=s(ba,"A",{id:!0,class:!0,href:!0});var Dy=a(A);K=s(Dy,"SPAN",{});var Hy=a(K);f(x.$$.fragment,Hy),Hy.forEach(t),Dy.forEach(t),nn=d(ba),X=s(ba,"SPAN",{});var Qy=a(X);Z=p(Qy,"Model outputs"),Qy.forEach(t),ba.forEach(t),M=d(e),C=s(e,"P",{});var Bc=a(C);$a=p(Bc,"All models have outputs that are instances of subclasses of "),qa=s(Bc,"A",{href:!0});var Iy=a(qa);Lh=p(Iy,"ModelOutput"),Iy.forEach(t),jh=p(Bc,`. Those are
data structures containing all the information returned by the model, but that can also be used as tuples or
dictionaries.`),Bc.forEach(t),Mu=d(e),xa=s(e,"P",{});var Vy=a(xa);Wh=p(Vy,"Let\u2019s see of this looks on an example:"),Vy.forEach(t),ku=d(e),f(sn.$$.fragment,e),Au=d(e),w=s(e,"P",{});var b=a(w);Dh=p(b,"The "),Ia=s(b,"CODE",{});var Ry=a(Ia);Hh=p(Ry,"outputs"),Ry.forEach(t),Qh=p(b," object is a "),Oa=s(b,"A",{href:!0});var Uy=a(Oa);Ih=p(Uy,"SequenceClassifierOutput"),Uy.forEach(t),Vh=p(b,`, as we can see in the
documentation of that class below, it means it has an optional `),Va=s(b,"CODE",{});var Yy=a(Va);Rh=p(Yy,"loss"),Yy.forEach(t),Uh=p(b,", a "),Ra=s(b,"CODE",{});var Jy=a(Ra);Yh=p(Jy,"logits"),Jy.forEach(t),Jh=p(b," an optional "),Ua=s(b,"CODE",{});var Gy=a(Ua);Gh=p(Gy,"hidden_states"),Gy.forEach(t),Kh=p(b,` and
an optional `),Ya=s(b,"CODE",{});var Ky=a(Ya);Xh=p(Ky,"attentions"),Ky.forEach(t),Zh=p(b," attribute. Here we have the "),Ja=s(b,"CODE",{});var Xy=a(Ja);ef=p(Xy,"loss"),Xy.forEach(t),tf=p(b," since we passed along "),Ga=s(b,"CODE",{});var Zy=a(Ga);of=p(Zy,"labels"),Zy.forEach(t),nf=p(b,`, but we don\u2019t have
`),Ka=s(b,"CODE",{});var eT=a(Ka);sf=p(eT,"hidden_states"),eT.forEach(t),af=p(b," and "),Xa=s(b,"CODE",{});var tT=a(Xa);rf=p(tT,"attentions"),tT.forEach(t),df=p(b," because we didn\u2019t pass "),Za=s(b,"CODE",{});var oT=a(Za);uf=p(oT,"output_hidden_states=True"),oT.forEach(t),lf=p(b,` or
`),er=s(b,"CODE",{});var nT=a(er);cf=p(nT,"output_attentions=True"),nT.forEach(t),pf=p(b,"."),b.forEach(t),Cu=d(e),O=s(e,"P",{});var Y=a(O);hf=p(Y,`You can access each attribute as you would usually do, and if that attribute has not been returned by the model, you
will get `),tr=s(Y,"CODE",{});var sT=a(tr);ff=p(sT,"None"),sT.forEach(t),_f=p(Y,". Here for instance "),or=s(Y,"CODE",{});var aT=a(or);mf=p(aT,"outputs.loss"),aT.forEach(t),gf=p(Y," is the loss computed by the model, and "),nr=s(Y,"CODE",{});var rT=a(nr);vf=p(rT,"outputs.attentions"),rT.forEach(t),yf=p(Y,` is
`),sr=s(Y,"CODE",{});var iT=a(sr);Tf=p(iT,"None"),iT.forEach(t),wf=p(Y,"."),Y.forEach(t),Nu=d(e),S=s(e,"P",{});var J=a(S);bf=p(J,"When considering our "),ar=s(J,"CODE",{});var dT=a(ar);$f=p(dT,"outputs"),dT.forEach(t),qf=p(J," object as tuple, it only considers the attributes that don\u2019t have "),rr=s(J,"CODE",{});var uT=a(rr);xf=p(uT,"None"),uT.forEach(t),Of=p(J,` values.
Here for instance, it has two elements, `),ir=s(J,"CODE",{});var lT=a(ir);Sf=p(lT,"loss"),lT.forEach(t),Ff=p(J," then "),dr=s(J,"CODE",{});var cT=a(dr);Mf=p(cT,"logits"),cT.forEach(t),kf=p(J,", so"),J.forEach(t),zu=d(e),f(an.$$.fragment,e),Eu=d(e),Lt=s(e,"P",{});var Lc=a(Lt);Af=p(Lc,"will return the tuple "),ur=s(Lc,"CODE",{});var pT=a(ur);Cf=p(pT,"(outputs.loss, outputs.logits)"),pT.forEach(t),Nf=p(Lc," for instance."),Lc.forEach(t),Pu=d(e),F=s(e,"P",{});var G=a(F);zf=p(G,"When considering our "),lr=s(G,"CODE",{});var hT=a(lr);Ef=p(hT,"outputs"),hT.forEach(t),Pf=p(G," object as dictionary, it only considers the attributes that don\u2019t have "),cr=s(G,"CODE",{});var fT=a(cr);Bf=p(fT,"None"),fT.forEach(t),Lf=p(G,`
values. Here for instance, it has two keys that are `),pr=s(G,"CODE",{});var _T=a(pr);jf=p(_T,"loss"),_T.forEach(t),Wf=p(G," and "),hr=s(G,"CODE",{});var mT=a(hr);Df=p(mT,"logits"),mT.forEach(t),Hf=p(G,"."),G.forEach(t),Bu=d(e),Sa=s(e,"P",{});var gT=a(Sa);Qf=p(gT,`We document here the generic model outputs that are used by more than one model type. Specific output types are
documented on their corresponding model page.`),gT.forEach(t),Lu=d(e),ee=s(e,"H2",{class:!0});var jc=a(ee);jt=s(jc,"A",{id:!0,class:!0,href:!0});var vT=a(jt);fr=s(vT,"SPAN",{});var yT=a(fr);f(rn.$$.fragment,yT),yT.forEach(t),vT.forEach(t),If=d(jc),_r=s(jc,"SPAN",{});var TT=a(_r);Vf=p(TT,"ModelOutput"),TT.forEach(t),jc.forEach(t),ju=d(e),k=s(e,"DIV",{class:!0});var on=a(k);f(dn.$$.fragment,on),Rf=d(on),te=s(on,"P",{});var Fa=a(te);Uf=p(Fa,"Base class for all model outputs as dataclass. Has a "),mr=s(Fa,"CODE",{});var wT=a(mr);Yf=p(wT,"__getitem__"),wT.forEach(t),Jf=p(Fa,` that allows indexing by integer or slice (like a
tuple) or strings (like a dictionary) that will ignore the `),gr=s(Fa,"CODE",{});var bT=a(gr);Gf=p(bT,"None"),bT.forEach(t),Kf=p(Fa,` attributes. Otherwise behaves like a regular
python dictionary.`),Fa.forEach(t),Xf=d(on),f(Wt.$$.fragment,on),Zf=d(on),Dt=s(on,"DIV",{class:!0});var Wc=a(Dt);f(un.$$.fragment,Wc),e_=d(Wc),ln=s(Wc,"P",{});var Dc=a(ln);t_=p(Dc,"Convert self to a tuple containing all the attributes/keys that are not "),vr=s(Dc,"CODE",{});var $T=a(vr);o_=p($T,"None"),$T.forEach(t),n_=p(Dc,"."),Dc.forEach(t),Wc.forEach(t),on.forEach(t),Wu=d(e),oe=s(e,"H2",{class:!0});var Hc=a(oe);Ht=s(Hc,"A",{id:!0,class:!0,href:!0});var qT=a(Ht);yr=s(qT,"SPAN",{});var xT=a(yr);f(cn.$$.fragment,xT),xT.forEach(t),qT.forEach(t),s_=d(Hc),Tr=s(Hc,"SPAN",{});var OT=a(Tr);a_=p(OT,"BaseModelOutput"),OT.forEach(t),Hc.forEach(t),Du=d(e),ne=s(e,"DIV",{class:!0});var Qc=a(ne);f(pn.$$.fragment,Qc),r_=d(Qc),wr=s(Qc,"P",{});var ST=a(wr);i_=p(ST,"Base class for model\u2019s outputs, with potential hidden states and attentions."),ST.forEach(t),Qc.forEach(t),Hu=d(e),se=s(e,"H2",{class:!0});var Ic=a(se);Qt=s(Ic,"A",{id:!0,class:!0,href:!0});var FT=a(Qt);br=s(FT,"SPAN",{});var MT=a(br);f(hn.$$.fragment,MT),MT.forEach(t),FT.forEach(t),d_=d(Ic),$r=s(Ic,"SPAN",{});var kT=a($r);u_=p(kT,"BaseModelOutputWithPooling"),kT.forEach(t),Ic.forEach(t),Qu=d(e),ae=s(e,"DIV",{class:!0});var Vc=a(ae);f(fn.$$.fragment,Vc),l_=d(Vc),qr=s(Vc,"P",{});var AT=a(qr);c_=p(AT,"Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),AT.forEach(t),Vc.forEach(t),Iu=d(e),re=s(e,"H2",{class:!0});var Rc=a(re);It=s(Rc,"A",{id:!0,class:!0,href:!0});var CT=a(It);xr=s(CT,"SPAN",{});var NT=a(xr);f(_n.$$.fragment,NT),NT.forEach(t),CT.forEach(t),p_=d(Rc),Or=s(Rc,"SPAN",{});var zT=a(Or);h_=p(zT,"BaseModelOutputWithCrossAttentions"),zT.forEach(t),Rc.forEach(t),Vu=d(e),ie=s(e,"DIV",{class:!0});var Uc=a(ie);f(mn.$$.fragment,Uc),f_=d(Uc),Sr=s(Uc,"P",{});var ET=a(Sr);__=p(ET,"Base class for model\u2019s outputs, with potential hidden states and attentions."),ET.forEach(t),Uc.forEach(t),Ru=d(e),de=s(e,"H2",{class:!0});var Yc=a(de);Vt=s(Yc,"A",{id:!0,class:!0,href:!0});var PT=a(Vt);Fr=s(PT,"SPAN",{});var BT=a(Fr);f(gn.$$.fragment,BT),BT.forEach(t),PT.forEach(t),m_=d(Yc),Mr=s(Yc,"SPAN",{});var LT=a(Mr);g_=p(LT,"BaseModelOutputWithPoolingAndCrossAttentions"),LT.forEach(t),Yc.forEach(t),Uu=d(e),ue=s(e,"DIV",{class:!0});var Jc=a(ue);f(vn.$$.fragment,Jc),v_=d(Jc),kr=s(Jc,"P",{});var jT=a(kr);y_=p(jT,"Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),jT.forEach(t),Jc.forEach(t),Yu=d(e),le=s(e,"H2",{class:!0});var Gc=a(le);Rt=s(Gc,"A",{id:!0,class:!0,href:!0});var WT=a(Rt);Ar=s(WT,"SPAN",{});var DT=a(Ar);f(yn.$$.fragment,DT),DT.forEach(t),WT.forEach(t),T_=d(Gc),Cr=s(Gc,"SPAN",{});var HT=a(Cr);w_=p(HT,"BaseModelOutputWithPast"),HT.forEach(t),Gc.forEach(t),Ju=d(e),ce=s(e,"DIV",{class:!0});var Kc=a(ce);f(Tn.$$.fragment,Kc),b_=d(Kc),Nr=s(Kc,"P",{});var QT=a(Nr);$_=p(QT,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),QT.forEach(t),Kc.forEach(t),Gu=d(e),pe=s(e,"H2",{class:!0});var Xc=a(pe);Ut=s(Xc,"A",{id:!0,class:!0,href:!0});var IT=a(Ut);zr=s(IT,"SPAN",{});var VT=a(zr);f(wn.$$.fragment,VT),VT.forEach(t),IT.forEach(t),q_=d(Xc),Er=s(Xc,"SPAN",{});var RT=a(Er);x_=p(RT,"BaseModelOutputWithPastAndCrossAttentions"),RT.forEach(t),Xc.forEach(t),Ku=d(e),he=s(e,"DIV",{class:!0});var Zc=a(he);f(bn.$$.fragment,Zc),O_=d(Zc),Pr=s(Zc,"P",{});var UT=a(Pr);S_=p(UT,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),UT.forEach(t),Zc.forEach(t),Xu=d(e),fe=s(e,"H2",{class:!0});var ep=a(fe);Yt=s(ep,"A",{id:!0,class:!0,href:!0});var YT=a(Yt);Br=s(YT,"SPAN",{});var JT=a(Br);f($n.$$.fragment,JT),JT.forEach(t),YT.forEach(t),F_=d(ep),Lr=s(ep,"SPAN",{});var GT=a(Lr);M_=p(GT,"Seq2SeqModelOutput"),GT.forEach(t),ep.forEach(t),Zu=d(e),_e=s(e,"DIV",{class:!0});var tp=a(_e);f(qn.$$.fragment,tp),k_=d(tp),jr=s(tp,"P",{});var KT=a(jr);A_=p(KT,`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),KT.forEach(t),tp.forEach(t),el=d(e),me=s(e,"H2",{class:!0});var op=a(me);Jt=s(op,"A",{id:!0,class:!0,href:!0});var XT=a(Jt);Wr=s(XT,"SPAN",{});var ZT=a(Wr);f(xn.$$.fragment,ZT),ZT.forEach(t),XT.forEach(t),C_=d(op),Dr=s(op,"SPAN",{});var ew=a(Dr);N_=p(ew,"CausalLMOutput"),ew.forEach(t),op.forEach(t),tl=d(e),ge=s(e,"DIV",{class:!0});var np=a(ge);f(On.$$.fragment,np),z_=d(np),Hr=s(np,"P",{});var tw=a(Hr);E_=p(tw,"Base class for causal language model (or autoregressive) outputs."),tw.forEach(t),np.forEach(t),ol=d(e),ve=s(e,"H2",{class:!0});var sp=a(ve);Gt=s(sp,"A",{id:!0,class:!0,href:!0});var ow=a(Gt);Qr=s(ow,"SPAN",{});var nw=a(Qr);f(Sn.$$.fragment,nw),nw.forEach(t),ow.forEach(t),P_=d(sp),Ir=s(sp,"SPAN",{});var sw=a(Ir);B_=p(sw,"CausalLMOutputWithCrossAttentions"),sw.forEach(t),sp.forEach(t),nl=d(e),ye=s(e,"DIV",{class:!0});var ap=a(ye);f(Fn.$$.fragment,ap),L_=d(ap),Vr=s(ap,"P",{});var aw=a(Vr);j_=p(aw,"Base class for causal language model (or autoregressive) outputs."),aw.forEach(t),ap.forEach(t),sl=d(e),Te=s(e,"H2",{class:!0});var rp=a(Te);Kt=s(rp,"A",{id:!0,class:!0,href:!0});var rw=a(Kt);Rr=s(rw,"SPAN",{});var iw=a(Rr);f(Mn.$$.fragment,iw),iw.forEach(t),rw.forEach(t),W_=d(rp),Ur=s(rp,"SPAN",{});var dw=a(Ur);D_=p(dw,"CausalLMOutputWithPast"),dw.forEach(t),rp.forEach(t),al=d(e),we=s(e,"DIV",{class:!0});var ip=a(we);f(kn.$$.fragment,ip),H_=d(ip),Yr=s(ip,"P",{});var uw=a(Yr);Q_=p(uw,"Base class for causal language model (or autoregressive) outputs."),uw.forEach(t),ip.forEach(t),rl=d(e),be=s(e,"H2",{class:!0});var dp=a(be);Xt=s(dp,"A",{id:!0,class:!0,href:!0});var lw=a(Xt);Jr=s(lw,"SPAN",{});var cw=a(Jr);f(An.$$.fragment,cw),cw.forEach(t),lw.forEach(t),I_=d(dp),Gr=s(dp,"SPAN",{});var pw=a(Gr);V_=p(pw,"MaskedLMOutput"),pw.forEach(t),dp.forEach(t),il=d(e),$e=s(e,"DIV",{class:!0});var up=a($e);f(Cn.$$.fragment,up),R_=d(up),Kr=s(up,"P",{});var hw=a(Kr);U_=p(hw,"Base class for masked language models outputs."),hw.forEach(t),up.forEach(t),dl=d(e),qe=s(e,"H2",{class:!0});var lp=a(qe);Zt=s(lp,"A",{id:!0,class:!0,href:!0});var fw=a(Zt);Xr=s(fw,"SPAN",{});var _w=a(Xr);f(Nn.$$.fragment,_w),_w.forEach(t),fw.forEach(t),Y_=d(lp),Zr=s(lp,"SPAN",{});var mw=a(Zr);J_=p(mw,"Seq2SeqLMOutput"),mw.forEach(t),lp.forEach(t),ul=d(e),xe=s(e,"DIV",{class:!0});var cp=a(xe);f(zn.$$.fragment,cp),G_=d(cp),ei=s(cp,"P",{});var gw=a(ei);K_=p(gw,"Base class for sequence-to-sequence language models outputs."),gw.forEach(t),cp.forEach(t),ll=d(e),Oe=s(e,"H2",{class:!0});var pp=a(Oe);eo=s(pp,"A",{id:!0,class:!0,href:!0});var vw=a(eo);ti=s(vw,"SPAN",{});var yw=a(ti);f(En.$$.fragment,yw),yw.forEach(t),vw.forEach(t),X_=d(pp),oi=s(pp,"SPAN",{});var Tw=a(oi);Z_=p(Tw,"NextSentencePredictorOutput"),Tw.forEach(t),pp.forEach(t),cl=d(e),Se=s(e,"DIV",{class:!0});var hp=a(Se);f(Pn.$$.fragment,hp),em=d(hp),ni=s(hp,"P",{});var ww=a(ni);tm=p(ww,"Base class for outputs of models predicting if two sentences are consecutive or not."),ww.forEach(t),hp.forEach(t),pl=d(e),Fe=s(e,"H2",{class:!0});var fp=a(Fe);to=s(fp,"A",{id:!0,class:!0,href:!0});var bw=a(to);si=s(bw,"SPAN",{});var $w=a(si);f(Bn.$$.fragment,$w),$w.forEach(t),bw.forEach(t),om=d(fp),ai=s(fp,"SPAN",{});var qw=a(ai);nm=p(qw,"SequenceClassifierOutput"),qw.forEach(t),fp.forEach(t),hl=d(e),Me=s(e,"DIV",{class:!0});var _p=a(Me);f(Ln.$$.fragment,_p),sm=d(_p),ri=s(_p,"P",{});var xw=a(ri);am=p(xw,"Base class for outputs of sentence classification models."),xw.forEach(t),_p.forEach(t),fl=d(e),ke=s(e,"H2",{class:!0});var mp=a(ke);oo=s(mp,"A",{id:!0,class:!0,href:!0});var Ow=a(oo);ii=s(Ow,"SPAN",{});var Sw=a(ii);f(jn.$$.fragment,Sw),Sw.forEach(t),Ow.forEach(t),rm=d(mp),di=s(mp,"SPAN",{});var Fw=a(di);im=p(Fw,"Seq2SeqSequenceClassifierOutput"),Fw.forEach(t),mp.forEach(t),_l=d(e),Ae=s(e,"DIV",{class:!0});var gp=a(Ae);f(Wn.$$.fragment,gp),dm=d(gp),ui=s(gp,"P",{});var Mw=a(ui);um=p(Mw,"Base class for outputs of sequence-to-sequence sentence classification models."),Mw.forEach(t),gp.forEach(t),ml=d(e),Ce=s(e,"H2",{class:!0});var vp=a(Ce);no=s(vp,"A",{id:!0,class:!0,href:!0});var kw=a(no);li=s(kw,"SPAN",{});var Aw=a(li);f(Dn.$$.fragment,Aw),Aw.forEach(t),kw.forEach(t),lm=d(vp),ci=s(vp,"SPAN",{});var Cw=a(ci);cm=p(Cw,"MultipleChoiceModelOutput"),Cw.forEach(t),vp.forEach(t),gl=d(e),Ne=s(e,"DIV",{class:!0});var yp=a(Ne);f(Hn.$$.fragment,yp),pm=d(yp),pi=s(yp,"P",{});var Nw=a(pi);hm=p(Nw,"Base class for outputs of multiple choice models."),Nw.forEach(t),yp.forEach(t),vl=d(e),ze=s(e,"H2",{class:!0});var Tp=a(ze);so=s(Tp,"A",{id:!0,class:!0,href:!0});var zw=a(so);hi=s(zw,"SPAN",{});var Ew=a(hi);f(Qn.$$.fragment,Ew),Ew.forEach(t),zw.forEach(t),fm=d(Tp),fi=s(Tp,"SPAN",{});var Pw=a(fi);_m=p(Pw,"TokenClassifierOutput"),Pw.forEach(t),Tp.forEach(t),yl=d(e),Ee=s(e,"DIV",{class:!0});var wp=a(Ee);f(In.$$.fragment,wp),mm=d(wp),_i=s(wp,"P",{});var Bw=a(_i);gm=p(Bw,"Base class for outputs of token classification models."),Bw.forEach(t),wp.forEach(t),Tl=d(e),Pe=s(e,"H2",{class:!0});var bp=a(Pe);ao=s(bp,"A",{id:!0,class:!0,href:!0});var Lw=a(ao);mi=s(Lw,"SPAN",{});var jw=a(mi);f(Vn.$$.fragment,jw),jw.forEach(t),Lw.forEach(t),vm=d(bp),gi=s(bp,"SPAN",{});var Ww=a(gi);ym=p(Ww,"QuestionAnsweringModelOutput"),Ww.forEach(t),bp.forEach(t),wl=d(e),Be=s(e,"DIV",{class:!0});var $p=a(Be);f(Rn.$$.fragment,$p),Tm=d($p),vi=s($p,"P",{});var Dw=a(vi);wm=p(Dw,"Base class for outputs of question answering models."),Dw.forEach(t),$p.forEach(t),bl=d(e),Le=s(e,"H2",{class:!0});var qp=a(Le);ro=s(qp,"A",{id:!0,class:!0,href:!0});var Hw=a(ro);yi=s(Hw,"SPAN",{});var Qw=a(yi);f(Un.$$.fragment,Qw),Qw.forEach(t),Hw.forEach(t),bm=d(qp),Ti=s(qp,"SPAN",{});var Iw=a(Ti);$m=p(Iw,"Seq2SeqQuestionAnsweringModelOutput"),Iw.forEach(t),qp.forEach(t),$l=d(e),je=s(e,"DIV",{class:!0});var xp=a(je);f(Yn.$$.fragment,xp),qm=d(xp),wi=s(xp,"P",{});var Vw=a(wi);xm=p(Vw,"Base class for outputs of sequence-to-sequence question answering models."),Vw.forEach(t),xp.forEach(t),ql=d(e),We=s(e,"H2",{class:!0});var Op=a(We);io=s(Op,"A",{id:!0,class:!0,href:!0});var Rw=a(io);bi=s(Rw,"SPAN",{});var Uw=a(bi);f(Jn.$$.fragment,Uw),Uw.forEach(t),Rw.forEach(t),Om=d(Op),$i=s(Op,"SPAN",{});var Yw=a($i);Sm=p(Yw,"TFBaseModelOutput"),Yw.forEach(t),Op.forEach(t),xl=d(e),De=s(e,"DIV",{class:!0});var Sp=a(De);f(Gn.$$.fragment,Sp),Fm=d(Sp),qi=s(Sp,"P",{});var Jw=a(qi);Mm=p(Jw,"Base class for model\u2019s outputs, with potential hidden states and attentions."),Jw.forEach(t),Sp.forEach(t),Ol=d(e),He=s(e,"H2",{class:!0});var Fp=a(He);uo=s(Fp,"A",{id:!0,class:!0,href:!0});var Gw=a(uo);xi=s(Gw,"SPAN",{});var Kw=a(xi);f(Kn.$$.fragment,Kw),Kw.forEach(t),Gw.forEach(t),km=d(Fp),Oi=s(Fp,"SPAN",{});var Xw=a(Oi);Am=p(Xw,"TFBaseModelOutputWithPooling"),Xw.forEach(t),Fp.forEach(t),Sl=d(e),Qe=s(e,"DIV",{class:!0});var Mp=a(Qe);f(Xn.$$.fragment,Mp),Cm=d(Mp),Si=s(Mp,"P",{});var Zw=a(Si);Nm=p(Zw,"Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),Zw.forEach(t),Mp.forEach(t),Fl=d(e),Ie=s(e,"H2",{class:!0});var kp=a(Ie);lo=s(kp,"A",{id:!0,class:!0,href:!0});var e2=a(lo);Fi=s(e2,"SPAN",{});var t2=a(Fi);f(Zn.$$.fragment,t2),t2.forEach(t),e2.forEach(t),zm=d(kp),Mi=s(kp,"SPAN",{});var o2=a(Mi);Em=p(o2,"TFBaseModelOutputWithPoolingAndCrossAttentions"),o2.forEach(t),kp.forEach(t),Ml=d(e),Ve=s(e,"DIV",{class:!0});var Ap=a(Ve);f(es.$$.fragment,Ap),Pm=d(Ap),ki=s(Ap,"P",{});var n2=a(ki);Bm=p(n2,"Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),n2.forEach(t),Ap.forEach(t),kl=d(e),Re=s(e,"H2",{class:!0});var Cp=a(Re);co=s(Cp,"A",{id:!0,class:!0,href:!0});var s2=a(co);Ai=s(s2,"SPAN",{});var a2=a(Ai);f(ts.$$.fragment,a2),a2.forEach(t),s2.forEach(t),Lm=d(Cp),Ci=s(Cp,"SPAN",{});var r2=a(Ci);jm=p(r2,"TFBaseModelOutputWithPast"),r2.forEach(t),Cp.forEach(t),Al=d(e),Ue=s(e,"DIV",{class:!0});var Np=a(Ue);f(os.$$.fragment,Np),Wm=d(Np),Ni=s(Np,"P",{});var i2=a(Ni);Dm=p(i2,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),i2.forEach(t),Np.forEach(t),Cl=d(e),Ye=s(e,"H2",{class:!0});var zp=a(Ye);po=s(zp,"A",{id:!0,class:!0,href:!0});var d2=a(po);zi=s(d2,"SPAN",{});var u2=a(zi);f(ns.$$.fragment,u2),u2.forEach(t),d2.forEach(t),Hm=d(zp),Ei=s(zp,"SPAN",{});var l2=a(Ei);Qm=p(l2,"TFBaseModelOutputWithPastAndCrossAttentions"),l2.forEach(t),zp.forEach(t),Nl=d(e),Je=s(e,"DIV",{class:!0});var Ep=a(Je);f(ss.$$.fragment,Ep),Im=d(Ep),Pi=s(Ep,"P",{});var c2=a(Pi);Vm=p(c2,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),c2.forEach(t),Ep.forEach(t),zl=d(e),Ge=s(e,"H2",{class:!0});var Pp=a(Ge);ho=s(Pp,"A",{id:!0,class:!0,href:!0});var p2=a(ho);Bi=s(p2,"SPAN",{});var h2=a(Bi);f(as.$$.fragment,h2),h2.forEach(t),p2.forEach(t),Rm=d(Pp),Li=s(Pp,"SPAN",{});var f2=a(Li);Um=p(f2,"TFSeq2SeqModelOutput"),f2.forEach(t),Pp.forEach(t),El=d(e),Ke=s(e,"DIV",{class:!0});var Bp=a(Ke);f(rs.$$.fragment,Bp),Ym=d(Bp),ji=s(Bp,"P",{});var _2=a(ji);Jm=p(_2,`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),_2.forEach(t),Bp.forEach(t),Pl=d(e),Xe=s(e,"H2",{class:!0});var Lp=a(Xe);fo=s(Lp,"A",{id:!0,class:!0,href:!0});var m2=a(fo);Wi=s(m2,"SPAN",{});var g2=a(Wi);f(is.$$.fragment,g2),g2.forEach(t),m2.forEach(t),Gm=d(Lp),Di=s(Lp,"SPAN",{});var v2=a(Di);Km=p(v2,"TFCausalLMOutput"),v2.forEach(t),Lp.forEach(t),Bl=d(e),Ze=s(e,"DIV",{class:!0});var jp=a(Ze);f(ds.$$.fragment,jp),Xm=d(jp),Hi=s(jp,"P",{});var y2=a(Hi);Zm=p(y2,"Base class for causal language model (or autoregressive) outputs."),y2.forEach(t),jp.forEach(t),Ll=d(e),et=s(e,"H2",{class:!0});var Wp=a(et);_o=s(Wp,"A",{id:!0,class:!0,href:!0});var T2=a(_o);Qi=s(T2,"SPAN",{});var w2=a(Qi);f(us.$$.fragment,w2),w2.forEach(t),T2.forEach(t),eg=d(Wp),Ii=s(Wp,"SPAN",{});var b2=a(Ii);tg=p(b2,"TFCausalLMOutputWithCrossAttentions"),b2.forEach(t),Wp.forEach(t),jl=d(e),tt=s(e,"DIV",{class:!0});var Dp=a(tt);f(ls.$$.fragment,Dp),og=d(Dp),Vi=s(Dp,"P",{});var $2=a(Vi);ng=p($2,"Base class for causal language model (or autoregressive) outputs."),$2.forEach(t),Dp.forEach(t),Wl=d(e),ot=s(e,"H2",{class:!0});var Hp=a(ot);mo=s(Hp,"A",{id:!0,class:!0,href:!0});var q2=a(mo);Ri=s(q2,"SPAN",{});var x2=a(Ri);f(cs.$$.fragment,x2),x2.forEach(t),q2.forEach(t),sg=d(Hp),Ui=s(Hp,"SPAN",{});var O2=a(Ui);ag=p(O2,"TFCausalLMOutputWithPast"),O2.forEach(t),Hp.forEach(t),Dl=d(e),nt=s(e,"DIV",{class:!0});var Qp=a(nt);f(ps.$$.fragment,Qp),rg=d(Qp),Yi=s(Qp,"P",{});var S2=a(Yi);ig=p(S2,"Base class for causal language model (or autoregressive) outputs."),S2.forEach(t),Qp.forEach(t),Hl=d(e),st=s(e,"H2",{class:!0});var Ip=a(st);go=s(Ip,"A",{id:!0,class:!0,href:!0});var F2=a(go);Ji=s(F2,"SPAN",{});var M2=a(Ji);f(hs.$$.fragment,M2),M2.forEach(t),F2.forEach(t),dg=d(Ip),Gi=s(Ip,"SPAN",{});var k2=a(Gi);ug=p(k2,"TFMaskedLMOutput"),k2.forEach(t),Ip.forEach(t),Ql=d(e),at=s(e,"DIV",{class:!0});var Vp=a(at);f(fs.$$.fragment,Vp),lg=d(Vp),Ki=s(Vp,"P",{});var A2=a(Ki);cg=p(A2,"Base class for masked language models outputs."),A2.forEach(t),Vp.forEach(t),Il=d(e),rt=s(e,"H2",{class:!0});var Rp=a(rt);vo=s(Rp,"A",{id:!0,class:!0,href:!0});var C2=a(vo);Xi=s(C2,"SPAN",{});var N2=a(Xi);f(_s.$$.fragment,N2),N2.forEach(t),C2.forEach(t),pg=d(Rp),Zi=s(Rp,"SPAN",{});var z2=a(Zi);hg=p(z2,"TFSeq2SeqLMOutput"),z2.forEach(t),Rp.forEach(t),Vl=d(e),it=s(e,"DIV",{class:!0});var Up=a(it);f(ms.$$.fragment,Up),fg=d(Up),ed=s(Up,"P",{});var E2=a(ed);_g=p(E2,"Base class for sequence-to-sequence language models outputs."),E2.forEach(t),Up.forEach(t),Rl=d(e),dt=s(e,"H2",{class:!0});var Yp=a(dt);yo=s(Yp,"A",{id:!0,class:!0,href:!0});var P2=a(yo);td=s(P2,"SPAN",{});var B2=a(td);f(gs.$$.fragment,B2),B2.forEach(t),P2.forEach(t),mg=d(Yp),od=s(Yp,"SPAN",{});var L2=a(od);gg=p(L2,"TFNextSentencePredictorOutput"),L2.forEach(t),Yp.forEach(t),Ul=d(e),ut=s(e,"DIV",{class:!0});var Jp=a(ut);f(vs.$$.fragment,Jp),vg=d(Jp),nd=s(Jp,"P",{});var j2=a(nd);yg=p(j2,"Base class for outputs of models predicting if two sentences are consecutive or not."),j2.forEach(t),Jp.forEach(t),Yl=d(e),lt=s(e,"H2",{class:!0});var Gp=a(lt);To=s(Gp,"A",{id:!0,class:!0,href:!0});var W2=a(To);sd=s(W2,"SPAN",{});var D2=a(sd);f(ys.$$.fragment,D2),D2.forEach(t),W2.forEach(t),Tg=d(Gp),ad=s(Gp,"SPAN",{});var H2=a(ad);wg=p(H2,"TFSequenceClassifierOutput"),H2.forEach(t),Gp.forEach(t),Jl=d(e),ct=s(e,"DIV",{class:!0});var Kp=a(ct);f(Ts.$$.fragment,Kp),bg=d(Kp),rd=s(Kp,"P",{});var Q2=a(rd);$g=p(Q2,"Base class for outputs of sentence classification models."),Q2.forEach(t),Kp.forEach(t),Gl=d(e),pt=s(e,"H2",{class:!0});var Xp=a(pt);wo=s(Xp,"A",{id:!0,class:!0,href:!0});var I2=a(wo);id=s(I2,"SPAN",{});var V2=a(id);f(ws.$$.fragment,V2),V2.forEach(t),I2.forEach(t),qg=d(Xp),dd=s(Xp,"SPAN",{});var R2=a(dd);xg=p(R2,"TFSeq2SeqSequenceClassifierOutput"),R2.forEach(t),Xp.forEach(t),Kl=d(e),ht=s(e,"DIV",{class:!0});var Zp=a(ht);f(bs.$$.fragment,Zp),Og=d(Zp),ud=s(Zp,"P",{});var U2=a(ud);Sg=p(U2,"Base class for outputs of sequence-to-sequence sentence classification models."),U2.forEach(t),Zp.forEach(t),Xl=d(e),ft=s(e,"H2",{class:!0});var eh=a(ft);bo=s(eh,"A",{id:!0,class:!0,href:!0});var Y2=a(bo);ld=s(Y2,"SPAN",{});var J2=a(ld);f($s.$$.fragment,J2),J2.forEach(t),Y2.forEach(t),Fg=d(eh),cd=s(eh,"SPAN",{});var G2=a(cd);Mg=p(G2,"TFMultipleChoiceModelOutput"),G2.forEach(t),eh.forEach(t),Zl=d(e),_t=s(e,"DIV",{class:!0});var th=a(_t);f(qs.$$.fragment,th),kg=d(th),pd=s(th,"P",{});var K2=a(pd);Ag=p(K2,"Base class for outputs of multiple choice models."),K2.forEach(t),th.forEach(t),ec=d(e),mt=s(e,"H2",{class:!0});var oh=a(mt);$o=s(oh,"A",{id:!0,class:!0,href:!0});var X2=a($o);hd=s(X2,"SPAN",{});var Z2=a(hd);f(xs.$$.fragment,Z2),Z2.forEach(t),X2.forEach(t),Cg=d(oh),fd=s(oh,"SPAN",{});var eb=a(fd);Ng=p(eb,"TFTokenClassifierOutput"),eb.forEach(t),oh.forEach(t),tc=d(e),gt=s(e,"DIV",{class:!0});var nh=a(gt);f(Os.$$.fragment,nh),zg=d(nh),_d=s(nh,"P",{});var tb=a(_d);Eg=p(tb,"Base class for outputs of token classification models."),tb.forEach(t),nh.forEach(t),oc=d(e),vt=s(e,"H2",{class:!0});var sh=a(vt);qo=s(sh,"A",{id:!0,class:!0,href:!0});var ob=a(qo);md=s(ob,"SPAN",{});var nb=a(md);f(Ss.$$.fragment,nb),nb.forEach(t),ob.forEach(t),Pg=d(sh),gd=s(sh,"SPAN",{});var sb=a(gd);Bg=p(sb,"TFQuestionAnsweringModelOutput"),sb.forEach(t),sh.forEach(t),nc=d(e),yt=s(e,"DIV",{class:!0});var ah=a(yt);f(Fs.$$.fragment,ah),Lg=d(ah),vd=s(ah,"P",{});var ab=a(vd);jg=p(ab,"Base class for outputs of question answering models."),ab.forEach(t),ah.forEach(t),sc=d(e),Tt=s(e,"H2",{class:!0});var rh=a(Tt);xo=s(rh,"A",{id:!0,class:!0,href:!0});var rb=a(xo);yd=s(rb,"SPAN",{});var ib=a(yd);f(Ms.$$.fragment,ib),ib.forEach(t),rb.forEach(t),Wg=d(rh),Td=s(rh,"SPAN",{});var db=a(Td);Dg=p(db,"TFSeq2SeqQuestionAnsweringModelOutput"),db.forEach(t),rh.forEach(t),ac=d(e),wt=s(e,"DIV",{class:!0});var ih=a(wt);f(ks.$$.fragment,ih),Hg=d(ih),wd=s(ih,"P",{});var ub=a(wd);Qg=p(ub,"Base class for outputs of sequence-to-sequence question answering models."),ub.forEach(t),ih.forEach(t),rc=d(e),bt=s(e,"H2",{class:!0});var dh=a(bt);Oo=s(dh,"A",{id:!0,class:!0,href:!0});var lb=a(Oo);bd=s(lb,"SPAN",{});var cb=a(bd);f(As.$$.fragment,cb),cb.forEach(t),lb.forEach(t),Ig=d(dh),$d=s(dh,"SPAN",{});var pb=a($d);Vg=p(pb,"FlaxBaseModelOutput"),pb.forEach(t),dh.forEach(t),ic=d(e),N=s(e,"DIV",{class:!0});var Ma=a(N);f(Cs.$$.fragment,Ma),Rg=d(Ma),qd=s(Ma,"P",{});var hb=a(qd);Ug=p(hb,"Base class for model\u2019s outputs, with potential hidden states and attentions."),hb.forEach(t),Yg=d(Ma),So=s(Ma,"DIV",{class:!0});var uh=a(So);f(Ns.$$.fragment,uh),Jg=d(uh),xd=s(uh,"P",{});var fb=a(xd);Gg=p(fb,"\u201CReturns a new object replacing the specified fields with new values."),fb.forEach(t),uh.forEach(t),Ma.forEach(t),dc=d(e),$t=s(e,"H2",{class:!0});var lh=a($t);Fo=s(lh,"A",{id:!0,class:!0,href:!0});var _b=a(Fo);Od=s(_b,"SPAN",{});var mb=a(Od);f(zs.$$.fragment,mb),mb.forEach(t),_b.forEach(t),Kg=d(lh),Sd=s(lh,"SPAN",{});var gb=a(Sd);Xg=p(gb,"FlaxBaseModelOutputWithPast"),gb.forEach(t),lh.forEach(t),uc=d(e),z=s(e,"DIV",{class:!0});var ka=a(z);f(Es.$$.fragment,ka),Zg=d(ka),Fd=s(ka,"P",{});var vb=a(Fd);ev=p(vb,"Base class for model\u2019s outputs, with potential hidden states and attentions."),vb.forEach(t),tv=d(ka),Mo=s(ka,"DIV",{class:!0});var ch=a(Mo);f(Ps.$$.fragment,ch),ov=d(ch),Md=s(ch,"P",{});var yb=a(Md);nv=p(yb,"\u201CReturns a new object replacing the specified fields with new values."),yb.forEach(t),ch.forEach(t),ka.forEach(t),lc=d(e),qt=s(e,"H2",{class:!0});var ph=a(qt);ko=s(ph,"A",{id:!0,class:!0,href:!0});var Tb=a(ko);kd=s(Tb,"SPAN",{});var wb=a(kd);f(Bs.$$.fragment,wb),wb.forEach(t),Tb.forEach(t),sv=d(ph),Ad=s(ph,"SPAN",{});var bb=a(Ad);av=p(bb,"FlaxBaseModelOutputWithPooling"),bb.forEach(t),ph.forEach(t),cc=d(e),E=s(e,"DIV",{class:!0});var Aa=a(E);f(Ls.$$.fragment,Aa),rv=d(Aa),Cd=s(Aa,"P",{});var $b=a(Cd);iv=p($b,"Base class for model\u2019s outputs that also contains a pooling of the last hidden states."),$b.forEach(t),dv=d(Aa),Ao=s(Aa,"DIV",{class:!0});var hh=a(Ao);f(js.$$.fragment,hh),uv=d(hh),Nd=s(hh,"P",{});var qb=a(Nd);lv=p(qb,"\u201CReturns a new object replacing the specified fields with new values."),qb.forEach(t),hh.forEach(t),Aa.forEach(t),pc=d(e),xt=s(e,"H2",{class:!0});var fh=a(xt);Co=s(fh,"A",{id:!0,class:!0,href:!0});var xb=a(Co);zd=s(xb,"SPAN",{});var Ob=a(zd);f(Ws.$$.fragment,Ob),Ob.forEach(t),xb.forEach(t),cv=d(fh),Ed=s(fh,"SPAN",{});var Sb=a(Ed);pv=p(Sb,"FlaxBaseModelOutputWithPastAndCrossAttentions"),Sb.forEach(t),fh.forEach(t),hc=d(e),P=s(e,"DIV",{class:!0});var Ca=a(P);f(Ds.$$.fragment,Ca),hv=d(Ca),Pd=s(Ca,"P",{});var Fb=a(Pd);fv=p(Fb,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Fb.forEach(t),_v=d(Ca),No=s(Ca,"DIV",{class:!0});var _h=a(No);f(Hs.$$.fragment,_h),mv=d(_h),Bd=s(_h,"P",{});var Mb=a(Bd);gv=p(Mb,"\u201CReturns a new object replacing the specified fields with new values."),Mb.forEach(t),_h.forEach(t),Ca.forEach(t),fc=d(e),Ot=s(e,"H2",{class:!0});var mh=a(Ot);zo=s(mh,"A",{id:!0,class:!0,href:!0});var kb=a(zo);Ld=s(kb,"SPAN",{});var Ab=a(Ld);f(Qs.$$.fragment,Ab),Ab.forEach(t),kb.forEach(t),vv=d(mh),jd=s(mh,"SPAN",{});var Cb=a(jd);yv=p(Cb,"FlaxSeq2SeqModelOutput"),Cb.forEach(t),mh.forEach(t),_c=d(e),B=s(e,"DIV",{class:!0});var Na=a(B);f(Is.$$.fragment,Na),Tv=d(Na),Wd=s(Na,"P",{});var Nb=a(Wd);wv=p(Nb,`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),Nb.forEach(t),bv=d(Na),Eo=s(Na,"DIV",{class:!0});var gh=a(Eo);f(Vs.$$.fragment,gh),$v=d(gh),Dd=s(gh,"P",{});var zb=a(Dd);qv=p(zb,"\u201CReturns a new object replacing the specified fields with new values."),zb.forEach(t),gh.forEach(t),Na.forEach(t),mc=d(e),St=s(e,"H2",{class:!0});var vh=a(St);Po=s(vh,"A",{id:!0,class:!0,href:!0});var Eb=a(Po);Hd=s(Eb,"SPAN",{});var Pb=a(Hd);f(Rs.$$.fragment,Pb),Pb.forEach(t),Eb.forEach(t),xv=d(vh),Qd=s(vh,"SPAN",{});var Bb=a(Qd);Ov=p(Bb,"FlaxCausalLMOutputWithCrossAttentions"),Bb.forEach(t),vh.forEach(t),gc=d(e),L=s(e,"DIV",{class:!0});var za=a(L);f(Us.$$.fragment,za),Sv=d(za),Id=s(za,"P",{});var Lb=a(Id);Fv=p(Lb,"Base class for causal language model (or autoregressive) outputs."),Lb.forEach(t),Mv=d(za),Bo=s(za,"DIV",{class:!0});var yh=a(Bo);f(Ys.$$.fragment,yh),kv=d(yh),Vd=s(yh,"P",{});var jb=a(Vd);Av=p(jb,"\u201CReturns a new object replacing the specified fields with new values."),jb.forEach(t),yh.forEach(t),za.forEach(t),vc=d(e),Ft=s(e,"H2",{class:!0});var Th=a(Ft);Lo=s(Th,"A",{id:!0,class:!0,href:!0});var Wb=a(Lo);Rd=s(Wb,"SPAN",{});var Db=a(Rd);f(Js.$$.fragment,Db),Db.forEach(t),Wb.forEach(t),Cv=d(Th),Ud=s(Th,"SPAN",{});var Hb=a(Ud);Nv=p(Hb,"FlaxMaskedLMOutput"),Hb.forEach(t),Th.forEach(t),yc=d(e),j=s(e,"DIV",{class:!0});var Ea=a(j);f(Gs.$$.fragment,Ea),zv=d(Ea),Yd=s(Ea,"P",{});var Qb=a(Yd);Ev=p(Qb,"Base class for masked language models outputs."),Qb.forEach(t),Pv=d(Ea),jo=s(Ea,"DIV",{class:!0});var wh=a(jo);f(Ks.$$.fragment,wh),Bv=d(wh),Jd=s(wh,"P",{});var Ib=a(Jd);Lv=p(Ib,"\u201CReturns a new object replacing the specified fields with new values."),Ib.forEach(t),wh.forEach(t),Ea.forEach(t),Tc=d(e),Mt=s(e,"H2",{class:!0});var bh=a(Mt);Wo=s(bh,"A",{id:!0,class:!0,href:!0});var Vb=a(Wo);Gd=s(Vb,"SPAN",{});var Rb=a(Gd);f(Xs.$$.fragment,Rb),Rb.forEach(t),Vb.forEach(t),jv=d(bh),Kd=s(bh,"SPAN",{});var Ub=a(Kd);Wv=p(Ub,"FlaxSeq2SeqLMOutput"),Ub.forEach(t),bh.forEach(t),wc=d(e),W=s(e,"DIV",{class:!0});var Pa=a(W);f(Zs.$$.fragment,Pa),Dv=d(Pa),Xd=s(Pa,"P",{});var Yb=a(Xd);Hv=p(Yb,"Base class for sequence-to-sequence language models outputs."),Yb.forEach(t),Qv=d(Pa),Do=s(Pa,"DIV",{class:!0});var $h=a(Do);f(ea.$$.fragment,$h),Iv=d($h),Zd=s($h,"P",{});var Jb=a(Zd);Vv=p(Jb,"\u201CReturns a new object replacing the specified fields with new values."),Jb.forEach(t),$h.forEach(t),Pa.forEach(t),bc=d(e),kt=s(e,"H2",{class:!0});var qh=a(kt);Ho=s(qh,"A",{id:!0,class:!0,href:!0});var Gb=a(Ho);eu=s(Gb,"SPAN",{});var Kb=a(eu);f(ta.$$.fragment,Kb),Kb.forEach(t),Gb.forEach(t),Rv=d(qh),tu=s(qh,"SPAN",{});var Xb=a(tu);Uv=p(Xb,"FlaxNextSentencePredictorOutput"),Xb.forEach(t),qh.forEach(t),$c=d(e),D=s(e,"DIV",{class:!0});var Ba=a(D);f(oa.$$.fragment,Ba),Yv=d(Ba),ou=s(Ba,"P",{});var Zb=a(ou);Jv=p(Zb,"Base class for outputs of models predicting if two sentences are consecutive or not."),Zb.forEach(t),Gv=d(Ba),Qo=s(Ba,"DIV",{class:!0});var xh=a(Qo);f(na.$$.fragment,xh),Kv=d(xh),nu=s(xh,"P",{});var e$=a(nu);Xv=p(e$,"\u201CReturns a new object replacing the specified fields with new values."),e$.forEach(t),xh.forEach(t),Ba.forEach(t),qc=d(e),At=s(e,"H2",{class:!0});var Oh=a(At);Io=s(Oh,"A",{id:!0,class:!0,href:!0});var t$=a(Io);su=s(t$,"SPAN",{});var o$=a(su);f(sa.$$.fragment,o$),o$.forEach(t),t$.forEach(t),Zv=d(Oh),au=s(Oh,"SPAN",{});var n$=a(au);ey=p(n$,"FlaxSequenceClassifierOutput"),n$.forEach(t),Oh.forEach(t),xc=d(e),H=s(e,"DIV",{class:!0});var La=a(H);f(aa.$$.fragment,La),ty=d(La),ru=s(La,"P",{});var s$=a(ru);oy=p(s$,"Base class for outputs of sentence classification models."),s$.forEach(t),ny=d(La),Vo=s(La,"DIV",{class:!0});var Sh=a(Vo);f(ra.$$.fragment,Sh),sy=d(Sh),iu=s(Sh,"P",{});var a$=a(iu);ay=p(a$,"\u201CReturns a new object replacing the specified fields with new values."),a$.forEach(t),Sh.forEach(t),La.forEach(t),Oc=d(e),Ct=s(e,"H2",{class:!0});var Fh=a(Ct);Ro=s(Fh,"A",{id:!0,class:!0,href:!0});var r$=a(Ro);du=s(r$,"SPAN",{});var i$=a(du);f(ia.$$.fragment,i$),i$.forEach(t),r$.forEach(t),ry=d(Fh),uu=s(Fh,"SPAN",{});var d$=a(uu);iy=p(d$,"FlaxSeq2SeqSequenceClassifierOutput"),d$.forEach(t),Fh.forEach(t),Sc=d(e),Q=s(e,"DIV",{class:!0});var ja=a(Q);f(da.$$.fragment,ja),dy=d(ja),lu=s(ja,"P",{});var u$=a(lu);uy=p(u$,"Base class for outputs of sequence-to-sequence sentence classification models."),u$.forEach(t),ly=d(ja),Uo=s(ja,"DIV",{class:!0});var Mh=a(Uo);f(ua.$$.fragment,Mh),cy=d(Mh),cu=s(Mh,"P",{});var l$=a(cu);py=p(l$,"\u201CReturns a new object replacing the specified fields with new values."),l$.forEach(t),Mh.forEach(t),ja.forEach(t),Fc=d(e),Nt=s(e,"H2",{class:!0});var kh=a(Nt);Yo=s(kh,"A",{id:!0,class:!0,href:!0});var c$=a(Yo);pu=s(c$,"SPAN",{});var p$=a(pu);f(la.$$.fragment,p$),p$.forEach(t),c$.forEach(t),hy=d(kh),hu=s(kh,"SPAN",{});var h$=a(hu);fy=p(h$,"FlaxMultipleChoiceModelOutput"),h$.forEach(t),kh.forEach(t),Mc=d(e),I=s(e,"DIV",{class:!0});var Wa=a(I);f(ca.$$.fragment,Wa),_y=d(Wa),fu=s(Wa,"P",{});var f$=a(fu);my=p(f$,"Base class for outputs of multiple choice models."),f$.forEach(t),gy=d(Wa),Jo=s(Wa,"DIV",{class:!0});var Ah=a(Jo);f(pa.$$.fragment,Ah),vy=d(Ah),_u=s(Ah,"P",{});var _$=a(_u);yy=p(_$,"\u201CReturns a new object replacing the specified fields with new values."),_$.forEach(t),Ah.forEach(t),Wa.forEach(t),kc=d(e),zt=s(e,"H2",{class:!0});var Ch=a(zt);Go=s(Ch,"A",{id:!0,class:!0,href:!0});var m$=a(Go);mu=s(m$,"SPAN",{});var g$=a(mu);f(ha.$$.fragment,g$),g$.forEach(t),m$.forEach(t),Ty=d(Ch),gu=s(Ch,"SPAN",{});var v$=a(gu);wy=p(v$,"FlaxTokenClassifierOutput"),v$.forEach(t),Ch.forEach(t),Ac=d(e),V=s(e,"DIV",{class:!0});var Da=a(V);f(fa.$$.fragment,Da),by=d(Da),vu=s(Da,"P",{});var y$=a(vu);$y=p(y$,"Base class for outputs of token classification models."),y$.forEach(t),qy=d(Da),Ko=s(Da,"DIV",{class:!0});var Nh=a(Ko);f(_a.$$.fragment,Nh),xy=d(Nh),yu=s(Nh,"P",{});var T$=a(yu);Oy=p(T$,"\u201CReturns a new object replacing the specified fields with new values."),T$.forEach(t),Nh.forEach(t),Da.forEach(t),Cc=d(e),Et=s(e,"H2",{class:!0});var zh=a(Et);Xo=s(zh,"A",{id:!0,class:!0,href:!0});var w$=a(Xo);Tu=s(w$,"SPAN",{});var b$=a(Tu);f(ma.$$.fragment,b$),b$.forEach(t),w$.forEach(t),Sy=d(zh),wu=s(zh,"SPAN",{});var $$=a(wu);Fy=p($$,"FlaxQuestionAnsweringModelOutput"),$$.forEach(t),zh.forEach(t),Nc=d(e),R=s(e,"DIV",{class:!0});var Ha=a(R);f(ga.$$.fragment,Ha),My=d(Ha),bu=s(Ha,"P",{});var q$=a(bu);ky=p(q$,"Base class for outputs of question answering models."),q$.forEach(t),Ay=d(Ha),Zo=s(Ha,"DIV",{class:!0});var Eh=a(Zo);f(va.$$.fragment,Eh),Cy=d(Eh),$u=s(Eh,"P",{});var x$=a($u);Ny=p(x$,"\u201CReturns a new object replacing the specified fields with new values."),x$.forEach(t),Eh.forEach(t),Ha.forEach(t),zc=d(e),Pt=s(e,"H2",{class:!0});var Ph=a(Pt);en=s(Ph,"A",{id:!0,class:!0,href:!0});var O$=a(en);qu=s(O$,"SPAN",{});var S$=a(qu);f(ya.$$.fragment,S$),S$.forEach(t),O$.forEach(t),zy=d(Ph),xu=s(Ph,"SPAN",{});var F$=a(xu);Ey=p(F$,"FlaxSeq2SeqQuestionAnsweringModelOutput"),F$.forEach(t),Ph.forEach(t),Ec=d(e),U=s(e,"DIV",{class:!0});var Qa=a(U);f(Ta.$$.fragment,Qa),Py=d(Qa),Ou=s(Qa,"P",{});var M$=a(Ou);By=p(M$,"Base class for outputs of sequence-to-sequence question answering models."),M$.forEach(t),Ly=d(Qa),tn=s(Qa,"DIV",{class:!0});var Bh=a(tn);f(wa.$$.fragment,Bh),jy=d(Bh),Su=s(Bh,"P",{});var k$=a(Su);Wy=p(k$,"\u201CReturns a new object replacing the specified fields with new values."),k$.forEach(t),Bh.forEach(t),Qa.forEach(t),this.h()},h(){r($,"name","hf:doc:metadata"),r($,"content",JSON.stringify(W$)),r(A,"id","model-outputs"),r(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(A,"href","#model-outputs"),r(q,"class","relative group"),r(qa,"href","/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput"),r(Oa,"href","/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"),r(jt,"id","transformers.utils.ModelOutput"),r(jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(jt,"href","#transformers.utils.ModelOutput"),r(ee,"class","relative group"),r(Dt,"class","docstring"),r(k,"class","docstring"),r(Ht,"id","transformers.modeling_outputs.BaseModelOutput"),r(Ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Ht,"href","#transformers.modeling_outputs.BaseModelOutput"),r(oe,"class","relative group"),r(ne,"class","docstring"),r(Qt,"id","transformers.modeling_outputs.BaseModelOutputWithPooling"),r(Qt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Qt,"href","#transformers.modeling_outputs.BaseModelOutputWithPooling"),r(se,"class","relative group"),r(ae,"class","docstring"),r(It,"id","transformers.modeling_outputs.BaseModelOutputWithCrossAttentions"),r(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(It,"href","#transformers.modeling_outputs.BaseModelOutputWithCrossAttentions"),r(re,"class","relative group"),r(ie,"class","docstring"),r(Vt,"id","transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"),r(Vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Vt,"href","#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"),r(de,"class","relative group"),r(ue,"class","docstring"),r(Rt,"id","transformers.modeling_outputs.BaseModelOutputWithPast"),r(Rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Rt,"href","#transformers.modeling_outputs.BaseModelOutputWithPast"),r(le,"class","relative group"),r(ce,"class","docstring"),r(Ut,"id","transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"),r(Ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Ut,"href","#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"),r(pe,"class","relative group"),r(he,"class","docstring"),r(Yt,"id","transformers.modeling_outputs.Seq2SeqModelOutput"),r(Yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Yt,"href","#transformers.modeling_outputs.Seq2SeqModelOutput"),r(fe,"class","relative group"),r(_e,"class","docstring"),r(Jt,"id","transformers.modeling_outputs.CausalLMOutput"),r(Jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Jt,"href","#transformers.modeling_outputs.CausalLMOutput"),r(me,"class","relative group"),r(ge,"class","docstring"),r(Gt,"id","transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"),r(Gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Gt,"href","#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"),r(ve,"class","relative group"),r(ye,"class","docstring"),r(Kt,"id","transformers.modeling_outputs.CausalLMOutputWithPast"),r(Kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Kt,"href","#transformers.modeling_outputs.CausalLMOutputWithPast"),r(Te,"class","relative group"),r(we,"class","docstring"),r(Xt,"id","transformers.modeling_outputs.MaskedLMOutput"),r(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Xt,"href","#transformers.modeling_outputs.MaskedLMOutput"),r(be,"class","relative group"),r($e,"class","docstring"),r(Zt,"id","transformers.modeling_outputs.Seq2SeqLMOutput"),r(Zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Zt,"href","#transformers.modeling_outputs.Seq2SeqLMOutput"),r(qe,"class","relative group"),r(xe,"class","docstring"),r(eo,"id","transformers.modeling_outputs.NextSentencePredictorOutput"),r(eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(eo,"href","#transformers.modeling_outputs.NextSentencePredictorOutput"),r(Oe,"class","relative group"),r(Se,"class","docstring"),r(to,"id","transformers.modeling_outputs.SequenceClassifierOutput"),r(to,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(to,"href","#transformers.modeling_outputs.SequenceClassifierOutput"),r(Fe,"class","relative group"),r(Me,"class","docstring"),r(oo,"id","transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput"),r(oo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(oo,"href","#transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput"),r(ke,"class","relative group"),r(Ae,"class","docstring"),r(no,"id","transformers.modeling_outputs.MultipleChoiceModelOutput"),r(no,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(no,"href","#transformers.modeling_outputs.MultipleChoiceModelOutput"),r(Ce,"class","relative group"),r(Ne,"class","docstring"),r(so,"id","transformers.modeling_outputs.TokenClassifierOutput"),r(so,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(so,"href","#transformers.modeling_outputs.TokenClassifierOutput"),r(ze,"class","relative group"),r(Ee,"class","docstring"),r(ao,"id","transformers.modeling_outputs.QuestionAnsweringModelOutput"),r(ao,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(ao,"href","#transformers.modeling_outputs.QuestionAnsweringModelOutput"),r(Pe,"class","relative group"),r(Be,"class","docstring"),r(ro,"id","transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput"),r(ro,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(ro,"href","#transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput"),r(Le,"class","relative group"),r(je,"class","docstring"),r(io,"id","transformers.modeling_tf_outputs.TFBaseModelOutput"),r(io,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(io,"href","#transformers.modeling_tf_outputs.TFBaseModelOutput"),r(We,"class","relative group"),r(De,"class","docstring"),r(uo,"id","transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling"),r(uo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(uo,"href","#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling"),r(He,"class","relative group"),r(Qe,"class","docstring"),r(lo,"id","transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions"),r(lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(lo,"href","#transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions"),r(Ie,"class","relative group"),r(Ve,"class","docstring"),r(co,"id","transformers.modeling_tf_outputs.TFBaseModelOutputWithPast"),r(co,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(co,"href","#transformers.modeling_tf_outputs.TFBaseModelOutputWithPast"),r(Re,"class","relative group"),r(Ue,"class","docstring"),r(po,"id","transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions"),r(po,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(po,"href","#transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions"),r(Ye,"class","relative group"),r(Je,"class","docstring"),r(ho,"id","transformers.modeling_tf_outputs.TFSeq2SeqModelOutput"),r(ho,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(ho,"href","#transformers.modeling_tf_outputs.TFSeq2SeqModelOutput"),r(Ge,"class","relative group"),r(Ke,"class","docstring"),r(fo,"id","transformers.modeling_tf_outputs.TFCausalLMOutput"),r(fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(fo,"href","#transformers.modeling_tf_outputs.TFCausalLMOutput"),r(Xe,"class","relative group"),r(Ze,"class","docstring"),r(_o,"id","transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions"),r(_o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(_o,"href","#transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions"),r(et,"class","relative group"),r(tt,"class","docstring"),r(mo,"id","transformers.modeling_tf_outputs.TFCausalLMOutputWithPast"),r(mo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(mo,"href","#transformers.modeling_tf_outputs.TFCausalLMOutputWithPast"),r(ot,"class","relative group"),r(nt,"class","docstring"),r(go,"id","transformers.modeling_tf_outputs.TFMaskedLMOutput"),r(go,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(go,"href","#transformers.modeling_tf_outputs.TFMaskedLMOutput"),r(st,"class","relative group"),r(at,"class","docstring"),r(vo,"id","transformers.modeling_tf_outputs.TFSeq2SeqLMOutput"),r(vo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(vo,"href","#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput"),r(rt,"class","relative group"),r(it,"class","docstring"),r(yo,"id","transformers.modeling_tf_outputs.TFNextSentencePredictorOutput"),r(yo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(yo,"href","#transformers.modeling_tf_outputs.TFNextSentencePredictorOutput"),r(dt,"class","relative group"),r(ut,"class","docstring"),r(To,"id","transformers.modeling_tf_outputs.TFSequenceClassifierOutput"),r(To,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(To,"href","#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"),r(lt,"class","relative group"),r(ct,"class","docstring"),r(wo,"id","transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput"),r(wo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(wo,"href","#transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput"),r(pt,"class","relative group"),r(ht,"class","docstring"),r(bo,"id","transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"),r(bo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(bo,"href","#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"),r(ft,"class","relative group"),r(_t,"class","docstring"),r($o,"id","transformers.modeling_tf_outputs.TFTokenClassifierOutput"),r($o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r($o,"href","#transformers.modeling_tf_outputs.TFTokenClassifierOutput"),r(mt,"class","relative group"),r(gt,"class","docstring"),r(qo,"id","transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"),r(qo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(qo,"href","#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"),r(vt,"class","relative group"),r(yt,"class","docstring"),r(xo,"id","transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput"),r(xo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(xo,"href","#transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput"),r(Tt,"class","relative group"),r(wt,"class","docstring"),r(Oo,"id","transformers.modeling_flax_outputs.FlaxBaseModelOutput"),r(Oo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Oo,"href","#transformers.modeling_flax_outputs.FlaxBaseModelOutput"),r(bt,"class","relative group"),r(So,"class","docstring"),r(N,"class","docstring"),r(Fo,"id","transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast"),r(Fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Fo,"href","#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast"),r($t,"class","relative group"),r(Mo,"class","docstring"),r(z,"class","docstring"),r(ko,"id","transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling"),r(ko,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(ko,"href","#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling"),r(qt,"class","relative group"),r(Ao,"class","docstring"),r(E,"class","docstring"),r(Co,"id","transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions"),r(Co,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Co,"href","#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions"),r(xt,"class","relative group"),r(No,"class","docstring"),r(P,"class","docstring"),r(zo,"id","transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput"),r(zo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(zo,"href","#transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput"),r(Ot,"class","relative group"),r(Eo,"class","docstring"),r(B,"class","docstring"),r(Po,"id","transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions"),r(Po,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Po,"href","#transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions"),r(St,"class","relative group"),r(Bo,"class","docstring"),r(L,"class","docstring"),r(Lo,"id","transformers.modeling_flax_outputs.FlaxMaskedLMOutput"),r(Lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Lo,"href","#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"),r(Ft,"class","relative group"),r(jo,"class","docstring"),r(j,"class","docstring"),r(Wo,"id","transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput"),r(Wo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Wo,"href","#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput"),r(Mt,"class","relative group"),r(Do,"class","docstring"),r(W,"class","docstring"),r(Ho,"id","transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput"),r(Ho,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Ho,"href","#transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput"),r(kt,"class","relative group"),r(Qo,"class","docstring"),r(D,"class","docstring"),r(Io,"id","transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"),r(Io,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Io,"href","#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"),r(At,"class","relative group"),r(Vo,"class","docstring"),r(H,"class","docstring"),r(Ro,"id","transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput"),r(Ro,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Ro,"href","#transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput"),r(Ct,"class","relative group"),r(Uo,"class","docstring"),r(Q,"class","docstring"),r(Yo,"id","transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput"),r(Yo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Yo,"href","#transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput"),r(Nt,"class","relative group"),r(Jo,"class","docstring"),r(I,"class","docstring"),r(Go,"id","transformers.modeling_flax_outputs.FlaxTokenClassifierOutput"),r(Go,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Go,"href","#transformers.modeling_flax_outputs.FlaxTokenClassifierOutput"),r(zt,"class","relative group"),r(Ko,"class","docstring"),r(V,"class","docstring"),r(Xo,"id","transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput"),r(Xo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Xo,"href","#transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput"),r(Et,"class","relative group"),r(Zo,"class","docstring"),r(R,"class","docstring"),r(en,"id","transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput"),r(en,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(en,"href","#transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput"),r(Pt,"class","relative group"),r(tn,"class","docstring"),r(U,"class","docstring")},m(e,u){o(document.head,$),l(e,Bt,u),l(e,q,u),o(q,A),o(A,K),_(x,K,null),o(q,nn),o(q,X),o(X,Z),l(e,M,u),l(e,C,u),o(C,$a),o(C,qa),o(qa,Lh),o(C,jh),l(e,Mu,u),l(e,xa,u),o(xa,Wh),l(e,ku,u),_(sn,e,u),l(e,Au,u),l(e,w,u),o(w,Dh),o(w,Ia),o(Ia,Hh),o(w,Qh),o(w,Oa),o(Oa,Ih),o(w,Vh),o(w,Va),o(Va,Rh),o(w,Uh),o(w,Ra),o(Ra,Yh),o(w,Jh),o(w,Ua),o(Ua,Gh),o(w,Kh),o(w,Ya),o(Ya,Xh),o(w,Zh),o(w,Ja),o(Ja,ef),o(w,tf),o(w,Ga),o(Ga,of),o(w,nf),o(w,Ka),o(Ka,sf),o(w,af),o(w,Xa),o(Xa,rf),o(w,df),o(w,Za),o(Za,uf),o(w,lf),o(w,er),o(er,cf),o(w,pf),l(e,Cu,u),l(e,O,u),o(O,hf),o(O,tr),o(tr,ff),o(O,_f),o(O,or),o(or,mf),o(O,gf),o(O,nr),o(nr,vf),o(O,yf),o(O,sr),o(sr,Tf),o(O,wf),l(e,Nu,u),l(e,S,u),o(S,bf),o(S,ar),o(ar,$f),o(S,qf),o(S,rr),o(rr,xf),o(S,Of),o(S,ir),o(ir,Sf),o(S,Ff),o(S,dr),o(dr,Mf),o(S,kf),l(e,zu,u),_(an,e,u),l(e,Eu,u),l(e,Lt,u),o(Lt,Af),o(Lt,ur),o(ur,Cf),o(Lt,Nf),l(e,Pu,u),l(e,F,u),o(F,zf),o(F,lr),o(lr,Ef),o(F,Pf),o(F,cr),o(cr,Bf),o(F,Lf),o(F,pr),o(pr,jf),o(F,Wf),o(F,hr),o(hr,Df),o(F,Hf),l(e,Bu,u),l(e,Sa,u),o(Sa,Qf),l(e,Lu,u),l(e,ee,u),o(ee,jt),o(jt,fr),_(rn,fr,null),o(ee,If),o(ee,_r),o(_r,Vf),l(e,ju,u),l(e,k,u),_(dn,k,null),o(k,Rf),o(k,te),o(te,Uf),o(te,mr),o(mr,Yf),o(te,Jf),o(te,gr),o(gr,Gf),o(te,Kf),o(k,Xf),_(Wt,k,null),o(k,Zf),o(k,Dt),_(un,Dt,null),o(Dt,e_),o(Dt,ln),o(ln,t_),o(ln,vr),o(vr,o_),o(ln,n_),l(e,Wu,u),l(e,oe,u),o(oe,Ht),o(Ht,yr),_(cn,yr,null),o(oe,s_),o(oe,Tr),o(Tr,a_),l(e,Du,u),l(e,ne,u),_(pn,ne,null),o(ne,r_),o(ne,wr),o(wr,i_),l(e,Hu,u),l(e,se,u),o(se,Qt),o(Qt,br),_(hn,br,null),o(se,d_),o(se,$r),o($r,u_),l(e,Qu,u),l(e,ae,u),_(fn,ae,null),o(ae,l_),o(ae,qr),o(qr,c_),l(e,Iu,u),l(e,re,u),o(re,It),o(It,xr),_(_n,xr,null),o(re,p_),o(re,Or),o(Or,h_),l(e,Vu,u),l(e,ie,u),_(mn,ie,null),o(ie,f_),o(ie,Sr),o(Sr,__),l(e,Ru,u),l(e,de,u),o(de,Vt),o(Vt,Fr),_(gn,Fr,null),o(de,m_),o(de,Mr),o(Mr,g_),l(e,Uu,u),l(e,ue,u),_(vn,ue,null),o(ue,v_),o(ue,kr),o(kr,y_),l(e,Yu,u),l(e,le,u),o(le,Rt),o(Rt,Ar),_(yn,Ar,null),o(le,T_),o(le,Cr),o(Cr,w_),l(e,Ju,u),l(e,ce,u),_(Tn,ce,null),o(ce,b_),o(ce,Nr),o(Nr,$_),l(e,Gu,u),l(e,pe,u),o(pe,Ut),o(Ut,zr),_(wn,zr,null),o(pe,q_),o(pe,Er),o(Er,x_),l(e,Ku,u),l(e,he,u),_(bn,he,null),o(he,O_),o(he,Pr),o(Pr,S_),l(e,Xu,u),l(e,fe,u),o(fe,Yt),o(Yt,Br),_($n,Br,null),o(fe,F_),o(fe,Lr),o(Lr,M_),l(e,Zu,u),l(e,_e,u),_(qn,_e,null),o(_e,k_),o(_e,jr),o(jr,A_),l(e,el,u),l(e,me,u),o(me,Jt),o(Jt,Wr),_(xn,Wr,null),o(me,C_),o(me,Dr),o(Dr,N_),l(e,tl,u),l(e,ge,u),_(On,ge,null),o(ge,z_),o(ge,Hr),o(Hr,E_),l(e,ol,u),l(e,ve,u),o(ve,Gt),o(Gt,Qr),_(Sn,Qr,null),o(ve,P_),o(ve,Ir),o(Ir,B_),l(e,nl,u),l(e,ye,u),_(Fn,ye,null),o(ye,L_),o(ye,Vr),o(Vr,j_),l(e,sl,u),l(e,Te,u),o(Te,Kt),o(Kt,Rr),_(Mn,Rr,null),o(Te,W_),o(Te,Ur),o(Ur,D_),l(e,al,u),l(e,we,u),_(kn,we,null),o(we,H_),o(we,Yr),o(Yr,Q_),l(e,rl,u),l(e,be,u),o(be,Xt),o(Xt,Jr),_(An,Jr,null),o(be,I_),o(be,Gr),o(Gr,V_),l(e,il,u),l(e,$e,u),_(Cn,$e,null),o($e,R_),o($e,Kr),o(Kr,U_),l(e,dl,u),l(e,qe,u),o(qe,Zt),o(Zt,Xr),_(Nn,Xr,null),o(qe,Y_),o(qe,Zr),o(Zr,J_),l(e,ul,u),l(e,xe,u),_(zn,xe,null),o(xe,G_),o(xe,ei),o(ei,K_),l(e,ll,u),l(e,Oe,u),o(Oe,eo),o(eo,ti),_(En,ti,null),o(Oe,X_),o(Oe,oi),o(oi,Z_),l(e,cl,u),l(e,Se,u),_(Pn,Se,null),o(Se,em),o(Se,ni),o(ni,tm),l(e,pl,u),l(e,Fe,u),o(Fe,to),o(to,si),_(Bn,si,null),o(Fe,om),o(Fe,ai),o(ai,nm),l(e,hl,u),l(e,Me,u),_(Ln,Me,null),o(Me,sm),o(Me,ri),o(ri,am),l(e,fl,u),l(e,ke,u),o(ke,oo),o(oo,ii),_(jn,ii,null),o(ke,rm),o(ke,di),o(di,im),l(e,_l,u),l(e,Ae,u),_(Wn,Ae,null),o(Ae,dm),o(Ae,ui),o(ui,um),l(e,ml,u),l(e,Ce,u),o(Ce,no),o(no,li),_(Dn,li,null),o(Ce,lm),o(Ce,ci),o(ci,cm),l(e,gl,u),l(e,Ne,u),_(Hn,Ne,null),o(Ne,pm),o(Ne,pi),o(pi,hm),l(e,vl,u),l(e,ze,u),o(ze,so),o(so,hi),_(Qn,hi,null),o(ze,fm),o(ze,fi),o(fi,_m),l(e,yl,u),l(e,Ee,u),_(In,Ee,null),o(Ee,mm),o(Ee,_i),o(_i,gm),l(e,Tl,u),l(e,Pe,u),o(Pe,ao),o(ao,mi),_(Vn,mi,null),o(Pe,vm),o(Pe,gi),o(gi,ym),l(e,wl,u),l(e,Be,u),_(Rn,Be,null),o(Be,Tm),o(Be,vi),o(vi,wm),l(e,bl,u),l(e,Le,u),o(Le,ro),o(ro,yi),_(Un,yi,null),o(Le,bm),o(Le,Ti),o(Ti,$m),l(e,$l,u),l(e,je,u),_(Yn,je,null),o(je,qm),o(je,wi),o(wi,xm),l(e,ql,u),l(e,We,u),o(We,io),o(io,bi),_(Jn,bi,null),o(We,Om),o(We,$i),o($i,Sm),l(e,xl,u),l(e,De,u),_(Gn,De,null),o(De,Fm),o(De,qi),o(qi,Mm),l(e,Ol,u),l(e,He,u),o(He,uo),o(uo,xi),_(Kn,xi,null),o(He,km),o(He,Oi),o(Oi,Am),l(e,Sl,u),l(e,Qe,u),_(Xn,Qe,null),o(Qe,Cm),o(Qe,Si),o(Si,Nm),l(e,Fl,u),l(e,Ie,u),o(Ie,lo),o(lo,Fi),_(Zn,Fi,null),o(Ie,zm),o(Ie,Mi),o(Mi,Em),l(e,Ml,u),l(e,Ve,u),_(es,Ve,null),o(Ve,Pm),o(Ve,ki),o(ki,Bm),l(e,kl,u),l(e,Re,u),o(Re,co),o(co,Ai),_(ts,Ai,null),o(Re,Lm),o(Re,Ci),o(Ci,jm),l(e,Al,u),l(e,Ue,u),_(os,Ue,null),o(Ue,Wm),o(Ue,Ni),o(Ni,Dm),l(e,Cl,u),l(e,Ye,u),o(Ye,po),o(po,zi),_(ns,zi,null),o(Ye,Hm),o(Ye,Ei),o(Ei,Qm),l(e,Nl,u),l(e,Je,u),_(ss,Je,null),o(Je,Im),o(Je,Pi),o(Pi,Vm),l(e,zl,u),l(e,Ge,u),o(Ge,ho),o(ho,Bi),_(as,Bi,null),o(Ge,Rm),o(Ge,Li),o(Li,Um),l(e,El,u),l(e,Ke,u),_(rs,Ke,null),o(Ke,Ym),o(Ke,ji),o(ji,Jm),l(e,Pl,u),l(e,Xe,u),o(Xe,fo),o(fo,Wi),_(is,Wi,null),o(Xe,Gm),o(Xe,Di),o(Di,Km),l(e,Bl,u),l(e,Ze,u),_(ds,Ze,null),o(Ze,Xm),o(Ze,Hi),o(Hi,Zm),l(e,Ll,u),l(e,et,u),o(et,_o),o(_o,Qi),_(us,Qi,null),o(et,eg),o(et,Ii),o(Ii,tg),l(e,jl,u),l(e,tt,u),_(ls,tt,null),o(tt,og),o(tt,Vi),o(Vi,ng),l(e,Wl,u),l(e,ot,u),o(ot,mo),o(mo,Ri),_(cs,Ri,null),o(ot,sg),o(ot,Ui),o(Ui,ag),l(e,Dl,u),l(e,nt,u),_(ps,nt,null),o(nt,rg),o(nt,Yi),o(Yi,ig),l(e,Hl,u),l(e,st,u),o(st,go),o(go,Ji),_(hs,Ji,null),o(st,dg),o(st,Gi),o(Gi,ug),l(e,Ql,u),l(e,at,u),_(fs,at,null),o(at,lg),o(at,Ki),o(Ki,cg),l(e,Il,u),l(e,rt,u),o(rt,vo),o(vo,Xi),_(_s,Xi,null),o(rt,pg),o(rt,Zi),o(Zi,hg),l(e,Vl,u),l(e,it,u),_(ms,it,null),o(it,fg),o(it,ed),o(ed,_g),l(e,Rl,u),l(e,dt,u),o(dt,yo),o(yo,td),_(gs,td,null),o(dt,mg),o(dt,od),o(od,gg),l(e,Ul,u),l(e,ut,u),_(vs,ut,null),o(ut,vg),o(ut,nd),o(nd,yg),l(e,Yl,u),l(e,lt,u),o(lt,To),o(To,sd),_(ys,sd,null),o(lt,Tg),o(lt,ad),o(ad,wg),l(e,Jl,u),l(e,ct,u),_(Ts,ct,null),o(ct,bg),o(ct,rd),o(rd,$g),l(e,Gl,u),l(e,pt,u),o(pt,wo),o(wo,id),_(ws,id,null),o(pt,qg),o(pt,dd),o(dd,xg),l(e,Kl,u),l(e,ht,u),_(bs,ht,null),o(ht,Og),o(ht,ud),o(ud,Sg),l(e,Xl,u),l(e,ft,u),o(ft,bo),o(bo,ld),_($s,ld,null),o(ft,Fg),o(ft,cd),o(cd,Mg),l(e,Zl,u),l(e,_t,u),_(qs,_t,null),o(_t,kg),o(_t,pd),o(pd,Ag),l(e,ec,u),l(e,mt,u),o(mt,$o),o($o,hd),_(xs,hd,null),o(mt,Cg),o(mt,fd),o(fd,Ng),l(e,tc,u),l(e,gt,u),_(Os,gt,null),o(gt,zg),o(gt,_d),o(_d,Eg),l(e,oc,u),l(e,vt,u),o(vt,qo),o(qo,md),_(Ss,md,null),o(vt,Pg),o(vt,gd),o(gd,Bg),l(e,nc,u),l(e,yt,u),_(Fs,yt,null),o(yt,Lg),o(yt,vd),o(vd,jg),l(e,sc,u),l(e,Tt,u),o(Tt,xo),o(xo,yd),_(Ms,yd,null),o(Tt,Wg),o(Tt,Td),o(Td,Dg),l(e,ac,u),l(e,wt,u),_(ks,wt,null),o(wt,Hg),o(wt,wd),o(wd,Qg),l(e,rc,u),l(e,bt,u),o(bt,Oo),o(Oo,bd),_(As,bd,null),o(bt,Ig),o(bt,$d),o($d,Vg),l(e,ic,u),l(e,N,u),_(Cs,N,null),o(N,Rg),o(N,qd),o(qd,Ug),o(N,Yg),o(N,So),_(Ns,So,null),o(So,Jg),o(So,xd),o(xd,Gg),l(e,dc,u),l(e,$t,u),o($t,Fo),o(Fo,Od),_(zs,Od,null),o($t,Kg),o($t,Sd),o(Sd,Xg),l(e,uc,u),l(e,z,u),_(Es,z,null),o(z,Zg),o(z,Fd),o(Fd,ev),o(z,tv),o(z,Mo),_(Ps,Mo,null),o(Mo,ov),o(Mo,Md),o(Md,nv),l(e,lc,u),l(e,qt,u),o(qt,ko),o(ko,kd),_(Bs,kd,null),o(qt,sv),o(qt,Ad),o(Ad,av),l(e,cc,u),l(e,E,u),_(Ls,E,null),o(E,rv),o(E,Cd),o(Cd,iv),o(E,dv),o(E,Ao),_(js,Ao,null),o(Ao,uv),o(Ao,Nd),o(Nd,lv),l(e,pc,u),l(e,xt,u),o(xt,Co),o(Co,zd),_(Ws,zd,null),o(xt,cv),o(xt,Ed),o(Ed,pv),l(e,hc,u),l(e,P,u),_(Ds,P,null),o(P,hv),o(P,Pd),o(Pd,fv),o(P,_v),o(P,No),_(Hs,No,null),o(No,mv),o(No,Bd),o(Bd,gv),l(e,fc,u),l(e,Ot,u),o(Ot,zo),o(zo,Ld),_(Qs,Ld,null),o(Ot,vv),o(Ot,jd),o(jd,yv),l(e,_c,u),l(e,B,u),_(Is,B,null),o(B,Tv),o(B,Wd),o(Wd,wv),o(B,bv),o(B,Eo),_(Vs,Eo,null),o(Eo,$v),o(Eo,Dd),o(Dd,qv),l(e,mc,u),l(e,St,u),o(St,Po),o(Po,Hd),_(Rs,Hd,null),o(St,xv),o(St,Qd),o(Qd,Ov),l(e,gc,u),l(e,L,u),_(Us,L,null),o(L,Sv),o(L,Id),o(Id,Fv),o(L,Mv),o(L,Bo),_(Ys,Bo,null),o(Bo,kv),o(Bo,Vd),o(Vd,Av),l(e,vc,u),l(e,Ft,u),o(Ft,Lo),o(Lo,Rd),_(Js,Rd,null),o(Ft,Cv),o(Ft,Ud),o(Ud,Nv),l(e,yc,u),l(e,j,u),_(Gs,j,null),o(j,zv),o(j,Yd),o(Yd,Ev),o(j,Pv),o(j,jo),_(Ks,jo,null),o(jo,Bv),o(jo,Jd),o(Jd,Lv),l(e,Tc,u),l(e,Mt,u),o(Mt,Wo),o(Wo,Gd),_(Xs,Gd,null),o(Mt,jv),o(Mt,Kd),o(Kd,Wv),l(e,wc,u),l(e,W,u),_(Zs,W,null),o(W,Dv),o(W,Xd),o(Xd,Hv),o(W,Qv),o(W,Do),_(ea,Do,null),o(Do,Iv),o(Do,Zd),o(Zd,Vv),l(e,bc,u),l(e,kt,u),o(kt,Ho),o(Ho,eu),_(ta,eu,null),o(kt,Rv),o(kt,tu),o(tu,Uv),l(e,$c,u),l(e,D,u),_(oa,D,null),o(D,Yv),o(D,ou),o(ou,Jv),o(D,Gv),o(D,Qo),_(na,Qo,null),o(Qo,Kv),o(Qo,nu),o(nu,Xv),l(e,qc,u),l(e,At,u),o(At,Io),o(Io,su),_(sa,su,null),o(At,Zv),o(At,au),o(au,ey),l(e,xc,u),l(e,H,u),_(aa,H,null),o(H,ty),o(H,ru),o(ru,oy),o(H,ny),o(H,Vo),_(ra,Vo,null),o(Vo,sy),o(Vo,iu),o(iu,ay),l(e,Oc,u),l(e,Ct,u),o(Ct,Ro),o(Ro,du),_(ia,du,null),o(Ct,ry),o(Ct,uu),o(uu,iy),l(e,Sc,u),l(e,Q,u),_(da,Q,null),o(Q,dy),o(Q,lu),o(lu,uy),o(Q,ly),o(Q,Uo),_(ua,Uo,null),o(Uo,cy),o(Uo,cu),o(cu,py),l(e,Fc,u),l(e,Nt,u),o(Nt,Yo),o(Yo,pu),_(la,pu,null),o(Nt,hy),o(Nt,hu),o(hu,fy),l(e,Mc,u),l(e,I,u),_(ca,I,null),o(I,_y),o(I,fu),o(fu,my),o(I,gy),o(I,Jo),_(pa,Jo,null),o(Jo,vy),o(Jo,_u),o(_u,yy),l(e,kc,u),l(e,zt,u),o(zt,Go),o(Go,mu),_(ha,mu,null),o(zt,Ty),o(zt,gu),o(gu,wy),l(e,Ac,u),l(e,V,u),_(fa,V,null),o(V,by),o(V,vu),o(vu,$y),o(V,qy),o(V,Ko),_(_a,Ko,null),o(Ko,xy),o(Ko,yu),o(yu,Oy),l(e,Cc,u),l(e,Et,u),o(Et,Xo),o(Xo,Tu),_(ma,Tu,null),o(Et,Sy),o(Et,wu),o(wu,Fy),l(e,Nc,u),l(e,R,u),_(ga,R,null),o(R,My),o(R,bu),o(bu,ky),o(R,Ay),o(R,Zo),_(va,Zo,null),o(Zo,Cy),o(Zo,$u),o($u,Ny),l(e,zc,u),l(e,Pt,u),o(Pt,en),o(en,qu),_(ya,qu,null),o(Pt,zy),o(Pt,xu),o(xu,Ey),l(e,Ec,u),l(e,U,u),_(Ta,U,null),o(U,Py),o(U,Ou),o(Ou,By),o(U,Ly),o(U,tn),_(wa,tn,null),o(tn,jy),o(tn,Su),o(Su,Wy),Pc=!0},p(e,[u]){const ba={};u&2&&(ba.$$scope={dirty:u,ctx:e}),Wt.$set(ba)},i(e){Pc||(m(x.$$.fragment,e),m(sn.$$.fragment,e),m(an.$$.fragment,e),m(rn.$$.fragment,e),m(dn.$$.fragment,e),m(Wt.$$.fragment,e),m(un.$$.fragment,e),m(cn.$$.fragment,e),m(pn.$$.fragment,e),m(hn.$$.fragment,e),m(fn.$$.fragment,e),m(_n.$$.fragment,e),m(mn.$$.fragment,e),m(gn.$$.fragment,e),m(vn.$$.fragment,e),m(yn.$$.fragment,e),m(Tn.$$.fragment,e),m(wn.$$.fragment,e),m(bn.$$.fragment,e),m($n.$$.fragment,e),m(qn.$$.fragment,e),m(xn.$$.fragment,e),m(On.$$.fragment,e),m(Sn.$$.fragment,e),m(Fn.$$.fragment,e),m(Mn.$$.fragment,e),m(kn.$$.fragment,e),m(An.$$.fragment,e),m(Cn.$$.fragment,e),m(Nn.$$.fragment,e),m(zn.$$.fragment,e),m(En.$$.fragment,e),m(Pn.$$.fragment,e),m(Bn.$$.fragment,e),m(Ln.$$.fragment,e),m(jn.$$.fragment,e),m(Wn.$$.fragment,e),m(Dn.$$.fragment,e),m(Hn.$$.fragment,e),m(Qn.$$.fragment,e),m(In.$$.fragment,e),m(Vn.$$.fragment,e),m(Rn.$$.fragment,e),m(Un.$$.fragment,e),m(Yn.$$.fragment,e),m(Jn.$$.fragment,e),m(Gn.$$.fragment,e),m(Kn.$$.fragment,e),m(Xn.$$.fragment,e),m(Zn.$$.fragment,e),m(es.$$.fragment,e),m(ts.$$.fragment,e),m(os.$$.fragment,e),m(ns.$$.fragment,e),m(ss.$$.fragment,e),m(as.$$.fragment,e),m(rs.$$.fragment,e),m(is.$$.fragment,e),m(ds.$$.fragment,e),m(us.$$.fragment,e),m(ls.$$.fragment,e),m(cs.$$.fragment,e),m(ps.$$.fragment,e),m(hs.$$.fragment,e),m(fs.$$.fragment,e),m(_s.$$.fragment,e),m(ms.$$.fragment,e),m(gs.$$.fragment,e),m(vs.$$.fragment,e),m(ys.$$.fragment,e),m(Ts.$$.fragment,e),m(ws.$$.fragment,e),m(bs.$$.fragment,e),m($s.$$.fragment,e),m(qs.$$.fragment,e),m(xs.$$.fragment,e),m(Os.$$.fragment,e),m(Ss.$$.fragment,e),m(Fs.$$.fragment,e),m(Ms.$$.fragment,e),m(ks.$$.fragment,e),m(As.$$.fragment,e),m(Cs.$$.fragment,e),m(Ns.$$.fragment,e),m(zs.$$.fragment,e),m(Es.$$.fragment,e),m(Ps.$$.fragment,e),m(Bs.$$.fragment,e),m(Ls.$$.fragment,e),m(js.$$.fragment,e),m(Ws.$$.fragment,e),m(Ds.$$.fragment,e),m(Hs.$$.fragment,e),m(Qs.$$.fragment,e),m(Is.$$.fragment,e),m(Vs.$$.fragment,e),m(Rs.$$.fragment,e),m(Us.$$.fragment,e),m(Ys.$$.fragment,e),m(Js.$$.fragment,e),m(Gs.$$.fragment,e),m(Ks.$$.fragment,e),m(Xs.$$.fragment,e),m(Zs.$$.fragment,e),m(ea.$$.fragment,e),m(ta.$$.fragment,e),m(oa.$$.fragment,e),m(na.$$.fragment,e),m(sa.$$.fragment,e),m(aa.$$.fragment,e),m(ra.$$.fragment,e),m(ia.$$.fragment,e),m(da.$$.fragment,e),m(ua.$$.fragment,e),m(la.$$.fragment,e),m(ca.$$.fragment,e),m(pa.$$.fragment,e),m(ha.$$.fragment,e),m(fa.$$.fragment,e),m(_a.$$.fragment,e),m(ma.$$.fragment,e),m(ga.$$.fragment,e),m(va.$$.fragment,e),m(ya.$$.fragment,e),m(Ta.$$.fragment,e),m(wa.$$.fragment,e),Pc=!0)},o(e){g(x.$$.fragment,e),g(sn.$$.fragment,e),g(an.$$.fragment,e),g(rn.$$.fragment,e),g(dn.$$.fragment,e),g(Wt.$$.fragment,e),g(un.$$.fragment,e),g(cn.$$.fragment,e),g(pn.$$.fragment,e),g(hn.$$.fragment,e),g(fn.$$.fragment,e),g(_n.$$.fragment,e),g(mn.$$.fragment,e),g(gn.$$.fragment,e),g(vn.$$.fragment,e),g(yn.$$.fragment,e),g(Tn.$$.fragment,e),g(wn.$$.fragment,e),g(bn.$$.fragment,e),g($n.$$.fragment,e),g(qn.$$.fragment,e),g(xn.$$.fragment,e),g(On.$$.fragment,e),g(Sn.$$.fragment,e),g(Fn.$$.fragment,e),g(Mn.$$.fragment,e),g(kn.$$.fragment,e),g(An.$$.fragment,e),g(Cn.$$.fragment,e),g(Nn.$$.fragment,e),g(zn.$$.fragment,e),g(En.$$.fragment,e),g(Pn.$$.fragment,e),g(Bn.$$.fragment,e),g(Ln.$$.fragment,e),g(jn.$$.fragment,e),g(Wn.$$.fragment,e),g(Dn.$$.fragment,e),g(Hn.$$.fragment,e),g(Qn.$$.fragment,e),g(In.$$.fragment,e),g(Vn.$$.fragment,e),g(Rn.$$.fragment,e),g(Un.$$.fragment,e),g(Yn.$$.fragment,e),g(Jn.$$.fragment,e),g(Gn.$$.fragment,e),g(Kn.$$.fragment,e),g(Xn.$$.fragment,e),g(Zn.$$.fragment,e),g(es.$$.fragment,e),g(ts.$$.fragment,e),g(os.$$.fragment,e),g(ns.$$.fragment,e),g(ss.$$.fragment,e),g(as.$$.fragment,e),g(rs.$$.fragment,e),g(is.$$.fragment,e),g(ds.$$.fragment,e),g(us.$$.fragment,e),g(ls.$$.fragment,e),g(cs.$$.fragment,e),g(ps.$$.fragment,e),g(hs.$$.fragment,e),g(fs.$$.fragment,e),g(_s.$$.fragment,e),g(ms.$$.fragment,e),g(gs.$$.fragment,e),g(vs.$$.fragment,e),g(ys.$$.fragment,e),g(Ts.$$.fragment,e),g(ws.$$.fragment,e),g(bs.$$.fragment,e),g($s.$$.fragment,e),g(qs.$$.fragment,e),g(xs.$$.fragment,e),g(Os.$$.fragment,e),g(Ss.$$.fragment,e),g(Fs.$$.fragment,e),g(Ms.$$.fragment,e),g(ks.$$.fragment,e),g(As.$$.fragment,e),g(Cs.$$.fragment,e),g(Ns.$$.fragment,e),g(zs.$$.fragment,e),g(Es.$$.fragment,e),g(Ps.$$.fragment,e),g(Bs.$$.fragment,e),g(Ls.$$.fragment,e),g(js.$$.fragment,e),g(Ws.$$.fragment,e),g(Ds.$$.fragment,e),g(Hs.$$.fragment,e),g(Qs.$$.fragment,e),g(Is.$$.fragment,e),g(Vs.$$.fragment,e),g(Rs.$$.fragment,e),g(Us.$$.fragment,e),g(Ys.$$.fragment,e),g(Js.$$.fragment,e),g(Gs.$$.fragment,e),g(Ks.$$.fragment,e),g(Xs.$$.fragment,e),g(Zs.$$.fragment,e),g(ea.$$.fragment,e),g(ta.$$.fragment,e),g(oa.$$.fragment,e),g(na.$$.fragment,e),g(sa.$$.fragment,e),g(aa.$$.fragment,e),g(ra.$$.fragment,e),g(ia.$$.fragment,e),g(da.$$.fragment,e),g(ua.$$.fragment,e),g(la.$$.fragment,e),g(ca.$$.fragment,e),g(pa.$$.fragment,e),g(ha.$$.fragment,e),g(fa.$$.fragment,e),g(_a.$$.fragment,e),g(ma.$$.fragment,e),g(ga.$$.fragment,e),g(va.$$.fragment,e),g(ya.$$.fragment,e),g(Ta.$$.fragment,e),g(wa.$$.fragment,e),Pc=!1},d(e){t($),e&&t(Bt),e&&t(q),v(x),e&&t(M),e&&t(C),e&&t(Mu),e&&t(xa),e&&t(ku),v(sn,e),e&&t(Au),e&&t(w),e&&t(Cu),e&&t(O),e&&t(Nu),e&&t(S),e&&t(zu),v(an,e),e&&t(Eu),e&&t(Lt),e&&t(Pu),e&&t(F),e&&t(Bu),e&&t(Sa),e&&t(Lu),e&&t(ee),v(rn),e&&t(ju),e&&t(k),v(dn),v(Wt),v(un),e&&t(Wu),e&&t(oe),v(cn),e&&t(Du),e&&t(ne),v(pn),e&&t(Hu),e&&t(se),v(hn),e&&t(Qu),e&&t(ae),v(fn),e&&t(Iu),e&&t(re),v(_n),e&&t(Vu),e&&t(ie),v(mn),e&&t(Ru),e&&t(de),v(gn),e&&t(Uu),e&&t(ue),v(vn),e&&t(Yu),e&&t(le),v(yn),e&&t(Ju),e&&t(ce),v(Tn),e&&t(Gu),e&&t(pe),v(wn),e&&t(Ku),e&&t(he),v(bn),e&&t(Xu),e&&t(fe),v($n),e&&t(Zu),e&&t(_e),v(qn),e&&t(el),e&&t(me),v(xn),e&&t(tl),e&&t(ge),v(On),e&&t(ol),e&&t(ve),v(Sn),e&&t(nl),e&&t(ye),v(Fn),e&&t(sl),e&&t(Te),v(Mn),e&&t(al),e&&t(we),v(kn),e&&t(rl),e&&t(be),v(An),e&&t(il),e&&t($e),v(Cn),e&&t(dl),e&&t(qe),v(Nn),e&&t(ul),e&&t(xe),v(zn),e&&t(ll),e&&t(Oe),v(En),e&&t(cl),e&&t(Se),v(Pn),e&&t(pl),e&&t(Fe),v(Bn),e&&t(hl),e&&t(Me),v(Ln),e&&t(fl),e&&t(ke),v(jn),e&&t(_l),e&&t(Ae),v(Wn),e&&t(ml),e&&t(Ce),v(Dn),e&&t(gl),e&&t(Ne),v(Hn),e&&t(vl),e&&t(ze),v(Qn),e&&t(yl),e&&t(Ee),v(In),e&&t(Tl),e&&t(Pe),v(Vn),e&&t(wl),e&&t(Be),v(Rn),e&&t(bl),e&&t(Le),v(Un),e&&t($l),e&&t(je),v(Yn),e&&t(ql),e&&t(We),v(Jn),e&&t(xl),e&&t(De),v(Gn),e&&t(Ol),e&&t(He),v(Kn),e&&t(Sl),e&&t(Qe),v(Xn),e&&t(Fl),e&&t(Ie),v(Zn),e&&t(Ml),e&&t(Ve),v(es),e&&t(kl),e&&t(Re),v(ts),e&&t(Al),e&&t(Ue),v(os),e&&t(Cl),e&&t(Ye),v(ns),e&&t(Nl),e&&t(Je),v(ss),e&&t(zl),e&&t(Ge),v(as),e&&t(El),e&&t(Ke),v(rs),e&&t(Pl),e&&t(Xe),v(is),e&&t(Bl),e&&t(Ze),v(ds),e&&t(Ll),e&&t(et),v(us),e&&t(jl),e&&t(tt),v(ls),e&&t(Wl),e&&t(ot),v(cs),e&&t(Dl),e&&t(nt),v(ps),e&&t(Hl),e&&t(st),v(hs),e&&t(Ql),e&&t(at),v(fs),e&&t(Il),e&&t(rt),v(_s),e&&t(Vl),e&&t(it),v(ms),e&&t(Rl),e&&t(dt),v(gs),e&&t(Ul),e&&t(ut),v(vs),e&&t(Yl),e&&t(lt),v(ys),e&&t(Jl),e&&t(ct),v(Ts),e&&t(Gl),e&&t(pt),v(ws),e&&t(Kl),e&&t(ht),v(bs),e&&t(Xl),e&&t(ft),v($s),e&&t(Zl),e&&t(_t),v(qs),e&&t(ec),e&&t(mt),v(xs),e&&t(tc),e&&t(gt),v(Os),e&&t(oc),e&&t(vt),v(Ss),e&&t(nc),e&&t(yt),v(Fs),e&&t(sc),e&&t(Tt),v(Ms),e&&t(ac),e&&t(wt),v(ks),e&&t(rc),e&&t(bt),v(As),e&&t(ic),e&&t(N),v(Cs),v(Ns),e&&t(dc),e&&t($t),v(zs),e&&t(uc),e&&t(z),v(Es),v(Ps),e&&t(lc),e&&t(qt),v(Bs),e&&t(cc),e&&t(E),v(Ls),v(js),e&&t(pc),e&&t(xt),v(Ws),e&&t(hc),e&&t(P),v(Ds),v(Hs),e&&t(fc),e&&t(Ot),v(Qs),e&&t(_c),e&&t(B),v(Is),v(Vs),e&&t(mc),e&&t(St),v(Rs),e&&t(gc),e&&t(L),v(Us),v(Ys),e&&t(vc),e&&t(Ft),v(Js),e&&t(yc),e&&t(j),v(Gs),v(Ks),e&&t(Tc),e&&t(Mt),v(Xs),e&&t(wc),e&&t(W),v(Zs),v(ea),e&&t(bc),e&&t(kt),v(ta),e&&t($c),e&&t(D),v(oa),v(na),e&&t(qc),e&&t(At),v(sa),e&&t(xc),e&&t(H),v(aa),v(ra),e&&t(Oc),e&&t(Ct),v(ia),e&&t(Sc),e&&t(Q),v(da),v(ua),e&&t(Fc),e&&t(Nt),v(la),e&&t(Mc),e&&t(I),v(ca),v(pa),e&&t(kc),e&&t(zt),v(ha),e&&t(Ac),e&&t(V),v(fa),v(_a),e&&t(Cc),e&&t(Et),v(ma),e&&t(Nc),e&&t(R),v(ga),v(va),e&&t(zc),e&&t(Pt),v(ya),e&&t(Ec),e&&t(U),v(Ta),v(wa)}}}const W$={local:"model-outputs",sections:[{local:"transformers.utils.ModelOutput",title:"ModelOutput"},{local:"transformers.modeling_outputs.BaseModelOutput",title:"BaseModelOutput"},{local:"transformers.modeling_outputs.BaseModelOutputWithPooling",title:"BaseModelOutputWithPooling"},{local:"transformers.modeling_outputs.BaseModelOutputWithCrossAttentions",title:"BaseModelOutputWithCrossAttentions"},{local:"transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions",title:"BaseModelOutputWithPoolingAndCrossAttentions"},{local:"transformers.modeling_outputs.BaseModelOutputWithPast",title:"BaseModelOutputWithPast"},{local:"transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions",title:"BaseModelOutputWithPastAndCrossAttentions"},{local:"transformers.modeling_outputs.Seq2SeqModelOutput",title:"Seq2SeqModelOutput"},{local:"transformers.modeling_outputs.CausalLMOutput",title:"CausalLMOutput"},{local:"transformers.modeling_outputs.CausalLMOutputWithCrossAttentions",title:"CausalLMOutputWithCrossAttentions"},{local:"transformers.modeling_outputs.CausalLMOutputWithPast",title:"CausalLMOutputWithPast"},{local:"transformers.modeling_outputs.MaskedLMOutput",title:"MaskedLMOutput"},{local:"transformers.modeling_outputs.Seq2SeqLMOutput",title:"Seq2SeqLMOutput"},{local:"transformers.modeling_outputs.NextSentencePredictorOutput",title:"NextSentencePredictorOutput"},{local:"transformers.modeling_outputs.SequenceClassifierOutput",title:"SequenceClassifierOutput"},{local:"transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput",title:"Seq2SeqSequenceClassifierOutput"},{local:"transformers.modeling_outputs.MultipleChoiceModelOutput",title:"MultipleChoiceModelOutput"},{local:"transformers.modeling_outputs.TokenClassifierOutput",title:"TokenClassifierOutput"},{local:"transformers.modeling_outputs.QuestionAnsweringModelOutput",title:"QuestionAnsweringModelOutput"},{local:"transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput",title:"Seq2SeqQuestionAnsweringModelOutput"},{local:"transformers.modeling_tf_outputs.TFBaseModelOutput",title:"TFBaseModelOutput"},{local:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling",title:"TFBaseModelOutputWithPooling"},{local:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions",title:"TFBaseModelOutputWithPoolingAndCrossAttentions"},{local:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPast",title:"TFBaseModelOutputWithPast"},{local:"transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions",title:"TFBaseModelOutputWithPastAndCrossAttentions"},{local:"transformers.modeling_tf_outputs.TFSeq2SeqModelOutput",title:"TFSeq2SeqModelOutput"},{local:"transformers.modeling_tf_outputs.TFCausalLMOutput",title:"TFCausalLMOutput"},{local:"transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions",title:"TFCausalLMOutputWithCrossAttentions"},{local:"transformers.modeling_tf_outputs.TFCausalLMOutputWithPast",title:"TFCausalLMOutputWithPast"},{local:"transformers.modeling_tf_outputs.TFMaskedLMOutput",title:"TFMaskedLMOutput"},{local:"transformers.modeling_tf_outputs.TFSeq2SeqLMOutput",title:"TFSeq2SeqLMOutput"},{local:"transformers.modeling_tf_outputs.TFNextSentencePredictorOutput",title:"TFNextSentencePredictorOutput"},{local:"transformers.modeling_tf_outputs.TFSequenceClassifierOutput",title:"TFSequenceClassifierOutput"},{local:"transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput",title:"TFSeq2SeqSequenceClassifierOutput"},{local:"transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput",title:"TFMultipleChoiceModelOutput"},{local:"transformers.modeling_tf_outputs.TFTokenClassifierOutput",title:"TFTokenClassifierOutput"},{local:"transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput",title:"TFQuestionAnsweringModelOutput"},{local:"transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput",title:"TFSeq2SeqQuestionAnsweringModelOutput"},{local:"transformers.modeling_flax_outputs.FlaxBaseModelOutput",title:"FlaxBaseModelOutput"},{local:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast",title:"FlaxBaseModelOutputWithPast"},{local:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling",title:"FlaxBaseModelOutputWithPooling"},{local:"transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions",title:"FlaxBaseModelOutputWithPastAndCrossAttentions"},{local:"transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput",title:"FlaxSeq2SeqModelOutput"},{local:"transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions",title:"FlaxCausalLMOutputWithCrossAttentions"},{local:"transformers.modeling_flax_outputs.FlaxMaskedLMOutput",title:"FlaxMaskedLMOutput"},{local:"transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput",title:"FlaxSeq2SeqLMOutput"},{local:"transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput",title:"FlaxNextSentencePredictorOutput"},{local:"transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput",title:"FlaxSequenceClassifierOutput"},{local:"transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput",title:"FlaxSeq2SeqSequenceClassifierOutput"},{local:"transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput",title:"FlaxMultipleChoiceModelOutput"},{local:"transformers.modeling_flax_outputs.FlaxTokenClassifierOutput",title:"FlaxTokenClassifierOutput"},{local:"transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput",title:"FlaxQuestionAnsweringModelOutput"},{local:"transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput",title:"FlaxSeq2SeqQuestionAnsweringModelOutput"}],title:"Model outputs"};function D$(Fu){return P$(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class U$ extends C${constructor($){super();N$(this,$,D$,j$,z$,{})}}export{U$ as default,W$ as metadata};
