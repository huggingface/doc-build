import{S as Kr,i as Qr,s as Xr,e as o,k as i,w as y,t as c,M as Zr,c as a,d as s,m as l,a as t,x as $,h as m,b as n,G as r,g,y as b,q as x,o as P,B as w,v as es,L as rs}from"../../chunks/vendor-hf-doc-builder.js";import{D as oe}from"../../chunks/Docstring-hf-doc-builder.js";import{C as ss}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as He}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as os}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function as(we){let f,D,_,h,I;return h=new ss({props:{code:`# We can't instantiate directly the base class *ImageProcessingMixin* so let's show the examples on a
# derived class: *CLIPImageProcessor*
image_processor = CLIPImageProcessor.from_pretrained(
    "openai/clip-vit-base-patch32"
)  # Download image_processing_config from huggingface.co and cache.
image_processor = CLIPImageProcessor.from_pretrained(
    "./test/saved_model/"
)  # E.g. image processor (or model) was saved using *save_pretrained('./test/saved_model/')*
image_processor = CLIPImageProcessor.from_pretrained("./test/saved_model/preprocessor_config.json")
image_processor = CLIPImageProcessor.from_pretrained(
    "openai/clip-vit-base-patch32", do_normalize=False, foo=False
)
assert image_processor.do_normalize is False
image_processor, unused_kwargs = CLIPImageProcessor.from_pretrained(
    "openai/clip-vit-base-patch32", do_normalize=False, foo=False, return_unused_kwargs=True
)
assert image_processor.do_normalize is False
assert unused_kwargs == {"foo": False}`,highlighted:`<span class="hljs-comment"># We can&#x27;t instantiate directly the base class *ImageProcessingMixin* so let&#x27;s show the examples on a</span>
<span class="hljs-comment"># derived class: *CLIPImageProcessor*</span>
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>
)  <span class="hljs-comment"># Download image_processing_config from huggingface.co and cache.</span>
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;./test/saved_model/&quot;</span>
)  <span class="hljs-comment"># E.g. image processor (or model) was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*</span>
image_processor = CLIPImageProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/preprocessor_config.json&quot;</span>)
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>, do_normalize=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>
)
<span class="hljs-keyword">assert</span> image_processor.do_normalize <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
image_processor, unused_kwargs = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>, do_normalize=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
)
<span class="hljs-keyword">assert</span> image_processor.do_normalize <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
<span class="hljs-keyword">assert</span> unused_kwargs == {<span class="hljs-string">&quot;foo&quot;</span>: <span class="hljs-literal">False</span>}`}}),{c(){f=o("p"),D=c("Examples:"),_=i(),y(h.$$.fragment)},l(d){f=a(d,"P",{});var k=t(f);D=m(k,"Examples:"),k.forEach(s),_=l(d),$(h.$$.fragment,d)},m(d,k){g(d,f,k),r(f,D),g(d,_,k),b(h,d,k),I=!0},p:rs,i(d){I||(x(h.$$.fragment,d),I=!0)},o(d){P(h.$$.fragment,d),I=!1},d(d){d&&s(f),d&&s(_),w(h,d)}}}function ts(we){let f,D,_,h,I,d,k,de,Ue,Ie,ae,Ve,ke,F,A,pe,V,Oe,ge,We,Ee,v,O,Je,fe,Re,Ye,E,W,Ge,J,Ke,te,Qe,Xe,Ze,C,er,N,R,rr,M,sr,he,or,ar,ne,tr,nr,Te,j,q,ue,Y,ir,_e,cr,Fe,u,G,lr,B,mr,ie,dr,pr,ve,gr,fr,hr,ye,ur,_r,S,K,vr,$e,yr,$r,z,Q,br,X,xr,be,Pr,wr,Me,L,H,xe,Z,Ir,Pe,kr,je,ee,re,Be;return d=new He({}),V=new He({}),O=new oe({props:{name:"class transformers.ImageProcessingMixin",anchor:"transformers.ImageProcessingMixin",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_processing_utils.py#L58"}}),W=new oe({props:{name:"from_pretrained",anchor:"transformers.ImageProcessingMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained image_processor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a image processor file saved using the
<a href="/docs/transformers/main/en/internal/image_processing_utils#transformers.ImageProcessingMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved image processor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model image processor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the image processor files and override the cached versions if
they exist.`,name:"force_download"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, or not specified, will use
the token generated when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_processing_utils.py#L82",returnDescription:`
<p>A image processor of type <a
  href="/docs/transformers/main/en/internal/image_processing_utils#transformers.ImageProcessingMixin"
>ImageProcessingMixin</a>.</p>
`}}),C=new os({props:{anchor:"transformers.ImageProcessingMixin.from_pretrained.example",$$slots:{default:[as]},$$scope:{ctx:we}}}),R=new oe({props:{name:"save_pretrained",anchor:"transformers.ImageProcessingMixin.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike]"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the image processor JSON file will be saved (will be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.ImageProcessingMixin.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).
kwargs &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/main/en/internal/image_processing_utils#transformers.ImageProcessingMixin.push_to_hub">push_to_hub()</a> method.`,name:"push_to_hub"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_processing_utils.py#L165"}}),Y=new He({}),G=new oe({props:{name:"class transformers.BatchFeature",anchor:"transformers.BatchFeature",parameters:[{name:"data",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"tensor_type",val:": typing.Union[NoneType, str, transformers.utils.generic.TensorType] = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.data",description:`<strong>data</strong> (<code>dict</code>) &#x2014;
Dictionary of lists/arrays/tensors returned by the <strong>call</strong>/pad methods (&#x2018;input_values&#x2019;, &#x2018;attention_mask&#x2019;,
etc.).`,name:"data"},{anchor:"transformers.BatchFeature.tensor_type",description:`<strong>tensor_type</strong> (<code>Union[None, str, TensorType]</code>, <em>optional</em>) &#x2014;
You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.`,name:"tensor_type"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L58"}}),K=new oe({props:{name:"convert_to_tensors",anchor:"transformers.BatchFeature.convert_to_tensors",parameters:[{name:"tensor_type",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.convert_to_tensors.tensor_type",description:`<strong>tensor_type</strong> (<code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
The type of tensors to use. If <code>str</code>, should be one of the values of the enum <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>. If
<code>None</code>, no modification is done.`,name:"tensor_type"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L112"}}),Q=new oe({props:{name:"to",anchor:"transformers.BatchFeature.to",parameters:[{name:"device",val:": typing.Union[str, ForwardRef('torch.device')]"}],parametersDescription:[{anchor:"transformers.BatchFeature.to.device",description:"<strong>device</strong> (<code>str</code> or <code>torch.device</code>) &#x2014; The device to put the tensors on.",name:"device"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L177",returnDescription:`
<p>The same instance after modification.</p>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),Z=new He({}),re=new oe({props:{name:"class transformers.image_processing_utils.BaseImageProcessor",anchor:"transformers.image_processing_utils.BaseImageProcessor",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_processing_utils.py#L431"}}),{c(){f=o("meta"),D=i(),_=o("h1"),h=o("a"),I=o("span"),y(d.$$.fragment),k=i(),de=o("span"),Ue=c("Image Processor"),Ie=i(),ae=o("p"),Ve=c("An image processor is in charge of preparing input features for vision models and post processing their outputs. This includes transformations such as resizing, normalization, and conversion to PyTorch, TensorFlow, Flax and Numpy tensors. It may also include model specific post-processing such as converting logits to segmentation masks."),ke=i(),F=o("h2"),A=o("a"),pe=o("span"),y(V.$$.fragment),Oe=i(),ge=o("span"),We=c("ImageProcessingMixin"),Ee=i(),v=o("div"),y(O.$$.fragment),Je=i(),fe=o("p"),Re=c(`This is an image processor mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),Ye=i(),E=o("div"),y(W.$$.fragment),Ge=i(),J=o("p"),Ke=c("Instantiate a type of "),te=o("a"),Qe=c("ImageProcessingMixin"),Xe=c(" from an image processor."),Ze=i(),y(C.$$.fragment),er=i(),N=o("div"),y(R.$$.fragment),rr=i(),M=o("p"),sr=c("Save an image processor object to the directory "),he=o("code"),or=c("save_directory"),ar=c(`, so that it can be re-loaded using the
`),ne=o("a"),tr=c("from_pretrained()"),nr=c(" class method."),Te=i(),j=o("h2"),q=o("a"),ue=o("span"),y(Y.$$.fragment),ir=i(),_e=o("span"),cr=c("BatchFeature"),Fe=i(),u=o("div"),y(G.$$.fragment),lr=i(),B=o("p"),mr=c("Holds the output of the "),ie=o("a"),dr=c("pad()"),pr=c(" and feature extractor specific "),ve=o("code"),gr=c("__call__"),fr=c(" methods."),hr=i(),ye=o("p"),ur=c("This class is derived from a python dictionary and can be used as a dictionary."),_r=i(),S=o("div"),y(K.$$.fragment),vr=i(),$e=o("p"),yr=c("Convert the inner content to tensors."),$r=i(),z=o("div"),y(Q.$$.fragment),br=i(),X=o("p"),xr=c("Send all values to device by calling "),be=o("code"),Pr=c("v.to(device)"),wr=c(" (PyTorch only)."),Me=i(),L=o("h2"),H=o("a"),xe=o("span"),y(Z.$$.fragment),Ir=i(),Pe=o("span"),kr=c("BaseImageProcessor"),je=i(),ee=o("div"),y(re.$$.fragment),this.h()},l(e){const p=Zr('[data-svelte="svelte-1phssyn"]',document.head);f=a(p,"META",{name:!0,content:!0}),p.forEach(s),D=l(e),_=a(e,"H1",{class:!0});var se=t(_);h=a(se,"A",{id:!0,class:!0,href:!0});var Er=t(h);I=a(Er,"SPAN",{});var Tr=t(I);$(d.$$.fragment,Tr),Tr.forEach(s),Er.forEach(s),k=l(se),de=a(se,"SPAN",{});var Fr=t(de);Ue=m(Fr,"Image Processor"),Fr.forEach(s),se.forEach(s),Ie=l(e),ae=a(e,"P",{});var Mr=t(ae);Ve=m(Mr,"An image processor is in charge of preparing input features for vision models and post processing their outputs. This includes transformations such as resizing, normalization, and conversion to PyTorch, TensorFlow, Flax and Numpy tensors. It may also include model specific post-processing such as converting logits to segmentation masks."),Mr.forEach(s),ke=l(e),F=a(e,"H2",{class:!0});var Le=t(F);A=a(Le,"A",{id:!0,class:!0,href:!0});var jr=t(A);pe=a(jr,"SPAN",{});var Br=t(pe);$(V.$$.fragment,Br),Br.forEach(s),jr.forEach(s),Oe=l(Le),ge=a(Le,"SPAN",{});var Lr=t(ge);We=m(Lr,"ImageProcessingMixin"),Lr.forEach(s),Le.forEach(s),Ee=l(e),v=a(e,"DIV",{class:!0});var U=t(v);$(O.$$.fragment,U),Je=l(U),fe=a(U,"P",{});var Dr=t(fe);Re=m(Dr,`This is an image processor mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),Dr.forEach(s),Ye=l(U),E=a(U,"DIV",{class:!0});var ce=t(E);$(W.$$.fragment,ce),Ge=l(ce),J=a(ce,"P",{});var De=t(J);Ke=m(De,"Instantiate a type of "),te=a(De,"A",{href:!0});var Ar=t(te);Qe=m(Ar,"ImageProcessingMixin"),Ar.forEach(s),Xe=m(De," from an image processor."),De.forEach(s),Ze=l(ce),$(C.$$.fragment,ce),ce.forEach(s),er=l(U),N=a(U,"DIV",{class:!0});var Ae=t(N);$(R.$$.fragment,Ae),rr=l(Ae),M=a(Ae,"P",{});var le=t(M);sr=m(le,"Save an image processor object to the directory "),he=a(le,"CODE",{});var Cr=t(he);or=m(Cr,"save_directory"),Cr.forEach(s),ar=m(le,`, so that it can be re-loaded using the
`),ne=a(le,"A",{href:!0});var Nr=t(ne);tr=m(Nr,"from_pretrained()"),Nr.forEach(s),nr=m(le," class method."),le.forEach(s),Ae.forEach(s),U.forEach(s),Te=l(e),j=a(e,"H2",{class:!0});var Ce=t(j);q=a(Ce,"A",{id:!0,class:!0,href:!0});var qr=t(q);ue=a(qr,"SPAN",{});var Sr=t(ue);$(Y.$$.fragment,Sr),Sr.forEach(s),qr.forEach(s),ir=l(Ce),_e=a(Ce,"SPAN",{});var zr=t(_e);cr=m(zr,"BatchFeature"),zr.forEach(s),Ce.forEach(s),Fe=l(e),u=a(e,"DIV",{class:!0});var T=t(u);$(G.$$.fragment,T),lr=l(T),B=a(T,"P",{});var me=t(B);mr=m(me,"Holds the output of the "),ie=a(me,"A",{href:!0});var Hr=t(ie);dr=m(Hr,"pad()"),Hr.forEach(s),pr=m(me," and feature extractor specific "),ve=a(me,"CODE",{});var Ur=t(ve);gr=m(Ur,"__call__"),Ur.forEach(s),fr=m(me," methods."),me.forEach(s),hr=l(T),ye=a(T,"P",{});var Vr=t(ye);ur=m(Vr,"This class is derived from a python dictionary and can be used as a dictionary."),Vr.forEach(s),_r=l(T),S=a(T,"DIV",{class:!0});var Ne=t(S);$(K.$$.fragment,Ne),vr=l(Ne),$e=a(Ne,"P",{});var Or=t($e);yr=m(Or,"Convert the inner content to tensors."),Or.forEach(s),Ne.forEach(s),$r=l(T),z=a(T,"DIV",{class:!0});var qe=t(z);$(Q.$$.fragment,qe),br=l(qe),X=a(qe,"P",{});var Se=t(X);xr=m(Se,"Send all values to device by calling "),be=a(Se,"CODE",{});var Wr=t(be);Pr=m(Wr,"v.to(device)"),Wr.forEach(s),wr=m(Se," (PyTorch only)."),Se.forEach(s),qe.forEach(s),T.forEach(s),Me=l(e),L=a(e,"H2",{class:!0});var ze=t(L);H=a(ze,"A",{id:!0,class:!0,href:!0});var Jr=t(H);xe=a(Jr,"SPAN",{});var Rr=t(xe);$(Z.$$.fragment,Rr),Rr.forEach(s),Jr.forEach(s),Ir=l(ze),Pe=a(ze,"SPAN",{});var Yr=t(Pe);kr=m(Yr,"BaseImageProcessor"),Yr.forEach(s),ze.forEach(s),je=l(e),ee=a(e,"DIV",{class:!0});var Gr=t(ee);$(re.$$.fragment,Gr),Gr.forEach(s),this.h()},h(){n(f,"name","hf:doc:metadata"),n(f,"content",JSON.stringify(ns)),n(h,"id","image-processor"),n(h,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(h,"href","#image-processor"),n(_,"class","relative group"),n(A,"id","transformers.ImageProcessingMixin"),n(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(A,"href","#transformers.ImageProcessingMixin"),n(F,"class","relative group"),n(te,"href","/docs/transformers/main/en/internal/image_processing_utils#transformers.ImageProcessingMixin"),n(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(ne,"href","/docs/transformers/main/en/internal/image_processing_utils#transformers.ImageProcessingMixin.from_pretrained"),n(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(q,"id","transformers.BatchFeature"),n(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(q,"href","#transformers.BatchFeature"),n(j,"class","relative group"),n(ie,"href","/docs/transformers/main/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad"),n(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(u,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(H,"id","transformers.image_processing_utils.BaseImageProcessor"),n(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(H,"href","#transformers.image_processing_utils.BaseImageProcessor"),n(L,"class","relative group"),n(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,p){r(document.head,f),g(e,D,p),g(e,_,p),r(_,h),r(h,I),b(d,I,null),r(_,k),r(_,de),r(de,Ue),g(e,Ie,p),g(e,ae,p),r(ae,Ve),g(e,ke,p),g(e,F,p),r(F,A),r(A,pe),b(V,pe,null),r(F,Oe),r(F,ge),r(ge,We),g(e,Ee,p),g(e,v,p),b(O,v,null),r(v,Je),r(v,fe),r(fe,Re),r(v,Ye),r(v,E),b(W,E,null),r(E,Ge),r(E,J),r(J,Ke),r(J,te),r(te,Qe),r(J,Xe),r(E,Ze),b(C,E,null),r(v,er),r(v,N),b(R,N,null),r(N,rr),r(N,M),r(M,sr),r(M,he),r(he,or),r(M,ar),r(M,ne),r(ne,tr),r(M,nr),g(e,Te,p),g(e,j,p),r(j,q),r(q,ue),b(Y,ue,null),r(j,ir),r(j,_e),r(_e,cr),g(e,Fe,p),g(e,u,p),b(G,u,null),r(u,lr),r(u,B),r(B,mr),r(B,ie),r(ie,dr),r(B,pr),r(B,ve),r(ve,gr),r(B,fr),r(u,hr),r(u,ye),r(ye,ur),r(u,_r),r(u,S),b(K,S,null),r(S,vr),r(S,$e),r($e,yr),r(u,$r),r(u,z),b(Q,z,null),r(z,br),r(z,X),r(X,xr),r(X,be),r(be,Pr),r(X,wr),g(e,Me,p),g(e,L,p),r(L,H),r(H,xe),b(Z,xe,null),r(L,Ir),r(L,Pe),r(Pe,kr),g(e,je,p),g(e,ee,p),b(re,ee,null),Be=!0},p(e,[p]){const se={};p&2&&(se.$$scope={dirty:p,ctx:e}),C.$set(se)},i(e){Be||(x(d.$$.fragment,e),x(V.$$.fragment,e),x(O.$$.fragment,e),x(W.$$.fragment,e),x(C.$$.fragment,e),x(R.$$.fragment,e),x(Y.$$.fragment,e),x(G.$$.fragment,e),x(K.$$.fragment,e),x(Q.$$.fragment,e),x(Z.$$.fragment,e),x(re.$$.fragment,e),Be=!0)},o(e){P(d.$$.fragment,e),P(V.$$.fragment,e),P(O.$$.fragment,e),P(W.$$.fragment,e),P(C.$$.fragment,e),P(R.$$.fragment,e),P(Y.$$.fragment,e),P(G.$$.fragment,e),P(K.$$.fragment,e),P(Q.$$.fragment,e),P(Z.$$.fragment,e),P(re.$$.fragment,e),Be=!1},d(e){s(f),e&&s(D),e&&s(_),w(d),e&&s(Ie),e&&s(ae),e&&s(ke),e&&s(F),w(V),e&&s(Ee),e&&s(v),w(O),w(W),w(C),w(R),e&&s(Te),e&&s(j),w(Y),e&&s(Fe),e&&s(u),w(G),w(K),w(Q),e&&s(Me),e&&s(L),w(Z),e&&s(je),e&&s(ee),w(re)}}}const ns={local:"image-processor",sections:[{local:"transformers.ImageProcessingMixin",title:"ImageProcessingMixin"},{local:"transformers.BatchFeature",title:"BatchFeature"},{local:"transformers.image_processing_utils.BaseImageProcessor",title:"BaseImageProcessor"}],title:"Image Processor"};function is(we){return es(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class gs extends Kr{constructor(f){super();Qr(this,f,is,ts,Xr,{})}}export{gs as default,ns as metadata};
