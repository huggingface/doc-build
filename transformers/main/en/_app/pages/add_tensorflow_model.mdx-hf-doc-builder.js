import{S as Fw,i as $w,s as Pw,e as l,k as d,w as $,t as r,M as Iw,c as s,d as o,m as c,a as n,x as P,h as a,b as f,G as e,g as h,y as I,q as O,o as L,B,v as Ow}from"../chunks/vendor-hf-doc-builder.js";import{T as kw}from"../chunks/Tip-hf-doc-builder.js";import{I as dt}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as Zt}from"../chunks/CodeBlock-hf-doc-builder.js";function Lw(eo){let m,z,v,g,q,_,p,G,N,k,S,V,ae,ct,X,Ce,le,Z,De,se;return{c(){m=l("p"),z=r("Are you unsure whether the model you wish to use already has a corresponding TensorFlow architecture?"),v=d(),g=l("p"),q=r("\xA0"),_=d(),p=l("p"),G=r("Check the "),N=l("code"),k=r("model_type"),S=r(" field of the "),V=l("code"),ae=r("config.json"),ct=r(` of your model of choice
(`),X=l("a"),Ce=r("example"),le=r(`). If the corresponding model folder in
\u{1F917} Transformers has a file whose name starts with \u201Cmodeling_tf\u201D, it means that it has a corresponding TensorFlow
architecture (`),Z=l("a"),De=r("example"),se=r(")."),this.h()},l(y){m=s(y,"P",{});var U=n(m);z=a(U,"Are you unsure whether the model you wish to use already has a corresponding TensorFlow architecture?"),U.forEach(o),v=c(y),g=s(y,"P",{});var ee=n(g);q=a(ee,"\xA0"),ee.forEach(o),_=c(y),p=s(y,"P",{});var A=n(p);G=a(A,"Check the "),N=s(A,"CODE",{});var ve=n(N);k=a(ve,"model_type"),ve.forEach(o),S=a(A," field of the "),V=s(A,"CODE",{});var to=n(V);ae=a(to,"config.json"),to.forEach(o),ct=a(A,` of your model of choice
(`),X=s(A,"A",{href:!0,rel:!0});var oo=n(X);Ce=a(oo,"example"),oo.forEach(o),le=a(A,`). If the corresponding model folder in
\u{1F917} Transformers has a file whose name starts with \u201Cmodeling_tf\u201D, it means that it has a corresponding TensorFlow
architecture (`),Z=s(A,"A",{href:!0,rel:!0});var xe=n(Z);De=a(xe,"example"),xe.forEach(o),se=a(A,")."),A.forEach(o),this.h()},h(){f(X,"href","https://huggingface.co/bert-base-uncased/blob/main/config.json#L14"),f(X,"rel","nofollow"),f(Z,"href","https://github.com/huggingface/transformers/tree/main/src/transformers/models/bert"),f(Z,"rel","nofollow")},m(y,U){h(y,m,U),e(m,z),h(y,v,U),h(y,g,U),e(g,q),h(y,_,U),h(y,p,U),e(p,G),e(p,N),e(N,k),e(p,S),e(p,V),e(V,ae),e(p,ct),e(p,X),e(X,Ce),e(p,le),e(p,Z),e(Z,De),e(p,se)},d(y){y&&o(m),y&&o(v),y&&o(g),y&&o(_),y&&o(p)}}}function Bw(eo){let m,z,v,g,q,_,p,G;return{c(){m=l("p"),z=r(`Before starting the work on a TensorFlow model architecture, double-check that there is no ongoing effort to do so.
You can search for `),v=l("code"),g=r("BrandNewBert"),q=r(` on the
`),_=l("a"),p=r("pull request GitHub page"),G=r(` to confirm that there is no
TensorFlow-related pull request.`),this.h()},l(N){m=s(N,"P",{});var k=n(m);z=a(k,`Before starting the work on a TensorFlow model architecture, double-check that there is no ongoing effort to do so.
You can search for `),v=s(k,"CODE",{});var S=n(v);g=a(S,"BrandNewBert"),S.forEach(o),q=a(k,` on the
`),_=s(k,"A",{href:!0,rel:!0});var V=n(_);p=a(V,"pull request GitHub page"),V.forEach(o),G=a(k,` to confirm that there is no
TensorFlow-related pull request.`),k.forEach(o),this.h()},h(){f(_,"href","https://github.com/huggingface/transformers/pulls?q=is%3Apr"),f(_,"rel","nofollow")},m(N,k){h(N,m,k),e(m,z),e(m,v),e(v,g),e(m,q),e(m,_),e(_,p),e(m,G)},d(N){N&&o(m)}}}function Aw(eo){let m,z,v,g,q,_,p,G,N,k,S,V,ae,ct,X,Ce,le,Z,De,se,y,U,ee,A,ve,to,oo,xe,ft,ln,pl,ro,sn,wl,Ne,yl,_e,Se,jo,mt,nn,Yo,hn,vl,He,dn,ao,cn,fn,_l,ne,zo,mn,un,Vo,pn,wn,Ko,yn,bl,lo,vn,gl,E,Jo,_n,bn,Qo,gn,En,Xo,Tn,kn,Zo,Fn,$n,er,Pn,In,tr,On,Ln,or,Bn,El,be,Re,rr,ut,An,ar,Cn,Tl,so,lr,Dn,kl,Me,xn,no,Nn,Sn,Fl,ie,Hn,sr,Rn,Mn,io,qn,Gn,$l,qe,Pl,ho,nr,Un,Il,co,Wn,Ol,Ge,ir,pt,jn,wt,Yn,zn,Vn,hr,yt,Kn,dr,Jn,Qn,Ll,vt,Bl,_t,cr,Xn,Al,bt,Cl,gt,fr,Zn,ei,Dl,Et,mr,ti,xl,Tt,Nl,kt,ur,oi,Sl,Ft,Hl,ge,pr,te,ri,wr,ai,li,yr,si,ni,vr,ii,hi,di,_r,br,ci,Rl,$t,Ml,Ee,gr,Er,fi,mi,Tr,kr,ui,ql,Ue,pi,Fr,wi,yi,Gl,fo,$r,vi,Ul,he,_i,Pr,bi,gi,mo,Ei,Ti,Wl,uo,ki,jl,We,Fi,Pt,$i,Pi,Yl,Te,je,Ir,It,Ii,Or,Oi,zl,T,Li,Lr,Bi,Ai,Br,Ci,Di,Ar,xi,Ni,Cr,Si,Hi,Dr,Ri,Mi,xr,qi,Gi,Vl,po,Ui,Kl,u,oe,Wi,Nr,ji,Yi,Sr,zi,Vi,Hr,Ki,Ji,Qi,F,Xi,Rr,Zi,eh,Mr,th,oh,qr,rh,ah,Gr,lh,sh,Ot,nh,ih,Lt,hh,dh,ch,Ur,fh,mh,Wr,uh,ph,W,wh,jr,yh,vh,Bt,_h,bh,Yr,gh,Eh,At,Th,kh,Fh,K,$h,zr,Ph,Ih,Vr,Oh,Lh,Ct,Bh,Ah,Dt,Ch,Dh,xt,xh,Kr,Nh,Sh,Hh,j,Rh,Jr,Mh,qh,Qr,Gh,Uh,Xr,Wh,jh,Zr,Yh,zh,Vh,Y,Kh,ea,Jh,Qh,ta,Xh,Zh,Nt,ed,td,oa,od,rd,ad,re,ld,ra,sd,nd,aa,id,hd,St,dd,cd,fd,la,md,Jl,Ye,ud,Ht,pd,wd,Ql,b,ze,yd,sa,vd,_d,na,bd,gd,Ve,Ed,ia,Td,kd,ha,Fd,$d,wo,Pd,da,Id,Od,Ke,Ld,ca,Bd,Ad,fa,Cd,Dd,yo,xd,ma,Nd,Sd,Je,Hd,ua,Rd,Md,pa,qd,Gd,Qe,Ud,wa,Wd,jd,ya,Yd,zd,Xe,Vd,va,Kd,Jd,_a,Qd,Xl,vo,Xd,Zl,C,Rt,Zd,ba,ec,tc,oc,Mt,rc,ga,ac,lc,sc,ke,Ea,nc,ic,Ta,hc,dc,ka,cc,fc,qt,Fa,mc,uc,$a,pc,wc,_o,yc,Pa,vc,_c,Ia,bc,es,Fe,Ze,Oa,Gt,gc,La,Ec,ts,D,Tc,Ba,kc,Fc,Aa,$c,Pc,Ca,Ic,Oc,Da,Lc,Bc,xa,Ac,Cc,os,bo,Dc,rs,Ut,as,et,xc,Na,Nc,Sc,ls,go,Hc,ss,$e,tt,Sa,Wt,Rc,Ha,Mc,ns,Eo,Ra,qc,is,ot,Gc,Ma,Uc,Wc,hs,de,jc,qa,Yc,zc,Ga,Vc,Kc,ds,ce,Jc,Ua,Qc,Xc,Wa,Zc,ef,cs,To,tf,fs,jt,ms,ko,of,us,Fo,ja,rf,ps,$o,af,ws,Po,lf,ys,rt,Yt,sf,zt,nf,hf,df,Ya,cf,vs,Pe,at,za,Vt,ff,Va,mf,_s,Io,uf,bs,Oo,pf,gs,fe,Ie,wf,Ka,yf,vf,Kt,_f,bf,gf,Oe,Ef,Ja,Tf,kf,Qa,Ff,$f,Pf,Le,If,Xa,Of,Lf,Za,Bf,Af,Es,Lo,Cf,Ts,Be,lt,el,Jt,Df,tl,xf,ks,Bo,Nf,Fs,st,Sf,ol,Hf,Rf,$s,Ao,Mf,Ps,me,Qt,qf,rl,Gf,Uf,Wf,al,jf,Yf,ll,zf,Is,ue,Vf,sl,Kf,Jf,nl,Qf,Xf,Os;return _=new dt({}),Ne=new kw({props:{$$slots:{default:[Lw]},$$scope:{ctx:eo}}}),mt=new dt({}),ut=new dt({}),qe=new kw({props:{$$slots:{default:[Bw]},$$scope:{ctx:eo}}}),vt=new Zt({props:{code:`git clone https://github.com/[your Github handle]/transformers.git
cd transformers
git remote add upstream https://github.com/huggingface/transformers.git`,highlighted:`git <span class="hljs-built_in">clone</span> https://github.com/[your Github handle]/transformers.git
<span class="hljs-built_in">cd</span> transformers
git remote add upstream https://github.com/huggingface/transformers.git`}}),bt=new Zt({props:{code:`python -m venv .env
source .env/bin/activate
pip install -e ".[dev]"`,highlighted:`python -m venv .<span class="hljs-built_in">env</span>
<span class="hljs-built_in">source</span> .<span class="hljs-built_in">env</span>/bin/activate
pip install -e <span class="hljs-string">&quot;.[dev]&quot;</span>`}}),Tt=new Zt({props:{code:"git checkout -b add_tf_brand_new_bert",highlighted:"git checkout -b add_tf_brand_new_bert"}}),Ft=new Zt({props:{code:`git fetch upstream
git rebase upstream/main`,highlighted:`git fetch upstream
git rebase upstream/main`}}),$t=new Zt({props:{code:`git add .
git commit -m "initial commit"
git push -u origin add_tf_brand_new_bert`,highlighted:`git add .
git commit -m <span class="hljs-string">&quot;initial commit&quot;</span>
git push -u origin add_tf_brand_new_bert`}}),It=new dt({}),Gt=new dt({}),Ut=new Zt({props:{code:`NVIDIA_TF32_OVERRIDE=0 RUN_SLOW=1 RUN_PT_TF_CROSS_TESTS=1 \\
py.test -vv tests/models/brand_new_bert/test_modeling_tf_brand_new_bert.py`,highlighted:`NVIDIA_TF32_OVERRIDE=0 RUN_SLOW=1 RUN_PT_TF_CROSS_TESTS=1 \\
py.test -vv tests/models/brand_new_bert/test_modeling_tf_brand_new_bert.py`}}),Wt=new dt({}),jt=new Zt({props:{code:`NVIDIA_TF32_OVERRIDE=0 RUN_SLOW=1 RUN_PT_TF_CROSS_TESTS=1 \\
py.test -vv tests/models/brand_new_bert/test_modeling_tf_brand_new_bert.py`,highlighted:`NVIDIA_TF32_OVERRIDE=0 RUN_SLOW=1 RUN_PT_TF_CROSS_TESTS=1 \\
py.test -vv tests/models/brand_new_bert/test_modeling_tf_brand_new_bert.py`}}),Vt=new dt({}),Jt=new dt({}),{c(){m=l("meta"),z=d(),v=l("h1"),g=l("a"),q=l("span"),$(_.$$.fragment),p=d(),G=l("span"),N=r("How to convert a \u{1F917} Transformers model to TensorFlow?"),k=d(),S=l("p"),V=r(`Having multiple frameworks available to use with \u{1F917} Transformers gives you flexibility to play their strengths when
designing your application, but it implies that compatibility must be added on a per-model basis. The good news is that
adding TensorFlow compatibility to an existing model is simpler than `),ae=l("a"),ct=r("adding a new model from scratch"),X=r(`!
Whether you wish to have a deeper understanding of large TensorFlow models, make a major open-source contribution, or
enable TensorFlow for your model of choice, this guide is for you.`),Ce=d(),le=l("p"),Z=r(`This guide empowers you, a member of our community, to contribute TensorFlow model weights and/or
architectures to be used in \u{1F917} Transformers, with minimal supervision from the Hugging Face team. Writing a new model
is no small feat, but hopefully this guide will make it less of a rollercoaster \u{1F3A2} and more of a walk in the park \u{1F6B6}.
Harnessing our collective experiences is absolutely critical to make this process increasingly easier, and thus we
highly encourage that you suggest improvements to this guide!`),De=d(),se=l("p"),y=r("Before you dive deeper, it is recommended that you check the following resources if you\u2019re new to \u{1F917} Transformers:"),U=d(),ee=l("ul"),A=l("li"),ve=l("a"),to=r("General overview of \u{1F917} Transformers"),oo=d(),xe=l("li"),ft=l("a"),ln=r("Hugging Face\u2019s TensorFlow Philosophy"),pl=d(),ro=l("p"),sn=r(`In the remainder of this guide, you will learn what\u2019s needed to add a new TensorFlow model architecture, the
procedure to convert PyTorch into TensorFlow model weights, and how to efficiently debug mismatches across ML
frameworks. Let\u2019s get started!`),wl=d(),$(Ne.$$.fragment),yl=d(),_e=l("h2"),Se=l("a"),jo=l("span"),$(mt.$$.fragment),nn=d(),Yo=l("span"),hn=r("Step-by-step guide to add TensorFlow model architecture code"),vl=d(),He=l("p"),dn=r(`There are many ways to design a large model architecture, and multiple ways of implementing said design. However,
you might recall from our `),ao=l("a"),cn=r("general overview of \u{1F917} Transformers"),fn=r(`
that we are an opinionated bunch - the ease of use of \u{1F917} Transformers relies on consistent design choices. From
experience, we can tell you a few important things about adding TensorFlow models:`),_l=d(),ne=l("ul"),zo=l("li"),mn=r(`Don\u2019t reinvent the wheel! More often that not, there are at least two reference implementations you should check: the
PyTorch equivalent of the model you are implementing and other TensorFlow models for the same class of problems.`),un=d(),Vo=l("li"),pn=r(`Great model implementations survive the test of time. This doesn\u2019t happen because the code is pretty, but rather
because the code is clear, easy to debug and build upon. If you make the life of the maintainers easy with your
TensorFlow implementation, by replicating the same patterns as in other TensorFlow models and minimizing the mismatch
to the PyTorch implementation, you ensure your contribution will be long lived.`),wn=d(),Ko=l("li"),yn=r(`Ask for help when you\u2019re stuck! The \u{1F917} Transformers team is here to help, and we\u2019ve probably found solutions to the same
problems you\u2019re facing.`),bl=d(),lo=l("p"),vn=r("Here\u2019s an overview of the steps needed to add a TensorFlow model architecture:"),gl=d(),E=l("ol"),Jo=l("li"),_n=r("Select the model you wish to convert"),bn=d(),Qo=l("li"),gn=r("Prepare transformers dev environment"),En=d(),Xo=l("li"),Tn=r("(Optional) Understand theoretical aspects and the existing implementation"),kn=d(),Zo=l("li"),Fn=r("Implement the model architecture"),$n=d(),er=l("li"),Pn=r("Implement model tests"),In=d(),tr=l("li"),On=r("Submit the pull request"),Ln=d(),or=l("li"),Bn=r("(Optional) Build demos and share with the world"),El=d(),be=l("h3"),Re=l("a"),rr=l("span"),$(ut.$$.fragment),An=d(),ar=l("span"),Cn=r("1.-3. Prepare your model contribution"),Tl=d(),so=l("p"),lr=l("strong"),Dn=r("1. Select the model you wish to convert"),kl=d(),Me=l("p"),xn=r(`Let\u2019s start off with the basics: the first thing you need to know is the architecture you want to convert. If you
don\u2019t have your eyes set on a specific architecture, asking the \u{1F917} Transformers team for suggestions is a great way to
maximize your impact - we will guide you towards the most prominent architectures that are missing on the TensorFlow
side. If the specific model you want to use with TensorFlow already has a TensorFlow architecture implementation in
\u{1F917} Transformers but is lacking weights, feel free to jump straight into the
`),no=l("a"),Nn=r("weight conversion section"),Sn=r(`
of this page.`),Fl=d(),ie=l("p"),Hn=r(`For simplicity, the remainder of this guide assumes you\u2019ve decided to contribute with the TensorFlow version of
`),sr=l("em"),Rn=r("BrandNewBert"),Mn=r(" (the same example as in the "),io=l("a"),qn=r("guide"),Gn=r(" to add a new model from scratch)."),$l=d(),$(qe.$$.fragment),Pl=d(),ho=l("p"),nr=l("strong"),Un=r("2. Prepare transformers dev environment"),Il=d(),co=l("p"),Wn=r(`Having selected the model architecture, open an draft PR to signal your intention to work on it. Follow the
instructions below to set up your environment and open a draft PR.`),Ol=d(),Ge=l("ol"),ir=l("li"),pt=l("p"),jn=r("Fork the "),wt=l("a"),Yn=r("repository"),zn=r(` by clicking on the \u2018Fork\u2019 button on the
repository\u2019s page. This creates a copy of the code under your GitHub user account.`),Vn=d(),hr=l("li"),yt=l("p"),Kn=r("Clone your "),dr=l("code"),Jn=r("transformers"),Qn=r(" fork to your local disk, and add the base repository as a remote:"),Ll=d(),$(vt.$$.fragment),Bl=d(),_t=l("ol"),cr=l("li"),Xn=r("Set up a development environment, for instance by running the following command:"),Al=d(),$(bt.$$.fragment),Cl=d(),gt=l("p"),fr=l("strong"),Zn=r("Note:"),ei=r(" You don\u2019t need to have CUDA installed. Making the new model work on CPU is sufficient."),Dl=d(),Et=l("ol"),mr=l("li"),ti=r("Create a branch with a descriptive name from your main branch"),xl=d(),$(Tt.$$.fragment),Nl=d(),kt=l("ol"),ur=l("li"),oi=r("Fetch and rebase to current main"),Sl=d(),$(Ft.$$.fragment),Hl=d(),ge=l("ol"),pr=l("li"),te=l("p"),ri=r("Add an empty "),wr=l("code"),ai=r(".py"),li=r(" file in "),yr=l("code"),si=r("transformers/src/models/brandnewbert/"),ni=r(" named "),vr=l("code"),ii=r("modeling_tf_brandnewbert.py"),hi=r(`. This will
be your TensorFlow model file.`),di=d(),_r=l("li"),br=l("p"),ci=r("Push the changes to your account using:"),Rl=d(),$($t.$$.fragment),Ml=d(),Ee=l("ol"),gr=l("li"),Er=l("p"),fi=r(`Once you are satisfied, go to the webpage of your fork on GitHub. Click on \u201CPull request\u201D. Make sure to add the
GitHub handle of some members of the Hugging Face team as reviewers, so that the Hugging Face team gets notified for
future changes.`),mi=d(),Tr=l("li"),kr=l("p"),ui=r("Change the PR into a draft by clicking on \u201CConvert to draft\u201D on the right of the GitHub pull request web page."),ql=d(),Ue=l("p"),pi=r("Now you have set up a development environment to port "),Fr=l("em"),wi=r("BrandNewBert"),yi=r(" to TensorFlow in \u{1F917} Transformers."),Gl=d(),fo=l("p"),$r=l("strong"),vi=r("3. (Optional) Understand theoretical aspects and the existing implementation"),Ul=d(),he=l("p"),_i=r("You should take some time to read "),Pr=l("em"),bi=r("BrandNewBert\u2019s"),gi=r(` paper, if such descriptive work exists. There might be large
sections of the paper that are difficult to understand. If this is the case, this is fine - don\u2019t worry! The goal is
not to get a deep theoretical understanding of the paper, but to extract the necessary information required to
effectively re-implement the model in \u{1F917} Transformers using TensorFlow. That being said, you don\u2019t have to spend too
much time on the theoretical aspects, but rather focus on the practical ones, namely the existing model documentation
page (e.g. `),mo=l("a"),Ei=r("model docs for BERT"),Ti=r(")."),Wl=d(),uo=l("p"),ki=r(`After you\u2019ve grasped the basics of the models you are about to implement, it\u2019s important to understand the existing
implementation. This is a great chance to confirm that a working implementation matches your expectations for the
model, as well as to foresee technical challenges on the TensorFlow side.`),jl=d(),We=l("p"),Fi=r(`It\u2019s perfectly natural that you feel overwhelmed with the amount of information that you\u2019ve just absorbed. It is
definitely not a requirement that you understand all facets of the model at this stage. Nevertheless, we highly
encourage you to clear any pressing questions in our `),Pt=l("a"),$i=r("forum"),Pi=r("."),Yl=d(),Te=l("h3"),je=l("a"),Ir=l("span"),$(It.$$.fragment),Ii=d(),Or=l("span"),Oi=r("4. Model implementation"),zl=d(),T=l("p"),Li=r(`Now it\u2019s time to finally start coding. Our suggested starting point is the PyTorch file itself: copy the contents of
`),Lr=l("code"),Bi=r("modeling_brand_new_bert.py"),Ai=r(" inside "),Br=l("code"),Ci=r("src/transformers/models/brand_new_bert/"),Di=r(` into
`),Ar=l("code"),xi=r("modeling_tf_brand_new_bert.py"),Ni=r(`. The goal of this section is to modify the file and update the import structure of
\u{1F917} Transformers such that you can import `),Cr=l("code"),Si=r("TFBrandNewBert"),Hi=r(` and
`),Dr=l("code"),Ri=r("TFBrandNewBert.from_pretrained(model_repo, from_pt=True)"),Mi=r(" successfully loads a working TensorFlow "),xr=l("em"),qi=r("BrandNewBert"),Gi=r(" model."),Vl=d(),po=l("p"),Ui=r(`Sadly, there is no prescription to convert a PyTorch model into TensorFlow. You can, however, follow our selection of
tips to make the process as smooth as possible:`),Kl=d(),u=l("ul"),oe=l("li"),Wi=r("Prepend "),Nr=l("code"),ji=r("TF"),Yi=r(" to the name of all classes (e.g. "),Sr=l("code"),zi=r("BrandNewBert"),Vi=r(" becomes "),Hr=l("code"),Ki=r("TFBrandNewBert"),Ji=r(")."),Qi=d(),F=l("li"),Xi=r("Most PyTorch operations have a direct TensorFlow replacement. For example, "),Rr=l("code"),Zi=r("torch.nn.Linear"),eh=r(` corresponds to
`),Mr=l("code"),th=r("tf.keras.layers.Dense"),oh=r(", "),qr=l("code"),rh=r("torch.nn.Dropout"),ah=r(" corresponds to "),Gr=l("code"),lh=r("tf.keras.layers.Dropout"),sh=r(`, etc. If you\u2019re not sure
about a specific operation, you can use the `),Ot=l("a"),nh=r("TensorFlow documentation"),ih=r(`
or the `),Lt=l("a"),hh=r("PyTorch documentation"),dh=r("."),ch=d(),Ur=l("li"),fh=r(`Look for patterns in the \u{1F917} Transformers codebase. If you come across a certain operation that doesn\u2019t have a direct
replacement, the odds are that someone else already had the same problem.`),mh=d(),Wr=l("li"),uh=r(`By default, keep the same variable names and structure as in PyTorch. This will make it easier to debug, track
issues, and add fixes down the line.`),ph=d(),W=l("li"),wh=r(`Some layers have different default values in each framework. A notable example is the batch normalization layer\u2019s
epsilon (`),jr=l("code"),yh=r("1e-5"),vh=r(" in "),Bt=l("a"),_h=r("PyTorch"),bh=r(`
and `),Yr=l("code"),gh=r("1e-3"),Eh=r(" in "),At=l("a"),Th=r("TensorFlow"),kh=r(`).
Double-check the documentation!`),Fh=d(),K=l("li"),$h=r("PyTorch\u2019s "),zr=l("code"),Ph=r("nn.Parameter"),Ih=r(" variables typically need to be initialized within TF Layer\u2019s "),Vr=l("code"),Oh=r("build()"),Lh=r(`. See the following
example: `),Ct=l("a"),Bh=r("PyTorch"),Ah=r(` /
`),Dt=l("a"),Ch=r("TensorFlow"),Dh=d(),xt=l("li"),xh=r("If the PyTorch model has a "),Kr=l("code"),Nh=r("#copied from ..."),Sh=r(` on top of a function, the odds are that your TensorFlow model can also
borrow that function from the architecture it was copied from, assuming it has a TensorFlow architecture.`),Hh=d(),j=l("li"),Rh=r("Assigning the "),Jr=l("code"),Mh=r("name"),qh=r(" attribute correctly in TensorFlow functions is critical to do the "),Qr=l("code"),Gh=r("from_pt=True"),Uh=r(` weight
cross-loading. `),Xr=l("code"),Wh=r("name"),jh=r(" is almost always the name of the corresponding variable in the PyTorch code. If "),Zr=l("code"),Yh=r("name"),zh=r(` is not
properly set, you will see it in the error message when loading the model weights.`),Vh=d(),Y=l("li"),Kh=r("The logic of the base model class, "),ea=l("code"),Jh=r("BrandNewBertModel"),Qh=r(", will actually reside in "),ta=l("code"),Xh=r("TFBrandNewBertMainLayer"),Zh=r(`, a Keras
layer subclass (`),Nt=l("a"),ed=r("example"),td=r(`).
`),oa=l("code"),od=r("TFBrandNewBertModel"),rd=r(" will simply be a wrapper around this layer."),ad=d(),re=l("li"),ld=r("Keras models need to be built in order to load pretrained weights. For that reason, "),ra=l("code"),sd=r("TFBrandNewBertPreTrainedModel"),nd=r(`
will need to hold an example of inputs to the model, the `),aa=l("code"),id=r("dummy_inputs"),hd=r(`
(`),St=l("a"),dd=r("example"),cd=r(")."),fd=d(),la=l("li"),md=r("If you get stuck, ask for help - we\u2019re here to help you! \u{1F917}"),Jl=d(),Ye=l("p"),ud=r(`In addition to the model file itself, you will also need to add the pointers to the model classes and related
documentation pages. You can complete this part entirely following the patterns in other PRs
(`),Ht=l("a"),pd=r("example"),wd=r(`). Here\u2019s a list of the needed manual
changes:`),Ql=d(),b=l("ul"),ze=l("li"),yd=r("Include all public classes of "),sa=l("em"),vd=r("BrandNewBert"),_d=r(" in "),na=l("code"),bd=r("src/transformers/__init__.py"),gd=d(),Ve=l("li"),Ed=r("Add "),ia=l("em"),Td=r("BrandNewBert"),kd=r(" classes to the corresponding Auto classes in "),ha=l("code"),Fd=r("src/transformers/models/auto/modeling_tf_auto.py"),$d=d(),wo=l("li"),Pd=r("Include the modeling file in the documentation test file list in "),da=l("code"),Id=r("utils/documentation_tests.txt"),Od=d(),Ke=l("li"),Ld=r("Add the lazy loading classes related to "),ca=l("em"),Bd=r("BrandNewBert"),Ad=r(" in "),fa=l("code"),Cd=r("src/transformers/utils/dummy_tf_objects.py"),Dd=d(),yo=l("li"),xd=r("Update the import structures for the public classes in "),ma=l("code"),Nd=r("src/transformers/models/brand_new_bert/__init__.py"),Sd=d(),Je=l("li"),Hd=r("Add the documentation pointers to the public methods of "),ua=l("em"),Rd=r("BrandNewBert"),Md=r(" in "),pa=l("code"),qd=r("docs/source/en/model_doc/brand_new_bert.mdx"),Gd=d(),Qe=l("li"),Ud=r("Add yourself to the list of contributors to "),wa=l("em"),Wd=r("BrandNewBert"),jd=r(" in "),ya=l("code"),Yd=r("docs/source/en/model_doc/brand_new_bert.mdx"),zd=d(),Xe=l("li"),Vd=r("Finally, add a green tick \u2705 to the TensorFlow column of "),va=l("em"),Kd=r("BrandNewBert"),Jd=r(" in "),_a=l("code"),Qd=r("docs/source/en/index.mdx"),Xl=d(),vo=l("p"),Xd=r(`When you\u2019re happy with your implementation, run the following checklist to confirm that your model architecture is
ready:`),Zl=d(),C=l("ol"),Rt=l("li"),Zd=r("All layers that behave differently at train time (e.g. Dropout) are called with a "),ba=l("code"),ec=r("training"),tc=r(` argument, which is
propagated all the way from the top-level classes`),oc=d(),Mt=l("li"),rc=r("You have used "),ga=l("code"),ac=r("#copied from ..."),lc=r(" whenever possible"),sc=d(),ke=l("li"),Ea=l("code"),nc=r("TFBrandNewBertMainLayer"),ic=r(" and all classes that use it have their "),Ta=l("code"),hc=r("call"),dc=r(" function decorated with "),ka=l("code"),cc=r("@unpack_inputs"),fc=d(),qt=l("li"),Fa=l("code"),mc=r("TFBrandNewBertMainLayer"),uc=r(" is decorated with "),$a=l("code"),pc=r("@keras_serializable"),wc=d(),_o=l("li"),yc=r("A TensorFlow model can be loaded from PyTorch weights using "),Pa=l("code"),vc=r("TFBrandNewBert.from_pretrained(model_repo, from_pt=True)"),_c=d(),Ia=l("li"),bc=r("You can call the TensorFlow model using the expected input format"),es=d(),Fe=l("h3"),Ze=l("a"),Oa=l("span"),$(Gt.$$.fragment),gc=d(),La=l("span"),Ec=r("5. Add model tests"),ts=d(),D=l("p"),Tc=r(`Hurray, you\u2019ve implemented a TensorFlow model! Now it\u2019s time to add tests to make sure that your model behaves as
expected. As in the previous section, we suggest you start by copying the `),Ba=l("code"),kc=r("test_modeling_brand_new_bert.py"),Fc=r(` file in
`),Aa=l("code"),$c=r("tests/models/brand_new_bert/"),Pc=r(" into "),Ca=l("code"),Ic=r("test_modeling_tf_brand_new_bert.py"),Oc=r(`, and continue by making the necessary
TensorFlow replacements. For now, in all `),Da=l("code"),Lc=r(".from_pretrained()"),Bc=r(" calls, you should use the "),xa=l("code"),Ac=r("from_pt=True"),Cc=r(` flag to load
the existing PyTorch weights.`),os=d(),bo=l("p"),Dc=r("After you\u2019re done, it\u2019s time for the moment of truth: run the tests! \u{1F62C}"),rs=d(),$(Ut.$$.fragment),as=d(),et=l("p"),xc=r(`The most likely outcome is that you\u2019ll see a bunch of errors. Don\u2019t worry, this is expected! Debugging ML models is
notoriously hard, and the key ingredient to success is patience (and `),Na=l("code"),Nc=r("breakpoint()"),Sc=r(`). In our experience, the hardest
problems arise from subtle mismatches between ML frameworks, for which we have a few pointers at the end of this guide.
In other cases, a general test might not be directly applicable to your model, in which case we suggest an override
at the model test class level. Regardless of the issue, don\u2019t hesitate to ask for help in your draft pull request if
you\u2019re stuck.`),ls=d(),go=l("p"),Hc=r("When all tests pass, congratulations, your model is nearly ready to be added to the \u{1F917} Transformers library! \u{1F389}"),ss=d(),$e=l("h3"),tt=l("a"),Sa=l("span"),$(Wt.$$.fragment),Rc=d(),Ha=l("span"),Mc=r("6.-7. Ensure everyone can use your model"),ns=d(),Eo=l("p"),Ra=l("strong"),qc=r("6. Submit the pull request"),is=d(),ot=l("p"),Gc=r(`Once you\u2019re done with the implementation and the tests, it\u2019s time to submit a pull request. Before pushing your code,
run our code formatting utility, `),Ma=l("code"),Uc=r("make fixup"),Wc=r(` \u{1FA84}. This will automatically fix any formatting issues, which would cause
our automatic checks to fail.`),hs=d(),de=l("p"),jc=r(`It\u2019s now time to convert your draft pull request into a real pull request. To do so, click on the \u201CReady for
review\u201D button and add Joao (`),qa=l("code"),Yc=r("@gante"),zc=r(") and Matt ("),Ga=l("code"),Vc=r("@Rocketknight1"),Kc=r(`) as reviewers. A model pull request will need
at least 3 reviewers, but they will take care of finding appropriate additional reviewers for your model.`),ds=d(),ce=l("p"),Jc=r("After all reviewers are happy with the state of your PR, the final action point is to remove the "),Ua=l("code"),Qc=r("from_pt=True"),Xc=r(` flag in
`),Wa=l("code"),Zc=r(".from_pretrained()"),ef=r(` calls. Since there are no TensorFlow weights, you will have to add them! Check the section
below for instructions on how to do it.`),cs=d(),To=l("p"),tf=r(`Finally, when the TensorFlow weights get merged, you have at least 3 reviewer approvals, and all CI checks are
green, double-check the tests locally one last time`),fs=d(),$(jt.$$.fragment),ms=d(),ko=l("p"),of=r("and we will merge your PR! Congratulations on the milestone \u{1F389}"),us=d(),Fo=l("p"),ja=l("strong"),rf=r("7. (Optional) Build demos and share with the world"),ps=d(),$o=l("p"),af=r(`One of the hardest parts about open-source is discovery. How can the other users learn about the existence of your
fabulous TensorFlow contribution? With proper communication, of course! \u{1F4E3}`),ws=d(),Po=l("p"),lf=r("There are two main ways to share your model with the community:"),ys=d(),rt=l("ul"),Yt=l("li"),sf=r(`Build demos. These include Gradio demos, notebooks, and other fun ways to show off your model. We highly
encourage you to add a notebook to our `),zt=l("a"),nf=r("community-driven demos"),hf=r("."),df=d(),Ya=l("li"),cf=r(`Share stories on social media like Twitter and LinkedIn. You should be proud of your work and share
your achievement with the community - your model can now be used by thousands of engineers and researchers around
the world \u{1F30D}! We will be happy to retweet your posts and help you share your work with the community.`),vs=d(),Pe=l("h2"),at=l("a"),za=l("span"),$(Vt.$$.fragment),ff=d(),Va=l("span"),mf=r("Adding TensorFlow weights to \u{1F917} Hub"),_s=d(),Io=l("p"),uf=r(`Assuming that the TensorFlow model architecture is available in \u{1F917} Transformers, converting PyTorch weights into
TensorFlow weights is a breeze!`),bs=d(),Oo=l("p"),pf=r("Here\u2019s how to do it:"),gs=d(),fe=l("ol"),Ie=l("li"),wf=r(`Make sure you are logged into your Hugging Face account in your terminal. You can log in using the command
`),Ka=l("code"),yf=r("huggingface-cli login"),vf=r(" (you can find your access tokens "),Kt=l("a"),_f=r("here"),bf=r(")"),gf=d(),Oe=l("li"),Ef=r("Run "),Ja=l("code"),Tf=r("transformers-cli pt-to-tf --model-name foo/bar"),kf=r(", where "),Qa=l("code"),Ff=r("foo/bar"),$f=r(` is the name of the model repository
containing the PyTorch weights you want to convert`),Pf=d(),Le=l("li"),If=r("Tag "),Xa=l("code"),Of=r("@joaogante"),Lf=r(" and "),Za=l("code"),Bf=r("@Rocketknight1"),Af=r(" in the \u{1F917} Hub PR the command above has just created"),Es=d(),Lo=l("p"),Cf=r("That\u2019s it! \u{1F389}"),Ts=d(),Be=l("h2"),lt=l("a"),el=l("span"),$(Jt.$$.fragment),Df=d(),tl=l("span"),xf=r("Debugging mismatches across ML frameworks \u{1F41B}"),ks=d(),Bo=l("p"),Nf=r(`At some point, when adding a new architecture or when creating TensorFlow weights for an existing architecture, you
might come across errors compaining about mismatches between PyTorch and TensorFlow. You might even decide to open the
model architecture code for the two frameworks, and find that they look identical. What\u2019s going on? \u{1F914}`),Fs=d(),st=l("p"),Sf=r(`First of all, let\u2019s talk about why understanding these mismatches matters. Many community members will use \u{1F917}
Transformers models out of the box, and trust that our models behave as expected. When there is a large mismatch
between the two frameworks, it implies that the model is not following the reference implementation for at least one
of the frameworks. This might lead to silent failures, in which the model runs but has poor performance. This is
arguably worse than a model that fails to run at all! To that end, we aim at having a framework mismatch smaller than
`),ol=l("code"),Hf=r("1e-5"),Rf=r(" at all stages of the model."),$s=d(),Ao=l("p"),Mf=r(`As in other numerical problems, the devil is in the details. And as in any detail-oriented craft, the secret
ingredient here is patience. Here is our suggested workflow for when you come across this type of issues:`),Ps=d(),me=l("ol"),Qt=l("li"),qf=r(`Locate the source of mismatches. The model you\u2019re converting probably has near identical inner variables up to a
certain point. Place `),rl=l("code"),Gf=r("breakpoint()"),Uf=r(` statements in the two frameworks\u2019 architectures, and compare the values of the
numerical variables in a top-down fashion until you find the source of the problems.`),Wf=d(),al=l("li"),jf=r(`Now that you\u2019ve pinpointed the source of the issue, get in touch with the \u{1F917} Transformers team. It is possible
that we\u2019ve seen a similar problem before and can promptly provide a solution. As a fallback, scan popular pages
like StackOverflow and GitHub issues.`),Yf=d(),ll=l("li"),zf=r(`If there is no solution in sight, it means you\u2019ll have to go deeper. The good news is that you\u2019ve located the
issue, so you can focus on the problematic instruction, abstracting away the rest of the model! The bad news is
that you\u2019ll have to venture into the source implementation of said instruction. In some cases, you might find an
issue with a reference implementation - don\u2019t abstain from opening an issue in the upstream repository.`),Is=d(),ue=l("p"),Vf=r(`In some cases, in dicussion with the \u{1F917} Transformers team, we might find that the fixing the mismatch is infeasible.
When the mismatch is very small in the output layers of the model (but potentially large in the hidden states), we
might decide to ignore it in favor of distributing the model. The `),sl=l("code"),Kf=r("pt-to-tf"),Jf=r(" CLI mentioned above has a "),nl=l("code"),Qf=r("--max-error"),Xf=r(`
flag to override the error message at weight conversion time.`),this.h()},l(t){const i=Iw('[data-svelte="svelte-1phssyn"]',document.head);m=s(i,"META",{name:!0,content:!0}),i.forEach(o),z=c(t),v=s(t,"H1",{class:!0});var Xt=n(v);g=s(Xt,"A",{id:!0,class:!0,href:!0});var il=n(g);q=s(il,"SPAN",{});var rm=n(q);P(_.$$.fragment,rm),rm.forEach(o),il.forEach(o),p=c(Xt),G=s(Xt,"SPAN",{});var am=n(G);N=a(am,"How to convert a \u{1F917} Transformers model to TensorFlow?"),am.forEach(o),Xt.forEach(o),k=c(t),S=s(t,"P",{});var Ls=n(S);V=a(Ls,`Having multiple frameworks available to use with \u{1F917} Transformers gives you flexibility to play their strengths when
designing your application, but it implies that compatibility must be added on a per-model basis. The good news is that
adding TensorFlow compatibility to an existing model is simpler than `),ae=s(Ls,"A",{href:!0});var lm=n(ae);ct=a(lm,"adding a new model from scratch"),lm.forEach(o),X=a(Ls,`!
Whether you wish to have a deeper understanding of large TensorFlow models, make a major open-source contribution, or
enable TensorFlow for your model of choice, this guide is for you.`),Ls.forEach(o),Ce=c(t),le=s(t,"P",{});var sm=n(le);Z=a(sm,`This guide empowers you, a member of our community, to contribute TensorFlow model weights and/or
architectures to be used in \u{1F917} Transformers, with minimal supervision from the Hugging Face team. Writing a new model
is no small feat, but hopefully this guide will make it less of a rollercoaster \u{1F3A2} and more of a walk in the park \u{1F6B6}.
Harnessing our collective experiences is absolutely critical to make this process increasingly easier, and thus we
highly encourage that you suggest improvements to this guide!`),sm.forEach(o),De=c(t),se=s(t,"P",{});var nm=n(se);y=a(nm,"Before you dive deeper, it is recommended that you check the following resources if you\u2019re new to \u{1F917} Transformers:"),nm.forEach(o),U=c(t),ee=s(t,"UL",{});var Bs=n(ee);A=s(Bs,"LI",{});var im=n(A);ve=s(im,"A",{href:!0});var hm=n(ve);to=a(hm,"General overview of \u{1F917} Transformers"),hm.forEach(o),im.forEach(o),oo=c(Bs),xe=s(Bs,"LI",{});var dm=n(xe);ft=s(dm,"A",{href:!0,rel:!0});var cm=n(ft);ln=a(cm,"Hugging Face\u2019s TensorFlow Philosophy"),cm.forEach(o),dm.forEach(o),Bs.forEach(o),pl=c(t),ro=s(t,"P",{});var fm=n(ro);sn=a(fm,`In the remainder of this guide, you will learn what\u2019s needed to add a new TensorFlow model architecture, the
procedure to convert PyTorch into TensorFlow model weights, and how to efficiently debug mismatches across ML
frameworks. Let\u2019s get started!`),fm.forEach(o),wl=c(t),P(Ne.$$.fragment,t),yl=c(t),_e=s(t,"H2",{class:!0});var As=n(_e);Se=s(As,"A",{id:!0,class:!0,href:!0});var mm=n(Se);jo=s(mm,"SPAN",{});var um=n(jo);P(mt.$$.fragment,um),um.forEach(o),mm.forEach(o),nn=c(As),Yo=s(As,"SPAN",{});var pm=n(Yo);hn=a(pm,"Step-by-step guide to add TensorFlow model architecture code"),pm.forEach(o),As.forEach(o),vl=c(t),He=s(t,"P",{});var Cs=n(He);dn=a(Cs,`There are many ways to design a large model architecture, and multiple ways of implementing said design. However,
you might recall from our `),ao=s(Cs,"A",{href:!0});var wm=n(ao);cn=a(wm,"general overview of \u{1F917} Transformers"),wm.forEach(o),fn=a(Cs,`
that we are an opinionated bunch - the ease of use of \u{1F917} Transformers relies on consistent design choices. From
experience, we can tell you a few important things about adding TensorFlow models:`),Cs.forEach(o),_l=c(t),ne=s(t,"UL",{});var Co=n(ne);zo=s(Co,"LI",{});var ym=n(zo);mn=a(ym,`Don\u2019t reinvent the wheel! More often that not, there are at least two reference implementations you should check: the
PyTorch equivalent of the model you are implementing and other TensorFlow models for the same class of problems.`),ym.forEach(o),un=c(Co),Vo=s(Co,"LI",{});var vm=n(Vo);pn=a(vm,`Great model implementations survive the test of time. This doesn\u2019t happen because the code is pretty, but rather
because the code is clear, easy to debug and build upon. If you make the life of the maintainers easy with your
TensorFlow implementation, by replicating the same patterns as in other TensorFlow models and minimizing the mismatch
to the PyTorch implementation, you ensure your contribution will be long lived.`),vm.forEach(o),wn=c(Co),Ko=s(Co,"LI",{});var _m=n(Ko);yn=a(_m,`Ask for help when you\u2019re stuck! The \u{1F917} Transformers team is here to help, and we\u2019ve probably found solutions to the same
problems you\u2019re facing.`),_m.forEach(o),Co.forEach(o),bl=c(t),lo=s(t,"P",{});var bm=n(lo);vn=a(bm,"Here\u2019s an overview of the steps needed to add a TensorFlow model architecture:"),bm.forEach(o),gl=c(t),E=s(t,"OL",{});var H=n(E);Jo=s(H,"LI",{});var gm=n(Jo);_n=a(gm,"Select the model you wish to convert"),gm.forEach(o),bn=c(H),Qo=s(H,"LI",{});var Em=n(Qo);gn=a(Em,"Prepare transformers dev environment"),Em.forEach(o),En=c(H),Xo=s(H,"LI",{});var Tm=n(Xo);Tn=a(Tm,"(Optional) Understand theoretical aspects and the existing implementation"),Tm.forEach(o),kn=c(H),Zo=s(H,"LI",{});var km=n(Zo);Fn=a(km,"Implement the model architecture"),km.forEach(o),$n=c(H),er=s(H,"LI",{});var Fm=n(er);Pn=a(Fm,"Implement model tests"),Fm.forEach(o),In=c(H),tr=s(H,"LI",{});var $m=n(tr);On=a($m,"Submit the pull request"),$m.forEach(o),Ln=c(H),or=s(H,"LI",{});var Pm=n(or);Bn=a(Pm,"(Optional) Build demos and share with the world"),Pm.forEach(o),H.forEach(o),El=c(t),be=s(t,"H3",{class:!0});var Ds=n(be);Re=s(Ds,"A",{id:!0,class:!0,href:!0});var Im=n(Re);rr=s(Im,"SPAN",{});var Om=n(rr);P(ut.$$.fragment,Om),Om.forEach(o),Im.forEach(o),An=c(Ds),ar=s(Ds,"SPAN",{});var Lm=n(ar);Cn=a(Lm,"1.-3. Prepare your model contribution"),Lm.forEach(o),Ds.forEach(o),Tl=c(t),so=s(t,"P",{});var Bm=n(so);lr=s(Bm,"STRONG",{});var Am=n(lr);Dn=a(Am,"1. Select the model you wish to convert"),Am.forEach(o),Bm.forEach(o),kl=c(t),Me=s(t,"P",{});var xs=n(Me);xn=a(xs,`Let\u2019s start off with the basics: the first thing you need to know is the architecture you want to convert. If you
don\u2019t have your eyes set on a specific architecture, asking the \u{1F917} Transformers team for suggestions is a great way to
maximize your impact - we will guide you towards the most prominent architectures that are missing on the TensorFlow
side. If the specific model you want to use with TensorFlow already has a TensorFlow architecture implementation in
\u{1F917} Transformers but is lacking weights, feel free to jump straight into the
`),no=s(xs,"A",{href:!0});var Cm=n(no);Nn=a(Cm,"weight conversion section"),Cm.forEach(o),Sn=a(xs,`
of this page.`),xs.forEach(o),Fl=c(t),ie=s(t,"P",{});var Do=n(ie);Hn=a(Do,`For simplicity, the remainder of this guide assumes you\u2019ve decided to contribute with the TensorFlow version of
`),sr=s(Do,"EM",{});var Dm=n(sr);Rn=a(Dm,"BrandNewBert"),Dm.forEach(o),Mn=a(Do," (the same example as in the "),io=s(Do,"A",{href:!0});var xm=n(io);qn=a(xm,"guide"),xm.forEach(o),Gn=a(Do," to add a new model from scratch)."),Do.forEach(o),$l=c(t),P(qe.$$.fragment,t),Pl=c(t),ho=s(t,"P",{});var Nm=n(ho);nr=s(Nm,"STRONG",{});var Sm=n(nr);Un=a(Sm,"2. Prepare transformers dev environment"),Sm.forEach(o),Nm.forEach(o),Il=c(t),co=s(t,"P",{});var Hm=n(co);Wn=a(Hm,`Having selected the model architecture, open an draft PR to signal your intention to work on it. Follow the
instructions below to set up your environment and open a draft PR.`),Hm.forEach(o),Ol=c(t),Ge=s(t,"OL",{});var Ns=n(Ge);ir=s(Ns,"LI",{});var Rm=n(ir);pt=s(Rm,"P",{});var Ss=n(pt);jn=a(Ss,"Fork the "),wt=s(Ss,"A",{href:!0,rel:!0});var Mm=n(wt);Yn=a(Mm,"repository"),Mm.forEach(o),zn=a(Ss,` by clicking on the \u2018Fork\u2019 button on the
repository\u2019s page. This creates a copy of the code under your GitHub user account.`),Ss.forEach(o),Rm.forEach(o),Vn=c(Ns),hr=s(Ns,"LI",{});var qm=n(hr);yt=s(qm,"P",{});var Hs=n(yt);Kn=a(Hs,"Clone your "),dr=s(Hs,"CODE",{});var Gm=n(dr);Jn=a(Gm,"transformers"),Gm.forEach(o),Qn=a(Hs," fork to your local disk, and add the base repository as a remote:"),Hs.forEach(o),qm.forEach(o),Ns.forEach(o),Ll=c(t),P(vt.$$.fragment,t),Bl=c(t),_t=s(t,"OL",{start:!0});var Um=n(_t);cr=s(Um,"LI",{});var Wm=n(cr);Xn=a(Wm,"Set up a development environment, for instance by running the following command:"),Wm.forEach(o),Um.forEach(o),Al=c(t),P(bt.$$.fragment,t),Cl=c(t),gt=s(t,"P",{});var Zf=n(gt);fr=s(Zf,"STRONG",{});var jm=n(fr);Zn=a(jm,"Note:"),jm.forEach(o),ei=a(Zf," You don\u2019t need to have CUDA installed. Making the new model work on CPU is sufficient."),Zf.forEach(o),Dl=c(t),Et=s(t,"OL",{start:!0});var Ym=n(Et);mr=s(Ym,"LI",{});var zm=n(mr);ti=a(zm,"Create a branch with a descriptive name from your main branch"),zm.forEach(o),Ym.forEach(o),xl=c(t),P(Tt.$$.fragment,t),Nl=c(t),kt=s(t,"OL",{start:!0});var Vm=n(kt);ur=s(Vm,"LI",{});var Km=n(ur);oi=a(Km,"Fetch and rebase to current main"),Km.forEach(o),Vm.forEach(o),Sl=c(t),P(Ft.$$.fragment,t),Hl=c(t),ge=s(t,"OL",{start:!0});var Rs=n(ge);pr=s(Rs,"LI",{});var Jm=n(pr);te=s(Jm,"P",{});var nt=n(te);ri=a(nt,"Add an empty "),wr=s(nt,"CODE",{});var Qm=n(wr);ai=a(Qm,".py"),Qm.forEach(o),li=a(nt," file in "),yr=s(nt,"CODE",{});var Xm=n(yr);si=a(Xm,"transformers/src/models/brandnewbert/"),Xm.forEach(o),ni=a(nt," named "),vr=s(nt,"CODE",{});var Zm=n(vr);ii=a(Zm,"modeling_tf_brandnewbert.py"),Zm.forEach(o),hi=a(nt,`. This will
be your TensorFlow model file.`),nt.forEach(o),Jm.forEach(o),di=c(Rs),_r=s(Rs,"LI",{});var eu=n(_r);br=s(eu,"P",{});var tu=n(br);ci=a(tu,"Push the changes to your account using:"),tu.forEach(o),eu.forEach(o),Rs.forEach(o),Rl=c(t),P($t.$$.fragment,t),Ml=c(t),Ee=s(t,"OL",{start:!0});var Ms=n(Ee);gr=s(Ms,"LI",{});var ou=n(gr);Er=s(ou,"P",{});var ru=n(Er);fi=a(ru,`Once you are satisfied, go to the webpage of your fork on GitHub. Click on \u201CPull request\u201D. Make sure to add the
GitHub handle of some members of the Hugging Face team as reviewers, so that the Hugging Face team gets notified for
future changes.`),ru.forEach(o),ou.forEach(o),mi=c(Ms),Tr=s(Ms,"LI",{});var au=n(Tr);kr=s(au,"P",{});var lu=n(kr);ui=a(lu,"Change the PR into a draft by clicking on \u201CConvert to draft\u201D on the right of the GitHub pull request web page."),lu.forEach(o),au.forEach(o),Ms.forEach(o),ql=c(t),Ue=s(t,"P",{});var qs=n(Ue);pi=a(qs,"Now you have set up a development environment to port "),Fr=s(qs,"EM",{});var su=n(Fr);wi=a(su,"BrandNewBert"),su.forEach(o),yi=a(qs," to TensorFlow in \u{1F917} Transformers."),qs.forEach(o),Gl=c(t),fo=s(t,"P",{});var nu=n(fo);$r=s(nu,"STRONG",{});var iu=n($r);vi=a(iu,"3. (Optional) Understand theoretical aspects and the existing implementation"),iu.forEach(o),nu.forEach(o),Ul=c(t),he=s(t,"P",{});var xo=n(he);_i=a(xo,"You should take some time to read "),Pr=s(xo,"EM",{});var hu=n(Pr);bi=a(hu,"BrandNewBert\u2019s"),hu.forEach(o),gi=a(xo,` paper, if such descriptive work exists. There might be large
sections of the paper that are difficult to understand. If this is the case, this is fine - don\u2019t worry! The goal is
not to get a deep theoretical understanding of the paper, but to extract the necessary information required to
effectively re-implement the model in \u{1F917} Transformers using TensorFlow. That being said, you don\u2019t have to spend too
much time on the theoretical aspects, but rather focus on the practical ones, namely the existing model documentation
page (e.g. `),mo=s(xo,"A",{href:!0});var du=n(mo);Ei=a(du,"model docs for BERT"),du.forEach(o),Ti=a(xo,")."),xo.forEach(o),Wl=c(t),uo=s(t,"P",{});var cu=n(uo);ki=a(cu,`After you\u2019ve grasped the basics of the models you are about to implement, it\u2019s important to understand the existing
implementation. This is a great chance to confirm that a working implementation matches your expectations for the
model, as well as to foresee technical challenges on the TensorFlow side.`),cu.forEach(o),jl=c(t),We=s(t,"P",{});var Gs=n(We);Fi=a(Gs,`It\u2019s perfectly natural that you feel overwhelmed with the amount of information that you\u2019ve just absorbed. It is
definitely not a requirement that you understand all facets of the model at this stage. Nevertheless, we highly
encourage you to clear any pressing questions in our `),Pt=s(Gs,"A",{href:!0,rel:!0});var fu=n(Pt);$i=a(fu,"forum"),fu.forEach(o),Pi=a(Gs,"."),Gs.forEach(o),Yl=c(t),Te=s(t,"H3",{class:!0});var Us=n(Te);je=s(Us,"A",{id:!0,class:!0,href:!0});var mu=n(je);Ir=s(mu,"SPAN",{});var uu=n(Ir);P(It.$$.fragment,uu),uu.forEach(o),mu.forEach(o),Ii=c(Us),Or=s(Us,"SPAN",{});var pu=n(Or);Oi=a(pu,"4. Model implementation"),pu.forEach(o),Us.forEach(o),zl=c(t),T=s(t,"P",{});var R=n(T);Li=a(R,`Now it\u2019s time to finally start coding. Our suggested starting point is the PyTorch file itself: copy the contents of
`),Lr=s(R,"CODE",{});var wu=n(Lr);Bi=a(wu,"modeling_brand_new_bert.py"),wu.forEach(o),Ai=a(R," inside "),Br=s(R,"CODE",{});var yu=n(Br);Ci=a(yu,"src/transformers/models/brand_new_bert/"),yu.forEach(o),Di=a(R,` into
`),Ar=s(R,"CODE",{});var vu=n(Ar);xi=a(vu,"modeling_tf_brand_new_bert.py"),vu.forEach(o),Ni=a(R,`. The goal of this section is to modify the file and update the import structure of
\u{1F917} Transformers such that you can import `),Cr=s(R,"CODE",{});var _u=n(Cr);Si=a(_u,"TFBrandNewBert"),_u.forEach(o),Hi=a(R,` and
`),Dr=s(R,"CODE",{});var bu=n(Dr);Ri=a(bu,"TFBrandNewBert.from_pretrained(model_repo, from_pt=True)"),bu.forEach(o),Mi=a(R," successfully loads a working TensorFlow "),xr=s(R,"EM",{});var gu=n(xr);qi=a(gu,"BrandNewBert"),gu.forEach(o),Gi=a(R," model."),R.forEach(o),Vl=c(t),po=s(t,"P",{});var Eu=n(po);Ui=a(Eu,`Sadly, there is no prescription to convert a PyTorch model into TensorFlow. You can, however, follow our selection of
tips to make the process as smooth as possible:`),Eu.forEach(o),Kl=c(t),u=s(t,"UL",{});var w=n(u);oe=s(w,"LI",{});var it=n(oe);Wi=a(it,"Prepend "),Nr=s(it,"CODE",{});var Tu=n(Nr);ji=a(Tu,"TF"),Tu.forEach(o),Yi=a(it," to the name of all classes (e.g. "),Sr=s(it,"CODE",{});var ku=n(Sr);zi=a(ku,"BrandNewBert"),ku.forEach(o),Vi=a(it," becomes "),Hr=s(it,"CODE",{});var Fu=n(Hr);Ki=a(Fu,"TFBrandNewBert"),Fu.forEach(o),Ji=a(it,")."),it.forEach(o),Qi=c(w),F=s(w,"LI",{});var M=n(F);Xi=a(M,"Most PyTorch operations have a direct TensorFlow replacement. For example, "),Rr=s(M,"CODE",{});var $u=n(Rr);Zi=a($u,"torch.nn.Linear"),$u.forEach(o),eh=a(M,` corresponds to
`),Mr=s(M,"CODE",{});var Pu=n(Mr);th=a(Pu,"tf.keras.layers.Dense"),Pu.forEach(o),oh=a(M,", "),qr=s(M,"CODE",{});var Iu=n(qr);rh=a(Iu,"torch.nn.Dropout"),Iu.forEach(o),ah=a(M," corresponds to "),Gr=s(M,"CODE",{});var Ou=n(Gr);lh=a(Ou,"tf.keras.layers.Dropout"),Ou.forEach(o),sh=a(M,`, etc. If you\u2019re not sure
about a specific operation, you can use the `),Ot=s(M,"A",{href:!0,rel:!0});var Lu=n(Ot);nh=a(Lu,"TensorFlow documentation"),Lu.forEach(o),ih=a(M,`
or the `),Lt=s(M,"A",{href:!0,rel:!0});var Bu=n(Lt);hh=a(Bu,"PyTorch documentation"),Bu.forEach(o),dh=a(M,"."),M.forEach(o),ch=c(w),Ur=s(w,"LI",{});var Au=n(Ur);fh=a(Au,`Look for patterns in the \u{1F917} Transformers codebase. If you come across a certain operation that doesn\u2019t have a direct
replacement, the odds are that someone else already had the same problem.`),Au.forEach(o),mh=c(w),Wr=s(w,"LI",{});var Cu=n(Wr);uh=a(Cu,`By default, keep the same variable names and structure as in PyTorch. This will make it easier to debug, track
issues, and add fixes down the line.`),Cu.forEach(o),ph=c(w),W=s(w,"LI",{});var pe=n(W);wh=a(pe,`Some layers have different default values in each framework. A notable example is the batch normalization layer\u2019s
epsilon (`),jr=s(pe,"CODE",{});var Du=n(jr);yh=a(Du,"1e-5"),Du.forEach(o),vh=a(pe," in "),Bt=s(pe,"A",{href:!0,rel:!0});var xu=n(Bt);_h=a(xu,"PyTorch"),xu.forEach(o),bh=a(pe,`
and `),Yr=s(pe,"CODE",{});var Nu=n(Yr);gh=a(Nu,"1e-3"),Nu.forEach(o),Eh=a(pe," in "),At=s(pe,"A",{href:!0,rel:!0});var Su=n(At);Th=a(Su,"TensorFlow"),Su.forEach(o),kh=a(pe,`).
Double-check the documentation!`),pe.forEach(o),Fh=c(w),K=s(w,"LI",{});var Ae=n(K);$h=a(Ae,"PyTorch\u2019s "),zr=s(Ae,"CODE",{});var Hu=n(zr);Ph=a(Hu,"nn.Parameter"),Hu.forEach(o),Ih=a(Ae," variables typically need to be initialized within TF Layer\u2019s "),Vr=s(Ae,"CODE",{});var Ru=n(Vr);Oh=a(Ru,"build()"),Ru.forEach(o),Lh=a(Ae,`. See the following
example: `),Ct=s(Ae,"A",{href:!0,rel:!0});var Mu=n(Ct);Bh=a(Mu,"PyTorch"),Mu.forEach(o),Ah=a(Ae,` /
`),Dt=s(Ae,"A",{href:!0,rel:!0});var qu=n(Dt);Ch=a(qu,"TensorFlow"),qu.forEach(o),Ae.forEach(o),Dh=c(w),xt=s(w,"LI",{});var Ws=n(xt);xh=a(Ws,"If the PyTorch model has a "),Kr=s(Ws,"CODE",{});var Gu=n(Kr);Nh=a(Gu,"#copied from ..."),Gu.forEach(o),Sh=a(Ws,` on top of a function, the odds are that your TensorFlow model can also
borrow that function from the architecture it was copied from, assuming it has a TensorFlow architecture.`),Ws.forEach(o),Hh=c(w),j=s(w,"LI",{});var we=n(j);Rh=a(we,"Assigning the "),Jr=s(we,"CODE",{});var Uu=n(Jr);Mh=a(Uu,"name"),Uu.forEach(o),qh=a(we," attribute correctly in TensorFlow functions is critical to do the "),Qr=s(we,"CODE",{});var Wu=n(Qr);Gh=a(Wu,"from_pt=True"),Wu.forEach(o),Uh=a(we,` weight
cross-loading. `),Xr=s(we,"CODE",{});var ju=n(Xr);Wh=a(ju,"name"),ju.forEach(o),jh=a(we," is almost always the name of the corresponding variable in the PyTorch code. If "),Zr=s(we,"CODE",{});var Yu=n(Zr);Yh=a(Yu,"name"),Yu.forEach(o),zh=a(we,` is not
properly set, you will see it in the error message when loading the model weights.`),we.forEach(o),Vh=c(w),Y=s(w,"LI",{});var ye=n(Y);Kh=a(ye,"The logic of the base model class, "),ea=s(ye,"CODE",{});var zu=n(ea);Jh=a(zu,"BrandNewBertModel"),zu.forEach(o),Qh=a(ye,", will actually reside in "),ta=s(ye,"CODE",{});var Vu=n(ta);Xh=a(Vu,"TFBrandNewBertMainLayer"),Vu.forEach(o),Zh=a(ye,`, a Keras
layer subclass (`),Nt=s(ye,"A",{href:!0,rel:!0});var Ku=n(Nt);ed=a(Ku,"example"),Ku.forEach(o),td=a(ye,`).
`),oa=s(ye,"CODE",{});var Ju=n(oa);od=a(Ju,"TFBrandNewBertModel"),Ju.forEach(o),rd=a(ye," will simply be a wrapper around this layer."),ye.forEach(o),ad=c(w),re=s(w,"LI",{});var ht=n(re);ld=a(ht,"Keras models need to be built in order to load pretrained weights. For that reason, "),ra=s(ht,"CODE",{});var Qu=n(ra);sd=a(Qu,"TFBrandNewBertPreTrainedModel"),Qu.forEach(o),nd=a(ht,`
will need to hold an example of inputs to the model, the `),aa=s(ht,"CODE",{});var Xu=n(aa);id=a(Xu,"dummy_inputs"),Xu.forEach(o),hd=a(ht,`
(`),St=s(ht,"A",{href:!0,rel:!0});var Zu=n(St);dd=a(Zu,"example"),Zu.forEach(o),cd=a(ht,")."),ht.forEach(o),fd=c(w),la=s(w,"LI",{});var ep=n(la);md=a(ep,"If you get stuck, ask for help - we\u2019re here to help you! \u{1F917}"),ep.forEach(o),w.forEach(o),Jl=c(t),Ye=s(t,"P",{});var js=n(Ye);ud=a(js,`In addition to the model file itself, you will also need to add the pointers to the model classes and related
documentation pages. You can complete this part entirely following the patterns in other PRs
(`),Ht=s(js,"A",{href:!0,rel:!0});var tp=n(Ht);pd=a(tp,"example"),tp.forEach(o),wd=a(js,`). Here\u2019s a list of the needed manual
changes:`),js.forEach(o),Ql=c(t),b=s(t,"UL",{});var x=n(b);ze=s(x,"LI",{});var hl=n(ze);yd=a(hl,"Include all public classes of "),sa=s(hl,"EM",{});var op=n(sa);vd=a(op,"BrandNewBert"),op.forEach(o),_d=a(hl," in "),na=s(hl,"CODE",{});var rp=n(na);bd=a(rp,"src/transformers/__init__.py"),rp.forEach(o),hl.forEach(o),gd=c(x),Ve=s(x,"LI",{});var dl=n(Ve);Ed=a(dl,"Add "),ia=s(dl,"EM",{});var ap=n(ia);Td=a(ap,"BrandNewBert"),ap.forEach(o),kd=a(dl," classes to the corresponding Auto classes in "),ha=s(dl,"CODE",{});var lp=n(ha);Fd=a(lp,"src/transformers/models/auto/modeling_tf_auto.py"),lp.forEach(o),dl.forEach(o),$d=c(x),wo=s(x,"LI",{});var em=n(wo);Pd=a(em,"Include the modeling file in the documentation test file list in "),da=s(em,"CODE",{});var sp=n(da);Id=a(sp,"utils/documentation_tests.txt"),sp.forEach(o),em.forEach(o),Od=c(x),Ke=s(x,"LI",{});var cl=n(Ke);Ld=a(cl,"Add the lazy loading classes related to "),ca=s(cl,"EM",{});var np=n(ca);Bd=a(np,"BrandNewBert"),np.forEach(o),Ad=a(cl," in "),fa=s(cl,"CODE",{});var ip=n(fa);Cd=a(ip,"src/transformers/utils/dummy_tf_objects.py"),ip.forEach(o),cl.forEach(o),Dd=c(x),yo=s(x,"LI",{});var tm=n(yo);xd=a(tm,"Update the import structures for the public classes in "),ma=s(tm,"CODE",{});var hp=n(ma);Nd=a(hp,"src/transformers/models/brand_new_bert/__init__.py"),hp.forEach(o),tm.forEach(o),Sd=c(x),Je=s(x,"LI",{});var fl=n(Je);Hd=a(fl,"Add the documentation pointers to the public methods of "),ua=s(fl,"EM",{});var dp=n(ua);Rd=a(dp,"BrandNewBert"),dp.forEach(o),Md=a(fl," in "),pa=s(fl,"CODE",{});var cp=n(pa);qd=a(cp,"docs/source/en/model_doc/brand_new_bert.mdx"),cp.forEach(o),fl.forEach(o),Gd=c(x),Qe=s(x,"LI",{});var ml=n(Qe);Ud=a(ml,"Add yourself to the list of contributors to "),wa=s(ml,"EM",{});var fp=n(wa);Wd=a(fp,"BrandNewBert"),fp.forEach(o),jd=a(ml," in "),ya=s(ml,"CODE",{});var mp=n(ya);Yd=a(mp,"docs/source/en/model_doc/brand_new_bert.mdx"),mp.forEach(o),ml.forEach(o),zd=c(x),Xe=s(x,"LI",{});var ul=n(Xe);Vd=a(ul,"Finally, add a green tick \u2705 to the TensorFlow column of "),va=s(ul,"EM",{});var up=n(va);Kd=a(up,"BrandNewBert"),up.forEach(o),Jd=a(ul," in "),_a=s(ul,"CODE",{});var pp=n(_a);Qd=a(pp,"docs/source/en/index.mdx"),pp.forEach(o),ul.forEach(o),x.forEach(o),Xl=c(t),vo=s(t,"P",{});var wp=n(vo);Xd=a(wp,`When you\u2019re happy with your implementation, run the following checklist to confirm that your model architecture is
ready:`),wp.forEach(o),Zl=c(t),C=s(t,"OL",{});var J=n(C);Rt=s(J,"LI",{});var Ys=n(Rt);Zd=a(Ys,"All layers that behave differently at train time (e.g. Dropout) are called with a "),ba=s(Ys,"CODE",{});var yp=n(ba);ec=a(yp,"training"),yp.forEach(o),tc=a(Ys,` argument, which is
propagated all the way from the top-level classes`),Ys.forEach(o),oc=c(J),Mt=s(J,"LI",{});var zs=n(Mt);rc=a(zs,"You have used "),ga=s(zs,"CODE",{});var vp=n(ga);ac=a(vp,"#copied from ..."),vp.forEach(o),lc=a(zs," whenever possible"),zs.forEach(o),sc=c(J),ke=s(J,"LI",{});var No=n(ke);Ea=s(No,"CODE",{});var _p=n(Ea);nc=a(_p,"TFBrandNewBertMainLayer"),_p.forEach(o),ic=a(No," and all classes that use it have their "),Ta=s(No,"CODE",{});var bp=n(Ta);hc=a(bp,"call"),bp.forEach(o),dc=a(No," function decorated with "),ka=s(No,"CODE",{});var gp=n(ka);cc=a(gp,"@unpack_inputs"),gp.forEach(o),No.forEach(o),fc=c(J),qt=s(J,"LI",{});var Vs=n(qt);Fa=s(Vs,"CODE",{});var Ep=n(Fa);mc=a(Ep,"TFBrandNewBertMainLayer"),Ep.forEach(o),uc=a(Vs," is decorated with "),$a=s(Vs,"CODE",{});var Tp=n($a);pc=a(Tp,"@keras_serializable"),Tp.forEach(o),Vs.forEach(o),wc=c(J),_o=s(J,"LI",{});var om=n(_o);yc=a(om,"A TensorFlow model can be loaded from PyTorch weights using "),Pa=s(om,"CODE",{});var kp=n(Pa);vc=a(kp,"TFBrandNewBert.from_pretrained(model_repo, from_pt=True)"),kp.forEach(o),om.forEach(o),_c=c(J),Ia=s(J,"LI",{});var Fp=n(Ia);bc=a(Fp,"You can call the TensorFlow model using the expected input format"),Fp.forEach(o),J.forEach(o),es=c(t),Fe=s(t,"H3",{class:!0});var Ks=n(Fe);Ze=s(Ks,"A",{id:!0,class:!0,href:!0});var $p=n(Ze);Oa=s($p,"SPAN",{});var Pp=n(Oa);P(Gt.$$.fragment,Pp),Pp.forEach(o),$p.forEach(o),gc=c(Ks),La=s(Ks,"SPAN",{});var Ip=n(La);Ec=a(Ip,"5. Add model tests"),Ip.forEach(o),Ks.forEach(o),ts=c(t),D=s(t,"P",{});var Q=n(D);Tc=a(Q,`Hurray, you\u2019ve implemented a TensorFlow model! Now it\u2019s time to add tests to make sure that your model behaves as
expected. As in the previous section, we suggest you start by copying the `),Ba=s(Q,"CODE",{});var Op=n(Ba);kc=a(Op,"test_modeling_brand_new_bert.py"),Op.forEach(o),Fc=a(Q,` file in
`),Aa=s(Q,"CODE",{});var Lp=n(Aa);$c=a(Lp,"tests/models/brand_new_bert/"),Lp.forEach(o),Pc=a(Q," into "),Ca=s(Q,"CODE",{});var Bp=n(Ca);Ic=a(Bp,"test_modeling_tf_brand_new_bert.py"),Bp.forEach(o),Oc=a(Q,`, and continue by making the necessary
TensorFlow replacements. For now, in all `),Da=s(Q,"CODE",{});var Ap=n(Da);Lc=a(Ap,".from_pretrained()"),Ap.forEach(o),Bc=a(Q," calls, you should use the "),xa=s(Q,"CODE",{});var Cp=n(xa);Ac=a(Cp,"from_pt=True"),Cp.forEach(o),Cc=a(Q,` flag to load
the existing PyTorch weights.`),Q.forEach(o),os=c(t),bo=s(t,"P",{});var Dp=n(bo);Dc=a(Dp,"After you\u2019re done, it\u2019s time for the moment of truth: run the tests! \u{1F62C}"),Dp.forEach(o),rs=c(t),P(Ut.$$.fragment,t),as=c(t),et=s(t,"P",{});var Js=n(et);xc=a(Js,`The most likely outcome is that you\u2019ll see a bunch of errors. Don\u2019t worry, this is expected! Debugging ML models is
notoriously hard, and the key ingredient to success is patience (and `),Na=s(Js,"CODE",{});var xp=n(Na);Nc=a(xp,"breakpoint()"),xp.forEach(o),Sc=a(Js,`). In our experience, the hardest
problems arise from subtle mismatches between ML frameworks, for which we have a few pointers at the end of this guide.
In other cases, a general test might not be directly applicable to your model, in which case we suggest an override
at the model test class level. Regardless of the issue, don\u2019t hesitate to ask for help in your draft pull request if
you\u2019re stuck.`),Js.forEach(o),ls=c(t),go=s(t,"P",{});var Np=n(go);Hc=a(Np,"When all tests pass, congratulations, your model is nearly ready to be added to the \u{1F917} Transformers library! \u{1F389}"),Np.forEach(o),ss=c(t),$e=s(t,"H3",{class:!0});var Qs=n($e);tt=s(Qs,"A",{id:!0,class:!0,href:!0});var Sp=n(tt);Sa=s(Sp,"SPAN",{});var Hp=n(Sa);P(Wt.$$.fragment,Hp),Hp.forEach(o),Sp.forEach(o),Rc=c(Qs),Ha=s(Qs,"SPAN",{});var Rp=n(Ha);Mc=a(Rp,"6.-7. Ensure everyone can use your model"),Rp.forEach(o),Qs.forEach(o),ns=c(t),Eo=s(t,"P",{});var Mp=n(Eo);Ra=s(Mp,"STRONG",{});var qp=n(Ra);qc=a(qp,"6. Submit the pull request"),qp.forEach(o),Mp.forEach(o),is=c(t),ot=s(t,"P",{});var Xs=n(ot);Gc=a(Xs,`Once you\u2019re done with the implementation and the tests, it\u2019s time to submit a pull request. Before pushing your code,
run our code formatting utility, `),Ma=s(Xs,"CODE",{});var Gp=n(Ma);Uc=a(Gp,"make fixup"),Gp.forEach(o),Wc=a(Xs,` \u{1FA84}. This will automatically fix any formatting issues, which would cause
our automatic checks to fail.`),Xs.forEach(o),hs=c(t),de=s(t,"P",{});var So=n(de);jc=a(So,`It\u2019s now time to convert your draft pull request into a real pull request. To do so, click on the \u201CReady for
review\u201D button and add Joao (`),qa=s(So,"CODE",{});var Up=n(qa);Yc=a(Up,"@gante"),Up.forEach(o),zc=a(So,") and Matt ("),Ga=s(So,"CODE",{});var Wp=n(Ga);Vc=a(Wp,"@Rocketknight1"),Wp.forEach(o),Kc=a(So,`) as reviewers. A model pull request will need
at least 3 reviewers, but they will take care of finding appropriate additional reviewers for your model.`),So.forEach(o),ds=c(t),ce=s(t,"P",{});var Ho=n(ce);Jc=a(Ho,"After all reviewers are happy with the state of your PR, the final action point is to remove the "),Ua=s(Ho,"CODE",{});var jp=n(Ua);Qc=a(jp,"from_pt=True"),jp.forEach(o),Xc=a(Ho,` flag in
`),Wa=s(Ho,"CODE",{});var Yp=n(Wa);Zc=a(Yp,".from_pretrained()"),Yp.forEach(o),ef=a(Ho,` calls. Since there are no TensorFlow weights, you will have to add them! Check the section
below for instructions on how to do it.`),Ho.forEach(o),cs=c(t),To=s(t,"P",{});var zp=n(To);tf=a(zp,`Finally, when the TensorFlow weights get merged, you have at least 3 reviewer approvals, and all CI checks are
green, double-check the tests locally one last time`),zp.forEach(o),fs=c(t),P(jt.$$.fragment,t),ms=c(t),ko=s(t,"P",{});var Vp=n(ko);of=a(Vp,"and we will merge your PR! Congratulations on the milestone \u{1F389}"),Vp.forEach(o),us=c(t),Fo=s(t,"P",{});var Kp=n(Fo);ja=s(Kp,"STRONG",{});var Jp=n(ja);rf=a(Jp,"7. (Optional) Build demos and share with the world"),Jp.forEach(o),Kp.forEach(o),ps=c(t),$o=s(t,"P",{});var Qp=n($o);af=a(Qp,`One of the hardest parts about open-source is discovery. How can the other users learn about the existence of your
fabulous TensorFlow contribution? With proper communication, of course! \u{1F4E3}`),Qp.forEach(o),ws=c(t),Po=s(t,"P",{});var Xp=n(Po);lf=a(Xp,"There are two main ways to share your model with the community:"),Xp.forEach(o),ys=c(t),rt=s(t,"UL",{});var Zs=n(rt);Yt=s(Zs,"LI",{});var en=n(Yt);sf=a(en,`Build demos. These include Gradio demos, notebooks, and other fun ways to show off your model. We highly
encourage you to add a notebook to our `),zt=s(en,"A",{href:!0,rel:!0});var Zp=n(zt);nf=a(Zp,"community-driven demos"),Zp.forEach(o),hf=a(en,"."),en.forEach(o),df=c(Zs),Ya=s(Zs,"LI",{});var ew=n(Ya);cf=a(ew,`Share stories on social media like Twitter and LinkedIn. You should be proud of your work and share
your achievement with the community - your model can now be used by thousands of engineers and researchers around
the world \u{1F30D}! We will be happy to retweet your posts and help you share your work with the community.`),ew.forEach(o),Zs.forEach(o),vs=c(t),Pe=s(t,"H2",{class:!0});var tn=n(Pe);at=s(tn,"A",{id:!0,class:!0,href:!0});var tw=n(at);za=s(tw,"SPAN",{});var ow=n(za);P(Vt.$$.fragment,ow),ow.forEach(o),tw.forEach(o),ff=c(tn),Va=s(tn,"SPAN",{});var rw=n(Va);mf=a(rw,"Adding TensorFlow weights to \u{1F917} Hub"),rw.forEach(o),tn.forEach(o),_s=c(t),Io=s(t,"P",{});var aw=n(Io);uf=a(aw,`Assuming that the TensorFlow model architecture is available in \u{1F917} Transformers, converting PyTorch weights into
TensorFlow weights is a breeze!`),aw.forEach(o),bs=c(t),Oo=s(t,"P",{});var lw=n(Oo);pf=a(lw,"Here\u2019s how to do it:"),lw.forEach(o),gs=c(t),fe=s(t,"OL",{});var Ro=n(fe);Ie=s(Ro,"LI",{});var Mo=n(Ie);wf=a(Mo,`Make sure you are logged into your Hugging Face account in your terminal. You can log in using the command
`),Ka=s(Mo,"CODE",{});var sw=n(Ka);yf=a(sw,"huggingface-cli login"),sw.forEach(o),vf=a(Mo," (you can find your access tokens "),Kt=s(Mo,"A",{href:!0,rel:!0});var nw=n(Kt);_f=a(nw,"here"),nw.forEach(o),bf=a(Mo,")"),Mo.forEach(o),gf=c(Ro),Oe=s(Ro,"LI",{});var qo=n(Oe);Ef=a(qo,"Run "),Ja=s(qo,"CODE",{});var iw=n(Ja);Tf=a(iw,"transformers-cli pt-to-tf --model-name foo/bar"),iw.forEach(o),kf=a(qo,", where "),Qa=s(qo,"CODE",{});var hw=n(Qa);Ff=a(hw,"foo/bar"),hw.forEach(o),$f=a(qo,` is the name of the model repository
containing the PyTorch weights you want to convert`),qo.forEach(o),Pf=c(Ro),Le=s(Ro,"LI",{});var Go=n(Le);If=a(Go,"Tag "),Xa=s(Go,"CODE",{});var dw=n(Xa);Of=a(dw,"@joaogante"),dw.forEach(o),Lf=a(Go," and "),Za=s(Go,"CODE",{});var cw=n(Za);Bf=a(cw,"@Rocketknight1"),cw.forEach(o),Af=a(Go," in the \u{1F917} Hub PR the command above has just created"),Go.forEach(o),Ro.forEach(o),Es=c(t),Lo=s(t,"P",{});var fw=n(Lo);Cf=a(fw,"That\u2019s it! \u{1F389}"),fw.forEach(o),Ts=c(t),Be=s(t,"H2",{class:!0});var on=n(Be);lt=s(on,"A",{id:!0,class:!0,href:!0});var mw=n(lt);el=s(mw,"SPAN",{});var uw=n(el);P(Jt.$$.fragment,uw),uw.forEach(o),mw.forEach(o),Df=c(on),tl=s(on,"SPAN",{});var pw=n(tl);xf=a(pw,"Debugging mismatches across ML frameworks \u{1F41B}"),pw.forEach(o),on.forEach(o),ks=c(t),Bo=s(t,"P",{});var ww=n(Bo);Nf=a(ww,`At some point, when adding a new architecture or when creating TensorFlow weights for an existing architecture, you
might come across errors compaining about mismatches between PyTorch and TensorFlow. You might even decide to open the
model architecture code for the two frameworks, and find that they look identical. What\u2019s going on? \u{1F914}`),ww.forEach(o),Fs=c(t),st=s(t,"P",{});var rn=n(st);Sf=a(rn,`First of all, let\u2019s talk about why understanding these mismatches matters. Many community members will use \u{1F917}
Transformers models out of the box, and trust that our models behave as expected. When there is a large mismatch
between the two frameworks, it implies that the model is not following the reference implementation for at least one
of the frameworks. This might lead to silent failures, in which the model runs but has poor performance. This is
arguably worse than a model that fails to run at all! To that end, we aim at having a framework mismatch smaller than
`),ol=s(rn,"CODE",{});var yw=n(ol);Hf=a(yw,"1e-5"),yw.forEach(o),Rf=a(rn," at all stages of the model."),rn.forEach(o),$s=c(t),Ao=s(t,"P",{});var vw=n(Ao);Mf=a(vw,`As in other numerical problems, the devil is in the details. And as in any detail-oriented craft, the secret
ingredient here is patience. Here is our suggested workflow for when you come across this type of issues:`),vw.forEach(o),Ps=c(t),me=s(t,"OL",{});var Uo=n(me);Qt=s(Uo,"LI",{});var an=n(Qt);qf=a(an,`Locate the source of mismatches. The model you\u2019re converting probably has near identical inner variables up to a
certain point. Place `),rl=s(an,"CODE",{});var _w=n(rl);Gf=a(_w,"breakpoint()"),_w.forEach(o),Uf=a(an,` statements in the two frameworks\u2019 architectures, and compare the values of the
numerical variables in a top-down fashion until you find the source of the problems.`),an.forEach(o),Wf=c(Uo),al=s(Uo,"LI",{});var bw=n(al);jf=a(bw,`Now that you\u2019ve pinpointed the source of the issue, get in touch with the \u{1F917} Transformers team. It is possible
that we\u2019ve seen a similar problem before and can promptly provide a solution. As a fallback, scan popular pages
like StackOverflow and GitHub issues.`),bw.forEach(o),Yf=c(Uo),ll=s(Uo,"LI",{});var gw=n(ll);zf=a(gw,`If there is no solution in sight, it means you\u2019ll have to go deeper. The good news is that you\u2019ve located the
issue, so you can focus on the problematic instruction, abstracting away the rest of the model! The bad news is
that you\u2019ll have to venture into the source implementation of said instruction. In some cases, you might find an
issue with a reference implementation - don\u2019t abstain from opening an issue in the upstream repository.`),gw.forEach(o),Uo.forEach(o),Is=c(t),ue=s(t,"P",{});var Wo=n(ue);Vf=a(Wo,`In some cases, in dicussion with the \u{1F917} Transformers team, we might find that the fixing the mismatch is infeasible.
When the mismatch is very small in the output layers of the model (but potentially large in the hidden states), we
might decide to ignore it in favor of distributing the model. The `),sl=s(Wo,"CODE",{});var Ew=n(sl);Kf=a(Ew,"pt-to-tf"),Ew.forEach(o),Jf=a(Wo," CLI mentioned above has a "),nl=s(Wo,"CODE",{});var Tw=n(nl);Qf=a(Tw,"--max-error"),Tw.forEach(o),Xf=a(Wo,`
flag to override the error message at weight conversion time.`),Wo.forEach(o),this.h()},h(){f(m,"name","hf:doc:metadata"),f(m,"content",JSON.stringify(Cw)),f(g,"id","how-to-convert-a-transformers-model-to-tensorflow"),f(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(g,"href","#how-to-convert-a-transformers-model-to-tensorflow"),f(v,"class","relative group"),f(ae,"href","add_new_model"),f(ve,"href","add_new_model#general-overview-of-transformers"),f(ft,"href","https://huggingface.co/blog/tensorflow-philosophy"),f(ft,"rel","nofollow"),f(Se,"id","stepbystep-guide-to-add-tensorflow-model-architecture-code"),f(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Se,"href","#stepbystep-guide-to-add-tensorflow-model-architecture-code"),f(_e,"class","relative group"),f(ao,"href","add_new_model#general-overview-of-transformers"),f(Re,"id","13-prepare-your-model-contribution"),f(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Re,"href","#13-prepare-your-model-contribution"),f(be,"class","relative group"),f(no,"href","#adding-tensorflow-weights-to-hub"),f(io,"href","add_new_model"),f(wt,"href","https://github.com/huggingface/transformers"),f(wt,"rel","nofollow"),f(_t,"start","3"),f(Et,"start","4"),f(kt,"start","5"),f(ge,"start","6"),f(Ee,"start","8"),f(mo,"href","model_doc/bert"),f(Pt,"href","https://discuss.huggingface.co/"),f(Pt,"rel","nofollow"),f(je,"id","4-model-implementation"),f(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(je,"href","#4-model-implementation"),f(Te,"class","relative group"),f(Ot,"href","https://www.tensorflow.org/api_docs/python/tf"),f(Ot,"rel","nofollow"),f(Lt,"href","https://pytorch.org/docs/stable/"),f(Lt,"rel","nofollow"),f(Bt,"href","https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d"),f(Bt,"rel","nofollow"),f(At,"href","https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization"),f(At,"rel","nofollow"),f(Ct,"href","https://github.com/huggingface/transformers/blob/655f72a6896c0533b1bdee519ed65a059c2425ac/src/transformers/models/vit_mae/modeling_vit_mae.py#L212"),f(Ct,"rel","nofollow"),f(Dt,"href","https://github.com/huggingface/transformers/blob/655f72a6896c0533b1bdee519ed65a059c2425ac/src/transformers/models/vit_mae/modeling_tf_vit_mae.py#L220"),f(Dt,"rel","nofollow"),f(Nt,"href","https://github.com/huggingface/transformers/blob/4fd32a1f499e45f009c2c0dea4d81c321cba7e02/src/transformers/models/bert/modeling_tf_bert.py#L719"),f(Nt,"rel","nofollow"),f(St,"href","https://github.com/huggingface/transformers/blob/4fd32a1f499e45f009c2c0dea4d81c321cba7e02/src/transformers/models/bert/modeling_tf_bert.py#L916"),f(St,"rel","nofollow"),f(Ht,"href","https://github.com/huggingface/transformers/pull/18020/files"),f(Ht,"rel","nofollow"),f(Ze,"id","5-add-model-tests"),f(Ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Ze,"href","#5-add-model-tests"),f(Fe,"class","relative group"),f(tt,"id","67-ensure-everyone-can-use-your-model"),f(tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(tt,"href","#67-ensure-everyone-can-use-your-model"),f($e,"class","relative group"),f(zt,"href","https://huggingface.co/docs/transformers/community"),f(zt,"rel","nofollow"),f(at,"id","adding-tensorflow-weights-to-hub"),f(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(at,"href","#adding-tensorflow-weights-to-hub"),f(Pe,"class","relative group"),f(Kt,"href","https://huggingface.co/settings/tokens"),f(Kt,"rel","nofollow"),f(lt,"id","debugging-mismatches-across-ml-frameworks"),f(lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(lt,"href","#debugging-mismatches-across-ml-frameworks"),f(Be,"class","relative group")},m(t,i){e(document.head,m),h(t,z,i),h(t,v,i),e(v,g),e(g,q),I(_,q,null),e(v,p),e(v,G),e(G,N),h(t,k,i),h(t,S,i),e(S,V),e(S,ae),e(ae,ct),e(S,X),h(t,Ce,i),h(t,le,i),e(le,Z),h(t,De,i),h(t,se,i),e(se,y),h(t,U,i),h(t,ee,i),e(ee,A),e(A,ve),e(ve,to),e(ee,oo),e(ee,xe),e(xe,ft),e(ft,ln),h(t,pl,i),h(t,ro,i),e(ro,sn),h(t,wl,i),I(Ne,t,i),h(t,yl,i),h(t,_e,i),e(_e,Se),e(Se,jo),I(mt,jo,null),e(_e,nn),e(_e,Yo),e(Yo,hn),h(t,vl,i),h(t,He,i),e(He,dn),e(He,ao),e(ao,cn),e(He,fn),h(t,_l,i),h(t,ne,i),e(ne,zo),e(zo,mn),e(ne,un),e(ne,Vo),e(Vo,pn),e(ne,wn),e(ne,Ko),e(Ko,yn),h(t,bl,i),h(t,lo,i),e(lo,vn),h(t,gl,i),h(t,E,i),e(E,Jo),e(Jo,_n),e(E,bn),e(E,Qo),e(Qo,gn),e(E,En),e(E,Xo),e(Xo,Tn),e(E,kn),e(E,Zo),e(Zo,Fn),e(E,$n),e(E,er),e(er,Pn),e(E,In),e(E,tr),e(tr,On),e(E,Ln),e(E,or),e(or,Bn),h(t,El,i),h(t,be,i),e(be,Re),e(Re,rr),I(ut,rr,null),e(be,An),e(be,ar),e(ar,Cn),h(t,Tl,i),h(t,so,i),e(so,lr),e(lr,Dn),h(t,kl,i),h(t,Me,i),e(Me,xn),e(Me,no),e(no,Nn),e(Me,Sn),h(t,Fl,i),h(t,ie,i),e(ie,Hn),e(ie,sr),e(sr,Rn),e(ie,Mn),e(ie,io),e(io,qn),e(ie,Gn),h(t,$l,i),I(qe,t,i),h(t,Pl,i),h(t,ho,i),e(ho,nr),e(nr,Un),h(t,Il,i),h(t,co,i),e(co,Wn),h(t,Ol,i),h(t,Ge,i),e(Ge,ir),e(ir,pt),e(pt,jn),e(pt,wt),e(wt,Yn),e(pt,zn),e(Ge,Vn),e(Ge,hr),e(hr,yt),e(yt,Kn),e(yt,dr),e(dr,Jn),e(yt,Qn),h(t,Ll,i),I(vt,t,i),h(t,Bl,i),h(t,_t,i),e(_t,cr),e(cr,Xn),h(t,Al,i),I(bt,t,i),h(t,Cl,i),h(t,gt,i),e(gt,fr),e(fr,Zn),e(gt,ei),h(t,Dl,i),h(t,Et,i),e(Et,mr),e(mr,ti),h(t,xl,i),I(Tt,t,i),h(t,Nl,i),h(t,kt,i),e(kt,ur),e(ur,oi),h(t,Sl,i),I(Ft,t,i),h(t,Hl,i),h(t,ge,i),e(ge,pr),e(pr,te),e(te,ri),e(te,wr),e(wr,ai),e(te,li),e(te,yr),e(yr,si),e(te,ni),e(te,vr),e(vr,ii),e(te,hi),e(ge,di),e(ge,_r),e(_r,br),e(br,ci),h(t,Rl,i),I($t,t,i),h(t,Ml,i),h(t,Ee,i),e(Ee,gr),e(gr,Er),e(Er,fi),e(Ee,mi),e(Ee,Tr),e(Tr,kr),e(kr,ui),h(t,ql,i),h(t,Ue,i),e(Ue,pi),e(Ue,Fr),e(Fr,wi),e(Ue,yi),h(t,Gl,i),h(t,fo,i),e(fo,$r),e($r,vi),h(t,Ul,i),h(t,he,i),e(he,_i),e(he,Pr),e(Pr,bi),e(he,gi),e(he,mo),e(mo,Ei),e(he,Ti),h(t,Wl,i),h(t,uo,i),e(uo,ki),h(t,jl,i),h(t,We,i),e(We,Fi),e(We,Pt),e(Pt,$i),e(We,Pi),h(t,Yl,i),h(t,Te,i),e(Te,je),e(je,Ir),I(It,Ir,null),e(Te,Ii),e(Te,Or),e(Or,Oi),h(t,zl,i),h(t,T,i),e(T,Li),e(T,Lr),e(Lr,Bi),e(T,Ai),e(T,Br),e(Br,Ci),e(T,Di),e(T,Ar),e(Ar,xi),e(T,Ni),e(T,Cr),e(Cr,Si),e(T,Hi),e(T,Dr),e(Dr,Ri),e(T,Mi),e(T,xr),e(xr,qi),e(T,Gi),h(t,Vl,i),h(t,po,i),e(po,Ui),h(t,Kl,i),h(t,u,i),e(u,oe),e(oe,Wi),e(oe,Nr),e(Nr,ji),e(oe,Yi),e(oe,Sr),e(Sr,zi),e(oe,Vi),e(oe,Hr),e(Hr,Ki),e(oe,Ji),e(u,Qi),e(u,F),e(F,Xi),e(F,Rr),e(Rr,Zi),e(F,eh),e(F,Mr),e(Mr,th),e(F,oh),e(F,qr),e(qr,rh),e(F,ah),e(F,Gr),e(Gr,lh),e(F,sh),e(F,Ot),e(Ot,nh),e(F,ih),e(F,Lt),e(Lt,hh),e(F,dh),e(u,ch),e(u,Ur),e(Ur,fh),e(u,mh),e(u,Wr),e(Wr,uh),e(u,ph),e(u,W),e(W,wh),e(W,jr),e(jr,yh),e(W,vh),e(W,Bt),e(Bt,_h),e(W,bh),e(W,Yr),e(Yr,gh),e(W,Eh),e(W,At),e(At,Th),e(W,kh),e(u,Fh),e(u,K),e(K,$h),e(K,zr),e(zr,Ph),e(K,Ih),e(K,Vr),e(Vr,Oh),e(K,Lh),e(K,Ct),e(Ct,Bh),e(K,Ah),e(K,Dt),e(Dt,Ch),e(u,Dh),e(u,xt),e(xt,xh),e(xt,Kr),e(Kr,Nh),e(xt,Sh),e(u,Hh),e(u,j),e(j,Rh),e(j,Jr),e(Jr,Mh),e(j,qh),e(j,Qr),e(Qr,Gh),e(j,Uh),e(j,Xr),e(Xr,Wh),e(j,jh),e(j,Zr),e(Zr,Yh),e(j,zh),e(u,Vh),e(u,Y),e(Y,Kh),e(Y,ea),e(ea,Jh),e(Y,Qh),e(Y,ta),e(ta,Xh),e(Y,Zh),e(Y,Nt),e(Nt,ed),e(Y,td),e(Y,oa),e(oa,od),e(Y,rd),e(u,ad),e(u,re),e(re,ld),e(re,ra),e(ra,sd),e(re,nd),e(re,aa),e(aa,id),e(re,hd),e(re,St),e(St,dd),e(re,cd),e(u,fd),e(u,la),e(la,md),h(t,Jl,i),h(t,Ye,i),e(Ye,ud),e(Ye,Ht),e(Ht,pd),e(Ye,wd),h(t,Ql,i),h(t,b,i),e(b,ze),e(ze,yd),e(ze,sa),e(sa,vd),e(ze,_d),e(ze,na),e(na,bd),e(b,gd),e(b,Ve),e(Ve,Ed),e(Ve,ia),e(ia,Td),e(Ve,kd),e(Ve,ha),e(ha,Fd),e(b,$d),e(b,wo),e(wo,Pd),e(wo,da),e(da,Id),e(b,Od),e(b,Ke),e(Ke,Ld),e(Ke,ca),e(ca,Bd),e(Ke,Ad),e(Ke,fa),e(fa,Cd),e(b,Dd),e(b,yo),e(yo,xd),e(yo,ma),e(ma,Nd),e(b,Sd),e(b,Je),e(Je,Hd),e(Je,ua),e(ua,Rd),e(Je,Md),e(Je,pa),e(pa,qd),e(b,Gd),e(b,Qe),e(Qe,Ud),e(Qe,wa),e(wa,Wd),e(Qe,jd),e(Qe,ya),e(ya,Yd),e(b,zd),e(b,Xe),e(Xe,Vd),e(Xe,va),e(va,Kd),e(Xe,Jd),e(Xe,_a),e(_a,Qd),h(t,Xl,i),h(t,vo,i),e(vo,Xd),h(t,Zl,i),h(t,C,i),e(C,Rt),e(Rt,Zd),e(Rt,ba),e(ba,ec),e(Rt,tc),e(C,oc),e(C,Mt),e(Mt,rc),e(Mt,ga),e(ga,ac),e(Mt,lc),e(C,sc),e(C,ke),e(ke,Ea),e(Ea,nc),e(ke,ic),e(ke,Ta),e(Ta,hc),e(ke,dc),e(ke,ka),e(ka,cc),e(C,fc),e(C,qt),e(qt,Fa),e(Fa,mc),e(qt,uc),e(qt,$a),e($a,pc),e(C,wc),e(C,_o),e(_o,yc),e(_o,Pa),e(Pa,vc),e(C,_c),e(C,Ia),e(Ia,bc),h(t,es,i),h(t,Fe,i),e(Fe,Ze),e(Ze,Oa),I(Gt,Oa,null),e(Fe,gc),e(Fe,La),e(La,Ec),h(t,ts,i),h(t,D,i),e(D,Tc),e(D,Ba),e(Ba,kc),e(D,Fc),e(D,Aa),e(Aa,$c),e(D,Pc),e(D,Ca),e(Ca,Ic),e(D,Oc),e(D,Da),e(Da,Lc),e(D,Bc),e(D,xa),e(xa,Ac),e(D,Cc),h(t,os,i),h(t,bo,i),e(bo,Dc),h(t,rs,i),I(Ut,t,i),h(t,as,i),h(t,et,i),e(et,xc),e(et,Na),e(Na,Nc),e(et,Sc),h(t,ls,i),h(t,go,i),e(go,Hc),h(t,ss,i),h(t,$e,i),e($e,tt),e(tt,Sa),I(Wt,Sa,null),e($e,Rc),e($e,Ha),e(Ha,Mc),h(t,ns,i),h(t,Eo,i),e(Eo,Ra),e(Ra,qc),h(t,is,i),h(t,ot,i),e(ot,Gc),e(ot,Ma),e(Ma,Uc),e(ot,Wc),h(t,hs,i),h(t,de,i),e(de,jc),e(de,qa),e(qa,Yc),e(de,zc),e(de,Ga),e(Ga,Vc),e(de,Kc),h(t,ds,i),h(t,ce,i),e(ce,Jc),e(ce,Ua),e(Ua,Qc),e(ce,Xc),e(ce,Wa),e(Wa,Zc),e(ce,ef),h(t,cs,i),h(t,To,i),e(To,tf),h(t,fs,i),I(jt,t,i),h(t,ms,i),h(t,ko,i),e(ko,of),h(t,us,i),h(t,Fo,i),e(Fo,ja),e(ja,rf),h(t,ps,i),h(t,$o,i),e($o,af),h(t,ws,i),h(t,Po,i),e(Po,lf),h(t,ys,i),h(t,rt,i),e(rt,Yt),e(Yt,sf),e(Yt,zt),e(zt,nf),e(Yt,hf),e(rt,df),e(rt,Ya),e(Ya,cf),h(t,vs,i),h(t,Pe,i),e(Pe,at),e(at,za),I(Vt,za,null),e(Pe,ff),e(Pe,Va),e(Va,mf),h(t,_s,i),h(t,Io,i),e(Io,uf),h(t,bs,i),h(t,Oo,i),e(Oo,pf),h(t,gs,i),h(t,fe,i),e(fe,Ie),e(Ie,wf),e(Ie,Ka),e(Ka,yf),e(Ie,vf),e(Ie,Kt),e(Kt,_f),e(Ie,bf),e(fe,gf),e(fe,Oe),e(Oe,Ef),e(Oe,Ja),e(Ja,Tf),e(Oe,kf),e(Oe,Qa),e(Qa,Ff),e(Oe,$f),e(fe,Pf),e(fe,Le),e(Le,If),e(Le,Xa),e(Xa,Of),e(Le,Lf),e(Le,Za),e(Za,Bf),e(Le,Af),h(t,Es,i),h(t,Lo,i),e(Lo,Cf),h(t,Ts,i),h(t,Be,i),e(Be,lt),e(lt,el),I(Jt,el,null),e(Be,Df),e(Be,tl),e(tl,xf),h(t,ks,i),h(t,Bo,i),e(Bo,Nf),h(t,Fs,i),h(t,st,i),e(st,Sf),e(st,ol),e(ol,Hf),e(st,Rf),h(t,$s,i),h(t,Ao,i),e(Ao,Mf),h(t,Ps,i),h(t,me,i),e(me,Qt),e(Qt,qf),e(Qt,rl),e(rl,Gf),e(Qt,Uf),e(me,Wf),e(me,al),e(al,jf),e(me,Yf),e(me,ll),e(ll,zf),h(t,Is,i),h(t,ue,i),e(ue,Vf),e(ue,sl),e(sl,Kf),e(ue,Jf),e(ue,nl),e(nl,Qf),e(ue,Xf),Os=!0},p(t,[i]){const Xt={};i&2&&(Xt.$$scope={dirty:i,ctx:t}),Ne.$set(Xt);const il={};i&2&&(il.$$scope={dirty:i,ctx:t}),qe.$set(il)},i(t){Os||(O(_.$$.fragment,t),O(Ne.$$.fragment,t),O(mt.$$.fragment,t),O(ut.$$.fragment,t),O(qe.$$.fragment,t),O(vt.$$.fragment,t),O(bt.$$.fragment,t),O(Tt.$$.fragment,t),O(Ft.$$.fragment,t),O($t.$$.fragment,t),O(It.$$.fragment,t),O(Gt.$$.fragment,t),O(Ut.$$.fragment,t),O(Wt.$$.fragment,t),O(jt.$$.fragment,t),O(Vt.$$.fragment,t),O(Jt.$$.fragment,t),Os=!0)},o(t){L(_.$$.fragment,t),L(Ne.$$.fragment,t),L(mt.$$.fragment,t),L(ut.$$.fragment,t),L(qe.$$.fragment,t),L(vt.$$.fragment,t),L(bt.$$.fragment,t),L(Tt.$$.fragment,t),L(Ft.$$.fragment,t),L($t.$$.fragment,t),L(It.$$.fragment,t),L(Gt.$$.fragment,t),L(Ut.$$.fragment,t),L(Wt.$$.fragment,t),L(jt.$$.fragment,t),L(Vt.$$.fragment,t),L(Jt.$$.fragment,t),Os=!1},d(t){o(m),t&&o(z),t&&o(v),B(_),t&&o(k),t&&o(S),t&&o(Ce),t&&o(le),t&&o(De),t&&o(se),t&&o(U),t&&o(ee),t&&o(pl),t&&o(ro),t&&o(wl),B(Ne,t),t&&o(yl),t&&o(_e),B(mt),t&&o(vl),t&&o(He),t&&o(_l),t&&o(ne),t&&o(bl),t&&o(lo),t&&o(gl),t&&o(E),t&&o(El),t&&o(be),B(ut),t&&o(Tl),t&&o(so),t&&o(kl),t&&o(Me),t&&o(Fl),t&&o(ie),t&&o($l),B(qe,t),t&&o(Pl),t&&o(ho),t&&o(Il),t&&o(co),t&&o(Ol),t&&o(Ge),t&&o(Ll),B(vt,t),t&&o(Bl),t&&o(_t),t&&o(Al),B(bt,t),t&&o(Cl),t&&o(gt),t&&o(Dl),t&&o(Et),t&&o(xl),B(Tt,t),t&&o(Nl),t&&o(kt),t&&o(Sl),B(Ft,t),t&&o(Hl),t&&o(ge),t&&o(Rl),B($t,t),t&&o(Ml),t&&o(Ee),t&&o(ql),t&&o(Ue),t&&o(Gl),t&&o(fo),t&&o(Ul),t&&o(he),t&&o(Wl),t&&o(uo),t&&o(jl),t&&o(We),t&&o(Yl),t&&o(Te),B(It),t&&o(zl),t&&o(T),t&&o(Vl),t&&o(po),t&&o(Kl),t&&o(u),t&&o(Jl),t&&o(Ye),t&&o(Ql),t&&o(b),t&&o(Xl),t&&o(vo),t&&o(Zl),t&&o(C),t&&o(es),t&&o(Fe),B(Gt),t&&o(ts),t&&o(D),t&&o(os),t&&o(bo),t&&o(rs),B(Ut,t),t&&o(as),t&&o(et),t&&o(ls),t&&o(go),t&&o(ss),t&&o($e),B(Wt),t&&o(ns),t&&o(Eo),t&&o(is),t&&o(ot),t&&o(hs),t&&o(de),t&&o(ds),t&&o(ce),t&&o(cs),t&&o(To),t&&o(fs),B(jt,t),t&&o(ms),t&&o(ko),t&&o(us),t&&o(Fo),t&&o(ps),t&&o($o),t&&o(ws),t&&o(Po),t&&o(ys),t&&o(rt),t&&o(vs),t&&o(Pe),B(Vt),t&&o(_s),t&&o(Io),t&&o(bs),t&&o(Oo),t&&o(gs),t&&o(fe),t&&o(Es),t&&o(Lo),t&&o(Ts),t&&o(Be),B(Jt),t&&o(ks),t&&o(Bo),t&&o(Fs),t&&o(st),t&&o($s),t&&o(Ao),t&&o(Ps),t&&o(me),t&&o(Is),t&&o(ue)}}}const Cw={local:"how-to-convert-a-transformers-model-to-tensorflow",sections:[{local:"stepbystep-guide-to-add-tensorflow-model-architecture-code",sections:[{local:"13-prepare-your-model-contribution",title:"1.-3. Prepare your model contribution"},{local:"4-model-implementation",title:"4. Model implementation"},{local:"5-add-model-tests",title:"5. Add model tests"},{local:"67-ensure-everyone-can-use-your-model",title:"6.-7. Ensure everyone can use your model"}],title:"Step-by-step guide to add TensorFlow model architecture code"},{local:"adding-tensorflow-weights-to-hub",title:"Adding TensorFlow weights to \u{1F917} Hub"},{local:"debugging-mismatches-across-ml-frameworks",title:"Debugging mismatches across ML frameworks \u{1F41B}"}],title:"How to convert a \u{1F917} Transformers model to TensorFlow?"};function Dw(eo){return Ow(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Rw extends Fw{constructor(m){super();$w(this,m,Dw,Aw,Pw,{})}}export{Rw as default,Cw as metadata};
