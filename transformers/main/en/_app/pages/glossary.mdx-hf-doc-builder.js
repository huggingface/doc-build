import{S as A$,i as q$,s as x$,e as a,k as f,w as d,t as r,M as j$,c as n,d as t,m as c,a as o,x as u,h as l,b as p,G as s,g as h,y as m,q as v,o as g,B as b,v as P$}from"../chunks/vendor-hf-doc-builder.js";import{T as T$}from"../chunks/Tip-hf-doc-builder.js";import{Y as S1}from"../chunks/Youtube-hf-doc-builder.js";import{I as _}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as q}from"../chunks/CodeBlock-hf-doc-builder.js";function S$(Fl){let y,Ue;return{c(){y=a("p"),Ue=r(`Each model\u2019s labels may be different, so be sure to always check the documentation of each model for more information
about their specific labels!`)},l(x){y=n(x,"P",{});var S=o(y);Ue=l(S,`Each model\u2019s labels may be different, so be sure to always check the documentation of each model for more information
about their specific labels!`),S.forEach(t)},m(x,S){h(x,y,S),s(y,Ue)},d(x){x&&t(y)}}}function C$(Fl){let y,Ue,x,S,eo,as,df,to,uf,Il,Ea,mf,Bl,Y,Xe,so,ns,vf,ao,gf,Hl,Q,Ye,no,os,bf,oo,_f,Ol,ya,wf,Ml,rs,Rl,Aa,$f,Vl,qa,kf,Ll,ls,Gl,xa,Ef,Wl,is,Ul,ja,yf,Xl,Pa,Af,Yl,hs,Ql,Ta,qf,Jl,ps,Kl,N,xf,Sa,jf,Pf,ro,Tf,Sf,lo,Cf,Nf,Zl,fs,ei,J,Qe,io,cs,zf,ho,Df,ti,ds,Ff,Ca,If,si,K,Je,po,us,Bf,fo,Hf,ai,ms,Of,Na,Mf,ni,Z,Ke,co,vs,Rf,uo,Vf,oi,ee,Ze,mo,gs,Lf,vo,Gf,ri,j,Wf,za,Uf,Xf,Da,Yf,Qf,go,Jf,Kf,Fa,Zf,ec,li,te,et,bo,bs,tc,_o,sc,ii,se,tt,wo,_s,ac,$o,nc,hi,k,oc,ko,rc,lc,Eo,ic,hc,yo,pc,fc,Ao,cc,dc,qo,uc,mc,xo,vc,gc,pi,ae,st,jo,ws,bc,Po,_c,fi,Ia,wc,ci,ne,at,To,$s,$c,So,kc,di,Ba,Ec,ui,oe,nt,Co,ks,yc,No,Ac,mi,Ha,qc,vi,re,ot,zo,Es,xc,Do,jc,gi,le,rt,Fo,ys,Pc,Io,Tc,bi,Oa,Sc,_i,z,Cc,Bo,Nc,zc,Ho,Dc,Fc,Oo,Ic,Bc,wi,Ma,Hc,$i,ie,lt,Mo,As,Oc,Ro,Mc,ki,Ra,Rc,Ei,he,it,Vo,qs,Vc,Lo,Lc,yi,pe,ht,Go,xs,Gc,Wo,Wc,Ai,pt,Uc,Uo,Xc,Yc,qi,w,Qc,Xo,Jc,Kc,Yo,Zc,ed,js,td,sd,Qo,ad,nd,Jo,od,rd,Ko,ld,id,Zo,hd,pd,er,fd,cd,xi,D,dd,Va,ud,md,tr,vd,gd,sr,bd,_d,ji,fe,ft,ar,Ps,wd,nr,$d,Pi,ce,ct,or,Ts,kd,rr,Ed,Ti,La,yd,Si,I,dt,Ga,Ad,qd,Wa,xd,jd,Pd,B,Ua,Td,Sd,lr,Cd,Nd,Xa,zd,Dd,Fd,H,Ya,Id,Bd,Qa,Hd,Od,Ja,Md,Rd,Ci,de,ut,ir,Ss,Vd,hr,Ld,Ni,ue,mt,pr,Cs,Gd,fr,Wd,zi,vt,Ud,cr,Xd,Yd,Di,me,gt,dr,Ns,Qd,ur,Jd,Fi,Ka,Kd,Ii,zs,Bi,bt,Zd,Ds,eu,tu,Hi,Fs,Oi,Za,su,Mi,Is,Ri,en,au,Vi,Bs,Li,_t,nu,Hs,ou,ru,Gi,Os,Wi,wt,lu,mr,iu,hu,Ui,Ms,Xi,tn,pu,Yi,sn,fu,Qi,Rs,Ji,an,cu,Ki,Vs,Zi,$t,du,nn,uu,mu,eh,ve,kt,vr,Ls,vu,gr,gu,th,ge,Et,br,Gs,bu,_r,_u,sh,on,wu,ah,rn,$u,nh,$,be,ku,ln,Eu,yu,wr,Au,qu,xu,_e,ju,hn,Pu,Tu,$r,Su,Cu,Nu,we,zu,pn,Du,Fu,kr,Iu,Bu,Hu,C,Ou,fn,Mu,Ru,cn,Vu,Lu,Er,Gu,Wu,yr,Uu,Xu,Yu,$e,Qu,dn,Ju,Ku,Ar,Zu,em,tm,ke,sm,un,am,nm,qr,om,rm,lm,F,im,mn,hm,pm,xr,fm,cm,jr,dm,um,mm,Ee,vm,vn,gm,bm,Pr,_m,wm,oh,yt,rh,At,$m,gn,km,Em,lh,ye,qt,Tr,Ws,ym,Sr,Am,ih,Ae,xt,Cr,Us,qm,Nr,xm,hh,bn,jm,ph,qe,jt,zr,Xs,Pm,Dr,Tm,fh,_n,Sm,ch,xe,Pt,Fr,Ys,Cm,Ir,Nm,dh,je,Tt,Br,Qs,zm,Hr,Dm,uh,wn,Fm,mh,Pe,St,Or,Js,Im,Mr,Bm,vh,$n,Hm,gh,Te,Ct,Rr,Ks,Om,Vr,Mm,bh,kn,Rm,_h,Se,Nt,Lr,Zs,Vm,Gr,Lm,wh,Ce,zt,Wr,ea,Gm,Ur,Wm,$h,P,Um,Xr,Xm,Ym,Yr,Qm,Jm,Qr,Km,Zm,Jr,ev,tv,kh,Ne,Dt,Kr,ta,sv,Zr,av,Eh,En,nv,yh,ze,Ft,el,sa,ov,tl,rv,Ah,It,lv,sl,iv,hv,qh,Bt,pv,al,fv,cv,xh,Ht,dv,nl,uv,mv,jh,De,Ot,ol,aa,vv,rl,gv,Ph,O,bv,yn,_v,wv,An,$v,kv,Th,qn,Ev,Sh,Fe,Mt,ll,na,yv,il,Av,Ch,Ie,Rt,hl,oa,qv,pl,xv,Nh,xn,jv,zh,Be,Vt,fl,ra,Pv,cl,Tv,Dh,He,Lt,dl,la,Sv,ul,Cv,Fh,jn,Nv,Ih,Oe,Gt,ml,ia,zv,vl,Dv,Bh,Pn,Fv,Hh,Me,Wt,gl,ha,Iv,bl,Bv,Oh,M,Hv,Tn,Ov,Mv,Sn,Rv,Vv,Mh,Re,Ut,_l,pa,Lv,wl,Gv,Rh,R,Wv,Cn,Uv,Xv,Nn,Yv,Qv,Vh,Ve,Xt,$l,fa,Jv,kl,Kv,Lh,Le,Yt,El,ca,Zv,yl,e1,Gh,zn,t1,Wh,Ge,Qt,Al,da,s1,ql,a1,Uh,Dn,n1,Xh,ua,Yh,V,o1,xl,r1,l1,jl,i1,h1,Qh,ma,Jh,Jt,p1,Pl,f1,c1,Kh,va,Zh,Fn,d1,ep,ga,tp,In,u1,sp,Bn,m1,ap,ba,np,L,v1,Tl,g1,b1,Sl,_1,w1,op,G,$1,Hn,k1,E1,Cl,y1,A1,rp,We,Kt,Nl,_a,q1,zl,x1,lp,On,j1,ip;return as=new _({}),ns=new _({}),os=new _({}),rs=new S1({props:{id:"M6adb1j2jPI"}}),ls=new q({props:{code:`from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")

sequence_a = "This is a short sequence."
sequence_b = "This is a rather long sequence. It is at least longer than the sequence A."

encoded_sequence_a = tokenizer(sequence_a)["input_ids"]
encoded_sequence_b = tokenizer(sequence_b)["input_ids"]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>sequence_a = <span class="hljs-string">&quot;This is a short sequence.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>sequence_b = <span class="hljs-string">&quot;This is a rather long sequence. It is at least longer than the sequence A.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_sequence_a = tokenizer(sequence_a)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_sequence_b = tokenizer(sequence_b)[<span class="hljs-string">&quot;input_ids&quot;</span>]`}}),is=new q({props:{code:"len(encoded_sequence_a), len(encoded_sequence_b)",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(encoded_sequence_a), <span class="hljs-built_in">len</span>(encoded_sequence_b)
(<span class="hljs-number">8</span>, <span class="hljs-number">19</span>)`}}),hs=new q({props:{code:"padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>padded_sequences = tokenizer([sequence_a, sequence_b], padding=<span class="hljs-literal">True</span>)'}}),ps=new q({props:{code:'padded_sequences["input_ids"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>padded_sequences[<span class="hljs-string">&quot;input_ids&quot;</span>]
[[<span class="hljs-number">101</span>, <span class="hljs-number">1188</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">170</span>, <span class="hljs-number">1603</span>, <span class="hljs-number">4954</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">101</span>, <span class="hljs-number">1188</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">170</span>, <span class="hljs-number">1897</span>, <span class="hljs-number">1263</span>, <span class="hljs-number">4954</span>, <span class="hljs-number">119</span>, <span class="hljs-number">1135</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">1120</span>, <span class="hljs-number">1655</span>, <span class="hljs-number">2039</span>, <span class="hljs-number">1190</span>, <span class="hljs-number">1103</span>, <span class="hljs-number">4954</span>, <span class="hljs-number">138</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>]]`}}),fs=new q({props:{code:'padded_sequences["attention_mask"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>padded_sequences[<span class="hljs-string">&quot;attention_mask&quot;</span>]
[[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]`}}),cs=new _({}),us=new _({}),vs=new _({}),gs=new _({}),bs=new _({}),_s=new _({}),ws=new _({}),$s=new _({}),ks=new _({}),Es=new _({}),ys=new _({}),As=new _({}),qs=new _({}),xs=new _({}),Ps=new _({}),Ts=new _({}),Ss=new _({}),Cs=new _({}),Ns=new _({}),zs=new S1({props:{id:"VFp38yj8h3A"}}),Fs=new q({props:{code:`from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")

sequence = "A Titan RTX has 24GB of VRAM"`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>sequence = <span class="hljs-string">&quot;A Titan RTX has 24GB of VRAM&quot;</span>`}}),Is=new q({props:{code:"tokenized_sequence = tokenizer.tokenize(sequence)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_sequence = tokenizer.tokenize(sequence)'}}),Bs=new q({props:{code:"print(tokenized_sequence)",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tokenized_sequence)
[<span class="hljs-string">&#x27;A&#x27;</span>, <span class="hljs-string">&#x27;Titan&#x27;</span>, <span class="hljs-string">&#x27;R&#x27;</span>, <span class="hljs-string">&#x27;##T&#x27;</span>, <span class="hljs-string">&#x27;##X&#x27;</span>, <span class="hljs-string">&#x27;has&#x27;</span>, <span class="hljs-string">&#x27;24&#x27;</span>, <span class="hljs-string">&#x27;##GB&#x27;</span>, <span class="hljs-string">&#x27;of&#x27;</span>, <span class="hljs-string">&#x27;V&#x27;</span>, <span class="hljs-string">&#x27;##RA&#x27;</span>, <span class="hljs-string">&#x27;##M&#x27;</span>]`}}),Os=new q({props:{code:"inputs = tokenizer(sequence)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(sequence)'}}),Ms=new q({props:{code:`encoded_sequence = inputs["input_ids"]
print(encoded_sequence)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_sequence = inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_sequence)
[<span class="hljs-number">101</span>, <span class="hljs-number">138</span>, <span class="hljs-number">18696</span>, <span class="hljs-number">155</span>, <span class="hljs-number">1942</span>, <span class="hljs-number">3190</span>, <span class="hljs-number">1144</span>, <span class="hljs-number">1572</span>, <span class="hljs-number">13745</span>, <span class="hljs-number">1104</span>, <span class="hljs-number">159</span>, <span class="hljs-number">9664</span>, <span class="hljs-number">2107</span>, <span class="hljs-number">102</span>]`}}),Rs=new q({props:{code:"decoded_sequence = tokenizer.decode(encoded_sequence)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>decoded_sequence = tokenizer.decode(encoded_sequence)'}}),Vs=new q({props:{code:"print(decoded_sequence)",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(decoded_sequence)
[CLS] A Titan RTX has 24GB of VRAM [SEP]`}}),Ls=new _({}),Gs=new _({}),yt=new T$({props:{$$slots:{default:[S$]},$$scope:{ctx:Fl}}}),Ws=new _({}),Us=new _({}),Xs=new _({}),Ys=new _({}),Qs=new _({}),Js=new _({}),Ks=new _({}),Zs=new _({}),ea=new _({}),ta=new _({}),sa=new _({}),aa=new _({}),na=new _({}),oa=new _({}),ra=new _({}),la=new _({}),ia=new _({}),ha=new _({}),pa=new _({}),fa=new _({}),ca=new _({}),da=new _({}),ua=new S1({props:{id:"0u3ioSwev3s"}}),ma=new q({props:{code:"# [CLS] SEQUENCE_A [SEP] SEQUENCE_B [SEP]",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [CLS] SEQUENCE_A [SEP] SEQUENCE_B [SEP]</span>'}}),va=new q({props:{code:`from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
sequence_a = "HuggingFace is based in NYC"
sequence_b = "Where is HuggingFace based?"

encoded_dict = tokenizer(sequence_a, sequence_b)
decoded = tokenizer.decode(encoded_dict["input_ids"])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sequence_a = <span class="hljs-string">&quot;HuggingFace is based in NYC&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>sequence_b = <span class="hljs-string">&quot;Where is HuggingFace based?&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dict = tokenizer(sequence_a, sequence_b)
<span class="hljs-meta">&gt;&gt;&gt; </span>decoded = tokenizer.decode(encoded_dict[<span class="hljs-string">&quot;input_ids&quot;</span>])`}}),ga=new q({props:{code:"print(decoded)",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(decoded)
[CLS] HuggingFace <span class="hljs-keyword">is</span> based <span class="hljs-keyword">in</span> NYC [SEP] Where <span class="hljs-keyword">is</span> HuggingFace based? [SEP]`}}),ba=new q({props:{code:'encoded_dict["token_type_ids"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dict[<span class="hljs-string">&quot;token_type_ids&quot;</span>]
[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]`}}),_a=new _({}),{c(){y=a("meta"),Ue=f(),x=a("h1"),S=a("a"),eo=a("span"),d(as.$$.fragment),df=f(),to=a("span"),uf=r("Glossary"),Il=f(),Ea=a("p"),mf=r(`This glossary defines general machine learning and \u{1F917} Transformers terms to help you better understand the
documentation.`),Bl=f(),Y=a("h2"),Xe=a("a"),so=a("span"),d(ns.$$.fragment),vf=f(),ao=a("span"),gf=r("A"),Hl=f(),Q=a("h3"),Ye=a("a"),no=a("span"),d(os.$$.fragment),bf=f(),oo=a("span"),_f=r("attention mask"),Ol=f(),ya=a("p"),wf=r("The attention mask is an optional argument used when batching sequences together."),Ml=f(),d(rs.$$.fragment),Rl=f(),Aa=a("p"),$f=r("This argument indicates to the model which tokens should be attended to, and which should not."),Vl=f(),qa=a("p"),kf=r("For example, consider these two sequences:"),Ll=f(),d(ls.$$.fragment),Gl=f(),xa=a("p"),Ef=r("The encoded versions have different lengths:"),Wl=f(),d(is.$$.fragment),Ul=f(),ja=a("p"),yf=r(`Therefore, we can\u2019t put them together in the same tensor as-is. The first sequence needs to be padded up to the length
of the second one, or the second one needs to be truncated down to the length of the first one.`),Xl=f(),Pa=a("p"),Af=r(`In the first case, the list of IDs will be extended by the padding indices. We can pass a list to the tokenizer and ask
it to pad like this:`),Yl=f(),d(hs.$$.fragment),Ql=f(),Ta=a("p"),qf=r("We can see that 0s have been added on the right of the first sentence to make it the same length as the second one:"),Jl=f(),d(ps.$$.fragment),Kl=f(),N=a("p"),xf=r(`This can then be converted into a tensor in PyTorch or TensorFlow. The attention mask is a binary tensor indicating the
position of the padded indices so that the model does not attend to them. For the `),Sa=a("a"),jf=r("BertTokenizer"),Pf=r(", "),ro=a("code"),Tf=r("1"),Sf=r(` indicates a
value that should be attended to, while `),lo=a("code"),Cf=r("0"),Nf=r(` indicates a padded value. This attention mask is in the dictionary returned
by the tokenizer under the key \u201Cattention_mask\u201D:`),Zl=f(),d(fs.$$.fragment),ei=f(),J=a("h3"),Qe=a("a"),io=a("span"),d(cs.$$.fragment),zf=f(),ho=a("span"),Df=r("autoencoding models"),ti=f(),ds=a("p"),Ff=r("see "),Ca=a("a"),If=r("masked language modeling"),si=f(),K=a("h3"),Je=a("a"),po=a("span"),d(us.$$.fragment),Bf=f(),fo=a("span"),Hf=r("autoregressive models"),ai=f(),ms=a("p"),Of=r("see "),Na=a("a"),Mf=r("causal language modeling"),ni=f(),Z=a("h2"),Ke=a("a"),co=a("span"),d(vs.$$.fragment),Rf=f(),uo=a("span"),Vf=r("B"),oi=f(),ee=a("h3"),Ze=a("a"),mo=a("span"),d(gs.$$.fragment),Lf=f(),vo=a("span"),Gf=r("backbone"),ri=f(),j=a("p"),Wf=r("The backbone is the network (embeddings and layers) that outputs the raw hidden states or features. It is usually connected to a "),za=a("a"),Uf=r("head"),Xf=r(" which accepts the features as its input to make a prediction. For example, "),Da=a("a"),Yf=r("ViTModel"),Qf=r(" is a backbone without a specific head on top. Other models can also use "),go=a("code"),Jf=r("VitModel"),Kf=r(" as a backbone such as "),Fa=a("a"),Zf=r("DPT"),ec=r("."),li=f(),te=a("h2"),et=a("a"),bo=a("span"),d(bs.$$.fragment),tc=f(),_o=a("span"),sc=r("C"),ii=f(),se=a("h3"),tt=a("a"),wo=a("span"),d(_s.$$.fragment),ac=f(),$o=a("span"),nc=r("channel"),hi=f(),k=a("p"),oc=r("Color images are made up of some combination of values in three channels - red, green, and blue (RGB) - and grayscale images only have one channel. In \u{1F917} Transformers, the channel can be the first or last dimension of an image\u2019s tensor: ["),ko=a("code"),rc=r("n_channels"),lc=r(", "),Eo=a("code"),ic=r("height"),hc=r(", "),yo=a("code"),pc=r("width"),fc=r("] or ["),Ao=a("code"),cc=r("height"),dc=r(", "),qo=a("code"),uc=r("width"),mc=r(", "),xo=a("code"),vc=r("n_channels"),gc=r("]."),pi=f(),ae=a("h3"),st=a("a"),jo=a("span"),d(ws.$$.fragment),bc=f(),Po=a("span"),_c=r("causal language modeling"),fi=f(),Ia=a("p"),wc=r(`A pretraining task where the model reads the texts in order and has to predict the next word. It\u2019s usually done by
reading the whole sentence but using a mask inside the model to hide the future tokens at a certain timestep.`),ci=f(),ne=a("h3"),at=a("a"),To=a("span"),d($s.$$.fragment),$c=f(),So=a("span"),kc=r("connectionist temporal classification (CTC)"),di=f(),Ba=a("p"),Ec=r("An algorithm which allows a model to learn without knowing exactly how the input and output are aligned; CTC calculates the distribution of all possible outputs for a given input and chooses the most likely output from it. CTC is commonly used in speech recognition tasks because speech doesn\u2019t always cleanly align with the transcript for a variety of reasons such as a speaker\u2019s different speech rates."),ui=f(),oe=a("h3"),nt=a("a"),Co=a("span"),d(ks.$$.fragment),yc=f(),No=a("span"),Ac=r("convolution"),mi=f(),Ha=a("p"),qc=r("A type of layer in a neural network where the input matrix is multiplied element-wise by a smaller matrix (kernel or filter) and the values are summed up in a new matrix. This is known as a convolutional operation which is repeated over the entire input matrix. Each operation is applied to a different segment of the input matrix. Convolutional neural networks (CNNs) are commonly used in computer vision."),vi=f(),re=a("h2"),ot=a("a"),zo=a("span"),d(Es.$$.fragment),xc=f(),Do=a("span"),jc=r("D"),gi=f(),le=a("h3"),rt=a("a"),Fo=a("span"),d(ys.$$.fragment),Pc=f(),Io=a("span"),Tc=r("decoder input IDs"),bi=f(),Oa=a("p"),Sc=r(`This input is specific to encoder-decoder models, and contains the input IDs that will be fed to the decoder. These
inputs should be used for sequence to sequence tasks, such as translation or summarization, and are usually built in a
way specific to each model.`),_i=f(),z=a("p"),Cc=r("Most encoder-decoder models (BART, T5) create their "),Bo=a("code"),Nc=r("decoder_input_ids"),zc=r(" on their own from the "),Ho=a("code"),Dc=r("labels"),Fc=r(`. In such models,
passing the `),Oo=a("code"),Ic=r("labels"),Bc=r(" is the preferred way to handle training."),wi=f(),Ma=a("p"),Hc=r("Please check each model\u2019s docs to see how they handle these input IDs for sequence to sequence training."),$i=f(),ie=a("h3"),lt=a("a"),Mo=a("span"),d(As.$$.fragment),Oc=f(),Ro=a("span"),Mc=r("deep learning"),ki=f(),Ra=a("p"),Rc=r("Machine learning algorithms which uses neural networks with several layers."),Ei=f(),he=a("h2"),it=a("a"),Vo=a("span"),d(qs.$$.fragment),Vc=f(),Lo=a("span"),Lc=r("F"),yi=f(),pe=a("h3"),ht=a("a"),Go=a("span"),d(xs.$$.fragment),Gc=f(),Wo=a("span"),Wc=r("feed forward chunking"),Ai=f(),pt=a("p"),Uc=r(`In each residual attention block in transformers the self-attention layer is usually followed by 2 feed forward layers.
The intermediate embedding size of the feed forward layers is often bigger than the hidden size of the model (e.g., for
`),Uo=a("code"),Xc=r("bert-base-uncased"),Yc=r(")."),qi=f(),w=a("p"),Qc=r("For an input of size "),Xo=a("code"),Jc=r("[batch_size, sequence_length]"),Kc=r(`, the memory required to store the intermediate feed forward
embeddings `),Yo=a("code"),Zc=r("[batch_size, sequence_length, config.intermediate_size]"),ed=r(` can account for a large fraction of the memory
use. The authors of `),js=a("a"),td=r("Reformer: The Efficient Transformer"),sd=r(` noticed that since the
computation is independent of the `),Qo=a("code"),ad=r("sequence_length"),nd=r(` dimension, it is mathematically equivalent to compute the output
embeddings of both feed forward layers `),Jo=a("code"),od=r("[batch_size, config.hidden_size]_0, ..., [batch_size, config.hidden_size]_n"),rd=r(`
individually and concat them afterward to `),Ko=a("code"),ld=r("[batch_size, sequence_length, config.hidden_size]"),id=r(" with "),Zo=a("code"),hd=r("n = sequence_length"),pd=r(`, which trades increased computation time against reduced memory use, but yields a mathematically
`),er=a("strong"),fd=r("equivalent"),cd=r(" result."),xi=f(),D=a("p"),dd=r("For models employing the function "),Va=a("a"),ud=r("apply_chunking_to_forward()"),md=r(", the "),tr=a("code"),vd=r("chunk_size"),gd=r(` defines the number of output
embeddings that are computed in parallel and thus defines the trade-off between memory and time complexity. If
`),sr=a("code"),bd=r("chunk_size"),_d=r(" is set to 0, no feed forward chunking is done."),ji=f(),fe=a("h2"),ft=a("a"),ar=a("span"),d(Ps.$$.fragment),wd=f(),nr=a("span"),$d=r("H"),Pi=f(),ce=a("h3"),ct=a("a"),or=a("span"),d(Ts.$$.fragment),kd=f(),rr=a("span"),Ed=r("head"),Ti=f(),La=a("p"),yd=r("The model head refers to the last layer of a neural network that accepts the raw hidden states and projects them onto a different dimension. There is a different model head for each task. For example:"),Si=f(),I=a("ul"),dt=a("li"),Ga=a("a"),Ad=r("GPT2ForSequenceClassification"),qd=r(" is a sequence classification head - a linear layer - on top of the base "),Wa=a("a"),xd=r("GPT2Model"),jd=r("."),Pd=f(),B=a("li"),Ua=a("a"),Td=r("ViTForImageClassification"),Sd=r(" is an image classification head - a linear layer on top of the final hidden state of the "),lr=a("code"),Cd=r("CLS"),Nd=r(" token - on top of the base "),Xa=a("a"),zd=r("ViTModel"),Dd=r("."),Fd=f(),H=a("li"),Ya=a("a"),Id=r("Wav2Vec2ForCTC"),Bd=r(" ia a language modeling head with "),Qa=a("a"),Hd=r("CTC"),Od=r(" on top of the base "),Ja=a("a"),Md=r("Wav2Vec2Model"),Rd=r("."),Ci=f(),de=a("h2"),ut=a("a"),ir=a("span"),d(Ss.$$.fragment),Vd=f(),hr=a("span"),Ld=r("I"),Ni=f(),ue=a("h3"),mt=a("a"),pr=a("span"),d(Cs.$$.fragment),Gd=f(),fr=a("span"),Wd=r("image patch"),zi=f(),vt=a("p"),Ud=r("Vision-based Transformers models split an image into smaller patches which are linearly embedded, and then passed as a sequence to the model. You can find the "),cr=a("code"),Xd=r("patch_size"),Yd=r(" - or resolution - of the model in it\u2019s configuration."),Di=f(),me=a("h3"),gt=a("a"),dr=a("span"),d(Ns.$$.fragment),Qd=f(),ur=a("span"),Jd=r("input IDs"),Fi=f(),Ka=a("p"),Kd=r(`The input ids are often the only required parameters to be passed to the model as input. They are token indices,
numerical representations of tokens building the sequences that will be used as input by the model.`),Ii=f(),d(zs.$$.fragment),Bi=f(),bt=a("p"),Zd=r(`Each tokenizer works differently but the underlying mechanism remains the same. Here\u2019s an example using the BERT
tokenizer, which is a `),Ds=a("a"),eu=r("WordPiece"),tu=r(" tokenizer:"),Hi=f(),d(Fs.$$.fragment),Oi=f(),Za=a("p"),su=r("The tokenizer takes care of splitting the sequence into tokens available in the tokenizer vocabulary."),Mi=f(),d(Is.$$.fragment),Ri=f(),en=a("p"),au=r(`The tokens are either words or subwords. Here for instance, \u201CVRAM\u201D wasn\u2019t in the model vocabulary, so it\u2019s been split
in \u201CV\u201D, \u201CRA\u201D and \u201CM\u201D. To indicate those tokens are not separate words but parts of the same word, a double-hash prefix
is added for \u201CRA\u201D and \u201CM\u201D:`),Vi=f(),d(Bs.$$.fragment),Li=f(),_t=a("p"),nu=r(`These tokens can then be converted into IDs which are understandable by the model. This can be done by directly feeding
the sentence to the tokenizer, which leverages the Rust implementation of `),Hs=a("a"),ou=r(`\u{1F917}
Tokenizers`),ru=r(" for peak performance."),Gi=f(),d(Os.$$.fragment),Wi=f(),wt=a("p"),lu=r(`The tokenizer returns a dictionary with all the arguments necessary for its corresponding model to work properly. The
token indices are under the key `),mr=a("code"),iu=r("input_ids"),hu=r(":"),Ui=f(),d(Ms.$$.fragment),Xi=f(),tn=a("p"),pu=r(`Note that the tokenizer automatically adds \u201Cspecial tokens\u201D (if the associated model relies on them) which are special
IDs the model sometimes uses.`),Yi=f(),sn=a("p"),fu=r("If we decode the previous sequence of ids,"),Qi=f(),d(Rs.$$.fragment),Ji=f(),an=a("p"),cu=r("we will see"),Ki=f(),d(Vs.$$.fragment),Zi=f(),$t=a("p"),du=r("because this is the way a "),nn=a("a"),uu=r("BertModel"),mu=r(" is going to expect its inputs."),eh=f(),ve=a("h2"),kt=a("a"),vr=a("span"),d(Ls.$$.fragment),vu=f(),gr=a("span"),gu=r("L"),th=f(),ge=a("h3"),Et=a("a"),br=a("span"),d(Gs.$$.fragment),bu=f(),_r=a("span"),_u=r("labels"),sh=f(),on=a("p"),wu=r(`The labels are an optional argument which can be passed in order for the model to compute the loss itself. These labels
should be the expected prediction of the model: it will use the standard loss in order to compute the loss between its
predictions and the expected value (the label).`),ah=f(),rn=a("p"),$u=r("These labels are different according to the model head, for example:"),nh=f(),$=a("ul"),be=a("li"),ku=r("For sequence classification models, ("),ln=a("a"),Eu=r("BertForSequenceClassification"),yu=r(`), the model expects a tensor of dimension
`),wr=a("code"),Au=r("(batch_size)"),qu=r(" with each value of the batch corresponding to the expected label of the entire sequence."),xu=f(),_e=a("li"),ju=r("For token classification models, ("),hn=a("a"),Pu=r("BertForTokenClassification"),Tu=r(`), the model expects a tensor of dimension
`),$r=a("code"),Su=r("(batch_size, seq_length)"),Cu=r(" with each value corresponding to the expected label of each individual token."),Nu=f(),we=a("li"),zu=r("For masked language modeling, ("),pn=a("a"),Du=r("BertForMaskedLM"),Fu=r("), the model expects a tensor of dimension "),kr=a("code"),Iu=r("(batch_size, seq_length)"),Bu=r(` with each value corresponding to the expected label of each individual token: the labels being the token
ID for the masked token, and values to be ignored for the rest (usually -100).`),Hu=f(),C=a("li"),Ou=r("For sequence to sequence tasks, ("),fn=a("a"),Mu=r("BartForConditionalGeneration"),Ru=r(", "),cn=a("a"),Vu=r("MBartForConditionalGeneration"),Lu=r(`), the model
expects a tensor of dimension `),Er=a("code"),Gu=r("(batch_size, tgt_seq_length)"),Wu=r(` with each value corresponding to the target sequences
associated with each input sequence. During training, both BART and T5 will make the appropriate
`),yr=a("code"),Uu=r("decoder_input_ids"),Xu=r(` and decoder attention masks internally. They usually do not need to be supplied. This does not
apply to models leveraging the Encoder-Decoder framework.`),Yu=f(),$e=a("li"),Qu=r("For image classification models, ("),dn=a("a"),Ju=r("ViTForImageClassification"),Ku=r(`), the model expects a tensor of dimension
`),Ar=a("code"),Zu=r("(batch_size)"),em=r(" with each value of the batch corresponding to the expected label of each individual image."),tm=f(),ke=a("li"),sm=r("For semantic segmentation models, ("),un=a("a"),am=r("SegformerForSemanticSegmentation"),nm=r(`), the model expects a tensor of dimension
`),qr=a("code"),om=r("(batch_size, height, width)"),rm=r(" with each value of the batch corresponding to the expected label of each individual pixel."),lm=f(),F=a("li"),im=r("For object detection models, ("),mn=a("a"),hm=r("DetrForObjectDetection"),pm=r(`), the model expects a list of dictionaries with a
`),xr=a("code"),fm=r("class_labels"),cm=r(" and "),jr=a("code"),dm=r("boxes"),um=r(" key where each value of the batch corresponds to the expected label and number of bounding boxes of each individual image."),mm=f(),Ee=a("li"),vm=r("For automatic speech recognition models, ("),vn=a("a"),gm=r("Wav2Vec2ForCTC"),bm=r("), the model expects a tensor of dimension "),Pr=a("code"),_m=r("(batch_size, target_length)"),wm=r(" with each value corresponding to the expected label of each individual token."),oh=f(),d(yt.$$.fragment),rh=f(),At=a("p"),$m=r("The base models ("),gn=a("a"),km=r("BertModel"),Em=r(`) do not accept labels, as these are the base transformer models, simply outputting
features.`),lh=f(),ye=a("h2"),qt=a("a"),Tr=a("span"),d(Ws.$$.fragment),ym=f(),Sr=a("span"),Am=r("M"),ih=f(),Ae=a("h3"),xt=a("a"),Cr=a("span"),d(Us.$$.fragment),qm=f(),Nr=a("span"),xm=r("masked language modeling"),hh=f(),bn=a("p"),jm=r(`A pretraining task where the model sees a corrupted version of the texts, usually done by
masking some tokens randomly, and has to predict the original text.`),ph=f(),qe=a("h3"),jt=a("a"),zr=a("span"),d(Xs.$$.fragment),Pm=f(),Dr=a("span"),Tm=r("multimodal"),fh=f(),_n=a("p"),Sm=r("A task that combines texts with another kind of inputs (for instance images)."),ch=f(),xe=a("h2"),Pt=a("a"),Fr=a("span"),d(Ys.$$.fragment),Cm=f(),Ir=a("span"),Nm=r("N"),dh=f(),je=a("h3"),Tt=a("a"),Br=a("span"),d(Qs.$$.fragment),zm=f(),Hr=a("span"),Dm=r("Natural language generation"),uh=f(),wn=a("p"),Fm=r("All tasks related to generating text (for instance talk with transformers, translation)."),mh=f(),Pe=a("h3"),St=a("a"),Or=a("span"),d(Js.$$.fragment),Im=f(),Mr=a("span"),Bm=r("Natural language processing"),vh=f(),$n=a("p"),Hm=r("A generic way to say \u201Cdeal with texts\u201D."),gh=f(),Te=a("h3"),Ct=a("a"),Rr=a("span"),d(Ks.$$.fragment),Om=f(),Vr=a("span"),Mm=r("Natural language understanding"),bh=f(),kn=a("p"),Rm=r(`All tasks related to understanding what is in a text (for instance classifying the
whole text, individual words).`),_h=f(),Se=a("h2"),Nt=a("a"),Lr=a("span"),d(Zs.$$.fragment),Vm=f(),Gr=a("span"),Lm=r("P"),wh=f(),Ce=a("h3"),zt=a("a"),Wr=a("span"),d(ea.$$.fragment),Gm=f(),Ur=a("span"),Wm=r("pixel values"),$h=f(),P=a("p"),Um=r("A tensor of the numerical representations of an image that is passed to a model. The pixel values have a shape of ["),Xr=a("code"),Xm=r("batch_size"),Ym=r(", "),Yr=a("code"),Qm=r("num_channels"),Jm=r(", "),Qr=a("code"),Km=r("height"),Zm=r(", "),Jr=a("code"),ev=r("width"),tv=r("], and are generated from a feature extractor."),kh=f(),Ne=a("h3"),Dt=a("a"),Kr=a("span"),d(ta.$$.fragment),sv=f(),Zr=a("span"),av=r("pooling"),Eh=f(),En=a("p"),nv=r("An operation that reduces a matrix into a smaller matrix, either by taking the maximum or average of the pooled dimension(s). Pooling layers are commonly found between convolutional layers to downsample the feature representation."),yh=f(),ze=a("h3"),Ft=a("a"),el=a("span"),d(sa.$$.fragment),ov=f(),tl=a("span"),rv=r("position IDs"),Ah=f(),It=a("p"),lv=r(`Contrary to RNNs that have the position of each token embedded within them, transformers are unaware of the position of
each token. Therefore, the position IDs (`),sl=a("code"),iv=r("position_ids"),hv=r(`) are used by the model to identify each token\u2019s position in the
list of tokens.`),qh=f(),Bt=a("p"),pv=r("They are an optional parameter. If no "),al=a("code"),fv=r("position_ids"),cv=r(` are passed to the model, the IDs are automatically created as
absolute positional embeddings.`),xh=f(),Ht=a("p"),dv=r("Absolute positional embeddings are selected in the range "),nl=a("code"),uv=r("[0, config.max_position_embeddings - 1]"),mv=r(`. Some models use
other types of positional embeddings, such as sinusoidal position embeddings or relative position embeddings.`),jh=f(),De=a("h3"),Ot=a("a"),ol=a("span"),d(aa.$$.fragment),vv=f(),rl=a("span"),gv=r("pretrained model"),Ph=f(),O=a("p"),bv=r(`A model that has been pretrained on some data (for instance all of Wikipedia). Pretraining methods involve a
self-supervised objective, which can be reading the text and trying to predict the next word (see `),yn=a("a"),_v=r(`causal language
modeling`),wv=r(") or masking some words and trying to predict them (see "),An=a("a"),$v=r(`masked language
modeling`),kv=r(")."),Th=f(),qn=a("p"),Ev=r("Speech and vision models have their own pretraining objectives. For example, Wav2Vec2 is a speech model pretrained on a contrastive task which requires the model to identify the \u201Ctrue\u201D speech representation from a set of \u201Cfalse\u201D speech representations. On the other hand, BEiT is a vision model pretrained on a masked image modeling task which masks some of the image patches and requires the model to predict the masked patches (similar to the masked language modeling objective)."),Sh=f(),Fe=a("h2"),Mt=a("a"),ll=a("span"),d(na.$$.fragment),yv=f(),il=a("span"),Av=r("R"),Ch=f(),Ie=a("h3"),Rt=a("a"),hl=a("span"),d(oa.$$.fragment),qv=f(),pl=a("span"),xv=r("recurrent neural network"),Nh=f(),xn=a("p"),jv=r("A type of model that uses a loop over a layer to process texts."),zh=f(),Be=a("h2"),Vt=a("a"),fl=a("span"),d(ra.$$.fragment),Pv=f(),cl=a("span"),Tv=r("S"),Dh=f(),He=a("h3"),Lt=a("a"),dl=a("span"),d(la.$$.fragment),Sv=f(),ul=a("span"),Cv=r("sampling rate"),Fh=f(),jn=a("p"),Nv=r("A measurement in hertz of the number of samples (the audio signal) taken per second. The sampling rate is a result of discretizing a continuous signal such as speech."),Ih=f(),Oe=a("h3"),Gt=a("a"),ml=a("span"),d(ia.$$.fragment),zv=f(),vl=a("span"),Dv=r("self-attention"),Bh=f(),Pn=a("p"),Fv=r("Each element of the input finds out which other elements of the input they should attend to."),Hh=f(),Me=a("h3"),Wt=a("a"),gl=a("span"),d(ha.$$.fragment),Iv=f(),bl=a("span"),Bv=r("sequence-to-sequence (seq2seq)"),Oh=f(),M=a("p"),Hv=r(`Models that generate a new sequence from an input, like translation models, or summarization models (such as
`),Tn=a("a"),Ov=r("Bart"),Mv=r(" or "),Sn=a("a"),Rv=r("T5"),Vv=r(")."),Mh=f(),Re=a("h3"),Ut=a("a"),_l=a("span"),d(pa.$$.fragment),Lv=f(),wl=a("span"),Gv=r("stride"),Rh=f(),R=a("p"),Wv=r("In "),Cn=a("a"),Uv=r("convolution"),Xv=r(" or "),Nn=a("a"),Yv=r("pooling"),Qv=r(", the stride refers to the distance the kernel is moved over a matrix. A stride of 1 means the kernel is moved one pixel over at a time, and a stride of 2 means the kernel is moved two pixels over at a time."),Vh=f(),Ve=a("h2"),Xt=a("a"),$l=a("span"),d(fa.$$.fragment),Jv=f(),kl=a("span"),Kv=r("T"),Lh=f(),Le=a("h3"),Yt=a("a"),El=a("span"),d(ca.$$.fragment),Zv=f(),yl=a("span"),e1=r("token"),Gh=f(),zn=a("p"),t1=r(`A part of a sentence, usually a word, but can also be a subword (non-common words are often split in subwords) or a
punctuation symbol.`),Wh=f(),Ge=a("h3"),Qt=a("a"),Al=a("span"),d(da.$$.fragment),s1=f(),ql=a("span"),a1=r("token Type IDs"),Uh=f(),Dn=a("p"),n1=r("Some models\u2019 purpose is to do classification on pairs of sentences or question answering."),Xh=f(),d(ua.$$.fragment),Yh=f(),V=a("p"),o1=r(`These require two different sequences to be joined in a single \u201Cinput_ids\u201D entry, which usually is performed with the
help of special tokens, such as the classifier (`),xl=a("code"),r1=r("[CLS]"),l1=r(") and separator ("),jl=a("code"),i1=r("[SEP]"),h1=r(`) tokens. For example, the BERT model
builds its two sequence input as such:`),Qh=f(),d(ma.$$.fragment),Jh=f(),Jt=a("p"),p1=r("We can use our tokenizer to automatically generate such a sentence by passing the two sequences to "),Pl=a("code"),f1=r("tokenizer"),c1=r(` as two
arguments (and not a list, like before) like this:`),Kh=f(),d(va.$$.fragment),Zh=f(),Fn=a("p"),d1=r("which will return:"),ep=f(),d(ga.$$.fragment),tp=f(),In=a("p"),u1=r(`This is enough for some models to understand where one sequence ends and where another begins. However, other models,
such as BERT, also deploy token type IDs (also called segment IDs). They are represented as a binary mask identifying
the two types of sequence in the model.`),sp=f(),Bn=a("p"),m1=r("The tokenizer returns this mask as the \u201Ctoken_type_ids\u201D entry:"),ap=f(),d(ba.$$.fragment),np=f(),L=a("p"),v1=r("The first sequence, the \u201Ccontext\u201D used for the question, has all its tokens represented by a "),Tl=a("code"),g1=r("0"),b1=r(`, whereas the second
sequence, corresponding to the \u201Cquestion\u201D, has all its tokens represented by a `),Sl=a("code"),_1=r("1"),w1=r("."),op=f(),G=a("p"),$1=r("Some models, like "),Hn=a("a"),k1=r("XLNetModel"),E1=r(" use an additional token represented by a "),Cl=a("code"),y1=r("2"),A1=r("."),rp=f(),We=a("h3"),Kt=a("a"),Nl=a("span"),d(_a.$$.fragment),q1=f(),zl=a("span"),x1=r("transformer"),lp=f(),On=a("p"),j1=r("Self-attention based deep learning model architecture."),this.h()},l(e){const i=j$('[data-svelte="svelte-1phssyn"]',document.head);y=n(i,"META",{name:!0,content:!0}),i.forEach(t),Ue=c(e),x=n(e,"H1",{class:!0});var wa=o(x);S=n(wa,"A",{id:!0,class:!0,href:!0});var C1=o(S);eo=n(C1,"SPAN",{});var N1=o(eo);u(as.$$.fragment,N1),N1.forEach(t),C1.forEach(t),df=c(wa),to=n(wa,"SPAN",{});var z1=o(to);uf=l(z1,"Glossary"),z1.forEach(t),wa.forEach(t),Il=c(e),Ea=n(e,"P",{});var D1=o(Ea);mf=l(D1,`This glossary defines general machine learning and \u{1F917} Transformers terms to help you better understand the
documentation.`),D1.forEach(t),Bl=c(e),Y=n(e,"H2",{class:!0});var hp=o(Y);Xe=n(hp,"A",{id:!0,class:!0,href:!0});var F1=o(Xe);so=n(F1,"SPAN",{});var I1=o(so);u(ns.$$.fragment,I1),I1.forEach(t),F1.forEach(t),vf=c(hp),ao=n(hp,"SPAN",{});var B1=o(ao);gf=l(B1,"A"),B1.forEach(t),hp.forEach(t),Hl=c(e),Q=n(e,"H3",{class:!0});var pp=o(Q);Ye=n(pp,"A",{id:!0,class:!0,href:!0});var H1=o(Ye);no=n(H1,"SPAN",{});var O1=o(no);u(os.$$.fragment,O1),O1.forEach(t),H1.forEach(t),bf=c(pp),oo=n(pp,"SPAN",{});var M1=o(oo);_f=l(M1,"attention mask"),M1.forEach(t),pp.forEach(t),Ol=c(e),ya=n(e,"P",{});var R1=o(ya);wf=l(R1,"The attention mask is an optional argument used when batching sequences together."),R1.forEach(t),Ml=c(e),u(rs.$$.fragment,e),Rl=c(e),Aa=n(e,"P",{});var V1=o(Aa);$f=l(V1,"This argument indicates to the model which tokens should be attended to, and which should not."),V1.forEach(t),Vl=c(e),qa=n(e,"P",{});var L1=o(qa);kf=l(L1,"For example, consider these two sequences:"),L1.forEach(t),Ll=c(e),u(ls.$$.fragment,e),Gl=c(e),xa=n(e,"P",{});var G1=o(xa);Ef=l(G1,"The encoded versions have different lengths:"),G1.forEach(t),Wl=c(e),u(is.$$.fragment,e),Ul=c(e),ja=n(e,"P",{});var W1=o(ja);yf=l(W1,`Therefore, we can\u2019t put them together in the same tensor as-is. The first sequence needs to be padded up to the length
of the second one, or the second one needs to be truncated down to the length of the first one.`),W1.forEach(t),Xl=c(e),Pa=n(e,"P",{});var U1=o(Pa);Af=l(U1,`In the first case, the list of IDs will be extended by the padding indices. We can pass a list to the tokenizer and ask
it to pad like this:`),U1.forEach(t),Yl=c(e),u(hs.$$.fragment,e),Ql=c(e),Ta=n(e,"P",{});var X1=o(Ta);qf=l(X1,"We can see that 0s have been added on the right of the first sentence to make it the same length as the second one:"),X1.forEach(t),Jl=c(e),u(ps.$$.fragment,e),Kl=c(e),N=n(e,"P",{});var Zt=o(N);xf=l(Zt,`This can then be converted into a tensor in PyTorch or TensorFlow. The attention mask is a binary tensor indicating the
position of the padded indices so that the model does not attend to them. For the `),Sa=n(Zt,"A",{href:!0});var Y1=o(Sa);jf=l(Y1,"BertTokenizer"),Y1.forEach(t),Pf=l(Zt,", "),ro=n(Zt,"CODE",{});var Q1=o(ro);Tf=l(Q1,"1"),Q1.forEach(t),Sf=l(Zt,` indicates a
value that should be attended to, while `),lo=n(Zt,"CODE",{});var J1=o(lo);Cf=l(J1,"0"),J1.forEach(t),Nf=l(Zt,` indicates a padded value. This attention mask is in the dictionary returned
by the tokenizer under the key \u201Cattention_mask\u201D:`),Zt.forEach(t),Zl=c(e),u(fs.$$.fragment,e),ei=c(e),J=n(e,"H3",{class:!0});var fp=o(J);Qe=n(fp,"A",{id:!0,class:!0,href:!0});var K1=o(Qe);io=n(K1,"SPAN",{});var Z1=o(io);u(cs.$$.fragment,Z1),Z1.forEach(t),K1.forEach(t),zf=c(fp),ho=n(fp,"SPAN",{});var eg=o(ho);Df=l(eg,"autoencoding models"),eg.forEach(t),fp.forEach(t),ti=c(e),ds=n(e,"P",{});var P1=o(ds);Ff=l(P1,"see "),Ca=n(P1,"A",{href:!0});var tg=o(Ca);If=l(tg,"masked language modeling"),tg.forEach(t),P1.forEach(t),si=c(e),K=n(e,"H3",{class:!0});var cp=o(K);Je=n(cp,"A",{id:!0,class:!0,href:!0});var sg=o(Je);po=n(sg,"SPAN",{});var ag=o(po);u(us.$$.fragment,ag),ag.forEach(t),sg.forEach(t),Bf=c(cp),fo=n(cp,"SPAN",{});var ng=o(fo);Hf=l(ng,"autoregressive models"),ng.forEach(t),cp.forEach(t),ai=c(e),ms=n(e,"P",{});var T1=o(ms);Of=l(T1,"see "),Na=n(T1,"A",{href:!0});var og=o(Na);Mf=l(og,"causal language modeling"),og.forEach(t),T1.forEach(t),ni=c(e),Z=n(e,"H2",{class:!0});var dp=o(Z);Ke=n(dp,"A",{id:!0,class:!0,href:!0});var rg=o(Ke);co=n(rg,"SPAN",{});var lg=o(co);u(vs.$$.fragment,lg),lg.forEach(t),rg.forEach(t),Rf=c(dp),uo=n(dp,"SPAN",{});var ig=o(uo);Vf=l(ig,"B"),ig.forEach(t),dp.forEach(t),oi=c(e),ee=n(e,"H3",{class:!0});var up=o(ee);Ze=n(up,"A",{id:!0,class:!0,href:!0});var hg=o(Ze);mo=n(hg,"SPAN",{});var pg=o(mo);u(gs.$$.fragment,pg),pg.forEach(t),hg.forEach(t),Lf=c(up),vo=n(up,"SPAN",{});var fg=o(vo);Gf=l(fg,"backbone"),fg.forEach(t),up.forEach(t),ri=c(e),j=n(e,"P",{});var W=o(j);Wf=l(W,"The backbone is the network (embeddings and layers) that outputs the raw hidden states or features. It is usually connected to a "),za=n(W,"A",{href:!0});var cg=o(za);Uf=l(cg,"head"),cg.forEach(t),Xf=l(W," which accepts the features as its input to make a prediction. For example, "),Da=n(W,"A",{href:!0});var dg=o(Da);Yf=l(dg,"ViTModel"),dg.forEach(t),Qf=l(W," is a backbone without a specific head on top. Other models can also use "),go=n(W,"CODE",{});var ug=o(go);Jf=l(ug,"VitModel"),ug.forEach(t),Kf=l(W," as a backbone such as "),Fa=n(W,"A",{href:!0});var mg=o(Fa);Zf=l(mg,"DPT"),mg.forEach(t),ec=l(W,"."),W.forEach(t),li=c(e),te=n(e,"H2",{class:!0});var mp=o(te);et=n(mp,"A",{id:!0,class:!0,href:!0});var vg=o(et);bo=n(vg,"SPAN",{});var gg=o(bo);u(bs.$$.fragment,gg),gg.forEach(t),vg.forEach(t),tc=c(mp),_o=n(mp,"SPAN",{});var bg=o(_o);sc=l(bg,"C"),bg.forEach(t),mp.forEach(t),ii=c(e),se=n(e,"H3",{class:!0});var vp=o(se);tt=n(vp,"A",{id:!0,class:!0,href:!0});var _g=o(tt);wo=n(_g,"SPAN",{});var wg=o(wo);u(_s.$$.fragment,wg),wg.forEach(t),_g.forEach(t),ac=c(vp),$o=n(vp,"SPAN",{});var $g=o($o);nc=l($g,"channel"),$g.forEach(t),vp.forEach(t),hi=c(e),k=n(e,"P",{});var T=o(k);oc=l(T,"Color images are made up of some combination of values in three channels - red, green, and blue (RGB) - and grayscale images only have one channel. In \u{1F917} Transformers, the channel can be the first or last dimension of an image\u2019s tensor: ["),ko=n(T,"CODE",{});var kg=o(ko);rc=l(kg,"n_channels"),kg.forEach(t),lc=l(T,", "),Eo=n(T,"CODE",{});var Eg=o(Eo);ic=l(Eg,"height"),Eg.forEach(t),hc=l(T,", "),yo=n(T,"CODE",{});var yg=o(yo);pc=l(yg,"width"),yg.forEach(t),fc=l(T,"] or ["),Ao=n(T,"CODE",{});var Ag=o(Ao);cc=l(Ag,"height"),Ag.forEach(t),dc=l(T,", "),qo=n(T,"CODE",{});var qg=o(qo);uc=l(qg,"width"),qg.forEach(t),mc=l(T,", "),xo=n(T,"CODE",{});var xg=o(xo);vc=l(xg,"n_channels"),xg.forEach(t),gc=l(T,"]."),T.forEach(t),pi=c(e),ae=n(e,"H3",{class:!0});var gp=o(ae);st=n(gp,"A",{id:!0,class:!0,href:!0});var jg=o(st);jo=n(jg,"SPAN",{});var Pg=o(jo);u(ws.$$.fragment,Pg),Pg.forEach(t),jg.forEach(t),bc=c(gp),Po=n(gp,"SPAN",{});var Tg=o(Po);_c=l(Tg,"causal language modeling"),Tg.forEach(t),gp.forEach(t),fi=c(e),Ia=n(e,"P",{});var Sg=o(Ia);wc=l(Sg,`A pretraining task where the model reads the texts in order and has to predict the next word. It\u2019s usually done by
reading the whole sentence but using a mask inside the model to hide the future tokens at a certain timestep.`),Sg.forEach(t),ci=c(e),ne=n(e,"H3",{class:!0});var bp=o(ne);at=n(bp,"A",{id:!0,class:!0,href:!0});var Cg=o(at);To=n(Cg,"SPAN",{});var Ng=o(To);u($s.$$.fragment,Ng),Ng.forEach(t),Cg.forEach(t),$c=c(bp),So=n(bp,"SPAN",{});var zg=o(So);kc=l(zg,"connectionist temporal classification (CTC)"),zg.forEach(t),bp.forEach(t),di=c(e),Ba=n(e,"P",{});var Dg=o(Ba);Ec=l(Dg,"An algorithm which allows a model to learn without knowing exactly how the input and output are aligned; CTC calculates the distribution of all possible outputs for a given input and chooses the most likely output from it. CTC is commonly used in speech recognition tasks because speech doesn\u2019t always cleanly align with the transcript for a variety of reasons such as a speaker\u2019s different speech rates."),Dg.forEach(t),ui=c(e),oe=n(e,"H3",{class:!0});var _p=o(oe);nt=n(_p,"A",{id:!0,class:!0,href:!0});var Fg=o(nt);Co=n(Fg,"SPAN",{});var Ig=o(Co);u(ks.$$.fragment,Ig),Ig.forEach(t),Fg.forEach(t),yc=c(_p),No=n(_p,"SPAN",{});var Bg=o(No);Ac=l(Bg,"convolution"),Bg.forEach(t),_p.forEach(t),mi=c(e),Ha=n(e,"P",{});var Hg=o(Ha);qc=l(Hg,"A type of layer in a neural network where the input matrix is multiplied element-wise by a smaller matrix (kernel or filter) and the values are summed up in a new matrix. This is known as a convolutional operation which is repeated over the entire input matrix. Each operation is applied to a different segment of the input matrix. Convolutional neural networks (CNNs) are commonly used in computer vision."),Hg.forEach(t),vi=c(e),re=n(e,"H2",{class:!0});var wp=o(re);ot=n(wp,"A",{id:!0,class:!0,href:!0});var Og=o(ot);zo=n(Og,"SPAN",{});var Mg=o(zo);u(Es.$$.fragment,Mg),Mg.forEach(t),Og.forEach(t),xc=c(wp),Do=n(wp,"SPAN",{});var Rg=o(Do);jc=l(Rg,"D"),Rg.forEach(t),wp.forEach(t),gi=c(e),le=n(e,"H3",{class:!0});var $p=o(le);rt=n($p,"A",{id:!0,class:!0,href:!0});var Vg=o(rt);Fo=n(Vg,"SPAN",{});var Lg=o(Fo);u(ys.$$.fragment,Lg),Lg.forEach(t),Vg.forEach(t),Pc=c($p),Io=n($p,"SPAN",{});var Gg=o(Io);Tc=l(Gg,"decoder input IDs"),Gg.forEach(t),$p.forEach(t),bi=c(e),Oa=n(e,"P",{});var Wg=o(Oa);Sc=l(Wg,`This input is specific to encoder-decoder models, and contains the input IDs that will be fed to the decoder. These
inputs should be used for sequence to sequence tasks, such as translation or summarization, and are usually built in a
way specific to each model.`),Wg.forEach(t),_i=c(e),z=n(e,"P",{});var es=o(z);Cc=l(es,"Most encoder-decoder models (BART, T5) create their "),Bo=n(es,"CODE",{});var Ug=o(Bo);Nc=l(Ug,"decoder_input_ids"),Ug.forEach(t),zc=l(es," on their own from the "),Ho=n(es,"CODE",{});var Xg=o(Ho);Dc=l(Xg,"labels"),Xg.forEach(t),Fc=l(es,`. In such models,
passing the `),Oo=n(es,"CODE",{});var Yg=o(Oo);Ic=l(Yg,"labels"),Yg.forEach(t),Bc=l(es," is the preferred way to handle training."),es.forEach(t),wi=c(e),Ma=n(e,"P",{});var Qg=o(Ma);Hc=l(Qg,"Please check each model\u2019s docs to see how they handle these input IDs for sequence to sequence training."),Qg.forEach(t),$i=c(e),ie=n(e,"H3",{class:!0});var kp=o(ie);lt=n(kp,"A",{id:!0,class:!0,href:!0});var Jg=o(lt);Mo=n(Jg,"SPAN",{});var Kg=o(Mo);u(As.$$.fragment,Kg),Kg.forEach(t),Jg.forEach(t),Oc=c(kp),Ro=n(kp,"SPAN",{});var Zg=o(Ro);Mc=l(Zg,"deep learning"),Zg.forEach(t),kp.forEach(t),ki=c(e),Ra=n(e,"P",{});var eb=o(Ra);Rc=l(eb,"Machine learning algorithms which uses neural networks with several layers."),eb.forEach(t),Ei=c(e),he=n(e,"H2",{class:!0});var Ep=o(he);it=n(Ep,"A",{id:!0,class:!0,href:!0});var tb=o(it);Vo=n(tb,"SPAN",{});var sb=o(Vo);u(qs.$$.fragment,sb),sb.forEach(t),tb.forEach(t),Vc=c(Ep),Lo=n(Ep,"SPAN",{});var ab=o(Lo);Lc=l(ab,"F"),ab.forEach(t),Ep.forEach(t),yi=c(e),pe=n(e,"H3",{class:!0});var yp=o(pe);ht=n(yp,"A",{id:!0,class:!0,href:!0});var nb=o(ht);Go=n(nb,"SPAN",{});var ob=o(Go);u(xs.$$.fragment,ob),ob.forEach(t),nb.forEach(t),Gc=c(yp),Wo=n(yp,"SPAN",{});var rb=o(Wo);Wc=l(rb,"feed forward chunking"),rb.forEach(t),yp.forEach(t),Ai=c(e),pt=n(e,"P",{});var Ap=o(pt);Uc=l(Ap,`In each residual attention block in transformers the self-attention layer is usually followed by 2 feed forward layers.
The intermediate embedding size of the feed forward layers is often bigger than the hidden size of the model (e.g., for
`),Uo=n(Ap,"CODE",{});var lb=o(Uo);Xc=l(lb,"bert-base-uncased"),lb.forEach(t),Yc=l(Ap,")."),Ap.forEach(t),qi=c(e),w=n(e,"P",{});var E=o(w);Qc=l(E,"For an input of size "),Xo=n(E,"CODE",{});var ib=o(Xo);Jc=l(ib,"[batch_size, sequence_length]"),ib.forEach(t),Kc=l(E,`, the memory required to store the intermediate feed forward
embeddings `),Yo=n(E,"CODE",{});var hb=o(Yo);Zc=l(hb,"[batch_size, sequence_length, config.intermediate_size]"),hb.forEach(t),ed=l(E,` can account for a large fraction of the memory
use. The authors of `),js=n(E,"A",{href:!0,rel:!0});var pb=o(js);td=l(pb,"Reformer: The Efficient Transformer"),pb.forEach(t),sd=l(E,` noticed that since the
computation is independent of the `),Qo=n(E,"CODE",{});var fb=o(Qo);ad=l(fb,"sequence_length"),fb.forEach(t),nd=l(E,` dimension, it is mathematically equivalent to compute the output
embeddings of both feed forward layers `),Jo=n(E,"CODE",{});var cb=o(Jo);od=l(cb,"[batch_size, config.hidden_size]_0, ..., [batch_size, config.hidden_size]_n"),cb.forEach(t),rd=l(E,`
individually and concat them afterward to `),Ko=n(E,"CODE",{});var db=o(Ko);ld=l(db,"[batch_size, sequence_length, config.hidden_size]"),db.forEach(t),id=l(E," with "),Zo=n(E,"CODE",{});var ub=o(Zo);hd=l(ub,"n = sequence_length"),ub.forEach(t),pd=l(E,`, which trades increased computation time against reduced memory use, but yields a mathematically
`),er=n(E,"STRONG",{});var mb=o(er);fd=l(mb,"equivalent"),mb.forEach(t),cd=l(E," result."),E.forEach(t),xi=c(e),D=n(e,"P",{});var ts=o(D);dd=l(ts,"For models employing the function "),Va=n(ts,"A",{href:!0});var vb=o(Va);ud=l(vb,"apply_chunking_to_forward()"),vb.forEach(t),md=l(ts,", the "),tr=n(ts,"CODE",{});var gb=o(tr);vd=l(gb,"chunk_size"),gb.forEach(t),gd=l(ts,` defines the number of output
embeddings that are computed in parallel and thus defines the trade-off between memory and time complexity. If
`),sr=n(ts,"CODE",{});var bb=o(sr);bd=l(bb,"chunk_size"),bb.forEach(t),_d=l(ts," is set to 0, no feed forward chunking is done."),ts.forEach(t),ji=c(e),fe=n(e,"H2",{class:!0});var qp=o(fe);ft=n(qp,"A",{id:!0,class:!0,href:!0});var _b=o(ft);ar=n(_b,"SPAN",{});var wb=o(ar);u(Ps.$$.fragment,wb),wb.forEach(t),_b.forEach(t),wd=c(qp),nr=n(qp,"SPAN",{});var $b=o(nr);$d=l($b,"H"),$b.forEach(t),qp.forEach(t),Pi=c(e),ce=n(e,"H3",{class:!0});var xp=o(ce);ct=n(xp,"A",{id:!0,class:!0,href:!0});var kb=o(ct);or=n(kb,"SPAN",{});var Eb=o(or);u(Ts.$$.fragment,Eb),Eb.forEach(t),kb.forEach(t),kd=c(xp),rr=n(xp,"SPAN",{});var yb=o(rr);Ed=l(yb,"head"),yb.forEach(t),xp.forEach(t),Ti=c(e),La=n(e,"P",{});var Ab=o(La);yd=l(Ab,"The model head refers to the last layer of a neural network that accepts the raw hidden states and projects them onto a different dimension. There is a different model head for each task. For example:"),Ab.forEach(t),Si=c(e),I=n(e,"UL",{});var Mn=o(I);dt=n(Mn,"LI",{});var Dl=o(dt);Ga=n(Dl,"A",{href:!0});var qb=o(Ga);Ad=l(qb,"GPT2ForSequenceClassification"),qb.forEach(t),qd=l(Dl," is a sequence classification head - a linear layer - on top of the base "),Wa=n(Dl,"A",{href:!0});var xb=o(Wa);xd=l(xb,"GPT2Model"),xb.forEach(t),jd=l(Dl,"."),Dl.forEach(t),Pd=c(Mn),B=n(Mn,"LI",{});var $a=o(B);Ua=n($a,"A",{href:!0});var jb=o(Ua);Td=l(jb,"ViTForImageClassification"),jb.forEach(t),Sd=l($a," is an image classification head - a linear layer on top of the final hidden state of the "),lr=n($a,"CODE",{});var Pb=o(lr);Cd=l(Pb,"CLS"),Pb.forEach(t),Nd=l($a," token - on top of the base "),Xa=n($a,"A",{href:!0});var Tb=o(Xa);zd=l(Tb,"ViTModel"),Tb.forEach(t),Dd=l($a,"."),$a.forEach(t),Fd=c(Mn),H=n(Mn,"LI",{});var ka=o(H);Ya=n(ka,"A",{href:!0});var Sb=o(Ya);Id=l(Sb,"Wav2Vec2ForCTC"),Sb.forEach(t),Bd=l(ka," ia a language modeling head with "),Qa=n(ka,"A",{href:!0});var Cb=o(Qa);Hd=l(Cb,"CTC"),Cb.forEach(t),Od=l(ka," on top of the base "),Ja=n(ka,"A",{href:!0});var Nb=o(Ja);Md=l(Nb,"Wav2Vec2Model"),Nb.forEach(t),Rd=l(ka,"."),ka.forEach(t),Mn.forEach(t),Ci=c(e),de=n(e,"H2",{class:!0});var jp=o(de);ut=n(jp,"A",{id:!0,class:!0,href:!0});var zb=o(ut);ir=n(zb,"SPAN",{});var Db=o(ir);u(Ss.$$.fragment,Db),Db.forEach(t),zb.forEach(t),Vd=c(jp),hr=n(jp,"SPAN",{});var Fb=o(hr);Ld=l(Fb,"I"),Fb.forEach(t),jp.forEach(t),Ni=c(e),ue=n(e,"H3",{class:!0});var Pp=o(ue);mt=n(Pp,"A",{id:!0,class:!0,href:!0});var Ib=o(mt);pr=n(Ib,"SPAN",{});var Bb=o(pr);u(Cs.$$.fragment,Bb),Bb.forEach(t),Ib.forEach(t),Gd=c(Pp),fr=n(Pp,"SPAN",{});var Hb=o(fr);Wd=l(Hb,"image patch"),Hb.forEach(t),Pp.forEach(t),zi=c(e),vt=n(e,"P",{});var Tp=o(vt);Ud=l(Tp,"Vision-based Transformers models split an image into smaller patches which are linearly embedded, and then passed as a sequence to the model. You can find the "),cr=n(Tp,"CODE",{});var Ob=o(cr);Xd=l(Ob,"patch_size"),Ob.forEach(t),Yd=l(Tp," - or resolution - of the model in it\u2019s configuration."),Tp.forEach(t),Di=c(e),me=n(e,"H3",{class:!0});var Sp=o(me);gt=n(Sp,"A",{id:!0,class:!0,href:!0});var Mb=o(gt);dr=n(Mb,"SPAN",{});var Rb=o(dr);u(Ns.$$.fragment,Rb),Rb.forEach(t),Mb.forEach(t),Qd=c(Sp),ur=n(Sp,"SPAN",{});var Vb=o(ur);Jd=l(Vb,"input IDs"),Vb.forEach(t),Sp.forEach(t),Fi=c(e),Ka=n(e,"P",{});var Lb=o(Ka);Kd=l(Lb,`The input ids are often the only required parameters to be passed to the model as input. They are token indices,
numerical representations of tokens building the sequences that will be used as input by the model.`),Lb.forEach(t),Ii=c(e),u(zs.$$.fragment,e),Bi=c(e),bt=n(e,"P",{});var Cp=o(bt);Zd=l(Cp,`Each tokenizer works differently but the underlying mechanism remains the same. Here\u2019s an example using the BERT
tokenizer, which is a `),Ds=n(Cp,"A",{href:!0,rel:!0});var Gb=o(Ds);eu=l(Gb,"WordPiece"),Gb.forEach(t),tu=l(Cp," tokenizer:"),Cp.forEach(t),Hi=c(e),u(Fs.$$.fragment,e),Oi=c(e),Za=n(e,"P",{});var Wb=o(Za);su=l(Wb,"The tokenizer takes care of splitting the sequence into tokens available in the tokenizer vocabulary."),Wb.forEach(t),Mi=c(e),u(Is.$$.fragment,e),Ri=c(e),en=n(e,"P",{});var Ub=o(en);au=l(Ub,`The tokens are either words or subwords. Here for instance, \u201CVRAM\u201D wasn\u2019t in the model vocabulary, so it\u2019s been split
in \u201CV\u201D, \u201CRA\u201D and \u201CM\u201D. To indicate those tokens are not separate words but parts of the same word, a double-hash prefix
is added for \u201CRA\u201D and \u201CM\u201D:`),Ub.forEach(t),Vi=c(e),u(Bs.$$.fragment,e),Li=c(e),_t=n(e,"P",{});var Np=o(_t);nu=l(Np,`These tokens can then be converted into IDs which are understandable by the model. This can be done by directly feeding
the sentence to the tokenizer, which leverages the Rust implementation of `),Hs=n(Np,"A",{href:!0,rel:!0});var Xb=o(Hs);ou=l(Xb,`\u{1F917}
Tokenizers`),Xb.forEach(t),ru=l(Np," for peak performance."),Np.forEach(t),Gi=c(e),u(Os.$$.fragment,e),Wi=c(e),wt=n(e,"P",{});var zp=o(wt);lu=l(zp,`The tokenizer returns a dictionary with all the arguments necessary for its corresponding model to work properly. The
token indices are under the key `),mr=n(zp,"CODE",{});var Yb=o(mr);iu=l(Yb,"input_ids"),Yb.forEach(t),hu=l(zp,":"),zp.forEach(t),Ui=c(e),u(Ms.$$.fragment,e),Xi=c(e),tn=n(e,"P",{});var Qb=o(tn);pu=l(Qb,`Note that the tokenizer automatically adds \u201Cspecial tokens\u201D (if the associated model relies on them) which are special
IDs the model sometimes uses.`),Qb.forEach(t),Yi=c(e),sn=n(e,"P",{});var Jb=o(sn);fu=l(Jb,"If we decode the previous sequence of ids,"),Jb.forEach(t),Qi=c(e),u(Rs.$$.fragment,e),Ji=c(e),an=n(e,"P",{});var Kb=o(an);cu=l(Kb,"we will see"),Kb.forEach(t),Ki=c(e),u(Vs.$$.fragment,e),Zi=c(e),$t=n(e,"P",{});var Dp=o($t);du=l(Dp,"because this is the way a "),nn=n(Dp,"A",{href:!0});var Zb=o(nn);uu=l(Zb,"BertModel"),Zb.forEach(t),mu=l(Dp," is going to expect its inputs."),Dp.forEach(t),eh=c(e),ve=n(e,"H2",{class:!0});var Fp=o(ve);kt=n(Fp,"A",{id:!0,class:!0,href:!0});var e_=o(kt);vr=n(e_,"SPAN",{});var t_=o(vr);u(Ls.$$.fragment,t_),t_.forEach(t),e_.forEach(t),vu=c(Fp),gr=n(Fp,"SPAN",{});var s_=o(gr);gu=l(s_,"L"),s_.forEach(t),Fp.forEach(t),th=c(e),ge=n(e,"H3",{class:!0});var Ip=o(ge);Et=n(Ip,"A",{id:!0,class:!0,href:!0});var a_=o(Et);br=n(a_,"SPAN",{});var n_=o(br);u(Gs.$$.fragment,n_),n_.forEach(t),a_.forEach(t),bu=c(Ip),_r=n(Ip,"SPAN",{});var o_=o(_r);_u=l(o_,"labels"),o_.forEach(t),Ip.forEach(t),sh=c(e),on=n(e,"P",{});var r_=o(on);wu=l(r_,`The labels are an optional argument which can be passed in order for the model to compute the loss itself. These labels
should be the expected prediction of the model: it will use the standard loss in order to compute the loss between its
predictions and the expected value (the label).`),r_.forEach(t),ah=c(e),rn=n(e,"P",{});var l_=o(rn);$u=l(l_,"These labels are different according to the model head, for example:"),l_.forEach(t),nh=c(e),$=n(e,"UL",{});var A=o($);be=n(A,"LI",{});var Rn=o(be);ku=l(Rn,"For sequence classification models, ("),ln=n(Rn,"A",{href:!0});var i_=o(ln);Eu=l(i_,"BertForSequenceClassification"),i_.forEach(t),yu=l(Rn,`), the model expects a tensor of dimension
`),wr=n(Rn,"CODE",{});var h_=o(wr);Au=l(h_,"(batch_size)"),h_.forEach(t),qu=l(Rn," with each value of the batch corresponding to the expected label of the entire sequence."),Rn.forEach(t),xu=c(A),_e=n(A,"LI",{});var Vn=o(_e);ju=l(Vn,"For token classification models, ("),hn=n(Vn,"A",{href:!0});var p_=o(hn);Pu=l(p_,"BertForTokenClassification"),p_.forEach(t),Tu=l(Vn,`), the model expects a tensor of dimension
`),$r=n(Vn,"CODE",{});var f_=o($r);Su=l(f_,"(batch_size, seq_length)"),f_.forEach(t),Cu=l(Vn," with each value corresponding to the expected label of each individual token."),Vn.forEach(t),Nu=c(A),we=n(A,"LI",{});var Ln=o(we);zu=l(Ln,"For masked language modeling, ("),pn=n(Ln,"A",{href:!0});var c_=o(pn);Du=l(c_,"BertForMaskedLM"),c_.forEach(t),Fu=l(Ln,"), the model expects a tensor of dimension "),kr=n(Ln,"CODE",{});var d_=o(kr);Iu=l(d_,"(batch_size, seq_length)"),d_.forEach(t),Bu=l(Ln,` with each value corresponding to the expected label of each individual token: the labels being the token
ID for the masked token, and values to be ignored for the rest (usually -100).`),Ln.forEach(t),Hu=c(A),C=n(A,"LI",{});var U=o(C);Ou=l(U,"For sequence to sequence tasks, ("),fn=n(U,"A",{href:!0});var u_=o(fn);Mu=l(u_,"BartForConditionalGeneration"),u_.forEach(t),Ru=l(U,", "),cn=n(U,"A",{href:!0});var m_=o(cn);Vu=l(m_,"MBartForConditionalGeneration"),m_.forEach(t),Lu=l(U,`), the model
expects a tensor of dimension `),Er=n(U,"CODE",{});var v_=o(Er);Gu=l(v_,"(batch_size, tgt_seq_length)"),v_.forEach(t),Wu=l(U,` with each value corresponding to the target sequences
associated with each input sequence. During training, both BART and T5 will make the appropriate
`),yr=n(U,"CODE",{});var g_=o(yr);Uu=l(g_,"decoder_input_ids"),g_.forEach(t),Xu=l(U,` and decoder attention masks internally. They usually do not need to be supplied. This does not
apply to models leveraging the Encoder-Decoder framework.`),U.forEach(t),Yu=c(A),$e=n(A,"LI",{});var Gn=o($e);Qu=l(Gn,"For image classification models, ("),dn=n(Gn,"A",{href:!0});var b_=o(dn);Ju=l(b_,"ViTForImageClassification"),b_.forEach(t),Ku=l(Gn,`), the model expects a tensor of dimension
`),Ar=n(Gn,"CODE",{});var __=o(Ar);Zu=l(__,"(batch_size)"),__.forEach(t),em=l(Gn," with each value of the batch corresponding to the expected label of each individual image."),Gn.forEach(t),tm=c(A),ke=n(A,"LI",{});var Wn=o(ke);sm=l(Wn,"For semantic segmentation models, ("),un=n(Wn,"A",{href:!0});var w_=o(un);am=l(w_,"SegformerForSemanticSegmentation"),w_.forEach(t),nm=l(Wn,`), the model expects a tensor of dimension
`),qr=n(Wn,"CODE",{});var $_=o(qr);om=l($_,"(batch_size, height, width)"),$_.forEach(t),rm=l(Wn," with each value of the batch corresponding to the expected label of each individual pixel."),Wn.forEach(t),lm=c(A),F=n(A,"LI",{});var ss=o(F);im=l(ss,"For object detection models, ("),mn=n(ss,"A",{href:!0});var k_=o(mn);hm=l(k_,"DetrForObjectDetection"),k_.forEach(t),pm=l(ss,`), the model expects a list of dictionaries with a
`),xr=n(ss,"CODE",{});var E_=o(xr);fm=l(E_,"class_labels"),E_.forEach(t),cm=l(ss," and "),jr=n(ss,"CODE",{});var y_=o(jr);dm=l(y_,"boxes"),y_.forEach(t),um=l(ss," key where each value of the batch corresponds to the expected label and number of bounding boxes of each individual image."),ss.forEach(t),mm=c(A),Ee=n(A,"LI",{});var Un=o(Ee);vm=l(Un,"For automatic speech recognition models, ("),vn=n(Un,"A",{href:!0});var A_=o(vn);gm=l(A_,"Wav2Vec2ForCTC"),A_.forEach(t),bm=l(Un,"), the model expects a tensor of dimension "),Pr=n(Un,"CODE",{});var q_=o(Pr);_m=l(q_,"(batch_size, target_length)"),q_.forEach(t),wm=l(Un," with each value corresponding to the expected label of each individual token."),Un.forEach(t),A.forEach(t),oh=c(e),u(yt.$$.fragment,e),rh=c(e),At=n(e,"P",{});var Bp=o(At);$m=l(Bp,"The base models ("),gn=n(Bp,"A",{href:!0});var x_=o(gn);km=l(x_,"BertModel"),x_.forEach(t),Em=l(Bp,`) do not accept labels, as these are the base transformer models, simply outputting
features.`),Bp.forEach(t),lh=c(e),ye=n(e,"H2",{class:!0});var Hp=o(ye);qt=n(Hp,"A",{id:!0,class:!0,href:!0});var j_=o(qt);Tr=n(j_,"SPAN",{});var P_=o(Tr);u(Ws.$$.fragment,P_),P_.forEach(t),j_.forEach(t),ym=c(Hp),Sr=n(Hp,"SPAN",{});var T_=o(Sr);Am=l(T_,"M"),T_.forEach(t),Hp.forEach(t),ih=c(e),Ae=n(e,"H3",{class:!0});var Op=o(Ae);xt=n(Op,"A",{id:!0,class:!0,href:!0});var S_=o(xt);Cr=n(S_,"SPAN",{});var C_=o(Cr);u(Us.$$.fragment,C_),C_.forEach(t),S_.forEach(t),qm=c(Op),Nr=n(Op,"SPAN",{});var N_=o(Nr);xm=l(N_,"masked language modeling"),N_.forEach(t),Op.forEach(t),hh=c(e),bn=n(e,"P",{});var z_=o(bn);jm=l(z_,`A pretraining task where the model sees a corrupted version of the texts, usually done by
masking some tokens randomly, and has to predict the original text.`),z_.forEach(t),ph=c(e),qe=n(e,"H3",{class:!0});var Mp=o(qe);jt=n(Mp,"A",{id:!0,class:!0,href:!0});var D_=o(jt);zr=n(D_,"SPAN",{});var F_=o(zr);u(Xs.$$.fragment,F_),F_.forEach(t),D_.forEach(t),Pm=c(Mp),Dr=n(Mp,"SPAN",{});var I_=o(Dr);Tm=l(I_,"multimodal"),I_.forEach(t),Mp.forEach(t),fh=c(e),_n=n(e,"P",{});var B_=o(_n);Sm=l(B_,"A task that combines texts with another kind of inputs (for instance images)."),B_.forEach(t),ch=c(e),xe=n(e,"H2",{class:!0});var Rp=o(xe);Pt=n(Rp,"A",{id:!0,class:!0,href:!0});var H_=o(Pt);Fr=n(H_,"SPAN",{});var O_=o(Fr);u(Ys.$$.fragment,O_),O_.forEach(t),H_.forEach(t),Cm=c(Rp),Ir=n(Rp,"SPAN",{});var M_=o(Ir);Nm=l(M_,"N"),M_.forEach(t),Rp.forEach(t),dh=c(e),je=n(e,"H3",{class:!0});var Vp=o(je);Tt=n(Vp,"A",{id:!0,class:!0,href:!0});var R_=o(Tt);Br=n(R_,"SPAN",{});var V_=o(Br);u(Qs.$$.fragment,V_),V_.forEach(t),R_.forEach(t),zm=c(Vp),Hr=n(Vp,"SPAN",{});var L_=o(Hr);Dm=l(L_,"Natural language generation"),L_.forEach(t),Vp.forEach(t),uh=c(e),wn=n(e,"P",{});var G_=o(wn);Fm=l(G_,"All tasks related to generating text (for instance talk with transformers, translation)."),G_.forEach(t),mh=c(e),Pe=n(e,"H3",{class:!0});var Lp=o(Pe);St=n(Lp,"A",{id:!0,class:!0,href:!0});var W_=o(St);Or=n(W_,"SPAN",{});var U_=o(Or);u(Js.$$.fragment,U_),U_.forEach(t),W_.forEach(t),Im=c(Lp),Mr=n(Lp,"SPAN",{});var X_=o(Mr);Bm=l(X_,"Natural language processing"),X_.forEach(t),Lp.forEach(t),vh=c(e),$n=n(e,"P",{});var Y_=o($n);Hm=l(Y_,"A generic way to say \u201Cdeal with texts\u201D."),Y_.forEach(t),gh=c(e),Te=n(e,"H3",{class:!0});var Gp=o(Te);Ct=n(Gp,"A",{id:!0,class:!0,href:!0});var Q_=o(Ct);Rr=n(Q_,"SPAN",{});var J_=o(Rr);u(Ks.$$.fragment,J_),J_.forEach(t),Q_.forEach(t),Om=c(Gp),Vr=n(Gp,"SPAN",{});var K_=o(Vr);Mm=l(K_,"Natural language understanding"),K_.forEach(t),Gp.forEach(t),bh=c(e),kn=n(e,"P",{});var Z_=o(kn);Rm=l(Z_,`All tasks related to understanding what is in a text (for instance classifying the
whole text, individual words).`),Z_.forEach(t),_h=c(e),Se=n(e,"H2",{class:!0});var Wp=o(Se);Nt=n(Wp,"A",{id:!0,class:!0,href:!0});var ew=o(Nt);Lr=n(ew,"SPAN",{});var tw=o(Lr);u(Zs.$$.fragment,tw),tw.forEach(t),ew.forEach(t),Vm=c(Wp),Gr=n(Wp,"SPAN",{});var sw=o(Gr);Lm=l(sw,"P"),sw.forEach(t),Wp.forEach(t),wh=c(e),Ce=n(e,"H3",{class:!0});var Up=o(Ce);zt=n(Up,"A",{id:!0,class:!0,href:!0});var aw=o(zt);Wr=n(aw,"SPAN",{});var nw=o(Wr);u(ea.$$.fragment,nw),nw.forEach(t),aw.forEach(t),Gm=c(Up),Ur=n(Up,"SPAN",{});var ow=o(Ur);Wm=l(ow,"pixel values"),ow.forEach(t),Up.forEach(t),$h=c(e),P=n(e,"P",{});var X=o(P);Um=l(X,"A tensor of the numerical representations of an image that is passed to a model. The pixel values have a shape of ["),Xr=n(X,"CODE",{});var rw=o(Xr);Xm=l(rw,"batch_size"),rw.forEach(t),Ym=l(X,", "),Yr=n(X,"CODE",{});var lw=o(Yr);Qm=l(lw,"num_channels"),lw.forEach(t),Jm=l(X,", "),Qr=n(X,"CODE",{});var iw=o(Qr);Km=l(iw,"height"),iw.forEach(t),Zm=l(X,", "),Jr=n(X,"CODE",{});var hw=o(Jr);ev=l(hw,"width"),hw.forEach(t),tv=l(X,"], and are generated from a feature extractor."),X.forEach(t),kh=c(e),Ne=n(e,"H3",{class:!0});var Xp=o(Ne);Dt=n(Xp,"A",{id:!0,class:!0,href:!0});var pw=o(Dt);Kr=n(pw,"SPAN",{});var fw=o(Kr);u(ta.$$.fragment,fw),fw.forEach(t),pw.forEach(t),sv=c(Xp),Zr=n(Xp,"SPAN",{});var cw=o(Zr);av=l(cw,"pooling"),cw.forEach(t),Xp.forEach(t),Eh=c(e),En=n(e,"P",{});var dw=o(En);nv=l(dw,"An operation that reduces a matrix into a smaller matrix, either by taking the maximum or average of the pooled dimension(s). Pooling layers are commonly found between convolutional layers to downsample the feature representation."),dw.forEach(t),yh=c(e),ze=n(e,"H3",{class:!0});var Yp=o(ze);Ft=n(Yp,"A",{id:!0,class:!0,href:!0});var uw=o(Ft);el=n(uw,"SPAN",{});var mw=o(el);u(sa.$$.fragment,mw),mw.forEach(t),uw.forEach(t),ov=c(Yp),tl=n(Yp,"SPAN",{});var vw=o(tl);rv=l(vw,"position IDs"),vw.forEach(t),Yp.forEach(t),Ah=c(e),It=n(e,"P",{});var Qp=o(It);lv=l(Qp,`Contrary to RNNs that have the position of each token embedded within them, transformers are unaware of the position of
each token. Therefore, the position IDs (`),sl=n(Qp,"CODE",{});var gw=o(sl);iv=l(gw,"position_ids"),gw.forEach(t),hv=l(Qp,`) are used by the model to identify each token\u2019s position in the
list of tokens.`),Qp.forEach(t),qh=c(e),Bt=n(e,"P",{});var Jp=o(Bt);pv=l(Jp,"They are an optional parameter. If no "),al=n(Jp,"CODE",{});var bw=o(al);fv=l(bw,"position_ids"),bw.forEach(t),cv=l(Jp,` are passed to the model, the IDs are automatically created as
absolute positional embeddings.`),Jp.forEach(t),xh=c(e),Ht=n(e,"P",{});var Kp=o(Ht);dv=l(Kp,"Absolute positional embeddings are selected in the range "),nl=n(Kp,"CODE",{});var _w=o(nl);uv=l(_w,"[0, config.max_position_embeddings - 1]"),_w.forEach(t),mv=l(Kp,`. Some models use
other types of positional embeddings, such as sinusoidal position embeddings or relative position embeddings.`),Kp.forEach(t),jh=c(e),De=n(e,"H3",{class:!0});var Zp=o(De);Ot=n(Zp,"A",{id:!0,class:!0,href:!0});var ww=o(Ot);ol=n(ww,"SPAN",{});var $w=o(ol);u(aa.$$.fragment,$w),$w.forEach(t),ww.forEach(t),vv=c(Zp),rl=n(Zp,"SPAN",{});var kw=o(rl);gv=l(kw,"pretrained model"),kw.forEach(t),Zp.forEach(t),Ph=c(e),O=n(e,"P",{});var Xn=o(O);bv=l(Xn,`A model that has been pretrained on some data (for instance all of Wikipedia). Pretraining methods involve a
self-supervised objective, which can be reading the text and trying to predict the next word (see `),yn=n(Xn,"A",{href:!0});var Ew=o(yn);_v=l(Ew,`causal language
modeling`),Ew.forEach(t),wv=l(Xn,") or masking some words and trying to predict them (see "),An=n(Xn,"A",{href:!0});var yw=o(An);$v=l(yw,`masked language
modeling`),yw.forEach(t),kv=l(Xn,")."),Xn.forEach(t),Th=c(e),qn=n(e,"P",{});var Aw=o(qn);Ev=l(Aw,"Speech and vision models have their own pretraining objectives. For example, Wav2Vec2 is a speech model pretrained on a contrastive task which requires the model to identify the \u201Ctrue\u201D speech representation from a set of \u201Cfalse\u201D speech representations. On the other hand, BEiT is a vision model pretrained on a masked image modeling task which masks some of the image patches and requires the model to predict the masked patches (similar to the masked language modeling objective)."),Aw.forEach(t),Sh=c(e),Fe=n(e,"H2",{class:!0});var ef=o(Fe);Mt=n(ef,"A",{id:!0,class:!0,href:!0});var qw=o(Mt);ll=n(qw,"SPAN",{});var xw=o(ll);u(na.$$.fragment,xw),xw.forEach(t),qw.forEach(t),yv=c(ef),il=n(ef,"SPAN",{});var jw=o(il);Av=l(jw,"R"),jw.forEach(t),ef.forEach(t),Ch=c(e),Ie=n(e,"H3",{class:!0});var tf=o(Ie);Rt=n(tf,"A",{id:!0,class:!0,href:!0});var Pw=o(Rt);hl=n(Pw,"SPAN",{});var Tw=o(hl);u(oa.$$.fragment,Tw),Tw.forEach(t),Pw.forEach(t),qv=c(tf),pl=n(tf,"SPAN",{});var Sw=o(pl);xv=l(Sw,"recurrent neural network"),Sw.forEach(t),tf.forEach(t),Nh=c(e),xn=n(e,"P",{});var Cw=o(xn);jv=l(Cw,"A type of model that uses a loop over a layer to process texts."),Cw.forEach(t),zh=c(e),Be=n(e,"H2",{class:!0});var sf=o(Be);Vt=n(sf,"A",{id:!0,class:!0,href:!0});var Nw=o(Vt);fl=n(Nw,"SPAN",{});var zw=o(fl);u(ra.$$.fragment,zw),zw.forEach(t),Nw.forEach(t),Pv=c(sf),cl=n(sf,"SPAN",{});var Dw=o(cl);Tv=l(Dw,"S"),Dw.forEach(t),sf.forEach(t),Dh=c(e),He=n(e,"H3",{class:!0});var af=o(He);Lt=n(af,"A",{id:!0,class:!0,href:!0});var Fw=o(Lt);dl=n(Fw,"SPAN",{});var Iw=o(dl);u(la.$$.fragment,Iw),Iw.forEach(t),Fw.forEach(t),Sv=c(af),ul=n(af,"SPAN",{});var Bw=o(ul);Cv=l(Bw,"sampling rate"),Bw.forEach(t),af.forEach(t),Fh=c(e),jn=n(e,"P",{});var Hw=o(jn);Nv=l(Hw,"A measurement in hertz of the number of samples (the audio signal) taken per second. The sampling rate is a result of discretizing a continuous signal such as speech."),Hw.forEach(t),Ih=c(e),Oe=n(e,"H3",{class:!0});var nf=o(Oe);Gt=n(nf,"A",{id:!0,class:!0,href:!0});var Ow=o(Gt);ml=n(Ow,"SPAN",{});var Mw=o(ml);u(ia.$$.fragment,Mw),Mw.forEach(t),Ow.forEach(t),zv=c(nf),vl=n(nf,"SPAN",{});var Rw=o(vl);Dv=l(Rw,"self-attention"),Rw.forEach(t),nf.forEach(t),Bh=c(e),Pn=n(e,"P",{});var Vw=o(Pn);Fv=l(Vw,"Each element of the input finds out which other elements of the input they should attend to."),Vw.forEach(t),Hh=c(e),Me=n(e,"H3",{class:!0});var of=o(Me);Wt=n(of,"A",{id:!0,class:!0,href:!0});var Lw=o(Wt);gl=n(Lw,"SPAN",{});var Gw=o(gl);u(ha.$$.fragment,Gw),Gw.forEach(t),Lw.forEach(t),Iv=c(of),bl=n(of,"SPAN",{});var Ww=o(bl);Bv=l(Ww,"sequence-to-sequence (seq2seq)"),Ww.forEach(t),of.forEach(t),Oh=c(e),M=n(e,"P",{});var Yn=o(M);Hv=l(Yn,`Models that generate a new sequence from an input, like translation models, or summarization models (such as
`),Tn=n(Yn,"A",{href:!0});var Uw=o(Tn);Ov=l(Uw,"Bart"),Uw.forEach(t),Mv=l(Yn," or "),Sn=n(Yn,"A",{href:!0});var Xw=o(Sn);Rv=l(Xw,"T5"),Xw.forEach(t),Vv=l(Yn,")."),Yn.forEach(t),Mh=c(e),Re=n(e,"H3",{class:!0});var rf=o(Re);Ut=n(rf,"A",{id:!0,class:!0,href:!0});var Yw=o(Ut);_l=n(Yw,"SPAN",{});var Qw=o(_l);u(pa.$$.fragment,Qw),Qw.forEach(t),Yw.forEach(t),Lv=c(rf),wl=n(rf,"SPAN",{});var Jw=o(wl);Gv=l(Jw,"stride"),Jw.forEach(t),rf.forEach(t),Rh=c(e),R=n(e,"P",{});var Qn=o(R);Wv=l(Qn,"In "),Cn=n(Qn,"A",{href:!0});var Kw=o(Cn);Uv=l(Kw,"convolution"),Kw.forEach(t),Xv=l(Qn," or "),Nn=n(Qn,"A",{href:!0});var Zw=o(Nn);Yv=l(Zw,"pooling"),Zw.forEach(t),Qv=l(Qn,", the stride refers to the distance the kernel is moved over a matrix. A stride of 1 means the kernel is moved one pixel over at a time, and a stride of 2 means the kernel is moved two pixels over at a time."),Qn.forEach(t),Vh=c(e),Ve=n(e,"H2",{class:!0});var lf=o(Ve);Xt=n(lf,"A",{id:!0,class:!0,href:!0});var e$=o(Xt);$l=n(e$,"SPAN",{});var t$=o($l);u(fa.$$.fragment,t$),t$.forEach(t),e$.forEach(t),Jv=c(lf),kl=n(lf,"SPAN",{});var s$=o(kl);Kv=l(s$,"T"),s$.forEach(t),lf.forEach(t),Lh=c(e),Le=n(e,"H3",{class:!0});var hf=o(Le);Yt=n(hf,"A",{id:!0,class:!0,href:!0});var a$=o(Yt);El=n(a$,"SPAN",{});var n$=o(El);u(ca.$$.fragment,n$),n$.forEach(t),a$.forEach(t),Zv=c(hf),yl=n(hf,"SPAN",{});var o$=o(yl);e1=l(o$,"token"),o$.forEach(t),hf.forEach(t),Gh=c(e),zn=n(e,"P",{});var r$=o(zn);t1=l(r$,`A part of a sentence, usually a word, but can also be a subword (non-common words are often split in subwords) or a
punctuation symbol.`),r$.forEach(t),Wh=c(e),Ge=n(e,"H3",{class:!0});var pf=o(Ge);Qt=n(pf,"A",{id:!0,class:!0,href:!0});var l$=o(Qt);Al=n(l$,"SPAN",{});var i$=o(Al);u(da.$$.fragment,i$),i$.forEach(t),l$.forEach(t),s1=c(pf),ql=n(pf,"SPAN",{});var h$=o(ql);a1=l(h$,"token Type IDs"),h$.forEach(t),pf.forEach(t),Uh=c(e),Dn=n(e,"P",{});var p$=o(Dn);n1=l(p$,"Some models\u2019 purpose is to do classification on pairs of sentences or question answering."),p$.forEach(t),Xh=c(e),u(ua.$$.fragment,e),Yh=c(e),V=n(e,"P",{});var Jn=o(V);o1=l(Jn,`These require two different sequences to be joined in a single \u201Cinput_ids\u201D entry, which usually is performed with the
help of special tokens, such as the classifier (`),xl=n(Jn,"CODE",{});var f$=o(xl);r1=l(f$,"[CLS]"),f$.forEach(t),l1=l(Jn,") and separator ("),jl=n(Jn,"CODE",{});var c$=o(jl);i1=l(c$,"[SEP]"),c$.forEach(t),h1=l(Jn,`) tokens. For example, the BERT model
builds its two sequence input as such:`),Jn.forEach(t),Qh=c(e),u(ma.$$.fragment,e),Jh=c(e),Jt=n(e,"P",{});var ff=o(Jt);p1=l(ff,"We can use our tokenizer to automatically generate such a sentence by passing the two sequences to "),Pl=n(ff,"CODE",{});var d$=o(Pl);f1=l(d$,"tokenizer"),d$.forEach(t),c1=l(ff,` as two
arguments (and not a list, like before) like this:`),ff.forEach(t),Kh=c(e),u(va.$$.fragment,e),Zh=c(e),Fn=n(e,"P",{});var u$=o(Fn);d1=l(u$,"which will return:"),u$.forEach(t),ep=c(e),u(ga.$$.fragment,e),tp=c(e),In=n(e,"P",{});var m$=o(In);u1=l(m$,`This is enough for some models to understand where one sequence ends and where another begins. However, other models,
such as BERT, also deploy token type IDs (also called segment IDs). They are represented as a binary mask identifying
the two types of sequence in the model.`),m$.forEach(t),sp=c(e),Bn=n(e,"P",{});var v$=o(Bn);m1=l(v$,"The tokenizer returns this mask as the \u201Ctoken_type_ids\u201D entry:"),v$.forEach(t),ap=c(e),u(ba.$$.fragment,e),np=c(e),L=n(e,"P",{});var Kn=o(L);v1=l(Kn,"The first sequence, the \u201Ccontext\u201D used for the question, has all its tokens represented by a "),Tl=n(Kn,"CODE",{});var g$=o(Tl);g1=l(g$,"0"),g$.forEach(t),b1=l(Kn,`, whereas the second
sequence, corresponding to the \u201Cquestion\u201D, has all its tokens represented by a `),Sl=n(Kn,"CODE",{});var b$=o(Sl);_1=l(b$,"1"),b$.forEach(t),w1=l(Kn,"."),Kn.forEach(t),op=c(e),G=n(e,"P",{});var Zn=o(G);$1=l(Zn,"Some models, like "),Hn=n(Zn,"A",{href:!0});var _$=o(Hn);k1=l(_$,"XLNetModel"),_$.forEach(t),E1=l(Zn," use an additional token represented by a "),Cl=n(Zn,"CODE",{});var w$=o(Cl);y1=l(w$,"2"),w$.forEach(t),A1=l(Zn,"."),Zn.forEach(t),rp=c(e),We=n(e,"H3",{class:!0});var cf=o(We);Kt=n(cf,"A",{id:!0,class:!0,href:!0});var $$=o(Kt);Nl=n($$,"SPAN",{});var k$=o(Nl);u(_a.$$.fragment,k$),k$.forEach(t),$$.forEach(t),q1=c(cf),zl=n(cf,"SPAN",{});var E$=o(zl);x1=l(E$,"transformer"),E$.forEach(t),cf.forEach(t),lp=c(e),On=n(e,"P",{});var y$=o(On);j1=l(y$,"Self-attention based deep learning model architecture."),y$.forEach(t),this.h()},h(){p(y,"name","hf:doc:metadata"),p(y,"content",JSON.stringify(N$)),p(S,"id","glossary"),p(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(S,"href","#glossary"),p(x,"class","relative group"),p(Xe,"id","a"),p(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Xe,"href","#a"),p(Y,"class","relative group"),p(Ye,"id","attention-mask"),p(Ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ye,"href","#attention-mask"),p(Q,"class","relative group"),p(Sa,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),p(Qe,"id","autoencoding-models"),p(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Qe,"href","#autoencoding-models"),p(J,"class","relative group"),p(Ca,"href","#masked-language-modeling"),p(Je,"id","autoregressive-models"),p(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Je,"href","#autoregressive-models"),p(K,"class","relative group"),p(Na,"href","#causal-language-modeling"),p(Ke,"id","b"),p(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ke,"href","#b"),p(Z,"class","relative group"),p(Ze,"id","backbone"),p(Ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ze,"href","#backbone"),p(ee,"class","relative group"),p(za,"href","#head"),p(Da,"href","/docs/transformers/main/en/model_doc/vit#transformers.ViTModel"),p(Fa,"href","model_doc/dpt"),p(et,"id","c"),p(et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(et,"href","#c"),p(te,"class","relative group"),p(tt,"id","channel"),p(tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(tt,"href","#channel"),p(se,"class","relative group"),p(st,"id","causal-language-modeling"),p(st,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(st,"href","#causal-language-modeling"),p(ae,"class","relative group"),p(at,"id","connectionist-temporal-classification-ctc"),p(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(at,"href","#connectionist-temporal-classification-ctc"),p(ne,"class","relative group"),p(nt,"id","convolution"),p(nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(nt,"href","#convolution"),p(oe,"class","relative group"),p(ot,"id","d"),p(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ot,"href","#d"),p(re,"class","relative group"),p(rt,"id","decoder-input-ids"),p(rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(rt,"href","#decoder-input-ids"),p(le,"class","relative group"),p(lt,"id","deep-learning"),p(lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(lt,"href","#deep-learning"),p(ie,"class","relative group"),p(it,"id","f"),p(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(it,"href","#f"),p(he,"class","relative group"),p(ht,"id","feed-forward-chunking"),p(ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ht,"href","#feed-forward-chunking"),p(pe,"class","relative group"),p(js,"href","https://arxiv.org/abs/2001.04451"),p(js,"rel","nofollow"),p(Va,"href","/docs/transformers/main/en/internal/modeling_utils#transformers.apply_chunking_to_forward"),p(ft,"id","h"),p(ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ft,"href","#h"),p(fe,"class","relative group"),p(ct,"id","head"),p(ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ct,"href","#head"),p(ce,"class","relative group"),p(Ga,"href","/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification"),p(Wa,"href","/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Model"),p(Ua,"href","/docs/transformers/main/en/model_doc/vit#transformers.ViTForImageClassification"),p(Xa,"href","/docs/transformers/main/en/model_doc/vit#transformers.ViTModel"),p(Ya,"href","/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC"),p(Qa,"href","#connectionist-temporal-classification-(CTC)"),p(Ja,"href","/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Model"),p(ut,"id","i"),p(ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ut,"href","#i"),p(de,"class","relative group"),p(mt,"id","image-patch"),p(mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(mt,"href","#image-patch"),p(ue,"class","relative group"),p(gt,"id","input-ids"),p(gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(gt,"href","#input-ids"),p(me,"class","relative group"),p(Ds,"href","https://arxiv.org/pdf/1609.08144.pdf"),p(Ds,"rel","nofollow"),p(Hs,"href","https://github.com/huggingface/tokenizers"),p(Hs,"rel","nofollow"),p(nn,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertModel"),p(kt,"id","l"),p(kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(kt,"href","#l"),p(ve,"class","relative group"),p(Et,"id","labels"),p(Et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Et,"href","#labels"),p(ge,"class","relative group"),p(ln,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertForSequenceClassification"),p(hn,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertForTokenClassification"),p(pn,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertForMaskedLM"),p(fn,"href","/docs/transformers/main/en/model_doc/bart#transformers.BartForConditionalGeneration"),p(cn,"href","/docs/transformers/main/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),p(dn,"href","/docs/transformers/main/en/model_doc/vit#transformers.ViTForImageClassification"),p(un,"href","/docs/transformers/main/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation"),p(mn,"href","/docs/transformers/main/en/model_doc/detr#transformers.DetrForObjectDetection"),p(vn,"href","/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC"),p(gn,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertModel"),p(qt,"id","m"),p(qt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(qt,"href","#m"),p(ye,"class","relative group"),p(xt,"id","masked-language-modeling"),p(xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(xt,"href","#masked-language-modeling"),p(Ae,"class","relative group"),p(jt,"id","multimodal"),p(jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(jt,"href","#multimodal"),p(qe,"class","relative group"),p(Pt,"id","n"),p(Pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Pt,"href","#n"),p(xe,"class","relative group"),p(Tt,"id","natural-language-generation"),p(Tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Tt,"href","#natural-language-generation"),p(je,"class","relative group"),p(St,"id","natural-language-processing"),p(St,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(St,"href","#natural-language-processing"),p(Pe,"class","relative group"),p(Ct,"id","natural-language-understanding"),p(Ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ct,"href","#natural-language-understanding"),p(Te,"class","relative group"),p(Nt,"id","p"),p(Nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Nt,"href","#p"),p(Se,"class","relative group"),p(zt,"id","pixel-values"),p(zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(zt,"href","#pixel-values"),p(Ce,"class","relative group"),p(Dt,"id","pooling"),p(Dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Dt,"href","#pooling"),p(Ne,"class","relative group"),p(Ft,"id","position-ids"),p(Ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ft,"href","#position-ids"),p(ze,"class","relative group"),p(Ot,"id","pretrained-model"),p(Ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ot,"href","#pretrained-model"),p(De,"class","relative group"),p(yn,"href","#causal-language-modeling"),p(An,"href","#masked-language-modeling"),p(Mt,"id","r"),p(Mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Mt,"href","#r"),p(Fe,"class","relative group"),p(Rt,"id","recurrent-neural-network"),p(Rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Rt,"href","#recurrent-neural-network"),p(Ie,"class","relative group"),p(Vt,"id","s"),p(Vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Vt,"href","#s"),p(Be,"class","relative group"),p(Lt,"id","sampling-rate"),p(Lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Lt,"href","#sampling-rate"),p(He,"class","relative group"),p(Gt,"id","selfattention"),p(Gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Gt,"href","#selfattention"),p(Oe,"class","relative group"),p(Wt,"id","sequencetosequence-seq2seq"),p(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Wt,"href","#sequencetosequence-seq2seq"),p(Me,"class","relative group"),p(Tn,"href","model_doc/bart"),p(Sn,"href","model_doc/t5"),p(Ut,"id","stride"),p(Ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ut,"href","#stride"),p(Re,"class","relative group"),p(Cn,"href","#convolution"),p(Nn,"href","#pooling"),p(Xt,"id","t"),p(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Xt,"href","#t"),p(Ve,"class","relative group"),p(Yt,"id","token"),p(Yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Yt,"href","#token"),p(Le,"class","relative group"),p(Qt,"id","token-type-ids"),p(Qt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Qt,"href","#token-type-ids"),p(Ge,"class","relative group"),p(Hn,"href","/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetModel"),p(Kt,"id","transformer"),p(Kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Kt,"href","#transformer"),p(We,"class","relative group")},m(e,i){s(document.head,y),h(e,Ue,i),h(e,x,i),s(x,S),s(S,eo),m(as,eo,null),s(x,df),s(x,to),s(to,uf),h(e,Il,i),h(e,Ea,i),s(Ea,mf),h(e,Bl,i),h(e,Y,i),s(Y,Xe),s(Xe,so),m(ns,so,null),s(Y,vf),s(Y,ao),s(ao,gf),h(e,Hl,i),h(e,Q,i),s(Q,Ye),s(Ye,no),m(os,no,null),s(Q,bf),s(Q,oo),s(oo,_f),h(e,Ol,i),h(e,ya,i),s(ya,wf),h(e,Ml,i),m(rs,e,i),h(e,Rl,i),h(e,Aa,i),s(Aa,$f),h(e,Vl,i),h(e,qa,i),s(qa,kf),h(e,Ll,i),m(ls,e,i),h(e,Gl,i),h(e,xa,i),s(xa,Ef),h(e,Wl,i),m(is,e,i),h(e,Ul,i),h(e,ja,i),s(ja,yf),h(e,Xl,i),h(e,Pa,i),s(Pa,Af),h(e,Yl,i),m(hs,e,i),h(e,Ql,i),h(e,Ta,i),s(Ta,qf),h(e,Jl,i),m(ps,e,i),h(e,Kl,i),h(e,N,i),s(N,xf),s(N,Sa),s(Sa,jf),s(N,Pf),s(N,ro),s(ro,Tf),s(N,Sf),s(N,lo),s(lo,Cf),s(N,Nf),h(e,Zl,i),m(fs,e,i),h(e,ei,i),h(e,J,i),s(J,Qe),s(Qe,io),m(cs,io,null),s(J,zf),s(J,ho),s(ho,Df),h(e,ti,i),h(e,ds,i),s(ds,Ff),s(ds,Ca),s(Ca,If),h(e,si,i),h(e,K,i),s(K,Je),s(Je,po),m(us,po,null),s(K,Bf),s(K,fo),s(fo,Hf),h(e,ai,i),h(e,ms,i),s(ms,Of),s(ms,Na),s(Na,Mf),h(e,ni,i),h(e,Z,i),s(Z,Ke),s(Ke,co),m(vs,co,null),s(Z,Rf),s(Z,uo),s(uo,Vf),h(e,oi,i),h(e,ee,i),s(ee,Ze),s(Ze,mo),m(gs,mo,null),s(ee,Lf),s(ee,vo),s(vo,Gf),h(e,ri,i),h(e,j,i),s(j,Wf),s(j,za),s(za,Uf),s(j,Xf),s(j,Da),s(Da,Yf),s(j,Qf),s(j,go),s(go,Jf),s(j,Kf),s(j,Fa),s(Fa,Zf),s(j,ec),h(e,li,i),h(e,te,i),s(te,et),s(et,bo),m(bs,bo,null),s(te,tc),s(te,_o),s(_o,sc),h(e,ii,i),h(e,se,i),s(se,tt),s(tt,wo),m(_s,wo,null),s(se,ac),s(se,$o),s($o,nc),h(e,hi,i),h(e,k,i),s(k,oc),s(k,ko),s(ko,rc),s(k,lc),s(k,Eo),s(Eo,ic),s(k,hc),s(k,yo),s(yo,pc),s(k,fc),s(k,Ao),s(Ao,cc),s(k,dc),s(k,qo),s(qo,uc),s(k,mc),s(k,xo),s(xo,vc),s(k,gc),h(e,pi,i),h(e,ae,i),s(ae,st),s(st,jo),m(ws,jo,null),s(ae,bc),s(ae,Po),s(Po,_c),h(e,fi,i),h(e,Ia,i),s(Ia,wc),h(e,ci,i),h(e,ne,i),s(ne,at),s(at,To),m($s,To,null),s(ne,$c),s(ne,So),s(So,kc),h(e,di,i),h(e,Ba,i),s(Ba,Ec),h(e,ui,i),h(e,oe,i),s(oe,nt),s(nt,Co),m(ks,Co,null),s(oe,yc),s(oe,No),s(No,Ac),h(e,mi,i),h(e,Ha,i),s(Ha,qc),h(e,vi,i),h(e,re,i),s(re,ot),s(ot,zo),m(Es,zo,null),s(re,xc),s(re,Do),s(Do,jc),h(e,gi,i),h(e,le,i),s(le,rt),s(rt,Fo),m(ys,Fo,null),s(le,Pc),s(le,Io),s(Io,Tc),h(e,bi,i),h(e,Oa,i),s(Oa,Sc),h(e,_i,i),h(e,z,i),s(z,Cc),s(z,Bo),s(Bo,Nc),s(z,zc),s(z,Ho),s(Ho,Dc),s(z,Fc),s(z,Oo),s(Oo,Ic),s(z,Bc),h(e,wi,i),h(e,Ma,i),s(Ma,Hc),h(e,$i,i),h(e,ie,i),s(ie,lt),s(lt,Mo),m(As,Mo,null),s(ie,Oc),s(ie,Ro),s(Ro,Mc),h(e,ki,i),h(e,Ra,i),s(Ra,Rc),h(e,Ei,i),h(e,he,i),s(he,it),s(it,Vo),m(qs,Vo,null),s(he,Vc),s(he,Lo),s(Lo,Lc),h(e,yi,i),h(e,pe,i),s(pe,ht),s(ht,Go),m(xs,Go,null),s(pe,Gc),s(pe,Wo),s(Wo,Wc),h(e,Ai,i),h(e,pt,i),s(pt,Uc),s(pt,Uo),s(Uo,Xc),s(pt,Yc),h(e,qi,i),h(e,w,i),s(w,Qc),s(w,Xo),s(Xo,Jc),s(w,Kc),s(w,Yo),s(Yo,Zc),s(w,ed),s(w,js),s(js,td),s(w,sd),s(w,Qo),s(Qo,ad),s(w,nd),s(w,Jo),s(Jo,od),s(w,rd),s(w,Ko),s(Ko,ld),s(w,id),s(w,Zo),s(Zo,hd),s(w,pd),s(w,er),s(er,fd),s(w,cd),h(e,xi,i),h(e,D,i),s(D,dd),s(D,Va),s(Va,ud),s(D,md),s(D,tr),s(tr,vd),s(D,gd),s(D,sr),s(sr,bd),s(D,_d),h(e,ji,i),h(e,fe,i),s(fe,ft),s(ft,ar),m(Ps,ar,null),s(fe,wd),s(fe,nr),s(nr,$d),h(e,Pi,i),h(e,ce,i),s(ce,ct),s(ct,or),m(Ts,or,null),s(ce,kd),s(ce,rr),s(rr,Ed),h(e,Ti,i),h(e,La,i),s(La,yd),h(e,Si,i),h(e,I,i),s(I,dt),s(dt,Ga),s(Ga,Ad),s(dt,qd),s(dt,Wa),s(Wa,xd),s(dt,jd),s(I,Pd),s(I,B),s(B,Ua),s(Ua,Td),s(B,Sd),s(B,lr),s(lr,Cd),s(B,Nd),s(B,Xa),s(Xa,zd),s(B,Dd),s(I,Fd),s(I,H),s(H,Ya),s(Ya,Id),s(H,Bd),s(H,Qa),s(Qa,Hd),s(H,Od),s(H,Ja),s(Ja,Md),s(H,Rd),h(e,Ci,i),h(e,de,i),s(de,ut),s(ut,ir),m(Ss,ir,null),s(de,Vd),s(de,hr),s(hr,Ld),h(e,Ni,i),h(e,ue,i),s(ue,mt),s(mt,pr),m(Cs,pr,null),s(ue,Gd),s(ue,fr),s(fr,Wd),h(e,zi,i),h(e,vt,i),s(vt,Ud),s(vt,cr),s(cr,Xd),s(vt,Yd),h(e,Di,i),h(e,me,i),s(me,gt),s(gt,dr),m(Ns,dr,null),s(me,Qd),s(me,ur),s(ur,Jd),h(e,Fi,i),h(e,Ka,i),s(Ka,Kd),h(e,Ii,i),m(zs,e,i),h(e,Bi,i),h(e,bt,i),s(bt,Zd),s(bt,Ds),s(Ds,eu),s(bt,tu),h(e,Hi,i),m(Fs,e,i),h(e,Oi,i),h(e,Za,i),s(Za,su),h(e,Mi,i),m(Is,e,i),h(e,Ri,i),h(e,en,i),s(en,au),h(e,Vi,i),m(Bs,e,i),h(e,Li,i),h(e,_t,i),s(_t,nu),s(_t,Hs),s(Hs,ou),s(_t,ru),h(e,Gi,i),m(Os,e,i),h(e,Wi,i),h(e,wt,i),s(wt,lu),s(wt,mr),s(mr,iu),s(wt,hu),h(e,Ui,i),m(Ms,e,i),h(e,Xi,i),h(e,tn,i),s(tn,pu),h(e,Yi,i),h(e,sn,i),s(sn,fu),h(e,Qi,i),m(Rs,e,i),h(e,Ji,i),h(e,an,i),s(an,cu),h(e,Ki,i),m(Vs,e,i),h(e,Zi,i),h(e,$t,i),s($t,du),s($t,nn),s(nn,uu),s($t,mu),h(e,eh,i),h(e,ve,i),s(ve,kt),s(kt,vr),m(Ls,vr,null),s(ve,vu),s(ve,gr),s(gr,gu),h(e,th,i),h(e,ge,i),s(ge,Et),s(Et,br),m(Gs,br,null),s(ge,bu),s(ge,_r),s(_r,_u),h(e,sh,i),h(e,on,i),s(on,wu),h(e,ah,i),h(e,rn,i),s(rn,$u),h(e,nh,i),h(e,$,i),s($,be),s(be,ku),s(be,ln),s(ln,Eu),s(be,yu),s(be,wr),s(wr,Au),s(be,qu),s($,xu),s($,_e),s(_e,ju),s(_e,hn),s(hn,Pu),s(_e,Tu),s(_e,$r),s($r,Su),s(_e,Cu),s($,Nu),s($,we),s(we,zu),s(we,pn),s(pn,Du),s(we,Fu),s(we,kr),s(kr,Iu),s(we,Bu),s($,Hu),s($,C),s(C,Ou),s(C,fn),s(fn,Mu),s(C,Ru),s(C,cn),s(cn,Vu),s(C,Lu),s(C,Er),s(Er,Gu),s(C,Wu),s(C,yr),s(yr,Uu),s(C,Xu),s($,Yu),s($,$e),s($e,Qu),s($e,dn),s(dn,Ju),s($e,Ku),s($e,Ar),s(Ar,Zu),s($e,em),s($,tm),s($,ke),s(ke,sm),s(ke,un),s(un,am),s(ke,nm),s(ke,qr),s(qr,om),s(ke,rm),s($,lm),s($,F),s(F,im),s(F,mn),s(mn,hm),s(F,pm),s(F,xr),s(xr,fm),s(F,cm),s(F,jr),s(jr,dm),s(F,um),s($,mm),s($,Ee),s(Ee,vm),s(Ee,vn),s(vn,gm),s(Ee,bm),s(Ee,Pr),s(Pr,_m),s(Ee,wm),h(e,oh,i),m(yt,e,i),h(e,rh,i),h(e,At,i),s(At,$m),s(At,gn),s(gn,km),s(At,Em),h(e,lh,i),h(e,ye,i),s(ye,qt),s(qt,Tr),m(Ws,Tr,null),s(ye,ym),s(ye,Sr),s(Sr,Am),h(e,ih,i),h(e,Ae,i),s(Ae,xt),s(xt,Cr),m(Us,Cr,null),s(Ae,qm),s(Ae,Nr),s(Nr,xm),h(e,hh,i),h(e,bn,i),s(bn,jm),h(e,ph,i),h(e,qe,i),s(qe,jt),s(jt,zr),m(Xs,zr,null),s(qe,Pm),s(qe,Dr),s(Dr,Tm),h(e,fh,i),h(e,_n,i),s(_n,Sm),h(e,ch,i),h(e,xe,i),s(xe,Pt),s(Pt,Fr),m(Ys,Fr,null),s(xe,Cm),s(xe,Ir),s(Ir,Nm),h(e,dh,i),h(e,je,i),s(je,Tt),s(Tt,Br),m(Qs,Br,null),s(je,zm),s(je,Hr),s(Hr,Dm),h(e,uh,i),h(e,wn,i),s(wn,Fm),h(e,mh,i),h(e,Pe,i),s(Pe,St),s(St,Or),m(Js,Or,null),s(Pe,Im),s(Pe,Mr),s(Mr,Bm),h(e,vh,i),h(e,$n,i),s($n,Hm),h(e,gh,i),h(e,Te,i),s(Te,Ct),s(Ct,Rr),m(Ks,Rr,null),s(Te,Om),s(Te,Vr),s(Vr,Mm),h(e,bh,i),h(e,kn,i),s(kn,Rm),h(e,_h,i),h(e,Se,i),s(Se,Nt),s(Nt,Lr),m(Zs,Lr,null),s(Se,Vm),s(Se,Gr),s(Gr,Lm),h(e,wh,i),h(e,Ce,i),s(Ce,zt),s(zt,Wr),m(ea,Wr,null),s(Ce,Gm),s(Ce,Ur),s(Ur,Wm),h(e,$h,i),h(e,P,i),s(P,Um),s(P,Xr),s(Xr,Xm),s(P,Ym),s(P,Yr),s(Yr,Qm),s(P,Jm),s(P,Qr),s(Qr,Km),s(P,Zm),s(P,Jr),s(Jr,ev),s(P,tv),h(e,kh,i),h(e,Ne,i),s(Ne,Dt),s(Dt,Kr),m(ta,Kr,null),s(Ne,sv),s(Ne,Zr),s(Zr,av),h(e,Eh,i),h(e,En,i),s(En,nv),h(e,yh,i),h(e,ze,i),s(ze,Ft),s(Ft,el),m(sa,el,null),s(ze,ov),s(ze,tl),s(tl,rv),h(e,Ah,i),h(e,It,i),s(It,lv),s(It,sl),s(sl,iv),s(It,hv),h(e,qh,i),h(e,Bt,i),s(Bt,pv),s(Bt,al),s(al,fv),s(Bt,cv),h(e,xh,i),h(e,Ht,i),s(Ht,dv),s(Ht,nl),s(nl,uv),s(Ht,mv),h(e,jh,i),h(e,De,i),s(De,Ot),s(Ot,ol),m(aa,ol,null),s(De,vv),s(De,rl),s(rl,gv),h(e,Ph,i),h(e,O,i),s(O,bv),s(O,yn),s(yn,_v),s(O,wv),s(O,An),s(An,$v),s(O,kv),h(e,Th,i),h(e,qn,i),s(qn,Ev),h(e,Sh,i),h(e,Fe,i),s(Fe,Mt),s(Mt,ll),m(na,ll,null),s(Fe,yv),s(Fe,il),s(il,Av),h(e,Ch,i),h(e,Ie,i),s(Ie,Rt),s(Rt,hl),m(oa,hl,null),s(Ie,qv),s(Ie,pl),s(pl,xv),h(e,Nh,i),h(e,xn,i),s(xn,jv),h(e,zh,i),h(e,Be,i),s(Be,Vt),s(Vt,fl),m(ra,fl,null),s(Be,Pv),s(Be,cl),s(cl,Tv),h(e,Dh,i),h(e,He,i),s(He,Lt),s(Lt,dl),m(la,dl,null),s(He,Sv),s(He,ul),s(ul,Cv),h(e,Fh,i),h(e,jn,i),s(jn,Nv),h(e,Ih,i),h(e,Oe,i),s(Oe,Gt),s(Gt,ml),m(ia,ml,null),s(Oe,zv),s(Oe,vl),s(vl,Dv),h(e,Bh,i),h(e,Pn,i),s(Pn,Fv),h(e,Hh,i),h(e,Me,i),s(Me,Wt),s(Wt,gl),m(ha,gl,null),s(Me,Iv),s(Me,bl),s(bl,Bv),h(e,Oh,i),h(e,M,i),s(M,Hv),s(M,Tn),s(Tn,Ov),s(M,Mv),s(M,Sn),s(Sn,Rv),s(M,Vv),h(e,Mh,i),h(e,Re,i),s(Re,Ut),s(Ut,_l),m(pa,_l,null),s(Re,Lv),s(Re,wl),s(wl,Gv),h(e,Rh,i),h(e,R,i),s(R,Wv),s(R,Cn),s(Cn,Uv),s(R,Xv),s(R,Nn),s(Nn,Yv),s(R,Qv),h(e,Vh,i),h(e,Ve,i),s(Ve,Xt),s(Xt,$l),m(fa,$l,null),s(Ve,Jv),s(Ve,kl),s(kl,Kv),h(e,Lh,i),h(e,Le,i),s(Le,Yt),s(Yt,El),m(ca,El,null),s(Le,Zv),s(Le,yl),s(yl,e1),h(e,Gh,i),h(e,zn,i),s(zn,t1),h(e,Wh,i),h(e,Ge,i),s(Ge,Qt),s(Qt,Al),m(da,Al,null),s(Ge,s1),s(Ge,ql),s(ql,a1),h(e,Uh,i),h(e,Dn,i),s(Dn,n1),h(e,Xh,i),m(ua,e,i),h(e,Yh,i),h(e,V,i),s(V,o1),s(V,xl),s(xl,r1),s(V,l1),s(V,jl),s(jl,i1),s(V,h1),h(e,Qh,i),m(ma,e,i),h(e,Jh,i),h(e,Jt,i),s(Jt,p1),s(Jt,Pl),s(Pl,f1),s(Jt,c1),h(e,Kh,i),m(va,e,i),h(e,Zh,i),h(e,Fn,i),s(Fn,d1),h(e,ep,i),m(ga,e,i),h(e,tp,i),h(e,In,i),s(In,u1),h(e,sp,i),h(e,Bn,i),s(Bn,m1),h(e,ap,i),m(ba,e,i),h(e,np,i),h(e,L,i),s(L,v1),s(L,Tl),s(Tl,g1),s(L,b1),s(L,Sl),s(Sl,_1),s(L,w1),h(e,op,i),h(e,G,i),s(G,$1),s(G,Hn),s(Hn,k1),s(G,E1),s(G,Cl),s(Cl,y1),s(G,A1),h(e,rp,i),h(e,We,i),s(We,Kt),s(Kt,Nl),m(_a,Nl,null),s(We,q1),s(We,zl),s(zl,x1),h(e,lp,i),h(e,On,i),s(On,j1),ip=!0},p(e,[i]){const wa={};i&2&&(wa.$$scope={dirty:i,ctx:e}),yt.$set(wa)},i(e){ip||(v(as.$$.fragment,e),v(ns.$$.fragment,e),v(os.$$.fragment,e),v(rs.$$.fragment,e),v(ls.$$.fragment,e),v(is.$$.fragment,e),v(hs.$$.fragment,e),v(ps.$$.fragment,e),v(fs.$$.fragment,e),v(cs.$$.fragment,e),v(us.$$.fragment,e),v(vs.$$.fragment,e),v(gs.$$.fragment,e),v(bs.$$.fragment,e),v(_s.$$.fragment,e),v(ws.$$.fragment,e),v($s.$$.fragment,e),v(ks.$$.fragment,e),v(Es.$$.fragment,e),v(ys.$$.fragment,e),v(As.$$.fragment,e),v(qs.$$.fragment,e),v(xs.$$.fragment,e),v(Ps.$$.fragment,e),v(Ts.$$.fragment,e),v(Ss.$$.fragment,e),v(Cs.$$.fragment,e),v(Ns.$$.fragment,e),v(zs.$$.fragment,e),v(Fs.$$.fragment,e),v(Is.$$.fragment,e),v(Bs.$$.fragment,e),v(Os.$$.fragment,e),v(Ms.$$.fragment,e),v(Rs.$$.fragment,e),v(Vs.$$.fragment,e),v(Ls.$$.fragment,e),v(Gs.$$.fragment,e),v(yt.$$.fragment,e),v(Ws.$$.fragment,e),v(Us.$$.fragment,e),v(Xs.$$.fragment,e),v(Ys.$$.fragment,e),v(Qs.$$.fragment,e),v(Js.$$.fragment,e),v(Ks.$$.fragment,e),v(Zs.$$.fragment,e),v(ea.$$.fragment,e),v(ta.$$.fragment,e),v(sa.$$.fragment,e),v(aa.$$.fragment,e),v(na.$$.fragment,e),v(oa.$$.fragment,e),v(ra.$$.fragment,e),v(la.$$.fragment,e),v(ia.$$.fragment,e),v(ha.$$.fragment,e),v(pa.$$.fragment,e),v(fa.$$.fragment,e),v(ca.$$.fragment,e),v(da.$$.fragment,e),v(ua.$$.fragment,e),v(ma.$$.fragment,e),v(va.$$.fragment,e),v(ga.$$.fragment,e),v(ba.$$.fragment,e),v(_a.$$.fragment,e),ip=!0)},o(e){g(as.$$.fragment,e),g(ns.$$.fragment,e),g(os.$$.fragment,e),g(rs.$$.fragment,e),g(ls.$$.fragment,e),g(is.$$.fragment,e),g(hs.$$.fragment,e),g(ps.$$.fragment,e),g(fs.$$.fragment,e),g(cs.$$.fragment,e),g(us.$$.fragment,e),g(vs.$$.fragment,e),g(gs.$$.fragment,e),g(bs.$$.fragment,e),g(_s.$$.fragment,e),g(ws.$$.fragment,e),g($s.$$.fragment,e),g(ks.$$.fragment,e),g(Es.$$.fragment,e),g(ys.$$.fragment,e),g(As.$$.fragment,e),g(qs.$$.fragment,e),g(xs.$$.fragment,e),g(Ps.$$.fragment,e),g(Ts.$$.fragment,e),g(Ss.$$.fragment,e),g(Cs.$$.fragment,e),g(Ns.$$.fragment,e),g(zs.$$.fragment,e),g(Fs.$$.fragment,e),g(Is.$$.fragment,e),g(Bs.$$.fragment,e),g(Os.$$.fragment,e),g(Ms.$$.fragment,e),g(Rs.$$.fragment,e),g(Vs.$$.fragment,e),g(Ls.$$.fragment,e),g(Gs.$$.fragment,e),g(yt.$$.fragment,e),g(Ws.$$.fragment,e),g(Us.$$.fragment,e),g(Xs.$$.fragment,e),g(Ys.$$.fragment,e),g(Qs.$$.fragment,e),g(Js.$$.fragment,e),g(Ks.$$.fragment,e),g(Zs.$$.fragment,e),g(ea.$$.fragment,e),g(ta.$$.fragment,e),g(sa.$$.fragment,e),g(aa.$$.fragment,e),g(na.$$.fragment,e),g(oa.$$.fragment,e),g(ra.$$.fragment,e),g(la.$$.fragment,e),g(ia.$$.fragment,e),g(ha.$$.fragment,e),g(pa.$$.fragment,e),g(fa.$$.fragment,e),g(ca.$$.fragment,e),g(da.$$.fragment,e),g(ua.$$.fragment,e),g(ma.$$.fragment,e),g(va.$$.fragment,e),g(ga.$$.fragment,e),g(ba.$$.fragment,e),g(_a.$$.fragment,e),ip=!1},d(e){t(y),e&&t(Ue),e&&t(x),b(as),e&&t(Il),e&&t(Ea),e&&t(Bl),e&&t(Y),b(ns),e&&t(Hl),e&&t(Q),b(os),e&&t(Ol),e&&t(ya),e&&t(Ml),b(rs,e),e&&t(Rl),e&&t(Aa),e&&t(Vl),e&&t(qa),e&&t(Ll),b(ls,e),e&&t(Gl),e&&t(xa),e&&t(Wl),b(is,e),e&&t(Ul),e&&t(ja),e&&t(Xl),e&&t(Pa),e&&t(Yl),b(hs,e),e&&t(Ql),e&&t(Ta),e&&t(Jl),b(ps,e),e&&t(Kl),e&&t(N),e&&t(Zl),b(fs,e),e&&t(ei),e&&t(J),b(cs),e&&t(ti),e&&t(ds),e&&t(si),e&&t(K),b(us),e&&t(ai),e&&t(ms),e&&t(ni),e&&t(Z),b(vs),e&&t(oi),e&&t(ee),b(gs),e&&t(ri),e&&t(j),e&&t(li),e&&t(te),b(bs),e&&t(ii),e&&t(se),b(_s),e&&t(hi),e&&t(k),e&&t(pi),e&&t(ae),b(ws),e&&t(fi),e&&t(Ia),e&&t(ci),e&&t(ne),b($s),e&&t(di),e&&t(Ba),e&&t(ui),e&&t(oe),b(ks),e&&t(mi),e&&t(Ha),e&&t(vi),e&&t(re),b(Es),e&&t(gi),e&&t(le),b(ys),e&&t(bi),e&&t(Oa),e&&t(_i),e&&t(z),e&&t(wi),e&&t(Ma),e&&t($i),e&&t(ie),b(As),e&&t(ki),e&&t(Ra),e&&t(Ei),e&&t(he),b(qs),e&&t(yi),e&&t(pe),b(xs),e&&t(Ai),e&&t(pt),e&&t(qi),e&&t(w),e&&t(xi),e&&t(D),e&&t(ji),e&&t(fe),b(Ps),e&&t(Pi),e&&t(ce),b(Ts),e&&t(Ti),e&&t(La),e&&t(Si),e&&t(I),e&&t(Ci),e&&t(de),b(Ss),e&&t(Ni),e&&t(ue),b(Cs),e&&t(zi),e&&t(vt),e&&t(Di),e&&t(me),b(Ns),e&&t(Fi),e&&t(Ka),e&&t(Ii),b(zs,e),e&&t(Bi),e&&t(bt),e&&t(Hi),b(Fs,e),e&&t(Oi),e&&t(Za),e&&t(Mi),b(Is,e),e&&t(Ri),e&&t(en),e&&t(Vi),b(Bs,e),e&&t(Li),e&&t(_t),e&&t(Gi),b(Os,e),e&&t(Wi),e&&t(wt),e&&t(Ui),b(Ms,e),e&&t(Xi),e&&t(tn),e&&t(Yi),e&&t(sn),e&&t(Qi),b(Rs,e),e&&t(Ji),e&&t(an),e&&t(Ki),b(Vs,e),e&&t(Zi),e&&t($t),e&&t(eh),e&&t(ve),b(Ls),e&&t(th),e&&t(ge),b(Gs),e&&t(sh),e&&t(on),e&&t(ah),e&&t(rn),e&&t(nh),e&&t($),e&&t(oh),b(yt,e),e&&t(rh),e&&t(At),e&&t(lh),e&&t(ye),b(Ws),e&&t(ih),e&&t(Ae),b(Us),e&&t(hh),e&&t(bn),e&&t(ph),e&&t(qe),b(Xs),e&&t(fh),e&&t(_n),e&&t(ch),e&&t(xe),b(Ys),e&&t(dh),e&&t(je),b(Qs),e&&t(uh),e&&t(wn),e&&t(mh),e&&t(Pe),b(Js),e&&t(vh),e&&t($n),e&&t(gh),e&&t(Te),b(Ks),e&&t(bh),e&&t(kn),e&&t(_h),e&&t(Se),b(Zs),e&&t(wh),e&&t(Ce),b(ea),e&&t($h),e&&t(P),e&&t(kh),e&&t(Ne),b(ta),e&&t(Eh),e&&t(En),e&&t(yh),e&&t(ze),b(sa),e&&t(Ah),e&&t(It),e&&t(qh),e&&t(Bt),e&&t(xh),e&&t(Ht),e&&t(jh),e&&t(De),b(aa),e&&t(Ph),e&&t(O),e&&t(Th),e&&t(qn),e&&t(Sh),e&&t(Fe),b(na),e&&t(Ch),e&&t(Ie),b(oa),e&&t(Nh),e&&t(xn),e&&t(zh),e&&t(Be),b(ra),e&&t(Dh),e&&t(He),b(la),e&&t(Fh),e&&t(jn),e&&t(Ih),e&&t(Oe),b(ia),e&&t(Bh),e&&t(Pn),e&&t(Hh),e&&t(Me),b(ha),e&&t(Oh),e&&t(M),e&&t(Mh),e&&t(Re),b(pa),e&&t(Rh),e&&t(R),e&&t(Vh),e&&t(Ve),b(fa),e&&t(Lh),e&&t(Le),b(ca),e&&t(Gh),e&&t(zn),e&&t(Wh),e&&t(Ge),b(da),e&&t(Uh),e&&t(Dn),e&&t(Xh),b(ua,e),e&&t(Yh),e&&t(V),e&&t(Qh),b(ma,e),e&&t(Jh),e&&t(Jt),e&&t(Kh),b(va,e),e&&t(Zh),e&&t(Fn),e&&t(ep),b(ga,e),e&&t(tp),e&&t(In),e&&t(sp),e&&t(Bn),e&&t(ap),b(ba,e),e&&t(np),e&&t(L),e&&t(op),e&&t(G),e&&t(rp),e&&t(We),b(_a),e&&t(lp),e&&t(On)}}}const N$={local:"glossary",sections:[{local:"a",sections:[{local:"attention-mask",title:"attention mask"},{local:"autoencoding-models",title:"autoencoding models "},{local:"autoregressive-models",title:"autoregressive models"}],title:"A"},{local:"b",sections:[{local:"backbone",title:"backbone"}],title:"B"},{local:"c",sections:[{local:"channel",title:"channel"},{local:"causal-language-modeling",title:"causal language modeling"},{local:"connectionist-temporal-classification-ctc",title:"connectionist temporal classification (CTC)"},{local:"convolution",title:"convolution"}],title:"C"},{local:"d",sections:[{local:"decoder-input-ids",title:"decoder input IDs"},{local:"deep-learning",title:"deep learning"}],title:"D"},{local:"f",sections:[{local:"feed-forward-chunking",title:"feed forward chunking"}],title:"F"},{local:"h",sections:[{local:"head",title:"head"}],title:"H"},{local:"i",sections:[{local:"image-patch",title:"image patch"},{local:"input-ids",title:"input IDs"}],title:"I"},{local:"l",sections:[{local:"labels",title:"labels"}],title:"L"},{local:"m",sections:[{local:"masked-language-modeling",title:"masked language modeling"},{local:"multimodal",title:"multimodal"}],title:"M"},{local:"n",sections:[{local:"natural-language-generation",title:"Natural language generation"},{local:"natural-language-processing",title:"Natural language processing"},{local:"natural-language-understanding",title:"Natural language understanding"}],title:"N"},{local:"p",sections:[{local:"pixel-values",title:"pixel values"},{local:"pooling",title:"pooling"},{local:"position-ids",title:"position IDs"},{local:"pretrained-model",title:"pretrained model"}],title:"P"},{local:"r",sections:[{local:"recurrent-neural-network",title:"recurrent neural network"}],title:"R"},{local:"s",sections:[{local:"sampling-rate",title:"sampling rate"},{local:"selfattention",title:"self-attention"},{local:"sequencetosequence-seq2seq",title:"sequence-to-sequence (seq2seq)"},{local:"stride",title:"stride"}],title:"S"},{local:"t",sections:[{local:"token",title:"token"},{local:"token-type-ids",title:"token Type IDs"},{local:"transformer",title:"transformer"}],title:"T"}],title:"Glossary"};function z$(Fl){return P$(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class O$ extends A${constructor(y){super();q$(this,y,z$,C$,x$,{})}}export{O$ as default,N$ as metadata};
