import{S as mi,i as ii,s as bi,e as p,k as c,w as f,t as n,M as fi,c as r,d as e,m as h,a as o,x as j,h as t,b as m,N as ui,G as a,g as u,y as d,q as g,o as _,B as $,v as ji,L as ci}from"../chunks/vendor-hf-doc-builder.js";import{T as Np}from"../chunks/Tip-hf-doc-builder.js";import{Y as di}from"../chunks/Youtube-hf-doc-builder.js";import{I as Ss}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as w}from"../chunks/CodeBlock-hf-doc-builder.js";import{D as gi}from"../chunks/DocNotebookDropdown-hf-doc-builder.js";import{F as _i,M as hi}from"../chunks/Markdown-hf-doc-builder.js";function $i(A){let i,v,b,y,k,x,E;return{c(){i=p("p"),v=p("code"),b=n("AutoProcessor"),y=c(),k=p("strong"),x=n("always"),E=n(" works and automatically chooses the correct class for the model you\u2019re using, whether you\u2019re using a tokenizer, feature extractor or processor.")},l(q){i=r(q,"P",{});var O=o(i);v=r(O,"CODE",{});var Ns=o(v);b=t(Ns,"AutoProcessor"),Ns.forEach(e),y=h(O),k=r(O,"STRONG",{});var I=o(k);x=t(I,"always"),I.forEach(e),E=t(O," works and automatically chooses the correct class for the model you\u2019re using, whether you\u2019re using a tokenizer, feature extractor or processor."),O.forEach(e)},m(q,O){u(q,i,O),a(i,v),a(v,b),a(i,y),a(i,k),a(k,x),a(i,E)},d(q){q&&e(i)}}}function vi(A){let i,v,b,y,k;return{c(){i=p("p"),v=n("If you plan on using a pretrained model, it\u2019s important to use the associated pretrained tokenizer. This ensures the text is split the same way as the pretraining corpus, and uses the same corresponding tokens-to-index (usually referrred to as the "),b=p("em"),y=n("vocab"),k=n(") during pretraining.")},l(x){i=r(x,"P",{});var E=o(i);v=t(E,"If you plan on using a pretrained model, it\u2019s important to use the associated pretrained tokenizer. This ensures the text is split the same way as the pretraining corpus, and uses the same corresponding tokens-to-index (usually referrred to as the "),b=r(E,"EM",{});var q=o(b);y=t(q,"vocab"),q.forEach(e),k=t(E,") during pretraining."),E.forEach(e)},m(x,E){u(x,i,E),a(i,v),a(i,b),a(b,y),a(i,k)},d(x){x&&e(i)}}}function yi(A){let i,v,b,y,k;return{c(){i=p("p"),v=n("Check out the "),b=p("a"),y=n("Padding and truncation"),k=n(" concept guide to learn more different padding and truncation arguments."),this.h()},l(x){i=r(x,"P",{});var E=o(i);v=t(E,"Check out the "),b=r(E,"A",{href:!0});var q=o(b);y=t(q,"Padding and truncation"),q.forEach(e),k=t(E," concept guide to learn more different padding and truncation arguments."),E.forEach(e),this.h()},h(){m(b,"href","./pad_truncation")},m(x,E){u(x,i,E),a(i,v),a(i,b),a(b,y),a(i,k)},d(x){x&&e(i)}}}function wi(A){let i,v;return i=new w({props:{code:`batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="pt")
print(encoded_input)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(batch_sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([[<span class="hljs-number">101</span>, <span class="hljs-number">1252</span>, <span class="hljs-number">1184</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                      [<span class="hljs-number">101</span>, <span class="hljs-number">1790</span>, <span class="hljs-number">112</span>, <span class="hljs-number">189</span>, <span class="hljs-number">1341</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">3520</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">117</span>, <span class="hljs-number">21902</span>, <span class="hljs-number">1643</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
                      [<span class="hljs-number">101</span>, <span class="hljs-number">1327</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">5450</span>, <span class="hljs-number">23434</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]), 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                           [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                           [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]), 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                           [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                           [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])}`}}),{c(){f(i.$$.fragment)},l(b){j(i.$$.fragment,b)},m(b,y){d(i,b,y),v=!0},p:ci,i(b){v||(g(i.$$.fragment,b),v=!0)},o(b){_(i.$$.fragment,b),v=!1},d(b){$(i,b)}}}function ki(A){let i,v;return i=new hi({props:{$$slots:{default:[wi]},$$scope:{ctx:A}}}),{c(){f(i.$$.fragment)},l(b){j(i.$$.fragment,b)},m(b,y){d(i,b,y),v=!0},p(b,y){const k={};y&2&&(k.$$scope={dirty:y,ctx:b}),i.$set(k)},i(b){v||(g(i.$$.fragment,b),v=!0)},o(b){_(i.$$.fragment,b),v=!1},d(b){$(i,b)}}}function xi(A){let i,v;return i=new w({props:{code:`batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="tf")
print(encoded_input)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(batch_sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">9</span>), dtype=int32, numpy=
array([[<span class="hljs-number">101</span>, <span class="hljs-number">1252</span>, <span class="hljs-number">1184</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
       [<span class="hljs-number">101</span>, <span class="hljs-number">1790</span>, <span class="hljs-number">112</span>, <span class="hljs-number">189</span>, <span class="hljs-number">1341</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">3520</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">117</span>, <span class="hljs-number">21902</span>, <span class="hljs-number">1643</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
       [<span class="hljs-number">101</span>, <span class="hljs-number">1327</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">5450</span>, <span class="hljs-number">23434</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]],
      dtype=int32)&gt;, 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">9</span>), dtype=int32, numpy=
array([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
       [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
       [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], dtype=int32)&gt;, 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">2</span>, <span class="hljs-number">9</span>), dtype=int32, numpy=
array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
       [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
       [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], dtype=int32)&gt;}`}}),{c(){f(i.$$.fragment)},l(b){j(i.$$.fragment,b)},m(b,y){d(i,b,y),v=!0},p:ci,i(b){v||(g(i.$$.fragment,b),v=!0)},o(b){_(i.$$.fragment,b),v=!1},d(b){$(i,b)}}}function Ei(A){let i,v;return i=new hi({props:{$$slots:{default:[xi]},$$scope:{ctx:A}}}),{c(){f(i.$$.fragment)},l(b){j(i.$$.fragment,b)},m(b,y){d(i,b,y),v=!0},p(b,y){const k={};y&2&&(k.$$scope={dirty:y,ctx:b}),i.$set(k)},i(b){v||(g(i.$$.fragment,b),v=!0)},o(b){_(i.$$.fragment,b),v=!1},d(b){$(i,b)}}}function qi(A){let i,v,b,y,k;return{c(){i=p("p"),v=n("Use \u{1F917} Datasets "),b=p("code"),y=n("split"),k=n(" parameter to only load a small sample from the training split since the dataset is quite large!")},l(x){i=r(x,"P",{});var E=o(i);v=t(E,"Use \u{1F917} Datasets "),b=r(E,"CODE",{});var q=o(b);y=t(q,"split"),q.forEach(e),k=t(E," parameter to only load a small sample from the training split since the dataset is quite large!"),E.forEach(e)},m(x,E){u(x,i,E),a(i,v),a(i,b),a(b,y),a(i,k)},d(x){x&&e(i)}}}function Ai(A){let i,v,b,y,k,x,E,q,O,Ns,I,dt,te,Fp,gt,T,Fs,Rp,le,Bp,Mp,Wp,Rs,Jp,pe,Hp,Up,Yp,Bs,Gp,re,Vp,Kp,Qp,Ms,Xp,oe,Zp,sr,_t,rs,$t,ue,ar,vt,Ws,yt,Z,os,tn,Js,er,ln,nr,wt,Hs,kt,S,tr,ce,lr,pr,pn,rr,or,xt,us,Et,N,ur,he,cr,hr,rn,mr,ir,qt,Us,At,me,br,Pt,Ys,Tt,ie,fr,zt,F,be,fe,jr,dr,gr,je,de,_r,$r,vr,ge,_e,yr,wr,Ct,cs,kr,on,xr,Er,Dt,Gs,Ot,R,qr,un,Ar,Pr,cn,Tr,zr,Lt,$e,Cr,It,Vs,St,ss,hs,hn,Ks,Dr,mn,Or,Nt,ms,Lr,bn,Ir,Sr,Ft,B,Nr,fn,Fr,Rr,jn,Br,Mr,Rt,Qs,Bt,is,Wr,dn,Jr,Hr,Mt,as,bs,gn,Xs,Ur,_n,Yr,Wt,ve,Gr,Jt,M,Vr,$n,Kr,Qr,vn,Xr,Zr,Ht,Zs,Ut,fs,Yt,es,js,yn,sa,so,wn,ao,Gt,ye,eo,Vt,z,no,kn,to,lo,xn,po,ro,En,oo,uo,Kt,ds,Qt,ns,gs,qn,aa,co,An,ho,Xt,_s,mo,we,io,bo,Zt,W,fo,ea,jo,go,na,_o,$o,sl,ta,al,J,vo,Pn,yo,wo,Tn,ko,xo,el,la,nl,ke,Eo,tl,H,xe,zn,qo,Ao,Po,Ee,Cn,To,zo,Co,qe,Dn,Do,Oo,ll,$s,Lo,pa,Io,So,pl,Ae,ra,No,oa,Fo,Ro,rl,ua,ol,ca,ha,Bo,On,Mo,Wo,ul,ma,cl,C,Jo,Ln,Ho,Uo,In,Yo,Go,Sn,Vo,Ko,hl,vs,Qo,Pe,Xo,Zo,ml,ia,il,U,su,Nn,au,eu,Fn,nu,tu,bl,ba,fl,Te,lu,jl,fa,dl,ze,pu,gl,ja,_l,ys,ru,Rn,ou,uu,$l,da,vl,Ce,cu,yl,ga,wl,ts,ws,Bn,_a,hu,Mn,mu,kl,ks,iu,De,bu,fu,xl,Y,ju,$a,du,gu,va,_u,$u,El,xs,ql,ya,Al,Es,vu,wa,Wn,yu,wu,Pl,ka,Tl,xa,Jn,Gc,zl,qs,ku,Oe,xu,Eu,Cl,Ea,Dl,D,qu,qa,Hn,Au,Pu,Aa,Tu,zu,Pa,Cu,Du,Ol,Le,L,Ou,Ta,Un,Lu,Iu,za,Yn,Su,Nu,Ca,Gn,Fu,Ru,Ll,Da,Il,Oa,ls,Bu,Ie,Vn,Mu,Wu,Kn,Ju,Hu,Sl,La,Nl,Ia,Sa,Uu,Na,Qn,Yu,Gu,Fl,Fa,Rl,Ra,Ba,Vu,Xn,Ku,Qu,Bl,Ma,Ml,Se,Xu,Wl,Wa,Jl,Ja,Zn,Vc,Hl,ps,As,st,Ha,Zu,at,sc,Ul,Ps,ac,Ne,ec,nc,Yl,G,tc,Ua,lc,pc,Ya,rc,oc,Gl,Ga,Vl,V,uc,et,cc,hc,nt,mc,ic,Kl,Va,Ql,K,bc,tt,fc,jc,lt,dc,gc,Xl,Ka,Zl,Ts,_c,Fe,$c,vc,sp,Qa,ap,zs,yc,Re,wc,kc,ep,Xa,np,Be,P,xc,pt,Ec,qc,rt,Ac,Pc,ot,Tc,zc,ut,Cc,Dc,tp,Za,lp,se,ae,Oc,ct,Lc,Ic,pp,ee,rp,Q,Sc,ht,Nc,Fc,mt,Rc,Bc,op;return x=new Ss({}),I=new gi({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/preprocessing.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/preprocessing.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/preprocessing.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/preprocessing.ipynb"}]}}),rs=new Np({props:{$$slots:{default:[$i]},$$scope:{ctx:A}}}),Ws=new w({props:{code:"pip install datasets",highlighted:"pip install datasets"}}),Js=new Ss({}),Hs=new di({props:{id:"Yffk5aydLzg"}}),us=new Np({props:{$$slots:{default:[vi]},$$scope:{ctx:A}}}),Us=new w({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)`}}),Ys=new w({props:{code:`encoded_input = tokenizer("Do not meddle in the affairs of wizards, for they are subtle and quick to anger.")
print(encoded_input)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(<span class="hljs-string">&quot;Do not meddle in the affairs of wizards, for they are subtle and quick to anger.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">2079</span>, <span class="hljs-number">2025</span>, <span class="hljs-number">19960</span>, <span class="hljs-number">10362</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">3821</span>, <span class="hljs-number">1997</span>, <span class="hljs-number">16657</span>, <span class="hljs-number">1010</span>, <span class="hljs-number">2005</span>, <span class="hljs-number">2027</span>, <span class="hljs-number">2024</span>, <span class="hljs-number">11259</span>, <span class="hljs-number">1998</span>, <span class="hljs-number">4248</span>, <span class="hljs-number">2000</span>, <span class="hljs-number">4963</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Gs=new w({props:{code:'tokenizer.decode(encoded_input["input_ids"])',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(encoded_input[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-string">&#x27;[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger. [SEP]&#x27;</span>`}}),Vs=new w({props:{code:`batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_inputs = tokenizer(batch_sentences)
print(encoded_inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(batch_sentences)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_inputs)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [[<span class="hljs-number">101</span>, <span class="hljs-number">1252</span>, <span class="hljs-number">1184</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1790</span>, <span class="hljs-number">112</span>, <span class="hljs-number">189</span>, <span class="hljs-number">1341</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">3520</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">117</span>, <span class="hljs-number">21902</span>, <span class="hljs-number">1643</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1327</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">5450</span>, <span class="hljs-number">23434</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>]], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]}`}}),Ks=new Ss({}),Qs=new w({props:{code:`batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True)
print(encoded_input)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(batch_sentences, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [[<span class="hljs-number">101</span>, <span class="hljs-number">1252</span>, <span class="hljs-number">1184</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1790</span>, <span class="hljs-number">112</span>, <span class="hljs-number">189</span>, <span class="hljs-number">1341</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">3520</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">117</span>, <span class="hljs-number">21902</span>, <span class="hljs-number">1643</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1327</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">5450</span>, <span class="hljs-number">23434</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]}`}}),Xs=new Ss({}),Zs=new w({props:{code:`batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
]
encoded_input = tokenizer(batch_sentences, padding=True, truncation=True)
print(encoded_input)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;But what about second breakfast?&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Don&#x27;t think he knows about second breakfast, Pip.&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;What about elevensies?&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(batch_sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [[<span class="hljs-number">101</span>, <span class="hljs-number">1252</span>, <span class="hljs-number">1184</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1790</span>, <span class="hljs-number">112</span>, <span class="hljs-number">189</span>, <span class="hljs-number">1341</span>, <span class="hljs-number">1119</span>, <span class="hljs-number">3520</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">6462</span>, <span class="hljs-number">117</span>, <span class="hljs-number">21902</span>, <span class="hljs-number">1643</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1327</span>, <span class="hljs-number">1164</span>, <span class="hljs-number">5450</span>, <span class="hljs-number">23434</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]}`}}),fs=new Np({props:{$$slots:{default:[yi]},$$scope:{ctx:A}}}),sa=new Ss({}),ds=new _i({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ei],pytorch:[ki]},$$scope:{ctx:A}}}),aa=new Ss({}),ta=new w({props:{code:`from datasets import load_dataset, Audio

dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),la=new w({props:{code:'dataset[0]["audio"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.00024414</span>, -<span class="hljs-number">0.00024414</span>, ..., -<span class="hljs-number">0.00024414</span>,
         <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        ], dtype=float32),
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav&#x27;</span>,
 <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">8000</span>}`}}),ua=new w({props:{code:'dataset = dataset.cast_column("audio", Audio(sampling_rate=16_000))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16_000</span>))'}}),ma=new w({props:{code:'dataset[0]["audio"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">2.3443763e-05</span>,  <span class="hljs-number">2.1729663e-04</span>,  <span class="hljs-number">2.2145823e-04</span>, ...,
         <span class="hljs-number">3.8356509e-05</span>, -<span class="hljs-number">7.3497440e-06</span>, -<span class="hljs-number">2.1754686e-05</span>], dtype=float32),
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav&#x27;</span>,
 <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">16000</span>}`}}),ia=new w({props:{code:`from transformers import AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>)`}}),ba=new w({props:{code:`audio_input = [dataset[0]["audio"]["array"]]
feature_extractor(audio_input, sampling_rate=16000)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>audio_input = [dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor(audio_input, sampling_rate=<span class="hljs-number">16000</span>)
{<span class="hljs-string">&#x27;input_values&#x27;</span>: [array([ <span class="hljs-number">3.8106556e-04</span>,  <span class="hljs-number">2.7506407e-03</span>,  <span class="hljs-number">2.8015103e-03</span>, ...,
        <span class="hljs-number">5.6335266e-04</span>,  <span class="hljs-number">4.6588284e-06</span>, -<span class="hljs-number">1.7142107e-04</span>], dtype=float32)]}`}}),fa=new w({props:{code:`dataset[0]["audio"]["array"].shape

dataset[1]["audio"]["array"].shape`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>].shape
(<span class="hljs-number">173398</span>,)

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">1</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>].shape
(<span class="hljs-number">106496</span>,)`}}),ja=new w({props:{code:`def preprocess_function(examples):
    audio_arrays = [x["array"] for x in examples["audio"]]
    inputs = feature_extractor(
        audio_arrays,
        sampling_rate=16000,
        padding=True,
        max_length=100000,
        truncation=True,
    )
    return inputs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    audio_arrays = [x[<span class="hljs-string">&quot;array&quot;</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;audio&quot;</span>]]
<span class="hljs-meta">... </span>    inputs = feature_extractor(
<span class="hljs-meta">... </span>        audio_arrays,
<span class="hljs-meta">... </span>        sampling_rate=<span class="hljs-number">16000</span>,
<span class="hljs-meta">... </span>        padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>        max_length=<span class="hljs-number">100000</span>,
<span class="hljs-meta">... </span>        truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> inputs`}}),da=new w({props:{code:"processed_dataset = preprocess_function(dataset[:5])",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>processed_dataset = preprocess_function(dataset[:<span class="hljs-number">5</span>])'}}),ga=new w({props:{code:`processed_dataset["input_values"][0].shape

processed_dataset["input_values"][1].shape`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>processed_dataset[<span class="hljs-string">&quot;input_values&quot;</span>][<span class="hljs-number">0</span>].shape
(<span class="hljs-number">100000</span>,)

<span class="hljs-meta">&gt;&gt;&gt; </span>processed_dataset[<span class="hljs-string">&quot;input_values&quot;</span>][<span class="hljs-number">1</span>].shape
(<span class="hljs-number">100000</span>,)`}}),_a=new Ss({}),xs=new Np({props:{$$slots:{default:[qi]},$$scope:{ctx:A}}}),ya=new w({props:{code:`from datasets import load_dataset

dataset = load_dataset("food101", split="train[:100]")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;food101&quot;</span>, split=<span class="hljs-string">&quot;train[:100]&quot;</span>)`}}),ka=new w({props:{code:'dataset[0]["image"]',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;image&quot;</span>]'}}),Ea=new w({props:{code:`from transformers import AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained("google/vit-base-patch16-224")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;google/vit-base-patch16-224&quot;</span>)`}}),Da=new w({props:{code:`from torchvision.transforms import Compose, Normalize, RandomResizedCrop, ColorJitter, ToTensor

normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
size = (
    feature_extractor.size["shortest_edge"]
    if "shortest_edge" in feature_extractor.size
    else (feature_extractor.size["height"], feature_extractor.size["width"])
)
_transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5), ToTensor(), normalize])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> Compose, Normalize, RandomResizedCrop, ColorJitter, ToTensor

<span class="hljs-meta">&gt;&gt;&gt; </span>normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
<span class="hljs-meta">&gt;&gt;&gt; </span>size = (
<span class="hljs-meta">... </span>    feature_extractor.size[<span class="hljs-string">&quot;shortest_edge&quot;</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;shortest_edge&quot;</span> <span class="hljs-keyword">in</span> feature_extractor.size
<span class="hljs-meta">... </span>    <span class="hljs-keyword">else</span> (feature_extractor.size[<span class="hljs-string">&quot;height&quot;</span>], feature_extractor.size[<span class="hljs-string">&quot;width&quot;</span>])
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>_transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=<span class="hljs-number">0.5</span>, hue=<span class="hljs-number">0.5</span>), ToTensor(), normalize])`}}),La=new w({props:{code:`def transforms(examples):
    examples["pixel_values"] = [_transforms(image.convert("RGB")) for image in examples["image"]]
    return examples`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">transforms</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    examples[<span class="hljs-string">&quot;pixel_values&quot;</span>] = [_transforms(image.convert(<span class="hljs-string">&quot;RGB&quot;</span>)) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> examples`}}),Fa=new w({props:{code:"dataset.set_transform(transforms)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_transform(transforms)'}}),Ma=new w({props:{code:'dataset[0]["image"]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;image&quot;</span>]
{<span class="hljs-string">&#x27;image&#x27;</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=384x512 at <span class="hljs-number">0x7F1A7B0630D0</span>&gt;,
 <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">6</span>,
 <span class="hljs-string">&#x27;pixel_values&#x27;</span>: tensor([[[ <span class="hljs-number">0.0353</span>,  <span class="hljs-number">0.0745</span>,  <span class="hljs-number">0.1216</span>,  ..., -<span class="hljs-number">0.9922</span>, -<span class="hljs-number">0.9922</span>, -<span class="hljs-number">0.9922</span>],
          [-<span class="hljs-number">0.0196</span>,  <span class="hljs-number">0.0667</span>,  <span class="hljs-number">0.1294</span>,  ..., -<span class="hljs-number">0.9765</span>, -<span class="hljs-number">0.9843</span>, -<span class="hljs-number">0.9922</span>],
          [ <span class="hljs-number">0.0196</span>,  <span class="hljs-number">0.0824</span>,  <span class="hljs-number">0.1137</span>,  ..., -<span class="hljs-number">0.9765</span>, -<span class="hljs-number">0.9686</span>, -<span class="hljs-number">0.8667</span>],
          ...,
          [ <span class="hljs-number">0.0275</span>,  <span class="hljs-number">0.0745</span>,  <span class="hljs-number">0.0510</span>,  ..., -<span class="hljs-number">0.1137</span>, -<span class="hljs-number">0.1216</span>, -<span class="hljs-number">0.0824</span>],
          [ <span class="hljs-number">0.0667</span>,  <span class="hljs-number">0.0824</span>,  <span class="hljs-number">0.0667</span>,  ..., -<span class="hljs-number">0.0588</span>, -<span class="hljs-number">0.0745</span>, -<span class="hljs-number">0.0980</span>],
          [ <span class="hljs-number">0.0353</span>,  <span class="hljs-number">0.0353</span>,  <span class="hljs-number">0.0431</span>,  ..., -<span class="hljs-number">0.0039</span>, -<span class="hljs-number">0.0039</span>, -<span class="hljs-number">0.0588</span>]],
 
         [[ <span class="hljs-number">0.2078</span>,  <span class="hljs-number">0.2471</span>,  <span class="hljs-number">0.2863</span>,  ..., -<span class="hljs-number">0.9451</span>, -<span class="hljs-number">0.9373</span>, -<span class="hljs-number">0.9451</span>],
          [ <span class="hljs-number">0.1608</span>,  <span class="hljs-number">0.2471</span>,  <span class="hljs-number">0.3098</span>,  ..., -<span class="hljs-number">0.9373</span>, -<span class="hljs-number">0.9451</span>, -<span class="hljs-number">0.9373</span>],
          [ <span class="hljs-number">0.2078</span>,  <span class="hljs-number">0.2706</span>,  <span class="hljs-number">0.3020</span>,  ..., -<span class="hljs-number">0.9608</span>, -<span class="hljs-number">0.9373</span>, -<span class="hljs-number">0.8275</span>],
          ...,
          [-<span class="hljs-number">0.0353</span>,  <span class="hljs-number">0.0118</span>, -<span class="hljs-number">0.0039</span>,  ..., -<span class="hljs-number">0.2392</span>, -<span class="hljs-number">0.2471</span>, -<span class="hljs-number">0.2078</span>],
          [ <span class="hljs-number">0.0196</span>,  <span class="hljs-number">0.0353</span>,  <span class="hljs-number">0.0196</span>,  ..., -<span class="hljs-number">0.1843</span>, -<span class="hljs-number">0.2000</span>, -<span class="hljs-number">0.2235</span>],
          [-<span class="hljs-number">0.0118</span>, -<span class="hljs-number">0.0039</span>, -<span class="hljs-number">0.0039</span>,  ..., -<span class="hljs-number">0.0980</span>, -<span class="hljs-number">0.0980</span>, -<span class="hljs-number">0.1529</span>]],
 
         [[ <span class="hljs-number">0.3961</span>,  <span class="hljs-number">0.4431</span>,  <span class="hljs-number">0.4980</span>,  ..., -<span class="hljs-number">0.9216</span>, -<span class="hljs-number">0.9137</span>, -<span class="hljs-number">0.9216</span>],
          [ <span class="hljs-number">0.3569</span>,  <span class="hljs-number">0.4510</span>,  <span class="hljs-number">0.5216</span>,  ..., -<span class="hljs-number">0.9059</span>, -<span class="hljs-number">0.9137</span>, -<span class="hljs-number">0.9137</span>],
          [ <span class="hljs-number">0.4118</span>,  <span class="hljs-number">0.4745</span>,  <span class="hljs-number">0.5216</span>,  ..., -<span class="hljs-number">0.9137</span>, -<span class="hljs-number">0.8902</span>, -<span class="hljs-number">0.7804</span>],
          ...,
          [-<span class="hljs-number">0.2314</span>, -<span class="hljs-number">0.1922</span>, -<span class="hljs-number">0.2078</span>,  ..., -<span class="hljs-number">0.4196</span>, -<span class="hljs-number">0.4275</span>, -<span class="hljs-number">0.3882</span>],
          [-<span class="hljs-number">0.1843</span>, -<span class="hljs-number">0.1686</span>, -<span class="hljs-number">0.2000</span>,  ..., -<span class="hljs-number">0.3647</span>, -<span class="hljs-number">0.3804</span>, -<span class="hljs-number">0.4039</span>],
          [-<span class="hljs-number">0.1922</span>, -<span class="hljs-number">0.1922</span>, -<span class="hljs-number">0.1922</span>,  ..., -<span class="hljs-number">0.2941</span>, -<span class="hljs-number">0.2863</span>, -<span class="hljs-number">0.3412</span>]]])}`}}),Wa=new w({props:{code:`import numpy as np
import matplotlib.pyplot as plt

img = dataset[0]["pixel_values"]
plt.imshow(img.permute(1, 2, 0))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt

<span class="hljs-meta">&gt;&gt;&gt; </span>img = dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;pixel_values&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>plt.imshow(img.permute(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>))`}}),Ha=new Ss({}),Ga=new w({props:{code:`from datasets import load_dataset

lj_speech = load_dataset("lj_speech", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>lj_speech = load_dataset(<span class="hljs-string">&quot;lj_speech&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Va=new w({props:{code:'lj_speech = lj_speech.map(remove_columns=["file", "id", "normalized_text"])',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>lj_speech = lj_speech.<span class="hljs-built_in">map</span>(remove_columns=[<span class="hljs-string">&quot;file&quot;</span>, <span class="hljs-string">&quot;id&quot;</span>, <span class="hljs-string">&quot;normalized_text&quot;</span>])'}}),Ka=new w({props:{code:`lj_speech[0]["audio"]

lj_speech[0]["text"]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>lj_speech[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>]
{<span class="hljs-string">&#x27;array&#x27;</span>: array([-<span class="hljs-number">7.3242188e-04</span>, -<span class="hljs-number">7.6293945e-04</span>, -<span class="hljs-number">6.4086914e-04</span>, ...,
         <span class="hljs-number">7.3242188e-04</span>,  <span class="hljs-number">2.1362305e-04</span>,  <span class="hljs-number">6.1035156e-05</span>], dtype=float32),
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/917ece08c95cf0c4115e45294e3cd0dee724a1165b7fc11798369308a465bd26/LJSpeech-1.1/wavs/LJ001-0001.wav&#x27;</span>,
 <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">22050</span>}

<span class="hljs-meta">&gt;&gt;&gt; </span>lj_speech[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;text&quot;</span>]
<span class="hljs-string">&#x27;Printing, in the only sense with which we are at present concerned, differs from most if not from all the arts and crafts represented in the Exhibition&#x27;</span>`}}),Qa=new w({props:{code:'lj_speech = lj_speech.cast_column("audio", Audio(sampling_rate=16_000))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>lj_speech = lj_speech.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16_000</span>))'}}),Xa=new w({props:{code:`from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)`}}),Za=new w({props:{code:`def prepare_dataset(example):
    audio = example["audio"]

    example.update(processor(audio=audio["array"], text=example["text"], sampling_rate=16000))

    return example`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_dataset</span>(<span class="hljs-params">example</span>):
<span class="hljs-meta">... </span>    audio = example[<span class="hljs-string">&quot;audio&quot;</span>]

<span class="hljs-meta">... </span>    example.update(processor(audio=audio[<span class="hljs-string">&quot;array&quot;</span>], text=example[<span class="hljs-string">&quot;text&quot;</span>], sampling_rate=<span class="hljs-number">16000</span>))

<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> example`}}),ee=new w({props:{code:"prepare_dataset(lj_speech[0])",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>prepare_dataset(lj_speech[<span class="hljs-number">0</span>])'}}),{c(){i=p("meta"),v=c(),b=p("h1"),y=p("a"),k=p("span"),f(x.$$.fragment),E=c(),q=p("span"),O=n("Preprocess"),Ns=c(),f(I.$$.fragment),dt=c(),te=p("p"),Fp=n("Before you can train a model on a dataset, it needs to be preprocessed into the expected model input format. Whether your data is text, images, or audio, they need to be converted and assembled into batches of tensors. \u{1F917} Transformers provides a set of preprocessing classes to help prepare your data for the model. In this tutorial, you\u2019ll learn that for:"),gt=c(),T=p("ul"),Fs=p("li"),Rp=n("Text, use a "),le=p("a"),Bp=n("Tokenizer"),Mp=n(" to convert text into a sequence of tokens, create a numerical representation of the tokens, and assemble them into tensors."),Wp=c(),Rs=p("li"),Jp=n("Image inputs use a "),pe=p("a"),Hp=n("ImageProcessor"),Up=n(" to convert images into tensors."),Yp=c(),Bs=p("li"),Gp=n("Speech and audio, use a "),re=p("a"),Vp=n("Feature extractor"),Kp=n(" to extract sequential features from audio waveforms and convert them into tensors."),Qp=c(),Ms=p("li"),Xp=n("Multimodal inputs, use a "),oe=p("a"),Zp=n("Processor"),sr=n(" to combine a tokenizer and a feature extractor."),_t=c(),f(rs.$$.fragment),$t=c(),ue=p("p"),ar=n("Before you begin, install \u{1F917} Datasets so you can load some datasets to experiment with:"),vt=c(),f(Ws.$$.fragment),yt=c(),Z=p("h2"),os=p("a"),tn=p("span"),f(Js.$$.fragment),er=c(),ln=p("span"),nr=n("Natural Language Processing"),wt=c(),f(Hs.$$.fragment),kt=c(),S=p("p"),tr=n("The main tool for preprocessing textual data is a "),ce=p("a"),lr=n("tokenizer"),pr=n(". A tokenizer splits text into "),pn=p("em"),rr=n("tokens"),or=n(" according to a set of rules. The tokens are converted into numbers and then tensors, which become the model inputs. Any additional inputs required by the model are added by the tokenizer."),xt=c(),f(us.$$.fragment),Et=c(),N=p("p"),ur=n("Get started by loading a pretrained tokenizer with the "),he=p("a"),cr=n("AutoTokenizer.from_pretrained()"),hr=n(" method. This downloads the "),rn=p("em"),mr=n("vocab"),ir=n(" a model was pretrained with:"),qt=c(),f(Us.$$.fragment),At=c(),me=p("p"),br=n("Then pass your text to the tokenizer:"),Pt=c(),f(Ys.$$.fragment),Tt=c(),ie=p("p"),fr=n("The tokenizer returns a dictionary with three important items:"),zt=c(),F=p("ul"),be=p("li"),fe=p("a"),jr=n("input_ids"),dr=n(" are the indices corresponding to each token in the sentence."),gr=c(),je=p("li"),de=p("a"),_r=n("attention_mask"),$r=n(" indicates whether a token should be attended to or not."),vr=c(),ge=p("li"),_e=p("a"),yr=n("token_type_ids"),wr=n(" identifies which sequence a token belongs to when there is more than one sequence."),Ct=c(),cs=p("p"),kr=n("Return your input by decoding the "),on=p("code"),xr=n("input_ids"),Er=n(":"),Dt=c(),f(Gs.$$.fragment),Ot=c(),R=p("p"),qr=n("As you can see, the tokenizer added two special tokens - "),un=p("code"),Ar=n("CLS"),Pr=n(" and "),cn=p("code"),Tr=n("SEP"),zr=n(` (classifier and separator) - to the sentence. Not all models need
special tokens, but if they do, the tokenizer automatically adds them for you.`),Lt=c(),$e=p("p"),Cr=n("If there are several sentences you want to preprocess, pass them as a list to the tokenizer:"),It=c(),f(Vs.$$.fragment),St=c(),ss=p("h3"),hs=p("a"),hn=p("span"),f(Ks.$$.fragment),Dr=c(),mn=p("span"),Or=n("Pad"),Nt=c(),ms=p("p"),Lr=n("Sentences aren\u2019t always the same length which can be an issue because tensors, the model inputs, need to have a uniform shape. Padding is a strategy for ensuring tensors are rectangular by adding a special "),bn=p("em"),Ir=n("padding token"),Sr=n(" to shorter sentences."),Ft=c(),B=p("p"),Nr=n("Set the "),fn=p("code"),Fr=n("padding"),Rr=n(" parameter to "),jn=p("code"),Br=n("True"),Mr=n(" to pad the shorter sequences in the batch to match the longest sequence:"),Rt=c(),f(Qs.$$.fragment),Bt=c(),is=p("p"),Wr=n("The first and third sentences are now padded with "),dn=p("code"),Jr=n("0"),Hr=n("\u2019s because they are shorter."),Mt=c(),as=p("h3"),bs=p("a"),gn=p("span"),f(Xs.$$.fragment),Ur=c(),_n=p("span"),Yr=n("Truncation"),Wt=c(),ve=p("p"),Gr=n("On the other end of the spectrum, sometimes a sequence may be too long for a model to handle. In this case, you\u2019ll need to truncate the sequence to a shorter length."),Jt=c(),M=p("p"),Vr=n("Set the "),$n=p("code"),Kr=n("truncation"),Qr=n(" parameter to "),vn=p("code"),Xr=n("True"),Zr=n(" to truncate a sequence to the maximum length accepted by the model:"),Ht=c(),f(Zs.$$.fragment),Ut=c(),f(fs.$$.fragment),Yt=c(),es=p("h3"),js=p("a"),yn=p("span"),f(sa.$$.fragment),so=c(),wn=p("span"),ao=n("Build tensors"),Gt=c(),ye=p("p"),eo=n("Finally, you want the tokenizer to return the actual tensors that get fed to the model."),Vt=c(),z=p("p"),no=n("Set the "),kn=p("code"),to=n("return_tensors"),lo=n(" parameter to either "),xn=p("code"),po=n("pt"),ro=n(" for PyTorch, or "),En=p("code"),oo=n("tf"),uo=n(" for TensorFlow:"),Kt=c(),f(ds.$$.fragment),Qt=c(),ns=p("h2"),gs=p("a"),qn=p("span"),f(aa.$$.fragment),co=c(),An=p("span"),ho=n("Audio"),Xt=c(),_s=p("p"),mo=n("For audio tasks, you\u2019ll need a "),we=p("a"),io=n("feature extractor"),bo=n(" to prepare your dataset for the model. The feature extractor is designed to extract features from raw audio data, and convert them into tensors."),Zt=c(),W=p("p"),fo=n("Load the "),ea=p("a"),jo=n("MInDS-14"),go=n(" dataset (see the \u{1F917} "),na=p("a"),_o=n("Datasets tutorial"),$o=n(" for more details on how to load a dataset) to see how you can use a feature extractor with audio datasets:"),sl=c(),f(ta.$$.fragment),al=c(),J=p("p"),vo=n("Access the first element of the "),Pn=p("code"),yo=n("audio"),wo=n(" column to take a look at the input. Calling the "),Tn=p("code"),ko=n("audio"),xo=n(" column automatically loads and resamples the audio file:"),el=c(),f(la.$$.fragment),nl=c(),ke=p("p"),Eo=n("This returns three items:"),tl=c(),H=p("ul"),xe=p("li"),zn=p("code"),qo=n("array"),Ao=n(" is the speech signal loaded - and potentially resampled - as a 1D array."),Po=c(),Ee=p("li"),Cn=p("code"),To=n("path"),zo=n(" points to the location of the audio file."),Co=c(),qe=p("li"),Dn=p("code"),Do=n("sampling_rate"),Oo=n(" refers to how many data points in the speech signal are measured per second."),ll=c(),$s=p("p"),Lo=n("For this tutorial, you\u2019ll use the "),pa=p("a"),Io=n("Wav2Vec2"),So=n(" model. Take a look at the model card, and you\u2019ll learn Wav2Vec2 is pretrained on 16kHz sampled speech audio. It is important your audio data\u2019s sampling rate matches the sampling rate of the dataset used to pretrain the model. If your data\u2019s sampling rate isn\u2019t the same, then you need to resample your data."),pl=c(),Ae=p("ol"),ra=p("li"),No=n("Use \u{1F917} Datasets\u2019 "),oa=p("a"),Fo=n("cast_column"),Ro=n(" method to upsample the sampling rate to 16kHz:"),rl=c(),f(ua.$$.fragment),ol=c(),ca=p("ol"),ha=p("li"),Bo=n("Call the "),On=p("code"),Mo=n("audio"),Wo=n(" column again to resample the audio file:"),ul=c(),f(ma.$$.fragment),cl=c(),C=p("p"),Jo=n("Next, load a feature extractor to normalize and pad the input. When padding textual data, a "),Ln=p("code"),Ho=n("0"),Uo=n(" is added for shorter sequences. The same idea applies to audio data. The feature extractor adds a "),In=p("code"),Yo=n("0"),Go=n(" - interpreted as silence - to "),Sn=p("code"),Vo=n("array"),Ko=n("."),hl=c(),vs=p("p"),Qo=n("Load the feature extractor with "),Pe=p("a"),Xo=n("AutoFeatureExtractor.from_pretrained()"),Zo=n(":"),ml=c(),f(ia.$$.fragment),il=c(),U=p("p"),su=n("Pass the audio "),Nn=p("code"),au=n("array"),eu=n(" to the feature extractor. We also recommend adding the "),Fn=p("code"),nu=n("sampling_rate"),tu=n(" argument in the feature extractor in order to better debug any silent errors that may occur."),bl=c(),f(ba.$$.fragment),fl=c(),Te=p("p"),lu=n("Just like the tokenizer, you can apply padding or truncation to handle variable sequences in a batch. Take a look at the sequence length of these two audio samples:"),jl=c(),f(fa.$$.fragment),dl=c(),ze=p("p"),pu=n("Create a function to preprocess the dataset so the audio samples are the same lengths. Specify a maximum sample length, and the feature extractor will either pad or truncate the sequences to match it:"),gl=c(),f(ja.$$.fragment),_l=c(),ys=p("p"),ru=n("Apply the "),Rn=p("code"),ou=n("preprocess_function"),uu=n(" to the the first few examples in the dataset:"),$l=c(),f(da.$$.fragment),vl=c(),Ce=p("p"),cu=n("The sample lengths are now the same and match the specified maximum length. You can pass your processed dataset to the model now!"),yl=c(),f(ga.$$.fragment),wl=c(),ts=p("h2"),ws=p("a"),Bn=p("span"),f(_a.$$.fragment),hu=c(),Mn=p("span"),mu=n("Computer vision"),kl=c(),ks=p("p"),iu=n("For computer vision tasks, you\u2019ll need a "),De=p("a"),bu=n("feature extractor"),fu=n(" to prepare your dataset for the model. The feature extractor is designed to extract features from images, and convert them into tensors."),xl=c(),Y=p("p"),ju=n("Load the "),$a=p("a"),du=n("food101"),gu=n(" dataset (see the \u{1F917} "),va=p("a"),_u=n("Datasets tutorial"),$u=n(" for more details on how to load a dataset) to see how you can use a feature extractor with computer vision datasets:"),El=c(),f(xs.$$.fragment),ql=c(),f(ya.$$.fragment),Al=c(),Es=p("p"),vu=n("Next, take a look at the image with \u{1F917} Datasets "),wa=p("a"),Wn=p("code"),yu=n("Image"),wu=n(" feature:"),Pl=c(),f(ka.$$.fragment),Tl=c(),xa=p("div"),Jn=p("img"),zl=c(),qs=p("p"),ku=n("Load the feature extractor with "),Oe=p("a"),xu=n("AutoFeatureExtractor.from_pretrained()"),Eu=n(":"),Cl=c(),f(Ea.$$.fragment),Dl=c(),D=p("p"),qu=n("For computer vision tasks, it is common to add some type of data augmentation to the images as a part of preprocessing. You can add augmentations with any library you\u2019d like, but in this tutorial, you\u2019ll use torchvision\u2019s "),qa=p("a"),Hn=p("code"),Au=n("transforms"),Pu=n(" module. If you\u2019re interested in using another data augmentation library, learn how in the "),Aa=p("a"),Tu=n("Albumentations"),zu=n(" or "),Pa=p("a"),Cu=n("Kornia notebooks"),Du=n("."),Ol=c(),Le=p("ol"),L=p("li"),Ou=n("Normalize the image with the feature extractor and use "),Ta=p("a"),Un=p("code"),Lu=n("Compose"),Iu=n(" to chain some transforms - "),za=p("a"),Yn=p("code"),Su=n("RandomResizedCrop"),Nu=n(" and "),Ca=p("a"),Gn=p("code"),Fu=n("ColorJitter"),Ru=n(" - together:"),Ll=c(),f(Da.$$.fragment),Il=c(),Oa=p("ol"),ls=p("li"),Bu=n("The model accepts "),Ie=p("a"),Vn=p("code"),Mu=n("pixel_values"),Wu=n(" as its input, which is generated by the feature extractor. Create a function that generates "),Kn=p("code"),Ju=n("pixel_values"),Hu=n(" from the transforms:"),Sl=c(),f(La.$$.fragment),Nl=c(),Ia=p("ol"),Sa=p("li"),Uu=n("Then use \u{1F917} Datasets "),Na=p("a"),Qn=p("code"),Yu=n("set_transform"),Gu=n(" to apply the transforms on the fly:"),Fl=c(),f(Fa.$$.fragment),Rl=c(),Ra=p("ol"),Ba=p("li"),Vu=n("Now when you access the image, you\u2019ll notice the feature extractor has added "),Xn=p("code"),Ku=n("pixel_values"),Qu=n(". You can pass your processed dataset to the model now!"),Bl=c(),f(Ma.$$.fragment),Ml=c(),Se=p("p"),Xu=n("Here is what the image looks like after the transforms are applied. The image has been randomly cropped and it\u2019s color properties are different."),Wl=c(),f(Wa.$$.fragment),Jl=c(),Ja=p("div"),Zn=p("img"),Hl=c(),ps=p("h2"),As=p("a"),st=p("span"),f(Ha.$$.fragment),Zu=c(),at=p("span"),sc=n("Multimodal"),Ul=c(),Ps=p("p"),ac=n("For tasks involving multimodal inputs, you\u2019ll need a "),Ne=p("a"),ec=n("processor"),nc=n(" to prepare your dataset for the model. A processor couples a tokenizer and feature extractor."),Yl=c(),G=p("p"),tc=n("Load the "),Ua=p("a"),lc=n("LJ Speech"),pc=n(" dataset (see the \u{1F917} "),Ya=p("a"),rc=n("Datasets tutorial"),oc=n(" for more details on how to load a dataset) to see how you can use a processor for automatic speech recognition (ASR):"),Gl=c(),f(Ga.$$.fragment),Vl=c(),V=p("p"),uc=n("For ASR, you\u2019re mainly focused on "),et=p("code"),cc=n("audio"),hc=n(" and "),nt=p("code"),mc=n("text"),ic=n(" so you can remove the other columns:"),Kl=c(),f(Va.$$.fragment),Ql=c(),K=p("p"),bc=n("Now take a look at the "),tt=p("code"),fc=n("audio"),jc=n(" and "),lt=p("code"),dc=n("text"),gc=n(" columns:"),Xl=c(),f(Ka.$$.fragment),Zl=c(),Ts=p("p"),_c=n("Remember you should always "),Fe=p("a"),$c=n("resample"),vc=n(" your audio dataset\u2019s sampling rate to match the sampling rate of the dataset used to pretrain a model!"),sp=c(),f(Qa.$$.fragment),ap=c(),zs=p("p"),yc=n("Load a processor with "),Re=p("a"),wc=n("AutoProcessor.from_pretrained()"),kc=n(":"),ep=c(),f(Xa.$$.fragment),np=c(),Be=p("ol"),P=p("li"),xc=n("Create a function to process the audio data contained in "),pt=p("code"),Ec=n("array"),qc=n(" to "),rt=p("code"),Ac=n("input_values"),Pc=n(", and tokenize "),ot=p("code"),Tc=n("text"),zc=n(" to "),ut=p("code"),Cc=n("labels"),Dc=n(". These are the inputs to the model:"),tp=c(),f(Za.$$.fragment),lp=c(),se=p("ol"),ae=p("li"),Oc=n("Apply the "),ct=p("code"),Lc=n("prepare_dataset"),Ic=n(" function to a sample:"),pp=c(),f(ee.$$.fragment),rp=c(),Q=p("p"),Sc=n("The processor has now added "),ht=p("code"),Nc=n("input_values"),Fc=n(" and "),mt=p("code"),Rc=n("labels"),Bc=n(", and the sampling rate has also been correctly downsampled to 16kHz. You can pass your processed dataset to the model now!"),this.h()},l(s){const l=fi('[data-svelte="svelte-1phssyn"]',document.head);i=r(l,"META",{name:!0,content:!0}),l.forEach(e),v=h(s),b=r(s,"H1",{class:!0});var ne=o(b);y=r(ne,"A",{id:!0,class:!0,href:!0});var it=o(y);k=r(it,"SPAN",{});var bt=o(k);j(x.$$.fragment,bt),bt.forEach(e),it.forEach(e),E=h(ne),q=r(ne,"SPAN",{});var ft=o(q);O=t(ft,"Preprocess"),ft.forEach(e),ne.forEach(e),Ns=h(s),j(I.$$.fragment,s),dt=h(s),te=r(s,"P",{});var jt=o(te);Fp=t(jt,"Before you can train a model on a dataset, it needs to be preprocessed into the expected model input format. Whether your data is text, images, or audio, they need to be converted and assembled into batches of tensors. \u{1F917} Transformers provides a set of preprocessing classes to help prepare your data for the model. In this tutorial, you\u2019ll learn that for:"),jt.forEach(e),gt=h(s),T=r(s,"UL",{});var Cs=o(T);Fs=r(Cs,"LI",{});var up=o(Fs);Rp=t(up,"Text, use a "),le=r(up,"A",{href:!0});var Kc=o(le);Bp=t(Kc,"Tokenizer"),Kc.forEach(e),Mp=t(up," to convert text into a sequence of tokens, create a numerical representation of the tokens, and assemble them into tensors."),up.forEach(e),Wp=h(Cs),Rs=r(Cs,"LI",{});var cp=o(Rs);Jp=t(cp,"Image inputs use a "),pe=r(cp,"A",{href:!0});var Qc=o(pe);Hp=t(Qc,"ImageProcessor"),Qc.forEach(e),Up=t(cp," to convert images into tensors."),cp.forEach(e),Yp=h(Cs),Bs=r(Cs,"LI",{});var hp=o(Bs);Gp=t(hp,"Speech and audio, use a "),re=r(hp,"A",{href:!0});var Xc=o(re);Vp=t(Xc,"Feature extractor"),Xc.forEach(e),Kp=t(hp," to extract sequential features from audio waveforms and convert them into tensors."),hp.forEach(e),Qp=h(Cs),Ms=r(Cs,"LI",{});var mp=o(Ms);Xp=t(mp,"Multimodal inputs, use a "),oe=r(mp,"A",{href:!0});var Zc=o(oe);Zp=t(Zc,"Processor"),Zc.forEach(e),sr=t(mp," to combine a tokenizer and a feature extractor."),mp.forEach(e),Cs.forEach(e),_t=h(s),j(rs.$$.fragment,s),$t=h(s),ue=r(s,"P",{});var sh=o(ue);ar=t(sh,"Before you begin, install \u{1F917} Datasets so you can load some datasets to experiment with:"),sh.forEach(e),vt=h(s),j(Ws.$$.fragment,s),yt=h(s),Z=r(s,"H2",{class:!0});var ip=o(Z);os=r(ip,"A",{id:!0,class:!0,href:!0});var ah=o(os);tn=r(ah,"SPAN",{});var eh=o(tn);j(Js.$$.fragment,eh),eh.forEach(e),ah.forEach(e),er=h(ip),ln=r(ip,"SPAN",{});var nh=o(ln);nr=t(nh,"Natural Language Processing"),nh.forEach(e),ip.forEach(e),wt=h(s),j(Hs.$$.fragment,s),kt=h(s),S=r(s,"P",{});var Me=o(S);tr=t(Me,"The main tool for preprocessing textual data is a "),ce=r(Me,"A",{href:!0});var th=o(ce);lr=t(th,"tokenizer"),th.forEach(e),pr=t(Me,". A tokenizer splits text into "),pn=r(Me,"EM",{});var lh=o(pn);rr=t(lh,"tokens"),lh.forEach(e),or=t(Me," according to a set of rules. The tokens are converted into numbers and then tensors, which become the model inputs. Any additional inputs required by the model are added by the tokenizer."),Me.forEach(e),xt=h(s),j(us.$$.fragment,s),Et=h(s),N=r(s,"P",{});var We=o(N);ur=t(We,"Get started by loading a pretrained tokenizer with the "),he=r(We,"A",{href:!0});var ph=o(he);cr=t(ph,"AutoTokenizer.from_pretrained()"),ph.forEach(e),hr=t(We," method. This downloads the "),rn=r(We,"EM",{});var rh=o(rn);mr=t(rh,"vocab"),rh.forEach(e),ir=t(We," a model was pretrained with:"),We.forEach(e),qt=h(s),j(Us.$$.fragment,s),At=h(s),me=r(s,"P",{});var oh=o(me);br=t(oh,"Then pass your text to the tokenizer:"),oh.forEach(e),Pt=h(s),j(Ys.$$.fragment,s),Tt=h(s),ie=r(s,"P",{});var uh=o(ie);fr=t(uh,"The tokenizer returns a dictionary with three important items:"),uh.forEach(e),zt=h(s),F=r(s,"UL",{});var Je=o(F);be=r(Je,"LI",{});var Mc=o(be);fe=r(Mc,"A",{href:!0});var ch=o(fe);jr=t(ch,"input_ids"),ch.forEach(e),dr=t(Mc," are the indices corresponding to each token in the sentence."),Mc.forEach(e),gr=h(Je),je=r(Je,"LI",{});var Wc=o(je);de=r(Wc,"A",{href:!0});var hh=o(de);_r=t(hh,"attention_mask"),hh.forEach(e),$r=t(Wc," indicates whether a token should be attended to or not."),Wc.forEach(e),vr=h(Je),ge=r(Je,"LI",{});var Jc=o(ge);_e=r(Jc,"A",{href:!0});var mh=o(_e);yr=t(mh,"token_type_ids"),mh.forEach(e),wr=t(Jc," identifies which sequence a token belongs to when there is more than one sequence."),Jc.forEach(e),Je.forEach(e),Ct=h(s),cs=r(s,"P",{});var bp=o(cs);kr=t(bp,"Return your input by decoding the "),on=r(bp,"CODE",{});var ih=o(on);xr=t(ih,"input_ids"),ih.forEach(e),Er=t(bp,":"),bp.forEach(e),Dt=h(s),j(Gs.$$.fragment,s),Ot=h(s),R=r(s,"P",{});var He=o(R);qr=t(He,"As you can see, the tokenizer added two special tokens - "),un=r(He,"CODE",{});var bh=o(un);Ar=t(bh,"CLS"),bh.forEach(e),Pr=t(He," and "),cn=r(He,"CODE",{});var fh=o(cn);Tr=t(fh,"SEP"),fh.forEach(e),zr=t(He,` (classifier and separator) - to the sentence. Not all models need
special tokens, but if they do, the tokenizer automatically adds them for you.`),He.forEach(e),Lt=h(s),$e=r(s,"P",{});var jh=o($e);Cr=t(jh,"If there are several sentences you want to preprocess, pass them as a list to the tokenizer:"),jh.forEach(e),It=h(s),j(Vs.$$.fragment,s),St=h(s),ss=r(s,"H3",{class:!0});var fp=o(ss);hs=r(fp,"A",{id:!0,class:!0,href:!0});var dh=o(hs);hn=r(dh,"SPAN",{});var gh=o(hn);j(Ks.$$.fragment,gh),gh.forEach(e),dh.forEach(e),Dr=h(fp),mn=r(fp,"SPAN",{});var _h=o(mn);Or=t(_h,"Pad"),_h.forEach(e),fp.forEach(e),Nt=h(s),ms=r(s,"P",{});var jp=o(ms);Lr=t(jp,"Sentences aren\u2019t always the same length which can be an issue because tensors, the model inputs, need to have a uniform shape. Padding is a strategy for ensuring tensors are rectangular by adding a special "),bn=r(jp,"EM",{});var $h=o(bn);Ir=t($h,"padding token"),$h.forEach(e),Sr=t(jp," to shorter sentences."),jp.forEach(e),Ft=h(s),B=r(s,"P",{});var Ue=o(B);Nr=t(Ue,"Set the "),fn=r(Ue,"CODE",{});var vh=o(fn);Fr=t(vh,"padding"),vh.forEach(e),Rr=t(Ue," parameter to "),jn=r(Ue,"CODE",{});var yh=o(jn);Br=t(yh,"True"),yh.forEach(e),Mr=t(Ue," to pad the shorter sequences in the batch to match the longest sequence:"),Ue.forEach(e),Rt=h(s),j(Qs.$$.fragment,s),Bt=h(s),is=r(s,"P",{});var dp=o(is);Wr=t(dp,"The first and third sentences are now padded with "),dn=r(dp,"CODE",{});var wh=o(dn);Jr=t(wh,"0"),wh.forEach(e),Hr=t(dp,"\u2019s because they are shorter."),dp.forEach(e),Mt=h(s),as=r(s,"H3",{class:!0});var gp=o(as);bs=r(gp,"A",{id:!0,class:!0,href:!0});var kh=o(bs);gn=r(kh,"SPAN",{});var xh=o(gn);j(Xs.$$.fragment,xh),xh.forEach(e),kh.forEach(e),Ur=h(gp),_n=r(gp,"SPAN",{});var Eh=o(_n);Yr=t(Eh,"Truncation"),Eh.forEach(e),gp.forEach(e),Wt=h(s),ve=r(s,"P",{});var qh=o(ve);Gr=t(qh,"On the other end of the spectrum, sometimes a sequence may be too long for a model to handle. In this case, you\u2019ll need to truncate the sequence to a shorter length."),qh.forEach(e),Jt=h(s),M=r(s,"P",{});var Ye=o(M);Vr=t(Ye,"Set the "),$n=r(Ye,"CODE",{});var Ah=o($n);Kr=t(Ah,"truncation"),Ah.forEach(e),Qr=t(Ye," parameter to "),vn=r(Ye,"CODE",{});var Ph=o(vn);Xr=t(Ph,"True"),Ph.forEach(e),Zr=t(Ye," to truncate a sequence to the maximum length accepted by the model:"),Ye.forEach(e),Ht=h(s),j(Zs.$$.fragment,s),Ut=h(s),j(fs.$$.fragment,s),Yt=h(s),es=r(s,"H3",{class:!0});var _p=o(es);js=r(_p,"A",{id:!0,class:!0,href:!0});var Th=o(js);yn=r(Th,"SPAN",{});var zh=o(yn);j(sa.$$.fragment,zh),zh.forEach(e),Th.forEach(e),so=h(_p),wn=r(_p,"SPAN",{});var Ch=o(wn);ao=t(Ch,"Build tensors"),Ch.forEach(e),_p.forEach(e),Gt=h(s),ye=r(s,"P",{});var Dh=o(ye);eo=t(Dh,"Finally, you want the tokenizer to return the actual tensors that get fed to the model."),Dh.forEach(e),Vt=h(s),z=r(s,"P",{});var Ds=o(z);no=t(Ds,"Set the "),kn=r(Ds,"CODE",{});var Oh=o(kn);to=t(Oh,"return_tensors"),Oh.forEach(e),lo=t(Ds," parameter to either "),xn=r(Ds,"CODE",{});var Lh=o(xn);po=t(Lh,"pt"),Lh.forEach(e),ro=t(Ds," for PyTorch, or "),En=r(Ds,"CODE",{});var Ih=o(En);oo=t(Ih,"tf"),Ih.forEach(e),uo=t(Ds," for TensorFlow:"),Ds.forEach(e),Kt=h(s),j(ds.$$.fragment,s),Qt=h(s),ns=r(s,"H2",{class:!0});var $p=o(ns);gs=r($p,"A",{id:!0,class:!0,href:!0});var Sh=o(gs);qn=r(Sh,"SPAN",{});var Nh=o(qn);j(aa.$$.fragment,Nh),Nh.forEach(e),Sh.forEach(e),co=h($p),An=r($p,"SPAN",{});var Fh=o(An);ho=t(Fh,"Audio"),Fh.forEach(e),$p.forEach(e),Xt=h(s),_s=r(s,"P",{});var vp=o(_s);mo=t(vp,"For audio tasks, you\u2019ll need a "),we=r(vp,"A",{href:!0});var Rh=o(we);io=t(Rh,"feature extractor"),Rh.forEach(e),bo=t(vp," to prepare your dataset for the model. The feature extractor is designed to extract features from raw audio data, and convert them into tensors."),vp.forEach(e),Zt=h(s),W=r(s,"P",{});var Ge=o(W);fo=t(Ge,"Load the "),ea=r(Ge,"A",{href:!0,rel:!0});var Bh=o(ea);jo=t(Bh,"MInDS-14"),Bh.forEach(e),go=t(Ge," dataset (see the \u{1F917} "),na=r(Ge,"A",{href:!0,rel:!0});var Mh=o(na);_o=t(Mh,"Datasets tutorial"),Mh.forEach(e),$o=t(Ge," for more details on how to load a dataset) to see how you can use a feature extractor with audio datasets:"),Ge.forEach(e),sl=h(s),j(ta.$$.fragment,s),al=h(s),J=r(s,"P",{});var Ve=o(J);vo=t(Ve,"Access the first element of the "),Pn=r(Ve,"CODE",{});var Wh=o(Pn);yo=t(Wh,"audio"),Wh.forEach(e),wo=t(Ve," column to take a look at the input. Calling the "),Tn=r(Ve,"CODE",{});var Jh=o(Tn);ko=t(Jh,"audio"),Jh.forEach(e),xo=t(Ve," column automatically loads and resamples the audio file:"),Ve.forEach(e),el=h(s),j(la.$$.fragment,s),nl=h(s),ke=r(s,"P",{});var Hh=o(ke);Eo=t(Hh,"This returns three items:"),Hh.forEach(e),tl=h(s),H=r(s,"UL",{});var Ke=o(H);xe=r(Ke,"LI",{});var Hc=o(xe);zn=r(Hc,"CODE",{});var Uh=o(zn);qo=t(Uh,"array"),Uh.forEach(e),Ao=t(Hc," is the speech signal loaded - and potentially resampled - as a 1D array."),Hc.forEach(e),Po=h(Ke),Ee=r(Ke,"LI",{});var Uc=o(Ee);Cn=r(Uc,"CODE",{});var Yh=o(Cn);To=t(Yh,"path"),Yh.forEach(e),zo=t(Uc," points to the location of the audio file."),Uc.forEach(e),Co=h(Ke),qe=r(Ke,"LI",{});var Yc=o(qe);Dn=r(Yc,"CODE",{});var Gh=o(Dn);Do=t(Gh,"sampling_rate"),Gh.forEach(e),Oo=t(Yc," refers to how many data points in the speech signal are measured per second."),Yc.forEach(e),Ke.forEach(e),ll=h(s),$s=r(s,"P",{});var yp=o($s);Lo=t(yp,"For this tutorial, you\u2019ll use the "),pa=r(yp,"A",{href:!0,rel:!0});var Vh=o(pa);Io=t(Vh,"Wav2Vec2"),Vh.forEach(e),So=t(yp," model. Take a look at the model card, and you\u2019ll learn Wav2Vec2 is pretrained on 16kHz sampled speech audio. It is important your audio data\u2019s sampling rate matches the sampling rate of the dataset used to pretrain the model. If your data\u2019s sampling rate isn\u2019t the same, then you need to resample your data."),yp.forEach(e),pl=h(s),Ae=r(s,"OL",{});var Kh=o(Ae);ra=r(Kh,"LI",{});var wp=o(ra);No=t(wp,"Use \u{1F917} Datasets\u2019 "),oa=r(wp,"A",{href:!0,rel:!0});var Qh=o(oa);Fo=t(Qh,"cast_column"),Qh.forEach(e),Ro=t(wp," method to upsample the sampling rate to 16kHz:"),wp.forEach(e),Kh.forEach(e),rl=h(s),j(ua.$$.fragment,s),ol=h(s),ca=r(s,"OL",{start:!0});var Xh=o(ca);ha=r(Xh,"LI",{});var kp=o(ha);Bo=t(kp,"Call the "),On=r(kp,"CODE",{});var Zh=o(On);Mo=t(Zh,"audio"),Zh.forEach(e),Wo=t(kp," column again to resample the audio file:"),kp.forEach(e),Xh.forEach(e),ul=h(s),j(ma.$$.fragment,s),cl=h(s),C=r(s,"P",{});var Os=o(C);Jo=t(Os,"Next, load a feature extractor to normalize and pad the input. When padding textual data, a "),Ln=r(Os,"CODE",{});var sm=o(Ln);Ho=t(sm,"0"),sm.forEach(e),Uo=t(Os," is added for shorter sequences. The same idea applies to audio data. The feature extractor adds a "),In=r(Os,"CODE",{});var am=o(In);Yo=t(am,"0"),am.forEach(e),Go=t(Os," - interpreted as silence - to "),Sn=r(Os,"CODE",{});var em=o(Sn);Vo=t(em,"array"),em.forEach(e),Ko=t(Os,"."),Os.forEach(e),hl=h(s),vs=r(s,"P",{});var xp=o(vs);Qo=t(xp,"Load the feature extractor with "),Pe=r(xp,"A",{href:!0});var nm=o(Pe);Xo=t(nm,"AutoFeatureExtractor.from_pretrained()"),nm.forEach(e),Zo=t(xp,":"),xp.forEach(e),ml=h(s),j(ia.$$.fragment,s),il=h(s),U=r(s,"P",{});var Qe=o(U);su=t(Qe,"Pass the audio "),Nn=r(Qe,"CODE",{});var tm=o(Nn);au=t(tm,"array"),tm.forEach(e),eu=t(Qe," to the feature extractor. We also recommend adding the "),Fn=r(Qe,"CODE",{});var lm=o(Fn);nu=t(lm,"sampling_rate"),lm.forEach(e),tu=t(Qe," argument in the feature extractor in order to better debug any silent errors that may occur."),Qe.forEach(e),bl=h(s),j(ba.$$.fragment,s),fl=h(s),Te=r(s,"P",{});var pm=o(Te);lu=t(pm,"Just like the tokenizer, you can apply padding or truncation to handle variable sequences in a batch. Take a look at the sequence length of these two audio samples:"),pm.forEach(e),jl=h(s),j(fa.$$.fragment,s),dl=h(s),ze=r(s,"P",{});var rm=o(ze);pu=t(rm,"Create a function to preprocess the dataset so the audio samples are the same lengths. Specify a maximum sample length, and the feature extractor will either pad or truncate the sequences to match it:"),rm.forEach(e),gl=h(s),j(ja.$$.fragment,s),_l=h(s),ys=r(s,"P",{});var Ep=o(ys);ru=t(Ep,"Apply the "),Rn=r(Ep,"CODE",{});var om=o(Rn);ou=t(om,"preprocess_function"),om.forEach(e),uu=t(Ep," to the the first few examples in the dataset:"),Ep.forEach(e),$l=h(s),j(da.$$.fragment,s),vl=h(s),Ce=r(s,"P",{});var um=o(Ce);cu=t(um,"The sample lengths are now the same and match the specified maximum length. You can pass your processed dataset to the model now!"),um.forEach(e),yl=h(s),j(ga.$$.fragment,s),wl=h(s),ts=r(s,"H2",{class:!0});var qp=o(ts);ws=r(qp,"A",{id:!0,class:!0,href:!0});var cm=o(ws);Bn=r(cm,"SPAN",{});var hm=o(Bn);j(_a.$$.fragment,hm),hm.forEach(e),cm.forEach(e),hu=h(qp),Mn=r(qp,"SPAN",{});var mm=o(Mn);mu=t(mm,"Computer vision"),mm.forEach(e),qp.forEach(e),kl=h(s),ks=r(s,"P",{});var Ap=o(ks);iu=t(Ap,"For computer vision tasks, you\u2019ll need a "),De=r(Ap,"A",{href:!0});var im=o(De);bu=t(im,"feature extractor"),im.forEach(e),fu=t(Ap," to prepare your dataset for the model. The feature extractor is designed to extract features from images, and convert them into tensors."),Ap.forEach(e),xl=h(s),Y=r(s,"P",{});var Xe=o(Y);ju=t(Xe,"Load the "),$a=r(Xe,"A",{href:!0,rel:!0});var bm=o($a);du=t(bm,"food101"),bm.forEach(e),gu=t(Xe," dataset (see the \u{1F917} "),va=r(Xe,"A",{href:!0,rel:!0});var fm=o(va);_u=t(fm,"Datasets tutorial"),fm.forEach(e),$u=t(Xe," for more details on how to load a dataset) to see how you can use a feature extractor with computer vision datasets:"),Xe.forEach(e),El=h(s),j(xs.$$.fragment,s),ql=h(s),j(ya.$$.fragment,s),Al=h(s),Es=r(s,"P",{});var Pp=o(Es);vu=t(Pp,"Next, take a look at the image with \u{1F917} Datasets "),wa=r(Pp,"A",{href:!0,rel:!0});var jm=o(wa);Wn=r(jm,"CODE",{});var dm=o(Wn);yu=t(dm,"Image"),dm.forEach(e),jm.forEach(e),wu=t(Pp," feature:"),Pp.forEach(e),Pl=h(s),j(ka.$$.fragment,s),Tl=h(s),xa=r(s,"DIV",{class:!0});var gm=o(xa);Jn=r(gm,"IMG",{src:!0}),gm.forEach(e),zl=h(s),qs=r(s,"P",{});var Tp=o(qs);ku=t(Tp,"Load the feature extractor with "),Oe=r(Tp,"A",{href:!0});var _m=o(Oe);xu=t(_m,"AutoFeatureExtractor.from_pretrained()"),_m.forEach(e),Eu=t(Tp,":"),Tp.forEach(e),Cl=h(s),j(Ea.$$.fragment,s),Dl=h(s),D=r(s,"P",{});var Ls=o(D);qu=t(Ls,"For computer vision tasks, it is common to add some type of data augmentation to the images as a part of preprocessing. You can add augmentations with any library you\u2019d like, but in this tutorial, you\u2019ll use torchvision\u2019s "),qa=r(Ls,"A",{href:!0,rel:!0});var $m=o(qa);Hn=r($m,"CODE",{});var vm=o(Hn);Au=t(vm,"transforms"),vm.forEach(e),$m.forEach(e),Pu=t(Ls," module. If you\u2019re interested in using another data augmentation library, learn how in the "),Aa=r(Ls,"A",{href:!0,rel:!0});var ym=o(Aa);Tu=t(ym,"Albumentations"),ym.forEach(e),zu=t(Ls," or "),Pa=r(Ls,"A",{href:!0,rel:!0});var wm=o(Pa);Cu=t(wm,"Kornia notebooks"),wm.forEach(e),Du=t(Ls,"."),Ls.forEach(e),Ol=h(s),Le=r(s,"OL",{});var km=o(Le);L=r(km,"LI",{});var Is=o(L);Ou=t(Is,"Normalize the image with the feature extractor and use "),Ta=r(Is,"A",{href:!0,rel:!0});var xm=o(Ta);Un=r(xm,"CODE",{});var Em=o(Un);Lu=t(Em,"Compose"),Em.forEach(e),xm.forEach(e),Iu=t(Is," to chain some transforms - "),za=r(Is,"A",{href:!0,rel:!0});var qm=o(za);Yn=r(qm,"CODE",{});var Am=o(Yn);Su=t(Am,"RandomResizedCrop"),Am.forEach(e),qm.forEach(e),Nu=t(Is," and "),Ca=r(Is,"A",{href:!0,rel:!0});var Pm=o(Ca);Gn=r(Pm,"CODE",{});var Tm=o(Gn);Fu=t(Tm,"ColorJitter"),Tm.forEach(e),Pm.forEach(e),Ru=t(Is," - together:"),Is.forEach(e),km.forEach(e),Ll=h(s),j(Da.$$.fragment,s),Il=h(s),Oa=r(s,"OL",{start:!0});var zm=o(Oa);ls=r(zm,"LI",{});var Ze=o(ls);Bu=t(Ze,"The model accepts "),Ie=r(Ze,"A",{href:!0});var Cm=o(Ie);Vn=r(Cm,"CODE",{});var Dm=o(Vn);Mu=t(Dm,"pixel_values"),Dm.forEach(e),Cm.forEach(e),Wu=t(Ze," as its input, which is generated by the feature extractor. Create a function that generates "),Kn=r(Ze,"CODE",{});var Om=o(Kn);Ju=t(Om,"pixel_values"),Om.forEach(e),Hu=t(Ze," from the transforms:"),Ze.forEach(e),zm.forEach(e),Sl=h(s),j(La.$$.fragment,s),Nl=h(s),Ia=r(s,"OL",{start:!0});var Lm=o(Ia);Sa=r(Lm,"LI",{});var zp=o(Sa);Uu=t(zp,"Then use \u{1F917} Datasets "),Na=r(zp,"A",{href:!0,rel:!0});var Im=o(Na);Qn=r(Im,"CODE",{});var Sm=o(Qn);Yu=t(Sm,"set_transform"),Sm.forEach(e),Im.forEach(e),Gu=t(zp," to apply the transforms on the fly:"),zp.forEach(e),Lm.forEach(e),Fl=h(s),j(Fa.$$.fragment,s),Rl=h(s),Ra=r(s,"OL",{start:!0});var Nm=o(Ra);Ba=r(Nm,"LI",{});var Cp=o(Ba);Vu=t(Cp,"Now when you access the image, you\u2019ll notice the feature extractor has added "),Xn=r(Cp,"CODE",{});var Fm=o(Xn);Ku=t(Fm,"pixel_values"),Fm.forEach(e),Qu=t(Cp,". You can pass your processed dataset to the model now!"),Cp.forEach(e),Nm.forEach(e),Bl=h(s),j(Ma.$$.fragment,s),Ml=h(s),Se=r(s,"P",{});var Rm=o(Se);Xu=t(Rm,"Here is what the image looks like after the transforms are applied. The image has been randomly cropped and it\u2019s color properties are different."),Rm.forEach(e),Wl=h(s),j(Wa.$$.fragment,s),Jl=h(s),Ja=r(s,"DIV",{class:!0});var Bm=o(Ja);Zn=r(Bm,"IMG",{src:!0}),Bm.forEach(e),Hl=h(s),ps=r(s,"H2",{class:!0});var Dp=o(ps);As=r(Dp,"A",{id:!0,class:!0,href:!0});var Mm=o(As);st=r(Mm,"SPAN",{});var Wm=o(st);j(Ha.$$.fragment,Wm),Wm.forEach(e),Mm.forEach(e),Zu=h(Dp),at=r(Dp,"SPAN",{});var Jm=o(at);sc=t(Jm,"Multimodal"),Jm.forEach(e),Dp.forEach(e),Ul=h(s),Ps=r(s,"P",{});var Op=o(Ps);ac=t(Op,"For tasks involving multimodal inputs, you\u2019ll need a "),Ne=r(Op,"A",{href:!0});var Hm=o(Ne);ec=t(Hm,"processor"),Hm.forEach(e),nc=t(Op," to prepare your dataset for the model. A processor couples a tokenizer and feature extractor."),Op.forEach(e),Yl=h(s),G=r(s,"P",{});var sn=o(G);tc=t(sn,"Load the "),Ua=r(sn,"A",{href:!0,rel:!0});var Um=o(Ua);lc=t(Um,"LJ Speech"),Um.forEach(e),pc=t(sn," dataset (see the \u{1F917} "),Ya=r(sn,"A",{href:!0,rel:!0});var Ym=o(Ya);rc=t(Ym,"Datasets tutorial"),Ym.forEach(e),oc=t(sn," for more details on how to load a dataset) to see how you can use a processor for automatic speech recognition (ASR):"),sn.forEach(e),Gl=h(s),j(Ga.$$.fragment,s),Vl=h(s),V=r(s,"P",{});var an=o(V);uc=t(an,"For ASR, you\u2019re mainly focused on "),et=r(an,"CODE",{});var Gm=o(et);cc=t(Gm,"audio"),Gm.forEach(e),hc=t(an," and "),nt=r(an,"CODE",{});var Vm=o(nt);mc=t(Vm,"text"),Vm.forEach(e),ic=t(an," so you can remove the other columns:"),an.forEach(e),Kl=h(s),j(Va.$$.fragment,s),Ql=h(s),K=r(s,"P",{});var en=o(K);bc=t(en,"Now take a look at the "),tt=r(en,"CODE",{});var Km=o(tt);fc=t(Km,"audio"),Km.forEach(e),jc=t(en," and "),lt=r(en,"CODE",{});var Qm=o(lt);dc=t(Qm,"text"),Qm.forEach(e),gc=t(en," columns:"),en.forEach(e),Xl=h(s),j(Ka.$$.fragment,s),Zl=h(s),Ts=r(s,"P",{});var Lp=o(Ts);_c=t(Lp,"Remember you should always "),Fe=r(Lp,"A",{href:!0});var Xm=o(Fe);$c=t(Xm,"resample"),Xm.forEach(e),vc=t(Lp," your audio dataset\u2019s sampling rate to match the sampling rate of the dataset used to pretrain a model!"),Lp.forEach(e),sp=h(s),j(Qa.$$.fragment,s),ap=h(s),zs=r(s,"P",{});var Ip=o(zs);yc=t(Ip,"Load a processor with "),Re=r(Ip,"A",{href:!0});var Zm=o(Re);wc=t(Zm,"AutoProcessor.from_pretrained()"),Zm.forEach(e),kc=t(Ip,":"),Ip.forEach(e),ep=h(s),j(Xa.$$.fragment,s),np=h(s),Be=r(s,"OL",{});var si=o(Be);P=r(si,"LI",{});var X=o(P);xc=t(X,"Create a function to process the audio data contained in "),pt=r(X,"CODE",{});var ai=o(pt);Ec=t(ai,"array"),ai.forEach(e),qc=t(X," to "),rt=r(X,"CODE",{});var ei=o(rt);Ac=t(ei,"input_values"),ei.forEach(e),Pc=t(X,", and tokenize "),ot=r(X,"CODE",{});var ni=o(ot);Tc=t(ni,"text"),ni.forEach(e),zc=t(X," to "),ut=r(X,"CODE",{});var ti=o(ut);Cc=t(ti,"labels"),ti.forEach(e),Dc=t(X,". These are the inputs to the model:"),X.forEach(e),si.forEach(e),tp=h(s),j(Za.$$.fragment,s),lp=h(s),se=r(s,"OL",{start:!0});var li=o(se);ae=r(li,"LI",{});var Sp=o(ae);Oc=t(Sp,"Apply the "),ct=r(Sp,"CODE",{});var pi=o(ct);Lc=t(pi,"prepare_dataset"),pi.forEach(e),Ic=t(Sp," function to a sample:"),Sp.forEach(e),li.forEach(e),pp=h(s),j(ee.$$.fragment,s),rp=h(s),Q=r(s,"P",{});var nn=o(Q);Sc=t(nn,"The processor has now added "),ht=r(nn,"CODE",{});var ri=o(ht);Nc=t(ri,"input_values"),ri.forEach(e),Fc=t(nn," and "),mt=r(nn,"CODE",{});var oi=o(mt);Rc=t(oi,"labels"),oi.forEach(e),Bc=t(nn,", and the sampling rate has also been correctly downsampled to 16kHz. You can pass your processed dataset to the model now!"),nn.forEach(e),this.h()},h(){m(i,"name","hf:doc:metadata"),m(i,"content",JSON.stringify(Pi)),m(y,"id","preprocess"),m(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(y,"href","#preprocess"),m(b,"class","relative group"),m(le,"href","./main_classes/tokenizer"),m(pe,"href","./main_classes/image"),m(re,"href","./main_classes/feature_extractor"),m(oe,"href","./main_classes/processors"),m(os,"id","natural-language-processing"),m(os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(os,"href","#natural-language-processing"),m(Z,"class","relative group"),m(ce,"href","main_classes/tokenizer"),m(he,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),m(fe,"href","glossary#input-ids"),m(de,"href","glossary#attention-mask"),m(_e,"href","glossary#token-type-ids"),m(hs,"id","pad"),m(hs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(hs,"href","#pad"),m(ss,"class","relative group"),m(bs,"id","truncation"),m(bs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(bs,"href","#truncation"),m(as,"class","relative group"),m(js,"id","build-tensors"),m(js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(js,"href","#build-tensors"),m(es,"class","relative group"),m(gs,"id","audio"),m(gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(gs,"href","#audio"),m(ns,"class","relative group"),m(we,"href","main_classes/feature_extractor"),m(ea,"href","https://huggingface.co/datasets/PolyAI/minds14"),m(ea,"rel","nofollow"),m(na,"href","https://huggingface.co/docs/datasets/load_hub.html"),m(na,"rel","nofollow"),m(pa,"href","https://huggingface.co/facebook/wav2vec2-base"),m(pa,"rel","nofollow"),m(oa,"href","https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.cast_column"),m(oa,"rel","nofollow"),m(ca,"start","2"),m(Pe,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),m(ws,"id","computer-vision"),m(ws,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ws,"href","#computer-vision"),m(ts,"class","relative group"),m(De,"href","main_classes/feature_extractor"),m($a,"href","https://huggingface.co/datasets/food101"),m($a,"rel","nofollow"),m(va,"href","https://huggingface.co/docs/datasets/load_hub.html"),m(va,"rel","nofollow"),m(wa,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=image#datasets.Image"),m(wa,"rel","nofollow"),ui(Jn.src,Gc="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vision-preprocess-tutorial.png")||m(Jn,"src",Gc),m(xa,"class","flex justify-center"),m(Oe,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),m(qa,"href","https://pytorch.org/vision/stable/transforms.html"),m(qa,"rel","nofollow"),m(Aa,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb"),m(Aa,"rel","nofollow"),m(Pa,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb"),m(Pa,"rel","nofollow"),m(Ta,"href","https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html"),m(Ta,"rel","nofollow"),m(za,"href","https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html"),m(za,"rel","nofollow"),m(Ca,"href","https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html"),m(Ca,"rel","nofollow"),m(Ie,"href","model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel.forward.pixel_values"),m(Oa,"start","2"),m(Na,"href","https://huggingface.co/docs/datasets/process.html#format-transform"),m(Na,"rel","nofollow"),m(Ia,"start","3"),m(Ra,"start","4"),ui(Zn.src,Vc="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/preprocessed_image.png")||m(Zn,"src",Vc),m(Ja,"class","flex justify-center"),m(As,"id","multimodal"),m(As,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(As,"href","#multimodal"),m(ps,"class","relative group"),m(Ne,"href","main_classes/processors"),m(Ua,"href","https://huggingface.co/datasets/lj_speech"),m(Ua,"rel","nofollow"),m(Ya,"href","https://huggingface.co/docs/datasets/load_hub.html"),m(Ya,"rel","nofollow"),m(Fe,"href","preprocessing#audio"),m(Re,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoProcessor.from_pretrained"),m(se,"start","2")},m(s,l){a(document.head,i),u(s,v,l),u(s,b,l),a(b,y),a(y,k),d(x,k,null),a(b,E),a(b,q),a(q,O),u(s,Ns,l),d(I,s,l),u(s,dt,l),u(s,te,l),a(te,Fp),u(s,gt,l),u(s,T,l),a(T,Fs),a(Fs,Rp),a(Fs,le),a(le,Bp),a(Fs,Mp),a(T,Wp),a(T,Rs),a(Rs,Jp),a(Rs,pe),a(pe,Hp),a(Rs,Up),a(T,Yp),a(T,Bs),a(Bs,Gp),a(Bs,re),a(re,Vp),a(Bs,Kp),a(T,Qp),a(T,Ms),a(Ms,Xp),a(Ms,oe),a(oe,Zp),a(Ms,sr),u(s,_t,l),d(rs,s,l),u(s,$t,l),u(s,ue,l),a(ue,ar),u(s,vt,l),d(Ws,s,l),u(s,yt,l),u(s,Z,l),a(Z,os),a(os,tn),d(Js,tn,null),a(Z,er),a(Z,ln),a(ln,nr),u(s,wt,l),d(Hs,s,l),u(s,kt,l),u(s,S,l),a(S,tr),a(S,ce),a(ce,lr),a(S,pr),a(S,pn),a(pn,rr),a(S,or),u(s,xt,l),d(us,s,l),u(s,Et,l),u(s,N,l),a(N,ur),a(N,he),a(he,cr),a(N,hr),a(N,rn),a(rn,mr),a(N,ir),u(s,qt,l),d(Us,s,l),u(s,At,l),u(s,me,l),a(me,br),u(s,Pt,l),d(Ys,s,l),u(s,Tt,l),u(s,ie,l),a(ie,fr),u(s,zt,l),u(s,F,l),a(F,be),a(be,fe),a(fe,jr),a(be,dr),a(F,gr),a(F,je),a(je,de),a(de,_r),a(je,$r),a(F,vr),a(F,ge),a(ge,_e),a(_e,yr),a(ge,wr),u(s,Ct,l),u(s,cs,l),a(cs,kr),a(cs,on),a(on,xr),a(cs,Er),u(s,Dt,l),d(Gs,s,l),u(s,Ot,l),u(s,R,l),a(R,qr),a(R,un),a(un,Ar),a(R,Pr),a(R,cn),a(cn,Tr),a(R,zr),u(s,Lt,l),u(s,$e,l),a($e,Cr),u(s,It,l),d(Vs,s,l),u(s,St,l),u(s,ss,l),a(ss,hs),a(hs,hn),d(Ks,hn,null),a(ss,Dr),a(ss,mn),a(mn,Or),u(s,Nt,l),u(s,ms,l),a(ms,Lr),a(ms,bn),a(bn,Ir),a(ms,Sr),u(s,Ft,l),u(s,B,l),a(B,Nr),a(B,fn),a(fn,Fr),a(B,Rr),a(B,jn),a(jn,Br),a(B,Mr),u(s,Rt,l),d(Qs,s,l),u(s,Bt,l),u(s,is,l),a(is,Wr),a(is,dn),a(dn,Jr),a(is,Hr),u(s,Mt,l),u(s,as,l),a(as,bs),a(bs,gn),d(Xs,gn,null),a(as,Ur),a(as,_n),a(_n,Yr),u(s,Wt,l),u(s,ve,l),a(ve,Gr),u(s,Jt,l),u(s,M,l),a(M,Vr),a(M,$n),a($n,Kr),a(M,Qr),a(M,vn),a(vn,Xr),a(M,Zr),u(s,Ht,l),d(Zs,s,l),u(s,Ut,l),d(fs,s,l),u(s,Yt,l),u(s,es,l),a(es,js),a(js,yn),d(sa,yn,null),a(es,so),a(es,wn),a(wn,ao),u(s,Gt,l),u(s,ye,l),a(ye,eo),u(s,Vt,l),u(s,z,l),a(z,no),a(z,kn),a(kn,to),a(z,lo),a(z,xn),a(xn,po),a(z,ro),a(z,En),a(En,oo),a(z,uo),u(s,Kt,l),d(ds,s,l),u(s,Qt,l),u(s,ns,l),a(ns,gs),a(gs,qn),d(aa,qn,null),a(ns,co),a(ns,An),a(An,ho),u(s,Xt,l),u(s,_s,l),a(_s,mo),a(_s,we),a(we,io),a(_s,bo),u(s,Zt,l),u(s,W,l),a(W,fo),a(W,ea),a(ea,jo),a(W,go),a(W,na),a(na,_o),a(W,$o),u(s,sl,l),d(ta,s,l),u(s,al,l),u(s,J,l),a(J,vo),a(J,Pn),a(Pn,yo),a(J,wo),a(J,Tn),a(Tn,ko),a(J,xo),u(s,el,l),d(la,s,l),u(s,nl,l),u(s,ke,l),a(ke,Eo),u(s,tl,l),u(s,H,l),a(H,xe),a(xe,zn),a(zn,qo),a(xe,Ao),a(H,Po),a(H,Ee),a(Ee,Cn),a(Cn,To),a(Ee,zo),a(H,Co),a(H,qe),a(qe,Dn),a(Dn,Do),a(qe,Oo),u(s,ll,l),u(s,$s,l),a($s,Lo),a($s,pa),a(pa,Io),a($s,So),u(s,pl,l),u(s,Ae,l),a(Ae,ra),a(ra,No),a(ra,oa),a(oa,Fo),a(ra,Ro),u(s,rl,l),d(ua,s,l),u(s,ol,l),u(s,ca,l),a(ca,ha),a(ha,Bo),a(ha,On),a(On,Mo),a(ha,Wo),u(s,ul,l),d(ma,s,l),u(s,cl,l),u(s,C,l),a(C,Jo),a(C,Ln),a(Ln,Ho),a(C,Uo),a(C,In),a(In,Yo),a(C,Go),a(C,Sn),a(Sn,Vo),a(C,Ko),u(s,hl,l),u(s,vs,l),a(vs,Qo),a(vs,Pe),a(Pe,Xo),a(vs,Zo),u(s,ml,l),d(ia,s,l),u(s,il,l),u(s,U,l),a(U,su),a(U,Nn),a(Nn,au),a(U,eu),a(U,Fn),a(Fn,nu),a(U,tu),u(s,bl,l),d(ba,s,l),u(s,fl,l),u(s,Te,l),a(Te,lu),u(s,jl,l),d(fa,s,l),u(s,dl,l),u(s,ze,l),a(ze,pu),u(s,gl,l),d(ja,s,l),u(s,_l,l),u(s,ys,l),a(ys,ru),a(ys,Rn),a(Rn,ou),a(ys,uu),u(s,$l,l),d(da,s,l),u(s,vl,l),u(s,Ce,l),a(Ce,cu),u(s,yl,l),d(ga,s,l),u(s,wl,l),u(s,ts,l),a(ts,ws),a(ws,Bn),d(_a,Bn,null),a(ts,hu),a(ts,Mn),a(Mn,mu),u(s,kl,l),u(s,ks,l),a(ks,iu),a(ks,De),a(De,bu),a(ks,fu),u(s,xl,l),u(s,Y,l),a(Y,ju),a(Y,$a),a($a,du),a(Y,gu),a(Y,va),a(va,_u),a(Y,$u),u(s,El,l),d(xs,s,l),u(s,ql,l),d(ya,s,l),u(s,Al,l),u(s,Es,l),a(Es,vu),a(Es,wa),a(wa,Wn),a(Wn,yu),a(Es,wu),u(s,Pl,l),d(ka,s,l),u(s,Tl,l),u(s,xa,l),a(xa,Jn),u(s,zl,l),u(s,qs,l),a(qs,ku),a(qs,Oe),a(Oe,xu),a(qs,Eu),u(s,Cl,l),d(Ea,s,l),u(s,Dl,l),u(s,D,l),a(D,qu),a(D,qa),a(qa,Hn),a(Hn,Au),a(D,Pu),a(D,Aa),a(Aa,Tu),a(D,zu),a(D,Pa),a(Pa,Cu),a(D,Du),u(s,Ol,l),u(s,Le,l),a(Le,L),a(L,Ou),a(L,Ta),a(Ta,Un),a(Un,Lu),a(L,Iu),a(L,za),a(za,Yn),a(Yn,Su),a(L,Nu),a(L,Ca),a(Ca,Gn),a(Gn,Fu),a(L,Ru),u(s,Ll,l),d(Da,s,l),u(s,Il,l),u(s,Oa,l),a(Oa,ls),a(ls,Bu),a(ls,Ie),a(Ie,Vn),a(Vn,Mu),a(ls,Wu),a(ls,Kn),a(Kn,Ju),a(ls,Hu),u(s,Sl,l),d(La,s,l),u(s,Nl,l),u(s,Ia,l),a(Ia,Sa),a(Sa,Uu),a(Sa,Na),a(Na,Qn),a(Qn,Yu),a(Sa,Gu),u(s,Fl,l),d(Fa,s,l),u(s,Rl,l),u(s,Ra,l),a(Ra,Ba),a(Ba,Vu),a(Ba,Xn),a(Xn,Ku),a(Ba,Qu),u(s,Bl,l),d(Ma,s,l),u(s,Ml,l),u(s,Se,l),a(Se,Xu),u(s,Wl,l),d(Wa,s,l),u(s,Jl,l),u(s,Ja,l),a(Ja,Zn),u(s,Hl,l),u(s,ps,l),a(ps,As),a(As,st),d(Ha,st,null),a(ps,Zu),a(ps,at),a(at,sc),u(s,Ul,l),u(s,Ps,l),a(Ps,ac),a(Ps,Ne),a(Ne,ec),a(Ps,nc),u(s,Yl,l),u(s,G,l),a(G,tc),a(G,Ua),a(Ua,lc),a(G,pc),a(G,Ya),a(Ya,rc),a(G,oc),u(s,Gl,l),d(Ga,s,l),u(s,Vl,l),u(s,V,l),a(V,uc),a(V,et),a(et,cc),a(V,hc),a(V,nt),a(nt,mc),a(V,ic),u(s,Kl,l),d(Va,s,l),u(s,Ql,l),u(s,K,l),a(K,bc),a(K,tt),a(tt,fc),a(K,jc),a(K,lt),a(lt,dc),a(K,gc),u(s,Xl,l),d(Ka,s,l),u(s,Zl,l),u(s,Ts,l),a(Ts,_c),a(Ts,Fe),a(Fe,$c),a(Ts,vc),u(s,sp,l),d(Qa,s,l),u(s,ap,l),u(s,zs,l),a(zs,yc),a(zs,Re),a(Re,wc),a(zs,kc),u(s,ep,l),d(Xa,s,l),u(s,np,l),u(s,Be,l),a(Be,P),a(P,xc),a(P,pt),a(pt,Ec),a(P,qc),a(P,rt),a(rt,Ac),a(P,Pc),a(P,ot),a(ot,Tc),a(P,zc),a(P,ut),a(ut,Cc),a(P,Dc),u(s,tp,l),d(Za,s,l),u(s,lp,l),u(s,se,l),a(se,ae),a(ae,Oc),a(ae,ct),a(ct,Lc),a(ae,Ic),u(s,pp,l),d(ee,s,l),u(s,rp,l),u(s,Q,l),a(Q,Sc),a(Q,ht),a(ht,Nc),a(Q,Fc),a(Q,mt),a(mt,Rc),a(Q,Bc),op=!0},p(s,[l]){const ne={};l&2&&(ne.$$scope={dirty:l,ctx:s}),rs.$set(ne);const it={};l&2&&(it.$$scope={dirty:l,ctx:s}),us.$set(it);const bt={};l&2&&(bt.$$scope={dirty:l,ctx:s}),fs.$set(bt);const ft={};l&2&&(ft.$$scope={dirty:l,ctx:s}),ds.$set(ft);const jt={};l&2&&(jt.$$scope={dirty:l,ctx:s}),xs.$set(jt)},i(s){op||(g(x.$$.fragment,s),g(I.$$.fragment,s),g(rs.$$.fragment,s),g(Ws.$$.fragment,s),g(Js.$$.fragment,s),g(Hs.$$.fragment,s),g(us.$$.fragment,s),g(Us.$$.fragment,s),g(Ys.$$.fragment,s),g(Gs.$$.fragment,s),g(Vs.$$.fragment,s),g(Ks.$$.fragment,s),g(Qs.$$.fragment,s),g(Xs.$$.fragment,s),g(Zs.$$.fragment,s),g(fs.$$.fragment,s),g(sa.$$.fragment,s),g(ds.$$.fragment,s),g(aa.$$.fragment,s),g(ta.$$.fragment,s),g(la.$$.fragment,s),g(ua.$$.fragment,s),g(ma.$$.fragment,s),g(ia.$$.fragment,s),g(ba.$$.fragment,s),g(fa.$$.fragment,s),g(ja.$$.fragment,s),g(da.$$.fragment,s),g(ga.$$.fragment,s),g(_a.$$.fragment,s),g(xs.$$.fragment,s),g(ya.$$.fragment,s),g(ka.$$.fragment,s),g(Ea.$$.fragment,s),g(Da.$$.fragment,s),g(La.$$.fragment,s),g(Fa.$$.fragment,s),g(Ma.$$.fragment,s),g(Wa.$$.fragment,s),g(Ha.$$.fragment,s),g(Ga.$$.fragment,s),g(Va.$$.fragment,s),g(Ka.$$.fragment,s),g(Qa.$$.fragment,s),g(Xa.$$.fragment,s),g(Za.$$.fragment,s),g(ee.$$.fragment,s),op=!0)},o(s){_(x.$$.fragment,s),_(I.$$.fragment,s),_(rs.$$.fragment,s),_(Ws.$$.fragment,s),_(Js.$$.fragment,s),_(Hs.$$.fragment,s),_(us.$$.fragment,s),_(Us.$$.fragment,s),_(Ys.$$.fragment,s),_(Gs.$$.fragment,s),_(Vs.$$.fragment,s),_(Ks.$$.fragment,s),_(Qs.$$.fragment,s),_(Xs.$$.fragment,s),_(Zs.$$.fragment,s),_(fs.$$.fragment,s),_(sa.$$.fragment,s),_(ds.$$.fragment,s),_(aa.$$.fragment,s),_(ta.$$.fragment,s),_(la.$$.fragment,s),_(ua.$$.fragment,s),_(ma.$$.fragment,s),_(ia.$$.fragment,s),_(ba.$$.fragment,s),_(fa.$$.fragment,s),_(ja.$$.fragment,s),_(da.$$.fragment,s),_(ga.$$.fragment,s),_(_a.$$.fragment,s),_(xs.$$.fragment,s),_(ya.$$.fragment,s),_(ka.$$.fragment,s),_(Ea.$$.fragment,s),_(Da.$$.fragment,s),_(La.$$.fragment,s),_(Fa.$$.fragment,s),_(Ma.$$.fragment,s),_(Wa.$$.fragment,s),_(Ha.$$.fragment,s),_(Ga.$$.fragment,s),_(Va.$$.fragment,s),_(Ka.$$.fragment,s),_(Qa.$$.fragment,s),_(Xa.$$.fragment,s),_(Za.$$.fragment,s),_(ee.$$.fragment,s),op=!1},d(s){e(i),s&&e(v),s&&e(b),$(x),s&&e(Ns),$(I,s),s&&e(dt),s&&e(te),s&&e(gt),s&&e(T),s&&e(_t),$(rs,s),s&&e($t),s&&e(ue),s&&e(vt),$(Ws,s),s&&e(yt),s&&e(Z),$(Js),s&&e(wt),$(Hs,s),s&&e(kt),s&&e(S),s&&e(xt),$(us,s),s&&e(Et),s&&e(N),s&&e(qt),$(Us,s),s&&e(At),s&&e(me),s&&e(Pt),$(Ys,s),s&&e(Tt),s&&e(ie),s&&e(zt),s&&e(F),s&&e(Ct),s&&e(cs),s&&e(Dt),$(Gs,s),s&&e(Ot),s&&e(R),s&&e(Lt),s&&e($e),s&&e(It),$(Vs,s),s&&e(St),s&&e(ss),$(Ks),s&&e(Nt),s&&e(ms),s&&e(Ft),s&&e(B),s&&e(Rt),$(Qs,s),s&&e(Bt),s&&e(is),s&&e(Mt),s&&e(as),$(Xs),s&&e(Wt),s&&e(ve),s&&e(Jt),s&&e(M),s&&e(Ht),$(Zs,s),s&&e(Ut),$(fs,s),s&&e(Yt),s&&e(es),$(sa),s&&e(Gt),s&&e(ye),s&&e(Vt),s&&e(z),s&&e(Kt),$(ds,s),s&&e(Qt),s&&e(ns),$(aa),s&&e(Xt),s&&e(_s),s&&e(Zt),s&&e(W),s&&e(sl),$(ta,s),s&&e(al),s&&e(J),s&&e(el),$(la,s),s&&e(nl),s&&e(ke),s&&e(tl),s&&e(H),s&&e(ll),s&&e($s),s&&e(pl),s&&e(Ae),s&&e(rl),$(ua,s),s&&e(ol),s&&e(ca),s&&e(ul),$(ma,s),s&&e(cl),s&&e(C),s&&e(hl),s&&e(vs),s&&e(ml),$(ia,s),s&&e(il),s&&e(U),s&&e(bl),$(ba,s),s&&e(fl),s&&e(Te),s&&e(jl),$(fa,s),s&&e(dl),s&&e(ze),s&&e(gl),$(ja,s),s&&e(_l),s&&e(ys),s&&e($l),$(da,s),s&&e(vl),s&&e(Ce),s&&e(yl),$(ga,s),s&&e(wl),s&&e(ts),$(_a),s&&e(kl),s&&e(ks),s&&e(xl),s&&e(Y),s&&e(El),$(xs,s),s&&e(ql),$(ya,s),s&&e(Al),s&&e(Es),s&&e(Pl),$(ka,s),s&&e(Tl),s&&e(xa),s&&e(zl),s&&e(qs),s&&e(Cl),$(Ea,s),s&&e(Dl),s&&e(D),s&&e(Ol),s&&e(Le),s&&e(Ll),$(Da,s),s&&e(Il),s&&e(Oa),s&&e(Sl),$(La,s),s&&e(Nl),s&&e(Ia),s&&e(Fl),$(Fa,s),s&&e(Rl),s&&e(Ra),s&&e(Bl),$(Ma,s),s&&e(Ml),s&&e(Se),s&&e(Wl),$(Wa,s),s&&e(Jl),s&&e(Ja),s&&e(Hl),s&&e(ps),$(Ha),s&&e(Ul),s&&e(Ps),s&&e(Yl),s&&e(G),s&&e(Gl),$(Ga,s),s&&e(Vl),s&&e(V),s&&e(Kl),$(Va,s),s&&e(Ql),s&&e(K),s&&e(Xl),$(Ka,s),s&&e(Zl),s&&e(Ts),s&&e(sp),$(Qa,s),s&&e(ap),s&&e(zs),s&&e(ep),$(Xa,s),s&&e(np),s&&e(Be),s&&e(tp),$(Za,s),s&&e(lp),s&&e(se),s&&e(pp),$(ee,s),s&&e(rp),s&&e(Q)}}}const Pi={local:"preprocess",sections:[{local:"natural-language-processing",sections:[{local:"pad",title:"Pad"},{local:"truncation",title:"Truncation"},{local:"build-tensors",title:"Build tensors"}],title:"Natural Language Processing"},{local:"audio",title:"Audio"},{local:"computer-vision",title:"Computer vision"},{local:"multimodal",title:"Multimodal"}],title:"Preprocess"};function Ti(A){return ji(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ni extends mi{constructor(i){super();ii(this,i,Ti,Ai,bi,{})}}export{Ni as default,Pi as metadata};
