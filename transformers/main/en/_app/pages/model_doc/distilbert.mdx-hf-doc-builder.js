import{S as $4,i as y4,s as D4,e as r,k as p,w as v,t as o,M as F4,c as a,d as t,m as h,a as i,x as T,h as s,b as c,G as e,g as _,y as $,q as y,o as D,B as F,v as E4,L as ue}from"../../chunks/vendor-hf-doc-builder.js";import{T as Se}from"../../chunks/Tip-hf-doc-builder.js";import{D as Y}from"../../chunks/Docstring-hf-doc-builder.js";import{C as me}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Ne}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as fe}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";import{P as Sg}from"../../chunks/PipelineTag-hf-doc-builder.js";function B4(B){let d,b,f,m,k;return m=new me({props:{code:`from transformers import DistilBertConfig, DistilBertModel

# Initializing a DistilBERT configuration
configuration = DistilBertConfig()

# Initializing a model (with random weights) from the configuration
model = DistilBertModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertConfig, DistilBertModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a DistilBERT configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = DistilBertConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){d=r("p"),b=o("Examples:"),f=p(),v(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Examples:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),k=!0},p:ue,i(l){k||(y(m.$$.fragment,l),k=!0)},o(l){D(m.$$.fragment,l),k=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function x4(B){let d,b,f,m,k;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),k=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),k=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,k)},d(l){l&&t(d)}}}function M4(B){let d,b,f,m,k;return m=new me({props:{code:`from transformers import DistilBertTokenizer, DistilBertModel
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertModel.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertModel.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){d=r("p"),b=o("Example:"),f=p(),v(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),k=!0},p:ue,i(l){k||(y(m.$$.fragment,l),k=!0)},o(l){D(m.$$.fragment,l),k=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function z4(B){let d,b,f,m,k;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),k=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),k=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,k)},d(l){l&&t(d)}}}function j4(B){let d,b,f,m,k;return m=new me({props:{code:`from transformers import DistilBertTokenizer, DistilBertForMaskedLM
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForMaskedLM.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

# retrieve index of [MASK]
mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]

predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)
tokenizer.decode(predicted_token_id)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve index of [MASK]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[<span class="hljs-number">0</span>].nonzero(as_tuple=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_id = logits[<span class="hljs-number">0</span>, mask_token_index].argmax(axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predicted_token_id)
`}}),{c(){d=r("p"),b=o("Example:"),f=p(),v(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),k=!0},p:ue,i(l){k||(y(m.$$.fragment,l),k=!0)},o(l){D(m.$$.fragment,l),k=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function C4(B){let d,b;return d=new me({props:{code:`labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]
# mask labels of non-[MASK] tokens
labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

outputs = model(**inputs, labels=labels)
round(outputs.loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># mask labels of non-[MASK] tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -<span class="hljs-number">100</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(outputs.loss.item(), <span class="hljs-number">2</span>)
`}}),{c(){v(d.$$.fragment)},l(f){T(d.$$.fragment,f)},m(f,m){$(d,f,m),b=!0},p:ue,i(f){b||(y(d.$$.fragment,f),b=!0)},o(f){D(d.$$.fragment,f),b=!1},d(f){F(d,f)}}}function P4(B){let d,b,f,m,k;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),k=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),k=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,k)},d(l){l&&t(d)}}}function q4(B){let d,b,f,m,k;return m=new me({props:{code:`import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
`}}),{c(){d=r("p"),b=o("Example of single-label classification:"),f=p(),v(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example of single-label classification:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),k=!0},p:ue,i(l){k||(y(m.$$.fragment,l),k=!0)},o(l){D(m.$$.fragment,l),k=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function A4(B){let d,b;return d=new me({props:{code:`# To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`
num_labels = len(model.config.id2label)
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=num_labels)

labels = torch.tensor([1])
loss = model(**inputs, labels=labels).loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=num_labels)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),{c(){v(d.$$.fragment)},l(f){T(d.$$.fragment,f)},m(f,m){$(d,f,m),b=!0},p:ue,i(f){b||(y(d.$$.fragment,f),b=!0)},o(f){D(d.$$.fragment,f),b=!1},d(f){F(d,f)}}}function O4(B){let d,b,f,m,k;return m=new me({props:{code:`import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", problem_type="multi_label_classification")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
`}}),{c(){d=r("p"),b=o("Example of multi-label classification:"),f=p(),v(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example of multi-label classification:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),k=!0},p:ue,i(l){k||(y(m.$$.fragment,l),k=!0)},o(l){D(m.$$.fragment,l),k=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function L4(B){let d,b;return d=new me({props:{code:`# To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`
num_labels = len(model.config.id2label)
model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=num_labels, problem_type="multi_label_classification"
)

labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(
    torch.float
)
loss = model(**inputs, labels=labels).loss
loss.backward()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=num_labels, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(
<span class="hljs-meta">... </span>    torch.<span class="hljs-built_in">float</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span>loss.backward()`}}),{c(){v(d.$$.fragment)},l(f){T(d.$$.fragment,f)},m(f,m){$(d,f,m),b=!0},p:ue,i(f){b||(y(d.$$.fragment,f),b=!0)},o(f){D(d.$$.fragment,f),b=!1},d(f){F(d,f)}}}function I4(B){let d,b,f,m,k;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),k=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),k=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,k)},d(l){l&&t(d)}}}function S4(B){let d,b,f,m,k;return m=new me({props:{code:`from transformers import DistilBertTokenizer, DistilBertForMultipleChoice
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-cased")
model = DistilBertForMultipleChoice.from_pretrained("distilbert-base-cased")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."
labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

encoding = tokenizer([[prompt, choice0], [prompt, choice1]], return_tensors="pt", padding=True)
outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1

# the linear classifier still needs to be trained
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;distilbert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># choice0 is correct (according to Wikipedia ;)), batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([[prompt, choice0], [prompt, choice1]], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v.unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}, labels=labels)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){d=r("p"),b=o("Examples:"),f=p(),v(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Examples:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),k=!0},p:ue,i(l){k||(y(m.$$.fragment,l),k=!0)},o(l){D(m.$$.fragment,l),k=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function N4(B){let d,b,f,m,k;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),k=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),k=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,k)},d(l){l&&t(d)}}}function R4(B){let d,b,f,m,k;return m=new me({props:{code:`from transformers import DistilBertTokenizer, DistilBertForTokenClassification
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForTokenClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer(
    "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="pt"
)

with torch.no_grad():
    logits = model(**inputs).logits

predicted_token_class_ids = logits.argmax(-1)

# Note that tokens are classified rather then input words which means that
# there might be more predicted token classes than words.
# Multiple token classes might account for the same word
predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]
predicted_tokens_classes
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;HuggingFace is a company based in Paris and New York&quot;</span>, add_special_tokens=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_class_ids = logits.argmax(-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Note that tokens are classified rather then input words which means that</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># there might be more predicted token classes than words.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multiple token classes might account for the same word</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes = [model.config.id2label[t.item()] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> predicted_token_class_ids[<span class="hljs-number">0</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes
`}}),{c(){d=r("p"),b=o("Example:"),f=p(),v(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),k=!0},p:ue,i(l){k||(y(m.$$.fragment,l),k=!0)},o(l){D(m.$$.fragment,l),k=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function W4(B){let d,b;return d=new me({props:{code:`labels = predicted_token_class_ids
loss = model(**inputs, labels=labels).loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = predicted_token_class_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),{c(){v(d.$$.fragment)},l(f){T(d.$$.fragment,f)},m(f,m){$(d,f,m),b=!0},p:ue,i(f){b||(y(d.$$.fragment,f),b=!0)},o(f){D(d.$$.fragment,f),b=!1},d(f){F(d,f)}}}function Q4(B){let d,b,f,m,k;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),k=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),k=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,k)},d(l){l&&t(d)}}}function U4(B){let d,b,f,m,k;return m=new me({props:{code:`from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

inputs = tokenizer(question, text, return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs)

answer_start_index = outputs.start_logits.argmax()
answer_end_index = outputs.end_logits.argmax()

predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>answer_start_index = outputs.start_logits.argmax()
<span class="hljs-meta">&gt;&gt;&gt; </span>answer_end_index = outputs.end_logits.argmax()

<span class="hljs-meta">&gt;&gt;&gt; </span>predict_answer_tokens = inputs.input_ids[<span class="hljs-number">0</span>, answer_start_index : answer_end_index + <span class="hljs-number">1</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predict_answer_tokens, skip_special_tokens=<span class="hljs-literal">True</span>)
`}}),{c(){d=r("p"),b=o("Example:"),f=p(),v(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),k=!0},p:ue,i(l){k||(y(m.$$.fragment,l),k=!0)},o(l){D(m.$$.fragment,l),k=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function H4(B){let d,b;return d=new me({props:{code:`# target is "nice puppet"
target_start_index = torch.tensor([14])
target_end_index = torch.tensor([15])

outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
loss = outputs.loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target is &quot;nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_start_index = torch.tensor([<span class="hljs-number">14</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>target_end_index = torch.tensor([<span class="hljs-number">15</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),{c(){v(d.$$.fragment)},l(f){T(d.$$.fragment,f)},m(f,m){$(d,f,m),b=!0},p:ue,i(f){b||(y(d.$$.fragment,f),b=!0)},o(f){D(d.$$.fragment,f),b=!1},d(f){F(d,f)}}}function K4(B){let d,b,f,m,k,l,u,M,ve,ge,I,re,oe,E,Te,Q,$e,_e,O,ye,ae,H,De,ie,K,Fe,de,V,Ee,be,ee,j,q,le,U,Be,ke,W,xe,we,C,se,J,ce,Me,G,pe,ze,S,he,X,je,ne,P,Ce,A,Pe,qe;return{c(){d=r("p"),b=o("TensorFlow models and layers in "),f=r("code"),m=o("transformers"),k=o(" accept two formats as input:"),l=p(),u=r("ul"),M=r("li"),ve=o("having all inputs as keyword arguments (like PyTorch models), or"),ge=p(),I=r("li"),re=o("having all inputs as a list, tuple or dict in the first positional argument."),oe=p(),E=r("p"),Te=o(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=r("code"),$e=o("model.fit()"),_e=o(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=r("code"),ye=o("model.fit()"),ae=o(` supports! If, however, you want to use the second
format outside of Keras methods like `),H=r("code"),De=o("fit()"),ie=o(" and "),K=r("code"),Fe=o("predict()"),de=o(`, such as when creating your own layers or models with
the Keras `),V=r("code"),Ee=o("Functional"),be=o(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),ee=p(),j=r("ul"),q=r("li"),le=o("a single Tensor with "),U=r("code"),Be=o("input_ids"),ke=o(" only and nothing else: "),W=r("code"),xe=o("model(input_ids)"),we=p(),C=r("li"),se=o(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=r("code"),ce=o("model([input_ids, attention_mask])"),Me=o(" or "),G=r("code"),pe=o("model([input_ids, attention_mask, token_type_ids])"),ze=p(),S=r("li"),he=o(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=r("code"),je=o('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ne=p(),P=r("p"),Ce=o(`Note that when creating models and layers with
`),A=r("a"),Pe=o("subclassing"),qe=o(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(w){d=a(w,"P",{});var x=i(d);b=s(x,"TensorFlow models and layers in "),f=a(x,"CODE",{});var He=i(f);m=s(He,"transformers"),He.forEach(t),k=s(x," accept two formats as input:"),x.forEach(t),l=h(w),u=a(w,"UL",{});var Z=i(u);M=a(Z,"LI",{});var Ke=i(M);ve=s(Ke,"having all inputs as keyword arguments (like PyTorch models), or"),Ke.forEach(t),ge=h(Z),I=a(Z,"LI",{});var Ve=i(I);re=s(Ve,"having all inputs as a list, tuple or dict in the first positional argument."),Ve.forEach(t),Z.forEach(t),oe=h(w),E=a(w,"P",{});var z=i(E);Te=s(z,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=a(z,"CODE",{});var Je=i(Q);$e=s(Je,"model.fit()"),Je.forEach(t),_e=s(z,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=a(z,"CODE",{});var Ge=i(O);ye=s(Ge,"model.fit()"),Ge.forEach(t),ae=s(z,` supports! If, however, you want to use the second
format outside of Keras methods like `),H=a(z,"CODE",{});var Le=i(H);De=s(Le,"fit()"),Le.forEach(t),ie=s(z," and "),K=a(z,"CODE",{});var Xe=i(K);Fe=s(Xe,"predict()"),Xe.forEach(t),de=s(z,`, such as when creating your own layers or models with
the Keras `),V=a(z,"CODE",{});var Ye=i(V);Ee=s(Ye,"Functional"),Ye.forEach(t),be=s(z,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),z.forEach(t),ee=h(w),j=a(w,"UL",{});var L=i(j);q=a(L,"LI",{});var N=i(q);le=s(N,"a single Tensor with "),U=a(N,"CODE",{});var Oe=i(U);Be=s(Oe,"input_ids"),Oe.forEach(t),ke=s(N," only and nothing else: "),W=a(N,"CODE",{});var Re=i(W);xe=s(Re,"model(input_ids)"),Re.forEach(t),N.forEach(t),we=h(L),C=a(L,"LI",{});var R=i(C);se=s(R,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=a(R,"CODE",{});var Ze=i(J);ce=s(Ze,"model([input_ids, attention_mask])"),Ze.forEach(t),Me=s(R," or "),G=a(R,"CODE",{});var We=i(G);pe=s(We,"model([input_ids, attention_mask, token_type_ids])"),We.forEach(t),R.forEach(t),ze=h(L),S=a(L,"LI",{});var Ae=i(S);he=s(Ae,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=a(Ae,"CODE",{});var Ue=i(X);je=s(Ue,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ue.forEach(t),Ae.forEach(t),L.forEach(t),ne=h(w),P=a(w,"P",{});var te=i(P);Ce=s(te,`Note that when creating models and layers with
`),A=a(te,"A",{href:!0,rel:!0});var et=i(A);Pe=s(et,"subclassing"),et.forEach(t),qe=s(te,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),te.forEach(t),this.h()},h(){c(A,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),c(A,"rel","nofollow")},m(w,x){_(w,d,x),e(d,b),e(d,f),e(f,m),e(d,k),_(w,l,x),_(w,u,x),e(u,M),e(M,ve),e(u,ge),e(u,I),e(I,re),_(w,oe,x),_(w,E,x),e(E,Te),e(E,Q),e(Q,$e),e(E,_e),e(E,O),e(O,ye),e(E,ae),e(E,H),e(H,De),e(E,ie),e(E,K),e(K,Fe),e(E,de),e(E,V),e(V,Ee),e(E,be),_(w,ee,x),_(w,j,x),e(j,q),e(q,le),e(q,U),e(U,Be),e(q,ke),e(q,W),e(W,xe),e(j,we),e(j,C),e(C,se),e(C,J),e(J,ce),e(C,Me),e(C,G),e(G,pe),e(j,ze),e(j,S),e(S,he),e(S,X),e(X,je),_(w,ne,x),_(w,P,x),e(P,Ce),e(P,A),e(A,Pe),e(P,qe)},d(w){w&&t(d),w&&t(l),w&&t(u),w&&t(oe),w&&t(E),w&&t(ee),w&&t(j),w&&t(ne),w&&t(P)}}}function V4(B){let d,b,f,m,k;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),k=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),k=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,k)},d(l){l&&t(d)}}}function J4(B){let d,b,f,m,k;return m=new me({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertModel
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertModel.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertModel.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){d=r("p"),b=o("Example:"),f=p(),v(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),k=!0},p:ue,i(l){k||(y(m.$$.fragment,l),k=!0)},o(l){D(m.$$.fragment,l),k=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function G4(B){let d,b,f,m,k,l,u,M,ve,ge,I,re,oe,E,Te,Q,$e,_e,O,ye,ae,H,De,ie,K,Fe,de,V,Ee,be,ee,j,q,le,U,Be,ke,W,xe,we,C,se,J,ce,Me,G,pe,ze,S,he,X,je,ne,P,Ce,A,Pe,qe;return{c(){d=r("p"),b=o("TensorFlow models and layers in "),f=r("code"),m=o("transformers"),k=o(" accept two formats as input:"),l=p(),u=r("ul"),M=r("li"),ve=o("having all inputs as keyword arguments (like PyTorch models), or"),ge=p(),I=r("li"),re=o("having all inputs as a list, tuple or dict in the first positional argument."),oe=p(),E=r("p"),Te=o(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=r("code"),$e=o("model.fit()"),_e=o(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=r("code"),ye=o("model.fit()"),ae=o(` supports! If, however, you want to use the second
format outside of Keras methods like `),H=r("code"),De=o("fit()"),ie=o(" and "),K=r("code"),Fe=o("predict()"),de=o(`, such as when creating your own layers or models with
the Keras `),V=r("code"),Ee=o("Functional"),be=o(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),ee=p(),j=r("ul"),q=r("li"),le=o("a single Tensor with "),U=r("code"),Be=o("input_ids"),ke=o(" only and nothing else: "),W=r("code"),xe=o("model(input_ids)"),we=p(),C=r("li"),se=o(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=r("code"),ce=o("model([input_ids, attention_mask])"),Me=o(" or "),G=r("code"),pe=o("model([input_ids, attention_mask, token_type_ids])"),ze=p(),S=r("li"),he=o(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=r("code"),je=o('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ne=p(),P=r("p"),Ce=o(`Note that when creating models and layers with
`),A=r("a"),Pe=o("subclassing"),qe=o(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(w){d=a(w,"P",{});var x=i(d);b=s(x,"TensorFlow models and layers in "),f=a(x,"CODE",{});var He=i(f);m=s(He,"transformers"),He.forEach(t),k=s(x," accept two formats as input:"),x.forEach(t),l=h(w),u=a(w,"UL",{});var Z=i(u);M=a(Z,"LI",{});var Ke=i(M);ve=s(Ke,"having all inputs as keyword arguments (like PyTorch models), or"),Ke.forEach(t),ge=h(Z),I=a(Z,"LI",{});var Ve=i(I);re=s(Ve,"having all inputs as a list, tuple or dict in the first positional argument."),Ve.forEach(t),Z.forEach(t),oe=h(w),E=a(w,"P",{});var z=i(E);Te=s(z,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=a(z,"CODE",{});var Je=i(Q);$e=s(Je,"model.fit()"),Je.forEach(t),_e=s(z,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=a(z,"CODE",{});var Ge=i(O);ye=s(Ge,"model.fit()"),Ge.forEach(t),ae=s(z,` supports! If, however, you want to use the second
format outside of Keras methods like `),H=a(z,"CODE",{});var Le=i(H);De=s(Le,"fit()"),Le.forEach(t),ie=s(z," and "),K=a(z,"CODE",{});var Xe=i(K);Fe=s(Xe,"predict()"),Xe.forEach(t),de=s(z,`, such as when creating your own layers or models with
the Keras `),V=a(z,"CODE",{});var Ye=i(V);Ee=s(Ye,"Functional"),Ye.forEach(t),be=s(z,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),z.forEach(t),ee=h(w),j=a(w,"UL",{});var L=i(j);q=a(L,"LI",{});var N=i(q);le=s(N,"a single Tensor with "),U=a(N,"CODE",{});var Oe=i(U);Be=s(Oe,"input_ids"),Oe.forEach(t),ke=s(N," only and nothing else: "),W=a(N,"CODE",{});var Re=i(W);xe=s(Re,"model(input_ids)"),Re.forEach(t),N.forEach(t),we=h(L),C=a(L,"LI",{});var R=i(C);se=s(R,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=a(R,"CODE",{});var Ze=i(J);ce=s(Ze,"model([input_ids, attention_mask])"),Ze.forEach(t),Me=s(R," or "),G=a(R,"CODE",{});var We=i(G);pe=s(We,"model([input_ids, attention_mask, token_type_ids])"),We.forEach(t),R.forEach(t),ze=h(L),S=a(L,"LI",{});var Ae=i(S);he=s(Ae,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=a(Ae,"CODE",{});var Ue=i(X);je=s(Ue,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ue.forEach(t),Ae.forEach(t),L.forEach(t),ne=h(w),P=a(w,"P",{});var te=i(P);Ce=s(te,`Note that when creating models and layers with
`),A=a(te,"A",{href:!0,rel:!0});var et=i(A);Pe=s(et,"subclassing"),et.forEach(t),qe=s(te,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),te.forEach(t),this.h()},h(){c(A,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),c(A,"rel","nofollow")},m(w,x){_(w,d,x),e(d,b),e(d,f),e(f,m),e(d,k),_(w,l,x),_(w,u,x),e(u,M),e(M,ve),e(u,ge),e(u,I),e(I,re),_(w,oe,x),_(w,E,x),e(E,Te),e(E,Q),e(Q,$e),e(E,_e),e(E,O),e(O,ye),e(E,ae),e(E,H),e(H,De),e(E,ie),e(E,K),e(K,Fe),e(E,de),e(E,V),e(V,Ee),e(E,be),_(w,ee,x),_(w,j,x),e(j,q),e(q,le),e(q,U),e(U,Be),e(q,ke),e(q,W),e(W,xe),e(j,we),e(j,C),e(C,se),e(C,J),e(J,ce),e(C,Me),e(C,G),e(G,pe),e(j,ze),e(j,S),e(S,he),e(S,X),e(X,je),_(w,ne,x),_(w,P,x),e(P,Ce),e(P,A),e(A,Pe),e(P,qe)},d(w){w&&t(d),w&&t(l),w&&t(u),w&&t(oe),w&&t(E),w&&t(ee),w&&t(j),w&&t(ne),w&&t(P)}}}function X4(B){let d,b,f,m,k;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),k=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),k=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,k)},d(l){l&&t(d)}}}function Y4(B){let d,b,f,m,k;return m=new me({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForMaskedLM
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForMaskedLM.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="tf")
logits = model(**inputs).logits

# retrieve index of [MASK]
mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])
selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)

predicted_token_id = tf.math.argmax(selected_logits, axis=-1)
tokenizer.decode(predicted_token_id)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve index of [MASK]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>selected_logits = tf.gather_nd(logits[<span class="hljs-number">0</span>], indices=mask_token_index)

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_id = tf.math.argmax(selected_logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predicted_token_id)
`}}),{c(){d=r("p"),b=o("Example:"),f=p(),v(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),k=!0},p:ue,i(l){k||(y(m.$$.fragment,l),k=!0)},o(l){D(m.$$.fragment,l),k=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function Z4(B){let d,b;return d=new me({props:{code:`labels = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]
# mask labels of non-[MASK] tokens
labels = tf.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

outputs = model(**inputs, labels=labels)
round(float(outputs.loss), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># mask labels of non-[MASK] tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tf.where(inputs.input_ids == tokenizer.mask_token_id, labels, -<span class="hljs-number">100</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(outputs.loss), <span class="hljs-number">2</span>)
`}}),{c(){v(d.$$.fragment)},l(f){T(d.$$.fragment,f)},m(f,m){$(d,f,m),b=!0},p:ue,i(f){b||(y(d.$$.fragment,f),b=!0)},o(f){D(d.$$.fragment,f),b=!1},d(f){F(d,f)}}}function e3(B){let d,b,f,m,k,l,u,M,ve,ge,I,re,oe,E,Te,Q,$e,_e,O,ye,ae,H,De,ie,K,Fe,de,V,Ee,be,ee,j,q,le,U,Be,ke,W,xe,we,C,se,J,ce,Me,G,pe,ze,S,he,X,je,ne,P,Ce,A,Pe,qe;return{c(){d=r("p"),b=o("TensorFlow models and layers in "),f=r("code"),m=o("transformers"),k=o(" accept two formats as input:"),l=p(),u=r("ul"),M=r("li"),ve=o("having all inputs as keyword arguments (like PyTorch models), or"),ge=p(),I=r("li"),re=o("having all inputs as a list, tuple or dict in the first positional argument."),oe=p(),E=r("p"),Te=o(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=r("code"),$e=o("model.fit()"),_e=o(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=r("code"),ye=o("model.fit()"),ae=o(` supports! If, however, you want to use the second
format outside of Keras methods like `),H=r("code"),De=o("fit()"),ie=o(" and "),K=r("code"),Fe=o("predict()"),de=o(`, such as when creating your own layers or models with
the Keras `),V=r("code"),Ee=o("Functional"),be=o(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),ee=p(),j=r("ul"),q=r("li"),le=o("a single Tensor with "),U=r("code"),Be=o("input_ids"),ke=o(" only and nothing else: "),W=r("code"),xe=o("model(input_ids)"),we=p(),C=r("li"),se=o(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=r("code"),ce=o("model([input_ids, attention_mask])"),Me=o(" or "),G=r("code"),pe=o("model([input_ids, attention_mask, token_type_ids])"),ze=p(),S=r("li"),he=o(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=r("code"),je=o('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ne=p(),P=r("p"),Ce=o(`Note that when creating models and layers with
`),A=r("a"),Pe=o("subclassing"),qe=o(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(w){d=a(w,"P",{});var x=i(d);b=s(x,"TensorFlow models and layers in "),f=a(x,"CODE",{});var He=i(f);m=s(He,"transformers"),He.forEach(t),k=s(x," accept two formats as input:"),x.forEach(t),l=h(w),u=a(w,"UL",{});var Z=i(u);M=a(Z,"LI",{});var Ke=i(M);ve=s(Ke,"having all inputs as keyword arguments (like PyTorch models), or"),Ke.forEach(t),ge=h(Z),I=a(Z,"LI",{});var Ve=i(I);re=s(Ve,"having all inputs as a list, tuple or dict in the first positional argument."),Ve.forEach(t),Z.forEach(t),oe=h(w),E=a(w,"P",{});var z=i(E);Te=s(z,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=a(z,"CODE",{});var Je=i(Q);$e=s(Je,"model.fit()"),Je.forEach(t),_e=s(z,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=a(z,"CODE",{});var Ge=i(O);ye=s(Ge,"model.fit()"),Ge.forEach(t),ae=s(z,` supports! If, however, you want to use the second
format outside of Keras methods like `),H=a(z,"CODE",{});var Le=i(H);De=s(Le,"fit()"),Le.forEach(t),ie=s(z," and "),K=a(z,"CODE",{});var Xe=i(K);Fe=s(Xe,"predict()"),Xe.forEach(t),de=s(z,`, such as when creating your own layers or models with
the Keras `),V=a(z,"CODE",{});var Ye=i(V);Ee=s(Ye,"Functional"),Ye.forEach(t),be=s(z,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),z.forEach(t),ee=h(w),j=a(w,"UL",{});var L=i(j);q=a(L,"LI",{});var N=i(q);le=s(N,"a single Tensor with "),U=a(N,"CODE",{});var Oe=i(U);Be=s(Oe,"input_ids"),Oe.forEach(t),ke=s(N," only and nothing else: "),W=a(N,"CODE",{});var Re=i(W);xe=s(Re,"model(input_ids)"),Re.forEach(t),N.forEach(t),we=h(L),C=a(L,"LI",{});var R=i(C);se=s(R,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=a(R,"CODE",{});var Ze=i(J);ce=s(Ze,"model([input_ids, attention_mask])"),Ze.forEach(t),Me=s(R," or "),G=a(R,"CODE",{});var We=i(G);pe=s(We,"model([input_ids, attention_mask, token_type_ids])"),We.forEach(t),R.forEach(t),ze=h(L),S=a(L,"LI",{});var Ae=i(S);he=s(Ae,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=a(Ae,"CODE",{});var Ue=i(X);je=s(Ue,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ue.forEach(t),Ae.forEach(t),L.forEach(t),ne=h(w),P=a(w,"P",{});var te=i(P);Ce=s(te,`Note that when creating models and layers with
`),A=a(te,"A",{href:!0,rel:!0});var et=i(A);Pe=s(et,"subclassing"),et.forEach(t),qe=s(te,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),te.forEach(t),this.h()},h(){c(A,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),c(A,"rel","nofollow")},m(w,x){_(w,d,x),e(d,b),e(d,f),e(f,m),e(d,k),_(w,l,x),_(w,u,x),e(u,M),e(M,ve),e(u,ge),e(u,I),e(I,re),_(w,oe,x),_(w,E,x),e(E,Te),e(E,Q),e(Q,$e),e(E,_e),e(E,O),e(O,ye),e(E,ae),e(E,H),e(H,De),e(E,ie),e(E,K),e(K,Fe),e(E,de),e(E,V),e(V,Ee),e(E,be),_(w,ee,x),_(w,j,x),e(j,q),e(q,le),e(q,U),e(U,Be),e(q,ke),e(q,W),e(W,xe),e(j,we),e(j,C),e(C,se),e(C,J),e(J,ce),e(C,Me),e(C,G),e(G,pe),e(j,ze),e(j,S),e(S,he),e(S,X),e(X,je),_(w,ne,x),_(w,P,x),e(P,Ce),e(P,A),e(A,Pe),e(P,qe)},d(w){w&&t(d),w&&t(l),w&&t(u),w&&t(oe),w&&t(E),w&&t(ee),w&&t(j),w&&t(ne),w&&t(P)}}}function t3(B){let d,b,f,m,k;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),k=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),k=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,k)},d(l){l&&t(d)}}}function o3(B){let d,b,f,m,k;return m=new me({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")

logits = model(**inputs).logits

predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])
model.config.id2label[predicted_class_id]
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = <span class="hljs-built_in">int</span>(tf.math.argmax(logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
`}}),{c(){d=r("p"),b=o("Example:"),f=p(),v(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),k=!0},p:ue,i(l){k||(y(m.$$.fragment,l),k=!0)},o(l){D(m.$$.fragment,l),k=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function s3(B){let d,b;return d=new me({props:{code:`# To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`
num_labels = len(model.config.id2label)
model = TFDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=num_labels)

labels = tf.constant(1)
loss = model(**inputs, labels=labels).loss
round(float(loss), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=num_labels)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tf.constant(<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(loss), <span class="hljs-number">2</span>)
`}}),{c(){v(d.$$.fragment)},l(f){T(d.$$.fragment,f)},m(f,m){$(d,f,m),b=!0},p:ue,i(f){b||(y(d.$$.fragment,f),b=!0)},o(f){D(d.$$.fragment,f),b=!1},d(f){F(d,f)}}}function n3(B){let d,b,f,m,k,l,u,M,ve,ge,I,re,oe,E,Te,Q,$e,_e,O,ye,ae,H,De,ie,K,Fe,de,V,Ee,be,ee,j,q,le,U,Be,ke,W,xe,we,C,se,J,ce,Me,G,pe,ze,S,he,X,je,ne,P,Ce,A,Pe,qe;return{c(){d=r("p"),b=o("TensorFlow models and layers in "),f=r("code"),m=o("transformers"),k=o(" accept two formats as input:"),l=p(),u=r("ul"),M=r("li"),ve=o("having all inputs as keyword arguments (like PyTorch models), or"),ge=p(),I=r("li"),re=o("having all inputs as a list, tuple or dict in the first positional argument."),oe=p(),E=r("p"),Te=o(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=r("code"),$e=o("model.fit()"),_e=o(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=r("code"),ye=o("model.fit()"),ae=o(` supports! If, however, you want to use the second
format outside of Keras methods like `),H=r("code"),De=o("fit()"),ie=o(" and "),K=r("code"),Fe=o("predict()"),de=o(`, such as when creating your own layers or models with
the Keras `),V=r("code"),Ee=o("Functional"),be=o(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),ee=p(),j=r("ul"),q=r("li"),le=o("a single Tensor with "),U=r("code"),Be=o("input_ids"),ke=o(" only and nothing else: "),W=r("code"),xe=o("model(input_ids)"),we=p(),C=r("li"),se=o(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=r("code"),ce=o("model([input_ids, attention_mask])"),Me=o(" or "),G=r("code"),pe=o("model([input_ids, attention_mask, token_type_ids])"),ze=p(),S=r("li"),he=o(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=r("code"),je=o('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ne=p(),P=r("p"),Ce=o(`Note that when creating models and layers with
`),A=r("a"),Pe=o("subclassing"),qe=o(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(w){d=a(w,"P",{});var x=i(d);b=s(x,"TensorFlow models and layers in "),f=a(x,"CODE",{});var He=i(f);m=s(He,"transformers"),He.forEach(t),k=s(x," accept two formats as input:"),x.forEach(t),l=h(w),u=a(w,"UL",{});var Z=i(u);M=a(Z,"LI",{});var Ke=i(M);ve=s(Ke,"having all inputs as keyword arguments (like PyTorch models), or"),Ke.forEach(t),ge=h(Z),I=a(Z,"LI",{});var Ve=i(I);re=s(Ve,"having all inputs as a list, tuple or dict in the first positional argument."),Ve.forEach(t),Z.forEach(t),oe=h(w),E=a(w,"P",{});var z=i(E);Te=s(z,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=a(z,"CODE",{});var Je=i(Q);$e=s(Je,"model.fit()"),Je.forEach(t),_e=s(z,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=a(z,"CODE",{});var Ge=i(O);ye=s(Ge,"model.fit()"),Ge.forEach(t),ae=s(z,` supports! If, however, you want to use the second
format outside of Keras methods like `),H=a(z,"CODE",{});var Le=i(H);De=s(Le,"fit()"),Le.forEach(t),ie=s(z," and "),K=a(z,"CODE",{});var Xe=i(K);Fe=s(Xe,"predict()"),Xe.forEach(t),de=s(z,`, such as when creating your own layers or models with
the Keras `),V=a(z,"CODE",{});var Ye=i(V);Ee=s(Ye,"Functional"),Ye.forEach(t),be=s(z,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),z.forEach(t),ee=h(w),j=a(w,"UL",{});var L=i(j);q=a(L,"LI",{});var N=i(q);le=s(N,"a single Tensor with "),U=a(N,"CODE",{});var Oe=i(U);Be=s(Oe,"input_ids"),Oe.forEach(t),ke=s(N," only and nothing else: "),W=a(N,"CODE",{});var Re=i(W);xe=s(Re,"model(input_ids)"),Re.forEach(t),N.forEach(t),we=h(L),C=a(L,"LI",{});var R=i(C);se=s(R,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=a(R,"CODE",{});var Ze=i(J);ce=s(Ze,"model([input_ids, attention_mask])"),Ze.forEach(t),Me=s(R," or "),G=a(R,"CODE",{});var We=i(G);pe=s(We,"model([input_ids, attention_mask, token_type_ids])"),We.forEach(t),R.forEach(t),ze=h(L),S=a(L,"LI",{});var Ae=i(S);he=s(Ae,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=a(Ae,"CODE",{});var Ue=i(X);je=s(Ue,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ue.forEach(t),Ae.forEach(t),L.forEach(t),ne=h(w),P=a(w,"P",{});var te=i(P);Ce=s(te,`Note that when creating models and layers with
`),A=a(te,"A",{href:!0,rel:!0});var et=i(A);Pe=s(et,"subclassing"),et.forEach(t),qe=s(te,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),te.forEach(t),this.h()},h(){c(A,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),c(A,"rel","nofollow")},m(w,x){_(w,d,x),e(d,b),e(d,f),e(f,m),e(d,k),_(w,l,x),_(w,u,x),e(u,M),e(M,ve),e(u,ge),e(u,I),e(I,re),_(w,oe,x),_(w,E,x),e(E,Te),e(E,Q),e(Q,$e),e(E,_e),e(E,O),e(O,ye),e(E,ae),e(E,H),e(H,De),e(E,ie),e(E,K),e(K,Fe),e(E,de),e(E,V),e(V,Ee),e(E,be),_(w,ee,x),_(w,j,x),e(j,q),e(q,le),e(q,U),e(U,Be),e(q,ke),e(q,W),e(W,xe),e(j,we),e(j,C),e(C,se),e(C,J),e(J,ce),e(C,Me),e(C,G),e(G,pe),e(j,ze),e(j,S),e(S,he),e(S,X),e(X,je),_(w,ne,x),_(w,P,x),e(P,Ce),e(P,A),e(A,Pe),e(P,qe)},d(w){w&&t(d),w&&t(l),w&&t(u),w&&t(oe),w&&t(E),w&&t(ee),w&&t(j),w&&t(ne),w&&t(P)}}}function r3(B){let d,b,f,m,k;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),k=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),k=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,k)},d(l){l&&t(d)}}}function a3(B){let d,b,f,m,k;return m=new me({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForMultipleChoice
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForMultipleChoice.from_pretrained("distilbert-base-uncased")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="tf", padding=True)
inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}
outputs = model(inputs)  # batch size is 1

# the linear classifier still needs to be trained
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;tf&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {k: tf.expand_dims(v, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){d=r("p"),b=o("Example:"),f=p(),v(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),k=!0},p:ue,i(l){k||(y(m.$$.fragment,l),k=!0)},o(l){D(m.$$.fragment,l),k=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function i3(B){let d,b,f,m,k,l,u,M,ve,ge,I,re,oe,E,Te,Q,$e,_e,O,ye,ae,H,De,ie,K,Fe,de,V,Ee,be,ee,j,q,le,U,Be,ke,W,xe,we,C,se,J,ce,Me,G,pe,ze,S,he,X,je,ne,P,Ce,A,Pe,qe;return{c(){d=r("p"),b=o("TensorFlow models and layers in "),f=r("code"),m=o("transformers"),k=o(" accept two formats as input:"),l=p(),u=r("ul"),M=r("li"),ve=o("having all inputs as keyword arguments (like PyTorch models), or"),ge=p(),I=r("li"),re=o("having all inputs as a list, tuple or dict in the first positional argument."),oe=p(),E=r("p"),Te=o(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=r("code"),$e=o("model.fit()"),_e=o(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=r("code"),ye=o("model.fit()"),ae=o(` supports! If, however, you want to use the second
format outside of Keras methods like `),H=r("code"),De=o("fit()"),ie=o(" and "),K=r("code"),Fe=o("predict()"),de=o(`, such as when creating your own layers or models with
the Keras `),V=r("code"),Ee=o("Functional"),be=o(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),ee=p(),j=r("ul"),q=r("li"),le=o("a single Tensor with "),U=r("code"),Be=o("input_ids"),ke=o(" only and nothing else: "),W=r("code"),xe=o("model(input_ids)"),we=p(),C=r("li"),se=o(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=r("code"),ce=o("model([input_ids, attention_mask])"),Me=o(" or "),G=r("code"),pe=o("model([input_ids, attention_mask, token_type_ids])"),ze=p(),S=r("li"),he=o(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=r("code"),je=o('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ne=p(),P=r("p"),Ce=o(`Note that when creating models and layers with
`),A=r("a"),Pe=o("subclassing"),qe=o(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(w){d=a(w,"P",{});var x=i(d);b=s(x,"TensorFlow models and layers in "),f=a(x,"CODE",{});var He=i(f);m=s(He,"transformers"),He.forEach(t),k=s(x," accept two formats as input:"),x.forEach(t),l=h(w),u=a(w,"UL",{});var Z=i(u);M=a(Z,"LI",{});var Ke=i(M);ve=s(Ke,"having all inputs as keyword arguments (like PyTorch models), or"),Ke.forEach(t),ge=h(Z),I=a(Z,"LI",{});var Ve=i(I);re=s(Ve,"having all inputs as a list, tuple or dict in the first positional argument."),Ve.forEach(t),Z.forEach(t),oe=h(w),E=a(w,"P",{});var z=i(E);Te=s(z,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=a(z,"CODE",{});var Je=i(Q);$e=s(Je,"model.fit()"),Je.forEach(t),_e=s(z,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=a(z,"CODE",{});var Ge=i(O);ye=s(Ge,"model.fit()"),Ge.forEach(t),ae=s(z,` supports! If, however, you want to use the second
format outside of Keras methods like `),H=a(z,"CODE",{});var Le=i(H);De=s(Le,"fit()"),Le.forEach(t),ie=s(z," and "),K=a(z,"CODE",{});var Xe=i(K);Fe=s(Xe,"predict()"),Xe.forEach(t),de=s(z,`, such as when creating your own layers or models with
the Keras `),V=a(z,"CODE",{});var Ye=i(V);Ee=s(Ye,"Functional"),Ye.forEach(t),be=s(z,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),z.forEach(t),ee=h(w),j=a(w,"UL",{});var L=i(j);q=a(L,"LI",{});var N=i(q);le=s(N,"a single Tensor with "),U=a(N,"CODE",{});var Oe=i(U);Be=s(Oe,"input_ids"),Oe.forEach(t),ke=s(N," only and nothing else: "),W=a(N,"CODE",{});var Re=i(W);xe=s(Re,"model(input_ids)"),Re.forEach(t),N.forEach(t),we=h(L),C=a(L,"LI",{});var R=i(C);se=s(R,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=a(R,"CODE",{});var Ze=i(J);ce=s(Ze,"model([input_ids, attention_mask])"),Ze.forEach(t),Me=s(R," or "),G=a(R,"CODE",{});var We=i(G);pe=s(We,"model([input_ids, attention_mask, token_type_ids])"),We.forEach(t),R.forEach(t),ze=h(L),S=a(L,"LI",{});var Ae=i(S);he=s(Ae,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=a(Ae,"CODE",{});var Ue=i(X);je=s(Ue,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ue.forEach(t),Ae.forEach(t),L.forEach(t),ne=h(w),P=a(w,"P",{});var te=i(P);Ce=s(te,`Note that when creating models and layers with
`),A=a(te,"A",{href:!0,rel:!0});var et=i(A);Pe=s(et,"subclassing"),et.forEach(t),qe=s(te,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),te.forEach(t),this.h()},h(){c(A,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),c(A,"rel","nofollow")},m(w,x){_(w,d,x),e(d,b),e(d,f),e(f,m),e(d,k),_(w,l,x),_(w,u,x),e(u,M),e(M,ve),e(u,ge),e(u,I),e(I,re),_(w,oe,x),_(w,E,x),e(E,Te),e(E,Q),e(Q,$e),e(E,_e),e(E,O),e(O,ye),e(E,ae),e(E,H),e(H,De),e(E,ie),e(E,K),e(K,Fe),e(E,de),e(E,V),e(V,Ee),e(E,be),_(w,ee,x),_(w,j,x),e(j,q),e(q,le),e(q,U),e(U,Be),e(q,ke),e(q,W),e(W,xe),e(j,we),e(j,C),e(C,se),e(C,J),e(J,ce),e(C,Me),e(C,G),e(G,pe),e(j,ze),e(j,S),e(S,he),e(S,X),e(X,je),_(w,ne,x),_(w,P,x),e(P,Ce),e(P,A),e(A,Pe),e(P,qe)},d(w){w&&t(d),w&&t(l),w&&t(u),w&&t(oe),w&&t(E),w&&t(ee),w&&t(j),w&&t(ne),w&&t(P)}}}function l3(B){let d,b,f,m,k;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),k=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),k=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,k)},d(l){l&&t(d)}}}function d3(B){let d,b,f,m,k;return m=new me({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForTokenClassification
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForTokenClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer(
    "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="tf"
)

logits = model(**inputs).logits
predicted_token_class_ids = tf.math.argmax(logits, axis=-1)

# Note that tokens are classified rather then input words which means that
# there might be more predicted token classes than words.
# Multiple token classes might account for the same word
predicted_tokens_classes = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]
predicted_tokens_classes
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;HuggingFace is a company based in Paris and New York&quot;</span>, add_special_tokens=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_class_ids = tf.math.argmax(logits, axis=-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Note that tokens are classified rather then input words which means that</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># there might be more predicted token classes than words.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multiple token classes might account for the same word</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes = [model.config.id2label[t] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> predicted_token_class_ids[<span class="hljs-number">0</span>].numpy().tolist()]
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes
`}}),{c(){d=r("p"),b=o("Example:"),f=p(),v(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),k=!0},p:ue,i(l){k||(y(m.$$.fragment,l),k=!0)},o(l){D(m.$$.fragment,l),k=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function c3(B){let d,b;return d=new me({props:{code:`labels = predicted_token_class_ids
loss = tf.math.reduce_mean(model(**inputs, labels=labels).loss)
round(float(loss), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = predicted_token_class_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = tf.math.reduce_mean(model(**inputs, labels=labels).loss)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(loss), <span class="hljs-number">2</span>)
`}}),{c(){v(d.$$.fragment)},l(f){T(d.$$.fragment,f)},m(f,m){$(d,f,m),b=!0},p:ue,i(f){b||(y(d.$$.fragment,f),b=!0)},o(f){D(d.$$.fragment,f),b=!1},d(f){F(d,f)}}}function p3(B){let d,b,f,m,k,l,u,M,ve,ge,I,re,oe,E,Te,Q,$e,_e,O,ye,ae,H,De,ie,K,Fe,de,V,Ee,be,ee,j,q,le,U,Be,ke,W,xe,we,C,se,J,ce,Me,G,pe,ze,S,he,X,je,ne,P,Ce,A,Pe,qe;return{c(){d=r("p"),b=o("TensorFlow models and layers in "),f=r("code"),m=o("transformers"),k=o(" accept two formats as input:"),l=p(),u=r("ul"),M=r("li"),ve=o("having all inputs as keyword arguments (like PyTorch models), or"),ge=p(),I=r("li"),re=o("having all inputs as a list, tuple or dict in the first positional argument."),oe=p(),E=r("p"),Te=o(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=r("code"),$e=o("model.fit()"),_e=o(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=r("code"),ye=o("model.fit()"),ae=o(` supports! If, however, you want to use the second
format outside of Keras methods like `),H=r("code"),De=o("fit()"),ie=o(" and "),K=r("code"),Fe=o("predict()"),de=o(`, such as when creating your own layers or models with
the Keras `),V=r("code"),Ee=o("Functional"),be=o(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),ee=p(),j=r("ul"),q=r("li"),le=o("a single Tensor with "),U=r("code"),Be=o("input_ids"),ke=o(" only and nothing else: "),W=r("code"),xe=o("model(input_ids)"),we=p(),C=r("li"),se=o(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=r("code"),ce=o("model([input_ids, attention_mask])"),Me=o(" or "),G=r("code"),pe=o("model([input_ids, attention_mask, token_type_ids])"),ze=p(),S=r("li"),he=o(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=r("code"),je=o('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ne=p(),P=r("p"),Ce=o(`Note that when creating models and layers with
`),A=r("a"),Pe=o("subclassing"),qe=o(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(w){d=a(w,"P",{});var x=i(d);b=s(x,"TensorFlow models and layers in "),f=a(x,"CODE",{});var He=i(f);m=s(He,"transformers"),He.forEach(t),k=s(x," accept two formats as input:"),x.forEach(t),l=h(w),u=a(w,"UL",{});var Z=i(u);M=a(Z,"LI",{});var Ke=i(M);ve=s(Ke,"having all inputs as keyword arguments (like PyTorch models), or"),Ke.forEach(t),ge=h(Z),I=a(Z,"LI",{});var Ve=i(I);re=s(Ve,"having all inputs as a list, tuple or dict in the first positional argument."),Ve.forEach(t),Z.forEach(t),oe=h(w),E=a(w,"P",{});var z=i(E);Te=s(z,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),Q=a(z,"CODE",{});var Je=i(Q);$e=s(Je,"model.fit()"),Je.forEach(t),_e=s(z,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),O=a(z,"CODE",{});var Ge=i(O);ye=s(Ge,"model.fit()"),Ge.forEach(t),ae=s(z,` supports! If, however, you want to use the second
format outside of Keras methods like `),H=a(z,"CODE",{});var Le=i(H);De=s(Le,"fit()"),Le.forEach(t),ie=s(z," and "),K=a(z,"CODE",{});var Xe=i(K);Fe=s(Xe,"predict()"),Xe.forEach(t),de=s(z,`, such as when creating your own layers or models with
the Keras `),V=a(z,"CODE",{});var Ye=i(V);Ee=s(Ye,"Functional"),Ye.forEach(t),be=s(z,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),z.forEach(t),ee=h(w),j=a(w,"UL",{});var L=i(j);q=a(L,"LI",{});var N=i(q);le=s(N,"a single Tensor with "),U=a(N,"CODE",{});var Oe=i(U);Be=s(Oe,"input_ids"),Oe.forEach(t),ke=s(N," only and nothing else: "),W=a(N,"CODE",{});var Re=i(W);xe=s(Re,"model(input_ids)"),Re.forEach(t),N.forEach(t),we=h(L),C=a(L,"LI",{});var R=i(C);se=s(R,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),J=a(R,"CODE",{});var Ze=i(J);ce=s(Ze,"model([input_ids, attention_mask])"),Ze.forEach(t),Me=s(R," or "),G=a(R,"CODE",{});var We=i(G);pe=s(We,"model([input_ids, attention_mask, token_type_ids])"),We.forEach(t),R.forEach(t),ze=h(L),S=a(L,"LI",{});var Ae=i(S);he=s(Ae,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),X=a(Ae,"CODE",{});var Ue=i(X);je=s(Ue,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ue.forEach(t),Ae.forEach(t),L.forEach(t),ne=h(w),P=a(w,"P",{});var te=i(P);Ce=s(te,`Note that when creating models and layers with
`),A=a(te,"A",{href:!0,rel:!0});var et=i(A);Pe=s(et,"subclassing"),et.forEach(t),qe=s(te,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),te.forEach(t),this.h()},h(){c(A,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),c(A,"rel","nofollow")},m(w,x){_(w,d,x),e(d,b),e(d,f),e(f,m),e(d,k),_(w,l,x),_(w,u,x),e(u,M),e(M,ve),e(u,ge),e(u,I),e(I,re),_(w,oe,x),_(w,E,x),e(E,Te),e(E,Q),e(Q,$e),e(E,_e),e(E,O),e(O,ye),e(E,ae),e(E,H),e(H,De),e(E,ie),e(E,K),e(K,Fe),e(E,de),e(E,V),e(V,Ee),e(E,be),_(w,ee,x),_(w,j,x),e(j,q),e(q,le),e(q,U),e(U,Be),e(q,ke),e(q,W),e(W,xe),e(j,we),e(j,C),e(C,se),e(C,J),e(J,ce),e(C,Me),e(C,G),e(G,pe),e(j,ze),e(j,S),e(S,he),e(S,X),e(X,je),_(w,ne,x),_(w,P,x),e(P,Ce),e(P,A),e(A,Pe),e(P,qe)},d(w){w&&t(d),w&&t(l),w&&t(u),w&&t(oe),w&&t(E),w&&t(ee),w&&t(j),w&&t(ne),w&&t(P)}}}function h3(B){let d,b,f,m,k;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),k=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),k=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,k)},d(l){l&&t(d)}}}function f3(B){let d,b,f,m,k;return m=new me({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

inputs = tokenizer(question, text, return_tensors="tf")
outputs = model(**inputs)

answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])
answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])

predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
tokenizer.decode(predict_answer_tokens)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>answer_start_index = <span class="hljs-built_in">int</span>(tf.math.argmax(outputs.start_logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>answer_end_index = <span class="hljs-built_in">int</span>(tf.math.argmax(outputs.end_logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>predict_answer_tokens = inputs.input_ids[<span class="hljs-number">0</span>, answer_start_index : answer_end_index + <span class="hljs-number">1</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predict_answer_tokens)
`}}),{c(){d=r("p"),b=o("Example:"),f=p(),v(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),k=!0},p:ue,i(l){k||(y(m.$$.fragment,l),k=!0)},o(l){D(m.$$.fragment,l),k=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function u3(B){let d,b;return d=new me({props:{code:`# target is "nice puppet"
target_start_index = tf.constant([14])
target_end_index = tf.constant([15])

outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
loss = tf.math.reduce_mean(outputs.loss)
round(float(loss), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target is &quot;nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_start_index = tf.constant([<span class="hljs-number">14</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>target_end_index = tf.constant([<span class="hljs-number">15</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = tf.math.reduce_mean(outputs.loss)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(loss), <span class="hljs-number">2</span>)
`}}),{c(){v(d.$$.fragment)},l(f){T(d.$$.fragment,f)},m(f,m){$(d,f,m),b=!0},p:ue,i(f){b||(y(d.$$.fragment,f),b=!0)},o(f){D(d.$$.fragment,f),b=!1},d(f){F(d,f)}}}function m3(B){let d,b,f,m,k;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),k=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),k=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,k)},d(l){l&&t(d)}}}function g3(B){let d,b,f,m,k;return m=new me({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertModel

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertModel.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertModel.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){d=r("p"),b=o("Example:"),f=p(),v(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),k=!0},p:ue,i(l){k||(y(m.$$.fragment,l),k=!0)},o(l){D(m.$$.fragment,l),k=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function _3(B){let d,b,f,m,k;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),k=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),k=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,k)},d(l){l&&t(d)}}}function b3(B){let d,b,f,m,k;return m=new me({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForMaskedLM

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForMaskedLM.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){d=r("p"),b=o("Example:"),f=p(),v(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),k=!0},p:ue,i(l){k||(y(m.$$.fragment,l),k=!0)},o(l){D(m.$$.fragment,l),k=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function k3(B){let d,b,f,m,k;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),k=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),k=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,k)},d(l){l&&t(d)}}}function w3(B){let d,b,f,m,k;return m=new me({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForSequenceClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){d=r("p"),b=o("Example:"),f=p(),v(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),k=!0},p:ue,i(l){k||(y(m.$$.fragment,l),k=!0)},o(l){D(m.$$.fragment,l),k=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function v3(B){let d,b,f,m,k;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),k=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),k=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,k)},d(l){l&&t(d)}}}function T3(B){let d,b,f,m,k;return m=new me({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForMultipleChoice

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForMultipleChoice.from_pretrained("distilbert-base-uncased")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="jax", padding=True)
outputs = model(**{k: v[None, :] for k, v in encoding.items()})

logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;jax&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v[<span class="hljs-literal">None</span>, :] <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()})

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){d=r("p"),b=o("Example:"),f=p(),v(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),k=!0},p:ue,i(l){k||(y(m.$$.fragment,l),k=!0)},o(l){D(m.$$.fragment,l),k=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function $3(B){let d,b,f,m,k;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),k=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),k=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,k)},d(l){l&&t(d)}}}function y3(B){let d,b,f,m,k;return m=new me({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForTokenClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForTokenClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){d=r("p"),b=o("Example:"),f=p(),v(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),k=!0},p:ue,i(l){k||(y(m.$$.fragment,l),k=!0)},o(l){D(m.$$.fragment,l),k=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function D3(B){let d,b,f,m,k;return{c(){d=r("p"),b=o("Although the recipe for forward pass needs to be defined within this function, one should call the "),f=r("code"),m=o("Module"),k=o(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),f=a(u,"CODE",{});var M=i(f);m=s(M,"Module"),M.forEach(t),k=s(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(l,u){_(l,d,u),e(d,b),e(d,f),e(f,m),e(d,k)},d(l){l&&t(d)}}}function F3(B){let d,b,f,m,k;return m=new me({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForQuestionAnswering

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
inputs = tokenizer(question, text, return_tensors="jax")

outputs = model(**inputs)
start_scores = outputs.start_logits
end_scores = outputs.end_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_scores = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_scores = outputs.end_logits`}}),{c(){d=r("p"),b=o("Example:"),f=p(),v(m.$$.fragment)},l(l){d=a(l,"P",{});var u=i(d);b=s(u,"Example:"),u.forEach(t),f=h(l),T(m.$$.fragment,l)},m(l,u){_(l,d,u),e(d,b),_(l,f,u),$(m,l,u),k=!0},p:ue,i(l){k||(y(m.$$.fragment,l),k=!0)},o(l){D(m.$$.fragment,l),k=!1},d(l){l&&t(d),l&&t(f),F(m,l)}}}function E3(B){let d,b,f,m,k,l,u,M,ve,ge,I,re,oe,E,Te,Q,$e,_e,O,ye,ae,H,De,ie,K,Fe,de,V,Ee,be,ee,j,q,le,U,Be,ke,W,xe,we,C,se,J,ce,Me,G,pe,ze,S,he,X,je,ne,P,Ce,A,Pe,qe,w,x,He,Z,Ke,Ve,z,Je,Ge,Le,Xe,Ye,L,N,Oe,Re,R,Ze,We,Ae,Ue,te,et,gu,Er,_u,Ie,Br,Ng,xr,Rg,Wg,Qg,Mr,Ug,zr,Hg,Kg,Vg,jr,Jg,Cr,Gg,Xg,Yg,Pr,Zg,qr,e_,t_,o_,Ar,s_,Or,n_,r_,a_,Lr,i_,Ir,l_,d_,c_,Sr,p_,Nr,h_,f_,u_,fo,Nd,m_,g_,Rr,__,b_,Wr,k_,w_,v_,uo,Rd,T_,$_,Qr,y_,D_,Ur,F_,E_,B_,mo,Wd,x_,M_,Hr,z_,j_,Kr,C_,P_,bu,Vr,ku,Ot,go,Qd,q_,A_,Jr,O_,L_,Gr,I_,S_,N_,_o,Ud,R_,W_,Xr,Q_,U_,Yr,H_,K_,V_,zs,Hd,J_,G_,Zr,X_,Y_,Z_,Kd,ea,eb,tb,wu,ta,vu,Lt,bo,Vd,ob,sb,oa,nb,rb,sa,ab,ib,lb,ko,Jd,db,cb,na,pb,hb,ra,fb,ub,mb,wo,Gd,gb,_b,aa,bb,kb,ia,wb,vb,Tb,Xd,la,$b,yb,Tu,da,$u,It,vo,Yd,Db,Fb,ca,Eb,Bb,pa,xb,Mb,zb,To,Zd,jb,Cb,ha,Pb,qb,fa,Ab,Ob,Lb,js,ec,Ib,Sb,ua,Nb,Rb,Wb,tc,ma,Qb,Ub,yu,oc,bp,Hb,Du,Cs,$o,sc,Kb,Vb,ga,Jb,Gb,_a,Xb,Yb,Zb,yo,nc,ek,tk,ba,ok,sk,ka,nk,rk,Fu,rc,ak,Eu,Do,wa,ik,va,lk,dk,ck,Ta,pk,$a,hk,fk,uk,ya,mk,Da,gk,_k,Bu,ac,bk,xu,Ps,Fa,kk,Ea,wk,vk,Tk,Ba,$k,xa,yk,Dk,Mu,ic,Fk,zu,Fo,Ma,Ek,za,Bk,xk,Mk,ja,zk,Ca,jk,Ck,Pk,Pa,qk,qa,Ak,Ok,ju,Ro,qs,kp,Aa,Lk,wp,Ik,Cu,Pt,Oa,Sk,oo,Nk,lc,Rk,Wk,dc,Qk,Uk,La,Hk,Kk,Vk,Wo,Jk,cc,Gk,Xk,pc,Yk,Zk,ew,As,Pu,Qo,Os,vp,Ia,tw,Tp,ow,qu,qt,Sa,sw,$p,nw,rw,Ls,hc,aw,iw,fc,lw,dw,cw,Na,pw,uc,hw,fw,Au,Uo,Is,yp,Ra,uw,Dp,mw,Ou,At,Wa,gw,Qa,_w,Fp,bw,kw,ww,Ss,mc,vw,Tw,gc,$w,yw,Dw,Ua,Fw,_c,Ew,Bw,Lu,Ho,Ns,Ep,Ha,xw,Bp,Mw,Iu,ut,Ka,zw,xp,jw,Cw,Va,Pw,bc,qw,Aw,Ow,Ja,Lw,Ga,Iw,Sw,Nw,St,Xa,Rw,Ko,Ww,kc,Qw,Uw,Mp,Hw,Kw,Vw,Rs,Jw,Ws,Su,Vo,Qs,zp,Ya,Gw,jp,Xw,Nu,mt,Za,Yw,ei,Zw,Cp,ev,tv,ov,ti,sv,wc,nv,rv,av,oi,iv,si,lv,dv,cv,wt,ni,pv,Jo,hv,vc,fv,uv,Pp,mv,gv,_v,Us,bv,Hs,kv,Ks,Ru,Go,Vs,qp,ri,wv,Ap,vv,Wu,gt,ai,Tv,Op,$v,yv,ii,Dv,Tc,Fv,Ev,Bv,li,xv,di,Mv,zv,jv,it,ci,Cv,Xo,Pv,$c,qv,Av,Lp,Ov,Lv,Iv,Js,Sv,Gs,Nv,Xs,Rv,Ys,Wv,Zs,Qu,Yo,en,Ip,pi,Qv,Sp,Uv,Uu,_t,hi,Hv,Np,Kv,Vv,fi,Jv,yc,Gv,Xv,Yv,ui,Zv,mi,eT,tT,oT,Nt,gi,sT,Zo,nT,Dc,rT,aT,Rp,iT,lT,dT,tn,cT,on,Hu,es,sn,Wp,_i,pT,Qp,hT,Ku,bt,bi,fT,Up,uT,mT,ki,gT,Fc,_T,bT,kT,wi,wT,vi,vT,TT,$T,vt,Ti,yT,ts,DT,Ec,FT,ET,Hp,BT,xT,MT,nn,zT,rn,jT,an,Vu,os,ln,Kp,$i,CT,Vp,PT,Ju,kt,yi,qT,ss,AT,Jp,OT,LT,Gp,IT,ST,NT,Di,RT,Bc,WT,QT,UT,Fi,HT,Ei,KT,VT,JT,Tt,Bi,GT,ns,XT,xc,YT,ZT,Xp,e$,t$,o$,dn,s$,cn,n$,pn,Gu,rs,hn,Yp,xi,r$,Zp,a$,Xu,lt,Mi,i$,eh,l$,d$,zi,c$,Mc,p$,h$,f$,ji,u$,Ci,m$,g$,_$,fn,b$,Rt,Pi,k$,as,w$,zc,v$,T$,th,$$,y$,D$,un,F$,mn,Yu,is,gn,oh,qi,E$,sh,B$,Zu,dt,Ai,x$,Oi,M$,nh,z$,j$,C$,Li,P$,jc,q$,A$,O$,Ii,L$,Si,I$,S$,N$,_n,R$,$t,Ni,W$,ls,Q$,Cc,U$,H$,rh,K$,V$,J$,bn,G$,kn,X$,wn,em,ds,vn,ah,Ri,Y$,ih,Z$,tm,ct,Wi,ey,lh,ty,oy,Qi,sy,Pc,ny,ry,ay,Ui,iy,Hi,ly,dy,cy,Tn,py,yt,Ki,hy,cs,fy,qc,uy,my,dh,gy,_y,by,$n,ky,yn,wy,Dn,om,ps,Fn,ch,Vi,vy,ph,Ty,sm,pt,Ji,$y,hh,yy,Dy,Gi,Fy,Ac,Ey,By,xy,Xi,My,Yi,zy,jy,Cy,En,Py,Wt,Zi,qy,hs,Ay,Oc,Oy,Ly,fh,Iy,Sy,Ny,Bn,Ry,xn,nm,fs,Mn,uh,el,Wy,mh,Qy,rm,ht,tl,Uy,gh,Hy,Ky,ol,Vy,Lc,Jy,Gy,Xy,sl,Yy,nl,Zy,e1,t1,zn,o1,Dt,rl,s1,us,n1,Ic,r1,a1,_h,i1,l1,d1,jn,c1,Cn,p1,Pn,am,ms,qn,bh,al,h1,kh,f1,im,ft,il,u1,gs,m1,wh,g1,_1,vh,b1,k1,w1,ll,v1,Sc,T1,$1,y1,dl,D1,cl,F1,E1,B1,An,x1,Ft,pl,M1,_s,z1,Nc,j1,C1,Th,P1,q1,A1,On,O1,Ln,L1,In,lm,bs,Sn,$h,hl,I1,yh,S1,dm,tt,fl,N1,Dh,R1,W1,ul,Q1,Rc,U1,H1,K1,ml,V1,gl,J1,G1,X1,Fh,Y1,Z1,so,Eh,_l,e2,t2,Bh,bl,o2,s2,xh,kl,n2,r2,Mh,wl,a2,i2,Qt,vl,l2,ks,d2,zh,c2,p2,jh,h2,f2,u2,Nn,m2,Rn,cm,ws,Wn,Ch,Tl,g2,Ph,_2,pm,ot,$l,b2,yl,k2,qh,w2,v2,T2,Dl,$2,Wc,y2,D2,F2,Fl,E2,El,B2,x2,M2,Ah,z2,j2,no,Oh,Bl,C2,P2,Lh,xl,q2,A2,Ih,Ml,O2,L2,Sh,zl,I2,S2,Ut,jl,N2,vs,R2,Nh,W2,Q2,Rh,U2,H2,K2,Qn,V2,Un,hm,Ts,Hn,Wh,Cl,J2,Qh,G2,fm,st,Pl,X2,Uh,Y2,Z2,ql,eD,Qc,tD,oD,sD,Al,nD,Ol,rD,aD,iD,Hh,lD,dD,ro,Kh,Ll,cD,pD,Vh,Il,hD,fD,Jh,Sl,uD,mD,Gh,Nl,gD,_D,Ht,Rl,bD,$s,kD,Xh,wD,vD,Yh,TD,$D,yD,Kn,DD,Vn,um,ys,Jn,Zh,Wl,FD,ef,ED,mm,nt,Ql,BD,tf,xD,MD,Ul,zD,Uc,jD,CD,PD,Hl,qD,Kl,AD,OD,LD,of,ID,SD,ao,sf,Vl,ND,RD,nf,Jl,WD,QD,rf,Gl,UD,HD,af,Xl,KD,VD,Kt,Yl,JD,Ds,GD,lf,XD,YD,df,ZD,eF,tF,Gn,oF,Xn,gm,Fs,Yn,cf,Zl,sF,pf,nF,_m,rt,ed,rF,hf,aF,iF,td,lF,Hc,dF,cF,pF,od,hF,sd,fF,uF,mF,ff,gF,_F,io,uf,nd,bF,kF,mf,rd,wF,vF,gf,ad,TF,$F,_f,id,yF,DF,Vt,ld,FF,Es,EF,bf,BF,xF,kf,MF,zF,jF,Zn,CF,er,bm,Bs,tr,wf,dd,PF,vf,qF,km,at,cd,AF,xs,OF,Tf,LF,IF,$f,SF,NF,RF,pd,WF,Kc,QF,UF,HF,hd,KF,fd,VF,JF,GF,yf,XF,YF,lo,Df,ud,ZF,eE,Ff,md,tE,oE,Ef,gd,sE,nE,Bf,_d,rE,aE,Jt,bd,iE,Ms,lE,xf,dE,cE,Mf,pE,hE,fE,or,uE,sr,wm;return l=new Ne({}),E=new Ne({}),R=new Ne({}),Er=new Sg({props:{pipeline:"text-classification"}}),Vr=new Sg({props:{pipeline:"token-classification"}}),ta=new Sg({props:{pipeline:"fill-mask"}}),da=new Sg({props:{pipeline:"question-answering"}}),Aa=new Ne({}),Oa=new Y({props:{name:"class transformers.DistilBertConfig",anchor:"transformers.DistilBertConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"max_position_embeddings",val:" = 512"},{name:"sinusoidal_pos_embds",val:" = False"},{name:"n_layers",val:" = 6"},{name:"n_heads",val:" = 12"},{name:"dim",val:" = 768"},{name:"hidden_dim",val:" = 3072"},{name:"dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.1"},{name:"activation",val:" = 'gelu'"},{name:"initializer_range",val:" = 0.02"},{name:"qa_dropout",val:" = 0.1"},{name:"seq_classif_dropout",val:" = 0.2"},{name:"pad_token_id",val:" = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DistilBertConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the DistilBERT model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> or <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a>.`,name:"vocab_size"},{anchor:"transformers.DistilBertConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.DistilBertConfig.sinusoidal_pos_embds",description:`<strong>sinusoidal_pos_embds</strong> (<code>boolean</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use sinusoidal positional embeddings.`,name:"sinusoidal_pos_embds"},{anchor:"transformers.DistilBertConfig.n_layers",description:`<strong>n_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"n_layers"},{anchor:"transformers.DistilBertConfig.n_heads",description:`<strong>n_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"n_heads"},{anchor:"transformers.DistilBertConfig.dim",description:`<strong>dim</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"dim"},{anchor:"transformers.DistilBertConfig.hidden_dim",description:`<strong>hidden_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
The size of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the Transformer encoder.`,name:"hidden_dim"},{anchor:"transformers.DistilBertConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.DistilBertConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.DistilBertConfig.activation",description:`<strong>activation</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation"},{anchor:"transformers.DistilBertConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DistilBertConfig.qa_dropout",description:`<strong>qa_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilities used in the question answering model <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a>.`,name:"qa_dropout"},{anchor:"transformers.DistilBertConfig.seq_classif_dropout",description:`<strong>seq_classif_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.2) &#x2014;
The dropout probabilities used in the sequence classification and the multiple choice model
<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a>.`,name:"seq_classif_dropout"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/configuration_distilbert.py#L45"}}),As=new fe({props:{anchor:"transformers.DistilBertConfig.example",$$slots:{default:[B4]},$$scope:{ctx:B}}}),Ia=new Ne({}),Sa=new Y({props:{name:"class transformers.DistilBertTokenizer",anchor:"transformers.DistilBertTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/tokenization_distilbert.py#L62"}}),Ra=new Ne({}),Wa=new Y({props:{name:"class transformers.DistilBertTokenizerFast",anchor:"transformers.DistilBertTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/tokenization_distilbert_fast.py#L79"}}),Ha=new Ne({}),Ka=new Y({props:{name:"class transformers.DistilBertModel",anchor:"transformers.DistilBertModel",parameters:[{name:"config",val:": PretrainedConfig"}],parametersDescription:[{anchor:"transformers.DistilBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L453"}}),Xa=new Y({props:{name:"forward",anchor:"transformers.DistilBertModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DistilBertModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L525",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Rs=new Se({props:{$$slots:{default:[x4]},$$scope:{ctx:B}}}),Ws=new fe({props:{anchor:"transformers.DistilBertModel.forward.example",$$slots:{default:[M4]},$$scope:{ctx:B}}}),Ya=new Ne({}),Za=new Y({props:{name:"class transformers.DistilBertForMaskedLM",anchor:"transformers.DistilBertForMaskedLM",parameters:[{name:"config",val:": PretrainedConfig"}],parametersDescription:[{anchor:"transformers.DistilBertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L581"}}),ni=new Y({props:{name:"forward",anchor:"transformers.DistilBertForMaskedLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DistilBertForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForMaskedLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForMaskedLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L625",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Us=new Se({props:{$$slots:{default:[z4]},$$scope:{ctx:B}}}),Hs=new fe({props:{anchor:"transformers.DistilBertForMaskedLM.forward.example",$$slots:{default:[j4]},$$scope:{ctx:B}}}),Ks=new fe({props:{anchor:"transformers.DistilBertForMaskedLM.forward.example-2",$$slots:{default:[C4]},$$scope:{ctx:B}}}),ri=new Ne({}),ai=new Y({props:{name:"class transformers.DistilBertForSequenceClassification",anchor:"transformers.DistilBertForSequenceClassification",parameters:[{name:"config",val:": PretrainedConfig"}],parametersDescription:[{anchor:"transformers.DistilBertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L689"}}),ci=new Y({props:{name:"forward",anchor:"transformers.DistilBertForSequenceClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DistilBertForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForSequenceClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L723",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Js=new Se({props:{$$slots:{default:[P4]},$$scope:{ctx:B}}}),Gs=new fe({props:{anchor:"transformers.DistilBertForSequenceClassification.forward.example",$$slots:{default:[q4]},$$scope:{ctx:B}}}),Xs=new fe({props:{anchor:"transformers.DistilBertForSequenceClassification.forward.example-2",$$slots:{default:[A4]},$$scope:{ctx:B}}}),Ys=new fe({props:{anchor:"transformers.DistilBertForSequenceClassification.forward.example-3",$$slots:{default:[O4]},$$scope:{ctx:B}}}),Zs=new fe({props:{anchor:"transformers.DistilBertForSequenceClassification.forward.example-4",$$slots:{default:[L4]},$$scope:{ctx:B}}}),pi=new Ne({}),hi=new Y({props:{name:"class transformers.DistilBertForMultipleChoice",anchor:"transformers.DistilBertForMultipleChoice",parameters:[{name:"config",val:": PretrainedConfig"}],parametersDescription:[{anchor:"transformers.DistilBertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L1019"}}),gi=new Y({props:{name:"forward",anchor:"transformers.DistilBertForMultipleChoice.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DistilBertForMultipleChoice.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForMultipleChoice.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForMultipleChoice.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForMultipleChoice.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForMultipleChoice.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForMultipleChoice.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForMultipleChoice.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForMultipleChoice.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices-1]</code> where <code>num_choices</code> is the size of the second dimension of the input tensors. (See
<code>input_ids</code> above)`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L1051",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),tn=new Se({props:{$$slots:{default:[I4]},$$scope:{ctx:B}}}),on=new fe({props:{anchor:"transformers.DistilBertForMultipleChoice.forward.example",$$slots:{default:[S4]},$$scope:{ctx:B}}}),_i=new Ne({}),bi=new Y({props:{name:"class transformers.DistilBertForTokenClassification",anchor:"transformers.DistilBertForTokenClassification",parameters:[{name:"config",val:": PretrainedConfig"}],parametersDescription:[{anchor:"transformers.DistilBertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L924"}}),Ti=new Y({props:{name:"forward",anchor:"transformers.DistilBertForTokenClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DistilBertForTokenClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>({0})</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForTokenClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>({0})</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForTokenClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForTokenClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>({0}, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForTokenClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForTokenClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForTokenClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForTokenClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L956",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),nn=new Se({props:{$$slots:{default:[N4]},$$scope:{ctx:B}}}),rn=new fe({props:{anchor:"transformers.DistilBertForTokenClassification.forward.example",$$slots:{default:[R4]},$$scope:{ctx:B}}}),an=new fe({props:{anchor:"transformers.DistilBertForTokenClassification.forward.example-2",$$slots:{default:[W4]},$$scope:{ctx:B}}}),$i=new Ne({}),yi=new Y({props:{name:"class transformers.DistilBertForQuestionAnswering",anchor:"transformers.DistilBertForQuestionAnswering",parameters:[{name:"config",val:": PretrainedConfig"}],parametersDescription:[{anchor:"transformers.DistilBertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L807"}}),Bi=new Y({props:{name:"forward",anchor:"transformers.DistilBertForQuestionAnswering.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"start_positions",val:": typing.Optional[torch.Tensor] = None"},{name:"end_positions",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.DistilBertForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L839",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),dn=new Se({props:{$$slots:{default:[Q4]},$$scope:{ctx:B}}}),cn=new fe({props:{anchor:"transformers.DistilBertForQuestionAnswering.forward.example",$$slots:{default:[U4]},$$scope:{ctx:B}}}),pn=new fe({props:{anchor:"transformers.DistilBertForQuestionAnswering.forward.example-2",$$slots:{default:[H4]},$$scope:{ctx:B}}}),xi=new Ne({}),Mi=new Y({props:{name:"class transformers.TFDistilBertModel",anchor:"transformers.TFDistilBertModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDistilBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L537"}}),fn=new Se({props:{$$slots:{default:[K4]},$$scope:{ctx:B}}}),Pi=new Y({props:{name:"call",anchor:"transformers.TFDistilBertModel.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"training",val:": typing.Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFDistilBertModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertModel.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L542",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),un=new Se({props:{$$slots:{default:[V4]},$$scope:{ctx:B}}}),mn=new fe({props:{anchor:"transformers.TFDistilBertModel.call.example",$$slots:{default:[J4]},$$scope:{ctx:B}}}),qi=new Ne({}),Ai=new Y({props:{name:"class transformers.TFDistilBertForMaskedLM",anchor:"transformers.TFDistilBertForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDistilBertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L624"}}),_n=new Se({props:{$$slots:{default:[G4]},$$scope:{ctx:B}}}),Ni=new Y({props:{name:"call",anchor:"transformers.TFDistilBertForMaskedLM.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFDistilBertForMaskedLM.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForMaskedLM.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForMaskedLM.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForMaskedLM.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForMaskedLM.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForMaskedLM.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForMaskedLM.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForMaskedLM.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForMaskedLM.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L644",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),bn=new Se({props:{$$slots:{default:[X4]},$$scope:{ctx:B}}}),kn=new fe({props:{anchor:"transformers.TFDistilBertForMaskedLM.call.example",$$slots:{default:[Y4]},$$scope:{ctx:B}}}),wn=new fe({props:{anchor:"transformers.TFDistilBertForMaskedLM.call.example-2",$$slots:{default:[Z4]},$$scope:{ctx:B}}}),Ri=new Ne({}),Wi=new Y({props:{name:"class transformers.TFDistilBertForSequenceClassification",anchor:"transformers.TFDistilBertForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDistilBertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L714"}}),Tn=new Se({props:{$$slots:{default:[e3]},$$scope:{ctx:B}}}),Ki=new Y({props:{name:"call",anchor:"transformers.TFDistilBertForSequenceClassification.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFDistilBertForSequenceClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L731",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),$n=new Se({props:{$$slots:{default:[t3]},$$scope:{ctx:B}}}),yn=new fe({props:{anchor:"transformers.TFDistilBertForSequenceClassification.call.example",$$slots:{default:[o3]},$$scope:{ctx:B}}}),Dn=new fe({props:{anchor:"transformers.TFDistilBertForSequenceClassification.call.example-2",$$slots:{default:[s3]},$$scope:{ctx:B}}}),Vi=new Ne({}),Ji=new Y({props:{name:"class transformers.TFDistilBertForMultipleChoice",anchor:"transformers.TFDistilBertForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDistilBertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L877"}}),En=new Se({props:{$$slots:{default:[n3]},$$scope:{ctx:B}}}),Zi=new Y({props:{name:"call",anchor:"transformers.TFDistilBertForMultipleChoice.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFDistilBertForMultipleChoice.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices]</code>
where <code>num_choices</code> is the size of the second dimension of the input tensors. (See <code>input_ids</code> above)`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L903",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <em>(batch_size, )</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Bn=new Se({props:{$$slots:{default:[r3]},$$scope:{ctx:B}}}),xn=new fe({props:{anchor:"transformers.TFDistilBertForMultipleChoice.call.example",$$slots:{default:[a3]},$$scope:{ctx:B}}}),el=new Ne({}),tl=new Y({props:{name:"class transformers.TFDistilBertForTokenClassification",anchor:"transformers.TFDistilBertForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDistilBertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L801"}}),zn=new Se({props:{$$slots:{default:[i3]},$$scope:{ctx:B}}}),rl=new Y({props:{name:"call",anchor:"transformers.TFDistilBertForTokenClassification.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFDistilBertForTokenClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForTokenClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForTokenClassification.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForTokenClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForTokenClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForTokenClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForTokenClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForTokenClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForTokenClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L812",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of unmasked labels, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),jn=new Se({props:{$$slots:{default:[l3]},$$scope:{ctx:B}}}),Cn=new fe({props:{anchor:"transformers.TFDistilBertForTokenClassification.call.example",$$slots:{default:[d3]},$$scope:{ctx:B}}}),Pn=new fe({props:{anchor:"transformers.TFDistilBertForTokenClassification.call.example-2",$$slots:{default:[c3]},$$scope:{ctx:B}}}),al=new Ne({}),il=new Y({props:{name:"class transformers.TFDistilBertForQuestionAnswering",anchor:"transformers.TFDistilBertForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDistilBertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L1002"}}),An=new Se({props:{$$slots:{default:[p3]},$$scope:{ctx:B}}}),pl=new Y({props:{name:"call",anchor:"transformers.TFDistilBertForQuestionAnswering.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"start_positions",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"end_positions",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFDistilBertForQuestionAnswering.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.start_positions",description:`<strong>start_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.end_positions",description:`<strong>end_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L1013",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>start_positions</code> and <code>end_positions</code> are provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),On=new Se({props:{$$slots:{default:[h3]},$$scope:{ctx:B}}}),Ln=new fe({props:{anchor:"transformers.TFDistilBertForQuestionAnswering.call.example",$$slots:{default:[f3]},$$scope:{ctx:B}}}),In=new fe({props:{anchor:"transformers.TFDistilBertForQuestionAnswering.call.example-2",$$slots:{default:[u3]},$$scope:{ctx:B}}}),hl=new Ne({}),fl=new Y({props:{name:"class transformers.FlaxDistilBertModel",anchor:"transformers.FlaxDistilBertModel",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxDistilBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L535"}}),vl=new Y({props:{name:"__call__",anchor:"transformers.FlaxDistilBertModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlaxDistilBertModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L458"}}),Nn=new Se({props:{$$slots:{default:[m3]},$$scope:{ctx:B}}}),Rn=new fe({props:{anchor:"transformers.FlaxDistilBertModel.__call__.example",$$slots:{default:[g3]},$$scope:{ctx:B}}}),Tl=new Ne({}),$l=new Y({props:{name:"class transformers.FlaxDistilBertForMaskedLM",anchor:"transformers.FlaxDistilBertForMaskedLM",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxDistilBertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L608"}}),jl=new Y({props:{name:"__call__",anchor:"transformers.FlaxDistilBertForMaskedLM.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L458",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Qn=new Se({props:{$$slots:{default:[_3]},$$scope:{ctx:B}}}),Un=new fe({props:{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.example",$$slots:{default:[b3]},$$scope:{ctx:B}}}),Cl=new Ne({}),Pl=new Y({props:{name:"class transformers.FlaxDistilBertForSequenceClassification",anchor:"transformers.FlaxDistilBertForSequenceClassification",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxDistilBertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L677"}}),Rl=new Y({props:{name:"__call__",anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L458",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"
>transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"
>transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Kn=new Se({props:{$$slots:{default:[k3]},$$scope:{ctx:B}}}),Vn=new fe({props:{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.example",$$slots:{default:[w3]},$$scope:{ctx:B}}}),Wl=new Ne({}),Ql=new Y({props:{name:"class transformers.FlaxDistilBertForMultipleChoice",anchor:"transformers.FlaxDistilBertForMultipleChoice",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxDistilBertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L757"}}),Yl=new Y({props:{name:"__call__",anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L458",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput"
>transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput"
>transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Gn=new Se({props:{$$slots:{default:[v3]},$$scope:{ctx:B}}}),Xn=new fe({props:{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.example",$$slots:{default:[T3]},$$scope:{ctx:B}}}),Zl=new Ne({}),ed=new Y({props:{name:"class transformers.FlaxDistilBertForTokenClassification",anchor:"transformers.FlaxDistilBertForTokenClassification",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxDistilBertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L823"}}),ld=new Y({props:{name:"__call__",anchor:"transformers.FlaxDistilBertForTokenClassification.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L458",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxTokenClassifierOutput"
>transformers.modeling_flax_outputs.FlaxTokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxTokenClassifierOutput"
>transformers.modeling_flax_outputs.FlaxTokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Zn=new Se({props:{$$slots:{default:[$3]},$$scope:{ctx:B}}}),er=new fe({props:{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.example",$$slots:{default:[y3]},$$scope:{ctx:B}}}),dd=new Ne({}),cd=new Y({props:{name:"class transformers.FlaxDistilBertForQuestionAnswering",anchor:"transformers.FlaxDistilBertForQuestionAnswering",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxDistilBertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L893"}}),bd=new Y({props:{name:"__call__",anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L458",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput"
>transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput"
>transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),or=new Se({props:{$$slots:{default:[D3]},$$scope:{ctx:B}}}),sr=new fe({props:{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.example",$$slots:{default:[F3]},$$scope:{ctx:B}}}),{c(){d=r("meta"),b=p(),f=r("h1"),m=r("a"),k=r("span"),v(l.$$.fragment),u=p(),M=r("span"),ve=o("DistilBERT"),ge=p(),I=r("h2"),re=r("a"),oe=r("span"),v(E.$$.fragment),Te=p(),Q=r("span"),$e=o("Overview"),_e=p(),O=r("p"),ye=o("The DistilBERT model was proposed in the blog post "),ae=r("a"),H=o(`Smaller, faster, cheaper, lighter: Introducing DistilBERT, a
distilled version of BERT`),De=o(", and the paper "),ie=r("a"),K=o(`DistilBERT, a
distilled version of BERT: smaller, faster, cheaper and lighter`),Fe=o(`. DistilBERT is a
small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than
`),de=r("em"),V=o("bert-base-uncased"),Ee=o(`, runs 60% faster while preserving over 95% of BERT\u2019s performances as measured on the GLUE language
understanding benchmark.`),be=p(),ee=r("p"),j=o("The abstract from the paper is the following:"),q=p(),le=r("p"),U=r("em"),Be=o(`As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP),
operating these large models in on-the-edge and/or under constrained computational training or inference budgets
remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation
model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger
counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage
knowledge distillation during the pretraining phase and show that it is possible to reduce the size of a BERT model by
40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive
biases learned by larger models during pretraining, we introduce a triple loss combining language modeling,
distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we
demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device
study.`),ke=p(),W=r("p"),xe=o("Tips:"),we=p(),C=r("ul"),se=r("li"),J=o("DistilBERT doesn\u2019t have "),ce=r("code"),Me=o("token_type_ids"),G=o(`, you don\u2019t need to indicate which token belongs to which segment. Just
separate your segments with the separation token `),pe=r("code"),ze=o("tokenizer.sep_token"),S=o(" (or "),he=r("code"),X=o("[SEP]"),je=o(")."),ne=p(),P=r("li"),Ce=o("DistilBERT doesn\u2019t have options to select the input positions ("),A=r("code"),Pe=o("position_ids"),qe=o(` input). This could be added if
necessary though, just let us know if you need this option.`),w=p(),x=r("p"),He=o("This model was contributed by "),Z=r("a"),Ke=o("victorsanh"),Ve=o(`. This model jax version was
contributed by `),z=r("a"),Je=o("kamalkraj"),Ge=o(". The original code can be found "),Le=r("a"),Xe=o("here"),Ye=o("."),L=p(),N=r("h2"),Oe=r("a"),Re=r("span"),v(R.$$.fragment),Ze=p(),We=r("span"),Ae=o("Resources"),Ue=p(),te=r("p"),et=o("A list of official Hugging Face and community (indicated by \u{1F30E}) resources to help you get started with DistilBERT. If you\u2019re interested in submitting a resource to be included here, please feel free to open a Pull Request and we\u2019ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource."),gu=p(),v(Er.$$.fragment),_u=p(),Ie=r("ul"),Br=r("li"),Ng=o("A blog post on "),xr=r("a"),Rg=o("Getting Started with Sentiment Analysis using Python"),Wg=o(" with DistilBERT."),Qg=p(),Mr=r("li"),Ug=o("A blog post on how to "),zr=r("a"),Hg=o("train DistilBERT with Blurr for sequence classification"),Kg=o("."),Vg=p(),jr=r("li"),Jg=o("A blog post on how to use "),Cr=r("a"),Gg=o("Ray to tune DistilBERT hyperparameters"),Xg=o("."),Yg=p(),Pr=r("li"),Zg=o("A blog post on how to "),qr=r("a"),e_=o("train DistilBERT with Hugging Face and Amazon SageMaker"),t_=o("."),o_=p(),Ar=r("li"),s_=o("A notebook on how to "),Or=r("a"),n_=o("finetune DistilBERT for multi-label classification"),r_=o(". \u{1F30E}"),a_=p(),Lr=r("li"),i_=o("A notebook on how to "),Ir=r("a"),l_=o("finetune DistilBERT for multiclass classification with PyTorch"),d_=o(". \u{1F30E}"),c_=p(),Sr=r("li"),p_=o("A notebook on how to "),Nr=r("a"),h_=o("finetune DistilBERT for text classification in TensorFlow"),f_=o(". \u{1F30E}"),u_=p(),fo=r("li"),Nd=r("a"),m_=o("DistilBertForSequenceClassification"),g_=o(" is supported by this "),Rr=r("a"),__=o("example script"),b_=o(" and "),Wr=r("a"),k_=o("notebook"),w_=o("."),v_=p(),uo=r("li"),Rd=r("a"),T_=o("TFDistilBertForSequenceClassification"),$_=o(" is supported by this "),Qr=r("a"),y_=o("example script"),D_=o(" and "),Ur=r("a"),F_=o("notebook"),E_=o("."),B_=p(),mo=r("li"),Wd=r("a"),x_=o("FlaxDistilBertForSequenceClassification"),M_=o(" is supported by this "),Hr=r("a"),z_=o("example script"),j_=o(" and "),Kr=r("a"),C_=o("notebook"),P_=o("."),bu=p(),v(Vr.$$.fragment),ku=p(),Ot=r("ul"),go=r("li"),Qd=r("a"),q_=o("DistilBertForTokenClassification"),A_=o(" is supported by this "),Jr=r("a"),O_=o("example script"),L_=o(" and "),Gr=r("a"),I_=o("notebook"),S_=o("."),N_=p(),_o=r("li"),Ud=r("a"),R_=o("TFDistilBertForTokenClassification"),W_=o(" is supported by this "),Xr=r("a"),Q_=o("example script"),U_=o(" and "),Yr=r("a"),H_=o("notebook"),K_=o("."),V_=p(),zs=r("li"),Hd=r("a"),J_=o("FlaxDistilBertForTokenClassification"),G_=o(" is supported by this "),Zr=r("a"),X_=o("example script"),Y_=o("."),Z_=p(),Kd=r("li"),ea=r("a"),eb=o("Token classification"),tb=o(" chapter of the \u{1F917} Hugging Face Course."),wu=p(),v(ta.$$.fragment),vu=p(),Lt=r("ul"),bo=r("li"),Vd=r("a"),ob=o("DistilBertForMaskedLM"),sb=o(" is supported by this "),oa=r("a"),nb=o("example script"),rb=o(" and "),sa=r("a"),ab=o("notebook"),ib=o("."),lb=p(),ko=r("li"),Jd=r("a"),db=o("TFDistilBertForMaskedLM"),cb=o(" is supported by this "),na=r("a"),pb=o("example script"),hb=o(" and "),ra=r("a"),fb=o("notebook"),ub=o("."),mb=p(),wo=r("li"),Gd=r("a"),gb=o("FlaxDistilBertForMaskedLM"),_b=o(" is supported by this "),aa=r("a"),bb=o("example script"),kb=o(" and "),ia=r("a"),wb=o("notebook"),vb=o("."),Tb=p(),Xd=r("li"),la=r("a"),$b=o("Masked language modeling"),yb=o(" chapter of the \u{1F917} Hugging Face Course."),Tu=p(),v(da.$$.fragment),$u=p(),It=r("ul"),vo=r("li"),Yd=r("a"),Db=o("DistilBertForQuestionAnswering"),Fb=o(" is supported by this "),ca=r("a"),Eb=o("example script"),Bb=o(" and "),pa=r("a"),xb=o("notebook"),Mb=o("."),zb=p(),To=r("li"),Zd=r("a"),jb=o("TFDistilBertForQuestionAnswering"),Cb=o(" is supported by this "),ha=r("a"),Pb=o("example script"),qb=o(" and "),fa=r("a"),Ab=o("notebook"),Ob=o("."),Lb=p(),js=r("li"),ec=r("a"),Ib=o("FlaxDistilBertForQuestionAnswering"),Sb=o(" is supported by this "),ua=r("a"),Nb=o("example script"),Rb=o("."),Wb=p(),tc=r("li"),ma=r("a"),Qb=o("Question answering"),Ub=o(" chapter of the \u{1F917} Hugging Face Course."),yu=p(),oc=r("p"),bp=r("strong"),Hb=o("Multiple choice"),Du=p(),Cs=r("ul"),$o=r("li"),sc=r("a"),Kb=o("DistilBertForMultipleChoice"),Vb=o(" is supported by this "),ga=r("a"),Jb=o("example script"),Gb=o(" and "),_a=r("a"),Xb=o("notebook"),Yb=o("."),Zb=p(),yo=r("li"),nc=r("a"),ek=o("TFDistilBertForMultipleChoice"),tk=o(" is supported by this "),ba=r("a"),ok=o("example script"),sk=o(" and "),ka=r("a"),nk=o("notebook"),rk=o("."),Fu=p(),rc=r("p"),ak=o("\u2697\uFE0F Optimization"),Eu=p(),Do=r("ul"),wa=r("li"),ik=o("A blog post on how to "),va=r("a"),lk=o("quantize DistilBERT with \u{1F917} Optimum and Intel"),dk=o("."),ck=p(),Ta=r("li"),pk=o("A blog post on how "),$a=r("a"),hk=o("Optimizing Transformers for GPUs with \u{1F917} Optimum"),fk=o("."),uk=p(),ya=r("li"),mk=o("A blog post on "),Da=r("a"),gk=o("Optimizing Transformers with Hugging Face Optimum"),_k=o("."),Bu=p(),ac=r("p"),bk=o("\u26A1\uFE0F Inference"),xu=p(),Ps=r("ul"),Fa=r("li"),kk=o("A blog post on how to "),Ea=r("a"),wk=o("Accelerate BERT inference with Hugging Face Transformers and AWS Inferentia"),vk=o(" with DistilBERT."),Tk=p(),Ba=r("li"),$k=o("A blog post on "),xa=r("a"),yk=o("Serverless Inference with Hugging Face\u2019s Transformers, DistilBERT and Amazon SageMaker"),Dk=o("."),Mu=p(),ic=r("p"),Fk=o("\u{1F680} Deploy"),zu=p(),Fo=r("ul"),Ma=r("li"),Ek=o("A blog post on how to "),za=r("a"),Bk=o("deploy DistilBERT on Google Cloud"),xk=o("."),Mk=p(),ja=r("li"),zk=o("A blog post on how to "),Ca=r("a"),jk=o("deploy DistilBERT with Amazon SageMaker"),Ck=o("."),Pk=p(),Pa=r("li"),qk=o("A blog post on how to "),qa=r("a"),Ak=o("Deploy BERT with Hugging Face Transformers, Amazon SageMaker and Terraform module"),Ok=o("."),ju=p(),Ro=r("h2"),qs=r("a"),kp=r("span"),v(Aa.$$.fragment),Lk=p(),wp=r("span"),Ik=o("DistilBertConfig"),Cu=p(),Pt=r("div"),v(Oa.$$.fragment),Sk=p(),oo=r("p"),Nk=o("This is the configuration class to store the configuration of a "),lc=r("a"),Rk=o("DistilBertModel"),Wk=o(" or a "),dc=r("a"),Qk=o("TFDistilBertModel"),Uk=o(`. It
is used to instantiate a DistilBERT model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the DistilBERT
`),La=r("a"),Hk=o("distilbert-base-uncased"),Kk=o(" architecture."),Vk=p(),Wo=r("p"),Jk=o("Configuration objects inherit from "),cc=r("a"),Gk=o("PretrainedConfig"),Xk=o(` and can be used to control the model outputs. Read the
documentation from `),pc=r("a"),Yk=o("PretrainedConfig"),Zk=o(" for more information."),ew=p(),v(As.$$.fragment),Pu=p(),Qo=r("h2"),Os=r("a"),vp=r("span"),v(Ia.$$.fragment),tw=p(),Tp=r("span"),ow=o("DistilBertTokenizer"),qu=p(),qt=r("div"),v(Sa.$$.fragment),sw=p(),$p=r("p"),nw=o("Construct a DistilBERT tokenizer."),rw=p(),Ls=r("p"),hc=r("a"),aw=o("DistilBertTokenizer"),iw=o(" is identical to "),fc=r("a"),lw=o("BertTokenizer"),dw=o(` and runs end-to-end tokenization: punctuation splitting
and wordpiece.`),cw=p(),Na=r("p"),pw=o("Refer to superclass "),uc=r("a"),hw=o("BertTokenizer"),fw=o(" for usage examples and documentation concerning parameters."),Au=p(),Uo=r("h2"),Is=r("a"),yp=r("span"),v(Ra.$$.fragment),uw=p(),Dp=r("span"),mw=o("DistilBertTokenizerFast"),Ou=p(),At=r("div"),v(Wa.$$.fragment),gw=p(),Qa=r("p"),_w=o("Construct a \u201Cfast\u201D DistilBERT tokenizer (backed by HuggingFace\u2019s "),Fp=r("em"),bw=o("tokenizers"),kw=o(" library)."),ww=p(),Ss=r("p"),mc=r("a"),vw=o("DistilBertTokenizerFast"),Tw=o(" is identical to "),gc=r("a"),$w=o("BertTokenizerFast"),yw=o(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Dw=p(),Ua=r("p"),Fw=o("Refer to superclass "),_c=r("a"),Ew=o("BertTokenizerFast"),Bw=o(" for usage examples and documentation concerning parameters."),Lu=p(),Ho=r("h2"),Ns=r("a"),Ep=r("span"),v(Ha.$$.fragment),xw=p(),Bp=r("span"),Mw=o("DistilBertModel"),Iu=p(),ut=r("div"),v(Ka.$$.fragment),zw=p(),xp=r("p"),jw=o("The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),Cw=p(),Va=r("p"),Pw=o("This model inherits from "),bc=r("a"),qw=o("PreTrainedModel"),Aw=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ow=p(),Ja=r("p"),Lw=o("This model is also a PyTorch "),Ga=r("a"),Iw=o("torch.nn.Module"),Sw=o(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Nw=p(),St=r("div"),v(Xa.$$.fragment),Rw=p(),Ko=r("p"),Ww=o("The "),kc=r("a"),Qw=o("DistilBertModel"),Uw=o(" forward method, overrides the "),Mp=r("code"),Hw=o("__call__"),Kw=o(" special method."),Vw=p(),v(Rs.$$.fragment),Jw=p(),v(Ws.$$.fragment),Su=p(),Vo=r("h2"),Qs=r("a"),zp=r("span"),v(Ya.$$.fragment),Gw=p(),jp=r("span"),Xw=o("DistilBertForMaskedLM"),Nu=p(),mt=r("div"),v(Za.$$.fragment),Yw=p(),ei=r("p"),Zw=o("DistilBert Model with a "),Cp=r("code"),ev=o("masked language modeling"),tv=o(" head on top."),ov=p(),ti=r("p"),sv=o("This model inherits from "),wc=r("a"),nv=o("PreTrainedModel"),rv=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),av=p(),oi=r("p"),iv=o("This model is also a PyTorch "),si=r("a"),lv=o("torch.nn.Module"),dv=o(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),cv=p(),wt=r("div"),v(ni.$$.fragment),pv=p(),Jo=r("p"),hv=o("The "),vc=r("a"),fv=o("DistilBertForMaskedLM"),uv=o(" forward method, overrides the "),Pp=r("code"),mv=o("__call__"),gv=o(" special method."),_v=p(),v(Us.$$.fragment),bv=p(),v(Hs.$$.fragment),kv=p(),v(Ks.$$.fragment),Ru=p(),Go=r("h2"),Vs=r("a"),qp=r("span"),v(ri.$$.fragment),wv=p(),Ap=r("span"),vv=o("DistilBertForSequenceClassification"),Wu=p(),gt=r("div"),v(ai.$$.fragment),Tv=p(),Op=r("p"),$v=o(`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),yv=p(),ii=r("p"),Dv=o("This model inherits from "),Tc=r("a"),Fv=o("PreTrainedModel"),Ev=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Bv=p(),li=r("p"),xv=o("This model is also a PyTorch "),di=r("a"),Mv=o("torch.nn.Module"),zv=o(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),jv=p(),it=r("div"),v(ci.$$.fragment),Cv=p(),Xo=r("p"),Pv=o("The "),$c=r("a"),qv=o("DistilBertForSequenceClassification"),Av=o(" forward method, overrides the "),Lp=r("code"),Ov=o("__call__"),Lv=o(" special method."),Iv=p(),v(Js.$$.fragment),Sv=p(),v(Gs.$$.fragment),Nv=p(),v(Xs.$$.fragment),Rv=p(),v(Ys.$$.fragment),Wv=p(),v(Zs.$$.fragment),Qu=p(),Yo=r("h2"),en=r("a"),Ip=r("span"),v(pi.$$.fragment),Qv=p(),Sp=r("span"),Uv=o("DistilBertForMultipleChoice"),Uu=p(),_t=r("div"),v(hi.$$.fragment),Hv=p(),Np=r("p"),Kv=o(`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),Vv=p(),fi=r("p"),Jv=o("This model inherits from "),yc=r("a"),Gv=o("PreTrainedModel"),Xv=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Yv=p(),ui=r("p"),Zv=o("This model is also a PyTorch "),mi=r("a"),eT=o("torch.nn.Module"),tT=o(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),oT=p(),Nt=r("div"),v(gi.$$.fragment),sT=p(),Zo=r("p"),nT=o("The "),Dc=r("a"),rT=o("DistilBertForMultipleChoice"),aT=o(" forward method, overrides the "),Rp=r("code"),iT=o("__call__"),lT=o(" special method."),dT=p(),v(tn.$$.fragment),cT=p(),v(on.$$.fragment),Hu=p(),es=r("h2"),sn=r("a"),Wp=r("span"),v(_i.$$.fragment),pT=p(),Qp=r("span"),hT=o("DistilBertForTokenClassification"),Ku=p(),bt=r("div"),v(bi.$$.fragment),fT=p(),Up=r("p"),uT=o(`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),mT=p(),ki=r("p"),gT=o("This model inherits from "),Fc=r("a"),_T=o("PreTrainedModel"),bT=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),kT=p(),wi=r("p"),wT=o("This model is also a PyTorch "),vi=r("a"),vT=o("torch.nn.Module"),TT=o(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),$T=p(),vt=r("div"),v(Ti.$$.fragment),yT=p(),ts=r("p"),DT=o("The "),Ec=r("a"),FT=o("DistilBertForTokenClassification"),ET=o(" forward method, overrides the "),Hp=r("code"),BT=o("__call__"),xT=o(" special method."),MT=p(),v(nn.$$.fragment),zT=p(),v(rn.$$.fragment),jT=p(),v(an.$$.fragment),Vu=p(),os=r("h2"),ln=r("a"),Kp=r("span"),v($i.$$.fragment),CT=p(),Vp=r("span"),PT=o("DistilBertForQuestionAnswering"),Ju=p(),kt=r("div"),v(yi.$$.fragment),qT=p(),ss=r("p"),AT=o(`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),Jp=r("code"),OT=o("span start logits"),LT=o(" and "),Gp=r("code"),IT=o("span end logits"),ST=o(")."),NT=p(),Di=r("p"),RT=o("This model inherits from "),Bc=r("a"),WT=o("PreTrainedModel"),QT=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),UT=p(),Fi=r("p"),HT=o("This model is also a PyTorch "),Ei=r("a"),KT=o("torch.nn.Module"),VT=o(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),JT=p(),Tt=r("div"),v(Bi.$$.fragment),GT=p(),ns=r("p"),XT=o("The "),xc=r("a"),YT=o("DistilBertForQuestionAnswering"),ZT=o(" forward method, overrides the "),Xp=r("code"),e$=o("__call__"),t$=o(" special method."),o$=p(),v(dn.$$.fragment),s$=p(),v(cn.$$.fragment),n$=p(),v(pn.$$.fragment),Gu=p(),rs=r("h2"),hn=r("a"),Yp=r("span"),v(xi.$$.fragment),r$=p(),Zp=r("span"),a$=o("TFDistilBertModel"),Xu=p(),lt=r("div"),v(Mi.$$.fragment),i$=p(),eh=r("p"),l$=o("The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),d$=p(),zi=r("p"),c$=o("This model inherits from "),Mc=r("a"),p$=o("TFPreTrainedModel"),h$=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),f$=p(),ji=r("p"),u$=o("This model is also a "),Ci=r("a"),m$=o("tf.keras.Model"),g$=o(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),_$=p(),v(fn.$$.fragment),b$=p(),Rt=r("div"),v(Pi.$$.fragment),k$=p(),as=r("p"),w$=o("The "),zc=r("a"),v$=o("TFDistilBertModel"),T$=o(" forward method, overrides the "),th=r("code"),$$=o("__call__"),y$=o(" special method."),D$=p(),v(un.$$.fragment),F$=p(),v(mn.$$.fragment),Yu=p(),is=r("h2"),gn=r("a"),oh=r("span"),v(qi.$$.fragment),E$=p(),sh=r("span"),B$=o("TFDistilBertForMaskedLM"),Zu=p(),dt=r("div"),v(Ai.$$.fragment),x$=p(),Oi=r("p"),M$=o("DistilBert Model with a "),nh=r("code"),z$=o("masked language modeling"),j$=o(" head on top."),C$=p(),Li=r("p"),P$=o("This model inherits from "),jc=r("a"),q$=o("TFPreTrainedModel"),A$=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),O$=p(),Ii=r("p"),L$=o("This model is also a "),Si=r("a"),I$=o("tf.keras.Model"),S$=o(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),N$=p(),v(_n.$$.fragment),R$=p(),$t=r("div"),v(Ni.$$.fragment),W$=p(),ls=r("p"),Q$=o("The "),Cc=r("a"),U$=o("TFDistilBertForMaskedLM"),H$=o(" forward method, overrides the "),rh=r("code"),K$=o("__call__"),V$=o(" special method."),J$=p(),v(bn.$$.fragment),G$=p(),v(kn.$$.fragment),X$=p(),v(wn.$$.fragment),em=p(),ds=r("h2"),vn=r("a"),ah=r("span"),v(Ri.$$.fragment),Y$=p(),ih=r("span"),Z$=o("TFDistilBertForSequenceClassification"),tm=p(),ct=r("div"),v(Wi.$$.fragment),ey=p(),lh=r("p"),ty=o(`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),oy=p(),Qi=r("p"),sy=o("This model inherits from "),Pc=r("a"),ny=o("TFPreTrainedModel"),ry=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ay=p(),Ui=r("p"),iy=o("This model is also a "),Hi=r("a"),ly=o("tf.keras.Model"),dy=o(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),cy=p(),v(Tn.$$.fragment),py=p(),yt=r("div"),v(Ki.$$.fragment),hy=p(),cs=r("p"),fy=o("The "),qc=r("a"),uy=o("TFDistilBertForSequenceClassification"),my=o(" forward method, overrides the "),dh=r("code"),gy=o("__call__"),_y=o(" special method."),by=p(),v($n.$$.fragment),ky=p(),v(yn.$$.fragment),wy=p(),v(Dn.$$.fragment),om=p(),ps=r("h2"),Fn=r("a"),ch=r("span"),v(Vi.$$.fragment),vy=p(),ph=r("span"),Ty=o("TFDistilBertForMultipleChoice"),sm=p(),pt=r("div"),v(Ji.$$.fragment),$y=p(),hh=r("p"),yy=o(`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),Dy=p(),Gi=r("p"),Fy=o("This model inherits from "),Ac=r("a"),Ey=o("TFPreTrainedModel"),By=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),xy=p(),Xi=r("p"),My=o("This model is also a "),Yi=r("a"),zy=o("tf.keras.Model"),jy=o(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Cy=p(),v(En.$$.fragment),Py=p(),Wt=r("div"),v(Zi.$$.fragment),qy=p(),hs=r("p"),Ay=o("The "),Oc=r("a"),Oy=o("TFDistilBertForMultipleChoice"),Ly=o(" forward method, overrides the "),fh=r("code"),Iy=o("__call__"),Sy=o(" special method."),Ny=p(),v(Bn.$$.fragment),Ry=p(),v(xn.$$.fragment),nm=p(),fs=r("h2"),Mn=r("a"),uh=r("span"),v(el.$$.fragment),Wy=p(),mh=r("span"),Qy=o("TFDistilBertForTokenClassification"),rm=p(),ht=r("div"),v(tl.$$.fragment),Uy=p(),gh=r("p"),Hy=o(`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),Ky=p(),ol=r("p"),Vy=o("This model inherits from "),Lc=r("a"),Jy=o("TFPreTrainedModel"),Gy=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Xy=p(),sl=r("p"),Yy=o("This model is also a "),nl=r("a"),Zy=o("tf.keras.Model"),e1=o(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),t1=p(),v(zn.$$.fragment),o1=p(),Dt=r("div"),v(rl.$$.fragment),s1=p(),us=r("p"),n1=o("The "),Ic=r("a"),r1=o("TFDistilBertForTokenClassification"),a1=o(" forward method, overrides the "),_h=r("code"),i1=o("__call__"),l1=o(" special method."),d1=p(),v(jn.$$.fragment),c1=p(),v(Cn.$$.fragment),p1=p(),v(Pn.$$.fragment),am=p(),ms=r("h2"),qn=r("a"),bh=r("span"),v(al.$$.fragment),h1=p(),kh=r("span"),f1=o("TFDistilBertForQuestionAnswering"),im=p(),ft=r("div"),v(il.$$.fragment),u1=p(),gs=r("p"),m1=o(`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layer on top of the hidden-states output to compute `),wh=r("code"),g1=o("span start logits"),_1=o(" and "),vh=r("code"),b1=o("span end logits"),k1=o(")."),w1=p(),ll=r("p"),v1=o("This model inherits from "),Sc=r("a"),T1=o("TFPreTrainedModel"),$1=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),y1=p(),dl=r("p"),D1=o("This model is also a "),cl=r("a"),F1=o("tf.keras.Model"),E1=o(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),B1=p(),v(An.$$.fragment),x1=p(),Ft=r("div"),v(pl.$$.fragment),M1=p(),_s=r("p"),z1=o("The "),Nc=r("a"),j1=o("TFDistilBertForQuestionAnswering"),C1=o(" forward method, overrides the "),Th=r("code"),P1=o("__call__"),q1=o(" special method."),A1=p(),v(On.$$.fragment),O1=p(),v(Ln.$$.fragment),L1=p(),v(In.$$.fragment),lm=p(),bs=r("h2"),Sn=r("a"),$h=r("span"),v(hl.$$.fragment),I1=p(),yh=r("span"),S1=o("FlaxDistilBertModel"),dm=p(),tt=r("div"),v(fl.$$.fragment),N1=p(),Dh=r("p"),R1=o("The bare DistilBert Model transformer outputting raw hidden-states without any specific head on top."),W1=p(),ul=r("p"),Q1=o("This model inherits from "),Rc=r("a"),U1=o("FlaxPreTrainedModel"),H1=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),K1=p(),ml=r("p"),V1=o("This model is also a Flax Linen "),gl=r("a"),J1=o("flax.linen.Module"),G1=o(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),X1=p(),Fh=r("p"),Y1=o("Finally, this model supports inherent JAX features such as:"),Z1=p(),so=r("ul"),Eh=r("li"),_l=r("a"),e2=o("Just-In-Time (JIT) compilation"),t2=p(),Bh=r("li"),bl=r("a"),o2=o("Automatic Differentiation"),s2=p(),xh=r("li"),kl=r("a"),n2=o("Vectorization"),r2=p(),Mh=r("li"),wl=r("a"),a2=o("Parallelization"),i2=p(),Qt=r("div"),v(vl.$$.fragment),l2=p(),ks=r("p"),d2=o("The "),zh=r("code"),c2=o("FlaxDistilBertPreTrainedModel"),p2=o(" forward method, overrides the "),jh=r("code"),h2=o("__call__"),f2=o(" special method."),u2=p(),v(Nn.$$.fragment),m2=p(),v(Rn.$$.fragment),cm=p(),ws=r("h2"),Wn=r("a"),Ch=r("span"),v(Tl.$$.fragment),g2=p(),Ph=r("span"),_2=o("FlaxDistilBertForMaskedLM"),pm=p(),ot=r("div"),v($l.$$.fragment),b2=p(),yl=r("p"),k2=o("DistilBert Model with a "),qh=r("code"),w2=o("language modeling"),v2=o(" head on top."),T2=p(),Dl=r("p"),$2=o("This model inherits from "),Wc=r("a"),y2=o("FlaxPreTrainedModel"),D2=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),F2=p(),Fl=r("p"),E2=o("This model is also a Flax Linen "),El=r("a"),B2=o("flax.linen.Module"),x2=o(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),M2=p(),Ah=r("p"),z2=o("Finally, this model supports inherent JAX features such as:"),j2=p(),no=r("ul"),Oh=r("li"),Bl=r("a"),C2=o("Just-In-Time (JIT) compilation"),P2=p(),Lh=r("li"),xl=r("a"),q2=o("Automatic Differentiation"),A2=p(),Ih=r("li"),Ml=r("a"),O2=o("Vectorization"),L2=p(),Sh=r("li"),zl=r("a"),I2=o("Parallelization"),S2=p(),Ut=r("div"),v(jl.$$.fragment),N2=p(),vs=r("p"),R2=o("The "),Nh=r("code"),W2=o("FlaxDistilBertPreTrainedModel"),Q2=o(" forward method, overrides the "),Rh=r("code"),U2=o("__call__"),H2=o(" special method."),K2=p(),v(Qn.$$.fragment),V2=p(),v(Un.$$.fragment),hm=p(),Ts=r("h2"),Hn=r("a"),Wh=r("span"),v(Cl.$$.fragment),J2=p(),Qh=r("span"),G2=o("FlaxDistilBertForSequenceClassification"),fm=p(),st=r("div"),v(Pl.$$.fragment),X2=p(),Uh=r("p"),Y2=o(`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),Z2=p(),ql=r("p"),eD=o("This model inherits from "),Qc=r("a"),tD=o("FlaxPreTrainedModel"),oD=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),sD=p(),Al=r("p"),nD=o("This model is also a Flax Linen "),Ol=r("a"),rD=o("flax.linen.Module"),aD=o(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),iD=p(),Hh=r("p"),lD=o("Finally, this model supports inherent JAX features such as:"),dD=p(),ro=r("ul"),Kh=r("li"),Ll=r("a"),cD=o("Just-In-Time (JIT) compilation"),pD=p(),Vh=r("li"),Il=r("a"),hD=o("Automatic Differentiation"),fD=p(),Jh=r("li"),Sl=r("a"),uD=o("Vectorization"),mD=p(),Gh=r("li"),Nl=r("a"),gD=o("Parallelization"),_D=p(),Ht=r("div"),v(Rl.$$.fragment),bD=p(),$s=r("p"),kD=o("The "),Xh=r("code"),wD=o("FlaxDistilBertPreTrainedModel"),vD=o(" forward method, overrides the "),Yh=r("code"),TD=o("__call__"),$D=o(" special method."),yD=p(),v(Kn.$$.fragment),DD=p(),v(Vn.$$.fragment),um=p(),ys=r("h2"),Jn=r("a"),Zh=r("span"),v(Wl.$$.fragment),FD=p(),ef=r("span"),ED=o("FlaxDistilBertForMultipleChoice"),mm=p(),nt=r("div"),v(Ql.$$.fragment),BD=p(),tf=r("p"),xD=o(`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),MD=p(),Ul=r("p"),zD=o("This model inherits from "),Uc=r("a"),jD=o("FlaxPreTrainedModel"),CD=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),PD=p(),Hl=r("p"),qD=o("This model is also a Flax Linen "),Kl=r("a"),AD=o("flax.linen.Module"),OD=o(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),LD=p(),of=r("p"),ID=o("Finally, this model supports inherent JAX features such as:"),SD=p(),ao=r("ul"),sf=r("li"),Vl=r("a"),ND=o("Just-In-Time (JIT) compilation"),RD=p(),nf=r("li"),Jl=r("a"),WD=o("Automatic Differentiation"),QD=p(),rf=r("li"),Gl=r("a"),UD=o("Vectorization"),HD=p(),af=r("li"),Xl=r("a"),KD=o("Parallelization"),VD=p(),Kt=r("div"),v(Yl.$$.fragment),JD=p(),Ds=r("p"),GD=o("The "),lf=r("code"),XD=o("FlaxDistilBertPreTrainedModel"),YD=o(" forward method, overrides the "),df=r("code"),ZD=o("__call__"),eF=o(" special method."),tF=p(),v(Gn.$$.fragment),oF=p(),v(Xn.$$.fragment),gm=p(),Fs=r("h2"),Yn=r("a"),cf=r("span"),v(Zl.$$.fragment),sF=p(),pf=r("span"),nF=o("FlaxDistilBertForTokenClassification"),_m=p(),rt=r("div"),v(ed.$$.fragment),rF=p(),hf=r("p"),aF=o(`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),iF=p(),td=r("p"),lF=o("This model inherits from "),Hc=r("a"),dF=o("FlaxPreTrainedModel"),cF=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),pF=p(),od=r("p"),hF=o("This model is also a Flax Linen "),sd=r("a"),fF=o("flax.linen.Module"),uF=o(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),mF=p(),ff=r("p"),gF=o("Finally, this model supports inherent JAX features such as:"),_F=p(),io=r("ul"),uf=r("li"),nd=r("a"),bF=o("Just-In-Time (JIT) compilation"),kF=p(),mf=r("li"),rd=r("a"),wF=o("Automatic Differentiation"),vF=p(),gf=r("li"),ad=r("a"),TF=o("Vectorization"),$F=p(),_f=r("li"),id=r("a"),yF=o("Parallelization"),DF=p(),Vt=r("div"),v(ld.$$.fragment),FF=p(),Es=r("p"),EF=o("The "),bf=r("code"),BF=o("FlaxDistilBertPreTrainedModel"),xF=o(" forward method, overrides the "),kf=r("code"),MF=o("__call__"),zF=o(" special method."),jF=p(),v(Zn.$$.fragment),CF=p(),v(er.$$.fragment),bm=p(),Bs=r("h2"),tr=r("a"),wf=r("span"),v(dd.$$.fragment),PF=p(),vf=r("span"),qF=o("FlaxDistilBertForQuestionAnswering"),km=p(),at=r("div"),v(cd.$$.fragment),AF=p(),xs=r("p"),OF=o(`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),Tf=r("code"),LF=o("span start logits"),IF=o(" and "),$f=r("code"),SF=o("span end logits"),NF=o(")."),RF=p(),pd=r("p"),WF=o("This model inherits from "),Kc=r("a"),QF=o("FlaxPreTrainedModel"),UF=o(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),HF=p(),hd=r("p"),KF=o("This model is also a Flax Linen "),fd=r("a"),VF=o("flax.linen.Module"),JF=o(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),GF=p(),yf=r("p"),XF=o("Finally, this model supports inherent JAX features such as:"),YF=p(),lo=r("ul"),Df=r("li"),ud=r("a"),ZF=o("Just-In-Time (JIT) compilation"),eE=p(),Ff=r("li"),md=r("a"),tE=o("Automatic Differentiation"),oE=p(),Ef=r("li"),gd=r("a"),sE=o("Vectorization"),nE=p(),Bf=r("li"),_d=r("a"),rE=o("Parallelization"),aE=p(),Jt=r("div"),v(bd.$$.fragment),iE=p(),Ms=r("p"),lE=o("The "),xf=r("code"),dE=o("FlaxDistilBertPreTrainedModel"),cE=o(" forward method, overrides the "),Mf=r("code"),pE=o("__call__"),hE=o(" special method."),fE=p(),v(or.$$.fragment),uE=p(),v(sr.$$.fragment),this.h()},l(n){const g=F4('[data-svelte="svelte-1phssyn"]',document.head);d=a(g,"META",{name:!0,content:!0}),g.forEach(t),b=h(n),f=a(n,"H1",{class:!0});var kd=i(f);m=a(kd,"A",{id:!0,class:!0,href:!0});var zf=i(m);k=a(zf,"SPAN",{});var jf=i(k);T(l.$$.fragment,jf),jf.forEach(t),zf.forEach(t),u=h(kd),M=a(kd,"SPAN",{});var Cf=i(M);ve=s(Cf,"DistilBERT"),Cf.forEach(t),kd.forEach(t),ge=h(n),I=a(n,"H2",{class:!0});var wd=i(I);re=a(wd,"A",{id:!0,class:!0,href:!0});var Pf=i(re);oe=a(Pf,"SPAN",{});var qf=i(oe);T(E.$$.fragment,qf),qf.forEach(t),Pf.forEach(t),Te=h(wd),Q=a(wd,"SPAN",{});var Af=i(Q);$e=s(Af,"Overview"),Af.forEach(t),wd.forEach(t),_e=h(n),O=a(n,"P",{});var co=i(O);ye=s(co,"The DistilBERT model was proposed in the blog post "),ae=a(co,"A",{href:!0,rel:!0});var Of=i(ae);H=s(Of,`Smaller, faster, cheaper, lighter: Introducing DistilBERT, a
distilled version of BERT`),Of.forEach(t),De=s(co,", and the paper "),ie=a(co,"A",{href:!0,rel:!0});var Lf=i(ie);K=s(Lf,`DistilBERT, a
distilled version of BERT: smaller, faster, cheaper and lighter`),Lf.forEach(t),Fe=s(co,`. DistilBERT is a
small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than
`),de=a(co,"EM",{});var If=i(de);V=s(If,"bert-base-uncased"),If.forEach(t),Ee=s(co,`, runs 60% faster while preserving over 95% of BERT\u2019s performances as measured on the GLUE language
understanding benchmark.`),co.forEach(t),be=h(n),ee=a(n,"P",{});var Sf=i(ee);j=s(Sf,"The abstract from the paper is the following:"),Sf.forEach(t),q=h(n),le=a(n,"P",{});var Nf=i(le);U=a(Nf,"EM",{});var Rf=i(U);Be=s(Rf,`As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP),
operating these large models in on-the-edge and/or under constrained computational training or inference budgets
remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation
model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger
counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage
knowledge distillation during the pretraining phase and show that it is possible to reduce the size of a BERT model by
40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive
biases learned by larger models during pretraining, we introduce a triple loss combining language modeling,
distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we
demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device
study.`),Rf.forEach(t),Nf.forEach(t),ke=h(n),W=a(n,"P",{});var Wf=i(W);xe=s(Wf,"Tips:"),Wf.forEach(t),we=h(n),C=a(n,"UL",{});var vd=i(C);se=a(vd,"LI",{});var po=i(se);J=s(po,"DistilBERT doesn\u2019t have "),ce=a(po,"CODE",{});var Qf=i(ce);Me=s(Qf,"token_type_ids"),Qf.forEach(t),G=s(po,`, you don\u2019t need to indicate which token belongs to which segment. Just
separate your segments with the separation token `),pe=a(po,"CODE",{});var Uf=i(pe);ze=s(Uf,"tokenizer.sep_token"),Uf.forEach(t),S=s(po," (or "),he=a(po,"CODE",{});var Hf=i(he);X=s(Hf,"[SEP]"),Hf.forEach(t),je=s(po,")."),po.forEach(t),ne=h(vd),P=a(vd,"LI",{});var Td=i(P);Ce=s(Td,"DistilBERT doesn\u2019t have options to select the input positions ("),A=a(Td,"CODE",{});var Kf=i(A);Pe=s(Kf,"position_ids"),Kf.forEach(t),qe=s(Td,` input). This could be added if
necessary though, just let us know if you need this option.`),Td.forEach(t),vd.forEach(t),w=h(n),x=a(n,"P",{});var ho=i(x);He=s(ho,"This model was contributed by "),Z=a(ho,"A",{href:!0,rel:!0});var Vf=i(Z);Ke=s(Vf,"victorsanh"),Vf.forEach(t),Ve=s(ho,`. This model jax version was
contributed by `),z=a(ho,"A",{href:!0,rel:!0});var Jf=i(z);Je=s(Jf,"kamalkraj"),Jf.forEach(t),Ge=s(ho,". The original code can be found "),Le=a(ho,"A",{href:!0,rel:!0});var Gf=i(Le);Xe=s(Gf,"here"),Gf.forEach(t),Ye=s(ho,"."),ho.forEach(t),L=h(n),N=a(n,"H2",{class:!0});var $d=i(N);Oe=a($d,"A",{id:!0,class:!0,href:!0});var Xf=i(Oe);Re=a(Xf,"SPAN",{});var Yf=i(Re);T(R.$$.fragment,Yf),Yf.forEach(t),Xf.forEach(t),Ze=h($d),We=a($d,"SPAN",{});var Zf=i(We);Ae=s(Zf,"Resources"),Zf.forEach(t),$d.forEach(t),Ue=h(n),te=a(n,"P",{});var eu=i(te);et=s(eu,"A list of official Hugging Face and community (indicated by \u{1F30E}) resources to help you get started with DistilBERT. If you\u2019re interested in submitting a resource to be included here, please feel free to open a Pull Request and we\u2019ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource."),eu.forEach(t),gu=h(n),T(Er.$$.fragment,n),_u=h(n),Ie=a(n,"UL",{});var Qe=i(Ie);Br=a(Qe,"LI",{});var yd=i(Br);Ng=s(yd,"A blog post on "),xr=a(yd,"A",{href:!0,rel:!0});var tu=i(xr);Rg=s(tu,"Getting Started with Sentiment Analysis using Python"),tu.forEach(t),Wg=s(yd," with DistilBERT."),yd.forEach(t),Qg=h(Qe),Mr=a(Qe,"LI",{});var Dd=i(Mr);Ug=s(Dd,"A blog post on how to "),zr=a(Dd,"A",{href:!0,rel:!0});var ou=i(zr);Hg=s(ou,"train DistilBERT with Blurr for sequence classification"),ou.forEach(t),Kg=s(Dd,"."),Dd.forEach(t),Vg=h(Qe),jr=a(Qe,"LI",{});var Fd=i(jr);Jg=s(Fd,"A blog post on how to use "),Cr=a(Fd,"A",{href:!0,rel:!0});var su=i(Cr);Gg=s(su,"Ray to tune DistilBERT hyperparameters"),su.forEach(t),Xg=s(Fd,"."),Fd.forEach(t),Yg=h(Qe),Pr=a(Qe,"LI",{});var Ed=i(Pr);Zg=s(Ed,"A blog post on how to "),qr=a(Ed,"A",{href:!0,rel:!0});var nu=i(qr);e_=s(nu,"train DistilBERT with Hugging Face and Amazon SageMaker"),nu.forEach(t),t_=s(Ed,"."),Ed.forEach(t),o_=h(Qe),Ar=a(Qe,"LI",{});var Bd=i(Ar);s_=s(Bd,"A notebook on how to "),Or=a(Bd,"A",{href:!0,rel:!0});var ru=i(Or);n_=s(ru,"finetune DistilBERT for multi-label classification"),ru.forEach(t),r_=s(Bd,". \u{1F30E}"),Bd.forEach(t),a_=h(Qe),Lr=a(Qe,"LI",{});var xd=i(Lr);i_=s(xd,"A notebook on how to "),Ir=a(xd,"A",{href:!0,rel:!0});var au=i(Ir);l_=s(au,"finetune DistilBERT for multiclass classification with PyTorch"),au.forEach(t),d_=s(xd,". \u{1F30E}"),xd.forEach(t),c_=h(Qe),Sr=a(Qe,"LI",{});var Md=i(Sr);p_=s(Md,"A notebook on how to "),Nr=a(Md,"A",{href:!0,rel:!0});var iu=i(Nr);h_=s(iu,"finetune DistilBERT for text classification in TensorFlow"),iu.forEach(t),f_=s(Md,". \u{1F30E}"),Md.forEach(t),u_=h(Qe),fo=a(Qe,"LI",{});var Eo=i(fo);Nd=a(Eo,"A",{href:!0});var lu=i(Nd);m_=s(lu,"DistilBertForSequenceClassification"),lu.forEach(t),g_=s(Eo," is supported by this "),Rr=a(Eo,"A",{href:!0,rel:!0});var du=i(Rr);__=s(du,"example script"),du.forEach(t),b_=s(Eo," and "),Wr=a(Eo,"A",{href:!0,rel:!0});var cu=i(Wr);k_=s(cu,"notebook"),cu.forEach(t),w_=s(Eo,"."),Eo.forEach(t),v_=h(Qe),uo=a(Qe,"LI",{});var Bo=i(uo);Rd=a(Bo,"A",{href:!0});var pu=i(Rd);T_=s(pu,"TFDistilBertForSequenceClassification"),pu.forEach(t),$_=s(Bo," is supported by this "),Qr=a(Bo,"A",{href:!0,rel:!0});var bE=i(Qr);y_=s(bE,"example script"),bE.forEach(t),D_=s(Bo," and "),Ur=a(Bo,"A",{href:!0,rel:!0});var kE=i(Ur);F_=s(kE,"notebook"),kE.forEach(t),E_=s(Bo,"."),Bo.forEach(t),B_=h(Qe),mo=a(Qe,"LI",{});var zd=i(mo);Wd=a(zd,"A",{href:!0});var wE=i(Wd);x_=s(wE,"FlaxDistilBertForSequenceClassification"),wE.forEach(t),M_=s(zd," is supported by this "),Hr=a(zd,"A",{href:!0,rel:!0});var vE=i(Hr);z_=s(vE,"example script"),vE.forEach(t),j_=s(zd," and "),Kr=a(zd,"A",{href:!0,rel:!0});var TE=i(Kr);C_=s(TE,"notebook"),TE.forEach(t),P_=s(zd,"."),zd.forEach(t),Qe.forEach(t),bu=h(n),T(Vr.$$.fragment,n),ku=h(n),Ot=a(n,"UL",{});var nr=i(Ot);go=a(nr,"LI",{});var jd=i(go);Qd=a(jd,"A",{href:!0});var $E=i(Qd);q_=s($E,"DistilBertForTokenClassification"),$E.forEach(t),A_=s(jd," is supported by this "),Jr=a(jd,"A",{href:!0,rel:!0});var yE=i(Jr);O_=s(yE,"example script"),yE.forEach(t),L_=s(jd," and "),Gr=a(jd,"A",{href:!0,rel:!0});var DE=i(Gr);I_=s(DE,"notebook"),DE.forEach(t),S_=s(jd,"."),jd.forEach(t),N_=h(nr),_o=a(nr,"LI",{});var Cd=i(_o);Ud=a(Cd,"A",{href:!0});var FE=i(Ud);R_=s(FE,"TFDistilBertForTokenClassification"),FE.forEach(t),W_=s(Cd," is supported by this "),Xr=a(Cd,"A",{href:!0,rel:!0});var EE=i(Xr);Q_=s(EE,"example script"),EE.forEach(t),U_=s(Cd," and "),Yr=a(Cd,"A",{href:!0,rel:!0});var BE=i(Yr);H_=s(BE,"notebook"),BE.forEach(t),K_=s(Cd,"."),Cd.forEach(t),V_=h(nr),zs=a(nr,"LI",{});var hu=i(zs);Hd=a(hu,"A",{href:!0});var xE=i(Hd);J_=s(xE,"FlaxDistilBertForTokenClassification"),xE.forEach(t),G_=s(hu," is supported by this "),Zr=a(hu,"A",{href:!0,rel:!0});var ME=i(Zr);X_=s(ME,"example script"),ME.forEach(t),Y_=s(hu,"."),hu.forEach(t),Z_=h(nr),Kd=a(nr,"LI",{});var mE=i(Kd);ea=a(mE,"A",{href:!0,rel:!0});var zE=i(ea);eb=s(zE,"Token classification"),zE.forEach(t),tb=s(mE," chapter of the \u{1F917} Hugging Face Course."),mE.forEach(t),nr.forEach(t),wu=h(n),T(ta.$$.fragment,n),vu=h(n),Lt=a(n,"UL",{});var rr=i(Lt);bo=a(rr,"LI",{});var Pd=i(bo);Vd=a(Pd,"A",{href:!0});var jE=i(Vd);ob=s(jE,"DistilBertForMaskedLM"),jE.forEach(t),sb=s(Pd," is supported by this "),oa=a(Pd,"A",{href:!0,rel:!0});var CE=i(oa);nb=s(CE,"example script"),CE.forEach(t),rb=s(Pd," and "),sa=a(Pd,"A",{href:!0,rel:!0});var PE=i(sa);ab=s(PE,"notebook"),PE.forEach(t),ib=s(Pd,"."),Pd.forEach(t),lb=h(rr),ko=a(rr,"LI",{});var qd=i(ko);Jd=a(qd,"A",{href:!0});var qE=i(Jd);db=s(qE,"TFDistilBertForMaskedLM"),qE.forEach(t),cb=s(qd," is supported by this "),na=a(qd,"A",{href:!0,rel:!0});var AE=i(na);pb=s(AE,"example script"),AE.forEach(t),hb=s(qd," and "),ra=a(qd,"A",{href:!0,rel:!0});var OE=i(ra);fb=s(OE,"notebook"),OE.forEach(t),ub=s(qd,"."),qd.forEach(t),mb=h(rr),wo=a(rr,"LI",{});var Ad=i(wo);Gd=a(Ad,"A",{href:!0});var LE=i(Gd);gb=s(LE,"FlaxDistilBertForMaskedLM"),LE.forEach(t),_b=s(Ad," is supported by this "),aa=a(Ad,"A",{href:!0,rel:!0});var IE=i(aa);bb=s(IE,"example script"),IE.forEach(t),kb=s(Ad," and "),ia=a(Ad,"A",{href:!0,rel:!0});var SE=i(ia);wb=s(SE,"notebook"),SE.forEach(t),vb=s(Ad,"."),Ad.forEach(t),Tb=h(rr),Xd=a(rr,"LI",{});var gE=i(Xd);la=a(gE,"A",{href:!0,rel:!0});var NE=i(la);$b=s(NE,"Masked language modeling"),NE.forEach(t),yb=s(gE," chapter of the \u{1F917} Hugging Face Course."),gE.forEach(t),rr.forEach(t),Tu=h(n),T(da.$$.fragment,n),$u=h(n),It=a(n,"UL",{});var ar=i(It);vo=a(ar,"LI",{});var Od=i(vo);Yd=a(Od,"A",{href:!0});var RE=i(Yd);Db=s(RE,"DistilBertForQuestionAnswering"),RE.forEach(t),Fb=s(Od," is supported by this "),ca=a(Od,"A",{href:!0,rel:!0});var WE=i(ca);Eb=s(WE,"example script"),WE.forEach(t),Bb=s(Od," and "),pa=a(Od,"A",{href:!0,rel:!0});var QE=i(pa);xb=s(QE,"notebook"),QE.forEach(t),Mb=s(Od,"."),Od.forEach(t),zb=h(ar),To=a(ar,"LI",{});var Ld=i(To);Zd=a(Ld,"A",{href:!0});var UE=i(Zd);jb=s(UE,"TFDistilBertForQuestionAnswering"),UE.forEach(t),Cb=s(Ld," is supported by this "),ha=a(Ld,"A",{href:!0,rel:!0});var HE=i(ha);Pb=s(HE,"example script"),HE.forEach(t),qb=s(Ld," and "),fa=a(Ld,"A",{href:!0,rel:!0});var KE=i(fa);Ab=s(KE,"notebook"),KE.forEach(t),Ob=s(Ld,"."),Ld.forEach(t),Lb=h(ar),js=a(ar,"LI",{});var fu=i(js);ec=a(fu,"A",{href:!0});var VE=i(ec);Ib=s(VE,"FlaxDistilBertForQuestionAnswering"),VE.forEach(t),Sb=s(fu," is supported by this "),ua=a(fu,"A",{href:!0,rel:!0});var JE=i(ua);Nb=s(JE,"example script"),JE.forEach(t),Rb=s(fu,"."),fu.forEach(t),Wb=h(ar),tc=a(ar,"LI",{});var _E=i(tc);ma=a(_E,"A",{href:!0,rel:!0});var GE=i(ma);Qb=s(GE,"Question answering"),GE.forEach(t),Ub=s(_E," chapter of the \u{1F917} Hugging Face Course."),_E.forEach(t),ar.forEach(t),yu=h(n),oc=a(n,"P",{});var XE=i(oc);bp=a(XE,"STRONG",{});var YE=i(bp);Hb=s(YE,"Multiple choice"),YE.forEach(t),XE.forEach(t),Du=h(n),Cs=a(n,"UL",{});var vm=i(Cs);$o=a(vm,"LI",{});var Id=i($o);sc=a(Id,"A",{href:!0});var ZE=i(sc);Kb=s(ZE,"DistilBertForMultipleChoice"),ZE.forEach(t),Vb=s(Id," is supported by this "),ga=a(Id,"A",{href:!0,rel:!0});var e0=i(ga);Jb=s(e0,"example script"),e0.forEach(t),Gb=s(Id," and "),_a=a(Id,"A",{href:!0,rel:!0});var t0=i(_a);Xb=s(t0,"notebook"),t0.forEach(t),Yb=s(Id,"."),Id.forEach(t),Zb=h(vm),yo=a(vm,"LI",{});var Sd=i(yo);nc=a(Sd,"A",{href:!0});var o0=i(nc);ek=s(o0,"TFDistilBertForMultipleChoice"),o0.forEach(t),tk=s(Sd," is supported by this "),ba=a(Sd,"A",{href:!0,rel:!0});var s0=i(ba);ok=s(s0,"example script"),s0.forEach(t),sk=s(Sd," and "),ka=a(Sd,"A",{href:!0,rel:!0});var n0=i(ka);nk=s(n0,"notebook"),n0.forEach(t),rk=s(Sd,"."),Sd.forEach(t),vm.forEach(t),Fu=h(n),rc=a(n,"P",{});var r0=i(rc);ak=s(r0,"\u2697\uFE0F Optimization"),r0.forEach(t),Eu=h(n),Do=a(n,"UL",{});var Vc=i(Do);wa=a(Vc,"LI",{});var Tm=i(wa);ik=s(Tm,"A blog post on how to "),va=a(Tm,"A",{href:!0,rel:!0});var a0=i(va);lk=s(a0,"quantize DistilBERT with \u{1F917} Optimum and Intel"),a0.forEach(t),dk=s(Tm,"."),Tm.forEach(t),ck=h(Vc),Ta=a(Vc,"LI",{});var $m=i(Ta);pk=s($m,"A blog post on how "),$a=a($m,"A",{href:!0,rel:!0});var i0=i($a);hk=s(i0,"Optimizing Transformers for GPUs with \u{1F917} Optimum"),i0.forEach(t),fk=s($m,"."),$m.forEach(t),uk=h(Vc),ya=a(Vc,"LI",{});var ym=i(ya);mk=s(ym,"A blog post on "),Da=a(ym,"A",{href:!0,rel:!0});var l0=i(Da);gk=s(l0,"Optimizing Transformers with Hugging Face Optimum"),l0.forEach(t),_k=s(ym,"."),ym.forEach(t),Vc.forEach(t),Bu=h(n),ac=a(n,"P",{});var d0=i(ac);bk=s(d0,"\u26A1\uFE0F Inference"),d0.forEach(t),xu=h(n),Ps=a(n,"UL",{});var Dm=i(Ps);Fa=a(Dm,"LI",{});var Fm=i(Fa);kk=s(Fm,"A blog post on how to "),Ea=a(Fm,"A",{href:!0,rel:!0});var c0=i(Ea);wk=s(c0,"Accelerate BERT inference with Hugging Face Transformers and AWS Inferentia"),c0.forEach(t),vk=s(Fm," with DistilBERT."),Fm.forEach(t),Tk=h(Dm),Ba=a(Dm,"LI",{});var Em=i(Ba);$k=s(Em,"A blog post on "),xa=a(Em,"A",{href:!0,rel:!0});var p0=i(xa);yk=s(p0,"Serverless Inference with Hugging Face\u2019s Transformers, DistilBERT and Amazon SageMaker"),p0.forEach(t),Dk=s(Em,"."),Em.forEach(t),Dm.forEach(t),Mu=h(n),ic=a(n,"P",{});var h0=i(ic);Fk=s(h0,"\u{1F680} Deploy"),h0.forEach(t),zu=h(n),Fo=a(n,"UL",{});var Jc=i(Fo);Ma=a(Jc,"LI",{});var Bm=i(Ma);Ek=s(Bm,"A blog post on how to "),za=a(Bm,"A",{href:!0,rel:!0});var f0=i(za);Bk=s(f0,"deploy DistilBERT on Google Cloud"),f0.forEach(t),xk=s(Bm,"."),Bm.forEach(t),Mk=h(Jc),ja=a(Jc,"LI",{});var xm=i(ja);zk=s(xm,"A blog post on how to "),Ca=a(xm,"A",{href:!0,rel:!0});var u0=i(Ca);jk=s(u0,"deploy DistilBERT with Amazon SageMaker"),u0.forEach(t),Ck=s(xm,"."),xm.forEach(t),Pk=h(Jc),Pa=a(Jc,"LI",{});var Mm=i(Pa);qk=s(Mm,"A blog post on how to "),qa=a(Mm,"A",{href:!0,rel:!0});var m0=i(qa);Ak=s(m0,"Deploy BERT with Hugging Face Transformers, Amazon SageMaker and Terraform module"),m0.forEach(t),Ok=s(Mm,"."),Mm.forEach(t),Jc.forEach(t),ju=h(n),Ro=a(n,"H2",{class:!0});var zm=i(Ro);qs=a(zm,"A",{id:!0,class:!0,href:!0});var g0=i(qs);kp=a(g0,"SPAN",{});var _0=i(kp);T(Aa.$$.fragment,_0),_0.forEach(t),g0.forEach(t),Lk=h(zm),wp=a(zm,"SPAN",{});var b0=i(wp);Ik=s(b0,"DistilBertConfig"),b0.forEach(t),zm.forEach(t),Cu=h(n),Pt=a(n,"DIV",{class:!0});var ir=i(Pt);T(Oa.$$.fragment,ir),Sk=h(ir),oo=a(ir,"P",{});var lr=i(oo);Nk=s(lr,"This is the configuration class to store the configuration of a "),lc=a(lr,"A",{href:!0});var k0=i(lc);Rk=s(k0,"DistilBertModel"),k0.forEach(t),Wk=s(lr," or a "),dc=a(lr,"A",{href:!0});var w0=i(dc);Qk=s(w0,"TFDistilBertModel"),w0.forEach(t),Uk=s(lr,`. It
is used to instantiate a DistilBERT model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the DistilBERT
`),La=a(lr,"A",{href:!0,rel:!0});var v0=i(La);Hk=s(v0,"distilbert-base-uncased"),v0.forEach(t),Kk=s(lr," architecture."),lr.forEach(t),Vk=h(ir),Wo=a(ir,"P",{});var Gc=i(Wo);Jk=s(Gc,"Configuration objects inherit from "),cc=a(Gc,"A",{href:!0});var T0=i(cc);Gk=s(T0,"PretrainedConfig"),T0.forEach(t),Xk=s(Gc,` and can be used to control the model outputs. Read the
documentation from `),pc=a(Gc,"A",{href:!0});var $0=i(pc);Yk=s($0,"PretrainedConfig"),$0.forEach(t),Zk=s(Gc," for more information."),Gc.forEach(t),ew=h(ir),T(As.$$.fragment,ir),ir.forEach(t),Pu=h(n),Qo=a(n,"H2",{class:!0});var jm=i(Qo);Os=a(jm,"A",{id:!0,class:!0,href:!0});var y0=i(Os);vp=a(y0,"SPAN",{});var D0=i(vp);T(Ia.$$.fragment,D0),D0.forEach(t),y0.forEach(t),tw=h(jm),Tp=a(jm,"SPAN",{});var F0=i(Tp);ow=s(F0,"DistilBertTokenizer"),F0.forEach(t),jm.forEach(t),qu=h(n),qt=a(n,"DIV",{class:!0});var dr=i(qt);T(Sa.$$.fragment,dr),sw=h(dr),$p=a(dr,"P",{});var E0=i($p);nw=s(E0,"Construct a DistilBERT tokenizer."),E0.forEach(t),rw=h(dr),Ls=a(dr,"P",{});var uu=i(Ls);hc=a(uu,"A",{href:!0});var B0=i(hc);aw=s(B0,"DistilBertTokenizer"),B0.forEach(t),iw=s(uu," is identical to "),fc=a(uu,"A",{href:!0});var x0=i(fc);lw=s(x0,"BertTokenizer"),x0.forEach(t),dw=s(uu,` and runs end-to-end tokenization: punctuation splitting
and wordpiece.`),uu.forEach(t),cw=h(dr),Na=a(dr,"P",{});var Cm=i(Na);pw=s(Cm,"Refer to superclass "),uc=a(Cm,"A",{href:!0});var M0=i(uc);hw=s(M0,"BertTokenizer"),M0.forEach(t),fw=s(Cm," for usage examples and documentation concerning parameters."),Cm.forEach(t),dr.forEach(t),Au=h(n),Uo=a(n,"H2",{class:!0});var Pm=i(Uo);Is=a(Pm,"A",{id:!0,class:!0,href:!0});var z0=i(Is);yp=a(z0,"SPAN",{});var j0=i(yp);T(Ra.$$.fragment,j0),j0.forEach(t),z0.forEach(t),uw=h(Pm),Dp=a(Pm,"SPAN",{});var C0=i(Dp);mw=s(C0,"DistilBertTokenizerFast"),C0.forEach(t),Pm.forEach(t),Ou=h(n),At=a(n,"DIV",{class:!0});var cr=i(At);T(Wa.$$.fragment,cr),gw=h(cr),Qa=a(cr,"P",{});var qm=i(Qa);_w=s(qm,"Construct a \u201Cfast\u201D DistilBERT tokenizer (backed by HuggingFace\u2019s "),Fp=a(qm,"EM",{});var P0=i(Fp);bw=s(P0,"tokenizers"),P0.forEach(t),kw=s(qm," library)."),qm.forEach(t),ww=h(cr),Ss=a(cr,"P",{});var mu=i(Ss);mc=a(mu,"A",{href:!0});var q0=i(mc);vw=s(q0,"DistilBertTokenizerFast"),q0.forEach(t),Tw=s(mu," is identical to "),gc=a(mu,"A",{href:!0});var A0=i(gc);$w=s(A0,"BertTokenizerFast"),A0.forEach(t),yw=s(mu,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),mu.forEach(t),Dw=h(cr),Ua=a(cr,"P",{});var Am=i(Ua);Fw=s(Am,"Refer to superclass "),_c=a(Am,"A",{href:!0});var O0=i(_c);Ew=s(O0,"BertTokenizerFast"),O0.forEach(t),Bw=s(Am," for usage examples and documentation concerning parameters."),Am.forEach(t),cr.forEach(t),Lu=h(n),Ho=a(n,"H2",{class:!0});var Om=i(Ho);Ns=a(Om,"A",{id:!0,class:!0,href:!0});var L0=i(Ns);Ep=a(L0,"SPAN",{});var I0=i(Ep);T(Ha.$$.fragment,I0),I0.forEach(t),L0.forEach(t),xw=h(Om),Bp=a(Om,"SPAN",{});var S0=i(Bp);Mw=s(S0,"DistilBertModel"),S0.forEach(t),Om.forEach(t),Iu=h(n),ut=a(n,"DIV",{class:!0});var xo=i(ut);T(Ka.$$.fragment,xo),zw=h(xo),xp=a(xo,"P",{});var N0=i(xp);jw=s(N0,"The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),N0.forEach(t),Cw=h(xo),Va=a(xo,"P",{});var Lm=i(Va);Pw=s(Lm,"This model inherits from "),bc=a(Lm,"A",{href:!0});var R0=i(bc);qw=s(R0,"PreTrainedModel"),R0.forEach(t),Aw=s(Lm,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Lm.forEach(t),Ow=h(xo),Ja=a(xo,"P",{});var Im=i(Ja);Lw=s(Im,"This model is also a PyTorch "),Ga=a(Im,"A",{href:!0,rel:!0});var W0=i(Ga);Iw=s(W0,"torch.nn.Module"),W0.forEach(t),Sw=s(Im,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Im.forEach(t),Nw=h(xo),St=a(xo,"DIV",{class:!0});var pr=i(St);T(Xa.$$.fragment,pr),Rw=h(pr),Ko=a(pr,"P",{});var Xc=i(Ko);Ww=s(Xc,"The "),kc=a(Xc,"A",{href:!0});var Q0=i(kc);Qw=s(Q0,"DistilBertModel"),Q0.forEach(t),Uw=s(Xc," forward method, overrides the "),Mp=a(Xc,"CODE",{});var U0=i(Mp);Hw=s(U0,"__call__"),U0.forEach(t),Kw=s(Xc," special method."),Xc.forEach(t),Vw=h(pr),T(Rs.$$.fragment,pr),Jw=h(pr),T(Ws.$$.fragment,pr),pr.forEach(t),xo.forEach(t),Su=h(n),Vo=a(n,"H2",{class:!0});var Sm=i(Vo);Qs=a(Sm,"A",{id:!0,class:!0,href:!0});var H0=i(Qs);zp=a(H0,"SPAN",{});var K0=i(zp);T(Ya.$$.fragment,K0),K0.forEach(t),H0.forEach(t),Gw=h(Sm),jp=a(Sm,"SPAN",{});var V0=i(jp);Xw=s(V0,"DistilBertForMaskedLM"),V0.forEach(t),Sm.forEach(t),Nu=h(n),mt=a(n,"DIV",{class:!0});var Mo=i(mt);T(Za.$$.fragment,Mo),Yw=h(Mo),ei=a(Mo,"P",{});var Nm=i(ei);Zw=s(Nm,"DistilBert Model with a "),Cp=a(Nm,"CODE",{});var J0=i(Cp);ev=s(J0,"masked language modeling"),J0.forEach(t),tv=s(Nm," head on top."),Nm.forEach(t),ov=h(Mo),ti=a(Mo,"P",{});var Rm=i(ti);sv=s(Rm,"This model inherits from "),wc=a(Rm,"A",{href:!0});var G0=i(wc);nv=s(G0,"PreTrainedModel"),G0.forEach(t),rv=s(Rm,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Rm.forEach(t),av=h(Mo),oi=a(Mo,"P",{});var Wm=i(oi);iv=s(Wm,"This model is also a PyTorch "),si=a(Wm,"A",{href:!0,rel:!0});var X0=i(si);lv=s(X0,"torch.nn.Module"),X0.forEach(t),dv=s(Wm,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Wm.forEach(t),cv=h(Mo),wt=a(Mo,"DIV",{class:!0});var zo=i(wt);T(ni.$$.fragment,zo),pv=h(zo),Jo=a(zo,"P",{});var Yc=i(Jo);hv=s(Yc,"The "),vc=a(Yc,"A",{href:!0});var Y0=i(vc);fv=s(Y0,"DistilBertForMaskedLM"),Y0.forEach(t),uv=s(Yc," forward method, overrides the "),Pp=a(Yc,"CODE",{});var Z0=i(Pp);mv=s(Z0,"__call__"),Z0.forEach(t),gv=s(Yc," special method."),Yc.forEach(t),_v=h(zo),T(Us.$$.fragment,zo),bv=h(zo),T(Hs.$$.fragment,zo),kv=h(zo),T(Ks.$$.fragment,zo),zo.forEach(t),Mo.forEach(t),Ru=h(n),Go=a(n,"H2",{class:!0});var Qm=i(Go);Vs=a(Qm,"A",{id:!0,class:!0,href:!0});var eB=i(Vs);qp=a(eB,"SPAN",{});var tB=i(qp);T(ri.$$.fragment,tB),tB.forEach(t),eB.forEach(t),wv=h(Qm),Ap=a(Qm,"SPAN",{});var oB=i(Ap);vv=s(oB,"DistilBertForSequenceClassification"),oB.forEach(t),Qm.forEach(t),Wu=h(n),gt=a(n,"DIV",{class:!0});var jo=i(gt);T(ai.$$.fragment,jo),Tv=h(jo),Op=a(jo,"P",{});var sB=i(Op);$v=s(sB,`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),sB.forEach(t),yv=h(jo),ii=a(jo,"P",{});var Um=i(ii);Dv=s(Um,"This model inherits from "),Tc=a(Um,"A",{href:!0});var nB=i(Tc);Fv=s(nB,"PreTrainedModel"),nB.forEach(t),Ev=s(Um,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Um.forEach(t),Bv=h(jo),li=a(jo,"P",{});var Hm=i(li);xv=s(Hm,"This model is also a PyTorch "),di=a(Hm,"A",{href:!0,rel:!0});var rB=i(di);Mv=s(rB,"torch.nn.Module"),rB.forEach(t),zv=s(Hm,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Hm.forEach(t),jv=h(jo),it=a(jo,"DIV",{class:!0});var Et=i(it);T(ci.$$.fragment,Et),Cv=h(Et),Xo=a(Et,"P",{});var Zc=i(Xo);Pv=s(Zc,"The "),$c=a(Zc,"A",{href:!0});var aB=i($c);qv=s(aB,"DistilBertForSequenceClassification"),aB.forEach(t),Av=s(Zc," forward method, overrides the "),Lp=a(Zc,"CODE",{});var iB=i(Lp);Ov=s(iB,"__call__"),iB.forEach(t),Lv=s(Zc," special method."),Zc.forEach(t),Iv=h(Et),T(Js.$$.fragment,Et),Sv=h(Et),T(Gs.$$.fragment,Et),Nv=h(Et),T(Xs.$$.fragment,Et),Rv=h(Et),T(Ys.$$.fragment,Et),Wv=h(Et),T(Zs.$$.fragment,Et),Et.forEach(t),jo.forEach(t),Qu=h(n),Yo=a(n,"H2",{class:!0});var Km=i(Yo);en=a(Km,"A",{id:!0,class:!0,href:!0});var lB=i(en);Ip=a(lB,"SPAN",{});var dB=i(Ip);T(pi.$$.fragment,dB),dB.forEach(t),lB.forEach(t),Qv=h(Km),Sp=a(Km,"SPAN",{});var cB=i(Sp);Uv=s(cB,"DistilBertForMultipleChoice"),cB.forEach(t),Km.forEach(t),Uu=h(n),_t=a(n,"DIV",{class:!0});var Co=i(_t);T(hi.$$.fragment,Co),Hv=h(Co),Np=a(Co,"P",{});var pB=i(Np);Kv=s(pB,`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),pB.forEach(t),Vv=h(Co),fi=a(Co,"P",{});var Vm=i(fi);Jv=s(Vm,"This model inherits from "),yc=a(Vm,"A",{href:!0});var hB=i(yc);Gv=s(hB,"PreTrainedModel"),hB.forEach(t),Xv=s(Vm,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Vm.forEach(t),Yv=h(Co),ui=a(Co,"P",{});var Jm=i(ui);Zv=s(Jm,"This model is also a PyTorch "),mi=a(Jm,"A",{href:!0,rel:!0});var fB=i(mi);eT=s(fB,"torch.nn.Module"),fB.forEach(t),tT=s(Jm,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Jm.forEach(t),oT=h(Co),Nt=a(Co,"DIV",{class:!0});var hr=i(Nt);T(gi.$$.fragment,hr),sT=h(hr),Zo=a(hr,"P",{});var ep=i(Zo);nT=s(ep,"The "),Dc=a(ep,"A",{href:!0});var uB=i(Dc);rT=s(uB,"DistilBertForMultipleChoice"),uB.forEach(t),aT=s(ep," forward method, overrides the "),Rp=a(ep,"CODE",{});var mB=i(Rp);iT=s(mB,"__call__"),mB.forEach(t),lT=s(ep," special method."),ep.forEach(t),dT=h(hr),T(tn.$$.fragment,hr),cT=h(hr),T(on.$$.fragment,hr),hr.forEach(t),Co.forEach(t),Hu=h(n),es=a(n,"H2",{class:!0});var Gm=i(es);sn=a(Gm,"A",{id:!0,class:!0,href:!0});var gB=i(sn);Wp=a(gB,"SPAN",{});var _B=i(Wp);T(_i.$$.fragment,_B),_B.forEach(t),gB.forEach(t),pT=h(Gm),Qp=a(Gm,"SPAN",{});var bB=i(Qp);hT=s(bB,"DistilBertForTokenClassification"),bB.forEach(t),Gm.forEach(t),Ku=h(n),bt=a(n,"DIV",{class:!0});var Po=i(bt);T(bi.$$.fragment,Po),fT=h(Po),Up=a(Po,"P",{});var kB=i(Up);uT=s(kB,`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),kB.forEach(t),mT=h(Po),ki=a(Po,"P",{});var Xm=i(ki);gT=s(Xm,"This model inherits from "),Fc=a(Xm,"A",{href:!0});var wB=i(Fc);_T=s(wB,"PreTrainedModel"),wB.forEach(t),bT=s(Xm,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Xm.forEach(t),kT=h(Po),wi=a(Po,"P",{});var Ym=i(wi);wT=s(Ym,"This model is also a PyTorch "),vi=a(Ym,"A",{href:!0,rel:!0});var vB=i(vi);vT=s(vB,"torch.nn.Module"),vB.forEach(t),TT=s(Ym,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ym.forEach(t),$T=h(Po),vt=a(Po,"DIV",{class:!0});var qo=i(vt);T(Ti.$$.fragment,qo),yT=h(qo),ts=a(qo,"P",{});var tp=i(ts);DT=s(tp,"The "),Ec=a(tp,"A",{href:!0});var TB=i(Ec);FT=s(TB,"DistilBertForTokenClassification"),TB.forEach(t),ET=s(tp," forward method, overrides the "),Hp=a(tp,"CODE",{});var $B=i(Hp);BT=s($B,"__call__"),$B.forEach(t),xT=s(tp," special method."),tp.forEach(t),MT=h(qo),T(nn.$$.fragment,qo),zT=h(qo),T(rn.$$.fragment,qo),jT=h(qo),T(an.$$.fragment,qo),qo.forEach(t),Po.forEach(t),Vu=h(n),os=a(n,"H2",{class:!0});var Zm=i(os);ln=a(Zm,"A",{id:!0,class:!0,href:!0});var yB=i(ln);Kp=a(yB,"SPAN",{});var DB=i(Kp);T($i.$$.fragment,DB),DB.forEach(t),yB.forEach(t),CT=h(Zm),Vp=a(Zm,"SPAN",{});var FB=i(Vp);PT=s(FB,"DistilBertForQuestionAnswering"),FB.forEach(t),Zm.forEach(t),Ju=h(n),kt=a(n,"DIV",{class:!0});var Ao=i(kt);T(yi.$$.fragment,Ao),qT=h(Ao),ss=a(Ao,"P",{});var op=i(ss);AT=s(op,`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),Jp=a(op,"CODE",{});var EB=i(Jp);OT=s(EB,"span start logits"),EB.forEach(t),LT=s(op," and "),Gp=a(op,"CODE",{});var BB=i(Gp);IT=s(BB,"span end logits"),BB.forEach(t),ST=s(op,")."),op.forEach(t),NT=h(Ao),Di=a(Ao,"P",{});var eg=i(Di);RT=s(eg,"This model inherits from "),Bc=a(eg,"A",{href:!0});var xB=i(Bc);WT=s(xB,"PreTrainedModel"),xB.forEach(t),QT=s(eg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),eg.forEach(t),UT=h(Ao),Fi=a(Ao,"P",{});var tg=i(Fi);HT=s(tg,"This model is also a PyTorch "),Ei=a(tg,"A",{href:!0,rel:!0});var MB=i(Ei);KT=s(MB,"torch.nn.Module"),MB.forEach(t),VT=s(tg,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),tg.forEach(t),JT=h(Ao),Tt=a(Ao,"DIV",{class:!0});var Oo=i(Tt);T(Bi.$$.fragment,Oo),GT=h(Oo),ns=a(Oo,"P",{});var sp=i(ns);XT=s(sp,"The "),xc=a(sp,"A",{href:!0});var zB=i(xc);YT=s(zB,"DistilBertForQuestionAnswering"),zB.forEach(t),ZT=s(sp," forward method, overrides the "),Xp=a(sp,"CODE",{});var jB=i(Xp);e$=s(jB,"__call__"),jB.forEach(t),t$=s(sp," special method."),sp.forEach(t),o$=h(Oo),T(dn.$$.fragment,Oo),s$=h(Oo),T(cn.$$.fragment,Oo),n$=h(Oo),T(pn.$$.fragment,Oo),Oo.forEach(t),Ao.forEach(t),Gu=h(n),rs=a(n,"H2",{class:!0});var og=i(rs);hn=a(og,"A",{id:!0,class:!0,href:!0});var CB=i(hn);Yp=a(CB,"SPAN",{});var PB=i(Yp);T(xi.$$.fragment,PB),PB.forEach(t),CB.forEach(t),r$=h(og),Zp=a(og,"SPAN",{});var qB=i(Zp);a$=s(qB,"TFDistilBertModel"),qB.forEach(t),og.forEach(t),Xu=h(n),lt=a(n,"DIV",{class:!0});var Gt=i(lt);T(Mi.$$.fragment,Gt),i$=h(Gt),eh=a(Gt,"P",{});var AB=i(eh);l$=s(AB,"The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),AB.forEach(t),d$=h(Gt),zi=a(Gt,"P",{});var sg=i(zi);c$=s(sg,"This model inherits from "),Mc=a(sg,"A",{href:!0});var OB=i(Mc);p$=s(OB,"TFPreTrainedModel"),OB.forEach(t),h$=s(sg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),sg.forEach(t),f$=h(Gt),ji=a(Gt,"P",{});var ng=i(ji);u$=s(ng,"This model is also a "),Ci=a(ng,"A",{href:!0,rel:!0});var LB=i(Ci);m$=s(LB,"tf.keras.Model"),LB.forEach(t),g$=s(ng,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),ng.forEach(t),_$=h(Gt),T(fn.$$.fragment,Gt),b$=h(Gt),Rt=a(Gt,"DIV",{class:!0});var fr=i(Rt);T(Pi.$$.fragment,fr),k$=h(fr),as=a(fr,"P",{});var np=i(as);w$=s(np,"The "),zc=a(np,"A",{href:!0});var IB=i(zc);v$=s(IB,"TFDistilBertModel"),IB.forEach(t),T$=s(np," forward method, overrides the "),th=a(np,"CODE",{});var SB=i(th);$$=s(SB,"__call__"),SB.forEach(t),y$=s(np," special method."),np.forEach(t),D$=h(fr),T(un.$$.fragment,fr),F$=h(fr),T(mn.$$.fragment,fr),fr.forEach(t),Gt.forEach(t),Yu=h(n),is=a(n,"H2",{class:!0});var rg=i(is);gn=a(rg,"A",{id:!0,class:!0,href:!0});var NB=i(gn);oh=a(NB,"SPAN",{});var RB=i(oh);T(qi.$$.fragment,RB),RB.forEach(t),NB.forEach(t),E$=h(rg),sh=a(rg,"SPAN",{});var WB=i(sh);B$=s(WB,"TFDistilBertForMaskedLM"),WB.forEach(t),rg.forEach(t),Zu=h(n),dt=a(n,"DIV",{class:!0});var Xt=i(dt);T(Ai.$$.fragment,Xt),x$=h(Xt),Oi=a(Xt,"P",{});var ag=i(Oi);M$=s(ag,"DistilBert Model with a "),nh=a(ag,"CODE",{});var QB=i(nh);z$=s(QB,"masked language modeling"),QB.forEach(t),j$=s(ag," head on top."),ag.forEach(t),C$=h(Xt),Li=a(Xt,"P",{});var ig=i(Li);P$=s(ig,"This model inherits from "),jc=a(ig,"A",{href:!0});var UB=i(jc);q$=s(UB,"TFPreTrainedModel"),UB.forEach(t),A$=s(ig,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ig.forEach(t),O$=h(Xt),Ii=a(Xt,"P",{});var lg=i(Ii);L$=s(lg,"This model is also a "),Si=a(lg,"A",{href:!0,rel:!0});var HB=i(Si);I$=s(HB,"tf.keras.Model"),HB.forEach(t),S$=s(lg,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),lg.forEach(t),N$=h(Xt),T(_n.$$.fragment,Xt),R$=h(Xt),$t=a(Xt,"DIV",{class:!0});var Lo=i($t);T(Ni.$$.fragment,Lo),W$=h(Lo),ls=a(Lo,"P",{});var rp=i(ls);Q$=s(rp,"The "),Cc=a(rp,"A",{href:!0});var KB=i(Cc);U$=s(KB,"TFDistilBertForMaskedLM"),KB.forEach(t),H$=s(rp," forward method, overrides the "),rh=a(rp,"CODE",{});var VB=i(rh);K$=s(VB,"__call__"),VB.forEach(t),V$=s(rp," special method."),rp.forEach(t),J$=h(Lo),T(bn.$$.fragment,Lo),G$=h(Lo),T(kn.$$.fragment,Lo),X$=h(Lo),T(wn.$$.fragment,Lo),Lo.forEach(t),Xt.forEach(t),em=h(n),ds=a(n,"H2",{class:!0});var dg=i(ds);vn=a(dg,"A",{id:!0,class:!0,href:!0});var JB=i(vn);ah=a(JB,"SPAN",{});var GB=i(ah);T(Ri.$$.fragment,GB),GB.forEach(t),JB.forEach(t),Y$=h(dg),ih=a(dg,"SPAN",{});var XB=i(ih);Z$=s(XB,"TFDistilBertForSequenceClassification"),XB.forEach(t),dg.forEach(t),tm=h(n),ct=a(n,"DIV",{class:!0});var Yt=i(ct);T(Wi.$$.fragment,Yt),ey=h(Yt),lh=a(Yt,"P",{});var YB=i(lh);ty=s(YB,`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),YB.forEach(t),oy=h(Yt),Qi=a(Yt,"P",{});var cg=i(Qi);sy=s(cg,"This model inherits from "),Pc=a(cg,"A",{href:!0});var ZB=i(Pc);ny=s(ZB,"TFPreTrainedModel"),ZB.forEach(t),ry=s(cg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),cg.forEach(t),ay=h(Yt),Ui=a(Yt,"P",{});var pg=i(Ui);iy=s(pg,"This model is also a "),Hi=a(pg,"A",{href:!0,rel:!0});var ex=i(Hi);ly=s(ex,"tf.keras.Model"),ex.forEach(t),dy=s(pg,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),pg.forEach(t),cy=h(Yt),T(Tn.$$.fragment,Yt),py=h(Yt),yt=a(Yt,"DIV",{class:!0});var Io=i(yt);T(Ki.$$.fragment,Io),hy=h(Io),cs=a(Io,"P",{});var ap=i(cs);fy=s(ap,"The "),qc=a(ap,"A",{href:!0});var tx=i(qc);uy=s(tx,"TFDistilBertForSequenceClassification"),tx.forEach(t),my=s(ap," forward method, overrides the "),dh=a(ap,"CODE",{});var ox=i(dh);gy=s(ox,"__call__"),ox.forEach(t),_y=s(ap," special method."),ap.forEach(t),by=h(Io),T($n.$$.fragment,Io),ky=h(Io),T(yn.$$.fragment,Io),wy=h(Io),T(Dn.$$.fragment,Io),Io.forEach(t),Yt.forEach(t),om=h(n),ps=a(n,"H2",{class:!0});var hg=i(ps);Fn=a(hg,"A",{id:!0,class:!0,href:!0});var sx=i(Fn);ch=a(sx,"SPAN",{});var nx=i(ch);T(Vi.$$.fragment,nx),nx.forEach(t),sx.forEach(t),vy=h(hg),ph=a(hg,"SPAN",{});var rx=i(ph);Ty=s(rx,"TFDistilBertForMultipleChoice"),rx.forEach(t),hg.forEach(t),sm=h(n),pt=a(n,"DIV",{class:!0});var Zt=i(pt);T(Ji.$$.fragment,Zt),$y=h(Zt),hh=a(Zt,"P",{});var ax=i(hh);yy=s(ax,`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),ax.forEach(t),Dy=h(Zt),Gi=a(Zt,"P",{});var fg=i(Gi);Fy=s(fg,"This model inherits from "),Ac=a(fg,"A",{href:!0});var ix=i(Ac);Ey=s(ix,"TFPreTrainedModel"),ix.forEach(t),By=s(fg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),fg.forEach(t),xy=h(Zt),Xi=a(Zt,"P",{});var ug=i(Xi);My=s(ug,"This model is also a "),Yi=a(ug,"A",{href:!0,rel:!0});var lx=i(Yi);zy=s(lx,"tf.keras.Model"),lx.forEach(t),jy=s(ug,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),ug.forEach(t),Cy=h(Zt),T(En.$$.fragment,Zt),Py=h(Zt),Wt=a(Zt,"DIV",{class:!0});var ur=i(Wt);T(Zi.$$.fragment,ur),qy=h(ur),hs=a(ur,"P",{});var ip=i(hs);Ay=s(ip,"The "),Oc=a(ip,"A",{href:!0});var dx=i(Oc);Oy=s(dx,"TFDistilBertForMultipleChoice"),dx.forEach(t),Ly=s(ip," forward method, overrides the "),fh=a(ip,"CODE",{});var cx=i(fh);Iy=s(cx,"__call__"),cx.forEach(t),Sy=s(ip," special method."),ip.forEach(t),Ny=h(ur),T(Bn.$$.fragment,ur),Ry=h(ur),T(xn.$$.fragment,ur),ur.forEach(t),Zt.forEach(t),nm=h(n),fs=a(n,"H2",{class:!0});var mg=i(fs);Mn=a(mg,"A",{id:!0,class:!0,href:!0});var px=i(Mn);uh=a(px,"SPAN",{});var hx=i(uh);T(el.$$.fragment,hx),hx.forEach(t),px.forEach(t),Wy=h(mg),mh=a(mg,"SPAN",{});var fx=i(mh);Qy=s(fx,"TFDistilBertForTokenClassification"),fx.forEach(t),mg.forEach(t),rm=h(n),ht=a(n,"DIV",{class:!0});var eo=i(ht);T(tl.$$.fragment,eo),Uy=h(eo),gh=a(eo,"P",{});var ux=i(gh);Hy=s(ux,`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),ux.forEach(t),Ky=h(eo),ol=a(eo,"P",{});var gg=i(ol);Vy=s(gg,"This model inherits from "),Lc=a(gg,"A",{href:!0});var mx=i(Lc);Jy=s(mx,"TFPreTrainedModel"),mx.forEach(t),Gy=s(gg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),gg.forEach(t),Xy=h(eo),sl=a(eo,"P",{});var _g=i(sl);Yy=s(_g,"This model is also a "),nl=a(_g,"A",{href:!0,rel:!0});var gx=i(nl);Zy=s(gx,"tf.keras.Model"),gx.forEach(t),e1=s(_g,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),_g.forEach(t),t1=h(eo),T(zn.$$.fragment,eo),o1=h(eo),Dt=a(eo,"DIV",{class:!0});var So=i(Dt);T(rl.$$.fragment,So),s1=h(So),us=a(So,"P",{});var lp=i(us);n1=s(lp,"The "),Ic=a(lp,"A",{href:!0});var _x=i(Ic);r1=s(_x,"TFDistilBertForTokenClassification"),_x.forEach(t),a1=s(lp," forward method, overrides the "),_h=a(lp,"CODE",{});var bx=i(_h);i1=s(bx,"__call__"),bx.forEach(t),l1=s(lp," special method."),lp.forEach(t),d1=h(So),T(jn.$$.fragment,So),c1=h(So),T(Cn.$$.fragment,So),p1=h(So),T(Pn.$$.fragment,So),So.forEach(t),eo.forEach(t),am=h(n),ms=a(n,"H2",{class:!0});var bg=i(ms);qn=a(bg,"A",{id:!0,class:!0,href:!0});var kx=i(qn);bh=a(kx,"SPAN",{});var wx=i(bh);T(al.$$.fragment,wx),wx.forEach(t),kx.forEach(t),h1=h(bg),kh=a(bg,"SPAN",{});var vx=i(kh);f1=s(vx,"TFDistilBertForQuestionAnswering"),vx.forEach(t),bg.forEach(t),im=h(n),ft=a(n,"DIV",{class:!0});var to=i(ft);T(il.$$.fragment,to),u1=h(to),gs=a(to,"P",{});var dp=i(gs);m1=s(dp,`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layer on top of the hidden-states output to compute `),wh=a(dp,"CODE",{});var Tx=i(wh);g1=s(Tx,"span start logits"),Tx.forEach(t),_1=s(dp," and "),vh=a(dp,"CODE",{});var $x=i(vh);b1=s($x,"span end logits"),$x.forEach(t),k1=s(dp,")."),dp.forEach(t),w1=h(to),ll=a(to,"P",{});var kg=i(ll);v1=s(kg,"This model inherits from "),Sc=a(kg,"A",{href:!0});var yx=i(Sc);T1=s(yx,"TFPreTrainedModel"),yx.forEach(t),$1=s(kg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),kg.forEach(t),y1=h(to),dl=a(to,"P",{});var wg=i(dl);D1=s(wg,"This model is also a "),cl=a(wg,"A",{href:!0,rel:!0});var Dx=i(cl);F1=s(Dx,"tf.keras.Model"),Dx.forEach(t),E1=s(wg,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),wg.forEach(t),B1=h(to),T(An.$$.fragment,to),x1=h(to),Ft=a(to,"DIV",{class:!0});var No=i(Ft);T(pl.$$.fragment,No),M1=h(No),_s=a(No,"P",{});var cp=i(_s);z1=s(cp,"The "),Nc=a(cp,"A",{href:!0});var Fx=i(Nc);j1=s(Fx,"TFDistilBertForQuestionAnswering"),Fx.forEach(t),C1=s(cp," forward method, overrides the "),Th=a(cp,"CODE",{});var Ex=i(Th);P1=s(Ex,"__call__"),Ex.forEach(t),q1=s(cp," special method."),cp.forEach(t),A1=h(No),T(On.$$.fragment,No),O1=h(No),T(Ln.$$.fragment,No),L1=h(No),T(In.$$.fragment,No),No.forEach(t),to.forEach(t),lm=h(n),bs=a(n,"H2",{class:!0});var vg=i(bs);Sn=a(vg,"A",{id:!0,class:!0,href:!0});var Bx=i(Sn);$h=a(Bx,"SPAN",{});var xx=i($h);T(hl.$$.fragment,xx),xx.forEach(t),Bx.forEach(t),I1=h(vg),yh=a(vg,"SPAN",{});var Mx=i(yh);S1=s(Mx,"FlaxDistilBertModel"),Mx.forEach(t),vg.forEach(t),dm=h(n),tt=a(n,"DIV",{class:!0});var Bt=i(tt);T(fl.$$.fragment,Bt),N1=h(Bt),Dh=a(Bt,"P",{});var zx=i(Dh);R1=s(zx,"The bare DistilBert Model transformer outputting raw hidden-states without any specific head on top."),zx.forEach(t),W1=h(Bt),ul=a(Bt,"P",{});var Tg=i(ul);Q1=s(Tg,"This model inherits from "),Rc=a(Tg,"A",{href:!0});var jx=i(Rc);U1=s(jx,"FlaxPreTrainedModel"),jx.forEach(t),H1=s(Tg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),Tg.forEach(t),K1=h(Bt),ml=a(Bt,"P",{});var $g=i(ml);V1=s($g,"This model is also a Flax Linen "),gl=a($g,"A",{href:!0,rel:!0});var Cx=i(gl);J1=s(Cx,"flax.linen.Module"),Cx.forEach(t),G1=s($g,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),$g.forEach(t),X1=h(Bt),Fh=a(Bt,"P",{});var Px=i(Fh);Y1=s(Px,"Finally, this model supports inherent JAX features such as:"),Px.forEach(t),Z1=h(Bt),so=a(Bt,"UL",{});var mr=i(so);Eh=a(mr,"LI",{});var qx=i(Eh);_l=a(qx,"A",{href:!0,rel:!0});var Ax=i(_l);e2=s(Ax,"Just-In-Time (JIT) compilation"),Ax.forEach(t),qx.forEach(t),t2=h(mr),Bh=a(mr,"LI",{});var Ox=i(Bh);bl=a(Ox,"A",{href:!0,rel:!0});var Lx=i(bl);o2=s(Lx,"Automatic Differentiation"),Lx.forEach(t),Ox.forEach(t),s2=h(mr),xh=a(mr,"LI",{});var Ix=i(xh);kl=a(Ix,"A",{href:!0,rel:!0});var Sx=i(kl);n2=s(Sx,"Vectorization"),Sx.forEach(t),Ix.forEach(t),r2=h(mr),Mh=a(mr,"LI",{});var Nx=i(Mh);wl=a(Nx,"A",{href:!0,rel:!0});var Rx=i(wl);a2=s(Rx,"Parallelization"),Rx.forEach(t),Nx.forEach(t),mr.forEach(t),i2=h(Bt),Qt=a(Bt,"DIV",{class:!0});var gr=i(Qt);T(vl.$$.fragment,gr),l2=h(gr),ks=a(gr,"P",{});var pp=i(ks);d2=s(pp,"The "),zh=a(pp,"CODE",{});var Wx=i(zh);c2=s(Wx,"FlaxDistilBertPreTrainedModel"),Wx.forEach(t),p2=s(pp," forward method, overrides the "),jh=a(pp,"CODE",{});var Qx=i(jh);h2=s(Qx,"__call__"),Qx.forEach(t),f2=s(pp," special method."),pp.forEach(t),u2=h(gr),T(Nn.$$.fragment,gr),m2=h(gr),T(Rn.$$.fragment,gr),gr.forEach(t),Bt.forEach(t),cm=h(n),ws=a(n,"H2",{class:!0});var yg=i(ws);Wn=a(yg,"A",{id:!0,class:!0,href:!0});var Ux=i(Wn);Ch=a(Ux,"SPAN",{});var Hx=i(Ch);T(Tl.$$.fragment,Hx),Hx.forEach(t),Ux.forEach(t),g2=h(yg),Ph=a(yg,"SPAN",{});var Kx=i(Ph);_2=s(Kx,"FlaxDistilBertForMaskedLM"),Kx.forEach(t),yg.forEach(t),pm=h(n),ot=a(n,"DIV",{class:!0});var xt=i(ot);T($l.$$.fragment,xt),b2=h(xt),yl=a(xt,"P",{});var Dg=i(yl);k2=s(Dg,"DistilBert Model with a "),qh=a(Dg,"CODE",{});var Vx=i(qh);w2=s(Vx,"language modeling"),Vx.forEach(t),v2=s(Dg," head on top."),Dg.forEach(t),T2=h(xt),Dl=a(xt,"P",{});var Fg=i(Dl);$2=s(Fg,"This model inherits from "),Wc=a(Fg,"A",{href:!0});var Jx=i(Wc);y2=s(Jx,"FlaxPreTrainedModel"),Jx.forEach(t),D2=s(Fg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),Fg.forEach(t),F2=h(xt),Fl=a(xt,"P",{});var Eg=i(Fl);E2=s(Eg,"This model is also a Flax Linen "),El=a(Eg,"A",{href:!0,rel:!0});var Gx=i(El);B2=s(Gx,"flax.linen.Module"),Gx.forEach(t),x2=s(Eg,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Eg.forEach(t),M2=h(xt),Ah=a(xt,"P",{});var Xx=i(Ah);z2=s(Xx,"Finally, this model supports inherent JAX features such as:"),Xx.forEach(t),j2=h(xt),no=a(xt,"UL",{});var _r=i(no);Oh=a(_r,"LI",{});var Yx=i(Oh);Bl=a(Yx,"A",{href:!0,rel:!0});var Zx=i(Bl);C2=s(Zx,"Just-In-Time (JIT) compilation"),Zx.forEach(t),Yx.forEach(t),P2=h(_r),Lh=a(_r,"LI",{});var eM=i(Lh);xl=a(eM,"A",{href:!0,rel:!0});var tM=i(xl);q2=s(tM,"Automatic Differentiation"),tM.forEach(t),eM.forEach(t),A2=h(_r),Ih=a(_r,"LI",{});var oM=i(Ih);Ml=a(oM,"A",{href:!0,rel:!0});var sM=i(Ml);O2=s(sM,"Vectorization"),sM.forEach(t),oM.forEach(t),L2=h(_r),Sh=a(_r,"LI",{});var nM=i(Sh);zl=a(nM,"A",{href:!0,rel:!0});var rM=i(zl);I2=s(rM,"Parallelization"),rM.forEach(t),nM.forEach(t),_r.forEach(t),S2=h(xt),Ut=a(xt,"DIV",{class:!0});var br=i(Ut);T(jl.$$.fragment,br),N2=h(br),vs=a(br,"P",{});var hp=i(vs);R2=s(hp,"The "),Nh=a(hp,"CODE",{});var aM=i(Nh);W2=s(aM,"FlaxDistilBertPreTrainedModel"),aM.forEach(t),Q2=s(hp," forward method, overrides the "),Rh=a(hp,"CODE",{});var iM=i(Rh);U2=s(iM,"__call__"),iM.forEach(t),H2=s(hp," special method."),hp.forEach(t),K2=h(br),T(Qn.$$.fragment,br),V2=h(br),T(Un.$$.fragment,br),br.forEach(t),xt.forEach(t),hm=h(n),Ts=a(n,"H2",{class:!0});var Bg=i(Ts);Hn=a(Bg,"A",{id:!0,class:!0,href:!0});var lM=i(Hn);Wh=a(lM,"SPAN",{});var dM=i(Wh);T(Cl.$$.fragment,dM),dM.forEach(t),lM.forEach(t),J2=h(Bg),Qh=a(Bg,"SPAN",{});var cM=i(Qh);G2=s(cM,"FlaxDistilBertForSequenceClassification"),cM.forEach(t),Bg.forEach(t),fm=h(n),st=a(n,"DIV",{class:!0});var Mt=i(st);T(Pl.$$.fragment,Mt),X2=h(Mt),Uh=a(Mt,"P",{});var pM=i(Uh);Y2=s(pM,`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),pM.forEach(t),Z2=h(Mt),ql=a(Mt,"P",{});var xg=i(ql);eD=s(xg,"This model inherits from "),Qc=a(xg,"A",{href:!0});var hM=i(Qc);tD=s(hM,"FlaxPreTrainedModel"),hM.forEach(t),oD=s(xg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),xg.forEach(t),sD=h(Mt),Al=a(Mt,"P",{});var Mg=i(Al);nD=s(Mg,"This model is also a Flax Linen "),Ol=a(Mg,"A",{href:!0,rel:!0});var fM=i(Ol);rD=s(fM,"flax.linen.Module"),fM.forEach(t),aD=s(Mg,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Mg.forEach(t),iD=h(Mt),Hh=a(Mt,"P",{});var uM=i(Hh);lD=s(uM,"Finally, this model supports inherent JAX features such as:"),uM.forEach(t),dD=h(Mt),ro=a(Mt,"UL",{});var kr=i(ro);Kh=a(kr,"LI",{});var mM=i(Kh);Ll=a(mM,"A",{href:!0,rel:!0});var gM=i(Ll);cD=s(gM,"Just-In-Time (JIT) compilation"),gM.forEach(t),mM.forEach(t),pD=h(kr),Vh=a(kr,"LI",{});var _M=i(Vh);Il=a(_M,"A",{href:!0,rel:!0});var bM=i(Il);hD=s(bM,"Automatic Differentiation"),bM.forEach(t),_M.forEach(t),fD=h(kr),Jh=a(kr,"LI",{});var kM=i(Jh);Sl=a(kM,"A",{href:!0,rel:!0});var wM=i(Sl);uD=s(wM,"Vectorization"),wM.forEach(t),kM.forEach(t),mD=h(kr),Gh=a(kr,"LI",{});var vM=i(Gh);Nl=a(vM,"A",{href:!0,rel:!0});var TM=i(Nl);gD=s(TM,"Parallelization"),TM.forEach(t),vM.forEach(t),kr.forEach(t),_D=h(Mt),Ht=a(Mt,"DIV",{class:!0});var wr=i(Ht);T(Rl.$$.fragment,wr),bD=h(wr),$s=a(wr,"P",{});var fp=i($s);kD=s(fp,"The "),Xh=a(fp,"CODE",{});var $M=i(Xh);wD=s($M,"FlaxDistilBertPreTrainedModel"),$M.forEach(t),vD=s(fp," forward method, overrides the "),Yh=a(fp,"CODE",{});var yM=i(Yh);TD=s(yM,"__call__"),yM.forEach(t),$D=s(fp," special method."),fp.forEach(t),yD=h(wr),T(Kn.$$.fragment,wr),DD=h(wr),T(Vn.$$.fragment,wr),wr.forEach(t),Mt.forEach(t),um=h(n),ys=a(n,"H2",{class:!0});var zg=i(ys);Jn=a(zg,"A",{id:!0,class:!0,href:!0});var DM=i(Jn);Zh=a(DM,"SPAN",{});var FM=i(Zh);T(Wl.$$.fragment,FM),FM.forEach(t),DM.forEach(t),FD=h(zg),ef=a(zg,"SPAN",{});var EM=i(ef);ED=s(EM,"FlaxDistilBertForMultipleChoice"),EM.forEach(t),zg.forEach(t),mm=h(n),nt=a(n,"DIV",{class:!0});var zt=i(nt);T(Ql.$$.fragment,zt),BD=h(zt),tf=a(zt,"P",{});var BM=i(tf);xD=s(BM,`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),BM.forEach(t),MD=h(zt),Ul=a(zt,"P",{});var jg=i(Ul);zD=s(jg,"This model inherits from "),Uc=a(jg,"A",{href:!0});var xM=i(Uc);jD=s(xM,"FlaxPreTrainedModel"),xM.forEach(t),CD=s(jg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),jg.forEach(t),PD=h(zt),Hl=a(zt,"P",{});var Cg=i(Hl);qD=s(Cg,"This model is also a Flax Linen "),Kl=a(Cg,"A",{href:!0,rel:!0});var MM=i(Kl);AD=s(MM,"flax.linen.Module"),MM.forEach(t),OD=s(Cg,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Cg.forEach(t),LD=h(zt),of=a(zt,"P",{});var zM=i(of);ID=s(zM,"Finally, this model supports inherent JAX features such as:"),zM.forEach(t),SD=h(zt),ao=a(zt,"UL",{});var vr=i(ao);sf=a(vr,"LI",{});var jM=i(sf);Vl=a(jM,"A",{href:!0,rel:!0});var CM=i(Vl);ND=s(CM,"Just-In-Time (JIT) compilation"),CM.forEach(t),jM.forEach(t),RD=h(vr),nf=a(vr,"LI",{});var PM=i(nf);Jl=a(PM,"A",{href:!0,rel:!0});var qM=i(Jl);WD=s(qM,"Automatic Differentiation"),qM.forEach(t),PM.forEach(t),QD=h(vr),rf=a(vr,"LI",{});var AM=i(rf);Gl=a(AM,"A",{href:!0,rel:!0});var OM=i(Gl);UD=s(OM,"Vectorization"),OM.forEach(t),AM.forEach(t),HD=h(vr),af=a(vr,"LI",{});var LM=i(af);Xl=a(LM,"A",{href:!0,rel:!0});var IM=i(Xl);KD=s(IM,"Parallelization"),IM.forEach(t),LM.forEach(t),vr.forEach(t),VD=h(zt),Kt=a(zt,"DIV",{class:!0});var Tr=i(Kt);T(Yl.$$.fragment,Tr),JD=h(Tr),Ds=a(Tr,"P",{});var up=i(Ds);GD=s(up,"The "),lf=a(up,"CODE",{});var SM=i(lf);XD=s(SM,"FlaxDistilBertPreTrainedModel"),SM.forEach(t),YD=s(up," forward method, overrides the "),df=a(up,"CODE",{});var NM=i(df);ZD=s(NM,"__call__"),NM.forEach(t),eF=s(up," special method."),up.forEach(t),tF=h(Tr),T(Gn.$$.fragment,Tr),oF=h(Tr),T(Xn.$$.fragment,Tr),Tr.forEach(t),zt.forEach(t),gm=h(n),Fs=a(n,"H2",{class:!0});var Pg=i(Fs);Yn=a(Pg,"A",{id:!0,class:!0,href:!0});var RM=i(Yn);cf=a(RM,"SPAN",{});var WM=i(cf);T(Zl.$$.fragment,WM),WM.forEach(t),RM.forEach(t),sF=h(Pg),pf=a(Pg,"SPAN",{});var QM=i(pf);nF=s(QM,"FlaxDistilBertForTokenClassification"),QM.forEach(t),Pg.forEach(t),_m=h(n),rt=a(n,"DIV",{class:!0});var jt=i(rt);T(ed.$$.fragment,jt),rF=h(jt),hf=a(jt,"P",{});var UM=i(hf);aF=s(UM,`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),UM.forEach(t),iF=h(jt),td=a(jt,"P",{});var qg=i(td);lF=s(qg,"This model inherits from "),Hc=a(qg,"A",{href:!0});var HM=i(Hc);dF=s(HM,"FlaxPreTrainedModel"),HM.forEach(t),cF=s(qg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),qg.forEach(t),pF=h(jt),od=a(jt,"P",{});var Ag=i(od);hF=s(Ag,"This model is also a Flax Linen "),sd=a(Ag,"A",{href:!0,rel:!0});var KM=i(sd);fF=s(KM,"flax.linen.Module"),KM.forEach(t),uF=s(Ag,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Ag.forEach(t),mF=h(jt),ff=a(jt,"P",{});var VM=i(ff);gF=s(VM,"Finally, this model supports inherent JAX features such as:"),VM.forEach(t),_F=h(jt),io=a(jt,"UL",{});var $r=i(io);uf=a($r,"LI",{});var JM=i(uf);nd=a(JM,"A",{href:!0,rel:!0});var GM=i(nd);bF=s(GM,"Just-In-Time (JIT) compilation"),GM.forEach(t),JM.forEach(t),kF=h($r),mf=a($r,"LI",{});var XM=i(mf);rd=a(XM,"A",{href:!0,rel:!0});var YM=i(rd);wF=s(YM,"Automatic Differentiation"),YM.forEach(t),XM.forEach(t),vF=h($r),gf=a($r,"LI",{});var ZM=i(gf);ad=a(ZM,"A",{href:!0,rel:!0});var e4=i(ad);TF=s(e4,"Vectorization"),e4.forEach(t),ZM.forEach(t),$F=h($r),_f=a($r,"LI",{});var t4=i(_f);id=a(t4,"A",{href:!0,rel:!0});var o4=i(id);yF=s(o4,"Parallelization"),o4.forEach(t),t4.forEach(t),$r.forEach(t),DF=h(jt),Vt=a(jt,"DIV",{class:!0});var yr=i(Vt);T(ld.$$.fragment,yr),FF=h(yr),Es=a(yr,"P",{});var mp=i(Es);EF=s(mp,"The "),bf=a(mp,"CODE",{});var s4=i(bf);BF=s(s4,"FlaxDistilBertPreTrainedModel"),s4.forEach(t),xF=s(mp," forward method, overrides the "),kf=a(mp,"CODE",{});var n4=i(kf);MF=s(n4,"__call__"),n4.forEach(t),zF=s(mp," special method."),mp.forEach(t),jF=h(yr),T(Zn.$$.fragment,yr),CF=h(yr),T(er.$$.fragment,yr),yr.forEach(t),jt.forEach(t),bm=h(n),Bs=a(n,"H2",{class:!0});var Og=i(Bs);tr=a(Og,"A",{id:!0,class:!0,href:!0});var r4=i(tr);wf=a(r4,"SPAN",{});var a4=i(wf);T(dd.$$.fragment,a4),a4.forEach(t),r4.forEach(t),PF=h(Og),vf=a(Og,"SPAN",{});var i4=i(vf);qF=s(i4,"FlaxDistilBertForQuestionAnswering"),i4.forEach(t),Og.forEach(t),km=h(n),at=a(n,"DIV",{class:!0});var Ct=i(at);T(cd.$$.fragment,Ct),AF=h(Ct),xs=a(Ct,"P",{});var gp=i(xs);OF=s(gp,`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),Tf=a(gp,"CODE",{});var l4=i(Tf);LF=s(l4,"span start logits"),l4.forEach(t),IF=s(gp," and "),$f=a(gp,"CODE",{});var d4=i($f);SF=s(d4,"span end logits"),d4.forEach(t),NF=s(gp,")."),gp.forEach(t),RF=h(Ct),pd=a(Ct,"P",{});var Lg=i(pd);WF=s(Lg,"This model inherits from "),Kc=a(Lg,"A",{href:!0});var c4=i(Kc);QF=s(c4,"FlaxPreTrainedModel"),c4.forEach(t),UF=s(Lg,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),Lg.forEach(t),HF=h(Ct),hd=a(Ct,"P",{});var Ig=i(hd);KF=s(Ig,"This model is also a Flax Linen "),fd=a(Ig,"A",{href:!0,rel:!0});var p4=i(fd);VF=s(p4,"flax.linen.Module"),p4.forEach(t),JF=s(Ig,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Ig.forEach(t),GF=h(Ct),yf=a(Ct,"P",{});var h4=i(yf);XF=s(h4,"Finally, this model supports inherent JAX features such as:"),h4.forEach(t),YF=h(Ct),lo=a(Ct,"UL",{});var Dr=i(lo);Df=a(Dr,"LI",{});var f4=i(Df);ud=a(f4,"A",{href:!0,rel:!0});var u4=i(ud);ZF=s(u4,"Just-In-Time (JIT) compilation"),u4.forEach(t),f4.forEach(t),eE=h(Dr),Ff=a(Dr,"LI",{});var m4=i(Ff);md=a(m4,"A",{href:!0,rel:!0});var g4=i(md);tE=s(g4,"Automatic Differentiation"),g4.forEach(t),m4.forEach(t),oE=h(Dr),Ef=a(Dr,"LI",{});var _4=i(Ef);gd=a(_4,"A",{href:!0,rel:!0});var b4=i(gd);sE=s(b4,"Vectorization"),b4.forEach(t),_4.forEach(t),nE=h(Dr),Bf=a(Dr,"LI",{});var k4=i(Bf);_d=a(k4,"A",{href:!0,rel:!0});var w4=i(_d);rE=s(w4,"Parallelization"),w4.forEach(t),k4.forEach(t),Dr.forEach(t),aE=h(Ct),Jt=a(Ct,"DIV",{class:!0});var Fr=i(Jt);T(bd.$$.fragment,Fr),iE=h(Fr),Ms=a(Fr,"P",{});var _p=i(Ms);lE=s(_p,"The "),xf=a(_p,"CODE",{});var v4=i(xf);dE=s(v4,"FlaxDistilBertPreTrainedModel"),v4.forEach(t),cE=s(_p," forward method, overrides the "),Mf=a(_p,"CODE",{});var T4=i(Mf);pE=s(T4,"__call__"),T4.forEach(t),hE=s(_p," special method."),_p.forEach(t),fE=h(Fr),T(or.$$.fragment,Fr),uE=h(Fr),T(sr.$$.fragment,Fr),Fr.forEach(t),Ct.forEach(t),this.h()},h(){c(d,"name","hf:doc:metadata"),c(d,"content",JSON.stringify(B3)),c(m,"id","distilbert"),c(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(m,"href","#distilbert"),c(f,"class","relative group"),c(re,"id","overview"),c(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(re,"href","#overview"),c(I,"class","relative group"),c(ae,"href","https://medium.com/huggingface/distilbert-8cf3380435b5"),c(ae,"rel","nofollow"),c(ie,"href","https://arxiv.org/abs/1910.01108"),c(ie,"rel","nofollow"),c(Z,"href","https://huggingface.co/victorsanh"),c(Z,"rel","nofollow"),c(z,"href","https://huggingface.co/kamalkraj"),c(z,"rel","nofollow"),c(Le,"href","https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation"),c(Le,"rel","nofollow"),c(Oe,"id","resources"),c(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Oe,"href","#resources"),c(N,"class","relative group"),c(xr,"href","https://huggingface.co/blog/sentiment-analysis-python"),c(xr,"rel","nofollow"),c(zr,"href","https://huggingface.co/blog/fastai"),c(zr,"rel","nofollow"),c(Cr,"href","https://huggingface.co/blog/ray-tune"),c(Cr,"rel","nofollow"),c(qr,"href","https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face"),c(qr,"rel","nofollow"),c(Or,"href","https://colab.research.google.com/github/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb"),c(Or,"rel","nofollow"),c(Ir,"href","https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb"),c(Ir,"rel","nofollow"),c(Nr,"href","https://colab.research.google.com/github/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb"),c(Nr,"rel","nofollow"),c(Nd,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),c(Rr,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification"),c(Rr,"rel","nofollow"),c(Wr,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb"),c(Wr,"rel","nofollow"),c(Rd,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),c(Qr,"href","https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification"),c(Qr,"rel","nofollow"),c(Ur,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb"),c(Ur,"rel","nofollow"),c(Wd,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification"),c(Hr,"href","https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification"),c(Hr,"rel","nofollow"),c(Kr,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb"),c(Kr,"rel","nofollow"),c(Qd,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),c(Jr,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification"),c(Jr,"rel","nofollow"),c(Gr,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb"),c(Gr,"rel","nofollow"),c(Ud,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),c(Xr,"href","https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification"),c(Xr,"rel","nofollow"),c(Yr,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb"),c(Yr,"rel","nofollow"),c(Hd,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification"),c(Zr,"href","https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification"),c(Zr,"rel","nofollow"),c(ea,"href","https://huggingface.co/course/chapter7/2?fw=pt"),c(ea,"rel","nofollow"),c(Vd,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(oa,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling"),c(oa,"rel","nofollow"),c(sa,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb"),c(sa,"rel","nofollow"),c(Jd,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(na,"href","https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy"),c(na,"rel","nofollow"),c(ra,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb"),c(ra,"rel","nofollow"),c(Gd,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM"),c(aa,"href","https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling"),c(aa,"rel","nofollow"),c(ia,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb"),c(ia,"rel","nofollow"),c(la,"href","https://huggingface.co/course/chapter7/3?fw=pt"),c(la,"rel","nofollow"),c(Yd,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),c(ca,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering"),c(ca,"rel","nofollow"),c(pa,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb"),c(pa,"rel","nofollow"),c(Zd,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),c(ha,"href","https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering"),c(ha,"rel","nofollow"),c(fa,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb"),c(fa,"rel","nofollow"),c(ec,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering"),c(ua,"href","https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering"),c(ua,"rel","nofollow"),c(ma,"href","https://huggingface.co/course/chapter7/7?fw=pt"),c(ma,"rel","nofollow"),c(sc,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),c(ga,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice"),c(ga,"rel","nofollow"),c(_a,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb"),c(_a,"rel","nofollow"),c(nc,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),c(ba,"href","https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice"),c(ba,"rel","nofollow"),c(ka,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb"),c(ka,"rel","nofollow"),c(va,"href","https://huggingface.co/blog/intel"),c(va,"rel","nofollow"),c($a,"href","https://www.philschmid.de/optimizing-transformers-with-optimum-gpu"),c($a,"rel","nofollow"),c(Da,"href","https://www.philschmid.de/optimizing-transformers-with-optimum"),c(Da,"rel","nofollow"),c(Ea,"href","https://huggingface.co/blog/bert-inferentia-sagemaker"),c(Ea,"rel","nofollow"),c(xa,"href","https://www.philschmid.de/sagemaker-serverless-huggingface-distilbert"),c(xa,"rel","nofollow"),c(za,"href","https://huggingface.co/blog/how-to-deploy-a-pipeline-to-google-clouds"),c(za,"rel","nofollow"),c(Ca,"href","https://huggingface.co/blog/deploy-hugging-face-models-easily-with-amazon-sagemaker"),c(Ca,"rel","nofollow"),c(qa,"href","https://www.philschmid.de/terraform-huggingface-amazon-sagemaker"),c(qa,"rel","nofollow"),c(qs,"id","transformers.DistilBertConfig"),c(qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qs,"href","#transformers.DistilBertConfig"),c(Ro,"class","relative group"),c(lc,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertModel"),c(dc,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(La,"href","https://huggingface.co/distilbert-base-uncased"),c(La,"rel","nofollow"),c(cc,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(pc,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(Pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Os,"id","transformers.DistilBertTokenizer"),c(Os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Os,"href","#transformers.DistilBertTokenizer"),c(Qo,"class","relative group"),c(hc,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer"),c(fc,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),c(uc,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),c(qt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Is,"id","transformers.DistilBertTokenizerFast"),c(Is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Is,"href","#transformers.DistilBertTokenizerFast"),c(Uo,"class","relative group"),c(mc,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),c(gc,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),c(_c,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),c(At,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Ns,"id","transformers.DistilBertModel"),c(Ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ns,"href","#transformers.DistilBertModel"),c(Ho,"class","relative group"),c(bc,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(Ga,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ga,"rel","nofollow"),c(kc,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertModel"),c(St,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Qs,"id","transformers.DistilBertForMaskedLM"),c(Qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qs,"href","#transformers.DistilBertForMaskedLM"),c(Vo,"class","relative group"),c(wc,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(si,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(si,"rel","nofollow"),c(vc,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(wt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(mt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Vs,"id","transformers.DistilBertForSequenceClassification"),c(Vs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Vs,"href","#transformers.DistilBertForSequenceClassification"),c(Go,"class","relative group"),c(Tc,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(di,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(di,"rel","nofollow"),c($c,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),c(it,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(gt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(en,"id","transformers.DistilBertForMultipleChoice"),c(en,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(en,"href","#transformers.DistilBertForMultipleChoice"),c(Yo,"class","relative group"),c(yc,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(mi,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(mi,"rel","nofollow"),c(Dc,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),c(Nt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(_t,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(sn,"id","transformers.DistilBertForTokenClassification"),c(sn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(sn,"href","#transformers.DistilBertForTokenClassification"),c(es,"class","relative group"),c(Fc,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(vi,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(vi,"rel","nofollow"),c(Ec,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),c(vt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(bt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ln,"id","transformers.DistilBertForQuestionAnswering"),c(ln,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ln,"href","#transformers.DistilBertForQuestionAnswering"),c(os,"class","relative group"),c(Bc,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(Ei,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ei,"rel","nofollow"),c(xc,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),c(Tt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(kt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(hn,"id","transformers.TFDistilBertModel"),c(hn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(hn,"href","#transformers.TFDistilBertModel"),c(rs,"class","relative group"),c(Mc,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(Ci,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Ci,"rel","nofollow"),c(zc,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(Rt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(lt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(gn,"id","transformers.TFDistilBertForMaskedLM"),c(gn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(gn,"href","#transformers.TFDistilBertForMaskedLM"),c(is,"class","relative group"),c(jc,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(Si,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Si,"rel","nofollow"),c(Cc,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c($t,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(dt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(vn,"id","transformers.TFDistilBertForSequenceClassification"),c(vn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(vn,"href","#transformers.TFDistilBertForSequenceClassification"),c(ds,"class","relative group"),c(Pc,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(Hi,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Hi,"rel","nofollow"),c(qc,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),c(yt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ct,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Fn,"id","transformers.TFDistilBertForMultipleChoice"),c(Fn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fn,"href","#transformers.TFDistilBertForMultipleChoice"),c(ps,"class","relative group"),c(Ac,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(Yi,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Yi,"rel","nofollow"),c(Oc,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),c(Wt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Mn,"id","transformers.TFDistilBertForTokenClassification"),c(Mn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Mn,"href","#transformers.TFDistilBertForTokenClassification"),c(fs,"class","relative group"),c(Lc,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(nl,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(nl,"rel","nofollow"),c(Ic,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),c(Dt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ht,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(qn,"id","transformers.TFDistilBertForQuestionAnswering"),c(qn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qn,"href","#transformers.TFDistilBertForQuestionAnswering"),c(ms,"class","relative group"),c(Sc,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(cl,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(cl,"rel","nofollow"),c(Nc,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),c(Ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Sn,"id","transformers.FlaxDistilBertModel"),c(Sn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Sn,"href","#transformers.FlaxDistilBertModel"),c(bs,"class","relative group"),c(Rc,"href","/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(gl,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(gl,"rel","nofollow"),c(_l,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(_l,"rel","nofollow"),c(bl,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(bl,"rel","nofollow"),c(kl,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(kl,"rel","nofollow"),c(wl,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(wl,"rel","nofollow"),c(Qt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(tt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Wn,"id","transformers.FlaxDistilBertForMaskedLM"),c(Wn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Wn,"href","#transformers.FlaxDistilBertForMaskedLM"),c(ws,"class","relative group"),c(Wc,"href","/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(El,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(El,"rel","nofollow"),c(Bl,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(Bl,"rel","nofollow"),c(xl,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(xl,"rel","nofollow"),c(Ml,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(Ml,"rel","nofollow"),c(zl,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(zl,"rel","nofollow"),c(Ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ot,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Hn,"id","transformers.FlaxDistilBertForSequenceClassification"),c(Hn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Hn,"href","#transformers.FlaxDistilBertForSequenceClassification"),c(Ts,"class","relative group"),c(Qc,"href","/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(Ol,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(Ol,"rel","nofollow"),c(Ll,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(Ll,"rel","nofollow"),c(Il,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(Il,"rel","nofollow"),c(Sl,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(Sl,"rel","nofollow"),c(Nl,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(Nl,"rel","nofollow"),c(Ht,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(st,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Jn,"id","transformers.FlaxDistilBertForMultipleChoice"),c(Jn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Jn,"href","#transformers.FlaxDistilBertForMultipleChoice"),c(ys,"class","relative group"),c(Uc,"href","/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(Kl,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(Kl,"rel","nofollow"),c(Vl,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(Vl,"rel","nofollow"),c(Jl,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(Jl,"rel","nofollow"),c(Gl,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(Gl,"rel","nofollow"),c(Xl,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(Xl,"rel","nofollow"),c(Kt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(nt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Yn,"id","transformers.FlaxDistilBertForTokenClassification"),c(Yn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Yn,"href","#transformers.FlaxDistilBertForTokenClassification"),c(Fs,"class","relative group"),c(Hc,"href","/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(sd,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(sd,"rel","nofollow"),c(nd,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(nd,"rel","nofollow"),c(rd,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(rd,"rel","nofollow"),c(ad,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(ad,"rel","nofollow"),c(id,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(id,"rel","nofollow"),c(Vt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(rt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(tr,"id","transformers.FlaxDistilBertForQuestionAnswering"),c(tr,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(tr,"href","#transformers.FlaxDistilBertForQuestionAnswering"),c(Bs,"class","relative group"),c(Kc,"href","/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(fd,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(fd,"rel","nofollow"),c(ud,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(ud,"rel","nofollow"),c(md,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(md,"rel","nofollow"),c(gd,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(gd,"rel","nofollow"),c(_d,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(_d,"rel","nofollow"),c(Jt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(at,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(n,g){e(document.head,d),_(n,b,g),_(n,f,g),e(f,m),e(m,k),$(l,k,null),e(f,u),e(f,M),e(M,ve),_(n,ge,g),_(n,I,g),e(I,re),e(re,oe),$(E,oe,null),e(I,Te),e(I,Q),e(Q,$e),_(n,_e,g),_(n,O,g),e(O,ye),e(O,ae),e(ae,H),e(O,De),e(O,ie),e(ie,K),e(O,Fe),e(O,de),e(de,V),e(O,Ee),_(n,be,g),_(n,ee,g),e(ee,j),_(n,q,g),_(n,le,g),e(le,U),e(U,Be),_(n,ke,g),_(n,W,g),e(W,xe),_(n,we,g),_(n,C,g),e(C,se),e(se,J),e(se,ce),e(ce,Me),e(se,G),e(se,pe),e(pe,ze),e(se,S),e(se,he),e(he,X),e(se,je),e(C,ne),e(C,P),e(P,Ce),e(P,A),e(A,Pe),e(P,qe),_(n,w,g),_(n,x,g),e(x,He),e(x,Z),e(Z,Ke),e(x,Ve),e(x,z),e(z,Je),e(x,Ge),e(x,Le),e(Le,Xe),e(x,Ye),_(n,L,g),_(n,N,g),e(N,Oe),e(Oe,Re),$(R,Re,null),e(N,Ze),e(N,We),e(We,Ae),_(n,Ue,g),_(n,te,g),e(te,et),_(n,gu,g),$(Er,n,g),_(n,_u,g),_(n,Ie,g),e(Ie,Br),e(Br,Ng),e(Br,xr),e(xr,Rg),e(Br,Wg),e(Ie,Qg),e(Ie,Mr),e(Mr,Ug),e(Mr,zr),e(zr,Hg),e(Mr,Kg),e(Ie,Vg),e(Ie,jr),e(jr,Jg),e(jr,Cr),e(Cr,Gg),e(jr,Xg),e(Ie,Yg),e(Ie,Pr),e(Pr,Zg),e(Pr,qr),e(qr,e_),e(Pr,t_),e(Ie,o_),e(Ie,Ar),e(Ar,s_),e(Ar,Or),e(Or,n_),e(Ar,r_),e(Ie,a_),e(Ie,Lr),e(Lr,i_),e(Lr,Ir),e(Ir,l_),e(Lr,d_),e(Ie,c_),e(Ie,Sr),e(Sr,p_),e(Sr,Nr),e(Nr,h_),e(Sr,f_),e(Ie,u_),e(Ie,fo),e(fo,Nd),e(Nd,m_),e(fo,g_),e(fo,Rr),e(Rr,__),e(fo,b_),e(fo,Wr),e(Wr,k_),e(fo,w_),e(Ie,v_),e(Ie,uo),e(uo,Rd),e(Rd,T_),e(uo,$_),e(uo,Qr),e(Qr,y_),e(uo,D_),e(uo,Ur),e(Ur,F_),e(uo,E_),e(Ie,B_),e(Ie,mo),e(mo,Wd),e(Wd,x_),e(mo,M_),e(mo,Hr),e(Hr,z_),e(mo,j_),e(mo,Kr),e(Kr,C_),e(mo,P_),_(n,bu,g),$(Vr,n,g),_(n,ku,g),_(n,Ot,g),e(Ot,go),e(go,Qd),e(Qd,q_),e(go,A_),e(go,Jr),e(Jr,O_),e(go,L_),e(go,Gr),e(Gr,I_),e(go,S_),e(Ot,N_),e(Ot,_o),e(_o,Ud),e(Ud,R_),e(_o,W_),e(_o,Xr),e(Xr,Q_),e(_o,U_),e(_o,Yr),e(Yr,H_),e(_o,K_),e(Ot,V_),e(Ot,zs),e(zs,Hd),e(Hd,J_),e(zs,G_),e(zs,Zr),e(Zr,X_),e(zs,Y_),e(Ot,Z_),e(Ot,Kd),e(Kd,ea),e(ea,eb),e(Kd,tb),_(n,wu,g),$(ta,n,g),_(n,vu,g),_(n,Lt,g),e(Lt,bo),e(bo,Vd),e(Vd,ob),e(bo,sb),e(bo,oa),e(oa,nb),e(bo,rb),e(bo,sa),e(sa,ab),e(bo,ib),e(Lt,lb),e(Lt,ko),e(ko,Jd),e(Jd,db),e(ko,cb),e(ko,na),e(na,pb),e(ko,hb),e(ko,ra),e(ra,fb),e(ko,ub),e(Lt,mb),e(Lt,wo),e(wo,Gd),e(Gd,gb),e(wo,_b),e(wo,aa),e(aa,bb),e(wo,kb),e(wo,ia),e(ia,wb),e(wo,vb),e(Lt,Tb),e(Lt,Xd),e(Xd,la),e(la,$b),e(Xd,yb),_(n,Tu,g),$(da,n,g),_(n,$u,g),_(n,It,g),e(It,vo),e(vo,Yd),e(Yd,Db),e(vo,Fb),e(vo,ca),e(ca,Eb),e(vo,Bb),e(vo,pa),e(pa,xb),e(vo,Mb),e(It,zb),e(It,To),e(To,Zd),e(Zd,jb),e(To,Cb),e(To,ha),e(ha,Pb),e(To,qb),e(To,fa),e(fa,Ab),e(To,Ob),e(It,Lb),e(It,js),e(js,ec),e(ec,Ib),e(js,Sb),e(js,ua),e(ua,Nb),e(js,Rb),e(It,Wb),e(It,tc),e(tc,ma),e(ma,Qb),e(tc,Ub),_(n,yu,g),_(n,oc,g),e(oc,bp),e(bp,Hb),_(n,Du,g),_(n,Cs,g),e(Cs,$o),e($o,sc),e(sc,Kb),e($o,Vb),e($o,ga),e(ga,Jb),e($o,Gb),e($o,_a),e(_a,Xb),e($o,Yb),e(Cs,Zb),e(Cs,yo),e(yo,nc),e(nc,ek),e(yo,tk),e(yo,ba),e(ba,ok),e(yo,sk),e(yo,ka),e(ka,nk),e(yo,rk),_(n,Fu,g),_(n,rc,g),e(rc,ak),_(n,Eu,g),_(n,Do,g),e(Do,wa),e(wa,ik),e(wa,va),e(va,lk),e(wa,dk),e(Do,ck),e(Do,Ta),e(Ta,pk),e(Ta,$a),e($a,hk),e(Ta,fk),e(Do,uk),e(Do,ya),e(ya,mk),e(ya,Da),e(Da,gk),e(ya,_k),_(n,Bu,g),_(n,ac,g),e(ac,bk),_(n,xu,g),_(n,Ps,g),e(Ps,Fa),e(Fa,kk),e(Fa,Ea),e(Ea,wk),e(Fa,vk),e(Ps,Tk),e(Ps,Ba),e(Ba,$k),e(Ba,xa),e(xa,yk),e(Ba,Dk),_(n,Mu,g),_(n,ic,g),e(ic,Fk),_(n,zu,g),_(n,Fo,g),e(Fo,Ma),e(Ma,Ek),e(Ma,za),e(za,Bk),e(Ma,xk),e(Fo,Mk),e(Fo,ja),e(ja,zk),e(ja,Ca),e(Ca,jk),e(ja,Ck),e(Fo,Pk),e(Fo,Pa),e(Pa,qk),e(Pa,qa),e(qa,Ak),e(Pa,Ok),_(n,ju,g),_(n,Ro,g),e(Ro,qs),e(qs,kp),$(Aa,kp,null),e(Ro,Lk),e(Ro,wp),e(wp,Ik),_(n,Cu,g),_(n,Pt,g),$(Oa,Pt,null),e(Pt,Sk),e(Pt,oo),e(oo,Nk),e(oo,lc),e(lc,Rk),e(oo,Wk),e(oo,dc),e(dc,Qk),e(oo,Uk),e(oo,La),e(La,Hk),e(oo,Kk),e(Pt,Vk),e(Pt,Wo),e(Wo,Jk),e(Wo,cc),e(cc,Gk),e(Wo,Xk),e(Wo,pc),e(pc,Yk),e(Wo,Zk),e(Pt,ew),$(As,Pt,null),_(n,Pu,g),_(n,Qo,g),e(Qo,Os),e(Os,vp),$(Ia,vp,null),e(Qo,tw),e(Qo,Tp),e(Tp,ow),_(n,qu,g),_(n,qt,g),$(Sa,qt,null),e(qt,sw),e(qt,$p),e($p,nw),e(qt,rw),e(qt,Ls),e(Ls,hc),e(hc,aw),e(Ls,iw),e(Ls,fc),e(fc,lw),e(Ls,dw),e(qt,cw),e(qt,Na),e(Na,pw),e(Na,uc),e(uc,hw),e(Na,fw),_(n,Au,g),_(n,Uo,g),e(Uo,Is),e(Is,yp),$(Ra,yp,null),e(Uo,uw),e(Uo,Dp),e(Dp,mw),_(n,Ou,g),_(n,At,g),$(Wa,At,null),e(At,gw),e(At,Qa),e(Qa,_w),e(Qa,Fp),e(Fp,bw),e(Qa,kw),e(At,ww),e(At,Ss),e(Ss,mc),e(mc,vw),e(Ss,Tw),e(Ss,gc),e(gc,$w),e(Ss,yw),e(At,Dw),e(At,Ua),e(Ua,Fw),e(Ua,_c),e(_c,Ew),e(Ua,Bw),_(n,Lu,g),_(n,Ho,g),e(Ho,Ns),e(Ns,Ep),$(Ha,Ep,null),e(Ho,xw),e(Ho,Bp),e(Bp,Mw),_(n,Iu,g),_(n,ut,g),$(Ka,ut,null),e(ut,zw),e(ut,xp),e(xp,jw),e(ut,Cw),e(ut,Va),e(Va,Pw),e(Va,bc),e(bc,qw),e(Va,Aw),e(ut,Ow),e(ut,Ja),e(Ja,Lw),e(Ja,Ga),e(Ga,Iw),e(Ja,Sw),e(ut,Nw),e(ut,St),$(Xa,St,null),e(St,Rw),e(St,Ko),e(Ko,Ww),e(Ko,kc),e(kc,Qw),e(Ko,Uw),e(Ko,Mp),e(Mp,Hw),e(Ko,Kw),e(St,Vw),$(Rs,St,null),e(St,Jw),$(Ws,St,null),_(n,Su,g),_(n,Vo,g),e(Vo,Qs),e(Qs,zp),$(Ya,zp,null),e(Vo,Gw),e(Vo,jp),e(jp,Xw),_(n,Nu,g),_(n,mt,g),$(Za,mt,null),e(mt,Yw),e(mt,ei),e(ei,Zw),e(ei,Cp),e(Cp,ev),e(ei,tv),e(mt,ov),e(mt,ti),e(ti,sv),e(ti,wc),e(wc,nv),e(ti,rv),e(mt,av),e(mt,oi),e(oi,iv),e(oi,si),e(si,lv),e(oi,dv),e(mt,cv),e(mt,wt),$(ni,wt,null),e(wt,pv),e(wt,Jo),e(Jo,hv),e(Jo,vc),e(vc,fv),e(Jo,uv),e(Jo,Pp),e(Pp,mv),e(Jo,gv),e(wt,_v),$(Us,wt,null),e(wt,bv),$(Hs,wt,null),e(wt,kv),$(Ks,wt,null),_(n,Ru,g),_(n,Go,g),e(Go,Vs),e(Vs,qp),$(ri,qp,null),e(Go,wv),e(Go,Ap),e(Ap,vv),_(n,Wu,g),_(n,gt,g),$(ai,gt,null),e(gt,Tv),e(gt,Op),e(Op,$v),e(gt,yv),e(gt,ii),e(ii,Dv),e(ii,Tc),e(Tc,Fv),e(ii,Ev),e(gt,Bv),e(gt,li),e(li,xv),e(li,di),e(di,Mv),e(li,zv),e(gt,jv),e(gt,it),$(ci,it,null),e(it,Cv),e(it,Xo),e(Xo,Pv),e(Xo,$c),e($c,qv),e(Xo,Av),e(Xo,Lp),e(Lp,Ov),e(Xo,Lv),e(it,Iv),$(Js,it,null),e(it,Sv),$(Gs,it,null),e(it,Nv),$(Xs,it,null),e(it,Rv),$(Ys,it,null),e(it,Wv),$(Zs,it,null),_(n,Qu,g),_(n,Yo,g),e(Yo,en),e(en,Ip),$(pi,Ip,null),e(Yo,Qv),e(Yo,Sp),e(Sp,Uv),_(n,Uu,g),_(n,_t,g),$(hi,_t,null),e(_t,Hv),e(_t,Np),e(Np,Kv),e(_t,Vv),e(_t,fi),e(fi,Jv),e(fi,yc),e(yc,Gv),e(fi,Xv),e(_t,Yv),e(_t,ui),e(ui,Zv),e(ui,mi),e(mi,eT),e(ui,tT),e(_t,oT),e(_t,Nt),$(gi,Nt,null),e(Nt,sT),e(Nt,Zo),e(Zo,nT),e(Zo,Dc),e(Dc,rT),e(Zo,aT),e(Zo,Rp),e(Rp,iT),e(Zo,lT),e(Nt,dT),$(tn,Nt,null),e(Nt,cT),$(on,Nt,null),_(n,Hu,g),_(n,es,g),e(es,sn),e(sn,Wp),$(_i,Wp,null),e(es,pT),e(es,Qp),e(Qp,hT),_(n,Ku,g),_(n,bt,g),$(bi,bt,null),e(bt,fT),e(bt,Up),e(Up,uT),e(bt,mT),e(bt,ki),e(ki,gT),e(ki,Fc),e(Fc,_T),e(ki,bT),e(bt,kT),e(bt,wi),e(wi,wT),e(wi,vi),e(vi,vT),e(wi,TT),e(bt,$T),e(bt,vt),$(Ti,vt,null),e(vt,yT),e(vt,ts),e(ts,DT),e(ts,Ec),e(Ec,FT),e(ts,ET),e(ts,Hp),e(Hp,BT),e(ts,xT),e(vt,MT),$(nn,vt,null),e(vt,zT),$(rn,vt,null),e(vt,jT),$(an,vt,null),_(n,Vu,g),_(n,os,g),e(os,ln),e(ln,Kp),$($i,Kp,null),e(os,CT),e(os,Vp),e(Vp,PT),_(n,Ju,g),_(n,kt,g),$(yi,kt,null),e(kt,qT),e(kt,ss),e(ss,AT),e(ss,Jp),e(Jp,OT),e(ss,LT),e(ss,Gp),e(Gp,IT),e(ss,ST),e(kt,NT),e(kt,Di),e(Di,RT),e(Di,Bc),e(Bc,WT),e(Di,QT),e(kt,UT),e(kt,Fi),e(Fi,HT),e(Fi,Ei),e(Ei,KT),e(Fi,VT),e(kt,JT),e(kt,Tt),$(Bi,Tt,null),e(Tt,GT),e(Tt,ns),e(ns,XT),e(ns,xc),e(xc,YT),e(ns,ZT),e(ns,Xp),e(Xp,e$),e(ns,t$),e(Tt,o$),$(dn,Tt,null),e(Tt,s$),$(cn,Tt,null),e(Tt,n$),$(pn,Tt,null),_(n,Gu,g),_(n,rs,g),e(rs,hn),e(hn,Yp),$(xi,Yp,null),e(rs,r$),e(rs,Zp),e(Zp,a$),_(n,Xu,g),_(n,lt,g),$(Mi,lt,null),e(lt,i$),e(lt,eh),e(eh,l$),e(lt,d$),e(lt,zi),e(zi,c$),e(zi,Mc),e(Mc,p$),e(zi,h$),e(lt,f$),e(lt,ji),e(ji,u$),e(ji,Ci),e(Ci,m$),e(ji,g$),e(lt,_$),$(fn,lt,null),e(lt,b$),e(lt,Rt),$(Pi,Rt,null),e(Rt,k$),e(Rt,as),e(as,w$),e(as,zc),e(zc,v$),e(as,T$),e(as,th),e(th,$$),e(as,y$),e(Rt,D$),$(un,Rt,null),e(Rt,F$),$(mn,Rt,null),_(n,Yu,g),_(n,is,g),e(is,gn),e(gn,oh),$(qi,oh,null),e(is,E$),e(is,sh),e(sh,B$),_(n,Zu,g),_(n,dt,g),$(Ai,dt,null),e(dt,x$),e(dt,Oi),e(Oi,M$),e(Oi,nh),e(nh,z$),e(Oi,j$),e(dt,C$),e(dt,Li),e(Li,P$),e(Li,jc),e(jc,q$),e(Li,A$),e(dt,O$),e(dt,Ii),e(Ii,L$),e(Ii,Si),e(Si,I$),e(Ii,S$),e(dt,N$),$(_n,dt,null),e(dt,R$),e(dt,$t),$(Ni,$t,null),e($t,W$),e($t,ls),e(ls,Q$),e(ls,Cc),e(Cc,U$),e(ls,H$),e(ls,rh),e(rh,K$),e(ls,V$),e($t,J$),$(bn,$t,null),e($t,G$),$(kn,$t,null),e($t,X$),$(wn,$t,null),_(n,em,g),_(n,ds,g),e(ds,vn),e(vn,ah),$(Ri,ah,null),e(ds,Y$),e(ds,ih),e(ih,Z$),_(n,tm,g),_(n,ct,g),$(Wi,ct,null),e(ct,ey),e(ct,lh),e(lh,ty),e(ct,oy),e(ct,Qi),e(Qi,sy),e(Qi,Pc),e(Pc,ny),e(Qi,ry),e(ct,ay),e(ct,Ui),e(Ui,iy),e(Ui,Hi),e(Hi,ly),e(Ui,dy),e(ct,cy),$(Tn,ct,null),e(ct,py),e(ct,yt),$(Ki,yt,null),e(yt,hy),e(yt,cs),e(cs,fy),e(cs,qc),e(qc,uy),e(cs,my),e(cs,dh),e(dh,gy),e(cs,_y),e(yt,by),$($n,yt,null),e(yt,ky),$(yn,yt,null),e(yt,wy),$(Dn,yt,null),_(n,om,g),_(n,ps,g),e(ps,Fn),e(Fn,ch),$(Vi,ch,null),e(ps,vy),e(ps,ph),e(ph,Ty),_(n,sm,g),_(n,pt,g),$(Ji,pt,null),e(pt,$y),e(pt,hh),e(hh,yy),e(pt,Dy),e(pt,Gi),e(Gi,Fy),e(Gi,Ac),e(Ac,Ey),e(Gi,By),e(pt,xy),e(pt,Xi),e(Xi,My),e(Xi,Yi),e(Yi,zy),e(Xi,jy),e(pt,Cy),$(En,pt,null),e(pt,Py),e(pt,Wt),$(Zi,Wt,null),e(Wt,qy),e(Wt,hs),e(hs,Ay),e(hs,Oc),e(Oc,Oy),e(hs,Ly),e(hs,fh),e(fh,Iy),e(hs,Sy),e(Wt,Ny),$(Bn,Wt,null),e(Wt,Ry),$(xn,Wt,null),_(n,nm,g),_(n,fs,g),e(fs,Mn),e(Mn,uh),$(el,uh,null),e(fs,Wy),e(fs,mh),e(mh,Qy),_(n,rm,g),_(n,ht,g),$(tl,ht,null),e(ht,Uy),e(ht,gh),e(gh,Hy),e(ht,Ky),e(ht,ol),e(ol,Vy),e(ol,Lc),e(Lc,Jy),e(ol,Gy),e(ht,Xy),e(ht,sl),e(sl,Yy),e(sl,nl),e(nl,Zy),e(sl,e1),e(ht,t1),$(zn,ht,null),e(ht,o1),e(ht,Dt),$(rl,Dt,null),e(Dt,s1),e(Dt,us),e(us,n1),e(us,Ic),e(Ic,r1),e(us,a1),e(us,_h),e(_h,i1),e(us,l1),e(Dt,d1),$(jn,Dt,null),e(Dt,c1),$(Cn,Dt,null),e(Dt,p1),$(Pn,Dt,null),_(n,am,g),_(n,ms,g),e(ms,qn),e(qn,bh),$(al,bh,null),e(ms,h1),e(ms,kh),e(kh,f1),_(n,im,g),_(n,ft,g),$(il,ft,null),e(ft,u1),e(ft,gs),e(gs,m1),e(gs,wh),e(wh,g1),e(gs,_1),e(gs,vh),e(vh,b1),e(gs,k1),e(ft,w1),e(ft,ll),e(ll,v1),e(ll,Sc),e(Sc,T1),e(ll,$1),e(ft,y1),e(ft,dl),e(dl,D1),e(dl,cl),e(cl,F1),e(dl,E1),e(ft,B1),$(An,ft,null),e(ft,x1),e(ft,Ft),$(pl,Ft,null),e(Ft,M1),e(Ft,_s),e(_s,z1),e(_s,Nc),e(Nc,j1),e(_s,C1),e(_s,Th),e(Th,P1),e(_s,q1),e(Ft,A1),$(On,Ft,null),e(Ft,O1),$(Ln,Ft,null),e(Ft,L1),$(In,Ft,null),_(n,lm,g),_(n,bs,g),e(bs,Sn),e(Sn,$h),$(hl,$h,null),e(bs,I1),e(bs,yh),e(yh,S1),_(n,dm,g),_(n,tt,g),$(fl,tt,null),e(tt,N1),e(tt,Dh),e(Dh,R1),e(tt,W1),e(tt,ul),e(ul,Q1),e(ul,Rc),e(Rc,U1),e(ul,H1),e(tt,K1),e(tt,ml),e(ml,V1),e(ml,gl),e(gl,J1),e(ml,G1),e(tt,X1),e(tt,Fh),e(Fh,Y1),e(tt,Z1),e(tt,so),e(so,Eh),e(Eh,_l),e(_l,e2),e(so,t2),e(so,Bh),e(Bh,bl),e(bl,o2),e(so,s2),e(so,xh),e(xh,kl),e(kl,n2),e(so,r2),e(so,Mh),e(Mh,wl),e(wl,a2),e(tt,i2),e(tt,Qt),$(vl,Qt,null),e(Qt,l2),e(Qt,ks),e(ks,d2),e(ks,zh),e(zh,c2),e(ks,p2),e(ks,jh),e(jh,h2),e(ks,f2),e(Qt,u2),$(Nn,Qt,null),e(Qt,m2),$(Rn,Qt,null),_(n,cm,g),_(n,ws,g),e(ws,Wn),e(Wn,Ch),$(Tl,Ch,null),e(ws,g2),e(ws,Ph),e(Ph,_2),_(n,pm,g),_(n,ot,g),$($l,ot,null),e(ot,b2),e(ot,yl),e(yl,k2),e(yl,qh),e(qh,w2),e(yl,v2),e(ot,T2),e(ot,Dl),e(Dl,$2),e(Dl,Wc),e(Wc,y2),e(Dl,D2),e(ot,F2),e(ot,Fl),e(Fl,E2),e(Fl,El),e(El,B2),e(Fl,x2),e(ot,M2),e(ot,Ah),e(Ah,z2),e(ot,j2),e(ot,no),e(no,Oh),e(Oh,Bl),e(Bl,C2),e(no,P2),e(no,Lh),e(Lh,xl),e(xl,q2),e(no,A2),e(no,Ih),e(Ih,Ml),e(Ml,O2),e(no,L2),e(no,Sh),e(Sh,zl),e(zl,I2),e(ot,S2),e(ot,Ut),$(jl,Ut,null),e(Ut,N2),e(Ut,vs),e(vs,R2),e(vs,Nh),e(Nh,W2),e(vs,Q2),e(vs,Rh),e(Rh,U2),e(vs,H2),e(Ut,K2),$(Qn,Ut,null),e(Ut,V2),$(Un,Ut,null),_(n,hm,g),_(n,Ts,g),e(Ts,Hn),e(Hn,Wh),$(Cl,Wh,null),e(Ts,J2),e(Ts,Qh),e(Qh,G2),_(n,fm,g),_(n,st,g),$(Pl,st,null),e(st,X2),e(st,Uh),e(Uh,Y2),e(st,Z2),e(st,ql),e(ql,eD),e(ql,Qc),e(Qc,tD),e(ql,oD),e(st,sD),e(st,Al),e(Al,nD),e(Al,Ol),e(Ol,rD),e(Al,aD),e(st,iD),e(st,Hh),e(Hh,lD),e(st,dD),e(st,ro),e(ro,Kh),e(Kh,Ll),e(Ll,cD),e(ro,pD),e(ro,Vh),e(Vh,Il),e(Il,hD),e(ro,fD),e(ro,Jh),e(Jh,Sl),e(Sl,uD),e(ro,mD),e(ro,Gh),e(Gh,Nl),e(Nl,gD),e(st,_D),e(st,Ht),$(Rl,Ht,null),e(Ht,bD),e(Ht,$s),e($s,kD),e($s,Xh),e(Xh,wD),e($s,vD),e($s,Yh),e(Yh,TD),e($s,$D),e(Ht,yD),$(Kn,Ht,null),e(Ht,DD),$(Vn,Ht,null),_(n,um,g),_(n,ys,g),e(ys,Jn),e(Jn,Zh),$(Wl,Zh,null),e(ys,FD),e(ys,ef),e(ef,ED),_(n,mm,g),_(n,nt,g),$(Ql,nt,null),e(nt,BD),e(nt,tf),e(tf,xD),e(nt,MD),e(nt,Ul),e(Ul,zD),e(Ul,Uc),e(Uc,jD),e(Ul,CD),e(nt,PD),e(nt,Hl),e(Hl,qD),e(Hl,Kl),e(Kl,AD),e(Hl,OD),e(nt,LD),e(nt,of),e(of,ID),e(nt,SD),e(nt,ao),e(ao,sf),e(sf,Vl),e(Vl,ND),e(ao,RD),e(ao,nf),e(nf,Jl),e(Jl,WD),e(ao,QD),e(ao,rf),e(rf,Gl),e(Gl,UD),e(ao,HD),e(ao,af),e(af,Xl),e(Xl,KD),e(nt,VD),e(nt,Kt),$(Yl,Kt,null),e(Kt,JD),e(Kt,Ds),e(Ds,GD),e(Ds,lf),e(lf,XD),e(Ds,YD),e(Ds,df),e(df,ZD),e(Ds,eF),e(Kt,tF),$(Gn,Kt,null),e(Kt,oF),$(Xn,Kt,null),_(n,gm,g),_(n,Fs,g),e(Fs,Yn),e(Yn,cf),$(Zl,cf,null),e(Fs,sF),e(Fs,pf),e(pf,nF),_(n,_m,g),_(n,rt,g),$(ed,rt,null),e(rt,rF),e(rt,hf),e(hf,aF),e(rt,iF),e(rt,td),e(td,lF),e(td,Hc),e(Hc,dF),e(td,cF),e(rt,pF),e(rt,od),e(od,hF),e(od,sd),e(sd,fF),e(od,uF),e(rt,mF),e(rt,ff),e(ff,gF),e(rt,_F),e(rt,io),e(io,uf),e(uf,nd),e(nd,bF),e(io,kF),e(io,mf),e(mf,rd),e(rd,wF),e(io,vF),e(io,gf),e(gf,ad),e(ad,TF),e(io,$F),e(io,_f),e(_f,id),e(id,yF),e(rt,DF),e(rt,Vt),$(ld,Vt,null),e(Vt,FF),e(Vt,Es),e(Es,EF),e(Es,bf),e(bf,BF),e(Es,xF),e(Es,kf),e(kf,MF),e(Es,zF),e(Vt,jF),$(Zn,Vt,null),e(Vt,CF),$(er,Vt,null),_(n,bm,g),_(n,Bs,g),e(Bs,tr),e(tr,wf),$(dd,wf,null),e(Bs,PF),e(Bs,vf),e(vf,qF),_(n,km,g),_(n,at,g),$(cd,at,null),e(at,AF),e(at,xs),e(xs,OF),e(xs,Tf),e(Tf,LF),e(xs,IF),e(xs,$f),e($f,SF),e(xs,NF),e(at,RF),e(at,pd),e(pd,WF),e(pd,Kc),e(Kc,QF),e(pd,UF),e(at,HF),e(at,hd),e(hd,KF),e(hd,fd),e(fd,VF),e(hd,JF),e(at,GF),e(at,yf),e(yf,XF),e(at,YF),e(at,lo),e(lo,Df),e(Df,ud),e(ud,ZF),e(lo,eE),e(lo,Ff),e(Ff,md),e(md,tE),e(lo,oE),e(lo,Ef),e(Ef,gd),e(gd,sE),e(lo,nE),e(lo,Bf),e(Bf,_d),e(_d,rE),e(at,aE),e(at,Jt),$(bd,Jt,null),e(Jt,iE),e(Jt,Ms),e(Ms,lE),e(Ms,xf),e(xf,dE),e(Ms,cE),e(Ms,Mf),e(Mf,pE),e(Ms,hE),e(Jt,fE),$(or,Jt,null),e(Jt,uE),$(sr,Jt,null),wm=!0},p(n,[g]){const kd={};g&2&&(kd.$$scope={dirty:g,ctx:n}),As.$set(kd);const zf={};g&2&&(zf.$$scope={dirty:g,ctx:n}),Rs.$set(zf);const jf={};g&2&&(jf.$$scope={dirty:g,ctx:n}),Ws.$set(jf);const Cf={};g&2&&(Cf.$$scope={dirty:g,ctx:n}),Us.$set(Cf);const wd={};g&2&&(wd.$$scope={dirty:g,ctx:n}),Hs.$set(wd);const Pf={};g&2&&(Pf.$$scope={dirty:g,ctx:n}),Ks.$set(Pf);const qf={};g&2&&(qf.$$scope={dirty:g,ctx:n}),Js.$set(qf);const Af={};g&2&&(Af.$$scope={dirty:g,ctx:n}),Gs.$set(Af);const co={};g&2&&(co.$$scope={dirty:g,ctx:n}),Xs.$set(co);const Of={};g&2&&(Of.$$scope={dirty:g,ctx:n}),Ys.$set(Of);const Lf={};g&2&&(Lf.$$scope={dirty:g,ctx:n}),Zs.$set(Lf);const If={};g&2&&(If.$$scope={dirty:g,ctx:n}),tn.$set(If);const Sf={};g&2&&(Sf.$$scope={dirty:g,ctx:n}),on.$set(Sf);const Nf={};g&2&&(Nf.$$scope={dirty:g,ctx:n}),nn.$set(Nf);const Rf={};g&2&&(Rf.$$scope={dirty:g,ctx:n}),rn.$set(Rf);const Wf={};g&2&&(Wf.$$scope={dirty:g,ctx:n}),an.$set(Wf);const vd={};g&2&&(vd.$$scope={dirty:g,ctx:n}),dn.$set(vd);const po={};g&2&&(po.$$scope={dirty:g,ctx:n}),cn.$set(po);const Qf={};g&2&&(Qf.$$scope={dirty:g,ctx:n}),pn.$set(Qf);const Uf={};g&2&&(Uf.$$scope={dirty:g,ctx:n}),fn.$set(Uf);const Hf={};g&2&&(Hf.$$scope={dirty:g,ctx:n}),un.$set(Hf);const Td={};g&2&&(Td.$$scope={dirty:g,ctx:n}),mn.$set(Td);const Kf={};g&2&&(Kf.$$scope={dirty:g,ctx:n}),_n.$set(Kf);const ho={};g&2&&(ho.$$scope={dirty:g,ctx:n}),bn.$set(ho);const Vf={};g&2&&(Vf.$$scope={dirty:g,ctx:n}),kn.$set(Vf);const Jf={};g&2&&(Jf.$$scope={dirty:g,ctx:n}),wn.$set(Jf);const Gf={};g&2&&(Gf.$$scope={dirty:g,ctx:n}),Tn.$set(Gf);const $d={};g&2&&($d.$$scope={dirty:g,ctx:n}),$n.$set($d);const Xf={};g&2&&(Xf.$$scope={dirty:g,ctx:n}),yn.$set(Xf);const Yf={};g&2&&(Yf.$$scope={dirty:g,ctx:n}),Dn.$set(Yf);const Zf={};g&2&&(Zf.$$scope={dirty:g,ctx:n}),En.$set(Zf);const eu={};g&2&&(eu.$$scope={dirty:g,ctx:n}),Bn.$set(eu);const Qe={};g&2&&(Qe.$$scope={dirty:g,ctx:n}),xn.$set(Qe);const yd={};g&2&&(yd.$$scope={dirty:g,ctx:n}),zn.$set(yd);const tu={};g&2&&(tu.$$scope={dirty:g,ctx:n}),jn.$set(tu);const Dd={};g&2&&(Dd.$$scope={dirty:g,ctx:n}),Cn.$set(Dd);const ou={};g&2&&(ou.$$scope={dirty:g,ctx:n}),Pn.$set(ou);const Fd={};g&2&&(Fd.$$scope={dirty:g,ctx:n}),An.$set(Fd);const su={};g&2&&(su.$$scope={dirty:g,ctx:n}),On.$set(su);const Ed={};g&2&&(Ed.$$scope={dirty:g,ctx:n}),Ln.$set(Ed);const nu={};g&2&&(nu.$$scope={dirty:g,ctx:n}),In.$set(nu);const Bd={};g&2&&(Bd.$$scope={dirty:g,ctx:n}),Nn.$set(Bd);const ru={};g&2&&(ru.$$scope={dirty:g,ctx:n}),Rn.$set(ru);const xd={};g&2&&(xd.$$scope={dirty:g,ctx:n}),Qn.$set(xd);const au={};g&2&&(au.$$scope={dirty:g,ctx:n}),Un.$set(au);const Md={};g&2&&(Md.$$scope={dirty:g,ctx:n}),Kn.$set(Md);const iu={};g&2&&(iu.$$scope={dirty:g,ctx:n}),Vn.$set(iu);const Eo={};g&2&&(Eo.$$scope={dirty:g,ctx:n}),Gn.$set(Eo);const lu={};g&2&&(lu.$$scope={dirty:g,ctx:n}),Xn.$set(lu);const du={};g&2&&(du.$$scope={dirty:g,ctx:n}),Zn.$set(du);const cu={};g&2&&(cu.$$scope={dirty:g,ctx:n}),er.$set(cu);const Bo={};g&2&&(Bo.$$scope={dirty:g,ctx:n}),or.$set(Bo);const pu={};g&2&&(pu.$$scope={dirty:g,ctx:n}),sr.$set(pu)},i(n){wm||(y(l.$$.fragment,n),y(E.$$.fragment,n),y(R.$$.fragment,n),y(Er.$$.fragment,n),y(Vr.$$.fragment,n),y(ta.$$.fragment,n),y(da.$$.fragment,n),y(Aa.$$.fragment,n),y(Oa.$$.fragment,n),y(As.$$.fragment,n),y(Ia.$$.fragment,n),y(Sa.$$.fragment,n),y(Ra.$$.fragment,n),y(Wa.$$.fragment,n),y(Ha.$$.fragment,n),y(Ka.$$.fragment,n),y(Xa.$$.fragment,n),y(Rs.$$.fragment,n),y(Ws.$$.fragment,n),y(Ya.$$.fragment,n),y(Za.$$.fragment,n),y(ni.$$.fragment,n),y(Us.$$.fragment,n),y(Hs.$$.fragment,n),y(Ks.$$.fragment,n),y(ri.$$.fragment,n),y(ai.$$.fragment,n),y(ci.$$.fragment,n),y(Js.$$.fragment,n),y(Gs.$$.fragment,n),y(Xs.$$.fragment,n),y(Ys.$$.fragment,n),y(Zs.$$.fragment,n),y(pi.$$.fragment,n),y(hi.$$.fragment,n),y(gi.$$.fragment,n),y(tn.$$.fragment,n),y(on.$$.fragment,n),y(_i.$$.fragment,n),y(bi.$$.fragment,n),y(Ti.$$.fragment,n),y(nn.$$.fragment,n),y(rn.$$.fragment,n),y(an.$$.fragment,n),y($i.$$.fragment,n),y(yi.$$.fragment,n),y(Bi.$$.fragment,n),y(dn.$$.fragment,n),y(cn.$$.fragment,n),y(pn.$$.fragment,n),y(xi.$$.fragment,n),y(Mi.$$.fragment,n),y(fn.$$.fragment,n),y(Pi.$$.fragment,n),y(un.$$.fragment,n),y(mn.$$.fragment,n),y(qi.$$.fragment,n),y(Ai.$$.fragment,n),y(_n.$$.fragment,n),y(Ni.$$.fragment,n),y(bn.$$.fragment,n),y(kn.$$.fragment,n),y(wn.$$.fragment,n),y(Ri.$$.fragment,n),y(Wi.$$.fragment,n),y(Tn.$$.fragment,n),y(Ki.$$.fragment,n),y($n.$$.fragment,n),y(yn.$$.fragment,n),y(Dn.$$.fragment,n),y(Vi.$$.fragment,n),y(Ji.$$.fragment,n),y(En.$$.fragment,n),y(Zi.$$.fragment,n),y(Bn.$$.fragment,n),y(xn.$$.fragment,n),y(el.$$.fragment,n),y(tl.$$.fragment,n),y(zn.$$.fragment,n),y(rl.$$.fragment,n),y(jn.$$.fragment,n),y(Cn.$$.fragment,n),y(Pn.$$.fragment,n),y(al.$$.fragment,n),y(il.$$.fragment,n),y(An.$$.fragment,n),y(pl.$$.fragment,n),y(On.$$.fragment,n),y(Ln.$$.fragment,n),y(In.$$.fragment,n),y(hl.$$.fragment,n),y(fl.$$.fragment,n),y(vl.$$.fragment,n),y(Nn.$$.fragment,n),y(Rn.$$.fragment,n),y(Tl.$$.fragment,n),y($l.$$.fragment,n),y(jl.$$.fragment,n),y(Qn.$$.fragment,n),y(Un.$$.fragment,n),y(Cl.$$.fragment,n),y(Pl.$$.fragment,n),y(Rl.$$.fragment,n),y(Kn.$$.fragment,n),y(Vn.$$.fragment,n),y(Wl.$$.fragment,n),y(Ql.$$.fragment,n),y(Yl.$$.fragment,n),y(Gn.$$.fragment,n),y(Xn.$$.fragment,n),y(Zl.$$.fragment,n),y(ed.$$.fragment,n),y(ld.$$.fragment,n),y(Zn.$$.fragment,n),y(er.$$.fragment,n),y(dd.$$.fragment,n),y(cd.$$.fragment,n),y(bd.$$.fragment,n),y(or.$$.fragment,n),y(sr.$$.fragment,n),wm=!0)},o(n){D(l.$$.fragment,n),D(E.$$.fragment,n),D(R.$$.fragment,n),D(Er.$$.fragment,n),D(Vr.$$.fragment,n),D(ta.$$.fragment,n),D(da.$$.fragment,n),D(Aa.$$.fragment,n),D(Oa.$$.fragment,n),D(As.$$.fragment,n),D(Ia.$$.fragment,n),D(Sa.$$.fragment,n),D(Ra.$$.fragment,n),D(Wa.$$.fragment,n),D(Ha.$$.fragment,n),D(Ka.$$.fragment,n),D(Xa.$$.fragment,n),D(Rs.$$.fragment,n),D(Ws.$$.fragment,n),D(Ya.$$.fragment,n),D(Za.$$.fragment,n),D(ni.$$.fragment,n),D(Us.$$.fragment,n),D(Hs.$$.fragment,n),D(Ks.$$.fragment,n),D(ri.$$.fragment,n),D(ai.$$.fragment,n),D(ci.$$.fragment,n),D(Js.$$.fragment,n),D(Gs.$$.fragment,n),D(Xs.$$.fragment,n),D(Ys.$$.fragment,n),D(Zs.$$.fragment,n),D(pi.$$.fragment,n),D(hi.$$.fragment,n),D(gi.$$.fragment,n),D(tn.$$.fragment,n),D(on.$$.fragment,n),D(_i.$$.fragment,n),D(bi.$$.fragment,n),D(Ti.$$.fragment,n),D(nn.$$.fragment,n),D(rn.$$.fragment,n),D(an.$$.fragment,n),D($i.$$.fragment,n),D(yi.$$.fragment,n),D(Bi.$$.fragment,n),D(dn.$$.fragment,n),D(cn.$$.fragment,n),D(pn.$$.fragment,n),D(xi.$$.fragment,n),D(Mi.$$.fragment,n),D(fn.$$.fragment,n),D(Pi.$$.fragment,n),D(un.$$.fragment,n),D(mn.$$.fragment,n),D(qi.$$.fragment,n),D(Ai.$$.fragment,n),D(_n.$$.fragment,n),D(Ni.$$.fragment,n),D(bn.$$.fragment,n),D(kn.$$.fragment,n),D(wn.$$.fragment,n),D(Ri.$$.fragment,n),D(Wi.$$.fragment,n),D(Tn.$$.fragment,n),D(Ki.$$.fragment,n),D($n.$$.fragment,n),D(yn.$$.fragment,n),D(Dn.$$.fragment,n),D(Vi.$$.fragment,n),D(Ji.$$.fragment,n),D(En.$$.fragment,n),D(Zi.$$.fragment,n),D(Bn.$$.fragment,n),D(xn.$$.fragment,n),D(el.$$.fragment,n),D(tl.$$.fragment,n),D(zn.$$.fragment,n),D(rl.$$.fragment,n),D(jn.$$.fragment,n),D(Cn.$$.fragment,n),D(Pn.$$.fragment,n),D(al.$$.fragment,n),D(il.$$.fragment,n),D(An.$$.fragment,n),D(pl.$$.fragment,n),D(On.$$.fragment,n),D(Ln.$$.fragment,n),D(In.$$.fragment,n),D(hl.$$.fragment,n),D(fl.$$.fragment,n),D(vl.$$.fragment,n),D(Nn.$$.fragment,n),D(Rn.$$.fragment,n),D(Tl.$$.fragment,n),D($l.$$.fragment,n),D(jl.$$.fragment,n),D(Qn.$$.fragment,n),D(Un.$$.fragment,n),D(Cl.$$.fragment,n),D(Pl.$$.fragment,n),D(Rl.$$.fragment,n),D(Kn.$$.fragment,n),D(Vn.$$.fragment,n),D(Wl.$$.fragment,n),D(Ql.$$.fragment,n),D(Yl.$$.fragment,n),D(Gn.$$.fragment,n),D(Xn.$$.fragment,n),D(Zl.$$.fragment,n),D(ed.$$.fragment,n),D(ld.$$.fragment,n),D(Zn.$$.fragment,n),D(er.$$.fragment,n),D(dd.$$.fragment,n),D(cd.$$.fragment,n),D(bd.$$.fragment,n),D(or.$$.fragment,n),D(sr.$$.fragment,n),wm=!1},d(n){t(d),n&&t(b),n&&t(f),F(l),n&&t(ge),n&&t(I),F(E),n&&t(_e),n&&t(O),n&&t(be),n&&t(ee),n&&t(q),n&&t(le),n&&t(ke),n&&t(W),n&&t(we),n&&t(C),n&&t(w),n&&t(x),n&&t(L),n&&t(N),F(R),n&&t(Ue),n&&t(te),n&&t(gu),F(Er,n),n&&t(_u),n&&t(Ie),n&&t(bu),F(Vr,n),n&&t(ku),n&&t(Ot),n&&t(wu),F(ta,n),n&&t(vu),n&&t(Lt),n&&t(Tu),F(da,n),n&&t($u),n&&t(It),n&&t(yu),n&&t(oc),n&&t(Du),n&&t(Cs),n&&t(Fu),n&&t(rc),n&&t(Eu),n&&t(Do),n&&t(Bu),n&&t(ac),n&&t(xu),n&&t(Ps),n&&t(Mu),n&&t(ic),n&&t(zu),n&&t(Fo),n&&t(ju),n&&t(Ro),F(Aa),n&&t(Cu),n&&t(Pt),F(Oa),F(As),n&&t(Pu),n&&t(Qo),F(Ia),n&&t(qu),n&&t(qt),F(Sa),n&&t(Au),n&&t(Uo),F(Ra),n&&t(Ou),n&&t(At),F(Wa),n&&t(Lu),n&&t(Ho),F(Ha),n&&t(Iu),n&&t(ut),F(Ka),F(Xa),F(Rs),F(Ws),n&&t(Su),n&&t(Vo),F(Ya),n&&t(Nu),n&&t(mt),F(Za),F(ni),F(Us),F(Hs),F(Ks),n&&t(Ru),n&&t(Go),F(ri),n&&t(Wu),n&&t(gt),F(ai),F(ci),F(Js),F(Gs),F(Xs),F(Ys),F(Zs),n&&t(Qu),n&&t(Yo),F(pi),n&&t(Uu),n&&t(_t),F(hi),F(gi),F(tn),F(on),n&&t(Hu),n&&t(es),F(_i),n&&t(Ku),n&&t(bt),F(bi),F(Ti),F(nn),F(rn),F(an),n&&t(Vu),n&&t(os),F($i),n&&t(Ju),n&&t(kt),F(yi),F(Bi),F(dn),F(cn),F(pn),n&&t(Gu),n&&t(rs),F(xi),n&&t(Xu),n&&t(lt),F(Mi),F(fn),F(Pi),F(un),F(mn),n&&t(Yu),n&&t(is),F(qi),n&&t(Zu),n&&t(dt),F(Ai),F(_n),F(Ni),F(bn),F(kn),F(wn),n&&t(em),n&&t(ds),F(Ri),n&&t(tm),n&&t(ct),F(Wi),F(Tn),F(Ki),F($n),F(yn),F(Dn),n&&t(om),n&&t(ps),F(Vi),n&&t(sm),n&&t(pt),F(Ji),F(En),F(Zi),F(Bn),F(xn),n&&t(nm),n&&t(fs),F(el),n&&t(rm),n&&t(ht),F(tl),F(zn),F(rl),F(jn),F(Cn),F(Pn),n&&t(am),n&&t(ms),F(al),n&&t(im),n&&t(ft),F(il),F(An),F(pl),F(On),F(Ln),F(In),n&&t(lm),n&&t(bs),F(hl),n&&t(dm),n&&t(tt),F(fl),F(vl),F(Nn),F(Rn),n&&t(cm),n&&t(ws),F(Tl),n&&t(pm),n&&t(ot),F($l),F(jl),F(Qn),F(Un),n&&t(hm),n&&t(Ts),F(Cl),n&&t(fm),n&&t(st),F(Pl),F(Rl),F(Kn),F(Vn),n&&t(um),n&&t(ys),F(Wl),n&&t(mm),n&&t(nt),F(Ql),F(Yl),F(Gn),F(Xn),n&&t(gm),n&&t(Fs),F(Zl),n&&t(_m),n&&t(rt),F(ed),F(ld),F(Zn),F(er),n&&t(bm),n&&t(Bs),F(dd),n&&t(km),n&&t(at),F(cd),F(bd),F(or),F(sr)}}}const B3={local:"distilbert",sections:[{local:"overview",title:"Overview"},{local:"resources",title:"Resources"},{local:"transformers.DistilBertConfig",title:"DistilBertConfig"},{local:"transformers.DistilBertTokenizer",title:"DistilBertTokenizer"},{local:"transformers.DistilBertTokenizerFast",title:"DistilBertTokenizerFast"},{local:"transformers.DistilBertModel",title:"DistilBertModel"},{local:"transformers.DistilBertForMaskedLM",title:"DistilBertForMaskedLM"},{local:"transformers.DistilBertForSequenceClassification",title:"DistilBertForSequenceClassification"},{local:"transformers.DistilBertForMultipleChoice",title:"DistilBertForMultipleChoice"},{local:"transformers.DistilBertForTokenClassification",title:"DistilBertForTokenClassification"},{local:"transformers.DistilBertForQuestionAnswering",title:"DistilBertForQuestionAnswering"},{local:"transformers.TFDistilBertModel",title:"TFDistilBertModel"},{local:"transformers.TFDistilBertForMaskedLM",title:"TFDistilBertForMaskedLM"},{local:"transformers.TFDistilBertForSequenceClassification",title:"TFDistilBertForSequenceClassification"},{local:"transformers.TFDistilBertForMultipleChoice",title:"TFDistilBertForMultipleChoice"},{local:"transformers.TFDistilBertForTokenClassification",title:"TFDistilBertForTokenClassification"},{local:"transformers.TFDistilBertForQuestionAnswering",title:"TFDistilBertForQuestionAnswering"},{local:"transformers.FlaxDistilBertModel",title:"FlaxDistilBertModel"},{local:"transformers.FlaxDistilBertForMaskedLM",title:"FlaxDistilBertForMaskedLM"},{local:"transformers.FlaxDistilBertForSequenceClassification",title:"FlaxDistilBertForSequenceClassification"},{local:"transformers.FlaxDistilBertForMultipleChoice",title:"FlaxDistilBertForMultipleChoice"},{local:"transformers.FlaxDistilBertForTokenClassification",title:"FlaxDistilBertForTokenClassification"},{local:"transformers.FlaxDistilBertForQuestionAnswering",title:"FlaxDistilBertForQuestionAnswering"}],title:"DistilBERT"};function x3(B){return E4(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class O3 extends $4{constructor(d){super();y4(this,d,x3,E3,D4,{})}}export{O3 as default,B3 as metadata};
