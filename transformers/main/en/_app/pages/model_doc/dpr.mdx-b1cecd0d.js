import{S as Af,i as Of,s as Nf,e as n,k as d,w as b,t as a,M as jf,c as r,d as t,m as l,a as s,x as T,h as i,b as c,F as e,g as p,y as k,q as P,o as w,B as E,v as Mf}from"../../chunks/vendor-6b77c823.js";import{T as At}from"../../chunks/Tip-39098574.js";import{D as W}from"../../chunks/Docstring-1088f2fb.js";import{C as Ln}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as ge}from"../../chunks/IconCopyLink-7a11ce68.js";function Qf(X){let f,x,m,g,R;return{c(){f=n("p"),x=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),g=a("Module"),R=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(v){f=r(v,"P",{});var _=s(f);x=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var $=s(m);g=i($,"Module"),$.forEach(t),R=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(v,_){p(v,f,_),e(f,x),e(f,m),e(m,g),e(f,R)},d(v){v&&t(f)}}}function Lf(X){let f,x,m,g,R;return{c(){f=n("p"),x=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),g=a("Module"),R=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(v){f=r(v,"P",{});var _=s(f);x=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var $=s(m);g=i($,"Module"),$.forEach(t),R=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(v,_){p(v,f,_),e(f,x),e(f,m),e(m,g),e(f,R)},d(v){v&&t(f)}}}function If(X){let f,x,m,g,R;return{c(){f=n("p"),x=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),g=a("Module"),R=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(v){f=r(v,"P",{});var _=s(f);x=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var $=s(m);g=i($,"Module"),$.forEach(t),R=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(v,_){p(v,f,_),e(f,x),e(f,m),e(m,g),e(f,R)},d(v){v&&t(f)}}}function Sf(X){let f,x,m,g,R,v,_,$,_e,Z,D,J,I,ee,ve,S,be,he,H,L,te,oe,q,C,ne,U,ue,re,B,Te,fe,z,ke,N,Pe,we,j,Ee,Re,M,V,O,se;return{c(){f=n("p"),x=a("TF 2.0 models accepts two formats as inputs:"),m=d(),g=n("ul"),R=n("li"),v=a("having all inputs as keyword arguments (like PyTorch models), or"),_=d(),$=n("li"),_e=a("having all inputs as a list, tuple or dict in the first positional arguments."),Z=d(),D=n("p"),J=a("This second option is useful when using "),I=n("code"),ee=a("tf.keras.Model.fit"),ve=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=n("code"),be=a("model(inputs)"),he=a("."),H=d(),L=n("p"),te=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),oe=d(),q=n("ul"),C=n("li"),ne=a("a single Tensor with "),U=n("code"),ue=a("input_ids"),re=a(" only and nothing else: "),B=n("code"),Te=a("model(inputs_ids)"),fe=d(),z=n("li"),ke=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),N=n("code"),Pe=a("model([input_ids, attention_mask])"),we=a(" or "),j=n("code"),Ee=a("model([input_ids, attention_mask, token_type_ids])"),Re=d(),M=n("li"),V=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),O=n("code"),se=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(h){f=r(h,"P",{});var y=s(f);x=i(y,"TF 2.0 models accepts two formats as inputs:"),y.forEach(t),m=l(h),g=r(h,"UL",{});var G=s(g);R=r(G,"LI",{});var Ie=s(R);v=i(Ie,"having all inputs as keyword arguments (like PyTorch models), or"),Ie.forEach(t),_=l(G),$=r(G,"LI",{});var Ne=s($);_e=i(Ne,"having all inputs as a list, tuple or dict in the first positional arguments."),Ne.forEach(t),G.forEach(t),Z=l(h),D=r(h,"P",{});var F=s(D);J=i(F,"This second option is useful when using "),I=r(F,"CODE",{});var me=s(I);ee=i(me,"tf.keras.Model.fit"),me.forEach(t),ve=i(F,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r(F,"CODE",{});var Se=s(S);be=i(Se,"model(inputs)"),Se.forEach(t),he=i(F,"."),F.forEach(t),H=l(h),L=r(h,"P",{});var ae=s(L);te=i(ae,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ae.forEach(t),oe=l(h),q=r(h,"UL",{});var A=s(q);C=r(A,"LI",{});var K=s(C);ne=i(K,"a single Tensor with "),U=r(K,"CODE",{});var He=s(U);ue=i(He,"input_ids"),He.forEach(t),re=i(K," only and nothing else: "),B=r(K,"CODE",{});var xe=s(B);Te=i(xe,"model(inputs_ids)"),xe.forEach(t),K.forEach(t),fe=l(A),z=r(A,"LI",{});var Y=s(z);ke=i(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),N=r(Y,"CODE",{});var Be=s(N);Pe=i(Be,"model([input_ids, attention_mask])"),Be.forEach(t),we=i(Y," or "),j=r(Y,"CODE",{});var We=s(j);Ee=i(We,"model([input_ids, attention_mask, token_type_ids])"),We.forEach(t),Y.forEach(t),Re=l(A),M=r(A,"LI",{});var Q=s(M);V=i(Q,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),O=r(Q,"CODE",{});var Ue=s(O);se=i(Ue,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ue.forEach(t),Q.forEach(t),A.forEach(t)},m(h,y){p(h,f,y),e(f,x),p(h,m,y),p(h,g,y),e(g,R),e(R,v),e(g,_),e(g,$),e($,_e),p(h,Z,y),p(h,D,y),e(D,J),e(D,I),e(I,ee),e(D,ve),e(D,S),e(S,be),e(D,he),p(h,H,y),p(h,L,y),e(L,te),p(h,oe,y),p(h,q,y),e(q,C),e(C,ne),e(C,U),e(U,ue),e(C,re),e(C,B),e(B,Te),e(q,fe),e(q,z),e(z,ke),e(z,N),e(N,Pe),e(z,we),e(z,j),e(j,Ee),e(q,Re),e(q,M),e(M,V),e(M,O),e(O,se)},d(h){h&&t(f),h&&t(m),h&&t(g),h&&t(Z),h&&t(D),h&&t(H),h&&t(L),h&&t(oe),h&&t(q)}}}function Hf(X){let f,x,m,g,R;return{c(){f=n("p"),x=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),g=a("Module"),R=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(v){f=r(v,"P",{});var _=s(f);x=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var $=s(m);g=i($,"Module"),$.forEach(t),R=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(v,_){p(v,f,_),e(f,x),e(f,m),e(m,g),e(f,R)},d(v){v&&t(f)}}}function Bf(X){let f,x,m,g,R,v,_,$,_e,Z,D,J,I,ee,ve,S,be,he,H,L,te,oe,q,C,ne,U,ue,re,B,Te,fe,z,ke,N,Pe,we,j,Ee,Re,M,V,O,se;return{c(){f=n("p"),x=a("TF 2.0 models accepts two formats as inputs:"),m=d(),g=n("ul"),R=n("li"),v=a("having all inputs as keyword arguments (like PyTorch models), or"),_=d(),$=n("li"),_e=a("having all inputs as a list, tuple or dict in the first positional arguments."),Z=d(),D=n("p"),J=a("This second option is useful when using "),I=n("code"),ee=a("tf.keras.Model.fit"),ve=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=n("code"),be=a("model(inputs)"),he=a("."),H=d(),L=n("p"),te=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),oe=d(),q=n("ul"),C=n("li"),ne=a("a single Tensor with "),U=n("code"),ue=a("input_ids"),re=a(" only and nothing else: "),B=n("code"),Te=a("model(inputs_ids)"),fe=d(),z=n("li"),ke=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),N=n("code"),Pe=a("model([input_ids, attention_mask])"),we=a(" or "),j=n("code"),Ee=a("model([input_ids, attention_mask, token_type_ids])"),Re=d(),M=n("li"),V=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),O=n("code"),se=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(h){f=r(h,"P",{});var y=s(f);x=i(y,"TF 2.0 models accepts two formats as inputs:"),y.forEach(t),m=l(h),g=r(h,"UL",{});var G=s(g);R=r(G,"LI",{});var Ie=s(R);v=i(Ie,"having all inputs as keyword arguments (like PyTorch models), or"),Ie.forEach(t),_=l(G),$=r(G,"LI",{});var Ne=s($);_e=i(Ne,"having all inputs as a list, tuple or dict in the first positional arguments."),Ne.forEach(t),G.forEach(t),Z=l(h),D=r(h,"P",{});var F=s(D);J=i(F,"This second option is useful when using "),I=r(F,"CODE",{});var me=s(I);ee=i(me,"tf.keras.Model.fit"),me.forEach(t),ve=i(F,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r(F,"CODE",{});var Se=s(S);be=i(Se,"model(inputs)"),Se.forEach(t),he=i(F,"."),F.forEach(t),H=l(h),L=r(h,"P",{});var ae=s(L);te=i(ae,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ae.forEach(t),oe=l(h),q=r(h,"UL",{});var A=s(q);C=r(A,"LI",{});var K=s(C);ne=i(K,"a single Tensor with "),U=r(K,"CODE",{});var He=s(U);ue=i(He,"input_ids"),He.forEach(t),re=i(K," only and nothing else: "),B=r(K,"CODE",{});var xe=s(B);Te=i(xe,"model(inputs_ids)"),xe.forEach(t),K.forEach(t),fe=l(A),z=r(A,"LI",{});var Y=s(z);ke=i(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),N=r(Y,"CODE",{});var Be=s(N);Pe=i(Be,"model([input_ids, attention_mask])"),Be.forEach(t),we=i(Y," or "),j=r(Y,"CODE",{});var We=s(j);Ee=i(We,"model([input_ids, attention_mask, token_type_ids])"),We.forEach(t),Y.forEach(t),Re=l(A),M=r(A,"LI",{});var Q=s(M);V=i(Q,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),O=r(Q,"CODE",{});var Ue=s(O);se=i(Ue,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ue.forEach(t),Q.forEach(t),A.forEach(t)},m(h,y){p(h,f,y),e(f,x),p(h,m,y),p(h,g,y),e(g,R),e(R,v),e(g,_),e(g,$),e($,_e),p(h,Z,y),p(h,D,y),e(D,J),e(D,I),e(I,ee),e(D,ve),e(D,S),e(S,be),e(D,he),p(h,H,y),p(h,L,y),e(L,te),p(h,oe,y),p(h,q,y),e(q,C),e(C,ne),e(C,U),e(U,ue),e(C,re),e(C,B),e(B,Te),e(q,fe),e(q,z),e(z,ke),e(z,N),e(N,Pe),e(z,we),e(z,j),e(j,Ee),e(q,Re),e(q,M),e(M,V),e(M,O),e(O,se)},d(h){h&&t(f),h&&t(m),h&&t(g),h&&t(Z),h&&t(D),h&&t(H),h&&t(L),h&&t(oe),h&&t(q)}}}function Wf(X){let f,x,m,g,R;return{c(){f=n("p"),x=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),g=a("Module"),R=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(v){f=r(v,"P",{});var _=s(f);x=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var $=s(m);g=i($,"Module"),$.forEach(t),R=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(v,_){p(v,f,_),e(f,x),e(f,m),e(m,g),e(f,R)},d(v){v&&t(f)}}}function Uf(X){let f,x,m,g,R,v,_,$,_e,Z,D,J,I,ee,ve,S,be,he,H,L,te,oe,q,C,ne,U,ue,re,B,Te,fe,z,ke,N,Pe,we,j,Ee,Re,M,V,O,se;return{c(){f=n("p"),x=a("TF 2.0 models accepts two formats as inputs:"),m=d(),g=n("ul"),R=n("li"),v=a("having all inputs as keyword arguments (like PyTorch models), or"),_=d(),$=n("li"),_e=a("having all inputs as a list, tuple or dict in the first positional arguments."),Z=d(),D=n("p"),J=a("This second option is useful when using "),I=n("code"),ee=a("tf.keras.Model.fit"),ve=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=n("code"),be=a("model(inputs)"),he=a("."),H=d(),L=n("p"),te=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),oe=d(),q=n("ul"),C=n("li"),ne=a("a single Tensor with "),U=n("code"),ue=a("input_ids"),re=a(" only and nothing else: "),B=n("code"),Te=a("model(inputs_ids)"),fe=d(),z=n("li"),ke=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),N=n("code"),Pe=a("model([input_ids, attention_mask])"),we=a(" or "),j=n("code"),Ee=a("model([input_ids, attention_mask, token_type_ids])"),Re=d(),M=n("li"),V=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),O=n("code"),se=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(h){f=r(h,"P",{});var y=s(f);x=i(y,"TF 2.0 models accepts two formats as inputs:"),y.forEach(t),m=l(h),g=r(h,"UL",{});var G=s(g);R=r(G,"LI",{});var Ie=s(R);v=i(Ie,"having all inputs as keyword arguments (like PyTorch models), or"),Ie.forEach(t),_=l(G),$=r(G,"LI",{});var Ne=s($);_e=i(Ne,"having all inputs as a list, tuple or dict in the first positional arguments."),Ne.forEach(t),G.forEach(t),Z=l(h),D=r(h,"P",{});var F=s(D);J=i(F,"This second option is useful when using "),I=r(F,"CODE",{});var me=s(I);ee=i(me,"tf.keras.Model.fit"),me.forEach(t),ve=i(F,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r(F,"CODE",{});var Se=s(S);be=i(Se,"model(inputs)"),Se.forEach(t),he=i(F,"."),F.forEach(t),H=l(h),L=r(h,"P",{});var ae=s(L);te=i(ae,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ae.forEach(t),oe=l(h),q=r(h,"UL",{});var A=s(q);C=r(A,"LI",{});var K=s(C);ne=i(K,"a single Tensor with "),U=r(K,"CODE",{});var He=s(U);ue=i(He,"input_ids"),He.forEach(t),re=i(K," only and nothing else: "),B=r(K,"CODE",{});var xe=s(B);Te=i(xe,"model(inputs_ids)"),xe.forEach(t),K.forEach(t),fe=l(A),z=r(A,"LI",{});var Y=s(z);ke=i(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),N=r(Y,"CODE",{});var Be=s(N);Pe=i(Be,"model([input_ids, attention_mask])"),Be.forEach(t),we=i(Y," or "),j=r(Y,"CODE",{});var We=s(j);Ee=i(We,"model([input_ids, attention_mask, token_type_ids])"),We.forEach(t),Y.forEach(t),Re=l(A),M=r(A,"LI",{});var Q=s(M);V=i(Q,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),O=r(Q,"CODE",{});var Ue=s(O);se=i(Ue,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ue.forEach(t),Q.forEach(t),A.forEach(t)},m(h,y){p(h,f,y),e(f,x),p(h,m,y),p(h,g,y),e(g,R),e(R,v),e(g,_),e(g,$),e($,_e),p(h,Z,y),p(h,D,y),e(D,J),e(D,I),e(I,ee),e(D,ve),e(D,S),e(S,be),e(D,he),p(h,H,y),p(h,L,y),e(L,te),p(h,oe,y),p(h,q,y),e(q,C),e(C,ne),e(C,U),e(U,ue),e(C,re),e(C,B),e(B,Te),e(q,fe),e(q,z),e(z,ke),e(z,N),e(N,Pe),e(z,we),e(z,j),e(j,Ee),e(q,Re),e(q,M),e(M,V),e(M,O),e(O,se)},d(h){h&&t(f),h&&t(m),h&&t(g),h&&t(Z),h&&t(D),h&&t(H),h&&t(L),h&&t(oe),h&&t(q)}}}function Vf(X){let f,x,m,g,R;return{c(){f=n("p"),x=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),g=a("Module"),R=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(v){f=r(v,"P",{});var _=s(f);x=i(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var $=s(m);g=i($,"Module"),$.forEach(t),R=i(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(v,_){p(v,f,_),e(f,x),e(f,m),e(m,g),e(f,R)},d(v){v&&t(f)}}}function Kf(X){let f,x,m,g,R,v,_,$,_e,Z,D,J,I,ee,ve,S,be,he,H,L,te,oe,q,C,ne,U,ue,re,B,Te,fe,z,ke,N,Pe,we,j,Ee,Re,M,V,O,se,h,y,G,Ie,Ne,F,me,Se,ae,A,K,He,xe,Y,Be,We,Q,Ue,In,ci,pi,Sn,hi,ui,Hn,fi,mi,mo,gi,_i,vi,go,bi,Bn,Ti,ki,Vs,ht,Ot,Cr,_o,Pi,Ar,wi,Ks,je,vo,Ei,Or,Ri,Di,Nt,Wn,yi,$i,Un,xi,zi,qi,bo,Fi,Vn,Ci,Ai,Ys,ut,jt,Nr,To,Oi,jr,Ni,Xs,Me,ko,ji,Po,Mi,Mr,Qi,Li,Ii,Mt,Kn,Si,Hi,Yn,Bi,Wi,Ui,wo,Vi,Xn,Ki,Yi,Js,ft,Qt,Qr,Eo,Xi,Lr,Ji,Gs,Qe,Ro,Gi,Ir,Zi,ed,Lt,Jn,td,od,Gn,nd,rd,sd,Do,ad,Zn,id,dd,Zs,mt,It,Sr,yo,ld,Hr,cd,ea,Le,$o,pd,xo,hd,Br,ud,fd,md,St,er,gd,_d,tr,vd,bd,Td,zo,kd,or,Pd,wd,ta,gt,Ht,Wr,qo,Ed,Ur,Rd,oa,ie,Fo,Dd,Vr,yd,$d,et,nr,xd,zd,rr,qd,Fd,sr,Cd,Ad,Od,Co,Nd,ar,jd,Md,Qd,Ge,Ld,Kr,Id,Sd,Yr,Hd,Bd,Xr,Wd,Ud,Vd,Ao,na,_t,Bt,Jr,Oo,Kd,Gr,Yd,ra,de,No,Xd,jo,Jd,Zr,Gd,Zd,el,tt,ir,tl,ol,dr,nl,rl,lr,sl,al,il,Mo,dl,cr,ll,cl,pl,Ze,hl,es,ul,fl,ts,ml,gl,os,_l,vl,bl,ns,Tl,sa,vt,Wt,rs,Qo,kl,ss,Pl,aa,bt,Lo,wl,Io,El,pr,Rl,Dl,ia,Tt,So,yl,Ho,$l,hr,xl,zl,da,kt,Bo,ql,Wo,Fl,ur,Cl,Al,la,Pt,Ut,as,Uo,Ol,is,Nl,ca,De,Vo,jl,ds,Ml,Ql,Ko,Ll,fr,Il,Sl,Hl,Yo,Bl,Xo,Wl,Ul,Vl,ze,Jo,Kl,wt,Yl,mr,Xl,Jl,ls,Gl,Zl,ec,Vt,tc,cs,oc,nc,Go,pa,Et,Kt,ps,Zo,rc,hs,sc,ha,ye,en,ac,us,ic,dc,tn,lc,gr,cc,pc,hc,on,uc,nn,fc,mc,gc,qe,rn,_c,Rt,vc,_r,bc,Tc,fs,kc,Pc,wc,Yt,Ec,ms,Rc,Dc,sn,ua,Dt,Xt,gs,an,yc,_s,$c,fa,$e,dn,xc,vs,zc,qc,ln,Fc,vr,Cc,Ac,Oc,cn,Nc,pn,jc,Mc,Qc,Fe,hn,Lc,yt,Ic,br,Sc,Hc,bs,Bc,Wc,Uc,Jt,Vc,Ts,Kc,Yc,un,ma,$t,Gt,ks,fn,Xc,Ps,Jc,ga,le,mn,Gc,ws,Zc,ep,gn,tp,Tr,op,np,rp,_n,sp,vn,ap,ip,dp,Zt,lp,Ce,bn,cp,xt,pp,kr,hp,up,Es,fp,mp,gp,eo,_p,Rs,vp,bp,Tn,_a,zt,to,Ds,kn,Tp,ys,kp,va,ce,Pn,Pp,$s,wp,Ep,wn,Rp,Pr,Dp,yp,$p,En,xp,Rn,zp,qp,Fp,oo,Cp,Ae,Dn,Ap,qt,Op,wr,Np,jp,xs,Mp,Qp,Lp,no,Ip,zs,Sp,Hp,yn,ba,Ft,ro,qs,$n,Bp,Fs,Wp,Ta,pe,xn,Up,Cs,Vp,Kp,zn,Yp,Er,Xp,Jp,Gp,qn,Zp,Fn,eh,th,oh,so,nh,Oe,Cn,rh,Ct,sh,Rr,ah,ih,As,dh,lh,ch,ao,ph,Os,hh,uh,An,ka;return v=new ge({}),ee=new ge({}),h=new ge({}),me=new W({props:{name:"class transformers.DPRConfig",anchor:"transformers.DPRConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"projection_dim",val:": int = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the DPR model. Defines the different tokens that can be represented by the <em>inputs_ids</em>
passed to the forward method of <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"vocab_size"},{anchor:"transformers.DPRConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.DPRConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.DPRConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.DPRConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.DPRConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.DPRConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.DPRConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.DPRConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.DPRConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <em>token_type_ids</em> passed into <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.DPRConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DPRConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.DPRConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.DPRConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Dimension of the projection for the context and question encoders. If it is set to zero (default), then no
projection is done.`,name:"projection_dim"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/configuration_dpr.py#L33"}}),_o=new ge({}),vo=new W({props:{name:"class transformers.DPRContextEncoderTokenizer",anchor:"transformers.DPRContextEncoderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/tokenization_dpr.py#L89"}}),To=new ge({}),ko=new W({props:{name:"class transformers.DPRContextEncoderTokenizerFast",anchor:"transformers.DPRContextEncoderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/tokenization_dpr_fast.py#L90"}}),Eo=new ge({}),Ro=new W({props:{name:"class transformers.DPRQuestionEncoderTokenizer",anchor:"transformers.DPRQuestionEncoderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/tokenization_dpr.py#L105"}}),yo=new ge({}),$o=new W({props:{name:"class transformers.DPRQuestionEncoderTokenizerFast",anchor:"transformers.DPRQuestionEncoderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/tokenization_dpr_fast.py#L107"}}),qo=new ge({}),Fo=new W({props:{name:"class transformers.DPRReaderTokenizer",anchor:"transformers.DPRReaderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRReaderTokenizer.questions",description:`<strong>questions</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code>[questions] * n_passages</code>. Otherwise you have to specify as many questions as in
<code>titles</code> or <code>texts</code>.`,name:"questions"},{anchor:"transformers.DPRReaderTokenizer.titles",description:`<strong>titles</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages titles to be encoded. This can be a string or a list of strings if there are several passages.`,name:"titles"},{anchor:"transformers.DPRReaderTokenizer.texts",description:`<strong>texts</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages texts to be encoded. This can be a string or a list of strings if there are several passages.`,name:"texts"},{anchor:"transformers.DPRReaderTokenizer.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DPRReaderTokenizer.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to
the maximum acceptable input length for the model if that argument is not provided. This will truncate
token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch
of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the first
sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the
second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.DPRReaderTokenizer.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.DPRReaderTokenizer.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DPRReaderTokenizer.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/tokenization_dpr.py#L372",returnDescription:`
<p>A dictionary with the following keys:</p>
<ul>
<li><code>input_ids</code>: List of token ids to be fed to a model.</li>
<li><code>attention_mask</code>: List of indices specifying which tokens should be attended to by the model.</li>
</ul>
`,returnType:`
<p><code>Dict[str, List[List[int]]]</code></p>
`}}),Ao=new Ln({props:{code:"[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>",highlighted:'[CLS] <span class="hljs-tag">&lt;<span class="hljs-name">question</span> <span class="hljs-attr">token</span> <span class="hljs-attr">ids</span>&gt;</span> [SEP] <span class="hljs-tag">&lt;<span class="hljs-name">titles</span> <span class="hljs-attr">ids</span>&gt;</span> [SEP] <span class="hljs-tag">&lt;<span class="hljs-name">texts</span> <span class="hljs-attr">ids</span>&gt;</span>'}}),Oo=new ge({}),No=new W({props:{name:"class transformers.DPRReaderTokenizerFast",anchor:"transformers.DPRReaderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRReaderTokenizerFast.questions",description:`<strong>questions</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code>[questions] * n_passages</code>. Otherwise you have to specify as many questions as in
<code>titles</code> or <code>texts</code>.`,name:"questions"},{anchor:"transformers.DPRReaderTokenizerFast.titles",description:`<strong>titles</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages titles to be encoded. This can be a string or a list of strings if there are several passages.`,name:"titles"},{anchor:"transformers.DPRReaderTokenizerFast.texts",description:`<strong>texts</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages texts to be encoded. This can be a string or a list of strings if there are several passages.`,name:"texts"},{anchor:"transformers.DPRReaderTokenizerFast.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DPRReaderTokenizerFast.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to
the maximum acceptable input length for the model if that argument is not provided. This will truncate
token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch
of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the first
sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the
second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.DPRReaderTokenizerFast.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.DPRReaderTokenizerFast.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DPRReaderTokenizerFast.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/tokenization_dpr_fast.py#L370",returnDescription:`
<p>A dictionary with the following keys:</p>
<ul>
<li><code>input_ids</code>: List of token ids to be fed to a model.</li>
<li><code>attention_mask</code>: List of indices specifying which tokens should be attended to by the model.</li>
</ul>
`,returnType:`
<p><code>Dict[str, List[List[int]]]</code></p>
`}}),Qo=new ge({}),Lo=new W({props:{name:"class transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",parameters:[{name:"pooler_output",val:": FloatTensor"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) &#x2014;
The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.`,name:"pooler_output"},{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L62"}}),So=new W({props:{name:"class transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput",anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput",parameters:[{name:"pooler_output",val:": FloatTensor"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) &#x2014;
The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.`,name:"pooler_output"},{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L90"}}),Bo=new W({props:{name:"class transformers.DPRReaderOutput",anchor:"transformers.DPRReaderOutput",parameters:[{name:"start_logits",val:": FloatTensor"},{name:"end_logits",val:": FloatTensor = None"},{name:"relevance_logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.DPRReaderOutput.start_logits",description:`<strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) &#x2014;
Logits of the start index of the span for each passage.`,name:"start_logits"},{anchor:"transformers.DPRReaderOutput.end_logits",description:`<strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) &#x2014;
Logits of the end index of the span for each passage.`,name:"end_logits"},{anchor:"transformers.DPRReaderOutput.relevance_logits",description:`<strong>relevance_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, )</code>) &#x2014;
Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.`,name:"relevance_logits"},{anchor:"transformers.DPRReaderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.DPRReaderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L118"}}),Uo=new ge({}),Vo=new W({props:{name:"class transformers.DPRContextEncoder",anchor:"transformers.DPRContextEncoder",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRContextEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L446"}}),Jo=new W({props:{name:"forward",anchor:"transformers.DPRContextEncoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRContextEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L454",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Vt=new At({props:{$$slots:{default:[Qf]},$$scope:{ctx:X}}}),Go=new Ln({props:{code:`from transformers import DPRContextEncoder, DPRContextEncoderTokenizer

tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
model = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="pt")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRContextEncoder, DPRContextEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRContextEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRContextEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),Zo=new ge({}),en=new W({props:{name:"class transformers.DPRQuestionEncoder",anchor:"transformers.DPRQuestionEncoder",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRQuestionEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L527"}}),rn=new W({props:{name:"forward",anchor:"transformers.DPRQuestionEncoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRQuestionEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L535",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Yt=new At({props:{$$slots:{default:[Lf]},$$scope:{ctx:X}}}),sn=new Ln({props:{code:`from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer

tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
model = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="pt")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRQuestionEncoder, DPRQuestionEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRQuestionEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),an=new ge({}),dn=new W({props:{name:"class transformers.DPRReader",anchor:"transformers.DPRReader",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L608"}}),hn=new W({props:{name:"forward",anchor:"transformers.DPRReader.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": bool = None"},{name:"output_hidden_states",val:": bool = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRReader.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DPRReader.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DPRReader.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DPRReader.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DPRReader.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L616",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRReaderOutput"
>transformers.models.dpr.modeling_dpr.DPRReaderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the start index of the span for each passage.</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the end index of the span for each passage.</p>
</li>
<li>
<p><strong>relevance_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, )</code>) \u2014 Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRReaderOutput"
>transformers.models.dpr.modeling_dpr.DPRReaderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Jt=new At({props:{$$slots:{default:[If]},$$scope:{ctx:X}}}),un=new Ln({props:{code:`from transformers import DPRReader, DPRReaderTokenizer

tokenizer = DPRReaderTokenizer.from_pretrained("facebook/dpr-reader-single-nq-base")
model = DPRReader.from_pretrained("facebook/dpr-reader-single-nq-base")
encoded_inputs = tokenizer(
    questions=["What is love ?"],
    titles=["Haddaway"],
    texts=["'What Is Love' is a song recorded by the artist Haddaway"],
    return_tensors="pt",
)
outputs = model(**encoded_inputs)
start_logits = outputs.start_logits
end_logits = outputs.end_logits
relevance_logits = outputs.relevance_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRReader, DPRReaderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRReaderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRReader.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(
<span class="hljs-meta">... </span>    questions=[<span class="hljs-string">&quot;What is love ?&quot;</span>],
<span class="hljs-meta">... </span>    titles=[<span class="hljs-string">&quot;Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    texts=[<span class="hljs-string">&quot;&#x27;What Is Love&#x27; is a song recorded by the artist Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**encoded_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_logits = outputs.relevance_logits`}}),fn=new ge({}),mn=new W({props:{name:"class transformers.TFDPRContextEncoder",anchor:"transformers.TFDPRContextEncoder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRContextEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_tf_dpr.py#L535"}}),Zt=new At({props:{$$slots:{default:[Sf]},$$scope:{ctx:X}}}),bn=new W({props:{name:"call",anchor:"transformers.TFDPRContextEncoder.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRContextEncoder.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_tf_dpr.py#L547",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),eo=new At({props:{$$slots:{default:[Hf]},$$scope:{ctx:X}}}),Tn=new Ln({props:{code:`from transformers import TFDPRContextEncoder, DPRContextEncoderTokenizer

tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
model = TFDPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base", from_pt=True)
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="tf")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRContextEncoder, DPRContextEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRContextEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRContextEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),kn=new ge({}),Pn=new W({props:{name:"class transformers.TFDPRQuestionEncoder",anchor:"transformers.TFDPRQuestionEncoder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRQuestionEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_tf_dpr.py#L622"}}),oo=new At({props:{$$slots:{default:[Bf]},$$scope:{ctx:X}}}),Dn=new W({props:{name:"call",anchor:"transformers.TFDPRQuestionEncoder.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRQuestionEncoder.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_tf_dpr.py#L634",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),no=new At({props:{$$slots:{default:[Wf]},$$scope:{ctx:X}}}),yn=new Ln({props:{code:`from transformers import TFDPRQuestionEncoder, DPRQuestionEncoderTokenizer

tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
model = TFDPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base", from_pt=True)
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="tf")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRQuestionEncoder, DPRQuestionEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRQuestionEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),$n=new ge({}),xn=new W({props:{name:"class transformers.TFDPRReader",anchor:"transformers.TFDPRReader",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_tf_dpr.py#L708"}}),so=new At({props:{$$slots:{default:[Uf]},$$scope:{ctx:X}}}),Cn=new W({props:{name:"call",anchor:"transformers.TFDPRReader.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:": bool = None"},{name:"output_hidden_states",val:": bool = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRReader.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDPRReader.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(n_passages, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDPRReader.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDPRReader.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDPRReader.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_tf_dpr.py#L720",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the start index of the span for each passage.</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the end index of the span for each passage.</p>
</li>
<li>
<p><strong>relevance_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, )</code>) \u2014 Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),ao=new At({props:{$$slots:{default:[Vf]},$$scope:{ctx:X}}}),An=new Ln({props:{code:`from transformers import TFDPRReader, DPRReaderTokenizer

tokenizer = DPRReaderTokenizer.from_pretrained("facebook/dpr-reader-single-nq-base")
model = TFDPRReader.from_pretrained("facebook/dpr-reader-single-nq-base", from_pt=True)
encoded_inputs = tokenizer(
    questions=["What is love ?"],
    titles=["Haddaway"],
    texts=["'What Is Love' is a song recorded by the artist Haddaway"],
    return_tensors="tf",
)
outputs = model(encoded_inputs)
start_logits = outputs.start_logits
end_logits = outputs.end_logits
relevance_logits = outputs.relevance_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRReader, DPRReaderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRReaderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRReader.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(
<span class="hljs-meta">... </span>    questions=[<span class="hljs-string">&quot;What is love ?&quot;</span>],
<span class="hljs-meta">... </span>    titles=[<span class="hljs-string">&quot;Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    texts=[<span class="hljs-string">&quot;&#x27;What Is Love&#x27; is a song recorded by the artist Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(encoded_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_logits = outputs.relevance_logits`}}),{c(){f=n("meta"),x=d(),m=n("h1"),g=n("a"),R=n("span"),b(v.$$.fragment),_=d(),$=n("span"),_e=a("DPR"),Z=d(),D=n("h2"),J=n("a"),I=n("span"),b(ee.$$.fragment),ve=d(),S=n("span"),be=a("Overview"),he=d(),H=n("p"),L=a(`Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. It was
introduced in `),te=n("a"),oe=a("Dense Passage Retrieval for Open-Domain Question Answering"),q=a(` by
Vladimir Karpukhin, Barlas O\u011Fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.`),C=d(),ne=n("p"),U=a("The abstract from the paper is the following:"),ue=d(),re=n("p"),B=n("em"),Te=a(`Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional
sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can
be practically implemented using dense representations alone, where embeddings are learned from a small number of
questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,
our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage
retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.`),fe=d(),z=n("p"),ke=a("This model was contributed by "),N=n("a"),Pe=a("lhoestq"),we=a(". The original code can be found "),j=n("a"),Ee=a("here"),Re=a("."),M=d(),V=n("h2"),O=n("a"),se=n("span"),b(h.$$.fragment),y=d(),G=n("span"),Ie=a("DPRConfig"),Ne=d(),F=n("div"),b(me.$$.fragment),Se=d(),ae=n("p"),A=n("a"),K=a("DPRConfig"),He=a(" is the configuration class to store the configuration of a "),xe=n("em"),Y=a("DPRModel"),Be=a("."),We=d(),Q=n("p"),Ue=a("This is the configuration class to store the configuration of a "),In=n("a"),ci=a("DPRContextEncoder"),pi=a(", "),Sn=n("a"),hi=a("DPRQuestionEncoder"),ui=a(`, or a
`),Hn=n("a"),fi=a("DPRReader"),mi=a(`. It is used to instantiate the components of the DPR model according to the specified arguments,
defining the model component architectures. Instantiating a configuration with the defaults will yield a similar
configuration to that of the DPRContextEncoder
`),mo=n("a"),gi=a("facebook/dpr-ctx_encoder-single-nq-base"),_i=a(`
architecture.`),vi=d(),go=n("p"),bi=a("This class is a subclass of "),Bn=n("a"),Ti=a("BertConfig"),ki=a(". Please check the superclass for the documentation of all kwargs."),Vs=d(),ht=n("h2"),Ot=n("a"),Cr=n("span"),b(_o.$$.fragment),Pi=d(),Ar=n("span"),wi=a("DPRContextEncoderTokenizer"),Ks=d(),je=n("div"),b(vo.$$.fragment),Ei=d(),Or=n("p"),Ri=a("Construct a DPRContextEncoder tokenizer."),Di=d(),Nt=n("p"),Wn=n("a"),yi=a("DPRContextEncoderTokenizer"),$i=a(" is identical to "),Un=n("a"),xi=a("BertTokenizer"),zi=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),qi=d(),bo=n("p"),Fi=a("Refer to superclass "),Vn=n("a"),Ci=a("BertTokenizer"),Ai=a(" for usage examples and documentation concerning parameters."),Ys=d(),ut=n("h2"),jt=n("a"),Nr=n("span"),b(To.$$.fragment),Oi=d(),jr=n("span"),Ni=a("DPRContextEncoderTokenizerFast"),Xs=d(),Me=n("div"),b(ko.$$.fragment),ji=d(),Po=n("p"),Mi=a("Construct a \u201Cfast\u201D DPRContextEncoder tokenizer (backed by HuggingFace\u2019s "),Mr=n("em"),Qi=a("tokenizers"),Li=a(" library)."),Ii=d(),Mt=n("p"),Kn=n("a"),Si=a("DPRContextEncoderTokenizerFast"),Hi=a(" is identical to "),Yn=n("a"),Bi=a("BertTokenizerFast"),Wi=a(` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Ui=d(),wo=n("p"),Vi=a("Refer to superclass "),Xn=n("a"),Ki=a("BertTokenizerFast"),Yi=a(" for usage examples and documentation concerning parameters."),Js=d(),ft=n("h2"),Qt=n("a"),Qr=n("span"),b(Eo.$$.fragment),Xi=d(),Lr=n("span"),Ji=a("DPRQuestionEncoderTokenizer"),Gs=d(),Qe=n("div"),b(Ro.$$.fragment),Gi=d(),Ir=n("p"),Zi=a("Constructs a DPRQuestionEncoder tokenizer."),ed=d(),Lt=n("p"),Jn=n("a"),td=a("DPRQuestionEncoderTokenizer"),od=a(" is identical to "),Gn=n("a"),nd=a("BertTokenizer"),rd=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),sd=d(),Do=n("p"),ad=a("Refer to superclass "),Zn=n("a"),id=a("BertTokenizer"),dd=a(" for usage examples and documentation concerning parameters."),Zs=d(),mt=n("h2"),It=n("a"),Sr=n("span"),b(yo.$$.fragment),ld=d(),Hr=n("span"),cd=a("DPRQuestionEncoderTokenizerFast"),ea=d(),Le=n("div"),b($o.$$.fragment),pd=d(),xo=n("p"),hd=a("Constructs a \u201Cfast\u201D DPRQuestionEncoder tokenizer (backed by HuggingFace\u2019s "),Br=n("em"),ud=a("tokenizers"),fd=a(" library)."),md=d(),St=n("p"),er=n("a"),gd=a("DPRQuestionEncoderTokenizerFast"),_d=a(" is identical to "),tr=n("a"),vd=a("BertTokenizerFast"),bd=a(` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Td=d(),zo=n("p"),kd=a("Refer to superclass "),or=n("a"),Pd=a("BertTokenizerFast"),wd=a(" for usage examples and documentation concerning parameters."),ta=d(),gt=n("h2"),Ht=n("a"),Wr=n("span"),b(qo.$$.fragment),Ed=d(),Ur=n("span"),Rd=a("DPRReaderTokenizer"),oa=d(),ie=n("div"),b(Fo.$$.fragment),Dd=d(),Vr=n("p"),yd=a("Construct a DPRReader tokenizer."),$d=d(),et=n("p"),nr=n("a"),xd=a("DPRReaderTokenizer"),zd=a(" is almost identical to "),rr=n("a"),qd=a("BertTokenizer"),Fd=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts that are
combined to be fed to the `),sr=n("a"),Cd=a("DPRReader"),Ad=a(" model."),Od=d(),Co=n("p"),Nd=a("Refer to superclass "),ar=n("a"),jd=a("BertTokenizer"),Md=a(" for usage examples and documentation concerning parameters."),Qd=d(),Ge=n("p"),Ld=a("Return a dictionary with the token ids of the input strings and other information to give to "),Kr=n("code"),Id=a(".decode_best_spans"),Sd=a(`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),Yr=n("code"),Hd=a("input_ids"),Bd=a(" is a matrix of size "),Xr=n("code"),Wd=a("(n_passages, sequence_length)"),Ud=a(`
with the format:`),Vd=d(),b(Ao.$$.fragment),na=d(),_t=n("h2"),Bt=n("a"),Jr=n("span"),b(Oo.$$.fragment),Kd=d(),Gr=n("span"),Yd=a("DPRReaderTokenizerFast"),ra=d(),de=n("div"),b(No.$$.fragment),Xd=d(),jo=n("p"),Jd=a("Constructs a \u201Cfast\u201D DPRReader tokenizer (backed by HuggingFace\u2019s "),Zr=n("em"),Gd=a("tokenizers"),Zd=a(" library)."),el=d(),tt=n("p"),ir=n("a"),tl=a("DPRReaderTokenizerFast"),ol=a(" is almost identical to "),dr=n("a"),nl=a("BertTokenizerFast"),rl=a(` and runs end-to-end tokenization:
punctuation splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts
that are combined to be fed to the `),lr=n("a"),sl=a("DPRReader"),al=a(" model."),il=d(),Mo=n("p"),dl=a("Refer to superclass "),cr=n("a"),ll=a("BertTokenizerFast"),cl=a(" for usage examples and documentation concerning parameters."),pl=d(),Ze=n("p"),hl=a("Return a dictionary with the token ids of the input strings and other information to give to "),es=n("code"),ul=a(".decode_best_spans"),fl=a(`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),ts=n("code"),ml=a("input_ids"),gl=a(" is a matrix of size "),os=n("code"),_l=a("(n_passages, sequence_length)"),vl=a(`
with the format:`),bl=d(),ns=n("p"),Tl=a("[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>"),sa=d(),vt=n("h2"),Wt=n("a"),rs=n("span"),b(Qo.$$.fragment),kl=d(),ss=n("span"),Pl=a("DPR specific outputs"),aa=d(),bt=n("div"),b(Lo.$$.fragment),wl=d(),Io=n("p"),El=a("Class for outputs of "),pr=n("a"),Rl=a("DPRQuestionEncoder"),Dl=a("."),ia=d(),Tt=n("div"),b(So.$$.fragment),yl=d(),Ho=n("p"),$l=a("Class for outputs of "),hr=n("a"),xl=a("DPRQuestionEncoder"),zl=a("."),da=d(),kt=n("div"),b(Bo.$$.fragment),ql=d(),Wo=n("p"),Fl=a("Class for outputs of "),ur=n("a"),Cl=a("DPRQuestionEncoder"),Al=a("."),la=d(),Pt=n("h2"),Ut=n("a"),as=n("span"),b(Uo.$$.fragment),Ol=d(),is=n("span"),Nl=a("DPRContextEncoder"),ca=d(),De=n("div"),b(Vo.$$.fragment),jl=d(),ds=n("p"),Ml=a("The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Ql=d(),Ko=n("p"),Ll=a("This model inherits from "),fr=n("a"),Il=a("PreTrainedModel"),Sl=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Hl=d(),Yo=n("p"),Bl=a("This model is also a PyTorch "),Xo=n("a"),Wl=a("torch.nn.Module"),Ul=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Vl=d(),ze=n("div"),b(Jo.$$.fragment),Kl=d(),wt=n("p"),Yl=a("The "),mr=n("a"),Xl=a("DPRContextEncoder"),Jl=a(" forward method, overrides the "),ls=n("code"),Gl=a("__call__"),Zl=a(" special method."),ec=d(),b(Vt.$$.fragment),tc=d(),cs=n("p"),oc=a("Examples:"),nc=d(),b(Go.$$.fragment),pa=d(),Et=n("h2"),Kt=n("a"),ps=n("span"),b(Zo.$$.fragment),rc=d(),hs=n("span"),sc=a("DPRQuestionEncoder"),ha=d(),ye=n("div"),b(en.$$.fragment),ac=d(),us=n("p"),ic=a("The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),dc=d(),tn=n("p"),lc=a("This model inherits from "),gr=n("a"),cc=a("PreTrainedModel"),pc=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),hc=d(),on=n("p"),uc=a("This model is also a PyTorch "),nn=n("a"),fc=a("torch.nn.Module"),mc=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),gc=d(),qe=n("div"),b(rn.$$.fragment),_c=d(),Rt=n("p"),vc=a("The "),_r=n("a"),bc=a("DPRQuestionEncoder"),Tc=a(" forward method, overrides the "),fs=n("code"),kc=a("__call__"),Pc=a(" special method."),wc=d(),b(Yt.$$.fragment),Ec=d(),ms=n("p"),Rc=a("Examples:"),Dc=d(),b(sn.$$.fragment),ua=d(),Dt=n("h2"),Xt=n("a"),gs=n("span"),b(an.$$.fragment),yc=d(),_s=n("span"),$c=a("DPRReader"),fa=d(),$e=n("div"),b(dn.$$.fragment),xc=d(),vs=n("p"),zc=a("The bare DPRReader transformer outputting span predictions."),qc=d(),ln=n("p"),Fc=a("This model inherits from "),vr=n("a"),Cc=a("PreTrainedModel"),Ac=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Oc=d(),cn=n("p"),Nc=a("This model is also a PyTorch "),pn=n("a"),jc=a("torch.nn.Module"),Mc=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Qc=d(),Fe=n("div"),b(hn.$$.fragment),Lc=d(),yt=n("p"),Ic=a("The "),br=n("a"),Sc=a("DPRReader"),Hc=a(" forward method, overrides the "),bs=n("code"),Bc=a("__call__"),Wc=a(" special method."),Uc=d(),b(Jt.$$.fragment),Vc=d(),Ts=n("p"),Kc=a("Examples:"),Yc=d(),b(un.$$.fragment),ma=d(),$t=n("h2"),Gt=n("a"),ks=n("span"),b(fn.$$.fragment),Xc=d(),Ps=n("span"),Jc=a("TFDPRContextEncoder"),ga=d(),le=n("div"),b(mn.$$.fragment),Gc=d(),ws=n("p"),Zc=a("The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),ep=d(),gn=n("p"),tp=a("This model inherits from "),Tr=n("a"),op=a("TFPreTrainedModel"),np=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),rp=d(),_n=n("p"),sp=a("This model is also a Tensorflow "),vn=n("a"),ap=a("tf.keras.Model"),ip=a(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),dp=d(),b(Zt.$$.fragment),lp=d(),Ce=n("div"),b(bn.$$.fragment),cp=d(),xt=n("p"),pp=a("The "),kr=n("a"),hp=a("TFDPRContextEncoder"),up=a(" forward method, overrides the "),Es=n("code"),fp=a("__call__"),mp=a(" special method."),gp=d(),b(eo.$$.fragment),_p=d(),Rs=n("p"),vp=a("Examples:"),bp=d(),b(Tn.$$.fragment),_a=d(),zt=n("h2"),to=n("a"),Ds=n("span"),b(kn.$$.fragment),Tp=d(),ys=n("span"),kp=a("TFDPRQuestionEncoder"),va=d(),ce=n("div"),b(Pn.$$.fragment),Pp=d(),$s=n("p"),wp=a("The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),Ep=d(),wn=n("p"),Rp=a("This model inherits from "),Pr=n("a"),Dp=a("TFPreTrainedModel"),yp=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),$p=d(),En=n("p"),xp=a("This model is also a Tensorflow "),Rn=n("a"),zp=a("tf.keras.Model"),qp=a(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),Fp=d(),b(oo.$$.fragment),Cp=d(),Ae=n("div"),b(Dn.$$.fragment),Ap=d(),qt=n("p"),Op=a("The "),wr=n("a"),Np=a("TFDPRQuestionEncoder"),jp=a(" forward method, overrides the "),xs=n("code"),Mp=a("__call__"),Qp=a(" special method."),Lp=d(),b(no.$$.fragment),Ip=d(),zs=n("p"),Sp=a("Examples:"),Hp=d(),b(yn.$$.fragment),ba=d(),Ft=n("h2"),ro=n("a"),qs=n("span"),b($n.$$.fragment),Bp=d(),Fs=n("span"),Wp=a("TFDPRReader"),Ta=d(),pe=n("div"),b(xn.$$.fragment),Up=d(),Cs=n("p"),Vp=a("The bare DPRReader transformer outputting span predictions."),Kp=d(),zn=n("p"),Yp=a("This model inherits from "),Er=n("a"),Xp=a("TFPreTrainedModel"),Jp=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Gp=d(),qn=n("p"),Zp=a("This model is also a Tensorflow "),Fn=n("a"),eh=a("tf.keras.Model"),th=a(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),oh=d(),b(so.$$.fragment),nh=d(),Oe=n("div"),b(Cn.$$.fragment),rh=d(),Ct=n("p"),sh=a("The "),Rr=n("a"),ah=a("TFDPRReader"),ih=a(" forward method, overrides the "),As=n("code"),dh=a("__call__"),lh=a(" special method."),ch=d(),b(ao.$$.fragment),ph=d(),Os=n("p"),hh=a("Examples:"),uh=d(),b(An.$$.fragment),this.h()},l(o){const u=jf('[data-svelte="svelte-1phssyn"]',document.head);f=r(u,"META",{name:!0,content:!0}),u.forEach(t),x=l(o),m=r(o,"H1",{class:!0});var On=s(m);g=r(On,"A",{id:!0,class:!0,href:!0});var Ns=s(g);R=r(Ns,"SPAN",{});var js=s(R);T(v.$$.fragment,js),js.forEach(t),Ns.forEach(t),_=l(On),$=r(On,"SPAN",{});var Ms=s($);_e=i(Ms,"DPR"),Ms.forEach(t),On.forEach(t),Z=l(o),D=r(o,"H2",{class:!0});var Nn=s(D);J=r(Nn,"A",{id:!0,class:!0,href:!0});var Qs=s(J);I=r(Qs,"SPAN",{});var Ls=s(I);T(ee.$$.fragment,Ls),Ls.forEach(t),Qs.forEach(t),ve=l(Nn),S=r(Nn,"SPAN",{});var Is=s(S);be=i(Is,"Overview"),Is.forEach(t),Nn.forEach(t),he=l(o),H=r(o,"P",{});var jn=s(H);L=i(jn,`Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. It was
introduced in `),te=r(jn,"A",{href:!0,rel:!0});var fh=s(te);oe=i(fh,"Dense Passage Retrieval for Open-Domain Question Answering"),fh.forEach(t),q=i(jn,` by
Vladimir Karpukhin, Barlas O\u011Fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.`),jn.forEach(t),C=l(o),ne=r(o,"P",{});var mh=s(ne);U=i(mh,"The abstract from the paper is the following:"),mh.forEach(t),ue=l(o),re=r(o,"P",{});var gh=s(re);B=r(gh,"EM",{});var _h=s(B);Te=i(_h,`Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional
sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can
be practically implemented using dense representations alone, where embeddings are learned from a small number of
questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,
our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage
retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.`),_h.forEach(t),gh.forEach(t),fe=l(o),z=r(o,"P",{});var Dr=s(z);ke=i(Dr,"This model was contributed by "),N=r(Dr,"A",{href:!0,rel:!0});var vh=s(N);Pe=i(vh,"lhoestq"),vh.forEach(t),we=i(Dr,". The original code can be found "),j=r(Dr,"A",{href:!0,rel:!0});var bh=s(j);Ee=i(bh,"here"),bh.forEach(t),Re=i(Dr,"."),Dr.forEach(t),M=l(o),V=r(o,"H2",{class:!0});var Pa=s(V);O=r(Pa,"A",{id:!0,class:!0,href:!0});var Th=s(O);se=r(Th,"SPAN",{});var kh=s(se);T(h.$$.fragment,kh),kh.forEach(t),Th.forEach(t),y=l(Pa),G=r(Pa,"SPAN",{});var Ph=s(G);Ie=i(Ph,"DPRConfig"),Ph.forEach(t),Pa.forEach(t),Ne=l(o),F=r(o,"DIV",{class:!0});var io=s(F);T(me.$$.fragment,io),Se=l(io),ae=r(io,"P",{});var Ss=s(ae);A=r(Ss,"A",{href:!0});var wh=s(A);K=i(wh,"DPRConfig"),wh.forEach(t),He=i(Ss," is the configuration class to store the configuration of a "),xe=r(Ss,"EM",{});var Eh=s(xe);Y=i(Eh,"DPRModel"),Eh.forEach(t),Be=i(Ss,"."),Ss.forEach(t),We=l(io),Q=r(io,"P",{});var ot=s(Q);Ue=i(ot,"This is the configuration class to store the configuration of a "),In=r(ot,"A",{href:!0});var Rh=s(In);ci=i(Rh,"DPRContextEncoder"),Rh.forEach(t),pi=i(ot,", "),Sn=r(ot,"A",{href:!0});var Dh=s(Sn);hi=i(Dh,"DPRQuestionEncoder"),Dh.forEach(t),ui=i(ot,`, or a
`),Hn=r(ot,"A",{href:!0});var yh=s(Hn);fi=i(yh,"DPRReader"),yh.forEach(t),mi=i(ot,`. It is used to instantiate the components of the DPR model according to the specified arguments,
defining the model component architectures. Instantiating a configuration with the defaults will yield a similar
configuration to that of the DPRContextEncoder
`),mo=r(ot,"A",{href:!0,rel:!0});var $h=s(mo);gi=i($h,"facebook/dpr-ctx_encoder-single-nq-base"),$h.forEach(t),_i=i(ot,`
architecture.`),ot.forEach(t),vi=l(io),go=r(io,"P",{});var wa=s(go);bi=i(wa,"This class is a subclass of "),Bn=r(wa,"A",{href:!0});var xh=s(Bn);Ti=i(xh,"BertConfig"),xh.forEach(t),ki=i(wa,". Please check the superclass for the documentation of all kwargs."),wa.forEach(t),io.forEach(t),Vs=l(o),ht=r(o,"H2",{class:!0});var Ea=s(ht);Ot=r(Ea,"A",{id:!0,class:!0,href:!0});var zh=s(Ot);Cr=r(zh,"SPAN",{});var qh=s(Cr);T(_o.$$.fragment,qh),qh.forEach(t),zh.forEach(t),Pi=l(Ea),Ar=r(Ea,"SPAN",{});var Fh=s(Ar);wi=i(Fh,"DPRContextEncoderTokenizer"),Fh.forEach(t),Ea.forEach(t),Ks=l(o),je=r(o,"DIV",{class:!0});var lo=s(je);T(vo.$$.fragment,lo),Ei=l(lo),Or=r(lo,"P",{});var Ch=s(Or);Ri=i(Ch,"Construct a DPRContextEncoder tokenizer."),Ch.forEach(t),Di=l(lo),Nt=r(lo,"P",{});var Hs=s(Nt);Wn=r(Hs,"A",{href:!0});var Ah=s(Wn);yi=i(Ah,"DPRContextEncoderTokenizer"),Ah.forEach(t),$i=i(Hs," is identical to "),Un=r(Hs,"A",{href:!0});var Oh=s(Un);xi=i(Oh,"BertTokenizer"),Oh.forEach(t),zi=i(Hs,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Hs.forEach(t),qi=l(lo),bo=r(lo,"P",{});var Ra=s(bo);Fi=i(Ra,"Refer to superclass "),Vn=r(Ra,"A",{href:!0});var Nh=s(Vn);Ci=i(Nh,"BertTokenizer"),Nh.forEach(t),Ai=i(Ra," for usage examples and documentation concerning parameters."),Ra.forEach(t),lo.forEach(t),Ys=l(o),ut=r(o,"H2",{class:!0});var Da=s(ut);jt=r(Da,"A",{id:!0,class:!0,href:!0});var jh=s(jt);Nr=r(jh,"SPAN",{});var Mh=s(Nr);T(To.$$.fragment,Mh),Mh.forEach(t),jh.forEach(t),Oi=l(Da),jr=r(Da,"SPAN",{});var Qh=s(jr);Ni=i(Qh,"DPRContextEncoderTokenizerFast"),Qh.forEach(t),Da.forEach(t),Xs=l(o),Me=r(o,"DIV",{class:!0});var co=s(Me);T(ko.$$.fragment,co),ji=l(co),Po=r(co,"P",{});var ya=s(Po);Mi=i(ya,"Construct a \u201Cfast\u201D DPRContextEncoder tokenizer (backed by HuggingFace\u2019s "),Mr=r(ya,"EM",{});var Lh=s(Mr);Qi=i(Lh,"tokenizers"),Lh.forEach(t),Li=i(ya," library)."),ya.forEach(t),Ii=l(co),Mt=r(co,"P",{});var Bs=s(Mt);Kn=r(Bs,"A",{href:!0});var Ih=s(Kn);Si=i(Ih,"DPRContextEncoderTokenizerFast"),Ih.forEach(t),Hi=i(Bs," is identical to "),Yn=r(Bs,"A",{href:!0});var Sh=s(Yn);Bi=i(Sh,"BertTokenizerFast"),Sh.forEach(t),Wi=i(Bs,` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Bs.forEach(t),Ui=l(co),wo=r(co,"P",{});var $a=s(wo);Vi=i($a,"Refer to superclass "),Xn=r($a,"A",{href:!0});var Hh=s(Xn);Ki=i(Hh,"BertTokenizerFast"),Hh.forEach(t),Yi=i($a," for usage examples and documentation concerning parameters."),$a.forEach(t),co.forEach(t),Js=l(o),ft=r(o,"H2",{class:!0});var xa=s(ft);Qt=r(xa,"A",{id:!0,class:!0,href:!0});var Bh=s(Qt);Qr=r(Bh,"SPAN",{});var Wh=s(Qr);T(Eo.$$.fragment,Wh),Wh.forEach(t),Bh.forEach(t),Xi=l(xa),Lr=r(xa,"SPAN",{});var Uh=s(Lr);Ji=i(Uh,"DPRQuestionEncoderTokenizer"),Uh.forEach(t),xa.forEach(t),Gs=l(o),Qe=r(o,"DIV",{class:!0});var po=s(Qe);T(Ro.$$.fragment,po),Gi=l(po),Ir=r(po,"P",{});var Vh=s(Ir);Zi=i(Vh,"Constructs a DPRQuestionEncoder tokenizer."),Vh.forEach(t),ed=l(po),Lt=r(po,"P",{});var Ws=s(Lt);Jn=r(Ws,"A",{href:!0});var Kh=s(Jn);td=i(Kh,"DPRQuestionEncoderTokenizer"),Kh.forEach(t),od=i(Ws," is identical to "),Gn=r(Ws,"A",{href:!0});var Yh=s(Gn);nd=i(Yh,"BertTokenizer"),Yh.forEach(t),rd=i(Ws,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Ws.forEach(t),sd=l(po),Do=r(po,"P",{});var za=s(Do);ad=i(za,"Refer to superclass "),Zn=r(za,"A",{href:!0});var Xh=s(Zn);id=i(Xh,"BertTokenizer"),Xh.forEach(t),dd=i(za," for usage examples and documentation concerning parameters."),za.forEach(t),po.forEach(t),Zs=l(o),mt=r(o,"H2",{class:!0});var qa=s(mt);It=r(qa,"A",{id:!0,class:!0,href:!0});var Jh=s(It);Sr=r(Jh,"SPAN",{});var Gh=s(Sr);T(yo.$$.fragment,Gh),Gh.forEach(t),Jh.forEach(t),ld=l(qa),Hr=r(qa,"SPAN",{});var Zh=s(Hr);cd=i(Zh,"DPRQuestionEncoderTokenizerFast"),Zh.forEach(t),qa.forEach(t),ea=l(o),Le=r(o,"DIV",{class:!0});var ho=s(Le);T($o.$$.fragment,ho),pd=l(ho),xo=r(ho,"P",{});var Fa=s(xo);hd=i(Fa,"Constructs a \u201Cfast\u201D DPRQuestionEncoder tokenizer (backed by HuggingFace\u2019s "),Br=r(Fa,"EM",{});var eu=s(Br);ud=i(eu,"tokenizers"),eu.forEach(t),fd=i(Fa," library)."),Fa.forEach(t),md=l(ho),St=r(ho,"P",{});var Us=s(St);er=r(Us,"A",{href:!0});var tu=s(er);gd=i(tu,"DPRQuestionEncoderTokenizerFast"),tu.forEach(t),_d=i(Us," is identical to "),tr=r(Us,"A",{href:!0});var ou=s(tr);vd=i(ou,"BertTokenizerFast"),ou.forEach(t),bd=i(Us,` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Us.forEach(t),Td=l(ho),zo=r(ho,"P",{});var Ca=s(zo);kd=i(Ca,"Refer to superclass "),or=r(Ca,"A",{href:!0});var nu=s(or);Pd=i(nu,"BertTokenizerFast"),nu.forEach(t),wd=i(Ca," for usage examples and documentation concerning parameters."),Ca.forEach(t),ho.forEach(t),ta=l(o),gt=r(o,"H2",{class:!0});var Aa=s(gt);Ht=r(Aa,"A",{id:!0,class:!0,href:!0});var ru=s(Ht);Wr=r(ru,"SPAN",{});var su=s(Wr);T(qo.$$.fragment,su),su.forEach(t),ru.forEach(t),Ed=l(Aa),Ur=r(Aa,"SPAN",{});var au=s(Ur);Rd=i(au,"DPRReaderTokenizer"),au.forEach(t),Aa.forEach(t),oa=l(o),ie=r(o,"DIV",{class:!0});var Ve=s(ie);T(Fo.$$.fragment,Ve),Dd=l(Ve),Vr=r(Ve,"P",{});var iu=s(Vr);yd=i(iu,"Construct a DPRReader tokenizer."),iu.forEach(t),$d=l(Ve),et=r(Ve,"P",{});var Mn=s(et);nr=r(Mn,"A",{href:!0});var du=s(nr);xd=i(du,"DPRReaderTokenizer"),du.forEach(t),zd=i(Mn," is almost identical to "),rr=r(Mn,"A",{href:!0});var lu=s(rr);qd=i(lu,"BertTokenizer"),lu.forEach(t),Fd=i(Mn,` and runs end-to-end tokenization: punctuation
splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts that are
combined to be fed to the `),sr=r(Mn,"A",{href:!0});var cu=s(sr);Cd=i(cu,"DPRReader"),cu.forEach(t),Ad=i(Mn," model."),Mn.forEach(t),Od=l(Ve),Co=r(Ve,"P",{});var Oa=s(Co);Nd=i(Oa,"Refer to superclass "),ar=r(Oa,"A",{href:!0});var pu=s(ar);jd=i(pu,"BertTokenizer"),pu.forEach(t),Md=i(Oa," for usage examples and documentation concerning parameters."),Oa.forEach(t),Qd=l(Ve),Ge=r(Ve,"P",{});var uo=s(Ge);Ld=i(uo,"Return a dictionary with the token ids of the input strings and other information to give to "),Kr=r(uo,"CODE",{});var hu=s(Kr);Id=i(hu,".decode_best_spans"),hu.forEach(t),Sd=i(uo,`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),Yr=r(uo,"CODE",{});var uu=s(Yr);Hd=i(uu,"input_ids"),uu.forEach(t),Bd=i(uo," is a matrix of size "),Xr=r(uo,"CODE",{});var fu=s(Xr);Wd=i(fu,"(n_passages, sequence_length)"),fu.forEach(t),Ud=i(uo,`
with the format:`),uo.forEach(t),Vd=l(Ve),T(Ao.$$.fragment,Ve),Ve.forEach(t),na=l(o),_t=r(o,"H2",{class:!0});var Na=s(_t);Bt=r(Na,"A",{id:!0,class:!0,href:!0});var mu=s(Bt);Jr=r(mu,"SPAN",{});var gu=s(Jr);T(Oo.$$.fragment,gu),gu.forEach(t),mu.forEach(t),Kd=l(Na),Gr=r(Na,"SPAN",{});var _u=s(Gr);Yd=i(_u,"DPRReaderTokenizerFast"),_u.forEach(t),Na.forEach(t),ra=l(o),de=r(o,"DIV",{class:!0});var Ke=s(de);T(No.$$.fragment,Ke),Xd=l(Ke),jo=r(Ke,"P",{});var ja=s(jo);Jd=i(ja,"Constructs a \u201Cfast\u201D DPRReader tokenizer (backed by HuggingFace\u2019s "),Zr=r(ja,"EM",{});var vu=s(Zr);Gd=i(vu,"tokenizers"),vu.forEach(t),Zd=i(ja," library)."),ja.forEach(t),el=l(Ke),tt=r(Ke,"P",{});var Qn=s(tt);ir=r(Qn,"A",{href:!0});var bu=s(ir);tl=i(bu,"DPRReaderTokenizerFast"),bu.forEach(t),ol=i(Qn," is almost identical to "),dr=r(Qn,"A",{href:!0});var Tu=s(dr);nl=i(Tu,"BertTokenizerFast"),Tu.forEach(t),rl=i(Qn,` and runs end-to-end tokenization:
punctuation splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts
that are combined to be fed to the `),lr=r(Qn,"A",{href:!0});var ku=s(lr);sl=i(ku,"DPRReader"),ku.forEach(t),al=i(Qn," model."),Qn.forEach(t),il=l(Ke),Mo=r(Ke,"P",{});var Ma=s(Mo);dl=i(Ma,"Refer to superclass "),cr=r(Ma,"A",{href:!0});var Pu=s(cr);ll=i(Pu,"BertTokenizerFast"),Pu.forEach(t),cl=i(Ma," for usage examples and documentation concerning parameters."),Ma.forEach(t),pl=l(Ke),Ze=r(Ke,"P",{});var fo=s(Ze);hl=i(fo,"Return a dictionary with the token ids of the input strings and other information to give to "),es=r(fo,"CODE",{});var wu=s(es);ul=i(wu,".decode_best_spans"),wu.forEach(t),fl=i(fo,`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),ts=r(fo,"CODE",{});var Eu=s(ts);ml=i(Eu,"input_ids"),Eu.forEach(t),gl=i(fo," is a matrix of size "),os=r(fo,"CODE",{});var Ru=s(os);_l=i(Ru,"(n_passages, sequence_length)"),Ru.forEach(t),vl=i(fo,`
with the format:`),fo.forEach(t),bl=l(Ke),ns=r(Ke,"P",{});var Du=s(ns);Tl=i(Du,"[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>"),Du.forEach(t),Ke.forEach(t),sa=l(o),vt=r(o,"H2",{class:!0});var Qa=s(vt);Wt=r(Qa,"A",{id:!0,class:!0,href:!0});var yu=s(Wt);rs=r(yu,"SPAN",{});var $u=s(rs);T(Qo.$$.fragment,$u),$u.forEach(t),yu.forEach(t),kl=l(Qa),ss=r(Qa,"SPAN",{});var xu=s(ss);Pl=i(xu,"DPR specific outputs"),xu.forEach(t),Qa.forEach(t),aa=l(o),bt=r(o,"DIV",{class:!0});var La=s(bt);T(Lo.$$.fragment,La),wl=l(La),Io=r(La,"P",{});var Ia=s(Io);El=i(Ia,"Class for outputs of "),pr=r(Ia,"A",{href:!0});var zu=s(pr);Rl=i(zu,"DPRQuestionEncoder"),zu.forEach(t),Dl=i(Ia,"."),Ia.forEach(t),La.forEach(t),ia=l(o),Tt=r(o,"DIV",{class:!0});var Sa=s(Tt);T(So.$$.fragment,Sa),yl=l(Sa),Ho=r(Sa,"P",{});var Ha=s(Ho);$l=i(Ha,"Class for outputs of "),hr=r(Ha,"A",{href:!0});var qu=s(hr);xl=i(qu,"DPRQuestionEncoder"),qu.forEach(t),zl=i(Ha,"."),Ha.forEach(t),Sa.forEach(t),da=l(o),kt=r(o,"DIV",{class:!0});var Ba=s(kt);T(Bo.$$.fragment,Ba),ql=l(Ba),Wo=r(Ba,"P",{});var Wa=s(Wo);Fl=i(Wa,"Class for outputs of "),ur=r(Wa,"A",{href:!0});var Fu=s(ur);Cl=i(Fu,"DPRQuestionEncoder"),Fu.forEach(t),Al=i(Wa,"."),Wa.forEach(t),Ba.forEach(t),la=l(o),Pt=r(o,"H2",{class:!0});var Ua=s(Pt);Ut=r(Ua,"A",{id:!0,class:!0,href:!0});var Cu=s(Ut);as=r(Cu,"SPAN",{});var Au=s(as);T(Uo.$$.fragment,Au),Au.forEach(t),Cu.forEach(t),Ol=l(Ua),is=r(Ua,"SPAN",{});var Ou=s(is);Nl=i(Ou,"DPRContextEncoder"),Ou.forEach(t),Ua.forEach(t),ca=l(o),De=r(o,"DIV",{class:!0});var nt=s(De);T(Vo.$$.fragment,nt),jl=l(nt),ds=r(nt,"P",{});var Nu=s(ds);Ml=i(Nu,"The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Nu.forEach(t),Ql=l(nt),Ko=r(nt,"P",{});var Va=s(Ko);Ll=i(Va,"This model inherits from "),fr=r(Va,"A",{href:!0});var ju=s(fr);Il=i(ju,"PreTrainedModel"),ju.forEach(t),Sl=i(Va,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Va.forEach(t),Hl=l(nt),Yo=r(nt,"P",{});var Ka=s(Yo);Bl=i(Ka,"This model is also a PyTorch "),Xo=r(Ka,"A",{href:!0,rel:!0});var Mu=s(Xo);Wl=i(Mu,"torch.nn.Module"),Mu.forEach(t),Ul=i(Ka,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ka.forEach(t),Vl=l(nt),ze=r(nt,"DIV",{class:!0});var rt=s(ze);T(Jo.$$.fragment,rt),Kl=l(rt),wt=r(rt,"P",{});var yr=s(wt);Yl=i(yr,"The "),mr=r(yr,"A",{href:!0});var Qu=s(mr);Xl=i(Qu,"DPRContextEncoder"),Qu.forEach(t),Jl=i(yr," forward method, overrides the "),ls=r(yr,"CODE",{});var Lu=s(ls);Gl=i(Lu,"__call__"),Lu.forEach(t),Zl=i(yr," special method."),yr.forEach(t),ec=l(rt),T(Vt.$$.fragment,rt),tc=l(rt),cs=r(rt,"P",{});var Iu=s(cs);oc=i(Iu,"Examples:"),Iu.forEach(t),nc=l(rt),T(Go.$$.fragment,rt),rt.forEach(t),nt.forEach(t),pa=l(o),Et=r(o,"H2",{class:!0});var Ya=s(Et);Kt=r(Ya,"A",{id:!0,class:!0,href:!0});var Su=s(Kt);ps=r(Su,"SPAN",{});var Hu=s(ps);T(Zo.$$.fragment,Hu),Hu.forEach(t),Su.forEach(t),rc=l(Ya),hs=r(Ya,"SPAN",{});var Bu=s(hs);sc=i(Bu,"DPRQuestionEncoder"),Bu.forEach(t),Ya.forEach(t),ha=l(o),ye=r(o,"DIV",{class:!0});var st=s(ye);T(en.$$.fragment,st),ac=l(st),us=r(st,"P",{});var Wu=s(us);ic=i(Wu,"The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),Wu.forEach(t),dc=l(st),tn=r(st,"P",{});var Xa=s(tn);lc=i(Xa,"This model inherits from "),gr=r(Xa,"A",{href:!0});var Uu=s(gr);cc=i(Uu,"PreTrainedModel"),Uu.forEach(t),pc=i(Xa,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Xa.forEach(t),hc=l(st),on=r(st,"P",{});var Ja=s(on);uc=i(Ja,"This model is also a PyTorch "),nn=r(Ja,"A",{href:!0,rel:!0});var Vu=s(nn);fc=i(Vu,"torch.nn.Module"),Vu.forEach(t),mc=i(Ja,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ja.forEach(t),gc=l(st),qe=r(st,"DIV",{class:!0});var at=s(qe);T(rn.$$.fragment,at),_c=l(at),Rt=r(at,"P",{});var $r=s(Rt);vc=i($r,"The "),_r=r($r,"A",{href:!0});var Ku=s(_r);bc=i(Ku,"DPRQuestionEncoder"),Ku.forEach(t),Tc=i($r," forward method, overrides the "),fs=r($r,"CODE",{});var Yu=s(fs);kc=i(Yu,"__call__"),Yu.forEach(t),Pc=i($r," special method."),$r.forEach(t),wc=l(at),T(Yt.$$.fragment,at),Ec=l(at),ms=r(at,"P",{});var Xu=s(ms);Rc=i(Xu,"Examples:"),Xu.forEach(t),Dc=l(at),T(sn.$$.fragment,at),at.forEach(t),st.forEach(t),ua=l(o),Dt=r(o,"H2",{class:!0});var Ga=s(Dt);Xt=r(Ga,"A",{id:!0,class:!0,href:!0});var Ju=s(Xt);gs=r(Ju,"SPAN",{});var Gu=s(gs);T(an.$$.fragment,Gu),Gu.forEach(t),Ju.forEach(t),yc=l(Ga),_s=r(Ga,"SPAN",{});var Zu=s(_s);$c=i(Zu,"DPRReader"),Zu.forEach(t),Ga.forEach(t),fa=l(o),$e=r(o,"DIV",{class:!0});var it=s($e);T(dn.$$.fragment,it),xc=l(it),vs=r(it,"P",{});var ef=s(vs);zc=i(ef,"The bare DPRReader transformer outputting span predictions."),ef.forEach(t),qc=l(it),ln=r(it,"P",{});var Za=s(ln);Fc=i(Za,"This model inherits from "),vr=r(Za,"A",{href:!0});var tf=s(vr);Cc=i(tf,"PreTrainedModel"),tf.forEach(t),Ac=i(Za,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Za.forEach(t),Oc=l(it),cn=r(it,"P",{});var ei=s(cn);Nc=i(ei,"This model is also a PyTorch "),pn=r(ei,"A",{href:!0,rel:!0});var of=s(pn);jc=i(of,"torch.nn.Module"),of.forEach(t),Mc=i(ei,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ei.forEach(t),Qc=l(it),Fe=r(it,"DIV",{class:!0});var dt=s(Fe);T(hn.$$.fragment,dt),Lc=l(dt),yt=r(dt,"P",{});var xr=s(yt);Ic=i(xr,"The "),br=r(xr,"A",{href:!0});var nf=s(br);Sc=i(nf,"DPRReader"),nf.forEach(t),Hc=i(xr," forward method, overrides the "),bs=r(xr,"CODE",{});var rf=s(bs);Bc=i(rf,"__call__"),rf.forEach(t),Wc=i(xr," special method."),xr.forEach(t),Uc=l(dt),T(Jt.$$.fragment,dt),Vc=l(dt),Ts=r(dt,"P",{});var sf=s(Ts);Kc=i(sf,"Examples:"),sf.forEach(t),Yc=l(dt),T(un.$$.fragment,dt),dt.forEach(t),it.forEach(t),ma=l(o),$t=r(o,"H2",{class:!0});var ti=s($t);Gt=r(ti,"A",{id:!0,class:!0,href:!0});var af=s(Gt);ks=r(af,"SPAN",{});var df=s(ks);T(fn.$$.fragment,df),df.forEach(t),af.forEach(t),Xc=l(ti),Ps=r(ti,"SPAN",{});var lf=s(Ps);Jc=i(lf,"TFDPRContextEncoder"),lf.forEach(t),ti.forEach(t),ga=l(o),le=r(o,"DIV",{class:!0});var Ye=s(le);T(mn.$$.fragment,Ye),Gc=l(Ye),ws=r(Ye,"P",{});var cf=s(ws);Zc=i(cf,"The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),cf.forEach(t),ep=l(Ye),gn=r(Ye,"P",{});var oi=s(gn);tp=i(oi,"This model inherits from "),Tr=r(oi,"A",{href:!0});var pf=s(Tr);op=i(pf,"TFPreTrainedModel"),pf.forEach(t),np=i(oi,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),oi.forEach(t),rp=l(Ye),_n=r(Ye,"P",{});var ni=s(_n);sp=i(ni,"This model is also a Tensorflow "),vn=r(ni,"A",{href:!0,rel:!0});var hf=s(vn);ap=i(hf,"tf.keras.Model"),hf.forEach(t),ip=i(ni,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),ni.forEach(t),dp=l(Ye),T(Zt.$$.fragment,Ye),lp=l(Ye),Ce=r(Ye,"DIV",{class:!0});var lt=s(Ce);T(bn.$$.fragment,lt),cp=l(lt),xt=r(lt,"P",{});var zr=s(xt);pp=i(zr,"The "),kr=r(zr,"A",{href:!0});var uf=s(kr);hp=i(uf,"TFDPRContextEncoder"),uf.forEach(t),up=i(zr," forward method, overrides the "),Es=r(zr,"CODE",{});var ff=s(Es);fp=i(ff,"__call__"),ff.forEach(t),mp=i(zr," special method."),zr.forEach(t),gp=l(lt),T(eo.$$.fragment,lt),_p=l(lt),Rs=r(lt,"P",{});var mf=s(Rs);vp=i(mf,"Examples:"),mf.forEach(t),bp=l(lt),T(Tn.$$.fragment,lt),lt.forEach(t),Ye.forEach(t),_a=l(o),zt=r(o,"H2",{class:!0});var ri=s(zt);to=r(ri,"A",{id:!0,class:!0,href:!0});var gf=s(to);Ds=r(gf,"SPAN",{});var _f=s(Ds);T(kn.$$.fragment,_f),_f.forEach(t),gf.forEach(t),Tp=l(ri),ys=r(ri,"SPAN",{});var vf=s(ys);kp=i(vf,"TFDPRQuestionEncoder"),vf.forEach(t),ri.forEach(t),va=l(o),ce=r(o,"DIV",{class:!0});var Xe=s(ce);T(Pn.$$.fragment,Xe),Pp=l(Xe),$s=r(Xe,"P",{});var bf=s($s);wp=i(bf,"The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),bf.forEach(t),Ep=l(Xe),wn=r(Xe,"P",{});var si=s(wn);Rp=i(si,"This model inherits from "),Pr=r(si,"A",{href:!0});var Tf=s(Pr);Dp=i(Tf,"TFPreTrainedModel"),Tf.forEach(t),yp=i(si,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),si.forEach(t),$p=l(Xe),En=r(Xe,"P",{});var ai=s(En);xp=i(ai,"This model is also a Tensorflow "),Rn=r(ai,"A",{href:!0,rel:!0});var kf=s(Rn);zp=i(kf,"tf.keras.Model"),kf.forEach(t),qp=i(ai,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),ai.forEach(t),Fp=l(Xe),T(oo.$$.fragment,Xe),Cp=l(Xe),Ae=r(Xe,"DIV",{class:!0});var ct=s(Ae);T(Dn.$$.fragment,ct),Ap=l(ct),qt=r(ct,"P",{});var qr=s(qt);Op=i(qr,"The "),wr=r(qr,"A",{href:!0});var Pf=s(wr);Np=i(Pf,"TFDPRQuestionEncoder"),Pf.forEach(t),jp=i(qr," forward method, overrides the "),xs=r(qr,"CODE",{});var wf=s(xs);Mp=i(wf,"__call__"),wf.forEach(t),Qp=i(qr," special method."),qr.forEach(t),Lp=l(ct),T(no.$$.fragment,ct),Ip=l(ct),zs=r(ct,"P",{});var Ef=s(zs);Sp=i(Ef,"Examples:"),Ef.forEach(t),Hp=l(ct),T(yn.$$.fragment,ct),ct.forEach(t),Xe.forEach(t),ba=l(o),Ft=r(o,"H2",{class:!0});var ii=s(Ft);ro=r(ii,"A",{id:!0,class:!0,href:!0});var Rf=s(ro);qs=r(Rf,"SPAN",{});var Df=s(qs);T($n.$$.fragment,Df),Df.forEach(t),Rf.forEach(t),Bp=l(ii),Fs=r(ii,"SPAN",{});var yf=s(Fs);Wp=i(yf,"TFDPRReader"),yf.forEach(t),ii.forEach(t),Ta=l(o),pe=r(o,"DIV",{class:!0});var Je=s(pe);T(xn.$$.fragment,Je),Up=l(Je),Cs=r(Je,"P",{});var $f=s(Cs);Vp=i($f,"The bare DPRReader transformer outputting span predictions."),$f.forEach(t),Kp=l(Je),zn=r(Je,"P",{});var di=s(zn);Yp=i(di,"This model inherits from "),Er=r(di,"A",{href:!0});var xf=s(Er);Xp=i(xf,"TFPreTrainedModel"),xf.forEach(t),Jp=i(di,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),di.forEach(t),Gp=l(Je),qn=r(Je,"P",{});var li=s(qn);Zp=i(li,"This model is also a Tensorflow "),Fn=r(li,"A",{href:!0,rel:!0});var zf=s(Fn);eh=i(zf,"tf.keras.Model"),zf.forEach(t),th=i(li,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),li.forEach(t),oh=l(Je),T(so.$$.fragment,Je),nh=l(Je),Oe=r(Je,"DIV",{class:!0});var pt=s(Oe);T(Cn.$$.fragment,pt),rh=l(pt),Ct=r(pt,"P",{});var Fr=s(Ct);sh=i(Fr,"The "),Rr=r(Fr,"A",{href:!0});var qf=s(Rr);ah=i(qf,"TFDPRReader"),qf.forEach(t),ih=i(Fr," forward method, overrides the "),As=r(Fr,"CODE",{});var Ff=s(As);dh=i(Ff,"__call__"),Ff.forEach(t),lh=i(Fr," special method."),Fr.forEach(t),ch=l(pt),T(ao.$$.fragment,pt),ph=l(pt),Os=r(pt,"P",{});var Cf=s(Os);hh=i(Cf,"Examples:"),Cf.forEach(t),uh=l(pt),T(An.$$.fragment,pt),pt.forEach(t),Je.forEach(t),this.h()},h(){c(f,"name","hf:doc:metadata"),c(f,"content",JSON.stringify(Yf)),c(g,"id","dpr"),c(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g,"href","#dpr"),c(m,"class","relative group"),c(J,"id","overview"),c(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J,"href","#overview"),c(D,"class","relative group"),c(te,"href","https://arxiv.org/abs/2004.04906"),c(te,"rel","nofollow"),c(N,"href","https://huggingface.co/lhoestq"),c(N,"rel","nofollow"),c(j,"href","https://github.com/facebookresearch/DPR"),c(j,"rel","nofollow"),c(O,"id","transformers.DPRConfig"),c(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(O,"href","#transformers.DPRConfig"),c(V,"class","relative group"),c(A,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig"),c(In,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRContextEncoder"),c(Sn,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(Hn,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRReader"),c(mo,"href","https://huggingface.co/facebook/dpr-ctx_encoder-single-nq-base"),c(mo,"rel","nofollow"),c(Bn,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertConfig"),c(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Ot,"id","transformers.DPRContextEncoderTokenizer"),c(Ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ot,"href","#transformers.DPRContextEncoderTokenizer"),c(ht,"class","relative group"),c(Wn,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRContextEncoderTokenizer"),c(Un,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),c(Vn,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),c(je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(jt,"id","transformers.DPRContextEncoderTokenizerFast"),c(jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(jt,"href","#transformers.DPRContextEncoderTokenizerFast"),c(ut,"class","relative group"),c(Kn,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRContextEncoderTokenizerFast"),c(Yn,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),c(Xn,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),c(Me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Qt,"id","transformers.DPRQuestionEncoderTokenizer"),c(Qt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qt,"href","#transformers.DPRQuestionEncoderTokenizer"),c(ft,"class","relative group"),c(Jn,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),c(Gn,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),c(Zn,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),c(Qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(It,"id","transformers.DPRQuestionEncoderTokenizerFast"),c(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(It,"href","#transformers.DPRQuestionEncoderTokenizerFast"),c(mt,"class","relative group"),c(er,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),c(tr,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),c(or,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),c(Le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Ht,"id","transformers.DPRReaderTokenizer"),c(Ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ht,"href","#transformers.DPRReaderTokenizer"),c(gt,"class","relative group"),c(nr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRReaderTokenizer"),c(rr,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),c(sr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRReader"),c(ar,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),c(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Bt,"id","transformers.DPRReaderTokenizerFast"),c(Bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bt,"href","#transformers.DPRReaderTokenizerFast"),c(_t,"class","relative group"),c(ir,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRReaderTokenizerFast"),c(dr,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),c(lr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRReader"),c(cr,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),c(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Wt,"id","transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"),c(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Wt,"href","#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"),c(vt,"class","relative group"),c(pr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(bt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(hr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(Tt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ur,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(kt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Ut,"id","transformers.DPRContextEncoder"),c(Ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ut,"href","#transformers.DPRContextEncoder"),c(Pt,"class","relative group"),c(fr,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(Xo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Xo,"rel","nofollow"),c(mr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRContextEncoder"),c(ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(De,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Kt,"id","transformers.DPRQuestionEncoder"),c(Kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Kt,"href","#transformers.DPRQuestionEncoder"),c(Et,"class","relative group"),c(gr,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(nn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(nn,"rel","nofollow"),c(_r,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Xt,"id","transformers.DPRReader"),c(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Xt,"href","#transformers.DPRReader"),c(Dt,"class","relative group"),c(vr,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(pn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(pn,"rel","nofollow"),c(br,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRReader"),c(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c($e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Gt,"id","transformers.TFDPRContextEncoder"),c(Gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Gt,"href","#transformers.TFDPRContextEncoder"),c($t,"class","relative group"),c(Tr,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(vn,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(vn,"rel","nofollow"),c(kr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.TFDPRContextEncoder"),c(Ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(to,"id","transformers.TFDPRQuestionEncoder"),c(to,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(to,"href","#transformers.TFDPRQuestionEncoder"),c(zt,"class","relative group"),c(Pr,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(Rn,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Rn,"rel","nofollow"),c(wr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),c(Ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ro,"id","transformers.TFDPRReader"),c(ro,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ro,"href","#transformers.TFDPRReader"),c(Ft,"class","relative group"),c(Er,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(Fn,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Fn,"rel","nofollow"),c(Rr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.TFDPRReader"),c(Oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,u){e(document.head,f),p(o,x,u),p(o,m,u),e(m,g),e(g,R),k(v,R,null),e(m,_),e(m,$),e($,_e),p(o,Z,u),p(o,D,u),e(D,J),e(J,I),k(ee,I,null),e(D,ve),e(D,S),e(S,be),p(o,he,u),p(o,H,u),e(H,L),e(H,te),e(te,oe),e(H,q),p(o,C,u),p(o,ne,u),e(ne,U),p(o,ue,u),p(o,re,u),e(re,B),e(B,Te),p(o,fe,u),p(o,z,u),e(z,ke),e(z,N),e(N,Pe),e(z,we),e(z,j),e(j,Ee),e(z,Re),p(o,M,u),p(o,V,u),e(V,O),e(O,se),k(h,se,null),e(V,y),e(V,G),e(G,Ie),p(o,Ne,u),p(o,F,u),k(me,F,null),e(F,Se),e(F,ae),e(ae,A),e(A,K),e(ae,He),e(ae,xe),e(xe,Y),e(ae,Be),e(F,We),e(F,Q),e(Q,Ue),e(Q,In),e(In,ci),e(Q,pi),e(Q,Sn),e(Sn,hi),e(Q,ui),e(Q,Hn),e(Hn,fi),e(Q,mi),e(Q,mo),e(mo,gi),e(Q,_i),e(F,vi),e(F,go),e(go,bi),e(go,Bn),e(Bn,Ti),e(go,ki),p(o,Vs,u),p(o,ht,u),e(ht,Ot),e(Ot,Cr),k(_o,Cr,null),e(ht,Pi),e(ht,Ar),e(Ar,wi),p(o,Ks,u),p(o,je,u),k(vo,je,null),e(je,Ei),e(je,Or),e(Or,Ri),e(je,Di),e(je,Nt),e(Nt,Wn),e(Wn,yi),e(Nt,$i),e(Nt,Un),e(Un,xi),e(Nt,zi),e(je,qi),e(je,bo),e(bo,Fi),e(bo,Vn),e(Vn,Ci),e(bo,Ai),p(o,Ys,u),p(o,ut,u),e(ut,jt),e(jt,Nr),k(To,Nr,null),e(ut,Oi),e(ut,jr),e(jr,Ni),p(o,Xs,u),p(o,Me,u),k(ko,Me,null),e(Me,ji),e(Me,Po),e(Po,Mi),e(Po,Mr),e(Mr,Qi),e(Po,Li),e(Me,Ii),e(Me,Mt),e(Mt,Kn),e(Kn,Si),e(Mt,Hi),e(Mt,Yn),e(Yn,Bi),e(Mt,Wi),e(Me,Ui),e(Me,wo),e(wo,Vi),e(wo,Xn),e(Xn,Ki),e(wo,Yi),p(o,Js,u),p(o,ft,u),e(ft,Qt),e(Qt,Qr),k(Eo,Qr,null),e(ft,Xi),e(ft,Lr),e(Lr,Ji),p(o,Gs,u),p(o,Qe,u),k(Ro,Qe,null),e(Qe,Gi),e(Qe,Ir),e(Ir,Zi),e(Qe,ed),e(Qe,Lt),e(Lt,Jn),e(Jn,td),e(Lt,od),e(Lt,Gn),e(Gn,nd),e(Lt,rd),e(Qe,sd),e(Qe,Do),e(Do,ad),e(Do,Zn),e(Zn,id),e(Do,dd),p(o,Zs,u),p(o,mt,u),e(mt,It),e(It,Sr),k(yo,Sr,null),e(mt,ld),e(mt,Hr),e(Hr,cd),p(o,ea,u),p(o,Le,u),k($o,Le,null),e(Le,pd),e(Le,xo),e(xo,hd),e(xo,Br),e(Br,ud),e(xo,fd),e(Le,md),e(Le,St),e(St,er),e(er,gd),e(St,_d),e(St,tr),e(tr,vd),e(St,bd),e(Le,Td),e(Le,zo),e(zo,kd),e(zo,or),e(or,Pd),e(zo,wd),p(o,ta,u),p(o,gt,u),e(gt,Ht),e(Ht,Wr),k(qo,Wr,null),e(gt,Ed),e(gt,Ur),e(Ur,Rd),p(o,oa,u),p(o,ie,u),k(Fo,ie,null),e(ie,Dd),e(ie,Vr),e(Vr,yd),e(ie,$d),e(ie,et),e(et,nr),e(nr,xd),e(et,zd),e(et,rr),e(rr,qd),e(et,Fd),e(et,sr),e(sr,Cd),e(et,Ad),e(ie,Od),e(ie,Co),e(Co,Nd),e(Co,ar),e(ar,jd),e(Co,Md),e(ie,Qd),e(ie,Ge),e(Ge,Ld),e(Ge,Kr),e(Kr,Id),e(Ge,Sd),e(Ge,Yr),e(Yr,Hd),e(Ge,Bd),e(Ge,Xr),e(Xr,Wd),e(Ge,Ud),e(ie,Vd),k(Ao,ie,null),p(o,na,u),p(o,_t,u),e(_t,Bt),e(Bt,Jr),k(Oo,Jr,null),e(_t,Kd),e(_t,Gr),e(Gr,Yd),p(o,ra,u),p(o,de,u),k(No,de,null),e(de,Xd),e(de,jo),e(jo,Jd),e(jo,Zr),e(Zr,Gd),e(jo,Zd),e(de,el),e(de,tt),e(tt,ir),e(ir,tl),e(tt,ol),e(tt,dr),e(dr,nl),e(tt,rl),e(tt,lr),e(lr,sl),e(tt,al),e(de,il),e(de,Mo),e(Mo,dl),e(Mo,cr),e(cr,ll),e(Mo,cl),e(de,pl),e(de,Ze),e(Ze,hl),e(Ze,es),e(es,ul),e(Ze,fl),e(Ze,ts),e(ts,ml),e(Ze,gl),e(Ze,os),e(os,_l),e(Ze,vl),e(de,bl),e(de,ns),e(ns,Tl),p(o,sa,u),p(o,vt,u),e(vt,Wt),e(Wt,rs),k(Qo,rs,null),e(vt,kl),e(vt,ss),e(ss,Pl),p(o,aa,u),p(o,bt,u),k(Lo,bt,null),e(bt,wl),e(bt,Io),e(Io,El),e(Io,pr),e(pr,Rl),e(Io,Dl),p(o,ia,u),p(o,Tt,u),k(So,Tt,null),e(Tt,yl),e(Tt,Ho),e(Ho,$l),e(Ho,hr),e(hr,xl),e(Ho,zl),p(o,da,u),p(o,kt,u),k(Bo,kt,null),e(kt,ql),e(kt,Wo),e(Wo,Fl),e(Wo,ur),e(ur,Cl),e(Wo,Al),p(o,la,u),p(o,Pt,u),e(Pt,Ut),e(Ut,as),k(Uo,as,null),e(Pt,Ol),e(Pt,is),e(is,Nl),p(o,ca,u),p(o,De,u),k(Vo,De,null),e(De,jl),e(De,ds),e(ds,Ml),e(De,Ql),e(De,Ko),e(Ko,Ll),e(Ko,fr),e(fr,Il),e(Ko,Sl),e(De,Hl),e(De,Yo),e(Yo,Bl),e(Yo,Xo),e(Xo,Wl),e(Yo,Ul),e(De,Vl),e(De,ze),k(Jo,ze,null),e(ze,Kl),e(ze,wt),e(wt,Yl),e(wt,mr),e(mr,Xl),e(wt,Jl),e(wt,ls),e(ls,Gl),e(wt,Zl),e(ze,ec),k(Vt,ze,null),e(ze,tc),e(ze,cs),e(cs,oc),e(ze,nc),k(Go,ze,null),p(o,pa,u),p(o,Et,u),e(Et,Kt),e(Kt,ps),k(Zo,ps,null),e(Et,rc),e(Et,hs),e(hs,sc),p(o,ha,u),p(o,ye,u),k(en,ye,null),e(ye,ac),e(ye,us),e(us,ic),e(ye,dc),e(ye,tn),e(tn,lc),e(tn,gr),e(gr,cc),e(tn,pc),e(ye,hc),e(ye,on),e(on,uc),e(on,nn),e(nn,fc),e(on,mc),e(ye,gc),e(ye,qe),k(rn,qe,null),e(qe,_c),e(qe,Rt),e(Rt,vc),e(Rt,_r),e(_r,bc),e(Rt,Tc),e(Rt,fs),e(fs,kc),e(Rt,Pc),e(qe,wc),k(Yt,qe,null),e(qe,Ec),e(qe,ms),e(ms,Rc),e(qe,Dc),k(sn,qe,null),p(o,ua,u),p(o,Dt,u),e(Dt,Xt),e(Xt,gs),k(an,gs,null),e(Dt,yc),e(Dt,_s),e(_s,$c),p(o,fa,u),p(o,$e,u),k(dn,$e,null),e($e,xc),e($e,vs),e(vs,zc),e($e,qc),e($e,ln),e(ln,Fc),e(ln,vr),e(vr,Cc),e(ln,Ac),e($e,Oc),e($e,cn),e(cn,Nc),e(cn,pn),e(pn,jc),e(cn,Mc),e($e,Qc),e($e,Fe),k(hn,Fe,null),e(Fe,Lc),e(Fe,yt),e(yt,Ic),e(yt,br),e(br,Sc),e(yt,Hc),e(yt,bs),e(bs,Bc),e(yt,Wc),e(Fe,Uc),k(Jt,Fe,null),e(Fe,Vc),e(Fe,Ts),e(Ts,Kc),e(Fe,Yc),k(un,Fe,null),p(o,ma,u),p(o,$t,u),e($t,Gt),e(Gt,ks),k(fn,ks,null),e($t,Xc),e($t,Ps),e(Ps,Jc),p(o,ga,u),p(o,le,u),k(mn,le,null),e(le,Gc),e(le,ws),e(ws,Zc),e(le,ep),e(le,gn),e(gn,tp),e(gn,Tr),e(Tr,op),e(gn,np),e(le,rp),e(le,_n),e(_n,sp),e(_n,vn),e(vn,ap),e(_n,ip),e(le,dp),k(Zt,le,null),e(le,lp),e(le,Ce),k(bn,Ce,null),e(Ce,cp),e(Ce,xt),e(xt,pp),e(xt,kr),e(kr,hp),e(xt,up),e(xt,Es),e(Es,fp),e(xt,mp),e(Ce,gp),k(eo,Ce,null),e(Ce,_p),e(Ce,Rs),e(Rs,vp),e(Ce,bp),k(Tn,Ce,null),p(o,_a,u),p(o,zt,u),e(zt,to),e(to,Ds),k(kn,Ds,null),e(zt,Tp),e(zt,ys),e(ys,kp),p(o,va,u),p(o,ce,u),k(Pn,ce,null),e(ce,Pp),e(ce,$s),e($s,wp),e(ce,Ep),e(ce,wn),e(wn,Rp),e(wn,Pr),e(Pr,Dp),e(wn,yp),e(ce,$p),e(ce,En),e(En,xp),e(En,Rn),e(Rn,zp),e(En,qp),e(ce,Fp),k(oo,ce,null),e(ce,Cp),e(ce,Ae),k(Dn,Ae,null),e(Ae,Ap),e(Ae,qt),e(qt,Op),e(qt,wr),e(wr,Np),e(qt,jp),e(qt,xs),e(xs,Mp),e(qt,Qp),e(Ae,Lp),k(no,Ae,null),e(Ae,Ip),e(Ae,zs),e(zs,Sp),e(Ae,Hp),k(yn,Ae,null),p(o,ba,u),p(o,Ft,u),e(Ft,ro),e(ro,qs),k($n,qs,null),e(Ft,Bp),e(Ft,Fs),e(Fs,Wp),p(o,Ta,u),p(o,pe,u),k(xn,pe,null),e(pe,Up),e(pe,Cs),e(Cs,Vp),e(pe,Kp),e(pe,zn),e(zn,Yp),e(zn,Er),e(Er,Xp),e(zn,Jp),e(pe,Gp),e(pe,qn),e(qn,Zp),e(qn,Fn),e(Fn,eh),e(qn,th),e(pe,oh),k(so,pe,null),e(pe,nh),e(pe,Oe),k(Cn,Oe,null),e(Oe,rh),e(Oe,Ct),e(Ct,sh),e(Ct,Rr),e(Rr,ah),e(Ct,ih),e(Ct,As),e(As,dh),e(Ct,lh),e(Oe,ch),k(ao,Oe,null),e(Oe,ph),e(Oe,Os),e(Os,hh),e(Oe,uh),k(An,Oe,null),ka=!0},p(o,[u]){const On={};u&2&&(On.$$scope={dirty:u,ctx:o}),Vt.$set(On);const Ns={};u&2&&(Ns.$$scope={dirty:u,ctx:o}),Yt.$set(Ns);const js={};u&2&&(js.$$scope={dirty:u,ctx:o}),Jt.$set(js);const Ms={};u&2&&(Ms.$$scope={dirty:u,ctx:o}),Zt.$set(Ms);const Nn={};u&2&&(Nn.$$scope={dirty:u,ctx:o}),eo.$set(Nn);const Qs={};u&2&&(Qs.$$scope={dirty:u,ctx:o}),oo.$set(Qs);const Ls={};u&2&&(Ls.$$scope={dirty:u,ctx:o}),no.$set(Ls);const Is={};u&2&&(Is.$$scope={dirty:u,ctx:o}),so.$set(Is);const jn={};u&2&&(jn.$$scope={dirty:u,ctx:o}),ao.$set(jn)},i(o){ka||(P(v.$$.fragment,o),P(ee.$$.fragment,o),P(h.$$.fragment,o),P(me.$$.fragment,o),P(_o.$$.fragment,o),P(vo.$$.fragment,o),P(To.$$.fragment,o),P(ko.$$.fragment,o),P(Eo.$$.fragment,o),P(Ro.$$.fragment,o),P(yo.$$.fragment,o),P($o.$$.fragment,o),P(qo.$$.fragment,o),P(Fo.$$.fragment,o),P(Ao.$$.fragment,o),P(Oo.$$.fragment,o),P(No.$$.fragment,o),P(Qo.$$.fragment,o),P(Lo.$$.fragment,o),P(So.$$.fragment,o),P(Bo.$$.fragment,o),P(Uo.$$.fragment,o),P(Vo.$$.fragment,o),P(Jo.$$.fragment,o),P(Vt.$$.fragment,o),P(Go.$$.fragment,o),P(Zo.$$.fragment,o),P(en.$$.fragment,o),P(rn.$$.fragment,o),P(Yt.$$.fragment,o),P(sn.$$.fragment,o),P(an.$$.fragment,o),P(dn.$$.fragment,o),P(hn.$$.fragment,o),P(Jt.$$.fragment,o),P(un.$$.fragment,o),P(fn.$$.fragment,o),P(mn.$$.fragment,o),P(Zt.$$.fragment,o),P(bn.$$.fragment,o),P(eo.$$.fragment,o),P(Tn.$$.fragment,o),P(kn.$$.fragment,o),P(Pn.$$.fragment,o),P(oo.$$.fragment,o),P(Dn.$$.fragment,o),P(no.$$.fragment,o),P(yn.$$.fragment,o),P($n.$$.fragment,o),P(xn.$$.fragment,o),P(so.$$.fragment,o),P(Cn.$$.fragment,o),P(ao.$$.fragment,o),P(An.$$.fragment,o),ka=!0)},o(o){w(v.$$.fragment,o),w(ee.$$.fragment,o),w(h.$$.fragment,o),w(me.$$.fragment,o),w(_o.$$.fragment,o),w(vo.$$.fragment,o),w(To.$$.fragment,o),w(ko.$$.fragment,o),w(Eo.$$.fragment,o),w(Ro.$$.fragment,o),w(yo.$$.fragment,o),w($o.$$.fragment,o),w(qo.$$.fragment,o),w(Fo.$$.fragment,o),w(Ao.$$.fragment,o),w(Oo.$$.fragment,o),w(No.$$.fragment,o),w(Qo.$$.fragment,o),w(Lo.$$.fragment,o),w(So.$$.fragment,o),w(Bo.$$.fragment,o),w(Uo.$$.fragment,o),w(Vo.$$.fragment,o),w(Jo.$$.fragment,o),w(Vt.$$.fragment,o),w(Go.$$.fragment,o),w(Zo.$$.fragment,o),w(en.$$.fragment,o),w(rn.$$.fragment,o),w(Yt.$$.fragment,o),w(sn.$$.fragment,o),w(an.$$.fragment,o),w(dn.$$.fragment,o),w(hn.$$.fragment,o),w(Jt.$$.fragment,o),w(un.$$.fragment,o),w(fn.$$.fragment,o),w(mn.$$.fragment,o),w(Zt.$$.fragment,o),w(bn.$$.fragment,o),w(eo.$$.fragment,o),w(Tn.$$.fragment,o),w(kn.$$.fragment,o),w(Pn.$$.fragment,o),w(oo.$$.fragment,o),w(Dn.$$.fragment,o),w(no.$$.fragment,o),w(yn.$$.fragment,o),w($n.$$.fragment,o),w(xn.$$.fragment,o),w(so.$$.fragment,o),w(Cn.$$.fragment,o),w(ao.$$.fragment,o),w(An.$$.fragment,o),ka=!1},d(o){t(f),o&&t(x),o&&t(m),E(v),o&&t(Z),o&&t(D),E(ee),o&&t(he),o&&t(H),o&&t(C),o&&t(ne),o&&t(ue),o&&t(re),o&&t(fe),o&&t(z),o&&t(M),o&&t(V),E(h),o&&t(Ne),o&&t(F),E(me),o&&t(Vs),o&&t(ht),E(_o),o&&t(Ks),o&&t(je),E(vo),o&&t(Ys),o&&t(ut),E(To),o&&t(Xs),o&&t(Me),E(ko),o&&t(Js),o&&t(ft),E(Eo),o&&t(Gs),o&&t(Qe),E(Ro),o&&t(Zs),o&&t(mt),E(yo),o&&t(ea),o&&t(Le),E($o),o&&t(ta),o&&t(gt),E(qo),o&&t(oa),o&&t(ie),E(Fo),E(Ao),o&&t(na),o&&t(_t),E(Oo),o&&t(ra),o&&t(de),E(No),o&&t(sa),o&&t(vt),E(Qo),o&&t(aa),o&&t(bt),E(Lo),o&&t(ia),o&&t(Tt),E(So),o&&t(da),o&&t(kt),E(Bo),o&&t(la),o&&t(Pt),E(Uo),o&&t(ca),o&&t(De),E(Vo),E(Jo),E(Vt),E(Go),o&&t(pa),o&&t(Et),E(Zo),o&&t(ha),o&&t(ye),E(en),E(rn),E(Yt),E(sn),o&&t(ua),o&&t(Dt),E(an),o&&t(fa),o&&t($e),E(dn),E(hn),E(Jt),E(un),o&&t(ma),o&&t($t),E(fn),o&&t(ga),o&&t(le),E(mn),E(Zt),E(bn),E(eo),E(Tn),o&&t(_a),o&&t(zt),E(kn),o&&t(va),o&&t(ce),E(Pn),E(oo),E(Dn),E(no),E(yn),o&&t(ba),o&&t(Ft),E($n),o&&t(Ta),o&&t(pe),E(xn),E(so),E(Cn),E(ao),E(An)}}}const Yf={local:"dpr",sections:[{local:"overview",title:"Overview"},{local:"transformers.DPRConfig",title:"DPRConfig"},{local:"transformers.DPRContextEncoderTokenizer",title:"DPRContextEncoderTokenizer"},{local:"transformers.DPRContextEncoderTokenizerFast",title:"DPRContextEncoderTokenizerFast"},{local:"transformers.DPRQuestionEncoderTokenizer",title:"DPRQuestionEncoderTokenizer"},{local:"transformers.DPRQuestionEncoderTokenizerFast",title:"DPRQuestionEncoderTokenizerFast"},{local:"transformers.DPRReaderTokenizer",title:"DPRReaderTokenizer"},{local:"transformers.DPRReaderTokenizerFast",title:"DPRReaderTokenizerFast"},{local:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",title:"DPR specific outputs"},{local:"transformers.DPRContextEncoder",title:"DPRContextEncoder"},{local:"transformers.DPRQuestionEncoder",title:"DPRQuestionEncoder"},{local:"transformers.DPRReader",title:"DPRReader"},{local:"transformers.TFDPRContextEncoder",title:"TFDPRContextEncoder"},{local:"transformers.TFDPRQuestionEncoder",title:"TFDPRQuestionEncoder"},{local:"transformers.TFDPRReader",title:"TFDPRReader"}],title:"DPR"};function Xf(X){return Mf(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class om extends Af{constructor(f){super();Of(this,f,Xf,Kf,Nf,{})}}export{om as default,Yf as metadata};
