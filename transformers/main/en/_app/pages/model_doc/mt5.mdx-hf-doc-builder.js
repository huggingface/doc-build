import{S as Wd,i as Bd,s as Hd,e as s,k as p,w as v,t as l,M as Rd,c as n,d as r,m,a as o,x as k,h as i,b as d,G as t,g as f,y as $,q as b,o as y,B as w,v as Xd,L as me}from"../../chunks/vendor-hf-doc-builder.js";import{D as z}from"../../chunks/Docstring-hf-doc-builder.js";import{C as ce}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as P}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as pe}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Kd(M){let g,x,_,u,T;return u=new ce({props:{code:`from transformers import MT5Model, T5Tokenizer

model = MT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="pt")
labels = tokenizer(text_target=summary, return_tensors="pt")

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=labels["input_ids"])
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(text_target=summary, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),{c(){g=s("p"),x=l("Examples:"),_=p(),v(u.$$.fragment)},l(a){g=n(a,"P",{});var h=o(g);x=i(h,"Examples:"),h.forEach(r),_=m(a),k(u.$$.fragment,a)},m(a,h){f(a,g,h),t(g,x),f(a,_,h),$(u,a,h),T=!0},p:me,i(a){T||(b(u.$$.fragment,a),T=!0)},o(a){y(u.$$.fragment,a),T=!1},d(a){a&&r(g),a&&r(_),w(u,a)}}}function Jd(M){let g,x,_,u,T;return u=new ce({props:{code:`from transformers import MT5ForConditionalGeneration, T5Tokenizer

model = MT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, text_target=summary, return_tensors="pt")

outputs = model(**inputs)
loss = outputs.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, text_target=summary, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),{c(){g=s("p"),x=l("Examples:"),_=p(),v(u.$$.fragment)},l(a){g=n(a,"P",{});var h=o(g);x=i(h,"Examples:"),h.forEach(r),_=m(a),k(u.$$.fragment,a)},m(a,h){f(a,g,h),t(g,x),f(a,_,h),$(u,a,h),T=!0},p:me,i(a){T||(b(u.$$.fragment,a),T=!0)},o(a){y(u.$$.fragment,a),T=!1},d(a){a&&r(g),a&&r(_),w(u,a)}}}function Qd(M){let g,x,_,u,T;return u=new ce({props:{code:`from transformers import MT5EncoderModel, T5Tokenizer

model = MT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="pt").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5EncoderModel, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_state = outputs.last_hidden_state`}}),{c(){g=s("p"),x=l("Examples:"),_=p(),v(u.$$.fragment)},l(a){g=n(a,"P",{});var h=o(g);x=i(h,"Examples:"),h.forEach(r),_=m(a),k(u.$$.fragment,a)},m(a,h){f(a,g,h),t(g,x),f(a,_,h),$(u,a,h),T=!0},p:me,i(a){T||(b(u.$$.fragment,a),T=!0)},o(a){y(u.$$.fragment,a),T=!1},d(a){a&&r(g),a&&r(_),w(u,a)}}}function Yd(M){let g,x,_,u,T;return u=new ce({props:{code:`from transformers import TFMT5Model, T5Tokenizer

model = TFMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="tf")
labels = tokenizer(text_target=summary, return_tensors="tf")

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=labels["input_ids"])
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(text_target=summary, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),{c(){g=s("p"),x=l("Examples:"),_=p(),v(u.$$.fragment)},l(a){g=n(a,"P",{});var h=o(g);x=i(h,"Examples:"),h.forEach(r),_=m(a),k(u.$$.fragment,a)},m(a,h){f(a,g,h),t(g,x),f(a,_,h),$(u,a,h),T=!0},p:me,i(a){T||(b(u.$$.fragment,a),T=!0)},o(a){y(u.$$.fragment,a),T=!1},d(a){a&&r(g),a&&r(_),w(u,a)}}}function Zd(M){let g,x,_,u,T;return u=new ce({props:{code:`from transformers import TFMT5ForConditionalGeneration, T5Tokenizer

model = TFMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, text_target=summary, return_tensors="tf")

outputs = model(**inputs)
loss = outputs.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, text_target=summary, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),{c(){g=s("p"),x=l("Examples:"),_=p(),v(u.$$.fragment)},l(a){g=n(a,"P",{});var h=o(g);x=i(h,"Examples:"),h.forEach(r),_=m(a),k(u.$$.fragment,a)},m(a,h){f(a,g,h),t(g,x),f(a,_,h),$(u,a,h),T=!0},p:me,i(a){T||(b(u.$$.fragment,a),T=!0)},o(a){y(u.$$.fragment,a),T=!1},d(a){a&&r(g),a&&r(_),w(u,a)}}}function ep(M){let g,x,_,u,T;return u=new ce({props:{code:`from transformers import TFMT5EncoderModel, T5Tokenizer

model = TFMT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="tf").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5EncoderModel, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_state = outputs.last_hidden_state`}}),{c(){g=s("p"),x=l("Examples:"),_=p(),v(u.$$.fragment)},l(a){g=n(a,"P",{});var h=o(g);x=i(h,"Examples:"),h.forEach(r),_=m(a),k(u.$$.fragment,a)},m(a,h){f(a,g,h),t(g,x),f(a,_,h),$(u,a,h),T=!0},p:me,i(a){T||(b(u.$$.fragment,a),T=!0)},o(a){y(u.$$.fragment,a),T=!1},d(a){a&&r(g),a&&r(_),w(u,a)}}}function tp(M){let g,x,_,u,T;return u=new ce({props:{code:`from transformers import FlaxMT5Model, T5Tokenizer

model = FlaxMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

decoder_input_ids = tokenizer(text_target=summary, return_tensors="np").input_ids

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=decoder_input_ids)
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxMT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxMT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(text_target=summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),{c(){g=s("p"),x=l("Examples:"),_=p(),v(u.$$.fragment)},l(a){g=n(a,"P",{});var h=o(g);x=i(h,"Examples:"),h.forEach(r),_=m(a),k(u.$$.fragment,a)},m(a,h){f(a,g,h),t(g,x),f(a,_,h),$(u,a,h),T=!0},p:me,i(a){T||(b(u.$$.fragment,a),T=!0)},o(a){y(u.$$.fragment,a),T=!1},d(a){a&&r(g),a&&r(_),w(u,a)}}}function rp(M){let g,x,_,u,T;return u=new ce({props:{code:`from transformers import FlaxMT5ForConditionalGeneration, T5Tokenizer

model = FlaxMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

decoder_input_ids = tokenizer(text_target=summary, return_tensors="np").input_ids

outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxMT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxMT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(text_target=summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){g=s("p"),x=l("Examples:"),_=p(),v(u.$$.fragment)},l(a){g=n(a,"P",{});var h=o(g);x=i(h,"Examples:"),h.forEach(r),_=m(a),k(u.$$.fragment,a)},m(a,h){f(a,g,h),t(g,x),f(a,_,h),$(u,a,h),T=!0},p:me,i(a){T||(b(u.$$.fragment,a),T=!0)},o(a){y(u.$$.fragment,a),T=!1},d(a){a&&r(g),a&&r(_),w(u,a)}}}function sp(M){let g,x,_,u,T;return u=new ce({props:{code:`from transformers import FlaxT5EncoderModel, T5Tokenizer

model = FlaxT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

decoder_input_ids = tokenizer(text_target=summary, return_tensors="np").input_ids

outputs = model(input_ids=inputs["input_ids"])
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxT5EncoderModel, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(text_target=summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),{c(){g=s("p"),x=l("Examples:"),_=p(),v(u.$$.fragment)},l(a){g=n(a,"P",{});var h=o(g);x=i(h,"Examples:"),h.forEach(r),_=m(a),k(u.$$.fragment,a)},m(a,h){f(a,g,h),t(g,x),f(a,_,h),$(u,a,h),T=!0},p:me,i(a){T||(b(u.$$.fragment,a),T=!0)},o(a){y(u.$$.fragment,a),T=!1},d(a){a&&r(g),a&&r(_),w(u,a)}}}function np(M){let g,x,_,u,T,a,h,Nr,Zn,Ns,K,fe,Dr,We,eo,Ir,to,Ds,ue,ro,Be,so,no,Is,Qt,oo,Gs,Yt,Gr,ao,Os,ge,lo,He,io,po,Vs,Zt,mo,Us,F,Or,Vr,Re,co,fo,Ur,Wr,Xe,uo,go,Br,Hr,Ke,ho,_o,Rr,Xr,Je,To,vo,Kr,er,Qe,ko,$o,Ws,W,bo,Ye,yo,wo,Ze,xo,Mo,Bs,J,he,Jr,et,Eo,Qr,zo,Hs,C,tt,qo,A,Fo,tr,jo,Po,rr,Co,Ao,rt,So,Lo,No,Q,Do,sr,Io,Go,nr,Oo,Vo,Rs,Y,_e,Yr,st,Uo,Zr,Wo,Xs,E,nt,Bo,ot,Ho,at,Ro,Xo,Ko,lt,Jo,or,Qo,Yo,Zo,B,it,ea,es,ta,ra,dt,ar,sa,ts,na,oa,lr,aa,rs,la,ia,Te,pt,da,ss,pa,ma,ve,mt,ca,ns,fa,ua,ke,ct,ga,ft,ha,os,_a,Ta,Ks,$e,va,ir,ka,$a,Js,Z,be,as,ut,ba,ls,ya,Qs,q,gt,wa,ee,xa,is,Ma,Ea,ht,za,qa,Fa,_t,ja,dr,Pa,Ca,Aa,H,Tt,Sa,ds,La,Na,vt,pr,Da,ps,Ia,Ga,mr,Oa,ms,Va,Ua,ye,kt,Wa,cs,Ba,Ys,we,Ha,cr,Ra,Xa,Zs,te,xe,fs,$t,Ka,us,Ja,en,S,bt,Qa,yt,Ya,fr,Za,el,tl,Me,tn,re,Ee,gs,wt,rl,hs,sl,rn,L,xt,nl,Mt,ol,ur,al,ll,il,ze,sn,se,qe,_s,Et,dl,Ts,pl,nn,N,zt,ml,qt,cl,gr,fl,ul,gl,Fe,on,ne,je,vs,Ft,hl,ks,_l,an,D,jt,Tl,Pt,vl,hr,kl,$l,bl,Pe,ln,oe,Ce,$s,Ct,yl,bs,wl,dn,I,At,xl,St,Ml,_r,El,zl,ql,Ae,pn,ae,Se,ys,Lt,Fl,ws,jl,mn,G,Nt,Pl,Dt,Cl,Tr,Al,Sl,Ll,Le,cn,le,Ne,xs,It,Nl,Ms,Dl,fn,O,Gt,Il,Ot,Gl,vr,Ol,Vl,Ul,De,un,ie,Ie,Es,Vt,Wl,zs,Bl,gn,V,Ut,Hl,Wt,Rl,kr,Xl,Kl,Jl,Ge,hn,de,Oe,qs,Bt,Ql,Fs,Yl,_n,U,Ht,Zl,Rt,ei,$r,ti,ri,si,Ve,Tn;return a=new P({}),We=new P({}),et=new P({}),tt=new z({props:{name:"class transformers.MT5Config",anchor:"transformers.MT5Config",parameters:[{name:"vocab_size",val:" = 250112"},{name:"d_model",val:" = 512"},{name:"d_kv",val:" = 64"},{name:"d_ff",val:" = 1024"},{name:"num_layers",val:" = 8"},{name:"num_decoder_layers",val:" = None"},{name:"num_heads",val:" = 6"},{name:"relative_attention_num_buckets",val:" = 32"},{name:"relative_attention_max_distance",val:" = 128"},{name:"dropout_rate",val:" = 0.1"},{name:"layer_norm_epsilon",val:" = 1e-06"},{name:"initializer_factor",val:" = 1.0"},{name:"feed_forward_proj",val:" = 'gated-gelu'"},{name:"is_encoder_decoder",val:" = True"},{name:"use_cache",val:" = True"},{name:"tokenizer_class",val:" = 'T5Tokenizer'"},{name:"tie_word_embeddings",val:" = False"},{name:"pad_token_id",val:" = 0"},{name:"eos_token_id",val:" = 1"},{name:"decoder_start_token_id",val:" = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MT5Config.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 250112) &#x2014;
Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/t5#transformers.T5Model">T5Model</a> or <a href="/docs/transformers/main/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a>.`,name:"vocab_size"},{anchor:"transformers.MT5Config.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Size of the encoder layers and the pooler layer.`,name:"d_model"},{anchor:"transformers.MT5Config.d_kv",description:`<strong>d_kv</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Size of the key, query, value projections per attention head. <code>d_kv</code> has to be equal to <code>d_model // num_heads</code>.`,name:"d_kv"},{anchor:"transformers.MT5Config.d_ff",description:`<strong>d_ff</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Size of the intermediate feed forward layer in each <code>T5Block</code>.`,name:"d_ff"},{anchor:"transformers.MT5Config.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_layers"},{anchor:"transformers.MT5Config.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Number of hidden layers in the Transformer decoder. Will use the same value as <code>num_layers</code> if not set.`,name:"num_decoder_layers"},{anchor:"transformers.MT5Config.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_heads"},{anchor:"transformers.MT5Config.relative_attention_num_buckets",description:`<strong>relative_attention_num_buckets</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of buckets to use for each attention layer.`,name:"relative_attention_num_buckets"},{anchor:"transformers.MT5Config.relative_attention_max_distance",description:`<strong>relative_attention_max_distance</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
The maximum distance of the longer sequences for the bucket separation.`,name:"relative_attention_max_distance"},{anchor:"transformers.MT5Config.dropout_rate",description:`<strong>dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The ratio for all dropout layers.`,name:"dropout_rate"},{anchor:"transformers.MT5Config.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-6) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.MT5Config.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.MT5Config.feed_forward_proj",description:`<strong>feed_forward_proj</strong> (<code>string</code>, <em>optional</em>, defaults to <code>&quot;gated-gelu&quot;</code>) &#x2014;
Type of feed forward layer to be used. Should be one of <code>&quot;relu&quot;</code> or <code>&quot;gated-gelu&quot;</code>.`,name:"feed_forward_proj"},{anchor:"transformers.MT5Config.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mt5/configuration_mt5.py#L26"}}),st=new P({}),nt=new z({props:{name:"class transformers.T5Tokenizer",anchor:"transformers.T5Tokenizer",parameters:[{name:"vocab_file",val:""},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 100"},{name:"additional_special_tokens",val:" = None"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.T5Tokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.T5Tokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.T5Tokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.T5Tokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.T5Tokenizer.extra_ids",description:`<strong>extra_ids</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
accessible as &#x201C;<extra<em>id{%d}&gt;&#x201D; where &#x201D;{%d}&#x201D; is a number between 0 and extra_ids-1. Extra tokens are
indexed from the end of the vocabulary up to beginning (&#x201C;<extra_id_0>&#x201D; is the last token in the vocabulary
like in T5 preprocessing see
<a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117" rel="nofollow">here</a>).</extra_id_0></extra<em>`,name:"extra_ids"},{anchor:"transformers.T5Tokenizer.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"},{anchor:"transformers.T5Tokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"},{anchor:"transformers.T5Tokenizer.sp_model",description:`<strong>sp_model</strong> (<code>SentencePieceProcessor</code>) &#x2014;
The <em>SentencePiece</em> processor that is used for every conversion (string, tokens and IDs).`,name:"sp_model"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5.py#L55"}}),it=new z({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5.py#L249",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),pt=new z({props:{name:"convert_tokens_to_string",anchor:"transformers.T5Tokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5.py#L310"}}),mt=new z({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5.py#L227",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ct=new z({props:{name:"get_special_tokens_mask",anchor:"transformers.T5Tokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5.py#L188",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ut=new P({}),gt=new z({props:{name:"class transformers.T5TokenizerFast",anchor:"transformers.T5TokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 100"},{name:"additional_special_tokens",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.T5TokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.T5TokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.T5TokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.T5TokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.T5TokenizerFast.extra_ids",description:`<strong>extra_ids</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
accessible as &#x201C;<extra<em>id{%d}&gt;&#x201D; where &#x201D;{%d}&#x201D; is a number between 0 and extra_ids-1. Extra tokens are
indexed from the end of the vocabulary up to beginning (&#x201C;<extra_id_0>&#x201D; is the last token in the vocabulary
like in T5 preprocessing see
<a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117" rel="nofollow">here</a>).</extra_id_0></extra<em>`,name:"extra_ids"},{anchor:"transformers.T5TokenizerFast.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5_fast.py#L65"}}),Tt=new z({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5_fast.py#L191",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),kt=new z({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5_fast.py#L217",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),$t=new P({}),bt=new z({props:{name:"class transformers.MT5Model",anchor:"transformers.MT5Model",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mt5/modeling_mt5.py#L28"}}),Me=new pe({props:{anchor:"transformers.MT5Model.example",$$slots:{default:[Kd]},$$scope:{ctx:M}}}),wt=new P({}),xt=new z({props:{name:"class transformers.MT5ForConditionalGeneration",anchor:"transformers.MT5ForConditionalGeneration",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mt5/modeling_mt5.py#L61"}}),ze=new pe({props:{anchor:"transformers.MT5ForConditionalGeneration.example",$$slots:{default:[Jd]},$$scope:{ctx:M}}}),Et=new P({}),zt=new z({props:{name:"class transformers.MT5EncoderModel",anchor:"transformers.MT5EncoderModel",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mt5/modeling_mt5.py#L91"}}),Fe=new pe({props:{anchor:"transformers.MT5EncoderModel.example",$$slots:{default:[Qd]},$$scope:{ctx:M}}}),Ft=new P({}),jt=new z({props:{name:"class transformers.TFMT5Model",anchor:"transformers.TFMT5Model",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mt5/modeling_tf_mt5.py#L28"}}),Pe=new pe({props:{anchor:"transformers.TFMT5Model.example",$$slots:{default:[Yd]},$$scope:{ctx:M}}}),Ct=new P({}),At=new z({props:{name:"class transformers.TFMT5ForConditionalGeneration",anchor:"transformers.TFMT5ForConditionalGeneration",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mt5/modeling_tf_mt5.py#L52"}}),Ae=new pe({props:{anchor:"transformers.TFMT5ForConditionalGeneration.example",$$slots:{default:[Zd]},$$scope:{ctx:M}}}),Lt=new P({}),Nt=new z({props:{name:"class transformers.TFMT5EncoderModel",anchor:"transformers.TFMT5EncoderModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mt5/modeling_tf_mt5.py#L76"}}),Le=new pe({props:{anchor:"transformers.TFMT5EncoderModel.example",$$slots:{default:[ep]},$$scope:{ctx:M}}}),It=new P({}),Gt=new z({props:{name:"class transformers.FlaxMT5Model",anchor:"transformers.FlaxMT5Model",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"gradient_checkpointing",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mt5/modeling_flax_mt5.py#L43"}}),De=new pe({props:{anchor:"transformers.FlaxMT5Model.example",$$slots:{default:[tp]},$$scope:{ctx:M}}}),Vt=new P({}),Ut=new z({props:{name:"class transformers.FlaxMT5ForConditionalGeneration",anchor:"transformers.FlaxMT5ForConditionalGeneration",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"gradient_checkpointing",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mt5/modeling_flax_mt5.py#L95"}}),Ge=new pe({props:{anchor:"transformers.FlaxMT5ForConditionalGeneration.example",$$slots:{default:[rp]},$$scope:{ctx:M}}}),Bt=new P({}),Ht=new z({props:{name:"class transformers.FlaxMT5EncoderModel",anchor:"transformers.FlaxMT5EncoderModel",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"gradient_checkpointing",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mt5/modeling_flax_mt5.py#L69"}}),Ve=new pe({props:{anchor:"transformers.FlaxMT5EncoderModel.example",$$slots:{default:[sp]},$$scope:{ctx:M}}}),{c(){g=s("meta"),x=p(),_=s("h1"),u=s("a"),T=s("span"),v(a.$$.fragment),h=p(),Nr=s("span"),Zn=l("mT5"),Ns=p(),K=s("h2"),fe=s("a"),Dr=s("span"),v(We.$$.fragment),eo=p(),Ir=s("span"),to=l("Overview"),Ds=p(),ue=s("p"),ro=l("The mT5 model was presented in "),Be=s("a"),so=l("mT5: A massively multilingual pre-trained text-to-text transformer"),no=l(` by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, Colin Raffel.`),Is=p(),Qt=s("p"),oo=l("The abstract from the paper is the following:"),Gs=p(),Yt=s("p"),Gr=s("em"),ao=l(`The recent \u201CText-to-Text Transfer Transformer\u201D (T5) leveraged a unified text-to-text format and scale to attain
state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail
the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. We also describe a simple technique to prevent \u201Caccidental translation\u201D in the zero-shot setting, where a
generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model
checkpoints used in this work are publicly available.`),Os=p(),ge=s("p"),lo=l("Note: mT5 was only pre-trained on "),He=s("a"),io=l("mC4"),po=l(` excluding any supervised training.
Therefore, this model has to be fine-tuned before it is usable on a downstream task, unlike the original T5 model.
Since mT5 was pre-trained unsupervisedly, there\u2019s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`),Vs=p(),Zt=s("p"),mo=l("Google has released the following variants:"),Us=p(),F=s("ul"),Or=s("li"),Vr=s("p"),Re=s("a"),co=l("google/mt5-small"),fo=p(),Ur=s("li"),Wr=s("p"),Xe=s("a"),uo=l("google/mt5-base"),go=p(),Br=s("li"),Hr=s("p"),Ke=s("a"),ho=l("google/mt5-large"),_o=p(),Rr=s("li"),Xr=s("p"),Je=s("a"),To=l("google/mt5-xl"),vo=p(),Kr=s("li"),er=s("p"),Qe=s("a"),ko=l("google/mt5-xxl"),$o=l("."),Ws=p(),W=s("p"),bo=l("This model was contributed by "),Ye=s("a"),yo=l("patrickvonplaten"),wo=l(`. The original code can be
found `),Ze=s("a"),xo=l("here"),Mo=l("."),Bs=p(),J=s("h2"),he=s("a"),Jr=s("span"),v(et.$$.fragment),Eo=p(),Qr=s("span"),zo=l("MT5Config"),Hs=p(),C=s("div"),v(tt.$$.fragment),qo=p(),A=s("p"),Fo=l("This is the configuration class to store the configuration of a "),tr=s("a"),jo=l("MT5Model"),Po=l(" or a "),rr=s("a"),Co=l("TFMT5Model"),Ao=l(`. It is used to
instantiate a mT5 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the mT5
`),rt=s("a"),So=l("google/mt5-small"),Lo=l(" architecture."),No=p(),Q=s("p"),Do=l("Configuration objects inherit from "),sr=s("a"),Io=l("PretrainedConfig"),Go=l(` and can be used to control the model outputs. Read the
documentation from `),nr=s("a"),Oo=l("PretrainedConfig"),Vo=l(" for more information."),Rs=p(),Y=s("h2"),_e=s("a"),Yr=s("span"),v(st.$$.fragment),Uo=p(),Zr=s("span"),Wo=l("MT5Tokenizer"),Xs=p(),E=s("div"),v(nt.$$.fragment),Bo=p(),ot=s("p"),Ho=l("Construct a T5 tokenizer. Based on "),at=s("a"),Ro=l("SentencePiece"),Xo=l("."),Ko=p(),lt=s("p"),Jo=l("This tokenizer inherits from "),or=s("a"),Qo=l("PreTrainedTokenizer"),Yo=l(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Zo=p(),B=s("div"),v(it.$$.fragment),ea=p(),es=s("p"),ta=l(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),ra=p(),dt=s("ul"),ar=s("li"),sa=l("single sequence: "),ts=s("code"),na=l("X </s>"),oa=p(),lr=s("li"),aa=l("pair of sequences: "),rs=s("code"),la=l("A </s> B </s>"),ia=p(),Te=s("div"),v(pt.$$.fragment),da=p(),ss=s("p"),pa=l("Converts a sequence of tokens (string) in a single string."),ma=p(),ve=s("div"),v(mt.$$.fragment),ca=p(),ns=s("p"),fa=l(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),ua=p(),ke=s("div"),v(ct.$$.fragment),ga=p(),ft=s("p"),ha=l(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),os=s("code"),_a=l("prepare_for_model"),Ta=l(" method."),Ks=p(),$e=s("p"),va=l("See "),ir=s("a"),ka=l("T5Tokenizer"),$a=l(" for all details."),Js=p(),Z=s("h2"),be=s("a"),as=s("span"),v(ut.$$.fragment),ba=p(),ls=s("span"),ya=l("MT5TokenizerFast"),Qs=p(),q=s("div"),v(gt.$$.fragment),wa=p(),ee=s("p"),xa=l("Construct a \u201Cfast\u201D T5 tokenizer (backed by HuggingFace\u2019s "),is=s("em"),Ma=l("tokenizers"),Ea=l(` library). Based on
`),ht=s("a"),za=l("Unigram"),qa=l("."),Fa=p(),_t=s("p"),ja=l("This tokenizer inherits from "),dr=s("a"),Pa=l("PreTrainedTokenizerFast"),Ca=l(` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),Aa=p(),H=s("div"),v(Tt.$$.fragment),Sa=p(),ds=s("p"),La=l(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),Na=p(),vt=s("ul"),pr=s("li"),Da=l("single sequence: "),ps=s("code"),Ia=l("X </s>"),Ga=p(),mr=s("li"),Oa=l("pair of sequences: "),ms=s("code"),Va=l("A </s> B </s>"),Ua=p(),ye=s("div"),v(kt.$$.fragment),Wa=p(),cs=s("p"),Ba=l(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),Ys=p(),we=s("p"),Ha=l("See "),cr=s("a"),Ra=l("T5TokenizerFast"),Xa=l(" for all details."),Zs=p(),te=s("h2"),xe=s("a"),fs=s("span"),v($t.$$.fragment),Ka=p(),us=s("span"),Ja=l("MT5Model"),en=p(),S=s("div"),v(bt.$$.fragment),Qa=p(),yt=s("p"),Ya=l("This class overrides "),fr=s("a"),Za=l("T5Model"),el=l(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),tl=p(),v(Me.$$.fragment),tn=p(),re=s("h2"),Ee=s("a"),gs=s("span"),v(wt.$$.fragment),rl=p(),hs=s("span"),sl=l("MT5ForConditionalGeneration"),rn=p(),L=s("div"),v(xt.$$.fragment),nl=p(),Mt=s("p"),ol=l("This class overrides "),ur=s("a"),al=l("T5ForConditionalGeneration"),ll=l(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),il=p(),v(ze.$$.fragment),sn=p(),se=s("h2"),qe=s("a"),_s=s("span"),v(Et.$$.fragment),dl=p(),Ts=s("span"),pl=l("MT5EncoderModel"),nn=p(),N=s("div"),v(zt.$$.fragment),ml=p(),qt=s("p"),cl=l("This class overrides "),gr=s("a"),fl=l("T5EncoderModel"),ul=l(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),gl=p(),v(Fe.$$.fragment),on=p(),ne=s("h2"),je=s("a"),vs=s("span"),v(Ft.$$.fragment),hl=p(),ks=s("span"),_l=l("TFMT5Model"),an=p(),D=s("div"),v(jt.$$.fragment),Tl=p(),Pt=s("p"),vl=l("This class overrides "),hr=s("a"),kl=l("TFT5Model"),$l=l(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),bl=p(),v(Pe.$$.fragment),ln=p(),oe=s("h2"),Ce=s("a"),$s=s("span"),v(Ct.$$.fragment),yl=p(),bs=s("span"),wl=l("TFMT5ForConditionalGeneration"),dn=p(),I=s("div"),v(At.$$.fragment),xl=p(),St=s("p"),Ml=l("This class overrides "),_r=s("a"),El=l("TFT5ForConditionalGeneration"),zl=l(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),ql=p(),v(Ae.$$.fragment),pn=p(),ae=s("h2"),Se=s("a"),ys=s("span"),v(Lt.$$.fragment),Fl=p(),ws=s("span"),jl=l("TFMT5EncoderModel"),mn=p(),G=s("div"),v(Nt.$$.fragment),Pl=p(),Dt=s("p"),Cl=l("This class overrides "),Tr=s("a"),Al=l("TFT5EncoderModel"),Sl=l(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Ll=p(),v(Le.$$.fragment),cn=p(),le=s("h2"),Ne=s("a"),xs=s("span"),v(It.$$.fragment),Nl=p(),Ms=s("span"),Dl=l("FlaxMT5Model"),fn=p(),O=s("div"),v(Gt.$$.fragment),Il=p(),Ot=s("p"),Gl=l("This class overrides "),vr=s("a"),Ol=l("FlaxT5Model"),Vl=l(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Ul=p(),v(De.$$.fragment),un=p(),ie=s("h2"),Ie=s("a"),Es=s("span"),v(Vt.$$.fragment),Wl=p(),zs=s("span"),Bl=l("FlaxMT5ForConditionalGeneration"),gn=p(),V=s("div"),v(Ut.$$.fragment),Hl=p(),Wt=s("p"),Rl=l("This class overrides "),kr=s("a"),Xl=l("FlaxT5ForConditionalGeneration"),Kl=l(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Jl=p(),v(Ge.$$.fragment),hn=p(),de=s("h2"),Oe=s("a"),qs=s("span"),v(Bt.$$.fragment),Ql=p(),Fs=s("span"),Yl=l("FlaxMT5EncoderModel"),_n=p(),U=s("div"),v(Ht.$$.fragment),Zl=p(),Rt=s("p"),ei=l("This class overrides "),$r=s("a"),ti=l("FlaxT5EncoderModel"),ri=l(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),si=p(),v(Ve.$$.fragment),this.h()},l(e){const c=Rd('[data-svelte="svelte-1phssyn"]',document.head);g=n(c,"META",{name:!0,content:!0}),c.forEach(r),x=m(e),_=n(e,"H1",{class:!0});var Xt=o(_);u=n(Xt,"A",{id:!0,class:!0,href:!0});var js=o(u);T=n(js,"SPAN",{});var Ps=o(T);k(a.$$.fragment,Ps),Ps.forEach(r),js.forEach(r),h=m(Xt),Nr=n(Xt,"SPAN",{});var Cs=o(Nr);Zn=i(Cs,"mT5"),Cs.forEach(r),Xt.forEach(r),Ns=m(e),K=n(e,"H2",{class:!0});var Kt=o(K);fe=n(Kt,"A",{id:!0,class:!0,href:!0});var As=o(fe);Dr=n(As,"SPAN",{});var Ss=o(Dr);k(We.$$.fragment,Ss),Ss.forEach(r),As.forEach(r),eo=m(Kt),Ir=n(Kt,"SPAN",{});var Ls=o(Ir);to=i(Ls,"Overview"),Ls.forEach(r),Kt.forEach(r),Ds=m(e),ue=n(e,"P",{});var Jt=o(ue);ro=i(Jt,"The mT5 model was presented in "),Be=n(Jt,"A",{href:!0,rel:!0});var di=o(Be);so=i(di,"mT5: A massively multilingual pre-trained text-to-text transformer"),di.forEach(r),no=i(Jt,` by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, Colin Raffel.`),Jt.forEach(r),Is=m(e),Qt=n(e,"P",{});var pi=o(Qt);oo=i(pi,"The abstract from the paper is the following:"),pi.forEach(r),Gs=m(e),Yt=n(e,"P",{});var mi=o(Yt);Gr=n(mi,"EM",{});var ci=o(Gr);ao=i(ci,`The recent \u201CText-to-Text Transfer Transformer\u201D (T5) leveraged a unified text-to-text format and scale to attain
state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail
the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. We also describe a simple technique to prevent \u201Caccidental translation\u201D in the zero-shot setting, where a
generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model
checkpoints used in this work are publicly available.`),ci.forEach(r),mi.forEach(r),Os=m(e),ge=n(e,"P",{});var vn=o(ge);lo=i(vn,"Note: mT5 was only pre-trained on "),He=n(vn,"A",{href:!0,rel:!0});var fi=o(He);io=i(fi,"mC4"),fi.forEach(r),po=i(vn,` excluding any supervised training.
Therefore, this model has to be fine-tuned before it is usable on a downstream task, unlike the original T5 model.
Since mT5 was pre-trained unsupervisedly, there\u2019s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`),vn.forEach(r),Vs=m(e),Zt=n(e,"P",{});var ui=o(Zt);mo=i(ui,"Google has released the following variants:"),ui.forEach(r),Us=m(e),F=n(e,"UL",{});var R=o(F);Or=n(R,"LI",{});var gi=o(Or);Vr=n(gi,"P",{});var hi=o(Vr);Re=n(hi,"A",{href:!0,rel:!0});var _i=o(Re);co=i(_i,"google/mt5-small"),_i.forEach(r),hi.forEach(r),gi.forEach(r),fo=m(R),Ur=n(R,"LI",{});var Ti=o(Ur);Wr=n(Ti,"P",{});var vi=o(Wr);Xe=n(vi,"A",{href:!0,rel:!0});var ki=o(Xe);uo=i(ki,"google/mt5-base"),ki.forEach(r),vi.forEach(r),Ti.forEach(r),go=m(R),Br=n(R,"LI",{});var $i=o(Br);Hr=n($i,"P",{});var bi=o(Hr);Ke=n(bi,"A",{href:!0,rel:!0});var yi=o(Ke);ho=i(yi,"google/mt5-large"),yi.forEach(r),bi.forEach(r),$i.forEach(r),_o=m(R),Rr=n(R,"LI",{});var wi=o(Rr);Xr=n(wi,"P",{});var xi=o(Xr);Je=n(xi,"A",{href:!0,rel:!0});var Mi=o(Je);To=i(Mi,"google/mt5-xl"),Mi.forEach(r),xi.forEach(r),wi.forEach(r),vo=m(R),Kr=n(R,"LI",{});var Ei=o(Kr);er=n(Ei,"P",{});var ni=o(er);Qe=n(ni,"A",{href:!0,rel:!0});var zi=o(Qe);ko=i(zi,"google/mt5-xxl"),zi.forEach(r),$o=i(ni,"."),ni.forEach(r),Ei.forEach(r),R.forEach(r),Ws=m(e),W=n(e,"P",{});var br=o(W);bo=i(br,"This model was contributed by "),Ye=n(br,"A",{href:!0,rel:!0});var qi=o(Ye);yo=i(qi,"patrickvonplaten"),qi.forEach(r),wo=i(br,`. The original code can be
found `),Ze=n(br,"A",{href:!0,rel:!0});var Fi=o(Ze);xo=i(Fi,"here"),Fi.forEach(r),Mo=i(br,"."),br.forEach(r),Bs=m(e),J=n(e,"H2",{class:!0});var kn=o(J);he=n(kn,"A",{id:!0,class:!0,href:!0});var ji=o(he);Jr=n(ji,"SPAN",{});var Pi=o(Jr);k(et.$$.fragment,Pi),Pi.forEach(r),ji.forEach(r),Eo=m(kn),Qr=n(kn,"SPAN",{});var Ci=o(Qr);zo=i(Ci,"MT5Config"),Ci.forEach(r),kn.forEach(r),Hs=m(e),C=n(e,"DIV",{class:!0});var yr=o(C);k(tt.$$.fragment,yr),qo=m(yr),A=n(yr,"P",{});var Ue=o(A);Fo=i(Ue,"This is the configuration class to store the configuration of a "),tr=n(Ue,"A",{href:!0});var Ai=o(tr);jo=i(Ai,"MT5Model"),Ai.forEach(r),Po=i(Ue," or a "),rr=n(Ue,"A",{href:!0});var Si=o(rr);Co=i(Si,"TFMT5Model"),Si.forEach(r),Ao=i(Ue,`. It is used to
instantiate a mT5 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the mT5
`),rt=n(Ue,"A",{href:!0,rel:!0});var Li=o(rt);So=i(Li,"google/mt5-small"),Li.forEach(r),Lo=i(Ue," architecture."),Ue.forEach(r),No=m(yr),Q=n(yr,"P",{});var wr=o(Q);Do=i(wr,"Configuration objects inherit from "),sr=n(wr,"A",{href:!0});var Ni=o(sr);Io=i(Ni,"PretrainedConfig"),Ni.forEach(r),Go=i(wr,` and can be used to control the model outputs. Read the
documentation from `),nr=n(wr,"A",{href:!0});var Di=o(nr);Oo=i(Di,"PretrainedConfig"),Di.forEach(r),Vo=i(wr," for more information."),wr.forEach(r),yr.forEach(r),Rs=m(e),Y=n(e,"H2",{class:!0});var $n=o(Y);_e=n($n,"A",{id:!0,class:!0,href:!0});var Ii=o(_e);Yr=n(Ii,"SPAN",{});var Gi=o(Yr);k(st.$$.fragment,Gi),Gi.forEach(r),Ii.forEach(r),Uo=m($n),Zr=n($n,"SPAN",{});var Oi=o(Zr);Wo=i(Oi,"MT5Tokenizer"),Oi.forEach(r),$n.forEach(r),Xs=m(e),E=n(e,"DIV",{class:!0});var j=o(E);k(nt.$$.fragment,j),Bo=m(j),ot=n(j,"P",{});var bn=o(ot);Ho=i(bn,"Construct a T5 tokenizer. Based on "),at=n(bn,"A",{href:!0,rel:!0});var Vi=o(at);Ro=i(Vi,"SentencePiece"),Vi.forEach(r),Xo=i(bn,"."),bn.forEach(r),Ko=m(j),lt=n(j,"P",{});var yn=o(lt);Jo=i(yn,"This tokenizer inherits from "),or=n(yn,"A",{href:!0});var Ui=o(or);Qo=i(Ui,"PreTrainedTokenizer"),Ui.forEach(r),Yo=i(yn,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),yn.forEach(r),Zo=m(j),B=n(j,"DIV",{class:!0});var xr=o(B);k(it.$$.fragment,xr),ea=m(xr),es=n(xr,"P",{});var Wi=o(es);ta=i(Wi,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),Wi.forEach(r),ra=m(xr),dt=n(xr,"UL",{});var wn=o(dt);ar=n(wn,"LI",{});var oi=o(ar);sa=i(oi,"single sequence: "),ts=n(oi,"CODE",{});var Bi=o(ts);na=i(Bi,"X </s>"),Bi.forEach(r),oi.forEach(r),oa=m(wn),lr=n(wn,"LI",{});var ai=o(lr);aa=i(ai,"pair of sequences: "),rs=n(ai,"CODE",{});var Hi=o(rs);la=i(Hi,"A </s> B </s>"),Hi.forEach(r),ai.forEach(r),wn.forEach(r),xr.forEach(r),ia=m(j),Te=n(j,"DIV",{class:!0});var xn=o(Te);k(pt.$$.fragment,xn),da=m(xn),ss=n(xn,"P",{});var Ri=o(ss);pa=i(Ri,"Converts a sequence of tokens (string) in a single string."),Ri.forEach(r),xn.forEach(r),ma=m(j),ve=n(j,"DIV",{class:!0});var Mn=o(ve);k(mt.$$.fragment,Mn),ca=m(Mn),ns=n(Mn,"P",{});var Xi=o(ns);fa=i(Xi,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),Xi.forEach(r),Mn.forEach(r),ua=m(j),ke=n(j,"DIV",{class:!0});var En=o(ke);k(ct.$$.fragment,En),ga=m(En),ft=n(En,"P",{});var zn=o(ft);ha=i(zn,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),os=n(zn,"CODE",{});var Ki=o(os);_a=i(Ki,"prepare_for_model"),Ki.forEach(r),Ta=i(zn," method."),zn.forEach(r),En.forEach(r),j.forEach(r),Ks=m(e),$e=n(e,"P",{});var qn=o($e);va=i(qn,"See "),ir=n(qn,"A",{href:!0});var Ji=o(ir);ka=i(Ji,"T5Tokenizer"),Ji.forEach(r),$a=i(qn," for all details."),qn.forEach(r),Js=m(e),Z=n(e,"H2",{class:!0});var Fn=o(Z);be=n(Fn,"A",{id:!0,class:!0,href:!0});var Qi=o(be);as=n(Qi,"SPAN",{});var Yi=o(as);k(ut.$$.fragment,Yi),Yi.forEach(r),Qi.forEach(r),ba=m(Fn),ls=n(Fn,"SPAN",{});var Zi=o(ls);ya=i(Zi,"MT5TokenizerFast"),Zi.forEach(r),Fn.forEach(r),Qs=m(e),q=n(e,"DIV",{class:!0});var X=o(q);k(gt.$$.fragment,X),wa=m(X),ee=n(X,"P",{});var Mr=o(ee);xa=i(Mr,"Construct a \u201Cfast\u201D T5 tokenizer (backed by HuggingFace\u2019s "),is=n(Mr,"EM",{});var ed=o(is);Ma=i(ed,"tokenizers"),ed.forEach(r),Ea=i(Mr,` library). Based on
`),ht=n(Mr,"A",{href:!0,rel:!0});var td=o(ht);za=i(td,"Unigram"),td.forEach(r),qa=i(Mr,"."),Mr.forEach(r),Fa=m(X),_t=n(X,"P",{});var jn=o(_t);ja=i(jn,"This tokenizer inherits from "),dr=n(jn,"A",{href:!0});var rd=o(dr);Pa=i(rd,"PreTrainedTokenizerFast"),rd.forEach(r),Ca=i(jn,` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),jn.forEach(r),Aa=m(X),H=n(X,"DIV",{class:!0});var Er=o(H);k(Tt.$$.fragment,Er),Sa=m(Er),ds=n(Er,"P",{});var sd=o(ds);La=i(sd,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),sd.forEach(r),Na=m(Er),vt=n(Er,"UL",{});var Pn=o(vt);pr=n(Pn,"LI",{});var li=o(pr);Da=i(li,"single sequence: "),ps=n(li,"CODE",{});var nd=o(ps);Ia=i(nd,"X </s>"),nd.forEach(r),li.forEach(r),Ga=m(Pn),mr=n(Pn,"LI",{});var ii=o(mr);Oa=i(ii,"pair of sequences: "),ms=n(ii,"CODE",{});var od=o(ms);Va=i(od,"A </s> B </s>"),od.forEach(r),ii.forEach(r),Pn.forEach(r),Er.forEach(r),Ua=m(X),ye=n(X,"DIV",{class:!0});var Cn=o(ye);k(kt.$$.fragment,Cn),Wa=m(Cn),cs=n(Cn,"P",{});var ad=o(cs);Ba=i(ad,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),ad.forEach(r),Cn.forEach(r),X.forEach(r),Ys=m(e),we=n(e,"P",{});var An=o(we);Ha=i(An,"See "),cr=n(An,"A",{href:!0});var ld=o(cr);Ra=i(ld,"T5TokenizerFast"),ld.forEach(r),Xa=i(An," for all details."),An.forEach(r),Zs=m(e),te=n(e,"H2",{class:!0});var Sn=o(te);xe=n(Sn,"A",{id:!0,class:!0,href:!0});var id=o(xe);fs=n(id,"SPAN",{});var dd=o(fs);k($t.$$.fragment,dd),dd.forEach(r),id.forEach(r),Ka=m(Sn),us=n(Sn,"SPAN",{});var pd=o(us);Ja=i(pd,"MT5Model"),pd.forEach(r),Sn.forEach(r),en=m(e),S=n(e,"DIV",{class:!0});var zr=o(S);k(bt.$$.fragment,zr),Qa=m(zr),yt=n(zr,"P",{});var Ln=o(yt);Ya=i(Ln,"This class overrides "),fr=n(Ln,"A",{href:!0});var md=o(fr);Za=i(md,"T5Model"),md.forEach(r),el=i(Ln,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Ln.forEach(r),tl=m(zr),k(Me.$$.fragment,zr),zr.forEach(r),tn=m(e),re=n(e,"H2",{class:!0});var Nn=o(re);Ee=n(Nn,"A",{id:!0,class:!0,href:!0});var cd=o(Ee);gs=n(cd,"SPAN",{});var fd=o(gs);k(wt.$$.fragment,fd),fd.forEach(r),cd.forEach(r),rl=m(Nn),hs=n(Nn,"SPAN",{});var ud=o(hs);sl=i(ud,"MT5ForConditionalGeneration"),ud.forEach(r),Nn.forEach(r),rn=m(e),L=n(e,"DIV",{class:!0});var qr=o(L);k(xt.$$.fragment,qr),nl=m(qr),Mt=n(qr,"P",{});var Dn=o(Mt);ol=i(Dn,"This class overrides "),ur=n(Dn,"A",{href:!0});var gd=o(ur);al=i(gd,"T5ForConditionalGeneration"),gd.forEach(r),ll=i(Dn,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Dn.forEach(r),il=m(qr),k(ze.$$.fragment,qr),qr.forEach(r),sn=m(e),se=n(e,"H2",{class:!0});var In=o(se);qe=n(In,"A",{id:!0,class:!0,href:!0});var hd=o(qe);_s=n(hd,"SPAN",{});var _d=o(_s);k(Et.$$.fragment,_d),_d.forEach(r),hd.forEach(r),dl=m(In),Ts=n(In,"SPAN",{});var Td=o(Ts);pl=i(Td,"MT5EncoderModel"),Td.forEach(r),In.forEach(r),nn=m(e),N=n(e,"DIV",{class:!0});var Fr=o(N);k(zt.$$.fragment,Fr),ml=m(Fr),qt=n(Fr,"P",{});var Gn=o(qt);cl=i(Gn,"This class overrides "),gr=n(Gn,"A",{href:!0});var vd=o(gr);fl=i(vd,"T5EncoderModel"),vd.forEach(r),ul=i(Gn,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Gn.forEach(r),gl=m(Fr),k(Fe.$$.fragment,Fr),Fr.forEach(r),on=m(e),ne=n(e,"H2",{class:!0});var On=o(ne);je=n(On,"A",{id:!0,class:!0,href:!0});var kd=o(je);vs=n(kd,"SPAN",{});var $d=o(vs);k(Ft.$$.fragment,$d),$d.forEach(r),kd.forEach(r),hl=m(On),ks=n(On,"SPAN",{});var bd=o(ks);_l=i(bd,"TFMT5Model"),bd.forEach(r),On.forEach(r),an=m(e),D=n(e,"DIV",{class:!0});var jr=o(D);k(jt.$$.fragment,jr),Tl=m(jr),Pt=n(jr,"P",{});var Vn=o(Pt);vl=i(Vn,"This class overrides "),hr=n(Vn,"A",{href:!0});var yd=o(hr);kl=i(yd,"TFT5Model"),yd.forEach(r),$l=i(Vn,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Vn.forEach(r),bl=m(jr),k(Pe.$$.fragment,jr),jr.forEach(r),ln=m(e),oe=n(e,"H2",{class:!0});var Un=o(oe);Ce=n(Un,"A",{id:!0,class:!0,href:!0});var wd=o(Ce);$s=n(wd,"SPAN",{});var xd=o($s);k(Ct.$$.fragment,xd),xd.forEach(r),wd.forEach(r),yl=m(Un),bs=n(Un,"SPAN",{});var Md=o(bs);wl=i(Md,"TFMT5ForConditionalGeneration"),Md.forEach(r),Un.forEach(r),dn=m(e),I=n(e,"DIV",{class:!0});var Pr=o(I);k(At.$$.fragment,Pr),xl=m(Pr),St=n(Pr,"P",{});var Wn=o(St);Ml=i(Wn,"This class overrides "),_r=n(Wn,"A",{href:!0});var Ed=o(_r);El=i(Ed,"TFT5ForConditionalGeneration"),Ed.forEach(r),zl=i(Wn,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Wn.forEach(r),ql=m(Pr),k(Ae.$$.fragment,Pr),Pr.forEach(r),pn=m(e),ae=n(e,"H2",{class:!0});var Bn=o(ae);Se=n(Bn,"A",{id:!0,class:!0,href:!0});var zd=o(Se);ys=n(zd,"SPAN",{});var qd=o(ys);k(Lt.$$.fragment,qd),qd.forEach(r),zd.forEach(r),Fl=m(Bn),ws=n(Bn,"SPAN",{});var Fd=o(ws);jl=i(Fd,"TFMT5EncoderModel"),Fd.forEach(r),Bn.forEach(r),mn=m(e),G=n(e,"DIV",{class:!0});var Cr=o(G);k(Nt.$$.fragment,Cr),Pl=m(Cr),Dt=n(Cr,"P",{});var Hn=o(Dt);Cl=i(Hn,"This class overrides "),Tr=n(Hn,"A",{href:!0});var jd=o(Tr);Al=i(jd,"TFT5EncoderModel"),jd.forEach(r),Sl=i(Hn,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Hn.forEach(r),Ll=m(Cr),k(Le.$$.fragment,Cr),Cr.forEach(r),cn=m(e),le=n(e,"H2",{class:!0});var Rn=o(le);Ne=n(Rn,"A",{id:!0,class:!0,href:!0});var Pd=o(Ne);xs=n(Pd,"SPAN",{});var Cd=o(xs);k(It.$$.fragment,Cd),Cd.forEach(r),Pd.forEach(r),Nl=m(Rn),Ms=n(Rn,"SPAN",{});var Ad=o(Ms);Dl=i(Ad,"FlaxMT5Model"),Ad.forEach(r),Rn.forEach(r),fn=m(e),O=n(e,"DIV",{class:!0});var Ar=o(O);k(Gt.$$.fragment,Ar),Il=m(Ar),Ot=n(Ar,"P",{});var Xn=o(Ot);Gl=i(Xn,"This class overrides "),vr=n(Xn,"A",{href:!0});var Sd=o(vr);Ol=i(Sd,"FlaxT5Model"),Sd.forEach(r),Vl=i(Xn,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Xn.forEach(r),Ul=m(Ar),k(De.$$.fragment,Ar),Ar.forEach(r),un=m(e),ie=n(e,"H2",{class:!0});var Kn=o(ie);Ie=n(Kn,"A",{id:!0,class:!0,href:!0});var Ld=o(Ie);Es=n(Ld,"SPAN",{});var Nd=o(Es);k(Vt.$$.fragment,Nd),Nd.forEach(r),Ld.forEach(r),Wl=m(Kn),zs=n(Kn,"SPAN",{});var Dd=o(zs);Bl=i(Dd,"FlaxMT5ForConditionalGeneration"),Dd.forEach(r),Kn.forEach(r),gn=m(e),V=n(e,"DIV",{class:!0});var Sr=o(V);k(Ut.$$.fragment,Sr),Hl=m(Sr),Wt=n(Sr,"P",{});var Jn=o(Wt);Rl=i(Jn,"This class overrides "),kr=n(Jn,"A",{href:!0});var Id=o(kr);Xl=i(Id,"FlaxT5ForConditionalGeneration"),Id.forEach(r),Kl=i(Jn,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Jn.forEach(r),Jl=m(Sr),k(Ge.$$.fragment,Sr),Sr.forEach(r),hn=m(e),de=n(e,"H2",{class:!0});var Qn=o(de);Oe=n(Qn,"A",{id:!0,class:!0,href:!0});var Gd=o(Oe);qs=n(Gd,"SPAN",{});var Od=o(qs);k(Bt.$$.fragment,Od),Od.forEach(r),Gd.forEach(r),Ql=m(Qn),Fs=n(Qn,"SPAN",{});var Vd=o(Fs);Yl=i(Vd,"FlaxMT5EncoderModel"),Vd.forEach(r),Qn.forEach(r),_n=m(e),U=n(e,"DIV",{class:!0});var Lr=o(U);k(Ht.$$.fragment,Lr),Zl=m(Lr),Rt=n(Lr,"P",{});var Yn=o(Rt);ei=i(Yn,"This class overrides "),$r=n(Yn,"A",{href:!0});var Ud=o($r);ti=i(Ud,"FlaxT5EncoderModel"),Ud.forEach(r),ri=i(Yn,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Yn.forEach(r),si=m(Lr),k(Ve.$$.fragment,Lr),Lr.forEach(r),this.h()},h(){d(g,"name","hf:doc:metadata"),d(g,"content",JSON.stringify(op)),d(u,"id","mt5"),d(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(u,"href","#mt5"),d(_,"class","relative group"),d(fe,"id","overview"),d(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(fe,"href","#overview"),d(K,"class","relative group"),d(Be,"href","https://arxiv.org/abs/2010.11934"),d(Be,"rel","nofollow"),d(He,"href","https://huggingface.co/datasets/mc4"),d(He,"rel","nofollow"),d(Re,"href","https://huggingface.co/google/mt5-small"),d(Re,"rel","nofollow"),d(Xe,"href","https://huggingface.co/google/mt5-base"),d(Xe,"rel","nofollow"),d(Ke,"href","https://huggingface.co/google/mt5-large"),d(Ke,"rel","nofollow"),d(Je,"href","https://huggingface.co/google/mt5-xl"),d(Je,"rel","nofollow"),d(Qe,"href","https://huggingface.co/google/mt5-xxl"),d(Qe,"rel","nofollow"),d(Ye,"href","https://huggingface.co/patrickvonplaten"),d(Ye,"rel","nofollow"),d(Ze,"href","https://github.com/google-research/multilingual-t5"),d(Ze,"rel","nofollow"),d(he,"id","transformers.MT5Config"),d(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(he,"href","#transformers.MT5Config"),d(J,"class","relative group"),d(tr,"href","/docs/transformers/main/en/model_doc/mt5#transformers.MT5Model"),d(rr,"href","/docs/transformers/main/en/model_doc/mt5#transformers.TFMT5Model"),d(rt,"href","https://huggingface.co/google/mt5-small"),d(rt,"rel","nofollow"),d(sr,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(nr,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(_e,"id","transformers.T5Tokenizer"),d(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_e,"href","#transformers.T5Tokenizer"),d(Y,"class","relative group"),d(at,"href","https://github.com/google/sentencepiece"),d(at,"rel","nofollow"),d(or,"href","/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),d(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ir,"href","/docs/transformers/main/en/model_doc/mt5#transformers.T5Tokenizer"),d(be,"id","transformers.T5TokenizerFast"),d(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(be,"href","#transformers.T5TokenizerFast"),d(Z,"class","relative group"),d(ht,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models"),d(ht,"rel","nofollow"),d(dr,"href","/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),d(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(cr,"href","/docs/transformers/main/en/model_doc/mt5#transformers.T5TokenizerFast"),d(xe,"id","transformers.MT5Model"),d(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(xe,"href","#transformers.MT5Model"),d(te,"class","relative group"),d(fr,"href","/docs/transformers/main/en/model_doc/t5#transformers.T5Model"),d(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ee,"id","transformers.MT5ForConditionalGeneration"),d(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ee,"href","#transformers.MT5ForConditionalGeneration"),d(re,"class","relative group"),d(ur,"href","/docs/transformers/main/en/model_doc/t5#transformers.T5ForConditionalGeneration"),d(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(qe,"id","transformers.MT5EncoderModel"),d(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(qe,"href","#transformers.MT5EncoderModel"),d(se,"class","relative group"),d(gr,"href","/docs/transformers/main/en/model_doc/t5#transformers.T5EncoderModel"),d(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(je,"id","transformers.TFMT5Model"),d(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(je,"href","#transformers.TFMT5Model"),d(ne,"class","relative group"),d(hr,"href","/docs/transformers/main/en/model_doc/t5#transformers.TFT5Model"),d(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ce,"id","transformers.TFMT5ForConditionalGeneration"),d(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ce,"href","#transformers.TFMT5ForConditionalGeneration"),d(oe,"class","relative group"),d(_r,"href","/docs/transformers/main/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),d(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Se,"id","transformers.TFMT5EncoderModel"),d(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Se,"href","#transformers.TFMT5EncoderModel"),d(ae,"class","relative group"),d(Tr,"href","/docs/transformers/main/en/model_doc/t5#transformers.TFT5EncoderModel"),d(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ne,"id","transformers.FlaxMT5Model"),d(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ne,"href","#transformers.FlaxMT5Model"),d(le,"class","relative group"),d(vr,"href","/docs/transformers/main/en/model_doc/t5#transformers.FlaxT5Model"),d(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ie,"id","transformers.FlaxMT5ForConditionalGeneration"),d(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ie,"href","#transformers.FlaxMT5ForConditionalGeneration"),d(ie,"class","relative group"),d(kr,"href","/docs/transformers/main/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),d(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Oe,"id","transformers.FlaxMT5EncoderModel"),d(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Oe,"href","#transformers.FlaxMT5EncoderModel"),d(de,"class","relative group"),d($r,"href","/docs/transformers/main/en/model_doc/t5#transformers.FlaxT5EncoderModel"),d(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,c){t(document.head,g),f(e,x,c),f(e,_,c),t(_,u),t(u,T),$(a,T,null),t(_,h),t(_,Nr),t(Nr,Zn),f(e,Ns,c),f(e,K,c),t(K,fe),t(fe,Dr),$(We,Dr,null),t(K,eo),t(K,Ir),t(Ir,to),f(e,Ds,c),f(e,ue,c),t(ue,ro),t(ue,Be),t(Be,so),t(ue,no),f(e,Is,c),f(e,Qt,c),t(Qt,oo),f(e,Gs,c),f(e,Yt,c),t(Yt,Gr),t(Gr,ao),f(e,Os,c),f(e,ge,c),t(ge,lo),t(ge,He),t(He,io),t(ge,po),f(e,Vs,c),f(e,Zt,c),t(Zt,mo),f(e,Us,c),f(e,F,c),t(F,Or),t(Or,Vr),t(Vr,Re),t(Re,co),t(F,fo),t(F,Ur),t(Ur,Wr),t(Wr,Xe),t(Xe,uo),t(F,go),t(F,Br),t(Br,Hr),t(Hr,Ke),t(Ke,ho),t(F,_o),t(F,Rr),t(Rr,Xr),t(Xr,Je),t(Je,To),t(F,vo),t(F,Kr),t(Kr,er),t(er,Qe),t(Qe,ko),t(er,$o),f(e,Ws,c),f(e,W,c),t(W,bo),t(W,Ye),t(Ye,yo),t(W,wo),t(W,Ze),t(Ze,xo),t(W,Mo),f(e,Bs,c),f(e,J,c),t(J,he),t(he,Jr),$(et,Jr,null),t(J,Eo),t(J,Qr),t(Qr,zo),f(e,Hs,c),f(e,C,c),$(tt,C,null),t(C,qo),t(C,A),t(A,Fo),t(A,tr),t(tr,jo),t(A,Po),t(A,rr),t(rr,Co),t(A,Ao),t(A,rt),t(rt,So),t(A,Lo),t(C,No),t(C,Q),t(Q,Do),t(Q,sr),t(sr,Io),t(Q,Go),t(Q,nr),t(nr,Oo),t(Q,Vo),f(e,Rs,c),f(e,Y,c),t(Y,_e),t(_e,Yr),$(st,Yr,null),t(Y,Uo),t(Y,Zr),t(Zr,Wo),f(e,Xs,c),f(e,E,c),$(nt,E,null),t(E,Bo),t(E,ot),t(ot,Ho),t(ot,at),t(at,Ro),t(ot,Xo),t(E,Ko),t(E,lt),t(lt,Jo),t(lt,or),t(or,Qo),t(lt,Yo),t(E,Zo),t(E,B),$(it,B,null),t(B,ea),t(B,es),t(es,ta),t(B,ra),t(B,dt),t(dt,ar),t(ar,sa),t(ar,ts),t(ts,na),t(dt,oa),t(dt,lr),t(lr,aa),t(lr,rs),t(rs,la),t(E,ia),t(E,Te),$(pt,Te,null),t(Te,da),t(Te,ss),t(ss,pa),t(E,ma),t(E,ve),$(mt,ve,null),t(ve,ca),t(ve,ns),t(ns,fa),t(E,ua),t(E,ke),$(ct,ke,null),t(ke,ga),t(ke,ft),t(ft,ha),t(ft,os),t(os,_a),t(ft,Ta),f(e,Ks,c),f(e,$e,c),t($e,va),t($e,ir),t(ir,ka),t($e,$a),f(e,Js,c),f(e,Z,c),t(Z,be),t(be,as),$(ut,as,null),t(Z,ba),t(Z,ls),t(ls,ya),f(e,Qs,c),f(e,q,c),$(gt,q,null),t(q,wa),t(q,ee),t(ee,xa),t(ee,is),t(is,Ma),t(ee,Ea),t(ee,ht),t(ht,za),t(ee,qa),t(q,Fa),t(q,_t),t(_t,ja),t(_t,dr),t(dr,Pa),t(_t,Ca),t(q,Aa),t(q,H),$(Tt,H,null),t(H,Sa),t(H,ds),t(ds,La),t(H,Na),t(H,vt),t(vt,pr),t(pr,Da),t(pr,ps),t(ps,Ia),t(vt,Ga),t(vt,mr),t(mr,Oa),t(mr,ms),t(ms,Va),t(q,Ua),t(q,ye),$(kt,ye,null),t(ye,Wa),t(ye,cs),t(cs,Ba),f(e,Ys,c),f(e,we,c),t(we,Ha),t(we,cr),t(cr,Ra),t(we,Xa),f(e,Zs,c),f(e,te,c),t(te,xe),t(xe,fs),$($t,fs,null),t(te,Ka),t(te,us),t(us,Ja),f(e,en,c),f(e,S,c),$(bt,S,null),t(S,Qa),t(S,yt),t(yt,Ya),t(yt,fr),t(fr,Za),t(yt,el),t(S,tl),$(Me,S,null),f(e,tn,c),f(e,re,c),t(re,Ee),t(Ee,gs),$(wt,gs,null),t(re,rl),t(re,hs),t(hs,sl),f(e,rn,c),f(e,L,c),$(xt,L,null),t(L,nl),t(L,Mt),t(Mt,ol),t(Mt,ur),t(ur,al),t(Mt,ll),t(L,il),$(ze,L,null),f(e,sn,c),f(e,se,c),t(se,qe),t(qe,_s),$(Et,_s,null),t(se,dl),t(se,Ts),t(Ts,pl),f(e,nn,c),f(e,N,c),$(zt,N,null),t(N,ml),t(N,qt),t(qt,cl),t(qt,gr),t(gr,fl),t(qt,ul),t(N,gl),$(Fe,N,null),f(e,on,c),f(e,ne,c),t(ne,je),t(je,vs),$(Ft,vs,null),t(ne,hl),t(ne,ks),t(ks,_l),f(e,an,c),f(e,D,c),$(jt,D,null),t(D,Tl),t(D,Pt),t(Pt,vl),t(Pt,hr),t(hr,kl),t(Pt,$l),t(D,bl),$(Pe,D,null),f(e,ln,c),f(e,oe,c),t(oe,Ce),t(Ce,$s),$(Ct,$s,null),t(oe,yl),t(oe,bs),t(bs,wl),f(e,dn,c),f(e,I,c),$(At,I,null),t(I,xl),t(I,St),t(St,Ml),t(St,_r),t(_r,El),t(St,zl),t(I,ql),$(Ae,I,null),f(e,pn,c),f(e,ae,c),t(ae,Se),t(Se,ys),$(Lt,ys,null),t(ae,Fl),t(ae,ws),t(ws,jl),f(e,mn,c),f(e,G,c),$(Nt,G,null),t(G,Pl),t(G,Dt),t(Dt,Cl),t(Dt,Tr),t(Tr,Al),t(Dt,Sl),t(G,Ll),$(Le,G,null),f(e,cn,c),f(e,le,c),t(le,Ne),t(Ne,xs),$(It,xs,null),t(le,Nl),t(le,Ms),t(Ms,Dl),f(e,fn,c),f(e,O,c),$(Gt,O,null),t(O,Il),t(O,Ot),t(Ot,Gl),t(Ot,vr),t(vr,Ol),t(Ot,Vl),t(O,Ul),$(De,O,null),f(e,un,c),f(e,ie,c),t(ie,Ie),t(Ie,Es),$(Vt,Es,null),t(ie,Wl),t(ie,zs),t(zs,Bl),f(e,gn,c),f(e,V,c),$(Ut,V,null),t(V,Hl),t(V,Wt),t(Wt,Rl),t(Wt,kr),t(kr,Xl),t(Wt,Kl),t(V,Jl),$(Ge,V,null),f(e,hn,c),f(e,de,c),t(de,Oe),t(Oe,qs),$(Bt,qs,null),t(de,Ql),t(de,Fs),t(Fs,Yl),f(e,_n,c),f(e,U,c),$(Ht,U,null),t(U,Zl),t(U,Rt),t(Rt,ei),t(Rt,$r),t($r,ti),t(Rt,ri),t(U,si),$(Ve,U,null),Tn=!0},p(e,[c]){const Xt={};c&2&&(Xt.$$scope={dirty:c,ctx:e}),Me.$set(Xt);const js={};c&2&&(js.$$scope={dirty:c,ctx:e}),ze.$set(js);const Ps={};c&2&&(Ps.$$scope={dirty:c,ctx:e}),Fe.$set(Ps);const Cs={};c&2&&(Cs.$$scope={dirty:c,ctx:e}),Pe.$set(Cs);const Kt={};c&2&&(Kt.$$scope={dirty:c,ctx:e}),Ae.$set(Kt);const As={};c&2&&(As.$$scope={dirty:c,ctx:e}),Le.$set(As);const Ss={};c&2&&(Ss.$$scope={dirty:c,ctx:e}),De.$set(Ss);const Ls={};c&2&&(Ls.$$scope={dirty:c,ctx:e}),Ge.$set(Ls);const Jt={};c&2&&(Jt.$$scope={dirty:c,ctx:e}),Ve.$set(Jt)},i(e){Tn||(b(a.$$.fragment,e),b(We.$$.fragment,e),b(et.$$.fragment,e),b(tt.$$.fragment,e),b(st.$$.fragment,e),b(nt.$$.fragment,e),b(it.$$.fragment,e),b(pt.$$.fragment,e),b(mt.$$.fragment,e),b(ct.$$.fragment,e),b(ut.$$.fragment,e),b(gt.$$.fragment,e),b(Tt.$$.fragment,e),b(kt.$$.fragment,e),b($t.$$.fragment,e),b(bt.$$.fragment,e),b(Me.$$.fragment,e),b(wt.$$.fragment,e),b(xt.$$.fragment,e),b(ze.$$.fragment,e),b(Et.$$.fragment,e),b(zt.$$.fragment,e),b(Fe.$$.fragment,e),b(Ft.$$.fragment,e),b(jt.$$.fragment,e),b(Pe.$$.fragment,e),b(Ct.$$.fragment,e),b(At.$$.fragment,e),b(Ae.$$.fragment,e),b(Lt.$$.fragment,e),b(Nt.$$.fragment,e),b(Le.$$.fragment,e),b(It.$$.fragment,e),b(Gt.$$.fragment,e),b(De.$$.fragment,e),b(Vt.$$.fragment,e),b(Ut.$$.fragment,e),b(Ge.$$.fragment,e),b(Bt.$$.fragment,e),b(Ht.$$.fragment,e),b(Ve.$$.fragment,e),Tn=!0)},o(e){y(a.$$.fragment,e),y(We.$$.fragment,e),y(et.$$.fragment,e),y(tt.$$.fragment,e),y(st.$$.fragment,e),y(nt.$$.fragment,e),y(it.$$.fragment,e),y(pt.$$.fragment,e),y(mt.$$.fragment,e),y(ct.$$.fragment,e),y(ut.$$.fragment,e),y(gt.$$.fragment,e),y(Tt.$$.fragment,e),y(kt.$$.fragment,e),y($t.$$.fragment,e),y(bt.$$.fragment,e),y(Me.$$.fragment,e),y(wt.$$.fragment,e),y(xt.$$.fragment,e),y(ze.$$.fragment,e),y(Et.$$.fragment,e),y(zt.$$.fragment,e),y(Fe.$$.fragment,e),y(Ft.$$.fragment,e),y(jt.$$.fragment,e),y(Pe.$$.fragment,e),y(Ct.$$.fragment,e),y(At.$$.fragment,e),y(Ae.$$.fragment,e),y(Lt.$$.fragment,e),y(Nt.$$.fragment,e),y(Le.$$.fragment,e),y(It.$$.fragment,e),y(Gt.$$.fragment,e),y(De.$$.fragment,e),y(Vt.$$.fragment,e),y(Ut.$$.fragment,e),y(Ge.$$.fragment,e),y(Bt.$$.fragment,e),y(Ht.$$.fragment,e),y(Ve.$$.fragment,e),Tn=!1},d(e){r(g),e&&r(x),e&&r(_),w(a),e&&r(Ns),e&&r(K),w(We),e&&r(Ds),e&&r(ue),e&&r(Is),e&&r(Qt),e&&r(Gs),e&&r(Yt),e&&r(Os),e&&r(ge),e&&r(Vs),e&&r(Zt),e&&r(Us),e&&r(F),e&&r(Ws),e&&r(W),e&&r(Bs),e&&r(J),w(et),e&&r(Hs),e&&r(C),w(tt),e&&r(Rs),e&&r(Y),w(st),e&&r(Xs),e&&r(E),w(nt),w(it),w(pt),w(mt),w(ct),e&&r(Ks),e&&r($e),e&&r(Js),e&&r(Z),w(ut),e&&r(Qs),e&&r(q),w(gt),w(Tt),w(kt),e&&r(Ys),e&&r(we),e&&r(Zs),e&&r(te),w($t),e&&r(en),e&&r(S),w(bt),w(Me),e&&r(tn),e&&r(re),w(wt),e&&r(rn),e&&r(L),w(xt),w(ze),e&&r(sn),e&&r(se),w(Et),e&&r(nn),e&&r(N),w(zt),w(Fe),e&&r(on),e&&r(ne),w(Ft),e&&r(an),e&&r(D),w(jt),w(Pe),e&&r(ln),e&&r(oe),w(Ct),e&&r(dn),e&&r(I),w(At),w(Ae),e&&r(pn),e&&r(ae),w(Lt),e&&r(mn),e&&r(G),w(Nt),w(Le),e&&r(cn),e&&r(le),w(It),e&&r(fn),e&&r(O),w(Gt),w(De),e&&r(un),e&&r(ie),w(Vt),e&&r(gn),e&&r(V),w(Ut),w(Ge),e&&r(hn),e&&r(de),w(Bt),e&&r(_n),e&&r(U),w(Ht),w(Ve)}}}const op={local:"mt5",sections:[{local:"overview",title:"Overview"},{local:"transformers.MT5Config",title:"MT5Config"},{local:"transformers.T5Tokenizer",title:"MT5Tokenizer"},{local:"transformers.T5TokenizerFast",title:"MT5TokenizerFast"},{local:"transformers.MT5Model",title:"MT5Model"},{local:"transformers.MT5ForConditionalGeneration",title:"MT5ForConditionalGeneration"},{local:"transformers.MT5EncoderModel",title:"MT5EncoderModel"},{local:"transformers.TFMT5Model",title:"TFMT5Model"},{local:"transformers.TFMT5ForConditionalGeneration",title:"TFMT5ForConditionalGeneration"},{local:"transformers.TFMT5EncoderModel",title:"TFMT5EncoderModel"},{local:"transformers.FlaxMT5Model",title:"FlaxMT5Model"},{local:"transformers.FlaxMT5ForConditionalGeneration",title:"FlaxMT5ForConditionalGeneration"},{local:"transformers.FlaxMT5EncoderModel",title:"FlaxMT5EncoderModel"}],title:"mT5"};function ap(M){return Xd(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class cp extends Wd{constructor(g){super();Bd(this,g,ap,np,Hd,{})}}export{cp as default,op as metadata};
