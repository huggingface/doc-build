import{S as qc,i as Ic,s as Lc,e as r,k as h,w as v,t as i,M as Fc,c as n,d as o,m as f,a,x as b,h as l,b as d,G as e,g as _,y as $,q as O,o as x,B as V,v as Ac,L as mt}from"../../chunks/vendor-hf-doc-builder.js";import{T as io}from"../../chunks/Tip-hf-doc-builder.js";import{D as k}from"../../chunks/Docstring-hf-doc-builder.js";import{C as be}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as H}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as pt}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Dc(y){let c,T,g,p,w;return p=new be({props:{code:`from transformers import OwlViTTextConfig, OwlViTTextModel

# Initializing a OwlViTTextModel with google/owlvit-base-patch32 style configuration
configuration = OwlViTTextConfig()

# Initializing a OwlViTTextConfig from the google/owlvit-base-patch32 style configuration
model = OwlViTTextModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTTextConfig, OwlViTTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTTextModel with google/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = OwlViTTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTTextConfig from the google/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){c=r("p"),T=i("Example:"),g=h(),v(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Example:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Wc(y){let c,T,g,p,w;return p=new be({props:{code:`from transformers import OwlViTVisionConfig, OwlViTVisionModel

# Initializing a OwlViTVisionModel with google/owlvit-base-patch32 style configuration
configuration = OwlViTVisionConfig()

# Initializing a OwlViTVisionModel model from the google/owlvit-base-patch32 style configuration
model = OwlViTVisionModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTVisionConfig, OwlViTVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTVisionModel with google/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = OwlViTVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTVisionModel model from the google/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){c=r("p"),T=i("Example:"),g=h(),v(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Example:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Nc(y){let c,T;return{c(){c=r("p"),T=i(`NumPy arrays and PyTorch tensors are converted to PIL images when resizing, so the most efficient is to pass
PIL images.`)},l(g){c=n(g,"P",{});var p=a(c);T=l(p,`NumPy arrays and PyTorch tensors are converted to PIL images when resizing, so the most efficient is to pass
PIL images.`),p.forEach(o)},m(g,p){_(g,c,p),e(c,T)},d(g){g&&o(c)}}}function Sc(y){let c,T,g,p,w;return{c(){c=r("p"),T=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,T),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Bc(y){let c,T,g,p,w;return p=new be({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(text=[["a photo of a cat", "a photo of a dog"]], images=image, return_tensors="pt")
outputs = model(**inputs)
logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`}}),{c(){c=r("p"),T=i("Examples:"),g=h(),v(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Examples:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Rc(y){let c,T,g,p,w;return{c(){c=r("p"),T=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,T),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Hc(y){let c,T,g,p,w;return p=new be({props:{code:`from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
inputs = processor(
    text=[["a photo of a cat", "a photo of a dog"], ["photo of a astranaut"]], return_tensors="pt"
)
text_features = model.get_text_features(**inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], [<span class="hljs-string">&quot;photo of a astranaut&quot;</span>]], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`}}),{c(){c=r("p"),T=i("Examples:"),g=h(),v(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Examples:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Uc(y){let c,T,g,p,w;return{c(){c=r("p"),T=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,T),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Gc(y){let c,T,g,p,w;return p=new be({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors="pt")
image_features = model.get_image_features(**inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`}}),{c(){c=r("p"),T=i("Examples:"),g=h(),v(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Examples:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Xc(y){let c,T,g,p,w;return{c(){c=r("p"),T=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,T),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Zc(y){let c,T,g,p,w;return p=new be({props:{code:`from transformers import OwlViTProcessor, OwlViTTextModel

model = OwlViTTextModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
inputs = processor(
    text=[["a photo of a cat", "a photo of a dog"], ["photo of a astranaut"]], return_tensors="pt"
)
outputs = model(**inputs)
last_hidden_state = outputs.last_hidden_state
pooled_output = outputs.pooler_output  # pooled (EOS token) states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTTextModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], [<span class="hljs-string">&quot;photo of a astranaut&quot;</span>]], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled (EOS token) states</span>`}}),{c(){c=r("p"),T=i("Examples:"),g=h(),v(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Examples:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Jc(y){let c,T,g,p,w;return{c(){c=r("p"),T=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,T),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Kc(y){let c,T,g,p,w;return p=new be({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTVisionModel

model = OwlViTVisionModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, return_tensors="pt")

outputs = model(**inputs)
last_hidden_state = outputs.last_hidden_state
pooled_output = outputs.pooler_output  # pooled CLS states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTVisionModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled CLS states</span>`}}),{c(){c=r("p"),T=i("Examples:"),g=h(),v(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Examples:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Yc(y){let c,T,g,p,w;return{c(){c=r("p"),T=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,T),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Qc(y){let c,T,g,p,w;return p=new be({props:{code:`import requests
from PIL import Image
import torch
from transformers import OwlViTProcessor, OwlViTForObjectDetection

processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch32")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
texts = [["a photo of a cat", "a photo of a dog"]]
inputs = processor(text=texts, images=image, return_tensors="pt")
outputs = model(**inputs)

# Target image sizes (height, width) to rescale box predictions [batch_size, 2]
target_sizes = torch.Tensor([image.size[::-1]])
# Convert outputs (bounding boxes and class logits) to COCO API
results = processor.post_process(outputs=outputs, target_sizes=target_sizes)

i = 0  # Retrieve predictions for the first image for the corresponding text queries
text = texts[i]
boxes, scores, labels = results[i]["boxes"], results[i]["scores"], results[i]["labels"]

score_threshold = 0.1
for box, score, label in zip(boxes, scores, labels):
    box = [round(i, 2) for i in box.tolist()]
    if score >= score_threshold:
        print(f"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTForObjectDetection.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>texts = [[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=texts, images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Target image sizes (height, width) to rescale box predictions [batch_size, 2]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_sizes = torch.Tensor([image.size[::-<span class="hljs-number">1</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Convert outputs (bounding boxes and class logits) to COCO API</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>results = processor.post_process(outputs=outputs, target_sizes=target_sizes)

<span class="hljs-meta">&gt;&gt;&gt; </span>i = <span class="hljs-number">0</span>  <span class="hljs-comment"># Retrieve predictions for the first image for the corresponding text queries</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = texts[i]
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes, scores, labels = results[i][<span class="hljs-string">&quot;boxes&quot;</span>], results[i][<span class="hljs-string">&quot;scores&quot;</span>], results[i][<span class="hljs-string">&quot;labels&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>score_threshold = <span class="hljs-number">0.1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> box, score, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(boxes, scores, labels):
<span class="hljs-meta">... </span>    box = [<span class="hljs-built_in">round</span>(i, <span class="hljs-number">2</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> box.tolist()]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">if</span> score &gt;= score_threshold:
<span class="hljs-meta">... </span>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Detected <span class="hljs-subst">{text[label]}</span> with confidence <span class="hljs-subst">{<span class="hljs-built_in">round</span>(score.item(), <span class="hljs-number">3</span>)}</span> at location <span class="hljs-subst">{box}</span>&quot;</span>)
Detected a photo of a cat <span class="hljs-keyword">with</span> confidence <span class="hljs-number">0.707</span> at location [<span class="hljs-number">324.97</span>, <span class="hljs-number">20.44</span>, <span class="hljs-number">640.58</span>, <span class="hljs-number">373.29</span>]
Detected a photo of a cat <span class="hljs-keyword">with</span> confidence <span class="hljs-number">0.717</span> at location [<span class="hljs-number">1.46</span>, <span class="hljs-number">55.26</span>, <span class="hljs-number">315.55</span>, <span class="hljs-number">472.17</span>]`}}),{c(){c=r("p"),T=i("Examples:"),g=h(),v(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Examples:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function ed(y){let c,T,g,p,w,s,m,j,Sr,Us,J,$e,os,ht,Br,ss,Rr,Gs,Oe,Hr,ft,Ur,Gr,Xs,lo,Xr,Zs,co,rs,Zr,Js,K,xe,ns,gt,Jr,as,Kr,Ks,Ve,Yr,po,Qr,en,Ys,z,mo,tn,on,ho,sn,rn,fo,nn,an,go,ln,cn,uo,dn,pn,_o,mn,hn,wo,fn,gn,Qs,ut,er,U,un,_t,_n,wn,wt,Tn,vn,tr,Y,ye,is,Tt,bn,ls,$n,or,C,vt,On,G,To,xn,Vn,vo,yn,jn,bt,kn,zn,Pn,Q,En,bo,Mn,Cn,$o,qn,In,Ln,je,$t,Fn,Ot,An,Oo,Dn,Wn,sr,ee,ke,cs,xt,Nn,ds,Sn,rr,q,Vt,Bn,te,Rn,xo,Hn,Un,yt,Gn,Xn,Zn,oe,Jn,Vo,Kn,Yn,yo,Qn,ea,ta,ze,nr,se,Pe,ps,jt,oa,ms,sa,ar,I,kt,ra,re,na,jo,aa,ia,zt,la,ca,da,ne,pa,ko,ma,ha,zo,fa,ga,ua,Ee,ir,ae,Me,hs,Pt,_a,fs,wa,lr,L,Et,Ta,gs,va,ba,Mt,$a,Po,Oa,xa,Va,X,Ct,ya,us,ja,ka,Ce,cr,ie,qe,_s,qt,za,ws,Pa,dr,P,It,Ea,E,Ma,Eo,Ca,qa,Mo,Ia,La,Co,Fa,Aa,Ts,Da,Wa,qo,Na,Sa,Ba,Ie,Lt,Ra,Ft,Ha,Io,Ua,Ga,Xa,Le,At,Za,Dt,Ja,Lo,Ka,Ya,Qa,Fe,Wt,ei,Nt,ti,vs,oi,si,pr,le,Ae,bs,St,ri,$s,ni,mr,F,Bt,ai,A,Rt,ii,ce,li,Fo,ci,di,Os,pi,mi,hi,De,fi,We,gi,D,Ht,ui,de,_i,Ao,wi,Ti,xs,vi,bi,$i,Ne,Oi,Se,xi,W,Ut,Vi,pe,yi,Do,ji,ki,Vs,zi,Pi,Ei,Be,Mi,Re,hr,me,He,ys,Gt,Ci,js,qi,fr,he,Xt,Ii,N,Zt,Li,fe,Fi,Wo,Ai,Di,ks,Wi,Ni,Si,Ue,Bi,Ge,gr,ge,Xe,zs,Jt,Ri,Ps,Hi,ur,ue,Kt,Ui,S,Yt,Gi,_e,Xi,No,Zi,Ji,Es,Ki,Yi,Qi,Ze,el,Je,_r,we,Ke,Ms,Qt,tl,Cs,ol,wr,Te,eo,sl,B,to,rl,ve,nl,So,al,il,qs,ll,cl,dl,Ye,pl,Qe,Tr;return s=new H({}),ht=new H({}),gt=new H({}),ut=new be({props:{code:`import requests
from PIL import Image
import torch

from transformers import OwlViTProcessor, OwlViTForObjectDetection

processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch32")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
texts = [["a photo of a cat", "a photo of a dog"]]
inputs = processor(text=texts, images=image, return_tensors="pt")
outputs = model(**inputs)

# Target image sizes (height, width) to rescale box predictions [batch_size, 2]
target_sizes = torch.Tensor([image.size[::-1]])
# Convert outputs (bounding boxes and class logits) to COCO API
results = processor.post_process(outputs=outputs, target_sizes=target_sizes)

i = 0  # Retrieve predictions for the first image for the corresponding text queries
text = texts[i]
boxes, scores, labels = results[i]["boxes"], results[i]["scores"], results[i]["labels"]

score_threshold = 0.1
for box, score, label in zip(boxes, scores, labels):
    box = [round(i, 2) for i in box.tolist()]
    if score >= score_threshold:
        print(f"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTForObjectDetection.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>texts = [[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=texts, images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Target image sizes (height, width) to rescale box predictions [batch_size, 2]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_sizes = torch.Tensor([image.size[::-<span class="hljs-number">1</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Convert outputs (bounding boxes and class logits) to COCO API</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>results = processor.post_process(outputs=outputs, target_sizes=target_sizes)

<span class="hljs-meta">&gt;&gt;&gt; </span>i = <span class="hljs-number">0</span>  <span class="hljs-comment"># Retrieve predictions for the first image for the corresponding text queries</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = texts[i]
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes, scores, labels = results[i][<span class="hljs-string">&quot;boxes&quot;</span>], results[i][<span class="hljs-string">&quot;scores&quot;</span>], results[i][<span class="hljs-string">&quot;labels&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>score_threshold = <span class="hljs-number">0.1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> box, score, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(boxes, scores, labels):
<span class="hljs-meta">... </span>    box = [<span class="hljs-built_in">round</span>(i, <span class="hljs-number">2</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> box.tolist()]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">if</span> score &gt;= score_threshold:
<span class="hljs-meta">... </span>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Detected <span class="hljs-subst">{text[label]}</span> with confidence <span class="hljs-subst">{<span class="hljs-built_in">round</span>(score.item(), <span class="hljs-number">3</span>)}</span> at location <span class="hljs-subst">{box}</span>&quot;</span>)
Detected a photo of a cat <span class="hljs-keyword">with</span> confidence <span class="hljs-number">0.707</span> at location [<span class="hljs-number">324.97</span>, <span class="hljs-number">20.44</span>, <span class="hljs-number">640.58</span>, <span class="hljs-number">373.29</span>]
Detected a photo of a cat <span class="hljs-keyword">with</span> confidence <span class="hljs-number">0.717</span> at location [<span class="hljs-number">1.46</span>, <span class="hljs-number">55.26</span>, <span class="hljs-number">315.55</span>, <span class="hljs-number">472.17</span>]`}}),Tt=new H({}),vt=new k({props:{name:"class transformers.OwlViTConfig",anchor:"transformers.OwlViTConfig",parameters:[{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"projection_dim",val:" = 512"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"return_dict",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTConfig.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextConfig">OwlViTTextConfig</a>.`,name:"text_config"},{anchor:"transformers.OwlViTConfig.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionConfig">OwlViTVisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.OwlViTConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.OwlViTConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> parameter. Default is used as per the original OWL-ViT
implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.OwlViTConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/configuration_owlvit.py#L252"}}),$t=new k({props:{name:"from_text_vision_configs",anchor:"transformers.OwlViTConfig.from_text_vision_configs",parameters:[{name:"text_config",val:": typing.Dict"},{name:"vision_config",val:": typing.Dict"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/configuration_owlvit.py#L318",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTConfig"
>OwlViTConfig</a></p>
`}}),xt=new H({}),Vt=new k({props:{name:"class transformers.OwlViTTextConfig",anchor:"transformers.OwlViTTextConfig",parameters:[{name:"vocab_size",val:" = 49408"},{name:"hidden_size",val:" = 512"},{name:"intermediate_size",val:" = 2048"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 8"},{name:"max_position_embeddings",val:" = 16"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 49406"},{name:"eos_token_id",val:" = 49407"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 49408) &#x2014;
Vocabulary size of the OWL-ViT text model. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel">OwlViTTextModel</a>.`,name:"vocab_size"},{anchor:"transformers.OwlViTTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.OwlViTTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.OwlViTTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.OwlViTTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.OwlViTTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.OwlViTTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported. layer_norm_eps (<code>float</code>, <em>optional</em>,
defaults to 1e-5): The epsilon used by the layer normalization layers.`,name:"hidden_act"},{anchor:"transformers.OwlViTTextConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.OwlViTTextConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.OwlViTTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.OwlViTTextConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/configuration_owlvit.py#L41"}}),ze=new pt({props:{anchor:"transformers.OwlViTTextConfig.example",$$slots:{default:[Dc]},$$scope:{ctx:y}}}),jt=new H({}),kt=new k({props:{name:"class transformers.OwlViTVisionConfig",anchor:"transformers.OwlViTVisionConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"intermediate_size",val:" = 3072"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"image_size",val:" = 768"},{name:"patch_size",val:" = 32"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.OwlViTVisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.OwlViTVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.OwlViTVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.OwlViTVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.OwlViTVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.OwlViTVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported. layer_norm_eps (<code>float</code>, <em>optional</em>,
defaults to 1e-5): The epsilon used by the layer normalization layers.`,name:"hidden_act"},{anchor:"transformers.OwlViTVisionConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.OwlViTVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.OwlViTVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.OwlViTVisionConfig.initializer_factor",description:`<strong>initializer_factor</strong> (\`float&#x201C;, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/configuration_owlvit.py#L149"}}),Ee=new pt({props:{anchor:"transformers.OwlViTVisionConfig.example",$$slots:{default:[Wc]},$$scope:{ctx:y}}}),Pt=new H({}),Et=new k({props:{name:"class transformers.OwlViTFeatureExtractor",anchor:"transformers.OwlViTFeatureExtractor",parameters:[{name:"do_resize",val:" = True"},{name:"size",val:" = (768, 768)"},{name:"resample",val:" = <Resampling.BICUBIC: 3>"},{name:"crop_size",val:" = 768"},{name:"do_center_crop",val:" = False"},{name:"do_normalize",val:" = True"},{name:"image_mean",val:" = None"},{name:"image_std",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTFeatureExtractor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the shorter edge of the input to a certain <code>size</code>.`,name:"do_resize"},{anchor:"transformers.OwlViTFeatureExtractor.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple[int, int]</code>, <em>optional</em>, defaults to (768, 768)) &#x2014;
The size to use for resizing the image. Only has an effect if <code>do_resize</code> is set to <code>True</code>. If <code>size</code> is a
sequence like (h, w), output size will be matched to this. If <code>size</code> is an int, then image will be resized
to (size, size).`,name:"size"},{anchor:"transformers.OwlViTFeatureExtractor.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.Resampling.BICUBIC</code>) &#x2014;
An optional resampling filter. This can be one of <code>PIL.Image.Resampling.NEAREST</code>,
<code>PIL.Image.Resampling.BOX</code>, <code>PIL.Image.Resampling.BILINEAR</code>, <code>PIL.Image.Resampling.HAMMING</code>,
<code>PIL.Image.Resampling.BICUBIC</code> or <code>PIL.Image.Resampling.LANCZOS</code>. Only has an effect if <code>do_resize</code> is set
to <code>True</code>.`,name:"resample"},{anchor:"transformers.OwlViTFeatureExtractor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to crop the input at the center. If the input size is smaller than <code>crop_size</code> along any edge, the
image is padded with 0&#x2019;s and then center cropped.`,name:"do_center_crop"},{anchor:"transformers.OwlViTFeatureExtractor.crop_size",description:"<strong>crop_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;",name:"crop_size"},{anchor:"transformers.OwlViTFeatureExtractor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input with <code>image_mean</code> and <code>image_std</code>. Desired output size when applying
center-cropping. Only has an effect if <code>do_center_crop</code> is set to <code>True</code>.`,name:"do_normalize"},{anchor:"transformers.OwlViTFeatureExtractor.image_mean",description:`<strong>image_mean</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[0.48145466, 0.4578275, 0.40821073]</code>) &#x2014;
The sequence of means for each channel, to be used when normalizing images.`,name:"image_mean"},{anchor:"transformers.OwlViTFeatureExtractor.image_std",description:`<strong>image_std</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[0.26862954, 0.26130258, 0.27577711]</code>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images.`,name:"image_std"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/feature_extraction_owlvit.py#L45"}}),Ct=new k({props:{name:"__call__",anchor:"transformers.OwlViTFeatureExtractor.__call__",parameters:[{name:"images",val:": typing.Union[PIL.Image.Image, numpy.ndarray, ForwardRef('torch.Tensor'), typing.List[PIL.Image.Image], typing.List[numpy.ndarray], typing.List[ForwardRef('torch.Tensor')]]"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTFeatureExtractor.__call__.images",description:`<strong>images</strong> (<code>PIL.Image.Image</code>, <code>np.ndarray</code>, <code>torch.Tensor</code>, <code>List[PIL.Image.Image]</code>, <code>List[np.ndarray]</code>, <code>List[torch.Tensor]</code>) &#x2014;
The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch
tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W) or (H, W, C),
where C is a number of channels, H and W are image height and width.`,name:"images"},{anchor:"transformers.OwlViTFeatureExtractor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>, defaults to <code>&apos;np&apos;</code>) &#x2014;
If set, will return tensors of a particular framework. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return NumPy <code>np.ndarray</code> objects.</li>
<li><code>&apos;jax&apos;</code>: Return JAX <code>jnp.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/feature_extraction_owlvit.py#L146",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a> with the following fields:</p>
<ul>
<li><strong>pixel_values</strong> \u2014 Pixel values to be fed to a model.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),Ce=new io({props:{warning:!0,$$slots:{default:[Nc]},$$scope:{ctx:y}}}),qt=new H({}),It=new k({props:{name:"class transformers.OwlViTProcessor",anchor:"transformers.OwlViTProcessor",parameters:[{name:"feature_extractor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.OwlViTProcessor.feature_extractor",description:`<strong>feature_extractor</strong> (<a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor">OwlViTFeatureExtractor</a>) &#x2014;
The feature extractor is a required input.`,name:"feature_extractor"},{anchor:"transformers.OwlViTProcessor.tokenizer",description:`<strong>tokenizer</strong> ([<code>CLIPTokenizer</code>, <code>CLIPTokenizerFast</code>]) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/processing_owlvit.py#L28"}}),Lt=new k({props:{name:"batch_decode",anchor:"transformers.OwlViTProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/processing_owlvit.py#L149"}}),At=new k({props:{name:"decode",anchor:"transformers.OwlViTProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/processing_owlvit.py#L156"}}),Wt=new k({props:{name:"post_process",anchor:"transformers.OwlViTProcessor.post_process",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/processing_owlvit.py#L142"}}),St=new H({}),Bt=new k({props:{name:"class transformers.OwlViTModel",anchor:"transformers.OwlViTModel",parameters:[{name:"config",val:": OwlViTConfig"}],parametersDescription:[{anchor:"transformers.OwlViTModel.This",description:`<strong>This</strong> model is a PyTorch [torch.nn.Module](https &#x2014;
//pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it`,name:"This"},{anchor:"transformers.OwlViTModel.as",description:`<strong>as</strong> a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and &#x2014;
behavior. &#x2014;
config (<a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTConfig">OwlViTConfig</a>): Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"as"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L879"}}),Rt=new k({props:{name:"forward",anchor:"transformers.OwlViTModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"return_loss",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_base_image_embeds",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTModel.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.OwlViTModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.forward.return_base_image_embeds",description:`<strong>return_base_image_embeds</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return unprojected image embeddings. Set to <code>True</code> when <code>OwlViTModel</code> is called within
<code>OwlViTForObjectDetection</code>.`,name:"return_base_image_embeds"},{anchor:"transformers.OwlViTModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L1009",returnDescription:`
<p>A <code>transformers.models.owlvit.modeling_owlvit.OwlViTOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) \u2014 Contrastive loss for image-text similarity.</li>
<li><strong>logits_per_image</strong> (<code>torch.FloatTensor</code> of shape <code>(image_batch_size, text_batch_size)</code>) \u2014 The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text</strong> (<code>torch.FloatTensor</code> of shape <code>(text_batch_size, image_batch_size)</code>) \u2014 The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size * num_max_text_queries, output_dim</code>) \u2014 The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>) \u2014 The image embeddings obtained by applying the projection layer to the pooled output of
<a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</li>
<li><strong>text_model_output</strong> (Tuple<code>BaseModelOutputWithPooling</code>) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>vision_model_output</strong> (<code>BaseModelOutputWithPooling</code>) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</li>
</ul>
`,returnType:`
<p><code>transformers.models.owlvit.modeling_owlvit.OwlViTOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),De=new io({props:{$$slots:{default:[Sc]},$$scope:{ctx:y}}}),We=new pt({props:{anchor:"transformers.OwlViTModel.forward.example",$$slots:{default:[Bc]},$$scope:{ctx:y}}}),Ht=new k({props:{name:"get_text_features",anchor:"transformers.OwlViTModel.get_text_features",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L914",returnDescription:`
<p>The text embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</p>
`,returnType:`
<p>text_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),Ne=new io({props:{$$slots:{default:[Rc]},$$scope:{ctx:y}}}),Se=new pt({props:{anchor:"transformers.OwlViTModel.get_text_features.example",$$slots:{default:[Hc]},$$scope:{ctx:y}}}),Ut=new k({props:{name:"get_image_features",anchor:"transformers.OwlViTModel.get_image_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"return_projected",val:": typing.Optional[bool] = True"}],parametersDescription:[{anchor:"transformers.OwlViTModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L959",returnDescription:`
<p>The image embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</p>
`,returnType:`
<p>image_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),Be=new io({props:{$$slots:{default:[Uc]},$$scope:{ctx:y}}}),Re=new pt({props:{anchor:"transformers.OwlViTModel.get_image_features.example",$$slots:{default:[Gc]},$$scope:{ctx:y}}}),Gt=new H({}),Xt=new k({props:{name:"class transformers.OwlViTTextModel",anchor:"transformers.OwlViTTextModel",parameters:[{name:"config",val:": OwlViTTextConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L718"}}),Zt=new k({props:{name:"forward",anchor:"transformers.OwlViTTextModel.forward",parameters:[{name:"input_ids",val:": Tensor"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L733",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ue=new io({props:{$$slots:{default:[Xc]},$$scope:{ctx:y}}}),Ge=new pt({props:{anchor:"transformers.OwlViTTextModel.forward.example",$$slots:{default:[Zc]},$$scope:{ctx:y}}}),Jt=new H({}),Kt=new k({props:{name:"class transformers.OwlViTVisionModel",anchor:"transformers.OwlViTVisionModel",parameters:[{name:"config",val:": OwlViTVisionConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L828"}}),Yt=new k({props:{name:"forward",anchor:"transformers.OwlViTVisionModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTVisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L841",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTVisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ze=new io({props:{$$slots:{default:[Jc]},$$scope:{ctx:y}}}),Je=new pt({props:{anchor:"transformers.OwlViTVisionModel.forward.example",$$slots:{default:[Kc]},$$scope:{ctx:y}}}),Qt=new H({}),eo=new k({props:{name:"class transformers.OwlViTForObjectDetection",anchor:"transformers.OwlViTForObjectDetection",parameters:[{name:"config",val:": OwlViTConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L1171"}}),to=new k({props:{name:"forward",anchor:"transformers.OwlViTForObjectDetection.forward",parameters:[{name:"input_ids",val:": Tensor"},{name:"pixel_values",val:": FloatTensor"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTForObjectDetection.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTForObjectDetection.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTForObjectDetection.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L1307",returnDescription:`
<p>A <code>transformers.models.owlvit.modeling_owlvit.OwlViTObjectDetectionOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> are provided)) \u2014 Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a
bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized
scale-invariant IoU loss.</li>
<li><strong>loss_dict</strong> (<code>Dict</code>, <em>optional</em>) \u2014 A dictionary containing the individual losses. Useful for logging.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches, num_queries)</code>) \u2014 Classification logits (including no-object) for all queries.</li>
<li><strong>pred_boxes</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches, 4)</code>) \u2014 Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These
values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding
possible padding). You can use <code>post_process()</code> to retrieve the unnormalized
bounding boxes.</li>
<li><strong>text_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_max_text_queries, output_dim</code>) \u2014 The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, patch_size, patch_size, output_dim</code>) \u2014 Pooled output of <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>. OWL-ViT represents images as a set of image patches and computes
image embeddings for each patch.</li>
<li><strong>class_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches, hidden_size)</code>) \u2014 Class embeddings of all image patches. OWL-ViT represents images as a set of image patches where the total
number of patches is (image_size / patch_size)**2.</li>
<li><strong>text_model_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>)) \u2014 Last hidden states extracted from the <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>vision_model_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches + 1, hidden_size)</code>)) \u2014 Last hidden states extracted from the <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>. OWL-ViT represents images as a set of image
patches where the total number of patches is (image_size / patch_size)**2.</li>
</ul>
`,returnType:`
<p><code>transformers.models.owlvit.modeling_owlvit.OwlViTObjectDetectionOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ye=new io({props:{$$slots:{default:[Yc]},$$scope:{ctx:y}}}),Qe=new pt({props:{anchor:"transformers.OwlViTForObjectDetection.forward.example",$$slots:{default:[Qc]},$$scope:{ctx:y}}}),{c(){c=r("meta"),T=h(),g=r("h1"),p=r("a"),w=r("span"),v(s.$$.fragment),m=h(),j=r("span"),Sr=i("OWL-ViT"),Us=h(),J=r("h2"),$e=r("a"),os=r("span"),v(ht.$$.fragment),Br=h(),ss=r("span"),Rr=i("Overview"),Gs=h(),Oe=r("p"),Hr=i("The OWL-ViT (short for Vision Transformer for Open-World Localization) was proposed in "),ft=r("a"),Ur=i("Simple Open-Vocabulary Object Detection with Vision Transformers"),Gr=i(" by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. OWL-ViT is an open-vocabulary object detection network trained on a variety of (image, text) pairs. It can be used to query an image with one or multiple text queries to search for and detect target objects described in text."),Xs=h(),lo=r("p"),Xr=i("The abstract from the paper is the following:"),Zs=h(),co=r("p"),rs=r("em"),Zr=i("Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub."),Js=h(),K=r("h2"),xe=r("a"),ns=r("span"),v(gt.$$.fragment),Jr=h(),as=r("span"),Kr=i("Usage"),Ks=h(),Ve=r("p"),Yr=i("OWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses "),po=r("a"),Qr=i("CLIP"),en=i(" as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection."),Ys=h(),z=r("p"),mo=r("a"),tn=i("OwlViTFeatureExtractor"),on=i(" can be used to resize (or rescale) and normalize images for the model and "),ho=r("a"),sn=i("CLIPTokenizer"),rn=i(" is used to encode the text. "),fo=r("a"),nn=i("OwlViTProcessor"),an=i(" wraps "),go=r("a"),ln=i("OwlViTFeatureExtractor"),cn=i(" and "),uo=r("a"),dn=i("CLIPTokenizer"),pn=i(" into a single instance to both encode the text and prepare the images. The following example shows how to perform object detection using "),_o=r("a"),mn=i("OwlViTProcessor"),hn=i(" and "),wo=r("a"),fn=i("OwlViTForObjectDetection"),gn=i("."),Qs=h(),v(ut.$$.fragment),er=h(),U=r("p"),un=i("This model was contributed by "),_t=r("a"),_n=i("adirik"),wn=i(". The original code can be found "),wt=r("a"),Tn=i("here"),vn=i("."),tr=h(),Y=r("h2"),ye=r("a"),is=r("span"),v(Tt.$$.fragment),bn=h(),ls=r("span"),$n=i("OwlViTConfig"),or=h(),C=r("div"),v(vt.$$.fragment),On=h(),G=r("p"),To=r("a"),xn=i("OwlViTConfig"),Vn=i(" is the configuration class to store the configuration of an "),vo=r("a"),yn=i("OwlViTModel"),jn=i(`. It is used to
instantiate an OWL-ViT model according to the specified arguments, defining the text model and vision model
configs. Instantiating a configuration with the defaults will yield a similar configuration to that of the OWL-ViT
`),bt=r("a"),kn=i("google/owlvit-base-patch32"),zn=i(" architecture."),Pn=h(),Q=r("p"),En=i("Configuration objects inherit from "),bo=r("a"),Mn=i("PretrainedConfig"),Cn=i(` and can be used to control the model outputs. Read the
documentation from `),$o=r("a"),qn=i("PretrainedConfig"),In=i(" for more information."),Ln=h(),je=r("div"),v($t.$$.fragment),Fn=h(),Ot=r("p"),An=i("Instantiate a "),Oo=r("a"),Dn=i("OwlViTConfig"),Wn=i(` (or a derived class) from owlvit text model configuration and owlvit vision
model configuration.`),sr=h(),ee=r("h2"),ke=r("a"),cs=r("span"),v(xt.$$.fragment),Nn=h(),ds=r("span"),Sn=i("OwlViTTextConfig"),rr=h(),q=r("div"),v(Vt.$$.fragment),Bn=h(),te=r("p"),Rn=i("This is the configuration class to store the configuration of an "),xo=r("a"),Hn=i("OwlViTTextModel"),Un=i(`. It is used to instantiate an
OwlViT text encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OwlViT
`),yt=r("a"),Gn=i("google/owlvit-base-patch32"),Xn=i(" architecture."),Zn=h(),oe=r("p"),Jn=i("Configuration objects inherit from "),Vo=r("a"),Kn=i("PretrainedConfig"),Yn=i(` and can be used to control the model outputs. Read the
documentation from `),yo=r("a"),Qn=i("PretrainedConfig"),ea=i(" for more information."),ta=h(),v(ze.$$.fragment),nr=h(),se=r("h2"),Pe=r("a"),ps=r("span"),v(jt.$$.fragment),oa=h(),ms=r("span"),sa=i("OwlViTVisionConfig"),ar=h(),I=r("div"),v(kt.$$.fragment),ra=h(),re=r("p"),na=i("This is the configuration class to store the configuration of an "),jo=r("a"),aa=i("OwlViTVisionModel"),ia=i(`. It is used to instantiate
an OWL-ViT image encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OWL-ViT
`),zt=r("a"),la=i("google/owlvit-base-patch32"),ca=i(" architecture."),da=h(),ne=r("p"),pa=i("Configuration objects inherit from "),ko=r("a"),ma=i("PretrainedConfig"),ha=i(` and can be used to control the model outputs. Read the
documentation from `),zo=r("a"),fa=i("PretrainedConfig"),ga=i(" for more information."),ua=h(),v(Ee.$$.fragment),ir=h(),ae=r("h2"),Me=r("a"),hs=r("span"),v(Pt.$$.fragment),_a=h(),fs=r("span"),wa=i("OwlViTFeatureExtractor"),lr=h(),L=r("div"),v(Et.$$.fragment),Ta=h(),gs=r("p"),va=i("Constructs an OWL-ViT feature extractor."),ba=h(),Mt=r("p"),$a=i("This feature extractor inherits from "),Po=r("a"),Oa=i("FeatureExtractionMixin"),xa=i(` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),Va=h(),X=r("div"),v(Ct.$$.fragment),ya=h(),us=r("p"),ja=i("Main method to prepare for the model one or several image(s)."),ka=h(),v(Ce.$$.fragment),cr=h(),ie=r("h2"),qe=r("a"),_s=r("span"),v(qt.$$.fragment),za=h(),ws=r("span"),Pa=i("OwlViTProcessor"),dr=h(),P=r("div"),v(It.$$.fragment),Ea=h(),E=r("p"),Ma=i("Constructs an OWL-ViT processor which wraps "),Eo=r("a"),Ca=i("OwlViTFeatureExtractor"),qa=i(" and "),Mo=r("a"),Ia=i("CLIPTokenizer"),La=i("/"),Co=r("a"),Fa=i("CLIPTokenizerFast"),Aa=i(`
into a single processor that interits both the feature extractor and tokenizer functionalities. See the
`),Ts=r("code"),Da=i("__call__()"),Wa=i(" and "),qo=r("a"),Na=i("decode()"),Sa=i(" for more information."),Ba=h(),Ie=r("div"),v(Lt.$$.fragment),Ra=h(),Ft=r("p"),Ha=i("This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Io=r("a"),Ua=i("batch_decode()"),Ga=i(`. Please
refer to the docstring of this method for more information.`),Xa=h(),Le=r("div"),v(At.$$.fragment),Za=h(),Dt=r("p"),Ja=i("This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Lo=r("a"),Ka=i("decode()"),Ya=i(`. Please refer to
the docstring of this method for more information.`),Qa=h(),Fe=r("div"),v(Wt.$$.fragment),ei=h(),Nt=r("p"),ti=i("This method forwards all its arguments to "),vs=r("code"),oi=i("OwlViTFeatureExtractor.post_process()"),si=i(`. Please refer to the
docstring of this method for more information.`),pr=h(),le=r("h2"),Ae=r("a"),bs=r("span"),v(St.$$.fragment),ri=h(),$s=r("span"),ni=i("OwlViTModel"),mr=h(),F=r("div"),v(Bt.$$.fragment),ai=h(),A=r("div"),v(Rt.$$.fragment),ii=h(),ce=r("p"),li=i("The "),Fo=r("a"),ci=i("OwlViTModel"),di=i(" forward method, overrides the "),Os=r("code"),pi=i("__call__"),mi=i(" special method."),hi=h(),v(De.$$.fragment),fi=h(),v(We.$$.fragment),gi=h(),D=r("div"),v(Ht.$$.fragment),ui=h(),de=r("p"),_i=i("The "),Ao=r("a"),wi=i("OwlViTModel"),Ti=i(" forward method, overrides the "),xs=r("code"),vi=i("__call__"),bi=i(" special method."),$i=h(),v(Ne.$$.fragment),Oi=h(),v(Se.$$.fragment),xi=h(),W=r("div"),v(Ut.$$.fragment),Vi=h(),pe=r("p"),yi=i("The "),Do=r("a"),ji=i("OwlViTModel"),ki=i(" forward method, overrides the "),Vs=r("code"),zi=i("__call__"),Pi=i(" special method."),Ei=h(),v(Be.$$.fragment),Mi=h(),v(Re.$$.fragment),hr=h(),me=r("h2"),He=r("a"),ys=r("span"),v(Gt.$$.fragment),Ci=h(),js=r("span"),qi=i("OwlViTTextModel"),fr=h(),he=r("div"),v(Xt.$$.fragment),Ii=h(),N=r("div"),v(Zt.$$.fragment),Li=h(),fe=r("p"),Fi=i("The "),Wo=r("a"),Ai=i("OwlViTTextModel"),Di=i(" forward method, overrides the "),ks=r("code"),Wi=i("__call__"),Ni=i(" special method."),Si=h(),v(Ue.$$.fragment),Bi=h(),v(Ge.$$.fragment),gr=h(),ge=r("h2"),Xe=r("a"),zs=r("span"),v(Jt.$$.fragment),Ri=h(),Ps=r("span"),Hi=i("OwlViTVisionModel"),ur=h(),ue=r("div"),v(Kt.$$.fragment),Ui=h(),S=r("div"),v(Yt.$$.fragment),Gi=h(),_e=r("p"),Xi=i("The "),No=r("a"),Zi=i("OwlViTVisionModel"),Ji=i(" forward method, overrides the "),Es=r("code"),Ki=i("__call__"),Yi=i(" special method."),Qi=h(),v(Ze.$$.fragment),el=h(),v(Je.$$.fragment),_r=h(),we=r("h2"),Ke=r("a"),Ms=r("span"),v(Qt.$$.fragment),tl=h(),Cs=r("span"),ol=i("OwlViTForObjectDetection"),wr=h(),Te=r("div"),v(eo.$$.fragment),sl=h(),B=r("div"),v(to.$$.fragment),rl=h(),ve=r("p"),nl=i("The "),So=r("a"),al=i("OwlViTForObjectDetection"),il=i(" forward method, overrides the "),qs=r("code"),ll=i("__call__"),cl=i(" special method."),dl=h(),v(Ye.$$.fragment),pl=h(),v(Qe.$$.fragment),this.h()},l(t){const u=Fc('[data-svelte="svelte-1phssyn"]',document.head);c=n(u,"META",{name:!0,content:!0}),u.forEach(o),T=f(t),g=n(t,"H1",{class:!0});var oo=a(g);p=n(oo,"A",{id:!0,class:!0,href:!0});var Is=a(p);w=n(Is,"SPAN",{});var Ls=a(w);b(s.$$.fragment,Ls),Ls.forEach(o),Is.forEach(o),m=f(oo),j=n(oo,"SPAN",{});var Fs=a(j);Sr=l(Fs,"OWL-ViT"),Fs.forEach(o),oo.forEach(o),Us=f(t),J=n(t,"H2",{class:!0});var so=a(J);$e=n(so,"A",{id:!0,class:!0,href:!0});var As=a($e);os=n(As,"SPAN",{});var Ds=a(os);b(ht.$$.fragment,Ds),Ds.forEach(o),As.forEach(o),Br=f(so),ss=n(so,"SPAN",{});var Ws=a(ss);Rr=l(Ws,"Overview"),Ws.forEach(o),so.forEach(o),Gs=f(t),Oe=n(t,"P",{});var ro=a(Oe);Hr=l(ro,"The OWL-ViT (short for Vision Transformer for Open-World Localization) was proposed in "),ft=n(ro,"A",{href:!0,rel:!0});var Ns=a(ft);Ur=l(Ns,"Simple Open-Vocabulary Object Detection with Vision Transformers"),Ns.forEach(o),Gr=l(ro," by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. OWL-ViT is an open-vocabulary object detection network trained on a variety of (image, text) pairs. It can be used to query an image with one or multiple text queries to search for and detect target objects described in text."),ro.forEach(o),Xs=f(t),lo=n(t,"P",{});var Ss=a(lo);Xr=l(Ss,"The abstract from the paper is the following:"),Ss.forEach(o),Zs=f(t),co=n(t,"P",{});var Bs=a(co);rs=n(Bs,"EM",{});var Rs=a(rs);Zr=l(Rs,"Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub."),Rs.forEach(o),Bs.forEach(o),Js=f(t),K=n(t,"H2",{class:!0});var no=a(K);xe=n(no,"A",{id:!0,class:!0,href:!0});var Hs=a(xe);ns=n(Hs,"SPAN",{});var ml=a(ns);b(gt.$$.fragment,ml),ml.forEach(o),Hs.forEach(o),Jr=f(no),as=n(no,"SPAN",{});var hl=a(as);Kr=l(hl,"Usage"),hl.forEach(o),no.forEach(o),Ks=f(t),Ve=n(t,"P",{});var vr=a(Ve);Yr=l(vr,"OWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses "),po=n(vr,"A",{href:!0});var fl=a(po);Qr=l(fl,"CLIP"),fl.forEach(o),en=l(vr," as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection."),vr.forEach(o),Ys=f(t),z=n(t,"P",{});var M=a(z);mo=n(M,"A",{href:!0});var gl=a(mo);tn=l(gl,"OwlViTFeatureExtractor"),gl.forEach(o),on=l(M," can be used to resize (or rescale) and normalize images for the model and "),ho=n(M,"A",{href:!0});var ul=a(ho);sn=l(ul,"CLIPTokenizer"),ul.forEach(o),rn=l(M," is used to encode the text. "),fo=n(M,"A",{href:!0});var _l=a(fo);nn=l(_l,"OwlViTProcessor"),_l.forEach(o),an=l(M," wraps "),go=n(M,"A",{href:!0});var wl=a(go);ln=l(wl,"OwlViTFeatureExtractor"),wl.forEach(o),cn=l(M," and "),uo=n(M,"A",{href:!0});var Tl=a(uo);dn=l(Tl,"CLIPTokenizer"),Tl.forEach(o),pn=l(M," into a single instance to both encode the text and prepare the images. The following example shows how to perform object detection using "),_o=n(M,"A",{href:!0});var vl=a(_o);mn=l(vl,"OwlViTProcessor"),vl.forEach(o),hn=l(M," and "),wo=n(M,"A",{href:!0});var bl=a(wo);fn=l(bl,"OwlViTForObjectDetection"),bl.forEach(o),gn=l(M,"."),M.forEach(o),Qs=f(t),b(ut.$$.fragment,t),er=f(t),U=n(t,"P",{});var Bo=a(U);un=l(Bo,"This model was contributed by "),_t=n(Bo,"A",{href:!0,rel:!0});var $l=a(_t);_n=l($l,"adirik"),$l.forEach(o),wn=l(Bo,". The original code can be found "),wt=n(Bo,"A",{href:!0,rel:!0});var Ol=a(wt);Tn=l(Ol,"here"),Ol.forEach(o),vn=l(Bo,"."),Bo.forEach(o),tr=f(t),Y=n(t,"H2",{class:!0});var br=a(Y);ye=n(br,"A",{id:!0,class:!0,href:!0});var xl=a(ye);is=n(xl,"SPAN",{});var Vl=a(is);b(Tt.$$.fragment,Vl),Vl.forEach(o),xl.forEach(o),bn=f(br),ls=n(br,"SPAN",{});var yl=a(ls);$n=l(yl,"OwlViTConfig"),yl.forEach(o),br.forEach(o),or=f(t),C=n(t,"DIV",{class:!0});var et=a(C);b(vt.$$.fragment,et),On=f(et),G=n(et,"P",{});var ao=a(G);To=n(ao,"A",{href:!0});var jl=a(To);xn=l(jl,"OwlViTConfig"),jl.forEach(o),Vn=l(ao," is the configuration class to store the configuration of an "),vo=n(ao,"A",{href:!0});var kl=a(vo);yn=l(kl,"OwlViTModel"),kl.forEach(o),jn=l(ao,`. It is used to
instantiate an OWL-ViT model according to the specified arguments, defining the text model and vision model
configs. Instantiating a configuration with the defaults will yield a similar configuration to that of the OWL-ViT
`),bt=n(ao,"A",{href:!0,rel:!0});var zl=a(bt);kn=l(zl,"google/owlvit-base-patch32"),zl.forEach(o),zn=l(ao," architecture."),ao.forEach(o),Pn=f(et),Q=n(et,"P",{});var Ro=a(Q);En=l(Ro,"Configuration objects inherit from "),bo=n(Ro,"A",{href:!0});var Pl=a(bo);Mn=l(Pl,"PretrainedConfig"),Pl.forEach(o),Cn=l(Ro,` and can be used to control the model outputs. Read the
documentation from `),$o=n(Ro,"A",{href:!0});var El=a($o);qn=l(El,"PretrainedConfig"),El.forEach(o),In=l(Ro," for more information."),Ro.forEach(o),Ln=f(et),je=n(et,"DIV",{class:!0});var $r=a(je);b($t.$$.fragment,$r),Fn=f($r),Ot=n($r,"P",{});var Or=a(Ot);An=l(Or,"Instantiate a "),Oo=n(Or,"A",{href:!0});var Ml=a(Oo);Dn=l(Ml,"OwlViTConfig"),Ml.forEach(o),Wn=l(Or,` (or a derived class) from owlvit text model configuration and owlvit vision
model configuration.`),Or.forEach(o),$r.forEach(o),et.forEach(o),sr=f(t),ee=n(t,"H2",{class:!0});var xr=a(ee);ke=n(xr,"A",{id:!0,class:!0,href:!0});var Cl=a(ke);cs=n(Cl,"SPAN",{});var ql=a(cs);b(xt.$$.fragment,ql),ql.forEach(o),Cl.forEach(o),Nn=f(xr),ds=n(xr,"SPAN",{});var Il=a(ds);Sn=l(Il,"OwlViTTextConfig"),Il.forEach(o),xr.forEach(o),rr=f(t),q=n(t,"DIV",{class:!0});var tt=a(q);b(Vt.$$.fragment,tt),Bn=f(tt),te=n(tt,"P",{});var Ho=a(te);Rn=l(Ho,"This is the configuration class to store the configuration of an "),xo=n(Ho,"A",{href:!0});var Ll=a(xo);Hn=l(Ll,"OwlViTTextModel"),Ll.forEach(o),Un=l(Ho,`. It is used to instantiate an
OwlViT text encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OwlViT
`),yt=n(Ho,"A",{href:!0,rel:!0});var Fl=a(yt);Gn=l(Fl,"google/owlvit-base-patch32"),Fl.forEach(o),Xn=l(Ho," architecture."),Ho.forEach(o),Zn=f(tt),oe=n(tt,"P",{});var Uo=a(oe);Jn=l(Uo,"Configuration objects inherit from "),Vo=n(Uo,"A",{href:!0});var Al=a(Vo);Kn=l(Al,"PretrainedConfig"),Al.forEach(o),Yn=l(Uo,` and can be used to control the model outputs. Read the
documentation from `),yo=n(Uo,"A",{href:!0});var Dl=a(yo);Qn=l(Dl,"PretrainedConfig"),Dl.forEach(o),ea=l(Uo," for more information."),Uo.forEach(o),ta=f(tt),b(ze.$$.fragment,tt),tt.forEach(o),nr=f(t),se=n(t,"H2",{class:!0});var Vr=a(se);Pe=n(Vr,"A",{id:!0,class:!0,href:!0});var Wl=a(Pe);ps=n(Wl,"SPAN",{});var Nl=a(ps);b(jt.$$.fragment,Nl),Nl.forEach(o),Wl.forEach(o),oa=f(Vr),ms=n(Vr,"SPAN",{});var Sl=a(ms);sa=l(Sl,"OwlViTVisionConfig"),Sl.forEach(o),Vr.forEach(o),ar=f(t),I=n(t,"DIV",{class:!0});var ot=a(I);b(kt.$$.fragment,ot),ra=f(ot),re=n(ot,"P",{});var Go=a(re);na=l(Go,"This is the configuration class to store the configuration of an "),jo=n(Go,"A",{href:!0});var Bl=a(jo);aa=l(Bl,"OwlViTVisionModel"),Bl.forEach(o),ia=l(Go,`. It is used to instantiate
an OWL-ViT image encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OWL-ViT
`),zt=n(Go,"A",{href:!0,rel:!0});var Rl=a(zt);la=l(Rl,"google/owlvit-base-patch32"),Rl.forEach(o),ca=l(Go," architecture."),Go.forEach(o),da=f(ot),ne=n(ot,"P",{});var Xo=a(ne);pa=l(Xo,"Configuration objects inherit from "),ko=n(Xo,"A",{href:!0});var Hl=a(ko);ma=l(Hl,"PretrainedConfig"),Hl.forEach(o),ha=l(Xo,` and can be used to control the model outputs. Read the
documentation from `),zo=n(Xo,"A",{href:!0});var Ul=a(zo);fa=l(Ul,"PretrainedConfig"),Ul.forEach(o),ga=l(Xo," for more information."),Xo.forEach(o),ua=f(ot),b(Ee.$$.fragment,ot),ot.forEach(o),ir=f(t),ae=n(t,"H2",{class:!0});var yr=a(ae);Me=n(yr,"A",{id:!0,class:!0,href:!0});var Gl=a(Me);hs=n(Gl,"SPAN",{});var Xl=a(hs);b(Pt.$$.fragment,Xl),Xl.forEach(o),Gl.forEach(o),_a=f(yr),fs=n(yr,"SPAN",{});var Zl=a(fs);wa=l(Zl,"OwlViTFeatureExtractor"),Zl.forEach(o),yr.forEach(o),lr=f(t),L=n(t,"DIV",{class:!0});var st=a(L);b(Et.$$.fragment,st),Ta=f(st),gs=n(st,"P",{});var Jl=a(gs);va=l(Jl,"Constructs an OWL-ViT feature extractor."),Jl.forEach(o),ba=f(st),Mt=n(st,"P",{});var jr=a(Mt);$a=l(jr,"This feature extractor inherits from "),Po=n(jr,"A",{href:!0});var Kl=a(Po);Oa=l(Kl,"FeatureExtractionMixin"),Kl.forEach(o),xa=l(jr,` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),jr.forEach(o),Va=f(st),X=n(st,"DIV",{class:!0});var Zo=a(X);b(Ct.$$.fragment,Zo),ya=f(Zo),us=n(Zo,"P",{});var Yl=a(us);ja=l(Yl,"Main method to prepare for the model one or several image(s)."),Yl.forEach(o),ka=f(Zo),b(Ce.$$.fragment,Zo),Zo.forEach(o),st.forEach(o),cr=f(t),ie=n(t,"H2",{class:!0});var kr=a(ie);qe=n(kr,"A",{id:!0,class:!0,href:!0});var Ql=a(qe);_s=n(Ql,"SPAN",{});var ec=a(_s);b(qt.$$.fragment,ec),ec.forEach(o),Ql.forEach(o),za=f(kr),ws=n(kr,"SPAN",{});var tc=a(ws);Pa=l(tc,"OwlViTProcessor"),tc.forEach(o),kr.forEach(o),dr=f(t),P=n(t,"DIV",{class:!0});var Z=a(P);b(It.$$.fragment,Z),Ea=f(Z),E=n(Z,"P",{});var R=a(E);Ma=l(R,"Constructs an OWL-ViT processor which wraps "),Eo=n(R,"A",{href:!0});var oc=a(Eo);Ca=l(oc,"OwlViTFeatureExtractor"),oc.forEach(o),qa=l(R," and "),Mo=n(R,"A",{href:!0});var sc=a(Mo);Ia=l(sc,"CLIPTokenizer"),sc.forEach(o),La=l(R,"/"),Co=n(R,"A",{href:!0});var rc=a(Co);Fa=l(rc,"CLIPTokenizerFast"),rc.forEach(o),Aa=l(R,`
into a single processor that interits both the feature extractor and tokenizer functionalities. See the
`),Ts=n(R,"CODE",{});var nc=a(Ts);Da=l(nc,"__call__()"),nc.forEach(o),Wa=l(R," and "),qo=n(R,"A",{href:!0});var ac=a(qo);Na=l(ac,"decode()"),ac.forEach(o),Sa=l(R," for more information."),R.forEach(o),Ba=f(Z),Ie=n(Z,"DIV",{class:!0});var zr=a(Ie);b(Lt.$$.fragment,zr),Ra=f(zr),Ft=n(zr,"P",{});var Pr=a(Ft);Ha=l(Pr,"This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Io=n(Pr,"A",{href:!0});var ic=a(Io);Ua=l(ic,"batch_decode()"),ic.forEach(o),Ga=l(Pr,`. Please
refer to the docstring of this method for more information.`),Pr.forEach(o),zr.forEach(o),Xa=f(Z),Le=n(Z,"DIV",{class:!0});var Er=a(Le);b(At.$$.fragment,Er),Za=f(Er),Dt=n(Er,"P",{});var Mr=a(Dt);Ja=l(Mr,"This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Lo=n(Mr,"A",{href:!0});var lc=a(Lo);Ka=l(lc,"decode()"),lc.forEach(o),Ya=l(Mr,`. Please refer to
the docstring of this method for more information.`),Mr.forEach(o),Er.forEach(o),Qa=f(Z),Fe=n(Z,"DIV",{class:!0});var Cr=a(Fe);b(Wt.$$.fragment,Cr),ei=f(Cr),Nt=n(Cr,"P",{});var qr=a(Nt);ti=l(qr,"This method forwards all its arguments to "),vs=n(qr,"CODE",{});var cc=a(vs);oi=l(cc,"OwlViTFeatureExtractor.post_process()"),cc.forEach(o),si=l(qr,`. Please refer to the
docstring of this method for more information.`),qr.forEach(o),Cr.forEach(o),Z.forEach(o),pr=f(t),le=n(t,"H2",{class:!0});var Ir=a(le);Ae=n(Ir,"A",{id:!0,class:!0,href:!0});var dc=a(Ae);bs=n(dc,"SPAN",{});var pc=a(bs);b(St.$$.fragment,pc),pc.forEach(o),dc.forEach(o),ri=f(Ir),$s=n(Ir,"SPAN",{});var mc=a($s);ni=l(mc,"OwlViTModel"),mc.forEach(o),Ir.forEach(o),mr=f(t),F=n(t,"DIV",{class:!0});var rt=a(F);b(Bt.$$.fragment,rt),ai=f(rt),A=n(rt,"DIV",{class:!0});var nt=a(A);b(Rt.$$.fragment,nt),ii=f(nt),ce=n(nt,"P",{});var Jo=a(ce);li=l(Jo,"The "),Fo=n(Jo,"A",{href:!0});var hc=a(Fo);ci=l(hc,"OwlViTModel"),hc.forEach(o),di=l(Jo," forward method, overrides the "),Os=n(Jo,"CODE",{});var fc=a(Os);pi=l(fc,"__call__"),fc.forEach(o),mi=l(Jo," special method."),Jo.forEach(o),hi=f(nt),b(De.$$.fragment,nt),fi=f(nt),b(We.$$.fragment,nt),nt.forEach(o),gi=f(rt),D=n(rt,"DIV",{class:!0});var at=a(D);b(Ht.$$.fragment,at),ui=f(at),de=n(at,"P",{});var Ko=a(de);_i=l(Ko,"The "),Ao=n(Ko,"A",{href:!0});var gc=a(Ao);wi=l(gc,"OwlViTModel"),gc.forEach(o),Ti=l(Ko," forward method, overrides the "),xs=n(Ko,"CODE",{});var uc=a(xs);vi=l(uc,"__call__"),uc.forEach(o),bi=l(Ko," special method."),Ko.forEach(o),$i=f(at),b(Ne.$$.fragment,at),Oi=f(at),b(Se.$$.fragment,at),at.forEach(o),xi=f(rt),W=n(rt,"DIV",{class:!0});var it=a(W);b(Ut.$$.fragment,it),Vi=f(it),pe=n(it,"P",{});var Yo=a(pe);yi=l(Yo,"The "),Do=n(Yo,"A",{href:!0});var _c=a(Do);ji=l(_c,"OwlViTModel"),_c.forEach(o),ki=l(Yo," forward method, overrides the "),Vs=n(Yo,"CODE",{});var wc=a(Vs);zi=l(wc,"__call__"),wc.forEach(o),Pi=l(Yo," special method."),Yo.forEach(o),Ei=f(it),b(Be.$$.fragment,it),Mi=f(it),b(Re.$$.fragment,it),it.forEach(o),rt.forEach(o),hr=f(t),me=n(t,"H2",{class:!0});var Lr=a(me);He=n(Lr,"A",{id:!0,class:!0,href:!0});var Tc=a(He);ys=n(Tc,"SPAN",{});var vc=a(ys);b(Gt.$$.fragment,vc),vc.forEach(o),Tc.forEach(o),Ci=f(Lr),js=n(Lr,"SPAN",{});var bc=a(js);qi=l(bc,"OwlViTTextModel"),bc.forEach(o),Lr.forEach(o),fr=f(t),he=n(t,"DIV",{class:!0});var Fr=a(he);b(Xt.$$.fragment,Fr),Ii=f(Fr),N=n(Fr,"DIV",{class:!0});var lt=a(N);b(Zt.$$.fragment,lt),Li=f(lt),fe=n(lt,"P",{});var Qo=a(fe);Fi=l(Qo,"The "),Wo=n(Qo,"A",{href:!0});var $c=a(Wo);Ai=l($c,"OwlViTTextModel"),$c.forEach(o),Di=l(Qo," forward method, overrides the "),ks=n(Qo,"CODE",{});var Oc=a(ks);Wi=l(Oc,"__call__"),Oc.forEach(o),Ni=l(Qo," special method."),Qo.forEach(o),Si=f(lt),b(Ue.$$.fragment,lt),Bi=f(lt),b(Ge.$$.fragment,lt),lt.forEach(o),Fr.forEach(o),gr=f(t),ge=n(t,"H2",{class:!0});var Ar=a(ge);Xe=n(Ar,"A",{id:!0,class:!0,href:!0});var xc=a(Xe);zs=n(xc,"SPAN",{});var Vc=a(zs);b(Jt.$$.fragment,Vc),Vc.forEach(o),xc.forEach(o),Ri=f(Ar),Ps=n(Ar,"SPAN",{});var yc=a(Ps);Hi=l(yc,"OwlViTVisionModel"),yc.forEach(o),Ar.forEach(o),ur=f(t),ue=n(t,"DIV",{class:!0});var Dr=a(ue);b(Kt.$$.fragment,Dr),Ui=f(Dr),S=n(Dr,"DIV",{class:!0});var ct=a(S);b(Yt.$$.fragment,ct),Gi=f(ct),_e=n(ct,"P",{});var es=a(_e);Xi=l(es,"The "),No=n(es,"A",{href:!0});var jc=a(No);Zi=l(jc,"OwlViTVisionModel"),jc.forEach(o),Ji=l(es," forward method, overrides the "),Es=n(es,"CODE",{});var kc=a(Es);Ki=l(kc,"__call__"),kc.forEach(o),Yi=l(es," special method."),es.forEach(o),Qi=f(ct),b(Ze.$$.fragment,ct),el=f(ct),b(Je.$$.fragment,ct),ct.forEach(o),Dr.forEach(o),_r=f(t),we=n(t,"H2",{class:!0});var Wr=a(we);Ke=n(Wr,"A",{id:!0,class:!0,href:!0});var zc=a(Ke);Ms=n(zc,"SPAN",{});var Pc=a(Ms);b(Qt.$$.fragment,Pc),Pc.forEach(o),zc.forEach(o),tl=f(Wr),Cs=n(Wr,"SPAN",{});var Ec=a(Cs);ol=l(Ec,"OwlViTForObjectDetection"),Ec.forEach(o),Wr.forEach(o),wr=f(t),Te=n(t,"DIV",{class:!0});var Nr=a(Te);b(eo.$$.fragment,Nr),sl=f(Nr),B=n(Nr,"DIV",{class:!0});var dt=a(B);b(to.$$.fragment,dt),rl=f(dt),ve=n(dt,"P",{});var ts=a(ve);nl=l(ts,"The "),So=n(ts,"A",{href:!0});var Mc=a(So);al=l(Mc,"OwlViTForObjectDetection"),Mc.forEach(o),il=l(ts," forward method, overrides the "),qs=n(ts,"CODE",{});var Cc=a(qs);ll=l(Cc,"__call__"),Cc.forEach(o),cl=l(ts," special method."),ts.forEach(o),dl=f(dt),b(Ye.$$.fragment,dt),pl=f(dt),b(Qe.$$.fragment,dt),dt.forEach(o),Nr.forEach(o),this.h()},h(){d(c,"name","hf:doc:metadata"),d(c,"content",JSON.stringify(td)),d(p,"id","owlvit"),d(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(p,"href","#owlvit"),d(g,"class","relative group"),d($e,"id","overview"),d($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d($e,"href","#overview"),d(J,"class","relative group"),d(ft,"href","https://arxiv.org/abs/2205.06230"),d(ft,"rel","nofollow"),d(xe,"id","usage"),d(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(xe,"href","#usage"),d(K,"class","relative group"),d(po,"href","clip"),d(mo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),d(ho,"href","/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer"),d(fo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTProcessor"),d(go,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),d(uo,"href","/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer"),d(_o,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTProcessor"),d(wo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTForObjectDetection"),d(_t,"href","https://huggingface.co/adirik"),d(_t,"rel","nofollow"),d(wt,"href","https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit"),d(wt,"rel","nofollow"),d(ye,"id","transformers.OwlViTConfig"),d(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ye,"href","#transformers.OwlViTConfig"),d(Y,"class","relative group"),d(To,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTConfig"),d(vo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTModel"),d(bt,"href","https://huggingface.co/google/owlvit-base-patch32"),d(bt,"rel","nofollow"),d(bo,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d($o,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(Oo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTConfig"),d(je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ke,"id","transformers.OwlViTTextConfig"),d(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ke,"href","#transformers.OwlViTTextConfig"),d(ee,"class","relative group"),d(xo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"),d(yt,"href","https://huggingface.co/google/owlvit-base-patch32"),d(yt,"rel","nofollow"),d(Vo,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(yo,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Pe,"id","transformers.OwlViTVisionConfig"),d(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Pe,"href","#transformers.OwlViTVisionConfig"),d(se,"class","relative group"),d(jo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"),d(zt,"href","https://huggingface.co/google/owlvit-base-patch32"),d(zt,"rel","nofollow"),d(ko,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(zo,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Me,"id","transformers.OwlViTFeatureExtractor"),d(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Me,"href","#transformers.OwlViTFeatureExtractor"),d(ae,"class","relative group"),d(Po,"href","/docs/transformers/main/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),d(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(qe,"id","transformers.OwlViTProcessor"),d(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(qe,"href","#transformers.OwlViTProcessor"),d(ie,"class","relative group"),d(Eo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),d(Mo,"href","/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer"),d(Co,"href","/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizerFast"),d(qo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTProcessor.decode"),d(Io,"href","/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode"),d(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Lo,"href","/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode"),d(Le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ae,"id","transformers.OwlViTModel"),d(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ae,"href","#transformers.OwlViTModel"),d(le,"class","relative group"),d(Fo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTModel"),d(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ao,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTModel"),d(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Do,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTModel"),d(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(He,"id","transformers.OwlViTTextModel"),d(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(He,"href","#transformers.OwlViTTextModel"),d(me,"class","relative group"),d(Wo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"),d(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Xe,"id","transformers.OwlViTVisionModel"),d(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Xe,"href","#transformers.OwlViTVisionModel"),d(ge,"class","relative group"),d(No,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"),d(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ke,"id","transformers.OwlViTForObjectDetection"),d(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ke,"href","#transformers.OwlViTForObjectDetection"),d(we,"class","relative group"),d(So,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTForObjectDetection"),d(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,u){e(document.head,c),_(t,T,u),_(t,g,u),e(g,p),e(p,w),$(s,w,null),e(g,m),e(g,j),e(j,Sr),_(t,Us,u),_(t,J,u),e(J,$e),e($e,os),$(ht,os,null),e(J,Br),e(J,ss),e(ss,Rr),_(t,Gs,u),_(t,Oe,u),e(Oe,Hr),e(Oe,ft),e(ft,Ur),e(Oe,Gr),_(t,Xs,u),_(t,lo,u),e(lo,Xr),_(t,Zs,u),_(t,co,u),e(co,rs),e(rs,Zr),_(t,Js,u),_(t,K,u),e(K,xe),e(xe,ns),$(gt,ns,null),e(K,Jr),e(K,as),e(as,Kr),_(t,Ks,u),_(t,Ve,u),e(Ve,Yr),e(Ve,po),e(po,Qr),e(Ve,en),_(t,Ys,u),_(t,z,u),e(z,mo),e(mo,tn),e(z,on),e(z,ho),e(ho,sn),e(z,rn),e(z,fo),e(fo,nn),e(z,an),e(z,go),e(go,ln),e(z,cn),e(z,uo),e(uo,dn),e(z,pn),e(z,_o),e(_o,mn),e(z,hn),e(z,wo),e(wo,fn),e(z,gn),_(t,Qs,u),$(ut,t,u),_(t,er,u),_(t,U,u),e(U,un),e(U,_t),e(_t,_n),e(U,wn),e(U,wt),e(wt,Tn),e(U,vn),_(t,tr,u),_(t,Y,u),e(Y,ye),e(ye,is),$(Tt,is,null),e(Y,bn),e(Y,ls),e(ls,$n),_(t,or,u),_(t,C,u),$(vt,C,null),e(C,On),e(C,G),e(G,To),e(To,xn),e(G,Vn),e(G,vo),e(vo,yn),e(G,jn),e(G,bt),e(bt,kn),e(G,zn),e(C,Pn),e(C,Q),e(Q,En),e(Q,bo),e(bo,Mn),e(Q,Cn),e(Q,$o),e($o,qn),e(Q,In),e(C,Ln),e(C,je),$($t,je,null),e(je,Fn),e(je,Ot),e(Ot,An),e(Ot,Oo),e(Oo,Dn),e(Ot,Wn),_(t,sr,u),_(t,ee,u),e(ee,ke),e(ke,cs),$(xt,cs,null),e(ee,Nn),e(ee,ds),e(ds,Sn),_(t,rr,u),_(t,q,u),$(Vt,q,null),e(q,Bn),e(q,te),e(te,Rn),e(te,xo),e(xo,Hn),e(te,Un),e(te,yt),e(yt,Gn),e(te,Xn),e(q,Zn),e(q,oe),e(oe,Jn),e(oe,Vo),e(Vo,Kn),e(oe,Yn),e(oe,yo),e(yo,Qn),e(oe,ea),e(q,ta),$(ze,q,null),_(t,nr,u),_(t,se,u),e(se,Pe),e(Pe,ps),$(jt,ps,null),e(se,oa),e(se,ms),e(ms,sa),_(t,ar,u),_(t,I,u),$(kt,I,null),e(I,ra),e(I,re),e(re,na),e(re,jo),e(jo,aa),e(re,ia),e(re,zt),e(zt,la),e(re,ca),e(I,da),e(I,ne),e(ne,pa),e(ne,ko),e(ko,ma),e(ne,ha),e(ne,zo),e(zo,fa),e(ne,ga),e(I,ua),$(Ee,I,null),_(t,ir,u),_(t,ae,u),e(ae,Me),e(Me,hs),$(Pt,hs,null),e(ae,_a),e(ae,fs),e(fs,wa),_(t,lr,u),_(t,L,u),$(Et,L,null),e(L,Ta),e(L,gs),e(gs,va),e(L,ba),e(L,Mt),e(Mt,$a),e(Mt,Po),e(Po,Oa),e(Mt,xa),e(L,Va),e(L,X),$(Ct,X,null),e(X,ya),e(X,us),e(us,ja),e(X,ka),$(Ce,X,null),_(t,cr,u),_(t,ie,u),e(ie,qe),e(qe,_s),$(qt,_s,null),e(ie,za),e(ie,ws),e(ws,Pa),_(t,dr,u),_(t,P,u),$(It,P,null),e(P,Ea),e(P,E),e(E,Ma),e(E,Eo),e(Eo,Ca),e(E,qa),e(E,Mo),e(Mo,Ia),e(E,La),e(E,Co),e(Co,Fa),e(E,Aa),e(E,Ts),e(Ts,Da),e(E,Wa),e(E,qo),e(qo,Na),e(E,Sa),e(P,Ba),e(P,Ie),$(Lt,Ie,null),e(Ie,Ra),e(Ie,Ft),e(Ft,Ha),e(Ft,Io),e(Io,Ua),e(Ft,Ga),e(P,Xa),e(P,Le),$(At,Le,null),e(Le,Za),e(Le,Dt),e(Dt,Ja),e(Dt,Lo),e(Lo,Ka),e(Dt,Ya),e(P,Qa),e(P,Fe),$(Wt,Fe,null),e(Fe,ei),e(Fe,Nt),e(Nt,ti),e(Nt,vs),e(vs,oi),e(Nt,si),_(t,pr,u),_(t,le,u),e(le,Ae),e(Ae,bs),$(St,bs,null),e(le,ri),e(le,$s),e($s,ni),_(t,mr,u),_(t,F,u),$(Bt,F,null),e(F,ai),e(F,A),$(Rt,A,null),e(A,ii),e(A,ce),e(ce,li),e(ce,Fo),e(Fo,ci),e(ce,di),e(ce,Os),e(Os,pi),e(ce,mi),e(A,hi),$(De,A,null),e(A,fi),$(We,A,null),e(F,gi),e(F,D),$(Ht,D,null),e(D,ui),e(D,de),e(de,_i),e(de,Ao),e(Ao,wi),e(de,Ti),e(de,xs),e(xs,vi),e(de,bi),e(D,$i),$(Ne,D,null),e(D,Oi),$(Se,D,null),e(F,xi),e(F,W),$(Ut,W,null),e(W,Vi),e(W,pe),e(pe,yi),e(pe,Do),e(Do,ji),e(pe,ki),e(pe,Vs),e(Vs,zi),e(pe,Pi),e(W,Ei),$(Be,W,null),e(W,Mi),$(Re,W,null),_(t,hr,u),_(t,me,u),e(me,He),e(He,ys),$(Gt,ys,null),e(me,Ci),e(me,js),e(js,qi),_(t,fr,u),_(t,he,u),$(Xt,he,null),e(he,Ii),e(he,N),$(Zt,N,null),e(N,Li),e(N,fe),e(fe,Fi),e(fe,Wo),e(Wo,Ai),e(fe,Di),e(fe,ks),e(ks,Wi),e(fe,Ni),e(N,Si),$(Ue,N,null),e(N,Bi),$(Ge,N,null),_(t,gr,u),_(t,ge,u),e(ge,Xe),e(Xe,zs),$(Jt,zs,null),e(ge,Ri),e(ge,Ps),e(Ps,Hi),_(t,ur,u),_(t,ue,u),$(Kt,ue,null),e(ue,Ui),e(ue,S),$(Yt,S,null),e(S,Gi),e(S,_e),e(_e,Xi),e(_e,No),e(No,Zi),e(_e,Ji),e(_e,Es),e(Es,Ki),e(_e,Yi),e(S,Qi),$(Ze,S,null),e(S,el),$(Je,S,null),_(t,_r,u),_(t,we,u),e(we,Ke),e(Ke,Ms),$(Qt,Ms,null),e(we,tl),e(we,Cs),e(Cs,ol),_(t,wr,u),_(t,Te,u),$(eo,Te,null),e(Te,sl),e(Te,B),$(to,B,null),e(B,rl),e(B,ve),e(ve,nl),e(ve,So),e(So,al),e(ve,il),e(ve,qs),e(qs,ll),e(ve,cl),e(B,dl),$(Ye,B,null),e(B,pl),$(Qe,B,null),Tr=!0},p(t,[u]){const oo={};u&2&&(oo.$$scope={dirty:u,ctx:t}),ze.$set(oo);const Is={};u&2&&(Is.$$scope={dirty:u,ctx:t}),Ee.$set(Is);const Ls={};u&2&&(Ls.$$scope={dirty:u,ctx:t}),Ce.$set(Ls);const Fs={};u&2&&(Fs.$$scope={dirty:u,ctx:t}),De.$set(Fs);const so={};u&2&&(so.$$scope={dirty:u,ctx:t}),We.$set(so);const As={};u&2&&(As.$$scope={dirty:u,ctx:t}),Ne.$set(As);const Ds={};u&2&&(Ds.$$scope={dirty:u,ctx:t}),Se.$set(Ds);const Ws={};u&2&&(Ws.$$scope={dirty:u,ctx:t}),Be.$set(Ws);const ro={};u&2&&(ro.$$scope={dirty:u,ctx:t}),Re.$set(ro);const Ns={};u&2&&(Ns.$$scope={dirty:u,ctx:t}),Ue.$set(Ns);const Ss={};u&2&&(Ss.$$scope={dirty:u,ctx:t}),Ge.$set(Ss);const Bs={};u&2&&(Bs.$$scope={dirty:u,ctx:t}),Ze.$set(Bs);const Rs={};u&2&&(Rs.$$scope={dirty:u,ctx:t}),Je.$set(Rs);const no={};u&2&&(no.$$scope={dirty:u,ctx:t}),Ye.$set(no);const Hs={};u&2&&(Hs.$$scope={dirty:u,ctx:t}),Qe.$set(Hs)},i(t){Tr||(O(s.$$.fragment,t),O(ht.$$.fragment,t),O(gt.$$.fragment,t),O(ut.$$.fragment,t),O(Tt.$$.fragment,t),O(vt.$$.fragment,t),O($t.$$.fragment,t),O(xt.$$.fragment,t),O(Vt.$$.fragment,t),O(ze.$$.fragment,t),O(jt.$$.fragment,t),O(kt.$$.fragment,t),O(Ee.$$.fragment,t),O(Pt.$$.fragment,t),O(Et.$$.fragment,t),O(Ct.$$.fragment,t),O(Ce.$$.fragment,t),O(qt.$$.fragment,t),O(It.$$.fragment,t),O(Lt.$$.fragment,t),O(At.$$.fragment,t),O(Wt.$$.fragment,t),O(St.$$.fragment,t),O(Bt.$$.fragment,t),O(Rt.$$.fragment,t),O(De.$$.fragment,t),O(We.$$.fragment,t),O(Ht.$$.fragment,t),O(Ne.$$.fragment,t),O(Se.$$.fragment,t),O(Ut.$$.fragment,t),O(Be.$$.fragment,t),O(Re.$$.fragment,t),O(Gt.$$.fragment,t),O(Xt.$$.fragment,t),O(Zt.$$.fragment,t),O(Ue.$$.fragment,t),O(Ge.$$.fragment,t),O(Jt.$$.fragment,t),O(Kt.$$.fragment,t),O(Yt.$$.fragment,t),O(Ze.$$.fragment,t),O(Je.$$.fragment,t),O(Qt.$$.fragment,t),O(eo.$$.fragment,t),O(to.$$.fragment,t),O(Ye.$$.fragment,t),O(Qe.$$.fragment,t),Tr=!0)},o(t){x(s.$$.fragment,t),x(ht.$$.fragment,t),x(gt.$$.fragment,t),x(ut.$$.fragment,t),x(Tt.$$.fragment,t),x(vt.$$.fragment,t),x($t.$$.fragment,t),x(xt.$$.fragment,t),x(Vt.$$.fragment,t),x(ze.$$.fragment,t),x(jt.$$.fragment,t),x(kt.$$.fragment,t),x(Ee.$$.fragment,t),x(Pt.$$.fragment,t),x(Et.$$.fragment,t),x(Ct.$$.fragment,t),x(Ce.$$.fragment,t),x(qt.$$.fragment,t),x(It.$$.fragment,t),x(Lt.$$.fragment,t),x(At.$$.fragment,t),x(Wt.$$.fragment,t),x(St.$$.fragment,t),x(Bt.$$.fragment,t),x(Rt.$$.fragment,t),x(De.$$.fragment,t),x(We.$$.fragment,t),x(Ht.$$.fragment,t),x(Ne.$$.fragment,t),x(Se.$$.fragment,t),x(Ut.$$.fragment,t),x(Be.$$.fragment,t),x(Re.$$.fragment,t),x(Gt.$$.fragment,t),x(Xt.$$.fragment,t),x(Zt.$$.fragment,t),x(Ue.$$.fragment,t),x(Ge.$$.fragment,t),x(Jt.$$.fragment,t),x(Kt.$$.fragment,t),x(Yt.$$.fragment,t),x(Ze.$$.fragment,t),x(Je.$$.fragment,t),x(Qt.$$.fragment,t),x(eo.$$.fragment,t),x(to.$$.fragment,t),x(Ye.$$.fragment,t),x(Qe.$$.fragment,t),Tr=!1},d(t){o(c),t&&o(T),t&&o(g),V(s),t&&o(Us),t&&o(J),V(ht),t&&o(Gs),t&&o(Oe),t&&o(Xs),t&&o(lo),t&&o(Zs),t&&o(co),t&&o(Js),t&&o(K),V(gt),t&&o(Ks),t&&o(Ve),t&&o(Ys),t&&o(z),t&&o(Qs),V(ut,t),t&&o(er),t&&o(U),t&&o(tr),t&&o(Y),V(Tt),t&&o(or),t&&o(C),V(vt),V($t),t&&o(sr),t&&o(ee),V(xt),t&&o(rr),t&&o(q),V(Vt),V(ze),t&&o(nr),t&&o(se),V(jt),t&&o(ar),t&&o(I),V(kt),V(Ee),t&&o(ir),t&&o(ae),V(Pt),t&&o(lr),t&&o(L),V(Et),V(Ct),V(Ce),t&&o(cr),t&&o(ie),V(qt),t&&o(dr),t&&o(P),V(It),V(Lt),V(At),V(Wt),t&&o(pr),t&&o(le),V(St),t&&o(mr),t&&o(F),V(Bt),V(Rt),V(De),V(We),V(Ht),V(Ne),V(Se),V(Ut),V(Be),V(Re),t&&o(hr),t&&o(me),V(Gt),t&&o(fr),t&&o(he),V(Xt),V(Zt),V(Ue),V(Ge),t&&o(gr),t&&o(ge),V(Jt),t&&o(ur),t&&o(ue),V(Kt),V(Yt),V(Ze),V(Je),t&&o(_r),t&&o(we),V(Qt),t&&o(wr),t&&o(Te),V(eo),V(to),V(Ye),V(Qe)}}}const td={local:"owlvit",sections:[{local:"overview",title:"Overview"},{local:"usage",title:"Usage"},{local:"transformers.OwlViTConfig",title:"OwlViTConfig"},{local:"transformers.OwlViTTextConfig",title:"OwlViTTextConfig"},{local:"transformers.OwlViTVisionConfig",title:"OwlViTVisionConfig"},{local:"transformers.OwlViTFeatureExtractor",title:"OwlViTFeatureExtractor"},{local:"transformers.OwlViTProcessor",title:"OwlViTProcessor"},{local:"transformers.OwlViTModel",title:"OwlViTModel"},{local:"transformers.OwlViTTextModel",title:"OwlViTTextModel"},{local:"transformers.OwlViTVisionModel",title:"OwlViTVisionModel"},{local:"transformers.OwlViTForObjectDetection",title:"OwlViTForObjectDetection"}],title:"OWL-ViT"};function od(y){return Ac(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class cd extends qc{constructor(c){super();Ic(this,c,od,ed,Lc,{})}}export{cd as default,td as metadata};
