import{S as qc,i as Ic,s as Lc,e as n,k as h,w as v,t as i,M as Fc,c as r,d as o,m as f,a,x as b,h as l,b as d,G as e,g as _,y as $,q as O,o as x,B as V,v as Ac,L as mt}from"../../chunks/vendor-hf-doc-builder.js";import{T as io}from"../../chunks/Tip-hf-doc-builder.js";import{D as k}from"../../chunks/Docstring-hf-doc-builder.js";import{C as be}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as H}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as pt}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Dc(y){let c,T,g,p,w;return p=new be({props:{code:`from transformers import OwlViTTextConfig, OwlViTTextModel

# Initializing a OwlViTTextModel with google/owlvit-base-patch32 style configuration
configuration = OwlViTTextConfig()

# Initializing a OwlViTTextConfig from the google/owlvit-base-patch32 style configuration
model = OwlViTTextModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTTextConfig, OwlViTTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTTextModel with google/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = OwlViTTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTTextConfig from the google/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){c=n("p"),T=i("Example:"),g=h(),v(p.$$.fragment)},l(s){c=r(s,"P",{});var m=a(c);T=l(m,"Example:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Wc(y){let c,T,g,p,w;return p=new be({props:{code:`from transformers import OwlViTVisionConfig, OwlViTVisionModel

# Initializing a OwlViTVisionModel with google/owlvit-base-patch32 style configuration
configuration = OwlViTVisionConfig()

# Initializing a OwlViTVisionModel model from the google/owlvit-base-patch32 style configuration
model = OwlViTVisionModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTVisionConfig, OwlViTVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTVisionModel with google/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = OwlViTVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTVisionModel model from the google/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){c=n("p"),T=i("Example:"),g=h(),v(p.$$.fragment)},l(s){c=r(s,"P",{});var m=a(c);T=l(m,"Example:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Nc(y){let c,T;return{c(){c=n("p"),T=i(`NumPy arrays and PyTorch tensors are converted to PIL images when resizing, so the most efficient is to pass
PIL images.`)},l(g){c=r(g,"P",{});var p=a(c);T=l(p,`NumPy arrays and PyTorch tensors are converted to PIL images when resizing, so the most efficient is to pass
PIL images.`),p.forEach(o)},m(g,p){_(g,c,p),e(c,T)},d(g){g&&o(c)}}}function Sc(y){let c,T,g,p,w;return{c(){c=n("p"),T=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=r(s,"P",{});var m=a(c);T=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,T),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Bc(y){let c,T,g,p,w;return p=new be({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(text=[["a photo of a cat", "a photo of a dog"]], images=image, return_tensors="pt")
outputs = model(**inputs)
logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`}}),{c(){c=n("p"),T=i("Examples:"),g=h(),v(p.$$.fragment)},l(s){c=r(s,"P",{});var m=a(c);T=l(m,"Examples:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Rc(y){let c,T,g,p,w;return{c(){c=n("p"),T=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=r(s,"P",{});var m=a(c);T=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,T),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Hc(y){let c,T,g,p,w;return p=new be({props:{code:`from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
inputs = processor(
    text=[["a photo of a cat", "a photo of a dog"], ["photo of a astranaut"]], return_tensors="pt"
)
text_features = model.get_text_features(**inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], [<span class="hljs-string">&quot;photo of a astranaut&quot;</span>]], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`}}),{c(){c=n("p"),T=i("Examples:"),g=h(),v(p.$$.fragment)},l(s){c=r(s,"P",{});var m=a(c);T=l(m,"Examples:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Uc(y){let c,T,g,p,w;return{c(){c=n("p"),T=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=r(s,"P",{});var m=a(c);T=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,T),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Gc(y){let c,T,g,p,w;return p=new be({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors="pt")
image_features = model.get_image_features(**inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`}}),{c(){c=n("p"),T=i("Examples:"),g=h(),v(p.$$.fragment)},l(s){c=r(s,"P",{});var m=a(c);T=l(m,"Examples:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Xc(y){let c,T,g,p,w;return{c(){c=n("p"),T=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=r(s,"P",{});var m=a(c);T=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,T),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Zc(y){let c,T,g,p,w;return p=new be({props:{code:`from transformers import OwlViTProcessor, OwlViTTextModel

model = OwlViTTextModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
inputs = processor(
    text=[["a photo of a cat", "a photo of a dog"], ["photo of a astranaut"]], return_tensors="pt"
)
outputs = model(**inputs)
last_hidden_state = outputs.last_hidden_state
pooled_output = outputs.pooler_output  # pooled (EOS token) states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTTextModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], [<span class="hljs-string">&quot;photo of a astranaut&quot;</span>]], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled (EOS token) states</span>`}}),{c(){c=n("p"),T=i("Examples:"),g=h(),v(p.$$.fragment)},l(s){c=r(s,"P",{});var m=a(c);T=l(m,"Examples:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Jc(y){let c,T,g,p,w;return{c(){c=n("p"),T=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=r(s,"P",{});var m=a(c);T=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,T),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Kc(y){let c,T,g,p,w;return p=new be({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTVisionModel

model = OwlViTVisionModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, return_tensors="pt")

outputs = model(**inputs)
last_hidden_state = outputs.last_hidden_state
pooled_output = outputs.pooler_output  # pooled CLS states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTVisionModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled CLS states</span>`}}),{c(){c=n("p"),T=i("Examples:"),g=h(),v(p.$$.fragment)},l(s){c=r(s,"P",{});var m=a(c);T=l(m,"Examples:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Yc(y){let c,T,g,p,w;return{c(){c=n("p"),T=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=r(s,"P",{});var m=a(c);T=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,T),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Qc(y){let c,T,g,p,w;return p=new be({props:{code:`import requests
from PIL import Image
import torch
from transformers import OwlViTProcessor, OwlViTForObjectDetection

processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch32")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
texts = [["a photo of a cat", "a photo of a dog"]]
inputs = processor(text=texts, images=image, return_tensors="pt")
outputs = model(**inputs)

# Target image sizes (height, width) to rescale box predictions [batch_size, 2]
target_sizes = torch.Tensor([image.size[::-1]])
# Convert outputs (bounding boxes and class logits) to COCO API
results = processor.post_process(outputs=outputs, target_sizes=target_sizes)

i = 0  # Retrieve predictions for the first image for the corresponding text queries
text = texts[i]
boxes, scores, labels = results[i]["boxes"], results[i]["scores"], results[i]["labels"]

score_threshold = 0.1
for box, score, label in zip(boxes, scores, labels):
    box = [round(i, 2) for i in box.tolist()]
    if score >= score_threshold:
        print(f"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTForObjectDetection.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>texts = [[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=texts, images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Target image sizes (height, width) to rescale box predictions [batch_size, 2]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_sizes = torch.Tensor([image.size[::-<span class="hljs-number">1</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Convert outputs (bounding boxes and class logits) to COCO API</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>results = processor.post_process(outputs=outputs, target_sizes=target_sizes)

<span class="hljs-meta">&gt;&gt;&gt; </span>i = <span class="hljs-number">0</span>  <span class="hljs-comment"># Retrieve predictions for the first image for the corresponding text queries</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = texts[i]
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes, scores, labels = results[i][<span class="hljs-string">&quot;boxes&quot;</span>], results[i][<span class="hljs-string">&quot;scores&quot;</span>], results[i][<span class="hljs-string">&quot;labels&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>score_threshold = <span class="hljs-number">0.1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> box, score, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(boxes, scores, labels):
<span class="hljs-meta">... </span>    box = [<span class="hljs-built_in">round</span>(i, <span class="hljs-number">2</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> box.tolist()]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">if</span> score &gt;= score_threshold:
<span class="hljs-meta">... </span>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Detected <span class="hljs-subst">{text[label]}</span> with confidence <span class="hljs-subst">{<span class="hljs-built_in">round</span>(score.item(), <span class="hljs-number">3</span>)}</span> at location <span class="hljs-subst">{box}</span>&quot;</span>)
Detected a photo of a cat <span class="hljs-keyword">with</span> confidence <span class="hljs-number">0.707</span> at location [<span class="hljs-number">324.97</span>, <span class="hljs-number">20.44</span>, <span class="hljs-number">640.58</span>, <span class="hljs-number">373.29</span>]
Detected a photo of a cat <span class="hljs-keyword">with</span> confidence <span class="hljs-number">0.717</span> at location [<span class="hljs-number">1.46</span>, <span class="hljs-number">55.26</span>, <span class="hljs-number">315.55</span>, <span class="hljs-number">472.17</span>]`}}),{c(){c=n("p"),T=i("Examples:"),g=h(),v(p.$$.fragment)},l(s){c=r(s,"P",{});var m=a(c);T=l(m,"Examples:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function ed(y){let c,T,g,p,w,s,m,j,Bn,Us,J,$e,os,ht,Rn,ss,Hn,Gs,Oe,Un,ft,Gn,Xn,Xs,lo,Zn,Zs,co,ns,Jn,Js,K,xe,rs,gt,Kn,as,Yn,Ks,Ve,Qn,po,er,tr,Ys,z,mo,or,sr,ho,nr,rr,fo,ar,ir,go,lr,cr,uo,dr,pr,_o,mr,hr,wo,fr,gr,Qs,ut,en,U,ur,_t,_r,wr,wt,Tr,vr,tn,Y,ye,is,Tt,br,ls,$r,on,C,vt,Or,G,To,xr,Vr,vo,yr,jr,bt,kr,zr,Pr,Q,Er,bo,Mr,Cr,$o,qr,Ir,Lr,je,$t,Fr,Ot,Ar,Oo,Dr,Wr,sn,ee,ke,cs,xt,Nr,ds,Sr,nn,q,Vt,Br,te,Rr,xo,Hr,Ur,yt,Gr,Xr,Zr,oe,Jr,Vo,Kr,Yr,yo,Qr,ea,ta,ze,rn,se,Pe,ps,jt,oa,ms,sa,an,I,kt,na,ne,ra,jo,aa,ia,zt,la,ca,da,re,pa,ko,ma,ha,zo,fa,ga,ua,Ee,ln,ae,Me,hs,Pt,_a,fs,wa,cn,L,Et,Ta,gs,va,ba,Mt,$a,Po,Oa,xa,Va,X,Ct,ya,us,ja,ka,Ce,dn,ie,qe,_s,qt,za,ws,Pa,pn,P,It,Ea,E,Ma,Eo,Ca,qa,Mo,Ia,La,Co,Fa,Aa,Ts,Da,Wa,qo,Na,Sa,Ba,Ie,Lt,Ra,Ft,Ha,Io,Ua,Ga,Xa,Le,At,Za,Dt,Ja,Lo,Ka,Ya,Qa,Fe,Wt,ei,Nt,ti,vs,oi,si,mn,le,Ae,bs,St,ni,$s,ri,hn,F,Bt,ai,A,Rt,ii,ce,li,Fo,ci,di,Os,pi,mi,hi,De,fi,We,gi,D,Ht,ui,de,_i,Ao,wi,Ti,xs,vi,bi,$i,Ne,Oi,Se,xi,W,Ut,Vi,pe,yi,Do,ji,ki,Vs,zi,Pi,Ei,Be,Mi,Re,fn,me,He,ys,Gt,Ci,js,qi,gn,he,Xt,Ii,N,Zt,Li,fe,Fi,Wo,Ai,Di,ks,Wi,Ni,Si,Ue,Bi,Ge,un,ge,Xe,zs,Jt,Ri,Ps,Hi,_n,ue,Kt,Ui,S,Yt,Gi,_e,Xi,No,Zi,Ji,Es,Ki,Yi,Qi,Ze,el,Je,wn,we,Ke,Ms,Qt,tl,Cs,ol,Tn,Te,eo,sl,B,to,nl,ve,rl,So,al,il,qs,ll,cl,dl,Ye,pl,Qe,vn;return s=new H({}),ht=new H({}),gt=new H({}),ut=new be({props:{code:`import requests
from PIL import Image
import torch

from transformers import OwlViTProcessor, OwlViTForObjectDetection

processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch32")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
texts = [["a photo of a cat", "a photo of a dog"]]
inputs = processor(text=texts, images=image, return_tensors="pt")
outputs = model(**inputs)

# Target image sizes (height, width) to rescale box predictions [batch_size, 2]
target_sizes = torch.Tensor([image.size[::-1]])
# Convert outputs (bounding boxes and class logits) to COCO API
results = processor.post_process(outputs=outputs, target_sizes=target_sizes)

i = 0  # Retrieve predictions for the first image for the corresponding text queries
text = texts[i]
boxes, scores, labels = results[i]["boxes"], results[i]["scores"], results[i]["labels"]

score_threshold = 0.1
for box, score, label in zip(boxes, scores, labels):
    box = [round(i, 2) for i in box.tolist()]
    if score >= score_threshold:
        print(f"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTForObjectDetection.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>texts = [[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=texts, images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Target image sizes (height, width) to rescale box predictions [batch_size, 2]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_sizes = torch.Tensor([image.size[::-<span class="hljs-number">1</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Convert outputs (bounding boxes and class logits) to COCO API</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>results = processor.post_process(outputs=outputs, target_sizes=target_sizes)

<span class="hljs-meta">&gt;&gt;&gt; </span>i = <span class="hljs-number">0</span>  <span class="hljs-comment"># Retrieve predictions for the first image for the corresponding text queries</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = texts[i]
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes, scores, labels = results[i][<span class="hljs-string">&quot;boxes&quot;</span>], results[i][<span class="hljs-string">&quot;scores&quot;</span>], results[i][<span class="hljs-string">&quot;labels&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>score_threshold = <span class="hljs-number">0.1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> box, score, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(boxes, scores, labels):
<span class="hljs-meta">... </span>    box = [<span class="hljs-built_in">round</span>(i, <span class="hljs-number">2</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> box.tolist()]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">if</span> score &gt;= score_threshold:
<span class="hljs-meta">... </span>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Detected <span class="hljs-subst">{text[label]}</span> with confidence <span class="hljs-subst">{<span class="hljs-built_in">round</span>(score.item(), <span class="hljs-number">3</span>)}</span> at location <span class="hljs-subst">{box}</span>&quot;</span>)
Detected a photo of a cat <span class="hljs-keyword">with</span> confidence <span class="hljs-number">0.707</span> at location [<span class="hljs-number">324.97</span>, <span class="hljs-number">20.44</span>, <span class="hljs-number">640.58</span>, <span class="hljs-number">373.29</span>]
Detected a photo of a cat <span class="hljs-keyword">with</span> confidence <span class="hljs-number">0.717</span> at location [<span class="hljs-number">1.46</span>, <span class="hljs-number">55.26</span>, <span class="hljs-number">315.55</span>, <span class="hljs-number">472.17</span>]`}}),Tt=new H({}),vt=new k({props:{name:"class transformers.OwlViTConfig",anchor:"transformers.OwlViTConfig",parameters:[{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"projection_dim",val:" = 512"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"return_dict",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTConfig.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextConfig">OwlViTTextConfig</a>.`,name:"text_config"},{anchor:"transformers.OwlViTConfig.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionConfig">OwlViTVisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.OwlViTConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.OwlViTConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> parameter. Default is used as per the original OWL-ViT
implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.OwlViTConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/configuration_owlvit.py#L256"}}),$t=new k({props:{name:"from_text_vision_configs",anchor:"transformers.OwlViTConfig.from_text_vision_configs",parameters:[{name:"text_config",val:": typing.Dict"},{name:"vision_config",val:": typing.Dict"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/configuration_owlvit.py#L322",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTConfig"
>OwlViTConfig</a></p>
`}}),xt=new H({}),Vt=new k({props:{name:"class transformers.OwlViTTextConfig",anchor:"transformers.OwlViTTextConfig",parameters:[{name:"vocab_size",val:" = 49408"},{name:"hidden_size",val:" = 512"},{name:"intermediate_size",val:" = 2048"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 8"},{name:"max_position_embeddings",val:" = 16"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 49406"},{name:"eos_token_id",val:" = 49407"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 49408) &#x2014;
Vocabulary size of the OWL-ViT text model. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel">OwlViTTextModel</a>.`,name:"vocab_size"},{anchor:"transformers.OwlViTTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.OwlViTTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.OwlViTTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.OwlViTTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.OwlViTTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.OwlViTTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported. layer_norm_eps (<code>float</code>, <em>optional</em>,
defaults to 1e-5): The epsilon used by the layer normalization layers.`,name:"hidden_act"},{anchor:"transformers.OwlViTTextConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.OwlViTTextConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.OwlViTTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.OwlViTTextConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/configuration_owlvit.py#L41"}}),ze=new pt({props:{anchor:"transformers.OwlViTTextConfig.example",$$slots:{default:[Dc]},$$scope:{ctx:y}}}),jt=new H({}),kt=new k({props:{name:"class transformers.OwlViTVisionConfig",anchor:"transformers.OwlViTVisionConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"intermediate_size",val:" = 3072"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"num_channels",val:" = 3"},{name:"image_size",val:" = 768"},{name:"patch_size",val:" = 32"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.OwlViTVisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.OwlViTVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.OwlViTVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.OwlViTVisionConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Number of channels in the input images.`,name:"num_channels"},{anchor:"transformers.OwlViTVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.OwlViTVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.OwlViTVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported. layer_norm_eps (<code>float</code>, <em>optional</em>,
defaults to 1e-5): The epsilon used by the layer normalization layers.`,name:"hidden_act"},{anchor:"transformers.OwlViTVisionConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.OwlViTVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.OwlViTVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.OwlViTVisionConfig.initializer_factor",description:`<strong>initializer_factor</strong> (\`float&#x201C;, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/configuration_owlvit.py#L149"}}),Ee=new pt({props:{anchor:"transformers.OwlViTVisionConfig.example",$$slots:{default:[Wc]},$$scope:{ctx:y}}}),Pt=new H({}),Et=new k({props:{name:"class transformers.OwlViTFeatureExtractor",anchor:"transformers.OwlViTFeatureExtractor",parameters:[{name:"do_resize",val:" = True"},{name:"size",val:" = (768, 768)"},{name:"resample",val:" = <Resampling.BICUBIC: 3>"},{name:"crop_size",val:" = 768"},{name:"do_center_crop",val:" = False"},{name:"do_normalize",val:" = True"},{name:"image_mean",val:" = None"},{name:"image_std",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTFeatureExtractor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the shorter edge of the input to a certain <code>size</code>.`,name:"do_resize"},{anchor:"transformers.OwlViTFeatureExtractor.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple[int, int]</code>, <em>optional</em>, defaults to (768, 768)) &#x2014;
The size to use for resizing the image. Only has an effect if <code>do_resize</code> is set to <code>True</code>. If <code>size</code> is a
sequence like (h, w), output size will be matched to this. If <code>size</code> is an int, then image will be resized
to (size, size).`,name:"size"},{anchor:"transformers.OwlViTFeatureExtractor.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PILImageResampling.BICUBIC</code>) &#x2014;
An optional resampling filter. This can be one of <code>PILImageResampling.NEAREST</code>, <code>PILImageResampling.BOX</code>,
<code>PILImageResampling.BILINEAR</code>, <code>PILImageResampling.HAMMING</code>, <code>PILImageResampling.BICUBIC</code> or
<code>PILImageResampling.LANCZOS</code>. Only has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.OwlViTFeatureExtractor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to crop the input at the center. If the input size is smaller than <code>crop_size</code> along any edge, the
image is padded with 0&#x2019;s and then center cropped.`,name:"do_center_crop"},{anchor:"transformers.OwlViTFeatureExtractor.crop_size",description:"<strong>crop_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;",name:"crop_size"},{anchor:"transformers.OwlViTFeatureExtractor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input with <code>image_mean</code> and <code>image_std</code>. Desired output size when applying
center-cropping. Only has an effect if <code>do_center_crop</code> is set to <code>True</code>.`,name:"do_normalize"},{anchor:"transformers.OwlViTFeatureExtractor.image_mean",description:`<strong>image_mean</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[0.48145466, 0.4578275, 0.40821073]</code>) &#x2014;
The sequence of means for each channel, to be used when normalizing images.`,name:"image_mean"},{anchor:"transformers.OwlViTFeatureExtractor.image_std",description:`<strong>image_std</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[0.26862954, 0.26130258, 0.27577711]</code>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images.`,name:"image_std"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/feature_extraction_owlvit.py#L45"}}),Ct=new k({props:{name:"__call__",anchor:"transformers.OwlViTFeatureExtractor.__call__",parameters:[{name:"images",val:": typing.Union[PIL.Image.Image, numpy.ndarray, ForwardRef('torch.Tensor'), typing.List[PIL.Image.Image], typing.List[numpy.ndarray], typing.List[ForwardRef('torch.Tensor')]]"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTFeatureExtractor.__call__.images",description:`<strong>images</strong> (<code>PIL.Image.Image</code>, <code>np.ndarray</code>, <code>torch.Tensor</code>, <code>List[PIL.Image.Image]</code>, <code>List[np.ndarray]</code>, <code>List[torch.Tensor]</code>) &#x2014;
The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch
tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W) or (H, W, C),
where C is a number of channels, H and W are image height and width.`,name:"images"},{anchor:"transformers.OwlViTFeatureExtractor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>, defaults to <code>&apos;np&apos;</code>) &#x2014;
If set, will return tensors of a particular framework. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return NumPy <code>np.ndarray</code> objects.</li>
<li><code>&apos;jax&apos;</code>: Return JAX <code>jnp.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/feature_extraction_owlvit.py#L145",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a> with the following fields:</p>
<ul>
<li><strong>pixel_values</strong> \u2014 Pixel values to be fed to a model.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),Ce=new io({props:{warning:!0,$$slots:{default:[Nc]},$$scope:{ctx:y}}}),qt=new H({}),It=new k({props:{name:"class transformers.OwlViTProcessor",anchor:"transformers.OwlViTProcessor",parameters:[{name:"feature_extractor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.OwlViTProcessor.feature_extractor",description:`<strong>feature_extractor</strong> (<a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor">OwlViTFeatureExtractor</a>) &#x2014;
The feature extractor is a required input.`,name:"feature_extractor"},{anchor:"transformers.OwlViTProcessor.tokenizer",description:`<strong>tokenizer</strong> ([<code>CLIPTokenizer</code>, <code>CLIPTokenizerFast</code>]) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/processing_owlvit.py#L28"}}),Lt=new k({props:{name:"batch_decode",anchor:"transformers.OwlViTProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/processing_owlvit.py#L149"}}),At=new k({props:{name:"decode",anchor:"transformers.OwlViTProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/processing_owlvit.py#L156"}}),Wt=new k({props:{name:"post_process",anchor:"transformers.OwlViTProcessor.post_process",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/processing_owlvit.py#L142"}}),St=new H({}),Bt=new k({props:{name:"class transformers.OwlViTModel",anchor:"transformers.OwlViTModel",parameters:[{name:"config",val:": OwlViTConfig"}],parametersDescription:[{anchor:"transformers.OwlViTModel.This",description:`<strong>This</strong> model is a PyTorch [torch.nn.Module](https &#x2014;
//pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it`,name:"This"},{anchor:"transformers.OwlViTModel.as",description:`<strong>as</strong> a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and &#x2014;
behavior. &#x2014;
config (<a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTConfig">OwlViTConfig</a>): Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"as"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L873"}}),Rt=new k({props:{name:"forward",anchor:"transformers.OwlViTModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"return_loss",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_base_image_embeds",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTModel.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.OwlViTModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L1000",returnDescription:`
<p>A <code>transformers.models.owlvit.modeling_owlvit.OwlViTOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) \u2014 Contrastive loss for image-text similarity.</li>
<li><strong>logits_per_image</strong> (<code>torch.FloatTensor</code> of shape <code>(image_batch_size, text_batch_size)</code>) \u2014 The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text</strong> (<code>torch.FloatTensor</code> of shape <code>(text_batch_size, image_batch_size)</code>) \u2014 The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size * num_max_text_queries, output_dim</code>) \u2014 The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>) \u2014 The image embeddings obtained by applying the projection layer to the pooled output of
<a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</li>
<li><strong>text_model_output</strong> (Tuple<code>BaseModelOutputWithPooling</code>) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>vision_model_output</strong> (<code>BaseModelOutputWithPooling</code>) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</li>
</ul>
`,returnType:`
<p><code>transformers.models.owlvit.modeling_owlvit.OwlViTOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),De=new io({props:{$$slots:{default:[Sc]},$$scope:{ctx:y}}}),We=new pt({props:{anchor:"transformers.OwlViTModel.forward.example",$$slots:{default:[Bc]},$$scope:{ctx:y}}}),Ht=new k({props:{name:"get_text_features",anchor:"transformers.OwlViTModel.get_text_features",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L908",returnDescription:`
<p>The text embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</p>
`,returnType:`
<p>text_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),Ne=new io({props:{$$slots:{default:[Rc]},$$scope:{ctx:y}}}),Se=new pt({props:{anchor:"transformers.OwlViTModel.get_text_features.example",$$slots:{default:[Hc]},$$scope:{ctx:y}}}),Ut=new k({props:{name:"get_image_features",anchor:"transformers.OwlViTModel.get_image_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L953",returnDescription:`
<p>The image embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</p>
`,returnType:`
<p>image_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),Be=new io({props:{$$slots:{default:[Uc]},$$scope:{ctx:y}}}),Re=new pt({props:{anchor:"transformers.OwlViTModel.get_image_features.example",$$slots:{default:[Gc]},$$scope:{ctx:y}}}),Gt=new H({}),Xt=new k({props:{name:"class transformers.OwlViTTextModel",anchor:"transformers.OwlViTTextModel",parameters:[{name:"config",val:": OwlViTTextConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L716"}}),Zt=new k({props:{name:"forward",anchor:"transformers.OwlViTTextModel.forward",parameters:[{name:"input_ids",val:": Tensor"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L731",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ue=new io({props:{$$slots:{default:[Xc]},$$scope:{ctx:y}}}),Ge=new pt({props:{anchor:"transformers.OwlViTTextModel.forward.example",$$slots:{default:[Zc]},$$scope:{ctx:y}}}),Jt=new H({}),Kt=new k({props:{name:"class transformers.OwlViTVisionModel",anchor:"transformers.OwlViTVisionModel",parameters:[{name:"config",val:": OwlViTVisionConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L822"}}),Yt=new k({props:{name:"forward",anchor:"transformers.OwlViTVisionModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTVisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L835",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTVisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ze=new io({props:{$$slots:{default:[Jc]},$$scope:{ctx:y}}}),Je=new pt({props:{anchor:"transformers.OwlViTVisionModel.forward.example",$$slots:{default:[Kc]},$$scope:{ctx:y}}}),Qt=new H({}),eo=new k({props:{name:"class transformers.OwlViTForObjectDetection",anchor:"transformers.OwlViTForObjectDetection",parameters:[{name:"config",val:": OwlViTConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L1160"}}),to=new k({props:{name:"forward",anchor:"transformers.OwlViTForObjectDetection.forward",parameters:[{name:"input_ids",val:": Tensor"},{name:"pixel_values",val:": FloatTensor"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTForObjectDetection.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTForObjectDetection.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTForObjectDetection.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L1297",returnDescription:`
<p>A <code>transformers.models.owlvit.modeling_owlvit.OwlViTObjectDetectionOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> are provided)) \u2014 Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a
bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized
scale-invariant IoU loss.</li>
<li><strong>loss_dict</strong> (<code>Dict</code>, <em>optional</em>) \u2014 A dictionary containing the individual losses. Useful for logging.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches, num_queries)</code>) \u2014 Classification logits (including no-object) for all queries.</li>
<li><strong>pred_boxes</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches, 4)</code>) \u2014 Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These
values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding
possible padding). You can use <code>post_process()</code> to retrieve the unnormalized
bounding boxes.</li>
<li><strong>text_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_max_text_queries, output_dim</code>) \u2014 The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, patch_size, patch_size, output_dim</code>) \u2014 Pooled output of <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>. OWL-ViT represents images as a set of image patches and computes
image embeddings for each patch.</li>
<li><strong>class_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches, hidden_size)</code>) \u2014 Class embeddings of all image patches. OWL-ViT represents images as a set of image patches where the total
number of patches is (image_size / patch_size)**2.</li>
<li><strong>text_model_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>)) \u2014 Last hidden states extracted from the <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>vision_model_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches + 1, hidden_size)</code>)) \u2014 Last hidden states extracted from the <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>. OWL-ViT represents images as a set of image
patches where the total number of patches is (image_size / patch_size)**2.</li>
</ul>
`,returnType:`
<p><code>transformers.models.owlvit.modeling_owlvit.OwlViTObjectDetectionOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ye=new io({props:{$$slots:{default:[Yc]},$$scope:{ctx:y}}}),Qe=new pt({props:{anchor:"transformers.OwlViTForObjectDetection.forward.example",$$slots:{default:[Qc]},$$scope:{ctx:y}}}),{c(){c=n("meta"),T=h(),g=n("h1"),p=n("a"),w=n("span"),v(s.$$.fragment),m=h(),j=n("span"),Bn=i("OWL-ViT"),Us=h(),J=n("h2"),$e=n("a"),os=n("span"),v(ht.$$.fragment),Rn=h(),ss=n("span"),Hn=i("Overview"),Gs=h(),Oe=n("p"),Un=i("The OWL-ViT (short for Vision Transformer for Open-World Localization) was proposed in "),ft=n("a"),Gn=i("Simple Open-Vocabulary Object Detection with Vision Transformers"),Xn=i(" by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. OWL-ViT is an open-vocabulary object detection network trained on a variety of (image, text) pairs. It can be used to query an image with one or multiple text queries to search for and detect target objects described in text."),Xs=h(),lo=n("p"),Zn=i("The abstract from the paper is the following:"),Zs=h(),co=n("p"),ns=n("em"),Jn=i("Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub."),Js=h(),K=n("h2"),xe=n("a"),rs=n("span"),v(gt.$$.fragment),Kn=h(),as=n("span"),Yn=i("Usage"),Ks=h(),Ve=n("p"),Qn=i("OWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses "),po=n("a"),er=i("CLIP"),tr=i(" as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection."),Ys=h(),z=n("p"),mo=n("a"),or=i("OwlViTFeatureExtractor"),sr=i(" can be used to resize (or rescale) and normalize images for the model and "),ho=n("a"),nr=i("CLIPTokenizer"),rr=i(" is used to encode the text. "),fo=n("a"),ar=i("OwlViTProcessor"),ir=i(" wraps "),go=n("a"),lr=i("OwlViTFeatureExtractor"),cr=i(" and "),uo=n("a"),dr=i("CLIPTokenizer"),pr=i(" into a single instance to both encode the text and prepare the images. The following example shows how to perform object detection using "),_o=n("a"),mr=i("OwlViTProcessor"),hr=i(" and "),wo=n("a"),fr=i("OwlViTForObjectDetection"),gr=i("."),Qs=h(),v(ut.$$.fragment),en=h(),U=n("p"),ur=i("This model was contributed by "),_t=n("a"),_r=i("adirik"),wr=i(". The original code can be found "),wt=n("a"),Tr=i("here"),vr=i("."),tn=h(),Y=n("h2"),ye=n("a"),is=n("span"),v(Tt.$$.fragment),br=h(),ls=n("span"),$r=i("OwlViTConfig"),on=h(),C=n("div"),v(vt.$$.fragment),Or=h(),G=n("p"),To=n("a"),xr=i("OwlViTConfig"),Vr=i(" is the configuration class to store the configuration of an "),vo=n("a"),yr=i("OwlViTModel"),jr=i(`. It is used to
instantiate an OWL-ViT model according to the specified arguments, defining the text model and vision model
configs. Instantiating a configuration with the defaults will yield a similar configuration to that of the OWL-ViT
`),bt=n("a"),kr=i("google/owlvit-base-patch32"),zr=i(" architecture."),Pr=h(),Q=n("p"),Er=i("Configuration objects inherit from "),bo=n("a"),Mr=i("PretrainedConfig"),Cr=i(` and can be used to control the model outputs. Read the
documentation from `),$o=n("a"),qr=i("PretrainedConfig"),Ir=i(" for more information."),Lr=h(),je=n("div"),v($t.$$.fragment),Fr=h(),Ot=n("p"),Ar=i("Instantiate a "),Oo=n("a"),Dr=i("OwlViTConfig"),Wr=i(` (or a derived class) from owlvit text model configuration and owlvit vision
model configuration.`),sn=h(),ee=n("h2"),ke=n("a"),cs=n("span"),v(xt.$$.fragment),Nr=h(),ds=n("span"),Sr=i("OwlViTTextConfig"),nn=h(),q=n("div"),v(Vt.$$.fragment),Br=h(),te=n("p"),Rr=i("This is the configuration class to store the configuration of an "),xo=n("a"),Hr=i("OwlViTTextModel"),Ur=i(`. It is used to instantiate an
OwlViT text encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OwlViT
`),yt=n("a"),Gr=i("google/owlvit-base-patch32"),Xr=i(" architecture."),Zr=h(),oe=n("p"),Jr=i("Configuration objects inherit from "),Vo=n("a"),Kr=i("PretrainedConfig"),Yr=i(` and can be used to control the model outputs. Read the
documentation from `),yo=n("a"),Qr=i("PretrainedConfig"),ea=i(" for more information."),ta=h(),v(ze.$$.fragment),rn=h(),se=n("h2"),Pe=n("a"),ps=n("span"),v(jt.$$.fragment),oa=h(),ms=n("span"),sa=i("OwlViTVisionConfig"),an=h(),I=n("div"),v(kt.$$.fragment),na=h(),ne=n("p"),ra=i("This is the configuration class to store the configuration of an "),jo=n("a"),aa=i("OwlViTVisionModel"),ia=i(`. It is used to instantiate
an OWL-ViT image encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OWL-ViT
`),zt=n("a"),la=i("google/owlvit-base-patch32"),ca=i(" architecture."),da=h(),re=n("p"),pa=i("Configuration objects inherit from "),ko=n("a"),ma=i("PretrainedConfig"),ha=i(` and can be used to control the model outputs. Read the
documentation from `),zo=n("a"),fa=i("PretrainedConfig"),ga=i(" for more information."),ua=h(),v(Ee.$$.fragment),ln=h(),ae=n("h2"),Me=n("a"),hs=n("span"),v(Pt.$$.fragment),_a=h(),fs=n("span"),wa=i("OwlViTFeatureExtractor"),cn=h(),L=n("div"),v(Et.$$.fragment),Ta=h(),gs=n("p"),va=i("Constructs an OWL-ViT feature extractor."),ba=h(),Mt=n("p"),$a=i("This feature extractor inherits from "),Po=n("a"),Oa=i("FeatureExtractionMixin"),xa=i(` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),Va=h(),X=n("div"),v(Ct.$$.fragment),ya=h(),us=n("p"),ja=i("Main method to prepare for the model one or several image(s)."),ka=h(),v(Ce.$$.fragment),dn=h(),ie=n("h2"),qe=n("a"),_s=n("span"),v(qt.$$.fragment),za=h(),ws=n("span"),Pa=i("OwlViTProcessor"),pn=h(),P=n("div"),v(It.$$.fragment),Ea=h(),E=n("p"),Ma=i("Constructs an OWL-ViT processor which wraps "),Eo=n("a"),Ca=i("OwlViTFeatureExtractor"),qa=i(" and "),Mo=n("a"),Ia=i("CLIPTokenizer"),La=i("/"),Co=n("a"),Fa=i("CLIPTokenizerFast"),Aa=i(`
into a single processor that interits both the feature extractor and tokenizer functionalities. See the
`),Ts=n("code"),Da=i("__call__()"),Wa=i(" and "),qo=n("a"),Na=i("decode()"),Sa=i(" for more information."),Ba=h(),Ie=n("div"),v(Lt.$$.fragment),Ra=h(),Ft=n("p"),Ha=i("This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Io=n("a"),Ua=i("batch_decode()"),Ga=i(`. Please
refer to the docstring of this method for more information.`),Xa=h(),Le=n("div"),v(At.$$.fragment),Za=h(),Dt=n("p"),Ja=i("This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Lo=n("a"),Ka=i("decode()"),Ya=i(`. Please refer to
the docstring of this method for more information.`),Qa=h(),Fe=n("div"),v(Wt.$$.fragment),ei=h(),Nt=n("p"),ti=i("This method forwards all its arguments to "),vs=n("code"),oi=i("OwlViTFeatureExtractor.post_process()"),si=i(`. Please refer to the
docstring of this method for more information.`),mn=h(),le=n("h2"),Ae=n("a"),bs=n("span"),v(St.$$.fragment),ni=h(),$s=n("span"),ri=i("OwlViTModel"),hn=h(),F=n("div"),v(Bt.$$.fragment),ai=h(),A=n("div"),v(Rt.$$.fragment),ii=h(),ce=n("p"),li=i("The "),Fo=n("a"),ci=i("OwlViTModel"),di=i(" forward method, overrides the "),Os=n("code"),pi=i("__call__"),mi=i(" special method."),hi=h(),v(De.$$.fragment),fi=h(),v(We.$$.fragment),gi=h(),D=n("div"),v(Ht.$$.fragment),ui=h(),de=n("p"),_i=i("The "),Ao=n("a"),wi=i("OwlViTModel"),Ti=i(" forward method, overrides the "),xs=n("code"),vi=i("__call__"),bi=i(" special method."),$i=h(),v(Ne.$$.fragment),Oi=h(),v(Se.$$.fragment),xi=h(),W=n("div"),v(Ut.$$.fragment),Vi=h(),pe=n("p"),yi=i("The "),Do=n("a"),ji=i("OwlViTModel"),ki=i(" forward method, overrides the "),Vs=n("code"),zi=i("__call__"),Pi=i(" special method."),Ei=h(),v(Be.$$.fragment),Mi=h(),v(Re.$$.fragment),fn=h(),me=n("h2"),He=n("a"),ys=n("span"),v(Gt.$$.fragment),Ci=h(),js=n("span"),qi=i("OwlViTTextModel"),gn=h(),he=n("div"),v(Xt.$$.fragment),Ii=h(),N=n("div"),v(Zt.$$.fragment),Li=h(),fe=n("p"),Fi=i("The "),Wo=n("a"),Ai=i("OwlViTTextModel"),Di=i(" forward method, overrides the "),ks=n("code"),Wi=i("__call__"),Ni=i(" special method."),Si=h(),v(Ue.$$.fragment),Bi=h(),v(Ge.$$.fragment),un=h(),ge=n("h2"),Xe=n("a"),zs=n("span"),v(Jt.$$.fragment),Ri=h(),Ps=n("span"),Hi=i("OwlViTVisionModel"),_n=h(),ue=n("div"),v(Kt.$$.fragment),Ui=h(),S=n("div"),v(Yt.$$.fragment),Gi=h(),_e=n("p"),Xi=i("The "),No=n("a"),Zi=i("OwlViTVisionModel"),Ji=i(" forward method, overrides the "),Es=n("code"),Ki=i("__call__"),Yi=i(" special method."),Qi=h(),v(Ze.$$.fragment),el=h(),v(Je.$$.fragment),wn=h(),we=n("h2"),Ke=n("a"),Ms=n("span"),v(Qt.$$.fragment),tl=h(),Cs=n("span"),ol=i("OwlViTForObjectDetection"),Tn=h(),Te=n("div"),v(eo.$$.fragment),sl=h(),B=n("div"),v(to.$$.fragment),nl=h(),ve=n("p"),rl=i("The "),So=n("a"),al=i("OwlViTForObjectDetection"),il=i(" forward method, overrides the "),qs=n("code"),ll=i("__call__"),cl=i(" special method."),dl=h(),v(Ye.$$.fragment),pl=h(),v(Qe.$$.fragment),this.h()},l(t){const u=Fc('[data-svelte="svelte-1phssyn"]',document.head);c=r(u,"META",{name:!0,content:!0}),u.forEach(o),T=f(t),g=r(t,"H1",{class:!0});var oo=a(g);p=r(oo,"A",{id:!0,class:!0,href:!0});var Is=a(p);w=r(Is,"SPAN",{});var Ls=a(w);b(s.$$.fragment,Ls),Ls.forEach(o),Is.forEach(o),m=f(oo),j=r(oo,"SPAN",{});var Fs=a(j);Bn=l(Fs,"OWL-ViT"),Fs.forEach(o),oo.forEach(o),Us=f(t),J=r(t,"H2",{class:!0});var so=a(J);$e=r(so,"A",{id:!0,class:!0,href:!0});var As=a($e);os=r(As,"SPAN",{});var Ds=a(os);b(ht.$$.fragment,Ds),Ds.forEach(o),As.forEach(o),Rn=f(so),ss=r(so,"SPAN",{});var Ws=a(ss);Hn=l(Ws,"Overview"),Ws.forEach(o),so.forEach(o),Gs=f(t),Oe=r(t,"P",{});var no=a(Oe);Un=l(no,"The OWL-ViT (short for Vision Transformer for Open-World Localization) was proposed in "),ft=r(no,"A",{href:!0,rel:!0});var Ns=a(ft);Gn=l(Ns,"Simple Open-Vocabulary Object Detection with Vision Transformers"),Ns.forEach(o),Xn=l(no," by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. OWL-ViT is an open-vocabulary object detection network trained on a variety of (image, text) pairs. It can be used to query an image with one or multiple text queries to search for and detect target objects described in text."),no.forEach(o),Xs=f(t),lo=r(t,"P",{});var Ss=a(lo);Zn=l(Ss,"The abstract from the paper is the following:"),Ss.forEach(o),Zs=f(t),co=r(t,"P",{});var Bs=a(co);ns=r(Bs,"EM",{});var Rs=a(ns);Jn=l(Rs,"Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub."),Rs.forEach(o),Bs.forEach(o),Js=f(t),K=r(t,"H2",{class:!0});var ro=a(K);xe=r(ro,"A",{id:!0,class:!0,href:!0});var Hs=a(xe);rs=r(Hs,"SPAN",{});var ml=a(rs);b(gt.$$.fragment,ml),ml.forEach(o),Hs.forEach(o),Kn=f(ro),as=r(ro,"SPAN",{});var hl=a(as);Yn=l(hl,"Usage"),hl.forEach(o),ro.forEach(o),Ks=f(t),Ve=r(t,"P",{});var bn=a(Ve);Qn=l(bn,"OWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses "),po=r(bn,"A",{href:!0});var fl=a(po);er=l(fl,"CLIP"),fl.forEach(o),tr=l(bn," as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection."),bn.forEach(o),Ys=f(t),z=r(t,"P",{});var M=a(z);mo=r(M,"A",{href:!0});var gl=a(mo);or=l(gl,"OwlViTFeatureExtractor"),gl.forEach(o),sr=l(M," can be used to resize (or rescale) and normalize images for the model and "),ho=r(M,"A",{href:!0});var ul=a(ho);nr=l(ul,"CLIPTokenizer"),ul.forEach(o),rr=l(M," is used to encode the text. "),fo=r(M,"A",{href:!0});var _l=a(fo);ar=l(_l,"OwlViTProcessor"),_l.forEach(o),ir=l(M," wraps "),go=r(M,"A",{href:!0});var wl=a(go);lr=l(wl,"OwlViTFeatureExtractor"),wl.forEach(o),cr=l(M," and "),uo=r(M,"A",{href:!0});var Tl=a(uo);dr=l(Tl,"CLIPTokenizer"),Tl.forEach(o),pr=l(M," into a single instance to both encode the text and prepare the images. The following example shows how to perform object detection using "),_o=r(M,"A",{href:!0});var vl=a(_o);mr=l(vl,"OwlViTProcessor"),vl.forEach(o),hr=l(M," and "),wo=r(M,"A",{href:!0});var bl=a(wo);fr=l(bl,"OwlViTForObjectDetection"),bl.forEach(o),gr=l(M,"."),M.forEach(o),Qs=f(t),b(ut.$$.fragment,t),en=f(t),U=r(t,"P",{});var Bo=a(U);ur=l(Bo,"This model was contributed by "),_t=r(Bo,"A",{href:!0,rel:!0});var $l=a(_t);_r=l($l,"adirik"),$l.forEach(o),wr=l(Bo,". The original code can be found "),wt=r(Bo,"A",{href:!0,rel:!0});var Ol=a(wt);Tr=l(Ol,"here"),Ol.forEach(o),vr=l(Bo,"."),Bo.forEach(o),tn=f(t),Y=r(t,"H2",{class:!0});var $n=a(Y);ye=r($n,"A",{id:!0,class:!0,href:!0});var xl=a(ye);is=r(xl,"SPAN",{});var Vl=a(is);b(Tt.$$.fragment,Vl),Vl.forEach(o),xl.forEach(o),br=f($n),ls=r($n,"SPAN",{});var yl=a(ls);$r=l(yl,"OwlViTConfig"),yl.forEach(o),$n.forEach(o),on=f(t),C=r(t,"DIV",{class:!0});var et=a(C);b(vt.$$.fragment,et),Or=f(et),G=r(et,"P",{});var ao=a(G);To=r(ao,"A",{href:!0});var jl=a(To);xr=l(jl,"OwlViTConfig"),jl.forEach(o),Vr=l(ao," is the configuration class to store the configuration of an "),vo=r(ao,"A",{href:!0});var kl=a(vo);yr=l(kl,"OwlViTModel"),kl.forEach(o),jr=l(ao,`. It is used to
instantiate an OWL-ViT model according to the specified arguments, defining the text model and vision model
configs. Instantiating a configuration with the defaults will yield a similar configuration to that of the OWL-ViT
`),bt=r(ao,"A",{href:!0,rel:!0});var zl=a(bt);kr=l(zl,"google/owlvit-base-patch32"),zl.forEach(o),zr=l(ao," architecture."),ao.forEach(o),Pr=f(et),Q=r(et,"P",{});var Ro=a(Q);Er=l(Ro,"Configuration objects inherit from "),bo=r(Ro,"A",{href:!0});var Pl=a(bo);Mr=l(Pl,"PretrainedConfig"),Pl.forEach(o),Cr=l(Ro,` and can be used to control the model outputs. Read the
documentation from `),$o=r(Ro,"A",{href:!0});var El=a($o);qr=l(El,"PretrainedConfig"),El.forEach(o),Ir=l(Ro," for more information."),Ro.forEach(o),Lr=f(et),je=r(et,"DIV",{class:!0});var On=a(je);b($t.$$.fragment,On),Fr=f(On),Ot=r(On,"P",{});var xn=a(Ot);Ar=l(xn,"Instantiate a "),Oo=r(xn,"A",{href:!0});var Ml=a(Oo);Dr=l(Ml,"OwlViTConfig"),Ml.forEach(o),Wr=l(xn,` (or a derived class) from owlvit text model configuration and owlvit vision
model configuration.`),xn.forEach(o),On.forEach(o),et.forEach(o),sn=f(t),ee=r(t,"H2",{class:!0});var Vn=a(ee);ke=r(Vn,"A",{id:!0,class:!0,href:!0});var Cl=a(ke);cs=r(Cl,"SPAN",{});var ql=a(cs);b(xt.$$.fragment,ql),ql.forEach(o),Cl.forEach(o),Nr=f(Vn),ds=r(Vn,"SPAN",{});var Il=a(ds);Sr=l(Il,"OwlViTTextConfig"),Il.forEach(o),Vn.forEach(o),nn=f(t),q=r(t,"DIV",{class:!0});var tt=a(q);b(Vt.$$.fragment,tt),Br=f(tt),te=r(tt,"P",{});var Ho=a(te);Rr=l(Ho,"This is the configuration class to store the configuration of an "),xo=r(Ho,"A",{href:!0});var Ll=a(xo);Hr=l(Ll,"OwlViTTextModel"),Ll.forEach(o),Ur=l(Ho,`. It is used to instantiate an
OwlViT text encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OwlViT
`),yt=r(Ho,"A",{href:!0,rel:!0});var Fl=a(yt);Gr=l(Fl,"google/owlvit-base-patch32"),Fl.forEach(o),Xr=l(Ho," architecture."),Ho.forEach(o),Zr=f(tt),oe=r(tt,"P",{});var Uo=a(oe);Jr=l(Uo,"Configuration objects inherit from "),Vo=r(Uo,"A",{href:!0});var Al=a(Vo);Kr=l(Al,"PretrainedConfig"),Al.forEach(o),Yr=l(Uo,` and can be used to control the model outputs. Read the
documentation from `),yo=r(Uo,"A",{href:!0});var Dl=a(yo);Qr=l(Dl,"PretrainedConfig"),Dl.forEach(o),ea=l(Uo," for more information."),Uo.forEach(o),ta=f(tt),b(ze.$$.fragment,tt),tt.forEach(o),rn=f(t),se=r(t,"H2",{class:!0});var yn=a(se);Pe=r(yn,"A",{id:!0,class:!0,href:!0});var Wl=a(Pe);ps=r(Wl,"SPAN",{});var Nl=a(ps);b(jt.$$.fragment,Nl),Nl.forEach(o),Wl.forEach(o),oa=f(yn),ms=r(yn,"SPAN",{});var Sl=a(ms);sa=l(Sl,"OwlViTVisionConfig"),Sl.forEach(o),yn.forEach(o),an=f(t),I=r(t,"DIV",{class:!0});var ot=a(I);b(kt.$$.fragment,ot),na=f(ot),ne=r(ot,"P",{});var Go=a(ne);ra=l(Go,"This is the configuration class to store the configuration of an "),jo=r(Go,"A",{href:!0});var Bl=a(jo);aa=l(Bl,"OwlViTVisionModel"),Bl.forEach(o),ia=l(Go,`. It is used to instantiate
an OWL-ViT image encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OWL-ViT
`),zt=r(Go,"A",{href:!0,rel:!0});var Rl=a(zt);la=l(Rl,"google/owlvit-base-patch32"),Rl.forEach(o),ca=l(Go," architecture."),Go.forEach(o),da=f(ot),re=r(ot,"P",{});var Xo=a(re);pa=l(Xo,"Configuration objects inherit from "),ko=r(Xo,"A",{href:!0});var Hl=a(ko);ma=l(Hl,"PretrainedConfig"),Hl.forEach(o),ha=l(Xo,` and can be used to control the model outputs. Read the
documentation from `),zo=r(Xo,"A",{href:!0});var Ul=a(zo);fa=l(Ul,"PretrainedConfig"),Ul.forEach(o),ga=l(Xo," for more information."),Xo.forEach(o),ua=f(ot),b(Ee.$$.fragment,ot),ot.forEach(o),ln=f(t),ae=r(t,"H2",{class:!0});var jn=a(ae);Me=r(jn,"A",{id:!0,class:!0,href:!0});var Gl=a(Me);hs=r(Gl,"SPAN",{});var Xl=a(hs);b(Pt.$$.fragment,Xl),Xl.forEach(o),Gl.forEach(o),_a=f(jn),fs=r(jn,"SPAN",{});var Zl=a(fs);wa=l(Zl,"OwlViTFeatureExtractor"),Zl.forEach(o),jn.forEach(o),cn=f(t),L=r(t,"DIV",{class:!0});var st=a(L);b(Et.$$.fragment,st),Ta=f(st),gs=r(st,"P",{});var Jl=a(gs);va=l(Jl,"Constructs an OWL-ViT feature extractor."),Jl.forEach(o),ba=f(st),Mt=r(st,"P",{});var kn=a(Mt);$a=l(kn,"This feature extractor inherits from "),Po=r(kn,"A",{href:!0});var Kl=a(Po);Oa=l(Kl,"FeatureExtractionMixin"),Kl.forEach(o),xa=l(kn,` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),kn.forEach(o),Va=f(st),X=r(st,"DIV",{class:!0});var Zo=a(X);b(Ct.$$.fragment,Zo),ya=f(Zo),us=r(Zo,"P",{});var Yl=a(us);ja=l(Yl,"Main method to prepare for the model one or several image(s)."),Yl.forEach(o),ka=f(Zo),b(Ce.$$.fragment,Zo),Zo.forEach(o),st.forEach(o),dn=f(t),ie=r(t,"H2",{class:!0});var zn=a(ie);qe=r(zn,"A",{id:!0,class:!0,href:!0});var Ql=a(qe);_s=r(Ql,"SPAN",{});var ec=a(_s);b(qt.$$.fragment,ec),ec.forEach(o),Ql.forEach(o),za=f(zn),ws=r(zn,"SPAN",{});var tc=a(ws);Pa=l(tc,"OwlViTProcessor"),tc.forEach(o),zn.forEach(o),pn=f(t),P=r(t,"DIV",{class:!0});var Z=a(P);b(It.$$.fragment,Z),Ea=f(Z),E=r(Z,"P",{});var R=a(E);Ma=l(R,"Constructs an OWL-ViT processor which wraps "),Eo=r(R,"A",{href:!0});var oc=a(Eo);Ca=l(oc,"OwlViTFeatureExtractor"),oc.forEach(o),qa=l(R," and "),Mo=r(R,"A",{href:!0});var sc=a(Mo);Ia=l(sc,"CLIPTokenizer"),sc.forEach(o),La=l(R,"/"),Co=r(R,"A",{href:!0});var nc=a(Co);Fa=l(nc,"CLIPTokenizerFast"),nc.forEach(o),Aa=l(R,`
into a single processor that interits both the feature extractor and tokenizer functionalities. See the
`),Ts=r(R,"CODE",{});var rc=a(Ts);Da=l(rc,"__call__()"),rc.forEach(o),Wa=l(R," and "),qo=r(R,"A",{href:!0});var ac=a(qo);Na=l(ac,"decode()"),ac.forEach(o),Sa=l(R," for more information."),R.forEach(o),Ba=f(Z),Ie=r(Z,"DIV",{class:!0});var Pn=a(Ie);b(Lt.$$.fragment,Pn),Ra=f(Pn),Ft=r(Pn,"P",{});var En=a(Ft);Ha=l(En,"This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Io=r(En,"A",{href:!0});var ic=a(Io);Ua=l(ic,"batch_decode()"),ic.forEach(o),Ga=l(En,`. Please
refer to the docstring of this method for more information.`),En.forEach(o),Pn.forEach(o),Xa=f(Z),Le=r(Z,"DIV",{class:!0});var Mn=a(Le);b(At.$$.fragment,Mn),Za=f(Mn),Dt=r(Mn,"P",{});var Cn=a(Dt);Ja=l(Cn,"This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Lo=r(Cn,"A",{href:!0});var lc=a(Lo);Ka=l(lc,"decode()"),lc.forEach(o),Ya=l(Cn,`. Please refer to
the docstring of this method for more information.`),Cn.forEach(o),Mn.forEach(o),Qa=f(Z),Fe=r(Z,"DIV",{class:!0});var qn=a(Fe);b(Wt.$$.fragment,qn),ei=f(qn),Nt=r(qn,"P",{});var In=a(Nt);ti=l(In,"This method forwards all its arguments to "),vs=r(In,"CODE",{});var cc=a(vs);oi=l(cc,"OwlViTFeatureExtractor.post_process()"),cc.forEach(o),si=l(In,`. Please refer to the
docstring of this method for more information.`),In.forEach(o),qn.forEach(o),Z.forEach(o),mn=f(t),le=r(t,"H2",{class:!0});var Ln=a(le);Ae=r(Ln,"A",{id:!0,class:!0,href:!0});var dc=a(Ae);bs=r(dc,"SPAN",{});var pc=a(bs);b(St.$$.fragment,pc),pc.forEach(o),dc.forEach(o),ni=f(Ln),$s=r(Ln,"SPAN",{});var mc=a($s);ri=l(mc,"OwlViTModel"),mc.forEach(o),Ln.forEach(o),hn=f(t),F=r(t,"DIV",{class:!0});var nt=a(F);b(Bt.$$.fragment,nt),ai=f(nt),A=r(nt,"DIV",{class:!0});var rt=a(A);b(Rt.$$.fragment,rt),ii=f(rt),ce=r(rt,"P",{});var Jo=a(ce);li=l(Jo,"The "),Fo=r(Jo,"A",{href:!0});var hc=a(Fo);ci=l(hc,"OwlViTModel"),hc.forEach(o),di=l(Jo," forward method, overrides the "),Os=r(Jo,"CODE",{});var fc=a(Os);pi=l(fc,"__call__"),fc.forEach(o),mi=l(Jo," special method."),Jo.forEach(o),hi=f(rt),b(De.$$.fragment,rt),fi=f(rt),b(We.$$.fragment,rt),rt.forEach(o),gi=f(nt),D=r(nt,"DIV",{class:!0});var at=a(D);b(Ht.$$.fragment,at),ui=f(at),de=r(at,"P",{});var Ko=a(de);_i=l(Ko,"The "),Ao=r(Ko,"A",{href:!0});var gc=a(Ao);wi=l(gc,"OwlViTModel"),gc.forEach(o),Ti=l(Ko," forward method, overrides the "),xs=r(Ko,"CODE",{});var uc=a(xs);vi=l(uc,"__call__"),uc.forEach(o),bi=l(Ko," special method."),Ko.forEach(o),$i=f(at),b(Ne.$$.fragment,at),Oi=f(at),b(Se.$$.fragment,at),at.forEach(o),xi=f(nt),W=r(nt,"DIV",{class:!0});var it=a(W);b(Ut.$$.fragment,it),Vi=f(it),pe=r(it,"P",{});var Yo=a(pe);yi=l(Yo,"The "),Do=r(Yo,"A",{href:!0});var _c=a(Do);ji=l(_c,"OwlViTModel"),_c.forEach(o),ki=l(Yo," forward method, overrides the "),Vs=r(Yo,"CODE",{});var wc=a(Vs);zi=l(wc,"__call__"),wc.forEach(o),Pi=l(Yo," special method."),Yo.forEach(o),Ei=f(it),b(Be.$$.fragment,it),Mi=f(it),b(Re.$$.fragment,it),it.forEach(o),nt.forEach(o),fn=f(t),me=r(t,"H2",{class:!0});var Fn=a(me);He=r(Fn,"A",{id:!0,class:!0,href:!0});var Tc=a(He);ys=r(Tc,"SPAN",{});var vc=a(ys);b(Gt.$$.fragment,vc),vc.forEach(o),Tc.forEach(o),Ci=f(Fn),js=r(Fn,"SPAN",{});var bc=a(js);qi=l(bc,"OwlViTTextModel"),bc.forEach(o),Fn.forEach(o),gn=f(t),he=r(t,"DIV",{class:!0});var An=a(he);b(Xt.$$.fragment,An),Ii=f(An),N=r(An,"DIV",{class:!0});var lt=a(N);b(Zt.$$.fragment,lt),Li=f(lt),fe=r(lt,"P",{});var Qo=a(fe);Fi=l(Qo,"The "),Wo=r(Qo,"A",{href:!0});var $c=a(Wo);Ai=l($c,"OwlViTTextModel"),$c.forEach(o),Di=l(Qo," forward method, overrides the "),ks=r(Qo,"CODE",{});var Oc=a(ks);Wi=l(Oc,"__call__"),Oc.forEach(o),Ni=l(Qo," special method."),Qo.forEach(o),Si=f(lt),b(Ue.$$.fragment,lt),Bi=f(lt),b(Ge.$$.fragment,lt),lt.forEach(o),An.forEach(o),un=f(t),ge=r(t,"H2",{class:!0});var Dn=a(ge);Xe=r(Dn,"A",{id:!0,class:!0,href:!0});var xc=a(Xe);zs=r(xc,"SPAN",{});var Vc=a(zs);b(Jt.$$.fragment,Vc),Vc.forEach(o),xc.forEach(o),Ri=f(Dn),Ps=r(Dn,"SPAN",{});var yc=a(Ps);Hi=l(yc,"OwlViTVisionModel"),yc.forEach(o),Dn.forEach(o),_n=f(t),ue=r(t,"DIV",{class:!0});var Wn=a(ue);b(Kt.$$.fragment,Wn),Ui=f(Wn),S=r(Wn,"DIV",{class:!0});var ct=a(S);b(Yt.$$.fragment,ct),Gi=f(ct),_e=r(ct,"P",{});var es=a(_e);Xi=l(es,"The "),No=r(es,"A",{href:!0});var jc=a(No);Zi=l(jc,"OwlViTVisionModel"),jc.forEach(o),Ji=l(es," forward method, overrides the "),Es=r(es,"CODE",{});var kc=a(Es);Ki=l(kc,"__call__"),kc.forEach(o),Yi=l(es," special method."),es.forEach(o),Qi=f(ct),b(Ze.$$.fragment,ct),el=f(ct),b(Je.$$.fragment,ct),ct.forEach(o),Wn.forEach(o),wn=f(t),we=r(t,"H2",{class:!0});var Nn=a(we);Ke=r(Nn,"A",{id:!0,class:!0,href:!0});var zc=a(Ke);Ms=r(zc,"SPAN",{});var Pc=a(Ms);b(Qt.$$.fragment,Pc),Pc.forEach(o),zc.forEach(o),tl=f(Nn),Cs=r(Nn,"SPAN",{});var Ec=a(Cs);ol=l(Ec,"OwlViTForObjectDetection"),Ec.forEach(o),Nn.forEach(o),Tn=f(t),Te=r(t,"DIV",{class:!0});var Sn=a(Te);b(eo.$$.fragment,Sn),sl=f(Sn),B=r(Sn,"DIV",{class:!0});var dt=a(B);b(to.$$.fragment,dt),nl=f(dt),ve=r(dt,"P",{});var ts=a(ve);rl=l(ts,"The "),So=r(ts,"A",{href:!0});var Mc=a(So);al=l(Mc,"OwlViTForObjectDetection"),Mc.forEach(o),il=l(ts," forward method, overrides the "),qs=r(ts,"CODE",{});var Cc=a(qs);ll=l(Cc,"__call__"),Cc.forEach(o),cl=l(ts," special method."),ts.forEach(o),dl=f(dt),b(Ye.$$.fragment,dt),pl=f(dt),b(Qe.$$.fragment,dt),dt.forEach(o),Sn.forEach(o),this.h()},h(){d(c,"name","hf:doc:metadata"),d(c,"content",JSON.stringify(td)),d(p,"id","owlvit"),d(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(p,"href","#owlvit"),d(g,"class","relative group"),d($e,"id","overview"),d($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d($e,"href","#overview"),d(J,"class","relative group"),d(ft,"href","https://arxiv.org/abs/2205.06230"),d(ft,"rel","nofollow"),d(xe,"id","usage"),d(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(xe,"href","#usage"),d(K,"class","relative group"),d(po,"href","clip"),d(mo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),d(ho,"href","/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer"),d(fo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTProcessor"),d(go,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),d(uo,"href","/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer"),d(_o,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTProcessor"),d(wo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTForObjectDetection"),d(_t,"href","https://huggingface.co/adirik"),d(_t,"rel","nofollow"),d(wt,"href","https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit"),d(wt,"rel","nofollow"),d(ye,"id","transformers.OwlViTConfig"),d(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ye,"href","#transformers.OwlViTConfig"),d(Y,"class","relative group"),d(To,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTConfig"),d(vo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTModel"),d(bt,"href","https://huggingface.co/google/owlvit-base-patch32"),d(bt,"rel","nofollow"),d(bo,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d($o,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(Oo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTConfig"),d(je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ke,"id","transformers.OwlViTTextConfig"),d(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ke,"href","#transformers.OwlViTTextConfig"),d(ee,"class","relative group"),d(xo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"),d(yt,"href","https://huggingface.co/google/owlvit-base-patch32"),d(yt,"rel","nofollow"),d(Vo,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(yo,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Pe,"id","transformers.OwlViTVisionConfig"),d(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Pe,"href","#transformers.OwlViTVisionConfig"),d(se,"class","relative group"),d(jo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"),d(zt,"href","https://huggingface.co/google/owlvit-base-patch32"),d(zt,"rel","nofollow"),d(ko,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(zo,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Me,"id","transformers.OwlViTFeatureExtractor"),d(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Me,"href","#transformers.OwlViTFeatureExtractor"),d(ae,"class","relative group"),d(Po,"href","/docs/transformers/main/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),d(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(qe,"id","transformers.OwlViTProcessor"),d(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(qe,"href","#transformers.OwlViTProcessor"),d(ie,"class","relative group"),d(Eo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),d(Mo,"href","/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer"),d(Co,"href","/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizerFast"),d(qo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTProcessor.decode"),d(Io,"href","/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode"),d(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Lo,"href","/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode"),d(Le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ae,"id","transformers.OwlViTModel"),d(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ae,"href","#transformers.OwlViTModel"),d(le,"class","relative group"),d(Fo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTModel"),d(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ao,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTModel"),d(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Do,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTModel"),d(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(He,"id","transformers.OwlViTTextModel"),d(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(He,"href","#transformers.OwlViTTextModel"),d(me,"class","relative group"),d(Wo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"),d(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Xe,"id","transformers.OwlViTVisionModel"),d(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Xe,"href","#transformers.OwlViTVisionModel"),d(ge,"class","relative group"),d(No,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"),d(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ke,"id","transformers.OwlViTForObjectDetection"),d(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ke,"href","#transformers.OwlViTForObjectDetection"),d(we,"class","relative group"),d(So,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTForObjectDetection"),d(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,u){e(document.head,c),_(t,T,u),_(t,g,u),e(g,p),e(p,w),$(s,w,null),e(g,m),e(g,j),e(j,Bn),_(t,Us,u),_(t,J,u),e(J,$e),e($e,os),$(ht,os,null),e(J,Rn),e(J,ss),e(ss,Hn),_(t,Gs,u),_(t,Oe,u),e(Oe,Un),e(Oe,ft),e(ft,Gn),e(Oe,Xn),_(t,Xs,u),_(t,lo,u),e(lo,Zn),_(t,Zs,u),_(t,co,u),e(co,ns),e(ns,Jn),_(t,Js,u),_(t,K,u),e(K,xe),e(xe,rs),$(gt,rs,null),e(K,Kn),e(K,as),e(as,Yn),_(t,Ks,u),_(t,Ve,u),e(Ve,Qn),e(Ve,po),e(po,er),e(Ve,tr),_(t,Ys,u),_(t,z,u),e(z,mo),e(mo,or),e(z,sr),e(z,ho),e(ho,nr),e(z,rr),e(z,fo),e(fo,ar),e(z,ir),e(z,go),e(go,lr),e(z,cr),e(z,uo),e(uo,dr),e(z,pr),e(z,_o),e(_o,mr),e(z,hr),e(z,wo),e(wo,fr),e(z,gr),_(t,Qs,u),$(ut,t,u),_(t,en,u),_(t,U,u),e(U,ur),e(U,_t),e(_t,_r),e(U,wr),e(U,wt),e(wt,Tr),e(U,vr),_(t,tn,u),_(t,Y,u),e(Y,ye),e(ye,is),$(Tt,is,null),e(Y,br),e(Y,ls),e(ls,$r),_(t,on,u),_(t,C,u),$(vt,C,null),e(C,Or),e(C,G),e(G,To),e(To,xr),e(G,Vr),e(G,vo),e(vo,yr),e(G,jr),e(G,bt),e(bt,kr),e(G,zr),e(C,Pr),e(C,Q),e(Q,Er),e(Q,bo),e(bo,Mr),e(Q,Cr),e(Q,$o),e($o,qr),e(Q,Ir),e(C,Lr),e(C,je),$($t,je,null),e(je,Fr),e(je,Ot),e(Ot,Ar),e(Ot,Oo),e(Oo,Dr),e(Ot,Wr),_(t,sn,u),_(t,ee,u),e(ee,ke),e(ke,cs),$(xt,cs,null),e(ee,Nr),e(ee,ds),e(ds,Sr),_(t,nn,u),_(t,q,u),$(Vt,q,null),e(q,Br),e(q,te),e(te,Rr),e(te,xo),e(xo,Hr),e(te,Ur),e(te,yt),e(yt,Gr),e(te,Xr),e(q,Zr),e(q,oe),e(oe,Jr),e(oe,Vo),e(Vo,Kr),e(oe,Yr),e(oe,yo),e(yo,Qr),e(oe,ea),e(q,ta),$(ze,q,null),_(t,rn,u),_(t,se,u),e(se,Pe),e(Pe,ps),$(jt,ps,null),e(se,oa),e(se,ms),e(ms,sa),_(t,an,u),_(t,I,u),$(kt,I,null),e(I,na),e(I,ne),e(ne,ra),e(ne,jo),e(jo,aa),e(ne,ia),e(ne,zt),e(zt,la),e(ne,ca),e(I,da),e(I,re),e(re,pa),e(re,ko),e(ko,ma),e(re,ha),e(re,zo),e(zo,fa),e(re,ga),e(I,ua),$(Ee,I,null),_(t,ln,u),_(t,ae,u),e(ae,Me),e(Me,hs),$(Pt,hs,null),e(ae,_a),e(ae,fs),e(fs,wa),_(t,cn,u),_(t,L,u),$(Et,L,null),e(L,Ta),e(L,gs),e(gs,va),e(L,ba),e(L,Mt),e(Mt,$a),e(Mt,Po),e(Po,Oa),e(Mt,xa),e(L,Va),e(L,X),$(Ct,X,null),e(X,ya),e(X,us),e(us,ja),e(X,ka),$(Ce,X,null),_(t,dn,u),_(t,ie,u),e(ie,qe),e(qe,_s),$(qt,_s,null),e(ie,za),e(ie,ws),e(ws,Pa),_(t,pn,u),_(t,P,u),$(It,P,null),e(P,Ea),e(P,E),e(E,Ma),e(E,Eo),e(Eo,Ca),e(E,qa),e(E,Mo),e(Mo,Ia),e(E,La),e(E,Co),e(Co,Fa),e(E,Aa),e(E,Ts),e(Ts,Da),e(E,Wa),e(E,qo),e(qo,Na),e(E,Sa),e(P,Ba),e(P,Ie),$(Lt,Ie,null),e(Ie,Ra),e(Ie,Ft),e(Ft,Ha),e(Ft,Io),e(Io,Ua),e(Ft,Ga),e(P,Xa),e(P,Le),$(At,Le,null),e(Le,Za),e(Le,Dt),e(Dt,Ja),e(Dt,Lo),e(Lo,Ka),e(Dt,Ya),e(P,Qa),e(P,Fe),$(Wt,Fe,null),e(Fe,ei),e(Fe,Nt),e(Nt,ti),e(Nt,vs),e(vs,oi),e(Nt,si),_(t,mn,u),_(t,le,u),e(le,Ae),e(Ae,bs),$(St,bs,null),e(le,ni),e(le,$s),e($s,ri),_(t,hn,u),_(t,F,u),$(Bt,F,null),e(F,ai),e(F,A),$(Rt,A,null),e(A,ii),e(A,ce),e(ce,li),e(ce,Fo),e(Fo,ci),e(ce,di),e(ce,Os),e(Os,pi),e(ce,mi),e(A,hi),$(De,A,null),e(A,fi),$(We,A,null),e(F,gi),e(F,D),$(Ht,D,null),e(D,ui),e(D,de),e(de,_i),e(de,Ao),e(Ao,wi),e(de,Ti),e(de,xs),e(xs,vi),e(de,bi),e(D,$i),$(Ne,D,null),e(D,Oi),$(Se,D,null),e(F,xi),e(F,W),$(Ut,W,null),e(W,Vi),e(W,pe),e(pe,yi),e(pe,Do),e(Do,ji),e(pe,ki),e(pe,Vs),e(Vs,zi),e(pe,Pi),e(W,Ei),$(Be,W,null),e(W,Mi),$(Re,W,null),_(t,fn,u),_(t,me,u),e(me,He),e(He,ys),$(Gt,ys,null),e(me,Ci),e(me,js),e(js,qi),_(t,gn,u),_(t,he,u),$(Xt,he,null),e(he,Ii),e(he,N),$(Zt,N,null),e(N,Li),e(N,fe),e(fe,Fi),e(fe,Wo),e(Wo,Ai),e(fe,Di),e(fe,ks),e(ks,Wi),e(fe,Ni),e(N,Si),$(Ue,N,null),e(N,Bi),$(Ge,N,null),_(t,un,u),_(t,ge,u),e(ge,Xe),e(Xe,zs),$(Jt,zs,null),e(ge,Ri),e(ge,Ps),e(Ps,Hi),_(t,_n,u),_(t,ue,u),$(Kt,ue,null),e(ue,Ui),e(ue,S),$(Yt,S,null),e(S,Gi),e(S,_e),e(_e,Xi),e(_e,No),e(No,Zi),e(_e,Ji),e(_e,Es),e(Es,Ki),e(_e,Yi),e(S,Qi),$(Ze,S,null),e(S,el),$(Je,S,null),_(t,wn,u),_(t,we,u),e(we,Ke),e(Ke,Ms),$(Qt,Ms,null),e(we,tl),e(we,Cs),e(Cs,ol),_(t,Tn,u),_(t,Te,u),$(eo,Te,null),e(Te,sl),e(Te,B),$(to,B,null),e(B,nl),e(B,ve),e(ve,rl),e(ve,So),e(So,al),e(ve,il),e(ve,qs),e(qs,ll),e(ve,cl),e(B,dl),$(Ye,B,null),e(B,pl),$(Qe,B,null),vn=!0},p(t,[u]){const oo={};u&2&&(oo.$$scope={dirty:u,ctx:t}),ze.$set(oo);const Is={};u&2&&(Is.$$scope={dirty:u,ctx:t}),Ee.$set(Is);const Ls={};u&2&&(Ls.$$scope={dirty:u,ctx:t}),Ce.$set(Ls);const Fs={};u&2&&(Fs.$$scope={dirty:u,ctx:t}),De.$set(Fs);const so={};u&2&&(so.$$scope={dirty:u,ctx:t}),We.$set(so);const As={};u&2&&(As.$$scope={dirty:u,ctx:t}),Ne.$set(As);const Ds={};u&2&&(Ds.$$scope={dirty:u,ctx:t}),Se.$set(Ds);const Ws={};u&2&&(Ws.$$scope={dirty:u,ctx:t}),Be.$set(Ws);const no={};u&2&&(no.$$scope={dirty:u,ctx:t}),Re.$set(no);const Ns={};u&2&&(Ns.$$scope={dirty:u,ctx:t}),Ue.$set(Ns);const Ss={};u&2&&(Ss.$$scope={dirty:u,ctx:t}),Ge.$set(Ss);const Bs={};u&2&&(Bs.$$scope={dirty:u,ctx:t}),Ze.$set(Bs);const Rs={};u&2&&(Rs.$$scope={dirty:u,ctx:t}),Je.$set(Rs);const ro={};u&2&&(ro.$$scope={dirty:u,ctx:t}),Ye.$set(ro);const Hs={};u&2&&(Hs.$$scope={dirty:u,ctx:t}),Qe.$set(Hs)},i(t){vn||(O(s.$$.fragment,t),O(ht.$$.fragment,t),O(gt.$$.fragment,t),O(ut.$$.fragment,t),O(Tt.$$.fragment,t),O(vt.$$.fragment,t),O($t.$$.fragment,t),O(xt.$$.fragment,t),O(Vt.$$.fragment,t),O(ze.$$.fragment,t),O(jt.$$.fragment,t),O(kt.$$.fragment,t),O(Ee.$$.fragment,t),O(Pt.$$.fragment,t),O(Et.$$.fragment,t),O(Ct.$$.fragment,t),O(Ce.$$.fragment,t),O(qt.$$.fragment,t),O(It.$$.fragment,t),O(Lt.$$.fragment,t),O(At.$$.fragment,t),O(Wt.$$.fragment,t),O(St.$$.fragment,t),O(Bt.$$.fragment,t),O(Rt.$$.fragment,t),O(De.$$.fragment,t),O(We.$$.fragment,t),O(Ht.$$.fragment,t),O(Ne.$$.fragment,t),O(Se.$$.fragment,t),O(Ut.$$.fragment,t),O(Be.$$.fragment,t),O(Re.$$.fragment,t),O(Gt.$$.fragment,t),O(Xt.$$.fragment,t),O(Zt.$$.fragment,t),O(Ue.$$.fragment,t),O(Ge.$$.fragment,t),O(Jt.$$.fragment,t),O(Kt.$$.fragment,t),O(Yt.$$.fragment,t),O(Ze.$$.fragment,t),O(Je.$$.fragment,t),O(Qt.$$.fragment,t),O(eo.$$.fragment,t),O(to.$$.fragment,t),O(Ye.$$.fragment,t),O(Qe.$$.fragment,t),vn=!0)},o(t){x(s.$$.fragment,t),x(ht.$$.fragment,t),x(gt.$$.fragment,t),x(ut.$$.fragment,t),x(Tt.$$.fragment,t),x(vt.$$.fragment,t),x($t.$$.fragment,t),x(xt.$$.fragment,t),x(Vt.$$.fragment,t),x(ze.$$.fragment,t),x(jt.$$.fragment,t),x(kt.$$.fragment,t),x(Ee.$$.fragment,t),x(Pt.$$.fragment,t),x(Et.$$.fragment,t),x(Ct.$$.fragment,t),x(Ce.$$.fragment,t),x(qt.$$.fragment,t),x(It.$$.fragment,t),x(Lt.$$.fragment,t),x(At.$$.fragment,t),x(Wt.$$.fragment,t),x(St.$$.fragment,t),x(Bt.$$.fragment,t),x(Rt.$$.fragment,t),x(De.$$.fragment,t),x(We.$$.fragment,t),x(Ht.$$.fragment,t),x(Ne.$$.fragment,t),x(Se.$$.fragment,t),x(Ut.$$.fragment,t),x(Be.$$.fragment,t),x(Re.$$.fragment,t),x(Gt.$$.fragment,t),x(Xt.$$.fragment,t),x(Zt.$$.fragment,t),x(Ue.$$.fragment,t),x(Ge.$$.fragment,t),x(Jt.$$.fragment,t),x(Kt.$$.fragment,t),x(Yt.$$.fragment,t),x(Ze.$$.fragment,t),x(Je.$$.fragment,t),x(Qt.$$.fragment,t),x(eo.$$.fragment,t),x(to.$$.fragment,t),x(Ye.$$.fragment,t),x(Qe.$$.fragment,t),vn=!1},d(t){o(c),t&&o(T),t&&o(g),V(s),t&&o(Us),t&&o(J),V(ht),t&&o(Gs),t&&o(Oe),t&&o(Xs),t&&o(lo),t&&o(Zs),t&&o(co),t&&o(Js),t&&o(K),V(gt),t&&o(Ks),t&&o(Ve),t&&o(Ys),t&&o(z),t&&o(Qs),V(ut,t),t&&o(en),t&&o(U),t&&o(tn),t&&o(Y),V(Tt),t&&o(on),t&&o(C),V(vt),V($t),t&&o(sn),t&&o(ee),V(xt),t&&o(nn),t&&o(q),V(Vt),V(ze),t&&o(rn),t&&o(se),V(jt),t&&o(an),t&&o(I),V(kt),V(Ee),t&&o(ln),t&&o(ae),V(Pt),t&&o(cn),t&&o(L),V(Et),V(Ct),V(Ce),t&&o(dn),t&&o(ie),V(qt),t&&o(pn),t&&o(P),V(It),V(Lt),V(At),V(Wt),t&&o(mn),t&&o(le),V(St),t&&o(hn),t&&o(F),V(Bt),V(Rt),V(De),V(We),V(Ht),V(Ne),V(Se),V(Ut),V(Be),V(Re),t&&o(fn),t&&o(me),V(Gt),t&&o(gn),t&&o(he),V(Xt),V(Zt),V(Ue),V(Ge),t&&o(un),t&&o(ge),V(Jt),t&&o(_n),t&&o(ue),V(Kt),V(Yt),V(Ze),V(Je),t&&o(wn),t&&o(we),V(Qt),t&&o(Tn),t&&o(Te),V(eo),V(to),V(Ye),V(Qe)}}}const td={local:"owlvit",sections:[{local:"overview",title:"Overview"},{local:"usage",title:"Usage"},{local:"transformers.OwlViTConfig",title:"OwlViTConfig"},{local:"transformers.OwlViTTextConfig",title:"OwlViTTextConfig"},{local:"transformers.OwlViTVisionConfig",title:"OwlViTVisionConfig"},{local:"transformers.OwlViTFeatureExtractor",title:"OwlViTFeatureExtractor"},{local:"transformers.OwlViTProcessor",title:"OwlViTProcessor"},{local:"transformers.OwlViTModel",title:"OwlViTModel"},{local:"transformers.OwlViTTextModel",title:"OwlViTTextModel"},{local:"transformers.OwlViTVisionModel",title:"OwlViTVisionModel"},{local:"transformers.OwlViTForObjectDetection",title:"OwlViTForObjectDetection"}],title:"OWL-ViT"};function od(y){return Ac(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class cd extends qc{constructor(c){super();Ic(this,c,od,ed,Lc,{})}}export{cd as default,td as metadata};
