import{S as Pc,i as Ec,s as Mc,e as r,k as h,w as v,t as i,M as Cc,c as n,d as o,m as f,a,x as b,h as l,b as d,G as e,g as _,y as $,q as O,o as x,B as V,v as qc,L as mt}from"../../chunks/vendor-hf-doc-builder.js";import{T as no}from"../../chunks/Tip-hf-doc-builder.js";import{D as k}from"../../chunks/Docstring-hf-doc-builder.js";import{C as ve}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as R}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as pt}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Ic(y){let c,T,g,p,w;return p=new ve({props:{code:`from transformers import OwlViTTextConfig, OwlViTTextModel

# Initializing a OwlViTTextModel with google/owlvit-base-patch32 style configuration
configuration = OwlViTTextConfig()

# Initializing a OwlViTTextConfig from the google/owlvit-base-patch32 style configuration
model = OwlViTTextModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTTextConfig, OwlViTTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTTextModel with google/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = OwlViTTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTTextConfig from the google/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){c=r("p"),T=i("Example:"),g=h(),v(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Example:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Fc(y){let c,T,g,p,w;return p=new ve({props:{code:`from transformers import OwlViTVisionConfig, OwlViTVisionModel

# Initializing a OwlViTVisionModel with google/owlvit-base-patch32 style configuration
configuration = OwlViTVisionConfig()

# Initializing a OwlViTVisionModel model from the google/owlvit-base-patch32 style configuration
model = OwlViTVisionModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTVisionConfig, OwlViTVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTVisionModel with google/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = OwlViTVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a OwlViTVisionModel model from the google/owlvit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){c=r("p"),T=i("Example:"),g=h(),v(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Example:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Lc(y){let c,T;return{c(){c=r("p"),T=i(`NumPy arrays and PyTorch tensors are converted to PIL images when resizing, so the most efficient is to pass
PIL images.`)},l(g){c=n(g,"P",{});var p=a(c);T=l(p,`NumPy arrays and PyTorch tensors are converted to PIL images when resizing, so the most efficient is to pass
PIL images.`),p.forEach(o)},m(g,p){_(g,c,p),e(c,T)},d(g){g&&o(c)}}}function Ac(y){let c,T,g,p,w;return{c(){c=r("p"),T=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,T),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Dc(y){let c,T,g,p,w;return p=new ve({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(text=[["a photo of a cat", "a photo of a dog"]], images=image, return_tensors="pt")
outputs = model(**inputs)
logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`}}),{c(){c=r("p"),T=i("Examples:"),g=h(),v(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Examples:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Wc(y){let c,T,g,p,w;return{c(){c=r("p"),T=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,T),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Nc(y){let c,T,g,p,w;return p=new ve({props:{code:`from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
inputs = processor(
    text=[["a photo of a cat", "a photo of a dog"], ["photo of a astranaut"]], return_tensors="pt"
)
text_features = model.get_text_features(**inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], [<span class="hljs-string">&quot;photo of a astranaut&quot;</span>]], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`}}),{c(){c=r("p"),T=i("Examples:"),g=h(),v(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Examples:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Sc(y){let c,T,g,p,w;return{c(){c=r("p"),T=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,T),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Bc(y){let c,T,g,p,w;return p=new ve({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTModel

model = OwlViTModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(images=image, return_tensors="pt")
image_features = model.get_image_features(**inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`}}),{c(){c=r("p"),T=i("Examples:"),g=h(),v(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Examples:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Hc(y){let c,T,g,p,w;return{c(){c=r("p"),T=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,T),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Rc(y){let c,T,g,p,w;return p=new ve({props:{code:`from transformers import OwlViTProcessor, OwlViTTextModel

model = OwlViTTextModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
inputs = processor(
    text=[["a photo of a cat", "a photo of a dog"], ["photo of a astranaut"]], return_tensors="pt"
)
outputs = model(**inputs)
last_hidden_state = outputs.last_hidden_state
pooled_output = outputs.pooler_output  # pooled (EOS token) states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTTextModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], [<span class="hljs-string">&quot;photo of a astranaut&quot;</span>]], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled (EOS token) states</span>`}}),{c(){c=r("p"),T=i("Examples:"),g=h(),v(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Examples:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Uc(y){let c,T,g,p,w;return{c(){c=r("p"),T=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,T),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Gc(y){let c,T,g,p,w;return p=new ve({props:{code:`from PIL import Image
import requests
from transformers import OwlViTProcessor, OwlViTVisionModel

model = OwlViTVisionModel.from_pretrained("google/owlvit-base-patch32")
processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, return_tensors="pt")

outputs = model(**inputs)
last_hidden_state = outputs.last_hidden_state
pooled_output = outputs.pooler_output  # pooled CLS states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTVisionModel.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled CLS states</span>`}}),{c(){c=r("p"),T=i("Examples:"),g=h(),v(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Examples:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Xc(y){let c,T,g,p,w;return{c(){c=r("p"),T=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),p=i("Module"),w=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(m,"CODE",{});var j=a(g);p=l(j,"Module"),j.forEach(o),w=l(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(s,m){_(s,c,m),e(c,T),e(c,g),e(g,p),e(c,w)},d(s){s&&o(c)}}}function Zc(y){let c,T,g,p,w;return p=new ve({props:{code:`import requests
from PIL import Image
import torch
from transformers import OwlViTProcessor, OwlViTForObjectDetection

processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch32")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
texts = [["a photo of a cat", "a photo of a dog"]]
inputs = processor(text=texts, images=image, return_tensors="pt")
outputs = model(**inputs)

# Target image sizes (height, width) to rescale box predictions [batch_size, 2]
target_sizes = torch.Tensor([image.size[::-1]])
# Convert outputs (bounding boxes and class logits) to COCO API
results = processor.post_process(outputs=outputs, target_sizes=target_sizes)

i = 0  # Retrieve predictions for the first image for the corresponding text queries
text = texts[i]
boxes, scores, labels = results[i]["boxes"], results[i]["scores"], results[i]["labels"]

score_threshold = 0.1
for box, score, label in zip(boxes, scores, labels):
    box = [round(i, 2) for i in box.tolist()]
    if score >= score_threshold:
        print(f"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTForObjectDetection.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>texts = [[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=texts, images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Target image sizes (height, width) to rescale box predictions [batch_size, 2]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_sizes = torch.Tensor([image.size[::-<span class="hljs-number">1</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Convert outputs (bounding boxes and class logits) to COCO API</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>results = processor.post_process(outputs=outputs, target_sizes=target_sizes)

<span class="hljs-meta">&gt;&gt;&gt; </span>i = <span class="hljs-number">0</span>  <span class="hljs-comment"># Retrieve predictions for the first image for the corresponding text queries</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = texts[i]
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes, scores, labels = results[i][<span class="hljs-string">&quot;boxes&quot;</span>], results[i][<span class="hljs-string">&quot;scores&quot;</span>], results[i][<span class="hljs-string">&quot;labels&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>score_threshold = <span class="hljs-number">0.1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> box, score, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(boxes, scores, labels):
<span class="hljs-meta">... </span>    box = [<span class="hljs-built_in">round</span>(i, <span class="hljs-number">2</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> box.tolist()]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">if</span> score &gt;= score_threshold:
<span class="hljs-meta">... </span>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Detected <span class="hljs-subst">{text[label]}</span> with confidence <span class="hljs-subst">{<span class="hljs-built_in">round</span>(score.item(), <span class="hljs-number">3</span>)}</span> at location <span class="hljs-subst">{box}</span>&quot;</span>)
Detected a photo of a cat <span class="hljs-keyword">with</span> confidence <span class="hljs-number">0.707</span> at location [<span class="hljs-number">324.97</span>, <span class="hljs-number">20.44</span>, <span class="hljs-number">640.58</span>, <span class="hljs-number">373.29</span>]
Detected a photo of a cat <span class="hljs-keyword">with</span> confidence <span class="hljs-number">0.717</span> at location [<span class="hljs-number">1.46</span>, <span class="hljs-number">55.26</span>, <span class="hljs-number">315.55</span>, <span class="hljs-number">472.17</span>]`}}),{c(){c=r("p"),T=i("Examples:"),g=h(),v(p.$$.fragment)},l(s){c=n(s,"P",{});var m=a(c);T=l(m,"Examples:"),m.forEach(o),g=f(s),b(p.$$.fragment,s)},m(s,m){_(s,c,m),e(c,T),_(s,g,m),$(p,s,m),w=!0},p:mt,i(s){w||(O(p.$$.fragment,s),w=!0)},o(s){x(p.$$.fragment,s),w=!1},d(s){s&&o(c),s&&o(g),V(p,s)}}}function Jc(y){let c,T,g,p,w,s,m,j,Nr,Rs,Z,be,es,ht,Sr,ts,Br,Us,$e,Hr,ft,Rr,Ur,Gs,ao,Gr,Xs,io,os,Xr,Zs,J,Oe,ss,gt,Zr,rs,Jr,Js,xe,Kr,lo,Yr,Qr,Ks,z,co,en,tn,po,on,sn,mo,rn,nn,ho,an,ln,fo,cn,dn,go,pn,mn,uo,hn,fn,Ys,ut,Qs,U,gn,_t,un,_n,wt,wn,Tn,er,K,Ve,ns,Tt,vn,as,bn,tr,C,vt,$n,ye,_o,On,xn,wo,Vn,yn,jn,Y,kn,To,zn,Pn,vo,En,Mn,Cn,je,bt,qn,$t,In,bo,Fn,Ln,or,Q,ke,is,Ot,An,ls,Dn,sr,q,xt,Wn,ee,Nn,$o,Sn,Bn,Vt,Hn,Rn,Un,te,Gn,Oo,Xn,Zn,xo,Jn,Kn,Yn,ze,rr,oe,Pe,cs,yt,Qn,ds,ea,nr,I,jt,ta,se,oa,Vo,sa,ra,kt,na,aa,ia,re,la,yo,ca,da,jo,pa,ma,ha,Ee,ar,ne,Me,ps,zt,fa,ms,ga,ir,F,Pt,ua,hs,_a,wa,Et,Ta,ko,va,ba,$a,G,Mt,Oa,fs,xa,Va,Ce,lr,ae,qe,gs,Ct,ya,us,ja,cr,P,qt,ka,E,za,zo,Pa,Ea,Po,Ma,Ca,Eo,qa,Ia,_s,Fa,La,Mo,Aa,Da,Wa,Ie,It,Na,Ft,Sa,Co,Ba,Ha,Ra,Fe,Lt,Ua,At,Ga,qo,Xa,Za,Ja,Le,Dt,Ka,Wt,Ya,ws,Qa,ei,dr,ie,Ae,Ts,Nt,ti,vs,oi,pr,L,St,si,A,Bt,ri,le,ni,Io,ai,ii,bs,li,ci,di,De,pi,We,mi,D,Ht,hi,ce,fi,Fo,gi,ui,$s,_i,wi,Ti,Ne,vi,Se,bi,W,Rt,$i,de,Oi,Lo,xi,Vi,Os,yi,ji,ki,Be,zi,He,mr,pe,Re,xs,Ut,Pi,Vs,Ei,hr,me,Gt,Mi,N,Xt,Ci,he,qi,Ao,Ii,Fi,ys,Li,Ai,Di,Ue,Wi,Ge,fr,fe,Xe,js,Zt,Ni,ks,Si,gr,ge,Jt,Bi,S,Kt,Hi,ue,Ri,Do,Ui,Gi,zs,Xi,Zi,Ji,Ze,Ki,Je,ur,_e,Ke,Ps,Yt,Yi,Es,Qi,_r,we,Qt,el,B,eo,tl,Te,ol,Wo,sl,rl,Ms,nl,al,il,Ye,ll,Qe,wr;return s=new R({}),ht=new R({}),gt=new R({}),ut=new ve({props:{code:`import requests
from PIL import Image
import torch

from transformers import OwlViTProcessor, OwlViTForObjectDetection

processor = OwlViTProcessor.from_pretrained("google/owlvit-base-patch32")
model = OwlViTForObjectDetection.from_pretrained("google/owlvit-base-patch32")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
texts = [["a photo of a cat", "a photo of a dog"]]
inputs = processor(text=texts, images=image, return_tensors="pt")
outputs = model(**inputs)

# Target image sizes (height, width) to rescale box predictions [batch_size, 2]
target_sizes = torch.Tensor([image.size[::-1]])
# Convert outputs (bounding boxes and class logits) to COCO API
results = processor.post_process(outputs=outputs, target_sizes=target_sizes)

i = 0  # Retrieve predictions for the first image for the corresponding text queries
text = texts[i]
boxes, scores, labels = results[i]["boxes"], results[i]["scores"], results[i]["labels"]

score_threshold = 0.1
for box, score, label in zip(boxes, scores, labels):
    box = [round(i, 2) for i in box.tolist()]
    if score >= score_threshold:
        print(f"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OwlViTProcessor, OwlViTForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = OwlViTProcessor.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OwlViTForObjectDetection.from_pretrained(<span class="hljs-string">&quot;google/owlvit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>texts = [[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=texts, images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Target image sizes (height, width) to rescale box predictions [batch_size, 2]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_sizes = torch.Tensor([image.size[::-<span class="hljs-number">1</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Convert outputs (bounding boxes and class logits) to COCO API</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>results = processor.post_process(outputs=outputs, target_sizes=target_sizes)

<span class="hljs-meta">&gt;&gt;&gt; </span>i = <span class="hljs-number">0</span>  <span class="hljs-comment"># Retrieve predictions for the first image for the corresponding text queries</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = texts[i]
<span class="hljs-meta">&gt;&gt;&gt; </span>boxes, scores, labels = results[i][<span class="hljs-string">&quot;boxes&quot;</span>], results[i][<span class="hljs-string">&quot;scores&quot;</span>], results[i][<span class="hljs-string">&quot;labels&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>score_threshold = <span class="hljs-number">0.1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> box, score, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(boxes, scores, labels):
<span class="hljs-meta">... </span>    box = [<span class="hljs-built_in">round</span>(i, <span class="hljs-number">2</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> box.tolist()]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">if</span> score &gt;= score_threshold:
<span class="hljs-meta">... </span>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Detected <span class="hljs-subst">{text[label]}</span> with confidence <span class="hljs-subst">{<span class="hljs-built_in">round</span>(score.item(), <span class="hljs-number">3</span>)}</span> at location <span class="hljs-subst">{box}</span>&quot;</span>)
Detected a photo of a cat <span class="hljs-keyword">with</span> confidence <span class="hljs-number">0.707</span> at location [<span class="hljs-number">324.97</span>, <span class="hljs-number">20.44</span>, <span class="hljs-number">640.58</span>, <span class="hljs-number">373.29</span>]
Detected a photo of a cat <span class="hljs-keyword">with</span> confidence <span class="hljs-number">0.717</span> at location [<span class="hljs-number">1.46</span>, <span class="hljs-number">55.26</span>, <span class="hljs-number">315.55</span>, <span class="hljs-number">472.17</span>]`}}),Tt=new R({}),vt=new k({props:{name:"class transformers.OwlViTConfig",anchor:"transformers.OwlViTConfig",parameters:[{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"projection_dim",val:" = 512"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"return_dict",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTConfig.text_config_dict",description:`<strong>text_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextConfig">OwlViTTextConfig</a>.`,name:"text_config_dict"},{anchor:"transformers.OwlViTConfig.vision_config_dict",description:`<strong>vision_config_dict</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionConfig">OwlViTVisionConfig</a>.`,name:"vision_config_dict"},{anchor:"transformers.OwlViTConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.OwlViTConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> parameter. Default is used as per the original OWL-ViT
implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.OwlViTConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/configuration_owlvit.py#L252"}}),bt=new k({props:{name:"from_text_vision_configs",anchor:"transformers.OwlViTConfig.from_text_vision_configs",parameters:[{name:"text_config",val:": typing.Dict"},{name:"vision_config",val:": typing.Dict"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/configuration_owlvit.py#L317",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTConfig"
>OwlViTConfig</a></p>
`}}),Ot=new R({}),xt=new k({props:{name:"class transformers.OwlViTTextConfig",anchor:"transformers.OwlViTTextConfig",parameters:[{name:"vocab_size",val:" = 49408"},{name:"hidden_size",val:" = 512"},{name:"intermediate_size",val:" = 2048"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 8"},{name:"max_position_embeddings",val:" = 16"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 49406"},{name:"eos_token_id",val:" = 49407"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 49408) &#x2014;
Vocabulary size of the OWL-ViT text model. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel">OwlViTTextModel</a>.`,name:"vocab_size"},{anchor:"transformers.OwlViTTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.OwlViTTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.OwlViTTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.OwlViTTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.OwlViTTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.OwlViTTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported. layer_norm_eps (<code>float</code>, <em>optional</em>,
defaults to 1e-5): The epsilon used by the layer normalization layers.`,name:"hidden_act"},{anchor:"transformers.OwlViTTextConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.OwlViTTextConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.OwlViTTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.OwlViTTextConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/configuration_owlvit.py#L41"}}),ze=new pt({props:{anchor:"transformers.OwlViTTextConfig.example",$$slots:{default:[Ic]},$$scope:{ctx:y}}}),yt=new R({}),jt=new k({props:{name:"class transformers.OwlViTVisionConfig",anchor:"transformers.OwlViTVisionConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"intermediate_size",val:" = 3072"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"image_size",val:" = 768"},{name:"patch_size",val:" = 32"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.OwlViTVisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.OwlViTVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.OwlViTVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.OwlViTVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.OwlViTVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.OwlViTVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported. layer_norm_eps (<code>float</code>, <em>optional</em>,
defaults to 1e-5): The epsilon used by the layer normalization layers.`,name:"hidden_act"},{anchor:"transformers.OwlViTVisionConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.OwlViTVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.OwlViTVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.OwlViTVisionConfig.initializer_factor",description:`<strong>initializer_factor</strong> (\`float&#x201C;, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/configuration_owlvit.py#L149"}}),Ee=new pt({props:{anchor:"transformers.OwlViTVisionConfig.example",$$slots:{default:[Fc]},$$scope:{ctx:y}}}),zt=new R({}),Pt=new k({props:{name:"class transformers.OwlViTFeatureExtractor",anchor:"transformers.OwlViTFeatureExtractor",parameters:[{name:"do_resize",val:" = True"},{name:"size",val:" = (768, 768)"},{name:"resample",val:" = <Resampling.BICUBIC: 3>"},{name:"crop_size",val:" = 768"},{name:"do_center_crop",val:" = False"},{name:"do_normalize",val:" = True"},{name:"image_mean",val:" = None"},{name:"image_std",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTFeatureExtractor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the shorter edge of the input to a certain <code>size</code>.`,name:"do_resize"},{anchor:"transformers.OwlViTFeatureExtractor.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple[int, int]</code>, <em>optional</em>, defaults to (768, 768)) &#x2014;
The size to use for resizing the image. Only has an effect if <code>do_resize</code> is set to <code>True</code>. If <code>size</code> is a
sequence like (h, w), output size will be matched to this. If <code>size</code> is an int, then image will be resized
to (size, size).`,name:"size"},{anchor:"transformers.OwlViTFeatureExtractor.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.BICUBIC</code>) &#x2014;
An optional resampling filter. This can be one of <code>PIL.Image.NEAREST</code>, <code>PIL.Image.BOX</code>,
<code>PIL.Image.BILINEAR</code>, <code>PIL.Image.HAMMING</code>, <code>PIL.Image.BICUBIC</code> or <code>PIL.Image.LANCZOS</code>. Only has an effect
if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.OwlViTFeatureExtractor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to crop the input at the center. If the input size is smaller than <code>crop_size</code> along any edge, the
image is padded with 0&#x2019;s and then center cropped.`,name:"do_center_crop"},{anchor:"transformers.OwlViTFeatureExtractor.crop_size",description:"<strong>crop_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;",name:"crop_size"},{anchor:"transformers.OwlViTFeatureExtractor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input with <code>image_mean</code> and <code>image_std</code>. Desired output size when applying
center-cropping. Only has an effect if <code>do_center_crop</code> is set to <code>True</code>.`,name:"do_normalize"},{anchor:"transformers.OwlViTFeatureExtractor.image_mean",description:`<strong>image_mean</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[0.48145466, 0.4578275, 0.40821073]</code>) &#x2014;
The sequence of means for each channel, to be used when normalizing images.`,name:"image_mean"},{anchor:"transformers.OwlViTFeatureExtractor.image_std",description:`<strong>image_std</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[0.26862954, 0.26130258, 0.27577711]</code>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images.`,name:"image_std"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/feature_extraction_owlvit.py#L43"}}),Mt=new k({props:{name:"__call__",anchor:"transformers.OwlViTFeatureExtractor.__call__",parameters:[{name:"images",val:": typing.Union[PIL.Image.Image, numpy.ndarray, ForwardRef('torch.Tensor'), typing.List[PIL.Image.Image], typing.List[numpy.ndarray], typing.List[ForwardRef('torch.Tensor')]]"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OwlViTFeatureExtractor.__call__.images",description:`<strong>images</strong> (<code>PIL.Image.Image</code>, <code>np.ndarray</code>, <code>torch.Tensor</code>, <code>List[PIL.Image.Image]</code>, <code>List[np.ndarray]</code>, <code>List[torch.Tensor]</code>) &#x2014;
The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch
tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W) or (H, W, C),
where C is a number of channels, H and W are image height and width.`,name:"images"},{anchor:"transformers.OwlViTFeatureExtractor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>, defaults to <code>&apos;np&apos;</code>) &#x2014;
If set, will return tensors of a particular framework. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return NumPy <code>np.ndarray</code> objects.</li>
<li><code>&apos;jax&apos;</code>: Return JAX <code>jnp.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/feature_extraction_owlvit.py#L143",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a> with the following fields:</p>
<ul>
<li><strong>pixel_values</strong> \u2014 Pixel values to be fed to a model.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),Ce=new no({props:{warning:!0,$$slots:{default:[Lc]},$$scope:{ctx:y}}}),Ct=new R({}),qt=new k({props:{name:"class transformers.OwlViTProcessor",anchor:"transformers.OwlViTProcessor",parameters:[{name:"feature_extractor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.OwlViTProcessor.feature_extractor",description:`<strong>feature_extractor</strong> (<a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor">OwlViTFeatureExtractor</a>) &#x2014;
The feature extractor is a required input.`,name:"feature_extractor"},{anchor:"transformers.OwlViTProcessor.tokenizer",description:`<strong>tokenizer</strong> ([<code>CLIPTokenizer</code>, <code>CLIPTokenizerFast</code>]) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/processing_owlvit.py#L28"}}),It=new k({props:{name:"batch_decode",anchor:"transformers.OwlViTProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/processing_owlvit.py#L149"}}),Lt=new k({props:{name:"decode",anchor:"transformers.OwlViTProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/processing_owlvit.py#L156"}}),Dt=new k({props:{name:"post_process",anchor:"transformers.OwlViTProcessor.post_process",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/processing_owlvit.py#L142"}}),Nt=new R({}),St=new k({props:{name:"class transformers.OwlViTModel",anchor:"transformers.OwlViTModel",parameters:[{name:"config",val:": OwlViTConfig"}],parametersDescription:[{anchor:"transformers.OwlViTModel.This",description:`<strong>This</strong> model is a PyTorch [torch.nn.Module](https &#x2014;
//pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it`,name:"This"},{anchor:"transformers.OwlViTModel.as",description:`<strong>as</strong> a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and &#x2014;
behavior. &#x2014;
config (<a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTConfig">OwlViTConfig</a>): Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"as"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L879"}}),Bt=new k({props:{name:"forward",anchor:"transformers.OwlViTModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"return_loss",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_base_image_embeds",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTModel.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.OwlViTModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.forward.return_base_image_embeds",description:`<strong>return_base_image_embeds</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return unprojected image embeddings. Set to <code>True</code> when <code>OwlViTModel</code> is called within
<code>OwlViTForObjectDetection</code>.`,name:"return_base_image_embeds"},{anchor:"transformers.OwlViTModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L1009",returnDescription:`
<p>A <code>transformers.models.owlvit.modeling_owlvit.OwlViTOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) \u2014 Contrastive loss for image-text similarity.</li>
<li><strong>logits_per_image</strong> (<code>torch.FloatTensor</code> of shape <code>(image_batch_size, text_batch_size)</code>) \u2014 The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text</strong> (<code>torch.FloatTensor</code> of shape <code>(text_batch_size, image_batch_size)</code>) \u2014 The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size * num_max_text_queries, output_dim</code>) \u2014 The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>) \u2014 The image embeddings obtained by applying the projection layer to the pooled output of
<a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</li>
<li><strong>text_model_output</strong> (Tuple<code>BaseModelOutputWithPooling</code>) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>vision_model_output</strong> (<code>BaseModelOutputWithPooling</code>) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</li>
</ul>
`,returnType:`
<p><code>transformers.models.owlvit.modeling_owlvit.OwlViTOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),De=new no({props:{$$slots:{default:[Ac]},$$scope:{ctx:y}}}),We=new pt({props:{anchor:"transformers.OwlViTModel.forward.example",$$slots:{default:[Dc]},$$scope:{ctx:y}}}),Ht=new k({props:{name:"get_text_features",anchor:"transformers.OwlViTModel.get_text_features",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L914",returnDescription:`
<p>The text embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</p>
`,returnType:`
<p>text_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),Ne=new no({props:{$$slots:{default:[Wc]},$$scope:{ctx:y}}}),Se=new pt({props:{anchor:"transformers.OwlViTModel.get_text_features.example",$$slots:{default:[Nc]},$$scope:{ctx:y}}}),Rt=new k({props:{name:"get_image_features",anchor:"transformers.OwlViTModel.get_image_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"return_projected",val:": typing.Optional[bool] = True"}],parametersDescription:[{anchor:"transformers.OwlViTModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L959",returnDescription:`
<p>The image embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>.</p>
`,returnType:`
<p>image_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),Be=new no({props:{$$slots:{default:[Sc]},$$scope:{ctx:y}}}),He=new pt({props:{anchor:"transformers.OwlViTModel.get_image_features.example",$$slots:{default:[Bc]},$$scope:{ctx:y}}}),Ut=new R({}),Gt=new k({props:{name:"class transformers.OwlViTTextModel",anchor:"transformers.OwlViTTextModel",parameters:[{name:"config",val:": OwlViTTextConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L718"}}),Xt=new k({props:{name:"forward",anchor:"transformers.OwlViTTextModel.forward",parameters:[{name:"input_ids",val:": Tensor"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OwlViTTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L733",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ue=new no({props:{$$slots:{default:[Hc]},$$scope:{ctx:y}}}),Ge=new pt({props:{anchor:"transformers.OwlViTTextModel.forward.example",$$slots:{default:[Rc]},$$scope:{ctx:y}}}),Zt=new R({}),Jt=new k({props:{name:"class transformers.OwlViTVisionModel",anchor:"transformers.OwlViTVisionModel",parameters:[{name:"config",val:": OwlViTVisionConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L828"}}),Kt=new k({props:{name:"forward",anchor:"transformers.OwlViTVisionModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTVisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OwlViTVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OwlViTVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L841",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTVisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ze=new no({props:{$$slots:{default:[Uc]},$$scope:{ctx:y}}}),Je=new pt({props:{anchor:"transformers.OwlViTVisionModel.forward.example",$$slots:{default:[Gc]},$$scope:{ctx:y}}}),Yt=new R({}),Qt=new k({props:{name:"class transformers.OwlViTForObjectDetection",anchor:"transformers.OwlViTForObjectDetection",parameters:[{name:"config",val:": OwlViTConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L1168"}}),eo=new k({props:{name:"forward",anchor:"transformers.OwlViTForObjectDetection.forward",parameters:[{name:"input_ids",val:": Tensor"},{name:"pixel_values",val:": FloatTensor"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.OwlViTForObjectDetection.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values.`,name:"pixel_values"},{anchor:"transformers.OwlViTForObjectDetection.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size * num_max_text_queries, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.OwlViTForObjectDetection.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, num_max_text_queries, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/owlvit/modeling_owlvit.py#L1304",returnDescription:`
<p>A <code>transformers.models.owlvit.modeling_owlvit.OwlViTObjectDetectionOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.owlvit.configuration_owlvit.OwlViTConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> are provided)) \u2014 Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a
bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized
scale-invariant IoU loss.</li>
<li><strong>loss_dict</strong> (<code>Dict</code>, <em>optional</em>) \u2014 A dictionary containing the individual losses. Useful for logging.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches, num_queries)</code>) \u2014 Classification logits (including no-object) for all queries.</li>
<li><strong>pred_boxes</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches, 4)</code>) \u2014 Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These
values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding
possible padding). You can use <code>post_process()</code> to retrieve the unnormalized
bounding boxes.</li>
<li><strong>text_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_max_text_queries, output_dim</code>) \u2014 The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, patch_size, patch_size, output_dim</code>) \u2014 Pooled output of <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>. OWL-ViT represents images as a set of image patches and computes
image embeddings for each patch.</li>
<li><strong>class_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches, hidden_size)</code>) \u2014 Class embeddings of all image patches. OWL-ViT represents images as a set of image patches where the total
number of patches is (image_size / patch_size)**2.</li>
<li><strong>text_model_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>)) \u2014 Last hidden states extracted from the <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"
>OwlViTTextModel</a>.</li>
<li><strong>vision_model_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches + 1, hidden_size)</code>)) \u2014 Last hidden states extracted from the <a
  href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"
>OwlViTVisionModel</a>. OWL-ViT represents images as a set of image
patches where the total number of patches is (image_size / patch_size)**2.</li>
</ul>
`,returnType:`
<p><code>transformers.models.owlvit.modeling_owlvit.OwlViTObjectDetectionOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ye=new no({props:{$$slots:{default:[Xc]},$$scope:{ctx:y}}}),Qe=new pt({props:{anchor:"transformers.OwlViTForObjectDetection.forward.example",$$slots:{default:[Zc]},$$scope:{ctx:y}}}),{c(){c=r("meta"),T=h(),g=r("h1"),p=r("a"),w=r("span"),v(s.$$.fragment),m=h(),j=r("span"),Nr=i("OWL-ViT"),Rs=h(),Z=r("h2"),be=r("a"),es=r("span"),v(ht.$$.fragment),Sr=h(),ts=r("span"),Br=i("Overview"),Us=h(),$e=r("p"),Hr=i("The OWL-ViT (short for Vision Transformer for Open-World Localization) was proposed in "),ft=r("a"),Rr=i("Simple Open-Vocabulary Object Detection with Vision Transformers"),Ur=i(" by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. OWL-ViT is an open-vocabulary object detection network trained on a variety of (image, text) pairs. It can be used to query an image with one or multiple text queries to search for and detect target objects described in text."),Gs=h(),ao=r("p"),Gr=i("The abstract from the paper is the following:"),Xs=h(),io=r("p"),os=r("em"),Xr=i("Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub."),Zs=h(),J=r("h2"),Oe=r("a"),ss=r("span"),v(gt.$$.fragment),Zr=h(),rs=r("span"),Jr=i("Usage"),Js=h(),xe=r("p"),Kr=i("OWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses "),lo=r("a"),Yr=i("CLIP"),Qr=i(" as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection."),Ks=h(),z=r("p"),co=r("a"),en=i("OwlViTFeatureExtractor"),tn=i(" can be used to resize (or rescale) and normalize images for the model and "),po=r("a"),on=i("CLIPTokenizer"),sn=i(" is used to encode the text. "),mo=r("a"),rn=i("OwlViTProcessor"),nn=i(" wraps "),ho=r("a"),an=i("OwlViTFeatureExtractor"),ln=i(" and "),fo=r("a"),cn=i("CLIPTokenizer"),dn=i(" into a single instance to both encode the text and prepare the images. The following example shows how to perform object detection using "),go=r("a"),pn=i("OwlViTProcessor"),mn=i(" and "),uo=r("a"),hn=i("OwlViTForObjectDetection"),fn=i("."),Ys=h(),v(ut.$$.fragment),Qs=h(),U=r("p"),gn=i("This model was contributed by "),_t=r("a"),un=i("adirik"),_n=i(". The original code can be found "),wt=r("a"),wn=i("here"),Tn=i("."),er=h(),K=r("h2"),Ve=r("a"),ns=r("span"),v(Tt.$$.fragment),vn=h(),as=r("span"),bn=i("OwlViTConfig"),tr=h(),C=r("div"),v(vt.$$.fragment),$n=h(),ye=r("p"),_o=r("a"),On=i("OwlViTConfig"),xn=i(" is the configuration class to store the configuration of an "),wo=r("a"),Vn=i("OwlViTModel"),yn=i(`. It is used to
instantiate an OWL-ViT model according to the specified arguments, defining the text model and vision model
configs.`),jn=h(),Y=r("p"),kn=i("Configuration objects inherit from "),To=r("a"),zn=i("PretrainedConfig"),Pn=i(` and can be used to control the model outputs. Read the
documentation from `),vo=r("a"),En=i("PretrainedConfig"),Mn=i(" for more information."),Cn=h(),je=r("div"),v(bt.$$.fragment),qn=h(),$t=r("p"),In=i("Instantiate a "),bo=r("a"),Fn=i("OwlViTConfig"),Ln=i(` (or a derived class) from owlvit text model configuration and owlvit vision
model configuration.`),or=h(),Q=r("h2"),ke=r("a"),is=r("span"),v(Ot.$$.fragment),An=h(),ls=r("span"),Dn=i("OwlViTTextConfig"),sr=h(),q=r("div"),v(xt.$$.fragment),Wn=h(),ee=r("p"),Nn=i("This is the configuration class to store the configuration of an "),$o=r("a"),Sn=i("OwlViTTextModel"),Bn=i(`. It is used to instantiate an
OwlViT text encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OwlViT
`),Vt=r("a"),Hn=i("google/owlvit-base-patch32"),Rn=i(" architecture."),Un=h(),te=r("p"),Gn=i("Configuration objects inherit from "),Oo=r("a"),Xn=i("PretrainedConfig"),Zn=i(` and can be used to control the model outputs. Read the
documentation from `),xo=r("a"),Jn=i("PretrainedConfig"),Kn=i(" for more information."),Yn=h(),v(ze.$$.fragment),rr=h(),oe=r("h2"),Pe=r("a"),cs=r("span"),v(yt.$$.fragment),Qn=h(),ds=r("span"),ea=i("OwlViTVisionConfig"),nr=h(),I=r("div"),v(jt.$$.fragment),ta=h(),se=r("p"),oa=i("This is the configuration class to store the configuration of an "),Vo=r("a"),sa=i("OwlViTVisionModel"),ra=i(`. It is used to instantiate
an OWL-ViT image encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OWL-ViT
`),kt=r("a"),na=i("google/owlvit-base-patch32"),aa=i(" architecture."),ia=h(),re=r("p"),la=i("Configuration objects inherit from "),yo=r("a"),ca=i("PretrainedConfig"),da=i(` and can be used to control the model outputs. Read the
documentation from `),jo=r("a"),pa=i("PretrainedConfig"),ma=i(" for more information."),ha=h(),v(Ee.$$.fragment),ar=h(),ne=r("h2"),Me=r("a"),ps=r("span"),v(zt.$$.fragment),fa=h(),ms=r("span"),ga=i("OwlViTFeatureExtractor"),ir=h(),F=r("div"),v(Pt.$$.fragment),ua=h(),hs=r("p"),_a=i("Constructs an OWL-ViT feature extractor."),wa=h(),Et=r("p"),Ta=i("This feature extractor inherits from "),ko=r("a"),va=i("FeatureExtractionMixin"),ba=i(` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),$a=h(),G=r("div"),v(Mt.$$.fragment),Oa=h(),fs=r("p"),xa=i("Main method to prepare for the model one or several image(s)."),Va=h(),v(Ce.$$.fragment),lr=h(),ae=r("h2"),qe=r("a"),gs=r("span"),v(Ct.$$.fragment),ya=h(),us=r("span"),ja=i("OwlViTProcessor"),cr=h(),P=r("div"),v(qt.$$.fragment),ka=h(),E=r("p"),za=i("Constructs an OWL-ViT processor which wraps "),zo=r("a"),Pa=i("OwlViTFeatureExtractor"),Ea=i(" and "),Po=r("a"),Ma=i("CLIPTokenizer"),Ca=i("/"),Eo=r("a"),qa=i("CLIPTokenizerFast"),Ia=i(`
into a single processor that interits both the feature extractor and tokenizer functionalities. See the
`),_s=r("code"),Fa=i("__call__()"),La=i(" and "),Mo=r("a"),Aa=i("decode()"),Da=i(" for more information."),Wa=h(),Ie=r("div"),v(It.$$.fragment),Na=h(),Ft=r("p"),Sa=i("This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Co=r("a"),Ba=i("batch_decode()"),Ha=i(`. Please
refer to the docstring of this method for more information.`),Ra=h(),Fe=r("div"),v(Lt.$$.fragment),Ua=h(),At=r("p"),Ga=i("This method forwards all its arguments to CLIPTokenizerFast\u2019s "),qo=r("a"),Xa=i("decode()"),Za=i(`. Please refer to
the docstring of this method for more information.`),Ja=h(),Le=r("div"),v(Dt.$$.fragment),Ka=h(),Wt=r("p"),Ya=i("This method forwards all its arguments to "),ws=r("code"),Qa=i("OwlViTFeatureExtractor.post_process()"),ei=i(`. Please refer to the
docstring of this method for more information.`),dr=h(),ie=r("h2"),Ae=r("a"),Ts=r("span"),v(Nt.$$.fragment),ti=h(),vs=r("span"),oi=i("OwlViTModel"),pr=h(),L=r("div"),v(St.$$.fragment),si=h(),A=r("div"),v(Bt.$$.fragment),ri=h(),le=r("p"),ni=i("The "),Io=r("a"),ai=i("OwlViTModel"),ii=i(" forward method, overrides the "),bs=r("code"),li=i("__call__"),ci=i(" special method."),di=h(),v(De.$$.fragment),pi=h(),v(We.$$.fragment),mi=h(),D=r("div"),v(Ht.$$.fragment),hi=h(),ce=r("p"),fi=i("The "),Fo=r("a"),gi=i("OwlViTModel"),ui=i(" forward method, overrides the "),$s=r("code"),_i=i("__call__"),wi=i(" special method."),Ti=h(),v(Ne.$$.fragment),vi=h(),v(Se.$$.fragment),bi=h(),W=r("div"),v(Rt.$$.fragment),$i=h(),de=r("p"),Oi=i("The "),Lo=r("a"),xi=i("OwlViTModel"),Vi=i(" forward method, overrides the "),Os=r("code"),yi=i("__call__"),ji=i(" special method."),ki=h(),v(Be.$$.fragment),zi=h(),v(He.$$.fragment),mr=h(),pe=r("h2"),Re=r("a"),xs=r("span"),v(Ut.$$.fragment),Pi=h(),Vs=r("span"),Ei=i("OwlViTTextModel"),hr=h(),me=r("div"),v(Gt.$$.fragment),Mi=h(),N=r("div"),v(Xt.$$.fragment),Ci=h(),he=r("p"),qi=i("The "),Ao=r("a"),Ii=i("OwlViTTextModel"),Fi=i(" forward method, overrides the "),ys=r("code"),Li=i("__call__"),Ai=i(" special method."),Di=h(),v(Ue.$$.fragment),Wi=h(),v(Ge.$$.fragment),fr=h(),fe=r("h2"),Xe=r("a"),js=r("span"),v(Zt.$$.fragment),Ni=h(),ks=r("span"),Si=i("OwlViTVisionModel"),gr=h(),ge=r("div"),v(Jt.$$.fragment),Bi=h(),S=r("div"),v(Kt.$$.fragment),Hi=h(),ue=r("p"),Ri=i("The "),Do=r("a"),Ui=i("OwlViTVisionModel"),Gi=i(" forward method, overrides the "),zs=r("code"),Xi=i("__call__"),Zi=i(" special method."),Ji=h(),v(Ze.$$.fragment),Ki=h(),v(Je.$$.fragment),ur=h(),_e=r("h2"),Ke=r("a"),Ps=r("span"),v(Yt.$$.fragment),Yi=h(),Es=r("span"),Qi=i("OwlViTForObjectDetection"),_r=h(),we=r("div"),v(Qt.$$.fragment),el=h(),B=r("div"),v(eo.$$.fragment),tl=h(),Te=r("p"),ol=i("The "),Wo=r("a"),sl=i("OwlViTForObjectDetection"),rl=i(" forward method, overrides the "),Ms=r("code"),nl=i("__call__"),al=i(" special method."),il=h(),v(Ye.$$.fragment),ll=h(),v(Qe.$$.fragment),this.h()},l(t){const u=Cc('[data-svelte="svelte-1phssyn"]',document.head);c=n(u,"META",{name:!0,content:!0}),u.forEach(o),T=f(t),g=n(t,"H1",{class:!0});var to=a(g);p=n(to,"A",{id:!0,class:!0,href:!0});var Cs=a(p);w=n(Cs,"SPAN",{});var qs=a(w);b(s.$$.fragment,qs),qs.forEach(o),Cs.forEach(o),m=f(to),j=n(to,"SPAN",{});var Is=a(j);Nr=l(Is,"OWL-ViT"),Is.forEach(o),to.forEach(o),Rs=f(t),Z=n(t,"H2",{class:!0});var oo=a(Z);be=n(oo,"A",{id:!0,class:!0,href:!0});var Fs=a(be);es=n(Fs,"SPAN",{});var Ls=a(es);b(ht.$$.fragment,Ls),Ls.forEach(o),Fs.forEach(o),Sr=f(oo),ts=n(oo,"SPAN",{});var As=a(ts);Br=l(As,"Overview"),As.forEach(o),oo.forEach(o),Us=f(t),$e=n(t,"P",{});var so=a($e);Hr=l(so,"The OWL-ViT (short for Vision Transformer for Open-World Localization) was proposed in "),ft=n(so,"A",{href:!0,rel:!0});var Ds=a(ft);Rr=l(Ds,"Simple Open-Vocabulary Object Detection with Vision Transformers"),Ds.forEach(o),Ur=l(so," by Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby. OWL-ViT is an open-vocabulary object detection network trained on a variety of (image, text) pairs. It can be used to query an image with one or multiple text queries to search for and detect target objects described in text."),so.forEach(o),Gs=f(t),ao=n(t,"P",{});var Ws=a(ao);Gr=l(Ws,"The abstract from the paper is the following:"),Ws.forEach(o),Xs=f(t),io=n(t,"P",{});var Ns=a(io);os=n(Ns,"EM",{});var Ss=a(os);Xr=l(Ss,"Combining simple architectures with large-scale pre-training has led to massive improvements in image classification. For object detection, pre-training and scaling approaches are less well established, especially in the long-tailed and open-vocabulary setting, where training data is relatively scarce. In this paper, we propose a strong recipe for transferring image-text models to open-vocabulary object detection. We use a standard Vision Transformer architecture with minimal modifications, contrastive image-text pre-training, and end-to-end detection fine-tuning. Our analysis of the scaling properties of this setup shows that increasing image-level pre-training and model size yield consistent improvements on the downstream detection task. We provide the adaptation strategies and regularizations needed to attain very strong performance on zero-shot text-conditioned and one-shot image-conditioned object detection. Code and models are available on GitHub."),Ss.forEach(o),Ns.forEach(o),Zs=f(t),J=n(t,"H2",{class:!0});var ro=a(J);Oe=n(ro,"A",{id:!0,class:!0,href:!0});var Bs=a(Oe);ss=n(Bs,"SPAN",{});var cl=a(ss);b(gt.$$.fragment,cl),cl.forEach(o),Bs.forEach(o),Zr=f(ro),rs=n(ro,"SPAN",{});var dl=a(rs);Jr=l(dl,"Usage"),dl.forEach(o),ro.forEach(o),Js=f(t),xe=n(t,"P",{});var Tr=a(xe);Kr=l(Tr,"OWL-ViT is a zero-shot text-conditioned object detection model. OWL-ViT uses "),lo=n(Tr,"A",{href:!0});var pl=a(lo);Yr=l(pl,"CLIP"),pl.forEach(o),Qr=l(Tr," as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection."),Tr.forEach(o),Ks=f(t),z=n(t,"P",{});var M=a(z);co=n(M,"A",{href:!0});var ml=a(co);en=l(ml,"OwlViTFeatureExtractor"),ml.forEach(o),tn=l(M," can be used to resize (or rescale) and normalize images for the model and "),po=n(M,"A",{href:!0});var hl=a(po);on=l(hl,"CLIPTokenizer"),hl.forEach(o),sn=l(M," is used to encode the text. "),mo=n(M,"A",{href:!0});var fl=a(mo);rn=l(fl,"OwlViTProcessor"),fl.forEach(o),nn=l(M," wraps "),ho=n(M,"A",{href:!0});var gl=a(ho);an=l(gl,"OwlViTFeatureExtractor"),gl.forEach(o),ln=l(M," and "),fo=n(M,"A",{href:!0});var ul=a(fo);cn=l(ul,"CLIPTokenizer"),ul.forEach(o),dn=l(M," into a single instance to both encode the text and prepare the images. The following example shows how to perform object detection using "),go=n(M,"A",{href:!0});var _l=a(go);pn=l(_l,"OwlViTProcessor"),_l.forEach(o),mn=l(M," and "),uo=n(M,"A",{href:!0});var wl=a(uo);hn=l(wl,"OwlViTForObjectDetection"),wl.forEach(o),fn=l(M,"."),M.forEach(o),Ys=f(t),b(ut.$$.fragment,t),Qs=f(t),U=n(t,"P",{});var No=a(U);gn=l(No,"This model was contributed by "),_t=n(No,"A",{href:!0,rel:!0});var Tl=a(_t);un=l(Tl,"adirik"),Tl.forEach(o),_n=l(No,". The original code can be found "),wt=n(No,"A",{href:!0,rel:!0});var vl=a(wt);wn=l(vl,"here"),vl.forEach(o),Tn=l(No,"."),No.forEach(o),er=f(t),K=n(t,"H2",{class:!0});var vr=a(K);Ve=n(vr,"A",{id:!0,class:!0,href:!0});var bl=a(Ve);ns=n(bl,"SPAN",{});var $l=a(ns);b(Tt.$$.fragment,$l),$l.forEach(o),bl.forEach(o),vn=f(vr),as=n(vr,"SPAN",{});var Ol=a(as);bn=l(Ol,"OwlViTConfig"),Ol.forEach(o),vr.forEach(o),tr=f(t),C=n(t,"DIV",{class:!0});var et=a(C);b(vt.$$.fragment,et),$n=f(et),ye=n(et,"P",{});var Hs=a(ye);_o=n(Hs,"A",{href:!0});var xl=a(_o);On=l(xl,"OwlViTConfig"),xl.forEach(o),xn=l(Hs," is the configuration class to store the configuration of an "),wo=n(Hs,"A",{href:!0});var Vl=a(wo);Vn=l(Vl,"OwlViTModel"),Vl.forEach(o),yn=l(Hs,`. It is used to
instantiate an OWL-ViT model according to the specified arguments, defining the text model and vision model
configs.`),Hs.forEach(o),jn=f(et),Y=n(et,"P",{});var So=a(Y);kn=l(So,"Configuration objects inherit from "),To=n(So,"A",{href:!0});var yl=a(To);zn=l(yl,"PretrainedConfig"),yl.forEach(o),Pn=l(So,` and can be used to control the model outputs. Read the
documentation from `),vo=n(So,"A",{href:!0});var jl=a(vo);En=l(jl,"PretrainedConfig"),jl.forEach(o),Mn=l(So," for more information."),So.forEach(o),Cn=f(et),je=n(et,"DIV",{class:!0});var br=a(je);b(bt.$$.fragment,br),qn=f(br),$t=n(br,"P",{});var $r=a($t);In=l($r,"Instantiate a "),bo=n($r,"A",{href:!0});var kl=a(bo);Fn=l(kl,"OwlViTConfig"),kl.forEach(o),Ln=l($r,` (or a derived class) from owlvit text model configuration and owlvit vision
model configuration.`),$r.forEach(o),br.forEach(o),et.forEach(o),or=f(t),Q=n(t,"H2",{class:!0});var Or=a(Q);ke=n(Or,"A",{id:!0,class:!0,href:!0});var zl=a(ke);is=n(zl,"SPAN",{});var Pl=a(is);b(Ot.$$.fragment,Pl),Pl.forEach(o),zl.forEach(o),An=f(Or),ls=n(Or,"SPAN",{});var El=a(ls);Dn=l(El,"OwlViTTextConfig"),El.forEach(o),Or.forEach(o),sr=f(t),q=n(t,"DIV",{class:!0});var tt=a(q);b(xt.$$.fragment,tt),Wn=f(tt),ee=n(tt,"P",{});var Bo=a(ee);Nn=l(Bo,"This is the configuration class to store the configuration of an "),$o=n(Bo,"A",{href:!0});var Ml=a($o);Sn=l(Ml,"OwlViTTextModel"),Ml.forEach(o),Bn=l(Bo,`. It is used to instantiate an
OwlViT text encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OwlViT
`),Vt=n(Bo,"A",{href:!0,rel:!0});var Cl=a(Vt);Hn=l(Cl,"google/owlvit-base-patch32"),Cl.forEach(o),Rn=l(Bo," architecture."),Bo.forEach(o),Un=f(tt),te=n(tt,"P",{});var Ho=a(te);Gn=l(Ho,"Configuration objects inherit from "),Oo=n(Ho,"A",{href:!0});var ql=a(Oo);Xn=l(ql,"PretrainedConfig"),ql.forEach(o),Zn=l(Ho,` and can be used to control the model outputs. Read the
documentation from `),xo=n(Ho,"A",{href:!0});var Il=a(xo);Jn=l(Il,"PretrainedConfig"),Il.forEach(o),Kn=l(Ho," for more information."),Ho.forEach(o),Yn=f(tt),b(ze.$$.fragment,tt),tt.forEach(o),rr=f(t),oe=n(t,"H2",{class:!0});var xr=a(oe);Pe=n(xr,"A",{id:!0,class:!0,href:!0});var Fl=a(Pe);cs=n(Fl,"SPAN",{});var Ll=a(cs);b(yt.$$.fragment,Ll),Ll.forEach(o),Fl.forEach(o),Qn=f(xr),ds=n(xr,"SPAN",{});var Al=a(ds);ea=l(Al,"OwlViTVisionConfig"),Al.forEach(o),xr.forEach(o),nr=f(t),I=n(t,"DIV",{class:!0});var ot=a(I);b(jt.$$.fragment,ot),ta=f(ot),se=n(ot,"P",{});var Ro=a(se);oa=l(Ro,"This is the configuration class to store the configuration of an "),Vo=n(Ro,"A",{href:!0});var Dl=a(Vo);sa=l(Dl,"OwlViTVisionModel"),Dl.forEach(o),ra=l(Ro,`. It is used to instantiate
an OWL-ViT image encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the OWL-ViT
`),kt=n(Ro,"A",{href:!0,rel:!0});var Wl=a(kt);na=l(Wl,"google/owlvit-base-patch32"),Wl.forEach(o),aa=l(Ro," architecture."),Ro.forEach(o),ia=f(ot),re=n(ot,"P",{});var Uo=a(re);la=l(Uo,"Configuration objects inherit from "),yo=n(Uo,"A",{href:!0});var Nl=a(yo);ca=l(Nl,"PretrainedConfig"),Nl.forEach(o),da=l(Uo,` and can be used to control the model outputs. Read the
documentation from `),jo=n(Uo,"A",{href:!0});var Sl=a(jo);pa=l(Sl,"PretrainedConfig"),Sl.forEach(o),ma=l(Uo," for more information."),Uo.forEach(o),ha=f(ot),b(Ee.$$.fragment,ot),ot.forEach(o),ar=f(t),ne=n(t,"H2",{class:!0});var Vr=a(ne);Me=n(Vr,"A",{id:!0,class:!0,href:!0});var Bl=a(Me);ps=n(Bl,"SPAN",{});var Hl=a(ps);b(zt.$$.fragment,Hl),Hl.forEach(o),Bl.forEach(o),fa=f(Vr),ms=n(Vr,"SPAN",{});var Rl=a(ms);ga=l(Rl,"OwlViTFeatureExtractor"),Rl.forEach(o),Vr.forEach(o),ir=f(t),F=n(t,"DIV",{class:!0});var st=a(F);b(Pt.$$.fragment,st),ua=f(st),hs=n(st,"P",{});var Ul=a(hs);_a=l(Ul,"Constructs an OWL-ViT feature extractor."),Ul.forEach(o),wa=f(st),Et=n(st,"P",{});var yr=a(Et);Ta=l(yr,"This feature extractor inherits from "),ko=n(yr,"A",{href:!0});var Gl=a(ko);va=l(Gl,"FeatureExtractionMixin"),Gl.forEach(o),ba=l(yr,` which contains most of the main methods. Users
should refer to this superclass for more information regarding those methods.`),yr.forEach(o),$a=f(st),G=n(st,"DIV",{class:!0});var Go=a(G);b(Mt.$$.fragment,Go),Oa=f(Go),fs=n(Go,"P",{});var Xl=a(fs);xa=l(Xl,"Main method to prepare for the model one or several image(s)."),Xl.forEach(o),Va=f(Go),b(Ce.$$.fragment,Go),Go.forEach(o),st.forEach(o),lr=f(t),ae=n(t,"H2",{class:!0});var jr=a(ae);qe=n(jr,"A",{id:!0,class:!0,href:!0});var Zl=a(qe);gs=n(Zl,"SPAN",{});var Jl=a(gs);b(Ct.$$.fragment,Jl),Jl.forEach(o),Zl.forEach(o),ya=f(jr),us=n(jr,"SPAN",{});var Kl=a(us);ja=l(Kl,"OwlViTProcessor"),Kl.forEach(o),jr.forEach(o),cr=f(t),P=n(t,"DIV",{class:!0});var X=a(P);b(qt.$$.fragment,X),ka=f(X),E=n(X,"P",{});var H=a(E);za=l(H,"Constructs an OWL-ViT processor which wraps "),zo=n(H,"A",{href:!0});var Yl=a(zo);Pa=l(Yl,"OwlViTFeatureExtractor"),Yl.forEach(o),Ea=l(H," and "),Po=n(H,"A",{href:!0});var Ql=a(Po);Ma=l(Ql,"CLIPTokenizer"),Ql.forEach(o),Ca=l(H,"/"),Eo=n(H,"A",{href:!0});var ec=a(Eo);qa=l(ec,"CLIPTokenizerFast"),ec.forEach(o),Ia=l(H,`
into a single processor that interits both the feature extractor and tokenizer functionalities. See the
`),_s=n(H,"CODE",{});var tc=a(_s);Fa=l(tc,"__call__()"),tc.forEach(o),La=l(H," and "),Mo=n(H,"A",{href:!0});var oc=a(Mo);Aa=l(oc,"decode()"),oc.forEach(o),Da=l(H," for more information."),H.forEach(o),Wa=f(X),Ie=n(X,"DIV",{class:!0});var kr=a(Ie);b(It.$$.fragment,kr),Na=f(kr),Ft=n(kr,"P",{});var zr=a(Ft);Sa=l(zr,"This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Co=n(zr,"A",{href:!0});var sc=a(Co);Ba=l(sc,"batch_decode()"),sc.forEach(o),Ha=l(zr,`. Please
refer to the docstring of this method for more information.`),zr.forEach(o),kr.forEach(o),Ra=f(X),Fe=n(X,"DIV",{class:!0});var Pr=a(Fe);b(Lt.$$.fragment,Pr),Ua=f(Pr),At=n(Pr,"P",{});var Er=a(At);Ga=l(Er,"This method forwards all its arguments to CLIPTokenizerFast\u2019s "),qo=n(Er,"A",{href:!0});var rc=a(qo);Xa=l(rc,"decode()"),rc.forEach(o),Za=l(Er,`. Please refer to
the docstring of this method for more information.`),Er.forEach(o),Pr.forEach(o),Ja=f(X),Le=n(X,"DIV",{class:!0});var Mr=a(Le);b(Dt.$$.fragment,Mr),Ka=f(Mr),Wt=n(Mr,"P",{});var Cr=a(Wt);Ya=l(Cr,"This method forwards all its arguments to "),ws=n(Cr,"CODE",{});var nc=a(ws);Qa=l(nc,"OwlViTFeatureExtractor.post_process()"),nc.forEach(o),ei=l(Cr,`. Please refer to the
docstring of this method for more information.`),Cr.forEach(o),Mr.forEach(o),X.forEach(o),dr=f(t),ie=n(t,"H2",{class:!0});var qr=a(ie);Ae=n(qr,"A",{id:!0,class:!0,href:!0});var ac=a(Ae);Ts=n(ac,"SPAN",{});var ic=a(Ts);b(Nt.$$.fragment,ic),ic.forEach(o),ac.forEach(o),ti=f(qr),vs=n(qr,"SPAN",{});var lc=a(vs);oi=l(lc,"OwlViTModel"),lc.forEach(o),qr.forEach(o),pr=f(t),L=n(t,"DIV",{class:!0});var rt=a(L);b(St.$$.fragment,rt),si=f(rt),A=n(rt,"DIV",{class:!0});var nt=a(A);b(Bt.$$.fragment,nt),ri=f(nt),le=n(nt,"P",{});var Xo=a(le);ni=l(Xo,"The "),Io=n(Xo,"A",{href:!0});var cc=a(Io);ai=l(cc,"OwlViTModel"),cc.forEach(o),ii=l(Xo," forward method, overrides the "),bs=n(Xo,"CODE",{});var dc=a(bs);li=l(dc,"__call__"),dc.forEach(o),ci=l(Xo," special method."),Xo.forEach(o),di=f(nt),b(De.$$.fragment,nt),pi=f(nt),b(We.$$.fragment,nt),nt.forEach(o),mi=f(rt),D=n(rt,"DIV",{class:!0});var at=a(D);b(Ht.$$.fragment,at),hi=f(at),ce=n(at,"P",{});var Zo=a(ce);fi=l(Zo,"The "),Fo=n(Zo,"A",{href:!0});var pc=a(Fo);gi=l(pc,"OwlViTModel"),pc.forEach(o),ui=l(Zo," forward method, overrides the "),$s=n(Zo,"CODE",{});var mc=a($s);_i=l(mc,"__call__"),mc.forEach(o),wi=l(Zo," special method."),Zo.forEach(o),Ti=f(at),b(Ne.$$.fragment,at),vi=f(at),b(Se.$$.fragment,at),at.forEach(o),bi=f(rt),W=n(rt,"DIV",{class:!0});var it=a(W);b(Rt.$$.fragment,it),$i=f(it),de=n(it,"P",{});var Jo=a(de);Oi=l(Jo,"The "),Lo=n(Jo,"A",{href:!0});var hc=a(Lo);xi=l(hc,"OwlViTModel"),hc.forEach(o),Vi=l(Jo," forward method, overrides the "),Os=n(Jo,"CODE",{});var fc=a(Os);yi=l(fc,"__call__"),fc.forEach(o),ji=l(Jo," special method."),Jo.forEach(o),ki=f(it),b(Be.$$.fragment,it),zi=f(it),b(He.$$.fragment,it),it.forEach(o),rt.forEach(o),mr=f(t),pe=n(t,"H2",{class:!0});var Ir=a(pe);Re=n(Ir,"A",{id:!0,class:!0,href:!0});var gc=a(Re);xs=n(gc,"SPAN",{});var uc=a(xs);b(Ut.$$.fragment,uc),uc.forEach(o),gc.forEach(o),Pi=f(Ir),Vs=n(Ir,"SPAN",{});var _c=a(Vs);Ei=l(_c,"OwlViTTextModel"),_c.forEach(o),Ir.forEach(o),hr=f(t),me=n(t,"DIV",{class:!0});var Fr=a(me);b(Gt.$$.fragment,Fr),Mi=f(Fr),N=n(Fr,"DIV",{class:!0});var lt=a(N);b(Xt.$$.fragment,lt),Ci=f(lt),he=n(lt,"P",{});var Ko=a(he);qi=l(Ko,"The "),Ao=n(Ko,"A",{href:!0});var wc=a(Ao);Ii=l(wc,"OwlViTTextModel"),wc.forEach(o),Fi=l(Ko," forward method, overrides the "),ys=n(Ko,"CODE",{});var Tc=a(ys);Li=l(Tc,"__call__"),Tc.forEach(o),Ai=l(Ko," special method."),Ko.forEach(o),Di=f(lt),b(Ue.$$.fragment,lt),Wi=f(lt),b(Ge.$$.fragment,lt),lt.forEach(o),Fr.forEach(o),fr=f(t),fe=n(t,"H2",{class:!0});var Lr=a(fe);Xe=n(Lr,"A",{id:!0,class:!0,href:!0});var vc=a(Xe);js=n(vc,"SPAN",{});var bc=a(js);b(Zt.$$.fragment,bc),bc.forEach(o),vc.forEach(o),Ni=f(Lr),ks=n(Lr,"SPAN",{});var $c=a(ks);Si=l($c,"OwlViTVisionModel"),$c.forEach(o),Lr.forEach(o),gr=f(t),ge=n(t,"DIV",{class:!0});var Ar=a(ge);b(Jt.$$.fragment,Ar),Bi=f(Ar),S=n(Ar,"DIV",{class:!0});var ct=a(S);b(Kt.$$.fragment,ct),Hi=f(ct),ue=n(ct,"P",{});var Yo=a(ue);Ri=l(Yo,"The "),Do=n(Yo,"A",{href:!0});var Oc=a(Do);Ui=l(Oc,"OwlViTVisionModel"),Oc.forEach(o),Gi=l(Yo," forward method, overrides the "),zs=n(Yo,"CODE",{});var xc=a(zs);Xi=l(xc,"__call__"),xc.forEach(o),Zi=l(Yo," special method."),Yo.forEach(o),Ji=f(ct),b(Ze.$$.fragment,ct),Ki=f(ct),b(Je.$$.fragment,ct),ct.forEach(o),Ar.forEach(o),ur=f(t),_e=n(t,"H2",{class:!0});var Dr=a(_e);Ke=n(Dr,"A",{id:!0,class:!0,href:!0});var Vc=a(Ke);Ps=n(Vc,"SPAN",{});var yc=a(Ps);b(Yt.$$.fragment,yc),yc.forEach(o),Vc.forEach(o),Yi=f(Dr),Es=n(Dr,"SPAN",{});var jc=a(Es);Qi=l(jc,"OwlViTForObjectDetection"),jc.forEach(o),Dr.forEach(o),_r=f(t),we=n(t,"DIV",{class:!0});var Wr=a(we);b(Qt.$$.fragment,Wr),el=f(Wr),B=n(Wr,"DIV",{class:!0});var dt=a(B);b(eo.$$.fragment,dt),tl=f(dt),Te=n(dt,"P",{});var Qo=a(Te);ol=l(Qo,"The "),Wo=n(Qo,"A",{href:!0});var kc=a(Wo);sl=l(kc,"OwlViTForObjectDetection"),kc.forEach(o),rl=l(Qo," forward method, overrides the "),Ms=n(Qo,"CODE",{});var zc=a(Ms);nl=l(zc,"__call__"),zc.forEach(o),al=l(Qo," special method."),Qo.forEach(o),il=f(dt),b(Ye.$$.fragment,dt),ll=f(dt),b(Qe.$$.fragment,dt),dt.forEach(o),Wr.forEach(o),this.h()},h(){d(c,"name","hf:doc:metadata"),d(c,"content",JSON.stringify(Kc)),d(p,"id","owlvit"),d(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(p,"href","#owlvit"),d(g,"class","relative group"),d(be,"id","overview"),d(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(be,"href","#overview"),d(Z,"class","relative group"),d(ft,"href","https://arxiv.org/abs/2205.06230"),d(ft,"rel","nofollow"),d(Oe,"id","usage"),d(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Oe,"href","#usage"),d(J,"class","relative group"),d(lo,"href","clip"),d(co,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),d(po,"href","/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer"),d(mo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTProcessor"),d(ho,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),d(fo,"href","/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer"),d(go,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTProcessor"),d(uo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTForObjectDetection"),d(_t,"href","https://huggingface.co/adirik"),d(_t,"rel","nofollow"),d(wt,"href","https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit"),d(wt,"rel","nofollow"),d(Ve,"id","transformers.OwlViTConfig"),d(Ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ve,"href","#transformers.OwlViTConfig"),d(K,"class","relative group"),d(_o,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTConfig"),d(wo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTModel"),d(To,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(vo,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(bo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTConfig"),d(je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ke,"id","transformers.OwlViTTextConfig"),d(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ke,"href","#transformers.OwlViTTextConfig"),d(Q,"class","relative group"),d($o,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"),d(Vt,"href","https://huggingface.co/google/owlvit-base-patch32"),d(Vt,"rel","nofollow"),d(Oo,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(xo,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Pe,"id","transformers.OwlViTVisionConfig"),d(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Pe,"href","#transformers.OwlViTVisionConfig"),d(oe,"class","relative group"),d(Vo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"),d(kt,"href","https://huggingface.co/google/owlvit-base-patch32"),d(kt,"rel","nofollow"),d(yo,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(jo,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Me,"id","transformers.OwlViTFeatureExtractor"),d(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Me,"href","#transformers.OwlViTFeatureExtractor"),d(ne,"class","relative group"),d(ko,"href","/docs/transformers/main/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin"),d(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(qe,"id","transformers.OwlViTProcessor"),d(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(qe,"href","#transformers.OwlViTProcessor"),d(ae,"class","relative group"),d(zo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor"),d(Po,"href","/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer"),d(Eo,"href","/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizerFast"),d(Mo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTProcessor.decode"),d(Co,"href","/docs/transformers/main/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer.batch_decode"),d(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(qo,"href","/docs/transformers/main/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer.decode"),d(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ae,"id","transformers.OwlViTModel"),d(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ae,"href","#transformers.OwlViTModel"),d(ie,"class","relative group"),d(Io,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTModel"),d(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Fo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTModel"),d(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Lo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTModel"),d(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Re,"id","transformers.OwlViTTextModel"),d(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Re,"href","#transformers.OwlViTTextModel"),d(pe,"class","relative group"),d(Ao,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTTextModel"),d(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Xe,"id","transformers.OwlViTVisionModel"),d(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Xe,"href","#transformers.OwlViTVisionModel"),d(fe,"class","relative group"),d(Do,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTVisionModel"),d(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ke,"id","transformers.OwlViTForObjectDetection"),d(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ke,"href","#transformers.OwlViTForObjectDetection"),d(_e,"class","relative group"),d(Wo,"href","/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTForObjectDetection"),d(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(we,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,u){e(document.head,c),_(t,T,u),_(t,g,u),e(g,p),e(p,w),$(s,w,null),e(g,m),e(g,j),e(j,Nr),_(t,Rs,u),_(t,Z,u),e(Z,be),e(be,es),$(ht,es,null),e(Z,Sr),e(Z,ts),e(ts,Br),_(t,Us,u),_(t,$e,u),e($e,Hr),e($e,ft),e(ft,Rr),e($e,Ur),_(t,Gs,u),_(t,ao,u),e(ao,Gr),_(t,Xs,u),_(t,io,u),e(io,os),e(os,Xr),_(t,Zs,u),_(t,J,u),e(J,Oe),e(Oe,ss),$(gt,ss,null),e(J,Zr),e(J,rs),e(rs,Jr),_(t,Js,u),_(t,xe,u),e(xe,Kr),e(xe,lo),e(lo,Yr),e(xe,Qr),_(t,Ks,u),_(t,z,u),e(z,co),e(co,en),e(z,tn),e(z,po),e(po,on),e(z,sn),e(z,mo),e(mo,rn),e(z,nn),e(z,ho),e(ho,an),e(z,ln),e(z,fo),e(fo,cn),e(z,dn),e(z,go),e(go,pn),e(z,mn),e(z,uo),e(uo,hn),e(z,fn),_(t,Ys,u),$(ut,t,u),_(t,Qs,u),_(t,U,u),e(U,gn),e(U,_t),e(_t,un),e(U,_n),e(U,wt),e(wt,wn),e(U,Tn),_(t,er,u),_(t,K,u),e(K,Ve),e(Ve,ns),$(Tt,ns,null),e(K,vn),e(K,as),e(as,bn),_(t,tr,u),_(t,C,u),$(vt,C,null),e(C,$n),e(C,ye),e(ye,_o),e(_o,On),e(ye,xn),e(ye,wo),e(wo,Vn),e(ye,yn),e(C,jn),e(C,Y),e(Y,kn),e(Y,To),e(To,zn),e(Y,Pn),e(Y,vo),e(vo,En),e(Y,Mn),e(C,Cn),e(C,je),$(bt,je,null),e(je,qn),e(je,$t),e($t,In),e($t,bo),e(bo,Fn),e($t,Ln),_(t,or,u),_(t,Q,u),e(Q,ke),e(ke,is),$(Ot,is,null),e(Q,An),e(Q,ls),e(ls,Dn),_(t,sr,u),_(t,q,u),$(xt,q,null),e(q,Wn),e(q,ee),e(ee,Nn),e(ee,$o),e($o,Sn),e(ee,Bn),e(ee,Vt),e(Vt,Hn),e(ee,Rn),e(q,Un),e(q,te),e(te,Gn),e(te,Oo),e(Oo,Xn),e(te,Zn),e(te,xo),e(xo,Jn),e(te,Kn),e(q,Yn),$(ze,q,null),_(t,rr,u),_(t,oe,u),e(oe,Pe),e(Pe,cs),$(yt,cs,null),e(oe,Qn),e(oe,ds),e(ds,ea),_(t,nr,u),_(t,I,u),$(jt,I,null),e(I,ta),e(I,se),e(se,oa),e(se,Vo),e(Vo,sa),e(se,ra),e(se,kt),e(kt,na),e(se,aa),e(I,ia),e(I,re),e(re,la),e(re,yo),e(yo,ca),e(re,da),e(re,jo),e(jo,pa),e(re,ma),e(I,ha),$(Ee,I,null),_(t,ar,u),_(t,ne,u),e(ne,Me),e(Me,ps),$(zt,ps,null),e(ne,fa),e(ne,ms),e(ms,ga),_(t,ir,u),_(t,F,u),$(Pt,F,null),e(F,ua),e(F,hs),e(hs,_a),e(F,wa),e(F,Et),e(Et,Ta),e(Et,ko),e(ko,va),e(Et,ba),e(F,$a),e(F,G),$(Mt,G,null),e(G,Oa),e(G,fs),e(fs,xa),e(G,Va),$(Ce,G,null),_(t,lr,u),_(t,ae,u),e(ae,qe),e(qe,gs),$(Ct,gs,null),e(ae,ya),e(ae,us),e(us,ja),_(t,cr,u),_(t,P,u),$(qt,P,null),e(P,ka),e(P,E),e(E,za),e(E,zo),e(zo,Pa),e(E,Ea),e(E,Po),e(Po,Ma),e(E,Ca),e(E,Eo),e(Eo,qa),e(E,Ia),e(E,_s),e(_s,Fa),e(E,La),e(E,Mo),e(Mo,Aa),e(E,Da),e(P,Wa),e(P,Ie),$(It,Ie,null),e(Ie,Na),e(Ie,Ft),e(Ft,Sa),e(Ft,Co),e(Co,Ba),e(Ft,Ha),e(P,Ra),e(P,Fe),$(Lt,Fe,null),e(Fe,Ua),e(Fe,At),e(At,Ga),e(At,qo),e(qo,Xa),e(At,Za),e(P,Ja),e(P,Le),$(Dt,Le,null),e(Le,Ka),e(Le,Wt),e(Wt,Ya),e(Wt,ws),e(ws,Qa),e(Wt,ei),_(t,dr,u),_(t,ie,u),e(ie,Ae),e(Ae,Ts),$(Nt,Ts,null),e(ie,ti),e(ie,vs),e(vs,oi),_(t,pr,u),_(t,L,u),$(St,L,null),e(L,si),e(L,A),$(Bt,A,null),e(A,ri),e(A,le),e(le,ni),e(le,Io),e(Io,ai),e(le,ii),e(le,bs),e(bs,li),e(le,ci),e(A,di),$(De,A,null),e(A,pi),$(We,A,null),e(L,mi),e(L,D),$(Ht,D,null),e(D,hi),e(D,ce),e(ce,fi),e(ce,Fo),e(Fo,gi),e(ce,ui),e(ce,$s),e($s,_i),e(ce,wi),e(D,Ti),$(Ne,D,null),e(D,vi),$(Se,D,null),e(L,bi),e(L,W),$(Rt,W,null),e(W,$i),e(W,de),e(de,Oi),e(de,Lo),e(Lo,xi),e(de,Vi),e(de,Os),e(Os,yi),e(de,ji),e(W,ki),$(Be,W,null),e(W,zi),$(He,W,null),_(t,mr,u),_(t,pe,u),e(pe,Re),e(Re,xs),$(Ut,xs,null),e(pe,Pi),e(pe,Vs),e(Vs,Ei),_(t,hr,u),_(t,me,u),$(Gt,me,null),e(me,Mi),e(me,N),$(Xt,N,null),e(N,Ci),e(N,he),e(he,qi),e(he,Ao),e(Ao,Ii),e(he,Fi),e(he,ys),e(ys,Li),e(he,Ai),e(N,Di),$(Ue,N,null),e(N,Wi),$(Ge,N,null),_(t,fr,u),_(t,fe,u),e(fe,Xe),e(Xe,js),$(Zt,js,null),e(fe,Ni),e(fe,ks),e(ks,Si),_(t,gr,u),_(t,ge,u),$(Jt,ge,null),e(ge,Bi),e(ge,S),$(Kt,S,null),e(S,Hi),e(S,ue),e(ue,Ri),e(ue,Do),e(Do,Ui),e(ue,Gi),e(ue,zs),e(zs,Xi),e(ue,Zi),e(S,Ji),$(Ze,S,null),e(S,Ki),$(Je,S,null),_(t,ur,u),_(t,_e,u),e(_e,Ke),e(Ke,Ps),$(Yt,Ps,null),e(_e,Yi),e(_e,Es),e(Es,Qi),_(t,_r,u),_(t,we,u),$(Qt,we,null),e(we,el),e(we,B),$(eo,B,null),e(B,tl),e(B,Te),e(Te,ol),e(Te,Wo),e(Wo,sl),e(Te,rl),e(Te,Ms),e(Ms,nl),e(Te,al),e(B,il),$(Ye,B,null),e(B,ll),$(Qe,B,null),wr=!0},p(t,[u]){const to={};u&2&&(to.$$scope={dirty:u,ctx:t}),ze.$set(to);const Cs={};u&2&&(Cs.$$scope={dirty:u,ctx:t}),Ee.$set(Cs);const qs={};u&2&&(qs.$$scope={dirty:u,ctx:t}),Ce.$set(qs);const Is={};u&2&&(Is.$$scope={dirty:u,ctx:t}),De.$set(Is);const oo={};u&2&&(oo.$$scope={dirty:u,ctx:t}),We.$set(oo);const Fs={};u&2&&(Fs.$$scope={dirty:u,ctx:t}),Ne.$set(Fs);const Ls={};u&2&&(Ls.$$scope={dirty:u,ctx:t}),Se.$set(Ls);const As={};u&2&&(As.$$scope={dirty:u,ctx:t}),Be.$set(As);const so={};u&2&&(so.$$scope={dirty:u,ctx:t}),He.$set(so);const Ds={};u&2&&(Ds.$$scope={dirty:u,ctx:t}),Ue.$set(Ds);const Ws={};u&2&&(Ws.$$scope={dirty:u,ctx:t}),Ge.$set(Ws);const Ns={};u&2&&(Ns.$$scope={dirty:u,ctx:t}),Ze.$set(Ns);const Ss={};u&2&&(Ss.$$scope={dirty:u,ctx:t}),Je.$set(Ss);const ro={};u&2&&(ro.$$scope={dirty:u,ctx:t}),Ye.$set(ro);const Bs={};u&2&&(Bs.$$scope={dirty:u,ctx:t}),Qe.$set(Bs)},i(t){wr||(O(s.$$.fragment,t),O(ht.$$.fragment,t),O(gt.$$.fragment,t),O(ut.$$.fragment,t),O(Tt.$$.fragment,t),O(vt.$$.fragment,t),O(bt.$$.fragment,t),O(Ot.$$.fragment,t),O(xt.$$.fragment,t),O(ze.$$.fragment,t),O(yt.$$.fragment,t),O(jt.$$.fragment,t),O(Ee.$$.fragment,t),O(zt.$$.fragment,t),O(Pt.$$.fragment,t),O(Mt.$$.fragment,t),O(Ce.$$.fragment,t),O(Ct.$$.fragment,t),O(qt.$$.fragment,t),O(It.$$.fragment,t),O(Lt.$$.fragment,t),O(Dt.$$.fragment,t),O(Nt.$$.fragment,t),O(St.$$.fragment,t),O(Bt.$$.fragment,t),O(De.$$.fragment,t),O(We.$$.fragment,t),O(Ht.$$.fragment,t),O(Ne.$$.fragment,t),O(Se.$$.fragment,t),O(Rt.$$.fragment,t),O(Be.$$.fragment,t),O(He.$$.fragment,t),O(Ut.$$.fragment,t),O(Gt.$$.fragment,t),O(Xt.$$.fragment,t),O(Ue.$$.fragment,t),O(Ge.$$.fragment,t),O(Zt.$$.fragment,t),O(Jt.$$.fragment,t),O(Kt.$$.fragment,t),O(Ze.$$.fragment,t),O(Je.$$.fragment,t),O(Yt.$$.fragment,t),O(Qt.$$.fragment,t),O(eo.$$.fragment,t),O(Ye.$$.fragment,t),O(Qe.$$.fragment,t),wr=!0)},o(t){x(s.$$.fragment,t),x(ht.$$.fragment,t),x(gt.$$.fragment,t),x(ut.$$.fragment,t),x(Tt.$$.fragment,t),x(vt.$$.fragment,t),x(bt.$$.fragment,t),x(Ot.$$.fragment,t),x(xt.$$.fragment,t),x(ze.$$.fragment,t),x(yt.$$.fragment,t),x(jt.$$.fragment,t),x(Ee.$$.fragment,t),x(zt.$$.fragment,t),x(Pt.$$.fragment,t),x(Mt.$$.fragment,t),x(Ce.$$.fragment,t),x(Ct.$$.fragment,t),x(qt.$$.fragment,t),x(It.$$.fragment,t),x(Lt.$$.fragment,t),x(Dt.$$.fragment,t),x(Nt.$$.fragment,t),x(St.$$.fragment,t),x(Bt.$$.fragment,t),x(De.$$.fragment,t),x(We.$$.fragment,t),x(Ht.$$.fragment,t),x(Ne.$$.fragment,t),x(Se.$$.fragment,t),x(Rt.$$.fragment,t),x(Be.$$.fragment,t),x(He.$$.fragment,t),x(Ut.$$.fragment,t),x(Gt.$$.fragment,t),x(Xt.$$.fragment,t),x(Ue.$$.fragment,t),x(Ge.$$.fragment,t),x(Zt.$$.fragment,t),x(Jt.$$.fragment,t),x(Kt.$$.fragment,t),x(Ze.$$.fragment,t),x(Je.$$.fragment,t),x(Yt.$$.fragment,t),x(Qt.$$.fragment,t),x(eo.$$.fragment,t),x(Ye.$$.fragment,t),x(Qe.$$.fragment,t),wr=!1},d(t){o(c),t&&o(T),t&&o(g),V(s),t&&o(Rs),t&&o(Z),V(ht),t&&o(Us),t&&o($e),t&&o(Gs),t&&o(ao),t&&o(Xs),t&&o(io),t&&o(Zs),t&&o(J),V(gt),t&&o(Js),t&&o(xe),t&&o(Ks),t&&o(z),t&&o(Ys),V(ut,t),t&&o(Qs),t&&o(U),t&&o(er),t&&o(K),V(Tt),t&&o(tr),t&&o(C),V(vt),V(bt),t&&o(or),t&&o(Q),V(Ot),t&&o(sr),t&&o(q),V(xt),V(ze),t&&o(rr),t&&o(oe),V(yt),t&&o(nr),t&&o(I),V(jt),V(Ee),t&&o(ar),t&&o(ne),V(zt),t&&o(ir),t&&o(F),V(Pt),V(Mt),V(Ce),t&&o(lr),t&&o(ae),V(Ct),t&&o(cr),t&&o(P),V(qt),V(It),V(Lt),V(Dt),t&&o(dr),t&&o(ie),V(Nt),t&&o(pr),t&&o(L),V(St),V(Bt),V(De),V(We),V(Ht),V(Ne),V(Se),V(Rt),V(Be),V(He),t&&o(mr),t&&o(pe),V(Ut),t&&o(hr),t&&o(me),V(Gt),V(Xt),V(Ue),V(Ge),t&&o(fr),t&&o(fe),V(Zt),t&&o(gr),t&&o(ge),V(Jt),V(Kt),V(Ze),V(Je),t&&o(ur),t&&o(_e),V(Yt),t&&o(_r),t&&o(we),V(Qt),V(eo),V(Ye),V(Qe)}}}const Kc={local:"owlvit",sections:[{local:"overview",title:"Overview"},{local:"usage",title:"Usage"},{local:"transformers.OwlViTConfig",title:"OwlViTConfig"},{local:"transformers.OwlViTTextConfig",title:"OwlViTTextConfig"},{local:"transformers.OwlViTVisionConfig",title:"OwlViTVisionConfig"},{local:"transformers.OwlViTFeatureExtractor",title:"OwlViTFeatureExtractor"},{local:"transformers.OwlViTProcessor",title:"OwlViTProcessor"},{local:"transformers.OwlViTModel",title:"OwlViTModel"},{local:"transformers.OwlViTTextModel",title:"OwlViTTextModel"},{local:"transformers.OwlViTVisionModel",title:"OwlViTVisionModel"},{local:"transformers.OwlViTForObjectDetection",title:"OwlViTForObjectDetection"}],title:"OWL-ViT"};function Yc(y){return qc(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class nd extends Pc{constructor(c){super();Ec(this,c,Yc,Jc,Mc,{})}}export{nd as default,Kc as metadata};
