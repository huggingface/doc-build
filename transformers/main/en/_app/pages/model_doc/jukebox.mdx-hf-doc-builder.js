import{S as jm,i as Cm,s as Vm,e as s,k as l,w as _,t as n,M as Tm,c as a,d as o,m as d,a as i,x as b,h as r,b as c,N as Am,G as e,g as u,y as v,q as k,o as x,B as w,v as zm,L as Zn}from"../../chunks/vendor-hf-doc-builder.js";import{T as Dm}from"../../chunks/Tip-hf-doc-builder.js";import{D as C}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Kn}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as fe}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Gn}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function qm(q){let p,$,g,f,E;return f=new Kn({props:{code:`from transformers import JukeboxModel, JukeboxConfig

# Initializing a Jukebox configuration
configuration = JukeboxConfig()

# Initializing a model from the configuration
model = JukeboxModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> JukeboxModel, JukeboxConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Jukebox configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = JukeboxConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = JukeboxModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){p=s("p"),$=n("Example:"),g=l(),_(f.$$.fragment)},l(m){p=a(m,"P",{});var y=i(p);$=r(y,"Example:"),y.forEach(o),g=d(m),b(f.$$.fragment,m)},m(m,y){u(m,p,y),e(p,$),u(m,g,y),v(f,m,y),E=!0},p:Zn,i(m){E||(k(f.$$.fragment,m),E=!0)},o(m){x(f.$$.fragment,m),E=!1},d(m){m&&o(p),m&&o(g),w(f,m)}}}function Mm(q){let p,$;return p=new Kn({props:{code:`from transformers import JukeboxTokenizer
tokenizer = JukeboxTokenizer.from_pretrained("openai/jukebox-1b-lyrics")
tokenizer("Alan Jackson", "Country Rock", "old town road")['input_ids']
`,highlighted:`&gt;&gt;&gt; from transformers import JukeboxTokenizer
&gt;&gt;&gt; tokenizer = JukeboxTokenizer.from_pretrained(<span class="hljs-string">&quot;openai/jukebox-1b-lyrics&quot;</span>)
&gt;&gt;&gt; tokenizer(<span class="hljs-string">&quot;Alan Jackson&quot;</span>, <span class="hljs-string">&quot;Country Rock&quot;</span>, <span class="hljs-string">&quot;old town road&quot;</span>)[<span class="hljs-string">&#x27;input_ids&#x27;</span>]
[tensor(<span class="hljs-string">[[   0,    0,    0, 6785,  546,   41,   38,   30,   76,   46,   41,   49,
           40,   76,   44,   41,   27,   30]]</span>), tensor(<span class="hljs-string">[[  0,   0,   0, 145,   0]]</span>), tensor(<span class="hljs-string">[[  0,   0,   0, 145,   0]]</span>)]
`}}),{c(){_(p.$$.fragment)},l(g){b(p.$$.fragment,g)},m(g,f){v(p,g,f),$=!0},p:Zn,i(g){$||(k(p.$$.fragment,g),$=!0)},o(g){x(p.$$.fragment,g),$=!1},d(g){w(p,g)}}}function Lm(q){let p,$;return{c(){p=s("p"),$=n("If nothing is provided, the genres and the artist will either be selected randomly or set to None")},l(g){p=a(g,"P",{});var f=i(p);$=r(f,"If nothing is provided, the genres and the artist will either be selected randomly or set to None"),f.forEach(o)},m(g,f){u(g,p,f),e(p,$)},d(g){g&&o(p)}}}function Qm(q){let p,$,g,f,E;return f=new Kn({props:{code:`from transformers import JukeboxTokenizer, JukeboxModel, set_seed

model = JukeboxModel.from_pretrained("openai/jukebox-1b-lyrics", min_duration=0).eval()
tokenizer = JukeboxTokenizer.from_pretrained("openai/jukebox-1b-lyrics")

lyrics = "Hey, are you awake? Can you talk to me?"
artist = "Zac Brown Band"
genre = "Country"
metas = tokenizer(artist=artist, genres=genre, lyrics=lyrics)
set_seed(0)
music_tokens = model.ancestral_sample(metas.input_ids, sample_length=400)

with torch.no_grad():
    model.decode(music_tokens)[:, :10].squeeze(-1)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> JukeboxTokenizer, JukeboxModel, set_seed

<span class="hljs-meta">&gt;&gt;&gt; </span>model = JukeboxModel.from_pretrained(<span class="hljs-string">&quot;openai/jukebox-1b-lyrics&quot;</span>, min_duration=<span class="hljs-number">0</span>).<span class="hljs-built_in">eval</span>()
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = JukeboxTokenizer.from_pretrained(<span class="hljs-string">&quot;openai/jukebox-1b-lyrics&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>lyrics = <span class="hljs-string">&quot;Hey, are you awake? Can you talk to me?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>artist = <span class="hljs-string">&quot;Zac Brown Band&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>genre = <span class="hljs-string">&quot;Country&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>metas = tokenizer(artist=artist, genres=genre, lyrics=lyrics)
<span class="hljs-meta">&gt;&gt;&gt; </span>set_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>music_tokens = model.ancestral_sample(metas.input_ids, sample_length=<span class="hljs-number">400</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    model.decode(music_tokens)[:, :<span class="hljs-number">10</span>].squeeze(-<span class="hljs-number">1</span>)
tensor([[-<span class="hljs-number">0.0219</span>, -<span class="hljs-number">0.0679</span>, -<span class="hljs-number">0.1050</span>, -<span class="hljs-number">0.1203</span>, -<span class="hljs-number">0.1271</span>, -<span class="hljs-number">0.0936</span>, -<span class="hljs-number">0.0396</span>, -<span class="hljs-number">0.0405</span>,
    -<span class="hljs-number">0.0818</span>, -<span class="hljs-number">0.0697</span>]])`}}),{c(){p=s("p"),$=n("Example:"),g=l(),_(f.$$.fragment)},l(m){p=a(m,"P",{});var y=i(p);$=r(y,"Example:"),y.forEach(o),g=d(m),b(f.$$.fragment,m)},m(m,y){u(m,p,y),e(p,$),u(m,g,y),v(f,m,y),E=!0},p:Zn,i(m){E||(k(f.$$.fragment,m),E=!0)},o(m){x(f.$$.fragment,m),E=!1},d(m){m&&o(p),m&&o(g),w(f,m)}}}function Im(q){let p,$,g,f,E;return f=new Kn({props:{code:`from transformers import JukeboxTokenizer, JukeboxModel, set_seed
import torch

metas = dict(artist="Zac Brown Band", genres="Country", lyrics="I met a traveller from an antique land")
tokenizer = JukeboxTokenizer.from_pretrained("openai/jukebox-1b-lyrics")
model = JukeboxModel.from_pretrained("openai/jukebox-1b-lyrics", min_duration=0).eval()

labels = tokenizer(**metas)["input_ids"]
set_seed(0)
zs = [torch.zeros(1, 0, dtype=torch.long) for _ in range(3)]
zs = model._sample(zs, labels, [0], sample_length=40 * model.priors[0].raw_to_tokens, save_results=False)
zs[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> JukeboxTokenizer, JukeboxModel, set_seed
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>metas = <span class="hljs-built_in">dict</span>(artist=<span class="hljs-string">&quot;Zac Brown Band&quot;</span>, genres=<span class="hljs-string">&quot;Country&quot;</span>, lyrics=<span class="hljs-string">&quot;I met a traveller from an antique land&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = JukeboxTokenizer.from_pretrained(<span class="hljs-string">&quot;openai/jukebox-1b-lyrics&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = JukeboxModel.from_pretrained(<span class="hljs-string">&quot;openai/jukebox-1b-lyrics&quot;</span>, min_duration=<span class="hljs-number">0</span>).<span class="hljs-built_in">eval</span>()

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(**metas)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>set_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>zs = [torch.zeros(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, dtype=torch.long) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>)]
<span class="hljs-meta">&gt;&gt;&gt; </span>zs = model._sample(zs, labels, [<span class="hljs-number">0</span>], sample_length=<span class="hljs-number">40</span> * model.priors[<span class="hljs-number">0</span>].raw_to_tokens, save_results=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>zs[<span class="hljs-number">0</span>]
tensor([[<span class="hljs-number">1853</span>, <span class="hljs-number">1369</span>, <span class="hljs-number">1150</span>, <span class="hljs-number">1869</span>, <span class="hljs-number">1379</span>, <span class="hljs-number">1789</span>,  <span class="hljs-number">519</span>,  <span class="hljs-number">710</span>, <span class="hljs-number">1306</span>, <span class="hljs-number">1100</span>, <span class="hljs-number">1229</span>,  <span class="hljs-number">519</span>,
      <span class="hljs-number">353</span>, <span class="hljs-number">1306</span>, <span class="hljs-number">1379</span>, <span class="hljs-number">1053</span>,  <span class="hljs-number">519</span>,  <span class="hljs-number">653</span>, <span class="hljs-number">1631</span>, <span class="hljs-number">1467</span>, <span class="hljs-number">1229</span>, <span class="hljs-number">1229</span>,   <span class="hljs-number">10</span>, <span class="hljs-number">1647</span>,
     <span class="hljs-number">1254</span>, <span class="hljs-number">1229</span>, <span class="hljs-number">1306</span>, <span class="hljs-number">1528</span>, <span class="hljs-number">1789</span>,  <span class="hljs-number">216</span>, <span class="hljs-number">1631</span>, <span class="hljs-number">1434</span>,  <span class="hljs-number">653</span>,  <span class="hljs-number">475</span>, <span class="hljs-number">1150</span>, <span class="hljs-number">1528</span>,
     <span class="hljs-number">1804</span>,  <span class="hljs-number">541</span>, <span class="hljs-number">1804</span>, <span class="hljs-number">1434</span>]])`}}),{c(){p=s("p"),$=n("Example:"),g=l(),_(f.$$.fragment)},l(m){p=a(m,"P",{});var y=i(p);$=r(y,"Example:"),y.forEach(o),g=d(m),b(f.$$.fragment,m)},m(m,y){u(m,p,y),e(p,$),u(m,g,y),v(f,m,y),E=!0},p:Zn,i(m){E||(k(f.$$.fragment,m),E=!0)},o(m){x(f.$$.fragment,m),E=!1},d(m){m&&o(p),m&&o(g),w(f,m)}}}function Nm(q){let p,$,g,f,E;return f=new Kn({props:{code:`from transformers import JukeboxVQVAE, set_seed
import torch

model = JukeboxVQVAE.from_pretrained("openai/jukebox-1b-lyrics").eval()
set_seed(0)
zs = [torch.randint(100, (4, 1))]
model.decode(zs).shape`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> JukeboxVQVAE, set_seed
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>model = JukeboxVQVAE.from_pretrained(<span class="hljs-string">&quot;openai/jukebox-1b-lyrics&quot;</span>).<span class="hljs-built_in">eval</span>()
<span class="hljs-meta">&gt;&gt;&gt; </span>set_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>zs = [torch.randint(<span class="hljs-number">100</span>, (<span class="hljs-number">4</span>, <span class="hljs-number">1</span>))]
<span class="hljs-meta">&gt;&gt;&gt; </span>model.decode(zs).shape
torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">8</span>, <span class="hljs-number">1</span>])`}}),{c(){p=s("p"),$=n("Example:"),g=l(),_(f.$$.fragment)},l(m){p=a(m,"P",{});var y=i(p);$=r(y,"Example:"),y.forEach(o),g=d(m),b(f.$$.fragment,m)},m(m,y){u(m,p,y),e(p,$),u(m,g,y),v(f,m,y),E=!0},p:Zn,i(m){E||(k(f.$$.fragment,m),E=!0)},o(m){x(f.$$.fragment,m),E=!1},d(m){m&&o(p),m&&o(g),w(f,m)}}}function Sm(q){let p,$,g,f,E,m,y,At,rs,Xn,ee,ge,zt,Ze,ss,Dt,as,Yn,_e,is,Ke,ls,ds,er,ot,cs,or,tt,qt,ms,tr,A,hs,Mt,us,ps,Xe,fs,gs,Lt,_s,bs,Qt,vs,ks,It,xs,ws,Nt,ys,$s,nr,nt,rt,Dd,rr,st,Js,sr,W,St,Es,Ps,Ye,js,Ot,Cs,Vs,Ts,R,As,Wt,zs,Ds,Ft,qs,Ms,Rt,Ls,Qs,Is,oe,Ns,Ht,Ss,Os,Ut,Ws,Fs,ar,Z,Rs,eo,Hs,Us,oo,Bs,Gs,ir,te,be,Bt,to,Zs,Gt,Ks,lr,V,no,Xs,ro,Ys,at,ea,oa,ta,H,na,it,ra,sa,lt,aa,ia,so,la,da,ca,Zt,ma,ha,ve,ua,ke,ao,pa,io,fa,dt,ga,_a,ba,xe,lo,va,co,ka,ct,xa,wa,dr,ne,we,Kt,mo,ya,Xt,$a,cr,N,ho,Ja,U,Ea,mt,Pa,ja,Yt,Ca,Va,uo,Ta,Aa,za,re,Da,ht,qa,Ma,ut,La,Qa,Ia,ye,po,Na,fo,Sa,pt,Oa,Wa,mr,se,$e,en,go,Fa,on,Ra,hr,B,_o,Ha,G,Ua,ft,Ba,Ga,tn,Za,Ka,bo,Xa,Ya,ei,ae,oi,gt,ti,ni,_t,ri,si,ur,ie,Je,nn,vo,ai,rn,ii,pr,J,ko,li,sn,di,ci,le,an,mi,hi,ln,ui,pi,dn,fi,gi,cn,_i,bi,xo,vi,mn,ki,xi,wi,Ee,yi,wo,$i,hn,Ji,Ei,Pi,Pe,ji,yo,Ci,bt,Vi,Ti,Ai,un,zi,Di,je,$o,qi,pn,Mi,fr,de,Ce,fn,Jo,Li,gn,Qi,gr,P,Eo,Ii,D,Ni,_n,Si,Oi,bn,Wi,Fi,vn,Ri,Hi,kn,Ui,Bi,xn,Gi,Zi,wn,Ki,Xi,Yi,Po,el,vt,ol,tl,nl,jo,rl,Co,sl,al,il,K,Vo,ll,To,dl,yn,cl,ml,hl,Ve,ul,Te,Ao,pl,zo,fl,$n,gl,_l,bl,Ae,Do,vl,Jn,kl,xl,ze,qo,wl,Mo,yl,En,$l,Jl,El,F,Lo,Pl,Pn,jl,Cl,jn,Vl,Tl,De,_r,ce,qe,Cn,Qo,Al,Vn,zl,br,S,Io,Dl,me,ql,Tn,Ml,Ll,An,Ql,Il,Nl,Me,No,Sl,zn,Ol,Wl,Le,So,Fl,O,Rl,Dn,Hl,Ul,qn,Bl,Gl,Mn,Zl,Kl,Ln,Xl,Yl,vr,he,Qe,Qn,Oo,ed,In,od,kr,T,Wo,td,Fo,nd,Ro,rd,sd,ad,Ho,id,kt,ld,dd,cd,Uo,md,Bo,hd,ud,pd,X,Go,fd,Zo,gd,Nn,_d,bd,vd,Ie,kd,Ne,Ko,xd,ue,wd,Sn,yd,$d,On,Jd,Ed,Pd,Se,Xo,jd,pe,Cd,Wn,Vd,Td,Fn,Ad,zd,xr;return m=new fe({}),Ze=new fe({}),to=new fe({}),no=new C({props:{name:"class transformers.JukeboxConfig",anchor:"transformers.JukeboxConfig",parameters:[{name:"vqvae_config",val:" = None"},{name:"prior_config_list",val:" = None"},{name:"nb_priors",val:" = 3"},{name:"sampling_rate",val:" = 44100"},{name:"timing_dims",val:" = 64"},{name:"min_duration",val:" = 0"},{name:"max_duration",val:" = 600.0"},{name:"max_nb_genres",val:" = 5"},{name:"metadata_conditioning",val:" = True"},{name:"init_std",val:" = 0.2"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.JukeboxConfig.vqvae_config",description:`<strong>vqvae_config</strong> (<code>JukeboxVQVAEConfig</code>, <em>optional</em>) &#x2014;
Configuration for the <code>JukeboxVQVAE</code> model.`,name:"vqvae_config"},{anchor:"transformers.JukeboxConfig.prior_config_list",description:`<strong>prior_config_list</strong> (<code>List[JukeboxPriorConfig]</code>, <em>optional</em>) &#x2014;
List of the configs for each of the <code>JukeboxPrior</code> of the model. The original architecture uses 3 priors.`,name:"prior_config_list"},{anchor:"transformers.JukeboxConfig.nb_priors",description:`<strong>nb_priors</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Number of prior models that will sequentially sample tokens. Each prior is conditional auto regressive
(decoder) model, apart from the top prior, which can include a lyric encoder. The available models were
trained using a top prior and 2 upsampler priors.`,name:"nb_priors"},{anchor:"transformers.JukeboxConfig.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, <em>optional</em>, defaults to 44100) &#x2014;
Sampling rate of the raw audio.`,name:"sampling_rate"},{anchor:"transformers.JukeboxConfig.timing_dims",description:`<strong>timing_dims</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensions of the JukeboxRangeEmbedding layer which is equivalent to traditional positional embedding
layer. The timing embedding layer converts the absolute and relative position in the currently sampled
audio to a tensor of length <code>timing_dims</code> that will be added to the music tokens.`,name:"timing_dims"},{anchor:"transformers.JukeboxConfig.min_duration",description:`<strong>min_duration</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Minimum duration of the audios to generate`,name:"min_duration"},{anchor:"transformers.JukeboxConfig.max_duration",description:`<strong>max_duration</strong> (<code>float</code>, <em>optional</em>, defaults to 600.0) &#x2014;
Maximum duration of the audios to generate`,name:"max_duration"},{anchor:"transformers.JukeboxConfig.max_nb_genres",description:`<strong>max_nb_genres</strong> (<code>int</code>, <em>optional</em>, defaults to 5) &#x2014;
Maximum number of genres that can be used to condition a single sample.`,name:"max_nb_genres"},{anchor:"transformers.JukeboxConfig.metadata_conditioning",description:`<strong>metadata_conditioning</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use metadata conditioning, corresponding to the artist, the genre and the min/maximum
duration.`,name:"metadata_conditioning"},{anchor:"transformers.JukeboxConfig.init_std",description:`<strong>init_std</strong> (<code>float</code>, <em>optional</em>, defaults to 0.2) &#x2014;
Standard deviation used to initial the model.`,name:"init_std"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/configuration_jukebox.py#L505"}}),ve=new Gn({props:{anchor:"transformers.JukeboxConfig.example",$$slots:{default:[qm]},$$scope:{ctx:q}}}),ao=new C({props:{name:"from_configs",anchor:"transformers.JukeboxConfig.from_configs",parameters:[{name:"prior_configs",val:": typing.List[transformers.models.jukebox.configuration_jukebox.JukeboxPriorConfig]"},{name:"vqvae_config",val:": JukeboxVQVAEConfig"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/configuration_jukebox.py#L614",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/jukebox#transformers.JukeboxConfig"
>JukeboxConfig</a></p>
`}}),lo=new C({props:{name:"to_dict",anchor:"transformers.JukeboxConfig.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/configuration_jukebox.py#L626",returnDescription:`
<p>Dictionary of all the attributes that make up this configuration instance,</p>
`,returnType:`
<p><code>Dict[str, any]</code></p>
`}}),mo=new fe({}),ho=new C({props:{name:"class transformers.JukeboxPriorConfig",anchor:"transformers.JukeboxPriorConfig",parameters:[{name:"act_fn",val:" = 'quick_gelu'"},{name:"level",val:" = 0"},{name:"alignment_head",val:" = 2"},{name:"alignment_layer",val:" = 68"},{name:"attention_multiplier",val:" = 0.25"},{name:"attention_pattern",val:" = 'enc_dec_with_lyrics'"},{name:"attn_dropout",val:" = 0"},{name:"attn_res_scale",val:" = False"},{name:"blocks",val:" = 64"},{name:"conv_res_scale",val:" = None"},{name:"num_layers",val:" = 72"},{name:"emb_dropout",val:" = 0"},{name:"encoder_config",val:" = None"},{name:"encoder_loss_fraction",val:" = 0.4"},{name:"hidden_size",val:" = 2048"},{name:"init_scale",val:" = 0.2"},{name:"is_encoder_decoder",val:" = True"},{name:"lyric_vocab_size",val:" = 80"},{name:"mask",val:" = False"},{name:"max_duration",val:" = 600"},{name:"max_nb_genres",val:" = 1"},{name:"merged_decoder",val:" = True"},{name:"metadata_conditioning",val:" = True"},{name:"metadata_dims",val:" = [604, 7898]"},{name:"min_duration",val:" = 0"},{name:"mlp_multiplier",val:" = 1.0"},{name:"music_vocab_size",val:" = 2048"},{name:"n_ctx",val:" = 6144"},{name:"n_heads",val:" = 2"},{name:"nb_relevant_lyric_tokens",val:" = 384"},{name:"res_conv_depth",val:" = 3"},{name:"res_conv_width",val:" = 128"},{name:"res_convolution_multiplier",val:" = 1"},{name:"res_dilation_cycle",val:" = None"},{name:"res_dilation_growth_rate",val:" = 1"},{name:"res_downs_t",val:" = [3, 2, 2]"},{name:"res_strides_t",val:" = [2, 2, 2]"},{name:"resid_dropout",val:" = 0"},{name:"sampling_rate",val:" = 44100"},{name:"spread",val:" = None"},{name:"timing_dims",val:" = 64"},{name:"zero_out",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.JukeboxPriorConfig.act_fn",description:`<strong>act_fn</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
Activation function.`,name:"act_fn"},{anchor:"transformers.JukeboxPriorConfig.alignment_head",description:`<strong>alignment_head</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Head that is responsible of the alignment between lyrics and music. Only used to compute the lyric to audio
alignment`,name:"alignment_head"},{anchor:"transformers.JukeboxPriorConfig.alignment_layer",description:`<strong>alignment_layer</strong> (<code>int</code>, <em>optional</em>, defaults to 68) &#x2014;
Index of the layer that is responsible of the alignment between lyrics and music. Only used to compute the
lyric to audio alignment`,name:"alignment_layer"},{anchor:"transformers.JukeboxPriorConfig.attention_multiplier",description:`<strong>attention_multiplier</strong> (<code>float</code>, <em>optional</em>, defaults to 0.25) &#x2014;
Multiplier coefficient used to define the hidden dimension of the attention layers. 0.25 means that
0.25*width of the model will be used.`,name:"attention_multiplier"},{anchor:"transformers.JukeboxPriorConfig.attention_pattern",description:`<strong>attention_pattern</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;enc_dec_with_lyrics&quot;</code>) &#x2014;
Which attention pattern to use for the decoder/`,name:"attention_pattern"},{anchor:"transformers.JukeboxPriorConfig.attn_dropout",description:`<strong>attn_dropout</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Dropout probability for the post-attention layer dropout in the decoder.`,name:"attn_dropout"},{anchor:"transformers.JukeboxPriorConfig.attn_res_scale",description:`<strong>attn_res_scale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to scale the residuals in the attention conditioner block.`,name:"attn_res_scale"},{anchor:"transformers.JukeboxPriorConfig.blocks",description:`<strong>blocks</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Number of blocks used in the <code>block_attn</code>. A sequence of length seq_len is factored as <code>[blocks, seq_len // blocks]</code> in the <code>JukeboxAttention</code> layer.`,name:"blocks"},{anchor:"transformers.JukeboxPriorConfig.conv_res_scale",description:`<strong>conv_res_scale</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Whether or not to scale the residuals in the conditioner block. Since the top level prior does not have a
conditioner, the default value is to None and should not be modified.`,name:"conv_res_scale"},{anchor:"transformers.JukeboxPriorConfig.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 72) &#x2014;
Number of layers of the transformer architecture.`,name:"num_layers"},{anchor:"transformers.JukeboxPriorConfig.emb_dropout",description:`<strong>emb_dropout</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Embedding dropout used in the lyric decoder.`,name:"emb_dropout"},{anchor:"transformers.JukeboxPriorConfig.encoder_config",description:`<strong>encoder_config</strong> (<code>JukeboxPriorConfig</code>, <em>optional</em>)  &#x2014;
Configuration of the encoder which models the prior on the lyrics.`,name:"encoder_config"},{anchor:"transformers.JukeboxPriorConfig.encoder_loss_fraction",description:`<strong>encoder_loss_fraction</strong> (<code>float</code>, <em>optional</em>, defaults to 0.4) &#x2014;
Multiplication factor used in front of the lyric encoder loss.`,name:"encoder_loss_fraction"},{anchor:"transformers.JukeboxPriorConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Hidden dimension of the attention layers.`,name:"hidden_size"},{anchor:"transformers.JukeboxPriorConfig.init_scale",description:`<strong>init_scale</strong> (<code>float</code>, <em>optional</em>, defaults to 0.2) &#x2014;
Initialization scales for the prior modules.`,name:"init_scale"},{anchor:"transformers.JukeboxPriorConfig.is_encoder_decoder",description:`<strong>is_encoder_decoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the prior is an encoder-decoder model. In case it is not, and <code>nb_relevant_lyric_tokens</code> is
greater than 0, the <code>encoder</code> args should be specified for the lyric encoding.`,name:"is_encoder_decoder"},{anchor:"transformers.JukeboxPriorConfig.mask",description:`<strong>mask</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to mask the previous positions in the attention.`,name:"mask"},{anchor:"transformers.JukeboxPriorConfig.max_duration",description:`<strong>max_duration</strong> (<code>int</code>, <em>optional</em>, defaults to 600) &#x2014;
Maximum supported duration of the generated song in seconds.`,name:"max_duration"},{anchor:"transformers.JukeboxPriorConfig.max_nb_genres",description:`<strong>max_nb_genres</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Maximum number of genres that can be used to condition the model.`,name:"max_nb_genres"},{anchor:"transformers.JukeboxPriorConfig.merged_decoder",description:`<strong>merged_decoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the decoder and the encoder inputs are merged. This is used for the separated
encoder-decoder architecture`,name:"merged_decoder"},{anchor:"transformers.JukeboxPriorConfig.metadata_conditioning",description:`<strong>metadata_conditioning</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True)</code> &#x2014;
Whether or not to condition on the artist and genre metadata.`,name:"metadata_conditioning"},{anchor:"transformers.JukeboxPriorConfig.metadata_dims",description:`<strong>metadata_dims</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[604, 7898]</code>) &#x2014;
Number of genres and the number of artists that were used to train the embedding layers of the prior
models.`,name:"metadata_dims"},{anchor:"transformers.JukeboxPriorConfig.min_duration",description:`<strong>min_duration</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Minimum duration of the generated audio on which the model was trained.`,name:"min_duration"},{anchor:"transformers.JukeboxPriorConfig.mlp_multiplier",description:`<strong>mlp_multiplier</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Multiplier coefficient used to define the hidden dimension of the MLP layers. 0.25 means that 0.25*width of
the model will be used.`,name:"mlp_multiplier"},{anchor:"transformers.JukeboxPriorConfig.music_vocab_size",description:`<strong>music_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Number of different music tokens. Should be similar to the <code>JukeboxVQVAEConfig.nb_discrete_codes</code>.`,name:"music_vocab_size"},{anchor:"transformers.JukeboxPriorConfig.n_ctx",description:`<strong>n_ctx</strong> (<code>int</code>, <em>optional</em>, defaults to 6144) &#x2014;
Number of context tokens for each prior. The context tokens are the music tokens that are attended to when
generating music tokens.`,name:"n_ctx"},{anchor:"transformers.JukeboxPriorConfig.n_heads",description:`<strong>n_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of attention heads.`,name:"n_heads"},{anchor:"transformers.JukeboxPriorConfig.nb_relevant_lyric_tokens",description:`<strong>nb_relevant_lyric_tokens</strong> (<code>int</code>, <em>optional</em>, defaults to 384) &#x2014;
Number of lyric tokens that are used when sampling a single window of length <code>n_ctx</code>`,name:"nb_relevant_lyric_tokens"},{anchor:"transformers.JukeboxPriorConfig.res_conv_depth",description:`<strong>res_conv_depth</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Depth of the <code>JukeboxDecoderConvBock</code> used to upsample the previously sampled audio in the
<code>JukeboxMusicTokenConditioner</code>.`,name:"res_conv_depth"},{anchor:"transformers.JukeboxPriorConfig.res_conv_width",description:`<strong>res_conv_width</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Width of the <code>JukeboxDecoderConvBock</code> used to upsample the previously sampled audio in the
<code>JukeboxMusicTokenConditioner</code>.`,name:"res_conv_width"},{anchor:"transformers.JukeboxPriorConfig.res_convolution_multiplier",description:`<strong>res_convolution_multiplier</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Multiplier used to scale the <code>hidden_dim</code> of the <code>JukeboxResConv1DBlock</code>.`,name:"res_convolution_multiplier"},{anchor:"transformers.JukeboxPriorConfig.res_dilation_cycle",description:`<strong>res_dilation_cycle</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Dilation cycle used to define the <code>JukeboxMusicTokenConditioner</code>. Usually similar to the ones used in the
corresponding level of the VQVAE. The first prior does not use it as it is not conditioned on upper level
tokens.`,name:"res_dilation_cycle"},{anchor:"transformers.JukeboxPriorConfig.res_dilation_growth_rate",description:`<strong>res_dilation_growth_rate</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Dilation grow rate used between each convolutionnal block of the <code>JukeboxMusicTokenConditioner</code>`,name:"res_dilation_growth_rate"},{anchor:"transformers.JukeboxPriorConfig.res_downs_t",description:`<strong>res_downs_t</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[3, 2, 2]</code>) &#x2014;
Downsampling rates used in the audio conditioning network`,name:"res_downs_t"},{anchor:"transformers.JukeboxPriorConfig.res_strides_t",description:`<strong>res_strides_t</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[2, 2, 2]</code>) &#x2014;
Striding used in the audio conditioning network`,name:"res_strides_t"},{anchor:"transformers.JukeboxPriorConfig.resid_dropout",description:`<strong>resid_dropout</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Residual dropout used in the attention pattern.`,name:"resid_dropout"},{anchor:"transformers.JukeboxPriorConfig.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, <em>optional</em>, defaults to 44100) &#x2014;
Sampling rate used for training.`,name:"sampling_rate"},{anchor:"transformers.JukeboxPriorConfig.spread",description:`<strong>spread</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Spread used in the <code>summary_spread_attention</code> pattern`,name:"spread"},{anchor:"transformers.JukeboxPriorConfig.timing_dims",description:`<strong>timing_dims</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimension of the timing embedding.`,name:"timing_dims"},{anchor:"transformers.JukeboxPriorConfig.zero_out",description:`<strong>zero_out</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to zero out convolution weights when initializing.`,name:"zero_out"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/configuration_jukebox.py#L144"}}),po=new C({props:{name:"to_dict",anchor:"transformers.JukeboxPriorConfig.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/configuration_jukebox.py#L370",returnDescription:`
<p>Dictionary of all the attributes that make up this configuration instance,</p>
`,returnType:`
<p><code>Dict[str, any]</code></p>
`}}),go=new fe({}),_o=new C({props:{name:"class transformers.JukeboxVQVAEConfig",anchor:"transformers.JukeboxVQVAEConfig",parameters:[{name:"act_fn",val:" = 'relu'"},{name:"nb_discrete_codes",val:" = 2048"},{name:"commit",val:" = 0.02"},{name:"conv_input_shape",val:" = 1"},{name:"conv_res_scale",val:" = False"},{name:"embed_dim",val:" = 64"},{name:"hop_fraction",val:" = [0.125, 0.5, 0.5]"},{name:"levels",val:" = 3"},{name:"lmu",val:" = 0.99"},{name:"multipliers",val:" = [2, 1, 1]"},{name:"res_conv_depth",val:" = 4"},{name:"res_conv_width",val:" = 32"},{name:"res_convolution_multiplier",val:" = 1"},{name:"res_dilation_cycle",val:" = None"},{name:"res_dilation_growth_rate",val:" = 3"},{name:"res_downs_t",val:" = [3, 2, 2]"},{name:"res_strides_t",val:" = [2, 2, 2]"},{name:"sample_length",val:" = 1058304"},{name:"init_scale",val:" = 0.2"},{name:"zero_out",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.JukeboxVQVAEConfig.act_fn",description:`<strong>act_fn</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;relu&quot;</code>) &#x2014;
Activation function of the model.`,name:"act_fn"},{anchor:"transformers.JukeboxVQVAEConfig.nb_discrete_codes",description:`<strong>nb_discrete_codes</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Number of codes of the VQVAE.`,name:"nb_discrete_codes"},{anchor:"transformers.JukeboxVQVAEConfig.commit",description:`<strong>commit</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
Commit loss multiplier.`,name:"commit"},{anchor:"transformers.JukeboxVQVAEConfig.conv_input_shape",description:`<strong>conv_input_shape</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of audio channels.`,name:"conv_input_shape"},{anchor:"transformers.JukeboxVQVAEConfig.conv_res_scale",description:`<strong>conv_res_scale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to scale the residuals of the <code>JukeboxResConv1DBlock</code>.`,name:"conv_res_scale"},{anchor:"transformers.JukeboxVQVAEConfig.embed_dim",description:`<strong>embed_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Embedding dimension of the codebook vectors.`,name:"embed_dim"},{anchor:"transformers.JukeboxVQVAEConfig.hop_fraction",description:`<strong>hop_fraction</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[0.125, 0.5, 0.5]</code>) &#x2014;
Fraction of non-intersecting window used when continuing the sampling process.`,name:"hop_fraction"},{anchor:"transformers.JukeboxVQVAEConfig.levels",description:`<strong>levels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Number of hierarchical levels that used in the VQVAE.`,name:"levels"},{anchor:"transformers.JukeboxVQVAEConfig.lmu",description:`<strong>lmu</strong> (<code>float</code>, <em>optional</em>, defaults to 0.99) &#x2014;
Used in the codebook update, exponential moving average coefficient. For more detail refer to Appendix A.1
of the original <a href="https://arxiv.org/pdf/1711.00937v2.pdf" rel="nofollow">VQVAE paper</a>`,name:"lmu"},{anchor:"transformers.JukeboxVQVAEConfig.multipliers",description:`<strong>multipliers</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[2, 1, 1]</code>) &#x2014;
Depth and width multipliers used for each level. Used on the <code>res_conv_width</code> and <code>res_conv_depth</code>`,name:"multipliers"},{anchor:"transformers.JukeboxVQVAEConfig.res_conv_depth",description:`<strong>res_conv_depth</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Depth of the encoder and decoder block. If no <code>multipliers</code> are used, this is the same for each level.`,name:"res_conv_depth"},{anchor:"transformers.JukeboxVQVAEConfig.res_conv_width",description:`<strong>res_conv_width</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Width of the encoder and decoder block. If no <code>multipliers</code> are used, this is the same for each level.`,name:"res_conv_width"},{anchor:"transformers.JukeboxVQVAEConfig.res_convolution_multiplier",description:`<strong>res_convolution_multiplier</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Scaling factor of the hidden dimension used in the <code>JukeboxResConv1DBlock</code>.`,name:"res_convolution_multiplier"},{anchor:"transformers.JukeboxVQVAEConfig.res_dilation_cycle",description:`<strong>res_dilation_cycle</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Dilation cycle value used in the <code>JukeboxResnet</code>. If an int is used, each new Conv1 block will have a depth
reduced by a power of <code>res_dilation_cycle</code>.`,name:"res_dilation_cycle"},{anchor:"transformers.JukeboxVQVAEConfig.res_dilation_growth_rate",description:`<strong>res_dilation_growth_rate</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Resnet dilation growth rate used in the VQVAE (dilation_growth_rate ** depth)`,name:"res_dilation_growth_rate"},{anchor:"transformers.JukeboxVQVAEConfig.res_downs_t",description:`<strong>res_downs_t</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[3, 2, 2]</code>) &#x2014;
Downsampling rate for each level of the hierarchical VQ-VAE.`,name:"res_downs_t"},{anchor:"transformers.JukeboxVQVAEConfig.res_strides_t",description:`<strong>res_strides_t</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[2, 2, 2]</code>) &#x2014;
Stride used for each level of the hierarchical VQ-VAE.`,name:"res_strides_t"},{anchor:"transformers.JukeboxVQVAEConfig.sample_length",description:`<strong>sample_length</strong> (<code>int</code>, <em>optional</em>, defaults to 1058304) &#x2014;
Provides the max input shape of the VQVAE. Is used to compute the input shape of each level.`,name:"sample_length"},{anchor:"transformers.JukeboxVQVAEConfig.init_scale",description:`<strong>init_scale</strong> (<code>float</code>, <em>optional</em>, defaults to 0.2) &#x2014;
Initialization scale.`,name:"init_scale"},{anchor:"transformers.JukeboxVQVAEConfig.zero_out",description:`<strong>zero_out</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to zero out convolution weights when initializing.`,name:"zero_out"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/configuration_jukebox.py#L383"}}),vo=new fe({}),ko=new C({props:{name:"class transformers.JukeboxTokenizer",anchor:"transformers.JukeboxTokenizer",parameters:[{name:"artists_file",val:""},{name:"genres_file",val:""},{name:"lyrics_file",val:""},{name:"version",val:" = ['v3', 'v2', 'v2']"},{name:"max_n_lyric_tokens",val:" = 512"},{name:"n_genres",val:" = 5"},{name:"unk_token",val:" = '<|endoftext|>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.JukeboxTokenizer.artists_file",description:`<strong>artists_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file which contains a mapping between artists and ids. The default file supports
both &#x201C;v2&#x201D; and &#x201C;v3&#x201D;`,name:"artists_file"},{anchor:"transformers.JukeboxTokenizer.genres_file",description:`<strong>genres_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file which contain a mapping between genres and ids.`,name:"genres_file"},{anchor:"transformers.JukeboxTokenizer.lyrics_file",description:`<strong>lyrics_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file which contains the accepted characters for the lyrics tokenization.`,name:"lyrics_file"},{anchor:"transformers.JukeboxTokenizer.version",description:`<strong>version</strong> (<code>List[str]</code>, <code>optional</code>, default to <code>[&quot;v3&quot;, &quot;v2&quot;, &quot;v2&quot;]</code>)  &#x2014;
List of the tokenizer versions. The <code>5b-lyrics</code>&#x2019;s top level prior model was trained using <code>v3</code> instead of
<code>v2</code>.`,name:"version"},{anchor:"transformers.JukeboxTokenizer.n_genres",description:`<strong>n_genres</strong> (<code>int</code>, <code>optional</code>, defaults to 1) &#x2014;
Maximum number of genres to use for composition.`,name:"n_genres"},{anchor:"transformers.JukeboxTokenizer.max_n_lyric_tokens",description:`<strong>max_n_lyric_tokens</strong> (<code>int</code>, <code>optional</code>, defaults to 512) &#x2014;
Maximum number of lyric tokens to keep.`,name:"max_n_lyric_tokens"},{anchor:"transformers.JukeboxTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|endoftext|&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/tokenization_jukebox.py#L60"}}),Ee=new Gn({props:{anchor:"transformers.JukeboxTokenizer.example",$$slots:{default:[Mm]},$$scope:{ctx:q}}}),Pe=new Dm({props:{$$slots:{default:[Lm]},$$scope:{ctx:q}}}),$o=new C({props:{name:"save_vocabulary",anchor:"transformers.JukeboxTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"transformers.JukeboxTokenizer.save_vocabulary.save_directory",description:`<strong>save_directory</strong> (<code>str</code>) &#x2014;
A path to the directory where to saved. It will be created if it doesn&#x2019;t exist.`,name:"save_directory"},{anchor:"transformers.JukeboxTokenizer.save_vocabulary.filename_prefix",description:`<strong>filename_prefix</strong> (<code>Optional[str]</code>, <em>optional</em>) &#x2014;
A prefix to add to the names of the files saved by the tokenizer.`,name:"filename_prefix"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/tokenization_jukebox.py#L373"}}),Jo=new fe({}),Eo=new C({props:{name:"class transformers.JukeboxModel",anchor:"transformers.JukeboxModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.JukeboxModel.config",description:`<strong>config</strong> (<code>JukeboxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/modeling_jukebox.py#L2297"}}),Vo=new C({props:{name:"ancestral_sample",anchor:"transformers.JukeboxModel.ancestral_sample",parameters:[{name:"labels",val:""},{name:"n_samples",val:" = 1"},{name:"**sampling_kwargs",val:""}],parametersDescription:[{anchor:"transformers.JukeboxModel.ancestral_sample.labels",description:`<strong>labels</strong> (<code>List[torch.LongTensor]</code>)  &#x2014;
List of length <code>n_sample</code>, and shape <code>(self.levels, 4 + self.config.max_nb_genre + lyric_sequence_length)</code> metadata such as <code>artist_id</code>, <code>genre_id</code> and the full list of lyric tokens
which are used to condition the generation.`,name:"labels"},{anchor:"transformers.JukeboxModel.ancestral_sample.n_samples",description:`<strong>n_samples</strong> (<code>int</code>, <em>optional</em>, default to 1)  &#x2014;
Number of samples to be generated in parallel.`,name:"n_samples"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/modeling_jukebox.py#L2571"}}),Ve=new Gn({props:{anchor:"transformers.JukeboxModel.ancestral_sample.example",$$slots:{default:[Qm]},$$scope:{ctx:q}}}),Ao=new C({props:{name:"primed_sample",anchor:"transformers.JukeboxModel.primed_sample",parameters:[{name:"raw_audio",val:""},{name:"labels",val:""},{name:"**sampling_kwargs",val:""}],parametersDescription:[{anchor:"transformers.JukeboxModel.primed_sample.raw_audio",description:`<strong>raw_audio</strong> (<code>List[torch.Tensor]</code> of length <code>n_samples</code> )  &#x2014;
A list of raw audio that will be used as conditioning information for each samples that will be
generated.`,name:"raw_audio"},{anchor:"transformers.JukeboxModel.primed_sample.labels",description:`<strong>labels</strong> (<code>List[torch.LongTensor]</code> of length <code>n_sample</code>, and shape <code>(self.levels, self.config.max_nb_genre + lyric_sequence_length)</code>  &#x2014;
List of metadata such as <code>artist_id</code>, <code>genre_id</code> and the full list of lyric tokens which are used to
condition the generation.`,name:"labels"},{anchor:"transformers.JukeboxModel.primed_sample.sampling_kwargs",description:`<strong>sampling_kwargs</strong> (<code>Dict[Any]</code>) &#x2014;
Various additional sampling arguments that are used by the <code>_sample</code> function. A detail list of the
arguments can bee seen in the <code>_sample</code> function documentation.`,name:"sampling_kwargs"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/modeling_jukebox.py#L2647"}}),Do=new C({props:{name:"continue_sample",anchor:"transformers.JukeboxModel.continue_sample",parameters:[{name:"music_tokens",val:""},{name:"labels",val:""},{name:"**sampling_kwargs",val:""}],parametersDescription:[{anchor:"transformers.JukeboxModel.continue_sample.music_tokens",description:`<strong>music_tokens</strong> (<code>List[torch.LongTensor]</code> of length <code>self.levels</code> )  &#x2014;
A sequence of music tokens which will be used as context to continue the sampling process. Should have
<code>self.levels</code> tensors, each corresponding to the generation at a certain level.`,name:"music_tokens"},{anchor:"transformers.JukeboxModel.continue_sample.labels",description:`<strong>labels</strong> (<code>List[torch.LongTensor]</code> of length <code>n_sample</code>, and shape <code>(self.levels, self.config.max_nb_genre + lyric_sequence_length)</code>  &#x2014;
List of metadata such as <code>artist_id</code>, <code>genre_id</code> and the full list of lyric tokens which are used to
condition the generation.`,name:"labels"},{anchor:"transformers.JukeboxModel.continue_sample.sampling_kwargs",description:`<strong>sampling_kwargs</strong> (<code>Dict[Any]</code>) &#x2014;
Various additional sampling arguments that are used by the <code>_sample</code> function. A detail list of the
arguments can bee seen in the <code>_sample</code> function documentation.`,name:"sampling_kwargs"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/modeling_jukebox.py#L2617"}}),qo=new C({props:{name:"upsample",anchor:"transformers.JukeboxModel.upsample",parameters:[{name:"music_tokens",val:""},{name:"labels",val:""},{name:"**sampling_kwargs",val:""}],parametersDescription:[{anchor:"transformers.JukeboxModel.upsample.music_tokens",description:`<strong>music_tokens</strong> (<code>List[torch.LongTensor]</code> of length <code>self.levels</code> )  &#x2014;
A sequence of music tokens which will be used as context to continue the sampling process. Should have
<code>self.levels</code> tensors, each corresponding to the generation at a certain level.`,name:"music_tokens"},{anchor:"transformers.JukeboxModel.upsample.labels",description:`<strong>labels</strong> (<code>List[torch.LongTensor]</code> of length <code>n_sample</code>, and shape <code>(self.levels, self.config.max_nb_genre + lyric_sequence_length)</code>  &#x2014;
List of metadata such as <code>artist_id</code>, <code>genre_id</code> and the full list of lyric tokens which are used to
condition the generation.`,name:"labels"},{anchor:"transformers.JukeboxModel.upsample.sampling_kwargs",description:`<strong>sampling_kwargs</strong> (<code>Dict[Any]</code>) &#x2014;
Various additional sampling arguments that are used by the <code>_sample</code> function. A detail list of the
arguments can bee seen in the <code>_sample</code> function documentation.`,name:"sampling_kwargs"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/modeling_jukebox.py#L2632"}}),Lo=new C({props:{name:"_sample",anchor:"transformers.JukeboxModel._sample",parameters:[{name:"music_tokens",val:""},{name:"labels",val:""},{name:"sample_levels",val:""},{name:"metas",val:" = None"},{name:"chunk_size",val:" = 32"},{name:"sampling_temperature",val:" = 0.98"},{name:"lower_batch_size",val:" = 16"},{name:"max_batch_size",val:" = 16"},{name:"sample_length_in_seconds",val:" = 24"},{name:"compute_alignments",val:" = False"},{name:"sample_tokens",val:" = None"},{name:"offset",val:" = 0"},{name:"save_results",val:" = True"},{name:"sample_length",val:" = None"}],parametersDescription:[{anchor:"transformers.JukeboxModel._sample.music_tokens",description:`<strong>music_tokens</strong> (<code>List[torch.LongTensor]</code>) &#x2014;
A sequence of music tokens of length <code>self.levels</code> which will be used as context to continue the
sampling process. Should have <code>self.levels</code> tensors, each corresponding to the generation at a certain
level.`,name:"music_tokens"},{anchor:"transformers.JukeboxModel._sample.labels",description:`<strong>labels</strong> (<code>List[torch.LongTensor]</code>) &#x2014;
List of length <code>n_sample</code>, and shape <code>(self.levels, 4 + self.config.max_nb_genre + lyric_sequence_length)</code> metadata such as <code>artist_id</code>, <code>genre_id</code> and the full list of lyric tokens
which are used to condition the generation.`,name:"labels"},{anchor:"transformers.JukeboxModel._sample.sample_levels",description:`<strong>sample_levels</strong> (<code>List[int]</code>) &#x2014;
List of the desired levels at which the sampling will be done. A level is equivalent to the index of
the prior in the list of priors`,name:"sample_levels"},{anchor:"transformers.JukeboxModel._sample.metas",description:`<strong>metas</strong> (<code>List[Any]</code>, <em>optional</em>) &#x2014;
Metadatas used to generate the <code>labels</code>`,name:"metas"},{anchor:"transformers.JukeboxModel._sample.chunk_size",description:`<strong>chunk_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Size of a chunk of audio, used to fill up the memory in chuncks to prevent OOM erros. Bigger chunks
means faster memory filling but more consumption.`,name:"chunk_size"},{anchor:"transformers.JukeboxModel._sample.sampling_temperature",description:`<strong>sampling_temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 0.98) &#x2014;
Temperature used to ajust the randomness of the sampling.`,name:"sampling_temperature"},{anchor:"transformers.JukeboxModel._sample.lower_batch_size",description:`<strong>lower_batch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Maximum batch size for the lower level priors`,name:"lower_batch_size"},{anchor:"transformers.JukeboxModel._sample.max_batch_size",description:`<strong>max_batch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Maximum batch size for the top level priors`,name:"max_batch_size"},{anchor:"transformers.JukeboxModel._sample.sample_length_in_seconds",description:`<strong>sample_length_in_seconds</strong> (<code>int</code>, <em>optional</em>, defaults to 24) &#x2014;
Desired length of the generation in seconds`,name:"sample_length_in_seconds"},{anchor:"transformers.JukeboxModel._sample.compute_alignments",description:`<strong>compute_alignments</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to compute the alignment between the lyrics and the audio using the top_prior`,name:"compute_alignments"},{anchor:"transformers.JukeboxModel._sample.sample_tokens",description:`<strong>sample_tokens</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Precise number of tokens that should be sampled at each level. This is mostly useful for running dummy
experiments`,name:"sample_tokens"},{anchor:"transformers.JukeboxModel._sample.offset",description:`<strong>offset</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Audio offset used as conditioning, corresponds to the starting sample in the music. If the offset is
greater than 0, the lyrics will be shifted take that intoaccount`,name:"offset"},{anchor:"transformers.JukeboxModel._sample.save_results",description:`<strong>save_results</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to save the intermediate results. If <code>True</code>, will generate a folder named with the start
time.`,name:"save_results"},{anchor:"transformers.JukeboxModel._sample.sample_length",description:`<strong>sample_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Desired length of the generation in samples.`,name:"sample_length"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/modeling_jukebox.py#L2432"}}),De=new Gn({props:{anchor:"transformers.JukeboxModel._sample.example",$$slots:{default:[Im]},$$scope:{ctx:q}}}),Qo=new fe({}),Io=new C({props:{name:"class transformers.JukeboxPrior",anchor:"transformers.JukeboxPrior",parameters:[{name:"config",val:": JukeboxPriorConfig"},{name:"level",val:" = None"},{name:"nb_priors",val:" = 3"},{name:"vqvae_encoder",val:" = None"},{name:"vqvae_decoder",val:" = None"}],parametersDescription:[{anchor:"transformers.JukeboxPrior.config",description:`<strong>config</strong> (<code>JukeboxPriorConfig</code>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.JukeboxPrior.level",description:`<strong>level</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Current level of the Prior. Should be in range <code>[0,nb_priors]</code>.`,name:"level"},{anchor:"transformers.JukeboxPrior.nb_priors",description:`<strong>nb_priors</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Total number of priors.`,name:"nb_priors"},{anchor:"transformers.JukeboxPrior.vqvae_encoder",description:`<strong>vqvae_encoder</strong> (<code>Callable</code>, <em>optional</em>) &#x2014;
Encoding method of the VQVAE encoder used in the forward pass of the model. Passing functions instead of
the vqvae module to avoid getting the parameters.`,name:"vqvae_encoder"},{anchor:"transformers.JukeboxPrior.vqvae_decoder",description:`<strong>vqvae_decoder</strong> (<code>Callable</code>, <em>optional</em>) &#x2014;
Decoding method of the VQVAE decoder used in the forward pass of the model. Passing functions instead of
the vqvae module to avoid getting the parameters.`,name:"vqvae_decoder"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/modeling_jukebox.py#L1771"}}),No=new C({props:{name:"sample",anchor:"transformers.JukeboxPrior.sample",parameters:[{name:"n_samples",val:""},{name:"music_tokens",val:" = None"},{name:"music_tokens_conds",val:" = None"},{name:"metadata",val:" = None"},{name:"temp",val:" = 1.0"},{name:"top_k",val:" = 0"},{name:"top_p",val:" = 0.0"},{name:"chunk_size",val:" = None"},{name:"sample_tokens",val:" = None"}],parametersDescription:[{anchor:"transformers.JukeboxPrior.sample.n_samples",description:`<strong>n_samples</strong> (<code>int</code>) &#x2014;
Number of samples to generate.`,name:"n_samples"},{anchor:"transformers.JukeboxPrior.sample.music_tokens",description:`<strong>music_tokens</strong> (<code>List[torch.LongTensor]</code>, <em>optional</em>) &#x2014;
Previously gemerated tokens at the current level. Used as context for the generation.`,name:"music_tokens"},{anchor:"transformers.JukeboxPrior.sample.music_tokens_conds",description:`<strong>music_tokens_conds</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>) &#x2014;
Upper-level music tokens generated by the previous prior model. Is <code>None</code> if the generation is not
conditionned on the upper-level tokens.`,name:"music_tokens_conds"},{anchor:"transformers.JukeboxPrior.sample.metadata",description:`<strong>metadata</strong> (<code>List[torch.LongTensor]</code>, <em>optional</em>) &#x2014;
List containing the metatdata tensor with the artist, genre and the lyric tokens.`,name:"metadata"},{anchor:"transformers.JukeboxPrior.sample.temp",description:`<strong>temp</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Sampling temperature.`,name:"temp"},{anchor:"transformers.JukeboxPrior.sample.top_k",description:`<strong>top_k</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Top k probabilities used for filtering.`,name:"top_k"},{anchor:"transformers.JukeboxPrior.sample.top_p",description:`<strong>top_p</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Top p probabilities used for filtering.`,name:"top_p"},{anchor:"transformers.JukeboxPrior.sample.chunk_size",description:`<strong>chunk_size</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Size of the chunks used to prepare the cache of the transformer.`,name:"chunk_size"},{anchor:"transformers.JukeboxPrior.sample.sample_tokens",description:`<strong>sample_tokens</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Number of tokens to sample.`,name:"sample_tokens"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/modeling_jukebox.py#L2063"}}),So=new C({props:{name:"forward",anchor:"transformers.JukeboxPrior.forward",parameters:[{name:"hidden_states",val:""},{name:"metadata",val:" = None"},{name:"decode",val:" = False"},{name:"get_preds",val:" = False"}],parametersDescription:[{anchor:"transformers.JukeboxPrior.forward.hidden_states",description:`<strong>hidden_states</strong> (<code>torch.Tensor</code>) &#x2014;
Hidden states which should be raw audio`,name:"hidden_states"},{anchor:"transformers.JukeboxPrior.forward.metadata",description:`<strong>metadata</strong> (<code>List[torch.LongTensor]</code>, <em>optional</em>) &#x2014;
List containing the metadata conditioning tensorwith the lyric and the metadata tokens.`,name:"metadata"},{anchor:"transformers.JukeboxPrior.forward.decode",description:`<strong>decode</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to decode the encoded to tokens.`,name:"decode"},{anchor:"transformers.JukeboxPrior.forward.get_preds",description:`<strong>get_preds</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the actual predicitons of the model.`,name:"get_preds"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/modeling_jukebox.py#L2231"}}),Oo=new fe({}),Wo=new C({props:{name:"class transformers.JukeboxVQVAE",anchor:"transformers.JukeboxVQVAE",parameters:[{name:"config",val:": JukeboxVQVAEConfig"}],parametersDescription:[{anchor:"transformers.JukeboxVQVAE.config",description:`<strong>config</strong> (<code>JukeboxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/modeling_jukebox.py#L602"}}),Go=new C({props:{name:"forward",anchor:"transformers.JukeboxVQVAE.forward",parameters:[{name:"raw_audio",val:""}],parametersDescription:[{anchor:"transformers.JukeboxVQVAE.forward.raw_audio",description:`<strong>raw_audio</strong> (<code>torch.FloatTensor</code>) &#x2014;
Audio input which will be encoded and decoded.`,name:"raw_audio"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/modeling_jukebox.py#L740",returnDescription:`
<p><code>Tuple[torch.Tensor, torch.Tensor</code></p>
`}}),Ie=new Gn({props:{anchor:"transformers.JukeboxVQVAE.forward.example",$$slots:{default:[Nm]},$$scope:{ctx:q}}}),Ko=new C({props:{name:"encode",anchor:"transformers.JukeboxVQVAE.encode",parameters:[{name:"input_audio",val:""},{name:"start_level",val:" = 0"},{name:"end_level",val:" = None"},{name:"bs_chunks",val:" = 1"}],parametersDescription:[{anchor:"transformers.JukeboxVQVAE.encode.input_audio",description:`<strong>input_audio</strong> (<code>torch.Tensor</code>) &#x2014;
Raw audio which will be encoded to its discrete representation using the codebook. The closest <code>code</code>
form the codebook will be computed for each sequence of samples.`,name:"input_audio"},{anchor:"transformers.JukeboxVQVAE.encode.start_level",description:`<strong>start_level</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Level at which the encoding process will start. Default to 0.`,name:"start_level"},{anchor:"transformers.JukeboxVQVAE.encode.end_level",description:`<strong>end_level</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Level at which the encoding process will start. Default to None.`,name:"end_level"},{anchor:"transformers.JukeboxVQVAE.encode.bs_chunks",description:`<strong>bs_chunks</strong> (int, <em>optional</em>, defaults to 1) &#x2014;
Number of chunks of raw audio to process at the same time.`,name:"bs_chunks"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/modeling_jukebox.py#L710"}}),Xo=new C({props:{name:"decode",anchor:"transformers.JukeboxVQVAE.decode",parameters:[{name:"music_tokens",val:""},{name:"start_level",val:" = 0"},{name:"end_level",val:" = None"},{name:"bs_chunks",val:" = 1"}],parametersDescription:[{anchor:"transformers.JukeboxVQVAE.decode.music_tokens",description:`<strong>music_tokens</strong> (<code>torch.LongTensor</code>) &#x2014;
Tensor of music tokens which will be decoded to raw audio by using the codebook. Each music token
should be an index to a corresponding <code>code</code> vector in the codebook.`,name:"music_tokens"},{anchor:"transformers.JukeboxVQVAE.decode.start_level",description:`<strong>start_level</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Level at which the decoding process will start. Default to 0.`,name:"start_level"},{anchor:"transformers.JukeboxVQVAE.decode.end_level",description:`<strong>end_level</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Level at which the decoding process will start. Default to None.`,name:"end_level"},{anchor:"transformers.JukeboxVQVAE.decode.bs_chunks",description:`<strong>bs_chunks</strong> (int, <em>optional</em>) &#x2014;
Number of chunks to process at the same time.`,name:"bs_chunks"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/jukebox/modeling_jukebox.py#L674"}}),{c(){p=s("meta"),$=l(),g=s("h1"),f=s("a"),E=s("span"),_(m.$$.fragment),y=l(),At=s("span"),rs=n("Jukebox"),Xn=l(),ee=s("h2"),ge=s("a"),zt=s("span"),_(Ze.$$.fragment),ss=l(),Dt=s("span"),as=n("Overview"),Yn=l(),_e=s("p"),is=n("The Jukebox model was proposed in "),Ke=s("a"),ls=n("Jukebox: A generative model for music"),ds=n(`
by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford,
Ilya Sutskever. It introduces a generative music model which can produce minute long samples that can be conditionned on
an artist, genres and lyrics.`),er=l(),ot=s("p"),cs=n("The abstract from the paper is the following:"),or=l(),tt=s("p"),qt=s("em"),ms=n("We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples, along with model weights and code."),tr=l(),A=s("p"),hs=n("As shown on the following figure, Jukebox is made of 3 "),Mt=s("code"),us=n("priors"),ps=n(" which are decoder only models. They follow the architecture described in "),Xe=s("a"),fs=n("Generating Long Sequences with Sparse Transformers"),gs=n(`, modified to support longer context length.
First, a autoencoder is used to encode the text lyrics. Next, the first (also called `),Lt=s("code"),_s=n("top_prior"),bs=n(") prior attends to the last hidden states extracted from the lyrics encoder. The priors are linked to the previous priors respectively via an "),Qt=s("code"),vs=n("AudioConditionner"),ks=n(" module. The"),It=s("code"),xs=n("AudioConditioner"),ws=n(` upsamples the outputs of the previous prior to raw tokens at a certain audio frame per second resolution.
The metadata such as `),Nt=s("em"),ys=n("artist, genre and timing"),$s=n(" are passed to each prior, in the form of a start token and positionnal embedding for the timing data.  The hidden states are mapped to the closest codebook vector from the VQVAE in order to convert them to raw audio."),nr=l(),nt=s("p"),rt=s("img"),rr=l(),st=s("p"),Js=n("Tips:"),sr=l(),W=s("ul"),St=s("li"),Es=n("This model only supports inference. This is for a few reasons, mostly because it requires a crazy amount of memory to train. Feel free to open a PR and add what\u2019s missing to have a full integration with the hugging face traineer!"),Ps=l(),Ye=s("li"),js=n("This model is very slow, and takes 8h to generate a minute long audio using the 5b top prior on a V100 GPU. In order automaticallay handle the device on which the model should execute, use "),Ot=s("code"),Cs=n("accelerate"),Vs=n("."),Ts=l(),R=s("li"),As=n("Contrary to the paper, the order of the priors goes from "),Wt=s("code"),zs=n("0"),Ds=n(" to "),Ft=s("code"),qs=n("1"),Ms=n(" as it felt more intuitive : we sample starting from "),Rt=s("code"),Ls=n("0"),Qs=n("."),Is=l(),oe=s("li"),Ns=n("Primed sampling (conditionning the sampling on raw audio) requires more memory than ancestral sampling and should be used with "),Ht=s("code"),Ss=n("fp16"),Os=n(" set to "),Ut=s("code"),Ws=n("True"),Fs=n("."),ar=l(),Z=s("p"),Rs=n("This model was contributed by "),eo=s("a"),Hs=n("Arthur Zucker"),Us=n(`.
The original code can be found `),oo=s("a"),Bs=n("here"),Gs=n("."),ir=l(),te=s("h2"),be=s("a"),Bt=s("span"),_(to.$$.fragment),Zs=l(),Gt=s("span"),Ks=n("JukeboxConfig"),lr=l(),V=s("div"),_(no.$$.fragment),Xs=l(),ro=s("p"),Ys=n("This is the configuration class to store the configuration of a "),at=s("a"),ea=n("JukeboxModel"),oa=n("."),ta=l(),H=s("p"),na=n("Configuration objects inherit from "),it=s("a"),ra=n("PretrainedConfig"),sa=n(` and can be used to control the model outputs. Read the
documentation from `),lt=s("a"),aa=n("PretrainedConfig"),ia=n(` for more information. Instantiating a configuration with the defaults will
yield a similar configuration to that of
`),so=s("a"),la=n("openai/jukebox-1b-lyrics"),da=n(" architecture."),ca=l(),Zt=s("p"),ma=n(`The downsampling and stride are used to determine downsampling of the input sequence. For example, downsampling =
(5,3), and strides = (2, 2) will downsample the audio by 2^5 = 32 to get the first level of codes, and 2**8 = 256
to get the second level codes. This is mostly true for training the top level prior and the upsamplers.`),ha=l(),_(ve.$$.fragment),ua=l(),ke=s("div"),_(ao.$$.fragment),pa=l(),io=s("p"),fa=n("Instantiate a "),dt=s("a"),ga=n("JukeboxConfig"),_a=n(` (or a derived class) from clip text model configuration and clip vision model
configuration.`),ba=l(),xe=s("div"),_(lo.$$.fragment),va=l(),co=s("p"),ka=n("Serializes this instance to a Python dictionary. Override the default "),ct=s("a"),xa=n("to_dict()"),wa=n("."),dr=l(),ne=s("h2"),we=s("a"),Kt=s("span"),_(mo.$$.fragment),ya=l(),Xt=s("span"),$a=n("JukeboxPriorConfig"),cr=l(),N=s("div"),_(ho.$$.fragment),Ja=l(),U=s("p"),Ea=n("This is the configuration class to store the configuration of a "),mt=s("a"),Pa=n("JukeboxPrior"),ja=n(`. It is used to instantiate a
`),Yt=s("code"),Ca=n("JukeboxPrior"),Va=n(` according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the top level prior from the
[openai/jukebox-1b-lyrics](`),uo=s("a"),Ta=n("https://huggingface.co/openai/jukebox"),Aa=n(`
-1b-lyrics) architecture.`),za=l(),re=s("p"),Da=n("Configuration objects inherit from "),ht=s("a"),qa=n("PretrainedConfig"),Ma=n(` and can be used to control the model outputs. Read the
documentation from `),ut=s("a"),La=n("PretrainedConfig"),Qa=n(" for more information."),Ia=l(),ye=s("div"),_(po.$$.fragment),Na=l(),fo=s("p"),Sa=n("Serializes this instance to a Python dictionary. Override the default "),pt=s("a"),Oa=n("to_dict()"),Wa=n("."),mr=l(),se=s("h2"),$e=s("a"),en=s("span"),_(go.$$.fragment),Fa=l(),on=s("span"),Ra=n("JukeboxVQVAEConfig"),hr=l(),B=s("div"),_(_o.$$.fragment),Ha=l(),G=s("p"),Ua=n("This is the configuration class to store the configuration of a "),ft=s("a"),Ba=n("JukeboxVQVAE"),Ga=n(`. It is used to instantiate a
`),tn=s("code"),Za=n("JukeboxVQVAE"),Ka=n(` according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the VQVAE from
`),bo=s("a"),Xa=n("openai/jukebox-1b-lyrics"),Ya=n(" architecture."),ei=l(),ae=s("p"),oi=n("Configuration objects inherit from "),gt=s("a"),ti=n("PretrainedConfig"),ni=n(` and can be used to control the model outputs. Read the
documentation from `),_t=s("a"),ri=n("PretrainedConfig"),si=n(" for more information."),ur=l(),ie=s("h2"),Je=s("a"),nn=s("span"),_(vo.$$.fragment),ai=l(),rn=s("span"),ii=n("JukeboxTokenizer"),pr=l(),J=s("div"),_(ko.$$.fragment),li=l(),sn=s("p"),di=n("Constructs a Jukebox tokenizer. Jukebox can be conditioned on 3 different inputs :"),ci=l(),le=s("ul"),an=s("li"),mi=n("Artists, unique ids are associated to each artist from the provided dictionary."),hi=l(),ln=s("li"),ui=n("Genres, unique ids are associated to each genre from the provided dictionary."),pi=l(),dn=s("li"),fi=n(`Lyrics, character based tokenization. Must be initialized with the list of characters that are inside the
vocabulary.`),gi=l(),cn=s("p"),_i=n(`This tokenizer does not require training. It should be able to process a different number of inputs:
as the conditioning of the model can be done on the three different queries. If None is provided, defaults values will be used.:`),bi=l(),xo=s("p"),vi=n("Depending on the number of genres on which the model should be conditioned ("),mn=s("code"),ki=n("n_genres"),xi=n(")."),wi=l(),_(Ee.$$.fragment),yi=l(),wo=s("p"),$i=n("You can get around that behavior by passing "),hn=s("code"),Ji=n("add_prefix_space=True"),Ei=n(` when instantiating this tokenizer or when you
call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.`),Pi=l(),_(Pe.$$.fragment),ji=l(),yo=s("p"),Ci=n("This tokenizer inherits from "),bt=s("a"),Vi=n("PreTrainedTokenizer"),Ti=n(` which contains most of the main methods. Users should refer to:
this superclass for more information regarding those methods.`),Ai=l(),un=s("p"),zi=n("However the code does not allow that and only supports composing from various genres."),Di=l(),je=s("div"),_($o.$$.fragment),qi=l(),pn=s("p"),Mi=n("Saves the tokenizer\u2019s vocabulary dictionary to the provided save_directory."),fr=l(),de=s("h2"),Ce=s("a"),fn=s("span"),_(Jo.$$.fragment),Li=l(),gn=s("span"),Qi=n("JukeboxModel"),gr=l(),P=s("div"),_(Eo.$$.fragment),Ii=l(),D=s("p"),Ni=n("The bare JUKEBOX Model used for music generation. 4 sampling techniques are supported : "),_n=s("code"),Si=n("primed_sample"),Oi=n(", "),bn=s("code"),Wi=n("upsample"),Fi=n(`,
`),vn=s("code"),Ri=n("continue_sample"),Hi=n(" and "),kn=s("code"),Ui=n("ancestral_sample"),Bi=n(". It does not have a "),xn=s("code"),Gi=n("forward"),Zi=n(` method as the training is not end to end. If
you want to fine-tune the model, it is recommended to use the `),wn=s("code"),Ki=n("JukeboxPrior"),Xi=n(` class and train each prior
individually.`),Yi=l(),Po=s("p"),el=n("This model inherits from "),vt=s("a"),ol=n("PreTrainedModel"),tl=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),nl=l(),jo=s("p"),rl=n("This model is also a PyTorch "),Co=s("a"),sl=n("torch.nn.Module"),al=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),il=l(),K=s("div"),_(Vo.$$.fragment),ll=l(),To=s("p"),dl=n("Generates music tokens based on the provided "),yn=s("code"),cl=n("labels. Will start at the desired prior level and automatically upsample the sequence. If you want to create the audio, you should call "),ml=n("model.decode(tokens)`, which will use\nthe VQ-VAE decoder to convert the music tokens to raw audio."),hl=l(),_(Ve.$$.fragment),ul=l(),Te=s("div"),_(Ao.$$.fragment),pl=l(),zo=s("p"),fl=n("Generate a raw audio conditioned on the provided "),$n=s("code"),gl=n("raw_audio"),_l=n(` which is used as conditioning at each of the
generation levels. The audio is encoded to music tokens using the 3 levels of the VQ-VAE. These tokens are
used: as conditioning for each level, which means that no ancestral sampling is required.`),bl=l(),Ae=s("div"),_(Do.$$.fragment),vl=l(),Jn=s("p"),kl=n("Generates a continuation of the previously generated tokens."),xl=l(),ze=s("div"),_(qo.$$.fragment),wl=l(),Mo=s("p"),yl=n("Upsamples a sequence of music tokens using the prior at level "),En=s("code"),$l=n("level"),Jl=n("."),El=l(),F=s("div"),_(Lo.$$.fragment),Pl=l(),Pn=s("p"),jl=n(`Core sampling function used to generate music tokens. Iterates over the provided list of levels, while saving
the generated raw audio at each step.`),Cl=l(),jn=s("p"),Vl=n("Returns: torch.Tensor"),Tl=l(),_(De.$$.fragment),_r=l(),ce=s("h2"),qe=s("a"),Cn=s("span"),_(Qo.$$.fragment),Al=l(),Vn=s("span"),zl=n("JukeboxPrior"),br=l(),S=s("div"),_(Io.$$.fragment),Dl=l(),me=s("p"),ql=n(`The JukeboxPrior class, which is a wrapper around the various conditioning and the transformer. JukeboxPrior can be
seen as language models trained on music. They model the next `),Tn=s("code"),Ml=n("music token"),Ll=n(" prediction task. If a (lyric) "),An=s("code"),Ql=n("encoder\xF9 is defined, it also models the "),Il=n("next character` prediction on the lyrics. Can be conditionned on timing, artist,\ngenre, lyrics and codes from lower-levels Priors."),Nl=l(),Me=s("div"),_(No.$$.fragment),Sl=l(),zn=s("p"),Ol=n("Ancestral/Prime sampling a window of tokens using the provided conditioning and metadatas."),Wl=l(),Le=s("div"),_(So.$$.fragment),Fl=l(),O=s("p"),Rl=n("Encode the hidden states using the "),Dn=s("code"),Hl=n("vqvae"),Ul=n(" encoder, and then predicts the next token in the "),qn=s("code"),Bl=n("forward_tokens"),Gl=n(`
function. The loss is the sum of the `),Mn=s("code"),Zl=n("encoder"),Kl=n(" loss and the "),Ln=s("code"),Xl=n("decoder"),Yl=n(" loss."),vr=l(),he=s("h2"),Qe=s("a"),Qn=s("span"),_(Oo.$$.fragment),ed=l(),In=s("span"),od=n("JukeboxVQVAE"),kr=l(),T=s("div"),_(Wo.$$.fragment),td=l(),Fo=s("p"),nd=n("The Hierarchical VQ-VAE model used in Jukebox. This model follows the Hierarchical VQVAE paper from "),Ro=s("a"),rd=n(`Will Williams, Sam
Ringer, Tom Ash, John Hughes, David MacLeod, Jamie Dougherty`),sd=n("."),ad=l(),Ho=s("p"),id=n("This model inherits from "),kt=s("a"),ld=n("PreTrainedModel"),dd=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),cd=l(),Uo=s("p"),md=n("This model is also a PyTorch "),Bo=s("a"),hd=n("torch.nn.Module"),ud=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),pd=l(),X=s("div"),_(Go.$$.fragment),fd=l(),Zo=s("p"),gd=n("Forward pass of the VQ-VAE, encodes the "),Nn=s("code"),_d=n("raw_audio"),bd=n(` to latent states, which are then decoded for each level.
The commit loss, which ensure that the encoder\u2019s computed embeddings are close to the codebook vectors, is
computed.`),vd=l(),_(Ie.$$.fragment),kd=l(),Ne=s("div"),_(Ko.$$.fragment),xd=l(),ue=s("p"),wd=n("Transforms the "),Sn=s("code"),yd=n("input_audio"),$d=n(" to a discrete representation made out of "),On=s("code"),Jd=n("music_tokens"),Ed=n("."),Pd=l(),Se=s("div"),_(Xo.$$.fragment),jd=l(),pe=s("p"),Cd=n("Transforms the input "),Wn=s("code"),Vd=n("music_tokens"),Td=n(" to their "),Fn=s("code"),Ad=n("raw_audio"),zd=n(" representation."),this.h()},l(t){const h=Tm('[data-svelte="svelte-1phssyn"]',document.head);p=a(h,"META",{name:!0,content:!0}),h.forEach(o),$=d(t),g=a(t,"H1",{class:!0});var Yo=i(g);f=a(Yo,"A",{id:!0,class:!0,href:!0});var Rn=i(f);E=a(Rn,"SPAN",{});var Hn=i(E);b(m.$$.fragment,Hn),Hn.forEach(o),Rn.forEach(o),y=d(Yo),At=a(Yo,"SPAN",{});var Un=i(At);rs=r(Un,"Jukebox"),Un.forEach(o),Yo.forEach(o),Xn=d(t),ee=a(t,"H2",{class:!0});var et=i(ee);ge=a(et,"A",{id:!0,class:!0,href:!0});var Bn=i(ge);zt=a(Bn,"SPAN",{});var qd=i(zt);b(Ze.$$.fragment,qd),qd.forEach(o),Bn.forEach(o),ss=d(et),Dt=a(et,"SPAN",{});var Md=i(Dt);as=r(Md,"Overview"),Md.forEach(o),et.forEach(o),Yn=d(t),_e=a(t,"P",{});var wr=i(_e);is=r(wr,"The Jukebox model was proposed in "),Ke=a(wr,"A",{href:!0,rel:!0});var Ld=i(Ke);ls=r(Ld,"Jukebox: A generative model for music"),Ld.forEach(o),ds=r(wr,`
by Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford,
Ilya Sutskever. It introduces a generative music model which can produce minute long samples that can be conditionned on
an artist, genres and lyrics.`),wr.forEach(o),er=d(t),ot=a(t,"P",{});var Qd=i(ot);cs=r(Qd,"The abstract from the paper is the following:"),Qd.forEach(o),or=d(t),tt=a(t,"P",{});var Id=i(tt);qt=a(Id,"EM",{});var Nd=i(qt);ms=r(Nd,"We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples, along with model weights and code."),Nd.forEach(o),Id.forEach(o),tr=d(t),A=a(t,"P",{});var M=i(A);hs=r(M,"As shown on the following figure, Jukebox is made of 3 "),Mt=a(M,"CODE",{});var Sd=i(Mt);us=r(Sd,"priors"),Sd.forEach(o),ps=r(M," which are decoder only models. They follow the architecture described in "),Xe=a(M,"A",{href:!0,rel:!0});var Od=i(Xe);fs=r(Od,"Generating Long Sequences with Sparse Transformers"),Od.forEach(o),gs=r(M,`, modified to support longer context length.
First, a autoencoder is used to encode the text lyrics. Next, the first (also called `),Lt=a(M,"CODE",{});var Wd=i(Lt);_s=r(Wd,"top_prior"),Wd.forEach(o),bs=r(M,") prior attends to the last hidden states extracted from the lyrics encoder. The priors are linked to the previous priors respectively via an "),Qt=a(M,"CODE",{});var Fd=i(Qt);vs=r(Fd,"AudioConditionner"),Fd.forEach(o),ks=r(M," module. The"),It=a(M,"CODE",{});var Rd=i(It);xs=r(Rd,"AudioConditioner"),Rd.forEach(o),ws=r(M,` upsamples the outputs of the previous prior to raw tokens at a certain audio frame per second resolution.
The metadata such as `),Nt=a(M,"EM",{});var Hd=i(Nt);ys=r(Hd,"artist, genre and timing"),Hd.forEach(o),$s=r(M," are passed to each prior, in the form of a start token and positionnal embedding for the timing data.  The hidden states are mapped to the closest codebook vector from the VQVAE in order to convert them to raw audio."),M.forEach(o),nr=d(t),nt=a(t,"P",{});var Ud=i(nt);rt=a(Ud,"IMG",{src:!0,alt:!0}),Ud.forEach(o),rr=d(t),st=a(t,"P",{});var Bd=i(st);Js=r(Bd,"Tips:"),Bd.forEach(o),sr=d(t),W=a(t,"UL",{});var Oe=i(W);St=a(Oe,"LI",{});var Gd=i(St);Es=r(Gd,"This model only supports inference. This is for a few reasons, mostly because it requires a crazy amount of memory to train. Feel free to open a PR and add what\u2019s missing to have a full integration with the hugging face traineer!"),Gd.forEach(o),Ps=d(Oe),Ye=a(Oe,"LI",{});var yr=i(Ye);js=r(yr,"This model is very slow, and takes 8h to generate a minute long audio using the 5b top prior on a V100 GPU. In order automaticallay handle the device on which the model should execute, use "),Ot=a(yr,"CODE",{});var Zd=i(Ot);Cs=r(Zd,"accelerate"),Zd.forEach(o),Vs=r(yr,"."),yr.forEach(o),Ts=d(Oe),R=a(Oe,"LI",{});var We=i(R);As=r(We,"Contrary to the paper, the order of the priors goes from "),Wt=a(We,"CODE",{});var Kd=i(Wt);zs=r(Kd,"0"),Kd.forEach(o),Ds=r(We," to "),Ft=a(We,"CODE",{});var Xd=i(Ft);qs=r(Xd,"1"),Xd.forEach(o),Ms=r(We," as it felt more intuitive : we sample starting from "),Rt=a(We,"CODE",{});var Yd=i(Rt);Ls=r(Yd,"0"),Yd.forEach(o),Qs=r(We,"."),We.forEach(o),Is=d(Oe),oe=a(Oe,"LI",{});var xt=i(oe);Ns=r(xt,"Primed sampling (conditionning the sampling on raw audio) requires more memory than ancestral sampling and should be used with "),Ht=a(xt,"CODE",{});var ec=i(Ht);Ss=r(ec,"fp16"),ec.forEach(o),Os=r(xt," set to "),Ut=a(xt,"CODE",{});var oc=i(Ut);Ws=r(oc,"True"),oc.forEach(o),Fs=r(xt,"."),xt.forEach(o),Oe.forEach(o),ar=d(t),Z=a(t,"P",{});var wt=i(Z);Rs=r(wt,"This model was contributed by "),eo=a(wt,"A",{href:!0,rel:!0});var tc=i(eo);Hs=r(tc,"Arthur Zucker"),tc.forEach(o),Us=r(wt,`.
The original code can be found `),oo=a(wt,"A",{href:!0,rel:!0});var nc=i(oo);Bs=r(nc,"here"),nc.forEach(o),Gs=r(wt,"."),wt.forEach(o),ir=d(t),te=a(t,"H2",{class:!0});var $r=i(te);be=a($r,"A",{id:!0,class:!0,href:!0});var rc=i(be);Bt=a(rc,"SPAN",{});var sc=i(Bt);b(to.$$.fragment,sc),sc.forEach(o),rc.forEach(o),Zs=d($r),Gt=a($r,"SPAN",{});var ac=i(Gt);Ks=r(ac,"JukeboxConfig"),ac.forEach(o),$r.forEach(o),lr=d(t),V=a(t,"DIV",{class:!0});var L=i(V);b(no.$$.fragment,L),Xs=d(L),ro=a(L,"P",{});var Jr=i(ro);Ys=r(Jr,"This is the configuration class to store the configuration of a "),at=a(Jr,"A",{href:!0});var ic=i(at);ea=r(ic,"JukeboxModel"),ic.forEach(o),oa=r(Jr,"."),Jr.forEach(o),ta=d(L),H=a(L,"P",{});var Fe=i(H);na=r(Fe,"Configuration objects inherit from "),it=a(Fe,"A",{href:!0});var lc=i(it);ra=r(lc,"PretrainedConfig"),lc.forEach(o),sa=r(Fe,` and can be used to control the model outputs. Read the
documentation from `),lt=a(Fe,"A",{href:!0});var dc=i(lt);aa=r(dc,"PretrainedConfig"),dc.forEach(o),ia=r(Fe,` for more information. Instantiating a configuration with the defaults will
yield a similar configuration to that of
`),so=a(Fe,"A",{href:!0,rel:!0});var cc=i(so);la=r(cc,"openai/jukebox-1b-lyrics"),cc.forEach(o),da=r(Fe," architecture."),Fe.forEach(o),ca=d(L),Zt=a(L,"P",{});var mc=i(Zt);ma=r(mc,`The downsampling and stride are used to determine downsampling of the input sequence. For example, downsampling =
(5,3), and strides = (2, 2) will downsample the audio by 2^5 = 32 to get the first level of codes, and 2**8 = 256
to get the second level codes. This is mostly true for training the top level prior and the upsamplers.`),mc.forEach(o),ha=d(L),b(ve.$$.fragment,L),ua=d(L),ke=a(L,"DIV",{class:!0});var Er=i(ke);b(ao.$$.fragment,Er),pa=d(Er),io=a(Er,"P",{});var Pr=i(io);fa=r(Pr,"Instantiate a "),dt=a(Pr,"A",{href:!0});var hc=i(dt);ga=r(hc,"JukeboxConfig"),hc.forEach(o),_a=r(Pr,` (or a derived class) from clip text model configuration and clip vision model
configuration.`),Pr.forEach(o),Er.forEach(o),ba=d(L),xe=a(L,"DIV",{class:!0});var jr=i(xe);b(lo.$$.fragment,jr),va=d(jr),co=a(jr,"P",{});var Cr=i(co);ka=r(Cr,"Serializes this instance to a Python dictionary. Override the default "),ct=a(Cr,"A",{href:!0});var uc=i(ct);xa=r(uc,"to_dict()"),uc.forEach(o),wa=r(Cr,"."),Cr.forEach(o),jr.forEach(o),L.forEach(o),dr=d(t),ne=a(t,"H2",{class:!0});var Vr=i(ne);we=a(Vr,"A",{id:!0,class:!0,href:!0});var pc=i(we);Kt=a(pc,"SPAN",{});var fc=i(Kt);b(mo.$$.fragment,fc),fc.forEach(o),pc.forEach(o),ya=d(Vr),Xt=a(Vr,"SPAN",{});var gc=i(Xt);$a=r(gc,"JukeboxPriorConfig"),gc.forEach(o),Vr.forEach(o),cr=d(t),N=a(t,"DIV",{class:!0});var Re=i(N);b(ho.$$.fragment,Re),Ja=d(Re),U=a(Re,"P",{});var He=i(U);Ea=r(He,"This is the configuration class to store the configuration of a "),mt=a(He,"A",{href:!0});var _c=i(mt);Pa=r(_c,"JukeboxPrior"),_c.forEach(o),ja=r(He,`. It is used to instantiate a
`),Yt=a(He,"CODE",{});var bc=i(Yt);Ca=r(bc,"JukeboxPrior"),bc.forEach(o),Va=r(He,` according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the top level prior from the
[openai/jukebox-1b-lyrics](`),uo=a(He,"A",{href:!0,rel:!0});var vc=i(uo);Ta=r(vc,"https://huggingface.co/openai/jukebox"),vc.forEach(o),Aa=r(He,`
-1b-lyrics) architecture.`),He.forEach(o),za=d(Re),re=a(Re,"P",{});var yt=i(re);Da=r(yt,"Configuration objects inherit from "),ht=a(yt,"A",{href:!0});var kc=i(ht);qa=r(kc,"PretrainedConfig"),kc.forEach(o),Ma=r(yt,` and can be used to control the model outputs. Read the
documentation from `),ut=a(yt,"A",{href:!0});var xc=i(ut);La=r(xc,"PretrainedConfig"),xc.forEach(o),Qa=r(yt," for more information."),yt.forEach(o),Ia=d(Re),ye=a(Re,"DIV",{class:!0});var Tr=i(ye);b(po.$$.fragment,Tr),Na=d(Tr),fo=a(Tr,"P",{});var Ar=i(fo);Sa=r(Ar,"Serializes this instance to a Python dictionary. Override the default "),pt=a(Ar,"A",{href:!0});var wc=i(pt);Oa=r(wc,"to_dict()"),wc.forEach(o),Wa=r(Ar,"."),Ar.forEach(o),Tr.forEach(o),Re.forEach(o),mr=d(t),se=a(t,"H2",{class:!0});var zr=i(se);$e=a(zr,"A",{id:!0,class:!0,href:!0});var yc=i($e);en=a(yc,"SPAN",{});var $c=i(en);b(go.$$.fragment,$c),$c.forEach(o),yc.forEach(o),Fa=d(zr),on=a(zr,"SPAN",{});var Jc=i(on);Ra=r(Jc,"JukeboxVQVAEConfig"),Jc.forEach(o),zr.forEach(o),hr=d(t),B=a(t,"DIV",{class:!0});var $t=i(B);b(_o.$$.fragment,$t),Ha=d($t),G=a($t,"P",{});var Ue=i(G);Ua=r(Ue,"This is the configuration class to store the configuration of a "),ft=a(Ue,"A",{href:!0});var Ec=i(ft);Ba=r(Ec,"JukeboxVQVAE"),Ec.forEach(o),Ga=r(Ue,`. It is used to instantiate a
`),tn=a(Ue,"CODE",{});var Pc=i(tn);Za=r(Pc,"JukeboxVQVAE"),Pc.forEach(o),Ka=r(Ue,` according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the VQVAE from
`),bo=a(Ue,"A",{href:!0,rel:!0});var jc=i(bo);Xa=r(jc,"openai/jukebox-1b-lyrics"),jc.forEach(o),Ya=r(Ue," architecture."),Ue.forEach(o),ei=d($t),ae=a($t,"P",{});var Jt=i(ae);oi=r(Jt,"Configuration objects inherit from "),gt=a(Jt,"A",{href:!0});var Cc=i(gt);ti=r(Cc,"PretrainedConfig"),Cc.forEach(o),ni=r(Jt,` and can be used to control the model outputs. Read the
documentation from `),_t=a(Jt,"A",{href:!0});var Vc=i(_t);ri=r(Vc,"PretrainedConfig"),Vc.forEach(o),si=r(Jt," for more information."),Jt.forEach(o),$t.forEach(o),ur=d(t),ie=a(t,"H2",{class:!0});var Dr=i(ie);Je=a(Dr,"A",{id:!0,class:!0,href:!0});var Tc=i(Je);nn=a(Tc,"SPAN",{});var Ac=i(nn);b(vo.$$.fragment,Ac),Ac.forEach(o),Tc.forEach(o),ai=d(Dr),rn=a(Dr,"SPAN",{});var zc=i(rn);ii=r(zc,"JukeboxTokenizer"),zc.forEach(o),Dr.forEach(o),pr=d(t),J=a(t,"DIV",{class:!0});var j=i(J);b(ko.$$.fragment,j),li=d(j),sn=a(j,"P",{});var Dc=i(sn);di=r(Dc,"Constructs a Jukebox tokenizer. Jukebox can be conditioned on 3 different inputs :"),Dc.forEach(o),ci=d(j),le=a(j,"UL",{});var Et=i(le);an=a(Et,"LI",{});var qc=i(an);mi=r(qc,"Artists, unique ids are associated to each artist from the provided dictionary."),qc.forEach(o),hi=d(Et),ln=a(Et,"LI",{});var Mc=i(ln);ui=r(Mc,"Genres, unique ids are associated to each genre from the provided dictionary."),Mc.forEach(o),pi=d(Et),dn=a(Et,"LI",{});var Lc=i(dn);fi=r(Lc,`Lyrics, character based tokenization. Must be initialized with the list of characters that are inside the
vocabulary.`),Lc.forEach(o),Et.forEach(o),gi=d(j),cn=a(j,"P",{});var Qc=i(cn);_i=r(Qc,`This tokenizer does not require training. It should be able to process a different number of inputs:
as the conditioning of the model can be done on the three different queries. If None is provided, defaults values will be used.:`),Qc.forEach(o),bi=d(j),xo=a(j,"P",{});var qr=i(xo);vi=r(qr,"Depending on the number of genres on which the model should be conditioned ("),mn=a(qr,"CODE",{});var Ic=i(mn);ki=r(Ic,"n_genres"),Ic.forEach(o),xi=r(qr,")."),qr.forEach(o),wi=d(j),b(Ee.$$.fragment,j),yi=d(j),wo=a(j,"P",{});var Mr=i(wo);$i=r(Mr,"You can get around that behavior by passing "),hn=a(Mr,"CODE",{});var Nc=i(hn);Ji=r(Nc,"add_prefix_space=True"),Nc.forEach(o),Ei=r(Mr,` when instantiating this tokenizer or when you
call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.`),Mr.forEach(o),Pi=d(j),b(Pe.$$.fragment,j),ji=d(j),yo=a(j,"P",{});var Lr=i(yo);Ci=r(Lr,"This tokenizer inherits from "),bt=a(Lr,"A",{href:!0});var Sc=i(bt);Vi=r(Sc,"PreTrainedTokenizer"),Sc.forEach(o),Ti=r(Lr,` which contains most of the main methods. Users should refer to:
this superclass for more information regarding those methods.`),Lr.forEach(o),Ai=d(j),un=a(j,"P",{});var Oc=i(un);zi=r(Oc,"However the code does not allow that and only supports composing from various genres."),Oc.forEach(o),Di=d(j),je=a(j,"DIV",{class:!0});var Qr=i(je);b($o.$$.fragment,Qr),qi=d(Qr),pn=a(Qr,"P",{});var Wc=i(pn);Mi=r(Wc,"Saves the tokenizer\u2019s vocabulary dictionary to the provided save_directory."),Wc.forEach(o),Qr.forEach(o),j.forEach(o),fr=d(t),de=a(t,"H2",{class:!0});var Ir=i(de);Ce=a(Ir,"A",{id:!0,class:!0,href:!0});var Fc=i(Ce);fn=a(Fc,"SPAN",{});var Rc=i(fn);b(Jo.$$.fragment,Rc),Rc.forEach(o),Fc.forEach(o),Li=d(Ir),gn=a(Ir,"SPAN",{});var Hc=i(gn);Qi=r(Hc,"JukeboxModel"),Hc.forEach(o),Ir.forEach(o),gr=d(t),P=a(t,"DIV",{class:!0});var z=i(P);b(Eo.$$.fragment,z),Ii=d(z),D=a(z,"P",{});var Q=i(D);Ni=r(Q,"The bare JUKEBOX Model used for music generation. 4 sampling techniques are supported : "),_n=a(Q,"CODE",{});var Uc=i(_n);Si=r(Uc,"primed_sample"),Uc.forEach(o),Oi=r(Q,", "),bn=a(Q,"CODE",{});var Bc=i(bn);Wi=r(Bc,"upsample"),Bc.forEach(o),Fi=r(Q,`,
`),vn=a(Q,"CODE",{});var Gc=i(vn);Ri=r(Gc,"continue_sample"),Gc.forEach(o),Hi=r(Q," and "),kn=a(Q,"CODE",{});var Zc=i(kn);Ui=r(Zc,"ancestral_sample"),Zc.forEach(o),Bi=r(Q,". It does not have a "),xn=a(Q,"CODE",{});var Kc=i(xn);Gi=r(Kc,"forward"),Kc.forEach(o),Zi=r(Q,` method as the training is not end to end. If
you want to fine-tune the model, it is recommended to use the `),wn=a(Q,"CODE",{});var Xc=i(wn);Ki=r(Xc,"JukeboxPrior"),Xc.forEach(o),Xi=r(Q,` class and train each prior
individually.`),Q.forEach(o),Yi=d(z),Po=a(z,"P",{});var Nr=i(Po);el=r(Nr,"This model inherits from "),vt=a(Nr,"A",{href:!0});var Yc=i(vt);ol=r(Yc,"PreTrainedModel"),Yc.forEach(o),tl=r(Nr,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Nr.forEach(o),nl=d(z),jo=a(z,"P",{});var Sr=i(jo);rl=r(Sr,"This model is also a PyTorch "),Co=a(Sr,"A",{href:!0,rel:!0});var em=i(Co);sl=r(em,"torch.nn.Module"),em.forEach(o),al=r(Sr,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Sr.forEach(o),il=d(z),K=a(z,"DIV",{class:!0});var Pt=i(K);b(Vo.$$.fragment,Pt),ll=d(Pt),To=a(Pt,"P",{});var Or=i(To);dl=r(Or,"Generates music tokens based on the provided "),yn=a(Or,"CODE",{});var om=i(yn);cl=r(om,"labels. Will start at the desired prior level and automatically upsample the sequence. If you want to create the audio, you should call "),om.forEach(o),ml=r(Or,"model.decode(tokens)`, which will use\nthe VQ-VAE decoder to convert the music tokens to raw audio."),Or.forEach(o),hl=d(Pt),b(Ve.$$.fragment,Pt),Pt.forEach(o),ul=d(z),Te=a(z,"DIV",{class:!0});var Wr=i(Te);b(Ao.$$.fragment,Wr),pl=d(Wr),zo=a(Wr,"P",{});var Fr=i(zo);fl=r(Fr,"Generate a raw audio conditioned on the provided "),$n=a(Fr,"CODE",{});var tm=i($n);gl=r(tm,"raw_audio"),tm.forEach(o),_l=r(Fr,` which is used as conditioning at each of the
generation levels. The audio is encoded to music tokens using the 3 levels of the VQ-VAE. These tokens are
used: as conditioning for each level, which means that no ancestral sampling is required.`),Fr.forEach(o),Wr.forEach(o),bl=d(z),Ae=a(z,"DIV",{class:!0});var Rr=i(Ae);b(Do.$$.fragment,Rr),vl=d(Rr),Jn=a(Rr,"P",{});var nm=i(Jn);kl=r(nm,"Generates a continuation of the previously generated tokens."),nm.forEach(o),Rr.forEach(o),xl=d(z),ze=a(z,"DIV",{class:!0});var Hr=i(ze);b(qo.$$.fragment,Hr),wl=d(Hr),Mo=a(Hr,"P",{});var Ur=i(Mo);yl=r(Ur,"Upsamples a sequence of music tokens using the prior at level "),En=a(Ur,"CODE",{});var rm=i(En);$l=r(rm,"level"),rm.forEach(o),Jl=r(Ur,"."),Ur.forEach(o),Hr.forEach(o),El=d(z),F=a(z,"DIV",{class:!0});var Be=i(F);b(Lo.$$.fragment,Be),Pl=d(Be),Pn=a(Be,"P",{});var sm=i(Pn);jl=r(sm,`Core sampling function used to generate music tokens. Iterates over the provided list of levels, while saving
the generated raw audio at each step.`),sm.forEach(o),Cl=d(Be),jn=a(Be,"P",{});var am=i(jn);Vl=r(am,"Returns: torch.Tensor"),am.forEach(o),Tl=d(Be),b(De.$$.fragment,Be),Be.forEach(o),z.forEach(o),_r=d(t),ce=a(t,"H2",{class:!0});var Br=i(ce);qe=a(Br,"A",{id:!0,class:!0,href:!0});var im=i(qe);Cn=a(im,"SPAN",{});var lm=i(Cn);b(Qo.$$.fragment,lm),lm.forEach(o),im.forEach(o),Al=d(Br),Vn=a(Br,"SPAN",{});var dm=i(Vn);zl=r(dm,"JukeboxPrior"),dm.forEach(o),Br.forEach(o),br=d(t),S=a(t,"DIV",{class:!0});var Ge=i(S);b(Io.$$.fragment,Ge),Dl=d(Ge),me=a(Ge,"P",{});var jt=i(me);ql=r(jt,`The JukeboxPrior class, which is a wrapper around the various conditioning and the transformer. JukeboxPrior can be
seen as language models trained on music. They model the next `),Tn=a(jt,"CODE",{});var cm=i(Tn);Ml=r(cm,"music token"),cm.forEach(o),Ll=r(jt," prediction task. If a (lyric) "),An=a(jt,"CODE",{});var mm=i(An);Ql=r(mm,"encoder\xF9 is defined, it also models the "),mm.forEach(o),Il=r(jt,"next character` prediction on the lyrics. Can be conditionned on timing, artist,\ngenre, lyrics and codes from lower-levels Priors."),jt.forEach(o),Nl=d(Ge),Me=a(Ge,"DIV",{class:!0});var Gr=i(Me);b(No.$$.fragment,Gr),Sl=d(Gr),zn=a(Gr,"P",{});var hm=i(zn);Ol=r(hm,"Ancestral/Prime sampling a window of tokens using the provided conditioning and metadatas."),hm.forEach(o),Gr.forEach(o),Wl=d(Ge),Le=a(Ge,"DIV",{class:!0});var Zr=i(Le);b(So.$$.fragment,Zr),Fl=d(Zr),O=a(Zr,"P",{});var Y=i(O);Rl=r(Y,"Encode the hidden states using the "),Dn=a(Y,"CODE",{});var um=i(Dn);Hl=r(um,"vqvae"),um.forEach(o),Ul=r(Y," encoder, and then predicts the next token in the "),qn=a(Y,"CODE",{});var pm=i(qn);Bl=r(pm,"forward_tokens"),pm.forEach(o),Gl=r(Y,`
function. The loss is the sum of the `),Mn=a(Y,"CODE",{});var fm=i(Mn);Zl=r(fm,"encoder"),fm.forEach(o),Kl=r(Y," loss and the "),Ln=a(Y,"CODE",{});var gm=i(Ln);Xl=r(gm,"decoder"),gm.forEach(o),Yl=r(Y," loss."),Y.forEach(o),Zr.forEach(o),Ge.forEach(o),vr=d(t),he=a(t,"H2",{class:!0});var Kr=i(he);Qe=a(Kr,"A",{id:!0,class:!0,href:!0});var _m=i(Qe);Qn=a(_m,"SPAN",{});var bm=i(Qn);b(Oo.$$.fragment,bm),bm.forEach(o),_m.forEach(o),ed=d(Kr),In=a(Kr,"SPAN",{});var vm=i(In);od=r(vm,"JukeboxVQVAE"),vm.forEach(o),Kr.forEach(o),kr=d(t),T=a(t,"DIV",{class:!0});var I=i(T);b(Wo.$$.fragment,I),td=d(I),Fo=a(I,"P",{});var Xr=i(Fo);nd=r(Xr,"The Hierarchical VQ-VAE model used in Jukebox. This model follows the Hierarchical VQVAE paper from "),Ro=a(Xr,"A",{href:!0,rel:!0});var km=i(Ro);rd=r(km,`Will Williams, Sam
Ringer, Tom Ash, John Hughes, David MacLeod, Jamie Dougherty`),km.forEach(o),sd=r(Xr,"."),Xr.forEach(o),ad=d(I),Ho=a(I,"P",{});var Yr=i(Ho);id=r(Yr,"This model inherits from "),kt=a(Yr,"A",{href:!0});var xm=i(kt);ld=r(xm,"PreTrainedModel"),xm.forEach(o),dd=r(Yr,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Yr.forEach(o),cd=d(I),Uo=a(I,"P",{});var es=i(Uo);md=r(es,"This model is also a PyTorch "),Bo=a(es,"A",{href:!0,rel:!0});var wm=i(Bo);hd=r(wm,"torch.nn.Module"),wm.forEach(o),ud=r(es,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),es.forEach(o),pd=d(I),X=a(I,"DIV",{class:!0});var Ct=i(X);b(Go.$$.fragment,Ct),fd=d(Ct),Zo=a(Ct,"P",{});var os=i(Zo);gd=r(os,"Forward pass of the VQ-VAE, encodes the "),Nn=a(os,"CODE",{});var ym=i(Nn);_d=r(ym,"raw_audio"),ym.forEach(o),bd=r(os,` to latent states, which are then decoded for each level.
The commit loss, which ensure that the encoder\u2019s computed embeddings are close to the codebook vectors, is
computed.`),os.forEach(o),vd=d(Ct),b(Ie.$$.fragment,Ct),Ct.forEach(o),kd=d(I),Ne=a(I,"DIV",{class:!0});var ts=i(Ne);b(Ko.$$.fragment,ts),xd=d(ts),ue=a(ts,"P",{});var Vt=i(ue);wd=r(Vt,"Transforms the "),Sn=a(Vt,"CODE",{});var $m=i(Sn);yd=r($m,"input_audio"),$m.forEach(o),$d=r(Vt," to a discrete representation made out of "),On=a(Vt,"CODE",{});var Jm=i(On);Jd=r(Jm,"music_tokens"),Jm.forEach(o),Ed=r(Vt,"."),Vt.forEach(o),ts.forEach(o),Pd=d(I),Se=a(I,"DIV",{class:!0});var ns=i(Se);b(Xo.$$.fragment,ns),jd=d(ns),pe=a(ns,"P",{});var Tt=i(pe);Cd=r(Tt,"Transforms the input "),Wn=a(Tt,"CODE",{});var Em=i(Wn);Vd=r(Em,"music_tokens"),Em.forEach(o),Td=r(Tt," to their "),Fn=a(Tt,"CODE",{});var Pm=i(Fn);Ad=r(Pm,"raw_audio"),Pm.forEach(o),zd=r(Tt," representation."),Tt.forEach(o),ns.forEach(o),I.forEach(o),this.h()},h(){c(p,"name","hf:doc:metadata"),c(p,"content",JSON.stringify(Om)),c(f,"id","jukebox"),c(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(f,"href","#jukebox"),c(g,"class","relative group"),c(ge,"id","overview"),c(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ge,"href","#overview"),c(ee,"class","relative group"),c(Ke,"href","https://arxiv.org/pdf/2005.00341.pdf"),c(Ke,"rel","nofollow"),c(Xe,"href","https://arxiv.org/abs/1904.10509"),c(Xe,"rel","nofollow"),Am(rt.src,Dd="https://gist.githubusercontent.com/ArthurZucker/92c1acaae62ebf1b6a951710bdd8b6af/raw/c9c517bf4eff61393f6c7dec9366ef02bdd059a3/jukebox.svg")||c(rt,"src",Dd),c(rt,"alt","JukeboxModel"),c(eo,"href","https://huggingface.co/ArthurZ"),c(eo,"rel","nofollow"),c(oo,"href","https://github.com/openai/jukebox"),c(oo,"rel","nofollow"),c(be,"id","transformers.JukeboxConfig"),c(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(be,"href","#transformers.JukeboxConfig"),c(te,"class","relative group"),c(at,"href","/docs/transformers/main/en/model_doc/jukebox#transformers.JukeboxModel"),c(it,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(lt,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(so,"href","https://huggingface.co/openai/jukebox-1b-lyrics"),c(so,"rel","nofollow"),c(dt,"href","/docs/transformers/main/en/model_doc/jukebox#transformers.JukeboxConfig"),c(ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ct,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.to_dict"),c(xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(we,"id","transformers.JukeboxPriorConfig"),c(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(we,"href","#transformers.JukeboxPriorConfig"),c(ne,"class","relative group"),c(mt,"href","/docs/transformers/main/en/model_doc/jukebox#transformers.JukeboxPrior"),c(uo,"href","https://huggingface.co/openai/jukebox"),c(uo,"rel","nofollow"),c(ht,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(ut,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(pt,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.to_dict"),c(ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c($e,"id","transformers.JukeboxVQVAEConfig"),c($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($e,"href","#transformers.JukeboxVQVAEConfig"),c(se,"class","relative group"),c(ft,"href","/docs/transformers/main/en/model_doc/jukebox#transformers.JukeboxVQVAE"),c(bo,"href","https://huggingface.co/openai/jukebox-1b-lyrics"),c(bo,"rel","nofollow"),c(gt,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(_t,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Je,"id","transformers.JukeboxTokenizer"),c(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Je,"href","#transformers.JukeboxTokenizer"),c(ie,"class","relative group"),c(bt,"href","/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Ce,"id","transformers.JukeboxModel"),c(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ce,"href","#transformers.JukeboxModel"),c(de,"class","relative group"),c(vt,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(Co,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Co,"rel","nofollow"),c(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(qe,"id","transformers.JukeboxPrior"),c(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qe,"href","#transformers.JukeboxPrior"),c(ce,"class","relative group"),c(Me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Qe,"id","transformers.JukeboxVQVAE"),c(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qe,"href","#transformers.JukeboxVQVAE"),c(he,"class","relative group"),c(Ro,"href","https://arxiv.org/abs/2002.08111"),c(Ro,"rel","nofollow"),c(kt,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(Bo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Bo,"rel","nofollow"),c(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,h){e(document.head,p),u(t,$,h),u(t,g,h),e(g,f),e(f,E),v(m,E,null),e(g,y),e(g,At),e(At,rs),u(t,Xn,h),u(t,ee,h),e(ee,ge),e(ge,zt),v(Ze,zt,null),e(ee,ss),e(ee,Dt),e(Dt,as),u(t,Yn,h),u(t,_e,h),e(_e,is),e(_e,Ke),e(Ke,ls),e(_e,ds),u(t,er,h),u(t,ot,h),e(ot,cs),u(t,or,h),u(t,tt,h),e(tt,qt),e(qt,ms),u(t,tr,h),u(t,A,h),e(A,hs),e(A,Mt),e(Mt,us),e(A,ps),e(A,Xe),e(Xe,fs),e(A,gs),e(A,Lt),e(Lt,_s),e(A,bs),e(A,Qt),e(Qt,vs),e(A,ks),e(A,It),e(It,xs),e(A,ws),e(A,Nt),e(Nt,ys),e(A,$s),u(t,nr,h),u(t,nt,h),e(nt,rt),u(t,rr,h),u(t,st,h),e(st,Js),u(t,sr,h),u(t,W,h),e(W,St),e(St,Es),e(W,Ps),e(W,Ye),e(Ye,js),e(Ye,Ot),e(Ot,Cs),e(Ye,Vs),e(W,Ts),e(W,R),e(R,As),e(R,Wt),e(Wt,zs),e(R,Ds),e(R,Ft),e(Ft,qs),e(R,Ms),e(R,Rt),e(Rt,Ls),e(R,Qs),e(W,Is),e(W,oe),e(oe,Ns),e(oe,Ht),e(Ht,Ss),e(oe,Os),e(oe,Ut),e(Ut,Ws),e(oe,Fs),u(t,ar,h),u(t,Z,h),e(Z,Rs),e(Z,eo),e(eo,Hs),e(Z,Us),e(Z,oo),e(oo,Bs),e(Z,Gs),u(t,ir,h),u(t,te,h),e(te,be),e(be,Bt),v(to,Bt,null),e(te,Zs),e(te,Gt),e(Gt,Ks),u(t,lr,h),u(t,V,h),v(no,V,null),e(V,Xs),e(V,ro),e(ro,Ys),e(ro,at),e(at,ea),e(ro,oa),e(V,ta),e(V,H),e(H,na),e(H,it),e(it,ra),e(H,sa),e(H,lt),e(lt,aa),e(H,ia),e(H,so),e(so,la),e(H,da),e(V,ca),e(V,Zt),e(Zt,ma),e(V,ha),v(ve,V,null),e(V,ua),e(V,ke),v(ao,ke,null),e(ke,pa),e(ke,io),e(io,fa),e(io,dt),e(dt,ga),e(io,_a),e(V,ba),e(V,xe),v(lo,xe,null),e(xe,va),e(xe,co),e(co,ka),e(co,ct),e(ct,xa),e(co,wa),u(t,dr,h),u(t,ne,h),e(ne,we),e(we,Kt),v(mo,Kt,null),e(ne,ya),e(ne,Xt),e(Xt,$a),u(t,cr,h),u(t,N,h),v(ho,N,null),e(N,Ja),e(N,U),e(U,Ea),e(U,mt),e(mt,Pa),e(U,ja),e(U,Yt),e(Yt,Ca),e(U,Va),e(U,uo),e(uo,Ta),e(U,Aa),e(N,za),e(N,re),e(re,Da),e(re,ht),e(ht,qa),e(re,Ma),e(re,ut),e(ut,La),e(re,Qa),e(N,Ia),e(N,ye),v(po,ye,null),e(ye,Na),e(ye,fo),e(fo,Sa),e(fo,pt),e(pt,Oa),e(fo,Wa),u(t,mr,h),u(t,se,h),e(se,$e),e($e,en),v(go,en,null),e(se,Fa),e(se,on),e(on,Ra),u(t,hr,h),u(t,B,h),v(_o,B,null),e(B,Ha),e(B,G),e(G,Ua),e(G,ft),e(ft,Ba),e(G,Ga),e(G,tn),e(tn,Za),e(G,Ka),e(G,bo),e(bo,Xa),e(G,Ya),e(B,ei),e(B,ae),e(ae,oi),e(ae,gt),e(gt,ti),e(ae,ni),e(ae,_t),e(_t,ri),e(ae,si),u(t,ur,h),u(t,ie,h),e(ie,Je),e(Je,nn),v(vo,nn,null),e(ie,ai),e(ie,rn),e(rn,ii),u(t,pr,h),u(t,J,h),v(ko,J,null),e(J,li),e(J,sn),e(sn,di),e(J,ci),e(J,le),e(le,an),e(an,mi),e(le,hi),e(le,ln),e(ln,ui),e(le,pi),e(le,dn),e(dn,fi),e(J,gi),e(J,cn),e(cn,_i),e(J,bi),e(J,xo),e(xo,vi),e(xo,mn),e(mn,ki),e(xo,xi),e(J,wi),v(Ee,J,null),e(J,yi),e(J,wo),e(wo,$i),e(wo,hn),e(hn,Ji),e(wo,Ei),e(J,Pi),v(Pe,J,null),e(J,ji),e(J,yo),e(yo,Ci),e(yo,bt),e(bt,Vi),e(yo,Ti),e(J,Ai),e(J,un),e(un,zi),e(J,Di),e(J,je),v($o,je,null),e(je,qi),e(je,pn),e(pn,Mi),u(t,fr,h),u(t,de,h),e(de,Ce),e(Ce,fn),v(Jo,fn,null),e(de,Li),e(de,gn),e(gn,Qi),u(t,gr,h),u(t,P,h),v(Eo,P,null),e(P,Ii),e(P,D),e(D,Ni),e(D,_n),e(_n,Si),e(D,Oi),e(D,bn),e(bn,Wi),e(D,Fi),e(D,vn),e(vn,Ri),e(D,Hi),e(D,kn),e(kn,Ui),e(D,Bi),e(D,xn),e(xn,Gi),e(D,Zi),e(D,wn),e(wn,Ki),e(D,Xi),e(P,Yi),e(P,Po),e(Po,el),e(Po,vt),e(vt,ol),e(Po,tl),e(P,nl),e(P,jo),e(jo,rl),e(jo,Co),e(Co,sl),e(jo,al),e(P,il),e(P,K),v(Vo,K,null),e(K,ll),e(K,To),e(To,dl),e(To,yn),e(yn,cl),e(To,ml),e(K,hl),v(Ve,K,null),e(P,ul),e(P,Te),v(Ao,Te,null),e(Te,pl),e(Te,zo),e(zo,fl),e(zo,$n),e($n,gl),e(zo,_l),e(P,bl),e(P,Ae),v(Do,Ae,null),e(Ae,vl),e(Ae,Jn),e(Jn,kl),e(P,xl),e(P,ze),v(qo,ze,null),e(ze,wl),e(ze,Mo),e(Mo,yl),e(Mo,En),e(En,$l),e(Mo,Jl),e(P,El),e(P,F),v(Lo,F,null),e(F,Pl),e(F,Pn),e(Pn,jl),e(F,Cl),e(F,jn),e(jn,Vl),e(F,Tl),v(De,F,null),u(t,_r,h),u(t,ce,h),e(ce,qe),e(qe,Cn),v(Qo,Cn,null),e(ce,Al),e(ce,Vn),e(Vn,zl),u(t,br,h),u(t,S,h),v(Io,S,null),e(S,Dl),e(S,me),e(me,ql),e(me,Tn),e(Tn,Ml),e(me,Ll),e(me,An),e(An,Ql),e(me,Il),e(S,Nl),e(S,Me),v(No,Me,null),e(Me,Sl),e(Me,zn),e(zn,Ol),e(S,Wl),e(S,Le),v(So,Le,null),e(Le,Fl),e(Le,O),e(O,Rl),e(O,Dn),e(Dn,Hl),e(O,Ul),e(O,qn),e(qn,Bl),e(O,Gl),e(O,Mn),e(Mn,Zl),e(O,Kl),e(O,Ln),e(Ln,Xl),e(O,Yl),u(t,vr,h),u(t,he,h),e(he,Qe),e(Qe,Qn),v(Oo,Qn,null),e(he,ed),e(he,In),e(In,od),u(t,kr,h),u(t,T,h),v(Wo,T,null),e(T,td),e(T,Fo),e(Fo,nd),e(Fo,Ro),e(Ro,rd),e(Fo,sd),e(T,ad),e(T,Ho),e(Ho,id),e(Ho,kt),e(kt,ld),e(Ho,dd),e(T,cd),e(T,Uo),e(Uo,md),e(Uo,Bo),e(Bo,hd),e(Uo,ud),e(T,pd),e(T,X),v(Go,X,null),e(X,fd),e(X,Zo),e(Zo,gd),e(Zo,Nn),e(Nn,_d),e(Zo,bd),e(X,vd),v(Ie,X,null),e(T,kd),e(T,Ne),v(Ko,Ne,null),e(Ne,xd),e(Ne,ue),e(ue,wd),e(ue,Sn),e(Sn,yd),e(ue,$d),e(ue,On),e(On,Jd),e(ue,Ed),e(T,Pd),e(T,Se),v(Xo,Se,null),e(Se,jd),e(Se,pe),e(pe,Cd),e(pe,Wn),e(Wn,Vd),e(pe,Td),e(pe,Fn),e(Fn,Ad),e(pe,zd),xr=!0},p(t,[h]){const Yo={};h&2&&(Yo.$$scope={dirty:h,ctx:t}),ve.$set(Yo);const Rn={};h&2&&(Rn.$$scope={dirty:h,ctx:t}),Ee.$set(Rn);const Hn={};h&2&&(Hn.$$scope={dirty:h,ctx:t}),Pe.$set(Hn);const Un={};h&2&&(Un.$$scope={dirty:h,ctx:t}),Ve.$set(Un);const et={};h&2&&(et.$$scope={dirty:h,ctx:t}),De.$set(et);const Bn={};h&2&&(Bn.$$scope={dirty:h,ctx:t}),Ie.$set(Bn)},i(t){xr||(k(m.$$.fragment,t),k(Ze.$$.fragment,t),k(to.$$.fragment,t),k(no.$$.fragment,t),k(ve.$$.fragment,t),k(ao.$$.fragment,t),k(lo.$$.fragment,t),k(mo.$$.fragment,t),k(ho.$$.fragment,t),k(po.$$.fragment,t),k(go.$$.fragment,t),k(_o.$$.fragment,t),k(vo.$$.fragment,t),k(ko.$$.fragment,t),k(Ee.$$.fragment,t),k(Pe.$$.fragment,t),k($o.$$.fragment,t),k(Jo.$$.fragment,t),k(Eo.$$.fragment,t),k(Vo.$$.fragment,t),k(Ve.$$.fragment,t),k(Ao.$$.fragment,t),k(Do.$$.fragment,t),k(qo.$$.fragment,t),k(Lo.$$.fragment,t),k(De.$$.fragment,t),k(Qo.$$.fragment,t),k(Io.$$.fragment,t),k(No.$$.fragment,t),k(So.$$.fragment,t),k(Oo.$$.fragment,t),k(Wo.$$.fragment,t),k(Go.$$.fragment,t),k(Ie.$$.fragment,t),k(Ko.$$.fragment,t),k(Xo.$$.fragment,t),xr=!0)},o(t){x(m.$$.fragment,t),x(Ze.$$.fragment,t),x(to.$$.fragment,t),x(no.$$.fragment,t),x(ve.$$.fragment,t),x(ao.$$.fragment,t),x(lo.$$.fragment,t),x(mo.$$.fragment,t),x(ho.$$.fragment,t),x(po.$$.fragment,t),x(go.$$.fragment,t),x(_o.$$.fragment,t),x(vo.$$.fragment,t),x(ko.$$.fragment,t),x(Ee.$$.fragment,t),x(Pe.$$.fragment,t),x($o.$$.fragment,t),x(Jo.$$.fragment,t),x(Eo.$$.fragment,t),x(Vo.$$.fragment,t),x(Ve.$$.fragment,t),x(Ao.$$.fragment,t),x(Do.$$.fragment,t),x(qo.$$.fragment,t),x(Lo.$$.fragment,t),x(De.$$.fragment,t),x(Qo.$$.fragment,t),x(Io.$$.fragment,t),x(No.$$.fragment,t),x(So.$$.fragment,t),x(Oo.$$.fragment,t),x(Wo.$$.fragment,t),x(Go.$$.fragment,t),x(Ie.$$.fragment,t),x(Ko.$$.fragment,t),x(Xo.$$.fragment,t),xr=!1},d(t){o(p),t&&o($),t&&o(g),w(m),t&&o(Xn),t&&o(ee),w(Ze),t&&o(Yn),t&&o(_e),t&&o(er),t&&o(ot),t&&o(or),t&&o(tt),t&&o(tr),t&&o(A),t&&o(nr),t&&o(nt),t&&o(rr),t&&o(st),t&&o(sr),t&&o(W),t&&o(ar),t&&o(Z),t&&o(ir),t&&o(te),w(to),t&&o(lr),t&&o(V),w(no),w(ve),w(ao),w(lo),t&&o(dr),t&&o(ne),w(mo),t&&o(cr),t&&o(N),w(ho),w(po),t&&o(mr),t&&o(se),w(go),t&&o(hr),t&&o(B),w(_o),t&&o(ur),t&&o(ie),w(vo),t&&o(pr),t&&o(J),w(ko),w(Ee),w(Pe),w($o),t&&o(fr),t&&o(de),w(Jo),t&&o(gr),t&&o(P),w(Eo),w(Vo),w(Ve),w(Ao),w(Do),w(qo),w(Lo),w(De),t&&o(_r),t&&o(ce),w(Qo),t&&o(br),t&&o(S),w(Io),w(No),w(So),t&&o(vr),t&&o(he),w(Oo),t&&o(kr),t&&o(T),w(Wo),w(Go),w(Ie),w(Ko),w(Xo)}}}const Om={local:"jukebox",sections:[{local:"overview",title:"Overview"},{local:"transformers.JukeboxConfig",title:"JukeboxConfig"},{local:"transformers.JukeboxPriorConfig",title:"JukeboxPriorConfig"},{local:"transformers.JukeboxVQVAEConfig",title:"JukeboxVQVAEConfig"},{local:"transformers.JukeboxTokenizer",title:"JukeboxTokenizer"},{local:"transformers.JukeboxModel",title:"JukeboxModel"},{local:"transformers.JukeboxPrior",title:"JukeboxPrior"},{local:"transformers.JukeboxVQVAE",title:"JukeboxVQVAE"}],title:"Jukebox"};function Wm(q){return zm(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Zm extends jm{constructor(p){super();Cm(this,p,Wm,Sm,Vm,{})}}export{Zm as default,Om as metadata};
