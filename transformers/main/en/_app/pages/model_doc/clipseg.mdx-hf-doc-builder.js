import{S as Fd,i as Vd,s as Od,e as r,k as p,w as I,t as i,M as Nd,c as s,d as o,m as f,a,x as v,h as l,b as c,N as Wd,G as e,g as _,y as $,q as S,o as b,B as L,v as Bd,L as be}from"../../chunks/vendor-hf-doc-builder.js";import{T as Xo}from"../../chunks/Tip-hf-doc-builder.js";import{D as y}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Le}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as H}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Se}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";import{P as Hd}from"../../chunks/PipelineTag-hf-doc-builder.js";function Rd(w){let d,P,u,m,C;return m=new Le({props:{code:`from transformers import CLIPSegConfig, CLIPSegModel

# Initializing a CLIPSegConfig with CIDAS/clipseg-rd64 style configuration
configuration = CLIPSegConfig()

# Initializing a CLIPSegModel (with random weights) from the CIDAS/clipseg-rd64 style configuration
model = CLIPSegModel(configuration)

# Accessing the model configuration
configuration = model.config

# We can also initialize a CLIPSegConfig from a CLIPSegTextConfig and a CLIPSegVisionConfig

# Initializing a CLIPSegText and CLIPSegVision configuration
config_text = CLIPSegTextConfig()
config_vision = CLIPSegVisionConfig()

config = CLIPSegConfig.from_text_vision_configs(config_text, config_vision)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPSegConfig, CLIPSegModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegConfig with CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = CLIPSegConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegModel (with random weights) from the CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># We can also initialize a CLIPSegConfig from a CLIPSegTextConfig and a CLIPSegVisionConfig</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegText and CLIPSegVision configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_text = CLIPSegTextConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>config_vision = CLIPSegVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = CLIPSegConfig.from_text_vision_configs(config_text, config_vision)`}}),{c(){d=r("p"),P=i("Example:"),u=p(),I(m.$$.fragment)},l(n){d=s(n,"P",{});var g=a(d);P=l(g,"Example:"),g.forEach(o),u=f(n),v(m.$$.fragment,n)},m(n,g){_(n,d,g),e(d,P),_(n,u,g),$(m,n,g),C=!0},p:be,i(n){C||(S(m.$$.fragment,n),C=!0)},o(n){b(m.$$.fragment,n),C=!1},d(n){n&&o(d),n&&o(u),L(m,n)}}}function Ud(w){let d,P,u,m,C;return m=new Le({props:{code:`from transformers import CLIPSegTextConfig, CLIPSegTextModel

# Initializing a CLIPSegTextConfig with CIDAS/clipseg-rd64 style configuration
configuration = CLIPSegTextConfig()

# Initializing a CLIPSegTextModel (with random weights) from the CIDAS/clipseg-rd64 style configuration
model = CLIPSegTextModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPSegTextConfig, CLIPSegTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegTextConfig with CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = CLIPSegTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegTextModel (with random weights) from the CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){d=r("p"),P=i("Example:"),u=p(),I(m.$$.fragment)},l(n){d=s(n,"P",{});var g=a(d);P=l(g,"Example:"),g.forEach(o),u=f(n),v(m.$$.fragment,n)},m(n,g){_(n,d,g),e(d,P),_(n,u,g),$(m,n,g),C=!0},p:be,i(n){C||(S(m.$$.fragment,n),C=!0)},o(n){b(m.$$.fragment,n),C=!1},d(n){n&&o(d),n&&o(u),L(m,n)}}}function Gd(w){let d,P,u,m,C;return m=new Le({props:{code:`from transformers import CLIPSegVisionConfig, CLIPSegVisionModel

# Initializing a CLIPSegVisionConfig with CIDAS/clipseg-rd64 style configuration
configuration = CLIPSegVisionConfig()

# Initializing a CLIPSegVisionModel (with random weights) from the CIDAS/clipseg-rd64 style configuration
model = CLIPSegVisionModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPSegVisionConfig, CLIPSegVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegVisionConfig with CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = CLIPSegVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegVisionModel (with random weights) from the CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){d=r("p"),P=i("Example:"),u=p(),I(m.$$.fragment)},l(n){d=s(n,"P",{});var g=a(d);P=l(g,"Example:"),g.forEach(o),u=f(n),v(m.$$.fragment,n)},m(n,g){_(n,d,g),e(d,P),_(n,u,g),$(m,n,g),C=!0},p:be,i(n){C||(S(m.$$.fragment,n),C=!0)},o(n){b(m.$$.fragment,n),C=!1},d(n){n&&o(d),n&&o(u),L(m,n)}}}function Jd(w){let d,P,u,m,C;return{c(){d=r("p"),P=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=r("code"),m=i("Module"),C=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(n){d=s(n,"P",{});var g=a(d);P=l(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=s(g,"CODE",{});var x=a(u);m=l(x,"Module"),x.forEach(o),C=l(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(o)},m(n,g){_(n,d,g),e(d,P),e(d,u),e(u,m),e(d,C)},d(n){n&&o(d)}}}function Zd(w){let d,P,u,m,C;return m=new Le({props:{code:`from PIL import Image
import requests
from transformers import CLIPSegProcessor, CLIPSegModel

processor = CLIPSegProcessor.from_pretrained("CIDAS/clipseg-rd64-refined")
model = CLIPSegModel.from_pretrained("CIDAS/clipseg-rd64-refined")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(
    text=["a photo of a cat", "a photo of a dog"], images=image, return_tensors="pt", padding=True
)

outputs = model(**inputs)
logits_per_image = outputs.logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPSegProcessor, CLIPSegModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = CLIPSegProcessor.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`}}),{c(){d=r("p"),P=i("Examples:"),u=p(),I(m.$$.fragment)},l(n){d=s(n,"P",{});var g=a(d);P=l(g,"Examples:"),g.forEach(o),u=f(n),v(m.$$.fragment,n)},m(n,g){_(n,d,g),e(d,P),_(n,u,g),$(m,n,g),C=!0},p:be,i(n){C||(S(m.$$.fragment,n),C=!0)},o(n){b(m.$$.fragment,n),C=!1},d(n){n&&o(d),n&&o(u),L(m,n)}}}function Kd(w){let d,P,u,m,C;return{c(){d=r("p"),P=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=r("code"),m=i("Module"),C=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(n){d=s(n,"P",{});var g=a(d);P=l(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=s(g,"CODE",{});var x=a(u);m=l(x,"Module"),x.forEach(o),C=l(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(o)},m(n,g){_(n,d,g),e(d,P),e(d,u),e(u,m),e(d,C)},d(n){n&&o(d)}}}function Qd(w){let d,P,u,m,C;return m=new Le({props:{code:`from transformers import CLIPTokenizer, CLIPSegModel

tokenizer = CLIPTokenizer.from_pretrained("CIDAS/clipseg-rd64-refined")
model = CLIPSegModel.from_pretrained("CIDAS/clipseg-rd64-refined")

inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")
text_features = model.get_text_features(**inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPTokenizer, CLIPSegModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = CLIPTokenizer.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`}}),{c(){d=r("p"),P=i("Examples:"),u=p(),I(m.$$.fragment)},l(n){d=s(n,"P",{});var g=a(d);P=l(g,"Examples:"),g.forEach(o),u=f(n),v(m.$$.fragment,n)},m(n,g){_(n,d,g),e(d,P),_(n,u,g),$(m,n,g),C=!0},p:be,i(n){C||(S(m.$$.fragment,n),C=!0)},o(n){b(m.$$.fragment,n),C=!1},d(n){n&&o(d),n&&o(u),L(m,n)}}}function Xd(w){let d,P,u,m,C;return{c(){d=r("p"),P=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=r("code"),m=i("Module"),C=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(n){d=s(n,"P",{});var g=a(d);P=l(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=s(g,"CODE",{});var x=a(u);m=l(x,"Module"),x.forEach(o),C=l(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(o)},m(n,g){_(n,d,g),e(d,P),e(d,u),e(u,m),e(d,C)},d(n){n&&o(d)}}}function Yd(w){let d,P,u,m,C;return m=new Le({props:{code:`from PIL import Image
import requests
from transformers import CLIPSegProcessor, CLIPSegModel

processor = CLIPSegProcessor.from_pretrained("CIDAS/clipseg-rd64-refined")
model = CLIPSegModel.from_pretrained("CIDAS/clipseg-rd64-refined")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, return_tensors="pt")

image_features = model.get_image_features(**inputs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPSegProcessor, CLIPSegModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = CLIPSegProcessor.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`}}),{c(){d=r("p"),P=i("Examples:"),u=p(),I(m.$$.fragment)},l(n){d=s(n,"P",{});var g=a(d);P=l(g,"Examples:"),g.forEach(o),u=f(n),v(m.$$.fragment,n)},m(n,g){_(n,d,g),e(d,P),_(n,u,g),$(m,n,g),C=!0},p:be,i(n){C||(S(m.$$.fragment,n),C=!0)},o(n){b(m.$$.fragment,n),C=!1},d(n){n&&o(d),n&&o(u),L(m,n)}}}function ec(w){let d,P,u,m,C;return{c(){d=r("p"),P=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=r("code"),m=i("Module"),C=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(n){d=s(n,"P",{});var g=a(d);P=l(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=s(g,"CODE",{});var x=a(u);m=l(x,"Module"),x.forEach(o),C=l(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(o)},m(n,g){_(n,d,g),e(d,P),e(d,u),e(u,m),e(d,C)},d(n){n&&o(d)}}}function tc(w){let d,P,u,m,C;return m=new Le({props:{code:`from transformers import CLIPTokenizer, CLIPSegTextModel

tokenizer = CLIPTokenizer.from_pretrained("CIDAS/clipseg-rd64-refined")
model = CLIPSegTextModel.from_pretrained("CIDAS/clipseg-rd64-refined")

inputs = tokenizer(["a photo of a cat", "a photo of a dog"], padding=True, return_tensors="pt")

outputs = model(**inputs)
last_hidden_state = outputs.last_hidden_state
pooled_output = outputs.pooler_output  # pooled (EOS token) states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPTokenizer, CLIPSegTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = CLIPTokenizer.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegTextModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled (EOS token) states</span>`}}),{c(){d=r("p"),P=i("Examples:"),u=p(),I(m.$$.fragment)},l(n){d=s(n,"P",{});var g=a(d);P=l(g,"Examples:"),g.forEach(o),u=f(n),v(m.$$.fragment,n)},m(n,g){_(n,d,g),e(d,P),_(n,u,g),$(m,n,g),C=!0},p:be,i(n){C||(S(m.$$.fragment,n),C=!0)},o(n){b(m.$$.fragment,n),C=!1},d(n){n&&o(d),n&&o(u),L(m,n)}}}function oc(w){let d,P,u,m,C;return{c(){d=r("p"),P=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=r("code"),m=i("Module"),C=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(n){d=s(n,"P",{});var g=a(d);P=l(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=s(g,"CODE",{});var x=a(u);m=l(x,"Module"),x.forEach(o),C=l(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(o)},m(n,g){_(n,d,g),e(d,P),e(d,u),e(u,m),e(d,C)},d(n){n&&o(d)}}}function nc(w){let d,P,u,m,C;return m=new Le({props:{code:`from PIL import Image
import requests
from transformers import CLIPSegProcessor, CLIPSegVisionModel

processor = CLIPSegProcessor.from_pretrained("CIDAS/clipseg-rd64-refined")
model = CLIPSegVisionModel.from_pretrained("CIDAS/clipseg-rd64-refined")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(images=image, return_tensors="pt")

outputs = model(**inputs)
last_hidden_state = outputs.last_hidden_state
pooled_output = outputs.pooler_output  # pooled CLS states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPSegProcessor, CLIPSegVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = CLIPSegProcessor.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegVisionModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled CLS states</span>`}}),{c(){d=r("p"),P=i("Examples:"),u=p(),I(m.$$.fragment)},l(n){d=s(n,"P",{});var g=a(d);P=l(g,"Examples:"),g.forEach(o),u=f(n),v(m.$$.fragment,n)},m(n,g){_(n,d,g),e(d,P),_(n,u,g),$(m,n,g),C=!0},p:be,i(n){C||(S(m.$$.fragment,n),C=!0)},o(n){b(m.$$.fragment,n),C=!1},d(n){n&&o(d),n&&o(u),L(m,n)}}}function rc(w){let d,P,u,m,C;return{c(){d=r("p"),P=i("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=r("code"),m=i("Module"),C=i(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(n){d=s(n,"P",{});var g=a(d);P=l(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=s(g,"CODE",{});var x=a(u);m=l(x,"Module"),x.forEach(o),C=l(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(o)},m(n,g){_(n,d,g),e(d,P),e(d,u),e(u,m),e(d,C)},d(n){n&&o(d)}}}function sc(w){let d,P,u,m,C;return m=new Le({props:{code:`from transformers import CLIPSegProcessor, CLIPSegForImageSegmentation
from PIL import Image
import requests

processor = CLIPSegProcessor.from_pretrained("CIDAS/clipseg-rd64-refined")
model = CLIPSegForImageSegmentation.from_pretrained("CIDAS/clipseg-rd64-refined")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)
texts = ["a cat", "a remote", "a blanket"]
inputs = processor(text=texts, images=[image] * len(texts), padding=True, return_tensors="pt")

outputs = model(**inputs)

logits = outputs.logits
print(logits.shape)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPSegProcessor, CLIPSegForImageSegmentation
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = CLIPSegProcessor.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>texts = [<span class="hljs-string">&quot;a cat&quot;</span>, <span class="hljs-string">&quot;a remote&quot;</span>, <span class="hljs-string">&quot;a blanket&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=texts, images=[image] * <span class="hljs-built_in">len</span>(texts), padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(logits.shape)
torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">352</span>, <span class="hljs-number">352</span>])`}}),{c(){d=r("p"),P=i("Examples:"),u=p(),I(m.$$.fragment)},l(n){d=s(n,"P",{});var g=a(d);P=l(g,"Examples:"),g.forEach(o),u=f(n),v(m.$$.fragment,n)},m(n,g){_(n,d,g),e(d,P),_(n,u,g),$(m,n,g),C=!0},p:be,i(n){C||(S(m.$$.fragment,n),C=!0)},o(n){b(m.$$.fragment,n),C=!1},d(n){n&&o(d),n&&o(u),L(m,n)}}}function ac(w){let d,P,u,m,C,n,g,x,Wr,Rn,X,we,Yo,gt,Br,en,Hr,Un,R,Rr,mt,Ur,Gr,so,Jr,Zr,Gn,ao,Kr,Jn,io,tn,Qr,Zn,lo,Xr,Kn,xe,U,co,Yr,es,go,ts,os,mo,ns,rs,ss,A,po,as,is,on,ls,ds,nn,cs,gs,rn,ms,ps,Qn,ye,fl,Xn,pt,fs,fo,hs,Yn,G,us,ft,_s,Cs,ht,Ps,Is,er,Y,Te,sn,ut,vs,an,$s,tr,ho,Ss,or,_t,nr,uo,Ct,bs,Pt,Ls,ws,rr,ee,ke,ln,It,xs,dn,ys,sr,T,vt,Ts,J,_o,ks,Es,Co,Ms,js,$t,zs,qs,As,te,Ds,Po,Fs,Vs,Io,Os,Ns,Ws,Ee,Bs,Me,St,Hs,bt,Rs,vo,Us,Gs,ar,oe,je,cn,Lt,Js,gn,Zs,ir,j,wt,Ks,ne,Qs,$o,Xs,Ys,xt,ea,ta,oa,re,na,So,ra,sa,bo,aa,ia,la,ze,lr,se,qe,mn,yt,da,pn,ca,dr,z,Tt,ga,ae,ma,Lo,pa,fa,kt,ha,ua,_a,ie,Ca,wo,Pa,Ia,xo,va,$a,Sa,Ae,cr,le,De,fn,Et,ba,hn,La,gr,k,Mt,wa,un,xa,ya,M,yo,Ta,ka,To,Ea,Ma,ko,ja,za,_n,qa,Aa,Eo,Da,Fa,Va,Fe,jt,Oa,zt,Na,Mo,Wa,Ba,Ha,Ve,qt,Ra,At,Ua,jo,Ga,Ja,mr,de,Oe,Cn,Dt,Za,Pn,Ka,pr,E,Ft,Qa,Vt,Xa,Ot,Ya,ei,ti,D,Nt,oi,ce,ni,zo,ri,si,In,ai,ii,li,Ne,di,We,ci,F,Wt,gi,ge,mi,qo,pi,fi,vn,hi,ui,_i,Be,Ci,He,Pi,V,Bt,Ii,me,vi,Ao,$i,Si,$n,bi,Li,wi,Re,xi,Ue,fr,pe,Ge,Sn,Ht,yi,bn,Ti,hr,fe,Rt,ki,O,Ut,Ei,he,Mi,Do,ji,zi,Ln,qi,Ai,Di,Je,Fi,Ze,ur,ue,Ke,wn,Gt,Vi,xn,Oi,_r,_e,Jt,Ni,N,Zt,Wi,Ce,Bi,Fo,Hi,Ri,yn,Ui,Gi,Ji,Qe,Zi,Xe,Cr,Pe,Ye,Tn,Kt,Ki,kn,Qi,Pr,q,Qt,Xi,En,Yi,el,Xt,tl,Yt,ol,nl,rl,W,eo,sl,Ie,al,Vo,il,ll,Mn,dl,cl,gl,et,ml,tt,Ir;return n=new H({}),gt=new H({}),ut=new H({}),_t=new Hd({props:{pipeline:"image-segmentation"}}),It=new H({}),vt=new y({props:{name:"class transformers.CLIPSegConfig",anchor:"transformers.CLIPSegConfig",parameters:[{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"projection_dim",val:" = 512"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"extract_layers",val:" = [3, 6, 9]"},{name:"reduce_dim",val:" = 64"},{name:"decoder_num_attention_heads",val:" = 4"},{name:"decoder_attention_dropout",val:" = 0.0"},{name:"decoder_hidden_act",val:" = 'quick_gelu'"},{name:"decoder_intermediate_size",val:" = 2048"},{name:"conditional_layer",val:" = 0"},{name:"use_complex_transposed_convolution",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPSegConfig.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegTextConfig">CLIPSegTextConfig</a>.`,name:"text_config"},{anchor:"transformers.CLIPSegConfig.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegVisionConfig">CLIPSegVisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.CLIPSegConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.CLIPSegConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> paramter. Default is used as per the original CLIPSeg implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.CLIPSegConfig.extract_layers",description:`<strong>extract_layers</strong> (<code>List[int]</code>, <em>optional</em>, defaults to [3, 6, 9]) &#x2014;
Layers to extract when forwarding the query image through the frozen visual backbone of CLIP.`,name:"extract_layers"},{anchor:"transformers.CLIPSegConfig.reduce_dim",description:`<strong>reduce_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality to reduce the CLIP vision embedding.`,name:"reduce_dim"},{anchor:"transformers.CLIPSegConfig.decoder_num_attention_heads",description:`<strong>decoder_num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Number of attention heads in the decoder of CLIPSeg.`,name:"decoder_num_attention_heads"},{anchor:"transformers.CLIPSegConfig.decoder_attention_dropout",description:`<strong>decoder_attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"decoder_attention_dropout"},{anchor:"transformers.CLIPSegConfig.decoder_hidden_act",description:`<strong>decoder_hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported. layer_norm_eps (<code>float</code>, <em>optional</em>,
defaults to 1e-5): The epsilon used by the layer normalization layers.`,name:"decoder_hidden_act"},{anchor:"transformers.CLIPSegConfig.decoder_intermediate_size",description:`<strong>decoder_intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layers in the Transformer decoder.`,name:"decoder_intermediate_size"},{anchor:"transformers.CLIPSegConfig.conditional_layer",description:`<strong>conditional_layer</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The layer to use of the Transformer encoder whose activations will be combined with the condition
embeddings using FiLM (Feature-wise Linear Modulation). If 0, the last layer is used.`,name:"conditional_layer"},{anchor:"transformers.CLIPSegConfig.use_complex_transposed_convolution",description:`<strong>use_complex_transposed_convolution</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use a more complex transposed convolution in the decoder, enabling more fine-grained
segmentation.`,name:"use_complex_transposed_convolution"},{anchor:"transformers.CLIPSegConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clipseg/configuration_clipseg.py#L244"}}),Ee=new Se({props:{anchor:"transformers.CLIPSegConfig.example",$$slots:{default:[Rd]},$$scope:{ctx:w}}}),St=new y({props:{name:"from_text_vision_configs",anchor:"transformers.CLIPSegConfig.from_text_vision_configs",parameters:[{name:"text_config",val:": CLIPSegTextConfig"},{name:"vision_config",val:": CLIPSegVisionConfig"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clipseg/configuration_clipseg.py#L360",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegConfig"
>CLIPSegConfig</a></p>
`}}),Lt=new H({}),wt=new y({props:{name:"class transformers.CLIPSegTextConfig",anchor:"transformers.CLIPSegTextConfig",parameters:[{name:"vocab_size",val:" = 49408"},{name:"hidden_size",val:" = 512"},{name:"intermediate_size",val:" = 2048"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 8"},{name:"max_position_embeddings",val:" = 77"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPSegTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 49408) &#x2014;
Vocabulary size of the CLIPSeg text model. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a>.`,name:"vocab_size"},{anchor:"transformers.CLIPSegTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.CLIPSegTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.CLIPSegTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.CLIPSegTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.CLIPSegTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 77) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.CLIPSegTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported. layer_norm_eps (<code>float</code>, <em>optional</em>,
defaults to 1e-5): The epsilon used by the layer normalization layers.`,name:"hidden_act"},{anchor:"transformers.CLIPSegTextConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.CLIPSegTextConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.CLIPSegTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.CLIPSegTextConfig.initializer_factor",description:`<strong>initializer_factor</strong> (\`float&#x201C;, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clipseg/configuration_clipseg.py#L32"}}),ze=new Se({props:{anchor:"transformers.CLIPSegTextConfig.example",$$slots:{default:[Ud]},$$scope:{ctx:w}}}),yt=new H({}),Tt=new y({props:{name:"class transformers.CLIPSegVisionConfig",anchor:"transformers.CLIPSegVisionConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"intermediate_size",val:" = 3072"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"num_channels",val:" = 3"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 32"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPSegVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.CLIPSegVisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.CLIPSegVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.CLIPSegVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.CLIPSegVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.CLIPSegVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.CLIPSegVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported. layer_norm_eps (<code>float</code>, <em>optional</em>,
defaults to 1e-5): The epsilon used by the layer normalization layers.`,name:"hidden_act"},{anchor:"transformers.CLIPSegVisionConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.CLIPSegVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.CLIPSegVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.CLIPSegVisionConfig.initializer_factor",description:`<strong>initializer_factor</strong> (\`float&#x201C;, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clipseg/configuration_clipseg.py#L139"}}),Ae=new Se({props:{anchor:"transformers.CLIPSegVisionConfig.example",$$slots:{default:[Gd]},$$scope:{ctx:w}}}),Et=new H({}),Mt=new y({props:{name:"class transformers.CLIPSegProcessor",anchor:"transformers.CLIPSegProcessor",parameters:[{name:"feature_extractor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.CLIPSegProcessor.feature_extractor",description:`<strong>feature_extractor</strong> (<a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor">ViTFeatureExtractor</a>) &#x2014;
The feature extractor is a required input.`,name:"feature_extractor"},{anchor:"transformers.CLIPSegProcessor.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a>) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clipseg/processing_clipseg.py#L22"}}),jt=new y({props:{name:"batch_decode",anchor:"transformers.CLIPSegProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clipseg/processing_clipseg.py#L96"}}),qt=new y({props:{name:"decode",anchor:"transformers.CLIPSegProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clipseg/processing_clipseg.py#L103"}}),Dt=new H({}),Ft=new y({props:{name:"class transformers.CLIPSegModel",anchor:"transformers.CLIPSegModel",parameters:[{name:"config",val:": CLIPSegConfig"}],parametersDescription:[{anchor:"transformers.CLIPSegModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L940"}}),Nt=new y({props:{name:"forward",anchor:"transformers.CLIPSegModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"return_loss",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.CLIPSegModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPSegModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPSegModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPSegModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPFeatureExtractor</a>. See <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor.__call__">CLIPFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.CLIPSegModel.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.CLIPSegModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L1071",returnDescription:`
<p>A <code>transformers.models.clipseg.modeling_clipseg.CLIPSegOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clipseg.configuration_clipseg.CLIPSegConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) \u2014 Contrastive loss for image-text similarity.</li>
<li><strong>logits_per_image:(<code>torch.FloatTensor</code></strong> of shape <code>(image_batch_size, text_batch_size)</code>) \u2014 The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text:(<code>torch.FloatTensor</code></strong> of shape <code>(text_batch_size, image_batch_size)</code>) \u2014 The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>) \u2014 The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegTextModel"
>CLIPSegTextModel</a>.</li>
<li><strong>image_embeds(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>) \u2014 The image embeddings obtained by applying the projection layer to the pooled output of
<a
  href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</li>
<li><strong>text_model_output(<code>BaseModelOutputWithPooling</code>):</strong>
The output of the <a
  href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegTextModel"
>CLIPSegTextModel</a>.</li>
<li><strong>vision_model_output(<code>BaseModelOutputWithPooling</code>):</strong>
The output of the <a
  href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</li>
</ul>
`,returnType:`
<p><code>transformers.models.clipseg.modeling_clipseg.CLIPSegOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ne=new Xo({props:{$$slots:{default:[Jd]},$$scope:{ctx:w}}}),We=new Se({props:{anchor:"transformers.CLIPSegModel.forward.example",$$slots:{default:[Zd]},$$scope:{ctx:w}}}),Wt=new y({props:{name:"get_text_features",anchor:"transformers.CLIPSegModel.get_text_features",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.CLIPSegModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPSegModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPSegModel.get_text_features.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPSegModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L975",returnDescription:`
<p>The text embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegTextModel"
>CLIPSegTextModel</a>.</p>
`,returnType:`
<p>text_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),Be=new Xo({props:{$$slots:{default:[Kd]},$$scope:{ctx:w}}}),He=new Se({props:{anchor:"transformers.CLIPSegModel.get_text_features.example",$$slots:{default:[Qd]},$$scope:{ctx:w}}}),Bt=new y({props:{name:"get_image_features",anchor:"transformers.CLIPSegModel.get_image_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.CLIPSegModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPFeatureExtractor</a>. See <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor.__call__">CLIPFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.CLIPSegModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L1022",returnDescription:`
<p>The image embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</p>
`,returnType:`
<p>image_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),Re=new Xo({props:{$$slots:{default:[Xd]},$$scope:{ctx:w}}}),Ue=new Se({props:{anchor:"transformers.CLIPSegModel.get_image_features.example",$$slots:{default:[Yd]},$$scope:{ctx:w}}}),Ht=new H({}),Rt=new y({props:{name:"class transformers.CLIPSegTextModel",anchor:"transformers.CLIPSegTextModel",parameters:[{name:"config",val:": CLIPSegTextConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L772"}}),Ut=new y({props:{name:"forward",anchor:"transformers.CLIPSegTextModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.CLIPSegTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPSegTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPSegTextModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPSegTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L789",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clipseg.configuration_clipseg.CLIPSegTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Je=new Xo({props:{$$slots:{default:[ec]},$$scope:{ctx:w}}}),Ze=new Se({props:{anchor:"transformers.CLIPSegTextModel.forward.example",$$slots:{default:[tc]},$$scope:{ctx:w}}}),Gt=new H({}),Jt=new y({props:{name:"class transformers.CLIPSegVisionModel",anchor:"transformers.CLIPSegVisionModel",parameters:[{name:"config",val:": CLIPSegVisionConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L887"}}),Zt=new y({props:{name:"forward",anchor:"transformers.CLIPSegVisionModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.CLIPSegVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPFeatureExtractor</a>. See <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor.__call__">CLIPFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.CLIPSegVisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L900",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clipseg.configuration_clipseg.CLIPSegVisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Qe=new Xo({props:{$$slots:{default:[oc]},$$scope:{ctx:w}}}),Xe=new Se({props:{anchor:"transformers.CLIPSegVisionModel.forward.example",$$slots:{default:[nc]},$$scope:{ctx:w}}}),Kt=new H({}),Qt=new y({props:{name:"class transformers.CLIPSegForImageSegmentation",anchor:"transformers.CLIPSegForImageSegmentation",parameters:[{name:"config",val:": CLIPSegConfig"}],parametersDescription:[{anchor:"transformers.CLIPSegForImageSegmentation.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L1328"}}),eo=new y({props:{name:"forward",anchor:"transformers.CLIPSegForImageSegmentation.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.FloatTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"conditional_pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"conditional_embeddings",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.CLIPSegForImageSegmentation.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPFeatureExtractor</a>. See <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor.__call__">CLIPFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L1373",returnDescription:`
<p>A <code>transformers.models.clipseg.modeling_clipseg.CLIPSegImageSegmentationOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clipseg.configuration_clipseg.CLIPSegTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) \u2014 Contrastive loss for image-text similarity.
\u2026</li>
<li><strong>vision_model_output</strong> (<code>BaseModelOutputWithPooling</code>) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</li>
</ul>
`,returnType:`
<p><code>transformers.models.clipseg.modeling_clipseg.CLIPSegImageSegmentationOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),et=new Xo({props:{$$slots:{default:[rc]},$$scope:{ctx:w}}}),tt=new Se({props:{anchor:"transformers.CLIPSegForImageSegmentation.forward.example",$$slots:{default:[sc]},$$scope:{ctx:w}}}),{c(){d=r("meta"),P=p(),u=r("h1"),m=r("a"),C=r("span"),I(n.$$.fragment),g=p(),x=r("span"),Wr=i("CLIPSeg"),Rn=p(),X=r("h2"),we=r("a"),Yo=r("span"),I(gt.$$.fragment),Br=p(),en=r("span"),Hr=i("Overview"),Un=p(),R=r("p"),Rr=i("The CLIPSeg model was proposed in "),mt=r("a"),Ur=i("Image Segmentation Using Text and Image Prompts"),Gr=i(` by Timo L\xFCddecke
and Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen `),so=r("a"),Jr=i("CLIP"),Zr=i(" model for zero- and one-shot image segmentation."),Gn=p(),ao=r("p"),Kr=i("The abstract from the paper is the following:"),Jn=p(),io=r("p"),tn=r("em"),Qr=i(`Image segmentation is usually addressed by training a
model for a fixed set of object classes. Incorporating additional classes or more complex queries later is expensive
as it requires re-training the model on a dataset that encompasses these expressions. Here we propose a system
that can generate image segmentations based on arbitrary
prompts at test time. A prompt can be either a text or an
image. This approach enables us to create a unified model
(trained once) for three common segmentation tasks, which
come with distinct challenges: referring expression segmentation, zero-shot segmentation and one-shot segmentation.
We build upon the CLIP model as a backbone which we extend with a transformer-based decoder that enables dense
prediction. After training on an extended version of the
PhraseCut dataset, our system generates a binary segmentation map for an image based on a free-text prompt or on
an additional image expressing the query. We analyze different variants of the latter image-based prompts in detail.
This novel hybrid input allows for dynamic adaptation not
only to the three segmentation tasks mentioned above, but
to any binary segmentation task where a text or image query
can be formulated. Finally, we find our system to adapt well
to generalized queries involving affordances or properties`),Zn=p(),lo=r("p"),Xr=i("Tips:"),Kn=p(),xe=r("ul"),U=r("li"),co=r("a"),Yr=i("CLIPSegForImageSegmentation"),es=i(" adds a decoder on top of "),go=r("a"),ts=i("CLIPSegModel"),os=i(". The latter is identical to "),mo=r("a"),ns=i("CLIPModel"),rs=i("."),ss=p(),A=r("li"),po=r("a"),as=i("CLIPSegForImageSegmentation"),is=i(` can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text
(provided to the model as `),on=r("code"),ls=i("input_ids"),ds=i(") or an image (provided to the model as "),nn=r("code"),cs=i("conditional_pixel_values"),gs=i(`). One can also provide custom
conditional embeddings (provided to the model as `),rn=r("code"),ms=i("conditional_embeddings"),ps=i(")."),Qn=p(),ye=r("img"),Xn=p(),pt=r("small"),fs=i("CLIPSeg overview. Taken from the "),fo=r("a"),hs=i("original paper."),Yn=p(),G=r("p"),us=i("This model was contributed by "),ft=r("a"),_s=i("nielsr"),Cs=i(`.
The original code can be found `),ht=r("a"),Ps=i("here"),Is=i("."),er=p(),Y=r("h2"),Te=r("a"),sn=r("span"),I(ut.$$.fragment),vs=p(),an=r("span"),$s=i("Resources"),tr=p(),ho=r("p"),Ss=i("A list of official Hugging Face and community (indicated by \u{1F30E}) resources to help you get started with CLIPSeg. If you\u2019re interested in submitting a resource to be included here, please feel free to open a Pull Request and we\u2019ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource."),or=p(),I(_t.$$.fragment),nr=p(),uo=r("ul"),Ct=r("li"),bs=i("A notebook that illustrates "),Pt=r("a"),Ls=i("zero-shot image segmentation with CLIPSeg"),ws=i("."),rr=p(),ee=r("h2"),ke=r("a"),ln=r("span"),I(It.$$.fragment),xs=p(),dn=r("span"),ys=i("CLIPSegConfig"),sr=p(),T=r("div"),I(vt.$$.fragment),Ts=p(),J=r("p"),_o=r("a"),ks=i("CLIPSegConfig"),Es=i(" is the configuration class to store the configuration of a "),Co=r("a"),Ms=i("CLIPSegModel"),js=i(`. It is used to
instantiate a CLIPSeg model according to the specified arguments, defining the text model and vision model configs.
Instantiating a configuration with the defaults will yield a similar configuration to that of the CLIPSeg
`),$t=r("a"),zs=i("CIDAS/clipseg-rd64"),qs=i(" architecture."),As=p(),te=r("p"),Ds=i("Configuration objects inherit from "),Po=r("a"),Fs=i("PretrainedConfig"),Vs=i(` and can be used to control the model outputs. Read the
documentation from `),Io=r("a"),Os=i("PretrainedConfig"),Ns=i(" for more information."),Ws=p(),I(Ee.$$.fragment),Bs=p(),Me=r("div"),I(St.$$.fragment),Hs=p(),bt=r("p"),Rs=i("Instantiate a "),vo=r("a"),Us=i("CLIPSegConfig"),Gs=i(` (or a derived class) from clipseg text model configuration and clipseg vision
model configuration.`),ar=p(),oe=r("h2"),je=r("a"),cn=r("span"),I(Lt.$$.fragment),Js=p(),gn=r("span"),Zs=i("CLIPSegTextConfig"),ir=p(),j=r("div"),I(wt.$$.fragment),Ks=p(),ne=r("p"),Qs=i("This is the configuration class to store the configuration of a "),$o=r("a"),Xs=i("CLIPSegModel"),Ys=i(`. It is used to instantiate an
CLIPSeg model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the CLIPSeg
`),xt=r("a"),ea=i("CIDAS/clipseg-rd64"),ta=i(" architecture."),oa=p(),re=r("p"),na=i("Configuration objects inherit from "),So=r("a"),ra=i("PretrainedConfig"),sa=i(` and can be used to control the model outputs. Read the
documentation from `),bo=r("a"),aa=i("PretrainedConfig"),ia=i(" for more information."),la=p(),I(ze.$$.fragment),lr=p(),se=r("h2"),qe=r("a"),mn=r("span"),I(yt.$$.fragment),da=p(),pn=r("span"),ca=i("CLIPSegVisionConfig"),dr=p(),z=r("div"),I(Tt.$$.fragment),ga=p(),ae=r("p"),ma=i("This is the configuration class to store the configuration of a "),Lo=r("a"),pa=i("CLIPSegModel"),fa=i(`. It is used to instantiate an
CLIPSeg model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the CLIPSeg
`),kt=r("a"),ha=i("CIDAS/clipseg-rd64"),ua=i(" architecture."),_a=p(),ie=r("p"),Ca=i("Configuration objects inherit from "),wo=r("a"),Pa=i("PretrainedConfig"),Ia=i(` and can be used to control the model outputs. Read the
documentation from `),xo=r("a"),va=i("PretrainedConfig"),$a=i(" for more information."),Sa=p(),I(Ae.$$.fragment),cr=p(),le=r("h2"),De=r("a"),fn=r("span"),I(Et.$$.fragment),ba=p(),hn=r("span"),La=i("CLIPSegProcessor"),gr=p(),k=r("div"),I(Mt.$$.fragment),wa=p(),un=r("p"),xa=i(`Constructs a CLIPSeg processor which wraps a CLIPSeg feature extractor and a CLIP tokenizer into a single
processor.`),ya=p(),M=r("p"),yo=r("a"),Ta=i("CLIPSegProcessor"),ka=i(" offers all the functionalities of "),To=r("a"),Ea=i("ViTFeatureExtractor"),Ma=i(" and "),ko=r("a"),ja=i("CLIPTokenizerFast"),za=i(`. See the
`),_n=r("code"),qa=i("__call__()"),Aa=i(" and "),Eo=r("a"),Da=i("decode()"),Fa=i(" for more information."),Va=p(),Fe=r("div"),I(jt.$$.fragment),Oa=p(),zt=r("p"),Na=i("This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Mo=r("a"),Wa=i("batch_decode()"),Ba=i(`. Please
refer to the docstring of this method for more information.`),Ha=p(),Ve=r("div"),I(qt.$$.fragment),Ra=p(),At=r("p"),Ua=i("This method forwards all its arguments to CLIPTokenizerFast\u2019s "),jo=r("a"),Ga=i("decode()"),Ja=i(`. Please refer to
the docstring of this method for more information.`),mr=p(),de=r("h2"),Oe=r("a"),Cn=r("span"),I(Dt.$$.fragment),Za=p(),Pn=r("span"),Ka=i("CLIPSegModel"),pr=p(),E=r("div"),I(Ft.$$.fragment),Qa=p(),Vt=r("p"),Xa=i("This model is a PyTorch "),Ot=r("a"),Ya=i("torch.nn.Module"),ei=i(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ti=p(),D=r("div"),I(Nt.$$.fragment),oi=p(),ce=r("p"),ni=i("The "),zo=r("a"),ri=i("CLIPSegModel"),si=i(" forward method, overrides the "),In=r("code"),ai=i("__call__"),ii=i(" special method."),li=p(),I(Ne.$$.fragment),di=p(),I(We.$$.fragment),ci=p(),F=r("div"),I(Wt.$$.fragment),gi=p(),ge=r("p"),mi=i("The "),qo=r("a"),pi=i("CLIPSegModel"),fi=i(" forward method, overrides the "),vn=r("code"),hi=i("__call__"),ui=i(" special method."),_i=p(),I(Be.$$.fragment),Ci=p(),I(He.$$.fragment),Pi=p(),V=r("div"),I(Bt.$$.fragment),Ii=p(),me=r("p"),vi=i("The "),Ao=r("a"),$i=i("CLIPSegModel"),Si=i(" forward method, overrides the "),$n=r("code"),bi=i("__call__"),Li=i(" special method."),wi=p(),I(Re.$$.fragment),xi=p(),I(Ue.$$.fragment),fr=p(),pe=r("h2"),Ge=r("a"),Sn=r("span"),I(Ht.$$.fragment),yi=p(),bn=r("span"),Ti=i("CLIPSegTextModel"),hr=p(),fe=r("div"),I(Rt.$$.fragment),ki=p(),O=r("div"),I(Ut.$$.fragment),Ei=p(),he=r("p"),Mi=i("The "),Do=r("a"),ji=i("CLIPSegTextModel"),zi=i(" forward method, overrides the "),Ln=r("code"),qi=i("__call__"),Ai=i(" special method."),Di=p(),I(Je.$$.fragment),Fi=p(),I(Ze.$$.fragment),ur=p(),ue=r("h2"),Ke=r("a"),wn=r("span"),I(Gt.$$.fragment),Vi=p(),xn=r("span"),Oi=i("CLIPSegVisionModel"),_r=p(),_e=r("div"),I(Jt.$$.fragment),Ni=p(),N=r("div"),I(Zt.$$.fragment),Wi=p(),Ce=r("p"),Bi=i("The "),Fo=r("a"),Hi=i("CLIPSegVisionModel"),Ri=i(" forward method, overrides the "),yn=r("code"),Ui=i("__call__"),Gi=i(" special method."),Ji=p(),I(Qe.$$.fragment),Zi=p(),I(Xe.$$.fragment),Cr=p(),Pe=r("h2"),Ye=r("a"),Tn=r("span"),I(Kt.$$.fragment),Ki=p(),kn=r("span"),Qi=i("CLIPSegForImageSegmentation"),Pr=p(),q=r("div"),I(Qt.$$.fragment),Xi=p(),En=r("p"),Yi=i("CLIPSeg model with a Transformer-based decoder on top for zero-shot and one-shot image segmentation."),el=p(),Xt=r("p"),tl=i("This model is a PyTorch "),Yt=r("a"),ol=i("torch.nn.Module"),nl=i(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),rl=p(),W=r("div"),I(eo.$$.fragment),sl=p(),Ie=r("p"),al=i("The "),Vo=r("a"),il=i("CLIPSegForImageSegmentation"),ll=i(" forward method, overrides the "),Mn=r("code"),dl=i("__call__"),cl=i(" special method."),gl=p(),I(et.$$.fragment),ml=p(),I(tt.$$.fragment),this.h()},l(t){const h=Nd('[data-svelte="svelte-1phssyn"]',document.head);d=s(h,"META",{name:!0,content:!0}),h.forEach(o),P=f(t),u=s(t,"H1",{class:!0});var to=a(u);m=s(to,"A",{id:!0,class:!0,href:!0});var jn=a(m);C=s(jn,"SPAN",{});var zn=a(C);v(n.$$.fragment,zn),zn.forEach(o),jn.forEach(o),g=f(to),x=s(to,"SPAN",{});var qn=a(x);Wr=l(qn,"CLIPSeg"),qn.forEach(o),to.forEach(o),Rn=f(t),X=s(t,"H2",{class:!0});var oo=a(X);we=s(oo,"A",{id:!0,class:!0,href:!0});var An=a(we);Yo=s(An,"SPAN",{});var Dn=a(Yo);v(gt.$$.fragment,Dn),Dn.forEach(o),An.forEach(o),Br=f(oo),en=s(oo,"SPAN",{});var Fn=a(en);Hr=l(Fn,"Overview"),Fn.forEach(o),oo.forEach(o),Un=f(t),R=s(t,"P",{});var ve=a(R);Rr=l(ve,"The CLIPSeg model was proposed in "),mt=s(ve,"A",{href:!0,rel:!0});var Vn=a(mt);Ur=l(Vn,"Image Segmentation Using Text and Image Prompts"),Vn.forEach(o),Gr=l(ve,` by Timo L\xFCddecke
and Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen `),so=s(ve,"A",{href:!0});var On=a(so);Jr=l(On,"CLIP"),On.forEach(o),Zr=l(ve," model for zero- and one-shot image segmentation."),ve.forEach(o),Gn=f(t),ao=s(t,"P",{});var Nn=a(ao);Kr=l(Nn,"The abstract from the paper is the following:"),Nn.forEach(o),Jn=f(t),io=s(t,"P",{});var Wn=a(io);tn=s(Wn,"EM",{});var Bn=a(tn);Qr=l(Bn,`Image segmentation is usually addressed by training a
model for a fixed set of object classes. Incorporating additional classes or more complex queries later is expensive
as it requires re-training the model on a dataset that encompasses these expressions. Here we propose a system
that can generate image segmentations based on arbitrary
prompts at test time. A prompt can be either a text or an
image. This approach enables us to create a unified model
(trained once) for three common segmentation tasks, which
come with distinct challenges: referring expression segmentation, zero-shot segmentation and one-shot segmentation.
We build upon the CLIP model as a backbone which we extend with a transformer-based decoder that enables dense
prediction. After training on an extended version of the
PhraseCut dataset, our system generates a binary segmentation map for an image based on a free-text prompt or on
an additional image expressing the query. We analyze different variants of the latter image-based prompts in detail.
This novel hybrid input allows for dynamic adaptation not
only to the three segmentation tasks mentioned above, but
to any binary segmentation task where a text or image query
can be formulated. Finally, we find our system to adapt well
to generalized queries involving affordances or properties`),Bn.forEach(o),Wn.forEach(o),Zn=f(t),lo=s(t,"P",{});var Hn=a(lo);Xr=l(Hn,"Tips:"),Hn.forEach(o),Kn=f(t),xe=s(t,"UL",{});var vr=a(xe);U=s(vr,"LI",{});var no=a(U);co=s(no,"A",{href:!0});var hl=a(co);Yr=l(hl,"CLIPSegForImageSegmentation"),hl.forEach(o),es=l(no," adds a decoder on top of "),go=s(no,"A",{href:!0});var ul=a(go);ts=l(ul,"CLIPSegModel"),ul.forEach(o),os=l(no,". The latter is identical to "),mo=s(no,"A",{href:!0});var _l=a(mo);ns=l(_l,"CLIPModel"),_l.forEach(o),rs=l(no,"."),no.forEach(o),ss=f(vr),A=s(vr,"LI",{});var $e=a(A);po=s($e,"A",{href:!0});var Cl=a(po);as=l(Cl,"CLIPSegForImageSegmentation"),Cl.forEach(o),is=l($e,` can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text
(provided to the model as `),on=s($e,"CODE",{});var Pl=a(on);ls=l(Pl,"input_ids"),Pl.forEach(o),ds=l($e,") or an image (provided to the model as "),nn=s($e,"CODE",{});var Il=a(nn);cs=l(Il,"conditional_pixel_values"),Il.forEach(o),gs=l($e,`). One can also provide custom
conditional embeddings (provided to the model as `),rn=s($e,"CODE",{});var vl=a(rn);ms=l(vl,"conditional_embeddings"),vl.forEach(o),ps=l($e,")."),$e.forEach(o),vr.forEach(o),Qn=f(t),ye=s(t,"IMG",{src:!0,alt:!0,width:!0}),Xn=f(t),pt=s(t,"SMALL",{});var pl=a(pt);fs=l(pl,"CLIPSeg overview. Taken from the "),fo=s(pl,"A",{href:!0});var $l=a(fo);hs=l($l,"original paper."),$l.forEach(o),pl.forEach(o),Yn=f(t),G=s(t,"P",{});var Oo=a(G);us=l(Oo,"This model was contributed by "),ft=s(Oo,"A",{href:!0,rel:!0});var Sl=a(ft);_s=l(Sl,"nielsr"),Sl.forEach(o),Cs=l(Oo,`.
The original code can be found `),ht=s(Oo,"A",{href:!0,rel:!0});var bl=a(ht);Ps=l(bl,"here"),bl.forEach(o),Is=l(Oo,"."),Oo.forEach(o),er=f(t),Y=s(t,"H2",{class:!0});var $r=a(Y);Te=s($r,"A",{id:!0,class:!0,href:!0});var Ll=a(Te);sn=s(Ll,"SPAN",{});var wl=a(sn);v(ut.$$.fragment,wl),wl.forEach(o),Ll.forEach(o),vs=f($r),an=s($r,"SPAN",{});var xl=a(an);$s=l(xl,"Resources"),xl.forEach(o),$r.forEach(o),tr=f(t),ho=s(t,"P",{});var yl=a(ho);Ss=l(yl,"A list of official Hugging Face and community (indicated by \u{1F30E}) resources to help you get started with CLIPSeg. If you\u2019re interested in submitting a resource to be included here, please feel free to open a Pull Request and we\u2019ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource."),yl.forEach(o),or=f(t),v(_t.$$.fragment,t),nr=f(t),uo=s(t,"UL",{});var Tl=a(uo);Ct=s(Tl,"LI",{});var Sr=a(Ct);bs=l(Sr,"A notebook that illustrates "),Pt=s(Sr,"A",{href:!0,rel:!0});var kl=a(Pt);Ls=l(kl,"zero-shot image segmentation with CLIPSeg"),kl.forEach(o),ws=l(Sr,"."),Sr.forEach(o),Tl.forEach(o),rr=f(t),ee=s(t,"H2",{class:!0});var br=a(ee);ke=s(br,"A",{id:!0,class:!0,href:!0});var El=a(ke);ln=s(El,"SPAN",{});var Ml=a(ln);v(It.$$.fragment,Ml),Ml.forEach(o),El.forEach(o),xs=f(br),dn=s(br,"SPAN",{});var jl=a(dn);ys=l(jl,"CLIPSegConfig"),jl.forEach(o),br.forEach(o),sr=f(t),T=s(t,"DIV",{class:!0});var Z=a(T);v(vt.$$.fragment,Z),Ts=f(Z),J=s(Z,"P",{});var ro=a(J);_o=s(ro,"A",{href:!0});var zl=a(_o);ks=l(zl,"CLIPSegConfig"),zl.forEach(o),Es=l(ro," is the configuration class to store the configuration of a "),Co=s(ro,"A",{href:!0});var ql=a(Co);Ms=l(ql,"CLIPSegModel"),ql.forEach(o),js=l(ro,`. It is used to
instantiate a CLIPSeg model according to the specified arguments, defining the text model and vision model configs.
Instantiating a configuration with the defaults will yield a similar configuration to that of the CLIPSeg
`),$t=s(ro,"A",{href:!0,rel:!0});var Al=a($t);zs=l(Al,"CIDAS/clipseg-rd64"),Al.forEach(o),qs=l(ro," architecture."),ro.forEach(o),As=f(Z),te=s(Z,"P",{});var No=a(te);Ds=l(No,"Configuration objects inherit from "),Po=s(No,"A",{href:!0});var Dl=a(Po);Fs=l(Dl,"PretrainedConfig"),Dl.forEach(o),Vs=l(No,` and can be used to control the model outputs. Read the
documentation from `),Io=s(No,"A",{href:!0});var Fl=a(Io);Os=l(Fl,"PretrainedConfig"),Fl.forEach(o),Ns=l(No," for more information."),No.forEach(o),Ws=f(Z),v(Ee.$$.fragment,Z),Bs=f(Z),Me=s(Z,"DIV",{class:!0});var Lr=a(Me);v(St.$$.fragment,Lr),Hs=f(Lr),bt=s(Lr,"P",{});var wr=a(bt);Rs=l(wr,"Instantiate a "),vo=s(wr,"A",{href:!0});var Vl=a(vo);Us=l(Vl,"CLIPSegConfig"),Vl.forEach(o),Gs=l(wr,` (or a derived class) from clipseg text model configuration and clipseg vision
model configuration.`),wr.forEach(o),Lr.forEach(o),Z.forEach(o),ar=f(t),oe=s(t,"H2",{class:!0});var xr=a(oe);je=s(xr,"A",{id:!0,class:!0,href:!0});var Ol=a(je);cn=s(Ol,"SPAN",{});var Nl=a(cn);v(Lt.$$.fragment,Nl),Nl.forEach(o),Ol.forEach(o),Js=f(xr),gn=s(xr,"SPAN",{});var Wl=a(gn);Zs=l(Wl,"CLIPSegTextConfig"),Wl.forEach(o),xr.forEach(o),ir=f(t),j=s(t,"DIV",{class:!0});var ot=a(j);v(wt.$$.fragment,ot),Ks=f(ot),ne=s(ot,"P",{});var Wo=a(ne);Qs=l(Wo,"This is the configuration class to store the configuration of a "),$o=s(Wo,"A",{href:!0});var Bl=a($o);Xs=l(Bl,"CLIPSegModel"),Bl.forEach(o),Ys=l(Wo,`. It is used to instantiate an
CLIPSeg model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the CLIPSeg
`),xt=s(Wo,"A",{href:!0,rel:!0});var Hl=a(xt);ea=l(Hl,"CIDAS/clipseg-rd64"),Hl.forEach(o),ta=l(Wo," architecture."),Wo.forEach(o),oa=f(ot),re=s(ot,"P",{});var Bo=a(re);na=l(Bo,"Configuration objects inherit from "),So=s(Bo,"A",{href:!0});var Rl=a(So);ra=l(Rl,"PretrainedConfig"),Rl.forEach(o),sa=l(Bo,` and can be used to control the model outputs. Read the
documentation from `),bo=s(Bo,"A",{href:!0});var Ul=a(bo);aa=l(Ul,"PretrainedConfig"),Ul.forEach(o),ia=l(Bo," for more information."),Bo.forEach(o),la=f(ot),v(ze.$$.fragment,ot),ot.forEach(o),lr=f(t),se=s(t,"H2",{class:!0});var yr=a(se);qe=s(yr,"A",{id:!0,class:!0,href:!0});var Gl=a(qe);mn=s(Gl,"SPAN",{});var Jl=a(mn);v(yt.$$.fragment,Jl),Jl.forEach(o),Gl.forEach(o),da=f(yr),pn=s(yr,"SPAN",{});var Zl=a(pn);ca=l(Zl,"CLIPSegVisionConfig"),Zl.forEach(o),yr.forEach(o),dr=f(t),z=s(t,"DIV",{class:!0});var nt=a(z);v(Tt.$$.fragment,nt),ga=f(nt),ae=s(nt,"P",{});var Ho=a(ae);ma=l(Ho,"This is the configuration class to store the configuration of a "),Lo=s(Ho,"A",{href:!0});var Kl=a(Lo);pa=l(Kl,"CLIPSegModel"),Kl.forEach(o),fa=l(Ho,`. It is used to instantiate an
CLIPSeg model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the CLIPSeg
`),kt=s(Ho,"A",{href:!0,rel:!0});var Ql=a(kt);ha=l(Ql,"CIDAS/clipseg-rd64"),Ql.forEach(o),ua=l(Ho," architecture."),Ho.forEach(o),_a=f(nt),ie=s(nt,"P",{});var Ro=a(ie);Ca=l(Ro,"Configuration objects inherit from "),wo=s(Ro,"A",{href:!0});var Xl=a(wo);Pa=l(Xl,"PretrainedConfig"),Xl.forEach(o),Ia=l(Ro,` and can be used to control the model outputs. Read the
documentation from `),xo=s(Ro,"A",{href:!0});var Yl=a(xo);va=l(Yl,"PretrainedConfig"),Yl.forEach(o),$a=l(Ro," for more information."),Ro.forEach(o),Sa=f(nt),v(Ae.$$.fragment,nt),nt.forEach(o),cr=f(t),le=s(t,"H2",{class:!0});var Tr=a(le);De=s(Tr,"A",{id:!0,class:!0,href:!0});var ed=a(De);fn=s(ed,"SPAN",{});var td=a(fn);v(Et.$$.fragment,td),td.forEach(o),ed.forEach(o),ba=f(Tr),hn=s(Tr,"SPAN",{});var od=a(hn);La=l(od,"CLIPSegProcessor"),od.forEach(o),Tr.forEach(o),gr=f(t),k=s(t,"DIV",{class:!0});var K=a(k);v(Mt.$$.fragment,K),wa=f(K),un=s(K,"P",{});var nd=a(un);xa=l(nd,`Constructs a CLIPSeg processor which wraps a CLIPSeg feature extractor and a CLIP tokenizer into a single
processor.`),nd.forEach(o),ya=f(K),M=s(K,"P",{});var B=a(M);yo=s(B,"A",{href:!0});var rd=a(yo);Ta=l(rd,"CLIPSegProcessor"),rd.forEach(o),ka=l(B," offers all the functionalities of "),To=s(B,"A",{href:!0});var sd=a(To);Ea=l(sd,"ViTFeatureExtractor"),sd.forEach(o),Ma=l(B," and "),ko=s(B,"A",{href:!0});var ad=a(ko);ja=l(ad,"CLIPTokenizerFast"),ad.forEach(o),za=l(B,`. See the
`),_n=s(B,"CODE",{});var id=a(_n);qa=l(id,"__call__()"),id.forEach(o),Aa=l(B," and "),Eo=s(B,"A",{href:!0});var ld=a(Eo);Da=l(ld,"decode()"),ld.forEach(o),Fa=l(B," for more information."),B.forEach(o),Va=f(K),Fe=s(K,"DIV",{class:!0});var kr=a(Fe);v(jt.$$.fragment,kr),Oa=f(kr),zt=s(kr,"P",{});var Er=a(zt);Na=l(Er,"This method forwards all its arguments to CLIPTokenizerFast\u2019s "),Mo=s(Er,"A",{href:!0});var dd=a(Mo);Wa=l(dd,"batch_decode()"),dd.forEach(o),Ba=l(Er,`. Please
refer to the docstring of this method for more information.`),Er.forEach(o),kr.forEach(o),Ha=f(K),Ve=s(K,"DIV",{class:!0});var Mr=a(Ve);v(qt.$$.fragment,Mr),Ra=f(Mr),At=s(Mr,"P",{});var jr=a(At);Ua=l(jr,"This method forwards all its arguments to CLIPTokenizerFast\u2019s "),jo=s(jr,"A",{href:!0});var cd=a(jo);Ga=l(cd,"decode()"),cd.forEach(o),Ja=l(jr,`. Please refer to
the docstring of this method for more information.`),jr.forEach(o),Mr.forEach(o),K.forEach(o),mr=f(t),de=s(t,"H2",{class:!0});var zr=a(de);Oe=s(zr,"A",{id:!0,class:!0,href:!0});var gd=a(Oe);Cn=s(gd,"SPAN",{});var md=a(Cn);v(Dt.$$.fragment,md),md.forEach(o),gd.forEach(o),Za=f(zr),Pn=s(zr,"SPAN",{});var pd=a(Pn);Ka=l(pd,"CLIPSegModel"),pd.forEach(o),zr.forEach(o),pr=f(t),E=s(t,"DIV",{class:!0});var Q=a(E);v(Ft.$$.fragment,Q),Qa=f(Q),Vt=s(Q,"P",{});var qr=a(Vt);Xa=l(qr,"This model is a PyTorch "),Ot=s(qr,"A",{href:!0,rel:!0});var fd=a(Ot);Ya=l(fd,"torch.nn.Module"),fd.forEach(o),ei=l(qr,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),qr.forEach(o),ti=f(Q),D=s(Q,"DIV",{class:!0});var rt=a(D);v(Nt.$$.fragment,rt),oi=f(rt),ce=s(rt,"P",{});var Uo=a(ce);ni=l(Uo,"The "),zo=s(Uo,"A",{href:!0});var hd=a(zo);ri=l(hd,"CLIPSegModel"),hd.forEach(o),si=l(Uo," forward method, overrides the "),In=s(Uo,"CODE",{});var ud=a(In);ai=l(ud,"__call__"),ud.forEach(o),ii=l(Uo," special method."),Uo.forEach(o),li=f(rt),v(Ne.$$.fragment,rt),di=f(rt),v(We.$$.fragment,rt),rt.forEach(o),ci=f(Q),F=s(Q,"DIV",{class:!0});var st=a(F);v(Wt.$$.fragment,st),gi=f(st),ge=s(st,"P",{});var Go=a(ge);mi=l(Go,"The "),qo=s(Go,"A",{href:!0});var _d=a(qo);pi=l(_d,"CLIPSegModel"),_d.forEach(o),fi=l(Go," forward method, overrides the "),vn=s(Go,"CODE",{});var Cd=a(vn);hi=l(Cd,"__call__"),Cd.forEach(o),ui=l(Go," special method."),Go.forEach(o),_i=f(st),v(Be.$$.fragment,st),Ci=f(st),v(He.$$.fragment,st),st.forEach(o),Pi=f(Q),V=s(Q,"DIV",{class:!0});var at=a(V);v(Bt.$$.fragment,at),Ii=f(at),me=s(at,"P",{});var Jo=a(me);vi=l(Jo,"The "),Ao=s(Jo,"A",{href:!0});var Pd=a(Ao);$i=l(Pd,"CLIPSegModel"),Pd.forEach(o),Si=l(Jo," forward method, overrides the "),$n=s(Jo,"CODE",{});var Id=a($n);bi=l(Id,"__call__"),Id.forEach(o),Li=l(Jo," special method."),Jo.forEach(o),wi=f(at),v(Re.$$.fragment,at),xi=f(at),v(Ue.$$.fragment,at),at.forEach(o),Q.forEach(o),fr=f(t),pe=s(t,"H2",{class:!0});var Ar=a(pe);Ge=s(Ar,"A",{id:!0,class:!0,href:!0});var vd=a(Ge);Sn=s(vd,"SPAN",{});var $d=a(Sn);v(Ht.$$.fragment,$d),$d.forEach(o),vd.forEach(o),yi=f(Ar),bn=s(Ar,"SPAN",{});var Sd=a(bn);Ti=l(Sd,"CLIPSegTextModel"),Sd.forEach(o),Ar.forEach(o),hr=f(t),fe=s(t,"DIV",{class:!0});var Dr=a(fe);v(Rt.$$.fragment,Dr),ki=f(Dr),O=s(Dr,"DIV",{class:!0});var it=a(O);v(Ut.$$.fragment,it),Ei=f(it),he=s(it,"P",{});var Zo=a(he);Mi=l(Zo,"The "),Do=s(Zo,"A",{href:!0});var bd=a(Do);ji=l(bd,"CLIPSegTextModel"),bd.forEach(o),zi=l(Zo," forward method, overrides the "),Ln=s(Zo,"CODE",{});var Ld=a(Ln);qi=l(Ld,"__call__"),Ld.forEach(o),Ai=l(Zo," special method."),Zo.forEach(o),Di=f(it),v(Je.$$.fragment,it),Fi=f(it),v(Ze.$$.fragment,it),it.forEach(o),Dr.forEach(o),ur=f(t),ue=s(t,"H2",{class:!0});var Fr=a(ue);Ke=s(Fr,"A",{id:!0,class:!0,href:!0});var wd=a(Ke);wn=s(wd,"SPAN",{});var xd=a(wn);v(Gt.$$.fragment,xd),xd.forEach(o),wd.forEach(o),Vi=f(Fr),xn=s(Fr,"SPAN",{});var yd=a(xn);Oi=l(yd,"CLIPSegVisionModel"),yd.forEach(o),Fr.forEach(o),_r=f(t),_e=s(t,"DIV",{class:!0});var Vr=a(_e);v(Jt.$$.fragment,Vr),Ni=f(Vr),N=s(Vr,"DIV",{class:!0});var lt=a(N);v(Zt.$$.fragment,lt),Wi=f(lt),Ce=s(lt,"P",{});var Ko=a(Ce);Bi=l(Ko,"The "),Fo=s(Ko,"A",{href:!0});var Td=a(Fo);Hi=l(Td,"CLIPSegVisionModel"),Td.forEach(o),Ri=l(Ko," forward method, overrides the "),yn=s(Ko,"CODE",{});var kd=a(yn);Ui=l(kd,"__call__"),kd.forEach(o),Gi=l(Ko," special method."),Ko.forEach(o),Ji=f(lt),v(Qe.$$.fragment,lt),Zi=f(lt),v(Xe.$$.fragment,lt),lt.forEach(o),Vr.forEach(o),Cr=f(t),Pe=s(t,"H2",{class:!0});var Or=a(Pe);Ye=s(Or,"A",{id:!0,class:!0,href:!0});var Ed=a(Ye);Tn=s(Ed,"SPAN",{});var Md=a(Tn);v(Kt.$$.fragment,Md),Md.forEach(o),Ed.forEach(o),Ki=f(Or),kn=s(Or,"SPAN",{});var jd=a(kn);Qi=l(jd,"CLIPSegForImageSegmentation"),jd.forEach(o),Or.forEach(o),Pr=f(t),q=s(t,"DIV",{class:!0});var dt=a(q);v(Qt.$$.fragment,dt),Xi=f(dt),En=s(dt,"P",{});var zd=a(En);Yi=l(zd,"CLIPSeg model with a Transformer-based decoder on top for zero-shot and one-shot image segmentation."),zd.forEach(o),el=f(dt),Xt=s(dt,"P",{});var Nr=a(Xt);tl=l(Nr,"This model is a PyTorch "),Yt=s(Nr,"A",{href:!0,rel:!0});var qd=a(Yt);ol=l(qd,"torch.nn.Module"),qd.forEach(o),nl=l(Nr,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Nr.forEach(o),rl=f(dt),W=s(dt,"DIV",{class:!0});var ct=a(W);v(eo.$$.fragment,ct),sl=f(ct),Ie=s(ct,"P",{});var Qo=a(Ie);al=l(Qo,"The "),Vo=s(Qo,"A",{href:!0});var Ad=a(Vo);il=l(Ad,"CLIPSegForImageSegmentation"),Ad.forEach(o),ll=l(Qo," forward method, overrides the "),Mn=s(Qo,"CODE",{});var Dd=a(Mn);dl=l(Dd,"__call__"),Dd.forEach(o),cl=l(Qo," special method."),Qo.forEach(o),gl=f(ct),v(et.$$.fragment,ct),ml=f(ct),v(tt.$$.fragment,ct),ct.forEach(o),dt.forEach(o),this.h()},h(){c(d,"name","hf:doc:metadata"),c(d,"content",JSON.stringify(ic)),c(m,"id","clipseg"),c(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(m,"href","#clipseg"),c(u,"class","relative group"),c(we,"id","overview"),c(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(we,"href","#overview"),c(X,"class","relative group"),c(mt,"href","https://arxiv.org/abs/2112.10003"),c(mt,"rel","nofollow"),c(so,"href","clip"),c(co,"href","/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegForImageSegmentation"),c(go,"href","/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel"),c(mo,"href","/docs/transformers/main/en/model_doc/clip#transformers.CLIPModel"),c(po,"href","/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegForImageSegmentation"),Wd(ye.src,fl="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/clipseg_architecture.png")||c(ye,"src",fl),c(ye,"alt","drawing"),c(ye,"width","600"),c(fo,"href","https://arxiv.org/abs/2112.10003"),c(ft,"href","https://huggingface.co/nielsr"),c(ft,"rel","nofollow"),c(ht,"href","https://github.com/timojl/clipseg"),c(ht,"rel","nofollow"),c(Te,"id","resources"),c(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Te,"href","#resources"),c(Y,"class","relative group"),c(Pt,"href","https://github.com/NielsRogge/Transformers-Tutorials/blob/master/CLIPSeg/Zero_shot_image_segmentation_with_CLIPSeg.ipynb"),c(Pt,"rel","nofollow"),c(ke,"id","transformers.CLIPSegConfig"),c(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ke,"href","#transformers.CLIPSegConfig"),c(ee,"class","relative group"),c(_o,"href","/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegConfig"),c(Co,"href","/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel"),c($t,"href","https://huggingface.co/CIDAS/clipseg-rd64"),c($t,"rel","nofollow"),c(Po,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(Io,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(vo,"href","/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegConfig"),c(Me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(je,"id","transformers.CLIPSegTextConfig"),c(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(je,"href","#transformers.CLIPSegTextConfig"),c(oe,"class","relative group"),c($o,"href","/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel"),c(xt,"href","https://huggingface.co/CIDAS/clipseg-rd64"),c(xt,"rel","nofollow"),c(So,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(bo,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(qe,"id","transformers.CLIPSegVisionConfig"),c(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qe,"href","#transformers.CLIPSegVisionConfig"),c(se,"class","relative group"),c(Lo,"href","/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel"),c(kt,"href","https://huggingface.co/CIDAS/clipseg-rd64"),c(kt,"rel","nofollow"),c(wo,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(xo,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(De,"id","transformers.CLIPSegProcessor"),c(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(De,"href","#transformers.CLIPSegProcessor"),c(le,"class","relative group"),c(yo,"href","/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegProcessor"),c(To,"href","/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor"),c(ko,"href","/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizerFast"),c(Eo,"href","/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegProcessor.decode"),c(Mo,"href","/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode"),c(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(jo,"href","/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode"),c(Ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Oe,"id","transformers.CLIPSegModel"),c(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Oe,"href","#transformers.CLIPSegModel"),c(de,"class","relative group"),c(Ot,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ot,"rel","nofollow"),c(zo,"href","/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel"),c(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(qo,"href","/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel"),c(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Ao,"href","/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel"),c(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Ge,"id","transformers.CLIPSegTextModel"),c(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ge,"href","#transformers.CLIPSegTextModel"),c(pe,"class","relative group"),c(Do,"href","/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegTextModel"),c(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Ke,"id","transformers.CLIPSegVisionModel"),c(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ke,"href","#transformers.CLIPSegVisionModel"),c(ue,"class","relative group"),c(Fo,"href","/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegVisionModel"),c(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(_e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Ye,"id","transformers.CLIPSegForImageSegmentation"),c(Ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ye,"href","#transformers.CLIPSegForImageSegmentation"),c(Pe,"class","relative group"),c(Yt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Yt,"rel","nofollow"),c(Vo,"href","/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegForImageSegmentation"),c(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,h){e(document.head,d),_(t,P,h),_(t,u,h),e(u,m),e(m,C),$(n,C,null),e(u,g),e(u,x),e(x,Wr),_(t,Rn,h),_(t,X,h),e(X,we),e(we,Yo),$(gt,Yo,null),e(X,Br),e(X,en),e(en,Hr),_(t,Un,h),_(t,R,h),e(R,Rr),e(R,mt),e(mt,Ur),e(R,Gr),e(R,so),e(so,Jr),e(R,Zr),_(t,Gn,h),_(t,ao,h),e(ao,Kr),_(t,Jn,h),_(t,io,h),e(io,tn),e(tn,Qr),_(t,Zn,h),_(t,lo,h),e(lo,Xr),_(t,Kn,h),_(t,xe,h),e(xe,U),e(U,co),e(co,Yr),e(U,es),e(U,go),e(go,ts),e(U,os),e(U,mo),e(mo,ns),e(U,rs),e(xe,ss),e(xe,A),e(A,po),e(po,as),e(A,is),e(A,on),e(on,ls),e(A,ds),e(A,nn),e(nn,cs),e(A,gs),e(A,rn),e(rn,ms),e(A,ps),_(t,Qn,h),_(t,ye,h),_(t,Xn,h),_(t,pt,h),e(pt,fs),e(pt,fo),e(fo,hs),_(t,Yn,h),_(t,G,h),e(G,us),e(G,ft),e(ft,_s),e(G,Cs),e(G,ht),e(ht,Ps),e(G,Is),_(t,er,h),_(t,Y,h),e(Y,Te),e(Te,sn),$(ut,sn,null),e(Y,vs),e(Y,an),e(an,$s),_(t,tr,h),_(t,ho,h),e(ho,Ss),_(t,or,h),$(_t,t,h),_(t,nr,h),_(t,uo,h),e(uo,Ct),e(Ct,bs),e(Ct,Pt),e(Pt,Ls),e(Ct,ws),_(t,rr,h),_(t,ee,h),e(ee,ke),e(ke,ln),$(It,ln,null),e(ee,xs),e(ee,dn),e(dn,ys),_(t,sr,h),_(t,T,h),$(vt,T,null),e(T,Ts),e(T,J),e(J,_o),e(_o,ks),e(J,Es),e(J,Co),e(Co,Ms),e(J,js),e(J,$t),e($t,zs),e(J,qs),e(T,As),e(T,te),e(te,Ds),e(te,Po),e(Po,Fs),e(te,Vs),e(te,Io),e(Io,Os),e(te,Ns),e(T,Ws),$(Ee,T,null),e(T,Bs),e(T,Me),$(St,Me,null),e(Me,Hs),e(Me,bt),e(bt,Rs),e(bt,vo),e(vo,Us),e(bt,Gs),_(t,ar,h),_(t,oe,h),e(oe,je),e(je,cn),$(Lt,cn,null),e(oe,Js),e(oe,gn),e(gn,Zs),_(t,ir,h),_(t,j,h),$(wt,j,null),e(j,Ks),e(j,ne),e(ne,Qs),e(ne,$o),e($o,Xs),e(ne,Ys),e(ne,xt),e(xt,ea),e(ne,ta),e(j,oa),e(j,re),e(re,na),e(re,So),e(So,ra),e(re,sa),e(re,bo),e(bo,aa),e(re,ia),e(j,la),$(ze,j,null),_(t,lr,h),_(t,se,h),e(se,qe),e(qe,mn),$(yt,mn,null),e(se,da),e(se,pn),e(pn,ca),_(t,dr,h),_(t,z,h),$(Tt,z,null),e(z,ga),e(z,ae),e(ae,ma),e(ae,Lo),e(Lo,pa),e(ae,fa),e(ae,kt),e(kt,ha),e(ae,ua),e(z,_a),e(z,ie),e(ie,Ca),e(ie,wo),e(wo,Pa),e(ie,Ia),e(ie,xo),e(xo,va),e(ie,$a),e(z,Sa),$(Ae,z,null),_(t,cr,h),_(t,le,h),e(le,De),e(De,fn),$(Et,fn,null),e(le,ba),e(le,hn),e(hn,La),_(t,gr,h),_(t,k,h),$(Mt,k,null),e(k,wa),e(k,un),e(un,xa),e(k,ya),e(k,M),e(M,yo),e(yo,Ta),e(M,ka),e(M,To),e(To,Ea),e(M,Ma),e(M,ko),e(ko,ja),e(M,za),e(M,_n),e(_n,qa),e(M,Aa),e(M,Eo),e(Eo,Da),e(M,Fa),e(k,Va),e(k,Fe),$(jt,Fe,null),e(Fe,Oa),e(Fe,zt),e(zt,Na),e(zt,Mo),e(Mo,Wa),e(zt,Ba),e(k,Ha),e(k,Ve),$(qt,Ve,null),e(Ve,Ra),e(Ve,At),e(At,Ua),e(At,jo),e(jo,Ga),e(At,Ja),_(t,mr,h),_(t,de,h),e(de,Oe),e(Oe,Cn),$(Dt,Cn,null),e(de,Za),e(de,Pn),e(Pn,Ka),_(t,pr,h),_(t,E,h),$(Ft,E,null),e(E,Qa),e(E,Vt),e(Vt,Xa),e(Vt,Ot),e(Ot,Ya),e(Vt,ei),e(E,ti),e(E,D),$(Nt,D,null),e(D,oi),e(D,ce),e(ce,ni),e(ce,zo),e(zo,ri),e(ce,si),e(ce,In),e(In,ai),e(ce,ii),e(D,li),$(Ne,D,null),e(D,di),$(We,D,null),e(E,ci),e(E,F),$(Wt,F,null),e(F,gi),e(F,ge),e(ge,mi),e(ge,qo),e(qo,pi),e(ge,fi),e(ge,vn),e(vn,hi),e(ge,ui),e(F,_i),$(Be,F,null),e(F,Ci),$(He,F,null),e(E,Pi),e(E,V),$(Bt,V,null),e(V,Ii),e(V,me),e(me,vi),e(me,Ao),e(Ao,$i),e(me,Si),e(me,$n),e($n,bi),e(me,Li),e(V,wi),$(Re,V,null),e(V,xi),$(Ue,V,null),_(t,fr,h),_(t,pe,h),e(pe,Ge),e(Ge,Sn),$(Ht,Sn,null),e(pe,yi),e(pe,bn),e(bn,Ti),_(t,hr,h),_(t,fe,h),$(Rt,fe,null),e(fe,ki),e(fe,O),$(Ut,O,null),e(O,Ei),e(O,he),e(he,Mi),e(he,Do),e(Do,ji),e(he,zi),e(he,Ln),e(Ln,qi),e(he,Ai),e(O,Di),$(Je,O,null),e(O,Fi),$(Ze,O,null),_(t,ur,h),_(t,ue,h),e(ue,Ke),e(Ke,wn),$(Gt,wn,null),e(ue,Vi),e(ue,xn),e(xn,Oi),_(t,_r,h),_(t,_e,h),$(Jt,_e,null),e(_e,Ni),e(_e,N),$(Zt,N,null),e(N,Wi),e(N,Ce),e(Ce,Bi),e(Ce,Fo),e(Fo,Hi),e(Ce,Ri),e(Ce,yn),e(yn,Ui),e(Ce,Gi),e(N,Ji),$(Qe,N,null),e(N,Zi),$(Xe,N,null),_(t,Cr,h),_(t,Pe,h),e(Pe,Ye),e(Ye,Tn),$(Kt,Tn,null),e(Pe,Ki),e(Pe,kn),e(kn,Qi),_(t,Pr,h),_(t,q,h),$(Qt,q,null),e(q,Xi),e(q,En),e(En,Yi),e(q,el),e(q,Xt),e(Xt,tl),e(Xt,Yt),e(Yt,ol),e(Xt,nl),e(q,rl),e(q,W),$(eo,W,null),e(W,sl),e(W,Ie),e(Ie,al),e(Ie,Vo),e(Vo,il),e(Ie,ll),e(Ie,Mn),e(Mn,dl),e(Ie,cl),e(W,gl),$(et,W,null),e(W,ml),$(tt,W,null),Ir=!0},p(t,[h]){const to={};h&2&&(to.$$scope={dirty:h,ctx:t}),Ee.$set(to);const jn={};h&2&&(jn.$$scope={dirty:h,ctx:t}),ze.$set(jn);const zn={};h&2&&(zn.$$scope={dirty:h,ctx:t}),Ae.$set(zn);const qn={};h&2&&(qn.$$scope={dirty:h,ctx:t}),Ne.$set(qn);const oo={};h&2&&(oo.$$scope={dirty:h,ctx:t}),We.$set(oo);const An={};h&2&&(An.$$scope={dirty:h,ctx:t}),Be.$set(An);const Dn={};h&2&&(Dn.$$scope={dirty:h,ctx:t}),He.$set(Dn);const Fn={};h&2&&(Fn.$$scope={dirty:h,ctx:t}),Re.$set(Fn);const ve={};h&2&&(ve.$$scope={dirty:h,ctx:t}),Ue.$set(ve);const Vn={};h&2&&(Vn.$$scope={dirty:h,ctx:t}),Je.$set(Vn);const On={};h&2&&(On.$$scope={dirty:h,ctx:t}),Ze.$set(On);const Nn={};h&2&&(Nn.$$scope={dirty:h,ctx:t}),Qe.$set(Nn);const Wn={};h&2&&(Wn.$$scope={dirty:h,ctx:t}),Xe.$set(Wn);const Bn={};h&2&&(Bn.$$scope={dirty:h,ctx:t}),et.$set(Bn);const Hn={};h&2&&(Hn.$$scope={dirty:h,ctx:t}),tt.$set(Hn)},i(t){Ir||(S(n.$$.fragment,t),S(gt.$$.fragment,t),S(ut.$$.fragment,t),S(_t.$$.fragment,t),S(It.$$.fragment,t),S(vt.$$.fragment,t),S(Ee.$$.fragment,t),S(St.$$.fragment,t),S(Lt.$$.fragment,t),S(wt.$$.fragment,t),S(ze.$$.fragment,t),S(yt.$$.fragment,t),S(Tt.$$.fragment,t),S(Ae.$$.fragment,t),S(Et.$$.fragment,t),S(Mt.$$.fragment,t),S(jt.$$.fragment,t),S(qt.$$.fragment,t),S(Dt.$$.fragment,t),S(Ft.$$.fragment,t),S(Nt.$$.fragment,t),S(Ne.$$.fragment,t),S(We.$$.fragment,t),S(Wt.$$.fragment,t),S(Be.$$.fragment,t),S(He.$$.fragment,t),S(Bt.$$.fragment,t),S(Re.$$.fragment,t),S(Ue.$$.fragment,t),S(Ht.$$.fragment,t),S(Rt.$$.fragment,t),S(Ut.$$.fragment,t),S(Je.$$.fragment,t),S(Ze.$$.fragment,t),S(Gt.$$.fragment,t),S(Jt.$$.fragment,t),S(Zt.$$.fragment,t),S(Qe.$$.fragment,t),S(Xe.$$.fragment,t),S(Kt.$$.fragment,t),S(Qt.$$.fragment,t),S(eo.$$.fragment,t),S(et.$$.fragment,t),S(tt.$$.fragment,t),Ir=!0)},o(t){b(n.$$.fragment,t),b(gt.$$.fragment,t),b(ut.$$.fragment,t),b(_t.$$.fragment,t),b(It.$$.fragment,t),b(vt.$$.fragment,t),b(Ee.$$.fragment,t),b(St.$$.fragment,t),b(Lt.$$.fragment,t),b(wt.$$.fragment,t),b(ze.$$.fragment,t),b(yt.$$.fragment,t),b(Tt.$$.fragment,t),b(Ae.$$.fragment,t),b(Et.$$.fragment,t),b(Mt.$$.fragment,t),b(jt.$$.fragment,t),b(qt.$$.fragment,t),b(Dt.$$.fragment,t),b(Ft.$$.fragment,t),b(Nt.$$.fragment,t),b(Ne.$$.fragment,t),b(We.$$.fragment,t),b(Wt.$$.fragment,t),b(Be.$$.fragment,t),b(He.$$.fragment,t),b(Bt.$$.fragment,t),b(Re.$$.fragment,t),b(Ue.$$.fragment,t),b(Ht.$$.fragment,t),b(Rt.$$.fragment,t),b(Ut.$$.fragment,t),b(Je.$$.fragment,t),b(Ze.$$.fragment,t),b(Gt.$$.fragment,t),b(Jt.$$.fragment,t),b(Zt.$$.fragment,t),b(Qe.$$.fragment,t),b(Xe.$$.fragment,t),b(Kt.$$.fragment,t),b(Qt.$$.fragment,t),b(eo.$$.fragment,t),b(et.$$.fragment,t),b(tt.$$.fragment,t),Ir=!1},d(t){o(d),t&&o(P),t&&o(u),L(n),t&&o(Rn),t&&o(X),L(gt),t&&o(Un),t&&o(R),t&&o(Gn),t&&o(ao),t&&o(Jn),t&&o(io),t&&o(Zn),t&&o(lo),t&&o(Kn),t&&o(xe),t&&o(Qn),t&&o(ye),t&&o(Xn),t&&o(pt),t&&o(Yn),t&&o(G),t&&o(er),t&&o(Y),L(ut),t&&o(tr),t&&o(ho),t&&o(or),L(_t,t),t&&o(nr),t&&o(uo),t&&o(rr),t&&o(ee),L(It),t&&o(sr),t&&o(T),L(vt),L(Ee),L(St),t&&o(ar),t&&o(oe),L(Lt),t&&o(ir),t&&o(j),L(wt),L(ze),t&&o(lr),t&&o(se),L(yt),t&&o(dr),t&&o(z),L(Tt),L(Ae),t&&o(cr),t&&o(le),L(Et),t&&o(gr),t&&o(k),L(Mt),L(jt),L(qt),t&&o(mr),t&&o(de),L(Dt),t&&o(pr),t&&o(E),L(Ft),L(Nt),L(Ne),L(We),L(Wt),L(Be),L(He),L(Bt),L(Re),L(Ue),t&&o(fr),t&&o(pe),L(Ht),t&&o(hr),t&&o(fe),L(Rt),L(Ut),L(Je),L(Ze),t&&o(ur),t&&o(ue),L(Gt),t&&o(_r),t&&o(_e),L(Jt),L(Zt),L(Qe),L(Xe),t&&o(Cr),t&&o(Pe),L(Kt),t&&o(Pr),t&&o(q),L(Qt),L(eo),L(et),L(tt)}}}const ic={local:"clipseg",sections:[{local:"overview",title:"Overview"},{local:"resources",title:"Resources"},{local:"transformers.CLIPSegConfig",title:"CLIPSegConfig"},{local:"transformers.CLIPSegTextConfig",title:"CLIPSegTextConfig"},{local:"transformers.CLIPSegVisionConfig",title:"CLIPSegVisionConfig"},{local:"transformers.CLIPSegProcessor",title:"CLIPSegProcessor"},{local:"transformers.CLIPSegModel",title:"CLIPSegModel"},{local:"transformers.CLIPSegTextModel",title:"CLIPSegTextModel"},{local:"transformers.CLIPSegVisionModel",title:"CLIPSegVisionModel"},{local:"transformers.CLIPSegForImageSegmentation",title:"CLIPSegForImageSegmentation"}],title:"CLIPSeg"};function lc(w){return Bd(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class uc extends Fd{constructor(d){super();Vd(this,d,lc,ac,Od,{})}}export{uc as default,ic as metadata};
