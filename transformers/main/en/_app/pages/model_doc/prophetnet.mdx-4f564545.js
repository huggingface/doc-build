import{S as Zl,i as eh,s as th,e as n,k as c,w as m,t as s,M as oh,c as r,d as o,m as l,a as d,x as f,h as a,b as i,F as e,g as p,y as _,q as g,o as v,B as k,v as nh}from"../../chunks/vendor-6b77c823.js";import{T as Rn}from"../../chunks/Tip-39098574.js";import{D as z}from"../../chunks/Docstring-af1d0ae0.js";import{C as Bo}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as re}from"../../chunks/IconCopyLink-7a11ce68.js";function rh(B){let u,N,T,w,P;return{c(){u=n("p"),N=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),T=n("code"),w=s("Module"),P=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(y){u=r(y,"P",{});var b=d(u);N=a(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),T=r(b,"CODE",{});var q=d(T);w=a(q,"Module"),q.forEach(o),P=a(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(y,b){p(y,u,b),e(u,N),e(u,T),e(T,w),e(u,P)},d(y){y&&o(u)}}}function sh(B){let u,N,T,w,P;return{c(){u=n("p"),N=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),T=n("code"),w=s("Module"),P=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(y){u=r(y,"P",{});var b=d(u);N=a(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),T=r(b,"CODE",{});var q=d(T);w=a(q,"Module"),q.forEach(o),P=a(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(y,b){p(y,u,b),e(u,N),e(u,T),e(T,w),e(u,P)},d(y){y&&o(u)}}}function ah(B){let u,N,T,w,P;return{c(){u=n("p"),N=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),T=n("code"),w=s("Module"),P=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(y){u=r(y,"P",{});var b=d(u);N=a(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),T=r(b,"CODE",{});var q=d(T);w=a(q,"Module"),q.forEach(o),P=a(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(y,b){p(y,u,b),e(u,N),e(u,T),e(T,w),e(u,P)},d(y){y&&o(u)}}}function dh(B){let u,N,T,w,P;return{c(){u=n("p"),N=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),T=n("code"),w=s("Module"),P=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(y){u=r(y,"P",{});var b=d(u);N=a(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),T=r(b,"CODE",{});var q=d(T);w=a(q,"Module"),q.forEach(o),P=a(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(y,b){p(y,u,b),e(u,N),e(u,T),e(T,w),e(u,P)},d(y){y&&o(u)}}}function ih(B){let u,N,T,w,P;return{c(){u=n("p"),N=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),T=n("code"),w=s("Module"),P=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(y){u=r(y,"P",{});var b=d(u);N=a(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),T=r(b,"CODE",{});var q=d(T);w=a(q,"Module"),q.forEach(o),P=a(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(y,b){p(y,u,b),e(u,N),e(u,T),e(T,w),e(u,P)},d(y){y&&o(u)}}}function ch(B){let u,N,T,w,P,y,b,q,Xr,Jn,se,Go,Zr,es,Qe,ts,os,Yn,ae,Ee,Ho,Xe,ns,Vo,rs,Kn,xe,ss,Ze,as,ds,Qn,ao,is,Xn,io,cs,Zn,co,Uo,ls,er,Ce,hs,et,ps,us,tr,de,Oe,Ro,tt,ms,Jo,fs,or,U,ot,_s,nt,gs,lo,vs,ks,Ts,ie,bs,ho,ys,ws,po,Ps,Ns,nr,ce,Se,Yo,rt,qs,Ko,zs,rr,$,st,$s,Qo,Fs,Ms,at,Es,uo,xs,Cs,Os,R,dt,Ss,Xo,Ds,Ls,it,mo,js,Zo,As,Is,fo,Ws,en,Bs,Gs,De,ct,Hs,tn,Vs,Us,G,lt,Rs,on,Js,Ys,ht,Ks,le,Qs,nn,Xs,Zs,rn,ea,ta,oa,Le,pt,na,ut,ra,sn,sa,aa,sr,he,je,an,mt,da,dn,ia,ar,pe,ft,ca,cn,la,dr,ue,_t,ha,ln,pa,ir,me,gt,ua,hn,ma,cr,fe,vt,fa,pn,_a,lr,_e,Ae,un,kt,ga,mn,va,hr,E,Tt,ka,bt,Ta,_o,ba,ya,wa,ge,Pa,yt,Na,qa,fn,za,$a,Fa,wt,Ma,Pt,Ea,xa,Ca,O,Nt,Oa,ve,Sa,go,Da,La,_n,ja,Aa,Ia,Ie,Wa,gn,Ba,Ga,qt,pr,ke,We,vn,zt,Ha,kn,Va,ur,F,$t,Ua,Ft,Ra,vo,Ja,Ya,Ka,Te,Qa,Mt,Xa,Za,Tn,ed,td,od,Et,nd,xt,rd,sd,ad,I,dd,bn,id,cd,yn,ld,hd,wn,pd,ud,ko,md,fd,_d,S,Ct,gd,be,vd,To,kd,Td,Pn,bd,yd,wd,Be,Pd,Nn,Nd,qd,Ot,mr,ye,Ge,qn,St,zd,zn,$d,fr,M,Dt,Fd,Lt,Md,bo,Ed,xd,Cd,we,Od,jt,Sd,Dd,$n,Ld,jd,Ad,At,Id,It,Wd,Bd,Gd,W,Hd,Fn,Vd,Ud,Mn,Rd,Jd,En,Yd,Kd,yo,Qd,Xd,Zd,D,Wt,ei,Pe,ti,wo,oi,ni,xn,ri,si,ai,He,di,Cn,ii,ci,Bt,_r,Ne,Ve,On,Gt,li,Sn,hi,gr,x,Ht,pi,Vt,ui,Po,mi,fi,_i,qe,gi,Ut,vi,ki,Dn,Ti,bi,yi,Rt,wi,Jt,Pi,Ni,qi,L,Yt,zi,ze,$i,No,Fi,Mi,Ln,Ei,xi,Ci,Ue,Oi,jn,Si,Di,Kt,vr,$e,Re,An,Qt,Li,In,ji,kr,C,Xt,Ai,Zt,Ii,qo,Wi,Bi,Gi,Fe,Hi,eo,Vi,Ui,Wn,Ri,Ji,Yi,to,Ki,oo,Qi,Xi,Zi,j,no,ec,Me,tc,zo,oc,nc,Bn,rc,sc,ac,Je,dc,Gn,ic,cc,ro,Tr;return y=new re({}),Xe=new re({}),tt=new re({}),ot=new z({props:{name:"class transformers.ProphetNetConfig",anchor:"transformers.ProphetNetConfig",parameters:[{name:"activation_dropout",val:" = 0.1"},{name:"activation_function",val:" = 'gelu'"},{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 1024"},{name:"encoder_ffn_dim",val:" = 4096"},{name:"num_encoder_layers",val:" = 12"},{name:"num_encoder_attention_heads",val:" = 16"},{name:"decoder_ffn_dim",val:" = 4096"},{name:"num_decoder_layers",val:" = 12"},{name:"num_decoder_attention_heads",val:" = 16"},{name:"attention_dropout",val:" = 0.1"},{name:"dropout",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"init_std",val:" = 0.02"},{name:"is_encoder_decoder",val:" = True"},{name:"add_cross_attention",val:" = True"},{name:"decoder_start_token_id",val:" = 0"},{name:"ngram",val:" = 2"},{name:"num_buckets",val:" = 32"},{name:"relative_max_distance",val:" = 128"},{name:"disable_ngram_loss",val:" = False"},{name:"eps",val:" = 0.0"},{name:"use_cache",val:" = True"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 1"},{name:"eos_token_id",val:" = 2"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/prophetnet/configuration_prophetnet.py#L29",parametersDescription:[{anchor:"transformers.ProphetNetConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for activations inside the fully connected layer.`,name:"activation_dropout"},{anchor:"transformers.ProphetNetConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.ProphetNetConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the ProphetNET model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a>.`,name:"vocab_size"},{anchor:"transformers.ProphetNetConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of the layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.ProphetNetConfig.encoder_ffn_dim",description:`<strong>encoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"encoder_ffn_dim"},{anchor:"transformers.ProphetNetConfig.num_encoder_layers",description:`<strong>num_encoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of encoder layers.`,name:"num_encoder_layers"},{anchor:"transformers.ProphetNetConfig.num_encoder_attention_heads",description:`<strong>num_encoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_encoder_attention_heads"},{anchor:"transformers.ProphetNetConfig.decoder_ffn_dim",description:`<strong>decoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the <code>intermediate</code> (often named feed-forward) layer in decoder.`,name:"decoder_ffn_dim"},{anchor:"transformers.ProphetNetConfig.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of decoder layers.`,name:"num_decoder_layers"},{anchor:"transformers.ProphetNetConfig.num_decoder_attention_heads",description:`<strong>num_decoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer decoder.`,name:"num_decoder_attention_heads"},{anchor:"transformers.ProphetNetConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.ProphetNetConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.ProphetNetConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.ProphetNetConfig.init_std",description:`<strong>init_std</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"init_std"},{anchor:"transformers.ProphetNetConfig.add_cross_attention",description:`<strong>add_cross_attention</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether cross-attention layers should be added to the model.`,name:"add_cross_attention"},{anchor:"transformers.ProphetNetConfig.is_encoder_decoder",description:`<strong>is_encoder_decoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether this is an encoder/decoder model.`,name:"is_encoder_decoder"},{anchor:"transformers.ProphetNetConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.ProphetNetConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Beginning of stream token id.`,name:"bos_token_id"},{anchor:"transformers.ProphetNetConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
End of stream token id.`,name:"eos_token_id"},{anchor:"transformers.ProphetNetConfig.ngram",description:`<strong>ngram</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of future tokens to predict. Set to 1 to be same as traditional Language model to predict next first
token.`,name:"ngram"},{anchor:"transformers.ProphetNetConfig.num_buckets",description:`<strong>num_buckets</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of buckets to use for each attention layer. This is for relative position calculation. See the
[T5 paper](see <a href="https://arxiv.org/abs/1910.10683" rel="nofollow">https://arxiv.org/abs/1910.10683</a>) for more details.`,name:"num_buckets"},{anchor:"transformers.ProphetNetConfig.relative_max_distance",description:`<strong>relative_max_distance</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Relative distances greater than this number will be put into the last same bucket. This is for relative
position calculation. See the [T5 paper](see <a href="https://arxiv.org/abs/1910.10683" rel="nofollow">https://arxiv.org/abs/1910.10683</a>) for more details.`,name:"relative_max_distance"},{anchor:"transformers.ProphetNetConfig.disable_ngram_loss",description:`<strong>disable_ngram_loss</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether be trained predicting only the next first token.`,name:"disable_ngram_loss"},{anchor:"transformers.ProphetNetConfig.eps",description:`<strong>eps</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Controls the <code>epsilon</code> parameter value for label smoothing in the loss calculation. If set to 0, no label
smoothing is performed.`,name:"eps"},{anchor:"transformers.ProphetNetConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}]}}),rt=new re({}),st=new z({props:{name:"class transformers.ProphetNetTokenizer",anchor:"transformers.ProphetNetTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"x_sep_token",val:" = '[X_SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/prophetnet/tokenization_prophetnet.py#L55",parametersDescription:[{anchor:"transformers.ProphetNetTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.ProphetNetTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.ProphetNetTokenizer.do_basic_tokenize",description:`<strong>do_basic_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to do basic tokenization before WordPiece.`,name:"do_basic_tokenize"},{anchor:"transformers.ProphetNetTokenizer.never_split",description:`<strong>never_split</strong> (<code>Iterable</code>, <em>optional</em>) &#x2014;
Collection of tokens which will never be split during tokenization. Only has an effect when
<code>do_basic_tokenize=True</code>`,name:"never_split"},{anchor:"transformers.ProphetNetTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.ProphetNetTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.ProphetNetTokenizer.x_sep_token",description:`<strong>x_sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[X_SEP]&quot;</code>) &#x2014;
Special second separator token, which can be generated by <a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a>. It is
used to separate bullet-point like sentences in summarization, <em>e.g.</em>.`,name:"x_sep_token"},{anchor:"transformers.ProphetNetTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.ProphetNetTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.ProphetNetTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.ProphetNetTokenizer.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters.</p>
<p>This should likely be deactivated for Japanese (see this
<a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">issue</a>).
strip_accents &#x2014; (<code>bool</code>, <em>optional</em>):
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"tokenize_chinese_chars"}]}}),dt=new z({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.ProphetNetTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/prophetnet/tokenization_prophetnet.py#L266",parametersDescription:[{anchor:"transformers.ProphetNetTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.ProphetNetTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ct=new z({props:{name:"convert_tokens_to_string",anchor:"transformers.ProphetNetTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/prophetnet/tokenization_prophetnet.py#L186"}}),lt=new z({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.ProphetNetTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/prophetnet/tokenization_prophetnet.py#L218",parametersDescription:[{anchor:"transformers.ProphetNetTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.ProphetNetTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ht=new Bo({props:{code:`0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),pt=new z({props:{name:"get_special_tokens_mask",anchor:"transformers.ProphetNetTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/prophetnet/tokenization_prophetnet.py#L191",parametersDescription:[{anchor:"transformers.ProphetNetTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.ProphetNetTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.ProphetNetTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),mt=new re({}),ft=new z({props:{name:"class transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput",anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"logits_ngram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_ngram_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_ngram_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/prophetnet/modeling_prophetnet.py#L252",parametersDescription:[{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss.`,name:"loss"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the main stream language modeling head (scores for each vocabulary token before
SoftMax).`,name:"logits"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.logits_ngram",description:`<strong>logits_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the predict stream language modeling head (scores for each vocabulary token before
SoftMax).`,name:"logits_ngram"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_ngram_hidden_states",description:`<strong>decoder_ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.`,name:"decoder_ngram_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_ngram_attentions",description:`<strong>decoder_ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the self-attention heads.`,name:"decoder_ngram_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the`,name:"cross_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)</code>. Attentions weights of the encoder, after the attention
softmax, used to compute the weighted average in the self-attention heads.`,name:"encoder_attentions"}]}}),_t=new z({props:{name:"class transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput",anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor"},{name:"last_hidden_state_ngram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_ngram_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_ngram_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/prophetnet/modeling_prophetnet.py#L336",parametersDescription:[{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>) &#x2014;
Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.last_hidden_state_ngram",description:`<strong>last_hidden_state_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,ngram * decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.`,name:"last_hidden_state_ngram"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_ngram_hidden_states",description:`<strong>decoder_ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.`,name:"decoder_ngram_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_ngram_attentions",description:`<strong>decoder_ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the`,name:"decoder_ngram_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the`,name:"cross_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}]}}),gt=new z({props:{name:"class transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput",anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor"},{name:"last_hidden_state_ngram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"hidden_states_ngram",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"ngram_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/prophetnet/modeling_prophetnet.py#L421",parametersDescription:[{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>) &#x2014;
Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.last_hidden_state_ngram",description:`<strong>last_hidden_state_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.`,name:"last_hidden_state_ngram"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.ngram_hidden_states",description:`<strong>ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.`,name:"ngram_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.ngram_attentions",description:`<strong>ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the`,name:"ngram_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the`,name:"cross_attentions"}]}}),vt=new z({props:{name:"class transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput",anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"logits_ngram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"hidden_states_ngram",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"ngram_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/prophetnet/modeling_prophetnet.py#L481",parametersDescription:[{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss.`,name:"loss"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the main stream language modeling head (scores for each vocabulary token before
SoftMax).`,name:"logits"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.logits_ngram",description:`<strong>logits_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the predict stream language modeling head (scores for each vocabulary token before
SoftMax).`,name:"logits_ngram"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.ngram_hidden_states",description:`<strong>ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.`,name:"ngram_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.ngram_attentions",description:`<strong>ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the`,name:"ngram_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the`,name:"cross_attentions"}]}}),kt=new re({}),Tt=new z({props:{name:"class transformers.ProphetNetModel",anchor:"transformers.ProphetNetModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/prophetnet/modeling_prophetnet.py#L1751",parametersDescription:[{anchor:"transformers.ProphetNetModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Nt=new z({props:{name:"forward",anchor:"transformers.ProphetNetModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_attention_mask",val:": typing.Optional[torch.BoolTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"cross_attn_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"encoder_outputs",val:": typing.Optional[typing.Tuple] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.Tensor]]] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/prophetnet/modeling_prophetnet.py#L1783",parametersDescription:[{anchor:"transformers.ProphetNetModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetModel.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>ProphetNet uses the <code>eos_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If
<code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).`,name:"decoder_input_ids"},{anchor:"transformers.ProphetNetModel.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.ProphetNetModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetModel.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.ProphetNetModel.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.ProphetNetModel.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) is a sequence of
hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.ProphetNetModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.ProphetNetModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.ProphetNetModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>) \u2014 Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>last_hidden_state_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,ngram * decoder_sequence_length, config.vocab_size)</code>) \u2014 Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>decoder_ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ie=new Rn({props:{$$slots:{default:[rh]},$$scope:{ctx:B}}}),qt=new Bo({props:{code:`from transformers import ProphetNetTokenizer, ProphetNetModel

tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = ProphetNetModel.from_pretrained("microsoft/prophetnet-large-uncased")

input_ids = tokenizer(
    "Studies have been shown that owning a dog is good for you", return_tensors="pt"
).input_ids  # Batch size 1
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

last_hidden_states = outputs.last_hidden_state  # main stream hidden states
last_hidden_states_ngram = outputs.last_hidden_state_ngram  # predict hidden states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetModel.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Studies have been shown that owning a dog is good for you&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(<span class="hljs-string">&quot;Studies show that&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state  <span class="hljs-comment"># main stream hidden states</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states_ngram = outputs.last_hidden_state_ngram  <span class="hljs-comment"># predict hidden states</span>`}}),zt=new re({}),$t=new z({props:{name:"class transformers.ProphetNetEncoder",anchor:"transformers.ProphetNetEncoder",parameters:[{name:"config",val:": ProphetNetConfig"},{name:"word_embeddings",val:": Embedding = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/prophetnet/modeling_prophetnet.py#L1244",parametersDescription:[{anchor:"transformers.ProphetNetEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ct=new z({props:{name:"forward",anchor:"transformers.ProphetNetEncoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/prophetnet/modeling_prophetnet.py#L1274",parametersDescription:[{anchor:"transformers.ProphetNetEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetEncoder.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetEncoder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetEncoder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetEncoder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetEncoder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Be=new Rn({props:{$$slots:{default:[sh]},$$scope:{ctx:B}}}),Ot=new Bo({props:{code:`from transformers import ProphetNetTokenizer, ProphetNetEncoder
import torch

tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = ProphetNetEncoder.from_pretrained("patrickvonplaten/prophetnet-large-uncased-standalone")
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetEncoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetEncoder.from_pretrained(<span class="hljs-string">&quot;patrickvonplaten/prophetnet-large-uncased-standalone&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),St=new re({}),Dt=new z({props:{name:"class transformers.ProphetNetDecoder",anchor:"transformers.ProphetNetDecoder",parameters:[{name:"config",val:": ProphetNetConfig"},{name:"word_embeddings",val:": Embedding = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/prophetnet/modeling_prophetnet.py#L1384",parametersDescription:[{anchor:"transformers.ProphetNetDecoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Wt=new z({props:{name:"forward",anchor:"transformers.ProphetNetDecoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[torch.Tensor] = None"},{name:"encoder_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"cross_attn_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.Tensor]]] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/prophetnet/modeling_prophetnet.py#L1421",parametersDescription:[{anchor:"transformers.ProphetNetDecoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetDecoder.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetDecoder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetDecoder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetDecoder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetDecoder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ProphetNetDecoder.forward.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong>  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
the model is configured as a decoder.`,name:"encoder_hidden_states"},{anchor:"transformers.ProphetNetDecoder.forward.encoder_attention_mask",description:`<strong>encoder_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:`,name:"encoder_attention_mask"},{anchor:"transformers.ProphetNetDecoder.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.ProphetNetDecoder.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.ProphetNetDecoder.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"use_cache"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>) \u2014 Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>last_hidden_state_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) \u2014 Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),He=new Rn({props:{$$slots:{default:[ah]},$$scope:{ctx:B}}}),Bt=new Bo({props:{code:`from transformers import ProphetNetTokenizer, ProphetNetDecoder
import torch

tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = ProphetNetDecoder.from_pretrained("microsoft/prophetnet-large-uncased", add_cross_attention=False)
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetDecoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetDecoder.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>, add_cross_attention=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Gt=new re({}),Ht=new z({props:{name:"class transformers.ProphetNetForConditionalGeneration",anchor:"transformers.ProphetNetForConditionalGeneration",parameters:[{name:"config",val:": ProphetNetConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/prophetnet/modeling_prophetnet.py#L1878",parametersDescription:[{anchor:"transformers.ProphetNetForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Yt=new z({props:{name:"forward",anchor:"transformers.ProphetNetForConditionalGeneration.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_attention_mask",val:": typing.Optional[torch.BoolTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"cross_attn_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"encoder_outputs",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.Tensor]]] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"decoder_inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/prophetnet/modeling_prophetnet.py#L1899",parametersDescription:[{anchor:"transformers.ProphetNetForConditionalGeneration.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>ProphetNet uses the <code>eos_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If
<code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).`,name:"decoder_input_ids"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) is a sequence of
hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[-100, 0, ..., config.vocab_size - 1]</code>. All labels set to <code>-100</code> are ignored (masked), the loss is only computed for
labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the main stream language modeling head (scores for each vocabulary token before
SoftMax).</p>
</li>
<li>
<p><strong>logits_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the predict stream language modeling head (scores for each vocabulary token before
SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>decoder_ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)</code>. Attentions weights of the encoder, after the attention
softmax, used to compute the weighted average in the self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ue=new Rn({props:{$$slots:{default:[dh]},$$scope:{ctx:B}}}),Kt=new Bo({props:{code:`from transformers import ProphetNetTokenizer, ProphetNetForConditionalGeneration

tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = ProphetNetForConditionalGeneration.from_pretrained("microsoft/prophetnet-large-uncased")

input_ids = tokenizer(
    "Studies have been shown that owning a dog is good for you", return_tensors="pt"
).input_ids  # Batch size 1
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

logits_next_token = outputs.logits  # logits to predict next token as usual
logits_ngram_next_tokens = outputs.logits_ngram  # logits to predict 2nd, 3rd, ... next tokens`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Studies have been shown that owning a dog is good for you&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(<span class="hljs-string">&quot;Studies show that&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits_next_token = outputs.logits  <span class="hljs-comment"># logits to predict next token as usual</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_ngram_next_tokens = outputs.logits_ngram  <span class="hljs-comment"># logits to predict 2nd, 3rd, ... next tokens</span>`}}),Qt=new re({}),Xt=new z({props:{name:"class transformers.ProphetNetForCausalLM",anchor:"transformers.ProphetNetForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/prophetnet/modeling_prophetnet.py#L2087",parametersDescription:[{anchor:"transformers.ProphetNetForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),no=new z({props:{name:"forward",anchor:"transformers.ProphetNetForCausalLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[torch.Tensor] = None"},{name:"encoder_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"cross_attn_head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[typing.Tuple[torch.Tensor]]] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/prophetnet/modeling_prophetnet.py#L2122",parametersDescription:[{anchor:"transformers.ProphetNetForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetForCausalLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ProphetNetForCausalLM.forward.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
the model is configured as a decoder.`,name:"encoder_hidden_states"},{anchor:"transformers.ProphetNetForCausalLM.forward.encoder_attention_mask",description:`<strong>encoder_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:`,name:"encoder_attention_mask"},{anchor:"transformers.ProphetNetForCausalLM.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.ProphetNetForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.ProphetNetForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"use_cache"},{anchor:"transformers.ProphetNetForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in
<code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are
ignored (masked), the loss is only computed for the tokens with labels n <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the main stream language modeling head (scores for each vocabulary token before
SoftMax).</p>
</li>
<li>
<p><strong>logits_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the predict stream language modeling head (scores for each vocabulary token before
SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Je=new Rn({props:{$$slots:{default:[ih]},$$scope:{ctx:B}}}),ro=new Bo({props:{code:`from transformers import ProphetNetTokenizer, ProphetNetForCausalLM
import torch

tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = ProphetNetForCausalLM.from_pretrained("microsoft/prophetnet-large-uncased")
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

logits = outputs.logits

# Model can also be used with EncoderDecoder framework
from transformers import BertTokenizer, EncoderDecoderModel, ProphetNetTokenizer
import torch

tokenizer_enc = BertTokenizer.from_pretrained("bert-large-uncased")
tokenizer_dec = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = EncoderDecoderModel.from_encoder_decoder_pretrained(
    "bert-large-uncased", "microsoft/prophetnet-large-uncased"
)

ARTICLE = (
    "the us state department said wednesday it had received no "
    "formal word from bolivia that it was expelling the us ambassador there "
    "but said the charges made against him are \`\` baseless ."
)
input_ids = tokenizer_enc(ARTICLE, return_tensors="pt").input_ids
labels = tokenizer_dec(
    "us rejects charges against its ambassador in bolivia", return_tensors="pt"
).input_ids
outputs = model(input_ids=input_ids, decoder_input_ids=labels[:, :-1], labels=labels[:, 1:])

loss = outputs.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetForCausalLM.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> model.config.is_decoder, <span class="hljs-string">f&quot;<span class="hljs-subst">{model.__class__}</span> has to be configured as a decoder.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Model can also be used with EncoderDecoder framework</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, EncoderDecoderModel, ProphetNetTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_enc = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_dec = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel.from_encoder_decoder_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;bert-large-uncased&quot;</span>, <span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>ARTICLE = (
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;the us state department said wednesday it had received no &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;formal word from bolivia that it was expelling the us ambassador there &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;but said the charges made against him are \`\` baseless .&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer_enc(ARTICLE, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer_dec(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;us rejects charges against its ambassador in bolivia&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=labels[:, :-<span class="hljs-number">1</span>], labels=labels[:, <span class="hljs-number">1</span>:])

<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),{c(){u=n("meta"),N=c(),T=n("h1"),w=n("a"),P=n("span"),m(y.$$.fragment),b=c(),q=n("span"),Xr=s("ProphetNet"),Jn=c(),se=n("p"),Go=n("strong"),Zr=s("DISCLAIMER:"),es=s(" If you see something strange, file a "),Qe=n("a"),ts=s("Github Issue"),os=s(` and assign
@patrickvonplaten`),Yn=c(),ae=n("h2"),Ee=n("a"),Ho=n("span"),m(Xe.$$.fragment),ns=c(),Vo=n("span"),rs=s("Overview"),Kn=c(),xe=n("p"),ss=s("The ProphetNet model was proposed in "),Ze=n("a"),as=s("ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,"),ds=s(` by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei
Zhang, Ming Zhou on 13 Jan, 2020.`),Qn=c(),ao=n("p"),is=s(`ProphetNet is an encoder-decoder model and can predict n-future tokens for \u201Cngram\u201D language modeling instead of just
the next token.`),Xn=c(),io=n("p"),cs=s("The abstract from the paper is the following:"),Zn=c(),co=n("p"),Uo=n("em"),ls=s(`In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel
self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of
the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by
n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time
step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent
overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale
dataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for
abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new
state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.`),er=c(),Ce=n("p"),hs=s("The Authors\u2019 code can be found "),et=n("a"),ps=s("here"),us=s("."),tr=c(),de=n("h2"),Oe=n("a"),Ro=n("span"),m(tt.$$.fragment),ms=c(),Jo=n("span"),fs=s("ProphetNetConfig"),or=c(),U=n("div"),m(ot.$$.fragment),_s=c(),nt=n("p"),gs=s("This is the configuration class to store the configuration of a "),lo=n("a"),vs=s("ProphetNetModel"),ks=s(`. It is used to instantiate a
ProphetNet model according to the specified arguments, defining the model architecture.`),Ts=c(),ie=n("p"),bs=s("Configuration objects inherit from "),ho=n("a"),ys=s("PretrainedConfig"),ws=s(` and can be used to control the model outputs. Read the
documentation from `),po=n("a"),Ps=s("PretrainedConfig"),Ns=s(" for more information."),nr=c(),ce=n("h2"),Se=n("a"),Yo=n("span"),m(rt.$$.fragment),qs=c(),Ko=n("span"),zs=s("ProphetNetTokenizer"),rr=c(),$=n("div"),m(st.$$.fragment),$s=c(),Qo=n("p"),Fs=s("Construct a ProphetNetTokenizer. Based on WordPiece."),Ms=c(),at=n("p"),Es=s("This tokenizer inherits from "),uo=n("a"),xs=s("PreTrainedTokenizer"),Cs=s(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Os=c(),R=n("div"),m(dt.$$.fragment),Ss=c(),Xo=n("p"),Ds=s(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),Ls=c(),it=n("ul"),mo=n("li"),js=s("single sequence: "),Zo=n("code"),As=s("[CLS] X [SEP]"),Is=c(),fo=n("li"),Ws=s("pair of sequences: "),en=n("code"),Bs=s("[CLS] A [SEP] B [SEP]"),Gs=c(),De=n("div"),m(ct.$$.fragment),Hs=c(),tn=n("p"),Vs=s("Converts a sequence of tokens (string) in a single string."),Us=c(),G=n("div"),m(lt.$$.fragment),Rs=c(),on=n("p"),Js=s(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A ProphetNet
sequence pair mask has the following format:`),Ys=c(),m(ht.$$.fragment),Ks=c(),le=n("p"),Qs=s("If "),nn=n("code"),Xs=s("token_ids_1"),Zs=s(" is "),rn=n("code"),ea=s("None"),ta=s(", this method only returns the first portion of the mask (0s)."),oa=c(),Le=n("div"),m(pt.$$.fragment),na=c(),ut=n("p"),ra=s(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),sn=n("code"),sa=s("prepare_for_model"),aa=s(" method."),sr=c(),he=n("h2"),je=n("a"),an=n("span"),m(mt.$$.fragment),da=c(),dn=n("span"),ia=s("ProphetNet specific outputs"),ar=c(),pe=n("div"),m(ft.$$.fragment),ca=c(),cn=n("p"),la=s("Base class for sequence-to-sequence language models outputs."),dr=c(),ue=n("div"),m(_t.$$.fragment),ha=c(),ln=n("p"),pa=s(`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),ir=c(),me=n("div"),m(gt.$$.fragment),ua=c(),hn=n("p"),ma=s("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),cr=c(),fe=n("div"),m(vt.$$.fragment),fa=c(),pn=n("p"),_a=s("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),lr=c(),_e=n("h2"),Ae=n("a"),un=n("span"),m(kt.$$.fragment),ga=c(),mn=n("span"),va=s("ProphetNetModel"),hr=c(),E=n("div"),m(Tt.$$.fragment),ka=c(),bt=n("p"),Ta=s(`The bare ProphetNet Model outputting raw hidden-states without any specific head on top.
This model inherits from `),_o=n("a"),ba=s("PreTrainedModel"),ya=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),wa=c(),ge=n("p"),Pa=s("Original ProphetNet code can be found "),yt=n("a"),Na=s("here"),qa=s(`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),fn=n("code"),za=s("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),$a=s("."),Fa=c(),wt=n("p"),Ma=s("This model is a PyTorch "),Pt=n("a"),Ea=s("torch.nn.Module"),xa=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Ca=c(),O=n("div"),m(Nt.$$.fragment),Oa=c(),ve=n("p"),Sa=s("The "),go=n("a"),Da=s("ProphetNetModel"),La=s(" forward method, overrides the "),_n=n("code"),ja=s("__call__"),Aa=s(" special method."),Ia=c(),m(Ie.$$.fragment),Wa=c(),gn=n("p"),Ba=s("Example:"),Ga=c(),m(qt.$$.fragment),pr=c(),ke=n("h2"),We=n("a"),vn=n("span"),m(zt.$$.fragment),Ha=c(),kn=n("span"),Va=s("ProphetNetEncoder"),ur=c(),F=n("div"),m($t.$$.fragment),Ua=c(),Ft=n("p"),Ra=s(`The standalone encoder part of the ProphetNetModel.
This model inherits from `),vo=n("a"),Ja=s("PreTrainedModel"),Ya=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ka=c(),Te=n("p"),Qa=s("Original ProphetNet code can be found "),Mt=n("a"),Xa=s("here"),Za=s(`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Tn=n("code"),ed=s("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),td=s("."),od=c(),Et=n("p"),nd=s("This model is a PyTorch "),xt=n("a"),rd=s("torch.nn.Module"),sd=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),ad=c(),I=n("p"),dd=s("word_embeddings  ("),bn=n("code"),id=s("torch.nn.Embeddings"),cd=s(" of shape "),yn=n("code"),ld=s("(config.vocab_size, config.hidden_size)"),hd=s(", "),wn=n("em"),pd=s("optional"),ud=s(`):
The word embedding parameters. This can be used to initialize `),ko=n("a"),md=s("ProphetNetEncoder"),fd=s(` with pre-defined word
embeddings instead of randomly initialized word embeddings.`),_d=c(),S=n("div"),m(Ct.$$.fragment),gd=c(),be=n("p"),vd=s("The "),To=n("a"),kd=s("ProphetNetEncoder"),Td=s(" forward method, overrides the "),Pn=n("code"),bd=s("__call__"),yd=s(" special method."),wd=c(),m(Be.$$.fragment),Pd=c(),Nn=n("p"),Nd=s("Example:"),qd=c(),m(Ot.$$.fragment),mr=c(),ye=n("h2"),Ge=n("a"),qn=n("span"),m(St.$$.fragment),zd=c(),zn=n("span"),$d=s("ProphetNetDecoder"),fr=c(),M=n("div"),m(Dt.$$.fragment),Fd=c(),Lt=n("p"),Md=s(`The standalone decoder part of the ProphetNetModel.
This model inherits from `),bo=n("a"),Ed=s("PreTrainedModel"),xd=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Cd=c(),we=n("p"),Od=s("Original ProphetNet code can be found "),jt=n("a"),Sd=s("here"),Dd=s(`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),$n=n("code"),Ld=s("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),jd=s("."),Ad=c(),At=n("p"),Id=s("This model is a PyTorch "),It=n("a"),Wd=s("torch.nn.Module"),Bd=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Gd=c(),W=n("p"),Hd=s("word_embeddings  ("),Fn=n("code"),Vd=s("torch.nn.Embeddings"),Ud=s(" of shape "),Mn=n("code"),Rd=s("(config.vocab_size, config.hidden_size)"),Jd=s(", "),En=n("em"),Yd=s("optional"),Kd=s(`):
The word embedding parameters. This can be used to initialize `),yo=n("a"),Qd=s("ProphetNetEncoder"),Xd=s(` with pre-defined word
embeddings instead of randomly initialized word embeddings.`),Zd=c(),D=n("div"),m(Wt.$$.fragment),ei=c(),Pe=n("p"),ti=s("The "),wo=n("a"),oi=s("ProphetNetDecoder"),ni=s(" forward method, overrides the "),xn=n("code"),ri=s("__call__"),si=s(" special method."),ai=c(),m(He.$$.fragment),di=c(),Cn=n("p"),ii=s("Example:"),ci=c(),m(Bt.$$.fragment),_r=c(),Ne=n("h2"),Ve=n("a"),On=n("span"),m(Gt.$$.fragment),li=c(),Sn=n("span"),hi=s("ProphetNetForConditionalGeneration"),gr=c(),x=n("div"),m(Ht.$$.fragment),pi=c(),Vt=n("p"),ui=s(`The ProphetNet Model with a language modeling head. Can be used for sequence generation tasks.
This model inherits from `),Po=n("a"),mi=s("PreTrainedModel"),fi=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),_i=c(),qe=n("p"),gi=s("Original ProphetNet code can be found "),Ut=n("a"),vi=s("here"),ki=s(`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Dn=n("code"),Ti=s("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),bi=s("."),yi=c(),Rt=n("p"),wi=s("This model is a PyTorch "),Jt=n("a"),Pi=s("torch.nn.Module"),Ni=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),qi=c(),L=n("div"),m(Yt.$$.fragment),zi=c(),ze=n("p"),$i=s("The "),No=n("a"),Fi=s("ProphetNetForConditionalGeneration"),Mi=s(" forward method, overrides the "),Ln=n("code"),Ei=s("__call__"),xi=s(" special method."),Ci=c(),m(Ue.$$.fragment),Oi=c(),jn=n("p"),Si=s("Example:"),Di=c(),m(Kt.$$.fragment),vr=c(),$e=n("h2"),Re=n("a"),An=n("span"),m(Qt.$$.fragment),Li=c(),In=n("span"),ji=s("ProphetNetForCausalLM"),kr=c(),C=n("div"),m(Xt.$$.fragment),Ai=c(),Zt=n("p"),Ii=s(`The standalone decoder part of the ProphetNetModel with a lm head on top. The model can be used for causal language modeling.
This model inherits from `),qo=n("a"),Wi=s("PreTrainedModel"),Bi=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Gi=c(),Fe=n("p"),Hi=s("Original ProphetNet code can be found "),eo=n("a"),Vi=s("here"),Ui=s(`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Wn=n("code"),Ri=s("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),Ji=s("."),Yi=c(),to=n("p"),Ki=s("This model is a PyTorch "),oo=n("a"),Qi=s("torch.nn.Module"),Xi=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Zi=c(),j=n("div"),m(no.$$.fragment),ec=c(),Me=n("p"),tc=s("The "),zo=n("a"),oc=s("ProphetNetForCausalLM"),nc=s(" forward method, overrides the "),Bn=n("code"),rc=s("__call__"),sc=s(" special method."),ac=c(),m(Je.$$.fragment),dc=c(),Gn=n("p"),ic=s("Example:"),cc=c(),m(ro.$$.fragment),this.h()},l(t){const h=oh('[data-svelte="svelte-1phssyn"]',document.head);u=r(h,"META",{name:!0,content:!0}),h.forEach(o),N=l(t),T=r(t,"H1",{class:!0});var so=d(T);w=r(so,"A",{id:!0,class:!0,href:!0});var Hn=d(w);P=r(Hn,"SPAN",{});var Vn=d(P);f(y.$$.fragment,Vn),Vn.forEach(o),Hn.forEach(o),b=l(so),q=r(so,"SPAN",{});var Un=d(q);Xr=a(Un,"ProphetNet"),Un.forEach(o),so.forEach(o),Jn=l(t),se=r(t,"P",{});var Ye=d(se);Go=r(Ye,"STRONG",{});var pc=d(Go);Zr=a(pc,"DISCLAIMER:"),pc.forEach(o),es=a(Ye," If you see something strange, file a "),Qe=r(Ye,"A",{href:!0,rel:!0});var uc=d(Qe);ts=a(uc,"Github Issue"),uc.forEach(o),os=a(Ye,` and assign
@patrickvonplaten`),Ye.forEach(o),Yn=l(t),ae=r(t,"H2",{class:!0});var br=d(ae);Ee=r(br,"A",{id:!0,class:!0,href:!0});var mc=d(Ee);Ho=r(mc,"SPAN",{});var fc=d(Ho);f(Xe.$$.fragment,fc),fc.forEach(o),mc.forEach(o),ns=l(br),Vo=r(br,"SPAN",{});var _c=d(Vo);rs=a(_c,"Overview"),_c.forEach(o),br.forEach(o),Kn=l(t),xe=r(t,"P",{});var yr=d(xe);ss=a(yr,"The ProphetNet model was proposed in "),Ze=r(yr,"A",{href:!0,rel:!0});var gc=d(Ze);as=a(gc,"ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,"),gc.forEach(o),ds=a(yr,` by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei
Zhang, Ming Zhou on 13 Jan, 2020.`),yr.forEach(o),Qn=l(t),ao=r(t,"P",{});var vc=d(ao);is=a(vc,`ProphetNet is an encoder-decoder model and can predict n-future tokens for \u201Cngram\u201D language modeling instead of just
the next token.`),vc.forEach(o),Xn=l(t),io=r(t,"P",{});var kc=d(io);cs=a(kc,"The abstract from the paper is the following:"),kc.forEach(o),Zn=l(t),co=r(t,"P",{});var Tc=d(co);Uo=r(Tc,"EM",{});var bc=d(Uo);ls=a(bc,`In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel
self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of
the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by
n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time
step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent
overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale
dataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for
abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new
state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.`),bc.forEach(o),Tc.forEach(o),er=l(t),Ce=r(t,"P",{});var wr=d(Ce);hs=a(wr,"The Authors\u2019 code can be found "),et=r(wr,"A",{href:!0,rel:!0});var yc=d(et);ps=a(yc,"here"),yc.forEach(o),us=a(wr,"."),wr.forEach(o),tr=l(t),de=r(t,"H2",{class:!0});var Pr=d(de);Oe=r(Pr,"A",{id:!0,class:!0,href:!0});var wc=d(Oe);Ro=r(wc,"SPAN",{});var Pc=d(Ro);f(tt.$$.fragment,Pc),Pc.forEach(o),wc.forEach(o),ms=l(Pr),Jo=r(Pr,"SPAN",{});var Nc=d(Jo);fs=a(Nc,"ProphetNetConfig"),Nc.forEach(o),Pr.forEach(o),or=l(t),U=r(t,"DIV",{class:!0});var $o=d(U);f(ot.$$.fragment,$o),_s=l($o),nt=r($o,"P",{});var Nr=d(nt);gs=a(Nr,"This is the configuration class to store the configuration of a "),lo=r(Nr,"A",{href:!0});var qc=d(lo);vs=a(qc,"ProphetNetModel"),qc.forEach(o),ks=a(Nr,`. It is used to instantiate a
ProphetNet model according to the specified arguments, defining the model architecture.`),Nr.forEach(o),Ts=l($o),ie=r($o,"P",{});var Fo=d(ie);bs=a(Fo,"Configuration objects inherit from "),ho=r(Fo,"A",{href:!0});var zc=d(ho);ys=a(zc,"PretrainedConfig"),zc.forEach(o),ws=a(Fo,` and can be used to control the model outputs. Read the
documentation from `),po=r(Fo,"A",{href:!0});var $c=d(po);Ps=a($c,"PretrainedConfig"),$c.forEach(o),Ns=a(Fo," for more information."),Fo.forEach(o),$o.forEach(o),nr=l(t),ce=r(t,"H2",{class:!0});var qr=d(ce);Se=r(qr,"A",{id:!0,class:!0,href:!0});var Fc=d(Se);Yo=r(Fc,"SPAN",{});var Mc=d(Yo);f(rt.$$.fragment,Mc),Mc.forEach(o),Fc.forEach(o),qs=l(qr),Ko=r(qr,"SPAN",{});var Ec=d(Ko);zs=a(Ec,"ProphetNetTokenizer"),Ec.forEach(o),qr.forEach(o),rr=l(t),$=r(t,"DIV",{class:!0});var A=d($);f(st.$$.fragment,A),$s=l(A),Qo=r(A,"P",{});var xc=d(Qo);Fs=a(xc,"Construct a ProphetNetTokenizer. Based on WordPiece."),xc.forEach(o),Ms=l(A),at=r(A,"P",{});var zr=d(at);Es=a(zr,"This tokenizer inherits from "),uo=r(zr,"A",{href:!0});var Cc=d(uo);xs=a(Cc,"PreTrainedTokenizer"),Cc.forEach(o),Cs=a(zr,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),zr.forEach(o),Os=l(A),R=r(A,"DIV",{class:!0});var Mo=d(R);f(dt.$$.fragment,Mo),Ss=l(Mo),Xo=r(Mo,"P",{});var Oc=d(Xo);Ds=a(Oc,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),Oc.forEach(o),Ls=l(Mo),it=r(Mo,"UL",{});var $r=d(it);mo=r($r,"LI",{});var lc=d(mo);js=a(lc,"single sequence: "),Zo=r(lc,"CODE",{});var Sc=d(Zo);As=a(Sc,"[CLS] X [SEP]"),Sc.forEach(o),lc.forEach(o),Is=l($r),fo=r($r,"LI",{});var hc=d(fo);Ws=a(hc,"pair of sequences: "),en=r(hc,"CODE",{});var Dc=d(en);Bs=a(Dc,"[CLS] A [SEP] B [SEP]"),Dc.forEach(o),hc.forEach(o),$r.forEach(o),Mo.forEach(o),Gs=l(A),De=r(A,"DIV",{class:!0});var Fr=d(De);f(ct.$$.fragment,Fr),Hs=l(Fr),tn=r(Fr,"P",{});var Lc=d(tn);Vs=a(Lc,"Converts a sequence of tokens (string) in a single string."),Lc.forEach(o),Fr.forEach(o),Us=l(A),G=r(A,"DIV",{class:!0});var Ke=d(G);f(lt.$$.fragment,Ke),Rs=l(Ke),on=r(Ke,"P",{});var jc=d(on);Js=a(jc,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A ProphetNet
sequence pair mask has the following format:`),jc.forEach(o),Ys=l(Ke),f(ht.$$.fragment,Ke),Ks=l(Ke),le=r(Ke,"P",{});var Eo=d(le);Qs=a(Eo,"If "),nn=r(Eo,"CODE",{});var Ac=d(nn);Xs=a(Ac,"token_ids_1"),Ac.forEach(o),Zs=a(Eo," is "),rn=r(Eo,"CODE",{});var Ic=d(rn);ea=a(Ic,"None"),Ic.forEach(o),ta=a(Eo,", this method only returns the first portion of the mask (0s)."),Eo.forEach(o),Ke.forEach(o),oa=l(A),Le=r(A,"DIV",{class:!0});var Mr=d(Le);f(pt.$$.fragment,Mr),na=l(Mr),ut=r(Mr,"P",{});var Er=d(ut);ra=a(Er,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),sn=r(Er,"CODE",{});var Wc=d(sn);sa=a(Wc,"prepare_for_model"),Wc.forEach(o),aa=a(Er," method."),Er.forEach(o),Mr.forEach(o),A.forEach(o),sr=l(t),he=r(t,"H2",{class:!0});var xr=d(he);je=r(xr,"A",{id:!0,class:!0,href:!0});var Bc=d(je);an=r(Bc,"SPAN",{});var Gc=d(an);f(mt.$$.fragment,Gc),Gc.forEach(o),Bc.forEach(o),da=l(xr),dn=r(xr,"SPAN",{});var Hc=d(dn);ia=a(Hc,"ProphetNet specific outputs"),Hc.forEach(o),xr.forEach(o),ar=l(t),pe=r(t,"DIV",{class:!0});var Cr=d(pe);f(ft.$$.fragment,Cr),ca=l(Cr),cn=r(Cr,"P",{});var Vc=d(cn);la=a(Vc,"Base class for sequence-to-sequence language models outputs."),Vc.forEach(o),Cr.forEach(o),dr=l(t),ue=r(t,"DIV",{class:!0});var Or=d(ue);f(_t.$$.fragment,Or),ha=l(Or),ln=r(Or,"P",{});var Uc=d(ln);pa=a(Uc,`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),Uc.forEach(o),Or.forEach(o),ir=l(t),me=r(t,"DIV",{class:!0});var Sr=d(me);f(gt.$$.fragment,Sr),ua=l(Sr),hn=r(Sr,"P",{});var Rc=d(hn);ma=a(Rc,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Rc.forEach(o),Sr.forEach(o),cr=l(t),fe=r(t,"DIV",{class:!0});var Dr=d(fe);f(vt.$$.fragment,Dr),fa=l(Dr),pn=r(Dr,"P",{});var Jc=d(pn);_a=a(Jc,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Jc.forEach(o),Dr.forEach(o),lr=l(t),_e=r(t,"H2",{class:!0});var Lr=d(_e);Ae=r(Lr,"A",{id:!0,class:!0,href:!0});var Yc=d(Ae);un=r(Yc,"SPAN",{});var Kc=d(un);f(kt.$$.fragment,Kc),Kc.forEach(o),Yc.forEach(o),ga=l(Lr),mn=r(Lr,"SPAN",{});var Qc=d(mn);va=a(Qc,"ProphetNetModel"),Qc.forEach(o),Lr.forEach(o),hr=l(t),E=r(t,"DIV",{class:!0});var J=d(E);f(Tt.$$.fragment,J),ka=l(J),bt=r(J,"P",{});var jr=d(bt);Ta=a(jr,`The bare ProphetNet Model outputting raw hidden-states without any specific head on top.
This model inherits from `),_o=r(jr,"A",{href:!0});var Xc=d(_o);ba=a(Xc,"PreTrainedModel"),Xc.forEach(o),ya=a(jr,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),jr.forEach(o),wa=l(J),ge=r(J,"P",{});var xo=d(ge);Pa=a(xo,"Original ProphetNet code can be found "),yt=r(xo,"A",{href:!0,rel:!0});var Zc=d(yt);Na=a(Zc,"here"),Zc.forEach(o),qa=a(xo,`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),fn=r(xo,"CODE",{});var el=d(fn);za=a(el,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),el.forEach(o),$a=a(xo,"."),xo.forEach(o),Fa=l(J),wt=r(J,"P",{});var Ar=d(wt);Ma=a(Ar,"This model is a PyTorch "),Pt=r(Ar,"A",{href:!0,rel:!0});var tl=d(Pt);Ea=a(tl,"torch.nn.Module"),tl.forEach(o),xa=a(Ar,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Ar.forEach(o),Ca=l(J),O=r(J,"DIV",{class:!0});var Y=d(O);f(Nt.$$.fragment,Y),Oa=l(Y),ve=r(Y,"P",{});var Co=d(ve);Sa=a(Co,"The "),go=r(Co,"A",{href:!0});var ol=d(go);Da=a(ol,"ProphetNetModel"),ol.forEach(o),La=a(Co," forward method, overrides the "),_n=r(Co,"CODE",{});var nl=d(_n);ja=a(nl,"__call__"),nl.forEach(o),Aa=a(Co," special method."),Co.forEach(o),Ia=l(Y),f(Ie.$$.fragment,Y),Wa=l(Y),gn=r(Y,"P",{});var rl=d(gn);Ba=a(rl,"Example:"),rl.forEach(o),Ga=l(Y),f(qt.$$.fragment,Y),Y.forEach(o),J.forEach(o),pr=l(t),ke=r(t,"H2",{class:!0});var Ir=d(ke);We=r(Ir,"A",{id:!0,class:!0,href:!0});var sl=d(We);vn=r(sl,"SPAN",{});var al=d(vn);f(zt.$$.fragment,al),al.forEach(o),sl.forEach(o),Ha=l(Ir),kn=r(Ir,"SPAN",{});var dl=d(kn);Va=a(dl,"ProphetNetEncoder"),dl.forEach(o),Ir.forEach(o),ur=l(t),F=r(t,"DIV",{class:!0});var H=d(F);f($t.$$.fragment,H),Ua=l(H),Ft=r(H,"P",{});var Wr=d(Ft);Ra=a(Wr,`The standalone encoder part of the ProphetNetModel.
This model inherits from `),vo=r(Wr,"A",{href:!0});var il=d(vo);Ja=a(il,"PreTrainedModel"),il.forEach(o),Ya=a(Wr,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Wr.forEach(o),Ka=l(H),Te=r(H,"P",{});var Oo=d(Te);Qa=a(Oo,"Original ProphetNet code can be found "),Mt=r(Oo,"A",{href:!0,rel:!0});var cl=d(Mt);Xa=a(cl,"here"),cl.forEach(o),Za=a(Oo,`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Tn=r(Oo,"CODE",{});var ll=d(Tn);ed=a(ll,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),ll.forEach(o),td=a(Oo,"."),Oo.forEach(o),od=l(H),Et=r(H,"P",{});var Br=d(Et);nd=a(Br,"This model is a PyTorch "),xt=r(Br,"A",{href:!0,rel:!0});var hl=d(xt);rd=a(hl,"torch.nn.Module"),hl.forEach(o),sd=a(Br,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Br.forEach(o),ad=l(H),I=r(H,"P",{});var K=d(I);dd=a(K,"word_embeddings  ("),bn=r(K,"CODE",{});var pl=d(bn);id=a(pl,"torch.nn.Embeddings"),pl.forEach(o),cd=a(K," of shape "),yn=r(K,"CODE",{});var ul=d(yn);ld=a(ul,"(config.vocab_size, config.hidden_size)"),ul.forEach(o),hd=a(K,", "),wn=r(K,"EM",{});var ml=d(wn);pd=a(ml,"optional"),ml.forEach(o),ud=a(K,`):
The word embedding parameters. This can be used to initialize `),ko=r(K,"A",{href:!0});var fl=d(ko);md=a(fl,"ProphetNetEncoder"),fl.forEach(o),fd=a(K,` with pre-defined word
embeddings instead of randomly initialized word embeddings.`),K.forEach(o),_d=l(H),S=r(H,"DIV",{class:!0});var Q=d(S);f(Ct.$$.fragment,Q),gd=l(Q),be=r(Q,"P",{});var So=d(be);vd=a(So,"The "),To=r(So,"A",{href:!0});var _l=d(To);kd=a(_l,"ProphetNetEncoder"),_l.forEach(o),Td=a(So," forward method, overrides the "),Pn=r(So,"CODE",{});var gl=d(Pn);bd=a(gl,"__call__"),gl.forEach(o),yd=a(So," special method."),So.forEach(o),wd=l(Q),f(Be.$$.fragment,Q),Pd=l(Q),Nn=r(Q,"P",{});var vl=d(Nn);Nd=a(vl,"Example:"),vl.forEach(o),qd=l(Q),f(Ot.$$.fragment,Q),Q.forEach(o),H.forEach(o),mr=l(t),ye=r(t,"H2",{class:!0});var Gr=d(ye);Ge=r(Gr,"A",{id:!0,class:!0,href:!0});var kl=d(Ge);qn=r(kl,"SPAN",{});var Tl=d(qn);f(St.$$.fragment,Tl),Tl.forEach(o),kl.forEach(o),zd=l(Gr),zn=r(Gr,"SPAN",{});var bl=d(zn);$d=a(bl,"ProphetNetDecoder"),bl.forEach(o),Gr.forEach(o),fr=l(t),M=r(t,"DIV",{class:!0});var V=d(M);f(Dt.$$.fragment,V),Fd=l(V),Lt=r(V,"P",{});var Hr=d(Lt);Md=a(Hr,`The standalone decoder part of the ProphetNetModel.
This model inherits from `),bo=r(Hr,"A",{href:!0});var yl=d(bo);Ed=a(yl,"PreTrainedModel"),yl.forEach(o),xd=a(Hr,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Hr.forEach(o),Cd=l(V),we=r(V,"P",{});var Do=d(we);Od=a(Do,"Original ProphetNet code can be found "),jt=r(Do,"A",{href:!0,rel:!0});var wl=d(jt);Sd=a(wl,"here"),wl.forEach(o),Dd=a(Do,`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),$n=r(Do,"CODE",{});var Pl=d($n);Ld=a(Pl,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),Pl.forEach(o),jd=a(Do,"."),Do.forEach(o),Ad=l(V),At=r(V,"P",{});var Vr=d(At);Id=a(Vr,"This model is a PyTorch "),It=r(Vr,"A",{href:!0,rel:!0});var Nl=d(It);Wd=a(Nl,"torch.nn.Module"),Nl.forEach(o),Bd=a(Vr,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Vr.forEach(o),Gd=l(V),W=r(V,"P",{});var X=d(W);Hd=a(X,"word_embeddings  ("),Fn=r(X,"CODE",{});var ql=d(Fn);Vd=a(ql,"torch.nn.Embeddings"),ql.forEach(o),Ud=a(X," of shape "),Mn=r(X,"CODE",{});var zl=d(Mn);Rd=a(zl,"(config.vocab_size, config.hidden_size)"),zl.forEach(o),Jd=a(X,", "),En=r(X,"EM",{});var $l=d(En);Yd=a($l,"optional"),$l.forEach(o),Kd=a(X,`):
The word embedding parameters. This can be used to initialize `),yo=r(X,"A",{href:!0});var Fl=d(yo);Qd=a(Fl,"ProphetNetEncoder"),Fl.forEach(o),Xd=a(X,` with pre-defined word
embeddings instead of randomly initialized word embeddings.`),X.forEach(o),Zd=l(V),D=r(V,"DIV",{class:!0});var Z=d(D);f(Wt.$$.fragment,Z),ei=l(Z),Pe=r(Z,"P",{});var Lo=d(Pe);ti=a(Lo,"The "),wo=r(Lo,"A",{href:!0});var Ml=d(wo);oi=a(Ml,"ProphetNetDecoder"),Ml.forEach(o),ni=a(Lo," forward method, overrides the "),xn=r(Lo,"CODE",{});var El=d(xn);ri=a(El,"__call__"),El.forEach(o),si=a(Lo," special method."),Lo.forEach(o),ai=l(Z),f(He.$$.fragment,Z),di=l(Z),Cn=r(Z,"P",{});var xl=d(Cn);ii=a(xl,"Example:"),xl.forEach(o),ci=l(Z),f(Bt.$$.fragment,Z),Z.forEach(o),V.forEach(o),_r=l(t),Ne=r(t,"H2",{class:!0});var Ur=d(Ne);Ve=r(Ur,"A",{id:!0,class:!0,href:!0});var Cl=d(Ve);On=r(Cl,"SPAN",{});var Ol=d(On);f(Gt.$$.fragment,Ol),Ol.forEach(o),Cl.forEach(o),li=l(Ur),Sn=r(Ur,"SPAN",{});var Sl=d(Sn);hi=a(Sl,"ProphetNetForConditionalGeneration"),Sl.forEach(o),Ur.forEach(o),gr=l(t),x=r(t,"DIV",{class:!0});var ee=d(x);f(Ht.$$.fragment,ee),pi=l(ee),Vt=r(ee,"P",{});var Rr=d(Vt);ui=a(Rr,`The ProphetNet Model with a language modeling head. Can be used for sequence generation tasks.
This model inherits from `),Po=r(Rr,"A",{href:!0});var Dl=d(Po);mi=a(Dl,"PreTrainedModel"),Dl.forEach(o),fi=a(Rr,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Rr.forEach(o),_i=l(ee),qe=r(ee,"P",{});var jo=d(qe);gi=a(jo,"Original ProphetNet code can be found "),Ut=r(jo,"A",{href:!0,rel:!0});var Ll=d(Ut);vi=a(Ll,"here"),Ll.forEach(o),ki=a(jo,`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Dn=r(jo,"CODE",{});var jl=d(Dn);Ti=a(jl,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),jl.forEach(o),bi=a(jo,"."),jo.forEach(o),yi=l(ee),Rt=r(ee,"P",{});var Jr=d(Rt);wi=a(Jr,"This model is a PyTorch "),Jt=r(Jr,"A",{href:!0,rel:!0});var Al=d(Jt);Pi=a(Al,"torch.nn.Module"),Al.forEach(o),Ni=a(Jr,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Jr.forEach(o),qi=l(ee),L=r(ee,"DIV",{class:!0});var te=d(L);f(Yt.$$.fragment,te),zi=l(te),ze=r(te,"P",{});var Ao=d(ze);$i=a(Ao,"The "),No=r(Ao,"A",{href:!0});var Il=d(No);Fi=a(Il,"ProphetNetForConditionalGeneration"),Il.forEach(o),Mi=a(Ao," forward method, overrides the "),Ln=r(Ao,"CODE",{});var Wl=d(Ln);Ei=a(Wl,"__call__"),Wl.forEach(o),xi=a(Ao," special method."),Ao.forEach(o),Ci=l(te),f(Ue.$$.fragment,te),Oi=l(te),jn=r(te,"P",{});var Bl=d(jn);Si=a(Bl,"Example:"),Bl.forEach(o),Di=l(te),f(Kt.$$.fragment,te),te.forEach(o),ee.forEach(o),vr=l(t),$e=r(t,"H2",{class:!0});var Yr=d($e);Re=r(Yr,"A",{id:!0,class:!0,href:!0});var Gl=d(Re);An=r(Gl,"SPAN",{});var Hl=d(An);f(Qt.$$.fragment,Hl),Hl.forEach(o),Gl.forEach(o),Li=l(Yr),In=r(Yr,"SPAN",{});var Vl=d(In);ji=a(Vl,"ProphetNetForCausalLM"),Vl.forEach(o),Yr.forEach(o),kr=l(t),C=r(t,"DIV",{class:!0});var oe=d(C);f(Xt.$$.fragment,oe),Ai=l(oe),Zt=r(oe,"P",{});var Kr=d(Zt);Ii=a(Kr,`The standalone decoder part of the ProphetNetModel with a lm head on top. The model can be used for causal language modeling.
This model inherits from `),qo=r(Kr,"A",{href:!0});var Ul=d(qo);Wi=a(Ul,"PreTrainedModel"),Ul.forEach(o),Bi=a(Kr,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Kr.forEach(o),Gi=l(oe),Fe=r(oe,"P",{});var Io=d(Fe);Hi=a(Io,"Original ProphetNet code can be found "),eo=r(Io,"A",{href:!0,rel:!0});var Rl=d(eo);Vi=a(Rl,"here"),Rl.forEach(o),Ui=a(Io,`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Wn=r(Io,"CODE",{});var Jl=d(Wn);Ri=a(Jl,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),Jl.forEach(o),Ji=a(Io,"."),Io.forEach(o),Yi=l(oe),to=r(oe,"P",{});var Qr=d(to);Ki=a(Qr,"This model is a PyTorch "),oo=r(Qr,"A",{href:!0,rel:!0});var Yl=d(oo);Qi=a(Yl,"torch.nn.Module"),Yl.forEach(o),Xi=a(Qr,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Qr.forEach(o),Zi=l(oe),j=r(oe,"DIV",{class:!0});var ne=d(j);f(no.$$.fragment,ne),ec=l(ne),Me=r(ne,"P",{});var Wo=d(Me);tc=a(Wo,"The "),zo=r(Wo,"A",{href:!0});var Kl=d(zo);oc=a(Kl,"ProphetNetForCausalLM"),Kl.forEach(o),nc=a(Wo," forward method, overrides the "),Bn=r(Wo,"CODE",{});var Ql=d(Bn);rc=a(Ql,"__call__"),Ql.forEach(o),sc=a(Wo," special method."),Wo.forEach(o),ac=l(ne),f(Je.$$.fragment,ne),dc=l(ne),Gn=r(ne,"P",{});var Xl=d(Gn);ic=a(Xl,"Example:"),Xl.forEach(o),cc=l(ne),f(ro.$$.fragment,ne),ne.forEach(o),oe.forEach(o),this.h()},h(){i(u,"name","hf:doc:metadata"),i(u,"content",JSON.stringify(lh)),i(w,"id","prophetnet"),i(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(w,"href","#prophetnet"),i(T,"class","relative group"),i(Qe,"href","https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title"),i(Qe,"rel","nofollow"),i(Ee,"id","overview"),i(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Ee,"href","#overview"),i(ae,"class","relative group"),i(Ze,"href","https://arxiv.org/abs/2001.04063"),i(Ze,"rel","nofollow"),i(et,"href","https://github.com/microsoft/ProphetNet"),i(et,"rel","nofollow"),i(Oe,"id","transformers.ProphetNetConfig"),i(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Oe,"href","#transformers.ProphetNetConfig"),i(de,"class","relative group"),i(lo,"href","/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetModel"),i(ho,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),i(po,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),i(U,"class","docstring"),i(Se,"id","transformers.ProphetNetTokenizer"),i(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Se,"href","#transformers.ProphetNetTokenizer"),i(ce,"class","relative group"),i(uo,"href","/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),i(R,"class","docstring"),i(De,"class","docstring"),i(G,"class","docstring"),i(Le,"class","docstring"),i($,"class","docstring"),i(je,"id","transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput"),i(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(je,"href","#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput"),i(he,"class","relative group"),i(pe,"class","docstring"),i(ue,"class","docstring"),i(me,"class","docstring"),i(fe,"class","docstring"),i(Ae,"id","transformers.ProphetNetModel"),i(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Ae,"href","#transformers.ProphetNetModel"),i(_e,"class","relative group"),i(_o,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),i(yt,"href","https://github.com/microsoft/ProphetNet"),i(yt,"rel","nofollow"),i(Pt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(Pt,"rel","nofollow"),i(go,"href","/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetModel"),i(O,"class","docstring"),i(E,"class","docstring"),i(We,"id","transformers.ProphetNetEncoder"),i(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(We,"href","#transformers.ProphetNetEncoder"),i(ke,"class","relative group"),i(vo,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),i(Mt,"href","https://github.com/microsoft/ProphetNet"),i(Mt,"rel","nofollow"),i(xt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(xt,"rel","nofollow"),i(ko,"href","/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetEncoder"),i(To,"href","/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetEncoder"),i(S,"class","docstring"),i(F,"class","docstring"),i(Ge,"id","transformers.ProphetNetDecoder"),i(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Ge,"href","#transformers.ProphetNetDecoder"),i(ye,"class","relative group"),i(bo,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),i(jt,"href","https://github.com/microsoft/ProphetNet"),i(jt,"rel","nofollow"),i(It,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(It,"rel","nofollow"),i(yo,"href","/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetEncoder"),i(wo,"href","/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetDecoder"),i(D,"class","docstring"),i(M,"class","docstring"),i(Ve,"id","transformers.ProphetNetForConditionalGeneration"),i(Ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Ve,"href","#transformers.ProphetNetForConditionalGeneration"),i(Ne,"class","relative group"),i(Po,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),i(Ut,"href","https://github.com/microsoft/ProphetNet"),i(Ut,"rel","nofollow"),i(Jt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(Jt,"rel","nofollow"),i(No,"href","/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),i(L,"class","docstring"),i(x,"class","docstring"),i(Re,"id","transformers.ProphetNetForCausalLM"),i(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Re,"href","#transformers.ProphetNetForCausalLM"),i($e,"class","relative group"),i(qo,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),i(eo,"href","https://github.com/microsoft/ProphetNet"),i(eo,"rel","nofollow"),i(oo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(oo,"rel","nofollow"),i(zo,"href","/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),i(j,"class","docstring"),i(C,"class","docstring")},m(t,h){e(document.head,u),p(t,N,h),p(t,T,h),e(T,w),e(w,P),_(y,P,null),e(T,b),e(T,q),e(q,Xr),p(t,Jn,h),p(t,se,h),e(se,Go),e(Go,Zr),e(se,es),e(se,Qe),e(Qe,ts),e(se,os),p(t,Yn,h),p(t,ae,h),e(ae,Ee),e(Ee,Ho),_(Xe,Ho,null),e(ae,ns),e(ae,Vo),e(Vo,rs),p(t,Kn,h),p(t,xe,h),e(xe,ss),e(xe,Ze),e(Ze,as),e(xe,ds),p(t,Qn,h),p(t,ao,h),e(ao,is),p(t,Xn,h),p(t,io,h),e(io,cs),p(t,Zn,h),p(t,co,h),e(co,Uo),e(Uo,ls),p(t,er,h),p(t,Ce,h),e(Ce,hs),e(Ce,et),e(et,ps),e(Ce,us),p(t,tr,h),p(t,de,h),e(de,Oe),e(Oe,Ro),_(tt,Ro,null),e(de,ms),e(de,Jo),e(Jo,fs),p(t,or,h),p(t,U,h),_(ot,U,null),e(U,_s),e(U,nt),e(nt,gs),e(nt,lo),e(lo,vs),e(nt,ks),e(U,Ts),e(U,ie),e(ie,bs),e(ie,ho),e(ho,ys),e(ie,ws),e(ie,po),e(po,Ps),e(ie,Ns),p(t,nr,h),p(t,ce,h),e(ce,Se),e(Se,Yo),_(rt,Yo,null),e(ce,qs),e(ce,Ko),e(Ko,zs),p(t,rr,h),p(t,$,h),_(st,$,null),e($,$s),e($,Qo),e(Qo,Fs),e($,Ms),e($,at),e(at,Es),e(at,uo),e(uo,xs),e(at,Cs),e($,Os),e($,R),_(dt,R,null),e(R,Ss),e(R,Xo),e(Xo,Ds),e(R,Ls),e(R,it),e(it,mo),e(mo,js),e(mo,Zo),e(Zo,As),e(it,Is),e(it,fo),e(fo,Ws),e(fo,en),e(en,Bs),e($,Gs),e($,De),_(ct,De,null),e(De,Hs),e(De,tn),e(tn,Vs),e($,Us),e($,G),_(lt,G,null),e(G,Rs),e(G,on),e(on,Js),e(G,Ys),_(ht,G,null),e(G,Ks),e(G,le),e(le,Qs),e(le,nn),e(nn,Xs),e(le,Zs),e(le,rn),e(rn,ea),e(le,ta),e($,oa),e($,Le),_(pt,Le,null),e(Le,na),e(Le,ut),e(ut,ra),e(ut,sn),e(sn,sa),e(ut,aa),p(t,sr,h),p(t,he,h),e(he,je),e(je,an),_(mt,an,null),e(he,da),e(he,dn),e(dn,ia),p(t,ar,h),p(t,pe,h),_(ft,pe,null),e(pe,ca),e(pe,cn),e(cn,la),p(t,dr,h),p(t,ue,h),_(_t,ue,null),e(ue,ha),e(ue,ln),e(ln,pa),p(t,ir,h),p(t,me,h),_(gt,me,null),e(me,ua),e(me,hn),e(hn,ma),p(t,cr,h),p(t,fe,h),_(vt,fe,null),e(fe,fa),e(fe,pn),e(pn,_a),p(t,lr,h),p(t,_e,h),e(_e,Ae),e(Ae,un),_(kt,un,null),e(_e,ga),e(_e,mn),e(mn,va),p(t,hr,h),p(t,E,h),_(Tt,E,null),e(E,ka),e(E,bt),e(bt,Ta),e(bt,_o),e(_o,ba),e(bt,ya),e(E,wa),e(E,ge),e(ge,Pa),e(ge,yt),e(yt,Na),e(ge,qa),e(ge,fn),e(fn,za),e(ge,$a),e(E,Fa),e(E,wt),e(wt,Ma),e(wt,Pt),e(Pt,Ea),e(wt,xa),e(E,Ca),e(E,O),_(Nt,O,null),e(O,Oa),e(O,ve),e(ve,Sa),e(ve,go),e(go,Da),e(ve,La),e(ve,_n),e(_n,ja),e(ve,Aa),e(O,Ia),_(Ie,O,null),e(O,Wa),e(O,gn),e(gn,Ba),e(O,Ga),_(qt,O,null),p(t,pr,h),p(t,ke,h),e(ke,We),e(We,vn),_(zt,vn,null),e(ke,Ha),e(ke,kn),e(kn,Va),p(t,ur,h),p(t,F,h),_($t,F,null),e(F,Ua),e(F,Ft),e(Ft,Ra),e(Ft,vo),e(vo,Ja),e(Ft,Ya),e(F,Ka),e(F,Te),e(Te,Qa),e(Te,Mt),e(Mt,Xa),e(Te,Za),e(Te,Tn),e(Tn,ed),e(Te,td),e(F,od),e(F,Et),e(Et,nd),e(Et,xt),e(xt,rd),e(Et,sd),e(F,ad),e(F,I),e(I,dd),e(I,bn),e(bn,id),e(I,cd),e(I,yn),e(yn,ld),e(I,hd),e(I,wn),e(wn,pd),e(I,ud),e(I,ko),e(ko,md),e(I,fd),e(F,_d),e(F,S),_(Ct,S,null),e(S,gd),e(S,be),e(be,vd),e(be,To),e(To,kd),e(be,Td),e(be,Pn),e(Pn,bd),e(be,yd),e(S,wd),_(Be,S,null),e(S,Pd),e(S,Nn),e(Nn,Nd),e(S,qd),_(Ot,S,null),p(t,mr,h),p(t,ye,h),e(ye,Ge),e(Ge,qn),_(St,qn,null),e(ye,zd),e(ye,zn),e(zn,$d),p(t,fr,h),p(t,M,h),_(Dt,M,null),e(M,Fd),e(M,Lt),e(Lt,Md),e(Lt,bo),e(bo,Ed),e(Lt,xd),e(M,Cd),e(M,we),e(we,Od),e(we,jt),e(jt,Sd),e(we,Dd),e(we,$n),e($n,Ld),e(we,jd),e(M,Ad),e(M,At),e(At,Id),e(At,It),e(It,Wd),e(At,Bd),e(M,Gd),e(M,W),e(W,Hd),e(W,Fn),e(Fn,Vd),e(W,Ud),e(W,Mn),e(Mn,Rd),e(W,Jd),e(W,En),e(En,Yd),e(W,Kd),e(W,yo),e(yo,Qd),e(W,Xd),e(M,Zd),e(M,D),_(Wt,D,null),e(D,ei),e(D,Pe),e(Pe,ti),e(Pe,wo),e(wo,oi),e(Pe,ni),e(Pe,xn),e(xn,ri),e(Pe,si),e(D,ai),_(He,D,null),e(D,di),e(D,Cn),e(Cn,ii),e(D,ci),_(Bt,D,null),p(t,_r,h),p(t,Ne,h),e(Ne,Ve),e(Ve,On),_(Gt,On,null),e(Ne,li),e(Ne,Sn),e(Sn,hi),p(t,gr,h),p(t,x,h),_(Ht,x,null),e(x,pi),e(x,Vt),e(Vt,ui),e(Vt,Po),e(Po,mi),e(Vt,fi),e(x,_i),e(x,qe),e(qe,gi),e(qe,Ut),e(Ut,vi),e(qe,ki),e(qe,Dn),e(Dn,Ti),e(qe,bi),e(x,yi),e(x,Rt),e(Rt,wi),e(Rt,Jt),e(Jt,Pi),e(Rt,Ni),e(x,qi),e(x,L),_(Yt,L,null),e(L,zi),e(L,ze),e(ze,$i),e(ze,No),e(No,Fi),e(ze,Mi),e(ze,Ln),e(Ln,Ei),e(ze,xi),e(L,Ci),_(Ue,L,null),e(L,Oi),e(L,jn),e(jn,Si),e(L,Di),_(Kt,L,null),p(t,vr,h),p(t,$e,h),e($e,Re),e(Re,An),_(Qt,An,null),e($e,Li),e($e,In),e(In,ji),p(t,kr,h),p(t,C,h),_(Xt,C,null),e(C,Ai),e(C,Zt),e(Zt,Ii),e(Zt,qo),e(qo,Wi),e(Zt,Bi),e(C,Gi),e(C,Fe),e(Fe,Hi),e(Fe,eo),e(eo,Vi),e(Fe,Ui),e(Fe,Wn),e(Wn,Ri),e(Fe,Ji),e(C,Yi),e(C,to),e(to,Ki),e(to,oo),e(oo,Qi),e(to,Xi),e(C,Zi),e(C,j),_(no,j,null),e(j,ec),e(j,Me),e(Me,tc),e(Me,zo),e(zo,oc),e(Me,nc),e(Me,Bn),e(Bn,rc),e(Me,sc),e(j,ac),_(Je,j,null),e(j,dc),e(j,Gn),e(Gn,ic),e(j,cc),_(ro,j,null),Tr=!0},p(t,[h]){const so={};h&2&&(so.$$scope={dirty:h,ctx:t}),Ie.$set(so);const Hn={};h&2&&(Hn.$$scope={dirty:h,ctx:t}),Be.$set(Hn);const Vn={};h&2&&(Vn.$$scope={dirty:h,ctx:t}),He.$set(Vn);const Un={};h&2&&(Un.$$scope={dirty:h,ctx:t}),Ue.$set(Un);const Ye={};h&2&&(Ye.$$scope={dirty:h,ctx:t}),Je.$set(Ye)},i(t){Tr||(g(y.$$.fragment,t),g(Xe.$$.fragment,t),g(tt.$$.fragment,t),g(ot.$$.fragment,t),g(rt.$$.fragment,t),g(st.$$.fragment,t),g(dt.$$.fragment,t),g(ct.$$.fragment,t),g(lt.$$.fragment,t),g(ht.$$.fragment,t),g(pt.$$.fragment,t),g(mt.$$.fragment,t),g(ft.$$.fragment,t),g(_t.$$.fragment,t),g(gt.$$.fragment,t),g(vt.$$.fragment,t),g(kt.$$.fragment,t),g(Tt.$$.fragment,t),g(Nt.$$.fragment,t),g(Ie.$$.fragment,t),g(qt.$$.fragment,t),g(zt.$$.fragment,t),g($t.$$.fragment,t),g(Ct.$$.fragment,t),g(Be.$$.fragment,t),g(Ot.$$.fragment,t),g(St.$$.fragment,t),g(Dt.$$.fragment,t),g(Wt.$$.fragment,t),g(He.$$.fragment,t),g(Bt.$$.fragment,t),g(Gt.$$.fragment,t),g(Ht.$$.fragment,t),g(Yt.$$.fragment,t),g(Ue.$$.fragment,t),g(Kt.$$.fragment,t),g(Qt.$$.fragment,t),g(Xt.$$.fragment,t),g(no.$$.fragment,t),g(Je.$$.fragment,t),g(ro.$$.fragment,t),Tr=!0)},o(t){v(y.$$.fragment,t),v(Xe.$$.fragment,t),v(tt.$$.fragment,t),v(ot.$$.fragment,t),v(rt.$$.fragment,t),v(st.$$.fragment,t),v(dt.$$.fragment,t),v(ct.$$.fragment,t),v(lt.$$.fragment,t),v(ht.$$.fragment,t),v(pt.$$.fragment,t),v(mt.$$.fragment,t),v(ft.$$.fragment,t),v(_t.$$.fragment,t),v(gt.$$.fragment,t),v(vt.$$.fragment,t),v(kt.$$.fragment,t),v(Tt.$$.fragment,t),v(Nt.$$.fragment,t),v(Ie.$$.fragment,t),v(qt.$$.fragment,t),v(zt.$$.fragment,t),v($t.$$.fragment,t),v(Ct.$$.fragment,t),v(Be.$$.fragment,t),v(Ot.$$.fragment,t),v(St.$$.fragment,t),v(Dt.$$.fragment,t),v(Wt.$$.fragment,t),v(He.$$.fragment,t),v(Bt.$$.fragment,t),v(Gt.$$.fragment,t),v(Ht.$$.fragment,t),v(Yt.$$.fragment,t),v(Ue.$$.fragment,t),v(Kt.$$.fragment,t),v(Qt.$$.fragment,t),v(Xt.$$.fragment,t),v(no.$$.fragment,t),v(Je.$$.fragment,t),v(ro.$$.fragment,t),Tr=!1},d(t){o(u),t&&o(N),t&&o(T),k(y),t&&o(Jn),t&&o(se),t&&o(Yn),t&&o(ae),k(Xe),t&&o(Kn),t&&o(xe),t&&o(Qn),t&&o(ao),t&&o(Xn),t&&o(io),t&&o(Zn),t&&o(co),t&&o(er),t&&o(Ce),t&&o(tr),t&&o(de),k(tt),t&&o(or),t&&o(U),k(ot),t&&o(nr),t&&o(ce),k(rt),t&&o(rr),t&&o($),k(st),k(dt),k(ct),k(lt),k(ht),k(pt),t&&o(sr),t&&o(he),k(mt),t&&o(ar),t&&o(pe),k(ft),t&&o(dr),t&&o(ue),k(_t),t&&o(ir),t&&o(me),k(gt),t&&o(cr),t&&o(fe),k(vt),t&&o(lr),t&&o(_e),k(kt),t&&o(hr),t&&o(E),k(Tt),k(Nt),k(Ie),k(qt),t&&o(pr),t&&o(ke),k(zt),t&&o(ur),t&&o(F),k($t),k(Ct),k(Be),k(Ot),t&&o(mr),t&&o(ye),k(St),t&&o(fr),t&&o(M),k(Dt),k(Wt),k(He),k(Bt),t&&o(_r),t&&o(Ne),k(Gt),t&&o(gr),t&&o(x),k(Ht),k(Yt),k(Ue),k(Kt),t&&o(vr),t&&o($e),k(Qt),t&&o(kr),t&&o(C),k(Xt),k(no),k(Je),k(ro)}}}const lh={local:"prophetnet",sections:[{local:"overview",title:"Overview"},{local:"transformers.ProphetNetConfig",title:"ProphetNetConfig"},{local:"transformers.ProphetNetTokenizer",title:"ProphetNetTokenizer"},{local:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput",title:"ProphetNet specific outputs"},{local:"transformers.ProphetNetModel",title:"ProphetNetModel"},{local:"transformers.ProphetNetEncoder",title:"ProphetNetEncoder"},{local:"transformers.ProphetNetDecoder",title:"ProphetNetDecoder"},{local:"transformers.ProphetNetForConditionalGeneration",title:"ProphetNetForConditionalGeneration"},{local:"transformers.ProphetNetForCausalLM",title:"ProphetNetForCausalLM"}],title:"ProphetNet"};function hh(B){return nh(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class gh extends Zl{constructor(u){super();eh(this,u,hh,ch,th,{})}}export{gh as default,lh as metadata};
