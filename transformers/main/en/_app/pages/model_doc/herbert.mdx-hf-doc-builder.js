import{S as gn,i as bn,s as vn,e as r,k as l,w as v,t as a,M as $n,c as n,d as s,m as c,a as o,x as $,h as i,b as p,G as e,g as f,y as w,q as y,o as E,B as T,v as wn,L as kn}from"../../chunks/vendor-hf-doc-builder.js";import{D as M}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Tr}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Ct}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as _n}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function yn($e){let u,H,g,h,L;return h=new Tr({props:{code:`0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),{c(){u=r("p"),H=a("pair mask has the following format:"),g=l(),v(h.$$.fragment)},l(d){u=n(d,"P",{});var z=o(u);H=i(z,"pair mask has the following format:"),z.forEach(s),g=c(d),$(h.$$.fragment,d)},m(d,z){f(d,u,z),e(u,H),f(d,g,z),w(h,d,z),L=!0},p:kn,i(d){L||(y(h.$$.fragment,d),L=!0)},o(d){E(h.$$.fragment,d),L=!1},d(d){d&&s(u),d&&s(g),T(h,d)}}}function En($e){let u,H,g,h,L;return h=new Tr({props:{code:`0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),{c(){u=r("p"),H=a("BERT sequence pair mask has the following format:"),g=l(),v(h.$$.fragment)},l(d){u=n(d,"P",{});var z=o(u);H=i(z,"BERT sequence pair mask has the following format:"),z.forEach(s),g=c(d),$(h.$$.fragment,d)},m(d,z){f(d,u,z),e(u,H),f(d,g,z),w(h,d,z),L=!0},p:kn,i(d){L||(y(h.$$.fragment,d),L=!0)},o(d){E(h.$$.fragment,d),L=!1},d(d){d&&s(u),d&&s(g),T(h,d)}}}function Tn($e){let u,H,g,h,L,d,z,Re,Ot,ht,D,N,Ie,G,Ut,Fe,St,ut,C,Xt,Q,Vt,Jt,_t,we,Kt,kt,ye,Me,Wt,gt,Ee,Gt,bt,Y,vt,j,Qt,Z,Yt,Zt,ee,es,ts,$t,R,O,Ne,te,ss,Ce,rs,wt,_,se,ns,Oe,os,as,Ue,is,ls,re,Se,Xe,cs,ds,Ve,Je,ps,ms,ne,fs,Te,hs,us,_s,B,oe,ks,Ke,gs,bs,ae,ze,vs,We,$s,ws,qe,ys,Ge,Es,Ts,U,ie,zs,Qe,qs,Ls,P,le,Hs,Ye,Ps,js,S,Bs,I,xs,Ze,As,Ds,et,Rs,Is,Fs,X,ce,Ms,de,Ns,tt,Cs,Os,yt,F,V,st,pe,Us,rt,Ss,Et,k,me,Xs,fe,Vs,nt,Js,Ks,Ws,ot,Gs,Qs,at,it,Ys,Zs,he,er,Le,tr,sr,rr,x,ue,nr,lt,or,ar,_e,He,ir,ct,lr,cr,Pe,dr,dt,pr,mr,A,ke,fr,pt,hr,ur,J,_r,K,ge,kr,be,gr,mt,br,vr,Tt;return d=new Ct({}),G=new Ct({}),Y=new Tr({props:{code:`from transformers import HerbertTokenizer, RobertaModel

tokenizer = HerbertTokenizer.from_pretrained("allegro/herbert-klej-cased-tokenizer-v1")
model = RobertaModel.from_pretrained("allegro/herbert-klej-cased-v1")

encoded_input = tokenizer.encode("Kto ma lepsz\u0105 sztuk\u0119, ma lepszy rz\u0105d \u2013 to jasne.", return_tensors="pt")
outputs = model(encoded_input)

# HerBERT can also be loaded using AutoTokenizer and AutoModel:
import torch
from transformers import AutoModel, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("allegro/herbert-klej-cased-tokenizer-v1")
model = AutoModel.from_pretrained("allegro/herbert-klej-cased-v1")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> HerbertTokenizer, RobertaModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = HerbertTokenizer.from_pretrained(<span class="hljs-string">&quot;allegro/herbert-klej-cased-tokenizer-v1&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RobertaModel.from_pretrained(<span class="hljs-string">&quot;allegro/herbert-klej-cased-v1&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer.encode(<span class="hljs-string">&quot;Kto ma lepsz\u0105 sztuk\u0119, ma lepszy rz\u0105d \u2013 to jasne.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(encoded_input)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># HerBERT can also be loaded using AutoTokenizer and AutoModel:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;allegro/herbert-klej-cased-tokenizer-v1&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;allegro/herbert-klej-cased-v1&quot;</span>)`}}),te=new Ct({}),se=new M({props:{name:"class transformers.HerbertTokenizer",anchor:"transformers.HerbertTokenizer",parameters:[{name:"vocab_file",val:""},{name:"merges_file",val:""},{name:"tokenizer_file",val:" = None"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"sep_token",val:" = '</s>'"},{name:"bos_token",val:" = '<s>'"},{name:"do_lowercase_and_remove_accent",val:" = False"},{name:"additional_special_tokens",val:" = ['<special0>', '<special1>', '<special2>', '<special3>', '<special4>', '<special5>', '<special6>', '<special7>', '<special8>', '<special9>']"},{name:"lang2id",val:" = None"},{name:"id2lang",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/herbert/tokenization_herbert.py#L277"}}),oe=new M({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.HerbertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.HerbertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.HerbertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/herbert/tokenization_herbert.py#L516",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ie=new M({props:{name:"convert_tokens_to_string",anchor:"transformers.HerbertTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/herbert/tokenization_herbert.py#L510"}}),le=new M({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.HerbertTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.HerbertTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.HerbertTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/herbert/tokenization_herbert.py#L573",returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),S=new _n({props:{anchor:"transformers.HerbertTokenizer.create_token_type_ids_from_sequences.example",$$slots:{default:[yn]},$$scope:{ctx:$e}}}),ce=new M({props:{name:"get_special_tokens_mask",anchor:"transformers.HerbertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.HerbertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.HerbertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.HerbertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/herbert/tokenization_herbert.py#L544",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),pe=new Ct({}),me=new M({props:{name:"class transformers.HerbertTokenizerFast",anchor:"transformers.HerbertTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"merges_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"sep_token",val:" = '</s>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.HerbertTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.HerbertTokenizerFast.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/herbert/tokenization_herbert_fast.py#L40"}}),ue=new M({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.HerbertTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.HerbertTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.HerbertTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/herbert/tokenization_herbert_fast.py#L90",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ke=new M({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.HerbertTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.HerbertTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.HerbertTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/herbert/tokenization_herbert_fast.py#L144",returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),J=new _n({props:{anchor:"transformers.HerbertTokenizerFast.create_token_type_ids_from_sequences.example",$$slots:{default:[En]},$$scope:{ctx:$e}}}),ge=new M({props:{name:"get_special_tokens_mask",anchor:"transformers.HerbertTokenizerFast.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.HerbertTokenizerFast.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.HerbertTokenizerFast.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.HerbertTokenizerFast.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/herbert/tokenization_herbert_fast.py#L117",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),{c(){u=r("meta"),H=l(),g=r("h1"),h=r("a"),L=r("span"),v(d.$$.fragment),z=l(),Re=r("span"),Ot=a("HerBERT"),ht=l(),D=r("h2"),N=r("a"),Ie=r("span"),v(G.$$.fragment),Ut=l(),Fe=r("span"),St=a("Overview"),ut=l(),C=r("p"),Xt=a("The HerBERT model was proposed in "),Q=r("a"),Vt=a("KLEJ: Comprehensive Benchmark for Polish Language Understanding"),Jt=a(` by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, and
Ireneusz Gawlik. It is a BERT-based Language Model trained on Polish Corpora using only MLM objective with dynamic
masking of whole words.`),_t=l(),we=r("p"),Kt=a("The abstract from the paper is the following:"),kt=l(),ye=r("p"),Me=r("em"),Wt=a(`In recent years, a series of Transformer-based models unlocked major improvements in general natural language
understanding (NLU) tasks. Such a fast pace of research would not be possible without general NLU benchmarks, which
allow for a fair comparison of the proposed methods. However, such benchmarks are available only for a handful of
languages. To alleviate this issue, we introduce a comprehensive multi-task benchmark for the Polish language
understanding, accompanied by an online leaderboard. It consists of a diverse set of tasks, adopted from existing
datasets for named entity recognition, question-answering, textual entailment, and others. We also introduce a new
sentiment analysis task for the e-commerce domain, named Allegro Reviews (AR). To ensure a common evaluation scheme and
promote models that generalize to different NLU tasks, the benchmark includes datasets from varying domains and
applications. Additionally, we release HerBERT, a Transformer-based model trained specifically for the Polish language,
which has the best average performance and obtains the best results for three out of nine tasks. Finally, we provide an
extensive evaluation, including several standard baselines and recently proposed, multilingual Transformer-based
models.`),gt=l(),Ee=r("p"),Gt=a("Examples of use:"),bt=l(),v(Y.$$.fragment),vt=l(),j=r("p"),Qt=a("This model was contributed by "),Z=r("a"),Yt=a("rmroczkowski"),Zt=a(`. The original code can be found
`),ee=r("a"),es=a("here"),ts=a("."),$t=l(),R=r("h2"),O=r("a"),Ne=r("span"),v(te.$$.fragment),ss=l(),Ce=r("span"),rs=a("HerbertTokenizer"),wt=l(),_=r("div"),v(se.$$.fragment),ns=l(),Oe=r("p"),os=a("Construct a BPE tokenizer for HerBERT."),as=l(),Ue=r("p"),is=a("Peculiarities:"),ls=l(),re=r("ul"),Se=r("li"),Xe=r("p"),cs=a(`uses BERT\u2019s pre-tokenizer: BaseTokenizer splits tokens on spaces, and also on punctuation. Each occurrence of a
punctuation character will be treated separately.`),ds=l(),Ve=r("li"),Je=r("p"),ps=a("Such pretokenized input is BPE subtokenized"),ms=l(),ne=r("p"),fs=a("This tokenizer inherits from "),Te=r("a"),hs=a("XLMTokenizer"),us=a(` which contains most of the methods. Users should refer to the
superclass for more information regarding methods.`),_s=l(),B=r("div"),v(oe.$$.fragment),ks=l(),Ke=r("p"),gs=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An XLM sequence has the following format:`),bs=l(),ae=r("ul"),ze=r("li"),vs=a("single sequence: "),We=r("code"),$s=a("<s> X </s>"),ws=l(),qe=r("li"),ys=a("pair of sequences: "),Ge=r("code"),Es=a("<s> A </s> B </s>"),Ts=l(),U=r("div"),v(ie.$$.fragment),zs=l(),Qe=r("p"),qs=a("Converts a sequence of tokens (string) in a single string."),Ls=l(),P=r("div"),v(le.$$.fragment),Hs=l(),Ye=r("p"),Ps=a("Create a mask from the two sequences passed to be used in a sequence-pair classification task. An XLM sequence"),js=l(),v(S.$$.fragment),Bs=l(),I=r("p"),xs=a("If "),Ze=r("code"),As=a("token_ids_1"),Ds=a(" is "),et=r("code"),Rs=a("None"),Is=a(", this method only returns the first portion of the mask (0s)."),Fs=l(),X=r("div"),v(ce.$$.fragment),Ms=l(),de=r("p"),Ns=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),tt=r("code"),Cs=a("prepare_for_model"),Os=a(" method."),yt=l(),F=r("h2"),V=r("a"),st=r("span"),v(pe.$$.fragment),Us=l(),rt=r("span"),Ss=a("HerbertTokenizerFast"),Et=l(),k=r("div"),v(me.$$.fragment),Xs=l(),fe=r("p"),Vs=a("Construct a \u201CFast\u201D BPE tokenizer for HerBERT (backed by HuggingFace\u2019s "),nt=r("em"),Js=a("tokenizers"),Ks=a(" library)."),Ws=l(),ot=r("p"),Gs=a("Peculiarities:"),Qs=l(),at=r("ul"),it=r("li"),Ys=a(`uses BERT\u2019s pre-tokenizer: BertPreTokenizer splits tokens on spaces, and also on punctuation. Each occurrence of
a punctuation character will be treated separately.`),Zs=l(),he=r("p"),er=a("This tokenizer inherits from "),Le=r("a"),tr=a("PreTrainedTokenizer"),sr=a(` which contains most of the methods. Users should refer to the
superclass for more information regarding methods.`),rr=l(),x=r("div"),v(ue.$$.fragment),nr=l(),lt=r("p"),or=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An HerBERT, like BERT sequence has the following format:`),ar=l(),_e=r("ul"),He=r("li"),ir=a("single sequence: "),ct=r("code"),lr=a("<s> X </s>"),cr=l(),Pe=r("li"),dr=a("pair of sequences: "),dt=r("code"),pr=a("<s> A </s> B </s>"),mr=l(),A=r("div"),v(ke.$$.fragment),fr=l(),pt=r("p"),hr=a("Create a mask from the two sequences passed to be used in a sequence-pair classification task. HerBERT, like"),ur=l(),v(J.$$.fragment),_r=l(),K=r("div"),v(ge.$$.fragment),kr=l(),be=r("p"),gr=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),mt=r("code"),br=a("prepare_for_model"),vr=a(" method."),this.h()},l(t){const m=$n('[data-svelte="svelte-1phssyn"]',document.head);u=n(m,"META",{name:!0,content:!0}),m.forEach(s),H=c(t),g=n(t,"H1",{class:!0});var ve=o(g);h=n(ve,"A",{id:!0,class:!0,href:!0});var ft=o(h);L=n(ft,"SPAN",{});var zr=o(L);$(d.$$.fragment,zr),zr.forEach(s),ft.forEach(s),z=c(ve),Re=n(ve,"SPAN",{});var qr=o(Re);Ot=i(qr,"HerBERT"),qr.forEach(s),ve.forEach(s),ht=c(t),D=n(t,"H2",{class:!0});var zt=o(D);N=n(zt,"A",{id:!0,class:!0,href:!0});var Lr=o(N);Ie=n(Lr,"SPAN",{});var Hr=o(Ie);$(G.$$.fragment,Hr),Hr.forEach(s),Lr.forEach(s),Ut=c(zt),Fe=n(zt,"SPAN",{});var Pr=o(Fe);St=i(Pr,"Overview"),Pr.forEach(s),zt.forEach(s),ut=c(t),C=n(t,"P",{});var qt=o(C);Xt=i(qt,"The HerBERT model was proposed in "),Q=n(qt,"A",{href:!0,rel:!0});var jr=o(Q);Vt=i(jr,"KLEJ: Comprehensive Benchmark for Polish Language Understanding"),jr.forEach(s),Jt=i(qt,` by Piotr Rybak, Robert Mroczkowski, Janusz Tracz, and
Ireneusz Gawlik. It is a BERT-based Language Model trained on Polish Corpora using only MLM objective with dynamic
masking of whole words.`),qt.forEach(s),_t=c(t),we=n(t,"P",{});var Br=o(we);Kt=i(Br,"The abstract from the paper is the following:"),Br.forEach(s),kt=c(t),ye=n(t,"P",{});var xr=o(ye);Me=n(xr,"EM",{});var Ar=o(Me);Wt=i(Ar,`In recent years, a series of Transformer-based models unlocked major improvements in general natural language
understanding (NLU) tasks. Such a fast pace of research would not be possible without general NLU benchmarks, which
allow for a fair comparison of the proposed methods. However, such benchmarks are available only for a handful of
languages. To alleviate this issue, we introduce a comprehensive multi-task benchmark for the Polish language
understanding, accompanied by an online leaderboard. It consists of a diverse set of tasks, adopted from existing
datasets for named entity recognition, question-answering, textual entailment, and others. We also introduce a new
sentiment analysis task for the e-commerce domain, named Allegro Reviews (AR). To ensure a common evaluation scheme and
promote models that generalize to different NLU tasks, the benchmark includes datasets from varying domains and
applications. Additionally, we release HerBERT, a Transformer-based model trained specifically for the Polish language,
which has the best average performance and obtains the best results for three out of nine tasks. Finally, we provide an
extensive evaluation, including several standard baselines and recently proposed, multilingual Transformer-based
models.`),Ar.forEach(s),xr.forEach(s),gt=c(t),Ee=n(t,"P",{});var Dr=o(Ee);Gt=i(Dr,"Examples of use:"),Dr.forEach(s),bt=c(t),$(Y.$$.fragment,t),vt=c(t),j=n(t,"P",{});var je=o(j);Qt=i(je,"This model was contributed by "),Z=n(je,"A",{href:!0,rel:!0});var Rr=o(Z);Yt=i(Rr,"rmroczkowski"),Rr.forEach(s),Zt=i(je,`. The original code can be found
`),ee=n(je,"A",{href:!0,rel:!0});var Ir=o(ee);es=i(Ir,"here"),Ir.forEach(s),ts=i(je,"."),je.forEach(s),$t=c(t),R=n(t,"H2",{class:!0});var Lt=o(R);O=n(Lt,"A",{id:!0,class:!0,href:!0});var Fr=o(O);Ne=n(Fr,"SPAN",{});var Mr=o(Ne);$(te.$$.fragment,Mr),Mr.forEach(s),Fr.forEach(s),ss=c(Lt),Ce=n(Lt,"SPAN",{});var Nr=o(Ce);rs=i(Nr,"HerbertTokenizer"),Nr.forEach(s),Lt.forEach(s),wt=c(t),_=n(t,"DIV",{class:!0});var b=o(_);$(se.$$.fragment,b),ns=c(b),Oe=n(b,"P",{});var Cr=o(Oe);os=i(Cr,"Construct a BPE tokenizer for HerBERT."),Cr.forEach(s),as=c(b),Ue=n(b,"P",{});var Or=o(Ue);is=i(Or,"Peculiarities:"),Or.forEach(s),ls=c(b),re=n(b,"UL",{});var Ht=o(re);Se=n(Ht,"LI",{});var Ur=o(Se);Xe=n(Ur,"P",{});var Sr=o(Xe);cs=i(Sr,`uses BERT\u2019s pre-tokenizer: BaseTokenizer splits tokens on spaces, and also on punctuation. Each occurrence of a
punctuation character will be treated separately.`),Sr.forEach(s),Ur.forEach(s),ds=c(Ht),Ve=n(Ht,"LI",{});var Xr=o(Ve);Je=n(Xr,"P",{});var Vr=o(Je);ps=i(Vr,"Such pretokenized input is BPE subtokenized"),Vr.forEach(s),Xr.forEach(s),Ht.forEach(s),ms=c(b),ne=n(b,"P",{});var Pt=o(ne);fs=i(Pt,"This tokenizer inherits from "),Te=n(Pt,"A",{href:!0});var Jr=o(Te);hs=i(Jr,"XLMTokenizer"),Jr.forEach(s),us=i(Pt,` which contains most of the methods. Users should refer to the
superclass for more information regarding methods.`),Pt.forEach(s),_s=c(b),B=n(b,"DIV",{class:!0});var Be=o(B);$(oe.$$.fragment,Be),ks=c(Be),Ke=n(Be,"P",{});var Kr=o(Ke);gs=i(Kr,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An XLM sequence has the following format:`),Kr.forEach(s),bs=c(Be),ae=n(Be,"UL",{});var jt=o(ae);ze=n(jt,"LI",{});var $r=o(ze);vs=i($r,"single sequence: "),We=n($r,"CODE",{});var Wr=o(We);$s=i(Wr,"<s> X </s>"),Wr.forEach(s),$r.forEach(s),ws=c(jt),qe=n(jt,"LI",{});var wr=o(qe);ys=i(wr,"pair of sequences: "),Ge=n(wr,"CODE",{});var Gr=o(Ge);Es=i(Gr,"<s> A </s> B </s>"),Gr.forEach(s),wr.forEach(s),jt.forEach(s),Be.forEach(s),Ts=c(b),U=n(b,"DIV",{class:!0});var Bt=o(U);$(ie.$$.fragment,Bt),zs=c(Bt),Qe=n(Bt,"P",{});var Qr=o(Qe);qs=i(Qr,"Converts a sequence of tokens (string) in a single string."),Qr.forEach(s),Bt.forEach(s),Ls=c(b),P=n(b,"DIV",{class:!0});var W=o(P);$(le.$$.fragment,W),Hs=c(W),Ye=n(W,"P",{});var Yr=o(Ye);Ps=i(Yr,"Create a mask from the two sequences passed to be used in a sequence-pair classification task. An XLM sequence"),Yr.forEach(s),js=c(W),$(S.$$.fragment,W),Bs=c(W),I=n(W,"P",{});var xe=o(I);xs=i(xe,"If "),Ze=n(xe,"CODE",{});var Zr=o(Ze);As=i(Zr,"token_ids_1"),Zr.forEach(s),Ds=i(xe," is "),et=n(xe,"CODE",{});var en=o(et);Rs=i(en,"None"),en.forEach(s),Is=i(xe,", this method only returns the first portion of the mask (0s)."),xe.forEach(s),W.forEach(s),Fs=c(b),X=n(b,"DIV",{class:!0});var xt=o(X);$(ce.$$.fragment,xt),Ms=c(xt),de=n(xt,"P",{});var At=o(de);Ns=i(At,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),tt=n(At,"CODE",{});var tn=o(tt);Cs=i(tn,"prepare_for_model"),tn.forEach(s),Os=i(At," method."),At.forEach(s),xt.forEach(s),b.forEach(s),yt=c(t),F=n(t,"H2",{class:!0});var Dt=o(F);V=n(Dt,"A",{id:!0,class:!0,href:!0});var sn=o(V);st=n(sn,"SPAN",{});var rn=o(st);$(pe.$$.fragment,rn),rn.forEach(s),sn.forEach(s),Us=c(Dt),rt=n(Dt,"SPAN",{});var nn=o(rt);Ss=i(nn,"HerbertTokenizerFast"),nn.forEach(s),Dt.forEach(s),Et=c(t),k=n(t,"DIV",{class:!0});var q=o(k);$(me.$$.fragment,q),Xs=c(q),fe=n(q,"P",{});var Rt=o(fe);Vs=i(Rt,"Construct a \u201CFast\u201D BPE tokenizer for HerBERT (backed by HuggingFace\u2019s "),nt=n(Rt,"EM",{});var on=o(nt);Js=i(on,"tokenizers"),on.forEach(s),Ks=i(Rt," library)."),Rt.forEach(s),Ws=c(q),ot=n(q,"P",{});var an=o(ot);Gs=i(an,"Peculiarities:"),an.forEach(s),Qs=c(q),at=n(q,"UL",{});var ln=o(at);it=n(ln,"LI",{});var cn=o(it);Ys=i(cn,`uses BERT\u2019s pre-tokenizer: BertPreTokenizer splits tokens on spaces, and also on punctuation. Each occurrence of
a punctuation character will be treated separately.`),cn.forEach(s),ln.forEach(s),Zs=c(q),he=n(q,"P",{});var It=o(he);er=i(It,"This tokenizer inherits from "),Le=n(It,"A",{href:!0});var dn=o(Le);tr=i(dn,"PreTrainedTokenizer"),dn.forEach(s),sr=i(It,` which contains most of the methods. Users should refer to the
superclass for more information regarding methods.`),It.forEach(s),rr=c(q),x=n(q,"DIV",{class:!0});var Ae=o(x);$(ue.$$.fragment,Ae),nr=c(Ae),lt=n(Ae,"P",{});var pn=o(lt);or=i(pn,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An HerBERT, like BERT sequence has the following format:`),pn.forEach(s),ar=c(Ae),_e=n(Ae,"UL",{});var Ft=o(_e);He=n(Ft,"LI",{});var yr=o(He);ir=i(yr,"single sequence: "),ct=n(yr,"CODE",{});var mn=o(ct);lr=i(mn,"<s> X </s>"),mn.forEach(s),yr.forEach(s),cr=c(Ft),Pe=n(Ft,"LI",{});var Er=o(Pe);dr=i(Er,"pair of sequences: "),dt=n(Er,"CODE",{});var fn=o(dt);pr=i(fn,"<s> A </s> B </s>"),fn.forEach(s),Er.forEach(s),Ft.forEach(s),Ae.forEach(s),mr=c(q),A=n(q,"DIV",{class:!0});var De=o(A);$(ke.$$.fragment,De),fr=c(De),pt=n(De,"P",{});var hn=o(pt);hr=i(hn,"Create a mask from the two sequences passed to be used in a sequence-pair classification task. HerBERT, like"),hn.forEach(s),ur=c(De),$(J.$$.fragment,De),De.forEach(s),_r=c(q),K=n(q,"DIV",{class:!0});var Mt=o(K);$(ge.$$.fragment,Mt),kr=c(Mt),be=n(Mt,"P",{});var Nt=o(be);gr=i(Nt,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),mt=n(Nt,"CODE",{});var un=o(mt);br=i(un,"prepare_for_model"),un.forEach(s),vr=i(Nt," method."),Nt.forEach(s),Mt.forEach(s),q.forEach(s),this.h()},h(){p(u,"name","hf:doc:metadata"),p(u,"content",JSON.stringify(zn)),p(h,"id","herbert"),p(h,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(h,"href","#herbert"),p(g,"class","relative group"),p(N,"id","overview"),p(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(N,"href","#overview"),p(D,"class","relative group"),p(Q,"href","https://www.aclweb.org/anthology/2020.acl-main.111.pdf"),p(Q,"rel","nofollow"),p(Z,"href","https://huggingface.co/rmroczkowski"),p(Z,"rel","nofollow"),p(ee,"href","https://github.com/allegro/HerBERT"),p(ee,"rel","nofollow"),p(O,"id","transformers.HerbertTokenizer"),p(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(O,"href","#transformers.HerbertTokenizer"),p(R,"class","relative group"),p(Te,"href","/docs/transformers/main/en/model_doc/xlm#transformers.XLMTokenizer"),p(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(_,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(V,"id","transformers.HerbertTokenizerFast"),p(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(V,"href","#transformers.HerbertTokenizerFast"),p(F,"class","relative group"),p(Le,"href","/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),p(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,m){e(document.head,u),f(t,H,m),f(t,g,m),e(g,h),e(h,L),w(d,L,null),e(g,z),e(g,Re),e(Re,Ot),f(t,ht,m),f(t,D,m),e(D,N),e(N,Ie),w(G,Ie,null),e(D,Ut),e(D,Fe),e(Fe,St),f(t,ut,m),f(t,C,m),e(C,Xt),e(C,Q),e(Q,Vt),e(C,Jt),f(t,_t,m),f(t,we,m),e(we,Kt),f(t,kt,m),f(t,ye,m),e(ye,Me),e(Me,Wt),f(t,gt,m),f(t,Ee,m),e(Ee,Gt),f(t,bt,m),w(Y,t,m),f(t,vt,m),f(t,j,m),e(j,Qt),e(j,Z),e(Z,Yt),e(j,Zt),e(j,ee),e(ee,es),e(j,ts),f(t,$t,m),f(t,R,m),e(R,O),e(O,Ne),w(te,Ne,null),e(R,ss),e(R,Ce),e(Ce,rs),f(t,wt,m),f(t,_,m),w(se,_,null),e(_,ns),e(_,Oe),e(Oe,os),e(_,as),e(_,Ue),e(Ue,is),e(_,ls),e(_,re),e(re,Se),e(Se,Xe),e(Xe,cs),e(re,ds),e(re,Ve),e(Ve,Je),e(Je,ps),e(_,ms),e(_,ne),e(ne,fs),e(ne,Te),e(Te,hs),e(ne,us),e(_,_s),e(_,B),w(oe,B,null),e(B,ks),e(B,Ke),e(Ke,gs),e(B,bs),e(B,ae),e(ae,ze),e(ze,vs),e(ze,We),e(We,$s),e(ae,ws),e(ae,qe),e(qe,ys),e(qe,Ge),e(Ge,Es),e(_,Ts),e(_,U),w(ie,U,null),e(U,zs),e(U,Qe),e(Qe,qs),e(_,Ls),e(_,P),w(le,P,null),e(P,Hs),e(P,Ye),e(Ye,Ps),e(P,js),w(S,P,null),e(P,Bs),e(P,I),e(I,xs),e(I,Ze),e(Ze,As),e(I,Ds),e(I,et),e(et,Rs),e(I,Is),e(_,Fs),e(_,X),w(ce,X,null),e(X,Ms),e(X,de),e(de,Ns),e(de,tt),e(tt,Cs),e(de,Os),f(t,yt,m),f(t,F,m),e(F,V),e(V,st),w(pe,st,null),e(F,Us),e(F,rt),e(rt,Ss),f(t,Et,m),f(t,k,m),w(me,k,null),e(k,Xs),e(k,fe),e(fe,Vs),e(fe,nt),e(nt,Js),e(fe,Ks),e(k,Ws),e(k,ot),e(ot,Gs),e(k,Qs),e(k,at),e(at,it),e(it,Ys),e(k,Zs),e(k,he),e(he,er),e(he,Le),e(Le,tr),e(he,sr),e(k,rr),e(k,x),w(ue,x,null),e(x,nr),e(x,lt),e(lt,or),e(x,ar),e(x,_e),e(_e,He),e(He,ir),e(He,ct),e(ct,lr),e(_e,cr),e(_e,Pe),e(Pe,dr),e(Pe,dt),e(dt,pr),e(k,mr),e(k,A),w(ke,A,null),e(A,fr),e(A,pt),e(pt,hr),e(A,ur),w(J,A,null),e(k,_r),e(k,K),w(ge,K,null),e(K,kr),e(K,be),e(be,gr),e(be,mt),e(mt,br),e(be,vr),Tt=!0},p(t,[m]){const ve={};m&2&&(ve.$$scope={dirty:m,ctx:t}),S.$set(ve);const ft={};m&2&&(ft.$$scope={dirty:m,ctx:t}),J.$set(ft)},i(t){Tt||(y(d.$$.fragment,t),y(G.$$.fragment,t),y(Y.$$.fragment,t),y(te.$$.fragment,t),y(se.$$.fragment,t),y(oe.$$.fragment,t),y(ie.$$.fragment,t),y(le.$$.fragment,t),y(S.$$.fragment,t),y(ce.$$.fragment,t),y(pe.$$.fragment,t),y(me.$$.fragment,t),y(ue.$$.fragment,t),y(ke.$$.fragment,t),y(J.$$.fragment,t),y(ge.$$.fragment,t),Tt=!0)},o(t){E(d.$$.fragment,t),E(G.$$.fragment,t),E(Y.$$.fragment,t),E(te.$$.fragment,t),E(se.$$.fragment,t),E(oe.$$.fragment,t),E(ie.$$.fragment,t),E(le.$$.fragment,t),E(S.$$.fragment,t),E(ce.$$.fragment,t),E(pe.$$.fragment,t),E(me.$$.fragment,t),E(ue.$$.fragment,t),E(ke.$$.fragment,t),E(J.$$.fragment,t),E(ge.$$.fragment,t),Tt=!1},d(t){s(u),t&&s(H),t&&s(g),T(d),t&&s(ht),t&&s(D),T(G),t&&s(ut),t&&s(C),t&&s(_t),t&&s(we),t&&s(kt),t&&s(ye),t&&s(gt),t&&s(Ee),t&&s(bt),T(Y,t),t&&s(vt),t&&s(j),t&&s($t),t&&s(R),T(te),t&&s(wt),t&&s(_),T(se),T(oe),T(ie),T(le),T(S),T(ce),t&&s(yt),t&&s(F),T(pe),t&&s(Et),t&&s(k),T(me),T(ue),T(ke),T(J),T(ge)}}}const zn={local:"herbert",sections:[{local:"overview",title:"Overview"},{local:"transformers.HerbertTokenizer",title:"HerbertTokenizer"},{local:"transformers.HerbertTokenizerFast",title:"HerbertTokenizerFast"}],title:"HerBERT"};function qn($e){return wn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class xn extends gn{constructor(u){super();bn(this,u,qn,Tn,vn,{})}}export{xn as default,zn as metadata};
