import{S as Zf,i as Xf,s as Qf,e as t,k as m,w as b,t as s,M as ep,c as n,d as a,m as f,a as r,x as k,h as i,b as d,G as e,g as _,y as F,q as y,o as $,B as T,v as op,L as xo}from"../../chunks/vendor-hf-doc-builder.js";import{T as at}from"../../chunks/Tip-hf-doc-builder.js";import{D as M}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Mo}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as j}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as wo}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function ap(x){let c,w,u,g,v;return g=new Mo({props:{code:`from transformers import FlavaConfig, FlavaModel, FlavaForPreTraining

# Initializing a FlavaConfig with style configuration
configuration = FlavaConfig()

# Initializing a FlavaModel and FlavaForPreTraining model (with random weights) from the style configuration
model = FlavaModel(configuration)
model_pre = FlavaForPreTraining(configuration)

# Accessing the model configuration
configuration = model.config
configuration_pre = model_pre.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaConfig, FlavaModel, FlavaForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaConfig with style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FlavaConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaModel and FlavaForPreTraining model (with random weights) from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaModel(configuration)
<span class="hljs-meta">&gt;&gt;&gt; </span>model_pre = FlavaForPreTraining(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration_pre = model_pre.config`}}),{c(){c=t("p"),w=s("Example:"),u=m(),b(g.$$.fragment)},l(l){c=n(l,"P",{});var p=r(c);w=i(p,"Example:"),p.forEach(a),u=f(l),k(g.$$.fragment,l)},m(l,p){_(l,c,p),e(c,w),_(l,u,p),F(g,l,p),v=!0},p:xo,i(l){v||(y(g.$$.fragment,l),v=!0)},o(l){$(g.$$.fragment,l),v=!1},d(l){l&&a(c),l&&a(u),T(g,l)}}}function tp(x){let c,w,u,g,v;return g=new Mo({props:{code:`from transformers import FlavaTextConfig, FlavaTextModel

# Initializing a FlavaTextModel with  style configuration
configuration = FlavaTextConfig()

# Initializing a FlavaTextModel model (with random weights) from the style configuration
model = FlavaTextModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaTextConfig, FlavaTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaTextModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FlavaTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaTextModel model (with random weights) from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){c=t("p"),w=s("Example:"),u=m(),b(g.$$.fragment)},l(l){c=n(l,"P",{});var p=r(c);w=i(p,"Example:"),p.forEach(a),u=f(l),k(g.$$.fragment,l)},m(l,p){_(l,c,p),e(c,w),_(l,u,p),F(g,l,p),v=!0},p:xo,i(l){v||(y(g.$$.fragment,l),v=!0)},o(l){$(g.$$.fragment,l),v=!1},d(l){l&&a(c),l&&a(u),T(g,l)}}}function np(x){let c,w,u,g,v;return g=new Mo({props:{code:`from transformers import FlavaImageConfig, FlavaImageModel

# Initializing a FlavaImageModel with  style configuration
configuration = FlavaImageConfig()

# Initializing a FlavaImageModel model (with random weights) from the style configuration
model = FlavaImageModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaImageConfig, FlavaImageModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaImageModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FlavaImageConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaImageModel model (with random weights) from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaImageModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){c=t("p"),w=s("Example:"),u=m(),b(g.$$.fragment)},l(l){c=n(l,"P",{});var p=r(c);w=i(p,"Example:"),p.forEach(a),u=f(l),k(g.$$.fragment,l)},m(l,p){_(l,c,p),e(c,w),_(l,u,p),F(g,l,p),v=!0},p:xo,i(l){v||(y(g.$$.fragment,l),v=!0)},o(l){$(g.$$.fragment,l),v=!1},d(l){l&&a(c),l&&a(u),T(g,l)}}}function rp(x){let c,w,u,g,v;return g=new Mo({props:{code:`from transformers import FlavaMultimodalConfig, FlavaMultimodalModel

# Initializing a FlavaMultimodalModel with  style configuration
configuration = FlavaMultimodalConfig()

# Initializing a FlavaMultimodalModel model (with random weights) from the style configuration
model = FlavaMultimodalModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaMultimodalConfig, FlavaMultimodalModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaMultimodalModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FlavaMultimodalConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaMultimodalModel model (with random weights) from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaMultimodalModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){c=t("p"),w=s("Example:"),u=m(),b(g.$$.fragment)},l(l){c=n(l,"P",{});var p=r(c);w=i(p,"Example:"),p.forEach(a),u=f(l),k(g.$$.fragment,l)},m(l,p){_(l,c,p),e(c,w),_(l,u,p),F(g,l,p),v=!0},p:xo,i(l){v||(y(g.$$.fragment,l),v=!0)},o(l){$(g.$$.fragment,l),v=!1},d(l){l&&a(c),l&&a(u),T(g,l)}}}function sp(x){let c,w,u,g,v;return{c(){c=t("p"),w=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=t("code"),g=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){c=n(l,"P",{});var p=r(c);w=i(p,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(p,"CODE",{});var z=r(u);g=i(z,"Module"),z.forEach(a),v=i(p,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),p.forEach(a)},m(l,p){_(l,c,p),e(c,w),e(c,u),e(u,g),e(c,v)},d(l){l&&a(c)}}}function ip(x){let c,w,u,g,v;return{c(){c=t("p"),w=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=t("code"),g=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){c=n(l,"P",{});var p=r(c);w=i(p,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(p,"CODE",{});var z=r(u);g=i(z,"Module"),z.forEach(a),v=i(p,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),p.forEach(a)},m(l,p){_(l,c,p),e(c,w),e(c,u),e(u,g),e(c,v)},d(l){l&&a(c)}}}function lp(x){let c,w,u,g,v;return g=new Mo({props:{code:`from PIL import Image
import requests
from transformers import FlavaProcessor, FlavaModel

model = FlavaModel.from_pretrained("facebook/flava-full")
processor = FlavaProcessor.from_pretrained("facebook/flava-full")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=["a photo of a cat"], images=image, return_tensors="pt", padding=True)

outputs = model(**inputs)
logits_per_image = outputs.contrastive_logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaProcessor, FlavaModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaModel.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = FlavaProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.contrastive_logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`}}),{c(){c=t("p"),w=s("Examples:"),u=m(),b(g.$$.fragment)},l(l){c=n(l,"P",{});var p=r(c);w=i(p,"Examples:"),p.forEach(a),u=f(l),k(g.$$.fragment,l)},m(l,p){_(l,c,p),e(c,w),_(l,u,p),F(g,l,p),v=!0},p:xo,i(l){v||(y(g.$$.fragment,l),v=!0)},o(l){$(g.$$.fragment,l),v=!1},d(l){l&&a(c),l&&a(u),T(g,l)}}}function dp(x){let c,w,u,g,v;return{c(){c=t("p"),w=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=t("code"),g=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){c=n(l,"P",{});var p=r(c);w=i(p,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(p,"CODE",{});var z=r(u);g=i(z,"Module"),z.forEach(a),v=i(p,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),p.forEach(a)},m(l,p){_(l,c,p),e(c,w),e(c,u),e(u,g),e(c,v)},d(l){l&&a(c)}}}function cp(x){let c,w,u,g,v;return{c(){c=t("p"),w=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=t("code"),g=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){c=n(l,"P",{});var p=r(c);w=i(p,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(p,"CODE",{});var z=r(u);g=i(z,"Module"),z.forEach(a),v=i(p,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),p.forEach(a)},m(l,p){_(l,c,p),e(c,w),e(c,u),e(u,g),e(c,v)},d(l){l&&a(c)}}}function mp(x){let c,w,u,g,v;return{c(){c=t("p"),w=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=t("code"),g=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){c=n(l,"P",{});var p=r(c);w=i(p,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(p,"CODE",{});var z=r(u);g=i(z,"Module"),z.forEach(a),v=i(p,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),p.forEach(a)},m(l,p){_(l,c,p),e(c,w),e(c,u),e(u,g),e(c,v)},d(l){l&&a(c)}}}function fp(x){let c,w,u,g,v;return g=new Mo({props:{code:`from transformers import BertTokenizer, FlavaTextModel
import torch

tokenizer = BertTokenizer.from_pretrained("facebook/flava-full")
model = FlavaTextModel.from_pretrained("facebook/flava-full")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, FlavaTextModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaTextModel.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){c=t("p"),w=s("Example:"),u=m(),b(g.$$.fragment)},l(l){c=n(l,"P",{});var p=r(c);w=i(p,"Example:"),p.forEach(a),u=f(l),k(g.$$.fragment,l)},m(l,p){_(l,c,p),e(c,w),_(l,u,p),F(g,l,p),v=!0},p:xo,i(l){v||(y(g.$$.fragment,l),v=!0)},o(l){$(g.$$.fragment,l),v=!1},d(l){l&&a(c),l&&a(u),T(g,l)}}}function pp(x){let c,w,u,g,v;return{c(){c=t("p"),w=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=t("code"),g=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){c=n(l,"P",{});var p=r(c);w=i(p,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(p,"CODE",{});var z=r(u);g=i(z,"Module"),z.forEach(a),v=i(p,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),p.forEach(a)},m(l,p){_(l,c,p),e(c,w),e(c,u),e(u,g),e(c,v)},d(l){l&&a(c)}}}function gp(x){let c,w,u,g,v;return g=new Mo({props:{code:`from transformers import FlavaFeatureExtractor, FlavaImageModel
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

feature_extractor = FlavaFeatureExtractor.from_pretrained("facebook/flava-full")
model = FlavaImageModel.from_pretrained("facebook/flava-full")

inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
list(last_hidden_states.shape)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaFeatureExtractor, FlavaImageModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = FlavaFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaImageModel.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">197</span>, <span class="hljs-number">768</span>]`}}),{c(){c=t("p"),w=s("Example:"),u=m(),b(g.$$.fragment)},l(l){c=n(l,"P",{});var p=r(c);w=i(p,"Example:"),p.forEach(a),u=f(l),k(g.$$.fragment,l)},m(l,p){_(l,c,p),e(c,w),_(l,u,p),F(g,l,p),v=!0},p:xo,i(l){v||(y(g.$$.fragment,l),v=!0)},o(l){$(g.$$.fragment,l),v=!1},d(l){l&&a(c),l&&a(u),T(g,l)}}}function hp(x){let c,w,u,g,v;return{c(){c=t("p"),w=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=t("code"),g=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){c=n(l,"P",{});var p=r(c);w=i(p,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n(p,"CODE",{});var z=r(u);g=i(z,"Module"),z.forEach(a),v=i(p,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),p.forEach(a)},m(l,p){_(l,c,p),e(c,w),e(c,u),e(u,g),e(c,v)},d(l){l&&a(c)}}}function up(x){let c,w,u,g,v;return g=new Mo({props:{code:`from transformers import BertTokenizer, FlavaMultimodalModel
import torch

tokenizer = BertTokenizer.from_pretrained("facebook/flava-full")
model = FlavaMultimodalModel.from_pretrained("facebook/flava-full")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, FlavaMultimodalModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaMultimodalModel.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){c=t("p"),w=s("Example:"),u=m(),b(g.$$.fragment)},l(l){c=n(l,"P",{});var p=r(c);w=i(p,"Example:"),p.forEach(a),u=f(l),k(g.$$.fragment,l)},m(l,p){_(l,c,p),e(c,w),_(l,u,p),F(g,l,p),v=!0},p:xo,i(l){v||(y(g.$$.fragment,l),v=!0)},o(l){$(g.$$.fragment,l),v=!1},d(l){l&&a(c),l&&a(u),T(g,l)}}}function _p(x){let c,w,u,g,v,l,p,z,ws,sr,ie,Ne,on,zo,xs,an,Ms,ir,qe,zs,Io,Is,Ps,lr,tt,Cs,dr,nt,Es,cr,rt,tn,As,mr,Z,Ns,Po,qs,Ls,Co,js,Os,fr,le,Le,nn,Eo,Ds,rn,Ws,pr,P,Ao,Ss,X,st,Vs,Bs,it,Rs,Us,No,Hs,Gs,Js,de,Ks,lt,Ys,Zs,dt,Xs,Qs,ei,je,oi,Oe,qo,ai,Lo,ti,ct,ni,ri,si,De,jo,ii,Oo,li,mt,di,ci,gr,ce,We,sn,Do,mi,ln,fi,hr,E,Wo,pi,So,gi,ft,hi,ui,_i,Vo,vi,Bo,bi,ki,Fi,me,yi,pt,$i,Ti,gt,wi,xi,Mi,Se,ur,fe,Ve,dn,Ro,zi,cn,Ii,_r,A,Uo,Pi,Ho,Ci,ht,Ei,Ai,Ni,Go,qi,Jo,Li,ji,Oi,pe,Di,ut,Wi,Si,_t,Vi,Bi,Ri,Be,vr,ge,Re,mn,Ko,Ui,fn,Hi,br,N,Yo,Gi,Zo,Ji,vt,Ki,Yi,Zi,Xo,Xi,Qo,Qi,el,ol,he,al,bt,tl,nl,kt,rl,sl,il,Ue,kr,ue,He,pn,ea,ll,gn,dl,Fr,oa,aa,yr,_e,Ge,hn,ta,cl,un,ml,$r,q,na,fl,_n,pl,gl,O,Ft,hl,ul,yt,_l,vl,$t,bl,kl,vn,Fl,yl,Tt,$l,Tl,wl,Je,ra,xl,sa,Ml,wt,zl,Il,Pl,Ke,ia,Cl,la,El,xt,Al,Nl,Tr,ve,Ye,bn,da,ql,kn,Ll,wr,I,ca,jl,Fn,Ol,Dl,Ze,ma,Wl,be,Sl,yn,Vl,Bl,$n,Rl,Ul,Hl,Xe,fa,Gl,Tn,Jl,Kl,Qe,pa,Yl,wn,Zl,Xl,eo,ga,Ql,xn,ed,od,oo,ha,ad,ua,td,Mn,nd,rd,xr,ke,ao,zn,_a,sd,In,id,Mr,W,va,ld,Pn,dd,cd,ba,md,ka,fd,pd,gd,Q,Fa,hd,Fe,ud,Mt,_d,vd,Cn,bd,kd,Fd,to,zr,ye,no,En,ya,yd,An,$d,Ir,L,$a,Td,Ta,wd,wa,xd,Md,zd,S,xa,Id,$e,Pd,zt,Cd,Ed,Nn,Ad,Nd,qd,ro,Ld,so,jd,ee,Ma,Od,Te,Dd,It,Wd,Sd,qn,Vd,Bd,Rd,io,Ud,oe,za,Hd,we,Gd,Pt,Jd,Kd,Ln,Yd,Zd,Xd,lo,Pr,xe,co,jn,Ia,Qd,On,ec,Cr,C,Pa,oc,Ca,ac,Dn,tc,nc,rc,Ea,sc,Aa,ic,lc,dc,Ct,Na,cc,Et,qa,mc,At,La,Er,Me,mo,Wn,ja,fc,Sn,pc,Ar,G,Oa,gc,Da,hc,Wa,uc,_c,vc,V,Sa,bc,ze,kc,Nt,Fc,yc,Vn,$c,Tc,wc,fo,xc,po,Nr,Ie,go,Bn,Va,Mc,Rn,zc,qr,J,Ba,Ic,Ra,Pc,Ua,Cc,Ec,Ac,B,Ha,Nc,Pe,qc,qt,Lc,jc,Un,Oc,Dc,Wc,ho,Sc,uo,Lr,Ce,_o,Hn,Ga,Vc,Gn,Bc,jr,K,Ja,Rc,Ka,Uc,Ya,Hc,Gc,Jc,R,Za,Kc,Ee,Yc,Lt,Zc,Xc,Jn,Qc,em,om,vo,am,bo,Or;return l=new j({}),zo=new j({}),Eo=new j({}),Ao=new M({props:{name:"class transformers.FlavaConfig",anchor:"transformers.FlavaConfig",parameters:[{name:"image_config",val:": typing.Dict[str, typing.Any] = None"},{name:"text_config",val:": typing.Dict[str, typing.Any] = None"},{name:"multimodal_config",val:": typing.Dict[str, typing.Any] = None"},{name:"image_codebook_config",val:": typing.Dict[str, typing.Any] = None"},{name:"hidden_size",val:": int = 768"},{name:"layer_norm_eps",val:": float = 1e-12"},{name:"projection_dim",val:": int = 768"},{name:"init_codebook",val:": bool = True"},{name:"logit_scale_init_value",val:": float = 2.6592"},{name:"initializer_range",val:": float = 0.02"},{name:"ce_ignore_index",val:": int = -100"},{name:"mim_weight",val:": float = 1.0"},{name:"mlm_weight",val:": float = 1.0"},{name:"global_contrastive_weight",val:": float = 1.0"},{name:"itm_weight",val:": float = 1.0"},{name:"mmm_image_weight",val:": float = 1.0"},{name:"mmm_text_weight",val:": float = 1.0"},{name:"global_backprop_contrastive",val:": bool = True"},{name:"skip_unmasked_multimodal_encoder",val:": bool = True"},{name:"return_loss",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaConfig.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextConfig">FlavaTextConfig</a>.`,name:"text_config"},{anchor:"transformers.FlavaConfig.image_config",description:`<strong>image_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageConfig">FlavaImageConfig</a>.`,name:"image_config"},{anchor:"transformers.FlavaConfig.multimodal_config",description:`<strong>multimodal_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalConfig">FlavaMultimodalConfig</a>.`,name:"multimodal_config"},{anchor:"transformers.FlavaConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FlavaConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FlavaConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimentionality of text and image projection layers.`,name:"projection_dim"},{anchor:"transformers.FlavaConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> paramter. Default is used as per the original FLAVA/CLIP
implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.FlavaConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FlavaConfig.ce_ignore_index",description:`<strong>ce_ignore_index</strong> (<code>int</code>, <em>optional</em>, defaults to -100) &#x2014;
Cross entropy index to ignore.`,name:"ce_ignore_index"},{anchor:"transformers.FlavaConfig.mim_weight",description:`<strong>mim_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MIM (Masked Image Modeling) unimodal loss`,name:"mim_weight"},{anchor:"transformers.FlavaConfig.mlm_weight",description:`<strong>mlm_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MLM (Masked Language Modeling) unimodal loss`,name:"mlm_weight"},{anchor:"transformers.FlavaConfig.global_contrastive_weight",description:`<strong>global_contrastive_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to global contrastive cross-alignment loss.`,name:"global_contrastive_weight"},{anchor:"transformers.FlavaConfig.itm_weight",description:`<strong>itm_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to image-text matching multimodal loss.`,name:"itm_weight"},{anchor:"transformers.FlavaConfig.mmm_image_weight",description:`<strong>mmm_image_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MMM loss&#x2019;s image part.`,name:"mmm_image_weight"},{anchor:"transformers.FlavaConfig.mmm_text_weight",description:`<strong>mmm_text_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MMM loss&#x2019;s text part.`,name:"mmm_text_weight"},{anchor:"transformers.FlavaConfig.global_backprop_contrastive",description:`<strong>global_backprop_contrastive</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use global backpropgation through all workers in contrastive loss.`,name:"global_backprop_contrastive"},{anchor:"transformers.FlavaConfig.skip_unmasked_multimodal_encoder",description:`<strong>skip_unmasked_multimodal_encoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to skip running unmasked multimodal encoder whose outputs are not used by FLAVA losses.`,name:"skip_unmasked_multimodal_encoder"},{anchor:"transformers.FlavaConfig.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to return loss or not`,name:"return_loss"},{anchor:"transformers.FlavaConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/configuration_flava.py#L463"}}),je=new wo({props:{anchor:"transformers.FlavaConfig.example",$$slots:{default:[ap]},$$scope:{ctx:x}}}),qo=new M({props:{name:"from_configs",anchor:"transformers.FlavaConfig.from_configs",parameters:[{name:"image_config",val:": FlavaImageConfig"},{name:"text_config",val:": FlavaTextConfig"},{name:"multimodal_config",val:": FlavaMultimodalConfig"},{name:"image_codebook_config",val:": FlavaImageCodebookConfig"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/configuration_flava.py#L617",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig"
>FlavaConfig</a></p>
`}}),jo=new M({props:{name:"to_dict",anchor:"transformers.FlavaConfig.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/configuration_flava.py#L642",returnDescription:`
<p>Dictionary of all the attributes that make up this configuration instance,</p>
`,returnType:`
<p><code>Dict[str, any]</code></p>
`}}),Do=new j({}),Wo=new M({props:{name:"class transformers.FlavaTextConfig",anchor:"transformers.FlavaTextConfig",parameters:[{name:"vocab_size",val:": int = 30522"},{name:"type_vocab_size",val:": int = 2"},{name:"max_position_embeddings",val:": int = 512"},{name:"position_embedding_type",val:": str = 'absolute'"},{name:"hidden_size",val:": int = 768"},{name:"num_hidden_layers",val:": int = 12"},{name:"num_attention_heads",val:": int = 12"},{name:"intermediate_size",val:": int = 3072"},{name:"hidden_act",val:": str = 'gelu'"},{name:"hidden_dropout_prob",val:": float = 0.0"},{name:"attention_probs_dropout_prob",val:": float = 0.0"},{name:"initializer_range",val:": float = 0.02"},{name:"layer_norm_eps",val:": float = 1e-12"},{name:"pad_token_id",val:": int = 0"},{name:"qkv_bias",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel">FlavaTextModel</a>.`,name:"vocab_size"},{anchor:"transformers.FlavaTextConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel">FlavaTextModel</a>. Note that even though
text encoder allows <code>token_type_ids</code>&#x2019;s value as 2, for text-only pretraining and fine-tuning, only 1 is
used similar to RoBERTa.`,name:"type_vocab_size"},{anchor:"transformers.FlavaTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048). For VL, max_length passed to model is 77.`,name:"max_position_embeddings"},{anchor:"transformers.FlavaTextConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.FlavaTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FlavaTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FlavaTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FlavaTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FlavaTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FlavaTextConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FlavaTextConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FlavaTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FlavaTextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FlavaTextConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.FlavaTextConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.FlavaTextConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.FlavaTextConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/configuration_flava.py#L150"}}),Se=new wo({props:{anchor:"transformers.FlavaTextConfig.example",$$slots:{default:[tp]},$$scope:{ctx:x}}}),Ro=new j({}),Uo=new M({props:{name:"class transformers.FlavaImageConfig",anchor:"transformers.FlavaImageConfig",parameters:[{name:"hidden_size",val:": int = 768"},{name:"num_hidden_layers",val:": int = 12"},{name:"num_attention_heads",val:": int = 12"},{name:"intermediate_size",val:": int = 3072"},{name:"hidden_act",val:": int = 'gelu'"},{name:"hidden_dropout_prob",val:": float = 0.0"},{name:"attention_probs_dropout_prob",val:": float = 0.0"},{name:"initializer_range",val:": float = 0.02"},{name:"layer_norm_eps",val:": float = 1e-12"},{name:"image_size",val:": int = 224"},{name:"patch_size",val:": int = 16"},{name:"num_channels",val:": int = 3"},{name:"qkv_bias",val:": bool = True"},{name:"mask_token",val:": bool = True"},{name:"vocab_size",val:": int = 8192"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaImageConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FlavaImageConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FlavaImageConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FlavaImageConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FlavaImageConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FlavaImageConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FlavaImageConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FlavaImageConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FlavaImageConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FlavaImageConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.FlavaImageConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.FlavaImageConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.FlavaImageConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.FlavaImageConfig.mask_token",description:`<strong>mask_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use a mask token or not. Used in MIM (Masked Image Modeling) loss for FLAVA.`,name:"mask_token"},{anchor:"transformers.FlavaImageConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8192) &#x2014;
Vocabulary size of the <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageCodebook">FlavaImageCodebook</a> used in conjunction with <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel">FlavaImageModel</a> for MIM (Masked
Image Modeling) loss for FLAVA.`,name:"vocab_size"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/configuration_flava.py#L32"}}),Be=new wo({props:{anchor:"transformers.FlavaImageConfig.example",$$slots:{default:[np]},$$scope:{ctx:x}}}),Ko=new j({}),Yo=new M({props:{name:"class transformers.FlavaMultimodalConfig",anchor:"transformers.FlavaMultimodalConfig",parameters:[{name:"hidden_size",val:": int = 768"},{name:"num_hidden_layers",val:": int = 6"},{name:"num_attention_heads",val:": int = 12"},{name:"intermediate_size",val:": int = 3072"},{name:"hidden_act",val:": int = 'gelu'"},{name:"hidden_dropout_prob",val:": int = 0.0"},{name:"attention_probs_dropout_prob",val:": int = 0.0"},{name:"initializer_range",val:": float = 0.02"},{name:"layer_norm_eps",val:": float = 1e-12"},{name:"qkv_bias",val:": bool = True"},{name:"use_cls_token",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaMultimodalConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FlavaMultimodalConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FlavaMultimodalConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FlavaMultimodalConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FlavaMultimodalConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FlavaMultimodalConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FlavaMultimodalConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FlavaMultimodalConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FlavaMultimodalConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FlavaMultimodalConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.FlavaMultimodalConfig.use_cls_token",description:`<strong>use_cls_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use an extra CLS token for multimodal settings. Usually needed by the FLAVA model.`,name:"use_cls_token"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/configuration_flava.py#L278"}}),Ue=new wo({props:{anchor:"transformers.FlavaMultimodalConfig.example",$$slots:{default:[rp]},$$scope:{ctx:x}}}),ea=new j({}),aa=new M({props:{name:"class transformers.FlavaImageCodebookConfig",anchor:"transformers.FlavaImageCodebookConfig",parameters:[{name:"num_groups",val:": int = 4"},{name:"input_channels",val:": int = 3"},{name:"num_blocks_per_group",val:": int = 2"},{name:"hidden_size",val:": int = 256"},{name:"vocab_size",val:": int = 8192"},{name:"freeze",val:": int = True"},{name:"initializer_range",val:": float = 0.02"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/configuration_flava.py#L379"}}),ta=new j({}),na=new M({props:{name:"class transformers.FlavaProcessor",anchor:"transformers.FlavaProcessor",parameters:[{name:"feature_extractor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.FlavaProcessor.feature_extractor",description:'<strong>feature_extractor</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.models.flava.image_processing_flava.FlavaImageProcessor">FlavaFeatureExtractor</a>) &#x2014; The feature extractor is a required input.',name:"feature_extractor"},{anchor:"transformers.FlavaProcessor.tokenizer",description:'<strong>tokenizer</strong> (<a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a>) &#x2014; The tokenizer is a required input.',name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/processing_flava.py#L26"}}),ra=new M({props:{name:"batch_decode",anchor:"transformers.FlavaProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/processing_flava.py#L112"}}),ia=new M({props:{name:"decode",anchor:"transformers.FlavaProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/processing_flava.py#L119"}}),da=new j({}),ca=new M({props:{name:"class transformers.models.flava.image_processing_flava.FlavaImageProcessor",anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": typing.Dict[str, int] = None"},{name:"resample",val:": Resampling = <Resampling.BICUBIC: 3>"},{name:"do_center_crop",val:": bool = True"},{name:"crop_size",val:": typing.Dict[str, int] = None"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": typing.Union[int, float] = 0.00392156862745098"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": typing.Union[float, typing.Iterable[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, typing.Iterable[float], NoneType] = None"},{name:"return_image_mask",val:": bool = False"},{name:"input_size_patches",val:": int = 14"},{name:"total_mask_patches",val:": int = 75"},{name:"mask_group_min_patches",val:": int = 16"},{name:"mask_group_max_patches",val:": typing.Optional[int] = None"},{name:"mask_group_min_aspect_ratio",val:": float = 0.3"},{name:"mask_group_max_aspect_ratio",val:": typing.Optional[float] = None"},{name:"return_codebook_pixels",val:": bool = False"},{name:"codebook_do_resize",val:": bool = True"},{name:"codebook_size",val:": bool = None"},{name:"codebook_resample",val:": int = <Resampling.LANCZOS: 1>"},{name:"codebook_do_center_crop",val:": bool = True"},{name:"codebook_crop_size",val:": int = None"},{name:"codebook_do_rescale",val:": bool = True"},{name:"codebook_rescale_factor",val:": typing.Union[int, float] = 0.00392156862745098"},{name:"codebook_do_map_pixels",val:": bool = True"},{name:"codebook_do_normalize",val:": bool = True"},{name:"codebook_image_mean",val:": typing.Union[float, typing.Iterable[float], NoneType] = None"},{name:"codebook_image_std",val:": typing.Union[float, typing.Iterable[float], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden by the
<code>do_resize</code> parameter in <code>preprocess</code>.`,name:"do_resize"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.size",description:`<strong>size</strong> (<code>Dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Size of the image after resizing. Can be overridden by the <code>size</code> parameter in <code>preprocess</code>.`,name:"size"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>PILImageResampling.BICUBIC</code>) &#x2014;
Resampling filter to use if resizing the image. Can be overridden by the <code>resample</code> parameter in
<code>preprocess</code>.`,name:"resample"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to center crop the images. Can be overridden by the <code>do_center_crop</code> parameter in <code>preprocess</code>.`,name:"do_center_crop"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.crop_size",description:`<strong>crop_size</strong> (<code>Dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Size of image after the center crop <code>(crop_size[&quot;height&quot;], crop_size[&quot;width&quot;])</code>. Can be overridden by the
<code>crop_size</code> parameter in <code>preprocess</code>.`,name:"crop_size"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by the <code>do_rescale</code>
parameter in <code>preprocess</code>.`,name:"do_rescale"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overridden by the <code>rescale_factor</code> parameter in
<code>preprocess</code>.`,name:"rescale_factor"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by the <code>do_normalize</code> parameter in <code>preprocess</code>.`,name:"do_normalize"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_MEAN</code>) &#x2014;
Mean to use if normalizing the image. This is a float or list of floats the length of the number of
channels in the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_STD</code>) &#x2014;
Standard deviation to use if normalizing the image. This is a float or list of floats the length of the
number of channels in the image. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.return_image_mask",description:`<strong>return_image_mask</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to return the image mask. Can be overridden by the <code>return_image_mask</code> parameter in <code>preprocess</code>.`,name:"return_image_mask"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.input_size_patches",description:`<strong>input_size_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 14) &#x2014;
Number of patches in the image in height and width direction. 14x14 = 196 total patches. Can be overridden
by the <code>input_size_patches</code> parameter in <code>preprocess</code>.`,name:"input_size_patches"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.total_mask_patches",description:`<strong>total_mask_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 75) &#x2014;
Total number of patches that should be masked. Can be overridden by the <code>total_mask_patches</code> parameter in
<code>preprocess</code>.`,name:"total_mask_patches"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.mask_group_min_patches",description:`<strong>mask_group_min_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Minimum number of patches that should be masked. Can be overridden by the <code>mask_group_min_patches</code>
parameter in <code>preprocess</code>.`,name:"mask_group_min_patches"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.mask_group_max_patches",description:`<strong>mask_group_max_patches</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum number of patches that should be masked. Can be overridden by the <code>mask_group_max_patches</code>
parameter in <code>preprocess</code>.`,name:"mask_group_max_patches"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.mask_group_min_aspect_ratio",description:`<strong>mask_group_min_aspect_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 0.3) &#x2014;
Minimum aspect ratio of the mask window. Can be overridden by the <code>mask_group_min_aspect_ratio</code> parameter
in <code>preprocess</code>.`,name:"mask_group_min_aspect_ratio"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.mask_group_max_aspect_ratio",description:`<strong>mask_group_max_aspect_ratio</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Maximum aspect ratio of the mask window. Can be overridden by the <code>mask_group_max_aspect_ratio</code> parameter
in <code>preprocess</code>.`,name:"mask_group_max_aspect_ratio"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.codebook_do_resize",description:`<strong>codebook_do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the input for codebook to a certain. Can be overridden by the <code>codebook_do_resize</code>
parameter in <code>preprocess</code>. <code>codebook_size</code>.`,name:"codebook_do_resize"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.codebook_size",description:`<strong>codebook_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Resize the input for codebook to the given size. Can be overridden by the <code>codebook_size</code> parameter in
<code>preprocess</code>.`,name:"codebook_size"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.codebook_resample",description:`<strong>codebook_resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>PILImageResampling.LANCZOS</code>) &#x2014;
Resampling filter to use if resizing the codebook image. Can be overridden by the <code>codebook_resample</code>
parameter in <code>preprocess</code>.`,name:"codebook_resample"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.codebook_do_center_crop",description:`<strong>codebook_do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to crop the input for codebook at the center. If the input size is smaller than
<code>codebook_crop_size</code> along any edge, the image is padded with 0&#x2019;s and then center cropped. Can be
overridden by the <code>codebook_do_center_crop</code> parameter in <code>preprocess</code>.`,name:"codebook_do_center_crop"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.codebook_crop_size",description:`<strong>codebook_crop_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Desired output size for codebook input when applying center-cropping. Can be overridden by the
<code>codebook_crop_size</code> parameter in <code>preprocess</code>.`,name:"codebook_crop_size"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.codebook_do_rescale",description:`<strong>codebook_do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the input for codebook by the specified scale <code>codebook_rescale_factor</code>. Can be
overridden by the <code>codebook_do_rescale</code> parameter in <code>preprocess</code>.`,name:"codebook_do_rescale"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.codebook_rescale_factor",description:`<strong>codebook_rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Defines the scale factor to use if rescaling the codebook image. Can be overridden by the
<code>codebook_rescale_factor</code> parameter in <code>preprocess</code>.`,name:"codebook_rescale_factor"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.codebook_do_map_pixels",description:`<strong>codebook_do_map_pixels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to map the pixel values of the codebook input to (1 - 2e)x + e. Can be overridden by the
<code>codebook_do_map_pixels</code> parameter in <code>preprocess</code>.`,name:"codebook_do_map_pixels"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.codebook_do_normalize",description:`<strong>codebook_do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input for codebook with <code>codebook_image_mean</code> and <code>codebook_image_std</code>. Can
be overridden by the <code>codebook_do_normalize</code> parameter in <code>preprocess</code>.`,name:"codebook_do_normalize"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.codebook_image_mean",description:`<strong>codebook_image_mean</strong> (<code>Optional[Union[float, Iterable[float]]]</code>, <em>optional</em>, defaults to <code>[0, 0, 0]</code>) &#x2014;
The sequence of means for each channel, to be used when normalizing images for codebook. Can be overridden
by the <code>codebook_image_mean</code> parameter in <code>preprocess</code>.`,name:"codebook_image_mean"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.codebook_image_std",description:`<strong>codebook_image_std</strong> (<code>Optional[Union[float, Iterable[float]]]</code>, <em>optional</em>, defaults to <code>[0.5, 0.5, 0.5]</code>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images for codebook. Can
be overridden by the <code>codebook_image_std</code> parameter in <code>preprocess</code>.`,name:"codebook_image_std"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/image_processing_flava.py#L127"}}),ma=new M({props:{name:"center_crop",anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.center_crop",parameters:[{name:"image",val:": ndarray"},{name:"size",val:": typing.Dict[str, int]"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.center_crop.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to center crop.`,name:"image"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.center_crop.size",description:`<strong>size</strong> (<code>Dict[str, int]</code>) &#x2014;
Size of the output image.`,name:"size"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.center_crop.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the image. If not provided, it will be the same as the input image.`,name:"data_format"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/image_processing_flava.py#L343"}}),fa=new M({props:{name:"normalize",anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.normalize",parameters:[{name:"image",val:": ndarray"},{name:"mean",val:": typing.Union[float, typing.List[float]]"},{name:"std",val:": typing.Union[float, typing.List[float]]"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.normalize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to normalize.`,name:"image"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.normalize.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>) &#x2014;
Image mean.`,name:"image_mean"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.normalize.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>) &#x2014;
Image standard deviation.`,name:"image_std"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.normalize.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the image. If not provided, it will be the same as the input image.`,name:"data_format"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/image_processing_flava.py#L385"}}),pa=new M({props:{name:"preprocess",anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), typing.List[ForwardRef('PIL.Image.Image')], typing.List[numpy.ndarray], typing.List[ForwardRef('torch.Tensor')]]"},{name:"do_resize",val:": typing.Optional[bool] = None"},{name:"size",val:": typing.Dict[str, int] = None"},{name:"resample",val:": Resampling = None"},{name:"do_center_crop",val:": typing.Optional[bool] = None"},{name:"crop_size",val:": typing.Union[typing.Dict[str, int], NoneType] = None"},{name:"do_rescale",val:": typing.Optional[bool] = None"},{name:"rescale_factor",val:": typing.Optional[float] = None"},{name:"do_normalize",val:": typing.Optional[bool] = None"},{name:"image_mean",val:": typing.Union[float, typing.List[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, typing.List[float], NoneType] = None"},{name:"return_image_mask",val:": typing.Optional[bool] = None"},{name:"input_size_patches",val:": typing.Optional[int] = None"},{name:"total_mask_patches",val:": typing.Optional[int] = None"},{name:"mask_group_min_patches",val:": typing.Optional[int] = None"},{name:"mask_group_max_patches",val:": typing.Optional[int] = None"},{name:"mask_group_min_aspect_ratio",val:": typing.Optional[float] = None"},{name:"mask_group_max_aspect_ratio",val:": typing.Optional[float] = None"},{name:"return_codebook_pixels",val:": typing.Optional[bool] = None"},{name:"codebook_do_resize",val:": typing.Optional[bool] = None"},{name:"codebook_size",val:": typing.Union[typing.Dict[str, int], NoneType] = None"},{name:"codebook_resample",val:": typing.Optional[int] = None"},{name:"codebook_do_center_crop",val:": typing.Optional[bool] = None"},{name:"codebook_crop_size",val:": typing.Union[typing.Dict[str, int], NoneType] = None"},{name:"codebook_do_rescale",val:": typing.Optional[bool] = None"},{name:"codebook_rescale_factor",val:": typing.Optional[float] = None"},{name:"codebook_do_map_pixels",val:": typing.Optional[bool] = None"},{name:"codebook_do_normalize",val:": typing.Optional[bool] = None"},{name:"codebook_image_mean",val:": typing.Optional[typing.Iterable[float]] = None"},{name:"codebook_image_std",val:": typing.Optional[typing.Iterable[float]] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"}],parametersDescription:[{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess.`,name:"images"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the image.`,name:"size"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of the enum <code>PILImageResampling</code>, Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_center_crop</code>) &#x2014;
Whether to center crop the image.`,name:"do_center_crop"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.crop_size</code>) &#x2014;
Size of the center crop. Only has an effect if <code>do_center_crop</code> is set to <code>True</code>.`,name:"crop_size"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean.`,name:"image_mean"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation.`,name:"image_std"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.return_image_mask",description:`<strong>return_image_mask</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.return_image_mask</code>) &#x2014;
Whether to return the image mask.`,name:"return_image_mask"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.input_size_patches",description:`<strong>input_size_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.input_size_patches</code>) &#x2014;
Size of the patches to extract from the image.`,name:"input_size_patches"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.total_mask_patches",description:`<strong>total_mask_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.total_mask_patches</code>) &#x2014;
Total number of patches to extract from the image.`,name:"total_mask_patches"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.mask_group_min_patches",description:`<strong>mask_group_min_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.mask_group_min_patches</code>) &#x2014;
Minimum number of patches to extract from the image.`,name:"mask_group_min_patches"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.mask_group_max_patches",description:`<strong>mask_group_max_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.mask_group_max_patches</code>) &#x2014;
Maximum number of patches to extract from the image.`,name:"mask_group_max_patches"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.mask_group_min_aspect_ratio",description:`<strong>mask_group_min_aspect_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.mask_group_min_aspect_ratio</code>) &#x2014;
Minimum aspect ratio of the patches to extract from the image.`,name:"mask_group_min_aspect_ratio"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.mask_group_max_aspect_ratio",description:`<strong>mask_group_max_aspect_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.mask_group_max_aspect_ratio</code>) &#x2014;
Maximum aspect ratio of the patches to extract from the image.`,name:"mask_group_max_aspect_ratio"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.return_codebook_pixels",description:`<strong>return_codebook_pixels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.return_codebook_pixels</code>) &#x2014;
Whether to return the codebook pixels.`,name:"return_codebook_pixels"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.codebook_do_resize",description:`<strong>codebook_do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_resize</code>) &#x2014;
Whether to resize the codebook pixels.`,name:"codebook_do_resize"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.codebook_size",description:`<strong>codebook_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.codebook_size</code>) &#x2014;
Size of the codebook pixels.`,name:"codebook_size"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.codebook_resample",description:`<strong>codebook_resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.codebook_resample</code>) &#x2014;
Resampling filter to use if resizing the codebook pixels. This can be one of the enum
<code>PILImageResampling</code>, Only has an effect if <code>codebook_do_resize</code> is set to <code>True</code>.`,name:"codebook_resample"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.codebook_do_center_crop",description:`<strong>codebook_do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_center_crop</code>) &#x2014;
Whether to center crop the codebook pixels.`,name:"codebook_do_center_crop"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.codebook_crop_size",description:`<strong>codebook_crop_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.codebook_crop_size</code>) &#x2014;
Size of the center crop of the codebook pixels. Only has an effect if <code>codebook_do_center_crop</code> is set
to <code>True</code>.`,name:"codebook_crop_size"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.codebook_do_rescale",description:`<strong>codebook_do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_rescale</code>) &#x2014;
Whether to rescale the codebook pixels values between [0 - 1].`,name:"codebook_do_rescale"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.codebook_rescale_factor",description:`<strong>codebook_rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.codebook_rescale_factor</code>) &#x2014;
Rescale factor to rescale the codebook pixels by if <code>codebook_do_rescale</code> is set to <code>True</code>.`,name:"codebook_rescale_factor"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.codebook_do_map_pixels",description:`<strong>codebook_do_map_pixels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_map_pixels</code>) &#x2014;
Whether to map the codebook pixels values.`,name:"codebook_do_map_pixels"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.codebook_do_normalize",description:`<strong>codebook_do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_normalize</code>) &#x2014;
Whether to normalize the codebook pixels.`,name:"codebook_do_normalize"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.codebook_image_mean",description:`<strong>codebook_image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.codebook_image_mean</code>) &#x2014;
Codebook pixels mean to normalize the codebook pixels by if <code>codebook_do_normalize</code> is set to <code>True</code>.`,name:"codebook_image_mean"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.codebook_image_std",description:`<strong>codebook_image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.codebook_image_std</code>) &#x2014;
Codebook pixels standard deviation to normalize the codebook pixels by if <code>codebook_do_normalize</code> is
set to <code>True</code>.`,name:"codebook_image_std"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"data_format"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/image_processing_flava.py#L459"}}),ga=new M({props:{name:"rescale",anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.rescale",parameters:[{name:"image",val:": ndarray"},{name:"scale",val:": typing.Union[int, float]"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.rescale.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to rescale.`,name:"image"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.rescale.scale",description:`<strong>scale</strong> (<code>int</code> or <code>float</code>) &#x2014;
Scale to apply to the image.`,name:"scale"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.rescale.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the image. If not provided, it will be the same as the input image.`,name:"data_format"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/image_processing_flava.py#L365"}}),ha=new M({props:{name:"resize",anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.resize",parameters:[{name:"image",val:": ndarray"},{name:"size",val:": typing.Dict[str, int]"},{name:"resample",val:": Resampling = <Resampling.BICUBIC: 3>"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.resize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to resize.`,name:"image"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.resize.size",description:`<strong>size</strong> (<code>Dict[str, int]</code>) &#x2014;
Size of the output image.`,name:"size"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.resize.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>PILImageResampling.BICUBIC</code>) &#x2014;
Resampling filter to use when resiizing the image.`,name:"resample"},{anchor:"transformers.models.flava.image_processing_flava.FlavaImageProcessor.resize.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the image. If not provided, it will be the same as the input image.`,name:"data_format"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/image_processing_flava.py#L315"}}),_a=new j({}),va=new M({props:{name:"class transformers.FlavaForPreTraining",anchor:"transformers.FlavaForPreTraining",parameters:[{name:"config",val:": FlavaConfig"},{name:"image_codebook",val:": typing.Optional[torch.nn.modules.module.Module] = None"}],parametersDescription:[{anchor:"transformers.FlavaForPreTraining.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig">FlavaConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlavaForPreTraining.image_codebook",description:`<strong>image_codebook</strong> (<code>nn.Module</code>) &#x2014; If passed, the image codebook will be set to this. Otherwise. it will
be initialized using the image_codebook_config defined in the config first as the first parameter.`,name:"image_codebook"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1731"}}),Fa=new M({props:{name:"forward",anchor:"transformers.FlavaForPreTraining.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"input_ids_masked",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"codebook_pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"image_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"skip_unmasked_multimodal_encoder",val:": bool = None"},{name:"mlm_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"mim_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"itm_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": bool = True"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"return_loss",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaForPreTraining.forward.input_ids_masked",description:`<strong>input_ids_masked</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. These ones are the masked version of the original task
to be used with MLM. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> along with
<code>DataCollatorForMaskedLanguageModeling</code>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids_masked"},{anchor:"transformers.FlavaForPreTraining.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlavaForPreTraining.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FlavaForPreTraining.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/flava#transformers.models.flava.image_processing_flava.FlavaImageProcessor">FlavaFeatureExtractor</a>. See
<a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.models.layoutlmv2.image_processing_layoutlmv2.LayoutLMv2ImageProcessor.__call__">FlavaFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.FlavaForPreTraining.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FlavaForPreTraining.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FlavaForPreTraining.forward.image_attention_mask",description:`<strong>image_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices specifically for images. Mask values selected
in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"image_attention_mask"},{anchor:"transformers.FlavaForPreTraining.forward.skip_unmasked_multimodal_encoder",description:`<strong>skip_unmasked_multimodal_encoder</strong> (<em>bool</em>, <em>optional</em>) &#x2014;
Skip any calculations for multimodal encoder for unmasked inputs. FLAVA pretraining doesn&#x2019;t need unmasked
multimodal embeddings or outputs as of now.`,name:"skip_unmasked_multimodal_encoder"},{anchor:"transformers.FlavaForPreTraining.forward.mlm_labels",description:`<strong>mlm_labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>, <em>optional</em>) &#x2014;
Labels for computing the left-to-right language and multimodal masked modeling loss (next word prediction).
Indices should be in <code>[-100, 0, ..., text_config.vocab_size - 1]</code> (see <code>input_ids</code> docstring). Tokens with
indices set to <code>-100</code> are ignored (masked), the loss is only computed for the tokens with labels in <code>[0, ..., text_config.vocab_size - 1]</code>.`,name:"mlm_labels"},{anchor:"transformers.FlavaForPreTraining.forward.mim_labels",description:`<strong>mim_labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Labels for computing the image and multimodal masked modeling loss. Indices should be in <code>[-100, 0, ..., image_config.vocab_size - 1]</code>. Tokens with indices set to <code>-100</code> are ignored (masked), the loss is only
computed for the tokens with labels in <code>[0, ..., image_config.vocab_size - 1]</code>. If not passed, they are
generated automatically using the image codebook assigned to the model. By default, it uses
<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageCodebook">FlavaImageCodebook</a>. See <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageCodebook">FlavaImageCodebook</a> to understand how to generate mim_labels.`,name:"mim_labels"},{anchor:"transformers.FlavaForPreTraining.forward.itm_labels",description:`<strong>itm_labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, 1)</code>, <em>optional</em>) &#x2014;
Labels for computing the image-text matching loss. 0 means the pairs don&#x2019;t match and 1 means they match.
The pairs with 0 will be skipped for calculation of MMM and global contrastive losses as well.`,name:"itm_labels"},{anchor:"transformers.FlavaForPreTraining.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>, default to None) &#x2014;
Whether to return calculated loss or not.`,name:"return_loss"},{anchor:"transformers.FlavaForPreTraining.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_len)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaForPreTraining.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaForPreTraining.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaForPreTraining.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaForPreTraining.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.</p>
<p>Examples &#x2014;`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1767",returnDescription:`
<p>A <code>transformers.models.flava.modeling_flava.FlavaForPreTrainingOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.flava.configuration_flava.FlavaConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code>, <em>optional</em>, returned when <code>return_loss</code> is True) \u2014 Total loss calculated for this model.</p>
</li>
<li>
<p><strong>loss_info</strong> (<code>FlavaLosses</code>) \u2014 Detailed info for FLAVA Pretraining losses. Check <code>FlavaLosses</code> class description for the information on
the keys.</p>
</li>
<li>
<p><strong>image_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>.</p>
</li>
<li>
<p><strong>image_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>.</p>
</li>
<li>
<p><strong>text_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>text_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_unmasked_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_output</strong> (<code>BaseModelOutputWithPooling</code>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_unmasked_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalModel"
>FlavaMultimodalModel</a>.</p>
</li>
<li>
<p><strong>image_masked_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>. Uses <code>bool_masked_pos</code>
to create masked images.</p>
</li>
<li>
<p><strong>image_masked_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>. Uses <code>bool_masked_pos</code> to create masked images.</p>
</li>
<li>
<p><strong>text_masked_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>text_masked_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>input_ids_masked</code> are present) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_masked_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_masked_output</strong> (<code>BaseModelOutputWithPooling</code>, returned when <code>input_ids_masked</code> and <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalModel"
>FlavaMultimodalModel</a>.</p>
</li>
<li>
<p><strong>mim_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_image_patches, image_vocab_size)</code> or of shape <code>(total_masked_patches, image_vocab_size)</code> , <em>optional</em>, returned when <code>pixel_values</code> are present and <code>input_ids_masked</code> are not) \u2014 The logits for MIM unimodal loss. Uses <code>book_masked_pos</code> to get masked patches. The flattened output is
returned when <code>bool_masked_pos</code> has some of the patches masked.</p>
</li>
<li>
<p><strong>mlm_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length, text_vocab_size)</code> or of shape <code>(total_masked_seq_length, text_vocab_size)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> are present and <code>pixel_values</code> are not) \u2014 The logits for MLM unimodal loss. The flattened output is returned when <code>input_ids_masked</code> has some of
the tokens masked.</p>
</li>
<li>
<p><strong>itm_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, 2)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> and <code>pixel_values</code> are present) \u2014 The logits for ITM loss. Note that ITM loss is calculated on masked pairs in FLAVA.</p>
</li>
<li>
<p><strong>mmm_image_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_image_patches, image_vocab_size)</code> or of shape<code>(total_masked_patches, image_vocab_size)</code>, <em>optional</em>, returned when <code>pixel_values</code> and <code>input_ids_masked</code> are present) \u2014 The logits for MMM image multimodal loss. Uses <code>book_masked_pos</code> to get masked patches. The flattened
output is returned when <code>bool_masked_pos</code> has some of the patches masked.</p>
</li>
<li>
<p><strong>mmm_text_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length, text_vocab_size)</code> or of shape <code>(</code>(total_masked_seq_length, text_vocab_size)<code>), *optional*, returned when </code>pixel_values<code>and</code>input_ids_masked<code>are present) -- The logits for MMM text multimodal loss. The flattened output is returned when</code>input_ids_masked\` has
some of the tokens masked.</p>
</li>
<li>
<p><strong>contrastive_logits_per_image</strong> (<code>torch.FloatTensor</code> of shape <code>(image_batch_size, text_batch_size)</code>) \u2014 The scaled dot product scores between <code>image_embeddings</code> and <code>text_embeddings</code> but passed through FLAVA\u2019s
<code>image_projection</code> and <code>text_projection</code> layers respectively. This represents the image-text similarity
scores. This is calculated on unmasked images and texts.</p>
</li>
<li>
<p><strong>contrastive_logits_per_text</strong> (<code>torch.FloatTensor</code> of shape <code>(text_batch_size, image_batch_size)</code>) \u2014 The scaled dot product scores between <code>text_embeddings</code> and <code>image_embeddings</code> but passed through FLAVA\u2019s
<code>text_projection</code> and <code>image_projection</code> layers respectively. This is calculated on unmasked images and
texts.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.flava.modeling_flava.FlavaForPreTrainingOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),to=new at({props:{$$slots:{default:[sp]},$$scope:{ctx:x}}}),ya=new j({}),$a=new M({props:{name:"class transformers.FlavaModel",anchor:"transformers.FlavaModel",parameters:[{name:"config",val:": FlavaConfig"}],parametersDescription:[{anchor:"transformers.FlavaModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig">FlavaConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1199"}}),xa=new M({props:{name:"forward",anchor:"transformers.FlavaModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"image_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"skip_multimodal_encoder",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": bool = True"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/flava#transformers.models.flava.image_processing_flava.FlavaImageProcessor">FlavaFeatureExtractor</a>. See
<a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.models.layoutlmv2.image_processing_layoutlmv2.LayoutLMv2ImageProcessor.__call__">FlavaFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.FlavaModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FlavaModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FlavaModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlavaModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FlavaModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FlavaModel.forward.skip_multimodal_encoder",description:`<strong>skip_multimodal_encoder</strong> (<em>bool</em>, <em>optional</em>) &#x2014;
Skip any calculations for multimodal encoder. Useful if multimodal encoding is not going to be used.`,name:"skip_multimodal_encoder"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1343",returnDescription:`
<p>A <code>transformers.models.flava.modeling_flava.FlavaModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.flava.configuration_flava.FlavaConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>image_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>.</li>
<li><strong>image_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>.</li>
<li><strong>text_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</li>
<li><strong>text_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</li>
<li><strong>multimodal_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</li>
<li><strong>multimodal_output</strong> (<code>BaseModelOutputWithPooling</code>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalModel"
>FlavaMultimodalModel</a>.</li>
</ul>
`,returnType:`
<p><code>transformers.models.flava.modeling_flava.FlavaModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ro=new at({props:{$$slots:{default:[ip]},$$scope:{ctx:x}}}),so=new wo({props:{anchor:"transformers.FlavaModel.forward.example",$$slots:{default:[lp]},$$scope:{ctx:x}}}),Ma=new M({props:{name:"get_text_features",anchor:"transformers.FlavaModel.get_text_features",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlavaModel.get_text_features.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FlavaModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaModel.get_text_features.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1245"}}),io=new at({props:{$$slots:{default:[dp]},$$scope:{ctx:x}}}),za=new M({props:{name:"get_image_features",anchor:"transformers.FlavaModel.get_image_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"interpolate_pos_encoding",val:": typing.Optional[bool] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/flava#transformers.models.flava.image_processing_flava.FlavaImageProcessor">FlavaFeatureExtractor</a>. See
<a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.models.layoutlmv2.image_processing_layoutlmv2.LayoutLMv2ImageProcessor.__call__">FlavaFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.FlavaModel.get_image_features.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FlavaModel.get_image_features.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FlavaModel.get_image_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaModel.get_image_features.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1291"}}),lo=new at({props:{$$slots:{default:[cp]},$$scope:{ctx:x}}}),Ia=new j({}),Pa=new M({props:{name:"class transformers.FlavaImageCodebook",anchor:"transformers.FlavaImageCodebook",parameters:[{name:"config",val:": FlavaImageCodebookConfig"},{name:"**kwargs",val:": typing.Any"}],parametersDescription:[{anchor:"transformers.FlavaImageCodebook.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageCodebookConfig">FlavaImageCodebookConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1517"}}),Na=new M({props:{name:"forward",anchor:"transformers.FlavaImageCodebook.forward",parameters:[{name:"pixel_values",val:": FloatTensor"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1601"}}),qa=new M({props:{name:"get_codebook_indices",anchor:"transformers.FlavaImageCodebook.get_codebook_indices",parameters:[{name:"pixel_values",val:": Tensor"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1567"}}),La=new M({props:{name:"get_codebook_probs",anchor:"transformers.FlavaImageCodebook.get_codebook_probs",parameters:[{name:"pixel_values",val:": Tensor"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1597"}}),ja=new j({}),Oa=new M({props:{name:"class transformers.FlavaTextModel",anchor:"transformers.FlavaTextModel",parameters:[{name:"config",val:": FlavaTextConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.FlavaTextModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextConfig">FlavaTextConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L994"}}),Sa=new M({props:{name:"forward",anchor:"transformers.FlavaTextModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlavaTextModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FlavaTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaTextModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1025",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextConfig"
>FlavaTextConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),fo=new at({props:{$$slots:{default:[mp]},$$scope:{ctx:x}}}),po=new wo({props:{anchor:"transformers.FlavaTextModel.forward.example",$$slots:{default:[fp]},$$scope:{ctx:x}}}),Va=new j({}),Ba=new M({props:{name:"class transformers.FlavaImageModel",anchor:"transformers.FlavaImageModel",parameters:[{name:"config",val:": FlavaImageConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.FlavaImageModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageConfig">FlavaImageConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L894"}}),Ha=new M({props:{name:"forward",anchor:"transformers.FlavaImageModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"interpolate_pos_encoding",val:": typing.Optional[bool] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaImageModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/flava#transformers.models.flava.image_processing_flava.FlavaImageProcessor">FlavaFeatureExtractor</a>. See
<a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.models.layoutlmv2.image_processing_layoutlmv2.LayoutLMv2ImageProcessor.__call__">FlavaFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.FlavaImageModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FlavaImageModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FlavaImageModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaImageModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaImageModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaImageModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaImageModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L927",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageConfig"
>FlavaImageConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ho=new at({props:{$$slots:{default:[pp]},$$scope:{ctx:x}}}),uo=new wo({props:{anchor:"transformers.FlavaImageModel.forward.example",$$slots:{default:[gp]},$$scope:{ctx:x}}}),Ga=new j({}),Ja=new M({props:{name:"class transformers.FlavaMultimodalModel",anchor:"transformers.FlavaMultimodalModel",parameters:[{name:"config",val:": FlavaMultimodalConfig"},{name:"add_pooling_layer",val:" = True"}],parametersDescription:[{anchor:"transformers.FlavaMultimodalModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalConfig">FlavaMultimodalConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1100"}}),Za=new M({props:{name:"forward",anchor:"transformers.FlavaMultimodalModel.forward",parameters:[{name:"hidden_states",val:": Tensor"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaMultimodalModel.forward.hidden_states",description:`<strong>hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len, hidden_size)</code>) &#x2014;
The concatenated hidden states of unimodal encoders.`,name:"hidden_states"},{anchor:"transformers.FlavaMultimodalModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaMultimodalModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaMultimodalModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaMultimodalModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaMultimodalModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1128",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalConfig"
>FlavaMultimodalConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),vo=new at({props:{$$slots:{default:[hp]},$$scope:{ctx:x}}}),bo=new wo({props:{anchor:"transformers.FlavaMultimodalModel.forward.example",$$slots:{default:[up]},$$scope:{ctx:x}}}),{c(){c=t("meta"),w=m(),u=t("h1"),g=t("a"),v=t("span"),b(l.$$.fragment),p=m(),z=t("span"),ws=s("FLAVA"),sr=m(),ie=t("h2"),Ne=t("a"),on=t("span"),b(zo.$$.fragment),xs=m(),an=t("span"),Ms=s("Overview"),ir=m(),qe=t("p"),zs=s("The FLAVA model was proposed in "),Io=t("a"),Is=s("FLAVA: A Foundational Language And Vision Alignment Model"),Ps=s(" by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022."),lr=m(),tt=t("p"),Cs=s(`The paper aims at creating a single unified foundation model which can work across vision, language
as well as vision-and-language multimodal tasks.`),dr=m(),nt=t("p"),Es=s("The abstract from the paper is the following:"),cr=m(),rt=t("p"),tn=t("em"),As=s(`State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety
of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal
(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising
direction would be to use a single holistic universal model, as a \u201Cfoundation\u201D, that targets all modalities
at once \u2014 a true vision and language foundation model should be good at vision tasks, language tasks, and
cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate
impressive performance on a wide range of 35 tasks spanning these target modalities.`),mr=m(),Z=t("p"),Ns=s("This model was contributed by "),Po=t("a"),qs=s("aps"),Ls=s(". The original code can be found "),Co=t("a"),js=s("here"),Os=s("."),fr=m(),le=t("h2"),Le=t("a"),nn=t("span"),b(Eo.$$.fragment),Ds=m(),rn=t("span"),Ws=s("FlavaConfig"),pr=m(),P=t("div"),b(Ao.$$.fragment),Ss=m(),X=t("p"),st=t("a"),Vs=s("FlavaConfig"),Bs=s(" is the configuration class to store the configuration of a "),it=t("a"),Rs=s("FlavaModel"),Us=s(`. It is used to
instantiate FLAVA model according to the specified arguments, defining the text model, image model, image codebook
and multimodal model configs. Instantiating a configuration with the defaults will yield a similar configuration to
that of the FLAVA `),No=t("a"),Hs=s("facebook/flava-full"),Gs=s(" architecture."),Js=m(),de=t("p"),Ks=s("Configuration objects inherit from "),lt=t("a"),Ys=s("PretrainedConfig"),Zs=s(` and can be used to control the model outputs. Read the
documentation from `),dt=t("a"),Xs=s("PretrainedConfig"),Qs=s(" for more information."),ei=m(),b(je.$$.fragment),oi=m(),Oe=t("div"),b(qo.$$.fragment),ai=m(),Lo=t("p"),ti=s("Instantiate a "),ct=t("a"),ni=s("FlavaConfig"),ri=s(` (or a derived class) from flava text model configuration, flava image model
configuration, flava multimodal model and flava codebook model configuration.`),si=m(),De=t("div"),b(jo.$$.fragment),ii=m(),Oo=t("p"),li=s("Serializes this instance to a Python dictionary. Override the default "),mt=t("a"),di=s("to_dict()"),ci=s("."),gr=m(),ce=t("h2"),We=t("a"),sn=t("span"),b(Do.$$.fragment),mi=m(),ln=t("span"),fi=s("FlavaTextConfig"),hr=m(),E=t("div"),b(Wo.$$.fragment),pi=m(),So=t("p"),gi=s("This is the configuration class to store the configuration of a "),ft=t("a"),hi=s("FlavaTextModel"),ui=s(`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture.`),_i=m(),Vo=t("p"),vi=s(`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
`),Bo=t("a"),bi=s("facebook/flava-full"),ki=s(" architecture."),Fi=m(),me=t("p"),yi=s("Configuration objects inherit from "),pt=t("a"),$i=s("PretrainedConfig"),Ti=s(` and can be used to control the model outputs. Read the
documentation from `),gt=t("a"),wi=s("PretrainedConfig"),xi=s(" for more information."),Mi=m(),b(Se.$$.fragment),ur=m(),fe=t("h2"),Ve=t("a"),dn=t("span"),b(Ro.$$.fragment),zi=m(),cn=t("span"),Ii=s("FlavaImageConfig"),_r=m(),A=t("div"),b(Uo.$$.fragment),Pi=m(),Ho=t("p"),Ci=s("This is the configuration class to store the configuration of a "),ht=t("a"),Ei=s("FlavaImageModel"),Ai=s(`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture.`),Ni=m(),Go=t("p"),qi=s(`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
`),Jo=t("a"),Li=s("facebook/flava-full"),ji=s(" architecture."),Oi=m(),pe=t("p"),Di=s("Configuration objects inherit from "),ut=t("a"),Wi=s("PretrainedConfig"),Si=s(` and can be used to control the model outputs. Read the
documentation from `),_t=t("a"),Vi=s("PretrainedConfig"),Bi=s(" for more information."),Ri=m(),b(Be.$$.fragment),vr=m(),ge=t("h2"),Re=t("a"),mn=t("span"),b(Ko.$$.fragment),Ui=m(),fn=t("span"),Hi=s("FlavaMultimodalConfig"),br=m(),N=t("div"),b(Yo.$$.fragment),Gi=m(),Zo=t("p"),Ji=s("This is the configuration class to store the configuration of a "),vt=t("a"),Ki=s("FlavaMultimodalModel"),Yi=s(`. It is used to instantiate
an FLAVA model according to the specified arguments, defining the model architecture.`),Zi=m(),Xo=t("p"),Xi=s(`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
`),Qo=t("a"),Qi=s("facebook/flava-full"),el=s(" architecture."),ol=m(),he=t("p"),al=s("Configuration objects inherit from "),bt=t("a"),tl=s("PretrainedConfig"),nl=s(` and can be used to control the model outputs. Read the
documentation from `),kt=t("a"),rl=s("PretrainedConfig"),sl=s(" for more information."),il=m(),b(Ue.$$.fragment),kr=m(),ue=t("h2"),He=t("a"),pn=t("span"),b(ea.$$.fragment),ll=m(),gn=t("span"),dl=s("FlavaImageCodebookConfig"),Fr=m(),oa=t("div"),b(aa.$$.fragment),yr=m(),_e=t("h2"),Ge=t("a"),hn=t("span"),b(ta.$$.fragment),cl=m(),un=t("span"),ml=s("FlavaProcessor"),$r=m(),q=t("div"),b(na.$$.fragment),fl=m(),_n=t("p"),pl=s("Constructs a FLAVA processor which wraps a FLAVA feature extractor and a FLAVA tokenizer into a single processor."),gl=m(),O=t("p"),Ft=t("a"),hl=s("FlavaProcessor"),ul=s(" offers all the functionalities of "),yt=t("a"),_l=s("FlavaFeatureExtractor"),vl=s(" and "),$t=t("a"),bl=s("BertTokenizerFast"),kl=s(`. See the
`),vn=t("code"),Fl=s("__call__()"),yl=s(" and "),Tt=t("a"),$l=s("decode()"),Tl=s(" for more information."),wl=m(),Je=t("div"),b(ra.$$.fragment),xl=m(),sa=t("p"),Ml=s("This method forwards all its arguments to BertTokenizerFast\u2019s "),wt=t("a"),zl=s("batch_decode()"),Il=s(`. Please
refer to the docstring of this method for more information.`),Pl=m(),Ke=t("div"),b(ia.$$.fragment),Cl=m(),la=t("p"),El=s("This method forwards all its arguments to BertTokenizerFast\u2019s "),xt=t("a"),Al=s("decode()"),Nl=s(`. Please refer to
the docstring of this method for more information.`),Tr=m(),ve=t("h2"),Ye=t("a"),bn=t("span"),b(da.$$.fragment),ql=m(),kn=t("span"),Ll=s("FlavaFeatureExtractor"),wr=m(),I=t("div"),b(ca.$$.fragment),jl=m(),Fn=t("p"),Ol=s("Constructs a Flava image processor."),Dl=m(),Ze=t("div"),b(ma.$$.fragment),Wl=m(),be=t("p"),Sl=s("Center crop an image to "),yn=t("code"),Vl=s('(size["height"], size["width"])'),Bl=s(". If the input size is smaller than "),$n=t("code"),Rl=s("crop_size"),Ul=s(` along
any edge, the image is padded with 0\u2019s and then center cropped.`),Hl=m(),Xe=t("div"),b(fa.$$.fragment),Gl=m(),Tn=t("p"),Jl=s("Normalize an image. image = (image - image_mean) / image_std."),Kl=m(),Qe=t("div"),b(pa.$$.fragment),Yl=m(),wn=t("p"),Zl=s("Preprocess an image or batch of images."),Xl=m(),eo=t("div"),b(ga.$$.fragment),Ql=m(),xn=t("p"),ed=s("Rescale an image by a scale factor. image = image * scale."),od=m(),oo=t("div"),b(ha.$$.fragment),ad=m(),ua=t("p"),td=s("Resize an image to "),Mn=t("code"),nd=s('(size["height"], size["width"])'),rd=s("."),xr=m(),ke=t("h2"),ao=t("a"),zn=t("span"),b(_a.$$.fragment),sd=m(),In=t("span"),id=s("FlavaForPreTraining"),Mr=m(),W=t("div"),b(va.$$.fragment),ld=m(),Pn=t("p"),dd=s("The FLAVA model for pretraining which outputs losses, embeddings, logits and transformer outputs."),cd=m(),ba=t("p"),md=s("This model is a PyTorch "),ka=t("a"),fd=s("torch.nn.Module"),pd=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),gd=m(),Q=t("div"),b(Fa.$$.fragment),hd=m(),Fe=t("p"),ud=s("The "),Mt=t("a"),_d=s("FlavaForPreTraining"),vd=s(" forward method, overrides the "),Cn=t("code"),bd=s("__call__"),kd=s(" special method."),Fd=m(),b(to.$$.fragment),zr=m(),ye=t("h2"),no=t("a"),En=t("span"),b(ya.$$.fragment),yd=m(),An=t("span"),$d=s("FlavaModel"),Ir=m(),L=t("div"),b($a.$$.fragment),Td=m(),Ta=t("p"),wd=s(`The bare FLAVA Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),wa=t("a"),xd=s("torch.nn.Module"),Md=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),zd=m(),S=t("div"),b(xa.$$.fragment),Id=m(),$e=t("p"),Pd=s("The "),zt=t("a"),Cd=s("FlavaModel"),Ed=s(" forward method, overrides the "),Nn=t("code"),Ad=s("__call__"),Nd=s(" special method."),qd=m(),b(ro.$$.fragment),Ld=m(),b(so.$$.fragment),jd=m(),ee=t("div"),b(Ma.$$.fragment),Od=m(),Te=t("p"),Dd=s("The "),It=t("a"),Wd=s("FlavaModel"),Sd=s(" forward method, overrides the "),qn=t("code"),Vd=s("__call__"),Bd=s(" special method."),Rd=m(),b(io.$$.fragment),Ud=m(),oe=t("div"),b(za.$$.fragment),Hd=m(),we=t("p"),Gd=s("The "),Pt=t("a"),Jd=s("FlavaModel"),Kd=s(" forward method, overrides the "),Ln=t("code"),Yd=s("__call__"),Zd=s(" special method."),Xd=m(),b(lo.$$.fragment),Pr=m(),xe=t("h2"),co=t("a"),jn=t("span"),b(Ia.$$.fragment),Qd=m(),On=t("span"),ec=s("FlavaImageCodebook"),Cr=m(),C=t("div"),b(Pa.$$.fragment),oc=m(),Ca=t("p"),ac=s(`The FLAVA\u2019s image codebook model inspired from DALL-E\u2019s original encoder. Outputs raw hidden states and can be used
to generate image tokens for an image based on DALL-E\u2019s vocab. Used to generate labels for MIM. Use
`),Dn=t("code"),tc=s("get_codebook_indices"),nc=s(" to get image tokens for an image."),rc=m(),Ea=t("p"),sc=s("This model is a PyTorch "),Aa=t("a"),ic=s("torch.nn.Module"),lc=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),dc=m(),Ct=t("div"),b(Na.$$.fragment),cc=m(),Et=t("div"),b(qa.$$.fragment),mc=m(),At=t("div"),b(La.$$.fragment),Er=m(),Me=t("h2"),mo=t("a"),Wn=t("span"),b(ja.$$.fragment),fc=m(),Sn=t("span"),pc=s("FlavaTextModel"),Ar=m(),G=t("div"),b(Oa.$$.fragment),gc=m(),Da=t("p"),hc=s(`The bare FLAVA Text Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Wa=t("a"),uc=s("torch.nn.Module"),_c=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),vc=m(),V=t("div"),b(Sa.$$.fragment),bc=m(),ze=t("p"),kc=s("The "),Nt=t("a"),Fc=s("FlavaTextModel"),yc=s(" forward method, overrides the "),Vn=t("code"),$c=s("__call__"),Tc=s(" special method."),wc=m(),b(fo.$$.fragment),xc=m(),b(po.$$.fragment),Nr=m(),Ie=t("h2"),go=t("a"),Bn=t("span"),b(Va.$$.fragment),Mc=m(),Rn=t("span"),zc=s("FlavaImageModel"),qr=m(),J=t("div"),b(Ba.$$.fragment),Ic=m(),Ra=t("p"),Pc=s(`The bare FLAVA Image Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Ua=t("a"),Cc=s("torch.nn.Module"),Ec=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ac=m(),B=t("div"),b(Ha.$$.fragment),Nc=m(),Pe=t("p"),qc=s("The "),qt=t("a"),Lc=s("FlavaImageModel"),jc=s(" forward method, overrides the "),Un=t("code"),Oc=s("__call__"),Dc=s(" special method."),Wc=m(),b(ho.$$.fragment),Sc=m(),b(uo.$$.fragment),Lr=m(),Ce=t("h2"),_o=t("a"),Hn=t("span"),b(Ga.$$.fragment),Vc=m(),Gn=t("span"),Bc=s("FlavaMultimodalModel"),jr=m(),K=t("div"),b(Ja.$$.fragment),Rc=m(),Ka=t("p"),Uc=s(`The bare FLAVA Multimodal Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Ya=t("a"),Hc=s("torch.nn.Module"),Gc=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Jc=m(),R=t("div"),b(Za.$$.fragment),Kc=m(),Ee=t("p"),Yc=s("The "),Lt=t("a"),Zc=s("FlavaMultimodalModel"),Xc=s(" forward method, overrides the "),Jn=t("code"),Qc=s("__call__"),em=s(" special method."),om=m(),b(vo.$$.fragment),am=m(),b(bo.$$.fragment),this.h()},l(o){const h=ep('[data-svelte="svelte-1phssyn"]',document.head);c=n(h,"META",{name:!0,content:!0}),h.forEach(a),w=f(o),u=n(o,"H1",{class:!0});var Xa=r(u);g=n(Xa,"A",{id:!0,class:!0,href:!0});var Kn=r(g);v=n(Kn,"SPAN",{});var Yn=r(v);k(l.$$.fragment,Yn),Yn.forEach(a),Kn.forEach(a),p=f(Xa),z=n(Xa,"SPAN",{});var Zn=r(z);ws=i(Zn,"FLAVA"),Zn.forEach(a),Xa.forEach(a),sr=f(o),ie=n(o,"H2",{class:!0});var Qa=r(ie);Ne=n(Qa,"A",{id:!0,class:!0,href:!0});var Xn=r(Ne);on=n(Xn,"SPAN",{});var Qn=r(on);k(zo.$$.fragment,Qn),Qn.forEach(a),Xn.forEach(a),xs=f(Qa),an=n(Qa,"SPAN",{});var er=r(an);Ms=i(er,"Overview"),er.forEach(a),Qa.forEach(a),ir=f(o),qe=n(o,"P",{});var et=r(qe);zs=i(et,"The FLAVA model was proposed in "),Io=n(et,"A",{href:!0,rel:!0});var or=r(Io);Is=i(or,"FLAVA: A Foundational Language And Vision Alignment Model"),or.forEach(a),Ps=i(et," by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022."),et.forEach(a),lr=f(o),tt=n(o,"P",{});var ar=r(tt);Cs=i(ar,`The paper aims at creating a single unified foundation model which can work across vision, language
as well as vision-and-language multimodal tasks.`),ar.forEach(a),dr=f(o),nt=n(o,"P",{});var tr=r(nt);Es=i(tr,"The abstract from the paper is the following:"),tr.forEach(a),cr=f(o),rt=n(o,"P",{});var nr=r(rt);tn=n(nr,"EM",{});var rr=r(tn);As=i(rr,`State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety
of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal
(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising
direction would be to use a single holistic universal model, as a \u201Cfoundation\u201D, that targets all modalities
at once \u2014 a true vision and language foundation model should be good at vision tasks, language tasks, and
cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate
impressive performance on a wide range of 35 tasks spanning these target modalities.`),rr.forEach(a),nr.forEach(a),mr=f(o),Z=n(o,"P",{});var Ae=r(Z);Ns=i(Ae,"This model was contributed by "),Po=n(Ae,"A",{href:!0,rel:!0});var tm=r(Po);qs=i(tm,"aps"),tm.forEach(a),Ls=i(Ae,". The original code can be found "),Co=n(Ae,"A",{href:!0,rel:!0});var nm=r(Co);js=i(nm,"here"),nm.forEach(a),Os=i(Ae,"."),Ae.forEach(a),fr=f(o),le=n(o,"H2",{class:!0});var Dr=r(le);Le=n(Dr,"A",{id:!0,class:!0,href:!0});var rm=r(Le);nn=n(rm,"SPAN",{});var sm=r(nn);k(Eo.$$.fragment,sm),sm.forEach(a),rm.forEach(a),Ds=f(Dr),rn=n(Dr,"SPAN",{});var im=r(rn);Ws=i(im,"FlavaConfig"),im.forEach(a),Dr.forEach(a),pr=f(o),P=n(o,"DIV",{class:!0});var U=r(P);k(Ao.$$.fragment,U),Ss=f(U),X=n(U,"P",{});var ot=r(X);st=n(ot,"A",{href:!0});var lm=r(st);Vs=i(lm,"FlavaConfig"),lm.forEach(a),Bs=i(ot," is the configuration class to store the configuration of a "),it=n(ot,"A",{href:!0});var dm=r(it);Rs=i(dm,"FlavaModel"),dm.forEach(a),Us=i(ot,`. It is used to
instantiate FLAVA model according to the specified arguments, defining the text model, image model, image codebook
and multimodal model configs. Instantiating a configuration with the defaults will yield a similar configuration to
that of the FLAVA `),No=n(ot,"A",{href:!0,rel:!0});var cm=r(No);Hs=i(cm,"facebook/flava-full"),cm.forEach(a),Gs=i(ot," architecture."),ot.forEach(a),Js=f(U),de=n(U,"P",{});var jt=r(de);Ks=i(jt,"Configuration objects inherit from "),lt=n(jt,"A",{href:!0});var mm=r(lt);Ys=i(mm,"PretrainedConfig"),mm.forEach(a),Zs=i(jt,` and can be used to control the model outputs. Read the
documentation from `),dt=n(jt,"A",{href:!0});var fm=r(dt);Xs=i(fm,"PretrainedConfig"),fm.forEach(a),Qs=i(jt," for more information."),jt.forEach(a),ei=f(U),k(je.$$.fragment,U),oi=f(U),Oe=n(U,"DIV",{class:!0});var Wr=r(Oe);k(qo.$$.fragment,Wr),ai=f(Wr),Lo=n(Wr,"P",{});var Sr=r(Lo);ti=i(Sr,"Instantiate a "),ct=n(Sr,"A",{href:!0});var pm=r(ct);ni=i(pm,"FlavaConfig"),pm.forEach(a),ri=i(Sr,` (or a derived class) from flava text model configuration, flava image model
configuration, flava multimodal model and flava codebook model configuration.`),Sr.forEach(a),Wr.forEach(a),si=f(U),De=n(U,"DIV",{class:!0});var Vr=r(De);k(jo.$$.fragment,Vr),ii=f(Vr),Oo=n(Vr,"P",{});var Br=r(Oo);li=i(Br,"Serializes this instance to a Python dictionary. Override the default "),mt=n(Br,"A",{href:!0});var gm=r(mt);di=i(gm,"to_dict()"),gm.forEach(a),ci=i(Br,"."),Br.forEach(a),Vr.forEach(a),U.forEach(a),gr=f(o),ce=n(o,"H2",{class:!0});var Rr=r(ce);We=n(Rr,"A",{id:!0,class:!0,href:!0});var hm=r(We);sn=n(hm,"SPAN",{});var um=r(sn);k(Do.$$.fragment,um),um.forEach(a),hm.forEach(a),mi=f(Rr),ln=n(Rr,"SPAN",{});var _m=r(ln);fi=i(_m,"FlavaTextConfig"),_m.forEach(a),Rr.forEach(a),hr=f(o),E=n(o,"DIV",{class:!0});var ae=r(E);k(Wo.$$.fragment,ae),pi=f(ae),So=n(ae,"P",{});var Ur=r(So);gi=i(Ur,"This is the configuration class to store the configuration of a "),ft=n(Ur,"A",{href:!0});var vm=r(ft);hi=i(vm,"FlavaTextModel"),vm.forEach(a),ui=i(Ur,`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture.`),Ur.forEach(a),_i=f(ae),Vo=n(ae,"P",{});var Hr=r(Vo);vi=i(Hr,`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
`),Bo=n(Hr,"A",{href:!0,rel:!0});var bm=r(Bo);bi=i(bm,"facebook/flava-full"),bm.forEach(a),ki=i(Hr," architecture."),Hr.forEach(a),Fi=f(ae),me=n(ae,"P",{});var Ot=r(me);yi=i(Ot,"Configuration objects inherit from "),pt=n(Ot,"A",{href:!0});var km=r(pt);$i=i(km,"PretrainedConfig"),km.forEach(a),Ti=i(Ot,` and can be used to control the model outputs. Read the
documentation from `),gt=n(Ot,"A",{href:!0});var Fm=r(gt);wi=i(Fm,"PretrainedConfig"),Fm.forEach(a),xi=i(Ot," for more information."),Ot.forEach(a),Mi=f(ae),k(Se.$$.fragment,ae),ae.forEach(a),ur=f(o),fe=n(o,"H2",{class:!0});var Gr=r(fe);Ve=n(Gr,"A",{id:!0,class:!0,href:!0});var ym=r(Ve);dn=n(ym,"SPAN",{});var $m=r(dn);k(Ro.$$.fragment,$m),$m.forEach(a),ym.forEach(a),zi=f(Gr),cn=n(Gr,"SPAN",{});var Tm=r(cn);Ii=i(Tm,"FlavaImageConfig"),Tm.forEach(a),Gr.forEach(a),_r=f(o),A=n(o,"DIV",{class:!0});var te=r(A);k(Uo.$$.fragment,te),Pi=f(te),Ho=n(te,"P",{});var Jr=r(Ho);Ci=i(Jr,"This is the configuration class to store the configuration of a "),ht=n(Jr,"A",{href:!0});var wm=r(ht);Ei=i(wm,"FlavaImageModel"),wm.forEach(a),Ai=i(Jr,`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture.`),Jr.forEach(a),Ni=f(te),Go=n(te,"P",{});var Kr=r(Go);qi=i(Kr,`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
`),Jo=n(Kr,"A",{href:!0,rel:!0});var xm=r(Jo);Li=i(xm,"facebook/flava-full"),xm.forEach(a),ji=i(Kr," architecture."),Kr.forEach(a),Oi=f(te),pe=n(te,"P",{});var Dt=r(pe);Di=i(Dt,"Configuration objects inherit from "),ut=n(Dt,"A",{href:!0});var Mm=r(ut);Wi=i(Mm,"PretrainedConfig"),Mm.forEach(a),Si=i(Dt,` and can be used to control the model outputs. Read the
documentation from `),_t=n(Dt,"A",{href:!0});var zm=r(_t);Vi=i(zm,"PretrainedConfig"),zm.forEach(a),Bi=i(Dt," for more information."),Dt.forEach(a),Ri=f(te),k(Be.$$.fragment,te),te.forEach(a),vr=f(o),ge=n(o,"H2",{class:!0});var Yr=r(ge);Re=n(Yr,"A",{id:!0,class:!0,href:!0});var Im=r(Re);mn=n(Im,"SPAN",{});var Pm=r(mn);k(Ko.$$.fragment,Pm),Pm.forEach(a),Im.forEach(a),Ui=f(Yr),fn=n(Yr,"SPAN",{});var Cm=r(fn);Hi=i(Cm,"FlavaMultimodalConfig"),Cm.forEach(a),Yr.forEach(a),br=f(o),N=n(o,"DIV",{class:!0});var ne=r(N);k(Yo.$$.fragment,ne),Gi=f(ne),Zo=n(ne,"P",{});var Zr=r(Zo);Ji=i(Zr,"This is the configuration class to store the configuration of a "),vt=n(Zr,"A",{href:!0});var Em=r(vt);Ki=i(Em,"FlavaMultimodalModel"),Em.forEach(a),Yi=i(Zr,`. It is used to instantiate
an FLAVA model according to the specified arguments, defining the model architecture.`),Zr.forEach(a),Zi=f(ne),Xo=n(ne,"P",{});var Xr=r(Xo);Xi=i(Xr,`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
`),Qo=n(Xr,"A",{href:!0,rel:!0});var Am=r(Qo);Qi=i(Am,"facebook/flava-full"),Am.forEach(a),el=i(Xr," architecture."),Xr.forEach(a),ol=f(ne),he=n(ne,"P",{});var Wt=r(he);al=i(Wt,"Configuration objects inherit from "),bt=n(Wt,"A",{href:!0});var Nm=r(bt);tl=i(Nm,"PretrainedConfig"),Nm.forEach(a),nl=i(Wt,` and can be used to control the model outputs. Read the
documentation from `),kt=n(Wt,"A",{href:!0});var qm=r(kt);rl=i(qm,"PretrainedConfig"),qm.forEach(a),sl=i(Wt," for more information."),Wt.forEach(a),il=f(ne),k(Ue.$$.fragment,ne),ne.forEach(a),kr=f(o),ue=n(o,"H2",{class:!0});var Qr=r(ue);He=n(Qr,"A",{id:!0,class:!0,href:!0});var Lm=r(He);pn=n(Lm,"SPAN",{});var jm=r(pn);k(ea.$$.fragment,jm),jm.forEach(a),Lm.forEach(a),ll=f(Qr),gn=n(Qr,"SPAN",{});var Om=r(gn);dl=i(Om,"FlavaImageCodebookConfig"),Om.forEach(a),Qr.forEach(a),Fr=f(o),oa=n(o,"DIV",{class:!0});var Dm=r(oa);k(aa.$$.fragment,Dm),Dm.forEach(a),yr=f(o),_e=n(o,"H2",{class:!0});var es=r(_e);Ge=n(es,"A",{id:!0,class:!0,href:!0});var Wm=r(Ge);hn=n(Wm,"SPAN",{});var Sm=r(hn);k(ta.$$.fragment,Sm),Sm.forEach(a),Wm.forEach(a),cl=f(es),un=n(es,"SPAN",{});var Vm=r(un);ml=i(Vm,"FlavaProcessor"),Vm.forEach(a),es.forEach(a),$r=f(o),q=n(o,"DIV",{class:!0});var re=r(q);k(na.$$.fragment,re),fl=f(re),_n=n(re,"P",{});var Bm=r(_n);pl=i(Bm,"Constructs a FLAVA processor which wraps a FLAVA feature extractor and a FLAVA tokenizer into a single processor."),Bm.forEach(a),gl=f(re),O=n(re,"P",{});var Y=r(O);Ft=n(Y,"A",{href:!0});var Rm=r(Ft);hl=i(Rm,"FlavaProcessor"),Rm.forEach(a),ul=i(Y," offers all the functionalities of "),yt=n(Y,"A",{href:!0});var Um=r(yt);_l=i(Um,"FlavaFeatureExtractor"),Um.forEach(a),vl=i(Y," and "),$t=n(Y,"A",{href:!0});var Hm=r($t);bl=i(Hm,"BertTokenizerFast"),Hm.forEach(a),kl=i(Y,`. See the
`),vn=n(Y,"CODE",{});var Gm=r(vn);Fl=i(Gm,"__call__()"),Gm.forEach(a),yl=i(Y," and "),Tt=n(Y,"A",{href:!0});var Jm=r(Tt);$l=i(Jm,"decode()"),Jm.forEach(a),Tl=i(Y," for more information."),Y.forEach(a),wl=f(re),Je=n(re,"DIV",{class:!0});var os=r(Je);k(ra.$$.fragment,os),xl=f(os),sa=n(os,"P",{});var as=r(sa);Ml=i(as,"This method forwards all its arguments to BertTokenizerFast\u2019s "),wt=n(as,"A",{href:!0});var Km=r(wt);zl=i(Km,"batch_decode()"),Km.forEach(a),Il=i(as,`. Please
refer to the docstring of this method for more information.`),as.forEach(a),os.forEach(a),Pl=f(re),Ke=n(re,"DIV",{class:!0});var ts=r(Ke);k(ia.$$.fragment,ts),Cl=f(ts),la=n(ts,"P",{});var ns=r(la);El=i(ns,"This method forwards all its arguments to BertTokenizerFast\u2019s "),xt=n(ns,"A",{href:!0});var Ym=r(xt);Al=i(Ym,"decode()"),Ym.forEach(a),Nl=i(ns,`. Please refer to
the docstring of this method for more information.`),ns.forEach(a),ts.forEach(a),re.forEach(a),Tr=f(o),ve=n(o,"H2",{class:!0});var rs=r(ve);Ye=n(rs,"A",{id:!0,class:!0,href:!0});var Zm=r(Ye);bn=n(Zm,"SPAN",{});var Xm=r(bn);k(da.$$.fragment,Xm),Xm.forEach(a),Zm.forEach(a),ql=f(rs),kn=n(rs,"SPAN",{});var Qm=r(kn);Ll=i(Qm,"FlavaFeatureExtractor"),Qm.forEach(a),rs.forEach(a),wr=f(o),I=n(o,"DIV",{class:!0});var D=r(I);k(ca.$$.fragment,D),jl=f(D),Fn=n(D,"P",{});var ef=r(Fn);Ol=i(ef,"Constructs a Flava image processor."),ef.forEach(a),Dl=f(D),Ze=n(D,"DIV",{class:!0});var ss=r(Ze);k(ma.$$.fragment,ss),Wl=f(ss),be=n(ss,"P",{});var St=r(be);Sl=i(St,"Center crop an image to "),yn=n(St,"CODE",{});var of=r(yn);Vl=i(of,'(size["height"], size["width"])'),of.forEach(a),Bl=i(St,". If the input size is smaller than "),$n=n(St,"CODE",{});var af=r($n);Rl=i(af,"crop_size"),af.forEach(a),Ul=i(St,` along
any edge, the image is padded with 0\u2019s and then center cropped.`),St.forEach(a),ss.forEach(a),Hl=f(D),Xe=n(D,"DIV",{class:!0});var is=r(Xe);k(fa.$$.fragment,is),Gl=f(is),Tn=n(is,"P",{});var tf=r(Tn);Jl=i(tf,"Normalize an image. image = (image - image_mean) / image_std."),tf.forEach(a),is.forEach(a),Kl=f(D),Qe=n(D,"DIV",{class:!0});var ls=r(Qe);k(pa.$$.fragment,ls),Yl=f(ls),wn=n(ls,"P",{});var nf=r(wn);Zl=i(nf,"Preprocess an image or batch of images."),nf.forEach(a),ls.forEach(a),Xl=f(D),eo=n(D,"DIV",{class:!0});var ds=r(eo);k(ga.$$.fragment,ds),Ql=f(ds),xn=n(ds,"P",{});var rf=r(xn);ed=i(rf,"Rescale an image by a scale factor. image = image * scale."),rf.forEach(a),ds.forEach(a),od=f(D),oo=n(D,"DIV",{class:!0});var cs=r(oo);k(ha.$$.fragment,cs),ad=f(cs),ua=n(cs,"P",{});var ms=r(ua);td=i(ms,"Resize an image to "),Mn=n(ms,"CODE",{});var sf=r(Mn);nd=i(sf,'(size["height"], size["width"])'),sf.forEach(a),rd=i(ms,"."),ms.forEach(a),cs.forEach(a),D.forEach(a),xr=f(o),ke=n(o,"H2",{class:!0});var fs=r(ke);ao=n(fs,"A",{id:!0,class:!0,href:!0});var lf=r(ao);zn=n(lf,"SPAN",{});var df=r(zn);k(_a.$$.fragment,df),df.forEach(a),lf.forEach(a),sd=f(fs),In=n(fs,"SPAN",{});var cf=r(In);id=i(cf,"FlavaForPreTraining"),cf.forEach(a),fs.forEach(a),Mr=f(o),W=n(o,"DIV",{class:!0});var ko=r(W);k(va.$$.fragment,ko),ld=f(ko),Pn=n(ko,"P",{});var mf=r(Pn);dd=i(mf,"The FLAVA model for pretraining which outputs losses, embeddings, logits and transformer outputs."),mf.forEach(a),cd=f(ko),ba=n(ko,"P",{});var ps=r(ba);md=i(ps,"This model is a PyTorch "),ka=n(ps,"A",{href:!0,rel:!0});var ff=r(ka);fd=i(ff,"torch.nn.Module"),ff.forEach(a),pd=i(ps,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ps.forEach(a),gd=f(ko),Q=n(ko,"DIV",{class:!0});var Vt=r(Q);k(Fa.$$.fragment,Vt),hd=f(Vt),Fe=n(Vt,"P",{});var Bt=r(Fe);ud=i(Bt,"The "),Mt=n(Bt,"A",{href:!0});var pf=r(Mt);_d=i(pf,"FlavaForPreTraining"),pf.forEach(a),vd=i(Bt," forward method, overrides the "),Cn=n(Bt,"CODE",{});var gf=r(Cn);bd=i(gf,"__call__"),gf.forEach(a),kd=i(Bt," special method."),Bt.forEach(a),Fd=f(Vt),k(to.$$.fragment,Vt),Vt.forEach(a),ko.forEach(a),zr=f(o),ye=n(o,"H2",{class:!0});var gs=r(ye);no=n(gs,"A",{id:!0,class:!0,href:!0});var hf=r(no);En=n(hf,"SPAN",{});var uf=r(En);k(ya.$$.fragment,uf),uf.forEach(a),hf.forEach(a),yd=f(gs),An=n(gs,"SPAN",{});var _f=r(An);$d=i(_f,"FlavaModel"),_f.forEach(a),gs.forEach(a),Ir=f(o),L=n(o,"DIV",{class:!0});var se=r(L);k($a.$$.fragment,se),Td=f(se),Ta=n(se,"P",{});var hs=r(Ta);wd=i(hs,`The bare FLAVA Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),wa=n(hs,"A",{href:!0,rel:!0});var vf=r(wa);xd=i(vf,"torch.nn.Module"),vf.forEach(a),Md=i(hs,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),hs.forEach(a),zd=f(se),S=n(se,"DIV",{class:!0});var Fo=r(S);k(xa.$$.fragment,Fo),Id=f(Fo),$e=n(Fo,"P",{});var Rt=r($e);Pd=i(Rt,"The "),zt=n(Rt,"A",{href:!0});var bf=r(zt);Cd=i(bf,"FlavaModel"),bf.forEach(a),Ed=i(Rt," forward method, overrides the "),Nn=n(Rt,"CODE",{});var kf=r(Nn);Ad=i(kf,"__call__"),kf.forEach(a),Nd=i(Rt," special method."),Rt.forEach(a),qd=f(Fo),k(ro.$$.fragment,Fo),Ld=f(Fo),k(so.$$.fragment,Fo),Fo.forEach(a),jd=f(se),ee=n(se,"DIV",{class:!0});var Ut=r(ee);k(Ma.$$.fragment,Ut),Od=f(Ut),Te=n(Ut,"P",{});var Ht=r(Te);Dd=i(Ht,"The "),It=n(Ht,"A",{href:!0});var Ff=r(It);Wd=i(Ff,"FlavaModel"),Ff.forEach(a),Sd=i(Ht," forward method, overrides the "),qn=n(Ht,"CODE",{});var yf=r(qn);Vd=i(yf,"__call__"),yf.forEach(a),Bd=i(Ht," special method."),Ht.forEach(a),Rd=f(Ut),k(io.$$.fragment,Ut),Ut.forEach(a),Ud=f(se),oe=n(se,"DIV",{class:!0});var Gt=r(oe);k(za.$$.fragment,Gt),Hd=f(Gt),we=n(Gt,"P",{});var Jt=r(we);Gd=i(Jt,"The "),Pt=n(Jt,"A",{href:!0});var $f=r(Pt);Jd=i($f,"FlavaModel"),$f.forEach(a),Kd=i(Jt," forward method, overrides the "),Ln=n(Jt,"CODE",{});var Tf=r(Ln);Yd=i(Tf,"__call__"),Tf.forEach(a),Zd=i(Jt," special method."),Jt.forEach(a),Xd=f(Gt),k(lo.$$.fragment,Gt),Gt.forEach(a),se.forEach(a),Pr=f(o),xe=n(o,"H2",{class:!0});var us=r(xe);co=n(us,"A",{id:!0,class:!0,href:!0});var wf=r(co);jn=n(wf,"SPAN",{});var xf=r(jn);k(Ia.$$.fragment,xf),xf.forEach(a),wf.forEach(a),Qd=f(us),On=n(us,"SPAN",{});var Mf=r(On);ec=i(Mf,"FlavaImageCodebook"),Mf.forEach(a),us.forEach(a),Cr=f(o),C=n(o,"DIV",{class:!0});var H=r(C);k(Pa.$$.fragment,H),oc=f(H),Ca=n(H,"P",{});var _s=r(Ca);ac=i(_s,`The FLAVA\u2019s image codebook model inspired from DALL-E\u2019s original encoder. Outputs raw hidden states and can be used
to generate image tokens for an image based on DALL-E\u2019s vocab. Used to generate labels for MIM. Use
`),Dn=n(_s,"CODE",{});var zf=r(Dn);tc=i(zf,"get_codebook_indices"),zf.forEach(a),nc=i(_s," to get image tokens for an image."),_s.forEach(a),rc=f(H),Ea=n(H,"P",{});var vs=r(Ea);sc=i(vs,"This model is a PyTorch "),Aa=n(vs,"A",{href:!0,rel:!0});var If=r(Aa);ic=i(If,"torch.nn.Module"),If.forEach(a),lc=i(vs,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),vs.forEach(a),dc=f(H),Ct=n(H,"DIV",{class:!0});var Pf=r(Ct);k(Na.$$.fragment,Pf),Pf.forEach(a),cc=f(H),Et=n(H,"DIV",{class:!0});var Cf=r(Et);k(qa.$$.fragment,Cf),Cf.forEach(a),mc=f(H),At=n(H,"DIV",{class:!0});var Ef=r(At);k(La.$$.fragment,Ef),Ef.forEach(a),H.forEach(a),Er=f(o),Me=n(o,"H2",{class:!0});var bs=r(Me);mo=n(bs,"A",{id:!0,class:!0,href:!0});var Af=r(mo);Wn=n(Af,"SPAN",{});var Nf=r(Wn);k(ja.$$.fragment,Nf),Nf.forEach(a),Af.forEach(a),fc=f(bs),Sn=n(bs,"SPAN",{});var qf=r(Sn);pc=i(qf,"FlavaTextModel"),qf.forEach(a),bs.forEach(a),Ar=f(o),G=n(o,"DIV",{class:!0});var Kt=r(G);k(Oa.$$.fragment,Kt),gc=f(Kt),Da=n(Kt,"P",{});var ks=r(Da);hc=i(ks,`The bare FLAVA Text Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Wa=n(ks,"A",{href:!0,rel:!0});var Lf=r(Wa);uc=i(Lf,"torch.nn.Module"),Lf.forEach(a),_c=i(ks,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ks.forEach(a),vc=f(Kt),V=n(Kt,"DIV",{class:!0});var yo=r(V);k(Sa.$$.fragment,yo),bc=f(yo),ze=n(yo,"P",{});var Yt=r(ze);kc=i(Yt,"The "),Nt=n(Yt,"A",{href:!0});var jf=r(Nt);Fc=i(jf,"FlavaTextModel"),jf.forEach(a),yc=i(Yt," forward method, overrides the "),Vn=n(Yt,"CODE",{});var Of=r(Vn);$c=i(Of,"__call__"),Of.forEach(a),Tc=i(Yt," special method."),Yt.forEach(a),wc=f(yo),k(fo.$$.fragment,yo),xc=f(yo),k(po.$$.fragment,yo),yo.forEach(a),Kt.forEach(a),Nr=f(o),Ie=n(o,"H2",{class:!0});var Fs=r(Ie);go=n(Fs,"A",{id:!0,class:!0,href:!0});var Df=r(go);Bn=n(Df,"SPAN",{});var Wf=r(Bn);k(Va.$$.fragment,Wf),Wf.forEach(a),Df.forEach(a),Mc=f(Fs),Rn=n(Fs,"SPAN",{});var Sf=r(Rn);zc=i(Sf,"FlavaImageModel"),Sf.forEach(a),Fs.forEach(a),qr=f(o),J=n(o,"DIV",{class:!0});var Zt=r(J);k(Ba.$$.fragment,Zt),Ic=f(Zt),Ra=n(Zt,"P",{});var ys=r(Ra);Pc=i(ys,`The bare FLAVA Image Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Ua=n(ys,"A",{href:!0,rel:!0});var Vf=r(Ua);Cc=i(Vf,"torch.nn.Module"),Vf.forEach(a),Ec=i(ys,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ys.forEach(a),Ac=f(Zt),B=n(Zt,"DIV",{class:!0});var $o=r(B);k(Ha.$$.fragment,$o),Nc=f($o),Pe=n($o,"P",{});var Xt=r(Pe);qc=i(Xt,"The "),qt=n(Xt,"A",{href:!0});var Bf=r(qt);Lc=i(Bf,"FlavaImageModel"),Bf.forEach(a),jc=i(Xt," forward method, overrides the "),Un=n(Xt,"CODE",{});var Rf=r(Un);Oc=i(Rf,"__call__"),Rf.forEach(a),Dc=i(Xt," special method."),Xt.forEach(a),Wc=f($o),k(ho.$$.fragment,$o),Sc=f($o),k(uo.$$.fragment,$o),$o.forEach(a),Zt.forEach(a),Lr=f(o),Ce=n(o,"H2",{class:!0});var $s=r(Ce);_o=n($s,"A",{id:!0,class:!0,href:!0});var Uf=r(_o);Hn=n(Uf,"SPAN",{});var Hf=r(Hn);k(Ga.$$.fragment,Hf),Hf.forEach(a),Uf.forEach(a),Vc=f($s),Gn=n($s,"SPAN",{});var Gf=r(Gn);Bc=i(Gf,"FlavaMultimodalModel"),Gf.forEach(a),$s.forEach(a),jr=f(o),K=n(o,"DIV",{class:!0});var Qt=r(K);k(Ja.$$.fragment,Qt),Rc=f(Qt),Ka=n(Qt,"P",{});var Ts=r(Ka);Uc=i(Ts,`The bare FLAVA Multimodal Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Ya=n(Ts,"A",{href:!0,rel:!0});var Jf=r(Ya);Hc=i(Jf,"torch.nn.Module"),Jf.forEach(a),Gc=i(Ts,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ts.forEach(a),Jc=f(Qt),R=n(Qt,"DIV",{class:!0});var To=r(R);k(Za.$$.fragment,To),Kc=f(To),Ee=n(To,"P",{});var en=r(Ee);Yc=i(en,"The "),Lt=n(en,"A",{href:!0});var Kf=r(Lt);Zc=i(Kf,"FlavaMultimodalModel"),Kf.forEach(a),Xc=i(en," forward method, overrides the "),Jn=n(en,"CODE",{});var Yf=r(Jn);Qc=i(Yf,"__call__"),Yf.forEach(a),em=i(en," special method."),en.forEach(a),om=f(To),k(vo.$$.fragment,To),am=f(To),k(bo.$$.fragment,To),To.forEach(a),Qt.forEach(a),this.h()},h(){d(c,"name","hf:doc:metadata"),d(c,"content",JSON.stringify(vp)),d(g,"id","flava"),d(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(g,"href","#flava"),d(u,"class","relative group"),d(Ne,"id","overview"),d(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ne,"href","#overview"),d(ie,"class","relative group"),d(Io,"href","https://arxiv.org/abs/2112.04482"),d(Io,"rel","nofollow"),d(Po,"href","https://huggingface.co/aps"),d(Po,"rel","nofollow"),d(Co,"href","https://github.com/facebookresearch/multimodal/tree/main/examples/flava"),d(Co,"rel","nofollow"),d(Le,"id","transformers.FlavaConfig"),d(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Le,"href","#transformers.FlavaConfig"),d(le,"class","relative group"),d(st,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig"),d(it,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaModel"),d(No,"href","https://huggingface.co/facebook/flava-full"),d(No,"rel","nofollow"),d(lt,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(dt,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(ct,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig"),d(Oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(mt,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.to_dict"),d(De,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(We,"id","transformers.FlavaTextConfig"),d(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(We,"href","#transformers.FlavaTextConfig"),d(ce,"class","relative group"),d(ft,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"),d(Bo,"href","https://huggingface.co/facebook/flava-full"),d(Bo,"rel","nofollow"),d(pt,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(gt,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ve,"id","transformers.FlavaImageConfig"),d(Ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ve,"href","#transformers.FlavaImageConfig"),d(fe,"class","relative group"),d(ht,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"),d(Jo,"href","https://huggingface.co/facebook/flava-full"),d(Jo,"rel","nofollow"),d(ut,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(_t,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Re,"id","transformers.FlavaMultimodalConfig"),d(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Re,"href","#transformers.FlavaMultimodalConfig"),d(ge,"class","relative group"),d(vt,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalModel"),d(Qo,"href","https://huggingface.co/facebook/flava-full"),d(Qo,"rel","nofollow"),d(bt,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(kt,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(He,"id","transformers.FlavaImageCodebookConfig"),d(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(He,"href","#transformers.FlavaImageCodebookConfig"),d(ue,"class","relative group"),d(oa,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ge,"id","transformers.FlavaProcessor"),d(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ge,"href","#transformers.FlavaProcessor"),d(_e,"class","relative group"),d(Ft,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaProcessor"),d(yt,"href","/docs/transformers/main/en/model_doc/flava#transformers.models.flava.image_processing_flava.FlavaImageProcessor"),d($t,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),d(Tt,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaProcessor.decode"),d(wt,"href","/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode"),d(Je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(xt,"href","/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode"),d(Ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ye,"id","transformers.models.flava.image_processing_flava.FlavaImageProcessor"),d(Ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ye,"href","#transformers.models.flava.image_processing_flava.FlavaImageProcessor"),d(ve,"class","relative group"),d(Ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(eo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(oo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ao,"id","transformers.FlavaForPreTraining"),d(ao,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ao,"href","#transformers.FlavaForPreTraining"),d(ke,"class","relative group"),d(ka,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(ka,"rel","nofollow"),d(Mt,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaForPreTraining"),d(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(no,"id","transformers.FlavaModel"),d(no,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(no,"href","#transformers.FlavaModel"),d(ye,"class","relative group"),d(wa,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(wa,"rel","nofollow"),d(zt,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaModel"),d(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(It,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaModel"),d(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Pt,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaModel"),d(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(co,"id","transformers.FlavaImageCodebook"),d(co,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(co,"href","#transformers.FlavaImageCodebook"),d(xe,"class","relative group"),d(Aa,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(Aa,"rel","nofollow"),d(Ct,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Et,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(At,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(mo,"id","transformers.FlavaTextModel"),d(mo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(mo,"href","#transformers.FlavaTextModel"),d(Me,"class","relative group"),d(Wa,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(Wa,"rel","nofollow"),d(Nt,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"),d(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(go,"id","transformers.FlavaImageModel"),d(go,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(go,"href","#transformers.FlavaImageModel"),d(Ie,"class","relative group"),d(Ua,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(Ua,"rel","nofollow"),d(qt,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"),d(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(_o,"id","transformers.FlavaMultimodalModel"),d(_o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_o,"href","#transformers.FlavaMultimodalModel"),d(Ce,"class","relative group"),d(Ya,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(Ya,"rel","nofollow"),d(Lt,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalModel"),d(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,h){e(document.head,c),_(o,w,h),_(o,u,h),e(u,g),e(g,v),F(l,v,null),e(u,p),e(u,z),e(z,ws),_(o,sr,h),_(o,ie,h),e(ie,Ne),e(Ne,on),F(zo,on,null),e(ie,xs),e(ie,an),e(an,Ms),_(o,ir,h),_(o,qe,h),e(qe,zs),e(qe,Io),e(Io,Is),e(qe,Ps),_(o,lr,h),_(o,tt,h),e(tt,Cs),_(o,dr,h),_(o,nt,h),e(nt,Es),_(o,cr,h),_(o,rt,h),e(rt,tn),e(tn,As),_(o,mr,h),_(o,Z,h),e(Z,Ns),e(Z,Po),e(Po,qs),e(Z,Ls),e(Z,Co),e(Co,js),e(Z,Os),_(o,fr,h),_(o,le,h),e(le,Le),e(Le,nn),F(Eo,nn,null),e(le,Ds),e(le,rn),e(rn,Ws),_(o,pr,h),_(o,P,h),F(Ao,P,null),e(P,Ss),e(P,X),e(X,st),e(st,Vs),e(X,Bs),e(X,it),e(it,Rs),e(X,Us),e(X,No),e(No,Hs),e(X,Gs),e(P,Js),e(P,de),e(de,Ks),e(de,lt),e(lt,Ys),e(de,Zs),e(de,dt),e(dt,Xs),e(de,Qs),e(P,ei),F(je,P,null),e(P,oi),e(P,Oe),F(qo,Oe,null),e(Oe,ai),e(Oe,Lo),e(Lo,ti),e(Lo,ct),e(ct,ni),e(Lo,ri),e(P,si),e(P,De),F(jo,De,null),e(De,ii),e(De,Oo),e(Oo,li),e(Oo,mt),e(mt,di),e(Oo,ci),_(o,gr,h),_(o,ce,h),e(ce,We),e(We,sn),F(Do,sn,null),e(ce,mi),e(ce,ln),e(ln,fi),_(o,hr,h),_(o,E,h),F(Wo,E,null),e(E,pi),e(E,So),e(So,gi),e(So,ft),e(ft,hi),e(So,ui),e(E,_i),e(E,Vo),e(Vo,vi),e(Vo,Bo),e(Bo,bi),e(Vo,ki),e(E,Fi),e(E,me),e(me,yi),e(me,pt),e(pt,$i),e(me,Ti),e(me,gt),e(gt,wi),e(me,xi),e(E,Mi),F(Se,E,null),_(o,ur,h),_(o,fe,h),e(fe,Ve),e(Ve,dn),F(Ro,dn,null),e(fe,zi),e(fe,cn),e(cn,Ii),_(o,_r,h),_(o,A,h),F(Uo,A,null),e(A,Pi),e(A,Ho),e(Ho,Ci),e(Ho,ht),e(ht,Ei),e(Ho,Ai),e(A,Ni),e(A,Go),e(Go,qi),e(Go,Jo),e(Jo,Li),e(Go,ji),e(A,Oi),e(A,pe),e(pe,Di),e(pe,ut),e(ut,Wi),e(pe,Si),e(pe,_t),e(_t,Vi),e(pe,Bi),e(A,Ri),F(Be,A,null),_(o,vr,h),_(o,ge,h),e(ge,Re),e(Re,mn),F(Ko,mn,null),e(ge,Ui),e(ge,fn),e(fn,Hi),_(o,br,h),_(o,N,h),F(Yo,N,null),e(N,Gi),e(N,Zo),e(Zo,Ji),e(Zo,vt),e(vt,Ki),e(Zo,Yi),e(N,Zi),e(N,Xo),e(Xo,Xi),e(Xo,Qo),e(Qo,Qi),e(Xo,el),e(N,ol),e(N,he),e(he,al),e(he,bt),e(bt,tl),e(he,nl),e(he,kt),e(kt,rl),e(he,sl),e(N,il),F(Ue,N,null),_(o,kr,h),_(o,ue,h),e(ue,He),e(He,pn),F(ea,pn,null),e(ue,ll),e(ue,gn),e(gn,dl),_(o,Fr,h),_(o,oa,h),F(aa,oa,null),_(o,yr,h),_(o,_e,h),e(_e,Ge),e(Ge,hn),F(ta,hn,null),e(_e,cl),e(_e,un),e(un,ml),_(o,$r,h),_(o,q,h),F(na,q,null),e(q,fl),e(q,_n),e(_n,pl),e(q,gl),e(q,O),e(O,Ft),e(Ft,hl),e(O,ul),e(O,yt),e(yt,_l),e(O,vl),e(O,$t),e($t,bl),e(O,kl),e(O,vn),e(vn,Fl),e(O,yl),e(O,Tt),e(Tt,$l),e(O,Tl),e(q,wl),e(q,Je),F(ra,Je,null),e(Je,xl),e(Je,sa),e(sa,Ml),e(sa,wt),e(wt,zl),e(sa,Il),e(q,Pl),e(q,Ke),F(ia,Ke,null),e(Ke,Cl),e(Ke,la),e(la,El),e(la,xt),e(xt,Al),e(la,Nl),_(o,Tr,h),_(o,ve,h),e(ve,Ye),e(Ye,bn),F(da,bn,null),e(ve,ql),e(ve,kn),e(kn,Ll),_(o,wr,h),_(o,I,h),F(ca,I,null),e(I,jl),e(I,Fn),e(Fn,Ol),e(I,Dl),e(I,Ze),F(ma,Ze,null),e(Ze,Wl),e(Ze,be),e(be,Sl),e(be,yn),e(yn,Vl),e(be,Bl),e(be,$n),e($n,Rl),e(be,Ul),e(I,Hl),e(I,Xe),F(fa,Xe,null),e(Xe,Gl),e(Xe,Tn),e(Tn,Jl),e(I,Kl),e(I,Qe),F(pa,Qe,null),e(Qe,Yl),e(Qe,wn),e(wn,Zl),e(I,Xl),e(I,eo),F(ga,eo,null),e(eo,Ql),e(eo,xn),e(xn,ed),e(I,od),e(I,oo),F(ha,oo,null),e(oo,ad),e(oo,ua),e(ua,td),e(ua,Mn),e(Mn,nd),e(ua,rd),_(o,xr,h),_(o,ke,h),e(ke,ao),e(ao,zn),F(_a,zn,null),e(ke,sd),e(ke,In),e(In,id),_(o,Mr,h),_(o,W,h),F(va,W,null),e(W,ld),e(W,Pn),e(Pn,dd),e(W,cd),e(W,ba),e(ba,md),e(ba,ka),e(ka,fd),e(ba,pd),e(W,gd),e(W,Q),F(Fa,Q,null),e(Q,hd),e(Q,Fe),e(Fe,ud),e(Fe,Mt),e(Mt,_d),e(Fe,vd),e(Fe,Cn),e(Cn,bd),e(Fe,kd),e(Q,Fd),F(to,Q,null),_(o,zr,h),_(o,ye,h),e(ye,no),e(no,En),F(ya,En,null),e(ye,yd),e(ye,An),e(An,$d),_(o,Ir,h),_(o,L,h),F($a,L,null),e(L,Td),e(L,Ta),e(Ta,wd),e(Ta,wa),e(wa,xd),e(Ta,Md),e(L,zd),e(L,S),F(xa,S,null),e(S,Id),e(S,$e),e($e,Pd),e($e,zt),e(zt,Cd),e($e,Ed),e($e,Nn),e(Nn,Ad),e($e,Nd),e(S,qd),F(ro,S,null),e(S,Ld),F(so,S,null),e(L,jd),e(L,ee),F(Ma,ee,null),e(ee,Od),e(ee,Te),e(Te,Dd),e(Te,It),e(It,Wd),e(Te,Sd),e(Te,qn),e(qn,Vd),e(Te,Bd),e(ee,Rd),F(io,ee,null),e(L,Ud),e(L,oe),F(za,oe,null),e(oe,Hd),e(oe,we),e(we,Gd),e(we,Pt),e(Pt,Jd),e(we,Kd),e(we,Ln),e(Ln,Yd),e(we,Zd),e(oe,Xd),F(lo,oe,null),_(o,Pr,h),_(o,xe,h),e(xe,co),e(co,jn),F(Ia,jn,null),e(xe,Qd),e(xe,On),e(On,ec),_(o,Cr,h),_(o,C,h),F(Pa,C,null),e(C,oc),e(C,Ca),e(Ca,ac),e(Ca,Dn),e(Dn,tc),e(Ca,nc),e(C,rc),e(C,Ea),e(Ea,sc),e(Ea,Aa),e(Aa,ic),e(Ea,lc),e(C,dc),e(C,Ct),F(Na,Ct,null),e(C,cc),e(C,Et),F(qa,Et,null),e(C,mc),e(C,At),F(La,At,null),_(o,Er,h),_(o,Me,h),e(Me,mo),e(mo,Wn),F(ja,Wn,null),e(Me,fc),e(Me,Sn),e(Sn,pc),_(o,Ar,h),_(o,G,h),F(Oa,G,null),e(G,gc),e(G,Da),e(Da,hc),e(Da,Wa),e(Wa,uc),e(Da,_c),e(G,vc),e(G,V),F(Sa,V,null),e(V,bc),e(V,ze),e(ze,kc),e(ze,Nt),e(Nt,Fc),e(ze,yc),e(ze,Vn),e(Vn,$c),e(ze,Tc),e(V,wc),F(fo,V,null),e(V,xc),F(po,V,null),_(o,Nr,h),_(o,Ie,h),e(Ie,go),e(go,Bn),F(Va,Bn,null),e(Ie,Mc),e(Ie,Rn),e(Rn,zc),_(o,qr,h),_(o,J,h),F(Ba,J,null),e(J,Ic),e(J,Ra),e(Ra,Pc),e(Ra,Ua),e(Ua,Cc),e(Ra,Ec),e(J,Ac),e(J,B),F(Ha,B,null),e(B,Nc),e(B,Pe),e(Pe,qc),e(Pe,qt),e(qt,Lc),e(Pe,jc),e(Pe,Un),e(Un,Oc),e(Pe,Dc),e(B,Wc),F(ho,B,null),e(B,Sc),F(uo,B,null),_(o,Lr,h),_(o,Ce,h),e(Ce,_o),e(_o,Hn),F(Ga,Hn,null),e(Ce,Vc),e(Ce,Gn),e(Gn,Bc),_(o,jr,h),_(o,K,h),F(Ja,K,null),e(K,Rc),e(K,Ka),e(Ka,Uc),e(Ka,Ya),e(Ya,Hc),e(Ka,Gc),e(K,Jc),e(K,R),F(Za,R,null),e(R,Kc),e(R,Ee),e(Ee,Yc),e(Ee,Lt),e(Lt,Zc),e(Ee,Xc),e(Ee,Jn),e(Jn,Qc),e(Ee,em),e(R,om),F(vo,R,null),e(R,am),F(bo,R,null),Or=!0},p(o,[h]){const Xa={};h&2&&(Xa.$$scope={dirty:h,ctx:o}),je.$set(Xa);const Kn={};h&2&&(Kn.$$scope={dirty:h,ctx:o}),Se.$set(Kn);const Yn={};h&2&&(Yn.$$scope={dirty:h,ctx:o}),Be.$set(Yn);const Zn={};h&2&&(Zn.$$scope={dirty:h,ctx:o}),Ue.$set(Zn);const Qa={};h&2&&(Qa.$$scope={dirty:h,ctx:o}),to.$set(Qa);const Xn={};h&2&&(Xn.$$scope={dirty:h,ctx:o}),ro.$set(Xn);const Qn={};h&2&&(Qn.$$scope={dirty:h,ctx:o}),so.$set(Qn);const er={};h&2&&(er.$$scope={dirty:h,ctx:o}),io.$set(er);const et={};h&2&&(et.$$scope={dirty:h,ctx:o}),lo.$set(et);const or={};h&2&&(or.$$scope={dirty:h,ctx:o}),fo.$set(or);const ar={};h&2&&(ar.$$scope={dirty:h,ctx:o}),po.$set(ar);const tr={};h&2&&(tr.$$scope={dirty:h,ctx:o}),ho.$set(tr);const nr={};h&2&&(nr.$$scope={dirty:h,ctx:o}),uo.$set(nr);const rr={};h&2&&(rr.$$scope={dirty:h,ctx:o}),vo.$set(rr);const Ae={};h&2&&(Ae.$$scope={dirty:h,ctx:o}),bo.$set(Ae)},i(o){Or||(y(l.$$.fragment,o),y(zo.$$.fragment,o),y(Eo.$$.fragment,o),y(Ao.$$.fragment,o),y(je.$$.fragment,o),y(qo.$$.fragment,o),y(jo.$$.fragment,o),y(Do.$$.fragment,o),y(Wo.$$.fragment,o),y(Se.$$.fragment,o),y(Ro.$$.fragment,o),y(Uo.$$.fragment,o),y(Be.$$.fragment,o),y(Ko.$$.fragment,o),y(Yo.$$.fragment,o),y(Ue.$$.fragment,o),y(ea.$$.fragment,o),y(aa.$$.fragment,o),y(ta.$$.fragment,o),y(na.$$.fragment,o),y(ra.$$.fragment,o),y(ia.$$.fragment,o),y(da.$$.fragment,o),y(ca.$$.fragment,o),y(ma.$$.fragment,o),y(fa.$$.fragment,o),y(pa.$$.fragment,o),y(ga.$$.fragment,o),y(ha.$$.fragment,o),y(_a.$$.fragment,o),y(va.$$.fragment,o),y(Fa.$$.fragment,o),y(to.$$.fragment,o),y(ya.$$.fragment,o),y($a.$$.fragment,o),y(xa.$$.fragment,o),y(ro.$$.fragment,o),y(so.$$.fragment,o),y(Ma.$$.fragment,o),y(io.$$.fragment,o),y(za.$$.fragment,o),y(lo.$$.fragment,o),y(Ia.$$.fragment,o),y(Pa.$$.fragment,o),y(Na.$$.fragment,o),y(qa.$$.fragment,o),y(La.$$.fragment,o),y(ja.$$.fragment,o),y(Oa.$$.fragment,o),y(Sa.$$.fragment,o),y(fo.$$.fragment,o),y(po.$$.fragment,o),y(Va.$$.fragment,o),y(Ba.$$.fragment,o),y(Ha.$$.fragment,o),y(ho.$$.fragment,o),y(uo.$$.fragment,o),y(Ga.$$.fragment,o),y(Ja.$$.fragment,o),y(Za.$$.fragment,o),y(vo.$$.fragment,o),y(bo.$$.fragment,o),Or=!0)},o(o){$(l.$$.fragment,o),$(zo.$$.fragment,o),$(Eo.$$.fragment,o),$(Ao.$$.fragment,o),$(je.$$.fragment,o),$(qo.$$.fragment,o),$(jo.$$.fragment,o),$(Do.$$.fragment,o),$(Wo.$$.fragment,o),$(Se.$$.fragment,o),$(Ro.$$.fragment,o),$(Uo.$$.fragment,o),$(Be.$$.fragment,o),$(Ko.$$.fragment,o),$(Yo.$$.fragment,o),$(Ue.$$.fragment,o),$(ea.$$.fragment,o),$(aa.$$.fragment,o),$(ta.$$.fragment,o),$(na.$$.fragment,o),$(ra.$$.fragment,o),$(ia.$$.fragment,o),$(da.$$.fragment,o),$(ca.$$.fragment,o),$(ma.$$.fragment,o),$(fa.$$.fragment,o),$(pa.$$.fragment,o),$(ga.$$.fragment,o),$(ha.$$.fragment,o),$(_a.$$.fragment,o),$(va.$$.fragment,o),$(Fa.$$.fragment,o),$(to.$$.fragment,o),$(ya.$$.fragment,o),$($a.$$.fragment,o),$(xa.$$.fragment,o),$(ro.$$.fragment,o),$(so.$$.fragment,o),$(Ma.$$.fragment,o),$(io.$$.fragment,o),$(za.$$.fragment,o),$(lo.$$.fragment,o),$(Ia.$$.fragment,o),$(Pa.$$.fragment,o),$(Na.$$.fragment,o),$(qa.$$.fragment,o),$(La.$$.fragment,o),$(ja.$$.fragment,o),$(Oa.$$.fragment,o),$(Sa.$$.fragment,o),$(fo.$$.fragment,o),$(po.$$.fragment,o),$(Va.$$.fragment,o),$(Ba.$$.fragment,o),$(Ha.$$.fragment,o),$(ho.$$.fragment,o),$(uo.$$.fragment,o),$(Ga.$$.fragment,o),$(Ja.$$.fragment,o),$(Za.$$.fragment,o),$(vo.$$.fragment,o),$(bo.$$.fragment,o),Or=!1},d(o){a(c),o&&a(w),o&&a(u),T(l),o&&a(sr),o&&a(ie),T(zo),o&&a(ir),o&&a(qe),o&&a(lr),o&&a(tt),o&&a(dr),o&&a(nt),o&&a(cr),o&&a(rt),o&&a(mr),o&&a(Z),o&&a(fr),o&&a(le),T(Eo),o&&a(pr),o&&a(P),T(Ao),T(je),T(qo),T(jo),o&&a(gr),o&&a(ce),T(Do),o&&a(hr),o&&a(E),T(Wo),T(Se),o&&a(ur),o&&a(fe),T(Ro),o&&a(_r),o&&a(A),T(Uo),T(Be),o&&a(vr),o&&a(ge),T(Ko),o&&a(br),o&&a(N),T(Yo),T(Ue),o&&a(kr),o&&a(ue),T(ea),o&&a(Fr),o&&a(oa),T(aa),o&&a(yr),o&&a(_e),T(ta),o&&a($r),o&&a(q),T(na),T(ra),T(ia),o&&a(Tr),o&&a(ve),T(da),o&&a(wr),o&&a(I),T(ca),T(ma),T(fa),T(pa),T(ga),T(ha),o&&a(xr),o&&a(ke),T(_a),o&&a(Mr),o&&a(W),T(va),T(Fa),T(to),o&&a(zr),o&&a(ye),T(ya),o&&a(Ir),o&&a(L),T($a),T(xa),T(ro),T(so),T(Ma),T(io),T(za),T(lo),o&&a(Pr),o&&a(xe),T(Ia),o&&a(Cr),o&&a(C),T(Pa),T(Na),T(qa),T(La),o&&a(Er),o&&a(Me),T(ja),o&&a(Ar),o&&a(G),T(Oa),T(Sa),T(fo),T(po),o&&a(Nr),o&&a(Ie),T(Va),o&&a(qr),o&&a(J),T(Ba),T(Ha),T(ho),T(uo),o&&a(Lr),o&&a(Ce),T(Ga),o&&a(jr),o&&a(K),T(Ja),T(Za),T(vo),T(bo)}}}const vp={local:"flava",sections:[{local:"overview",title:"Overview"},{local:"transformers.FlavaConfig",title:"FlavaConfig"},{local:"transformers.FlavaTextConfig",title:"FlavaTextConfig"},{local:"transformers.FlavaImageConfig",title:"FlavaImageConfig"},{local:"transformers.FlavaMultimodalConfig",title:"FlavaMultimodalConfig"},{local:"transformers.FlavaImageCodebookConfig",title:"FlavaImageCodebookConfig"},{local:"transformers.FlavaProcessor",title:"FlavaProcessor"},{local:"transformers.models.flava.image_processing_flava.FlavaImageProcessor",title:"FlavaFeatureExtractor"},{local:"transformers.FlavaForPreTraining",title:"FlavaForPreTraining"},{local:"transformers.FlavaModel",title:"FlavaModel"},{local:"transformers.FlavaImageCodebook",title:"FlavaImageCodebook"},{local:"transformers.FlavaTextModel",title:"FlavaTextModel"},{local:"transformers.FlavaImageModel",title:"FlavaImageModel"},{local:"transformers.FlavaMultimodalModel",title:"FlavaMultimodalModel"}],title:"FLAVA"};function bp(x){return op(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class xp extends Zf{constructor(c){super();Xf(this,c,bp,_p,Qf,{})}}export{xp as default,vp as metadata};
