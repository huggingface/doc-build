import{S as $f,i as wf,s as zf,e as t,k as c,w as b,t as s,M as If,c as n,d as a,m,a as r,x as k,h as i,b as d,G as e,g as u,y as F,q as y,o as x,B as T,v as Mf,L as Po}from"../../chunks/vendor-hf-doc-builder.js";import{T as dt}from"../../chunks/Tip-hf-doc-builder.js";import{D as w}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Co}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as E}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Mo}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Pf(z){let p,$,_,h,v;return h=new Co({props:{code:`from transformers import FlavaConfig, FlavaModel, FlavaForPreTraining

# Initializing a FlavaConfig with style configuration
configuration = FlavaConfig()

# Initializing a FlavaModel and FlavaForPreTraining model (with random weights) from the style configuration
model = FlavaModel(configuration)
model_pre = FlavaForPreTraining(configuration)

# Accessing the model configuration
configuration = model.config
configuration_pre = model_pre.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaConfig, FlavaModel, FlavaForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaConfig with style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FlavaConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaModel and FlavaForPreTraining model (with random weights) from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaModel(configuration)
<span class="hljs-meta">&gt;&gt;&gt; </span>model_pre = FlavaForPreTraining(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration_pre = model_pre.config`}}),{c(){p=t("p"),$=s("Example:"),_=c(),b(h.$$.fragment)},l(l){p=n(l,"P",{});var f=r(p);$=i(f,"Example:"),f.forEach(a),_=m(l),k(h.$$.fragment,l)},m(l,f){u(l,p,f),e(p,$),u(l,_,f),F(h,l,f),v=!0},p:Po,i(l){v||(y(h.$$.fragment,l),v=!0)},o(l){x(h.$$.fragment,l),v=!1},d(l){l&&a(p),l&&a(_),T(h,l)}}}function Cf(z){let p,$,_,h,v;return h=new Co({props:{code:`from transformers import FlavaTextConfig, FlavaTextModel

# Initializing a FlavaTextModel with  style configuration
configuration = FlavaTextConfig()

# Initializing a FlavaTextModel model (with random weights) from the style configuration
model = FlavaTextModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaTextConfig, FlavaTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaTextModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FlavaTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaTextModel model (with random weights) from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){p=t("p"),$=s("Example:"),_=c(),b(h.$$.fragment)},l(l){p=n(l,"P",{});var f=r(p);$=i(f,"Example:"),f.forEach(a),_=m(l),k(h.$$.fragment,l)},m(l,f){u(l,p,f),e(p,$),u(l,_,f),F(h,l,f),v=!0},p:Po,i(l){v||(y(h.$$.fragment,l),v=!0)},o(l){x(h.$$.fragment,l),v=!1},d(l){l&&a(p),l&&a(_),T(h,l)}}}function Ef(z){let p,$,_,h,v;return h=new Co({props:{code:`from transformers import FlavaImageConfig, FlavaImageModel

# Initializing a FlavaImageModel with  style configuration
configuration = FlavaImageConfig()

# Initializing a FlavaImageModel model (with random weights) from the style configuration
model = FlavaImageModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaImageConfig, FlavaImageModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaImageModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FlavaImageConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaImageModel model (with random weights) from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaImageModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){p=t("p"),$=s("Example:"),_=c(),b(h.$$.fragment)},l(l){p=n(l,"P",{});var f=r(p);$=i(f,"Example:"),f.forEach(a),_=m(l),k(h.$$.fragment,l)},m(l,f){u(l,p,f),e(p,$),u(l,_,f),F(h,l,f),v=!0},p:Po,i(l){v||(y(h.$$.fragment,l),v=!0)},o(l){x(h.$$.fragment,l),v=!1},d(l){l&&a(p),l&&a(_),T(h,l)}}}function Af(z){let p,$,_,h,v;return h=new Co({props:{code:`from transformers import FlavaMultimodalConfig, FlavaMultimodalModel

# Initializing a FlavaMultimodalModel with  style configuration
configuration = FlavaMultimodalConfig()

# Initializing a FlavaMultimodalModel model (with random weights) from the style configuration
model = FlavaMultimodalModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaMultimodalConfig, FlavaMultimodalModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaMultimodalModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FlavaMultimodalConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaMultimodalModel model (with random weights) from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaMultimodalModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){p=t("p"),$=s("Example:"),_=c(),b(h.$$.fragment)},l(l){p=n(l,"P",{});var f=r(p);$=i(f,"Example:"),f.forEach(a),_=m(l),k(h.$$.fragment,l)},m(l,f){u(l,p,f),e(p,$),u(l,_,f),F(h,l,f),v=!0},p:Po,i(l){v||(y(h.$$.fragment,l),v=!0)},o(l){x(h.$$.fragment,l),v=!1},d(l){l&&a(p),l&&a(_),T(h,l)}}}function Nf(z){let p,$,_,h,v;return{c(){p=t("p"),$=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=t("code"),h=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){p=n(l,"P",{});var f=r(p);$=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=n(f,"CODE",{});var I=r(_);h=i(I,"Module"),I.forEach(a),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(a)},m(l,f){u(l,p,f),e(p,$),e(p,_),e(_,h),e(p,v)},d(l){l&&a(p)}}}function Of(z){let p,$,_,h,v;return{c(){p=t("p"),$=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=t("code"),h=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){p=n(l,"P",{});var f=r(p);$=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=n(f,"CODE",{});var I=r(_);h=i(I,"Module"),I.forEach(a),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(a)},m(l,f){u(l,p,f),e(p,$),e(p,_),e(_,h),e(p,v)},d(l){l&&a(p)}}}function qf(z){let p,$,_,h,v;return h=new Co({props:{code:`from PIL import Image
import requests
from transformers import FlavaProcessor, FlavaModel

model = FlavaModel.from_pretrained("facebook/flava-full")
processor = FlavaProcessor.from_pretrained("facebook/flava-full")

url = "http://images.cocodataset.org/val2017/000000039769.jpg"
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=["a photo of a cat"], images=image, return_tensors="pt", padding=True)

outputs = model(**inputs)
logits_per_image = outputs.contrastive_logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaProcessor, FlavaModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaModel.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = FlavaProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.contrastive_logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`}}),{c(){p=t("p"),$=s("Examples:"),_=c(),b(h.$$.fragment)},l(l){p=n(l,"P",{});var f=r(p);$=i(f,"Examples:"),f.forEach(a),_=m(l),k(h.$$.fragment,l)},m(l,f){u(l,p,f),e(p,$),u(l,_,f),F(h,l,f),v=!0},p:Po,i(l){v||(y(h.$$.fragment,l),v=!0)},o(l){x(h.$$.fragment,l),v=!1},d(l){l&&a(p),l&&a(_),T(h,l)}}}function Lf(z){let p,$,_,h,v;return{c(){p=t("p"),$=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=t("code"),h=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){p=n(l,"P",{});var f=r(p);$=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=n(f,"CODE",{});var I=r(_);h=i(I,"Module"),I.forEach(a),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(a)},m(l,f){u(l,p,f),e(p,$),e(p,_),e(_,h),e(p,v)},d(l){l&&a(p)}}}function Df(z){let p,$,_,h,v;return{c(){p=t("p"),$=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=t("code"),h=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){p=n(l,"P",{});var f=r(p);$=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=n(f,"CODE",{});var I=r(_);h=i(I,"Module"),I.forEach(a),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(a)},m(l,f){u(l,p,f),e(p,$),e(p,_),e(_,h),e(p,v)},d(l){l&&a(p)}}}function jf(z){let p,$,_,h,v;return{c(){p=t("p"),$=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=t("code"),h=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){p=n(l,"P",{});var f=r(p);$=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=n(f,"CODE",{});var I=r(_);h=i(I,"Module"),I.forEach(a),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(a)},m(l,f){u(l,p,f),e(p,$),e(p,_),e(_,h),e(p,v)},d(l){l&&a(p)}}}function Wf(z){let p,$,_,h,v;return h=new Co({props:{code:`from transformers import BertTokenizer, FlavaTextModel
import torch

tokenizer = BertTokenizer.from_pretrained("facebook/flava-full")
model = FlavaTextModel.from_pretrained("facebook/flava-full")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, FlavaTextModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaTextModel.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){p=t("p"),$=s("Example:"),_=c(),b(h.$$.fragment)},l(l){p=n(l,"P",{});var f=r(p);$=i(f,"Example:"),f.forEach(a),_=m(l),k(h.$$.fragment,l)},m(l,f){u(l,p,f),e(p,$),u(l,_,f),F(h,l,f),v=!0},p:Po,i(l){v||(y(h.$$.fragment,l),v=!0)},o(l){x(h.$$.fragment,l),v=!1},d(l){l&&a(p),l&&a(_),T(h,l)}}}function Sf(z){let p,$,_,h,v;return{c(){p=t("p"),$=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=t("code"),h=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){p=n(l,"P",{});var f=r(p);$=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=n(f,"CODE",{});var I=r(_);h=i(I,"Module"),I.forEach(a),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(a)},m(l,f){u(l,p,f),e(p,$),e(p,_),e(_,h),e(p,v)},d(l){l&&a(p)}}}function Rf(z){let p,$,_,h,v;return h=new Co({props:{code:`from transformers import FlavaFeatureExtractor, FlavaImageModel
import torch
from datasets import load_dataset

dataset = load_dataset("huggingface/cats-image")
image = dataset["test"]["image"][0]

feature_extractor = FlavaFeatureExtractor.from_pretrained("facebook/flava-full")
model = FlavaImageModel.from_pretrained("facebook/flava-full")

inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
list(last_hidden_states.shape)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaFeatureExtractor, FlavaImageModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = FlavaFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaImageModel.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">197</span>, <span class="hljs-number">768</span>]`}}),{c(){p=t("p"),$=s("Example:"),_=c(),b(h.$$.fragment)},l(l){p=n(l,"P",{});var f=r(p);$=i(f,"Example:"),f.forEach(a),_=m(l),k(h.$$.fragment,l)},m(l,f){u(l,p,f),e(p,$),u(l,_,f),F(h,l,f),v=!0},p:Po,i(l){v||(y(h.$$.fragment,l),v=!0)},o(l){x(h.$$.fragment,l),v=!1},d(l){l&&a(p),l&&a(_),T(h,l)}}}function Bf(z){let p,$,_,h,v;return{c(){p=t("p"),$=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=t("code"),h=s("Module"),v=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(l){p=n(l,"P",{});var f=r(p);$=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=n(f,"CODE",{});var I=r(_);h=i(I,"Module"),I.forEach(a),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(a)},m(l,f){u(l,p,f),e(p,$),e(p,_),e(_,h),e(p,v)},d(l){l&&a(p)}}}function Vf(z){let p,$,_,h,v;return h=new Co({props:{code:`from transformers import BertTokenizer, FlavaMultimodalModel
import torch

tokenizer = BertTokenizer.from_pretrained("facebook/flava-full")
model = FlavaMultimodalModel.from_pretrained("facebook/flava-full")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, FlavaMultimodalModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaMultimodalModel.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){p=t("p"),$=s("Example:"),_=c(),b(h.$$.fragment)},l(l){p=n(l,"P",{});var f=r(p);$=i(f,"Example:"),f.forEach(a),_=m(l),k(h.$$.fragment,l)},m(l,f){u(l,p,f),e(p,$),u(l,_,f),F(h,l,f),v=!0},p:Po,i(l){v||(y(h.$$.fragment,l),v=!0)},o(l){x(h.$$.fragment,l),v=!1},d(l){l&&a(p),l&&a(_),T(h,l)}}}function Uf(z){let p,$,_,h,v,l,f,I,Ss,vr,le,qe,cn,Eo,Rs,mn,Bs,br,Le,Vs,Ao,Us,Hs,kr,ct,Gs,Fr,mt,Ys,yr,pt,pn,Zs,xr,X,Js,No,Ks,Xs,Oo,Qs,ei,Tr,de,De,fn,qo,oi,gn,ai,$r,P,Lo,ti,Q,ft,ni,ri,gt,si,ii,Do,li,di,ci,ce,mi,ht,pi,fi,ut,gi,hi,ui,je,_i,We,jo,vi,Wo,bi,_t,ki,Fi,yi,Se,So,xi,Ro,Ti,vt,$i,wi,wr,me,Re,hn,Bo,zi,un,Ii,zr,A,Vo,Mi,Uo,Pi,bt,Ci,Ei,Ai,Ho,Ni,Go,Oi,qi,Li,pe,Di,kt,ji,Wi,Ft,Si,Ri,Bi,Be,Ir,fe,Ve,_n,Yo,Vi,vn,Ui,Mr,N,Zo,Hi,Jo,Gi,yt,Yi,Zi,Ji,Ko,Ki,Xo,Xi,Qi,el,ge,ol,xt,al,tl,Tt,nl,rl,sl,Ue,Pr,he,He,bn,Qo,il,kn,ll,Cr,O,ea,dl,oa,cl,$t,ml,pl,fl,aa,gl,ta,hl,ul,_l,ue,vl,wt,bl,kl,zt,Fl,yl,xl,Ge,Er,_e,Ye,Fn,na,Tl,yn,$l,Ar,ra,sa,Nr,ve,Ze,xn,ia,wl,Tn,zl,Or,q,la,Il,$n,Ml,Pl,D,It,Cl,El,Mt,Al,Nl,Pt,Ol,ql,wn,Ll,Dl,Ct,jl,Wl,Sl,Je,da,Rl,ca,Bl,Et,Vl,Ul,Hl,Ke,ma,Gl,pa,Yl,At,Zl,Jl,qr,be,Xe,zn,fa,Kl,In,Xl,Lr,M,ga,Ql,Mn,ed,od,Qe,ha,ad,ke,td,Pn,nd,rd,Cn,sd,id,ld,eo,ua,dd,En,cd,md,oo,_a,pd,An,fd,gd,ao,va,hd,Nn,ud,_d,to,ba,vd,ka,bd,On,kd,Fd,Dr,Fe,no,qn,Fa,yd,Ln,xd,jr,G,ya,Td,Dn,$d,wd,ro,xa,zd,jn,Id,Wr,ye,so,Wn,Ta,Md,Sn,Pd,Sr,W,$a,Cd,Rn,Ed,Ad,wa,Nd,za,Od,qd,Ld,ee,Ia,Dd,xe,jd,Nt,Wd,Sd,Bn,Rd,Bd,Vd,io,Rr,Te,lo,Vn,Ma,Ud,Un,Hd,Br,L,Pa,Gd,Ca,Yd,Ea,Zd,Jd,Kd,S,Aa,Xd,$e,Qd,Ot,ec,oc,Hn,ac,tc,nc,co,rc,mo,sc,oe,Na,ic,we,lc,qt,dc,cc,Gn,mc,pc,fc,po,gc,ae,Oa,hc,ze,uc,Lt,_c,vc,Yn,bc,kc,Fc,fo,Vr,Ie,go,Zn,qa,yc,Jn,xc,Ur,C,La,Tc,Da,$c,Kn,wc,zc,Ic,ja,Mc,Wa,Pc,Cc,Ec,Dt,Sa,Ac,jt,Ra,Nc,Wt,Ba,Hr,Me,ho,Xn,Va,Oc,Qn,qc,Gr,Y,Ua,Lc,Ha,Dc,Ga,jc,Wc,Sc,R,Ya,Rc,Pe,Bc,St,Vc,Uc,er,Hc,Gc,Yc,uo,Zc,_o,Yr,Ce,vo,or,Za,Jc,ar,Kc,Zr,Z,Ja,Xc,Ka,Qc,Xa,em,om,am,B,Qa,tm,Ee,nm,Rt,rm,sm,tr,im,lm,dm,bo,cm,ko,Jr,Ae,Fo,nr,et,mm,rr,pm,Kr,J,ot,fm,at,gm,tt,hm,um,_m,V,nt,vm,Ne,bm,Bt,km,Fm,sr,ym,xm,Tm,yo,$m,xo,Xr;return l=new E({}),Eo=new E({}),qo=new E({}),Lo=new w({props:{name:"class transformers.FlavaConfig",anchor:"transformers.FlavaConfig",parameters:[{name:"image_config",val:": typing.Dict[str, typing.Any] = None"},{name:"text_config",val:": typing.Dict[str, typing.Any] = None"},{name:"multimodal_config",val:": typing.Dict[str, typing.Any] = None"},{name:"image_codebook_config",val:": typing.Dict[str, typing.Any] = None"},{name:"hidden_size",val:": int = 768"},{name:"layer_norm_eps",val:": float = 1e-12"},{name:"projection_dim",val:": int = 768"},{name:"init_codebook",val:": bool = True"},{name:"logit_scale_init_value",val:": float = 2.6592"},{name:"initializer_range",val:": float = 0.02"},{name:"ce_ignore_index",val:": int = -100"},{name:"mim_weight",val:": float = 1.0"},{name:"mlm_weight",val:": float = 1.0"},{name:"global_contrastive_weight",val:": float = 1.0"},{name:"itm_weight",val:": float = 1.0"},{name:"mmm_image_weight",val:": float = 1.0"},{name:"mmm_text_weight",val:": float = 1.0"},{name:"global_backprop_contrastive",val:": bool = True"},{name:"skip_unmasked_multimodal_encoder",val:": bool = True"},{name:"return_loss",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaConfig.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextConfig">FlavaTextConfig</a>.`,name:"text_config"},{anchor:"transformers.FlavaConfig.image_config",description:`<strong>image_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageConfig">FlavaImageConfig</a>.`,name:"image_config"},{anchor:"transformers.FlavaConfig.multimodal_config",description:`<strong>multimodal_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalConfig">FlavaMultimodalConfig</a>.`,name:"multimodal_config"},{anchor:"transformers.FlavaConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FlavaConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FlavaConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimentionality of text and image projection layers.`,name:"projection_dim"},{anchor:"transformers.FlavaConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> paramter. Default is used as per the original FLAVA/CLIP
implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.FlavaConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FlavaConfig.ce_ignore_index",description:`<strong>ce_ignore_index</strong> (<code>int</code>, <em>optional</em>, defaults to -100) &#x2014;
Cross entropy index to ignore.`,name:"ce_ignore_index"},{anchor:"transformers.FlavaConfig.mim_weight",description:`<strong>mim_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MIM (Masked Image Modeling) unimodal loss`,name:"mim_weight"},{anchor:"transformers.FlavaConfig.mlm_weight",description:`<strong>mlm_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MLM (Masked Language Modeling) unimodal loss`,name:"mlm_weight"},{anchor:"transformers.FlavaConfig.global_contrastive_weight",description:`<strong>global_contrastive_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to global contrastive cross-alignment loss.`,name:"global_contrastive_weight"},{anchor:"transformers.FlavaConfig.itm_weight",description:`<strong>itm_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to image-text matching multimodal loss.`,name:"itm_weight"},{anchor:"transformers.FlavaConfig.mmm_image_weight",description:`<strong>mmm_image_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MMM loss&#x2019;s image part.`,name:"mmm_image_weight"},{anchor:"transformers.FlavaConfig.mmm_text_weight",description:`<strong>mmm_text_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MMM loss&#x2019;s text part.`,name:"mmm_text_weight"},{anchor:"transformers.FlavaConfig.global_backprop_contrastive",description:`<strong>global_backprop_contrastive</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use global backpropgation through all workers in contrastive loss.`,name:"global_backprop_contrastive"},{anchor:"transformers.FlavaConfig.skip_unmasked_multimodal_encoder",description:`<strong>skip_unmasked_multimodal_encoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to skip running unmasked multimodal encoder whose outputs are not used by FLAVA losses.`,name:"skip_unmasked_multimodal_encoder"},{anchor:"transformers.FlavaConfig.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to return loss or not`,name:"return_loss"},{anchor:"transformers.FlavaConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/configuration_flava.py#L463"}}),je=new Mo({props:{anchor:"transformers.FlavaConfig.example",$$slots:{default:[Pf]},$$scope:{ctx:z}}}),jo=new w({props:{name:"from_configs",anchor:"transformers.FlavaConfig.from_configs",parameters:[{name:"image_config",val:": FlavaImageConfig"},{name:"text_config",val:": FlavaTextConfig"},{name:"multimodal_config",val:": FlavaMultimodalConfig"},{name:"image_codebook_config",val:": FlavaImageCodebookConfig"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/configuration_flava.py#L617",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig"
>FlavaConfig</a></p>
`}}),So=new w({props:{name:"to_dict",anchor:"transformers.FlavaConfig.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/configuration_flava.py#L642",returnDescription:`
<p>Dictionary of all the attributes that make up this configuration instance,</p>
`,returnType:`
<p><code>Dict[str, any]</code></p>
`}}),Bo=new E({}),Vo=new w({props:{name:"class transformers.FlavaTextConfig",anchor:"transformers.FlavaTextConfig",parameters:[{name:"vocab_size",val:": int = 30522"},{name:"type_vocab_size",val:": int = 2"},{name:"max_position_embeddings",val:": int = 512"},{name:"position_embedding_type",val:": str = 'absolute'"},{name:"hidden_size",val:": int = 768"},{name:"num_hidden_layers",val:": int = 12"},{name:"num_attention_heads",val:": int = 12"},{name:"intermediate_size",val:": int = 3072"},{name:"hidden_act",val:": str = 'gelu'"},{name:"hidden_dropout_prob",val:": float = 0.0"},{name:"attention_probs_dropout_prob",val:": float = 0.0"},{name:"initializer_range",val:": float = 0.02"},{name:"layer_norm_eps",val:": float = 1e-12"},{name:"pad_token_id",val:": int = 0"},{name:"qkv_bias",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel">FlavaTextModel</a>.`,name:"vocab_size"},{anchor:"transformers.FlavaTextConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel">FlavaTextModel</a>. Note that even though
text encoder allows <code>token_type_ids</code>&#x2019;s value as 2, for text-only pretraining and fine-tuning, only 1 is
used similar to RoBERTa.`,name:"type_vocab_size"},{anchor:"transformers.FlavaTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048). For VL, max_length passed to model is 77.`,name:"max_position_embeddings"},{anchor:"transformers.FlavaTextConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.FlavaTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FlavaTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FlavaTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FlavaTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FlavaTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FlavaTextConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FlavaTextConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FlavaTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FlavaTextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FlavaTextConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.FlavaTextConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.FlavaTextConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.FlavaTextConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/configuration_flava.py#L150"}}),Be=new Mo({props:{anchor:"transformers.FlavaTextConfig.example",$$slots:{default:[Cf]},$$scope:{ctx:z}}}),Yo=new E({}),Zo=new w({props:{name:"class transformers.FlavaImageConfig",anchor:"transformers.FlavaImageConfig",parameters:[{name:"hidden_size",val:": int = 768"},{name:"num_hidden_layers",val:": int = 12"},{name:"num_attention_heads",val:": int = 12"},{name:"intermediate_size",val:": int = 3072"},{name:"hidden_act",val:": int = 'gelu'"},{name:"hidden_dropout_prob",val:": float = 0.0"},{name:"attention_probs_dropout_prob",val:": float = 0.0"},{name:"initializer_range",val:": float = 0.02"},{name:"layer_norm_eps",val:": float = 1e-12"},{name:"image_size",val:": int = 224"},{name:"patch_size",val:": int = 16"},{name:"num_channels",val:": int = 3"},{name:"qkv_bias",val:": bool = True"},{name:"mask_token",val:": bool = True"},{name:"vocab_size",val:": int = 8192"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaImageConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FlavaImageConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FlavaImageConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FlavaImageConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FlavaImageConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FlavaImageConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FlavaImageConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FlavaImageConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FlavaImageConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FlavaImageConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.FlavaImageConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.FlavaImageConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.FlavaImageConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.FlavaImageConfig.mask_token",description:`<strong>mask_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use a mask token or not. Used in MIM (Masked Image Modeling) loss for FLAVA.`,name:"mask_token"},{anchor:"transformers.FlavaImageConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8192) &#x2014;
Vocabulary size of the <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageCodebook">FlavaImageCodebook</a> used in conjunction with <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel">FlavaImageModel</a> for MIM (Masked
Image Modeling) loss for FLAVA.`,name:"vocab_size"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/configuration_flava.py#L32"}}),Ue=new Mo({props:{anchor:"transformers.FlavaImageConfig.example",$$slots:{default:[Ef]},$$scope:{ctx:z}}}),Qo=new E({}),ea=new w({props:{name:"class transformers.FlavaMultimodalConfig",anchor:"transformers.FlavaMultimodalConfig",parameters:[{name:"hidden_size",val:": int = 768"},{name:"num_hidden_layers",val:": int = 6"},{name:"num_attention_heads",val:": int = 12"},{name:"intermediate_size",val:": int = 3072"},{name:"hidden_act",val:": int = 'gelu'"},{name:"hidden_dropout_prob",val:": int = 0.0"},{name:"attention_probs_dropout_prob",val:": int = 0.0"},{name:"initializer_range",val:": float = 0.02"},{name:"layer_norm_eps",val:": float = 1e-12"},{name:"qkv_bias",val:": bool = True"},{name:"use_cls_token",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaMultimodalConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FlavaMultimodalConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FlavaMultimodalConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FlavaMultimodalConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FlavaMultimodalConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FlavaMultimodalConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FlavaMultimodalConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FlavaMultimodalConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FlavaMultimodalConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FlavaMultimodalConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.FlavaMultimodalConfig.use_cls_token",description:`<strong>use_cls_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use an extra CLS token for multimodal settings. Usually needed by the FLAVA model.`,name:"use_cls_token"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/configuration_flava.py#L278"}}),Ge=new Mo({props:{anchor:"transformers.FlavaMultimodalConfig.example",$$slots:{default:[Af]},$$scope:{ctx:z}}}),na=new E({}),sa=new w({props:{name:"class transformers.FlavaImageCodebookConfig",anchor:"transformers.FlavaImageCodebookConfig",parameters:[{name:"num_groups",val:": int = 4"},{name:"input_channels",val:": int = 3"},{name:"num_blocks_per_group",val:": int = 2"},{name:"hidden_size",val:": int = 256"},{name:"vocab_size",val:": int = 8192"},{name:"freeze",val:": int = True"},{name:"initializer_range",val:": float = 0.02"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/configuration_flava.py#L379"}}),ia=new E({}),la=new w({props:{name:"class transformers.FlavaProcessor",anchor:"transformers.FlavaProcessor",parameters:[{name:"feature_extractor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.FlavaProcessor.feature_extractor",description:'<strong>feature_extractor</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageProcessor">FlavaFeatureExtractor</a>) &#x2014; The feature extractor is a required input.',name:"feature_extractor"},{anchor:"transformers.FlavaProcessor.tokenizer",description:'<strong>tokenizer</strong> (<a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a>) &#x2014; The tokenizer is a required input.',name:"tokenizer"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/processing_flava.py#L26"}}),da=new w({props:{name:"batch_decode",anchor:"transformers.FlavaProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/processing_flava.py#L112"}}),ma=new w({props:{name:"decode",anchor:"transformers.FlavaProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/processing_flava.py#L119"}}),fa=new E({}),ga=new w({props:{name:"class transformers.FlavaImageProcessor",anchor:"transformers.FlavaImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": typing.Dict[str, int] = None"},{name:"resample",val:": Resampling = <Resampling.BICUBIC: 3>"},{name:"do_center_crop",val:": bool = True"},{name:"crop_size",val:": typing.Dict[str, int] = None"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": typing.Union[int, float] = 0.00392156862745098"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": typing.Union[float, typing.Iterable[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, typing.Iterable[float], NoneType] = None"},{name:"return_image_mask",val:": bool = False"},{name:"input_size_patches",val:": int = 14"},{name:"total_mask_patches",val:": int = 75"},{name:"mask_group_min_patches",val:": int = 16"},{name:"mask_group_max_patches",val:": typing.Optional[int] = None"},{name:"mask_group_min_aspect_ratio",val:": float = 0.3"},{name:"mask_group_max_aspect_ratio",val:": typing.Optional[float] = None"},{name:"return_codebook_pixels",val:": bool = False"},{name:"codebook_do_resize",val:": bool = True"},{name:"codebook_size",val:": bool = None"},{name:"codebook_resample",val:": int = <Resampling.LANCZOS: 1>"},{name:"codebook_do_center_crop",val:": bool = True"},{name:"codebook_crop_size",val:": int = None"},{name:"codebook_do_rescale",val:": bool = True"},{name:"codebook_rescale_factor",val:": typing.Union[int, float] = 0.00392156862745098"},{name:"codebook_do_map_pixels",val:": bool = True"},{name:"codebook_do_normalize",val:": bool = True"},{name:"codebook_image_mean",val:": typing.Union[float, typing.Iterable[float], NoneType] = None"},{name:"codebook_image_std",val:": typing.Union[float, typing.Iterable[float], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden by the
<code>do_resize</code> parameter in <code>preprocess</code>.`,name:"do_resize"},{anchor:"transformers.FlavaImageProcessor.size",description:`<strong>size</strong> (<code>Dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Size of the image after resizing. Can be overridden by the <code>size</code> parameter in <code>preprocess</code>.`,name:"size"},{anchor:"transformers.FlavaImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>PILImageResampling.BICUBIC</code>) &#x2014;
Resampling filter to use if resizing the image. Can be overridden by the <code>resample</code> parameter in
<code>preprocess</code>.`,name:"resample"},{anchor:"transformers.FlavaImageProcessor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to center crop the images. Can be overridden by the <code>do_center_crop</code> parameter in <code>preprocess</code>.`,name:"do_center_crop"},{anchor:"transformers.FlavaImageProcessor.crop_size",description:`<strong>crop_size</strong> (<code>Dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Size of image after the center crop <code>(crop_size[&quot;height&quot;], crop_size[&quot;width&quot;])</code>. Can be overridden by the
<code>crop_size</code> parameter in <code>preprocess</code>.`,name:"crop_size"},{anchor:"transformers.FlavaImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by the <code>do_rescale</code>
parameter in <code>preprocess</code>.`,name:"do_rescale"},{anchor:"transformers.FlavaImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overridden by the <code>rescale_factor</code> parameter in
<code>preprocess</code>.`,name:"rescale_factor"},{anchor:"transformers.FlavaImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by the <code>do_normalize</code> parameter in <code>preprocess</code>.`,name:"do_normalize"},{anchor:"transformers.FlavaImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_MEAN</code>) &#x2014;
Mean to use if normalizing the image. This is a float or list of floats the length of the number of
channels in the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.FlavaImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_STD</code>) &#x2014;
Standard deviation to use if normalizing the image. This is a float or list of floats the length of the
number of channels in the image. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"},{anchor:"transformers.FlavaImageProcessor.return_image_mask",description:`<strong>return_image_mask</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to return the image mask. Can be overridden by the <code>return_image_mask</code> parameter in <code>preprocess</code>.`,name:"return_image_mask"},{anchor:"transformers.FlavaImageProcessor.input_size_patches",description:`<strong>input_size_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 14) &#x2014;
Number of patches in the image in height and width direction. 14x14 = 196 total patches. Can be overridden
by the <code>input_size_patches</code> parameter in <code>preprocess</code>.`,name:"input_size_patches"},{anchor:"transformers.FlavaImageProcessor.total_mask_patches",description:`<strong>total_mask_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 75) &#x2014;
Total number of patches that should be masked. Can be overridden by the <code>total_mask_patches</code> parameter in
<code>preprocess</code>.`,name:"total_mask_patches"},{anchor:"transformers.FlavaImageProcessor.mask_group_min_patches",description:`<strong>mask_group_min_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Minimum number of patches that should be masked. Can be overridden by the <code>mask_group_min_patches</code>
parameter in <code>preprocess</code>.`,name:"mask_group_min_patches"},{anchor:"transformers.FlavaImageProcessor.mask_group_max_patches",description:`<strong>mask_group_max_patches</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum number of patches that should be masked. Can be overridden by the <code>mask_group_max_patches</code>
parameter in <code>preprocess</code>.`,name:"mask_group_max_patches"},{anchor:"transformers.FlavaImageProcessor.mask_group_min_aspect_ratio",description:`<strong>mask_group_min_aspect_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 0.3) &#x2014;
Minimum aspect ratio of the mask window. Can be overridden by the <code>mask_group_min_aspect_ratio</code> parameter
in <code>preprocess</code>.`,name:"mask_group_min_aspect_ratio"},{anchor:"transformers.FlavaImageProcessor.mask_group_max_aspect_ratio",description:`<strong>mask_group_max_aspect_ratio</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Maximum aspect ratio of the mask window. Can be overridden by the <code>mask_group_max_aspect_ratio</code> parameter
in <code>preprocess</code>.`,name:"mask_group_max_aspect_ratio"},{anchor:"transformers.FlavaImageProcessor.codebook_do_resize",description:`<strong>codebook_do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the input for codebook to a certain. Can be overridden by the <code>codebook_do_resize</code>
parameter in <code>preprocess</code>. <code>codebook_size</code>.`,name:"codebook_do_resize"},{anchor:"transformers.FlavaImageProcessor.codebook_size",description:`<strong>codebook_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Resize the input for codebook to the given size. Can be overridden by the <code>codebook_size</code> parameter in
<code>preprocess</code>.`,name:"codebook_size"},{anchor:"transformers.FlavaImageProcessor.codebook_resample",description:`<strong>codebook_resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>PILImageResampling.LANCZOS</code>) &#x2014;
Resampling filter to use if resizing the codebook image. Can be overridden by the <code>codebook_resample</code>
parameter in <code>preprocess</code>.`,name:"codebook_resample"},{anchor:"transformers.FlavaImageProcessor.codebook_do_center_crop",description:`<strong>codebook_do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to crop the input for codebook at the center. If the input size is smaller than
<code>codebook_crop_size</code> along any edge, the image is padded with 0&#x2019;s and then center cropped. Can be
overridden by the <code>codebook_do_center_crop</code> parameter in <code>preprocess</code>.`,name:"codebook_do_center_crop"},{anchor:"transformers.FlavaImageProcessor.codebook_crop_size",description:`<strong>codebook_crop_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Desired output size for codebook input when applying center-cropping. Can be overridden by the
<code>codebook_crop_size</code> parameter in <code>preprocess</code>.`,name:"codebook_crop_size"},{anchor:"transformers.FlavaImageProcessor.codebook_do_rescale",description:`<strong>codebook_do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the input for codebook by the specified scale <code>codebook_rescale_factor</code>. Can be
overridden by the <code>codebook_do_rescale</code> parameter in <code>preprocess</code>.`,name:"codebook_do_rescale"},{anchor:"transformers.FlavaImageProcessor.codebook_rescale_factor",description:`<strong>codebook_rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Defines the scale factor to use if rescaling the codebook image. Can be overridden by the
<code>codebook_rescale_factor</code> parameter in <code>preprocess</code>.`,name:"codebook_rescale_factor"},{anchor:"transformers.FlavaImageProcessor.codebook_do_map_pixels",description:`<strong>codebook_do_map_pixels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to map the pixel values of the codebook input to (1 - 2e)x + e. Can be overridden by the
<code>codebook_do_map_pixels</code> parameter in <code>preprocess</code>.`,name:"codebook_do_map_pixels"},{anchor:"transformers.FlavaImageProcessor.codebook_do_normalize",description:`<strong>codebook_do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input for codebook with <code>codebook_image_mean</code> and <code>codebook_image_std</code>. Can
be overridden by the <code>codebook_do_normalize</code> parameter in <code>preprocess</code>.`,name:"codebook_do_normalize"},{anchor:"transformers.FlavaImageProcessor.codebook_image_mean",description:`<strong>codebook_image_mean</strong> (<code>Optional[Union[float, Iterable[float]]]</code>, <em>optional</em>, defaults to <code>[0, 0, 0]</code>) &#x2014;
The sequence of means for each channel, to be used when normalizing images for codebook. Can be overridden
by the <code>codebook_image_mean</code> parameter in <code>preprocess</code>.`,name:"codebook_image_mean"},{anchor:"transformers.FlavaImageProcessor.codebook_image_std",description:`<strong>codebook_image_std</strong> (<code>Optional[Union[float, Iterable[float]]]</code>, <em>optional</em>, defaults to <code>[0.5, 0.5, 0.5]</code>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images for codebook. Can
be overridden by the <code>codebook_image_std</code> parameter in <code>preprocess</code>.`,name:"codebook_image_std"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/image_processing_flava.py#L127"}}),ha=new w({props:{name:"center_crop",anchor:"transformers.FlavaImageProcessor.center_crop",parameters:[{name:"image",val:": ndarray"},{name:"size",val:": typing.Dict[str, int]"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaImageProcessor.center_crop.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to center crop.`,name:"image"},{anchor:"transformers.FlavaImageProcessor.center_crop.size",description:`<strong>size</strong> (<code>Dict[str, int]</code>) &#x2014;
Size of the output image.`,name:"size"},{anchor:"transformers.FlavaImageProcessor.center_crop.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the image. If not provided, it will be the same as the input image.`,name:"data_format"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/image_processing_flava.py#L343"}}),ua=new w({props:{name:"normalize",anchor:"transformers.FlavaImageProcessor.normalize",parameters:[{name:"image",val:": ndarray"},{name:"mean",val:": typing.Union[float, typing.List[float]]"},{name:"std",val:": typing.Union[float, typing.List[float]]"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaImageProcessor.normalize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to normalize.`,name:"image"},{anchor:"transformers.FlavaImageProcessor.normalize.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>) &#x2014;
Image mean.`,name:"image_mean"},{anchor:"transformers.FlavaImageProcessor.normalize.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>) &#x2014;
Image standard deviation.`,name:"image_std"},{anchor:"transformers.FlavaImageProcessor.normalize.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the image. If not provided, it will be the same as the input image.`,name:"data_format"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/image_processing_flava.py#L385"}}),_a=new w({props:{name:"preprocess",anchor:"transformers.FlavaImageProcessor.preprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), typing.List[ForwardRef('PIL.Image.Image')], typing.List[numpy.ndarray], typing.List[ForwardRef('torch.Tensor')]]"},{name:"do_resize",val:": typing.Optional[bool] = None"},{name:"size",val:": typing.Dict[str, int] = None"},{name:"resample",val:": Resampling = None"},{name:"do_center_crop",val:": typing.Optional[bool] = None"},{name:"crop_size",val:": typing.Union[typing.Dict[str, int], NoneType] = None"},{name:"do_rescale",val:": typing.Optional[bool] = None"},{name:"rescale_factor",val:": typing.Optional[float] = None"},{name:"do_normalize",val:": typing.Optional[bool] = None"},{name:"image_mean",val:": typing.Union[float, typing.List[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, typing.List[float], NoneType] = None"},{name:"return_image_mask",val:": typing.Optional[bool] = None"},{name:"input_size_patches",val:": typing.Optional[int] = None"},{name:"total_mask_patches",val:": typing.Optional[int] = None"},{name:"mask_group_min_patches",val:": typing.Optional[int] = None"},{name:"mask_group_max_patches",val:": typing.Optional[int] = None"},{name:"mask_group_min_aspect_ratio",val:": typing.Optional[float] = None"},{name:"mask_group_max_aspect_ratio",val:": typing.Optional[float] = None"},{name:"return_codebook_pixels",val:": typing.Optional[bool] = None"},{name:"codebook_do_resize",val:": typing.Optional[bool] = None"},{name:"codebook_size",val:": typing.Union[typing.Dict[str, int], NoneType] = None"},{name:"codebook_resample",val:": typing.Optional[int] = None"},{name:"codebook_do_center_crop",val:": typing.Optional[bool] = None"},{name:"codebook_crop_size",val:": typing.Union[typing.Dict[str, int], NoneType] = None"},{name:"codebook_do_rescale",val:": typing.Optional[bool] = None"},{name:"codebook_rescale_factor",val:": typing.Optional[float] = None"},{name:"codebook_do_map_pixels",val:": typing.Optional[bool] = None"},{name:"codebook_do_normalize",val:": typing.Optional[bool] = None"},{name:"codebook_image_mean",val:": typing.Optional[typing.Iterable[float]] = None"},{name:"codebook_image_std",val:": typing.Optional[typing.Iterable[float]] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess.`,name:"images"},{anchor:"transformers.FlavaImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.FlavaImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the image.`,name:"size"},{anchor:"transformers.FlavaImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of the enum <code>PILImageResampling</code>, Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.FlavaImageProcessor.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_center_crop</code>) &#x2014;
Whether to center crop the image.`,name:"do_center_crop"},{anchor:"transformers.FlavaImageProcessor.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.crop_size</code>) &#x2014;
Size of the center crop. Only has an effect if <code>do_center_crop</code> is set to <code>True</code>.`,name:"crop_size"},{anchor:"transformers.FlavaImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.FlavaImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.FlavaImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.FlavaImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean.`,name:"image_mean"},{anchor:"transformers.FlavaImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation.`,name:"image_std"},{anchor:"transformers.FlavaImageProcessor.preprocess.return_image_mask",description:`<strong>return_image_mask</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.return_image_mask</code>) &#x2014;
Whether to return the image mask.`,name:"return_image_mask"},{anchor:"transformers.FlavaImageProcessor.preprocess.input_size_patches",description:`<strong>input_size_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.input_size_patches</code>) &#x2014;
Size of the patches to extract from the image.`,name:"input_size_patches"},{anchor:"transformers.FlavaImageProcessor.preprocess.total_mask_patches",description:`<strong>total_mask_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.total_mask_patches</code>) &#x2014;
Total number of patches to extract from the image.`,name:"total_mask_patches"},{anchor:"transformers.FlavaImageProcessor.preprocess.mask_group_min_patches",description:`<strong>mask_group_min_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.mask_group_min_patches</code>) &#x2014;
Minimum number of patches to extract from the image.`,name:"mask_group_min_patches"},{anchor:"transformers.FlavaImageProcessor.preprocess.mask_group_max_patches",description:`<strong>mask_group_max_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.mask_group_max_patches</code>) &#x2014;
Maximum number of patches to extract from the image.`,name:"mask_group_max_patches"},{anchor:"transformers.FlavaImageProcessor.preprocess.mask_group_min_aspect_ratio",description:`<strong>mask_group_min_aspect_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.mask_group_min_aspect_ratio</code>) &#x2014;
Minimum aspect ratio of the patches to extract from the image.`,name:"mask_group_min_aspect_ratio"},{anchor:"transformers.FlavaImageProcessor.preprocess.mask_group_max_aspect_ratio",description:`<strong>mask_group_max_aspect_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.mask_group_max_aspect_ratio</code>) &#x2014;
Maximum aspect ratio of the patches to extract from the image.`,name:"mask_group_max_aspect_ratio"},{anchor:"transformers.FlavaImageProcessor.preprocess.return_codebook_pixels",description:`<strong>return_codebook_pixels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.return_codebook_pixels</code>) &#x2014;
Whether to return the codebook pixels.`,name:"return_codebook_pixels"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_do_resize",description:`<strong>codebook_do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_resize</code>) &#x2014;
Whether to resize the codebook pixels.`,name:"codebook_do_resize"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_size",description:`<strong>codebook_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.codebook_size</code>) &#x2014;
Size of the codebook pixels.`,name:"codebook_size"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_resample",description:`<strong>codebook_resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.codebook_resample</code>) &#x2014;
Resampling filter to use if resizing the codebook pixels. This can be one of the enum
<code>PILImageResampling</code>, Only has an effect if <code>codebook_do_resize</code> is set to <code>True</code>.`,name:"codebook_resample"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_do_center_crop",description:`<strong>codebook_do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_center_crop</code>) &#x2014;
Whether to center crop the codebook pixels.`,name:"codebook_do_center_crop"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_crop_size",description:`<strong>codebook_crop_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.codebook_crop_size</code>) &#x2014;
Size of the center crop of the codebook pixels. Only has an effect if <code>codebook_do_center_crop</code> is set
to <code>True</code>.`,name:"codebook_crop_size"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_do_rescale",description:`<strong>codebook_do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_rescale</code>) &#x2014;
Whether to rescale the codebook pixels values between [0 - 1].`,name:"codebook_do_rescale"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_rescale_factor",description:`<strong>codebook_rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.codebook_rescale_factor</code>) &#x2014;
Rescale factor to rescale the codebook pixels by if <code>codebook_do_rescale</code> is set to <code>True</code>.`,name:"codebook_rescale_factor"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_do_map_pixels",description:`<strong>codebook_do_map_pixels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_map_pixels</code>) &#x2014;
Whether to map the codebook pixels values.`,name:"codebook_do_map_pixels"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_do_normalize",description:`<strong>codebook_do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_normalize</code>) &#x2014;
Whether to normalize the codebook pixels.`,name:"codebook_do_normalize"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_image_mean",description:`<strong>codebook_image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.codebook_image_mean</code>) &#x2014;
Codebook pixels mean to normalize the codebook pixels by if <code>codebook_do_normalize</code> is set to <code>True</code>.`,name:"codebook_image_mean"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_image_std",description:`<strong>codebook_image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.codebook_image_std</code>) &#x2014;
Codebook pixels standard deviation to normalize the codebook pixels by if <code>codebook_do_normalize</code> is
set to <code>True</code>.`,name:"codebook_image_std"},{anchor:"transformers.FlavaImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.FlavaImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"data_format"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/image_processing_flava.py#L459"}}),va=new w({props:{name:"rescale",anchor:"transformers.FlavaImageProcessor.rescale",parameters:[{name:"image",val:": ndarray"},{name:"scale",val:": typing.Union[int, float]"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaImageProcessor.rescale.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to rescale.`,name:"image"},{anchor:"transformers.FlavaImageProcessor.rescale.scale",description:`<strong>scale</strong> (<code>int</code> or <code>float</code>) &#x2014;
Scale to apply to the image.`,name:"scale"},{anchor:"transformers.FlavaImageProcessor.rescale.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the image. If not provided, it will be the same as the input image.`,name:"data_format"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/image_processing_flava.py#L365"}}),ba=new w({props:{name:"resize",anchor:"transformers.FlavaImageProcessor.resize",parameters:[{name:"image",val:": ndarray"},{name:"size",val:": typing.Dict[str, int]"},{name:"resample",val:": Resampling = <Resampling.BICUBIC: 3>"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaImageProcessor.resize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to resize.`,name:"image"},{anchor:"transformers.FlavaImageProcessor.resize.size",description:`<strong>size</strong> (<code>Dict[str, int]</code>) &#x2014;
Size of the output image.`,name:"size"},{anchor:"transformers.FlavaImageProcessor.resize.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>PILImageResampling.BICUBIC</code>) &#x2014;
Resampling filter to use when resiizing the image.`,name:"resample"},{anchor:"transformers.FlavaImageProcessor.resize.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the image. If not provided, it will be the same as the input image.`,name:"data_format"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/image_processing_flava.py#L315"}}),Fa=new E({}),ya=new w({props:{name:"class transformers.FlavaImageProcessor",anchor:"transformers.FlavaImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": typing.Dict[str, int] = None"},{name:"resample",val:": Resampling = <Resampling.BICUBIC: 3>"},{name:"do_center_crop",val:": bool = True"},{name:"crop_size",val:": typing.Dict[str, int] = None"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": typing.Union[int, float] = 0.00392156862745098"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": typing.Union[float, typing.Iterable[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, typing.Iterable[float], NoneType] = None"},{name:"return_image_mask",val:": bool = False"},{name:"input_size_patches",val:": int = 14"},{name:"total_mask_patches",val:": int = 75"},{name:"mask_group_min_patches",val:": int = 16"},{name:"mask_group_max_patches",val:": typing.Optional[int] = None"},{name:"mask_group_min_aspect_ratio",val:": float = 0.3"},{name:"mask_group_max_aspect_ratio",val:": typing.Optional[float] = None"},{name:"return_codebook_pixels",val:": bool = False"},{name:"codebook_do_resize",val:": bool = True"},{name:"codebook_size",val:": bool = None"},{name:"codebook_resample",val:": int = <Resampling.LANCZOS: 1>"},{name:"codebook_do_center_crop",val:": bool = True"},{name:"codebook_crop_size",val:": int = None"},{name:"codebook_do_rescale",val:": bool = True"},{name:"codebook_rescale_factor",val:": typing.Union[int, float] = 0.00392156862745098"},{name:"codebook_do_map_pixels",val:": bool = True"},{name:"codebook_do_normalize",val:": bool = True"},{name:"codebook_image_mean",val:": typing.Union[float, typing.Iterable[float], NoneType] = None"},{name:"codebook_image_std",val:": typing.Union[float, typing.Iterable[float], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden by the
<code>do_resize</code> parameter in <code>preprocess</code>.`,name:"do_resize"},{anchor:"transformers.FlavaImageProcessor.size",description:`<strong>size</strong> (<code>Dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Size of the image after resizing. Can be overridden by the <code>size</code> parameter in <code>preprocess</code>.`,name:"size"},{anchor:"transformers.FlavaImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>PILImageResampling.BICUBIC</code>) &#x2014;
Resampling filter to use if resizing the image. Can be overridden by the <code>resample</code> parameter in
<code>preprocess</code>.`,name:"resample"},{anchor:"transformers.FlavaImageProcessor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to center crop the images. Can be overridden by the <code>do_center_crop</code> parameter in <code>preprocess</code>.`,name:"do_center_crop"},{anchor:"transformers.FlavaImageProcessor.crop_size",description:`<strong>crop_size</strong> (<code>Dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Size of image after the center crop <code>(crop_size[&quot;height&quot;], crop_size[&quot;width&quot;])</code>. Can be overridden by the
<code>crop_size</code> parameter in <code>preprocess</code>.`,name:"crop_size"},{anchor:"transformers.FlavaImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by the <code>do_rescale</code>
parameter in <code>preprocess</code>.`,name:"do_rescale"},{anchor:"transformers.FlavaImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overridden by the <code>rescale_factor</code> parameter in
<code>preprocess</code>.`,name:"rescale_factor"},{anchor:"transformers.FlavaImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by the <code>do_normalize</code> parameter in <code>preprocess</code>.`,name:"do_normalize"},{anchor:"transformers.FlavaImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_MEAN</code>) &#x2014;
Mean to use if normalizing the image. This is a float or list of floats the length of the number of
channels in the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.FlavaImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_STD</code>) &#x2014;
Standard deviation to use if normalizing the image. This is a float or list of floats the length of the
number of channels in the image. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"},{anchor:"transformers.FlavaImageProcessor.return_image_mask",description:`<strong>return_image_mask</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to return the image mask. Can be overridden by the <code>return_image_mask</code> parameter in <code>preprocess</code>.`,name:"return_image_mask"},{anchor:"transformers.FlavaImageProcessor.input_size_patches",description:`<strong>input_size_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 14) &#x2014;
Number of patches in the image in height and width direction. 14x14 = 196 total patches. Can be overridden
by the <code>input_size_patches</code> parameter in <code>preprocess</code>.`,name:"input_size_patches"},{anchor:"transformers.FlavaImageProcessor.total_mask_patches",description:`<strong>total_mask_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 75) &#x2014;
Total number of patches that should be masked. Can be overridden by the <code>total_mask_patches</code> parameter in
<code>preprocess</code>.`,name:"total_mask_patches"},{anchor:"transformers.FlavaImageProcessor.mask_group_min_patches",description:`<strong>mask_group_min_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Minimum number of patches that should be masked. Can be overridden by the <code>mask_group_min_patches</code>
parameter in <code>preprocess</code>.`,name:"mask_group_min_patches"},{anchor:"transformers.FlavaImageProcessor.mask_group_max_patches",description:`<strong>mask_group_max_patches</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum number of patches that should be masked. Can be overridden by the <code>mask_group_max_patches</code>
parameter in <code>preprocess</code>.`,name:"mask_group_max_patches"},{anchor:"transformers.FlavaImageProcessor.mask_group_min_aspect_ratio",description:`<strong>mask_group_min_aspect_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 0.3) &#x2014;
Minimum aspect ratio of the mask window. Can be overridden by the <code>mask_group_min_aspect_ratio</code> parameter
in <code>preprocess</code>.`,name:"mask_group_min_aspect_ratio"},{anchor:"transformers.FlavaImageProcessor.mask_group_max_aspect_ratio",description:`<strong>mask_group_max_aspect_ratio</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Maximum aspect ratio of the mask window. Can be overridden by the <code>mask_group_max_aspect_ratio</code> parameter
in <code>preprocess</code>.`,name:"mask_group_max_aspect_ratio"},{anchor:"transformers.FlavaImageProcessor.codebook_do_resize",description:`<strong>codebook_do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the input for codebook to a certain. Can be overridden by the <code>codebook_do_resize</code>
parameter in <code>preprocess</code>. <code>codebook_size</code>.`,name:"codebook_do_resize"},{anchor:"transformers.FlavaImageProcessor.codebook_size",description:`<strong>codebook_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Resize the input for codebook to the given size. Can be overridden by the <code>codebook_size</code> parameter in
<code>preprocess</code>.`,name:"codebook_size"},{anchor:"transformers.FlavaImageProcessor.codebook_resample",description:`<strong>codebook_resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>PILImageResampling.LANCZOS</code>) &#x2014;
Resampling filter to use if resizing the codebook image. Can be overridden by the <code>codebook_resample</code>
parameter in <code>preprocess</code>.`,name:"codebook_resample"},{anchor:"transformers.FlavaImageProcessor.codebook_do_center_crop",description:`<strong>codebook_do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to crop the input for codebook at the center. If the input size is smaller than
<code>codebook_crop_size</code> along any edge, the image is padded with 0&#x2019;s and then center cropped. Can be
overridden by the <code>codebook_do_center_crop</code> parameter in <code>preprocess</code>.`,name:"codebook_do_center_crop"},{anchor:"transformers.FlavaImageProcessor.codebook_crop_size",description:`<strong>codebook_crop_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Desired output size for codebook input when applying center-cropping. Can be overridden by the
<code>codebook_crop_size</code> parameter in <code>preprocess</code>.`,name:"codebook_crop_size"},{anchor:"transformers.FlavaImageProcessor.codebook_do_rescale",description:`<strong>codebook_do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the input for codebook by the specified scale <code>codebook_rescale_factor</code>. Can be
overridden by the <code>codebook_do_rescale</code> parameter in <code>preprocess</code>.`,name:"codebook_do_rescale"},{anchor:"transformers.FlavaImageProcessor.codebook_rescale_factor",description:`<strong>codebook_rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Defines the scale factor to use if rescaling the codebook image. Can be overridden by the
<code>codebook_rescale_factor</code> parameter in <code>preprocess</code>.`,name:"codebook_rescale_factor"},{anchor:"transformers.FlavaImageProcessor.codebook_do_map_pixels",description:`<strong>codebook_do_map_pixels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to map the pixel values of the codebook input to (1 - 2e)x + e. Can be overridden by the
<code>codebook_do_map_pixels</code> parameter in <code>preprocess</code>.`,name:"codebook_do_map_pixels"},{anchor:"transformers.FlavaImageProcessor.codebook_do_normalize",description:`<strong>codebook_do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input for codebook with <code>codebook_image_mean</code> and <code>codebook_image_std</code>. Can
be overridden by the <code>codebook_do_normalize</code> parameter in <code>preprocess</code>.`,name:"codebook_do_normalize"},{anchor:"transformers.FlavaImageProcessor.codebook_image_mean",description:`<strong>codebook_image_mean</strong> (<code>Optional[Union[float, Iterable[float]]]</code>, <em>optional</em>, defaults to <code>[0, 0, 0]</code>) &#x2014;
The sequence of means for each channel, to be used when normalizing images for codebook. Can be overridden
by the <code>codebook_image_mean</code> parameter in <code>preprocess</code>.`,name:"codebook_image_mean"},{anchor:"transformers.FlavaImageProcessor.codebook_image_std",description:`<strong>codebook_image_std</strong> (<code>Optional[Union[float, Iterable[float]]]</code>, <em>optional</em>, defaults to <code>[0.5, 0.5, 0.5]</code>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images for codebook. Can
be overridden by the <code>codebook_image_std</code> parameter in <code>preprocess</code>.`,name:"codebook_image_std"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/image_processing_flava.py#L127"}}),xa=new w({props:{name:"preprocess",anchor:"transformers.FlavaImageProcessor.preprocess",parameters:[{name:"images",val:": typing.Union[ForwardRef('PIL.Image.Image'), numpy.ndarray, ForwardRef('torch.Tensor'), typing.List[ForwardRef('PIL.Image.Image')], typing.List[numpy.ndarray], typing.List[ForwardRef('torch.Tensor')]]"},{name:"do_resize",val:": typing.Optional[bool] = None"},{name:"size",val:": typing.Dict[str, int] = None"},{name:"resample",val:": Resampling = None"},{name:"do_center_crop",val:": typing.Optional[bool] = None"},{name:"crop_size",val:": typing.Union[typing.Dict[str, int], NoneType] = None"},{name:"do_rescale",val:": typing.Optional[bool] = None"},{name:"rescale_factor",val:": typing.Optional[float] = None"},{name:"do_normalize",val:": typing.Optional[bool] = None"},{name:"image_mean",val:": typing.Union[float, typing.List[float], NoneType] = None"},{name:"image_std",val:": typing.Union[float, typing.List[float], NoneType] = None"},{name:"return_image_mask",val:": typing.Optional[bool] = None"},{name:"input_size_patches",val:": typing.Optional[int] = None"},{name:"total_mask_patches",val:": typing.Optional[int] = None"},{name:"mask_group_min_patches",val:": typing.Optional[int] = None"},{name:"mask_group_max_patches",val:": typing.Optional[int] = None"},{name:"mask_group_min_aspect_ratio",val:": typing.Optional[float] = None"},{name:"mask_group_max_aspect_ratio",val:": typing.Optional[float] = None"},{name:"return_codebook_pixels",val:": typing.Optional[bool] = None"},{name:"codebook_do_resize",val:": typing.Optional[bool] = None"},{name:"codebook_size",val:": typing.Union[typing.Dict[str, int], NoneType] = None"},{name:"codebook_resample",val:": typing.Optional[int] = None"},{name:"codebook_do_center_crop",val:": typing.Optional[bool] = None"},{name:"codebook_crop_size",val:": typing.Union[typing.Dict[str, int], NoneType] = None"},{name:"codebook_do_rescale",val:": typing.Optional[bool] = None"},{name:"codebook_rescale_factor",val:": typing.Optional[float] = None"},{name:"codebook_do_map_pixels",val:": typing.Optional[bool] = None"},{name:"codebook_do_normalize",val:": typing.Optional[bool] = None"},{name:"codebook_image_mean",val:": typing.Optional[typing.Iterable[float]] = None"},{name:"codebook_image_std",val:": typing.Optional[typing.Iterable[float]] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.utils.generic.TensorType, NoneType] = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess.`,name:"images"},{anchor:"transformers.FlavaImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.FlavaImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the image.`,name:"size"},{anchor:"transformers.FlavaImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of the enum <code>PILImageResampling</code>, Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.FlavaImageProcessor.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_center_crop</code>) &#x2014;
Whether to center crop the image.`,name:"do_center_crop"},{anchor:"transformers.FlavaImageProcessor.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.crop_size</code>) &#x2014;
Size of the center crop. Only has an effect if <code>do_center_crop</code> is set to <code>True</code>.`,name:"crop_size"},{anchor:"transformers.FlavaImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.FlavaImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.FlavaImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.FlavaImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean.`,name:"image_mean"},{anchor:"transformers.FlavaImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation.`,name:"image_std"},{anchor:"transformers.FlavaImageProcessor.preprocess.return_image_mask",description:`<strong>return_image_mask</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.return_image_mask</code>) &#x2014;
Whether to return the image mask.`,name:"return_image_mask"},{anchor:"transformers.FlavaImageProcessor.preprocess.input_size_patches",description:`<strong>input_size_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.input_size_patches</code>) &#x2014;
Size of the patches to extract from the image.`,name:"input_size_patches"},{anchor:"transformers.FlavaImageProcessor.preprocess.total_mask_patches",description:`<strong>total_mask_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.total_mask_patches</code>) &#x2014;
Total number of patches to extract from the image.`,name:"total_mask_patches"},{anchor:"transformers.FlavaImageProcessor.preprocess.mask_group_min_patches",description:`<strong>mask_group_min_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.mask_group_min_patches</code>) &#x2014;
Minimum number of patches to extract from the image.`,name:"mask_group_min_patches"},{anchor:"transformers.FlavaImageProcessor.preprocess.mask_group_max_patches",description:`<strong>mask_group_max_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.mask_group_max_patches</code>) &#x2014;
Maximum number of patches to extract from the image.`,name:"mask_group_max_patches"},{anchor:"transformers.FlavaImageProcessor.preprocess.mask_group_min_aspect_ratio",description:`<strong>mask_group_min_aspect_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.mask_group_min_aspect_ratio</code>) &#x2014;
Minimum aspect ratio of the patches to extract from the image.`,name:"mask_group_min_aspect_ratio"},{anchor:"transformers.FlavaImageProcessor.preprocess.mask_group_max_aspect_ratio",description:`<strong>mask_group_max_aspect_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.mask_group_max_aspect_ratio</code>) &#x2014;
Maximum aspect ratio of the patches to extract from the image.`,name:"mask_group_max_aspect_ratio"},{anchor:"transformers.FlavaImageProcessor.preprocess.return_codebook_pixels",description:`<strong>return_codebook_pixels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.return_codebook_pixels</code>) &#x2014;
Whether to return the codebook pixels.`,name:"return_codebook_pixels"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_do_resize",description:`<strong>codebook_do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_resize</code>) &#x2014;
Whether to resize the codebook pixels.`,name:"codebook_do_resize"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_size",description:`<strong>codebook_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.codebook_size</code>) &#x2014;
Size of the codebook pixels.`,name:"codebook_size"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_resample",description:`<strong>codebook_resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.codebook_resample</code>) &#x2014;
Resampling filter to use if resizing the codebook pixels. This can be one of the enum
<code>PILImageResampling</code>, Only has an effect if <code>codebook_do_resize</code> is set to <code>True</code>.`,name:"codebook_resample"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_do_center_crop",description:`<strong>codebook_do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_center_crop</code>) &#x2014;
Whether to center crop the codebook pixels.`,name:"codebook_do_center_crop"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_crop_size",description:`<strong>codebook_crop_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.codebook_crop_size</code>) &#x2014;
Size of the center crop of the codebook pixels. Only has an effect if <code>codebook_do_center_crop</code> is set
to <code>True</code>.`,name:"codebook_crop_size"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_do_rescale",description:`<strong>codebook_do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_rescale</code>) &#x2014;
Whether to rescale the codebook pixels values between [0 - 1].`,name:"codebook_do_rescale"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_rescale_factor",description:`<strong>codebook_rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.codebook_rescale_factor</code>) &#x2014;
Rescale factor to rescale the codebook pixels by if <code>codebook_do_rescale</code> is set to <code>True</code>.`,name:"codebook_rescale_factor"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_do_map_pixels",description:`<strong>codebook_do_map_pixels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_map_pixels</code>) &#x2014;
Whether to map the codebook pixels values.`,name:"codebook_do_map_pixels"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_do_normalize",description:`<strong>codebook_do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_normalize</code>) &#x2014;
Whether to normalize the codebook pixels.`,name:"codebook_do_normalize"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_image_mean",description:`<strong>codebook_image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.codebook_image_mean</code>) &#x2014;
Codebook pixels mean to normalize the codebook pixels by if <code>codebook_do_normalize</code> is set to <code>True</code>.`,name:"codebook_image_mean"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_image_std",description:`<strong>codebook_image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.codebook_image_std</code>) &#x2014;
Codebook pixels standard deviation to normalize the codebook pixels by if <code>codebook_do_normalize</code> is
set to <code>True</code>.`,name:"codebook_image_std"},{anchor:"transformers.FlavaImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.FlavaImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"data_format"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/image_processing_flava.py#L459"}}),Ta=new E({}),$a=new w({props:{name:"class transformers.FlavaForPreTraining",anchor:"transformers.FlavaForPreTraining",parameters:[{name:"config",val:": FlavaConfig"},{name:"image_codebook",val:": typing.Optional[torch.nn.modules.module.Module] = None"}],parametersDescription:[{anchor:"transformers.FlavaForPreTraining.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig">FlavaConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlavaForPreTraining.image_codebook",description:`<strong>image_codebook</strong> (<code>nn.Module</code>) &#x2014; If passed, the image codebook will be set to this. Otherwise. it will
be initialized using the image_codebook_config defined in the config first as the first parameter.`,name:"image_codebook"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1731"}}),Ia=new w({props:{name:"forward",anchor:"transformers.FlavaForPreTraining.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"input_ids_masked",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"codebook_pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"image_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"skip_unmasked_multimodal_encoder",val:": bool = None"},{name:"mlm_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"mim_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"itm_labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": bool = True"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"return_loss",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaForPreTraining.forward.input_ids_masked",description:`<strong>input_ids_masked</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. These ones are the masked version of the original task
to be used with MLM. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> along with
<code>DataCollatorForMaskedLanguageModeling</code>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids_masked"},{anchor:"transformers.FlavaForPreTraining.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlavaForPreTraining.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FlavaForPreTraining.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageProcessor">FlavaFeatureExtractor</a>. See
<a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor.__call__">FlavaFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.FlavaForPreTraining.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FlavaForPreTraining.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FlavaForPreTraining.forward.image_attention_mask",description:`<strong>image_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices specifically for images. Mask values selected
in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"image_attention_mask"},{anchor:"transformers.FlavaForPreTraining.forward.skip_unmasked_multimodal_encoder",description:`<strong>skip_unmasked_multimodal_encoder</strong> (<em>bool</em>, <em>optional</em>) &#x2014;
Skip any calculations for multimodal encoder for unmasked inputs. FLAVA pretraining doesn&#x2019;t need unmasked
multimodal embeddings or outputs as of now.`,name:"skip_unmasked_multimodal_encoder"},{anchor:"transformers.FlavaForPreTraining.forward.mlm_labels",description:`<strong>mlm_labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>, <em>optional</em>) &#x2014;
Labels for computing the left-to-right language and multimodal masked modeling loss (next word prediction).
Indices should be in <code>[-100, 0, ..., text_config.vocab_size - 1]</code> (see <code>input_ids</code> docstring). Tokens with
indices set to <code>-100</code> are ignored (masked), the loss is only computed for the tokens with labels in <code>[0, ..., text_config.vocab_size - 1]</code>.`,name:"mlm_labels"},{anchor:"transformers.FlavaForPreTraining.forward.mim_labels",description:`<strong>mim_labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Labels for computing the image and multimodal masked modeling loss. Indices should be in <code>[-100, 0, ..., image_config.vocab_size - 1]</code>. Tokens with indices set to <code>-100</code> are ignored (masked), the loss is only
computed for the tokens with labels in <code>[0, ..., image_config.vocab_size - 1]</code>. If not passed, they are
generated automatically using the image codebook assigned to the model. By default, it uses
<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageCodebook">FlavaImageCodebook</a>. See <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageCodebook">FlavaImageCodebook</a> to understand how to generate mim_labels.`,name:"mim_labels"},{anchor:"transformers.FlavaForPreTraining.forward.itm_labels",description:`<strong>itm_labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, 1)</code>, <em>optional</em>) &#x2014;
Labels for computing the image-text matching loss. 0 means the pairs don&#x2019;t match and 1 means they match.
The pairs with 0 will be skipped for calculation of MMM and global contrastive losses as well.`,name:"itm_labels"},{anchor:"transformers.FlavaForPreTraining.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>, default to None) &#x2014;
Whether to return calculated loss or not.`,name:"return_loss"},{anchor:"transformers.FlavaForPreTraining.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_len)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaForPreTraining.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaForPreTraining.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaForPreTraining.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaForPreTraining.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.</p>
<p>Examples &#x2014;`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1767",returnDescription:`
<p>A <code>transformers.models.flava.modeling_flava.FlavaForPreTrainingOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.flava.configuration_flava.FlavaConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code>, <em>optional</em>, returned when <code>return_loss</code> is True) \u2014 Total loss calculated for this model.</p>
</li>
<li>
<p><strong>loss_info</strong> (<code>FlavaLosses</code>) \u2014 Detailed info for FLAVA Pretraining losses. Check <code>FlavaLosses</code> class description for the information on
the keys.</p>
</li>
<li>
<p><strong>image_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>.</p>
</li>
<li>
<p><strong>image_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>.</p>
</li>
<li>
<p><strong>text_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>text_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_unmasked_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_output</strong> (<code>BaseModelOutputWithPooling</code>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_unmasked_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalModel"
>FlavaMultimodalModel</a>.</p>
</li>
<li>
<p><strong>image_masked_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>. Uses <code>bool_masked_pos</code>
to create masked images.</p>
</li>
<li>
<p><strong>image_masked_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>. Uses <code>bool_masked_pos</code> to create masked images.</p>
</li>
<li>
<p><strong>text_masked_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>text_masked_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>input_ids_masked</code> are present) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_masked_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_masked_output</strong> (<code>BaseModelOutputWithPooling</code>, returned when <code>input_ids_masked</code> and <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalModel"
>FlavaMultimodalModel</a>.</p>
</li>
<li>
<p><strong>mim_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_image_patches, image_vocab_size)</code> or of shape <code>(total_masked_patches, image_vocab_size)</code> , <em>optional</em>, returned when <code>pixel_values</code> are present and <code>input_ids_masked</code> are not) \u2014 The logits for MIM unimodal loss. Uses <code>book_masked_pos</code> to get masked patches. The flattened output is
returned when <code>bool_masked_pos</code> has some of the patches masked.</p>
</li>
<li>
<p><strong>mlm_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length, text_vocab_size)</code> or of shape <code>(total_masked_seq_length, text_vocab_size)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> are present and <code>pixel_values</code> are not) \u2014 The logits for MLM unimodal loss. The flattened output is returned when <code>input_ids_masked</code> has some of
the tokens masked.</p>
</li>
<li>
<p><strong>itm_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, 2)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> and <code>pixel_values</code> are present) \u2014 The logits for ITM loss. Note that ITM loss is calculated on masked pairs in FLAVA.</p>
</li>
<li>
<p><strong>mmm_image_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_image_patches, image_vocab_size)</code> or of shape<code>(total_masked_patches, image_vocab_size)</code>, <em>optional</em>, returned when <code>pixel_values</code> and <code>input_ids_masked</code> are present) \u2014 The logits for MMM image multimodal loss. Uses <code>book_masked_pos</code> to get masked patches. The flattened
output is returned when <code>bool_masked_pos</code> has some of the patches masked.</p>
</li>
<li>
<p><strong>mmm_text_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length, text_vocab_size)</code> or of shape <code>(</code>(total_masked_seq_length, text_vocab_size)<code>), *optional*, returned when </code>pixel_values<code>and</code>input_ids_masked<code>are present) -- The logits for MMM text multimodal loss. The flattened output is returned when</code>input_ids_masked\` has
some of the tokens masked.</p>
</li>
<li>
<p><strong>contrastive_logits_per_image</strong> (<code>torch.FloatTensor</code> of shape <code>(image_batch_size, text_batch_size)</code>) \u2014 The scaled dot product scores between <code>image_embeddings</code> and <code>text_embeddings</code> but passed through FLAVA\u2019s
<code>image_projection</code> and <code>text_projection</code> layers respectively. This represents the image-text similarity
scores. This is calculated on unmasked images and texts.</p>
</li>
<li>
<p><strong>contrastive_logits_per_text</strong> (<code>torch.FloatTensor</code> of shape <code>(text_batch_size, image_batch_size)</code>) \u2014 The scaled dot product scores between <code>text_embeddings</code> and <code>image_embeddings</code> but passed through FLAVA\u2019s
<code>text_projection</code> and <code>image_projection</code> layers respectively. This is calculated on unmasked images and
texts.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.flava.modeling_flava.FlavaForPreTrainingOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),io=new dt({props:{$$slots:{default:[Nf]},$$scope:{ctx:z}}}),Ma=new E({}),Pa=new w({props:{name:"class transformers.FlavaModel",anchor:"transformers.FlavaModel",parameters:[{name:"config",val:": FlavaConfig"}],parametersDescription:[{anchor:"transformers.FlavaModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig">FlavaConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1199"}}),Aa=new w({props:{name:"forward",anchor:"transformers.FlavaModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"pixel_values",val:": typing.Optional[torch.FloatTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"image_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"skip_multimodal_encoder",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": bool = True"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageProcessor">FlavaFeatureExtractor</a>. See
<a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor.__call__">FlavaFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.FlavaModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FlavaModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FlavaModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlavaModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FlavaModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FlavaModel.forward.skip_multimodal_encoder",description:`<strong>skip_multimodal_encoder</strong> (<em>bool</em>, <em>optional</em>) &#x2014;
Skip any calculations for multimodal encoder. Useful if multimodal encoding is not going to be used.`,name:"skip_multimodal_encoder"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1343",returnDescription:`
<p>A <code>transformers.models.flava.modeling_flava.FlavaModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.flava.configuration_flava.FlavaConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>image_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>.</li>
<li><strong>image_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>.</li>
<li><strong>text_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</li>
<li><strong>text_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>input_ids</code> are present) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</li>
<li><strong>multimodal_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</li>
<li><strong>multimodal_output</strong> (<code>BaseModelOutputWithPooling</code>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_multimodal_encoder</code> is <code>None</code> or <code>False</code>) \u2014 The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalModel"
>FlavaMultimodalModel</a>.</li>
</ul>
`,returnType:`
<p><code>transformers.models.flava.modeling_flava.FlavaModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),co=new dt({props:{$$slots:{default:[Of]},$$scope:{ctx:z}}}),mo=new Mo({props:{anchor:"transformers.FlavaModel.forward.example",$$slots:{default:[qf]},$$scope:{ctx:z}}}),Na=new w({props:{name:"get_text_features",anchor:"transformers.FlavaModel.get_text_features",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlavaModel.get_text_features.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FlavaModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaModel.get_text_features.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1245"}}),po=new dt({props:{$$slots:{default:[Lf]},$$scope:{ctx:z}}}),Oa=new w({props:{name:"get_image_features",anchor:"transformers.FlavaModel.get_image_features",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"interpolate_pos_encoding",val:": typing.Optional[bool] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageProcessor">FlavaFeatureExtractor</a>. See
<a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor.__call__">FlavaFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.FlavaModel.get_image_features.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FlavaModel.get_image_features.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FlavaModel.get_image_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaModel.get_image_features.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1291"}}),fo=new dt({props:{$$slots:{default:[Df]},$$scope:{ctx:z}}}),qa=new E({}),La=new w({props:{name:"class transformers.FlavaImageCodebook",anchor:"transformers.FlavaImageCodebook",parameters:[{name:"config",val:": FlavaImageCodebookConfig"},{name:"**kwargs",val:": typing.Any"}],parametersDescription:[{anchor:"transformers.FlavaImageCodebook.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageCodebookConfig">FlavaImageCodebookConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1517"}}),Sa=new w({props:{name:"forward",anchor:"transformers.FlavaImageCodebook.forward",parameters:[{name:"pixel_values",val:": FloatTensor"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1601"}}),Ra=new w({props:{name:"get_codebook_indices",anchor:"transformers.FlavaImageCodebook.get_codebook_indices",parameters:[{name:"pixel_values",val:": Tensor"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1567"}}),Ba=new w({props:{name:"get_codebook_probs",anchor:"transformers.FlavaImageCodebook.get_codebook_probs",parameters:[{name:"pixel_values",val:": Tensor"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1597"}}),Va=new E({}),Ua=new w({props:{name:"class transformers.FlavaTextModel",anchor:"transformers.FlavaTextModel",parameters:[{name:"config",val:": FlavaTextConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.FlavaTextModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextConfig">FlavaTextConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L994"}}),Ya=new w({props:{name:"forward",anchor:"transformers.FlavaTextModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlavaTextModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FlavaTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaTextModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1025",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextConfig"
>FlavaTextConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),uo=new dt({props:{$$slots:{default:[jf]},$$scope:{ctx:z}}}),_o=new Mo({props:{anchor:"transformers.FlavaTextModel.forward.example",$$slots:{default:[Wf]},$$scope:{ctx:z}}}),Za=new E({}),Ja=new w({props:{name:"class transformers.FlavaImageModel",anchor:"transformers.FlavaImageModel",parameters:[{name:"config",val:": FlavaImageConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.FlavaImageModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageConfig">FlavaImageConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L894"}}),Qa=new w({props:{name:"forward",anchor:"transformers.FlavaImageModel.forward",parameters:[{name:"pixel_values",val:": typing.Optional[torch.Tensor] = None"},{name:"bool_masked_pos",val:": typing.Optional[torch.BoolTensor] = None"},{name:"interpolate_pos_encoding",val:": typing.Optional[bool] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaImageModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageProcessor">FlavaFeatureExtractor</a>. See
<a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor.__call__">FlavaFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.FlavaImageModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FlavaImageModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FlavaImageModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaImageModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaImageModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaImageModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaImageModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L927",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageConfig"
>FlavaImageConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),bo=new dt({props:{$$slots:{default:[Sf]},$$scope:{ctx:z}}}),ko=new Mo({props:{anchor:"transformers.FlavaImageModel.forward.example",$$slots:{default:[Rf]},$$scope:{ctx:z}}}),et=new E({}),ot=new w({props:{name:"class transformers.FlavaMultimodalModel",anchor:"transformers.FlavaMultimodalModel",parameters:[{name:"config",val:": FlavaMultimodalConfig"},{name:"add_pooling_layer",val:" = True"}],parametersDescription:[{anchor:"transformers.FlavaMultimodalModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalConfig">FlavaMultimodalConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1100"}}),nt=new w({props:{name:"forward",anchor:"transformers.FlavaMultimodalModel.forward",parameters:[{name:"hidden_states",val:": Tensor"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.FlavaMultimodalModel.forward.hidden_states",description:`<strong>hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len, hidden_size)</code>) &#x2014;
The concatenated hidden states of unimodal encoders.`,name:"hidden_states"},{anchor:"transformers.FlavaMultimodalModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaMultimodalModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaMultimodalModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaMultimodalModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaMultimodalModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1128",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalConfig"
>FlavaMultimodalConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),yo=new dt({props:{$$slots:{default:[Bf]},$$scope:{ctx:z}}}),xo=new Mo({props:{anchor:"transformers.FlavaMultimodalModel.forward.example",$$slots:{default:[Vf]},$$scope:{ctx:z}}}),{c(){p=t("meta"),$=c(),_=t("h1"),h=t("a"),v=t("span"),b(l.$$.fragment),f=c(),I=t("span"),Ss=s("FLAVA"),vr=c(),le=t("h2"),qe=t("a"),cn=t("span"),b(Eo.$$.fragment),Rs=c(),mn=t("span"),Bs=s("Overview"),br=c(),Le=t("p"),Vs=s("The FLAVA model was proposed in "),Ao=t("a"),Us=s("FLAVA: A Foundational Language And Vision Alignment Model"),Hs=s(" by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022."),kr=c(),ct=t("p"),Gs=s(`The paper aims at creating a single unified foundation model which can work across vision, language
as well as vision-and-language multimodal tasks.`),Fr=c(),mt=t("p"),Ys=s("The abstract from the paper is the following:"),yr=c(),pt=t("p"),pn=t("em"),Zs=s(`State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety
of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal
(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising
direction would be to use a single holistic universal model, as a \u201Cfoundation\u201D, that targets all modalities
at once \u2014 a true vision and language foundation model should be good at vision tasks, language tasks, and
cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate
impressive performance on a wide range of 35 tasks spanning these target modalities.`),xr=c(),X=t("p"),Js=s("This model was contributed by "),No=t("a"),Ks=s("aps"),Xs=s(". The original code can be found "),Oo=t("a"),Qs=s("here"),ei=s("."),Tr=c(),de=t("h2"),De=t("a"),fn=t("span"),b(qo.$$.fragment),oi=c(),gn=t("span"),ai=s("FlavaConfig"),$r=c(),P=t("div"),b(Lo.$$.fragment),ti=c(),Q=t("p"),ft=t("a"),ni=s("FlavaConfig"),ri=s(" is the configuration class to store the configuration of a "),gt=t("a"),si=s("FlavaModel"),ii=s(`. It is used to
instantiate FLAVA model according to the specified arguments, defining the text model, image model, image codebook
and multimodal model configs. Instantiating a configuration with the defaults will yield a similar configuration to
that of the FLAVA `),Do=t("a"),li=s("facebook/flava-full"),di=s(" architecture."),ci=c(),ce=t("p"),mi=s("Configuration objects inherit from "),ht=t("a"),pi=s("PretrainedConfig"),fi=s(` and can be used to control the model outputs. Read the
documentation from `),ut=t("a"),gi=s("PretrainedConfig"),hi=s(" for more information."),ui=c(),b(je.$$.fragment),_i=c(),We=t("div"),b(jo.$$.fragment),vi=c(),Wo=t("p"),bi=s("Instantiate a "),_t=t("a"),ki=s("FlavaConfig"),Fi=s(` (or a derived class) from flava text model configuration, flava image model
configuration, flava multimodal model and flava codebook model configuration.`),yi=c(),Se=t("div"),b(So.$$.fragment),xi=c(),Ro=t("p"),Ti=s("Serializes this instance to a Python dictionary. Override the default "),vt=t("a"),$i=s("to_dict()"),wi=s("."),wr=c(),me=t("h2"),Re=t("a"),hn=t("span"),b(Bo.$$.fragment),zi=c(),un=t("span"),Ii=s("FlavaTextConfig"),zr=c(),A=t("div"),b(Vo.$$.fragment),Mi=c(),Uo=t("p"),Pi=s("This is the configuration class to store the configuration of a "),bt=t("a"),Ci=s("FlavaTextModel"),Ei=s(`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture.`),Ai=c(),Ho=t("p"),Ni=s(`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
`),Go=t("a"),Oi=s("facebook/flava-full"),qi=s(" architecture."),Li=c(),pe=t("p"),Di=s("Configuration objects inherit from "),kt=t("a"),ji=s("PretrainedConfig"),Wi=s(` and can be used to control the model outputs. Read the
documentation from `),Ft=t("a"),Si=s("PretrainedConfig"),Ri=s(" for more information."),Bi=c(),b(Be.$$.fragment),Ir=c(),fe=t("h2"),Ve=t("a"),_n=t("span"),b(Yo.$$.fragment),Vi=c(),vn=t("span"),Ui=s("FlavaImageConfig"),Mr=c(),N=t("div"),b(Zo.$$.fragment),Hi=c(),Jo=t("p"),Gi=s("This is the configuration class to store the configuration of a "),yt=t("a"),Yi=s("FlavaImageModel"),Zi=s(`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture.`),Ji=c(),Ko=t("p"),Ki=s(`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
`),Xo=t("a"),Xi=s("facebook/flava-full"),Qi=s(" architecture."),el=c(),ge=t("p"),ol=s("Configuration objects inherit from "),xt=t("a"),al=s("PretrainedConfig"),tl=s(` and can be used to control the model outputs. Read the
documentation from `),Tt=t("a"),nl=s("PretrainedConfig"),rl=s(" for more information."),sl=c(),b(Ue.$$.fragment),Pr=c(),he=t("h2"),He=t("a"),bn=t("span"),b(Qo.$$.fragment),il=c(),kn=t("span"),ll=s("FlavaMultimodalConfig"),Cr=c(),O=t("div"),b(ea.$$.fragment),dl=c(),oa=t("p"),cl=s("This is the configuration class to store the configuration of a "),$t=t("a"),ml=s("FlavaMultimodalModel"),pl=s(`. It is used to instantiate
an FLAVA model according to the specified arguments, defining the model architecture.`),fl=c(),aa=t("p"),gl=s(`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
`),ta=t("a"),hl=s("facebook/flava-full"),ul=s(" architecture."),_l=c(),ue=t("p"),vl=s("Configuration objects inherit from "),wt=t("a"),bl=s("PretrainedConfig"),kl=s(` and can be used to control the model outputs. Read the
documentation from `),zt=t("a"),Fl=s("PretrainedConfig"),yl=s(" for more information."),xl=c(),b(Ge.$$.fragment),Er=c(),_e=t("h2"),Ye=t("a"),Fn=t("span"),b(na.$$.fragment),Tl=c(),yn=t("span"),$l=s("FlavaImageCodebookConfig"),Ar=c(),ra=t("div"),b(sa.$$.fragment),Nr=c(),ve=t("h2"),Ze=t("a"),xn=t("span"),b(ia.$$.fragment),wl=c(),Tn=t("span"),zl=s("FlavaProcessor"),Or=c(),q=t("div"),b(la.$$.fragment),Il=c(),$n=t("p"),Ml=s("Constructs a FLAVA processor which wraps a FLAVA feature extractor and a FLAVA tokenizer into a single processor."),Pl=c(),D=t("p"),It=t("a"),Cl=s("FlavaProcessor"),El=s(" offers all the functionalities of "),Mt=t("a"),Al=s("FlavaFeatureExtractor"),Nl=s(" and "),Pt=t("a"),Ol=s("BertTokenizerFast"),ql=s(`. See the
`),wn=t("code"),Ll=s("__call__()"),Dl=s(" and "),Ct=t("a"),jl=s("decode()"),Wl=s(" for more information."),Sl=c(),Je=t("div"),b(da.$$.fragment),Rl=c(),ca=t("p"),Bl=s("This method forwards all its arguments to BertTokenizerFast\u2019s "),Et=t("a"),Vl=s("batch_decode()"),Ul=s(`. Please
refer to the docstring of this method for more information.`),Hl=c(),Ke=t("div"),b(ma.$$.fragment),Gl=c(),pa=t("p"),Yl=s("This method forwards all its arguments to BertTokenizerFast\u2019s "),At=t("a"),Zl=s("decode()"),Jl=s(`. Please refer to
the docstring of this method for more information.`),qr=c(),be=t("h2"),Xe=t("a"),zn=t("span"),b(fa.$$.fragment),Kl=c(),In=t("span"),Xl=s("FlavaFeatureExtractor"),Lr=c(),M=t("div"),b(ga.$$.fragment),Ql=c(),Mn=t("p"),ed=s("Constructs a Flava image processor."),od=c(),Qe=t("div"),b(ha.$$.fragment),ad=c(),ke=t("p"),td=s("Center crop an image to "),Pn=t("code"),nd=s('(size["height"], size["width"])'),rd=s(". If the input size is smaller than "),Cn=t("code"),sd=s("crop_size"),id=s(` along
any edge, the image is padded with 0\u2019s and then center cropped.`),ld=c(),eo=t("div"),b(ua.$$.fragment),dd=c(),En=t("p"),cd=s("Normalize an image. image = (image - image_mean) / image_std."),md=c(),oo=t("div"),b(_a.$$.fragment),pd=c(),An=t("p"),fd=s("Preprocess an image or batch of images."),gd=c(),ao=t("div"),b(va.$$.fragment),hd=c(),Nn=t("p"),ud=s("Rescale an image by a scale factor. image = image * scale."),_d=c(),to=t("div"),b(ba.$$.fragment),vd=c(),ka=t("p"),bd=s("Resize an image to "),On=t("code"),kd=s('(size["height"], size["width"])'),Fd=s("."),Dr=c(),Fe=t("h2"),no=t("a"),qn=t("span"),b(Fa.$$.fragment),yd=c(),Ln=t("span"),xd=s("FlavaImageProcessor"),jr=c(),G=t("div"),b(ya.$$.fragment),Td=c(),Dn=t("p"),$d=s("Constructs a Flava image processor."),wd=c(),ro=t("div"),b(xa.$$.fragment),zd=c(),jn=t("p"),Id=s("Preprocess an image or batch of images."),Wr=c(),ye=t("h2"),so=t("a"),Wn=t("span"),b(Ta.$$.fragment),Md=c(),Sn=t("span"),Pd=s("FlavaForPreTraining"),Sr=c(),W=t("div"),b($a.$$.fragment),Cd=c(),Rn=t("p"),Ed=s("The FLAVA model for pretraining which outputs losses, embeddings, logits and transformer outputs."),Ad=c(),wa=t("p"),Nd=s("This model is a PyTorch "),za=t("a"),Od=s("torch.nn.Module"),qd=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ld=c(),ee=t("div"),b(Ia.$$.fragment),Dd=c(),xe=t("p"),jd=s("The "),Nt=t("a"),Wd=s("FlavaForPreTraining"),Sd=s(" forward method, overrides the "),Bn=t("code"),Rd=s("__call__"),Bd=s(" special method."),Vd=c(),b(io.$$.fragment),Rr=c(),Te=t("h2"),lo=t("a"),Vn=t("span"),b(Ma.$$.fragment),Ud=c(),Un=t("span"),Hd=s("FlavaModel"),Br=c(),L=t("div"),b(Pa.$$.fragment),Gd=c(),Ca=t("p"),Yd=s(`The bare FLAVA Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Ea=t("a"),Zd=s("torch.nn.Module"),Jd=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Kd=c(),S=t("div"),b(Aa.$$.fragment),Xd=c(),$e=t("p"),Qd=s("The "),Ot=t("a"),ec=s("FlavaModel"),oc=s(" forward method, overrides the "),Hn=t("code"),ac=s("__call__"),tc=s(" special method."),nc=c(),b(co.$$.fragment),rc=c(),b(mo.$$.fragment),sc=c(),oe=t("div"),b(Na.$$.fragment),ic=c(),we=t("p"),lc=s("The "),qt=t("a"),dc=s("FlavaModel"),cc=s(" forward method, overrides the "),Gn=t("code"),mc=s("__call__"),pc=s(" special method."),fc=c(),b(po.$$.fragment),gc=c(),ae=t("div"),b(Oa.$$.fragment),hc=c(),ze=t("p"),uc=s("The "),Lt=t("a"),_c=s("FlavaModel"),vc=s(" forward method, overrides the "),Yn=t("code"),bc=s("__call__"),kc=s(" special method."),Fc=c(),b(fo.$$.fragment),Vr=c(),Ie=t("h2"),go=t("a"),Zn=t("span"),b(qa.$$.fragment),yc=c(),Jn=t("span"),xc=s("FlavaImageCodebook"),Ur=c(),C=t("div"),b(La.$$.fragment),Tc=c(),Da=t("p"),$c=s(`The FLAVA\u2019s image codebook model inspired from DALL-E\u2019s original encoder. Outputs raw hidden states and can be used
to generate image tokens for an image based on DALL-E\u2019s vocab. Used to generate labels for MIM. Use
`),Kn=t("code"),wc=s("get_codebook_indices"),zc=s(" to get image tokens for an image."),Ic=c(),ja=t("p"),Mc=s("This model is a PyTorch "),Wa=t("a"),Pc=s("torch.nn.Module"),Cc=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ec=c(),Dt=t("div"),b(Sa.$$.fragment),Ac=c(),jt=t("div"),b(Ra.$$.fragment),Nc=c(),Wt=t("div"),b(Ba.$$.fragment),Hr=c(),Me=t("h2"),ho=t("a"),Xn=t("span"),b(Va.$$.fragment),Oc=c(),Qn=t("span"),qc=s("FlavaTextModel"),Gr=c(),Y=t("div"),b(Ua.$$.fragment),Lc=c(),Ha=t("p"),Dc=s(`The bare FLAVA Text Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Ga=t("a"),jc=s("torch.nn.Module"),Wc=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Sc=c(),R=t("div"),b(Ya.$$.fragment),Rc=c(),Pe=t("p"),Bc=s("The "),St=t("a"),Vc=s("FlavaTextModel"),Uc=s(" forward method, overrides the "),er=t("code"),Hc=s("__call__"),Gc=s(" special method."),Yc=c(),b(uo.$$.fragment),Zc=c(),b(_o.$$.fragment),Yr=c(),Ce=t("h2"),vo=t("a"),or=t("span"),b(Za.$$.fragment),Jc=c(),ar=t("span"),Kc=s("FlavaImageModel"),Zr=c(),Z=t("div"),b(Ja.$$.fragment),Xc=c(),Ka=t("p"),Qc=s(`The bare FLAVA Image Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Xa=t("a"),em=s("torch.nn.Module"),om=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),am=c(),B=t("div"),b(Qa.$$.fragment),tm=c(),Ee=t("p"),nm=s("The "),Rt=t("a"),rm=s("FlavaImageModel"),sm=s(" forward method, overrides the "),tr=t("code"),im=s("__call__"),lm=s(" special method."),dm=c(),b(bo.$$.fragment),cm=c(),b(ko.$$.fragment),Jr=c(),Ae=t("h2"),Fo=t("a"),nr=t("span"),b(et.$$.fragment),mm=c(),rr=t("span"),pm=s("FlavaMultimodalModel"),Kr=c(),J=t("div"),b(ot.$$.fragment),fm=c(),at=t("p"),gm=s(`The bare FLAVA Multimodal Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),tt=t("a"),hm=s("torch.nn.Module"),um=s(` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),_m=c(),V=t("div"),b(nt.$$.fragment),vm=c(),Ne=t("p"),bm=s("The "),Bt=t("a"),km=s("FlavaMultimodalModel"),Fm=s(" forward method, overrides the "),sr=t("code"),ym=s("__call__"),xm=s(" special method."),Tm=c(),b(yo.$$.fragment),$m=c(),b(xo.$$.fragment),this.h()},l(o){const g=If('[data-svelte="svelte-1phssyn"]',document.head);p=n(g,"META",{name:!0,content:!0}),g.forEach(a),$=m(o),_=n(o,"H1",{class:!0});var rt=r(_);h=n(rt,"A",{id:!0,class:!0,href:!0});var ir=r(h);v=n(ir,"SPAN",{});var lr=r(v);k(l.$$.fragment,lr),lr.forEach(a),ir.forEach(a),f=m(rt),I=n(rt,"SPAN",{});var dr=r(I);Ss=i(dr,"FLAVA"),dr.forEach(a),rt.forEach(a),vr=m(o),le=n(o,"H2",{class:!0});var st=r(le);qe=n(st,"A",{id:!0,class:!0,href:!0});var cr=r(qe);cn=n(cr,"SPAN",{});var mr=r(cn);k(Eo.$$.fragment,mr),mr.forEach(a),cr.forEach(a),Rs=m(st),mn=n(st,"SPAN",{});var pr=r(mn);Bs=i(pr,"Overview"),pr.forEach(a),st.forEach(a),br=m(o),Le=n(o,"P",{});var it=r(Le);Vs=i(it,"The FLAVA model was proposed in "),Ao=n(it,"A",{href:!0,rel:!0});var fr=r(Ao);Us=i(fr,"FLAVA: A Foundational Language And Vision Alignment Model"),fr.forEach(a),Hs=i(it," by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022."),it.forEach(a),kr=m(o),ct=n(o,"P",{});var gr=r(ct);Gs=i(gr,`The paper aims at creating a single unified foundation model which can work across vision, language
as well as vision-and-language multimodal tasks.`),gr.forEach(a),Fr=m(o),mt=n(o,"P",{});var hr=r(mt);Ys=i(hr,"The abstract from the paper is the following:"),hr.forEach(a),yr=m(o),pt=n(o,"P",{});var ur=r(pt);pn=n(ur,"EM",{});var _r=r(pn);Zs=i(_r,`State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety
of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal
(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising
direction would be to use a single holistic universal model, as a \u201Cfoundation\u201D, that targets all modalities
at once \u2014 a true vision and language foundation model should be good at vision tasks, language tasks, and
cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate
impressive performance on a wide range of 35 tasks spanning these target modalities.`),_r.forEach(a),ur.forEach(a),xr=m(o),X=n(o,"P",{});var Oe=r(X);Js=i(Oe,"This model was contributed by "),No=n(Oe,"A",{href:!0,rel:!0});var wm=r(No);Ks=i(wm,"aps"),wm.forEach(a),Xs=i(Oe,". The original code can be found "),Oo=n(Oe,"A",{href:!0,rel:!0});var zm=r(Oo);Qs=i(zm,"here"),zm.forEach(a),ei=i(Oe,"."),Oe.forEach(a),Tr=m(o),de=n(o,"H2",{class:!0});var Qr=r(de);De=n(Qr,"A",{id:!0,class:!0,href:!0});var Im=r(De);fn=n(Im,"SPAN",{});var Mm=r(fn);k(qo.$$.fragment,Mm),Mm.forEach(a),Im.forEach(a),oi=m(Qr),gn=n(Qr,"SPAN",{});var Pm=r(gn);ai=i(Pm,"FlavaConfig"),Pm.forEach(a),Qr.forEach(a),$r=m(o),P=n(o,"DIV",{class:!0});var U=r(P);k(Lo.$$.fragment,U),ti=m(U),Q=n(U,"P",{});var lt=r(Q);ft=n(lt,"A",{href:!0});var Cm=r(ft);ni=i(Cm,"FlavaConfig"),Cm.forEach(a),ri=i(lt," is the configuration class to store the configuration of a "),gt=n(lt,"A",{href:!0});var Em=r(gt);si=i(Em,"FlavaModel"),Em.forEach(a),ii=i(lt,`. It is used to
instantiate FLAVA model according to the specified arguments, defining the text model, image model, image codebook
and multimodal model configs. Instantiating a configuration with the defaults will yield a similar configuration to
that of the FLAVA `),Do=n(lt,"A",{href:!0,rel:!0});var Am=r(Do);li=i(Am,"facebook/flava-full"),Am.forEach(a),di=i(lt," architecture."),lt.forEach(a),ci=m(U),ce=n(U,"P",{});var Vt=r(ce);mi=i(Vt,"Configuration objects inherit from "),ht=n(Vt,"A",{href:!0});var Nm=r(ht);pi=i(Nm,"PretrainedConfig"),Nm.forEach(a),fi=i(Vt,` and can be used to control the model outputs. Read the
documentation from `),ut=n(Vt,"A",{href:!0});var Om=r(ut);gi=i(Om,"PretrainedConfig"),Om.forEach(a),hi=i(Vt," for more information."),Vt.forEach(a),ui=m(U),k(je.$$.fragment,U),_i=m(U),We=n(U,"DIV",{class:!0});var es=r(We);k(jo.$$.fragment,es),vi=m(es),Wo=n(es,"P",{});var os=r(Wo);bi=i(os,"Instantiate a "),_t=n(os,"A",{href:!0});var qm=r(_t);ki=i(qm,"FlavaConfig"),qm.forEach(a),Fi=i(os,` (or a derived class) from flava text model configuration, flava image model
configuration, flava multimodal model and flava codebook model configuration.`),os.forEach(a),es.forEach(a),yi=m(U),Se=n(U,"DIV",{class:!0});var as=r(Se);k(So.$$.fragment,as),xi=m(as),Ro=n(as,"P",{});var ts=r(Ro);Ti=i(ts,"Serializes this instance to a Python dictionary. Override the default "),vt=n(ts,"A",{href:!0});var Lm=r(vt);$i=i(Lm,"to_dict()"),Lm.forEach(a),wi=i(ts,"."),ts.forEach(a),as.forEach(a),U.forEach(a),wr=m(o),me=n(o,"H2",{class:!0});var ns=r(me);Re=n(ns,"A",{id:!0,class:!0,href:!0});var Dm=r(Re);hn=n(Dm,"SPAN",{});var jm=r(hn);k(Bo.$$.fragment,jm),jm.forEach(a),Dm.forEach(a),zi=m(ns),un=n(ns,"SPAN",{});var Wm=r(un);Ii=i(Wm,"FlavaTextConfig"),Wm.forEach(a),ns.forEach(a),zr=m(o),A=n(o,"DIV",{class:!0});var te=r(A);k(Vo.$$.fragment,te),Mi=m(te),Uo=n(te,"P",{});var rs=r(Uo);Pi=i(rs,"This is the configuration class to store the configuration of a "),bt=n(rs,"A",{href:!0});var Sm=r(bt);Ci=i(Sm,"FlavaTextModel"),Sm.forEach(a),Ei=i(rs,`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture.`),rs.forEach(a),Ai=m(te),Ho=n(te,"P",{});var ss=r(Ho);Ni=i(ss,`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
`),Go=n(ss,"A",{href:!0,rel:!0});var Rm=r(Go);Oi=i(Rm,"facebook/flava-full"),Rm.forEach(a),qi=i(ss," architecture."),ss.forEach(a),Li=m(te),pe=n(te,"P",{});var Ut=r(pe);Di=i(Ut,"Configuration objects inherit from "),kt=n(Ut,"A",{href:!0});var Bm=r(kt);ji=i(Bm,"PretrainedConfig"),Bm.forEach(a),Wi=i(Ut,` and can be used to control the model outputs. Read the
documentation from `),Ft=n(Ut,"A",{href:!0});var Vm=r(Ft);Si=i(Vm,"PretrainedConfig"),Vm.forEach(a),Ri=i(Ut," for more information."),Ut.forEach(a),Bi=m(te),k(Be.$$.fragment,te),te.forEach(a),Ir=m(o),fe=n(o,"H2",{class:!0});var is=r(fe);Ve=n(is,"A",{id:!0,class:!0,href:!0});var Um=r(Ve);_n=n(Um,"SPAN",{});var Hm=r(_n);k(Yo.$$.fragment,Hm),Hm.forEach(a),Um.forEach(a),Vi=m(is),vn=n(is,"SPAN",{});var Gm=r(vn);Ui=i(Gm,"FlavaImageConfig"),Gm.forEach(a),is.forEach(a),Mr=m(o),N=n(o,"DIV",{class:!0});var ne=r(N);k(Zo.$$.fragment,ne),Hi=m(ne),Jo=n(ne,"P",{});var ls=r(Jo);Gi=i(ls,"This is the configuration class to store the configuration of a "),yt=n(ls,"A",{href:!0});var Ym=r(yt);Yi=i(Ym,"FlavaImageModel"),Ym.forEach(a),Zi=i(ls,`. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture.`),ls.forEach(a),Ji=m(ne),Ko=n(ne,"P",{});var ds=r(Ko);Ki=i(ds,`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
`),Xo=n(ds,"A",{href:!0,rel:!0});var Zm=r(Xo);Xi=i(Zm,"facebook/flava-full"),Zm.forEach(a),Qi=i(ds," architecture."),ds.forEach(a),el=m(ne),ge=n(ne,"P",{});var Ht=r(ge);ol=i(Ht,"Configuration objects inherit from "),xt=n(Ht,"A",{href:!0});var Jm=r(xt);al=i(Jm,"PretrainedConfig"),Jm.forEach(a),tl=i(Ht,` and can be used to control the model outputs. Read the
documentation from `),Tt=n(Ht,"A",{href:!0});var Km=r(Tt);nl=i(Km,"PretrainedConfig"),Km.forEach(a),rl=i(Ht," for more information."),Ht.forEach(a),sl=m(ne),k(Ue.$$.fragment,ne),ne.forEach(a),Pr=m(o),he=n(o,"H2",{class:!0});var cs=r(he);He=n(cs,"A",{id:!0,class:!0,href:!0});var Xm=r(He);bn=n(Xm,"SPAN",{});var Qm=r(bn);k(Qo.$$.fragment,Qm),Qm.forEach(a),Xm.forEach(a),il=m(cs),kn=n(cs,"SPAN",{});var ep=r(kn);ll=i(ep,"FlavaMultimodalConfig"),ep.forEach(a),cs.forEach(a),Cr=m(o),O=n(o,"DIV",{class:!0});var re=r(O);k(ea.$$.fragment,re),dl=m(re),oa=n(re,"P",{});var ms=r(oa);cl=i(ms,"This is the configuration class to store the configuration of a "),$t=n(ms,"A",{href:!0});var op=r($t);ml=i(op,"FlavaMultimodalModel"),op.forEach(a),pl=i(ms,`. It is used to instantiate
an FLAVA model according to the specified arguments, defining the model architecture.`),ms.forEach(a),fl=m(re),aa=n(re,"P",{});var ps=r(aa);gl=i(ps,`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
`),ta=n(ps,"A",{href:!0,rel:!0});var ap=r(ta);hl=i(ap,"facebook/flava-full"),ap.forEach(a),ul=i(ps," architecture."),ps.forEach(a),_l=m(re),ue=n(re,"P",{});var Gt=r(ue);vl=i(Gt,"Configuration objects inherit from "),wt=n(Gt,"A",{href:!0});var tp=r(wt);bl=i(tp,"PretrainedConfig"),tp.forEach(a),kl=i(Gt,` and can be used to control the model outputs. Read the
documentation from `),zt=n(Gt,"A",{href:!0});var np=r(zt);Fl=i(np,"PretrainedConfig"),np.forEach(a),yl=i(Gt," for more information."),Gt.forEach(a),xl=m(re),k(Ge.$$.fragment,re),re.forEach(a),Er=m(o),_e=n(o,"H2",{class:!0});var fs=r(_e);Ye=n(fs,"A",{id:!0,class:!0,href:!0});var rp=r(Ye);Fn=n(rp,"SPAN",{});var sp=r(Fn);k(na.$$.fragment,sp),sp.forEach(a),rp.forEach(a),Tl=m(fs),yn=n(fs,"SPAN",{});var ip=r(yn);$l=i(ip,"FlavaImageCodebookConfig"),ip.forEach(a),fs.forEach(a),Ar=m(o),ra=n(o,"DIV",{class:!0});var lp=r(ra);k(sa.$$.fragment,lp),lp.forEach(a),Nr=m(o),ve=n(o,"H2",{class:!0});var gs=r(ve);Ze=n(gs,"A",{id:!0,class:!0,href:!0});var dp=r(Ze);xn=n(dp,"SPAN",{});var cp=r(xn);k(ia.$$.fragment,cp),cp.forEach(a),dp.forEach(a),wl=m(gs),Tn=n(gs,"SPAN",{});var mp=r(Tn);zl=i(mp,"FlavaProcessor"),mp.forEach(a),gs.forEach(a),Or=m(o),q=n(o,"DIV",{class:!0});var se=r(q);k(la.$$.fragment,se),Il=m(se),$n=n(se,"P",{});var pp=r($n);Ml=i(pp,"Constructs a FLAVA processor which wraps a FLAVA feature extractor and a FLAVA tokenizer into a single processor."),pp.forEach(a),Pl=m(se),D=n(se,"P",{});var K=r(D);It=n(K,"A",{href:!0});var fp=r(It);Cl=i(fp,"FlavaProcessor"),fp.forEach(a),El=i(K," offers all the functionalities of "),Mt=n(K,"A",{href:!0});var gp=r(Mt);Al=i(gp,"FlavaFeatureExtractor"),gp.forEach(a),Nl=i(K," and "),Pt=n(K,"A",{href:!0});var hp=r(Pt);Ol=i(hp,"BertTokenizerFast"),hp.forEach(a),ql=i(K,`. See the
`),wn=n(K,"CODE",{});var up=r(wn);Ll=i(up,"__call__()"),up.forEach(a),Dl=i(K," and "),Ct=n(K,"A",{href:!0});var _p=r(Ct);jl=i(_p,"decode()"),_p.forEach(a),Wl=i(K," for more information."),K.forEach(a),Sl=m(se),Je=n(se,"DIV",{class:!0});var hs=r(Je);k(da.$$.fragment,hs),Rl=m(hs),ca=n(hs,"P",{});var us=r(ca);Bl=i(us,"This method forwards all its arguments to BertTokenizerFast\u2019s "),Et=n(us,"A",{href:!0});var vp=r(Et);Vl=i(vp,"batch_decode()"),vp.forEach(a),Ul=i(us,`. Please
refer to the docstring of this method for more information.`),us.forEach(a),hs.forEach(a),Hl=m(se),Ke=n(se,"DIV",{class:!0});var _s=r(Ke);k(ma.$$.fragment,_s),Gl=m(_s),pa=n(_s,"P",{});var vs=r(pa);Yl=i(vs,"This method forwards all its arguments to BertTokenizerFast\u2019s "),At=n(vs,"A",{href:!0});var bp=r(At);Zl=i(bp,"decode()"),bp.forEach(a),Jl=i(vs,`. Please refer to
the docstring of this method for more information.`),vs.forEach(a),_s.forEach(a),se.forEach(a),qr=m(o),be=n(o,"H2",{class:!0});var bs=r(be);Xe=n(bs,"A",{id:!0,class:!0,href:!0});var kp=r(Xe);zn=n(kp,"SPAN",{});var Fp=r(zn);k(fa.$$.fragment,Fp),Fp.forEach(a),kp.forEach(a),Kl=m(bs),In=n(bs,"SPAN",{});var yp=r(In);Xl=i(yp,"FlavaFeatureExtractor"),yp.forEach(a),bs.forEach(a),Lr=m(o),M=n(o,"DIV",{class:!0});var j=r(M);k(ga.$$.fragment,j),Ql=m(j),Mn=n(j,"P",{});var xp=r(Mn);ed=i(xp,"Constructs a Flava image processor."),xp.forEach(a),od=m(j),Qe=n(j,"DIV",{class:!0});var ks=r(Qe);k(ha.$$.fragment,ks),ad=m(ks),ke=n(ks,"P",{});var Yt=r(ke);td=i(Yt,"Center crop an image to "),Pn=n(Yt,"CODE",{});var Tp=r(Pn);nd=i(Tp,'(size["height"], size["width"])'),Tp.forEach(a),rd=i(Yt,". If the input size is smaller than "),Cn=n(Yt,"CODE",{});var $p=r(Cn);sd=i($p,"crop_size"),$p.forEach(a),id=i(Yt,` along
any edge, the image is padded with 0\u2019s and then center cropped.`),Yt.forEach(a),ks.forEach(a),ld=m(j),eo=n(j,"DIV",{class:!0});var Fs=r(eo);k(ua.$$.fragment,Fs),dd=m(Fs),En=n(Fs,"P",{});var wp=r(En);cd=i(wp,"Normalize an image. image = (image - image_mean) / image_std."),wp.forEach(a),Fs.forEach(a),md=m(j),oo=n(j,"DIV",{class:!0});var ys=r(oo);k(_a.$$.fragment,ys),pd=m(ys),An=n(ys,"P",{});var zp=r(An);fd=i(zp,"Preprocess an image or batch of images."),zp.forEach(a),ys.forEach(a),gd=m(j),ao=n(j,"DIV",{class:!0});var xs=r(ao);k(va.$$.fragment,xs),hd=m(xs),Nn=n(xs,"P",{});var Ip=r(Nn);ud=i(Ip,"Rescale an image by a scale factor. image = image * scale."),Ip.forEach(a),xs.forEach(a),_d=m(j),to=n(j,"DIV",{class:!0});var Ts=r(to);k(ba.$$.fragment,Ts),vd=m(Ts),ka=n(Ts,"P",{});var $s=r(ka);bd=i($s,"Resize an image to "),On=n($s,"CODE",{});var Mp=r(On);kd=i(Mp,'(size["height"], size["width"])'),Mp.forEach(a),Fd=i($s,"."),$s.forEach(a),Ts.forEach(a),j.forEach(a),Dr=m(o),Fe=n(o,"H2",{class:!0});var ws=r(Fe);no=n(ws,"A",{id:!0,class:!0,href:!0});var Pp=r(no);qn=n(Pp,"SPAN",{});var Cp=r(qn);k(Fa.$$.fragment,Cp),Cp.forEach(a),Pp.forEach(a),yd=m(ws),Ln=n(ws,"SPAN",{});var Ep=r(Ln);xd=i(Ep,"FlavaImageProcessor"),Ep.forEach(a),ws.forEach(a),jr=m(o),G=n(o,"DIV",{class:!0});var Zt=r(G);k(ya.$$.fragment,Zt),Td=m(Zt),Dn=n(Zt,"P",{});var Ap=r(Dn);$d=i(Ap,"Constructs a Flava image processor."),Ap.forEach(a),wd=m(Zt),ro=n(Zt,"DIV",{class:!0});var zs=r(ro);k(xa.$$.fragment,zs),zd=m(zs),jn=n(zs,"P",{});var Np=r(jn);Id=i(Np,"Preprocess an image or batch of images."),Np.forEach(a),zs.forEach(a),Zt.forEach(a),Wr=m(o),ye=n(o,"H2",{class:!0});var Is=r(ye);so=n(Is,"A",{id:!0,class:!0,href:!0});var Op=r(so);Wn=n(Op,"SPAN",{});var qp=r(Wn);k(Ta.$$.fragment,qp),qp.forEach(a),Op.forEach(a),Md=m(Is),Sn=n(Is,"SPAN",{});var Lp=r(Sn);Pd=i(Lp,"FlavaForPreTraining"),Lp.forEach(a),Is.forEach(a),Sr=m(o),W=n(o,"DIV",{class:!0});var To=r(W);k($a.$$.fragment,To),Cd=m(To),Rn=n(To,"P",{});var Dp=r(Rn);Ed=i(Dp,"The FLAVA model for pretraining which outputs losses, embeddings, logits and transformer outputs."),Dp.forEach(a),Ad=m(To),wa=n(To,"P",{});var Ms=r(wa);Nd=i(Ms,"This model is a PyTorch "),za=n(Ms,"A",{href:!0,rel:!0});var jp=r(za);Od=i(jp,"torch.nn.Module"),jp.forEach(a),qd=i(Ms,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ms.forEach(a),Ld=m(To),ee=n(To,"DIV",{class:!0});var Jt=r(ee);k(Ia.$$.fragment,Jt),Dd=m(Jt),xe=n(Jt,"P",{});var Kt=r(xe);jd=i(Kt,"The "),Nt=n(Kt,"A",{href:!0});var Wp=r(Nt);Wd=i(Wp,"FlavaForPreTraining"),Wp.forEach(a),Sd=i(Kt," forward method, overrides the "),Bn=n(Kt,"CODE",{});var Sp=r(Bn);Rd=i(Sp,"__call__"),Sp.forEach(a),Bd=i(Kt," special method."),Kt.forEach(a),Vd=m(Jt),k(io.$$.fragment,Jt),Jt.forEach(a),To.forEach(a),Rr=m(o),Te=n(o,"H2",{class:!0});var Ps=r(Te);lo=n(Ps,"A",{id:!0,class:!0,href:!0});var Rp=r(lo);Vn=n(Rp,"SPAN",{});var Bp=r(Vn);k(Ma.$$.fragment,Bp),Bp.forEach(a),Rp.forEach(a),Ud=m(Ps),Un=n(Ps,"SPAN",{});var Vp=r(Un);Hd=i(Vp,"FlavaModel"),Vp.forEach(a),Ps.forEach(a),Br=m(o),L=n(o,"DIV",{class:!0});var ie=r(L);k(Pa.$$.fragment,ie),Gd=m(ie),Ca=n(ie,"P",{});var Cs=r(Ca);Yd=i(Cs,`The bare FLAVA Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Ea=n(Cs,"A",{href:!0,rel:!0});var Up=r(Ea);Zd=i(Up,"torch.nn.Module"),Up.forEach(a),Jd=i(Cs,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Cs.forEach(a),Kd=m(ie),S=n(ie,"DIV",{class:!0});var $o=r(S);k(Aa.$$.fragment,$o),Xd=m($o),$e=n($o,"P",{});var Xt=r($e);Qd=i(Xt,"The "),Ot=n(Xt,"A",{href:!0});var Hp=r(Ot);ec=i(Hp,"FlavaModel"),Hp.forEach(a),oc=i(Xt," forward method, overrides the "),Hn=n(Xt,"CODE",{});var Gp=r(Hn);ac=i(Gp,"__call__"),Gp.forEach(a),tc=i(Xt," special method."),Xt.forEach(a),nc=m($o),k(co.$$.fragment,$o),rc=m($o),k(mo.$$.fragment,$o),$o.forEach(a),sc=m(ie),oe=n(ie,"DIV",{class:!0});var Qt=r(oe);k(Na.$$.fragment,Qt),ic=m(Qt),we=n(Qt,"P",{});var en=r(we);lc=i(en,"The "),qt=n(en,"A",{href:!0});var Yp=r(qt);dc=i(Yp,"FlavaModel"),Yp.forEach(a),cc=i(en," forward method, overrides the "),Gn=n(en,"CODE",{});var Zp=r(Gn);mc=i(Zp,"__call__"),Zp.forEach(a),pc=i(en," special method."),en.forEach(a),fc=m(Qt),k(po.$$.fragment,Qt),Qt.forEach(a),gc=m(ie),ae=n(ie,"DIV",{class:!0});var on=r(ae);k(Oa.$$.fragment,on),hc=m(on),ze=n(on,"P",{});var an=r(ze);uc=i(an,"The "),Lt=n(an,"A",{href:!0});var Jp=r(Lt);_c=i(Jp,"FlavaModel"),Jp.forEach(a),vc=i(an," forward method, overrides the "),Yn=n(an,"CODE",{});var Kp=r(Yn);bc=i(Kp,"__call__"),Kp.forEach(a),kc=i(an," special method."),an.forEach(a),Fc=m(on),k(fo.$$.fragment,on),on.forEach(a),ie.forEach(a),Vr=m(o),Ie=n(o,"H2",{class:!0});var Es=r(Ie);go=n(Es,"A",{id:!0,class:!0,href:!0});var Xp=r(go);Zn=n(Xp,"SPAN",{});var Qp=r(Zn);k(qa.$$.fragment,Qp),Qp.forEach(a),Xp.forEach(a),yc=m(Es),Jn=n(Es,"SPAN",{});var ef=r(Jn);xc=i(ef,"FlavaImageCodebook"),ef.forEach(a),Es.forEach(a),Ur=m(o),C=n(o,"DIV",{class:!0});var H=r(C);k(La.$$.fragment,H),Tc=m(H),Da=n(H,"P",{});var As=r(Da);$c=i(As,`The FLAVA\u2019s image codebook model inspired from DALL-E\u2019s original encoder. Outputs raw hidden states and can be used
to generate image tokens for an image based on DALL-E\u2019s vocab. Used to generate labels for MIM. Use
`),Kn=n(As,"CODE",{});var of=r(Kn);wc=i(of,"get_codebook_indices"),of.forEach(a),zc=i(As," to get image tokens for an image."),As.forEach(a),Ic=m(H),ja=n(H,"P",{});var Ns=r(ja);Mc=i(Ns,"This model is a PyTorch "),Wa=n(Ns,"A",{href:!0,rel:!0});var af=r(Wa);Pc=i(af,"torch.nn.Module"),af.forEach(a),Cc=i(Ns,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ns.forEach(a),Ec=m(H),Dt=n(H,"DIV",{class:!0});var tf=r(Dt);k(Sa.$$.fragment,tf),tf.forEach(a),Ac=m(H),jt=n(H,"DIV",{class:!0});var nf=r(jt);k(Ra.$$.fragment,nf),nf.forEach(a),Nc=m(H),Wt=n(H,"DIV",{class:!0});var rf=r(Wt);k(Ba.$$.fragment,rf),rf.forEach(a),H.forEach(a),Hr=m(o),Me=n(o,"H2",{class:!0});var Os=r(Me);ho=n(Os,"A",{id:!0,class:!0,href:!0});var sf=r(ho);Xn=n(sf,"SPAN",{});var lf=r(Xn);k(Va.$$.fragment,lf),lf.forEach(a),sf.forEach(a),Oc=m(Os),Qn=n(Os,"SPAN",{});var df=r(Qn);qc=i(df,"FlavaTextModel"),df.forEach(a),Os.forEach(a),Gr=m(o),Y=n(o,"DIV",{class:!0});var tn=r(Y);k(Ua.$$.fragment,tn),Lc=m(tn),Ha=n(tn,"P",{});var qs=r(Ha);Dc=i(qs,`The bare FLAVA Text Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Ga=n(qs,"A",{href:!0,rel:!0});var cf=r(Ga);jc=i(cf,"torch.nn.Module"),cf.forEach(a),Wc=i(qs,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),qs.forEach(a),Sc=m(tn),R=n(tn,"DIV",{class:!0});var wo=r(R);k(Ya.$$.fragment,wo),Rc=m(wo),Pe=n(wo,"P",{});var nn=r(Pe);Bc=i(nn,"The "),St=n(nn,"A",{href:!0});var mf=r(St);Vc=i(mf,"FlavaTextModel"),mf.forEach(a),Uc=i(nn," forward method, overrides the "),er=n(nn,"CODE",{});var pf=r(er);Hc=i(pf,"__call__"),pf.forEach(a),Gc=i(nn," special method."),nn.forEach(a),Yc=m(wo),k(uo.$$.fragment,wo),Zc=m(wo),k(_o.$$.fragment,wo),wo.forEach(a),tn.forEach(a),Yr=m(o),Ce=n(o,"H2",{class:!0});var Ls=r(Ce);vo=n(Ls,"A",{id:!0,class:!0,href:!0});var ff=r(vo);or=n(ff,"SPAN",{});var gf=r(or);k(Za.$$.fragment,gf),gf.forEach(a),ff.forEach(a),Jc=m(Ls),ar=n(Ls,"SPAN",{});var hf=r(ar);Kc=i(hf,"FlavaImageModel"),hf.forEach(a),Ls.forEach(a),Zr=m(o),Z=n(o,"DIV",{class:!0});var rn=r(Z);k(Ja.$$.fragment,rn),Xc=m(rn),Ka=n(rn,"P",{});var Ds=r(Ka);Qc=i(Ds,`The bare FLAVA Image Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),Xa=n(Ds,"A",{href:!0,rel:!0});var uf=r(Xa);em=i(uf,"torch.nn.Module"),uf.forEach(a),om=i(Ds,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ds.forEach(a),am=m(rn),B=n(rn,"DIV",{class:!0});var zo=r(B);k(Qa.$$.fragment,zo),tm=m(zo),Ee=n(zo,"P",{});var sn=r(Ee);nm=i(sn,"The "),Rt=n(sn,"A",{href:!0});var _f=r(Rt);rm=i(_f,"FlavaImageModel"),_f.forEach(a),sm=i(sn," forward method, overrides the "),tr=n(sn,"CODE",{});var vf=r(tr);im=i(vf,"__call__"),vf.forEach(a),lm=i(sn," special method."),sn.forEach(a),dm=m(zo),k(bo.$$.fragment,zo),cm=m(zo),k(ko.$$.fragment,zo),zo.forEach(a),rn.forEach(a),Jr=m(o),Ae=n(o,"H2",{class:!0});var js=r(Ae);Fo=n(js,"A",{id:!0,class:!0,href:!0});var bf=r(Fo);nr=n(bf,"SPAN",{});var kf=r(nr);k(et.$$.fragment,kf),kf.forEach(a),bf.forEach(a),mm=m(js),rr=n(js,"SPAN",{});var Ff=r(rr);pm=i(Ff,"FlavaMultimodalModel"),Ff.forEach(a),js.forEach(a),Kr=m(o),J=n(o,"DIV",{class:!0});var ln=r(J);k(ot.$$.fragment,ln),fm=m(ln),at=n(ln,"P",{});var Ws=r(at);gm=i(Ws,`The bare FLAVA Multimodal Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch `),tt=n(Ws,"A",{href:!0,rel:!0});var yf=r(tt);hm=i(yf,"torch.nn.Module"),yf.forEach(a),um=i(Ws,` subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ws.forEach(a),_m=m(ln),V=n(ln,"DIV",{class:!0});var Io=r(V);k(nt.$$.fragment,Io),vm=m(Io),Ne=n(Io,"P",{});var dn=r(Ne);bm=i(dn,"The "),Bt=n(dn,"A",{href:!0});var xf=r(Bt);km=i(xf,"FlavaMultimodalModel"),xf.forEach(a),Fm=i(dn," forward method, overrides the "),sr=n(dn,"CODE",{});var Tf=r(sr);ym=i(Tf,"__call__"),Tf.forEach(a),xm=i(dn," special method."),dn.forEach(a),Tm=m(Io),k(yo.$$.fragment,Io),$m=m(Io),k(xo.$$.fragment,Io),Io.forEach(a),ln.forEach(a),this.h()},h(){d(p,"name","hf:doc:metadata"),d(p,"content",JSON.stringify(Hf)),d(h,"id","flava"),d(h,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(h,"href","#flava"),d(_,"class","relative group"),d(qe,"id","overview"),d(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(qe,"href","#overview"),d(le,"class","relative group"),d(Ao,"href","https://arxiv.org/abs/2112.04482"),d(Ao,"rel","nofollow"),d(No,"href","https://huggingface.co/aps"),d(No,"rel","nofollow"),d(Oo,"href","https://github.com/facebookresearch/multimodal/tree/main/examples/flava"),d(Oo,"rel","nofollow"),d(De,"id","transformers.FlavaConfig"),d(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(De,"href","#transformers.FlavaConfig"),d(de,"class","relative group"),d(ft,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig"),d(gt,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaModel"),d(Do,"href","https://huggingface.co/facebook/flava-full"),d(Do,"rel","nofollow"),d(ht,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(ut,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(_t,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig"),d(We,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(vt,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.to_dict"),d(Se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Re,"id","transformers.FlavaTextConfig"),d(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Re,"href","#transformers.FlavaTextConfig"),d(me,"class","relative group"),d(bt,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"),d(Go,"href","https://huggingface.co/facebook/flava-full"),d(Go,"rel","nofollow"),d(kt,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(Ft,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ve,"id","transformers.FlavaImageConfig"),d(Ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ve,"href","#transformers.FlavaImageConfig"),d(fe,"class","relative group"),d(yt,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"),d(Xo,"href","https://huggingface.co/facebook/flava-full"),d(Xo,"rel","nofollow"),d(xt,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(Tt,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(He,"id","transformers.FlavaMultimodalConfig"),d(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(He,"href","#transformers.FlavaMultimodalConfig"),d(he,"class","relative group"),d($t,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalModel"),d(ta,"href","https://huggingface.co/facebook/flava-full"),d(ta,"rel","nofollow"),d(wt,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(zt,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ye,"id","transformers.FlavaImageCodebookConfig"),d(Ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ye,"href","#transformers.FlavaImageCodebookConfig"),d(_e,"class","relative group"),d(ra,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ze,"id","transformers.FlavaProcessor"),d(Ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ze,"href","#transformers.FlavaProcessor"),d(ve,"class","relative group"),d(It,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaProcessor"),d(Mt,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageProcessor"),d(Pt,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),d(Ct,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaProcessor.decode"),d(Et,"href","/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode"),d(Je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(At,"href","/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode"),d(Ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Xe,"id","transformers.FlavaImageProcessor"),d(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Xe,"href","#transformers.FlavaImageProcessor"),d(be,"class","relative group"),d(Qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(eo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(oo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ao,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(to,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(no,"id","transformers.FlavaImageProcessor"),d(no,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(no,"href","#transformers.FlavaImageProcessor"),d(Fe,"class","relative group"),d(ro,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(so,"id","transformers.FlavaForPreTraining"),d(so,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(so,"href","#transformers.FlavaForPreTraining"),d(ye,"class","relative group"),d(za,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(za,"rel","nofollow"),d(Nt,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaForPreTraining"),d(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(lo,"id","transformers.FlavaModel"),d(lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(lo,"href","#transformers.FlavaModel"),d(Te,"class","relative group"),d(Ea,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(Ea,"rel","nofollow"),d(Ot,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaModel"),d(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(qt,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaModel"),d(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Lt,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaModel"),d(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(go,"id","transformers.FlavaImageCodebook"),d(go,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(go,"href","#transformers.FlavaImageCodebook"),d(Ie,"class","relative group"),d(Wa,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(Wa,"rel","nofollow"),d(Dt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(jt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Wt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ho,"id","transformers.FlavaTextModel"),d(ho,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ho,"href","#transformers.FlavaTextModel"),d(Me,"class","relative group"),d(Ga,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(Ga,"rel","nofollow"),d(St,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"),d(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(vo,"id","transformers.FlavaImageModel"),d(vo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(vo,"href","#transformers.FlavaImageModel"),d(Ce,"class","relative group"),d(Xa,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(Xa,"rel","nofollow"),d(Rt,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"),d(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Fo,"id","transformers.FlavaMultimodalModel"),d(Fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Fo,"href","#transformers.FlavaMultimodalModel"),d(Ae,"class","relative group"),d(tt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(tt,"rel","nofollow"),d(Bt,"href","/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalModel"),d(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,g){e(document.head,p),u(o,$,g),u(o,_,g),e(_,h),e(h,v),F(l,v,null),e(_,f),e(_,I),e(I,Ss),u(o,vr,g),u(o,le,g),e(le,qe),e(qe,cn),F(Eo,cn,null),e(le,Rs),e(le,mn),e(mn,Bs),u(o,br,g),u(o,Le,g),e(Le,Vs),e(Le,Ao),e(Ao,Us),e(Le,Hs),u(o,kr,g),u(o,ct,g),e(ct,Gs),u(o,Fr,g),u(o,mt,g),e(mt,Ys),u(o,yr,g),u(o,pt,g),e(pt,pn),e(pn,Zs),u(o,xr,g),u(o,X,g),e(X,Js),e(X,No),e(No,Ks),e(X,Xs),e(X,Oo),e(Oo,Qs),e(X,ei),u(o,Tr,g),u(o,de,g),e(de,De),e(De,fn),F(qo,fn,null),e(de,oi),e(de,gn),e(gn,ai),u(o,$r,g),u(o,P,g),F(Lo,P,null),e(P,ti),e(P,Q),e(Q,ft),e(ft,ni),e(Q,ri),e(Q,gt),e(gt,si),e(Q,ii),e(Q,Do),e(Do,li),e(Q,di),e(P,ci),e(P,ce),e(ce,mi),e(ce,ht),e(ht,pi),e(ce,fi),e(ce,ut),e(ut,gi),e(ce,hi),e(P,ui),F(je,P,null),e(P,_i),e(P,We),F(jo,We,null),e(We,vi),e(We,Wo),e(Wo,bi),e(Wo,_t),e(_t,ki),e(Wo,Fi),e(P,yi),e(P,Se),F(So,Se,null),e(Se,xi),e(Se,Ro),e(Ro,Ti),e(Ro,vt),e(vt,$i),e(Ro,wi),u(o,wr,g),u(o,me,g),e(me,Re),e(Re,hn),F(Bo,hn,null),e(me,zi),e(me,un),e(un,Ii),u(o,zr,g),u(o,A,g),F(Vo,A,null),e(A,Mi),e(A,Uo),e(Uo,Pi),e(Uo,bt),e(bt,Ci),e(Uo,Ei),e(A,Ai),e(A,Ho),e(Ho,Ni),e(Ho,Go),e(Go,Oi),e(Ho,qi),e(A,Li),e(A,pe),e(pe,Di),e(pe,kt),e(kt,ji),e(pe,Wi),e(pe,Ft),e(Ft,Si),e(pe,Ri),e(A,Bi),F(Be,A,null),u(o,Ir,g),u(o,fe,g),e(fe,Ve),e(Ve,_n),F(Yo,_n,null),e(fe,Vi),e(fe,vn),e(vn,Ui),u(o,Mr,g),u(o,N,g),F(Zo,N,null),e(N,Hi),e(N,Jo),e(Jo,Gi),e(Jo,yt),e(yt,Yi),e(Jo,Zi),e(N,Ji),e(N,Ko),e(Ko,Ki),e(Ko,Xo),e(Xo,Xi),e(Ko,Qi),e(N,el),e(N,ge),e(ge,ol),e(ge,xt),e(xt,al),e(ge,tl),e(ge,Tt),e(Tt,nl),e(ge,rl),e(N,sl),F(Ue,N,null),u(o,Pr,g),u(o,he,g),e(he,He),e(He,bn),F(Qo,bn,null),e(he,il),e(he,kn),e(kn,ll),u(o,Cr,g),u(o,O,g),F(ea,O,null),e(O,dl),e(O,oa),e(oa,cl),e(oa,$t),e($t,ml),e(oa,pl),e(O,fl),e(O,aa),e(aa,gl),e(aa,ta),e(ta,hl),e(aa,ul),e(O,_l),e(O,ue),e(ue,vl),e(ue,wt),e(wt,bl),e(ue,kl),e(ue,zt),e(zt,Fl),e(ue,yl),e(O,xl),F(Ge,O,null),u(o,Er,g),u(o,_e,g),e(_e,Ye),e(Ye,Fn),F(na,Fn,null),e(_e,Tl),e(_e,yn),e(yn,$l),u(o,Ar,g),u(o,ra,g),F(sa,ra,null),u(o,Nr,g),u(o,ve,g),e(ve,Ze),e(Ze,xn),F(ia,xn,null),e(ve,wl),e(ve,Tn),e(Tn,zl),u(o,Or,g),u(o,q,g),F(la,q,null),e(q,Il),e(q,$n),e($n,Ml),e(q,Pl),e(q,D),e(D,It),e(It,Cl),e(D,El),e(D,Mt),e(Mt,Al),e(D,Nl),e(D,Pt),e(Pt,Ol),e(D,ql),e(D,wn),e(wn,Ll),e(D,Dl),e(D,Ct),e(Ct,jl),e(D,Wl),e(q,Sl),e(q,Je),F(da,Je,null),e(Je,Rl),e(Je,ca),e(ca,Bl),e(ca,Et),e(Et,Vl),e(ca,Ul),e(q,Hl),e(q,Ke),F(ma,Ke,null),e(Ke,Gl),e(Ke,pa),e(pa,Yl),e(pa,At),e(At,Zl),e(pa,Jl),u(o,qr,g),u(o,be,g),e(be,Xe),e(Xe,zn),F(fa,zn,null),e(be,Kl),e(be,In),e(In,Xl),u(o,Lr,g),u(o,M,g),F(ga,M,null),e(M,Ql),e(M,Mn),e(Mn,ed),e(M,od),e(M,Qe),F(ha,Qe,null),e(Qe,ad),e(Qe,ke),e(ke,td),e(ke,Pn),e(Pn,nd),e(ke,rd),e(ke,Cn),e(Cn,sd),e(ke,id),e(M,ld),e(M,eo),F(ua,eo,null),e(eo,dd),e(eo,En),e(En,cd),e(M,md),e(M,oo),F(_a,oo,null),e(oo,pd),e(oo,An),e(An,fd),e(M,gd),e(M,ao),F(va,ao,null),e(ao,hd),e(ao,Nn),e(Nn,ud),e(M,_d),e(M,to),F(ba,to,null),e(to,vd),e(to,ka),e(ka,bd),e(ka,On),e(On,kd),e(ka,Fd),u(o,Dr,g),u(o,Fe,g),e(Fe,no),e(no,qn),F(Fa,qn,null),e(Fe,yd),e(Fe,Ln),e(Ln,xd),u(o,jr,g),u(o,G,g),F(ya,G,null),e(G,Td),e(G,Dn),e(Dn,$d),e(G,wd),e(G,ro),F(xa,ro,null),e(ro,zd),e(ro,jn),e(jn,Id),u(o,Wr,g),u(o,ye,g),e(ye,so),e(so,Wn),F(Ta,Wn,null),e(ye,Md),e(ye,Sn),e(Sn,Pd),u(o,Sr,g),u(o,W,g),F($a,W,null),e(W,Cd),e(W,Rn),e(Rn,Ed),e(W,Ad),e(W,wa),e(wa,Nd),e(wa,za),e(za,Od),e(wa,qd),e(W,Ld),e(W,ee),F(Ia,ee,null),e(ee,Dd),e(ee,xe),e(xe,jd),e(xe,Nt),e(Nt,Wd),e(xe,Sd),e(xe,Bn),e(Bn,Rd),e(xe,Bd),e(ee,Vd),F(io,ee,null),u(o,Rr,g),u(o,Te,g),e(Te,lo),e(lo,Vn),F(Ma,Vn,null),e(Te,Ud),e(Te,Un),e(Un,Hd),u(o,Br,g),u(o,L,g),F(Pa,L,null),e(L,Gd),e(L,Ca),e(Ca,Yd),e(Ca,Ea),e(Ea,Zd),e(Ca,Jd),e(L,Kd),e(L,S),F(Aa,S,null),e(S,Xd),e(S,$e),e($e,Qd),e($e,Ot),e(Ot,ec),e($e,oc),e($e,Hn),e(Hn,ac),e($e,tc),e(S,nc),F(co,S,null),e(S,rc),F(mo,S,null),e(L,sc),e(L,oe),F(Na,oe,null),e(oe,ic),e(oe,we),e(we,lc),e(we,qt),e(qt,dc),e(we,cc),e(we,Gn),e(Gn,mc),e(we,pc),e(oe,fc),F(po,oe,null),e(L,gc),e(L,ae),F(Oa,ae,null),e(ae,hc),e(ae,ze),e(ze,uc),e(ze,Lt),e(Lt,_c),e(ze,vc),e(ze,Yn),e(Yn,bc),e(ze,kc),e(ae,Fc),F(fo,ae,null),u(o,Vr,g),u(o,Ie,g),e(Ie,go),e(go,Zn),F(qa,Zn,null),e(Ie,yc),e(Ie,Jn),e(Jn,xc),u(o,Ur,g),u(o,C,g),F(La,C,null),e(C,Tc),e(C,Da),e(Da,$c),e(Da,Kn),e(Kn,wc),e(Da,zc),e(C,Ic),e(C,ja),e(ja,Mc),e(ja,Wa),e(Wa,Pc),e(ja,Cc),e(C,Ec),e(C,Dt),F(Sa,Dt,null),e(C,Ac),e(C,jt),F(Ra,jt,null),e(C,Nc),e(C,Wt),F(Ba,Wt,null),u(o,Hr,g),u(o,Me,g),e(Me,ho),e(ho,Xn),F(Va,Xn,null),e(Me,Oc),e(Me,Qn),e(Qn,qc),u(o,Gr,g),u(o,Y,g),F(Ua,Y,null),e(Y,Lc),e(Y,Ha),e(Ha,Dc),e(Ha,Ga),e(Ga,jc),e(Ha,Wc),e(Y,Sc),e(Y,R),F(Ya,R,null),e(R,Rc),e(R,Pe),e(Pe,Bc),e(Pe,St),e(St,Vc),e(Pe,Uc),e(Pe,er),e(er,Hc),e(Pe,Gc),e(R,Yc),F(uo,R,null),e(R,Zc),F(_o,R,null),u(o,Yr,g),u(o,Ce,g),e(Ce,vo),e(vo,or),F(Za,or,null),e(Ce,Jc),e(Ce,ar),e(ar,Kc),u(o,Zr,g),u(o,Z,g),F(Ja,Z,null),e(Z,Xc),e(Z,Ka),e(Ka,Qc),e(Ka,Xa),e(Xa,em),e(Ka,om),e(Z,am),e(Z,B),F(Qa,B,null),e(B,tm),e(B,Ee),e(Ee,nm),e(Ee,Rt),e(Rt,rm),e(Ee,sm),e(Ee,tr),e(tr,im),e(Ee,lm),e(B,dm),F(bo,B,null),e(B,cm),F(ko,B,null),u(o,Jr,g),u(o,Ae,g),e(Ae,Fo),e(Fo,nr),F(et,nr,null),e(Ae,mm),e(Ae,rr),e(rr,pm),u(o,Kr,g),u(o,J,g),F(ot,J,null),e(J,fm),e(J,at),e(at,gm),e(at,tt),e(tt,hm),e(at,um),e(J,_m),e(J,V),F(nt,V,null),e(V,vm),e(V,Ne),e(Ne,bm),e(Ne,Bt),e(Bt,km),e(Ne,Fm),e(Ne,sr),e(sr,ym),e(Ne,xm),e(V,Tm),F(yo,V,null),e(V,$m),F(xo,V,null),Xr=!0},p(o,[g]){const rt={};g&2&&(rt.$$scope={dirty:g,ctx:o}),je.$set(rt);const ir={};g&2&&(ir.$$scope={dirty:g,ctx:o}),Be.$set(ir);const lr={};g&2&&(lr.$$scope={dirty:g,ctx:o}),Ue.$set(lr);const dr={};g&2&&(dr.$$scope={dirty:g,ctx:o}),Ge.$set(dr);const st={};g&2&&(st.$$scope={dirty:g,ctx:o}),io.$set(st);const cr={};g&2&&(cr.$$scope={dirty:g,ctx:o}),co.$set(cr);const mr={};g&2&&(mr.$$scope={dirty:g,ctx:o}),mo.$set(mr);const pr={};g&2&&(pr.$$scope={dirty:g,ctx:o}),po.$set(pr);const it={};g&2&&(it.$$scope={dirty:g,ctx:o}),fo.$set(it);const fr={};g&2&&(fr.$$scope={dirty:g,ctx:o}),uo.$set(fr);const gr={};g&2&&(gr.$$scope={dirty:g,ctx:o}),_o.$set(gr);const hr={};g&2&&(hr.$$scope={dirty:g,ctx:o}),bo.$set(hr);const ur={};g&2&&(ur.$$scope={dirty:g,ctx:o}),ko.$set(ur);const _r={};g&2&&(_r.$$scope={dirty:g,ctx:o}),yo.$set(_r);const Oe={};g&2&&(Oe.$$scope={dirty:g,ctx:o}),xo.$set(Oe)},i(o){Xr||(y(l.$$.fragment,o),y(Eo.$$.fragment,o),y(qo.$$.fragment,o),y(Lo.$$.fragment,o),y(je.$$.fragment,o),y(jo.$$.fragment,o),y(So.$$.fragment,o),y(Bo.$$.fragment,o),y(Vo.$$.fragment,o),y(Be.$$.fragment,o),y(Yo.$$.fragment,o),y(Zo.$$.fragment,o),y(Ue.$$.fragment,o),y(Qo.$$.fragment,o),y(ea.$$.fragment,o),y(Ge.$$.fragment,o),y(na.$$.fragment,o),y(sa.$$.fragment,o),y(ia.$$.fragment,o),y(la.$$.fragment,o),y(da.$$.fragment,o),y(ma.$$.fragment,o),y(fa.$$.fragment,o),y(ga.$$.fragment,o),y(ha.$$.fragment,o),y(ua.$$.fragment,o),y(_a.$$.fragment,o),y(va.$$.fragment,o),y(ba.$$.fragment,o),y(Fa.$$.fragment,o),y(ya.$$.fragment,o),y(xa.$$.fragment,o),y(Ta.$$.fragment,o),y($a.$$.fragment,o),y(Ia.$$.fragment,o),y(io.$$.fragment,o),y(Ma.$$.fragment,o),y(Pa.$$.fragment,o),y(Aa.$$.fragment,o),y(co.$$.fragment,o),y(mo.$$.fragment,o),y(Na.$$.fragment,o),y(po.$$.fragment,o),y(Oa.$$.fragment,o),y(fo.$$.fragment,o),y(qa.$$.fragment,o),y(La.$$.fragment,o),y(Sa.$$.fragment,o),y(Ra.$$.fragment,o),y(Ba.$$.fragment,o),y(Va.$$.fragment,o),y(Ua.$$.fragment,o),y(Ya.$$.fragment,o),y(uo.$$.fragment,o),y(_o.$$.fragment,o),y(Za.$$.fragment,o),y(Ja.$$.fragment,o),y(Qa.$$.fragment,o),y(bo.$$.fragment,o),y(ko.$$.fragment,o),y(et.$$.fragment,o),y(ot.$$.fragment,o),y(nt.$$.fragment,o),y(yo.$$.fragment,o),y(xo.$$.fragment,o),Xr=!0)},o(o){x(l.$$.fragment,o),x(Eo.$$.fragment,o),x(qo.$$.fragment,o),x(Lo.$$.fragment,o),x(je.$$.fragment,o),x(jo.$$.fragment,o),x(So.$$.fragment,o),x(Bo.$$.fragment,o),x(Vo.$$.fragment,o),x(Be.$$.fragment,o),x(Yo.$$.fragment,o),x(Zo.$$.fragment,o),x(Ue.$$.fragment,o),x(Qo.$$.fragment,o),x(ea.$$.fragment,o),x(Ge.$$.fragment,o),x(na.$$.fragment,o),x(sa.$$.fragment,o),x(ia.$$.fragment,o),x(la.$$.fragment,o),x(da.$$.fragment,o),x(ma.$$.fragment,o),x(fa.$$.fragment,o),x(ga.$$.fragment,o),x(ha.$$.fragment,o),x(ua.$$.fragment,o),x(_a.$$.fragment,o),x(va.$$.fragment,o),x(ba.$$.fragment,o),x(Fa.$$.fragment,o),x(ya.$$.fragment,o),x(xa.$$.fragment,o),x(Ta.$$.fragment,o),x($a.$$.fragment,o),x(Ia.$$.fragment,o),x(io.$$.fragment,o),x(Ma.$$.fragment,o),x(Pa.$$.fragment,o),x(Aa.$$.fragment,o),x(co.$$.fragment,o),x(mo.$$.fragment,o),x(Na.$$.fragment,o),x(po.$$.fragment,o),x(Oa.$$.fragment,o),x(fo.$$.fragment,o),x(qa.$$.fragment,o),x(La.$$.fragment,o),x(Sa.$$.fragment,o),x(Ra.$$.fragment,o),x(Ba.$$.fragment,o),x(Va.$$.fragment,o),x(Ua.$$.fragment,o),x(Ya.$$.fragment,o),x(uo.$$.fragment,o),x(_o.$$.fragment,o),x(Za.$$.fragment,o),x(Ja.$$.fragment,o),x(Qa.$$.fragment,o),x(bo.$$.fragment,o),x(ko.$$.fragment,o),x(et.$$.fragment,o),x(ot.$$.fragment,o),x(nt.$$.fragment,o),x(yo.$$.fragment,o),x(xo.$$.fragment,o),Xr=!1},d(o){a(p),o&&a($),o&&a(_),T(l),o&&a(vr),o&&a(le),T(Eo),o&&a(br),o&&a(Le),o&&a(kr),o&&a(ct),o&&a(Fr),o&&a(mt),o&&a(yr),o&&a(pt),o&&a(xr),o&&a(X),o&&a(Tr),o&&a(de),T(qo),o&&a($r),o&&a(P),T(Lo),T(je),T(jo),T(So),o&&a(wr),o&&a(me),T(Bo),o&&a(zr),o&&a(A),T(Vo),T(Be),o&&a(Ir),o&&a(fe),T(Yo),o&&a(Mr),o&&a(N),T(Zo),T(Ue),o&&a(Pr),o&&a(he),T(Qo),o&&a(Cr),o&&a(O),T(ea),T(Ge),o&&a(Er),o&&a(_e),T(na),o&&a(Ar),o&&a(ra),T(sa),o&&a(Nr),o&&a(ve),T(ia),o&&a(Or),o&&a(q),T(la),T(da),T(ma),o&&a(qr),o&&a(be),T(fa),o&&a(Lr),o&&a(M),T(ga),T(ha),T(ua),T(_a),T(va),T(ba),o&&a(Dr),o&&a(Fe),T(Fa),o&&a(jr),o&&a(G),T(ya),T(xa),o&&a(Wr),o&&a(ye),T(Ta),o&&a(Sr),o&&a(W),T($a),T(Ia),T(io),o&&a(Rr),o&&a(Te),T(Ma),o&&a(Br),o&&a(L),T(Pa),T(Aa),T(co),T(mo),T(Na),T(po),T(Oa),T(fo),o&&a(Vr),o&&a(Ie),T(qa),o&&a(Ur),o&&a(C),T(La),T(Sa),T(Ra),T(Ba),o&&a(Hr),o&&a(Me),T(Va),o&&a(Gr),o&&a(Y),T(Ua),T(Ya),T(uo),T(_o),o&&a(Yr),o&&a(Ce),T(Za),o&&a(Zr),o&&a(Z),T(Ja),T(Qa),T(bo),T(ko),o&&a(Jr),o&&a(Ae),T(et),o&&a(Kr),o&&a(J),T(ot),T(nt),T(yo),T(xo)}}}const Hf={local:"flava",sections:[{local:"overview",title:"Overview"},{local:"transformers.FlavaConfig",title:"FlavaConfig"},{local:"transformers.FlavaTextConfig",title:"FlavaTextConfig"},{local:"transformers.FlavaImageConfig",title:"FlavaImageConfig"},{local:"transformers.FlavaMultimodalConfig",title:"FlavaMultimodalConfig"},{local:"transformers.FlavaImageCodebookConfig",title:"FlavaImageCodebookConfig"},{local:"transformers.FlavaProcessor",title:"FlavaProcessor"},{local:"transformers.FlavaImageProcessor",title:"FlavaFeatureExtractor"},{local:"transformers.FlavaImageProcessor",title:"FlavaImageProcessor"},{local:"transformers.FlavaForPreTraining",title:"FlavaForPreTraining"},{local:"transformers.FlavaModel",title:"FlavaModel"},{local:"transformers.FlavaImageCodebook",title:"FlavaImageCodebook"},{local:"transformers.FlavaTextModel",title:"FlavaTextModel"},{local:"transformers.FlavaImageModel",title:"FlavaImageModel"},{local:"transformers.FlavaMultimodalModel",title:"FlavaMultimodalModel"}],title:"FLAVA"};function Gf(z){return Mf(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class eg extends $f{constructor(p){super();wf(this,p,Gf,Uf,zf,{})}}export{eg as default,Hf as metadata};
