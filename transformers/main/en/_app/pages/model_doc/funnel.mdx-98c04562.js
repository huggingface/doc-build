import{S as w$,i as b$,s as $$,e as r,k as l,w as v,t,M as E$,c as a,d as n,m as d,a as i,x as y,h as o,b as c,F as e,g as h,y as w,q as b,o as $,B as E}from"../../chunks/vendor-6b77c823.js";import{T as ze}from"../../chunks/Tip-39098574.js";import{D as X}from"../../chunks/Docstring-abef54e3.js";import{C as Se}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as qe}from"../../chunks/IconCopyLink-7a11ce68.js";function M$(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function z$(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function q$(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function P$(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function C$(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function j$(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function x$(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function L$(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function O$(W){let p,M,m,g,F,T,_,z,ce,K,q,J,A,ne,ue,N,pe,ie,Y,L,te,G,P,j,oe,Q,le,se,I,he,de,C,fe,B,ee,ae,U,me,S,O,re,R,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),U=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var k=i(p);M=o(k,"TF 2.0 models accepts two formats as inputs:"),k.forEach(n),m=d(u),g=a(u,"UL",{});var Z=i(g);F=a(Z,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(Z),z=a(Z,"LI",{});var ye=i(z);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var Fe=i(A);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);pe=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var H=i(j);oe=o(H,"a single Tensor with "),Q=a(H,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(H," only and nothing else: "),I=a(H,"CODE",{});var ke=i(I);he=o(ke,"model(inputs_ids)"),ke.forEach(n),H.forEach(n),de=d(x),C=a(x,"LI",{});var V=i(C);fe=o(V,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(V,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(V," or "),U=a(V,"CODE",{});var ve=i(U);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),V.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=a(_e,"CODE",{});var Me=i(R);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,k){h(u,p,k),e(p,M),h(u,m,k),h(u,g,k),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,K,k),h(u,q,k),e(q,J),e(q,A),e(A,ne),e(q,ue),e(q,N),e(N,pe),e(q,ie),h(u,Y,k),h(u,L,k),e(L,te),h(u,G,k),h(u,P,k),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,U),e(U,me),e(P,S),e(P,O),e(O,re),e(O,R),e(R,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(K),u&&n(q),u&&n(Y),u&&n(L),u&&n(G),u&&n(P)}}}function D$(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function A$(W){let p,M,m,g,F,T,_,z,ce,K,q,J,A,ne,ue,N,pe,ie,Y,L,te,G,P,j,oe,Q,le,se,I,he,de,C,fe,B,ee,ae,U,me,S,O,re,R,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),U=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var k=i(p);M=o(k,"TF 2.0 models accepts two formats as inputs:"),k.forEach(n),m=d(u),g=a(u,"UL",{});var Z=i(g);F=a(Z,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(Z),z=a(Z,"LI",{});var ye=i(z);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var Fe=i(A);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);pe=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var H=i(j);oe=o(H,"a single Tensor with "),Q=a(H,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(H," only and nothing else: "),I=a(H,"CODE",{});var ke=i(I);he=o(ke,"model(inputs_ids)"),ke.forEach(n),H.forEach(n),de=d(x),C=a(x,"LI",{});var V=i(C);fe=o(V,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(V,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(V," or "),U=a(V,"CODE",{});var ve=i(U);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),V.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=a(_e,"CODE",{});var Me=i(R);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,k){h(u,p,k),e(p,M),h(u,m,k),h(u,g,k),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,K,k),h(u,q,k),e(q,J),e(q,A),e(A,ne),e(q,ue),e(q,N),e(N,pe),e(q,ie),h(u,Y,k),h(u,L,k),e(L,te),h(u,G,k),h(u,P,k),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,U),e(U,me),e(P,S),e(P,O),e(O,re),e(O,R),e(R,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(K),u&&n(q),u&&n(Y),u&&n(L),u&&n(G),u&&n(P)}}}function N$(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function I$(W){let p,M,m,g,F,T,_,z,ce,K,q,J,A,ne,ue,N,pe,ie,Y,L,te,G,P,j,oe,Q,le,se,I,he,de,C,fe,B,ee,ae,U,me,S,O,re,R,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),U=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var k=i(p);M=o(k,"TF 2.0 models accepts two formats as inputs:"),k.forEach(n),m=d(u),g=a(u,"UL",{});var Z=i(g);F=a(Z,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(Z),z=a(Z,"LI",{});var ye=i(z);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var Fe=i(A);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);pe=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var H=i(j);oe=o(H,"a single Tensor with "),Q=a(H,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(H," only and nothing else: "),I=a(H,"CODE",{});var ke=i(I);he=o(ke,"model(inputs_ids)"),ke.forEach(n),H.forEach(n),de=d(x),C=a(x,"LI",{});var V=i(C);fe=o(V,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(V,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(V," or "),U=a(V,"CODE",{});var ve=i(U);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),V.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=a(_e,"CODE",{});var Me=i(R);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,k){h(u,p,k),e(p,M),h(u,m,k),h(u,g,k),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,K,k),h(u,q,k),e(q,J),e(q,A),e(A,ne),e(q,ue),e(q,N),e(N,pe),e(q,ie),h(u,Y,k),h(u,L,k),e(L,te),h(u,G,k),h(u,P,k),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,U),e(U,me),e(P,S),e(P,O),e(O,re),e(O,R),e(R,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(K),u&&n(q),u&&n(Y),u&&n(L),u&&n(G),u&&n(P)}}}function S$(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function B$(W){let p,M,m,g,F,T,_,z,ce,K,q,J,A,ne,ue,N,pe,ie,Y,L,te,G,P,j,oe,Q,le,se,I,he,de,C,fe,B,ee,ae,U,me,S,O,re,R,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),U=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var k=i(p);M=o(k,"TF 2.0 models accepts two formats as inputs:"),k.forEach(n),m=d(u),g=a(u,"UL",{});var Z=i(g);F=a(Z,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(Z),z=a(Z,"LI",{});var ye=i(z);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var Fe=i(A);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);pe=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var H=i(j);oe=o(H,"a single Tensor with "),Q=a(H,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(H," only and nothing else: "),I=a(H,"CODE",{});var ke=i(I);he=o(ke,"model(inputs_ids)"),ke.forEach(n),H.forEach(n),de=d(x),C=a(x,"LI",{});var V=i(C);fe=o(V,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(V,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(V," or "),U=a(V,"CODE",{});var ve=i(U);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),V.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=a(_e,"CODE",{});var Me=i(R);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,k){h(u,p,k),e(p,M),h(u,m,k),h(u,g,k),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,K,k),h(u,q,k),e(q,J),e(q,A),e(A,ne),e(q,ue),e(q,N),e(N,pe),e(q,ie),h(u,Y,k),h(u,L,k),e(L,te),h(u,G,k),h(u,P,k),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,U),e(U,me),e(P,S),e(P,O),e(O,re),e(O,R),e(R,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(K),u&&n(q),u&&n(Y),u&&n(L),u&&n(G),u&&n(P)}}}function W$(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function Q$(W){let p,M,m,g,F,T,_,z,ce,K,q,J,A,ne,ue,N,pe,ie,Y,L,te,G,P,j,oe,Q,le,se,I,he,de,C,fe,B,ee,ae,U,me,S,O,re,R,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),U=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var k=i(p);M=o(k,"TF 2.0 models accepts two formats as inputs:"),k.forEach(n),m=d(u),g=a(u,"UL",{});var Z=i(g);F=a(Z,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(Z),z=a(Z,"LI",{});var ye=i(z);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var Fe=i(A);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);pe=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var H=i(j);oe=o(H,"a single Tensor with "),Q=a(H,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(H," only and nothing else: "),I=a(H,"CODE",{});var ke=i(I);he=o(ke,"model(inputs_ids)"),ke.forEach(n),H.forEach(n),de=d(x),C=a(x,"LI",{});var V=i(C);fe=o(V,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(V,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(V," or "),U=a(V,"CODE",{});var ve=i(U);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),V.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=a(_e,"CODE",{});var Me=i(R);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,k){h(u,p,k),e(p,M),h(u,m,k),h(u,g,k),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,K,k),h(u,q,k),e(q,J),e(q,A),e(A,ne),e(q,ue),e(q,N),e(N,pe),e(q,ie),h(u,Y,k),h(u,L,k),e(L,te),h(u,G,k),h(u,P,k),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,U),e(U,me),e(P,S),e(P,O),e(O,re),e(O,R),e(R,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(K),u&&n(q),u&&n(Y),u&&n(L),u&&n(G),u&&n(P)}}}function U$(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function R$(W){let p,M,m,g,F,T,_,z,ce,K,q,J,A,ne,ue,N,pe,ie,Y,L,te,G,P,j,oe,Q,le,se,I,he,de,C,fe,B,ee,ae,U,me,S,O,re,R,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),U=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var k=i(p);M=o(k,"TF 2.0 models accepts two formats as inputs:"),k.forEach(n),m=d(u),g=a(u,"UL",{});var Z=i(g);F=a(Z,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(Z),z=a(Z,"LI",{});var ye=i(z);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var Fe=i(A);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);pe=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var H=i(j);oe=o(H,"a single Tensor with "),Q=a(H,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(H," only and nothing else: "),I=a(H,"CODE",{});var ke=i(I);he=o(ke,"model(inputs_ids)"),ke.forEach(n),H.forEach(n),de=d(x),C=a(x,"LI",{});var V=i(C);fe=o(V,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(V,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(V," or "),U=a(V,"CODE",{});var ve=i(U);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),V.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=a(_e,"CODE",{});var Me=i(R);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,k){h(u,p,k),e(p,M),h(u,m,k),h(u,g,k),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,K,k),h(u,q,k),e(q,J),e(q,A),e(A,ne),e(q,ue),e(q,N),e(N,pe),e(q,ie),h(u,Y,k),h(u,L,k),e(L,te),h(u,G,k),h(u,P,k),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,U),e(U,me),e(P,S),e(P,O),e(O,re),e(O,R),e(R,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(K),u&&n(q),u&&n(Y),u&&n(L),u&&n(G),u&&n(P)}}}function H$(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function V$(W){let p,M,m,g,F,T,_,z,ce,K,q,J,A,ne,ue,N,pe,ie,Y,L,te,G,P,j,oe,Q,le,se,I,he,de,C,fe,B,ee,ae,U,me,S,O,re,R,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),U=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var k=i(p);M=o(k,"TF 2.0 models accepts two formats as inputs:"),k.forEach(n),m=d(u),g=a(u,"UL",{});var Z=i(g);F=a(Z,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(Z),z=a(Z,"LI",{});var ye=i(z);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var Fe=i(A);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);pe=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var H=i(j);oe=o(H,"a single Tensor with "),Q=a(H,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(H," only and nothing else: "),I=a(H,"CODE",{});var ke=i(I);he=o(ke,"model(inputs_ids)"),ke.forEach(n),H.forEach(n),de=d(x),C=a(x,"LI",{});var V=i(C);fe=o(V,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(V,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(V," or "),U=a(V,"CODE",{});var ve=i(U);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),V.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=a(_e,"CODE",{});var Me=i(R);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,k){h(u,p,k),e(p,M),h(u,m,k),h(u,g,k),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,K,k),h(u,q,k),e(q,J),e(q,A),e(A,ne),e(q,ue),e(q,N),e(N,pe),e(q,ie),h(u,Y,k),h(u,L,k),e(L,te),h(u,G,k),h(u,P,k),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,U),e(U,me),e(P,S),e(P,O),e(O,re),e(O,R),e(R,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(K),u&&n(q),u&&n(Y),u&&n(L),u&&n(G),u&&n(P)}}}function Y$(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function K$(W){let p,M,m,g,F,T,_,z,ce,K,q,J,A,ne,ue,N,pe,ie,Y,L,te,G,P,j,oe,Q,le,se,I,he,de,C,fe,B,ee,ae,U,me,S,O,re,R,ge;return{c(){p=r("p"),M=t("TF 2.0 models accepts two formats as inputs:"),m=l(),g=r("ul"),F=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),Q=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),U=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var k=i(p);M=o(k,"TF 2.0 models accepts two formats as inputs:"),k.forEach(n),m=d(u),g=a(u,"UL",{});var Z=i(g);F=a(Z,"LI",{});var Te=i(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),_=d(Z),z=a(Z,"LI",{});var ye=i(z);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var Fe=i(A);ne=o(Fe,"tf.keras.Model.fit"),Fe.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);pe=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var H=i(j);oe=o(H,"a single Tensor with "),Q=a(H,"CODE",{});var $e=i(Q);le=o($e,"input_ids"),$e.forEach(n),se=o(H," only and nothing else: "),I=a(H,"CODE",{});var ke=i(I);he=o(ke,"model(inputs_ids)"),ke.forEach(n),H.forEach(n),de=d(x),C=a(x,"LI",{});var V=i(C);fe=o(V,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(V,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(V," or "),U=a(V,"CODE",{});var ve=i(U);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),V.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),R=a(_e,"CODE",{});var Me=i(R);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,k){h(u,p,k),e(p,M),h(u,m,k),h(u,g,k),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,K,k),h(u,q,k),e(q,J),e(q,A),e(A,ne),e(q,ue),e(q,N),e(N,pe),e(q,ie),h(u,Y,k),h(u,L,k),e(L,te),h(u,G,k),h(u,P,k),e(P,j),e(j,oe),e(j,Q),e(Q,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,U),e(U,me),e(P,S),e(P,O),e(O,re),e(O,R),e(R,ge)},d(u){u&&n(p),u&&n(m),u&&n(g),u&&n(K),u&&n(q),u&&n(Y),u&&n(L),u&&n(G),u&&n(P)}}}function G$(W){let p,M,m,g,F;return{c(){p=r("p"),M=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r("code"),g=t("Module"),F=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var _=i(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=a(_,"CODE",{});var z=i(m);g=o(z,"Module"),z.forEach(n),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(n)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&n(p)}}}function Z$(W){let p,M,m,g,F,T,_,z,ce,K,q,J,A,ne,ue,N,pe,ie,Y,L,te,G,P,j,oe,Q,le,se,I,he,de,C,fe,B,ee,ae,U,me,S,O,re,R,ge,u,k,Z,Te,ye,D,Fe,we,be,x,H,$e,ke,V,Ee,ve,_e,Me,Ya,Ip,Sp,Ec,xn,Bp,No,Wp,Qp,Io,Up,Rp,Mc,Zn,Bt,ul,So,Hp,pl,Vp,zc,Cn,Bo,Yp,jn,Kp,Ka,Gp,Zp,Ga,Xp,Jp,Wo,eh,nh,th,Xn,oh,Za,sh,rh,Xa,ah,ih,qc,Jn,Wt,hl,Qo,lh,fl,dh,Pc,Pe,Uo,ch,ml,uh,ph,Qt,Ja,hh,fh,ei,mh,gh,_h,Ro,Th,ni,Fh,kh,vh,Ln,Ho,yh,gl,wh,bh,Vo,ti,$h,_l,Eh,Mh,oi,zh,Tl,qh,Ph,Ut,Yo,Ch,Ko,jh,Fl,xh,Lh,Oh,yn,Go,Dh,kl,Ah,Nh,Zo,Ih,et,Sh,vl,Bh,Wh,yl,Qh,Uh,Rh,si,Xo,Cc,nt,Rt,wl,Jo,Hh,bl,Vh,jc,Ge,es,Yh,ns,Kh,$l,Gh,Zh,Xh,Ht,ri,Jh,ef,ai,nf,tf,of,ts,sf,ii,rf,af,lf,wn,os,df,El,cf,uf,ss,pf,tt,hf,Ml,ff,mf,zl,gf,_f,xc,ot,Vt,ql,rs,Tf,Pl,Ff,Lc,st,as,kf,is,vf,li,yf,wf,Oc,rt,ls,bf,ds,$f,di,Ef,Mf,Dc,at,Yt,Cl,cs,zf,jl,qf,Ac,We,us,Pf,xl,Cf,jf,ps,xf,hs,Lf,Of,Df,fs,Af,ci,Nf,If,Sf,ms,Bf,gs,Wf,Qf,Uf,Ze,_s,Rf,it,Hf,ui,Vf,Yf,Ll,Kf,Gf,Zf,Kt,Xf,Ol,Jf,em,Ts,Nc,lt,Gt,Dl,Fs,nm,Al,tm,Ic,Qe,ks,om,Nl,sm,rm,vs,am,ys,im,lm,dm,ws,cm,pi,um,pm,hm,bs,fm,$s,mm,gm,_m,Xe,Es,Tm,dt,Fm,hi,km,vm,Il,ym,wm,bm,Zt,$m,Sl,Em,Mm,Ms,Sc,ct,Xt,Bl,zs,zm,Wl,qm,Bc,ut,qs,Pm,Je,Ps,Cm,pt,jm,fi,xm,Lm,Ql,Om,Dm,Am,Jt,Nm,Ul,Im,Sm,Cs,Wc,ht,eo,Rl,js,Bm,Hl,Wm,Qc,Ue,xs,Qm,Ls,Um,Vl,Rm,Hm,Vm,Os,Ym,Ds,Km,Gm,Zm,As,Xm,mi,Jm,eg,ng,Ns,tg,Is,og,sg,rg,en,Ss,ag,ft,ig,gi,lg,dg,Yl,cg,ug,pg,no,hg,Kl,fg,mg,Bs,Uc,mt,to,Gl,Ws,gg,Zl,_g,Rc,Re,Qs,Tg,Xl,Fg,kg,Us,vg,Rs,yg,wg,bg,Hs,$g,_i,Eg,Mg,zg,Vs,qg,Ys,Pg,Cg,jg,Be,Ks,xg,gt,Lg,Ti,Og,Dg,Jl,Ag,Ng,Ig,oo,Sg,ed,Bg,Wg,Gs,Qg,nd,Ug,Rg,Zs,Hc,_t,so,td,Xs,Hg,od,Vg,Vc,He,Js,Yg,sd,Kg,Gg,er,Zg,nr,Xg,Jg,e_,tr,n_,Fi,t_,o_,s_,or,r_,sr,a_,i_,l_,nn,rr,d_,Tt,c_,ki,u_,p_,rd,h_,f_,m_,ro,g_,ad,__,T_,ar,Yc,Ft,ao,id,ir,F_,ld,k_,Kc,Ve,lr,v_,dd,y_,w_,dr,b_,cr,$_,E_,M_,ur,z_,vi,q_,P_,C_,pr,j_,hr,x_,L_,O_,tn,fr,D_,kt,A_,yi,N_,I_,cd,S_,B_,W_,io,Q_,ud,U_,R_,mr,Gc,vt,lo,pd,gr,H_,hd,V_,Zc,Ye,_r,Y_,yt,K_,fd,G_,Z_,md,X_,J_,eT,Tr,nT,Fr,tT,oT,sT,kr,rT,wi,aT,iT,lT,vr,dT,yr,cT,uT,pT,on,wr,hT,wt,fT,bi,mT,gT,gd,_T,TT,FT,co,kT,_d,vT,yT,br,Xc,bt,uo,Td,$r,wT,Fd,bT,Jc,je,Er,$T,kd,ET,MT,Mr,zT,zr,qT,PT,CT,qr,jT,$i,xT,LT,OT,Pr,DT,Cr,AT,NT,IT,po,ST,sn,jr,BT,$t,WT,Ei,QT,UT,vd,RT,HT,VT,ho,YT,yd,KT,GT,xr,eu,Et,fo,wd,Lr,ZT,bd,XT,nu,xe,Or,JT,$d,eF,nF,Dr,tF,Ar,oF,sF,rF,Nr,aF,Mi,iF,lF,dF,Ir,cF,Sr,uF,pF,hF,mo,fF,rn,Br,mF,Mt,gF,zi,_F,TF,Ed,FF,kF,vF,go,yF,Md,wF,bF,Wr,tu,zt,_o,zd,Qr,$F,qd,EF,ou,Le,Ur,MF,Pd,zF,qF,Rr,PF,Hr,CF,jF,xF,Vr,LF,qi,OF,DF,AF,Yr,NF,Kr,IF,SF,BF,To,WF,an,Gr,QF,qt,UF,Pi,RF,HF,Cd,VF,YF,KF,Fo,GF,jd,ZF,XF,Zr,su,Pt,ko,xd,Xr,JF,Ld,ek,ru,Oe,Jr,nk,ea,tk,Od,ok,sk,rk,na,ak,ta,ik,lk,dk,oa,ck,Ci,uk,pk,hk,sa,fk,ra,mk,gk,_k,vo,Tk,ln,aa,Fk,Ct,kk,ji,vk,yk,Dd,wk,bk,$k,yo,Ek,Ad,Mk,zk,ia,au,jt,wo,Nd,la,qk,Id,Pk,iu,De,da,Ck,Sd,jk,xk,ca,Lk,ua,Ok,Dk,Ak,pa,Nk,xi,Ik,Sk,Bk,ha,Wk,fa,Qk,Uk,Rk,bo,Hk,dn,ma,Vk,xt,Yk,Li,Kk,Gk,Bd,Zk,Xk,Jk,$o,ev,Wd,nv,tv,ga,lu,Lt,Eo,Qd,_a,ov,Ud,sv,du,Ae,Ta,rv,Rd,av,iv,Fa,lv,ka,dv,cv,uv,va,pv,Oi,hv,fv,mv,ya,gv,wa,_v,Tv,Fv,Mo,kv,cn,ba,vv,Ot,yv,Di,wv,bv,Hd,$v,Ev,Mv,zo,zv,Vd,qv,Pv,$a,cu,Dt,qo,Yd,Ea,Cv,Kd,jv,uu,Ne,Ma,xv,Gd,Lv,Ov,za,Dv,qa,Av,Nv,Iv,Pa,Sv,Ai,Bv,Wv,Qv,Ca,Uv,ja,Rv,Hv,Vv,Po,Yv,un,xa,Kv,At,Gv,Ni,Zv,Xv,Zd,Jv,ey,ny,Co,ty,Xd,oy,sy,La,pu,Nt,jo,Jd,Oa,ry,ec,ay,hu,Ie,Da,iy,It,ly,nc,dy,cy,tc,uy,py,hy,Aa,fy,Na,my,gy,_y,Ia,Ty,Ii,Fy,ky,vy,Sa,yy,Ba,wy,by,$y,xo,Ey,pn,Wa,My,St,zy,Si,qy,Py,oc,Cy,jy,xy,Lo,Ly,sc,Oy,Dy,Qa,fu;return T=new qe({}),ne=new qe({}),So=new qe({}),Bo=new X({props:{name:"class transformers.FunnelConfig",anchor:"transformers.FunnelConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"block_sizes",val:" = [4, 4, 4]"},{name:"block_repeats",val:" = None"},{name:"num_decoder_layers",val:" = 2"},{name:"d_model",val:" = 768"},{name:"n_head",val:" = 12"},{name:"d_head",val:" = 64"},{name:"d_inner",val:" = 3072"},{name:"hidden_act",val:" = 'gelu_new'"},{name:"hidden_dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.1"},{name:"activation_dropout",val:" = 0.0"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 3"},{name:"initializer_range",val:" = 0.1"},{name:"initializer_std",val:" = None"},{name:"layer_norm_eps",val:" = 1e-09"},{name:"pooling_type",val:" = 'mean'"},{name:"attention_type",val:" = 'relative_shift'"},{name:"separate_cls",val:" = True"},{name:"truncate_seq",val:" = True"},{name:"pool_q_only",val:" = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/configuration_funnel.py#L37",parametersDescription:[{anchor:"transformers.FunnelConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the Funnel transformer. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a>.`,name:"vocab_size"},{anchor:"transformers.FunnelConfig.block_sizes",description:`<strong>block_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[4, 4, 4]</code>) &#x2014;
The sizes of the blocks used in the model.`,name:"block_sizes"},{anchor:"transformers.FunnelConfig.block_repeats",description:`<strong>block_repeats</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If passed along, each layer of each block is repeated the number of times indicated.`,name:"block_repeats"},{anchor:"transformers.FunnelConfig.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The number of layers in the decoder (when not using the base model).`,name:"num_decoder_layers"},{anchor:"transformers.FunnelConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the model&#x2019;s hidden states.`,name:"d_model"},{anchor:"transformers.FunnelConfig.n_head",description:`<strong>n_head</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"n_head"},{anchor:"transformers.FunnelConfig.d_head",description:`<strong>d_head</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality of the model&#x2019;s heads.`,name:"d_head"},{anchor:"transformers.FunnelConfig.d_inner",description:`<strong>d_inner</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Inner dimension in the feed-forward blocks.`,name:"d_inner"},{anchor:"transformers.FunnelConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>callable</code>, <em>optional</em>, defaults to <code>&quot;gelu_new&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FunnelConfig.hidden_dropout",description:`<strong>hidden_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout"},{anchor:"transformers.FunnelConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.FunnelConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability used between the two layers of the feed-forward blocks.`,name:"activation_dropout"},{anchor:"transformers.FunnelConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.FunnelConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.FunnelConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The upper bound of the <em>uniform initializer</em> for initializing all weight matrices in attention layers.`,name:"initializer_range"},{anchor:"transformers.FunnelConfig.initializer_std",description:`<strong>initializer_std</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The standard deviation of the <em>normal initializer</em> for initializing the embedding matrix and the weight of
linear layers. Will default to 1 for the embedding matrix and the value given by Xavier initialization for
linear layers.`,name:"initializer_std"},{anchor:"transformers.FunnelConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-9) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FunnelConfig.pooling_type",description:`<strong>pooling_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;mean&quot;</code>) &#x2014;
Possible values are <code>&quot;mean&quot;</code> or <code>&quot;max&quot;</code>. The way pooling is performed at the beginning of each block.`,name:"pooling_type"},{anchor:"transformers.FunnelConfig.attention_type",description:`<strong>attention_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;relative_shift&quot;</code>) &#x2014;
Possible values are <code>&quot;relative_shift&quot;</code> or <code>&quot;factorized&quot;</code>. The former is faster on CPU/GPU while the latter
is faster on TPU.`,name:"attention_type"},{anchor:"transformers.FunnelConfig.separate_cls",description:`<strong>separate_cls</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to separate the cls token when applying pooling.`,name:"separate_cls"},{anchor:"transformers.FunnelConfig.truncate_seq",description:`<strong>truncate_seq</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
When using <code>separate_cls</code>, whether or not to truncate the last token when pooling, to avoid getting a
sequence length that is not a multiple of 2.`,name:"truncate_seq"},{anchor:"transformers.FunnelConfig.pool_q_only",description:`<strong>pool_q_only</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to apply the pooling only to the query or to query, key and values for the attention layers.`,name:"pool_q_only"}]}}),Qo=new qe({}),Uo=new X({props:{name:"class transformers.FunnelTokenizer",anchor:"transformers.FunnelTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/tokenization_funnel.py#L58"}}),Ho=new X({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/tokenization_bert.py#L248",parametersDescription:[{anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Yo=new X({props:{name:"get_special_tokens_mask",anchor:"transformers.BertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/tokenization_bert.py#L273",parametersDescription:[{anchor:"transformers.BertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Go=new X({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/tokenization_funnel.py#L108",parametersDescription:[{anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Zo=new Se({props:{code:`2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`2<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),Xo=new X({props:{name:"save_vocabulary",anchor:"transformers.BertTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/tokenization_bert.py#L330"}}),Jo=new qe({}),es=new X({props:{name:"class transformers.FunnelTokenizerFast",anchor:"transformers.FunnelTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"clean_text",val:" = True"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"wordpieces_prefix",val:" = '##'"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/tokenization_funnel_fast.py#L71"}}),os=new X({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/tokenization_funnel_fast.py#L124",parametersDescription:[{anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ss=new Se({props:{code:`2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`2<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),rs=new qe({}),as=new X({props:{name:"class transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L834",parametersDescription:[{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.loss",description:`<strong>loss</strong> (<em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) &#x2014;
Total loss of the ELECTRA-style objective.`,name:"loss"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Prediction scores of the head (scores for each token before SoftMax).`,name:"logits"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),ls=new X({props:{name:"class transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput",anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput",parameters:[{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L980",parametersDescription:[{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Prediction scores of the head (scores for each token before SoftMax).`,name:"logits"},{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),cs=new qe({}),us=new X({props:{name:"class transformers.FunnelBaseModel",anchor:"transformers.FunnelBaseModel",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L927",parametersDescription:[{anchor:"transformers.FunnelBaseModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),_s=new X({props:{name:"forward",anchor:"transformers.FunnelBaseModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L943",parametersDescription:[{anchor:"transformers.FunnelBaseModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelBaseModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelBaseModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelBaseModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelBaseModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelBaseModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelBaseModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Kt=new ze({props:{$$slots:{default:[M$]},$$scope:{ctx:W}}}),Ts=new Se({props:{code:`from transformers import FunnelTokenizer, FunnelBaseModel
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelBaseModel.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelBaseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelBaseModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Fs=new qe({}),ks=new X({props:{name:"class transformers.FunnelModel",anchor:"transformers.FunnelModel",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1004",parametersDescription:[{anchor:"transformers.FunnelModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Es=new X({props:{name:"forward",anchor:"transformers.FunnelModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1021",parametersDescription:[{anchor:"transformers.FunnelModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Zt=new ze({props:{$$slots:{default:[z$]},$$scope:{ctx:W}}}),Ms=new Se({props:{code:`from transformers import FunnelTokenizer, FunnelModel
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelModel.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),zs=new qe({}),qs=new X({props:{name:"class transformers.FunnelForPreTraining",anchor:"transformers.FunnelForPreTraining",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1112"}}),Ps=new X({props:{name:"forward",anchor:"transformers.FunnelForPreTraining.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1121",parametersDescription:[{anchor:"transformers.FunnelForPreTraining.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForPreTraining.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForPreTraining.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForPreTraining.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForPreTraining.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForPreTraining.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForPreTraining.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForPreTraining.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the ELECTRA-style loss. Input should be a sequence of tokens (see <code>input_ids</code>
docstring) Indices should be in <code>[0, 1]</code>:</p>
<ul>
<li>0 indicates the token is an original token,</li>
<li>1 indicates the token was replaced.</li>
</ul>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) \u2014 Total loss of the ELECTRA-style objective.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Prediction scores of the head (scores for each token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Jt=new ze({props:{$$slots:{default:[q$]},$$scope:{ctx:W}}}),Cs=new Se({props:{code:`from transformers import FunnelTokenizer, FunnelForPreTraining
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForPreTraining.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
logits = model(**inputs).logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForPreTraining.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits`}}),js=new qe({}),xs=new X({props:{name:"class transformers.FunnelForMaskedLM",anchor:"transformers.FunnelForMaskedLM",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1195",parametersDescription:[{anchor:"transformers.FunnelForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ss=new X({props:{name:"forward",anchor:"transformers.FunnelForMaskedLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1211",parametersDescription:[{anchor:"transformers.FunnelForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForMaskedLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForMaskedLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),no=new ze({props:{$$slots:{default:[P$]},$$scope:{ctx:W}}}),Bs=new Se({props:{code:`from transformers import FunnelTokenizer, FunnelForMaskedLM
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForMaskedLM.from_pretrained("funnel-transformer/small")

inputs = tokenizer("The capital of France is <mask>.", return_tensors="pt")
labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is &lt;mask&gt;.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ws=new qe({}),Qs=new X({props:{name:"class transformers.FunnelForSequenceClassification",anchor:"transformers.FunnelForSequenceClassification",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1275",parametersDescription:[{anchor:"transformers.FunnelForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ks=new X({props:{name:"forward",anchor:"transformers.FunnelForSequenceClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1286",parametersDescription:[{anchor:"transformers.FunnelForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForSequenceClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),oo=new ze({props:{$$slots:{default:[C$]},$$scope:{ctx:W}}}),Gs=new Se({props:{code:`import torch
from transformers import FunnelTokenizer, FunnelForSequenceClassification

torch.manual_seed(0)
tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base", num_labels=2)

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits
list(logits.shape)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>, num_labels=<span class="hljs-number">2</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>]).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(logits.shape)
`}}),Zs=new Se({props:{code:`import torch
from transformers import FunnelTokenizer, FunnelForSequenceClassification

torch.manual_seed(0)
tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base", problem_type="multi_label_classification", num_labels=2)

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([[1, 1]], dtype=torch.float)  # need dtype=float for BCEWithLogitsLoss
outputs = model(**inputs, labels=labels)
loss = outputs.loss
list(logits.shape)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>, num_labels=<span class="hljs-number">2</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], dtype=torch.<span class="hljs-built_in">float</span>)  <span class="hljs-comment"># need dtype=float for BCEWithLogitsLoss</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(logits.shape)
`}}),Xs=new qe({}),Js=new X({props:{name:"class transformers.FunnelForMultipleChoice",anchor:"transformers.FunnelForMultipleChoice",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1368",parametersDescription:[{anchor:"transformers.FunnelForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),rr=new X({props:{name:"forward",anchor:"transformers.FunnelForMultipleChoice.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1377",parametersDescription:[{anchor:"transformers.FunnelForMultipleChoice.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForMultipleChoice.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForMultipleChoice.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForMultipleChoice.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForMultipleChoice.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForMultipleChoice.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForMultipleChoice.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForMultipleChoice.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices-1]</code> where <code>num_choices</code> is the size of the second dimension of the input tensors. (See
<code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ro=new ze({props:{$$slots:{default:[j$]},$$scope:{ctx:W}}}),ar=new Se({props:{code:`from transformers import FunnelTokenizer, FunnelForMultipleChoice
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForMultipleChoice.from_pretrained("funnel-transformer/small-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."
labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="pt", padding=True)
outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1

# the linear classifier still needs to be trained
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># choice0 is correct (according to Wikipedia ;)), batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v.unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}, labels=labels)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ir=new qe({}),lr=new X({props:{name:"class transformers.FunnelForTokenClassification",anchor:"transformers.FunnelForTokenClassification",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1452",parametersDescription:[{anchor:"transformers.FunnelForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),fr=new X({props:{name:"forward",anchor:"transformers.FunnelForTokenClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1464",parametersDescription:[{anchor:"transformers.FunnelForTokenClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForTokenClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForTokenClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForTokenClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForTokenClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForTokenClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForTokenClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForTokenClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),io=new ze({props:{$$slots:{default:[x$]},$$scope:{ctx:W}}}),mr=new Se({props:{code:`from transformers import FunnelTokenizer, FunnelForTokenClassification
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForTokenClassification.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1] * inputs["input_ids"].size(1)).unsqueeze(0)  # Batch size 1

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>] * inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].size(<span class="hljs-number">1</span>)).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),gr=new qe({}),_r=new X({props:{name:"class transformers.FunnelForQuestionAnswering",anchor:"transformers.FunnelForQuestionAnswering",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1526",parametersDescription:[{anchor:"transformers.FunnelForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),wr=new X({props:{name:"forward",anchor:"transformers.FunnelForQuestionAnswering.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"start_positions",val:": typing.Optional[torch.Tensor] = None"},{name:"end_positions",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1537",parametersDescription:[{anchor:"transformers.FunnelForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForQuestionAnswering.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForQuestionAnswering.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForQuestionAnswering.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForQuestionAnswering.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForQuestionAnswering.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.FunnelForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),co=new ze({props:{$$slots:{default:[L$]},$$scope:{ctx:W}}}),br=new Se({props:{code:`from transformers import FunnelTokenizer, FunnelForQuestionAnswering
import torch

torch.manual_seed(0)
tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForQuestionAnswering.from_pretrained("funnel-transformer/small")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
inputs = tokenizer(question, text, return_tensors="pt")
start_positions = torch.tensor([1])
end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
loss = outputs.loss
round(loss.item(), 2)


start_scores = outputs.start_logits
list(start_scores.shape)


end_scores = outputs.end_logits
list(end_scores.shape)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_positions = torch.tensor([<span class="hljs-number">1</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>end_positions = torch.tensor([<span class="hljs-number">3</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span>start_scores = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(start_scores.shape)


<span class="hljs-meta">&gt;&gt;&gt; </span>end_scores = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(end_scores.shape)
`}}),$r=new qe({}),Er=new X({props:{name:"class transformers.TFFunnelBaseModel",anchor:"transformers.TFFunnelBaseModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1097",parametersDescription:[{anchor:"transformers.TFFunnelBaseModel.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),po=new ze({props:{$$slots:{default:[O$]},$$scope:{ctx:W}}}),jr=new X({props:{name:"call",anchor:"transformers.TFFunnelBaseModel.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1102",parametersDescription:[{anchor:"transformers.TFFunnelBaseModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelBaseModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelBaseModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelBaseModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelBaseModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelBaseModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelBaseModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelBaseModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),ho=new ze({props:{$$slots:{default:[D$]},$$scope:{ctx:W}}}),xr=new Se({props:{code:`from transformers import FunnelTokenizer, TFFunnelBaseModel
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelBaseModel.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelBaseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelBaseModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Lr=new qe({}),Or=new X({props:{name:"class transformers.TFFunnelModel",anchor:"transformers.TFFunnelModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1145",parametersDescription:[{anchor:"transformers.TFFunnelModel.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),mo=new ze({props:{$$slots:{default:[A$]},$$scope:{ctx:W}}}),Br=new X({props:{name:"call",anchor:"transformers.TFFunnelModel.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1150",parametersDescription:[{anchor:"transformers.TFFunnelModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),go=new ze({props:{$$slots:{default:[N$]},$$scope:{ctx:W}}}),Wr=new Se({props:{code:`from transformers import FunnelTokenizer, TFFunnelModel
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelModel.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Qr=new qe({}),Ur=new X({props:{name:"class transformers.TFFunnelForPreTraining",anchor:"transformers.TFFunnelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1196",parametersDescription:[{anchor:"transformers.TFFunnelForPreTraining.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),To=new ze({props:{$$slots:{default:[I$]},$$scope:{ctx:W}}}),Gr=new X({props:{name:"call",anchor:"transformers.TFFunnelForPreTraining.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1203",parametersDescription:[{anchor:"transformers.TFFunnelForPreTraining.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForPreTraining.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForPreTraining.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForPreTraining.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForPreTraining.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForPreTraining.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForPreTraining.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForPreTraining.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Prediction scores of the head (scores for each token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Fo=new ze({props:{$$slots:{default:[S$]},$$scope:{ctx:W}}}),Zr=new Se({props:{code:`from transformers import FunnelTokenizer, TFFunnelForPreTraining
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForPreTraining.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
logits = model(inputs).logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForPreTraining.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(inputs).logits`}}),Xr=new qe({}),Jr=new X({props:{name:"class transformers.TFFunnelForMaskedLM",anchor:"transformers.TFFunnelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1263",parametersDescription:[{anchor:"transformers.TFFunnelForMaskedLM.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),vo=new ze({props:{$$slots:{default:[B$]},$$scope:{ctx:W}}}),aa=new X({props:{name:"call",anchor:"transformers.TFFunnelForMaskedLM.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1277",parametersDescription:[{anchor:"transformers.TFFunnelForMaskedLM.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForMaskedLM.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForMaskedLM.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForMaskedLM.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForMaskedLM.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForMaskedLM.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForMaskedLM.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForMaskedLM.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForMaskedLM.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),yo=new ze({props:{$$slots:{default:[W$]},$$scope:{ctx:W}}}),ia=new Se({props:{code:`from transformers import FunnelTokenizer, TFFunnelForMaskedLM
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForMaskedLM.from_pretrained("funnel-transformer/small")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="tf")
inputs["labels"] = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),la=new qe({}),da=new X({props:{name:"class transformers.TFFunnelForSequenceClassification",anchor:"transformers.TFFunnelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1345",parametersDescription:[{anchor:"transformers.TFFunnelForSequenceClassification.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),bo=new ze({props:{$$slots:{default:[Q$]},$$scope:{ctx:W}}}),ma=new X({props:{name:"call",anchor:"transformers.TFFunnelForSequenceClassification.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1353",parametersDescription:[{anchor:"transformers.TFFunnelForSequenceClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForSequenceClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForSequenceClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForSequenceClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForSequenceClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForSequenceClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForSequenceClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForSequenceClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForSequenceClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),$o=new ze({props:{$$slots:{default:[U$]},$$scope:{ctx:W}}}),ga=new Se({props:{code:`from transformers import FunnelTokenizer, TFFunnelForSequenceClassification
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
inputs["labels"] = tf.reshape(tf.constant(1), (-1, 1))  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(tf.constant(<span class="hljs-number">1</span>), (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),_a=new qe({}),Ta=new X({props:{name:"class transformers.TFFunnelForMultipleChoice",anchor:"transformers.TFFunnelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1422",parametersDescription:[{anchor:"transformers.TFFunnelForMultipleChoice.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Mo=new ze({props:{$$slots:{default:[R$]},$$scope:{ctx:W}}}),ba=new X({props:{name:"call",anchor:"transformers.TFFunnelForMultipleChoice.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1439",parametersDescription:[{anchor:"transformers.TFFunnelForMultipleChoice.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForMultipleChoice.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForMultipleChoice.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForMultipleChoice.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForMultipleChoice.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForMultipleChoice.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForMultipleChoice.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForMultipleChoice.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForMultipleChoice.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices]</code>
where <code>num_choices</code> is the size of the second dimension of the input tensors. (See <code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <em>(batch_size, )</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),zo=new ze({props:{$$slots:{default:[H$]},$$scope:{ctx:W}}}),$a=new Se({props:{code:`from transformers import FunnelTokenizer, TFFunnelForMultipleChoice
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelForMultipleChoice.from_pretrained("funnel-transformer/small-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="tf", padding=True)
inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}
outputs = model(inputs)  # batch size is 1

# the linear classifier still needs to be trained
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;tf&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {k: tf.expand_dims(v, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ea=new qe({}),Ma=new X({props:{name:"class transformers.TFFunnelForTokenClassification",anchor:"transformers.TFFunnelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1539",parametersDescription:[{anchor:"transformers.TFFunnelForTokenClassification.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Po=new ze({props:{$$slots:{default:[V$]},$$scope:{ctx:W}}}),xa=new X({props:{name:"call",anchor:"transformers.TFFunnelForTokenClassification.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1550",parametersDescription:[{anchor:"transformers.TFFunnelForTokenClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForTokenClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForTokenClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForTokenClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForTokenClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForTokenClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForTokenClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForTokenClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForTokenClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of unmasked labels, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Co=new ze({props:{$$slots:{default:[Y$]},$$scope:{ctx:W}}}),La=new Se({props:{code:`from transformers import FunnelTokenizer, TFFunnelForTokenClassification
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForTokenClassification.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
input_ids = inputs["input_ids"]
inputs["labels"] = tf.reshape(
    tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids))
)  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(
<span class="hljs-meta">... </span>    tf.constant([<span class="hljs-number">1</span>] * tf.size(input_ids).numpy()), (-<span class="hljs-number">1</span>, tf.size(input_ids))
<span class="hljs-meta">&gt;&gt;&gt; </span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Oa=new qe({}),Da=new X({props:{name:"class transformers.TFFunnelForQuestionAnswering",anchor:"transformers.TFFunnelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1618",parametersDescription:[{anchor:"transformers.TFFunnelForQuestionAnswering.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),xo=new ze({props:{$$slots:{default:[K$]},$$scope:{ctx:W}}}),Wa=new X({props:{name:"call",anchor:"transformers.TFFunnelForQuestionAnswering.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"start_positions",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"end_positions",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1628",parametersDescription:[{anchor:"transformers.TFFunnelForQuestionAnswering.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.start_positions",description:`<strong>start_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.end_positions",description:`<strong>end_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>start_positions</code> and <code>end_positions</code> are provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Lo=new ze({props:{$$slots:{default:[G$]},$$scope:{ctx:W}}}),Qa=new Se({props:{code:`from transformers import FunnelTokenizer, TFFunnelForQuestionAnswering
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForQuestionAnswering.from_pretrained("funnel-transformer/small")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
input_dict = tokenizer(question, text, return_tensors="tf")
outputs = model(input_dict)
start_logits = outputs.start_logits
end_logits = outputs.end_logits

all_tokens = tokenizer.convert_ids_to_tokens(input_dict["input_ids"].numpy()[0])
answer = " ".join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0] + 1])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_dict = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_dict)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits

<span class="hljs-meta">&gt;&gt;&gt; </span>all_tokens = tokenizer.convert_ids_to_tokens(input_dict[<span class="hljs-string">&quot;input_ids&quot;</span>].numpy()[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>answer = <span class="hljs-string">&quot; &quot;</span>.join(all_tokens[tf.math.argmax(start_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] : tf.math.argmax(end_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] + <span class="hljs-number">1</span>])`}}),{c(){p=r("meta"),M=l(),m=r("h1"),g=r("a"),F=r("span"),v(T.$$.fragment),_=l(),z=r("span"),ce=t("Funnel Transformer"),K=l(),q=r("h2"),J=r("a"),A=r("span"),v(ne.$$.fragment),ue=l(),N=r("span"),pe=t("Overview"),ie=l(),Y=r("p"),L=t("The Funnel Transformer model was proposed in the paper "),te=r("a"),G=t(`Funnel-Transformer: Filtering out Sequential Redundancy for
Efficient Language Processing`),P=t(`. It is a bidirectional transformer model, like
BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks
(CNN) in computer vision.`),j=l(),oe=r("p"),Q=t("The abstract from the paper is the following:"),le=l(),se=r("p"),I=r("em"),he=t(`With the success of language pretraining, it is highly desirable to develop more efficient architectures of good
scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the
much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only
require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which
gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More
importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further
improve the model capacity. In addition, to perform token-level predictions as required by common pretraining
objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence
via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on
a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading
comprehension.`),de=l(),C=r("p"),fe=t("Tips:"),B=l(),ee=r("ul"),ae=r("li"),U=t(`Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers.
The base model therefore has a final sequence length that is a quarter of the original one. This model can be used
directly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other
tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same
sequence length as the input.`),me=l(),S=r("li"),O=t(`The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be
used for `),re=r("a"),R=t("FunnelModel"),ge=t(", "),u=r("a"),k=t("FunnelForPreTraining"),Z=t(`,
`),Te=r("a"),ye=t("FunnelForMaskedLM"),D=t(", "),Fe=r("a"),we=t("FunnelForTokenClassification"),be=t(` and
class:`),x=r("em"),H=t("~transformers.FunnelForQuestionAnswering"),$e=t(`. The second ones should be used for
`),ke=r("a"),V=t("FunnelBaseModel"),Ee=t(", "),ve=r("a"),_e=t("FunnelForSequenceClassification"),Me=t(` and
`),Ya=r("a"),Ip=t("FunnelForMultipleChoice"),Sp=t("."),Ec=l(),xn=r("p"),Bp=t("This model was contributed by "),No=r("a"),Wp=t("sgugger"),Qp=t(". The original code can be found "),Io=r("a"),Up=t("here"),Rp=t("."),Mc=l(),Zn=r("h2"),Bt=r("a"),ul=r("span"),v(So.$$.fragment),Hp=l(),pl=r("span"),Vp=t("FunnelConfig"),zc=l(),Cn=r("div"),v(Bo.$$.fragment),Yp=l(),jn=r("p"),Kp=t("This is the configuration class to store the configuration of a "),Ka=r("a"),Gp=t("FunnelModel"),Zp=t(" or a "),Ga=r("a"),Xp=t("TFBertModel"),Jp=t(`. It is used to
instantiate a Funnel Transformer model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Funnel
Transformer `),Wo=r("a"),eh=t("funnel-transformer/small"),nh=t(" architecture."),th=l(),Xn=r("p"),oh=t("Configuration objects inherit from "),Za=r("a"),sh=t("PretrainedConfig"),rh=t(` and can be used to control the model outputs. Read the
documentation from `),Xa=r("a"),ah=t("PretrainedConfig"),ih=t(" for more information."),qc=l(),Jn=r("h2"),Wt=r("a"),hl=r("span"),v(Qo.$$.fragment),lh=l(),fl=r("span"),dh=t("FunnelTokenizer"),Pc=l(),Pe=r("div"),v(Uo.$$.fragment),ch=l(),ml=r("p"),uh=t("Construct a Funnel Transformer tokenizer."),ph=l(),Qt=r("p"),Ja=r("a"),hh=t("FunnelTokenizer"),fh=t(" is identical to "),ei=r("a"),mh=t("BertTokenizer"),gh=t(` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),_h=l(),Ro=r("p"),Th=t("Refer to superclass "),ni=r("a"),Fh=t("BertTokenizer"),kh=t(" for usage examples and documentation concerning parameters."),vh=l(),Ln=r("div"),v(Ho.$$.fragment),yh=l(),gl=r("p"),wh=t(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),bh=l(),Vo=r("ul"),ti=r("li"),$h=t("single sequence: "),_l=r("code"),Eh=t("[CLS] X [SEP]"),Mh=l(),oi=r("li"),zh=t("pair of sequences: "),Tl=r("code"),qh=t("[CLS] A [SEP] B [SEP]"),Ph=l(),Ut=r("div"),v(Yo.$$.fragment),Ch=l(),Ko=r("p"),jh=t(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Fl=r("code"),xh=t("prepare_for_model"),Lh=t(" method."),Oh=l(),yn=r("div"),v(Go.$$.fragment),Dh=l(),kl=r("p"),Ah=t(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),Nh=l(),v(Zo.$$.fragment),Ih=l(),et=r("p"),Sh=t("If "),vl=r("code"),Bh=t("token_ids_1"),Wh=t(" is "),yl=r("code"),Qh=t("None"),Uh=t(", this method only returns the first portion of the mask (0s)."),Rh=l(),si=r("div"),v(Xo.$$.fragment),Cc=l(),nt=r("h2"),Rt=r("a"),wl=r("span"),v(Jo.$$.fragment),Hh=l(),bl=r("span"),Vh=t("FunnelTokenizerFast"),jc=l(),Ge=r("div"),v(es.$$.fragment),Yh=l(),ns=r("p"),Kh=t("Construct a \u201Cfast\u201D Funnel Transformer tokenizer (backed by HuggingFace\u2019s "),$l=r("em"),Gh=t("tokenizers"),Zh=t(" library)."),Xh=l(),Ht=r("p"),ri=r("a"),Jh=t("FunnelTokenizerFast"),ef=t(" is identical to "),ai=r("a"),nf=t("BertTokenizerFast"),tf=t(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),of=l(),ts=r("p"),sf=t("Refer to superclass "),ii=r("a"),rf=t("BertTokenizerFast"),af=t(" for usage examples and documentation concerning parameters."),lf=l(),wn=r("div"),v(os.$$.fragment),df=l(),El=r("p"),cf=t(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),uf=l(),v(ss.$$.fragment),pf=l(),tt=r("p"),hf=t("If "),Ml=r("code"),ff=t("token_ids_1"),mf=t(" is "),zl=r("code"),gf=t("None"),_f=t(", this method only returns the first portion of the mask (0s)."),xc=l(),ot=r("h2"),Vt=r("a"),ql=r("span"),v(rs.$$.fragment),Tf=l(),Pl=r("span"),Ff=t("Funnel specific outputs"),Lc=l(),st=r("div"),v(as.$$.fragment),kf=l(),is=r("p"),vf=t("Output type of "),li=r("a"),yf=t("FunnelForPreTraining"),wf=t("."),Oc=l(),rt=r("div"),v(ls.$$.fragment),bf=l(),ds=r("p"),$f=t("Output type of "),di=r("a"),Ef=t("FunnelForPreTraining"),Mf=t("."),Dc=l(),at=r("h2"),Yt=r("a"),Cl=r("span"),v(cs.$$.fragment),zf=l(),jl=r("span"),qf=t("FunnelBaseModel"),Ac=l(),We=r("div"),v(us.$$.fragment),Pf=l(),xl=r("p"),Cf=t(`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),jf=l(),ps=r("p"),xf=t("The Funnel Transformer model was proposed in "),hs=r("a"),Lf=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Of=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Df=l(),fs=r("p"),Af=t("This model inherits from "),ci=r("a"),Nf=t("PreTrainedModel"),If=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Sf=l(),ms=r("p"),Bf=t("This model is also a PyTorch "),gs=r("a"),Wf=t("torch.nn.Module"),Qf=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Uf=l(),Ze=r("div"),v(_s.$$.fragment),Rf=l(),it=r("p"),Hf=t("The "),ui=r("a"),Vf=t("FunnelBaseModel"),Yf=t(" forward method, overrides the "),Ll=r("code"),Kf=t("__call__"),Gf=t(" special method."),Zf=l(),v(Kt.$$.fragment),Xf=l(),Ol=r("p"),Jf=t("Example:"),em=l(),v(Ts.$$.fragment),Nc=l(),lt=r("h2"),Gt=r("a"),Dl=r("span"),v(Fs.$$.fragment),nm=l(),Al=r("span"),tm=t("FunnelModel"),Ic=l(),Qe=r("div"),v(ks.$$.fragment),om=l(),Nl=r("p"),sm=t("The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),rm=l(),vs=r("p"),am=t("The Funnel Transformer model was proposed in "),ys=r("a"),im=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),lm=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),dm=l(),ws=r("p"),cm=t("This model inherits from "),pi=r("a"),um=t("PreTrainedModel"),pm=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),hm=l(),bs=r("p"),fm=t("This model is also a PyTorch "),$s=r("a"),mm=t("torch.nn.Module"),gm=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),_m=l(),Xe=r("div"),v(Es.$$.fragment),Tm=l(),dt=r("p"),Fm=t("The "),hi=r("a"),km=t("FunnelModel"),vm=t(" forward method, overrides the "),Il=r("code"),ym=t("__call__"),wm=t(" special method."),bm=l(),v(Zt.$$.fragment),$m=l(),Sl=r("p"),Em=t("Example:"),Mm=l(),v(Ms.$$.fragment),Sc=l(),ct=r("h2"),Xt=r("a"),Bl=r("span"),v(zs.$$.fragment),zm=l(),Wl=r("span"),qm=t("FunnelModelForPreTraining"),Bc=l(),ut=r("div"),v(qs.$$.fragment),Pm=l(),Je=r("div"),v(Ps.$$.fragment),Cm=l(),pt=r("p"),jm=t("The "),fi=r("a"),xm=t("FunnelForPreTraining"),Lm=t(" forward method, overrides the "),Ql=r("code"),Om=t("__call__"),Dm=t(" special method."),Am=l(),v(Jt.$$.fragment),Nm=l(),Ul=r("p"),Im=t("Examples:"),Sm=l(),v(Cs.$$.fragment),Wc=l(),ht=r("h2"),eo=r("a"),Rl=r("span"),v(js.$$.fragment),Bm=l(),Hl=r("span"),Wm=t("FunnelForMaskedLM"),Qc=l(),Ue=r("div"),v(xs.$$.fragment),Qm=l(),Ls=r("p"),Um=t("Funnel Transformer Model with a "),Vl=r("code"),Rm=t("language modeling"),Hm=t(" head on top."),Vm=l(),Os=r("p"),Ym=t("The Funnel Transformer model was proposed in "),Ds=r("a"),Km=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Gm=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Zm=l(),As=r("p"),Xm=t("This model inherits from "),mi=r("a"),Jm=t("PreTrainedModel"),eg=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ng=l(),Ns=r("p"),tg=t("This model is also a PyTorch "),Is=r("a"),og=t("torch.nn.Module"),sg=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),rg=l(),en=r("div"),v(Ss.$$.fragment),ag=l(),ft=r("p"),ig=t("The "),gi=r("a"),lg=t("FunnelForMaskedLM"),dg=t(" forward method, overrides the "),Yl=r("code"),cg=t("__call__"),ug=t(" special method."),pg=l(),v(no.$$.fragment),hg=l(),Kl=r("p"),fg=t("Example:"),mg=l(),v(Bs.$$.fragment),Uc=l(),mt=r("h2"),to=r("a"),Gl=r("span"),v(Ws.$$.fragment),gg=l(),Zl=r("span"),_g=t("FunnelForSequenceClassification"),Rc=l(),Re=r("div"),v(Qs.$$.fragment),Tg=l(),Xl=r("p"),Fg=t(`Funnel Transformer Model with a sequence classification/regression head on top (two linear layer on top of the
first timestep of the last hidden state) e.g. for GLUE tasks.`),kg=l(),Us=r("p"),vg=t("The Funnel Transformer model was proposed in "),Rs=r("a"),yg=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),wg=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),bg=l(),Hs=r("p"),$g=t("This model inherits from "),_i=r("a"),Eg=t("PreTrainedModel"),Mg=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),zg=l(),Vs=r("p"),qg=t("This model is also a PyTorch "),Ys=r("a"),Pg=t("torch.nn.Module"),Cg=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),jg=l(),Be=r("div"),v(Ks.$$.fragment),xg=l(),gt=r("p"),Lg=t("The "),Ti=r("a"),Og=t("FunnelForSequenceClassification"),Dg=t(" forward method, overrides the "),Jl=r("code"),Ag=t("__call__"),Ng=t(" special method."),Ig=l(),v(oo.$$.fragment),Sg=l(),ed=r("p"),Bg=t("Example of single-label classification:"),Wg=l(),v(Gs.$$.fragment),Qg=l(),nd=r("p"),Ug=t("Example of multi-label classification:"),Rg=l(),v(Zs.$$.fragment),Hc=l(),_t=r("h2"),so=r("a"),td=r("span"),v(Xs.$$.fragment),Hg=l(),od=r("span"),Vg=t("FunnelForMultipleChoice"),Vc=l(),He=r("div"),v(Js.$$.fragment),Yg=l(),sd=r("p"),Kg=t(`Funnel Transformer Model with a multiple choice classification head on top (two linear layer on top of the first
timestep of the last hidden state, and a softmax) e.g. for RocStories/SWAG tasks.`),Gg=l(),er=r("p"),Zg=t("The Funnel Transformer model was proposed in "),nr=r("a"),Xg=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Jg=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),e_=l(),tr=r("p"),n_=t("This model inherits from "),Fi=r("a"),t_=t("PreTrainedModel"),o_=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),s_=l(),or=r("p"),r_=t("This model is also a PyTorch "),sr=r("a"),a_=t("torch.nn.Module"),i_=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),l_=l(),nn=r("div"),v(rr.$$.fragment),d_=l(),Tt=r("p"),c_=t("The "),ki=r("a"),u_=t("FunnelForMultipleChoice"),p_=t(" forward method, overrides the "),rd=r("code"),h_=t("__call__"),f_=t(" special method."),m_=l(),v(ro.$$.fragment),g_=l(),ad=r("p"),__=t("Example:"),T_=l(),v(ar.$$.fragment),Yc=l(),Ft=r("h2"),ao=r("a"),id=r("span"),v(ir.$$.fragment),F_=l(),ld=r("span"),k_=t("FunnelForTokenClassification"),Kc=l(),Ve=r("div"),v(lr.$$.fragment),v_=l(),dd=r("p"),y_=t(`Funnel Transformer Model with a token classification head on top (a linear layer on top of the hidden-states
output) e.g. for Named-Entity-Recognition (NER) tasks.`),w_=l(),dr=r("p"),b_=t("The Funnel Transformer model was proposed in "),cr=r("a"),$_=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),E_=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),M_=l(),ur=r("p"),z_=t("This model inherits from "),vi=r("a"),q_=t("PreTrainedModel"),P_=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),C_=l(),pr=r("p"),j_=t("This model is also a PyTorch "),hr=r("a"),x_=t("torch.nn.Module"),L_=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),O_=l(),tn=r("div"),v(fr.$$.fragment),D_=l(),kt=r("p"),A_=t("The "),yi=r("a"),N_=t("FunnelForTokenClassification"),I_=t(" forward method, overrides the "),cd=r("code"),S_=t("__call__"),B_=t(" special method."),W_=l(),v(io.$$.fragment),Q_=l(),ud=r("p"),U_=t("Example:"),R_=l(),v(mr.$$.fragment),Gc=l(),vt=r("h2"),lo=r("a"),pd=r("span"),v(gr.$$.fragment),H_=l(),hd=r("span"),V_=t("FunnelForQuestionAnswering"),Zc=l(),Ye=r("div"),v(_r.$$.fragment),Y_=l(),yt=r("p"),K_=t(`Funnel Transformer Model with a span classification head on top for extractive question-answering tasks like SQuAD
(a linear layer on top of the hidden-states output to compute `),fd=r("code"),G_=t("span start logits"),Z_=t(" and "),md=r("code"),X_=t("span end logits"),J_=t(")."),eT=l(),Tr=r("p"),nT=t("The Funnel Transformer model was proposed in "),Fr=r("a"),tT=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),oT=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),sT=l(),kr=r("p"),rT=t("This model inherits from "),wi=r("a"),aT=t("PreTrainedModel"),iT=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),lT=l(),vr=r("p"),dT=t("This model is also a PyTorch "),yr=r("a"),cT=t("torch.nn.Module"),uT=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),pT=l(),on=r("div"),v(wr.$$.fragment),hT=l(),wt=r("p"),fT=t("The "),bi=r("a"),mT=t("FunnelForQuestionAnswering"),gT=t(" forward method, overrides the "),gd=r("code"),_T=t("__call__"),TT=t(" special method."),FT=l(),v(co.$$.fragment),kT=l(),_d=r("p"),vT=t("Example:"),yT=l(),v(br.$$.fragment),Xc=l(),bt=r("h2"),uo=r("a"),Td=r("span"),v($r.$$.fragment),wT=l(),Fd=r("span"),bT=t("TFFunnelBaseModel"),Jc=l(),je=r("div"),v(Er.$$.fragment),$T=l(),kd=r("p"),ET=t(`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),MT=l(),Mr=r("p"),zT=t("The Funnel Transformer model was proposed in "),zr=r("a"),qT=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),PT=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),CT=l(),qr=r("p"),jT=t("This model inherits from "),$i=r("a"),xT=t("TFPreTrainedModel"),LT=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),OT=l(),Pr=r("p"),DT=t("This model is also a "),Cr=r("a"),AT=t("tf.keras.Model"),NT=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),IT=l(),v(po.$$.fragment),ST=l(),sn=r("div"),v(jr.$$.fragment),BT=l(),$t=r("p"),WT=t("The "),Ei=r("a"),QT=t("TFFunnelBaseModel"),UT=t(" forward method, overrides the "),vd=r("code"),RT=t("__call__"),HT=t(" special method."),VT=l(),v(ho.$$.fragment),YT=l(),yd=r("p"),KT=t("Example:"),GT=l(),v(xr.$$.fragment),eu=l(),Et=r("h2"),fo=r("a"),wd=r("span"),v(Lr.$$.fragment),ZT=l(),bd=r("span"),XT=t("TFFunnelModel"),nu=l(),xe=r("div"),v(Or.$$.fragment),JT=l(),$d=r("p"),eF=t("The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),nF=l(),Dr=r("p"),tF=t("The Funnel Transformer model was proposed in "),Ar=r("a"),oF=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),sF=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),rF=l(),Nr=r("p"),aF=t("This model inherits from "),Mi=r("a"),iF=t("TFPreTrainedModel"),lF=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),dF=l(),Ir=r("p"),cF=t("This model is also a "),Sr=r("a"),uF=t("tf.keras.Model"),pF=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),hF=l(),v(mo.$$.fragment),fF=l(),rn=r("div"),v(Br.$$.fragment),mF=l(),Mt=r("p"),gF=t("The "),zi=r("a"),_F=t("TFFunnelModel"),TF=t(" forward method, overrides the "),Ed=r("code"),FF=t("__call__"),kF=t(" special method."),vF=l(),v(go.$$.fragment),yF=l(),Md=r("p"),wF=t("Example:"),bF=l(),v(Wr.$$.fragment),tu=l(),zt=r("h2"),_o=r("a"),zd=r("span"),v(Qr.$$.fragment),$F=l(),qd=r("span"),EF=t("TFFunnelModelForPreTraining"),ou=l(),Le=r("div"),v(Ur.$$.fragment),MF=l(),Pd=r("p"),zF=t("Funnel model with a binary classification head on top as used during pretraining for identifying generated tokens."),qF=l(),Rr=r("p"),PF=t("The Funnel Transformer model was proposed in "),Hr=r("a"),CF=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),jF=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),xF=l(),Vr=r("p"),LF=t("This model inherits from "),qi=r("a"),OF=t("TFPreTrainedModel"),DF=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),AF=l(),Yr=r("p"),NF=t("This model is also a "),Kr=r("a"),IF=t("tf.keras.Model"),SF=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),BF=l(),v(To.$$.fragment),WF=l(),an=r("div"),v(Gr.$$.fragment),QF=l(),qt=r("p"),UF=t("The "),Pi=r("a"),RF=t("TFFunnelForPreTraining"),HF=t(" forward method, overrides the "),Cd=r("code"),VF=t("__call__"),YF=t(" special method."),KF=l(),v(Fo.$$.fragment),GF=l(),jd=r("p"),ZF=t("Examples:"),XF=l(),v(Zr.$$.fragment),su=l(),Pt=r("h2"),ko=r("a"),xd=r("span"),v(Xr.$$.fragment),JF=l(),Ld=r("span"),ek=t("TFFunnelForMaskedLM"),ru=l(),Oe=r("div"),v(Jr.$$.fragment),nk=l(),ea=r("p"),tk=t("Funnel Model with a "),Od=r("code"),ok=t("language modeling"),sk=t(" head on top."),rk=l(),na=r("p"),ak=t("The Funnel Transformer model was proposed in "),ta=r("a"),ik=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),lk=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),dk=l(),oa=r("p"),ck=t("This model inherits from "),Ci=r("a"),uk=t("TFPreTrainedModel"),pk=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),hk=l(),sa=r("p"),fk=t("This model is also a "),ra=r("a"),mk=t("tf.keras.Model"),gk=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),_k=l(),v(vo.$$.fragment),Tk=l(),ln=r("div"),v(aa.$$.fragment),Fk=l(),Ct=r("p"),kk=t("The "),ji=r("a"),vk=t("TFFunnelForMaskedLM"),yk=t(" forward method, overrides the "),Dd=r("code"),wk=t("__call__"),bk=t(" special method."),$k=l(),v(yo.$$.fragment),Ek=l(),Ad=r("p"),Mk=t("Example:"),zk=l(),v(ia.$$.fragment),au=l(),jt=r("h2"),wo=r("a"),Nd=r("span"),v(la.$$.fragment),qk=l(),Id=r("span"),Pk=t("TFFunnelForSequenceClassification"),iu=l(),De=r("div"),v(da.$$.fragment),Ck=l(),Sd=r("p"),jk=t(`Funnel Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
output) e.g. for GLUE tasks.`),xk=l(),ca=r("p"),Lk=t("The Funnel Transformer model was proposed in "),ua=r("a"),Ok=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Dk=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Ak=l(),pa=r("p"),Nk=t("This model inherits from "),xi=r("a"),Ik=t("TFPreTrainedModel"),Sk=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Bk=l(),ha=r("p"),Wk=t("This model is also a "),fa=r("a"),Qk=t("tf.keras.Model"),Uk=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Rk=l(),v(bo.$$.fragment),Hk=l(),dn=r("div"),v(ma.$$.fragment),Vk=l(),xt=r("p"),Yk=t("The "),Li=r("a"),Kk=t("TFFunnelForSequenceClassification"),Gk=t(" forward method, overrides the "),Bd=r("code"),Zk=t("__call__"),Xk=t(" special method."),Jk=l(),v($o.$$.fragment),ev=l(),Wd=r("p"),nv=t("Example:"),tv=l(),v(ga.$$.fragment),lu=l(),Lt=r("h2"),Eo=r("a"),Qd=r("span"),v(_a.$$.fragment),ov=l(),Ud=r("span"),sv=t("TFFunnelForMultipleChoice"),du=l(),Ae=r("div"),v(Ta.$$.fragment),rv=l(),Rd=r("p"),av=t(`Funnel Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),iv=l(),Fa=r("p"),lv=t("The Funnel Transformer model was proposed in "),ka=r("a"),dv=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),cv=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),uv=l(),va=r("p"),pv=t("This model inherits from "),Oi=r("a"),hv=t("TFPreTrainedModel"),fv=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),mv=l(),ya=r("p"),gv=t("This model is also a "),wa=r("a"),_v=t("tf.keras.Model"),Tv=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Fv=l(),v(Mo.$$.fragment),kv=l(),cn=r("div"),v(ba.$$.fragment),vv=l(),Ot=r("p"),yv=t("The "),Di=r("a"),wv=t("TFFunnelForMultipleChoice"),bv=t(" forward method, overrides the "),Hd=r("code"),$v=t("__call__"),Ev=t(" special method."),Mv=l(),v(zo.$$.fragment),zv=l(),Vd=r("p"),qv=t("Example:"),Pv=l(),v($a.$$.fragment),cu=l(),Dt=r("h2"),qo=r("a"),Yd=r("span"),v(Ea.$$.fragment),Cv=l(),Kd=r("span"),jv=t("TFFunnelForTokenClassification"),uu=l(),Ne=r("div"),v(Ma.$$.fragment),xv=l(),Gd=r("p"),Lv=t(`Funnel Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),Ov=l(),za=r("p"),Dv=t("The Funnel Transformer model was proposed in "),qa=r("a"),Av=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Nv=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Iv=l(),Pa=r("p"),Sv=t("This model inherits from "),Ai=r("a"),Bv=t("TFPreTrainedModel"),Wv=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Qv=l(),Ca=r("p"),Uv=t("This model is also a "),ja=r("a"),Rv=t("tf.keras.Model"),Hv=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Vv=l(),v(Po.$$.fragment),Yv=l(),un=r("div"),v(xa.$$.fragment),Kv=l(),At=r("p"),Gv=t("The "),Ni=r("a"),Zv=t("TFFunnelForTokenClassification"),Xv=t(" forward method, overrides the "),Zd=r("code"),Jv=t("__call__"),ey=t(" special method."),ny=l(),v(Co.$$.fragment),ty=l(),Xd=r("p"),oy=t("Example:"),sy=l(),v(La.$$.fragment),pu=l(),Nt=r("h2"),jo=r("a"),Jd=r("span"),v(Oa.$$.fragment),ry=l(),ec=r("span"),ay=t("TFFunnelForQuestionAnswering"),hu=l(),Ie=r("div"),v(Da.$$.fragment),iy=l(),It=r("p"),ly=t(`Funnel Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),nc=r("code"),dy=t("span start logits"),cy=t(" and "),tc=r("code"),uy=t("span end logits"),py=t(")."),hy=l(),Aa=r("p"),fy=t("The Funnel Transformer model was proposed in "),Na=r("a"),my=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),gy=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),_y=l(),Ia=r("p"),Ty=t("This model inherits from "),Ii=r("a"),Fy=t("TFPreTrainedModel"),ky=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),vy=l(),Sa=r("p"),yy=t("This model is also a "),Ba=r("a"),wy=t("tf.keras.Model"),by=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),$y=l(),v(xo.$$.fragment),Ey=l(),pn=r("div"),v(Wa.$$.fragment),My=l(),St=r("p"),zy=t("The "),Si=r("a"),qy=t("TFFunnelForQuestionAnswering"),Py=t(" forward method, overrides the "),oc=r("code"),Cy=t("__call__"),jy=t(" special method."),xy=l(),v(Lo.$$.fragment),Ly=l(),sc=r("p"),Oy=t("Example:"),Dy=l(),v(Qa.$$.fragment),this.h()},l(s){const f=E$('[data-svelte="svelte-1phssyn"]',document.head);p=a(f,"META",{name:!0,content:!0}),f.forEach(n),M=d(s),m=a(s,"H1",{class:!0});var Ua=i(m);g=a(Ua,"A",{id:!0,class:!0,href:!0});var rc=i(g);F=a(rc,"SPAN",{});var ac=i(F);y(T.$$.fragment,ac),ac.forEach(n),rc.forEach(n),_=d(Ua),z=a(Ua,"SPAN",{});var ic=i(z);ce=o(ic,"Funnel Transformer"),ic.forEach(n),Ua.forEach(n),K=d(s),q=a(s,"H2",{class:!0});var Ra=i(q);J=a(Ra,"A",{id:!0,class:!0,href:!0});var lc=i(J);A=a(lc,"SPAN",{});var dc=i(A);y(ne.$$.fragment,dc),dc.forEach(n),lc.forEach(n),ue=d(Ra),N=a(Ra,"SPAN",{});var cc=i(N);pe=o(cc,"Overview"),cc.forEach(n),Ra.forEach(n),ie=d(s),Y=a(s,"P",{});var Ha=i(Y);L=o(Ha,"The Funnel Transformer model was proposed in the paper "),te=a(Ha,"A",{href:!0,rel:!0});var uc=i(te);G=o(uc,`Funnel-Transformer: Filtering out Sequential Redundancy for
Efficient Language Processing`),uc.forEach(n),P=o(Ha,`. It is a bidirectional transformer model, like
BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks
(CNN) in computer vision.`),Ha.forEach(n),j=d(s),oe=a(s,"P",{});var pc=i(oe);Q=o(pc,"The abstract from the paper is the following:"),pc.forEach(n),le=d(s),se=a(s,"P",{});var hc=i(se);I=a(hc,"EM",{});var fc=i(I);he=o(fc,`With the success of language pretraining, it is highly desirable to develop more efficient architectures of good
scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the
much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only
require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which
gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More
importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further
improve the model capacity. In addition, to perform token-level predictions as required by common pretraining
objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence
via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on
a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading
comprehension.`),fc.forEach(n),hc.forEach(n),de=d(s),C=a(s,"P",{});var mc=i(C);fe=o(mc,"Tips:"),mc.forEach(n),B=d(s),ee=a(s,"UL",{});var Va=i(ee);ae=a(Va,"LI",{});var gc=i(ae);U=o(gc,`Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers.
The base model therefore has a final sequence length that is a quarter of the original one. This model can be used
directly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other
tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same
sequence length as the input.`),gc.forEach(n),me=d(Va),S=a(Va,"LI",{});var Ce=i(S);O=o(Ce,`The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be
used for `),re=a(Ce,"A",{href:!0});var _c=i(re);R=o(_c,"FunnelModel"),_c.forEach(n),ge=o(Ce,", "),u=a(Ce,"A",{href:!0});var Tc=i(u);k=o(Tc,"FunnelForPreTraining"),Tc.forEach(n),Z=o(Ce,`,
`),Te=a(Ce,"A",{href:!0});var Fc=i(Te);ye=o(Fc,"FunnelForMaskedLM"),Fc.forEach(n),D=o(Ce,", "),Fe=a(Ce,"A",{href:!0});var kc=i(Fe);we=o(kc,"FunnelForTokenClassification"),kc.forEach(n),be=o(Ce,` and
class:`),x=a(Ce,"EM",{});var vc=i(x);H=o(vc,"~transformers.FunnelForQuestionAnswering"),vc.forEach(n),$e=o(Ce,`. The second ones should be used for
`),ke=a(Ce,"A",{href:!0});var yc=i(ke);V=o(yc,"FunnelBaseModel"),yc.forEach(n),Ee=o(Ce,", "),ve=a(Ce,"A",{href:!0});var wc=i(ve);_e=o(wc,"FunnelForSequenceClassification"),wc.forEach(n),Me=o(Ce,` and
`),Ya=a(Ce,"A",{href:!0});var Iy=i(Ya);Ip=o(Iy,"FunnelForMultipleChoice"),Iy.forEach(n),Sp=o(Ce,"."),Ce.forEach(n),Va.forEach(n),Ec=d(s),xn=a(s,"P",{});var Bi=i(xn);Bp=o(Bi,"This model was contributed by "),No=a(Bi,"A",{href:!0,rel:!0});var Sy=i(No);Wp=o(Sy,"sgugger"),Sy.forEach(n),Qp=o(Bi,". The original code can be found "),Io=a(Bi,"A",{href:!0,rel:!0});var By=i(Io);Up=o(By,"here"),By.forEach(n),Rp=o(Bi,"."),Bi.forEach(n),Mc=d(s),Zn=a(s,"H2",{class:!0});var mu=i(Zn);Bt=a(mu,"A",{id:!0,class:!0,href:!0});var Wy=i(Bt);ul=a(Wy,"SPAN",{});var Qy=i(ul);y(So.$$.fragment,Qy),Qy.forEach(n),Wy.forEach(n),Hp=d(mu),pl=a(mu,"SPAN",{});var Uy=i(pl);Vp=o(Uy,"FunnelConfig"),Uy.forEach(n),mu.forEach(n),zc=d(s),Cn=a(s,"DIV",{class:!0});var Wi=i(Cn);y(Bo.$$.fragment,Wi),Yp=d(Wi),jn=a(Wi,"P",{});var Oo=i(jn);Kp=o(Oo,"This is the configuration class to store the configuration of a "),Ka=a(Oo,"A",{href:!0});var Ry=i(Ka);Gp=o(Ry,"FunnelModel"),Ry.forEach(n),Zp=o(Oo," or a "),Ga=a(Oo,"A",{href:!0});var Hy=i(Ga);Xp=o(Hy,"TFBertModel"),Hy.forEach(n),Jp=o(Oo,`. It is used to
instantiate a Funnel Transformer model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Funnel
Transformer `),Wo=a(Oo,"A",{href:!0,rel:!0});var Vy=i(Wo);eh=o(Vy,"funnel-transformer/small"),Vy.forEach(n),nh=o(Oo," architecture."),Oo.forEach(n),th=d(Wi),Xn=a(Wi,"P",{});var Qi=i(Xn);oh=o(Qi,"Configuration objects inherit from "),Za=a(Qi,"A",{href:!0});var Yy=i(Za);sh=o(Yy,"PretrainedConfig"),Yy.forEach(n),rh=o(Qi,` and can be used to control the model outputs. Read the
documentation from `),Xa=a(Qi,"A",{href:!0});var Ky=i(Xa);ah=o(Ky,"PretrainedConfig"),Ky.forEach(n),ih=o(Qi," for more information."),Qi.forEach(n),Wi.forEach(n),qc=d(s),Jn=a(s,"H2",{class:!0});var gu=i(Jn);Wt=a(gu,"A",{id:!0,class:!0,href:!0});var Gy=i(Wt);hl=a(Gy,"SPAN",{});var Zy=i(hl);y(Qo.$$.fragment,Zy),Zy.forEach(n),Gy.forEach(n),lh=d(gu),fl=a(gu,"SPAN",{});var Xy=i(fl);dh=o(Xy,"FunnelTokenizer"),Xy.forEach(n),gu.forEach(n),Pc=d(s),Pe=a(s,"DIV",{class:!0});var Ke=i(Pe);y(Uo.$$.fragment,Ke),ch=d(Ke),ml=a(Ke,"P",{});var Jy=i(ml);uh=o(Jy,"Construct a Funnel Transformer tokenizer."),Jy.forEach(n),ph=d(Ke),Qt=a(Ke,"P",{});var bc=i(Qt);Ja=a(bc,"A",{href:!0});var ew=i(Ja);hh=o(ew,"FunnelTokenizer"),ew.forEach(n),fh=o(bc," is identical to "),ei=a(bc,"A",{href:!0});var nw=i(ei);mh=o(nw,"BertTokenizer"),nw.forEach(n),gh=o(bc,` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),bc.forEach(n),_h=d(Ke),Ro=a(Ke,"P",{});var _u=i(Ro);Th=o(_u,"Refer to superclass "),ni=a(_u,"A",{href:!0});var tw=i(ni);Fh=o(tw,"BertTokenizer"),tw.forEach(n),kh=o(_u," for usage examples and documentation concerning parameters."),_u.forEach(n),vh=d(Ke),Ln=a(Ke,"DIV",{class:!0});var Ui=i(Ln);y(Ho.$$.fragment,Ui),yh=d(Ui),gl=a(Ui,"P",{});var ow=i(gl);wh=o(ow,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),ow.forEach(n),bh=d(Ui),Vo=a(Ui,"UL",{});var Tu=i(Vo);ti=a(Tu,"LI",{});var Ay=i(ti);$h=o(Ay,"single sequence: "),_l=a(Ay,"CODE",{});var sw=i(_l);Eh=o(sw,"[CLS] X [SEP]"),sw.forEach(n),Ay.forEach(n),Mh=d(Tu),oi=a(Tu,"LI",{});var Ny=i(oi);zh=o(Ny,"pair of sequences: "),Tl=a(Ny,"CODE",{});var rw=i(Tl);qh=o(rw,"[CLS] A [SEP] B [SEP]"),rw.forEach(n),Ny.forEach(n),Tu.forEach(n),Ui.forEach(n),Ph=d(Ke),Ut=a(Ke,"DIV",{class:!0});var Fu=i(Ut);y(Yo.$$.fragment,Fu),Ch=d(Fu),Ko=a(Fu,"P",{});var ku=i(Ko);jh=o(ku,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Fl=a(ku,"CODE",{});var aw=i(Fl);xh=o(aw,"prepare_for_model"),aw.forEach(n),Lh=o(ku," method."),ku.forEach(n),Fu.forEach(n),Oh=d(Ke),yn=a(Ke,"DIV",{class:!0});var Do=i(yn);y(Go.$$.fragment,Do),Dh=d(Do),kl=a(Do,"P",{});var iw=i(kl);Ah=o(iw,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),iw.forEach(n),Nh=d(Do),y(Zo.$$.fragment,Do),Ih=d(Do),et=a(Do,"P",{});var Ri=i(et);Sh=o(Ri,"If "),vl=a(Ri,"CODE",{});var lw=i(vl);Bh=o(lw,"token_ids_1"),lw.forEach(n),Wh=o(Ri," is "),yl=a(Ri,"CODE",{});var dw=i(yl);Qh=o(dw,"None"),dw.forEach(n),Uh=o(Ri,", this method only returns the first portion of the mask (0s)."),Ri.forEach(n),Do.forEach(n),Rh=d(Ke),si=a(Ke,"DIV",{class:!0});var cw=i(si);y(Xo.$$.fragment,cw),cw.forEach(n),Ke.forEach(n),Cc=d(s),nt=a(s,"H2",{class:!0});var vu=i(nt);Rt=a(vu,"A",{id:!0,class:!0,href:!0});var uw=i(Rt);wl=a(uw,"SPAN",{});var pw=i(wl);y(Jo.$$.fragment,pw),pw.forEach(n),uw.forEach(n),Hh=d(vu),bl=a(vu,"SPAN",{});var hw=i(bl);Vh=o(hw,"FunnelTokenizerFast"),hw.forEach(n),vu.forEach(n),jc=d(s),Ge=a(s,"DIV",{class:!0});var On=i(Ge);y(es.$$.fragment,On),Yh=d(On),ns=a(On,"P",{});var yu=i(ns);Kh=o(yu,"Construct a \u201Cfast\u201D Funnel Transformer tokenizer (backed by HuggingFace\u2019s "),$l=a(yu,"EM",{});var fw=i($l);Gh=o(fw,"tokenizers"),fw.forEach(n),Zh=o(yu," library)."),yu.forEach(n),Xh=d(On),Ht=a(On,"P",{});var $c=i(Ht);ri=a($c,"A",{href:!0});var mw=i(ri);Jh=o(mw,"FunnelTokenizerFast"),mw.forEach(n),ef=o($c," is identical to "),ai=a($c,"A",{href:!0});var gw=i(ai);nf=o(gw,"BertTokenizerFast"),gw.forEach(n),tf=o($c,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),$c.forEach(n),of=d(On),ts=a(On,"P",{});var wu=i(ts);sf=o(wu,"Refer to superclass "),ii=a(wu,"A",{href:!0});var _w=i(ii);rf=o(_w,"BertTokenizerFast"),_w.forEach(n),af=o(wu," for usage examples and documentation concerning parameters."),wu.forEach(n),lf=d(On),wn=a(On,"DIV",{class:!0});var Ao=i(wn);y(os.$$.fragment,Ao),df=d(Ao),El=a(Ao,"P",{});var Tw=i(El);cf=o(Tw,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),Tw.forEach(n),uf=d(Ao),y(ss.$$.fragment,Ao),pf=d(Ao),tt=a(Ao,"P",{});var Hi=i(tt);hf=o(Hi,"If "),Ml=a(Hi,"CODE",{});var Fw=i(Ml);ff=o(Fw,"token_ids_1"),Fw.forEach(n),mf=o(Hi," is "),zl=a(Hi,"CODE",{});var kw=i(zl);gf=o(kw,"None"),kw.forEach(n),_f=o(Hi,", this method only returns the first portion of the mask (0s)."),Hi.forEach(n),Ao.forEach(n),On.forEach(n),xc=d(s),ot=a(s,"H2",{class:!0});var bu=i(ot);Vt=a(bu,"A",{id:!0,class:!0,href:!0});var vw=i(Vt);ql=a(vw,"SPAN",{});var yw=i(ql);y(rs.$$.fragment,yw),yw.forEach(n),vw.forEach(n),Tf=d(bu),Pl=a(bu,"SPAN",{});var ww=i(Pl);Ff=o(ww,"Funnel specific outputs"),ww.forEach(n),bu.forEach(n),Lc=d(s),st=a(s,"DIV",{class:!0});var $u=i(st);y(as.$$.fragment,$u),kf=d($u),is=a($u,"P",{});var Eu=i(is);vf=o(Eu,"Output type of "),li=a(Eu,"A",{href:!0});var bw=i(li);yf=o(bw,"FunnelForPreTraining"),bw.forEach(n),wf=o(Eu,"."),Eu.forEach(n),$u.forEach(n),Oc=d(s),rt=a(s,"DIV",{class:!0});var Mu=i(rt);y(ls.$$.fragment,Mu),bf=d(Mu),ds=a(Mu,"P",{});var zu=i(ds);$f=o(zu,"Output type of "),di=a(zu,"A",{href:!0});var $w=i(di);Ef=o($w,"FunnelForPreTraining"),$w.forEach(n),Mf=o(zu,"."),zu.forEach(n),Mu.forEach(n),Dc=d(s),at=a(s,"H2",{class:!0});var qu=i(at);Yt=a(qu,"A",{id:!0,class:!0,href:!0});var Ew=i(Yt);Cl=a(Ew,"SPAN",{});var Mw=i(Cl);y(cs.$$.fragment,Mw),Mw.forEach(n),Ew.forEach(n),zf=d(qu),jl=a(qu,"SPAN",{});var zw=i(jl);qf=o(zw,"FunnelBaseModel"),zw.forEach(n),qu.forEach(n),Ac=d(s),We=a(s,"DIV",{class:!0});var bn=i(We);y(us.$$.fragment,bn),Pf=d(bn),xl=a(bn,"P",{});var qw=i(xl);Cf=o(qw,`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),qw.forEach(n),jf=d(bn),ps=a(bn,"P",{});var Pu=i(ps);xf=o(Pu,"The Funnel Transformer model was proposed in "),hs=a(Pu,"A",{href:!0,rel:!0});var Pw=i(hs);Lf=o(Pw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Pw.forEach(n),Of=o(Pu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Pu.forEach(n),Df=d(bn),fs=a(bn,"P",{});var Cu=i(fs);Af=o(Cu,"This model inherits from "),ci=a(Cu,"A",{href:!0});var Cw=i(ci);Nf=o(Cw,"PreTrainedModel"),Cw.forEach(n),If=o(Cu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Cu.forEach(n),Sf=d(bn),ms=a(bn,"P",{});var ju=i(ms);Bf=o(ju,"This model is also a PyTorch "),gs=a(ju,"A",{href:!0,rel:!0});var jw=i(gs);Wf=o(jw,"torch.nn.Module"),jw.forEach(n),Qf=o(ju,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ju.forEach(n),Uf=d(bn),Ze=a(bn,"DIV",{class:!0});var Dn=i(Ze);y(_s.$$.fragment,Dn),Rf=d(Dn),it=a(Dn,"P",{});var Vi=i(it);Hf=o(Vi,"The "),ui=a(Vi,"A",{href:!0});var xw=i(ui);Vf=o(xw,"FunnelBaseModel"),xw.forEach(n),Yf=o(Vi," forward method, overrides the "),Ll=a(Vi,"CODE",{});var Lw=i(Ll);Kf=o(Lw,"__call__"),Lw.forEach(n),Gf=o(Vi," special method."),Vi.forEach(n),Zf=d(Dn),y(Kt.$$.fragment,Dn),Xf=d(Dn),Ol=a(Dn,"P",{});var Ow=i(Ol);Jf=o(Ow,"Example:"),Ow.forEach(n),em=d(Dn),y(Ts.$$.fragment,Dn),Dn.forEach(n),bn.forEach(n),Nc=d(s),lt=a(s,"H2",{class:!0});var xu=i(lt);Gt=a(xu,"A",{id:!0,class:!0,href:!0});var Dw=i(Gt);Dl=a(Dw,"SPAN",{});var Aw=i(Dl);y(Fs.$$.fragment,Aw),Aw.forEach(n),Dw.forEach(n),nm=d(xu),Al=a(xu,"SPAN",{});var Nw=i(Al);tm=o(Nw,"FunnelModel"),Nw.forEach(n),xu.forEach(n),Ic=d(s),Qe=a(s,"DIV",{class:!0});var $n=i(Qe);y(ks.$$.fragment,$n),om=d($n),Nl=a($n,"P",{});var Iw=i(Nl);sm=o(Iw,"The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),Iw.forEach(n),rm=d($n),vs=a($n,"P",{});var Lu=i(vs);am=o(Lu,"The Funnel Transformer model was proposed in "),ys=a(Lu,"A",{href:!0,rel:!0});var Sw=i(ys);im=o(Sw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Sw.forEach(n),lm=o(Lu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Lu.forEach(n),dm=d($n),ws=a($n,"P",{});var Ou=i(ws);cm=o(Ou,"This model inherits from "),pi=a(Ou,"A",{href:!0});var Bw=i(pi);um=o(Bw,"PreTrainedModel"),Bw.forEach(n),pm=o(Ou,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ou.forEach(n),hm=d($n),bs=a($n,"P",{});var Du=i(bs);fm=o(Du,"This model is also a PyTorch "),$s=a(Du,"A",{href:!0,rel:!0});var Ww=i($s);mm=o(Ww,"torch.nn.Module"),Ww.forEach(n),gm=o(Du,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Du.forEach(n),_m=d($n),Xe=a($n,"DIV",{class:!0});var An=i(Xe);y(Es.$$.fragment,An),Tm=d(An),dt=a(An,"P",{});var Yi=i(dt);Fm=o(Yi,"The "),hi=a(Yi,"A",{href:!0});var Qw=i(hi);km=o(Qw,"FunnelModel"),Qw.forEach(n),vm=o(Yi," forward method, overrides the "),Il=a(Yi,"CODE",{});var Uw=i(Il);ym=o(Uw,"__call__"),Uw.forEach(n),wm=o(Yi," special method."),Yi.forEach(n),bm=d(An),y(Zt.$$.fragment,An),$m=d(An),Sl=a(An,"P",{});var Rw=i(Sl);Em=o(Rw,"Example:"),Rw.forEach(n),Mm=d(An),y(Ms.$$.fragment,An),An.forEach(n),$n.forEach(n),Sc=d(s),ct=a(s,"H2",{class:!0});var Au=i(ct);Xt=a(Au,"A",{id:!0,class:!0,href:!0});var Hw=i(Xt);Bl=a(Hw,"SPAN",{});var Vw=i(Bl);y(zs.$$.fragment,Vw),Vw.forEach(n),Hw.forEach(n),zm=d(Au),Wl=a(Au,"SPAN",{});var Yw=i(Wl);qm=o(Yw,"FunnelModelForPreTraining"),Yw.forEach(n),Au.forEach(n),Bc=d(s),ut=a(s,"DIV",{class:!0});var Nu=i(ut);y(qs.$$.fragment,Nu),Pm=d(Nu),Je=a(Nu,"DIV",{class:!0});var Nn=i(Je);y(Ps.$$.fragment,Nn),Cm=d(Nn),pt=a(Nn,"P",{});var Ki=i(pt);jm=o(Ki,"The "),fi=a(Ki,"A",{href:!0});var Kw=i(fi);xm=o(Kw,"FunnelForPreTraining"),Kw.forEach(n),Lm=o(Ki," forward method, overrides the "),Ql=a(Ki,"CODE",{});var Gw=i(Ql);Om=o(Gw,"__call__"),Gw.forEach(n),Dm=o(Ki," special method."),Ki.forEach(n),Am=d(Nn),y(Jt.$$.fragment,Nn),Nm=d(Nn),Ul=a(Nn,"P",{});var Zw=i(Ul);Im=o(Zw,"Examples:"),Zw.forEach(n),Sm=d(Nn),y(Cs.$$.fragment,Nn),Nn.forEach(n),Nu.forEach(n),Wc=d(s),ht=a(s,"H2",{class:!0});var Iu=i(ht);eo=a(Iu,"A",{id:!0,class:!0,href:!0});var Xw=i(eo);Rl=a(Xw,"SPAN",{});var Jw=i(Rl);y(js.$$.fragment,Jw),Jw.forEach(n),Xw.forEach(n),Bm=d(Iu),Hl=a(Iu,"SPAN",{});var eb=i(Hl);Wm=o(eb,"FunnelForMaskedLM"),eb.forEach(n),Iu.forEach(n),Qc=d(s),Ue=a(s,"DIV",{class:!0});var En=i(Ue);y(xs.$$.fragment,En),Qm=d(En),Ls=a(En,"P",{});var Su=i(Ls);Um=o(Su,"Funnel Transformer Model with a "),Vl=a(Su,"CODE",{});var nb=i(Vl);Rm=o(nb,"language modeling"),nb.forEach(n),Hm=o(Su," head on top."),Su.forEach(n),Vm=d(En),Os=a(En,"P",{});var Bu=i(Os);Ym=o(Bu,"The Funnel Transformer model was proposed in "),Ds=a(Bu,"A",{href:!0,rel:!0});var tb=i(Ds);Km=o(tb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),tb.forEach(n),Gm=o(Bu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Bu.forEach(n),Zm=d(En),As=a(En,"P",{});var Wu=i(As);Xm=o(Wu,"This model inherits from "),mi=a(Wu,"A",{href:!0});var ob=i(mi);Jm=o(ob,"PreTrainedModel"),ob.forEach(n),eg=o(Wu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Wu.forEach(n),ng=d(En),Ns=a(En,"P",{});var Qu=i(Ns);tg=o(Qu,"This model is also a PyTorch "),Is=a(Qu,"A",{href:!0,rel:!0});var sb=i(Is);og=o(sb,"torch.nn.Module"),sb.forEach(n),sg=o(Qu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Qu.forEach(n),rg=d(En),en=a(En,"DIV",{class:!0});var In=i(en);y(Ss.$$.fragment,In),ag=d(In),ft=a(In,"P",{});var Gi=i(ft);ig=o(Gi,"The "),gi=a(Gi,"A",{href:!0});var rb=i(gi);lg=o(rb,"FunnelForMaskedLM"),rb.forEach(n),dg=o(Gi," forward method, overrides the "),Yl=a(Gi,"CODE",{});var ab=i(Yl);cg=o(ab,"__call__"),ab.forEach(n),ug=o(Gi," special method."),Gi.forEach(n),pg=d(In),y(no.$$.fragment,In),hg=d(In),Kl=a(In,"P",{});var ib=i(Kl);fg=o(ib,"Example:"),ib.forEach(n),mg=d(In),y(Bs.$$.fragment,In),In.forEach(n),En.forEach(n),Uc=d(s),mt=a(s,"H2",{class:!0});var Uu=i(mt);to=a(Uu,"A",{id:!0,class:!0,href:!0});var lb=i(to);Gl=a(lb,"SPAN",{});var db=i(Gl);y(Ws.$$.fragment,db),db.forEach(n),lb.forEach(n),gg=d(Uu),Zl=a(Uu,"SPAN",{});var cb=i(Zl);_g=o(cb,"FunnelForSequenceClassification"),cb.forEach(n),Uu.forEach(n),Rc=d(s),Re=a(s,"DIV",{class:!0});var Mn=i(Re);y(Qs.$$.fragment,Mn),Tg=d(Mn),Xl=a(Mn,"P",{});var ub=i(Xl);Fg=o(ub,`Funnel Transformer Model with a sequence classification/regression head on top (two linear layer on top of the
first timestep of the last hidden state) e.g. for GLUE tasks.`),ub.forEach(n),kg=d(Mn),Us=a(Mn,"P",{});var Ru=i(Us);vg=o(Ru,"The Funnel Transformer model was proposed in "),Rs=a(Ru,"A",{href:!0,rel:!0});var pb=i(Rs);yg=o(pb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),pb.forEach(n),wg=o(Ru," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Ru.forEach(n),bg=d(Mn),Hs=a(Mn,"P",{});var Hu=i(Hs);$g=o(Hu,"This model inherits from "),_i=a(Hu,"A",{href:!0});var hb=i(_i);Eg=o(hb,"PreTrainedModel"),hb.forEach(n),Mg=o(Hu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Hu.forEach(n),zg=d(Mn),Vs=a(Mn,"P",{});var Vu=i(Vs);qg=o(Vu,"This model is also a PyTorch "),Ys=a(Vu,"A",{href:!0,rel:!0});var fb=i(Ys);Pg=o(fb,"torch.nn.Module"),fb.forEach(n),Cg=o(Vu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Vu.forEach(n),jg=d(Mn),Be=a(Mn,"DIV",{class:!0});var hn=i(Be);y(Ks.$$.fragment,hn),xg=d(hn),gt=a(hn,"P",{});var Zi=i(gt);Lg=o(Zi,"The "),Ti=a(Zi,"A",{href:!0});var mb=i(Ti);Og=o(mb,"FunnelForSequenceClassification"),mb.forEach(n),Dg=o(Zi," forward method, overrides the "),Jl=a(Zi,"CODE",{});var gb=i(Jl);Ag=o(gb,"__call__"),gb.forEach(n),Ng=o(Zi," special method."),Zi.forEach(n),Ig=d(hn),y(oo.$$.fragment,hn),Sg=d(hn),ed=a(hn,"P",{});var _b=i(ed);Bg=o(_b,"Example of single-label classification:"),_b.forEach(n),Wg=d(hn),y(Gs.$$.fragment,hn),Qg=d(hn),nd=a(hn,"P",{});var Tb=i(nd);Ug=o(Tb,"Example of multi-label classification:"),Tb.forEach(n),Rg=d(hn),y(Zs.$$.fragment,hn),hn.forEach(n),Mn.forEach(n),Hc=d(s),_t=a(s,"H2",{class:!0});var Yu=i(_t);so=a(Yu,"A",{id:!0,class:!0,href:!0});var Fb=i(so);td=a(Fb,"SPAN",{});var kb=i(td);y(Xs.$$.fragment,kb),kb.forEach(n),Fb.forEach(n),Hg=d(Yu),od=a(Yu,"SPAN",{});var vb=i(od);Vg=o(vb,"FunnelForMultipleChoice"),vb.forEach(n),Yu.forEach(n),Vc=d(s),He=a(s,"DIV",{class:!0});var zn=i(He);y(Js.$$.fragment,zn),Yg=d(zn),sd=a(zn,"P",{});var yb=i(sd);Kg=o(yb,`Funnel Transformer Model with a multiple choice classification head on top (two linear layer on top of the first
timestep of the last hidden state, and a softmax) e.g. for RocStories/SWAG tasks.`),yb.forEach(n),Gg=d(zn),er=a(zn,"P",{});var Ku=i(er);Zg=o(Ku,"The Funnel Transformer model was proposed in "),nr=a(Ku,"A",{href:!0,rel:!0});var wb=i(nr);Xg=o(wb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),wb.forEach(n),Jg=o(Ku," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Ku.forEach(n),e_=d(zn),tr=a(zn,"P",{});var Gu=i(tr);n_=o(Gu,"This model inherits from "),Fi=a(Gu,"A",{href:!0});var bb=i(Fi);t_=o(bb,"PreTrainedModel"),bb.forEach(n),o_=o(Gu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Gu.forEach(n),s_=d(zn),or=a(zn,"P",{});var Zu=i(or);r_=o(Zu,"This model is also a PyTorch "),sr=a(Zu,"A",{href:!0,rel:!0});var $b=i(sr);a_=o($b,"torch.nn.Module"),$b.forEach(n),i_=o(Zu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Zu.forEach(n),l_=d(zn),nn=a(zn,"DIV",{class:!0});var Sn=i(nn);y(rr.$$.fragment,Sn),d_=d(Sn),Tt=a(Sn,"P",{});var Xi=i(Tt);c_=o(Xi,"The "),ki=a(Xi,"A",{href:!0});var Eb=i(ki);u_=o(Eb,"FunnelForMultipleChoice"),Eb.forEach(n),p_=o(Xi," forward method, overrides the "),rd=a(Xi,"CODE",{});var Mb=i(rd);h_=o(Mb,"__call__"),Mb.forEach(n),f_=o(Xi," special method."),Xi.forEach(n),m_=d(Sn),y(ro.$$.fragment,Sn),g_=d(Sn),ad=a(Sn,"P",{});var zb=i(ad);__=o(zb,"Example:"),zb.forEach(n),T_=d(Sn),y(ar.$$.fragment,Sn),Sn.forEach(n),zn.forEach(n),Yc=d(s),Ft=a(s,"H2",{class:!0});var Xu=i(Ft);ao=a(Xu,"A",{id:!0,class:!0,href:!0});var qb=i(ao);id=a(qb,"SPAN",{});var Pb=i(id);y(ir.$$.fragment,Pb),Pb.forEach(n),qb.forEach(n),F_=d(Xu),ld=a(Xu,"SPAN",{});var Cb=i(ld);k_=o(Cb,"FunnelForTokenClassification"),Cb.forEach(n),Xu.forEach(n),Kc=d(s),Ve=a(s,"DIV",{class:!0});var qn=i(Ve);y(lr.$$.fragment,qn),v_=d(qn),dd=a(qn,"P",{});var jb=i(dd);y_=o(jb,`Funnel Transformer Model with a token classification head on top (a linear layer on top of the hidden-states
output) e.g. for Named-Entity-Recognition (NER) tasks.`),jb.forEach(n),w_=d(qn),dr=a(qn,"P",{});var Ju=i(dr);b_=o(Ju,"The Funnel Transformer model was proposed in "),cr=a(Ju,"A",{href:!0,rel:!0});var xb=i(cr);$_=o(xb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),xb.forEach(n),E_=o(Ju," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Ju.forEach(n),M_=d(qn),ur=a(qn,"P",{});var ep=i(ur);z_=o(ep,"This model inherits from "),vi=a(ep,"A",{href:!0});var Lb=i(vi);q_=o(Lb,"PreTrainedModel"),Lb.forEach(n),P_=o(ep,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ep.forEach(n),C_=d(qn),pr=a(qn,"P",{});var np=i(pr);j_=o(np,"This model is also a PyTorch "),hr=a(np,"A",{href:!0,rel:!0});var Ob=i(hr);x_=o(Ob,"torch.nn.Module"),Ob.forEach(n),L_=o(np,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),np.forEach(n),O_=d(qn),tn=a(qn,"DIV",{class:!0});var Bn=i(tn);y(fr.$$.fragment,Bn),D_=d(Bn),kt=a(Bn,"P",{});var Ji=i(kt);A_=o(Ji,"The "),yi=a(Ji,"A",{href:!0});var Db=i(yi);N_=o(Db,"FunnelForTokenClassification"),Db.forEach(n),I_=o(Ji," forward method, overrides the "),cd=a(Ji,"CODE",{});var Ab=i(cd);S_=o(Ab,"__call__"),Ab.forEach(n),B_=o(Ji," special method."),Ji.forEach(n),W_=d(Bn),y(io.$$.fragment,Bn),Q_=d(Bn),ud=a(Bn,"P",{});var Nb=i(ud);U_=o(Nb,"Example:"),Nb.forEach(n),R_=d(Bn),y(mr.$$.fragment,Bn),Bn.forEach(n),qn.forEach(n),Gc=d(s),vt=a(s,"H2",{class:!0});var tp=i(vt);lo=a(tp,"A",{id:!0,class:!0,href:!0});var Ib=i(lo);pd=a(Ib,"SPAN",{});var Sb=i(pd);y(gr.$$.fragment,Sb),Sb.forEach(n),Ib.forEach(n),H_=d(tp),hd=a(tp,"SPAN",{});var Bb=i(hd);V_=o(Bb,"FunnelForQuestionAnswering"),Bb.forEach(n),tp.forEach(n),Zc=d(s),Ye=a(s,"DIV",{class:!0});var Pn=i(Ye);y(_r.$$.fragment,Pn),Y_=d(Pn),yt=a(Pn,"P",{});var el=i(yt);K_=o(el,`Funnel Transformer Model with a span classification head on top for extractive question-answering tasks like SQuAD
(a linear layer on top of the hidden-states output to compute `),fd=a(el,"CODE",{});var Wb=i(fd);G_=o(Wb,"span start logits"),Wb.forEach(n),Z_=o(el," and "),md=a(el,"CODE",{});var Qb=i(md);X_=o(Qb,"span end logits"),Qb.forEach(n),J_=o(el,")."),el.forEach(n),eT=d(Pn),Tr=a(Pn,"P",{});var op=i(Tr);nT=o(op,"The Funnel Transformer model was proposed in "),Fr=a(op,"A",{href:!0,rel:!0});var Ub=i(Fr);tT=o(Ub,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Ub.forEach(n),oT=o(op," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),op.forEach(n),sT=d(Pn),kr=a(Pn,"P",{});var sp=i(kr);rT=o(sp,"This model inherits from "),wi=a(sp,"A",{href:!0});var Rb=i(wi);aT=o(Rb,"PreTrainedModel"),Rb.forEach(n),iT=o(sp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),sp.forEach(n),lT=d(Pn),vr=a(Pn,"P",{});var rp=i(vr);dT=o(rp,"This model is also a PyTorch "),yr=a(rp,"A",{href:!0,rel:!0});var Hb=i(yr);cT=o(Hb,"torch.nn.Module"),Hb.forEach(n),uT=o(rp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),rp.forEach(n),pT=d(Pn),on=a(Pn,"DIV",{class:!0});var Wn=i(on);y(wr.$$.fragment,Wn),hT=d(Wn),wt=a(Wn,"P",{});var nl=i(wt);fT=o(nl,"The "),bi=a(nl,"A",{href:!0});var Vb=i(bi);mT=o(Vb,"FunnelForQuestionAnswering"),Vb.forEach(n),gT=o(nl," forward method, overrides the "),gd=a(nl,"CODE",{});var Yb=i(gd);_T=o(Yb,"__call__"),Yb.forEach(n),TT=o(nl," special method."),nl.forEach(n),FT=d(Wn),y(co.$$.fragment,Wn),kT=d(Wn),_d=a(Wn,"P",{});var Kb=i(_d);vT=o(Kb,"Example:"),Kb.forEach(n),yT=d(Wn),y(br.$$.fragment,Wn),Wn.forEach(n),Pn.forEach(n),Xc=d(s),bt=a(s,"H2",{class:!0});var ap=i(bt);uo=a(ap,"A",{id:!0,class:!0,href:!0});var Gb=i(uo);Td=a(Gb,"SPAN",{});var Zb=i(Td);y($r.$$.fragment,Zb),Zb.forEach(n),Gb.forEach(n),wT=d(ap),Fd=a(ap,"SPAN",{});var Xb=i(Fd);bT=o(Xb,"TFFunnelBaseModel"),Xb.forEach(n),ap.forEach(n),Jc=d(s),je=a(s,"DIV",{class:!0});var fn=i(je);y(Er.$$.fragment,fn),$T=d(fn),kd=a(fn,"P",{});var Jb=i(kd);ET=o(Jb,`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),Jb.forEach(n),MT=d(fn),Mr=a(fn,"P",{});var ip=i(Mr);zT=o(ip,"The Funnel Transformer model was proposed in "),zr=a(ip,"A",{href:!0,rel:!0});var e1=i(zr);qT=o(e1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),e1.forEach(n),PT=o(ip," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),ip.forEach(n),CT=d(fn),qr=a(fn,"P",{});var lp=i(qr);jT=o(lp,"This model inherits from "),$i=a(lp,"A",{href:!0});var n1=i($i);xT=o(n1,"TFPreTrainedModel"),n1.forEach(n),LT=o(lp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),lp.forEach(n),OT=d(fn),Pr=a(fn,"P",{});var dp=i(Pr);DT=o(dp,"This model is also a "),Cr=a(dp,"A",{href:!0,rel:!0});var t1=i(Cr);AT=o(t1,"tf.keras.Model"),t1.forEach(n),NT=o(dp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),dp.forEach(n),IT=d(fn),y(po.$$.fragment,fn),ST=d(fn),sn=a(fn,"DIV",{class:!0});var Qn=i(sn);y(jr.$$.fragment,Qn),BT=d(Qn),$t=a(Qn,"P",{});var tl=i($t);WT=o(tl,"The "),Ei=a(tl,"A",{href:!0});var o1=i(Ei);QT=o(o1,"TFFunnelBaseModel"),o1.forEach(n),UT=o(tl," forward method, overrides the "),vd=a(tl,"CODE",{});var s1=i(vd);RT=o(s1,"__call__"),s1.forEach(n),HT=o(tl," special method."),tl.forEach(n),VT=d(Qn),y(ho.$$.fragment,Qn),YT=d(Qn),yd=a(Qn,"P",{});var r1=i(yd);KT=o(r1,"Example:"),r1.forEach(n),GT=d(Qn),y(xr.$$.fragment,Qn),Qn.forEach(n),fn.forEach(n),eu=d(s),Et=a(s,"H2",{class:!0});var cp=i(Et);fo=a(cp,"A",{id:!0,class:!0,href:!0});var a1=i(fo);wd=a(a1,"SPAN",{});var i1=i(wd);y(Lr.$$.fragment,i1),i1.forEach(n),a1.forEach(n),ZT=d(cp),bd=a(cp,"SPAN",{});var l1=i(bd);XT=o(l1,"TFFunnelModel"),l1.forEach(n),cp.forEach(n),nu=d(s),xe=a(s,"DIV",{class:!0});var mn=i(xe);y(Or.$$.fragment,mn),JT=d(mn),$d=a(mn,"P",{});var d1=i($d);eF=o(d1,"The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),d1.forEach(n),nF=d(mn),Dr=a(mn,"P",{});var up=i(Dr);tF=o(up,"The Funnel Transformer model was proposed in "),Ar=a(up,"A",{href:!0,rel:!0});var c1=i(Ar);oF=o(c1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),c1.forEach(n),sF=o(up," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),up.forEach(n),rF=d(mn),Nr=a(mn,"P",{});var pp=i(Nr);aF=o(pp,"This model inherits from "),Mi=a(pp,"A",{href:!0});var u1=i(Mi);iF=o(u1,"TFPreTrainedModel"),u1.forEach(n),lF=o(pp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),pp.forEach(n),dF=d(mn),Ir=a(mn,"P",{});var hp=i(Ir);cF=o(hp,"This model is also a "),Sr=a(hp,"A",{href:!0,rel:!0});var p1=i(Sr);uF=o(p1,"tf.keras.Model"),p1.forEach(n),pF=o(hp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),hp.forEach(n),hF=d(mn),y(mo.$$.fragment,mn),fF=d(mn),rn=a(mn,"DIV",{class:!0});var Un=i(rn);y(Br.$$.fragment,Un),mF=d(Un),Mt=a(Un,"P",{});var ol=i(Mt);gF=o(ol,"The "),zi=a(ol,"A",{href:!0});var h1=i(zi);_F=o(h1,"TFFunnelModel"),h1.forEach(n),TF=o(ol," forward method, overrides the "),Ed=a(ol,"CODE",{});var f1=i(Ed);FF=o(f1,"__call__"),f1.forEach(n),kF=o(ol," special method."),ol.forEach(n),vF=d(Un),y(go.$$.fragment,Un),yF=d(Un),Md=a(Un,"P",{});var m1=i(Md);wF=o(m1,"Example:"),m1.forEach(n),bF=d(Un),y(Wr.$$.fragment,Un),Un.forEach(n),mn.forEach(n),tu=d(s),zt=a(s,"H2",{class:!0});var fp=i(zt);_o=a(fp,"A",{id:!0,class:!0,href:!0});var g1=i(_o);zd=a(g1,"SPAN",{});var _1=i(zd);y(Qr.$$.fragment,_1),_1.forEach(n),g1.forEach(n),$F=d(fp),qd=a(fp,"SPAN",{});var T1=i(qd);EF=o(T1,"TFFunnelModelForPreTraining"),T1.forEach(n),fp.forEach(n),ou=d(s),Le=a(s,"DIV",{class:!0});var gn=i(Le);y(Ur.$$.fragment,gn),MF=d(gn),Pd=a(gn,"P",{});var F1=i(Pd);zF=o(F1,"Funnel model with a binary classification head on top as used during pretraining for identifying generated tokens."),F1.forEach(n),qF=d(gn),Rr=a(gn,"P",{});var mp=i(Rr);PF=o(mp,"The Funnel Transformer model was proposed in "),Hr=a(mp,"A",{href:!0,rel:!0});var k1=i(Hr);CF=o(k1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),k1.forEach(n),jF=o(mp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),mp.forEach(n),xF=d(gn),Vr=a(gn,"P",{});var gp=i(Vr);LF=o(gp,"This model inherits from "),qi=a(gp,"A",{href:!0});var v1=i(qi);OF=o(v1,"TFPreTrainedModel"),v1.forEach(n),DF=o(gp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),gp.forEach(n),AF=d(gn),Yr=a(gn,"P",{});var _p=i(Yr);NF=o(_p,"This model is also a "),Kr=a(_p,"A",{href:!0,rel:!0});var y1=i(Kr);IF=o(y1,"tf.keras.Model"),y1.forEach(n),SF=o(_p,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),_p.forEach(n),BF=d(gn),y(To.$$.fragment,gn),WF=d(gn),an=a(gn,"DIV",{class:!0});var Rn=i(an);y(Gr.$$.fragment,Rn),QF=d(Rn),qt=a(Rn,"P",{});var sl=i(qt);UF=o(sl,"The "),Pi=a(sl,"A",{href:!0});var w1=i(Pi);RF=o(w1,"TFFunnelForPreTraining"),w1.forEach(n),HF=o(sl," forward method, overrides the "),Cd=a(sl,"CODE",{});var b1=i(Cd);VF=o(b1,"__call__"),b1.forEach(n),YF=o(sl," special method."),sl.forEach(n),KF=d(Rn),y(Fo.$$.fragment,Rn),GF=d(Rn),jd=a(Rn,"P",{});var $1=i(jd);ZF=o($1,"Examples:"),$1.forEach(n),XF=d(Rn),y(Zr.$$.fragment,Rn),Rn.forEach(n),gn.forEach(n),su=d(s),Pt=a(s,"H2",{class:!0});var Tp=i(Pt);ko=a(Tp,"A",{id:!0,class:!0,href:!0});var E1=i(ko);xd=a(E1,"SPAN",{});var M1=i(xd);y(Xr.$$.fragment,M1),M1.forEach(n),E1.forEach(n),JF=d(Tp),Ld=a(Tp,"SPAN",{});var z1=i(Ld);ek=o(z1,"TFFunnelForMaskedLM"),z1.forEach(n),Tp.forEach(n),ru=d(s),Oe=a(s,"DIV",{class:!0});var _n=i(Oe);y(Jr.$$.fragment,_n),nk=d(_n),ea=a(_n,"P",{});var Fp=i(ea);tk=o(Fp,"Funnel Model with a "),Od=a(Fp,"CODE",{});var q1=i(Od);ok=o(q1,"language modeling"),q1.forEach(n),sk=o(Fp," head on top."),Fp.forEach(n),rk=d(_n),na=a(_n,"P",{});var kp=i(na);ak=o(kp,"The Funnel Transformer model was proposed in "),ta=a(kp,"A",{href:!0,rel:!0});var P1=i(ta);ik=o(P1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),P1.forEach(n),lk=o(kp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),kp.forEach(n),dk=d(_n),oa=a(_n,"P",{});var vp=i(oa);ck=o(vp,"This model inherits from "),Ci=a(vp,"A",{href:!0});var C1=i(Ci);uk=o(C1,"TFPreTrainedModel"),C1.forEach(n),pk=o(vp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),vp.forEach(n),hk=d(_n),sa=a(_n,"P",{});var yp=i(sa);fk=o(yp,"This model is also a "),ra=a(yp,"A",{href:!0,rel:!0});var j1=i(ra);mk=o(j1,"tf.keras.Model"),j1.forEach(n),gk=o(yp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),yp.forEach(n),_k=d(_n),y(vo.$$.fragment,_n),Tk=d(_n),ln=a(_n,"DIV",{class:!0});var Hn=i(ln);y(aa.$$.fragment,Hn),Fk=d(Hn),Ct=a(Hn,"P",{});var rl=i(Ct);kk=o(rl,"The "),ji=a(rl,"A",{href:!0});var x1=i(ji);vk=o(x1,"TFFunnelForMaskedLM"),x1.forEach(n),yk=o(rl," forward method, overrides the "),Dd=a(rl,"CODE",{});var L1=i(Dd);wk=o(L1,"__call__"),L1.forEach(n),bk=o(rl," special method."),rl.forEach(n),$k=d(Hn),y(yo.$$.fragment,Hn),Ek=d(Hn),Ad=a(Hn,"P",{});var O1=i(Ad);Mk=o(O1,"Example:"),O1.forEach(n),zk=d(Hn),y(ia.$$.fragment,Hn),Hn.forEach(n),_n.forEach(n),au=d(s),jt=a(s,"H2",{class:!0});var wp=i(jt);wo=a(wp,"A",{id:!0,class:!0,href:!0});var D1=i(wo);Nd=a(D1,"SPAN",{});var A1=i(Nd);y(la.$$.fragment,A1),A1.forEach(n),D1.forEach(n),qk=d(wp),Id=a(wp,"SPAN",{});var N1=i(Id);Pk=o(N1,"TFFunnelForSequenceClassification"),N1.forEach(n),wp.forEach(n),iu=d(s),De=a(s,"DIV",{class:!0});var Tn=i(De);y(da.$$.fragment,Tn),Ck=d(Tn),Sd=a(Tn,"P",{});var I1=i(Sd);jk=o(I1,`Funnel Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
output) e.g. for GLUE tasks.`),I1.forEach(n),xk=d(Tn),ca=a(Tn,"P",{});var bp=i(ca);Lk=o(bp,"The Funnel Transformer model was proposed in "),ua=a(bp,"A",{href:!0,rel:!0});var S1=i(ua);Ok=o(S1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),S1.forEach(n),Dk=o(bp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),bp.forEach(n),Ak=d(Tn),pa=a(Tn,"P",{});var $p=i(pa);Nk=o($p,"This model inherits from "),xi=a($p,"A",{href:!0});var B1=i(xi);Ik=o(B1,"TFPreTrainedModel"),B1.forEach(n),Sk=o($p,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),$p.forEach(n),Bk=d(Tn),ha=a(Tn,"P",{});var Ep=i(ha);Wk=o(Ep,"This model is also a "),fa=a(Ep,"A",{href:!0,rel:!0});var W1=i(fa);Qk=o(W1,"tf.keras.Model"),W1.forEach(n),Uk=o(Ep,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Ep.forEach(n),Rk=d(Tn),y(bo.$$.fragment,Tn),Hk=d(Tn),dn=a(Tn,"DIV",{class:!0});var Vn=i(dn);y(ma.$$.fragment,Vn),Vk=d(Vn),xt=a(Vn,"P",{});var al=i(xt);Yk=o(al,"The "),Li=a(al,"A",{href:!0});var Q1=i(Li);Kk=o(Q1,"TFFunnelForSequenceClassification"),Q1.forEach(n),Gk=o(al," forward method, overrides the "),Bd=a(al,"CODE",{});var U1=i(Bd);Zk=o(U1,"__call__"),U1.forEach(n),Xk=o(al," special method."),al.forEach(n),Jk=d(Vn),y($o.$$.fragment,Vn),ev=d(Vn),Wd=a(Vn,"P",{});var R1=i(Wd);nv=o(R1,"Example:"),R1.forEach(n),tv=d(Vn),y(ga.$$.fragment,Vn),Vn.forEach(n),Tn.forEach(n),lu=d(s),Lt=a(s,"H2",{class:!0});var Mp=i(Lt);Eo=a(Mp,"A",{id:!0,class:!0,href:!0});var H1=i(Eo);Qd=a(H1,"SPAN",{});var V1=i(Qd);y(_a.$$.fragment,V1),V1.forEach(n),H1.forEach(n),ov=d(Mp),Ud=a(Mp,"SPAN",{});var Y1=i(Ud);sv=o(Y1,"TFFunnelForMultipleChoice"),Y1.forEach(n),Mp.forEach(n),du=d(s),Ae=a(s,"DIV",{class:!0});var Fn=i(Ae);y(Ta.$$.fragment,Fn),rv=d(Fn),Rd=a(Fn,"P",{});var K1=i(Rd);av=o(K1,`Funnel Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),K1.forEach(n),iv=d(Fn),Fa=a(Fn,"P",{});var zp=i(Fa);lv=o(zp,"The Funnel Transformer model was proposed in "),ka=a(zp,"A",{href:!0,rel:!0});var G1=i(ka);dv=o(G1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),G1.forEach(n),cv=o(zp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),zp.forEach(n),uv=d(Fn),va=a(Fn,"P",{});var qp=i(va);pv=o(qp,"This model inherits from "),Oi=a(qp,"A",{href:!0});var Z1=i(Oi);hv=o(Z1,"TFPreTrainedModel"),Z1.forEach(n),fv=o(qp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),qp.forEach(n),mv=d(Fn),ya=a(Fn,"P",{});var Pp=i(ya);gv=o(Pp,"This model is also a "),wa=a(Pp,"A",{href:!0,rel:!0});var X1=i(wa);_v=o(X1,"tf.keras.Model"),X1.forEach(n),Tv=o(Pp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Pp.forEach(n),Fv=d(Fn),y(Mo.$$.fragment,Fn),kv=d(Fn),cn=a(Fn,"DIV",{class:!0});var Yn=i(cn);y(ba.$$.fragment,Yn),vv=d(Yn),Ot=a(Yn,"P",{});var il=i(Ot);yv=o(il,"The "),Di=a(il,"A",{href:!0});var J1=i(Di);wv=o(J1,"TFFunnelForMultipleChoice"),J1.forEach(n),bv=o(il," forward method, overrides the "),Hd=a(il,"CODE",{});var e$=i(Hd);$v=o(e$,"__call__"),e$.forEach(n),Ev=o(il," special method."),il.forEach(n),Mv=d(Yn),y(zo.$$.fragment,Yn),zv=d(Yn),Vd=a(Yn,"P",{});var n$=i(Vd);qv=o(n$,"Example:"),n$.forEach(n),Pv=d(Yn),y($a.$$.fragment,Yn),Yn.forEach(n),Fn.forEach(n),cu=d(s),Dt=a(s,"H2",{class:!0});var Cp=i(Dt);qo=a(Cp,"A",{id:!0,class:!0,href:!0});var t$=i(qo);Yd=a(t$,"SPAN",{});var o$=i(Yd);y(Ea.$$.fragment,o$),o$.forEach(n),t$.forEach(n),Cv=d(Cp),Kd=a(Cp,"SPAN",{});var s$=i(Kd);jv=o(s$,"TFFunnelForTokenClassification"),s$.forEach(n),Cp.forEach(n),uu=d(s),Ne=a(s,"DIV",{class:!0});var kn=i(Ne);y(Ma.$$.fragment,kn),xv=d(kn),Gd=a(kn,"P",{});var r$=i(Gd);Lv=o(r$,`Funnel Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),r$.forEach(n),Ov=d(kn),za=a(kn,"P",{});var jp=i(za);Dv=o(jp,"The Funnel Transformer model was proposed in "),qa=a(jp,"A",{href:!0,rel:!0});var a$=i(qa);Av=o(a$,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),a$.forEach(n),Nv=o(jp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),jp.forEach(n),Iv=d(kn),Pa=a(kn,"P",{});var xp=i(Pa);Sv=o(xp,"This model inherits from "),Ai=a(xp,"A",{href:!0});var i$=i(Ai);Bv=o(i$,"TFPreTrainedModel"),i$.forEach(n),Wv=o(xp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),xp.forEach(n),Qv=d(kn),Ca=a(kn,"P",{});var Lp=i(Ca);Uv=o(Lp,"This model is also a "),ja=a(Lp,"A",{href:!0,rel:!0});var l$=i(ja);Rv=o(l$,"tf.keras.Model"),l$.forEach(n),Hv=o(Lp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Lp.forEach(n),Vv=d(kn),y(Po.$$.fragment,kn),Yv=d(kn),un=a(kn,"DIV",{class:!0});var Kn=i(un);y(xa.$$.fragment,Kn),Kv=d(Kn),At=a(Kn,"P",{});var ll=i(At);Gv=o(ll,"The "),Ni=a(ll,"A",{href:!0});var d$=i(Ni);Zv=o(d$,"TFFunnelForTokenClassification"),d$.forEach(n),Xv=o(ll," forward method, overrides the "),Zd=a(ll,"CODE",{});var c$=i(Zd);Jv=o(c$,"__call__"),c$.forEach(n),ey=o(ll," special method."),ll.forEach(n),ny=d(Kn),y(Co.$$.fragment,Kn),ty=d(Kn),Xd=a(Kn,"P",{});var u$=i(Xd);oy=o(u$,"Example:"),u$.forEach(n),sy=d(Kn),y(La.$$.fragment,Kn),Kn.forEach(n),kn.forEach(n),pu=d(s),Nt=a(s,"H2",{class:!0});var Op=i(Nt);jo=a(Op,"A",{id:!0,class:!0,href:!0});var p$=i(jo);Jd=a(p$,"SPAN",{});var h$=i(Jd);y(Oa.$$.fragment,h$),h$.forEach(n),p$.forEach(n),ry=d(Op),ec=a(Op,"SPAN",{});var f$=i(ec);ay=o(f$,"TFFunnelForQuestionAnswering"),f$.forEach(n),Op.forEach(n),hu=d(s),Ie=a(s,"DIV",{class:!0});var vn=i(Ie);y(Da.$$.fragment,vn),iy=d(vn),It=a(vn,"P",{});var dl=i(It);ly=o(dl,`Funnel Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),nc=a(dl,"CODE",{});var m$=i(nc);dy=o(m$,"span start logits"),m$.forEach(n),cy=o(dl," and "),tc=a(dl,"CODE",{});var g$=i(tc);uy=o(g$,"span end logits"),g$.forEach(n),py=o(dl,")."),dl.forEach(n),hy=d(vn),Aa=a(vn,"P",{});var Dp=i(Aa);fy=o(Dp,"The Funnel Transformer model was proposed in "),Na=a(Dp,"A",{href:!0,rel:!0});var _$=i(Na);my=o(_$,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),_$.forEach(n),gy=o(Dp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Dp.forEach(n),_y=d(vn),Ia=a(vn,"P",{});var Ap=i(Ia);Ty=o(Ap,"This model inherits from "),Ii=a(Ap,"A",{href:!0});var T$=i(Ii);Fy=o(T$,"TFPreTrainedModel"),T$.forEach(n),ky=o(Ap,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ap.forEach(n),vy=d(vn),Sa=a(vn,"P",{});var Np=i(Sa);yy=o(Np,"This model is also a "),Ba=a(Np,"A",{href:!0,rel:!0});var F$=i(Ba);wy=o(F$,"tf.keras.Model"),F$.forEach(n),by=o(Np,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Np.forEach(n),$y=d(vn),y(xo.$$.fragment,vn),Ey=d(vn),pn=a(vn,"DIV",{class:!0});var Gn=i(pn);y(Wa.$$.fragment,Gn),My=d(Gn),St=a(Gn,"P",{});var cl=i(St);zy=o(cl,"The "),Si=a(cl,"A",{href:!0});var k$=i(Si);qy=o(k$,"TFFunnelForQuestionAnswering"),k$.forEach(n),Py=o(cl," forward method, overrides the "),oc=a(cl,"CODE",{});var v$=i(oc);Cy=o(v$,"__call__"),v$.forEach(n),jy=o(cl," special method."),cl.forEach(n),xy=d(Gn),y(Lo.$$.fragment,Gn),Ly=d(Gn),sc=a(Gn,"P",{});var y$=i(sc);Oy=o(y$,"Example:"),y$.forEach(n),Dy=d(Gn),y(Qa.$$.fragment,Gn),Gn.forEach(n),vn.forEach(n),this.h()},h(){c(p,"name","hf:doc:metadata"),c(p,"content",JSON.stringify(X$)),c(g,"id","funnel-transformer"),c(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g,"href","#funnel-transformer"),c(m,"class","relative group"),c(J,"id","overview"),c(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J,"href","#overview"),c(q,"class","relative group"),c(te,"href","https://arxiv.org/abs/2006.03236"),c(te,"rel","nofollow"),c(re,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelModel"),c(u,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(Te,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(Fe,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(ke,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelBaseModel"),c(ve,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(Ya,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(No,"href","https://huggingface.co/sgugger"),c(No,"rel","nofollow"),c(Io,"href","https://github.com/laiguokun/Funnel-Transformer"),c(Io,"rel","nofollow"),c(Bt,"id","transformers.FunnelConfig"),c(Bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bt,"href","#transformers.FunnelConfig"),c(Zn,"class","relative group"),c(Ka,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelModel"),c(Ga,"href","/docs/transformers/main/en/model_doc/bert#transformers.TFBertModel"),c(Wo,"href","https://huggingface.co/funnel-transformer/small"),c(Wo,"rel","nofollow"),c(Za,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(Xa,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(Cn,"class","docstring"),c(Wt,"id","transformers.FunnelTokenizer"),c(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Wt,"href","#transformers.FunnelTokenizer"),c(Jn,"class","relative group"),c(Ja,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizer"),c(ei,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),c(ni,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),c(Ln,"class","docstring"),c(Ut,"class","docstring"),c(yn,"class","docstring"),c(si,"class","docstring"),c(Pe,"class","docstring"),c(Rt,"id","transformers.FunnelTokenizerFast"),c(Rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Rt,"href","#transformers.FunnelTokenizerFast"),c(nt,"class","relative group"),c(ri,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(ai,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),c(ii,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),c(wn,"class","docstring"),c(Ge,"class","docstring"),c(Vt,"id","transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"),c(Vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Vt,"href","#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"),c(ot,"class","relative group"),c(li,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(st,"class","docstring"),c(di,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(rt,"class","docstring"),c(Yt,"id","transformers.FunnelBaseModel"),c(Yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Yt,"href","#transformers.FunnelBaseModel"),c(at,"class","relative group"),c(hs,"href","https://arxiv.org/abs/2006.03236"),c(hs,"rel","nofollow"),c(ci,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(gs,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(gs,"rel","nofollow"),c(ui,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelBaseModel"),c(Ze,"class","docstring"),c(We,"class","docstring"),c(Gt,"id","transformers.FunnelModel"),c(Gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Gt,"href","#transformers.FunnelModel"),c(lt,"class","relative group"),c(ys,"href","https://arxiv.org/abs/2006.03236"),c(ys,"rel","nofollow"),c(pi,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c($s,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c($s,"rel","nofollow"),c(hi,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelModel"),c(Xe,"class","docstring"),c(Qe,"class","docstring"),c(Xt,"id","transformers.FunnelForPreTraining"),c(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Xt,"href","#transformers.FunnelForPreTraining"),c(ct,"class","relative group"),c(fi,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(Je,"class","docstring"),c(ut,"class","docstring"),c(eo,"id","transformers.FunnelForMaskedLM"),c(eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(eo,"href","#transformers.FunnelForMaskedLM"),c(ht,"class","relative group"),c(Ds,"href","https://arxiv.org/abs/2006.03236"),c(Ds,"rel","nofollow"),c(mi,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(Is,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Is,"rel","nofollow"),c(gi,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(en,"class","docstring"),c(Ue,"class","docstring"),c(to,"id","transformers.FunnelForSequenceClassification"),c(to,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(to,"href","#transformers.FunnelForSequenceClassification"),c(mt,"class","relative group"),c(Rs,"href","https://arxiv.org/abs/2006.03236"),c(Rs,"rel","nofollow"),c(_i,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(Ys,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ys,"rel","nofollow"),c(Ti,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(Be,"class","docstring"),c(Re,"class","docstring"),c(so,"id","transformers.FunnelForMultipleChoice"),c(so,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(so,"href","#transformers.FunnelForMultipleChoice"),c(_t,"class","relative group"),c(nr,"href","https://arxiv.org/abs/2006.03236"),c(nr,"rel","nofollow"),c(Fi,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(sr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(sr,"rel","nofollow"),c(ki,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(nn,"class","docstring"),c(He,"class","docstring"),c(ao,"id","transformers.FunnelForTokenClassification"),c(ao,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ao,"href","#transformers.FunnelForTokenClassification"),c(Ft,"class","relative group"),c(cr,"href","https://arxiv.org/abs/2006.03236"),c(cr,"rel","nofollow"),c(vi,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(hr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(hr,"rel","nofollow"),c(yi,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(tn,"class","docstring"),c(Ve,"class","docstring"),c(lo,"id","transformers.FunnelForQuestionAnswering"),c(lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lo,"href","#transformers.FunnelForQuestionAnswering"),c(vt,"class","relative group"),c(Fr,"href","https://arxiv.org/abs/2006.03236"),c(Fr,"rel","nofollow"),c(wi,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(yr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(yr,"rel","nofollow"),c(bi,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(on,"class","docstring"),c(Ye,"class","docstring"),c(uo,"id","transformers.TFFunnelBaseModel"),c(uo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(uo,"href","#transformers.TFFunnelBaseModel"),c(bt,"class","relative group"),c(zr,"href","https://arxiv.org/abs/2006.03236"),c(zr,"rel","nofollow"),c($i,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(Cr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Cr,"rel","nofollow"),c(Ei,"href","/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(sn,"class","docstring"),c(je,"class","docstring"),c(fo,"id","transformers.TFFunnelModel"),c(fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fo,"href","#transformers.TFFunnelModel"),c(Et,"class","relative group"),c(Ar,"href","https://arxiv.org/abs/2006.03236"),c(Ar,"rel","nofollow"),c(Mi,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(Sr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Sr,"rel","nofollow"),c(zi,"href","/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelModel"),c(rn,"class","docstring"),c(xe,"class","docstring"),c(_o,"id","transformers.TFFunnelForPreTraining"),c(_o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_o,"href","#transformers.TFFunnelForPreTraining"),c(zt,"class","relative group"),c(Hr,"href","https://arxiv.org/abs/2006.03236"),c(Hr,"rel","nofollow"),c(qi,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(Kr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Kr,"rel","nofollow"),c(Pi,"href","/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(an,"class","docstring"),c(Le,"class","docstring"),c(ko,"id","transformers.TFFunnelForMaskedLM"),c(ko,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ko,"href","#transformers.TFFunnelForMaskedLM"),c(Pt,"class","relative group"),c(ta,"href","https://arxiv.org/abs/2006.03236"),c(ta,"rel","nofollow"),c(Ci,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(ra,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ra,"rel","nofollow"),c(ji,"href","/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(ln,"class","docstring"),c(Oe,"class","docstring"),c(wo,"id","transformers.TFFunnelForSequenceClassification"),c(wo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(wo,"href","#transformers.TFFunnelForSequenceClassification"),c(jt,"class","relative group"),c(ua,"href","https://arxiv.org/abs/2006.03236"),c(ua,"rel","nofollow"),c(xi,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(fa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(fa,"rel","nofollow"),c(Li,"href","/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(dn,"class","docstring"),c(De,"class","docstring"),c(Eo,"id","transformers.TFFunnelForMultipleChoice"),c(Eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Eo,"href","#transformers.TFFunnelForMultipleChoice"),c(Lt,"class","relative group"),c(ka,"href","https://arxiv.org/abs/2006.03236"),c(ka,"rel","nofollow"),c(Oi,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(wa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(wa,"rel","nofollow"),c(Di,"href","/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(cn,"class","docstring"),c(Ae,"class","docstring"),c(qo,"id","transformers.TFFunnelForTokenClassification"),c(qo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qo,"href","#transformers.TFFunnelForTokenClassification"),c(Dt,"class","relative group"),c(qa,"href","https://arxiv.org/abs/2006.03236"),c(qa,"rel","nofollow"),c(Ai,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(ja,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ja,"rel","nofollow"),c(Ni,"href","/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(un,"class","docstring"),c(Ne,"class","docstring"),c(jo,"id","transformers.TFFunnelForQuestionAnswering"),c(jo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(jo,"href","#transformers.TFFunnelForQuestionAnswering"),c(Nt,"class","relative group"),c(Na,"href","https://arxiv.org/abs/2006.03236"),c(Na,"rel","nofollow"),c(Ii,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(Ba,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Ba,"rel","nofollow"),c(Si,"href","/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(pn,"class","docstring"),c(Ie,"class","docstring")},m(s,f){e(document.head,p),h(s,M,f),h(s,m,f),e(m,g),e(g,F),w(T,F,null),e(m,_),e(m,z),e(z,ce),h(s,K,f),h(s,q,f),e(q,J),e(J,A),w(ne,A,null),e(q,ue),e(q,N),e(N,pe),h(s,ie,f),h(s,Y,f),e(Y,L),e(Y,te),e(te,G),e(Y,P),h(s,j,f),h(s,oe,f),e(oe,Q),h(s,le,f),h(s,se,f),e(se,I),e(I,he),h(s,de,f),h(s,C,f),e(C,fe),h(s,B,f),h(s,ee,f),e(ee,ae),e(ae,U),e(ee,me),e(ee,S),e(S,O),e(S,re),e(re,R),e(S,ge),e(S,u),e(u,k),e(S,Z),e(S,Te),e(Te,ye),e(S,D),e(S,Fe),e(Fe,we),e(S,be),e(S,x),e(x,H),e(S,$e),e(S,ke),e(ke,V),e(S,Ee),e(S,ve),e(ve,_e),e(S,Me),e(S,Ya),e(Ya,Ip),e(S,Sp),h(s,Ec,f),h(s,xn,f),e(xn,Bp),e(xn,No),e(No,Wp),e(xn,Qp),e(xn,Io),e(Io,Up),e(xn,Rp),h(s,Mc,f),h(s,Zn,f),e(Zn,Bt),e(Bt,ul),w(So,ul,null),e(Zn,Hp),e(Zn,pl),e(pl,Vp),h(s,zc,f),h(s,Cn,f),w(Bo,Cn,null),e(Cn,Yp),e(Cn,jn),e(jn,Kp),e(jn,Ka),e(Ka,Gp),e(jn,Zp),e(jn,Ga),e(Ga,Xp),e(jn,Jp),e(jn,Wo),e(Wo,eh),e(jn,nh),e(Cn,th),e(Cn,Xn),e(Xn,oh),e(Xn,Za),e(Za,sh),e(Xn,rh),e(Xn,Xa),e(Xa,ah),e(Xn,ih),h(s,qc,f),h(s,Jn,f),e(Jn,Wt),e(Wt,hl),w(Qo,hl,null),e(Jn,lh),e(Jn,fl),e(fl,dh),h(s,Pc,f),h(s,Pe,f),w(Uo,Pe,null),e(Pe,ch),e(Pe,ml),e(ml,uh),e(Pe,ph),e(Pe,Qt),e(Qt,Ja),e(Ja,hh),e(Qt,fh),e(Qt,ei),e(ei,mh),e(Qt,gh),e(Pe,_h),e(Pe,Ro),e(Ro,Th),e(Ro,ni),e(ni,Fh),e(Ro,kh),e(Pe,vh),e(Pe,Ln),w(Ho,Ln,null),e(Ln,yh),e(Ln,gl),e(gl,wh),e(Ln,bh),e(Ln,Vo),e(Vo,ti),e(ti,$h),e(ti,_l),e(_l,Eh),e(Vo,Mh),e(Vo,oi),e(oi,zh),e(oi,Tl),e(Tl,qh),e(Pe,Ph),e(Pe,Ut),w(Yo,Ut,null),e(Ut,Ch),e(Ut,Ko),e(Ko,jh),e(Ko,Fl),e(Fl,xh),e(Ko,Lh),e(Pe,Oh),e(Pe,yn),w(Go,yn,null),e(yn,Dh),e(yn,kl),e(kl,Ah),e(yn,Nh),w(Zo,yn,null),e(yn,Ih),e(yn,et),e(et,Sh),e(et,vl),e(vl,Bh),e(et,Wh),e(et,yl),e(yl,Qh),e(et,Uh),e(Pe,Rh),e(Pe,si),w(Xo,si,null),h(s,Cc,f),h(s,nt,f),e(nt,Rt),e(Rt,wl),w(Jo,wl,null),e(nt,Hh),e(nt,bl),e(bl,Vh),h(s,jc,f),h(s,Ge,f),w(es,Ge,null),e(Ge,Yh),e(Ge,ns),e(ns,Kh),e(ns,$l),e($l,Gh),e(ns,Zh),e(Ge,Xh),e(Ge,Ht),e(Ht,ri),e(ri,Jh),e(Ht,ef),e(Ht,ai),e(ai,nf),e(Ht,tf),e(Ge,of),e(Ge,ts),e(ts,sf),e(ts,ii),e(ii,rf),e(ts,af),e(Ge,lf),e(Ge,wn),w(os,wn,null),e(wn,df),e(wn,El),e(El,cf),e(wn,uf),w(ss,wn,null),e(wn,pf),e(wn,tt),e(tt,hf),e(tt,Ml),e(Ml,ff),e(tt,mf),e(tt,zl),e(zl,gf),e(tt,_f),h(s,xc,f),h(s,ot,f),e(ot,Vt),e(Vt,ql),w(rs,ql,null),e(ot,Tf),e(ot,Pl),e(Pl,Ff),h(s,Lc,f),h(s,st,f),w(as,st,null),e(st,kf),e(st,is),e(is,vf),e(is,li),e(li,yf),e(is,wf),h(s,Oc,f),h(s,rt,f),w(ls,rt,null),e(rt,bf),e(rt,ds),e(ds,$f),e(ds,di),e(di,Ef),e(ds,Mf),h(s,Dc,f),h(s,at,f),e(at,Yt),e(Yt,Cl),w(cs,Cl,null),e(at,zf),e(at,jl),e(jl,qf),h(s,Ac,f),h(s,We,f),w(us,We,null),e(We,Pf),e(We,xl),e(xl,Cf),e(We,jf),e(We,ps),e(ps,xf),e(ps,hs),e(hs,Lf),e(ps,Of),e(We,Df),e(We,fs),e(fs,Af),e(fs,ci),e(ci,Nf),e(fs,If),e(We,Sf),e(We,ms),e(ms,Bf),e(ms,gs),e(gs,Wf),e(ms,Qf),e(We,Uf),e(We,Ze),w(_s,Ze,null),e(Ze,Rf),e(Ze,it),e(it,Hf),e(it,ui),e(ui,Vf),e(it,Yf),e(it,Ll),e(Ll,Kf),e(it,Gf),e(Ze,Zf),w(Kt,Ze,null),e(Ze,Xf),e(Ze,Ol),e(Ol,Jf),e(Ze,em),w(Ts,Ze,null),h(s,Nc,f),h(s,lt,f),e(lt,Gt),e(Gt,Dl),w(Fs,Dl,null),e(lt,nm),e(lt,Al),e(Al,tm),h(s,Ic,f),h(s,Qe,f),w(ks,Qe,null),e(Qe,om),e(Qe,Nl),e(Nl,sm),e(Qe,rm),e(Qe,vs),e(vs,am),e(vs,ys),e(ys,im),e(vs,lm),e(Qe,dm),e(Qe,ws),e(ws,cm),e(ws,pi),e(pi,um),e(ws,pm),e(Qe,hm),e(Qe,bs),e(bs,fm),e(bs,$s),e($s,mm),e(bs,gm),e(Qe,_m),e(Qe,Xe),w(Es,Xe,null),e(Xe,Tm),e(Xe,dt),e(dt,Fm),e(dt,hi),e(hi,km),e(dt,vm),e(dt,Il),e(Il,ym),e(dt,wm),e(Xe,bm),w(Zt,Xe,null),e(Xe,$m),e(Xe,Sl),e(Sl,Em),e(Xe,Mm),w(Ms,Xe,null),h(s,Sc,f),h(s,ct,f),e(ct,Xt),e(Xt,Bl),w(zs,Bl,null),e(ct,zm),e(ct,Wl),e(Wl,qm),h(s,Bc,f),h(s,ut,f),w(qs,ut,null),e(ut,Pm),e(ut,Je),w(Ps,Je,null),e(Je,Cm),e(Je,pt),e(pt,jm),e(pt,fi),e(fi,xm),e(pt,Lm),e(pt,Ql),e(Ql,Om),e(pt,Dm),e(Je,Am),w(Jt,Je,null),e(Je,Nm),e(Je,Ul),e(Ul,Im),e(Je,Sm),w(Cs,Je,null),h(s,Wc,f),h(s,ht,f),e(ht,eo),e(eo,Rl),w(js,Rl,null),e(ht,Bm),e(ht,Hl),e(Hl,Wm),h(s,Qc,f),h(s,Ue,f),w(xs,Ue,null),e(Ue,Qm),e(Ue,Ls),e(Ls,Um),e(Ls,Vl),e(Vl,Rm),e(Ls,Hm),e(Ue,Vm),e(Ue,Os),e(Os,Ym),e(Os,Ds),e(Ds,Km),e(Os,Gm),e(Ue,Zm),e(Ue,As),e(As,Xm),e(As,mi),e(mi,Jm),e(As,eg),e(Ue,ng),e(Ue,Ns),e(Ns,tg),e(Ns,Is),e(Is,og),e(Ns,sg),e(Ue,rg),e(Ue,en),w(Ss,en,null),e(en,ag),e(en,ft),e(ft,ig),e(ft,gi),e(gi,lg),e(ft,dg),e(ft,Yl),e(Yl,cg),e(ft,ug),e(en,pg),w(no,en,null),e(en,hg),e(en,Kl),e(Kl,fg),e(en,mg),w(Bs,en,null),h(s,Uc,f),h(s,mt,f),e(mt,to),e(to,Gl),w(Ws,Gl,null),e(mt,gg),e(mt,Zl),e(Zl,_g),h(s,Rc,f),h(s,Re,f),w(Qs,Re,null),e(Re,Tg),e(Re,Xl),e(Xl,Fg),e(Re,kg),e(Re,Us),e(Us,vg),e(Us,Rs),e(Rs,yg),e(Us,wg),e(Re,bg),e(Re,Hs),e(Hs,$g),e(Hs,_i),e(_i,Eg),e(Hs,Mg),e(Re,zg),e(Re,Vs),e(Vs,qg),e(Vs,Ys),e(Ys,Pg),e(Vs,Cg),e(Re,jg),e(Re,Be),w(Ks,Be,null),e(Be,xg),e(Be,gt),e(gt,Lg),e(gt,Ti),e(Ti,Og),e(gt,Dg),e(gt,Jl),e(Jl,Ag),e(gt,Ng),e(Be,Ig),w(oo,Be,null),e(Be,Sg),e(Be,ed),e(ed,Bg),e(Be,Wg),w(Gs,Be,null),e(Be,Qg),e(Be,nd),e(nd,Ug),e(Be,Rg),w(Zs,Be,null),h(s,Hc,f),h(s,_t,f),e(_t,so),e(so,td),w(Xs,td,null),e(_t,Hg),e(_t,od),e(od,Vg),h(s,Vc,f),h(s,He,f),w(Js,He,null),e(He,Yg),e(He,sd),e(sd,Kg),e(He,Gg),e(He,er),e(er,Zg),e(er,nr),e(nr,Xg),e(er,Jg),e(He,e_),e(He,tr),e(tr,n_),e(tr,Fi),e(Fi,t_),e(tr,o_),e(He,s_),e(He,or),e(or,r_),e(or,sr),e(sr,a_),e(or,i_),e(He,l_),e(He,nn),w(rr,nn,null),e(nn,d_),e(nn,Tt),e(Tt,c_),e(Tt,ki),e(ki,u_),e(Tt,p_),e(Tt,rd),e(rd,h_),e(Tt,f_),e(nn,m_),w(ro,nn,null),e(nn,g_),e(nn,ad),e(ad,__),e(nn,T_),w(ar,nn,null),h(s,Yc,f),h(s,Ft,f),e(Ft,ao),e(ao,id),w(ir,id,null),e(Ft,F_),e(Ft,ld),e(ld,k_),h(s,Kc,f),h(s,Ve,f),w(lr,Ve,null),e(Ve,v_),e(Ve,dd),e(dd,y_),e(Ve,w_),e(Ve,dr),e(dr,b_),e(dr,cr),e(cr,$_),e(dr,E_),e(Ve,M_),e(Ve,ur),e(ur,z_),e(ur,vi),e(vi,q_),e(ur,P_),e(Ve,C_),e(Ve,pr),e(pr,j_),e(pr,hr),e(hr,x_),e(pr,L_),e(Ve,O_),e(Ve,tn),w(fr,tn,null),e(tn,D_),e(tn,kt),e(kt,A_),e(kt,yi),e(yi,N_),e(kt,I_),e(kt,cd),e(cd,S_),e(kt,B_),e(tn,W_),w(io,tn,null),e(tn,Q_),e(tn,ud),e(ud,U_),e(tn,R_),w(mr,tn,null),h(s,Gc,f),h(s,vt,f),e(vt,lo),e(lo,pd),w(gr,pd,null),e(vt,H_),e(vt,hd),e(hd,V_),h(s,Zc,f),h(s,Ye,f),w(_r,Ye,null),e(Ye,Y_),e(Ye,yt),e(yt,K_),e(yt,fd),e(fd,G_),e(yt,Z_),e(yt,md),e(md,X_),e(yt,J_),e(Ye,eT),e(Ye,Tr),e(Tr,nT),e(Tr,Fr),e(Fr,tT),e(Tr,oT),e(Ye,sT),e(Ye,kr),e(kr,rT),e(kr,wi),e(wi,aT),e(kr,iT),e(Ye,lT),e(Ye,vr),e(vr,dT),e(vr,yr),e(yr,cT),e(vr,uT),e(Ye,pT),e(Ye,on),w(wr,on,null),e(on,hT),e(on,wt),e(wt,fT),e(wt,bi),e(bi,mT),e(wt,gT),e(wt,gd),e(gd,_T),e(wt,TT),e(on,FT),w(co,on,null),e(on,kT),e(on,_d),e(_d,vT),e(on,yT),w(br,on,null),h(s,Xc,f),h(s,bt,f),e(bt,uo),e(uo,Td),w($r,Td,null),e(bt,wT),e(bt,Fd),e(Fd,bT),h(s,Jc,f),h(s,je,f),w(Er,je,null),e(je,$T),e(je,kd),e(kd,ET),e(je,MT),e(je,Mr),e(Mr,zT),e(Mr,zr),e(zr,qT),e(Mr,PT),e(je,CT),e(je,qr),e(qr,jT),e(qr,$i),e($i,xT),e(qr,LT),e(je,OT),e(je,Pr),e(Pr,DT),e(Pr,Cr),e(Cr,AT),e(Pr,NT),e(je,IT),w(po,je,null),e(je,ST),e(je,sn),w(jr,sn,null),e(sn,BT),e(sn,$t),e($t,WT),e($t,Ei),e(Ei,QT),e($t,UT),e($t,vd),e(vd,RT),e($t,HT),e(sn,VT),w(ho,sn,null),e(sn,YT),e(sn,yd),e(yd,KT),e(sn,GT),w(xr,sn,null),h(s,eu,f),h(s,Et,f),e(Et,fo),e(fo,wd),w(Lr,wd,null),e(Et,ZT),e(Et,bd),e(bd,XT),h(s,nu,f),h(s,xe,f),w(Or,xe,null),e(xe,JT),e(xe,$d),e($d,eF),e(xe,nF),e(xe,Dr),e(Dr,tF),e(Dr,Ar),e(Ar,oF),e(Dr,sF),e(xe,rF),e(xe,Nr),e(Nr,aF),e(Nr,Mi),e(Mi,iF),e(Nr,lF),e(xe,dF),e(xe,Ir),e(Ir,cF),e(Ir,Sr),e(Sr,uF),e(Ir,pF),e(xe,hF),w(mo,xe,null),e(xe,fF),e(xe,rn),w(Br,rn,null),e(rn,mF),e(rn,Mt),e(Mt,gF),e(Mt,zi),e(zi,_F),e(Mt,TF),e(Mt,Ed),e(Ed,FF),e(Mt,kF),e(rn,vF),w(go,rn,null),e(rn,yF),e(rn,Md),e(Md,wF),e(rn,bF),w(Wr,rn,null),h(s,tu,f),h(s,zt,f),e(zt,_o),e(_o,zd),w(Qr,zd,null),e(zt,$F),e(zt,qd),e(qd,EF),h(s,ou,f),h(s,Le,f),w(Ur,Le,null),e(Le,MF),e(Le,Pd),e(Pd,zF),e(Le,qF),e(Le,Rr),e(Rr,PF),e(Rr,Hr),e(Hr,CF),e(Rr,jF),e(Le,xF),e(Le,Vr),e(Vr,LF),e(Vr,qi),e(qi,OF),e(Vr,DF),e(Le,AF),e(Le,Yr),e(Yr,NF),e(Yr,Kr),e(Kr,IF),e(Yr,SF),e(Le,BF),w(To,Le,null),e(Le,WF),e(Le,an),w(Gr,an,null),e(an,QF),e(an,qt),e(qt,UF),e(qt,Pi),e(Pi,RF),e(qt,HF),e(qt,Cd),e(Cd,VF),e(qt,YF),e(an,KF),w(Fo,an,null),e(an,GF),e(an,jd),e(jd,ZF),e(an,XF),w(Zr,an,null),h(s,su,f),h(s,Pt,f),e(Pt,ko),e(ko,xd),w(Xr,xd,null),e(Pt,JF),e(Pt,Ld),e(Ld,ek),h(s,ru,f),h(s,Oe,f),w(Jr,Oe,null),e(Oe,nk),e(Oe,ea),e(ea,tk),e(ea,Od),e(Od,ok),e(ea,sk),e(Oe,rk),e(Oe,na),e(na,ak),e(na,ta),e(ta,ik),e(na,lk),e(Oe,dk),e(Oe,oa),e(oa,ck),e(oa,Ci),e(Ci,uk),e(oa,pk),e(Oe,hk),e(Oe,sa),e(sa,fk),e(sa,ra),e(ra,mk),e(sa,gk),e(Oe,_k),w(vo,Oe,null),e(Oe,Tk),e(Oe,ln),w(aa,ln,null),e(ln,Fk),e(ln,Ct),e(Ct,kk),e(Ct,ji),e(ji,vk),e(Ct,yk),e(Ct,Dd),e(Dd,wk),e(Ct,bk),e(ln,$k),w(yo,ln,null),e(ln,Ek),e(ln,Ad),e(Ad,Mk),e(ln,zk),w(ia,ln,null),h(s,au,f),h(s,jt,f),e(jt,wo),e(wo,Nd),w(la,Nd,null),e(jt,qk),e(jt,Id),e(Id,Pk),h(s,iu,f),h(s,De,f),w(da,De,null),e(De,Ck),e(De,Sd),e(Sd,jk),e(De,xk),e(De,ca),e(ca,Lk),e(ca,ua),e(ua,Ok),e(ca,Dk),e(De,Ak),e(De,pa),e(pa,Nk),e(pa,xi),e(xi,Ik),e(pa,Sk),e(De,Bk),e(De,ha),e(ha,Wk),e(ha,fa),e(fa,Qk),e(ha,Uk),e(De,Rk),w(bo,De,null),e(De,Hk),e(De,dn),w(ma,dn,null),e(dn,Vk),e(dn,xt),e(xt,Yk),e(xt,Li),e(Li,Kk),e(xt,Gk),e(xt,Bd),e(Bd,Zk),e(xt,Xk),e(dn,Jk),w($o,dn,null),e(dn,ev),e(dn,Wd),e(Wd,nv),e(dn,tv),w(ga,dn,null),h(s,lu,f),h(s,Lt,f),e(Lt,Eo),e(Eo,Qd),w(_a,Qd,null),e(Lt,ov),e(Lt,Ud),e(Ud,sv),h(s,du,f),h(s,Ae,f),w(Ta,Ae,null),e(Ae,rv),e(Ae,Rd),e(Rd,av),e(Ae,iv),e(Ae,Fa),e(Fa,lv),e(Fa,ka),e(ka,dv),e(Fa,cv),e(Ae,uv),e(Ae,va),e(va,pv),e(va,Oi),e(Oi,hv),e(va,fv),e(Ae,mv),e(Ae,ya),e(ya,gv),e(ya,wa),e(wa,_v),e(ya,Tv),e(Ae,Fv),w(Mo,Ae,null),e(Ae,kv),e(Ae,cn),w(ba,cn,null),e(cn,vv),e(cn,Ot),e(Ot,yv),e(Ot,Di),e(Di,wv),e(Ot,bv),e(Ot,Hd),e(Hd,$v),e(Ot,Ev),e(cn,Mv),w(zo,cn,null),e(cn,zv),e(cn,Vd),e(Vd,qv),e(cn,Pv),w($a,cn,null),h(s,cu,f),h(s,Dt,f),e(Dt,qo),e(qo,Yd),w(Ea,Yd,null),e(Dt,Cv),e(Dt,Kd),e(Kd,jv),h(s,uu,f),h(s,Ne,f),w(Ma,Ne,null),e(Ne,xv),e(Ne,Gd),e(Gd,Lv),e(Ne,Ov),e(Ne,za),e(za,Dv),e(za,qa),e(qa,Av),e(za,Nv),e(Ne,Iv),e(Ne,Pa),e(Pa,Sv),e(Pa,Ai),e(Ai,Bv),e(Pa,Wv),e(Ne,Qv),e(Ne,Ca),e(Ca,Uv),e(Ca,ja),e(ja,Rv),e(Ca,Hv),e(Ne,Vv),w(Po,Ne,null),e(Ne,Yv),e(Ne,un),w(xa,un,null),e(un,Kv),e(un,At),e(At,Gv),e(At,Ni),e(Ni,Zv),e(At,Xv),e(At,Zd),e(Zd,Jv),e(At,ey),e(un,ny),w(Co,un,null),e(un,ty),e(un,Xd),e(Xd,oy),e(un,sy),w(La,un,null),h(s,pu,f),h(s,Nt,f),e(Nt,jo),e(jo,Jd),w(Oa,Jd,null),e(Nt,ry),e(Nt,ec),e(ec,ay),h(s,hu,f),h(s,Ie,f),w(Da,Ie,null),e(Ie,iy),e(Ie,It),e(It,ly),e(It,nc),e(nc,dy),e(It,cy),e(It,tc),e(tc,uy),e(It,py),e(Ie,hy),e(Ie,Aa),e(Aa,fy),e(Aa,Na),e(Na,my),e(Aa,gy),e(Ie,_y),e(Ie,Ia),e(Ia,Ty),e(Ia,Ii),e(Ii,Fy),e(Ia,ky),e(Ie,vy),e(Ie,Sa),e(Sa,yy),e(Sa,Ba),e(Ba,wy),e(Sa,by),e(Ie,$y),w(xo,Ie,null),e(Ie,Ey),e(Ie,pn),w(Wa,pn,null),e(pn,My),e(pn,St),e(St,zy),e(St,Si),e(Si,qy),e(St,Py),e(St,oc),e(oc,Cy),e(St,jy),e(pn,xy),w(Lo,pn,null),e(pn,Ly),e(pn,sc),e(sc,Oy),e(pn,Dy),w(Qa,pn,null),fu=!0},p(s,[f]){const Ua={};f&2&&(Ua.$$scope={dirty:f,ctx:s}),Kt.$set(Ua);const rc={};f&2&&(rc.$$scope={dirty:f,ctx:s}),Zt.$set(rc);const ac={};f&2&&(ac.$$scope={dirty:f,ctx:s}),Jt.$set(ac);const ic={};f&2&&(ic.$$scope={dirty:f,ctx:s}),no.$set(ic);const Ra={};f&2&&(Ra.$$scope={dirty:f,ctx:s}),oo.$set(Ra);const lc={};f&2&&(lc.$$scope={dirty:f,ctx:s}),ro.$set(lc);const dc={};f&2&&(dc.$$scope={dirty:f,ctx:s}),io.$set(dc);const cc={};f&2&&(cc.$$scope={dirty:f,ctx:s}),co.$set(cc);const Ha={};f&2&&(Ha.$$scope={dirty:f,ctx:s}),po.$set(Ha);const uc={};f&2&&(uc.$$scope={dirty:f,ctx:s}),ho.$set(uc);const pc={};f&2&&(pc.$$scope={dirty:f,ctx:s}),mo.$set(pc);const hc={};f&2&&(hc.$$scope={dirty:f,ctx:s}),go.$set(hc);const fc={};f&2&&(fc.$$scope={dirty:f,ctx:s}),To.$set(fc);const mc={};f&2&&(mc.$$scope={dirty:f,ctx:s}),Fo.$set(mc);const Va={};f&2&&(Va.$$scope={dirty:f,ctx:s}),vo.$set(Va);const gc={};f&2&&(gc.$$scope={dirty:f,ctx:s}),yo.$set(gc);const Ce={};f&2&&(Ce.$$scope={dirty:f,ctx:s}),bo.$set(Ce);const _c={};f&2&&(_c.$$scope={dirty:f,ctx:s}),$o.$set(_c);const Tc={};f&2&&(Tc.$$scope={dirty:f,ctx:s}),Mo.$set(Tc);const Fc={};f&2&&(Fc.$$scope={dirty:f,ctx:s}),zo.$set(Fc);const kc={};f&2&&(kc.$$scope={dirty:f,ctx:s}),Po.$set(kc);const vc={};f&2&&(vc.$$scope={dirty:f,ctx:s}),Co.$set(vc);const yc={};f&2&&(yc.$$scope={dirty:f,ctx:s}),xo.$set(yc);const wc={};f&2&&(wc.$$scope={dirty:f,ctx:s}),Lo.$set(wc)},i(s){fu||(b(T.$$.fragment,s),b(ne.$$.fragment,s),b(So.$$.fragment,s),b(Bo.$$.fragment,s),b(Qo.$$.fragment,s),b(Uo.$$.fragment,s),b(Ho.$$.fragment,s),b(Yo.$$.fragment,s),b(Go.$$.fragment,s),b(Zo.$$.fragment,s),b(Xo.$$.fragment,s),b(Jo.$$.fragment,s),b(es.$$.fragment,s),b(os.$$.fragment,s),b(ss.$$.fragment,s),b(rs.$$.fragment,s),b(as.$$.fragment,s),b(ls.$$.fragment,s),b(cs.$$.fragment,s),b(us.$$.fragment,s),b(_s.$$.fragment,s),b(Kt.$$.fragment,s),b(Ts.$$.fragment,s),b(Fs.$$.fragment,s),b(ks.$$.fragment,s),b(Es.$$.fragment,s),b(Zt.$$.fragment,s),b(Ms.$$.fragment,s),b(zs.$$.fragment,s),b(qs.$$.fragment,s),b(Ps.$$.fragment,s),b(Jt.$$.fragment,s),b(Cs.$$.fragment,s),b(js.$$.fragment,s),b(xs.$$.fragment,s),b(Ss.$$.fragment,s),b(no.$$.fragment,s),b(Bs.$$.fragment,s),b(Ws.$$.fragment,s),b(Qs.$$.fragment,s),b(Ks.$$.fragment,s),b(oo.$$.fragment,s),b(Gs.$$.fragment,s),b(Zs.$$.fragment,s),b(Xs.$$.fragment,s),b(Js.$$.fragment,s),b(rr.$$.fragment,s),b(ro.$$.fragment,s),b(ar.$$.fragment,s),b(ir.$$.fragment,s),b(lr.$$.fragment,s),b(fr.$$.fragment,s),b(io.$$.fragment,s),b(mr.$$.fragment,s),b(gr.$$.fragment,s),b(_r.$$.fragment,s),b(wr.$$.fragment,s),b(co.$$.fragment,s),b(br.$$.fragment,s),b($r.$$.fragment,s),b(Er.$$.fragment,s),b(po.$$.fragment,s),b(jr.$$.fragment,s),b(ho.$$.fragment,s),b(xr.$$.fragment,s),b(Lr.$$.fragment,s),b(Or.$$.fragment,s),b(mo.$$.fragment,s),b(Br.$$.fragment,s),b(go.$$.fragment,s),b(Wr.$$.fragment,s),b(Qr.$$.fragment,s),b(Ur.$$.fragment,s),b(To.$$.fragment,s),b(Gr.$$.fragment,s),b(Fo.$$.fragment,s),b(Zr.$$.fragment,s),b(Xr.$$.fragment,s),b(Jr.$$.fragment,s),b(vo.$$.fragment,s),b(aa.$$.fragment,s),b(yo.$$.fragment,s),b(ia.$$.fragment,s),b(la.$$.fragment,s),b(da.$$.fragment,s),b(bo.$$.fragment,s),b(ma.$$.fragment,s),b($o.$$.fragment,s),b(ga.$$.fragment,s),b(_a.$$.fragment,s),b(Ta.$$.fragment,s),b(Mo.$$.fragment,s),b(ba.$$.fragment,s),b(zo.$$.fragment,s),b($a.$$.fragment,s),b(Ea.$$.fragment,s),b(Ma.$$.fragment,s),b(Po.$$.fragment,s),b(xa.$$.fragment,s),b(Co.$$.fragment,s),b(La.$$.fragment,s),b(Oa.$$.fragment,s),b(Da.$$.fragment,s),b(xo.$$.fragment,s),b(Wa.$$.fragment,s),b(Lo.$$.fragment,s),b(Qa.$$.fragment,s),fu=!0)},o(s){$(T.$$.fragment,s),$(ne.$$.fragment,s),$(So.$$.fragment,s),$(Bo.$$.fragment,s),$(Qo.$$.fragment,s),$(Uo.$$.fragment,s),$(Ho.$$.fragment,s),$(Yo.$$.fragment,s),$(Go.$$.fragment,s),$(Zo.$$.fragment,s),$(Xo.$$.fragment,s),$(Jo.$$.fragment,s),$(es.$$.fragment,s),$(os.$$.fragment,s),$(ss.$$.fragment,s),$(rs.$$.fragment,s),$(as.$$.fragment,s),$(ls.$$.fragment,s),$(cs.$$.fragment,s),$(us.$$.fragment,s),$(_s.$$.fragment,s),$(Kt.$$.fragment,s),$(Ts.$$.fragment,s),$(Fs.$$.fragment,s),$(ks.$$.fragment,s),$(Es.$$.fragment,s),$(Zt.$$.fragment,s),$(Ms.$$.fragment,s),$(zs.$$.fragment,s),$(qs.$$.fragment,s),$(Ps.$$.fragment,s),$(Jt.$$.fragment,s),$(Cs.$$.fragment,s),$(js.$$.fragment,s),$(xs.$$.fragment,s),$(Ss.$$.fragment,s),$(no.$$.fragment,s),$(Bs.$$.fragment,s),$(Ws.$$.fragment,s),$(Qs.$$.fragment,s),$(Ks.$$.fragment,s),$(oo.$$.fragment,s),$(Gs.$$.fragment,s),$(Zs.$$.fragment,s),$(Xs.$$.fragment,s),$(Js.$$.fragment,s),$(rr.$$.fragment,s),$(ro.$$.fragment,s),$(ar.$$.fragment,s),$(ir.$$.fragment,s),$(lr.$$.fragment,s),$(fr.$$.fragment,s),$(io.$$.fragment,s),$(mr.$$.fragment,s),$(gr.$$.fragment,s),$(_r.$$.fragment,s),$(wr.$$.fragment,s),$(co.$$.fragment,s),$(br.$$.fragment,s),$($r.$$.fragment,s),$(Er.$$.fragment,s),$(po.$$.fragment,s),$(jr.$$.fragment,s),$(ho.$$.fragment,s),$(xr.$$.fragment,s),$(Lr.$$.fragment,s),$(Or.$$.fragment,s),$(mo.$$.fragment,s),$(Br.$$.fragment,s),$(go.$$.fragment,s),$(Wr.$$.fragment,s),$(Qr.$$.fragment,s),$(Ur.$$.fragment,s),$(To.$$.fragment,s),$(Gr.$$.fragment,s),$(Fo.$$.fragment,s),$(Zr.$$.fragment,s),$(Xr.$$.fragment,s),$(Jr.$$.fragment,s),$(vo.$$.fragment,s),$(aa.$$.fragment,s),$(yo.$$.fragment,s),$(ia.$$.fragment,s),$(la.$$.fragment,s),$(da.$$.fragment,s),$(bo.$$.fragment,s),$(ma.$$.fragment,s),$($o.$$.fragment,s),$(ga.$$.fragment,s),$(_a.$$.fragment,s),$(Ta.$$.fragment,s),$(Mo.$$.fragment,s),$(ba.$$.fragment,s),$(zo.$$.fragment,s),$($a.$$.fragment,s),$(Ea.$$.fragment,s),$(Ma.$$.fragment,s),$(Po.$$.fragment,s),$(xa.$$.fragment,s),$(Co.$$.fragment,s),$(La.$$.fragment,s),$(Oa.$$.fragment,s),$(Da.$$.fragment,s),$(xo.$$.fragment,s),$(Wa.$$.fragment,s),$(Lo.$$.fragment,s),$(Qa.$$.fragment,s),fu=!1},d(s){n(p),s&&n(M),s&&n(m),E(T),s&&n(K),s&&n(q),E(ne),s&&n(ie),s&&n(Y),s&&n(j),s&&n(oe),s&&n(le),s&&n(se),s&&n(de),s&&n(C),s&&n(B),s&&n(ee),s&&n(Ec),s&&n(xn),s&&n(Mc),s&&n(Zn),E(So),s&&n(zc),s&&n(Cn),E(Bo),s&&n(qc),s&&n(Jn),E(Qo),s&&n(Pc),s&&n(Pe),E(Uo),E(Ho),E(Yo),E(Go),E(Zo),E(Xo),s&&n(Cc),s&&n(nt),E(Jo),s&&n(jc),s&&n(Ge),E(es),E(os),E(ss),s&&n(xc),s&&n(ot),E(rs),s&&n(Lc),s&&n(st),E(as),s&&n(Oc),s&&n(rt),E(ls),s&&n(Dc),s&&n(at),E(cs),s&&n(Ac),s&&n(We),E(us),E(_s),E(Kt),E(Ts),s&&n(Nc),s&&n(lt),E(Fs),s&&n(Ic),s&&n(Qe),E(ks),E(Es),E(Zt),E(Ms),s&&n(Sc),s&&n(ct),E(zs),s&&n(Bc),s&&n(ut),E(qs),E(Ps),E(Jt),E(Cs),s&&n(Wc),s&&n(ht),E(js),s&&n(Qc),s&&n(Ue),E(xs),E(Ss),E(no),E(Bs),s&&n(Uc),s&&n(mt),E(Ws),s&&n(Rc),s&&n(Re),E(Qs),E(Ks),E(oo),E(Gs),E(Zs),s&&n(Hc),s&&n(_t),E(Xs),s&&n(Vc),s&&n(He),E(Js),E(rr),E(ro),E(ar),s&&n(Yc),s&&n(Ft),E(ir),s&&n(Kc),s&&n(Ve),E(lr),E(fr),E(io),E(mr),s&&n(Gc),s&&n(vt),E(gr),s&&n(Zc),s&&n(Ye),E(_r),E(wr),E(co),E(br),s&&n(Xc),s&&n(bt),E($r),s&&n(Jc),s&&n(je),E(Er),E(po),E(jr),E(ho),E(xr),s&&n(eu),s&&n(Et),E(Lr),s&&n(nu),s&&n(xe),E(Or),E(mo),E(Br),E(go),E(Wr),s&&n(tu),s&&n(zt),E(Qr),s&&n(ou),s&&n(Le),E(Ur),E(To),E(Gr),E(Fo),E(Zr),s&&n(su),s&&n(Pt),E(Xr),s&&n(ru),s&&n(Oe),E(Jr),E(vo),E(aa),E(yo),E(ia),s&&n(au),s&&n(jt),E(la),s&&n(iu),s&&n(De),E(da),E(bo),E(ma),E($o),E(ga),s&&n(lu),s&&n(Lt),E(_a),s&&n(du),s&&n(Ae),E(Ta),E(Mo),E(ba),E(zo),E($a),s&&n(cu),s&&n(Dt),E(Ea),s&&n(uu),s&&n(Ne),E(Ma),E(Po),E(xa),E(Co),E(La),s&&n(pu),s&&n(Nt),E(Oa),s&&n(hu),s&&n(Ie),E(Da),E(xo),E(Wa),E(Lo),E(Qa)}}}const X$={local:"funnel-transformer",sections:[{local:"overview",title:"Overview"},{local:"transformers.FunnelConfig",title:"FunnelConfig"},{local:"transformers.FunnelTokenizer",title:"FunnelTokenizer"},{local:"transformers.FunnelTokenizerFast",title:"FunnelTokenizerFast"},{local:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",title:"Funnel specific outputs"},{local:"transformers.FunnelBaseModel",title:"FunnelBaseModel"},{local:"transformers.FunnelModel",title:"FunnelModel"},{local:"transformers.FunnelForPreTraining",title:"FunnelModelForPreTraining"},{local:"transformers.FunnelForMaskedLM",title:"FunnelForMaskedLM"},{local:"transformers.FunnelForSequenceClassification",title:"FunnelForSequenceClassification"},{local:"transformers.FunnelForMultipleChoice",title:"FunnelForMultipleChoice"},{local:"transformers.FunnelForTokenClassification",title:"FunnelForTokenClassification"},{local:"transformers.FunnelForQuestionAnswering",title:"FunnelForQuestionAnswering"},{local:"transformers.TFFunnelBaseModel",title:"TFFunnelBaseModel"},{local:"transformers.TFFunnelModel",title:"TFFunnelModel"},{local:"transformers.TFFunnelForPreTraining",title:"TFFunnelModelForPreTraining"},{local:"transformers.TFFunnelForMaskedLM",title:"TFFunnelForMaskedLM"},{local:"transformers.TFFunnelForSequenceClassification",title:"TFFunnelForSequenceClassification"},{local:"transformers.TFFunnelForMultipleChoice",title:"TFFunnelForMultipleChoice"},{local:"transformers.TFFunnelForTokenClassification",title:"TFFunnelForTokenClassification"},{local:"transformers.TFFunnelForQuestionAnswering",title:"TFFunnelForQuestionAnswering"}],title:"Funnel Transformer"};function J$(W,p,M){let{fw:m}=p;return W.$$set=g=>{"fw"in g&&M(0,m=g.fw)},[m]}class r2 extends w${constructor(p){super();b$(this,p,J$,Z$,$$,{fw:0})}}export{r2 as default,X$ as metadata};
