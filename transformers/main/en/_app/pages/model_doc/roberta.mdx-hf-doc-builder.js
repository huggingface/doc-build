import{S as fe,i as ge,s as _e,O as M,P as z,a as s,d as o,b as n,g as b,G as e,L as j,f as CR,t as a,h as r,w as y,l as FR,x as C,y as T,n as RR,o as $,B as x,p as ER,q as F,e as l,k as g,c as d,m as _,j as MR,M as zR,v as qR}from"../../chunks/vendor-hf-doc-builder.js";import{T as ye}from"../../chunks/Tip-hf-doc-builder.js";import{D as W}from"../../chunks/Docstring-hf-doc-builder.js";import{C as he}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as We}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as pe}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";const W8={"text-classification":{name:"Text Classification",subtasks:[{type:"acceptability-classification",name:"Acceptability Classification"},{type:"entity-linking-classification",name:"Entity Linking Classification"},{type:"fact-checking",name:"Fact Checking"},{type:"intent-classification",name:"Intent Classification"},{type:"multi-class-classification",name:"Multi Class Classification"},{type:"multi-label-classification",name:"Multi Label Classification"},{type:"multi-input-text-classification",name:"Multi-input Text Classification"},{type:"natural-language-inference",name:"Natural Language Inference"},{type:"semantic-similarity-classification",name:"Semantic Similarity Classification"},{type:"sentiment-classification",name:"Sentiment Classification"},{type:"topic-classification",name:"Topic Classification"},{type:"semantic-similarity-scoring",name:"Semantic Similarity Scoring"},{type:"sentiment-scoring",name:"Sentiment Scoring"},{type:"sentiment-analysis",name:"Sentiment Analysis"},{type:"hate-speech-detection",name:"Hate Speech Detection"},{type:"text-scoring",name:"Text Scoring"}],modality:"nlp",color:"orange"},"token-classification":{name:"Token Classification",subtasks:[{type:"named-entity-recognition",name:"Named Entity Recognition"},{type:"part-of-speech",name:"Part of Speech"},{type:"parsing",name:"Parsing"},{type:"lemmatization",name:"Lemmatization"},{type:"word-sense-disambiguation",name:"Word Sense Disambiguation"},{type:"coreference-resolution",name:"Coreference-resolution"}],modality:"nlp",color:"blue"},"table-question-answering":{name:"Table Question Answering",modality:"nlp",color:"green"},"question-answering":{name:"Question Answering",subtasks:[{type:"extractive-qa",name:"Extractive QA"},{type:"open-domain-qa",name:"Open Domain QA"},{type:"closed-domain-qa",name:"Closed Domain QA"}],modality:"nlp",color:"blue"},"zero-shot-classification":{name:"Zero-Shot Classification",modality:"nlp",color:"yellow"},translation:{name:"Translation",modality:"nlp",color:"green"},summarization:{name:"Summarization",subtasks:[{type:"news-articles-summarization",name:"News Articles Summarization"},{type:"news-articles-headline-generation",name:"News Articles Headline Generation"}],modality:"nlp",color:"indigo"},conversational:{name:"Conversational",subtasks:[{type:"dialogue-generation",name:"Dialogue Generation"}],modality:"nlp",color:"green"},"feature-extraction":{name:"Feature Extraction",modality:"multimodal",color:"red"},"text-generation":{name:"Text Generation",subtasks:[{type:"dialogue-modeling",name:"Dialogue Modeling"},{type:"language-modeling",name:"Language Modeling"}],modality:"nlp",color:"indigo"},"text2text-generation":{name:"Text2Text Generation",subtasks:[{type:"text-simplification",name:"Text simplification"},{type:"explanation-generation",name:"Explanation Generation"},{type:"abstractive-qa",name:"Abstractive QA"},{type:"open-domain-abstractive-qa",name:"Open Domain Abstractive QA"},{type:"closed-domain-qa",name:"Closed Domain QA"},{type:"open-book-qa",name:"Open Book QA"},{type:"closed-book-qa",name:"Closed Book QA"}],modality:"nlp",color:"indigo"},"fill-mask":{name:"Fill-Mask",subtasks:[{type:"slot-filling",name:"Slot Filling"},{type:"masked-language-modeling",name:"Masked Language Modeling"}],modality:"nlp",color:"red"},"sentence-similarity":{name:"Sentence Similarity",modality:"nlp",color:"yellow"},"text-to-speech":{name:"Text-to-Speech",modality:"audio",color:"yellow"},"automatic-speech-recognition":{name:"Automatic Speech Recognition",modality:"audio",color:"yellow"},"audio-to-audio":{name:"Audio-to-Audio",modality:"audio",color:"blue"},"audio-classification":{name:"Audio Classification",subtasks:[{type:"keyword-spotting",name:"Keyword Spotting"},{type:"speaker-identification",name:"Speaker Identification"},{type:"audio-intent-classification",name:"Audio Intent Classification"},{type:"audio-emotion-recognition",name:"Audio Emotion Recognition"},{type:"audio-language-identification",name:"Audio Language Identification"}],modality:"audio",color:"green"},"voice-activity-detection":{name:"Voice Activity Detection",modality:"audio",color:"red"},"image-classification":{name:"Image Classification",subtasks:[{type:"multi-label-image-classification",name:"Multi Label Image Classification"},{type:"multi-class-image-classification",name:"Multi Class Image Classification"}],modality:"cv",color:"blue"},"object-detection":{name:"Object Detection",subtasks:[{type:"face-detection",name:"Face Detection"},{type:"vehicle-detection",name:"Vehicle Detection"}],modality:"cv",color:"yellow"},"image-segmentation":{name:"Image Segmentation",subtasks:[{type:"instance-segmentation",name:"Instance Segmentation"},{type:"semantic-segmentation",name:"Semantic Segmentation"},{type:"panoptic-segmentation",name:"Panoptic Segmentation"}],modality:"cv",color:"green"},"text-to-image":{name:"Text-to-Image",modality:"multimodal",color:"yellow"},"image-to-text":{name:"Image-to-Text",subtasks:[{type:"image-captioning",name:"Image Captioning"}],modality:"multimodal",color:"red"},"image-to-image":{name:"Image-to-Image",modality:"cv",color:"indigo"},"unconditional-image-generation":{name:"Unconditional Image Generation",modality:"cv",color:"green"},"reinforcement-learning":{name:"Reinforcement Learning",modality:"rl",color:"red",hideInDatasets:!0},robotics:{name:"Robotics",modality:"rl",subtasks:[{type:"grasping",name:"Grasping"},{type:"task-planning",name:"Task Planning"}],color:"blue",hideInDatasets:!0},"tabular-classification":{name:"Tabular Classification",modality:"tabular",subtasks:[{type:"tabular-multi-class-classification",name:"Tabular Multi Class Classification"},{type:"tabular-multi-label-classification",name:"Tabular Multi Label Classification"}],color:"blue"},"tabular-regression":{name:"Tabular Regression",modality:"tabular",subtasks:[{type:"tabular-single-column-regression",name:"Tabular Single Column Regression"}],color:"blue"},"tabular-to-text":{name:"Tabular to Text",modality:"tabular",subtasks:[{type:"rdf-to-text",name:"RDF to text"}],color:"blue",hideInModels:!0},"table-to-text":{name:"Table to Text",modality:"nlp",color:"blue",hideInModels:!0},"multiple-choice":{name:"Multiple Choice",subtasks:[{type:"multiple-choice-qa",name:"Multiple Choice QA"},{type:"multiple-choice-coreference-resolution",name:"Multiple Choice Coreference Resolution"}],modality:"nlp",color:"blue",hideInModels:!0},"text-retrieval":{name:"Text Retrieval",subtasks:[{type:"document-retrieval",name:"Document Retrieval"},{type:"utterance-retrieval",name:"Utterance Retrieval"},{type:"entity-linking-retrieval",name:"Entity Linking Retrieval"},{type:"fact-checking-retrieval",name:"Fact Checking Retrieval"}],modality:"nlp",color:"indigo",hideInModels:!0},"time-series-forecasting":{name:"Time Series Forecasting",modality:"tabular",subtasks:[{type:"univariate-time-series-forecasting",name:"Univariate Time Series Forecasting"},{type:"multivariate-time-series-forecasting",name:"Multivariate Time Series Forecasting"}],color:"blue",hideInModels:!0},"visual-question-answering":{name:"Visual Question Answering",subtasks:[{type:"visual-question-answering",name:"Visual Question Answering"}],modality:"multimodal",color:"red"},"document-question-answering":{name:"Document Question Answering",subtasks:[{type:"document-question-answering",name:"Document Question Answering"}],modality:"multimodal",color:"blue",hideInDatasets:!0},"zero-shot-image-classification":{name:"Zero-Shot Image Classification",modality:"cv",color:"yellow"},other:{name:"Other",modality:"other",color:"blue",hideInModels:!0}};function jR(k){let t,u;return{c(){t=M("svg"),u=M("path"),this.h()},l(c){t=z(c,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var h=s(t);u=z(h,"path",{d:!0,fill:!0}),s(u).forEach(o),h.forEach(o),this.h()},h(){n(u,"d","M25 4H10a2.002 2.002 0 0 0-2 2v14.556A3.955 3.955 0 0 0 6 20a4 4 0 1 0 4 4V12h15v8.556A3.954 3.954 0 0 0 23 20a4 4 0 1 0 4 4V6a2.002 2.002 0 0 0-2-2zM6 26a2 2 0 1 1 2-2a2.002 2.002 0 0 1-2 2zm17 0a2 2 0 1 1 2-2a2.003 2.003 0 0 1-2 2zM10 6h15v4H10z"),n(u,"fill","currentColor"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 32 32")},m(c,h){b(c,t,h),e(t,u)},p(c,[h]){h&1&&n(t,"class",c[0])},i:j,o:j,d(c){c&&o(t)}}}function AR(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class PR extends fe{constructor(t){super();ge(this,t,AR,jR,_e,{classNames:0})}}function LR(k){let t,u;return{c(){t=M("svg"),u=M("path"),this.h()},l(c){t=z(c,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var h=s(t);u=z(h,"path",{"fill-rule":!0,"clip-rule":!0,d:!0,fill:!0}),s(u).forEach(o),h.forEach(o),this.h()},h(){n(u,"fill-rule","evenodd"),n(u,"clip-rule","evenodd"),n(u,"d","M4.5 11.12H5C5.26513 11.1203 5.51933 11.2257 5.70681 11.4132C5.89429 11.6007 5.99973 11.8549 6 12.12V13.12C5.99973 13.3851 5.89429 13.6393 5.70681 13.8268C5.51933 14.0143 5.26513 14.1197 5 14.12H4.5V16.12H3.5V14.12H3C2.73486 14.1197 2.48066 14.0143 2.29319 13.8268C2.10571 13.6393 2.00026 13.3851 2 13.12V12.12C2.00026 11.8549 2.10571 11.6007 2.29319 11.4132C2.48066 11.2257 2.73486 11.1203 3 11.12H3.5V2.12H4.5V11.12ZM3 13.12H5V12.12H3V13.12ZM10 4.12H9.5V2.12H8.5V4.12H8C7.73487 4.12027 7.48067 4.22571 7.29319 4.41319C7.10571 4.60067 7.00027 4.85487 7 5.12V6.12C7.00027 6.38514 7.10571 6.63934 7.29319 6.82682C7.48067 7.01429 7.73487 7.11974 8 7.12H8.5V16.12H9.5V7.12H10C10.2651 7.11974 10.5193 7.01429 10.7068 6.82682C10.8943 6.63934 10.9997 6.38514 11 6.12V5.12C10.9997 4.85487 10.8943 4.60067 10.7068 4.41319C10.5193 4.22571 10.2651 4.12027 10 4.12ZM10 6.12H8V5.12H10V6.12ZM15 8.12H14.5V2.12H13.5V8.12H13C12.7349 8.12027 12.4807 8.22571 12.2932 8.41319C12.1057 8.60067 12.0003 8.85486 12 9.12V10.12C12.0003 10.3851 12.1057 10.6393 12.2932 10.8268C12.4807 11.0143 12.7349 11.1197 13 11.12H13.5V16.12H14.5V11.12H15C15.2651 11.1196 15.5192 11.0141 15.7067 10.8267C15.8941 10.6392 15.9996 10.3851 16 10.12V9.12C15.9997 8.85486 15.8943 8.60067 15.7068 8.41319C15.5193 8.22571 15.2651 8.12027 15 8.12ZM15 10.12H13V9.12H15V10.12Z"),n(u,"fill","currentColor"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 18 18")},m(c,h){b(c,t,h),e(t,u)},p(c,[h]){h&1&&n(t,"class",c[0])},i:j,o:j,d(c){c&&o(t)}}}function NR(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class IR extends fe{constructor(t){super();ge(this,t,NR,LR,_e,{classNames:0})}}function OR(k){let t,u;return{c(){t=M("svg"),u=M("path"),this.h()},l(c){t=z(c,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var h=s(t);u=z(h,"path",{"fill-rule":!0,"clip-rule":!0,d:!0,fill:!0}),s(u).forEach(o),h.forEach(o),this.h()},h(){n(u,"fill-rule","evenodd"),n(u,"clip-rule","evenodd"),n(u,"d","M8.38893 3.42133C7.9778 3.14662 7.49446 3 7 3C6.33696 3 5.70108 3.26339 5.23223 3.73223C4.76339 4.20107 4.5 4.83696 4.5 5.5C4.5 5.99445 4.64662 6.4778 4.92133 6.88893C5.19603 7.30005 5.58648 7.62048 6.04329 7.8097C6.50011 7.99892 7.00278 8.04843 7.48773 7.95196C7.97268 7.8555 8.41814 7.6174 8.76777 7.26777C9.1174 6.91814 9.3555 6.47268 9.45197 5.98773C9.54843 5.50277 9.49892 5.00011 9.3097 4.54329C9.12048 4.08648 8.80005 3.69603 8.38893 3.42133ZM5.05551 2.58986C5.63108 2.20527 6.30777 2 7 2C7.92826 2 8.8185 2.36875 9.47488 3.02513C10.1313 3.6815 10.5 4.57174 10.5 5.5C10.5 6.19223 10.2947 6.86892 9.91015 7.4445C9.52556 8.02007 8.97894 8.46867 8.33939 8.73358C7.69985 8.99849 6.99612 9.0678 6.31719 8.93275C5.63825 8.7977 5.01461 8.46436 4.52513 7.97487C4.03564 7.48539 3.7023 6.86175 3.56725 6.18282C3.4322 5.50388 3.50152 4.80015 3.76642 4.16061C4.03133 3.52107 4.47993 2.97444 5.05551 2.58986ZM14.85 9.6425L15.7075 10.5C15.8005 10.5927 15.8743 10.7029 15.9245 10.8242C15.9747 10.9456 16.0004 11.0757 16 11.207V16H2V13.5C2.00106 12.5721 2.37015 11.6824 3.0263 11.0263C3.68244 10.3701 4.57207 10.0011 5.5 10H8.5C9.42793 10.0011 10.3176 10.3701 10.9737 11.0263C11.6299 11.6824 11.9989 12.5721 12 13.5V15H15V11.207L14.143 10.35C13.9426 10.4476 13.7229 10.4989 13.5 10.5C13.2033 10.5 12.9133 10.412 12.6666 10.2472C12.42 10.0824 12.2277 9.84811 12.1142 9.57403C12.0006 9.29994 11.9709 8.99834 12.0288 8.70737C12.0867 8.41639 12.2296 8.14912 12.4393 7.93934C12.6491 7.72956 12.9164 7.5867 13.2074 7.52882C13.4983 7.47094 13.7999 7.50065 14.074 7.61418C14.3481 7.72771 14.5824 7.91997 14.7472 8.16665C14.912 8.41332 15 8.70333 15 9C14.9988 9.22271 14.9475 9.44229 14.85 9.6425ZM3.73311 11.7331C3.26444 12.2018 3.00079 12.8372 3 13.5V15H11V13.5C10.9992 12.8372 10.7356 12.2018 10.2669 11.7331C9.79822 11.2644 9.1628 11.0008 8.5 11H5.5C4.8372 11.0008 4.20178 11.2644 3.73311 11.7331Z"),n(u,"fill","currentColor"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 18 18")},m(c,h){b(c,t,h),e(t,u)},p(c,[h]){h&1&&n(t,"class",c[0])},i:j,o:j,d(c){c&&o(t)}}}function DR(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class SR extends fe{constructor(t){super();ge(this,t,DR,OR,_e,{classNames:0})}}function HR(k){let t,u,c,h;return{c(){t=M("svg"),u=M("path"),c=M("path"),h=M("path"),this.h()},l(f){t=z(f,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var p=s(t);u=z(p,"path",{d:!0}),s(u).forEach(o),c=z(p,"path",{d:!0}),s(c).forEach(o),h=z(p,"path",{d:!0}),s(h).forEach(o),p.forEach(o),this.h()},h(){n(u,"d","M10.0461 16.7125L9.1 16.1687L11.275 12.3625H14.5375C14.8259 12.3625 15.1025 12.2479 15.3065 12.044C15.5104 11.84 15.625 11.5634 15.625 11.275V4.74998C15.625 4.46156 15.5104 4.18495 15.3065 3.981C15.1025 3.77706 14.8259 3.66248 14.5375 3.66248H3.6625C3.37407 3.66248 3.09746 3.77706 2.89352 3.981C2.68957 4.18495 2.575 4.46156 2.575 4.74998V11.275C2.575 11.5634 2.68957 11.84 2.89352 12.044C3.09746 12.2479 3.37407 12.3625 3.6625 12.3625H8.55625V13.45H3.6625C3.08565 13.45 2.53243 13.2208 2.12454 12.8129C1.71665 12.405 1.4875 11.8518 1.4875 11.275V4.74998C1.4875 4.17314 1.71665 3.61992 2.12454 3.21202C2.53243 2.80413 3.08565 2.57498 3.6625 2.57498H14.5375C15.1143 2.57498 15.6676 2.80413 16.0755 3.21202C16.4833 3.61992 16.7125 4.17314 16.7125 4.74998V11.275C16.7125 11.8518 16.4833 12.405 16.0755 12.8129C15.6676 13.2208 15.1143 13.45 14.5375 13.45H11.9057L10.0461 16.7125Z"),n(c,"d","M4.75 5.83746H13.45V6.92496H4.75V5.83746Z"),n(h,"d","M4.75 9.10004H10.1875V10.1875H4.75V9.10004Z"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 18 18")},m(f,p){b(f,t,p),e(t,u),e(t,c),e(t,h)},p(f,[p]){p&1&&n(t,"class",f[0])},i:j,o:j,d(f){f&&o(t)}}}function VR(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class BR extends fe{constructor(t){super();ge(this,t,VR,HR,_e,{classNames:0})}}function WR(k){let t,u;return{c(){t=M("svg"),u=M("path"),this.h()},l(c){t=z(c,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var h=s(t);u=z(h,"path",{d:!0}),s(u).forEach(o),h.forEach(o),this.h()},h(){n(u,"d","M27 3H5a2 2 0 0 0-2 2v22a2 2 0 0 0 2 2h22a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2zm0 2v4H5V5zm-10 6h10v7H17zm-2 7H5v-7h10zM5 20h10v7H5zm12 7v-7h10v7z"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 32 32")},m(c,h){b(c,t,h),e(t,u)},p(c,[h]){h&1&&n(t,"class",c[0])},i:j,o:j,d(c){c&&o(t)}}}function UR(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class QR extends fe{constructor(t){super();ge(this,t,UR,WR,_e,{classNames:0})}}function ZR(k){let t,u,c,h;return{c(){t=M("svg"),u=M("path"),c=M("path"),h=M("path"),this.h()},l(f){t=z(f,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var p=s(t);u=z(p,"path",{d:!0}),s(u).forEach(o),c=z(p,"path",{d:!0}),s(c).forEach(o),h=z(p,"path",{d:!0}),s(h).forEach(o),p.forEach(o),this.h()},h(){n(u,"d","M12.3625 13.85H10.1875V12.7625H12.3625V10.5875H13.45V12.7625C13.4497 13.0508 13.335 13.3272 13.1312 13.5311C12.9273 13.735 12.6508 13.8497 12.3625 13.85V13.85Z"),n(c,"d","M5.8375 8.41246H4.75V6.23746C4.75029 5.94913 4.86496 5.67269 5.06884 5.4688C5.27272 5.26492 5.54917 5.15025 5.8375 5.14996H8.0125V6.23746H5.8375V8.41246Z"),n(h,"d","M15.625 5.14998H13.45V2.97498C13.4497 2.68665 13.335 2.4102 13.1312 2.20632C12.9273 2.00244 12.6508 1.88777 12.3625 1.88748H2.575C2.28666 1.88777 2.01022 2.00244 1.80633 2.20632C1.60245 2.4102 1.48778 2.68665 1.4875 2.97498V12.7625C1.48778 13.0508 1.60245 13.3273 1.80633 13.5311C2.01022 13.735 2.28666 13.8497 2.575 13.85H4.75V16.025C4.75028 16.3133 4.86495 16.5898 5.06883 16.7936C5.27272 16.9975 5.54916 17.1122 5.8375 17.1125H15.625C15.9133 17.1122 16.1898 16.9975 16.3937 16.7936C16.5975 16.5898 16.7122 16.3133 16.7125 16.025V6.23748C16.7122 5.94915 16.5975 5.6727 16.3937 5.46882C16.1898 5.26494 15.9133 5.15027 15.625 5.14998V5.14998ZM15.625 16.025H5.8375V13.85H8.0125V12.7625H5.8375V10.5875H4.75V12.7625H2.575V2.97498H12.3625V5.14998H10.1875V6.23748H12.3625V8.41248H13.45V6.23748H15.625V16.025Z"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 18 19")},m(f,p){b(f,t,p),e(t,u),e(t,c),e(t,h)},p(f,[p]){p&1&&n(t,"class",f[0])},i:j,o:j,d(f){f&&o(t)}}}function KR(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class U8 extends fe{constructor(t){super();ge(this,t,KR,ZR,_e,{classNames:0})}}function JR(k){let t,u,c,h;return{c(){t=M("svg"),u=M("polygon"),c=M("path"),h=M("path"),this.h()},l(f){t=z(f,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var p=s(t);u=z(p,"polygon",{points:!0}),s(u).forEach(o),c=z(p,"path",{d:!0}),s(c).forEach(o),h=z(p,"path",{d:!0}),s(h).forEach(o),p.forEach(o),this.h()},h(){n(u,"points","4 20 4 22 8.586 22 2 28.586 3.414 30 10 23.414 10 28 12 28 12 20 4 20"),n(c,"d","M19,14a3,3,0,1,0-3-3A3,3,0,0,0,19,14Zm0-4a1,1,0,1,1-1,1A1,1,0,0,1,19,10Z"),n(h,"d","M26,4H6A2,2,0,0,0,4,6V16H6V6H26V21.17l-3.59-3.59a2,2,0,0,0-2.82,0L18,19.17,11.8308,13l-1.4151,1.4155L14,18l2.59,2.59a2,2,0,0,0,2.82,0L21,19l5,5v2H16v2H26a2,2,0,0,0,2-2V6A2,2,0,0,0,26,4Z"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 32 32")},m(f,p){b(f,t,p),e(t,u),e(t,c),e(t,h)},p(f,[p]){p&1&&n(t,"class",f[0])},i:j,o:j,d(f){f&&o(t)}}}function GR(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class YR extends fe{constructor(t){super();ge(this,t,GR,JR,_e,{classNames:0})}}function XR(k){let t,u,c;return{c(){t=M("svg"),u=M("path"),c=M("path"),this.h()},l(h){t=z(h,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var f=s(t);u=z(f,"path",{d:!0}),s(u).forEach(o),c=z(f,"path",{d:!0}),s(c).forEach(o),f.forEach(o),this.h()},h(){n(u,"d","M30,3.4141,28.5859,2,2,28.5859,3.4141,30l2-2H26a2.0027,2.0027,0,0,0,2-2V5.4141ZM26,26H7.4141l7.7929-7.793,2.3788,2.3787a2,2,0,0,0,2.8284,0L22,19l4,3.9973Zm0-5.8318-2.5858-2.5859a2,2,0,0,0-2.8284,0L19,19.1682l-2.377-2.3771L26,7.4141Z"),n(c,"d","M6,22V19l5-4.9966,1.3733,1.3733,1.4159-1.416-1.375-1.375a2,2,0,0,0-2.8284,0L6,16.1716V6H22V4H6A2.002,2.002,0,0,0,4,6V22Z"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 32 32")},m(h,f){b(h,t,f),e(t,u),e(t,c)},p(h,[f]){f&1&&n(t,"class",h[0])},i:j,o:j,d(h){h&&o(t)}}}function eE(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class tE extends fe{constructor(t){super();ge(this,t,eE,XR,_e,{classNames:0})}}function oE(k){let t,u,c,h;return{c(){t=M("svg"),u=M("path"),c=M("path"),h=M("path"),this.h()},l(f){t=z(f,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var p=s(t);u=z(p,"path",{d:!0}),s(u).forEach(o),c=z(p,"path",{d:!0}),s(c).forEach(o),h=z(p,"path",{d:!0}),s(h).forEach(o),p.forEach(o),this.h()},h(){n(u,"d","M24,14a5.99,5.99,0,0,0-4.885,9.4712L14,28.5859,15.4141,30l5.1147-5.1147A5.9971,5.9971,0,1,0,24,14Zm0,10a4,4,0,1,1,4-4A4.0045,4.0045,0,0,1,24,24Z"),n(c,"d","M17,12a3,3,0,1,0-3-3A3.0033,3.0033,0,0,0,17,12Zm0-4a1,1,0,1,1-1,1A1.0009,1.0009,0,0,1,17,8Z"),n(h,"d","M12,24H4V17.9966L9,13l5.5859,5.5859L16,17.168l-5.5859-5.5855a2,2,0,0,0-2.8282,0L4,15.168V4H24v6h2V4a2.0023,2.0023,0,0,0-2-2H4A2.002,2.002,0,0,0,2,4V24a2.0023,2.0023,0,0,0,2,2h8Z"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 32 32")},m(f,p){b(f,t,p),e(t,u),e(t,c),e(t,h)},p(f,[p]){p&1&&n(t,"class",f[0])},i:j,o:j,d(f){f&&o(t)}}}function nE(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class sE extends fe{constructor(t){super();ge(this,t,nE,oE,_e,{classNames:0})}}function aE(k){let t,u,c,h,f;return{c(){t=M("svg"),u=M("path"),c=M("path"),h=M("path"),f=M("path"),this.h()},l(p){t=z(p,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var m=s(t);u=z(m,"path",{d:!0,fill:!0}),s(u).forEach(o),c=z(m,"path",{d:!0,fill:!0}),s(c).forEach(o),h=z(m,"path",{d:!0,fill:!0}),s(h).forEach(o),f=z(m,"path",{d:!0,fill:!0}),s(f).forEach(o),m.forEach(o),this.h()},h(){n(u,"d","M2 9h9V2H2zm2-5h5v3H4z"),n(u,"fill","currentColor"),n(c,"d","M2 19h9v-7H2zm2-5h5v3H4z"),n(c,"fill","currentColor"),n(h,"d","M2 29h9v-7H2zm2-5h5v3H4z"),n(h,"fill","currentColor"),n(f,"d","M27 9h-9l3.41-3.59L20 4l-6 6l6 6l1.41-1.41L18 11h9a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H15v2h12a3 3 0 0 0 3-3V12a3 3 0 0 0-3-3z"),n(f,"fill","currentColor"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 32 32")},m(p,m){b(p,t,m),e(t,u),e(t,c),e(t,h),e(t,f)},p(p,[m]){m&1&&n(t,"class",p[0])},i:j,o:j,d(p){p&&o(t)}}}function rE(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class iE extends fe{constructor(t){super();ge(this,t,rE,aE,_e,{classNames:0})}}function lE(k){let t,u,c,h,f,p;return{c(){t=M("svg"),u=M("path"),c=M("path"),h=M("path"),f=M("path"),p=M("path"),this.h()},l(m){t=z(m,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var R=s(t);u=z(R,"path",{d:!0}),s(u).forEach(o),c=z(R,"path",{d:!0}),s(c).forEach(o),h=z(R,"path",{d:!0}),s(h).forEach(o),f=z(R,"path",{d:!0}),s(f).forEach(o),p=z(R,"path",{d:!0}),s(p).forEach(o),R.forEach(o),this.h()},h(){n(u,"d","M30 15H17V2h-2v13H2v2h13v13h2V17h13v-2z"),n(c,"d","M25.586 20L27 21.414L23.414 25L27 28.586L25.586 30l-5-5l5-5z"),n(h,"d","M11 30H3a1 1 0 0 1-.894-1.447l4-8a1.041 1.041 0 0 1 1.789 0l4 8A1 1 0 0 1 11 30zm-6.382-2h4.764L7 23.236z"),n(f,"d","M28 12h-6a2.002 2.002 0 0 1-2-2V4a2.002 2.002 0 0 1 2-2h6a2.002 2.002 0 0 1 2 2v6a2.002 2.002 0 0 1-2 2zm-6-8v6h6.001L28 4z"),n(p,"d","M7 12a5 5 0 1 1 5-5a5.006 5.006 0 0 1-5 5zm0-8a3 3 0 1 0 3 3a3.003 3.003 0 0 0-3-3z"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 32 32")},m(m,R){b(m,t,R),e(t,u),e(t,c),e(t,h),e(t,f),e(t,p)},p(m,[R]){R&1&&n(t,"class",m[0])},i:j,o:j,d(m){m&&o(t)}}}function dE(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class cE extends fe{constructor(t){super();ge(this,t,dE,lE,_e,{classNames:0})}}function pE(k){let t,u;return{c(){t=M("svg"),u=M("path"),this.h()},l(c){t=z(c,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var h=s(t);u=z(h,"path",{d:!0,fill:!0}),s(u).forEach(o),h.forEach(o),this.h()},h(){n(u,"d","M29 5a2 2 0 0 0-2-2H5a2 2 0 0 0-2 2v22a2 2 0 0 0 2 2h22a2 2 0 0 0 2-2zm-2 0v4H5V5zm0 22H5v-4h22zm0-6H5v-4h22zm0-6H5v-4h22z"),n(u,"fill","currentColor"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 32 32")},m(c,h){b(c,t,h),e(t,u)},p(c,[h]){h&1&&n(t,"class",c[0])},i:j,o:j,d(c){c&&o(t)}}}function hE(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class uE extends fe{constructor(t){super();ge(this,t,hE,pE,_e,{classNames:0})}}function mE(k){let t,u;return{c(){t=M("svg"),u=M("path"),this.h()},l(c){t=z(c,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var h=s(t);u=z(h,"path",{fill:!0,d:!0}),s(u).forEach(o),h.forEach(o),this.h()},h(){n(u,"fill","currentColor"),n(u,"d","m4.67 28l6.39-12l7.3 6.49a2 2 0 0 0 1.7.47a2 2 0 0 0 1.42-1.07L27 10.9l-1.82-.9l-5.49 11l-7.3-6.49a2 2 0 0 0-1.68-.51a2 2 0 0 0-1.42 1L4 25V2H2v26a2 2 0 0 0 2 2h26v-2Z"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 32 32")},m(c,h){b(c,t,h),e(t,u)},p(c,[h]){h&1&&n(t,"class",c[0])},i:j,o:j,d(c){c&&o(t)}}}function fE(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class gE extends fe{constructor(t){super();ge(this,t,fE,mE,_e,{classNames:0})}}function _E(k){let t,u,c;return{c(){t=M("svg"),u=M("path"),c=M("path"),this.h()},l(h){t=z(h,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var f=s(t);u=z(f,"path",{d:!0}),s(u).forEach(o),c=z(f,"path",{d:!0}),s(c).forEach(o),f.forEach(o),this.h()},h(){n(u,"d","M15.4988 8.79309L12.1819 5.47621C12.0188 5.25871 11.7469 5.14996 11.475 5.14996H7.12501C6.52688 5.14996 6.03751 5.63934 6.03751 6.23746V16.025C6.03751 16.6231 6.52688 17.1125 7.12501 17.1125H14.7375C15.3356 17.1125 15.825 16.6231 15.825 16.025V9.55434C15.825 9.28246 15.7163 9.01059 15.4988 8.79309V8.79309ZM11.475 6.23746L14.6831 9.49996H11.475V6.23746ZM7.12501 16.025V6.23746H10.3875V9.49996C10.3875 10.0981 10.8769 10.5875 11.475 10.5875H14.7375V16.025H7.12501Z"),n(c,"d","M3.8625 10.5875H2.775V2.97498C2.775 2.37686 3.26438 1.88748 3.8625 1.88748H11.475V2.97498H3.8625V10.5875Z"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 18 19")},m(h,f){b(h,t,f),e(t,u),e(t,c)},p(h,[f]){f&1&&n(t,"class",h[0])},i:j,o:j,d(h){h&&o(t)}}}function bE(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class kE extends fe{constructor(t){super();ge(this,t,bE,_E,_e,{classNames:0})}}function vE(k){let t,u;return{c(){t=M("svg"),u=M("path"),this.h()},l(c){t=z(c,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var h=s(t);u=z(h,"path",{d:!0}),s(u).forEach(o),h.forEach(o),this.h()},h(){n(u,"d","M15.825 1.88748H6.0375C5.74917 1.88777 5.47272 2.00244 5.26884 2.20632C5.06496 2.4102 4.95029 2.68665 4.95 2.97498V4.60623H2.775C2.48667 4.60652 2.21022 4.72119 2.00634 4.92507C1.80246 5.12895 1.68779 5.4054 1.6875 5.69373V16.025C1.68779 16.3133 1.80246 16.5898 2.00634 16.7936C2.21022 16.9975 2.48667 17.1122 2.775 17.1125H15.825C16.1133 17.1122 16.3898 16.9975 16.5937 16.7936C16.7975 16.5898 16.9122 16.3133 16.9125 16.025V2.97498C16.9122 2.68665 16.7975 2.4102 16.5937 2.20632C16.3898 2.00244 16.1133 1.88777 15.825 1.88748ZM6.0375 2.97498H15.825V4.60623H6.0375V2.97498ZM15.825 8.41248H11.475V5.69373H15.825V8.41248ZM6.0375 12.2187V9.49998H10.3875V12.2187H6.0375ZM10.3875 13.3062V16.025H6.0375V13.3062H10.3875ZM4.95 12.2187H2.775V9.49998H4.95V12.2187ZM10.3875 5.69373V8.41248H6.0375V5.69373H10.3875ZM11.475 9.49998H15.825V12.2187H11.475V9.49998ZM4.95 5.69373V8.41248H2.775V5.69373H4.95ZM2.775 13.3062H4.95V16.025H2.775V13.3062ZM11.475 16.025V13.3062H15.825V16.025H11.475Z"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 18 19")},m(c,h){b(c,t,h),e(t,u)},p(c,[h]){h&1&&n(t,"class",c[0])},i:j,o:j,d(c){c&&o(t)}}}function wE(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class yE extends fe{constructor(t){super();ge(this,t,wE,vE,_e,{classNames:0})}}function TE(k){let t,u,c,h;return{c(){t=M("svg"),u=M("path"),c=M("path"),h=M("path"),this.h()},l(f){t=z(f,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var p=s(t);u=z(p,"path",{d:!0}),s(u).forEach(o),c=z(p,"path",{d:!0}),s(c).forEach(o),h=z(p,"path",{d:!0}),s(h).forEach(o),p.forEach(o),this.h()},h(){n(u,"d","M4.00626 16.5125C3.46854 16.5125 2.9429 16.353 2.4958 16.0543C2.0487 15.7556 1.70024 15.3309 1.49446 14.8342C1.28868 14.3374 1.23484 13.7907 1.33975 13.2633C1.44465 12.7359 1.70359 12.2515 2.08381 11.8713C2.46403 11.4911 2.94847 11.2321 3.47586 11.1272C4.00324 11.0223 4.54989 11.0762 5.04668 11.2819C5.54346 11.4877 5.96807 11.8362 6.26681 12.2833C6.56555 12.7304 6.72501 13.256 6.72501 13.7937C6.72414 14.5145 6.43743 15.2055 5.92775 15.7152C5.41807 16.2249 4.72705 16.5116 4.00626 16.5125V16.5125ZM4.00626 12.1625C3.68363 12.1625 3.36824 12.2582 3.09998 12.4374C2.83173 12.6166 2.62264 12.8714 2.49918 13.1695C2.37571 13.4676 2.34341 13.7955 2.40635 14.112C2.46929 14.4284 2.62465 14.7191 2.85279 14.9472C3.08092 15.1753 3.37158 15.3307 3.68802 15.3936C4.00445 15.4566 4.33244 15.4243 4.63051 15.3008C4.92858 15.1773 5.18335 14.9683 5.36259 14.7C5.54184 14.4317 5.63751 14.1164 5.63751 13.7937C5.63708 13.3612 5.46507 12.9466 5.15925 12.6407C4.85342 12.3349 4.43876 12.1629 4.00626 12.1625Z"),n(c,"d","M13.25 14.3375H7.81251V13.25H13.25V9.44371H4.55001C4.26167 9.44343 3.98523 9.32876 3.78135 9.12487C3.57747 8.92099 3.4628 8.64455 3.46251 8.35621V4.54996C3.4628 4.26163 3.57747 3.98519 3.78135 3.7813C3.98523 3.57742 4.26167 3.46275 4.55001 3.46246H9.98751V4.54996H4.55001V8.35621H13.25C13.5383 8.3565 13.8148 8.47117 14.0187 8.67505C14.2226 8.87894 14.3372 9.15538 14.3375 9.44371V13.25C14.3372 13.5383 14.2226 13.8147 14.0187 14.0186C13.8148 14.2225 13.5383 14.3372 13.25 14.3375V14.3375Z"),n(h,"d","M15.425 6.72504H12.1625C11.8742 6.72475 11.5977 6.61008 11.3939 6.4062C11.19 6.20231 11.0753 5.92587 11.075 5.63754V2.37504C11.0753 2.0867 11.19 1.81026 11.3939 1.60638C11.5977 1.40249 11.8742 1.28782 12.1625 1.28754H15.425C15.7133 1.28782 15.9898 1.40249 16.1937 1.60638C16.3976 1.81026 16.5122 2.0867 16.5125 2.37504V5.63754C16.5122 5.92587 16.3976 6.20231 16.1937 6.4062C15.9898 6.61008 15.7133 6.72475 15.425 6.72504V6.72504ZM12.1625 2.37504V5.63754H15.425V2.37504H12.1625Z"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 18 18")},m(f,p){b(f,t,p),e(t,u),e(t,c),e(t,h)},p(f,[p]){p&1&&n(t,"class",f[0])},i:j,o:j,d(f){f&&o(t)}}}function $E(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class xE extends fe{constructor(t){super();ge(this,t,$E,TE,_e,{classNames:0})}}function FE(k){let t,u,c,h,f,p,m,R,I,P,L,H,V,E,ie;return{c(){t=M("svg"),u=M("circle"),c=M("circle"),h=M("circle"),f=M("circle"),p=M("circle"),m=M("circle"),R=M("circle"),I=M("circle"),P=M("circle"),L=M("circle"),H=M("circle"),V=M("circle"),E=M("circle"),ie=M("circle"),this.h()},l(U){t=z(U,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0,style:!0});var Z=s(t);u=z(Z,"circle",{cx:!0,cy:!0,r:!0,fill:!0}),s(u).forEach(o),c=z(Z,"circle",{cx:!0,cy:!0,r:!0,fill:!0}),s(c).forEach(o),h=z(Z,"circle",{cx:!0,cy:!0,r:!0,fill:!0}),s(h).forEach(o),f=z(Z,"circle",{cx:!0,cy:!0,r:!0,fill:!0}),s(f).forEach(o),p=z(Z,"circle",{cx:!0,cy:!0,r:!0,fill:!0}),s(p).forEach(o),m=z(Z,"circle",{cx:!0,cy:!0,r:!0,fill:!0}),s(m).forEach(o),R=z(Z,"circle",{cx:!0,cy:!0,r:!0,fill:!0}),s(R).forEach(o),I=z(Z,"circle",{cx:!0,cy:!0,r:!0,fill:!0}),s(I).forEach(o),P=z(Z,"circle",{cx:!0,cy:!0,r:!0,fill:!0}),s(P).forEach(o),L=z(Z,"circle",{cx:!0,cy:!0,r:!0,fill:!0}),s(L).forEach(o),H=z(Z,"circle",{cx:!0,cy:!0,r:!0,fill:!0}),s(H).forEach(o),V=z(Z,"circle",{cx:!0,cy:!0,r:!0,fill:!0}),s(V).forEach(o),E=z(Z,"circle",{cx:!0,cy:!0,r:!0,fill:!0}),s(E).forEach(o),ie=z(Z,"circle",{cx:!0,cy:!0,r:!0,fill:!0}),s(ie).forEach(o),Z.forEach(o),this.h()},h(){n(u,"cx","10"),n(u,"cy","20"),n(u,"r","2"),n(u,"fill","currentColor"),n(c,"cx","10"),n(c,"cy","28"),n(c,"r","2"),n(c,"fill","currentColor"),n(h,"cx","10"),n(h,"cy","14"),n(h,"r","2"),n(h,"fill","currentColor"),n(f,"cx","28"),n(f,"cy","4"),n(f,"r","2"),n(f,"fill","currentColor"),n(p,"cx","22"),n(p,"cy","6"),n(p,"r","2"),n(p,"fill","currentColor"),n(m,"cx","28"),n(m,"cy","10"),n(m,"r","2"),n(m,"fill","currentColor"),n(R,"cx","20"),n(R,"cy","12"),n(R,"r","2"),n(R,"fill","currentColor"),n(I,"cx","28"),n(I,"cy","22"),n(I,"r","2"),n(I,"fill","currentColor"),n(P,"cx","26"),n(P,"cy","28"),n(P,"r","2"),n(P,"fill","currentColor"),n(L,"cx","20"),n(L,"cy","26"),n(L,"r","2"),n(L,"fill","currentColor"),n(H,"cx","22"),n(H,"cy","20"),n(H,"r","2"),n(H,"fill","currentColor"),n(V,"cx","16"),n(V,"cy","4"),n(V,"r","2"),n(V,"fill","currentColor"),n(E,"cx","4"),n(E,"cy","24"),n(E,"r","2"),n(E,"fill","currentColor"),n(ie,"cx","4"),n(ie,"cy","16"),n(ie,"r","2"),n(ie,"fill","currentColor"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 32 32"),CR(t,"transform","rotate(360deg)")},m(U,Z){b(U,t,Z),e(t,u),e(t,c),e(t,h),e(t,f),e(t,p),e(t,m),e(t,R),e(t,I),e(t,P),e(t,L),e(t,H),e(t,V),e(t,E),e(t,ie)},p(U,[Z]){Z&1&&n(t,"class",U[0])},i:j,o:j,d(U){U&&o(t)}}}function CE(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class RE extends fe{constructor(t){super();ge(this,t,CE,FE,_e,{classNames:0})}}function EE(k){let t,u,c,h;return{c(){t=M("svg"),u=M("path"),c=M("path"),h=M("path"),this.h()},l(f){t=z(f,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var p=s(t);u=z(p,"path",{d:!0}),s(u).forEach(o),c=z(p,"path",{d:!0}),s(c).forEach(o),h=z(p,"path",{d:!0}),s(h).forEach(o),p.forEach(o),this.h()},h(){n(u,"d","M16.2607 8.08202L14.468 6.28928C14.3063 6.12804 14.0873 6.03749 13.859 6.03749C13.6307 6.03749 13.4117 6.12804 13.25 6.28928L5.6375 13.904V16.9125H8.64607L16.2607 9.30002C16.422 9.13836 16.5125 8.91935 16.5125 8.69102C16.5125 8.4627 16.422 8.24369 16.2607 8.08202V8.08202ZM8.1953 15.825H6.725V14.3547L11.858 9.22118L13.3288 10.6915L8.1953 15.825ZM14.0982 9.92262L12.6279 8.45232L13.8606 7.21964L15.3309 8.68994L14.0982 9.92262Z"),n(c,"d","M6.18125 9.84373H7.26875V6.03748H8.9V4.94998H4.55V6.03748H6.18125V9.84373Z"),n(h,"d","M4.55 11.475H2.375V2.775H11.075V4.95H12.1625V2.775C12.1625 2.48658 12.0479 2.20997 11.844 2.00602C11.64 1.80208 11.3634 1.6875 11.075 1.6875H2.375C2.08658 1.6875 1.80997 1.80208 1.60602 2.00602C1.40207 2.20997 1.2875 2.48658 1.2875 2.775V11.475C1.2875 11.7634 1.40207 12.04 1.60602 12.244C1.80997 12.4479 2.08658 12.5625 2.375 12.5625H4.55V11.475Z"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 18 18")},m(f,p){b(f,t,p),e(t,u),e(t,c),e(t,h)},p(f,[p]){p&1&&n(t,"class",f[0])},i:j,o:j,d(f){f&&o(t)}}}function ME(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class zE extends fe{constructor(t){super();ge(this,t,ME,EE,_e,{classNames:0})}}function qE(k){let t,u,c,h;return{c(){t=M("svg"),u=M("path"),c=M("path"),h=M("path"),this.h()},l(f){t=z(f,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var p=s(t);u=z(p,"path",{d:!0,fill:!0}),s(u).forEach(o),c=z(p,"path",{d:!0,fill:!0}),s(c).forEach(o),h=z(p,"path",{d:!0,fill:!0}),s(h).forEach(o),p.forEach(o),this.h()},h(){n(u,"d","M29.537 13.76l-3.297-3.297a1.586 1.586 0 0 0-2.24 0L10 24.467V30h5.533l14.004-14a1.586 1.586 0 0 0 0-2.24zM14.704 28H12v-2.704l9.44-9.441l2.705 2.704zM25.56 17.145l-2.704-2.704l2.267-2.267l2.704 2.704z"),n(u,"fill","currentColor"),n(c,"d","M11 17h2v-7h3V8H8v2h3v7z"),n(c,"fill","currentColor"),n(h,"d","M8 20H4V4h16v4h2V4a2 2 0 0 0-2-2H4a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4z"),n(h,"fill","currentColor"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 32 32")},m(f,p){b(f,t,p),e(t,u),e(t,c),e(t,h)},p(f,[p]){p&1&&n(t,"class",f[0])},i:j,o:j,d(f){f&&o(t)}}}function jE(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class AE extends fe{constructor(t){super();ge(this,t,jE,qE,_e,{classNames:0})}}function PE(k){let t,u,c,h,f;return{c(){t=a(`\u200B
`),u=M("svg"),c=M("path"),h=M("path"),f=M("path"),this.h()},l(p){t=r(p,`\u200B
`),u=z(p,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var m=s(u);c=z(m,"path",{d:!0,fill:!0}),s(c).forEach(o),h=z(m,"path",{d:!0,fill:!0}),s(h).forEach(o),f=z(m,"path",{d:!0,fill:!0}),s(f).forEach(o),m.forEach(o),this.h()},h(){n(c,"d","M29.707 19.293l-3-3a1 1 0 0 0-1.414 0L16 25.586V30h4.414l9.293-9.293a1 1 0 0 0 0-1.414zM19.586 28H18v-1.586l5-5L24.586 23zM26 21.586L24.414 20L26 18.414L27.586 20z"),n(c,"fill","currentColor"),n(h,"d","M20 13v-2h-2.142a3.94 3.94 0 0 0-.425-1.019l1.517-1.517l-1.414-1.414l-1.517 1.517A3.944 3.944 0 0 0 15 8.142V6h-2v2.142a3.944 3.944 0 0 0-1.019.425L10.464 7.05L9.05 8.464l1.517 1.517A3.94 3.94 0 0 0 10.142 11H8v2h2.142a3.94 3.94 0 0 0 .425 1.019L9.05 15.536l1.414 1.414l1.517-1.517a3.944 3.944 0 0 0 1.019.425V18h2v-2.142a3.944 3.944 0 0 0 1.019-.425l1.517 1.517l1.414-1.414l-1.517-1.517A3.94 3.94 0 0 0 17.858 13zm-6 1a2 2 0 1 1 2-2a2.002 2.002 0 0 1-2 2z"),n(h,"fill","currentColor"),n(f,"d","M12 30H6a2.002 2.002 0 0 1-2-2V4a2.002 2.002 0 0 1 2-2h16a2.002 2.002 0 0 1 2 2v10h-2V4H6v24h6z"),n(f,"fill","currentColor"),n(u,"class",k[0]),n(u,"xmlns","http://www.w3.org/2000/svg"),n(u,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(u,"aria-hidden","true"),n(u,"role","img"),n(u,"width","1em"),n(u,"height","1em"),n(u,"preserveAspectRatio","xMidYMid meet"),n(u,"viewBox","0 0 32 32")},m(p,m){b(p,t,m),b(p,u,m),e(u,c),e(u,h),e(u,f)},p(p,[m]){m&1&&n(u,"class",p[0])},i:j,o:j,d(p){p&&o(t),p&&o(u)}}}function LE(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class NE extends fe{constructor(t){super();ge(this,t,LE,PE,_e,{classNames:0})}}function IE(k){let t,u;return{c(){t=M("svg"),u=M("path"),this.h()},l(c){t=z(c,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var h=s(t);u=z(h,"path",{"fill-rule":!0,"clip-rule":!0,d:!0,fill:!0}),s(u).forEach(o),h.forEach(o),this.h()},h(){n(u,"fill-rule","evenodd"),n(u,"clip-rule","evenodd"),n(u,"d","M3.0625 3.0625L10 3.0625V2H3.0625C2.78071 2 2.51045 2.11194 2.3112 2.3112C2.11194 2.51046 2 2.78071 2 3.0625V11.5625C2 11.8443 2.11194 12.1145 2.3112 12.3138C2.51045 12.5131 2.78071 12.625 3.0625 12.625H7V11.5625H3.0625L3.0625 3.0625ZM5.78125 9.96875H6.84375V6.25H8.4375V5.1875H4.1875V6.25H5.78125V9.96875ZM12.5 13C13.163 13 13.7989 12.7366 14.2678 12.2678C14.7366 11.7989 15 11.163 15 10.5V5.5C15 4.83696 14.7366 4.20107 14.2678 3.73223C13.7989 3.26339 13.163 3 12.5 3C11.837 3 11.2011 3.26339 10.7322 3.73223C10.2634 4.20107 10 4.83696 10 5.5V10.5C10 11.163 10.2634 11.7989 10.7322 12.2678C11.2011 12.7366 11.837 13 12.5 13ZM11 5.5C11 5.10218 11.158 4.72064 11.4393 4.43934C11.7206 4.15804 12.1022 4 12.5 4C12.8978 4 13.2794 4.15804 13.5607 4.43934C13.842 4.72064 14 5.10218 14 5.5V10.5C14 10.8978 13.842 11.2794 13.5607 11.5607C13.2794 11.842 12.8978 12 12.5 12C12.1022 12 11.7206 11.842 11.4393 11.5607C11.158 11.2794 11 10.8978 11 10.5V5.5ZM16 9V10.5C16 11.4283 15.6313 12.3185 14.9749 12.9749C14.3185 13.6313 13.4283 14 12.5 14C11.5717 14 10.6815 13.6313 10.0251 12.9749C9.36875 12.3185 9 11.4283 9 10.5V9H8V10.5C8.00053 11.6065 8.40873 12.6741 9.14661 13.4987C9.88449 14.3232 10.9003 14.8471 12 14.97V16H10V17H15V16H13V14.97C14.0997 14.8471 15.1155 14.3232 15.8534 13.4987C16.5913 12.6741 16.9995 11.6065 17 10.5V9H16Z"),n(u,"fill","currentColor"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 18 18")},m(c,h){b(c,t,h),e(t,u)},p(c,[h]){h&1&&n(t,"class",c[0])},i:j,o:j,d(c){c&&o(t)}}}function OE(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class DE extends fe{constructor(t){super();ge(this,t,OE,IE,_e,{classNames:0})}}function SE(k){let t,u,c,h,f,p,m,R;return{c(){t=M("svg"),u=M("path"),c=M("path"),h=M("path"),f=M("path"),p=M("path"),m=M("path"),R=M("path"),this.h()},l(I){t=z(I,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var P=s(t);u=z(P,"path",{d:!0}),s(u).forEach(o),c=z(P,"path",{d:!0}),s(c).forEach(o),h=z(P,"path",{d:!0}),s(h).forEach(o),f=z(P,"path",{d:!0}),s(f).forEach(o),p=z(P,"path",{d:!0}),s(p).forEach(o),m=z(P,"path",{d:!0}),s(m).forEach(o),R=z(P,"path",{d:!0}),s(R).forEach(o),P.forEach(o),this.h()},h(){n(u,"d","M11.075 10.1875H12.1625V11.275H11.075V10.1875Z"),n(c,"d","M15.425 9.10004H16.5125V10.1875H15.425V9.10004Z"),n(h,"d","M7.8125 3.66254H8.9V4.75004H7.8125V3.66254Z"),n(f,"d","M8.90001 12.3625H6.72501V9.09998C6.72472 8.81165 6.61005 8.5352 6.40617 8.33132C6.20228 8.12744 5.92584 8.01277 5.63751 8.01248H2.37501C2.08667 8.01277 1.81023 8.12744 1.60635 8.33132C1.40246 8.5352 1.28779 8.81165 1.28751 9.09998V12.3625C1.28779 12.6508 1.40246 12.9273 1.60635 13.1311C1.81023 13.335 2.08667 13.4497 2.37501 13.45H5.63751V15.625C5.63779 15.9133 5.75246 16.1898 5.95635 16.3936C6.16023 16.5975 6.43667 16.7122 6.72501 16.7125H8.90001C9.18834 16.7122 9.46478 16.5975 9.66867 16.3936C9.87255 16.1898 9.98722 15.9133 9.98751 15.625V13.45C9.98722 13.1616 9.87255 12.8852 9.66867 12.6813C9.46478 12.4774 9.18834 12.3628 8.90001 12.3625V12.3625ZM2.37501 12.3625V9.09998H5.63751V12.3625H2.37501ZM6.72501 15.625V13.45H8.90001V15.625H6.72501Z"),n(p,"d","M15.425 16.7125H13.25C12.9617 16.7122 12.6852 16.5976 12.4813 16.3937C12.2775 16.1898 12.1628 15.9134 12.1625 15.625V13.45C12.1628 13.1617 12.2775 12.8852 12.4813 12.6814C12.6852 12.4775 12.9617 12.3628 13.25 12.3625H15.425C15.7133 12.3628 15.9898 12.4775 16.1937 12.6814C16.3976 12.8852 16.5122 13.1617 16.5125 13.45V15.625C16.5122 15.9134 16.3976 16.1898 16.1937 16.3937C15.9898 16.5976 15.7133 16.7122 15.425 16.7125ZM13.25 13.45V15.625H15.425V13.45H13.25Z"),n(m,"d","M15.425 1.48752H12.1625C11.8742 1.48781 11.5977 1.60247 11.3938 1.80636C11.19 2.01024 11.0753 2.28668 11.075 2.57502V5.83752H9.98751C9.69917 5.83781 9.42273 5.95247 9.21885 6.15636C9.01496 6.36024 8.9003 6.63668 8.90001 6.92502V8.01252C8.9003 8.30085 9.01496 8.5773 9.21885 8.78118C9.42273 8.98506 9.69917 9.09973 9.98751 9.10002H11.075C11.3633 9.09973 11.6398 8.98506 11.8437 8.78118C12.0476 8.5773 12.1622 8.30085 12.1625 8.01252V6.92502H15.425C15.7133 6.92473 15.9898 6.81006 16.1937 6.60618C16.3976 6.4023 16.5122 6.12585 16.5125 5.83752V2.57502C16.5122 2.28668 16.3976 2.01024 16.1937 1.80636C15.9898 1.60247 15.7133 1.48781 15.425 1.48752ZM9.98751 8.01252V6.92502H11.075V8.01252H9.98751ZM12.1625 5.83752V2.57502H15.425V5.83752H12.1625Z"),n(R,"d","M4.55001 5.83752H2.37501C2.08667 5.83723 1.81023 5.72256 1.60635 5.51868C1.40246 5.3148 1.28779 5.03835 1.28751 4.75002V2.57502C1.28779 2.28668 1.40246 2.01024 1.60635 1.80636C1.81023 1.60247 2.08667 1.48781 2.37501 1.48752H4.55001C4.83834 1.48781 5.11478 1.60247 5.31867 1.80636C5.52255 2.01024 5.63722 2.28668 5.63751 2.57502V4.75002C5.63722 5.03835 5.52255 5.3148 5.31867 5.51868C5.11478 5.72256 4.83834 5.83723 4.55001 5.83752V5.83752ZM2.37501 2.57502V4.75002H4.55001V2.57502H2.37501Z"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 18 18")},m(I,P){b(I,t,P),e(t,u),e(t,c),e(t,h),e(t,f),e(t,p),e(t,m),e(t,R)},p(I,[P]){P&1&&n(t,"class",I[0])},i:j,o:j,d(I){I&&o(t)}}}function HE(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class VE extends fe{constructor(t){super();ge(this,t,HE,SE,_e,{classNames:0})}}function BE(k){let t,u,c;return{c(){t=M("svg"),u=M("path"),c=M("path"),this.h()},l(h){t=z(h,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var f=s(t);u=z(f,"path",{d:!0}),s(u).forEach(o),c=z(f,"path",{d:!0}),s(c).forEach(o),f.forEach(o),this.h()},h(){n(u,"d","M15.7435 16.3688H16.9125L13.65 8.21251H12.3722L9.1097 16.3688H10.2788L11.1488 14.1938H14.8735L15.7435 16.3688ZM11.5838 13.1063L13.0084 9.53926L14.4385 13.1063H11.5838Z"),n(c,"d","M10.3875 4.40625V3.31875H6.58125V1.6875H5.49375V3.31875H1.6875V4.40625H7.52737C7.2261 5.64892 6.63129 6.80125 5.79281 7.76663C5.24624 7.08884 4.8246 6.31923 4.54763 5.49375H3.40575C3.74803 6.60116 4.30202 7.63159 5.037 8.52787C4.2247 9.3158 3.27338 9.94633 2.23125 10.3875L2.63906 11.3989C3.81007 10.9044 4.87658 10.1922 5.78194 9.3C6.67088 10.2044 7.73719 10.9153 8.91394 11.388L9.3 10.3875C8.25187 9.98235 7.3026 9.35754 6.516 8.55506C7.55705 7.36858 8.2892 5.94351 8.6475 4.40625H10.3875Z"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 18 18")},m(h,f){b(h,t,f),e(t,u),e(t,c)},p(h,[f]){f&1&&n(t,"class",h[0])},i:j,o:j,d(h){h&&o(t)}}}function WE(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class UE extends fe{constructor(t){super();ge(this,t,WE,BE,_e,{classNames:0})}}function QE(k){let t,u;return{c(){t=M("svg"),u=M("path"),this.h()},l(c){t=z(c,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var h=s(t);u=z(h,"path",{"fill-rule":!0,"clip-rule":!0,d:!0,fill:!0}),s(u).forEach(o),h.forEach(o),this.h()},h(){n(u,"fill-rule","evenodd"),n(u,"clip-rule","evenodd"),n(u,"d","M6.34483 1.96552C5.1929 1.9668 4.08852 2.42496 3.27398 3.2395C2.45945 4.05404 2.00128 5.15842 2 6.31035H2.96552C2.96552 5.4141 3.32155 4.55456 3.95529 3.92081C4.58904 3.28707 5.44858 2.93103 6.34483 2.93103C7.24108 2.93103 8.10062 3.28707 8.73436 3.92081C9.3681 4.55456 9.72414 5.4141 9.72414 6.31035C9.72587 6.90867 9.57072 7.497 9.27416 8.01667C8.97761 8.53634 8.55001 8.96919 8.034 9.27207L7.7931 9.4111V10.8946C7.79547 11.0882 7.75813 11.2802 7.68341 11.4588C7.60868 11.6374 7.49814 11.7988 7.35862 11.933C7.08012 12.2328 6.7171 12.4409 6.31768 12.5297C5.91826 12.6185 5.50128 12.5838 5.122 12.4303C4.77411 12.2798 4.47685 12.0325 4.26568 11.7177C4.0545 11.403 3.93834 11.0341 3.93103 10.6552H2.96552C2.97422 11.222 3.14591 11.7743 3.46009 12.2462C3.77426 12.718 4.21764 13.0895 4.73724 13.3161C5.08947 13.4705 5.46979 13.5503 5.85435 13.5508C6.66479 13.5337 7.43811 13.2076 8.01614 12.6393C8.25262 12.4147 8.44052 12.144 8.56823 11.8439C8.69593 11.5438 8.76073 11.2207 8.75862 10.8946V9.9571C9.35428 9.55362 9.84183 9.01013 10.1785 8.37432C10.5152 7.73851 10.6907 7.02979 10.6897 6.31035C10.6884 5.15842 10.2302 4.05404 9.41567 3.2395C8.60114 2.42496 7.49676 1.9668 6.34483 1.96552ZM10.2069 15.4828V14.5172C11.4868 14.5158 12.7139 14.0068 13.619 13.1017C14.524 12.1967 15.0331 10.9696 15.0345 9.68966H16C15.9983 11.2256 15.3875 12.6981 14.3014 13.7842C13.2153 14.8702 11.7428 15.4811 10.2069 15.4828ZM10.2069 13.5517V12.5862C10.9748 12.5853 11.7111 12.2799 12.2541 11.7368C12.7971 11.1938 13.1026 10.4576 13.1034 9.68966H14.069C14.0677 10.7135 13.6604 11.6951 12.9364 12.4191C12.2124 13.1431 11.2308 13.5504 10.2069 13.5517ZM10.2069 10.6552V11.6207C10.7189 11.6202 11.2098 11.4166 11.5718 11.0545C11.9338 10.6925 12.1374 10.2016 12.1379 9.68966H11.1724C11.1722 9.94565 11.0704 10.1911 10.8893 10.3721C10.7083 10.5531 10.4629 10.6549 10.2069 10.6552ZM6.64823 4.89281C6.43337 4.84642 6.21077 4.84944 5.99724 4.90165C5.71804 4.9704 5.46559 5.12064 5.27203 5.33328C5.07846 5.54591 4.95252 5.81132 4.91024 6.09573C4.86795 6.38015 4.91122 6.67072 5.03456 6.93047C5.15789 7.19022 5.3557 7.40741 5.60283 7.55441C5.97226 7.7669 6.27967 8.07234 6.49452 8.44039C6.70938 8.80845 6.8242 9.22631 6.82759 9.65248V10.6552H5.86207V9.65296C5.85793 9.39379 5.78607 9.1402 5.65362 8.91739C5.52117 8.69458 5.33274 8.51029 5.10703 8.38282C4.66676 8.11775 4.3219 7.71987 4.12204 7.24641C3.92217 6.77295 3.87761 6.24831 3.99476 5.74793C4.09577 5.31261 4.31683 4.91433 4.63282 4.59834C4.94882 4.28234 5.34709 4.06128 5.78241 3.96027C6.13945 3.87659 6.51077 3.87468 6.86865 3.95468C7.22652 4.03469 7.56169 4.19453 7.8491 4.42227C8.13342 4.64789 8.36295 4.93497 8.52047 5.26196C8.67799 5.58896 8.75941 5.94738 8.75862 6.31034H7.7931C7.79296 6.09052 7.74288 5.87361 7.64664 5.67598C7.55041 5.47834 7.41053 5.30515 7.23757 5.16949C7.06462 5.03382 6.8631 4.93921 6.64823 4.89281Z"),n(u,"fill","currentColor"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 18 18")},m(c,h){b(c,t,h),e(t,u)},p(c,[h]){h&1&&n(t,"class",c[0])},i:j,o:j,d(c){c&&o(t)}}}function ZE(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class KE extends fe{constructor(t){super();ge(this,t,ZE,QE,_e,{classNames:0})}}function JE(k){let t,u,c,h,f,p,m,R,I,P,L,H;return{c(){t=M("svg"),u=M("path"),c=M("path"),h=M("path"),f=M("path"),p=M("path"),m=M("path"),R=M("path"),I=M("path"),P=M("path"),L=M("path"),H=M("path"),this.h()},l(V){t=z(V,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var E=s(t);u=z(E,"path",{d:!0}),s(u).forEach(o),c=z(E,"path",{d:!0}),s(c).forEach(o),h=z(E,"path",{d:!0}),s(h).forEach(o),f=z(E,"path",{d:!0}),s(f).forEach(o),p=z(E,"path",{d:!0}),s(p).forEach(o),m=z(E,"path",{d:!0}),s(m).forEach(o),R=z(E,"path",{d:!0}),s(R).forEach(o),I=z(E,"path",{d:!0}),s(I).forEach(o),P=z(E,"path",{d:!0}),s(P).forEach(o),L=z(E,"path",{d:!0}),s(L).forEach(o),H=z(E,"path",{d:!0}),s(H).forEach(o),E.forEach(o),this.h()},h(){n(u,"d","M16.7125 8.75625H9.64375V1.6875H8.55625V8.75625H1.4875V9.84375H8.55625V16.9125H9.64375V9.84375H16.7125V8.75625Z"),n(c,"d","M3.11875 16.9125C2.79612 16.9125 2.48073 16.8168 2.21247 16.6376C1.94421 16.4584 1.73513 16.2036 1.61167 15.9055C1.4882 15.6074 1.4559 15.2794 1.51884 14.963C1.58178 14.6466 1.73714 14.3559 1.96528 14.1278C2.19341 13.8997 2.48407 13.7443 2.8005 13.6814C3.11694 13.6184 3.44493 13.6507 3.743 13.7742C4.04107 13.8976 4.29584 14.1067 4.47508 14.375C4.65432 14.6432 4.75 14.9586 4.75 15.2813C4.74956 15.7138 4.57756 16.1284 4.27174 16.4343C3.96591 16.7401 3.55125 16.9121 3.11875 16.9125V16.9125ZM3.11875 14.7375C3.0112 14.7375 2.90607 14.7694 2.81665 14.8291C2.72724 14.8889 2.65754 14.9738 2.61639 15.0732C2.57523 15.1725 2.56446 15.2819 2.58544 15.3873C2.60642 15.4928 2.65821 15.5897 2.73426 15.6657C2.8103 15.7418 2.90719 15.7936 3.01267 15.8146C3.11814 15.8355 3.22747 15.8248 3.32683 15.7836C3.42619 15.7425 3.51111 15.6728 3.57086 15.5834C3.63061 15.4939 3.6625 15.3888 3.6625 15.2813C3.66235 15.1371 3.60502 14.9989 3.50308 14.8969C3.40113 14.795 3.26291 14.7377 3.11875 14.7375Z"),n(h,"d","M4.75 4.95C4.42737 4.95 4.11198 4.85433 3.84372 4.67509C3.57547 4.49584 3.36639 4.24107 3.24292 3.943C3.11945 3.64493 3.08715 3.31694 3.15009 3.00051C3.21303 2.68408 3.3684 2.39342 3.59653 2.16528C3.82466 1.93715 4.11533 1.78179 4.43176 1.71884C4.74819 1.6559 5.07618 1.68821 5.37425 1.81167C5.67232 1.93514 5.92709 2.14422 6.10633 2.41248C6.28558 2.68073 6.38125 2.99612 6.38125 3.31875C6.38082 3.75125 6.20881 4.16592 5.90299 4.47174C5.59716 4.77757 5.1825 4.94957 4.75 4.95ZM4.75 2.775C4.64245 2.775 4.53733 2.80689 4.44791 2.86664C4.35849 2.92639 4.28879 3.01131 4.24764 3.11067C4.20648 3.21002 4.19572 3.31935 4.2167 3.42483C4.23768 3.53031 4.28946 3.62719 4.36551 3.70324C4.44155 3.77928 4.53844 3.83107 4.64392 3.85205C4.7494 3.87303 4.85873 3.86227 4.95808 3.82111C5.05744 3.77995 5.14236 3.71026 5.20211 3.62084C5.26186 3.53142 5.29375 3.42629 5.29375 3.31875C5.2936 3.17458 5.23627 3.03636 5.13433 2.93442C5.03239 2.83248 4.89417 2.77514 4.75 2.775Z"),n(f,"d","M12.3625 7.66875C12.0399 7.66875 11.7245 7.57308 11.4562 7.39384C11.188 7.21459 10.9789 6.95982 10.8554 6.66175C10.732 6.36368 10.6996 6.03569 10.7626 5.71926C10.8255 5.40283 10.9809 5.11217 11.209 4.88403C11.4372 4.6559 11.7278 4.50054 12.0443 4.43759C12.3607 4.37465 12.6887 4.40696 12.9867 4.53042C13.2848 4.65389 13.5396 4.86297 13.7188 5.13123C13.8981 5.39948 13.9937 5.71487 13.9937 6.0375C13.9933 6.47 13.8213 6.88467 13.5155 7.19049C13.2097 7.49632 12.795 7.66832 12.3625 7.66875ZM12.3625 5.49375C12.255 5.49375 12.1498 5.52564 12.0604 5.58539C11.971 5.64514 11.9013 5.73006 11.8601 5.82942C11.819 5.92877 11.8082 6.0381 11.8292 6.14358C11.8502 6.24906 11.902 6.34595 11.978 6.42199C12.0541 6.49803 12.1509 6.54982 12.2564 6.5708C12.3619 6.59178 12.4712 6.58102 12.5706 6.53986C12.6699 6.4987 12.7549 6.42901 12.8146 6.33959C12.8744 6.25017 12.9062 6.14504 12.9062 6.0375C12.9061 5.89333 12.8488 5.75511 12.7468 5.65317C12.6449 5.55123 12.5067 5.49389 12.3625 5.49375Z"),n(p,"d","M6.38125 7.66876C6.98186 7.66876 7.46875 7.18187 7.46875 6.58126C7.46875 5.98065 6.98186 5.49376 6.38125 5.49376C5.78064 5.49376 5.29375 5.98065 5.29375 6.58126C5.29375 7.18187 5.78064 7.66876 6.38125 7.66876Z"),n(m,"d","M6.38125 13.1063C6.98186 13.1063 7.46875 12.6194 7.46875 12.0188C7.46875 11.4181 6.98186 10.9313 6.38125 10.9313C5.78064 10.9313 5.29375 11.4181 5.29375 12.0188C5.29375 12.6194 5.78064 13.1063 6.38125 13.1063Z"),n(R,"d","M11.8187 13.1063C12.4194 13.1063 12.9062 12.6194 12.9062 12.0188C12.9062 11.4181 12.4194 10.9313 11.8187 10.9313C11.2181 10.9313 10.7312 11.4181 10.7312 12.0188C10.7312 12.6194 11.2181 13.1063 11.8187 13.1063Z"),n(I,"d","M12.3625 16.9125C12.9631 16.9125 13.45 16.4256 13.45 15.825C13.45 15.2244 12.9631 14.7375 12.3625 14.7375C11.7619 14.7375 11.275 15.2244 11.275 15.825C11.275 16.4256 11.7619 16.9125 12.3625 16.9125Z"),n(P,"d","M15.625 14.7375C16.2256 14.7375 16.7125 14.2506 16.7125 13.65C16.7125 13.0494 16.2256 12.5625 15.625 12.5625C15.0244 12.5625 14.5375 13.0494 14.5375 13.65C14.5375 14.2506 15.0244 14.7375 15.625 14.7375Z"),n(L,"d","M2.575 7.66876C3.17561 7.66876 3.6625 7.18187 3.6625 6.58126C3.6625 5.98065 3.17561 5.49376 2.575 5.49376C1.97439 5.49376 1.4875 5.98065 1.4875 6.58126C1.4875 7.18187 1.97439 7.66876 2.575 7.66876Z"),n(H,"d","M15.625 3.8625C16.2256 3.8625 16.7125 3.37561 16.7125 2.775C16.7125 2.17439 16.2256 1.6875 15.625 1.6875C15.0244 1.6875 14.5375 2.17439 14.5375 2.775C14.5375 3.37561 15.0244 3.8625 15.625 3.8625Z"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 18 18")},m(V,E){b(V,t,E),e(t,u),e(t,c),e(t,h),e(t,f),e(t,p),e(t,m),e(t,R),e(t,I),e(t,P),e(t,L),e(t,H)},p(V,[E]){E&1&&n(t,"class",V[0])},i:j,o:j,d(V){V&&o(t)}}}function GE(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class YE extends fe{constructor(t){super();ge(this,t,GE,JE,_e,{classNames:0})}}function XE(k){let t,u,c;return{c(){t=M("svg"),u=M("path"),c=M("path"),this.h()},l(h){t=z(h,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var f=s(t);u=z(f,"path",{fill:!0,d:!0}),s(u).forEach(o),c=z(f,"path",{fill:!0,d:!0}),s(c).forEach(o),f.forEach(o),this.h()},h(){n(u,"fill","currentColor"),n(u,"d","M18 10h2v2h-2zm-6 0h2v2h-2z"),n(c,"fill","currentColor"),n(c,"d","M26 20h-5v-2h1a2.002 2.002 0 0 0 2-2v-4h2v-2h-2V8a2.002 2.002 0 0 0-2-2h-2V2h-2v4h-4V2h-2v4h-2a2.002 2.002 0 0 0-2 2v2H6v2h2v4a2.002 2.002 0 0 0 2 2h1v2H6a2.002 2.002 0 0 0-2 2v8h2v-8h20v8h2v-8a2.002 2.002 0 0 0-2-2ZM10 8h12v8H10Zm3 10h6v2h-6Z"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 32 32")},m(h,f){b(h,t,f),e(t,u),e(t,c)},p(h,[f]){f&1&&n(t,"class",h[0])},i:j,o:j,d(h){h&&o(t)}}}function eM(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class tM extends fe{constructor(t){super();ge(this,t,eM,XE,_e,{classNames:0})}}function oM(k){let t,u;return{c(){t=M("svg"),u=M("path"),this.h()},l(c){t=z(c,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var h=s(t);u=z(h,"path",{d:!0,fill:!0}),s(u).forEach(o),h.forEach(o),this.h()},h(){n(u,"d","M2.41562 9.48284H4.48563L1.79462 5.04284C1.61912 4.76855 1.51562 4.46427 1.51562 4.1257C1.51603 3.70884 1.67594 3.30641 1.96543 2.99375C2.25491 2.68109 2.65413 2.47963 3.08834 2.42708C3.52255 2.37454 3.96198 2.47451 4.32437 2.70829C4.68676 2.94206 4.94727 3.29361 5.05712 3.69713H6.91562V2.6257C6.91562 2.15427 7.32062 1.76855 7.81562 1.76855V2.87855L8.53112 2.19713H10.5156V3.05427H8.90012L7.81562 4.08713V4.16427L8.90012 5.19713H10.5156V6.05427H8.53112L7.81562 5.37284V6.48284C7.57693 6.48284 7.34801 6.39253 7.17923 6.23179C7.01045 6.07104 6.91562 5.85303 6.91562 5.6257V4.55427H5.05712C5.01212 4.72141 4.94012 4.87998 4.85012 5.0257L7.55012 9.48284H9.61562C9.85432 9.48284 10.0832 9.57315 10.252 9.73389C10.4208 9.89464 10.5156 10.1127 10.5156 10.34H1.51562C1.51562 9.86855 1.92062 9.48284 2.41562 9.48284ZM4.17512 5.6257C3.92312 5.76284 3.63062 5.83998 3.31562 5.83998L5.52512 9.48284H6.51063L4.17512 5.6257ZM3.31562 3.26855C3.07693 3.26855 2.84801 3.35886 2.67923 3.51961C2.51045 3.68035 2.41562 3.89837 2.41562 4.1257C2.41562 4.60141 2.81612 4.98284 3.31562 4.98284C3.81512 4.98284 4.21562 4.60141 4.21562 4.1257C4.21562 3.89837 4.1208 3.68035 3.95202 3.51961C3.78324 3.35886 3.55432 3.26855 3.31562 3.26855Z"),n(u,"fill","currentColor"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 12 12")},m(c,h){b(c,t,h),e(t,u)},p(c,[h]){h&1&&n(t,"class",c[0])},i:j,o:j,d(c){c&&o(t)}}}function nM(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class sM extends fe{constructor(t){super();ge(this,t,nM,oM,_e,{classNames:0})}}function aM(k){let t,u,c;return{c(){t=M("svg"),u=M("path"),c=M("path"),this.h()},l(h){t=z(h,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var f=s(t);u=z(f,"path",{fill:!0,d:!0}),s(u).forEach(o),c=z(f,"path",{fill:!0,d:!0}),s(c).forEach(o),f.forEach(o),this.h()},h(){n(u,"fill","currentColor"),n(u,"d","M4 22H2V4a2.002 2.002 0 0 1 2-2h18v2H4zm17-5a3 3 0 1 0-3-3a3.003 3.003 0 0 0 3 3zm0-4a1 1 0 1 1-1 1a1 1 0 0 1 1-1z"),n(c,"fill","currentColor"),n(c,"d","M28 7H9a2.002 2.002 0 0 0-2 2v19a2.002 2.002 0 0 0 2 2h19a2.002 2.002 0 0 0 2-2V9a2.002 2.002 0 0 0-2-2Zm0 21H9v-6l4-3.997l5.586 5.586a2 2 0 0 0 2.828 0L23 22.003L28 27Zm0-3.828l-3.586-3.586a2 2 0 0 0-2.828 0L20 22.172l-5.586-5.586a2 2 0 0 0-2.828 0L9 19.172V9h19Z"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 32 32")},m(h,f){b(h,t,f),e(t,u),e(t,c)},p(h,[f]){f&1&&n(t,"class",h[0])},i:j,o:j,d(h){h&&o(t)}}}function rM(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class iM extends fe{constructor(t){super();ge(this,t,rM,aM,_e,{classNames:0})}}function lM(k){let t,u,c;return{c(){t=M("svg"),u=M("path"),c=M("path"),this.h()},l(h){t=z(h,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,fill:!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var f=s(t);u=z(f,"path",{fill:!0,d:!0}),s(u).forEach(o),c=z(f,"path",{fill:!0,d:!0}),s(c).forEach(o),f.forEach(o),this.h()},h(){n(u,"fill","currentColor"),n(u,"d","M19 14a3 3 0 1 0-3-3a3 3 0 0 0 3 3Zm0-4a1 1 0 1 1-1 1a1 1 0 0 1 1-1Z"),n(c,"fill","currentColor"),n(c,"d","M26 4H6a2 2 0 0 0-2 2v20a2 2 0 0 0 2 2h20a2 2 0 0 0 2-2V6a2 2 0 0 0-2-2Zm0 22H6v-6l5-5l5.59 5.59a2 2 0 0 0 2.82 0L21 19l5 5Zm0-4.83l-3.59-3.59a2 2 0 0 0-2.82 0L18 19.17l-5.59-5.59a2 2 0 0 0-2.82 0L6 17.17V6h20Z"),n(t,"class",k[0]),n(t,"xmlns","http://www.w3.org/2000/svg"),n(t,"xmlns:xlink","http://www.w3.org/1999/xlink"),n(t,"aria-hidden","true"),n(t,"fill","currentColor"),n(t,"focusable","false"),n(t,"role","img"),n(t,"width","1em"),n(t,"height","1em"),n(t,"preserveAspectRatio","xMidYMid meet"),n(t,"viewBox","0 0 32 32")},m(h,f){b(h,t,f),e(t,u),e(t,c)},p(h,[f]){f&1&&n(t,"class",h[0])},i:j,o:j,d(h){h&&o(t)}}}function dM(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class cM extends fe{constructor(t){super();ge(this,t,dM,lM,_e,{classNames:0})}}function pM(k){let t,u,c;return{c(){t=M("svg"),u=M("path"),c=M("path"),this.h()},l(h){t=z(h,"svg",{class:!0,width:!0,height:!0,viewBox:!0});var f=s(t);u=z(f,"path",{fill:!0,d:!0}),s(u).forEach(o),c=z(f,"path",{fill:!0,d:!0}),s(c).forEach(o),f.forEach(o),this.h()},h(){n(u,"fill","currentColor"),n(u,"d","M24 30a6 6 0 1 1 6-6a6.007 6.007 0 0 1-6 6zm0-10a4 4 0 1 0 4 4a4.005 4.005 0 0 0-4-4zM8 22h8v2H8zm0-6h10v2H8z"),n(c,"fill","currentColor"),n(c,"d","M16 28H6V4h8v6a2.006 2.006 0 0 0 2 2h6v3h2v-5a.91.91 0 0 0-.3-.7l-7-7A.909.909 0 0 0 16 2H6a2.006 2.006 0 0 0-2 2v24a2.006 2.006 0 0 0 2 2h10Zm0-23.6l5.6 5.6H16Z"),n(t,"class",k[0]),n(t,"width","1em"),n(t,"height","1em"),n(t,"viewBox","0 0 32 32")},m(h,f){b(h,t,f),e(t,u),e(t,c)},p(h,[f]){f&1&&n(t,"class",h[0])},i:j,o:j,d(h){h&&o(t)}}}function hM(k,t,u){let{classNames:c=""}=t;return k.$$set=h=>{"classNames"in h&&u(0,c=h.classNames)},[c]}class uM extends fe{constructor(t){super();ge(this,t,hM,pM,_e,{classNames:0})}}function mM(k){var p;let t,u,c;var h=(p=k[2][k[1]])!=null?p:U8;function f(m){return{props:{classNames:m[0]+" tag-ico "+m[3][m[1]]}}}return h&&(t=new h(f(k))),{c(){t&&y(t.$$.fragment),u=FR()},l(m){t&&C(t.$$.fragment,m),u=FR()},m(m,R){t&&T(t,m,R),b(m,u,R),c=!0},p(m,[R]){var P;const I={};if(R&3&&(I.classNames=m[0]+" tag-ico "+m[3][m[1]]),h!==(h=(P=m[2][m[1]])!=null?P:U8)){if(t){RR();const L=t;$(L.$$.fragment,1,0,()=>{x(L,1)}),ER()}h?(t=new h(f(m)),y(t.$$.fragment),F(t.$$.fragment,1),T(t,u.parentNode,u)):t=null}else h&&t.$set(I)},i(m){c||(t&&F(t.$$.fragment,m),c=!0)},o(m){t&&$(t.$$.fragment,m),c=!1},d(m){m&&o(u),t&&x(t,m)}}}function fM(k,t,u){let{classNames:c=""}=t,{pipeline:h=""}=t;const f={"text-classification":RE,"token-classification":VE,"table-question-answering":yE,"question-answering":iE,"zero-shot-classification":YE,translation:UE,summarization:kE,conversational:BR,"feature-extraction":QR,"text-generation":zE,"text2text-generation":xE,"fill-mask":U8,"sentence-similarity":cE,"text-to-speech":DE,"automatic-speech-recognition":SR,"audio-to-audio":IR,"audio-classification":PR,"voice-activity-detection":KE,"image-classification":YR,"object-detection":sE,"image-segmentation":tE,"text-to-image":AE,"image-to-text":NE,"image-to-image":iM,"unconditional-image-generation":cM,"reinforcement-learning":tM,robotics:sM,"tabular-classification":uE,"tabular-regression":gE,"document-question-answering":uM},p=Object.fromEntries(Object.entries(W8).map(([m,R])=>[m,`tag-ico-${R.color}`]));return k.$$set=m=>{"classNames"in m&&u(0,c=m.classNames),"pipeline"in m&&u(1,h=m.pipeline)},[c,h,f,p]}class gM extends fe{constructor(t){super();ge(this,t,fM,mM,_e,{classNames:0,pipeline:1})}}function _M(k){var I;let t,u,c,h,f=((I=W8[k[1]].name)!=null?I:k[1])+"",p,m,R;return u=new gM({props:{classNames:"mr-1",pipeline:k[1]}}),{c(){t=l("div"),y(u.$$.fragment),c=g(),h=l("span"),p=a(f),this.h()},l(P){t=d(P,"DIV",{class:!0});var L=s(t);C(u.$$.fragment,L),c=_(L),h=d(L,"SPAN",{});var H=s(h);p=r(H,f),H.forEach(o),L.forEach(o),this.h()},h(){n(t,"class",m="inline-flex items-center border pr-1 rounded-xl "+k[0])},m(P,L){b(P,t,L),T(u,t,null),e(t,c),e(t,h),e(h,p),R=!0},p(P,[L]){var V;const H={};L&2&&(H.pipeline=P[1]),u.$set(H),(!R||L&2)&&f!==(f=((V=W8[P[1]].name)!=null?V:P[1])+"")&&MR(p,f),(!R||L&1&&m!==(m="inline-flex items-center border pr-1 rounded-xl "+P[0]))&&n(t,"class",m)},i(P){R||(F(u.$$.fragment,P),R=!0)},o(P){$(u.$$.fragment,P),R=!1},d(P){P&&o(t),x(u)}}}function bM(k,t,u){let{classNames:c=""}=t,{pipeline:h=""}=t;return k.$$set=f=>{"classNames"in f&&u(0,c=f.classNames),"pipeline"in f&&u(1,h=f.pipeline)},[c,h]}class N1 extends fe{constructor(t){super();ge(this,t,bM,_M,_e,{classNames:0,pipeline:1})}}function kM(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaConfig, RobertaModel

# Initializing a RoBERTa configuration
configuration = RobertaConfig()

# Initializing a model (with random weights) from the configuration
model = RobertaModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaConfig, RobertaModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a RoBERTa configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = RobertaConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RobertaModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){t=l("p"),u=a("Examples:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Examples:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function vM(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
tokenizer("Hello world")['input_ids']
tokenizer(" Hello world")['input_ids']`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer(<span class="hljs-string">&quot;Hello world&quot;</span>)[<span class="hljs-string">&#x27;input_ids&#x27;</span>]</span>
[0, 31414, 232, 328, 2]
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer(<span class="hljs-string">&quot; Hello world&quot;</span>)[<span class="hljs-string">&#x27;input_ids&#x27;</span>]</span>
[0, 20920, 232, 2]`}}),{c(){t=l("p"),u=a("be encoded differently whether it is at the beginning of the sentence (without space) or not:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"be encoded differently whether it is at the beginning of the sentence (without space) or not:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function wM(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("When used with "),c=l("code"),h=a("is_split_into_words=True"),f=a(", this tokenizer will add a space before each word (even the first one).")},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"When used with "),c=d(m,"CODE",{});var R=s(c);h=r(R,"is_split_into_words=True"),R.forEach(o),f=r(m,", this tokenizer will add a space before each word (even the first one)."),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function yM(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizerFast
tokenizer = RobertaTokenizerFast.from_pretrained("roberta-base")
tokenizer("Hello world")['input_ids']
tokenizer(" Hello world")['input_ids']`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizerFast</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer = RobertaTokenizerFast.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer(<span class="hljs-string">&quot;Hello world&quot;</span>)[<span class="hljs-string">&#x27;input_ids&#x27;</span>]</span>
[0, 31414, 232, 328, 2]
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer(<span class="hljs-string">&quot; Hello world&quot;</span>)[<span class="hljs-string">&#x27;input_ids&#x27;</span>]</span>
[0, 20920, 232, 2]`}}),{c(){t=l("p"),u=a("be encoded differently whether it is at the beginning of the sentence (without space) or not:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"be encoded differently whether it is at the beginning of the sentence (without space) or not:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function TM(k){let t,u,c,h,f,p,m,R;return{c(){t=l("p"),u=a("When used with "),c=l("code"),h=a("is_split_into_words=True"),f=a(", this tokenizer needs to be instantiated with "),p=l("code"),m=a("add_prefix_space=True"),R=a(".")},l(I){t=d(I,"P",{});var P=s(t);u=r(P,"When used with "),c=d(P,"CODE",{});var L=s(c);h=r(L,"is_split_into_words=True"),L.forEach(o),f=r(P,", this tokenizer needs to be instantiated with "),p=d(P,"CODE",{});var H=s(p);m=r(H,"add_prefix_space=True"),H.forEach(o),R=r(P,"."),P.forEach(o)},m(I,P){b(I,t,P),e(t,u),e(t,c),e(c,h),e(t,f),e(t,p),e(p,m),e(t,R)},d(I){I&&o(t)}}}function $M(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function xM(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer, RobertaModel
import torch

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
model = RobertaModel.from_pretrained("roberta-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, RobertaModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RobertaModel.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){t=l("p"),u=a("Example:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function FM(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function CM(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer, RobertaForCausalLM, RobertaConfig
import torch

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
config = RobertaConfig.from_pretrained("roberta-base")
config.is_decoder = True
model = RobertaForCausalLM.from_pretrained("roberta-base", config=config)

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

prediction_logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, RobertaForCausalLM, RobertaConfig
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config = RobertaConfig.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.is_decoder = <span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RobertaForCausalLM.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>, config=config)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>prediction_logits = outputs.logits`}}),{c(){t=l("p"),u=a("Example:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function RM(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function EM(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer, RobertaForMaskedLM
import torch

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
model = RobertaForMaskedLM.from_pretrained("roberta-base")

inputs = tokenizer("The capital of France is <mask>.", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

# retrieve index of <mask>
mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]

predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)
tokenizer.decode(predicted_token_id)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, RobertaForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RobertaForMaskedLM.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is &lt;mask&gt;.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve index of &lt;mask&gt;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[<span class="hljs-number">0</span>].nonzero(as_tuple=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_id = logits[<span class="hljs-number">0</span>, mask_token_index].argmax(axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predicted_token_id)
<span class="hljs-string">&#x27; Paris&#x27;</span>`}}),{c(){t=l("p"),u=a("Example:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function MM(k){let t,u;return t=new he({props:{code:`labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]
# mask labels of non-<mask> tokens
labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

outputs = model(**inputs, labels=labels)
round(outputs.loss.item(), 2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># mask labels of non-&lt;mask&gt; tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -<span class="hljs-number">100</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(outputs.loss.item(), <span class="hljs-number">2</span>)
<span class="hljs-number">0.1</span>`}}),{c(){y(t.$$.fragment)},l(c){C(t.$$.fragment,c)},m(c,h){T(t,c,h),u=!0},p:j,i(c){u||(F(t.$$.fragment,c),u=!0)},o(c){$(t.$$.fragment,c),u=!1},d(c){x(t,c)}}}function zM(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function qM(k){let t,u,c,h,f;return h=new he({props:{code:`import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification

tokenizer = RobertaTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-emotion")
model = RobertaForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-emotion")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, RobertaForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;cardiffnlp/twitter-roberta-base-emotion&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RobertaForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;cardiffnlp/twitter-roberta-base-emotion&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
<span class="hljs-string">&#x27;optimism&#x27;</span>`}}),{c(){t=l("p"),u=a("Example of single-label classification:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example of single-label classification:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function jM(k){let t,u;return t=new he({props:{code:'# To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\nnum_labels = len(model.config.id2label)\nmodel = RobertaForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-emotion", num_labels=num_labels)\n\nlabels = torch.tensor([1])\nloss = model(**inputs, labels=labels).loss\nround(loss.item(), 2)',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RobertaForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;cardiffnlp/twitter-roberta-base-emotion&quot;</span>, num_labels=num_labels)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
<span class="hljs-number">0.08</span>`}}),{c(){y(t.$$.fragment)},l(c){C(t.$$.fragment,c)},m(c,h){T(t,c,h),u=!0},p:j,i(c){u||(F(t.$$.fragment,c),u=!0)},o(c){$(t.$$.fragment,c),u=!1},d(c){x(t,c)}}}function AM(k){let t,u,c,h,f;return h=new he({props:{code:`import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification

tokenizer = RobertaTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-emotion")
model = RobertaForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-emotion", problem_type="multi_label_classification")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, RobertaForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;cardiffnlp/twitter-roberta-base-emotion&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RobertaForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;cardiffnlp/twitter-roberta-base-emotion&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
<span class="hljs-string">&#x27;optimism&#x27;</span>`}}),{c(){t=l("p"),u=a("Example of multi-label classification:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example of multi-label classification:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function PM(k){let t,u;return t=new he({props:{code:`# To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`
num_labels = len(model.config.id2label)
model = RobertaForSequenceClassification.from_pretrained(
    "cardiffnlp/twitter-roberta-base-emotion", num_labels=num_labels, problem_type="multi_label_classification"
)

labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(
    torch.float
)
loss = model(**inputs, labels=labels).loss
loss.backward()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RobertaForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;cardiffnlp/twitter-roberta-base-emotion&quot;</span>, num_labels=num_labels, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(
<span class="hljs-meta">... </span>    torch.<span class="hljs-built_in">float</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span>loss.backward()`}}),{c(){y(t.$$.fragment)},l(c){C(t.$$.fragment,c)},m(c,h){T(t,c,h),u=!0},p:j,i(c){u||(F(t.$$.fragment,c),u=!0)},o(c){$(t.$$.fragment,c),u=!1},d(c){x(t,c)}}}function LM(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function NM(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer, RobertaForMultipleChoice
import torch

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
model = RobertaForMultipleChoice.from_pretrained("roberta-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."
labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="pt", padding=True)
outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1

# the linear classifier still needs to be trained
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, RobertaForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RobertaForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># choice0 is correct (according to Wikipedia ;)), batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v.unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}, labels=labels)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){t=l("p"),u=a("Example:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function IM(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function OM(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer, RobertaForTokenClassification
import torch

tokenizer = RobertaTokenizer.from_pretrained("Jean-Baptiste/roberta-large-ner-english")
model = RobertaForTokenClassification.from_pretrained("Jean-Baptiste/roberta-large-ner-english")

inputs = tokenizer(
    "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="pt"
)

with torch.no_grad():
    logits = model(**inputs).logits

predicted_token_class_ids = logits.argmax(-1)

# Note that tokens are classified rather then input words which means that
# there might be more predicted token classes than words.
# Multiple token classes might account for the same word
predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]
predicted_tokens_classes`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, RobertaForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;Jean-Baptiste/roberta-large-ner-english&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RobertaForTokenClassification.from_pretrained(<span class="hljs-string">&quot;Jean-Baptiste/roberta-large-ner-english&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;HuggingFace is a company based in Paris and New York&quot;</span>, add_special_tokens=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_class_ids = logits.argmax(-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Note that tokens are classified rather then input words which means that</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># there might be more predicted token classes than words.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multiple token classes might account for the same word</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes = [model.config.id2label[t.item()] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> predicted_token_class_ids[<span class="hljs-number">0</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes
[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;ORG&#x27;</span>, <span class="hljs-string">&#x27;ORG&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;LOC&#x27;</span>, <span class="hljs-string">&#x27;LOC&#x27;</span>]`}}),{c(){t=l("p"),u=a("Example:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function DM(k){let t,u;return t=new he({props:{code:`labels = predicted_token_class_ids
loss = model(**inputs, labels=labels).loss
round(loss.item(), 2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = predicted_token_class_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
<span class="hljs-number">0.01</span>`}}),{c(){y(t.$$.fragment)},l(c){C(t.$$.fragment,c)},m(c,h){T(t,c,h),u=!0},p:j,i(c){u||(F(t.$$.fragment,c),u=!0)},o(c){$(t.$$.fragment,c),u=!1},d(c){x(t,c)}}}function SM(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function HM(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer, RobertaForQuestionAnswering
import torch

tokenizer = RobertaTokenizer.from_pretrained("deepset/roberta-base-squad2")
model = RobertaForQuestionAnswering.from_pretrained("deepset/roberta-base-squad2")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

inputs = tokenizer(question, text, return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs)

answer_start_index = outputs.start_logits.argmax()
answer_end_index = outputs.end_logits.argmax()

predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
tokenizer.decode(predict_answer_tokens)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, RobertaForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;deepset/roberta-base-squad2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RobertaForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;deepset/roberta-base-squad2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>answer_start_index = outputs.start_logits.argmax()
<span class="hljs-meta">&gt;&gt;&gt; </span>answer_end_index = outputs.end_logits.argmax()

<span class="hljs-meta">&gt;&gt;&gt; </span>predict_answer_tokens = inputs.input_ids[<span class="hljs-number">0</span>, answer_start_index : answer_end_index + <span class="hljs-number">1</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predict_answer_tokens)
<span class="hljs-string">&#x27; puppet&#x27;</span>`}}),{c(){t=l("p"),u=a("Example:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function VM(k){let t,u;return t=new he({props:{code:`# target is "nice puppet"
target_start_index = torch.tensor([14])
target_end_index = torch.tensor([15])

outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
loss = outputs.loss
round(loss.item(), 2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target is &quot;nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_start_index = torch.tensor([<span class="hljs-number">14</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>target_end_index = torch.tensor([<span class="hljs-number">15</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
<span class="hljs-number">0.86</span>`}}),{c(){y(t.$$.fragment)},l(c){C(t.$$.fragment,c)},m(c,h){T(t,c,h),u=!0},p:j,i(c){u||(F(t.$$.fragment,c),u=!0)},o(c){$(t.$$.fragment,c),u=!1},d(c){x(t,c)}}}function BM(k){let t,u,c,h,f,p,m,R,I,P,L,H,V,E,ie,U,Z,Te,G,Ee,be,oe,Me,$e,X,ze,xe,ee,qe,Fe,le,N,D,Ce,te,je,Re,Q,ke,Ae,O,Pe,ne,Le,ve,se,Ne,de,K,we,ae,Ie,ce,S,Oe,B,De,Se;return{c(){t=l("p"),u=a("TensorFlow models and layers in "),c=l("code"),h=a("transformers"),f=a(" accept two formats as input:"),p=g(),m=l("ul"),R=l("li"),I=a("having all inputs as keyword arguments (like PyTorch models), or"),P=g(),L=l("li"),H=a("having all inputs as a list, tuple or dict in the first positional argument."),V=g(),E=l("p"),ie=a(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),U=l("code"),Z=a("model.fit()"),Te=a(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),G=l("code"),Ee=a("model.fit()"),be=a(` supports! If, however, you want to use the second
format outside of Keras methods like `),oe=l("code"),Me=a("fit()"),$e=a(" and "),X=l("code"),ze=a("predict()"),xe=a(`, such as when creating your own layers or models with
the Keras `),ee=l("code"),qe=a("Functional"),Fe=a(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),le=g(),N=l("ul"),D=l("li"),Ce=a("a single Tensor with "),te=l("code"),je=a("input_ids"),Re=a(" only and nothing else: "),Q=l("code"),ke=a("model(input_ids)"),Ae=g(),O=l("li"),Pe=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),ne=l("code"),Le=a("model([input_ids, attention_mask])"),ve=a(" or "),se=l("code"),Ne=a("model([input_ids, attention_mask, token_type_ids])"),de=g(),K=l("li"),we=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),ae=l("code"),Ie=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ce=g(),S=l("p"),Oe=a(`Note that when creating models and layers with
`),B=l("a"),De=a("subclassing"),Se=a(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(w){t=d(w,"P",{});var q=s(t);u=r(q,"TensorFlow models and layers in "),c=d(q,"CODE",{});var Ke=s(c);h=r(Ke,"transformers"),Ke.forEach(o),f=r(q," accept two formats as input:"),q.forEach(o),p=_(w),m=d(w,"UL",{});var ue=s(m);R=d(ue,"LI",{});var ot=s(R);I=r(ot,"having all inputs as keyword arguments (like PyTorch models), or"),ot.forEach(o),P=_(ue),L=d(ue,"LI",{});var Ye=s(L);H=r(Ye,"having all inputs as a list, tuple or dict in the first positional argument."),Ye.forEach(o),ue.forEach(o),V=_(w),E=d(w,"P",{});var A=s(E);ie=r(A,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),U=d(A,"CODE",{});var nt=s(U);Z=r(nt,"model.fit()"),nt.forEach(o),Te=r(A,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),G=d(A,"CODE",{});var Ue=s(G);Ee=r(Ue,"model.fit()"),Ue.forEach(o),be=r(A,` supports! If, however, you want to use the second
format outside of Keras methods like `),oe=d(A,"CODE",{});var st=s(oe);Me=r(st,"fit()"),st.forEach(o),$e=r(A," and "),X=d(A,"CODE",{});var at=s(X);ze=r(at,"predict()"),at.forEach(o),xe=r(A,`, such as when creating your own layers or models with
the Keras `),ee=d(A,"CODE",{});var Qe=s(ee);qe=r(Qe,"Functional"),Qe.forEach(o),Fe=r(A,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),A.forEach(o),le=_(w),N=d(w,"UL",{});var J=s(N);D=d(J,"LI",{});var re=s(D);Ce=r(re,"a single Tensor with "),te=d(re,"CODE",{});var Xe=s(te);je=r(Xe,"input_ids"),Xe.forEach(o),Re=r(re," only and nothing else: "),Q=d(re,"CODE",{});var He=s(Q);ke=r(He,"model(input_ids)"),He.forEach(o),re.forEach(o),Ae=_(J),O=d(J,"LI",{});var Y=s(O);Pe=r(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),ne=d(Y,"CODE",{});var Je=s(ne);Le=r(Je,"model([input_ids, attention_mask])"),Je.forEach(o),ve=r(Y," or "),se=d(Y,"CODE",{});var Ze=s(se);Ne=r(Ze,"model([input_ids, attention_mask, token_type_ids])"),Ze.forEach(o),Y.forEach(o),de=_(J),K=d(J,"LI",{});var Ve=s(K);we=r(Ve,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),ae=d(Ve,"CODE",{});var Ge=s(ae);Ie=r(Ge,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ge.forEach(o),Ve.forEach(o),J.forEach(o),ce=_(w),S=d(w,"P",{});var me=s(S);Oe=r(me,`Note that when creating models and layers with
`),B=d(me,"A",{href:!0,rel:!0});var et=s(B);De=r(et,"subclassing"),et.forEach(o),Se=r(me,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),me.forEach(o),this.h()},h(){n(B,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),n(B,"rel","nofollow")},m(w,q){b(w,t,q),e(t,u),e(t,c),e(c,h),e(t,f),b(w,p,q),b(w,m,q),e(m,R),e(R,I),e(m,P),e(m,L),e(L,H),b(w,V,q),b(w,E,q),e(E,ie),e(E,U),e(U,Z),e(E,Te),e(E,G),e(G,Ee),e(E,be),e(E,oe),e(oe,Me),e(E,$e),e(E,X),e(X,ze),e(E,xe),e(E,ee),e(ee,qe),e(E,Fe),b(w,le,q),b(w,N,q),e(N,D),e(D,Ce),e(D,te),e(te,je),e(D,Re),e(D,Q),e(Q,ke),e(N,Ae),e(N,O),e(O,Pe),e(O,ne),e(ne,Le),e(O,ve),e(O,se),e(se,Ne),e(N,de),e(N,K),e(K,we),e(K,ae),e(ae,Ie),b(w,ce,q),b(w,S,q),e(S,Oe),e(S,B),e(B,De),e(S,Se)},d(w){w&&o(t),w&&o(p),w&&o(m),w&&o(V),w&&o(E),w&&o(le),w&&o(N),w&&o(ce),w&&o(S)}}}function WM(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function UM(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer, TFRobertaModel
import tensorflow as tf

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
model = TFRobertaModel.from_pretrained("roberta-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, TFRobertaModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFRobertaModel.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){t=l("p"),u=a("Example:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function QM(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function ZM(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer, TFRobertaForCausalLM
import tensorflow as tf

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
model = TFRobertaForCausalLM.from_pretrained("roberta-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, TFRobertaForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFRobertaForCausalLM.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){t=l("p"),u=a("Example:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function KM(k){let t,u,c,h,f,p,m,R,I,P,L,H,V,E,ie,U,Z,Te,G,Ee,be,oe,Me,$e,X,ze,xe,ee,qe,Fe,le,N,D,Ce,te,je,Re,Q,ke,Ae,O,Pe,ne,Le,ve,se,Ne,de,K,we,ae,Ie,ce,S,Oe,B,De,Se;return{c(){t=l("p"),u=a("TensorFlow models and layers in "),c=l("code"),h=a("transformers"),f=a(" accept two formats as input:"),p=g(),m=l("ul"),R=l("li"),I=a("having all inputs as keyword arguments (like PyTorch models), or"),P=g(),L=l("li"),H=a("having all inputs as a list, tuple or dict in the first positional argument."),V=g(),E=l("p"),ie=a(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),U=l("code"),Z=a("model.fit()"),Te=a(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),G=l("code"),Ee=a("model.fit()"),be=a(` supports! If, however, you want to use the second
format outside of Keras methods like `),oe=l("code"),Me=a("fit()"),$e=a(" and "),X=l("code"),ze=a("predict()"),xe=a(`, such as when creating your own layers or models with
the Keras `),ee=l("code"),qe=a("Functional"),Fe=a(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),le=g(),N=l("ul"),D=l("li"),Ce=a("a single Tensor with "),te=l("code"),je=a("input_ids"),Re=a(" only and nothing else: "),Q=l("code"),ke=a("model(input_ids)"),Ae=g(),O=l("li"),Pe=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),ne=l("code"),Le=a("model([input_ids, attention_mask])"),ve=a(" or "),se=l("code"),Ne=a("model([input_ids, attention_mask, token_type_ids])"),de=g(),K=l("li"),we=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),ae=l("code"),Ie=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ce=g(),S=l("p"),Oe=a(`Note that when creating models and layers with
`),B=l("a"),De=a("subclassing"),Se=a(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(w){t=d(w,"P",{});var q=s(t);u=r(q,"TensorFlow models and layers in "),c=d(q,"CODE",{});var Ke=s(c);h=r(Ke,"transformers"),Ke.forEach(o),f=r(q," accept two formats as input:"),q.forEach(o),p=_(w),m=d(w,"UL",{});var ue=s(m);R=d(ue,"LI",{});var ot=s(R);I=r(ot,"having all inputs as keyword arguments (like PyTorch models), or"),ot.forEach(o),P=_(ue),L=d(ue,"LI",{});var Ye=s(L);H=r(Ye,"having all inputs as a list, tuple or dict in the first positional argument."),Ye.forEach(o),ue.forEach(o),V=_(w),E=d(w,"P",{});var A=s(E);ie=r(A,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),U=d(A,"CODE",{});var nt=s(U);Z=r(nt,"model.fit()"),nt.forEach(o),Te=r(A,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),G=d(A,"CODE",{});var Ue=s(G);Ee=r(Ue,"model.fit()"),Ue.forEach(o),be=r(A,` supports! If, however, you want to use the second
format outside of Keras methods like `),oe=d(A,"CODE",{});var st=s(oe);Me=r(st,"fit()"),st.forEach(o),$e=r(A," and "),X=d(A,"CODE",{});var at=s(X);ze=r(at,"predict()"),at.forEach(o),xe=r(A,`, such as when creating your own layers or models with
the Keras `),ee=d(A,"CODE",{});var Qe=s(ee);qe=r(Qe,"Functional"),Qe.forEach(o),Fe=r(A,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),A.forEach(o),le=_(w),N=d(w,"UL",{});var J=s(N);D=d(J,"LI",{});var re=s(D);Ce=r(re,"a single Tensor with "),te=d(re,"CODE",{});var Xe=s(te);je=r(Xe,"input_ids"),Xe.forEach(o),Re=r(re," only and nothing else: "),Q=d(re,"CODE",{});var He=s(Q);ke=r(He,"model(input_ids)"),He.forEach(o),re.forEach(o),Ae=_(J),O=d(J,"LI",{});var Y=s(O);Pe=r(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),ne=d(Y,"CODE",{});var Je=s(ne);Le=r(Je,"model([input_ids, attention_mask])"),Je.forEach(o),ve=r(Y," or "),se=d(Y,"CODE",{});var Ze=s(se);Ne=r(Ze,"model([input_ids, attention_mask, token_type_ids])"),Ze.forEach(o),Y.forEach(o),de=_(J),K=d(J,"LI",{});var Ve=s(K);we=r(Ve,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),ae=d(Ve,"CODE",{});var Ge=s(ae);Ie=r(Ge,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ge.forEach(o),Ve.forEach(o),J.forEach(o),ce=_(w),S=d(w,"P",{});var me=s(S);Oe=r(me,`Note that when creating models and layers with
`),B=d(me,"A",{href:!0,rel:!0});var et=s(B);De=r(et,"subclassing"),et.forEach(o),Se=r(me,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),me.forEach(o),this.h()},h(){n(B,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),n(B,"rel","nofollow")},m(w,q){b(w,t,q),e(t,u),e(t,c),e(c,h),e(t,f),b(w,p,q),b(w,m,q),e(m,R),e(R,I),e(m,P),e(m,L),e(L,H),b(w,V,q),b(w,E,q),e(E,ie),e(E,U),e(U,Z),e(E,Te),e(E,G),e(G,Ee),e(E,be),e(E,oe),e(oe,Me),e(E,$e),e(E,X),e(X,ze),e(E,xe),e(E,ee),e(ee,qe),e(E,Fe),b(w,le,q),b(w,N,q),e(N,D),e(D,Ce),e(D,te),e(te,je),e(D,Re),e(D,Q),e(Q,ke),e(N,Ae),e(N,O),e(O,Pe),e(O,ne),e(ne,Le),e(O,ve),e(O,se),e(se,Ne),e(N,de),e(N,K),e(K,we),e(K,ae),e(ae,Ie),b(w,ce,q),b(w,S,q),e(S,Oe),e(S,B),e(B,De),e(S,Se)},d(w){w&&o(t),w&&o(p),w&&o(m),w&&o(V),w&&o(E),w&&o(le),w&&o(N),w&&o(ce),w&&o(S)}}}function JM(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function GM(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer, TFRobertaForMaskedLM
import tensorflow as tf

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
model = TFRobertaForMaskedLM.from_pretrained("roberta-base")

inputs = tokenizer("The capital of France is <mask>.", return_tensors="tf")
logits = model(**inputs).logits

# retrieve index of <mask>
mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[0])
selected_logits = tf.gather_nd(logits[0], indices=mask_token_index)

predicted_token_id = tf.math.argmax(selected_logits, axis=-1)
tokenizer.decode(predicted_token_id)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, TFRobertaForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFRobertaForMaskedLM.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is &lt;mask&gt;.&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve index of &lt;mask&gt;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_token_index = tf.where((inputs.input_ids == tokenizer.mask_token_id)[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>selected_logits = tf.gather_nd(logits[<span class="hljs-number">0</span>], indices=mask_token_index)

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_id = tf.math.argmax(selected_logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predicted_token_id)
<span class="hljs-string">&#x27; Paris&#x27;</span>`}}),{c(){t=l("p"),u=a("Example:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function YM(k){let t,u;return t=new he({props:{code:`labels = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]
# mask labels of non-<mask> tokens
labels = tf.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

outputs = model(**inputs, labels=labels)
round(float(outputs.loss), 2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># mask labels of non-&lt;mask&gt; tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tf.where(inputs.input_ids == tokenizer.mask_token_id, labels, -<span class="hljs-number">100</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(outputs.loss), <span class="hljs-number">2</span>)
<span class="hljs-number">0.1</span>`}}),{c(){y(t.$$.fragment)},l(c){C(t.$$.fragment,c)},m(c,h){T(t,c,h),u=!0},p:j,i(c){u||(F(t.$$.fragment,c),u=!0)},o(c){$(t.$$.fragment,c),u=!1},d(c){x(t,c)}}}function XM(k){let t,u,c,h,f,p,m,R,I,P,L,H,V,E,ie,U,Z,Te,G,Ee,be,oe,Me,$e,X,ze,xe,ee,qe,Fe,le,N,D,Ce,te,je,Re,Q,ke,Ae,O,Pe,ne,Le,ve,se,Ne,de,K,we,ae,Ie,ce,S,Oe,B,De,Se;return{c(){t=l("p"),u=a("TensorFlow models and layers in "),c=l("code"),h=a("transformers"),f=a(" accept two formats as input:"),p=g(),m=l("ul"),R=l("li"),I=a("having all inputs as keyword arguments (like PyTorch models), or"),P=g(),L=l("li"),H=a("having all inputs as a list, tuple or dict in the first positional argument."),V=g(),E=l("p"),ie=a(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),U=l("code"),Z=a("model.fit()"),Te=a(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),G=l("code"),Ee=a("model.fit()"),be=a(` supports! If, however, you want to use the second
format outside of Keras methods like `),oe=l("code"),Me=a("fit()"),$e=a(" and "),X=l("code"),ze=a("predict()"),xe=a(`, such as when creating your own layers or models with
the Keras `),ee=l("code"),qe=a("Functional"),Fe=a(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),le=g(),N=l("ul"),D=l("li"),Ce=a("a single Tensor with "),te=l("code"),je=a("input_ids"),Re=a(" only and nothing else: "),Q=l("code"),ke=a("model(input_ids)"),Ae=g(),O=l("li"),Pe=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),ne=l("code"),Le=a("model([input_ids, attention_mask])"),ve=a(" or "),se=l("code"),Ne=a("model([input_ids, attention_mask, token_type_ids])"),de=g(),K=l("li"),we=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),ae=l("code"),Ie=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ce=g(),S=l("p"),Oe=a(`Note that when creating models and layers with
`),B=l("a"),De=a("subclassing"),Se=a(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(w){t=d(w,"P",{});var q=s(t);u=r(q,"TensorFlow models and layers in "),c=d(q,"CODE",{});var Ke=s(c);h=r(Ke,"transformers"),Ke.forEach(o),f=r(q," accept two formats as input:"),q.forEach(o),p=_(w),m=d(w,"UL",{});var ue=s(m);R=d(ue,"LI",{});var ot=s(R);I=r(ot,"having all inputs as keyword arguments (like PyTorch models), or"),ot.forEach(o),P=_(ue),L=d(ue,"LI",{});var Ye=s(L);H=r(Ye,"having all inputs as a list, tuple or dict in the first positional argument."),Ye.forEach(o),ue.forEach(o),V=_(w),E=d(w,"P",{});var A=s(E);ie=r(A,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),U=d(A,"CODE",{});var nt=s(U);Z=r(nt,"model.fit()"),nt.forEach(o),Te=r(A,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),G=d(A,"CODE",{});var Ue=s(G);Ee=r(Ue,"model.fit()"),Ue.forEach(o),be=r(A,` supports! If, however, you want to use the second
format outside of Keras methods like `),oe=d(A,"CODE",{});var st=s(oe);Me=r(st,"fit()"),st.forEach(o),$e=r(A," and "),X=d(A,"CODE",{});var at=s(X);ze=r(at,"predict()"),at.forEach(o),xe=r(A,`, such as when creating your own layers or models with
the Keras `),ee=d(A,"CODE",{});var Qe=s(ee);qe=r(Qe,"Functional"),Qe.forEach(o),Fe=r(A,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),A.forEach(o),le=_(w),N=d(w,"UL",{});var J=s(N);D=d(J,"LI",{});var re=s(D);Ce=r(re,"a single Tensor with "),te=d(re,"CODE",{});var Xe=s(te);je=r(Xe,"input_ids"),Xe.forEach(o),Re=r(re," only and nothing else: "),Q=d(re,"CODE",{});var He=s(Q);ke=r(He,"model(input_ids)"),He.forEach(o),re.forEach(o),Ae=_(J),O=d(J,"LI",{});var Y=s(O);Pe=r(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),ne=d(Y,"CODE",{});var Je=s(ne);Le=r(Je,"model([input_ids, attention_mask])"),Je.forEach(o),ve=r(Y," or "),se=d(Y,"CODE",{});var Ze=s(se);Ne=r(Ze,"model([input_ids, attention_mask, token_type_ids])"),Ze.forEach(o),Y.forEach(o),de=_(J),K=d(J,"LI",{});var Ve=s(K);we=r(Ve,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),ae=d(Ve,"CODE",{});var Ge=s(ae);Ie=r(Ge,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ge.forEach(o),Ve.forEach(o),J.forEach(o),ce=_(w),S=d(w,"P",{});var me=s(S);Oe=r(me,`Note that when creating models and layers with
`),B=d(me,"A",{href:!0,rel:!0});var et=s(B);De=r(et,"subclassing"),et.forEach(o),Se=r(me,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),me.forEach(o),this.h()},h(){n(B,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),n(B,"rel","nofollow")},m(w,q){b(w,t,q),e(t,u),e(t,c),e(c,h),e(t,f),b(w,p,q),b(w,m,q),e(m,R),e(R,I),e(m,P),e(m,L),e(L,H),b(w,V,q),b(w,E,q),e(E,ie),e(E,U),e(U,Z),e(E,Te),e(E,G),e(G,Ee),e(E,be),e(E,oe),e(oe,Me),e(E,$e),e(E,X),e(X,ze),e(E,xe),e(E,ee),e(ee,qe),e(E,Fe),b(w,le,q),b(w,N,q),e(N,D),e(D,Ce),e(D,te),e(te,je),e(D,Re),e(D,Q),e(Q,ke),e(N,Ae),e(N,O),e(O,Pe),e(O,ne),e(ne,Le),e(O,ve),e(O,se),e(se,Ne),e(N,de),e(N,K),e(K,we),e(K,ae),e(ae,Ie),b(w,ce,q),b(w,S,q),e(S,Oe),e(S,B),e(B,De),e(S,Se)},d(w){w&&o(t),w&&o(p),w&&o(m),w&&o(V),w&&o(E),w&&o(le),w&&o(N),w&&o(ce),w&&o(S)}}}function ez(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function tz(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer, TFRobertaForSequenceClassification
import tensorflow as tf

tokenizer = RobertaTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-emotion")
model = TFRobertaForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-emotion")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")

logits = model(**inputs).logits

predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])
model.config.id2label[predicted_class_id]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, TFRobertaForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;cardiffnlp/twitter-roberta-base-emotion&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFRobertaForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;cardiffnlp/twitter-roberta-base-emotion&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = <span class="hljs-built_in">int</span>(tf.math.argmax(logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
<span class="hljs-string">&#x27;optimism&#x27;</span>`}}),{c(){t=l("p"),u=a("Example:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function oz(k){let t,u;return t=new he({props:{code:'# To train a model on `num_labels` classes, you can pass `num_labels=num_labels` to `.from_pretrained(...)`\nnum_labels = len(model.config.id2label)\nmodel = TFRobertaForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-emotion", num_labels=num_labels)\n\nlabels = tf.constant(1)\nloss = model(**inputs, labels=labels).loss\nround(float(loss), 2)',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFRobertaForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;cardiffnlp/twitter-roberta-base-emotion&quot;</span>, num_labels=num_labels)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tf.constant(<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(loss), <span class="hljs-number">2</span>)
<span class="hljs-number">0.08</span>`}}),{c(){y(t.$$.fragment)},l(c){C(t.$$.fragment,c)},m(c,h){T(t,c,h),u=!0},p:j,i(c){u||(F(t.$$.fragment,c),u=!0)},o(c){$(t.$$.fragment,c),u=!1},d(c){x(t,c)}}}function nz(k){let t,u,c,h,f,p,m,R,I,P,L,H,V,E,ie,U,Z,Te,G,Ee,be,oe,Me,$e,X,ze,xe,ee,qe,Fe,le,N,D,Ce,te,je,Re,Q,ke,Ae,O,Pe,ne,Le,ve,se,Ne,de,K,we,ae,Ie,ce,S,Oe,B,De,Se;return{c(){t=l("p"),u=a("TensorFlow models and layers in "),c=l("code"),h=a("transformers"),f=a(" accept two formats as input:"),p=g(),m=l("ul"),R=l("li"),I=a("having all inputs as keyword arguments (like PyTorch models), or"),P=g(),L=l("li"),H=a("having all inputs as a list, tuple or dict in the first positional argument."),V=g(),E=l("p"),ie=a(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),U=l("code"),Z=a("model.fit()"),Te=a(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),G=l("code"),Ee=a("model.fit()"),be=a(` supports! If, however, you want to use the second
format outside of Keras methods like `),oe=l("code"),Me=a("fit()"),$e=a(" and "),X=l("code"),ze=a("predict()"),xe=a(`, such as when creating your own layers or models with
the Keras `),ee=l("code"),qe=a("Functional"),Fe=a(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),le=g(),N=l("ul"),D=l("li"),Ce=a("a single Tensor with "),te=l("code"),je=a("input_ids"),Re=a(" only and nothing else: "),Q=l("code"),ke=a("model(input_ids)"),Ae=g(),O=l("li"),Pe=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),ne=l("code"),Le=a("model([input_ids, attention_mask])"),ve=a(" or "),se=l("code"),Ne=a("model([input_ids, attention_mask, token_type_ids])"),de=g(),K=l("li"),we=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),ae=l("code"),Ie=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ce=g(),S=l("p"),Oe=a(`Note that when creating models and layers with
`),B=l("a"),De=a("subclassing"),Se=a(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(w){t=d(w,"P",{});var q=s(t);u=r(q,"TensorFlow models and layers in "),c=d(q,"CODE",{});var Ke=s(c);h=r(Ke,"transformers"),Ke.forEach(o),f=r(q," accept two formats as input:"),q.forEach(o),p=_(w),m=d(w,"UL",{});var ue=s(m);R=d(ue,"LI",{});var ot=s(R);I=r(ot,"having all inputs as keyword arguments (like PyTorch models), or"),ot.forEach(o),P=_(ue),L=d(ue,"LI",{});var Ye=s(L);H=r(Ye,"having all inputs as a list, tuple or dict in the first positional argument."),Ye.forEach(o),ue.forEach(o),V=_(w),E=d(w,"P",{});var A=s(E);ie=r(A,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),U=d(A,"CODE",{});var nt=s(U);Z=r(nt,"model.fit()"),nt.forEach(o),Te=r(A,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),G=d(A,"CODE",{});var Ue=s(G);Ee=r(Ue,"model.fit()"),Ue.forEach(o),be=r(A,` supports! If, however, you want to use the second
format outside of Keras methods like `),oe=d(A,"CODE",{});var st=s(oe);Me=r(st,"fit()"),st.forEach(o),$e=r(A," and "),X=d(A,"CODE",{});var at=s(X);ze=r(at,"predict()"),at.forEach(o),xe=r(A,`, such as when creating your own layers or models with
the Keras `),ee=d(A,"CODE",{});var Qe=s(ee);qe=r(Qe,"Functional"),Qe.forEach(o),Fe=r(A,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),A.forEach(o),le=_(w),N=d(w,"UL",{});var J=s(N);D=d(J,"LI",{});var re=s(D);Ce=r(re,"a single Tensor with "),te=d(re,"CODE",{});var Xe=s(te);je=r(Xe,"input_ids"),Xe.forEach(o),Re=r(re," only and nothing else: "),Q=d(re,"CODE",{});var He=s(Q);ke=r(He,"model(input_ids)"),He.forEach(o),re.forEach(o),Ae=_(J),O=d(J,"LI",{});var Y=s(O);Pe=r(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),ne=d(Y,"CODE",{});var Je=s(ne);Le=r(Je,"model([input_ids, attention_mask])"),Je.forEach(o),ve=r(Y," or "),se=d(Y,"CODE",{});var Ze=s(se);Ne=r(Ze,"model([input_ids, attention_mask, token_type_ids])"),Ze.forEach(o),Y.forEach(o),de=_(J),K=d(J,"LI",{});var Ve=s(K);we=r(Ve,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),ae=d(Ve,"CODE",{});var Ge=s(ae);Ie=r(Ge,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ge.forEach(o),Ve.forEach(o),J.forEach(o),ce=_(w),S=d(w,"P",{});var me=s(S);Oe=r(me,`Note that when creating models and layers with
`),B=d(me,"A",{href:!0,rel:!0});var et=s(B);De=r(et,"subclassing"),et.forEach(o),Se=r(me,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),me.forEach(o),this.h()},h(){n(B,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),n(B,"rel","nofollow")},m(w,q){b(w,t,q),e(t,u),e(t,c),e(c,h),e(t,f),b(w,p,q),b(w,m,q),e(m,R),e(R,I),e(m,P),e(m,L),e(L,H),b(w,V,q),b(w,E,q),e(E,ie),e(E,U),e(U,Z),e(E,Te),e(E,G),e(G,Ee),e(E,be),e(E,oe),e(oe,Me),e(E,$e),e(E,X),e(X,ze),e(E,xe),e(E,ee),e(ee,qe),e(E,Fe),b(w,le,q),b(w,N,q),e(N,D),e(D,Ce),e(D,te),e(te,je),e(D,Re),e(D,Q),e(Q,ke),e(N,Ae),e(N,O),e(O,Pe),e(O,ne),e(ne,Le),e(O,ve),e(O,se),e(se,Ne),e(N,de),e(N,K),e(K,we),e(K,ae),e(ae,Ie),b(w,ce,q),b(w,S,q),e(S,Oe),e(S,B),e(B,De),e(S,Se)},d(w){w&&o(t),w&&o(p),w&&o(m),w&&o(V),w&&o(E),w&&o(le),w&&o(N),w&&o(ce),w&&o(S)}}}function sz(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function az(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer, TFRobertaForMultipleChoice
import tensorflow as tf

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
model = TFRobertaForMultipleChoice.from_pretrained("roberta-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="tf", padding=True)
inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}
outputs = model(inputs)  # batch size is 1

# the linear classifier still needs to be trained
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, TFRobertaForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFRobertaForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;tf&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {k: tf.expand_dims(v, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){t=l("p"),u=a("Example:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function rz(k){let t,u,c,h,f,p,m,R,I,P,L,H,V,E,ie,U,Z,Te,G,Ee,be,oe,Me,$e,X,ze,xe,ee,qe,Fe,le,N,D,Ce,te,je,Re,Q,ke,Ae,O,Pe,ne,Le,ve,se,Ne,de,K,we,ae,Ie,ce,S,Oe,B,De,Se;return{c(){t=l("p"),u=a("TensorFlow models and layers in "),c=l("code"),h=a("transformers"),f=a(" accept two formats as input:"),p=g(),m=l("ul"),R=l("li"),I=a("having all inputs as keyword arguments (like PyTorch models), or"),P=g(),L=l("li"),H=a("having all inputs as a list, tuple or dict in the first positional argument."),V=g(),E=l("p"),ie=a(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),U=l("code"),Z=a("model.fit()"),Te=a(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),G=l("code"),Ee=a("model.fit()"),be=a(` supports! If, however, you want to use the second
format outside of Keras methods like `),oe=l("code"),Me=a("fit()"),$e=a(" and "),X=l("code"),ze=a("predict()"),xe=a(`, such as when creating your own layers or models with
the Keras `),ee=l("code"),qe=a("Functional"),Fe=a(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),le=g(),N=l("ul"),D=l("li"),Ce=a("a single Tensor with "),te=l("code"),je=a("input_ids"),Re=a(" only and nothing else: "),Q=l("code"),ke=a("model(input_ids)"),Ae=g(),O=l("li"),Pe=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),ne=l("code"),Le=a("model([input_ids, attention_mask])"),ve=a(" or "),se=l("code"),Ne=a("model([input_ids, attention_mask, token_type_ids])"),de=g(),K=l("li"),we=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),ae=l("code"),Ie=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ce=g(),S=l("p"),Oe=a(`Note that when creating models and layers with
`),B=l("a"),De=a("subclassing"),Se=a(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(w){t=d(w,"P",{});var q=s(t);u=r(q,"TensorFlow models and layers in "),c=d(q,"CODE",{});var Ke=s(c);h=r(Ke,"transformers"),Ke.forEach(o),f=r(q," accept two formats as input:"),q.forEach(o),p=_(w),m=d(w,"UL",{});var ue=s(m);R=d(ue,"LI",{});var ot=s(R);I=r(ot,"having all inputs as keyword arguments (like PyTorch models), or"),ot.forEach(o),P=_(ue),L=d(ue,"LI",{});var Ye=s(L);H=r(Ye,"having all inputs as a list, tuple or dict in the first positional argument."),Ye.forEach(o),ue.forEach(o),V=_(w),E=d(w,"P",{});var A=s(E);ie=r(A,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),U=d(A,"CODE",{});var nt=s(U);Z=r(nt,"model.fit()"),nt.forEach(o),Te=r(A,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),G=d(A,"CODE",{});var Ue=s(G);Ee=r(Ue,"model.fit()"),Ue.forEach(o),be=r(A,` supports! If, however, you want to use the second
format outside of Keras methods like `),oe=d(A,"CODE",{});var st=s(oe);Me=r(st,"fit()"),st.forEach(o),$e=r(A," and "),X=d(A,"CODE",{});var at=s(X);ze=r(at,"predict()"),at.forEach(o),xe=r(A,`, such as when creating your own layers or models with
the Keras `),ee=d(A,"CODE",{});var Qe=s(ee);qe=r(Qe,"Functional"),Qe.forEach(o),Fe=r(A,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),A.forEach(o),le=_(w),N=d(w,"UL",{});var J=s(N);D=d(J,"LI",{});var re=s(D);Ce=r(re,"a single Tensor with "),te=d(re,"CODE",{});var Xe=s(te);je=r(Xe,"input_ids"),Xe.forEach(o),Re=r(re," only and nothing else: "),Q=d(re,"CODE",{});var He=s(Q);ke=r(He,"model(input_ids)"),He.forEach(o),re.forEach(o),Ae=_(J),O=d(J,"LI",{});var Y=s(O);Pe=r(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),ne=d(Y,"CODE",{});var Je=s(ne);Le=r(Je,"model([input_ids, attention_mask])"),Je.forEach(o),ve=r(Y," or "),se=d(Y,"CODE",{});var Ze=s(se);Ne=r(Ze,"model([input_ids, attention_mask, token_type_ids])"),Ze.forEach(o),Y.forEach(o),de=_(J),K=d(J,"LI",{});var Ve=s(K);we=r(Ve,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),ae=d(Ve,"CODE",{});var Ge=s(ae);Ie=r(Ge,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ge.forEach(o),Ve.forEach(o),J.forEach(o),ce=_(w),S=d(w,"P",{});var me=s(S);Oe=r(me,`Note that when creating models and layers with
`),B=d(me,"A",{href:!0,rel:!0});var et=s(B);De=r(et,"subclassing"),et.forEach(o),Se=r(me,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),me.forEach(o),this.h()},h(){n(B,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),n(B,"rel","nofollow")},m(w,q){b(w,t,q),e(t,u),e(t,c),e(c,h),e(t,f),b(w,p,q),b(w,m,q),e(m,R),e(R,I),e(m,P),e(m,L),e(L,H),b(w,V,q),b(w,E,q),e(E,ie),e(E,U),e(U,Z),e(E,Te),e(E,G),e(G,Ee),e(E,be),e(E,oe),e(oe,Me),e(E,$e),e(E,X),e(X,ze),e(E,xe),e(E,ee),e(ee,qe),e(E,Fe),b(w,le,q),b(w,N,q),e(N,D),e(D,Ce),e(D,te),e(te,je),e(D,Re),e(D,Q),e(Q,ke),e(N,Ae),e(N,O),e(O,Pe),e(O,ne),e(ne,Le),e(O,ve),e(O,se),e(se,Ne),e(N,de),e(N,K),e(K,we),e(K,ae),e(ae,Ie),b(w,ce,q),b(w,S,q),e(S,Oe),e(S,B),e(B,De),e(S,Se)},d(w){w&&o(t),w&&o(p),w&&o(m),w&&o(V),w&&o(E),w&&o(le),w&&o(N),w&&o(ce),w&&o(S)}}}function iz(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function lz(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer, TFRobertaForTokenClassification
import tensorflow as tf

tokenizer = RobertaTokenizer.from_pretrained("ydshieh/roberta-large-ner-english")
model = TFRobertaForTokenClassification.from_pretrained("ydshieh/roberta-large-ner-english")

inputs = tokenizer(
    "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="tf"
)

logits = model(**inputs).logits
predicted_token_class_ids = tf.math.argmax(logits, axis=-1)

# Note that tokens are classified rather then input words which means that
# there might be more predicted token classes than words.
# Multiple token classes might account for the same word
predicted_tokens_classes = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]
predicted_tokens_classes`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, TFRobertaForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;ydshieh/roberta-large-ner-english&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFRobertaForTokenClassification.from_pretrained(<span class="hljs-string">&quot;ydshieh/roberta-large-ner-english&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;HuggingFace is a company based in Paris and New York&quot;</span>, add_special_tokens=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_class_ids = tf.math.argmax(logits, axis=-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Note that tokens are classified rather then input words which means that</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># there might be more predicted token classes than words.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multiple token classes might account for the same word</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes = [model.config.id2label[t] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> predicted_token_class_ids[<span class="hljs-number">0</span>].numpy().tolist()]
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes
[<span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;ORG&#x27;</span>, <span class="hljs-string">&#x27;ORG&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;LOC&#x27;</span>, <span class="hljs-string">&#x27;O&#x27;</span>, <span class="hljs-string">&#x27;LOC&#x27;</span>, <span class="hljs-string">&#x27;LOC&#x27;</span>]`}}),{c(){t=l("p"),u=a("Example:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function dz(k){let t,u;return t=new he({props:{code:`labels = predicted_token_class_ids
loss = tf.math.reduce_mean(model(**inputs, labels=labels).loss)
round(float(loss), 2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = predicted_token_class_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = tf.math.reduce_mean(model(**inputs, labels=labels).loss)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(loss), <span class="hljs-number">2</span>)
<span class="hljs-number">0.01</span>`}}),{c(){y(t.$$.fragment)},l(c){C(t.$$.fragment,c)},m(c,h){T(t,c,h),u=!0},p:j,i(c){u||(F(t.$$.fragment,c),u=!0)},o(c){$(t.$$.fragment,c),u=!1},d(c){x(t,c)}}}function cz(k){let t,u,c,h,f,p,m,R,I,P,L,H,V,E,ie,U,Z,Te,G,Ee,be,oe,Me,$e,X,ze,xe,ee,qe,Fe,le,N,D,Ce,te,je,Re,Q,ke,Ae,O,Pe,ne,Le,ve,se,Ne,de,K,we,ae,Ie,ce,S,Oe,B,De,Se;return{c(){t=l("p"),u=a("TensorFlow models and layers in "),c=l("code"),h=a("transformers"),f=a(" accept two formats as input:"),p=g(),m=l("ul"),R=l("li"),I=a("having all inputs as keyword arguments (like PyTorch models), or"),P=g(),L=l("li"),H=a("having all inputs as a list, tuple or dict in the first positional argument."),V=g(),E=l("p"),ie=a(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),U=l("code"),Z=a("model.fit()"),Te=a(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),G=l("code"),Ee=a("model.fit()"),be=a(` supports! If, however, you want to use the second
format outside of Keras methods like `),oe=l("code"),Me=a("fit()"),$e=a(" and "),X=l("code"),ze=a("predict()"),xe=a(`, such as when creating your own layers or models with
the Keras `),ee=l("code"),qe=a("Functional"),Fe=a(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),le=g(),N=l("ul"),D=l("li"),Ce=a("a single Tensor with "),te=l("code"),je=a("input_ids"),Re=a(" only and nothing else: "),Q=l("code"),ke=a("model(input_ids)"),Ae=g(),O=l("li"),Pe=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),ne=l("code"),Le=a("model([input_ids, attention_mask])"),ve=a(" or "),se=l("code"),Ne=a("model([input_ids, attention_mask, token_type_ids])"),de=g(),K=l("li"),we=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),ae=l("code"),Ie=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ce=g(),S=l("p"),Oe=a(`Note that when creating models and layers with
`),B=l("a"),De=a("subclassing"),Se=a(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(w){t=d(w,"P",{});var q=s(t);u=r(q,"TensorFlow models and layers in "),c=d(q,"CODE",{});var Ke=s(c);h=r(Ke,"transformers"),Ke.forEach(o),f=r(q," accept two formats as input:"),q.forEach(o),p=_(w),m=d(w,"UL",{});var ue=s(m);R=d(ue,"LI",{});var ot=s(R);I=r(ot,"having all inputs as keyword arguments (like PyTorch models), or"),ot.forEach(o),P=_(ue),L=d(ue,"LI",{});var Ye=s(L);H=r(Ye,"having all inputs as a list, tuple or dict in the first positional argument."),Ye.forEach(o),ue.forEach(o),V=_(w),E=d(w,"P",{});var A=s(E);ie=r(A,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),U=d(A,"CODE",{});var nt=s(U);Z=r(nt,"model.fit()"),nt.forEach(o),Te=r(A,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),G=d(A,"CODE",{});var Ue=s(G);Ee=r(Ue,"model.fit()"),Ue.forEach(o),be=r(A,` supports! If, however, you want to use the second
format outside of Keras methods like `),oe=d(A,"CODE",{});var st=s(oe);Me=r(st,"fit()"),st.forEach(o),$e=r(A," and "),X=d(A,"CODE",{});var at=s(X);ze=r(at,"predict()"),at.forEach(o),xe=r(A,`, such as when creating your own layers or models with
the Keras `),ee=d(A,"CODE",{});var Qe=s(ee);qe=r(Qe,"Functional"),Qe.forEach(o),Fe=r(A,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),A.forEach(o),le=_(w),N=d(w,"UL",{});var J=s(N);D=d(J,"LI",{});var re=s(D);Ce=r(re,"a single Tensor with "),te=d(re,"CODE",{});var Xe=s(te);je=r(Xe,"input_ids"),Xe.forEach(o),Re=r(re," only and nothing else: "),Q=d(re,"CODE",{});var He=s(Q);ke=r(He,"model(input_ids)"),He.forEach(o),re.forEach(o),Ae=_(J),O=d(J,"LI",{});var Y=s(O);Pe=r(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),ne=d(Y,"CODE",{});var Je=s(ne);Le=r(Je,"model([input_ids, attention_mask])"),Je.forEach(o),ve=r(Y," or "),se=d(Y,"CODE",{});var Ze=s(se);Ne=r(Ze,"model([input_ids, attention_mask, token_type_ids])"),Ze.forEach(o),Y.forEach(o),de=_(J),K=d(J,"LI",{});var Ve=s(K);we=r(Ve,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),ae=d(Ve,"CODE",{});var Ge=s(ae);Ie=r(Ge,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Ge.forEach(o),Ve.forEach(o),J.forEach(o),ce=_(w),S=d(w,"P",{});var me=s(S);Oe=r(me,`Note that when creating models and layers with
`),B=d(me,"A",{href:!0,rel:!0});var et=s(B);De=r(et,"subclassing"),et.forEach(o),Se=r(me,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),me.forEach(o),this.h()},h(){n(B,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),n(B,"rel","nofollow")},m(w,q){b(w,t,q),e(t,u),e(t,c),e(c,h),e(t,f),b(w,p,q),b(w,m,q),e(m,R),e(R,I),e(m,P),e(m,L),e(L,H),b(w,V,q),b(w,E,q),e(E,ie),e(E,U),e(U,Z),e(E,Te),e(E,G),e(G,Ee),e(E,be),e(E,oe),e(oe,Me),e(E,$e),e(E,X),e(X,ze),e(E,xe),e(E,ee),e(ee,qe),e(E,Fe),b(w,le,q),b(w,N,q),e(N,D),e(D,Ce),e(D,te),e(te,je),e(D,Re),e(D,Q),e(Q,ke),e(N,Ae),e(N,O),e(O,Pe),e(O,ne),e(ne,Le),e(O,ve),e(O,se),e(se,Ne),e(N,de),e(N,K),e(K,we),e(K,ae),e(ae,Ie),b(w,ce,q),b(w,S,q),e(S,Oe),e(S,B),e(B,De),e(S,Se)},d(w){w&&o(t),w&&o(p),w&&o(m),w&&o(V),w&&o(E),w&&o(le),w&&o(N),w&&o(ce),w&&o(S)}}}function pz(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function hz(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer, TFRobertaForQuestionAnswering
import tensorflow as tf

tokenizer = RobertaTokenizer.from_pretrained("ydshieh/roberta-base-squad2")
model = TFRobertaForQuestionAnswering.from_pretrained("ydshieh/roberta-base-squad2")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

inputs = tokenizer(question, text, return_tensors="tf")
outputs = model(**inputs)

answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])
answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])

predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
tokenizer.decode(predict_answer_tokens)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, TFRobertaForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;ydshieh/roberta-base-squad2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFRobertaForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;ydshieh/roberta-base-squad2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>answer_start_index = <span class="hljs-built_in">int</span>(tf.math.argmax(outputs.start_logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>answer_end_index = <span class="hljs-built_in">int</span>(tf.math.argmax(outputs.end_logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>predict_answer_tokens = inputs.input_ids[<span class="hljs-number">0</span>, answer_start_index : answer_end_index + <span class="hljs-number">1</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predict_answer_tokens)
<span class="hljs-string">&#x27; puppet&#x27;</span>`}}),{c(){t=l("p"),u=a("Example:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function uz(k){let t,u;return t=new he({props:{code:`# target is "nice puppet"
target_start_index = tf.constant([14])
target_end_index = tf.constant([15])

outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
loss = tf.math.reduce_mean(outputs.loss)
round(float(loss), 2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target is &quot;nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_start_index = tf.constant([<span class="hljs-number">14</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>target_end_index = tf.constant([<span class="hljs-number">15</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = tf.math.reduce_mean(outputs.loss)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(loss), <span class="hljs-number">2</span>)
<span class="hljs-number">0.86</span>`}}),{c(){y(t.$$.fragment)},l(c){C(t.$$.fragment,c)},m(c,h){T(t,c,h),u=!0},p:j,i(c){u||(F(t.$$.fragment,c),u=!0)},o(c){$(t.$$.fragment,c),u=!1},d(c){x(t,c)}}}function mz(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function fz(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer, FlaxRobertaModel

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
model = FlaxRobertaModel.from_pretrained("roberta-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, FlaxRobertaModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxRobertaModel.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){t=l("p"),u=a("Example:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function gz(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function _z(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer, FlaxRobertaForCausalLM

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
model = FlaxRobertaForCausalLM.from_pretrained("roberta-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="np")
outputs = model(**inputs)

# retrieve logts for next token
next_token_logits = outputs.logits[:, -1]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, FlaxRobertaForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxRobertaForCausalLM.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve logts for next token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>next_token_logits = outputs.logits[:, -<span class="hljs-number">1</span>]`}}),{c(){t=l("p"),u=a("Example:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function bz(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function kz(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer, FlaxRobertaForMaskedLM

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
model = FlaxRobertaForMaskedLM.from_pretrained("roberta-base")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, FlaxRobertaForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxRobertaForMaskedLM.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){t=l("p"),u=a("Example:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function vz(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function wz(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer, FlaxRobertaForSequenceClassification

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
model = FlaxRobertaForSequenceClassification.from_pretrained("roberta-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, FlaxRobertaForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxRobertaForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){t=l("p"),u=a("Example:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function yz(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function Tz(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer, FlaxRobertaForMultipleChoice

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
model = FlaxRobertaForMultipleChoice.from_pretrained("roberta-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="jax", padding=True)
outputs = model(**{k: v[None, :] for k, v in encoding.items()})

logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, FlaxRobertaForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxRobertaForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;jax&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v[<span class="hljs-literal">None</span>, :] <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()})

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){t=l("p"),u=a("Example:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function $z(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function xz(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer, FlaxRobertaForTokenClassification

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
model = FlaxRobertaForTokenClassification.from_pretrained("roberta-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, FlaxRobertaForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxRobertaForTokenClassification.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){t=l("p"),u=a("Example:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function Fz(k){let t,u,c,h,f;return{c(){t=l("p"),u=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),c=l("code"),h=a("Module"),f=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),c=d(m,"CODE",{});var R=s(c);h=r(R,"Module"),R.forEach(o),f=r(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(o)},m(p,m){b(p,t,m),e(t,u),e(t,c),e(c,h),e(t,f)},d(p){p&&o(t)}}}function Cz(k){let t,u,c,h,f;return h=new he({props:{code:`from transformers import RobertaTokenizer, FlaxRobertaForQuestionAnswering

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
model = FlaxRobertaForQuestionAnswering.from_pretrained("roberta-base")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
inputs = tokenizer(question, text, return_tensors="jax")

outputs = model(**inputs)
start_scores = outputs.start_logits
end_scores = outputs.end_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RobertaTokenizer, FlaxRobertaForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxRobertaForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;roberta-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_scores = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_scores = outputs.end_logits`}}),{c(){t=l("p"),u=a("Example:"),c=g(),y(h.$$.fragment)},l(p){t=d(p,"P",{});var m=s(t);u=r(m,"Example:"),m.forEach(o),c=_(p),C(h.$$.fragment,p)},m(p,m){b(p,t,m),e(t,u),b(p,c,m),T(h,p,m),f=!0},p:j,i(p){f||(F(h.$$.fragment,p),f=!0)},o(p){$(h.$$.fragment,p),f=!1},d(p){p&&o(t),p&&o(c),x(h,p)}}}function Rz(k){let t,u,c,h,f,p,m,R,I,P,L,H,V,E,ie,U,Z,Te,G,Ee,be,oe,Me,$e,X,ze,xe,ee,qe,Fe,le,N,D,Ce,te,je,Re,Q,ke,Ae,O,Pe,ne,Le,ve,se,Ne,de,K,we,ae,Ie,ce,S,Oe,B,De,Se,w,q,Ke,ue,ot,Ye,A,nt,Ue,st,at,Qe,J,re,Xe,He,Y,Je,Ze,Ve,Ge,me,et,Pc,I1,sg,br,ag,Tt,pn,O1,kr,D1,S1,vr,H1,V1,B1,wr,W1,yr,U1,Q1,Z1,Tr,K1,$r,J1,G1,Y1,Mo,Lc,X1,e2,xr,t2,o2,Fr,n2,s2,a2,zo,Nc,r2,i2,Cr,l2,d2,Rr,c2,p2,h2,qo,Ic,u2,m2,Er,f2,g2,Mr,_2,b2,rg,zr,ig,Yt,jo,Oc,k2,v2,qr,w2,y2,jr,T2,$2,x2,Ao,Dc,F2,C2,Ar,R2,E2,Pr,M2,z2,q2,ps,Sc,j2,A2,Lr,P2,L2,N2,Hc,Nr,I2,O2,lg,Ir,dg,At,Or,D2,Dr,S2,H2,V2,Po,Vc,B2,W2,Sr,U2,Q2,Hr,Z2,K2,J2,Lo,Bc,G2,Y2,Vr,X2,eb,Br,tb,ob,nb,No,Wc,sb,ab,Wr,rb,ib,Ur,lb,db,cb,Uc,Qr,pb,hb,cg,Zr,pg,Pt,Kr,ub,Jr,mb,fb,gb,Io,Qc,_b,bb,Gr,kb,vb,Yr,wb,yb,Tb,Oo,Zc,$b,xb,Xr,Fb,Cb,ei,Rb,Eb,Mb,hs,Kc,zb,qb,ti,jb,Ab,Pb,Jc,oi,Lb,Nb,hg,Gc,vh,Ib,ug,us,Do,Yc,Ob,Db,ni,Sb,Hb,si,Vb,Bb,Wb,So,Xc,Ub,Qb,ai,Zb,Kb,ri,Jb,Gb,mg,hn,ms,wh,ii,Yb,yh,Xb,fg,Ft,li,e0,ko,t0,ep,o0,n0,tp,s0,a0,di,r0,i0,l0,un,d0,op,c0,p0,np,h0,u0,m0,mn,f0,sp,g0,_0,ap,b0,k0,v0,fs,gg,fn,gs,Th,ci,w0,$h,y0,_g,Be,pi,T0,xh,$0,x0,Fh,F0,C0,_s,R0,hi,E0,Ch,M0,z0,q0,bs,j0,ui,A0,rp,P0,L0,N0,Ho,mi,I0,Rh,O0,D0,fi,ip,S0,Eh,H0,V0,lp,B0,Mh,W0,U0,ks,gi,Q0,_i,Z0,zh,K0,J0,G0,vs,bi,Y0,qh,X0,ek,dp,ki,bg,gn,ws,jh,vi,tk,Ah,ok,kg,rt,wi,nk,yi,sk,Ph,ak,rk,ik,Lh,lk,dk,ys,ck,Ti,pk,Nh,hk,uk,mk,Ts,fk,$i,gk,cp,_k,bk,kk,pp,xi,vg,_n,$s,Ih,Fi,vk,Oh,wk,wg,it,Ci,yk,Dh,Tk,$k,Ri,xk,hp,Fk,Ck,Rk,Ei,Ek,Mi,Mk,zk,qk,zi,jk,Sh,Ak,Pk,Lk,gt,Nk,Hh,Ik,Ok,Vh,Dk,Sk,Bh,Hk,Vk,Wh,Bk,Wk,Uh,Uk,Qk,Qh,Zk,Kk,Jk,xs,Gk,Zh,Yk,Xk,qi,e5,t5,Xt,ji,o5,bn,n5,up,s5,a5,Kh,r5,i5,l5,Fs,d5,Cs,yg,kn,Rs,Jh,Ai,c5,Gh,p5,Tg,Ct,Pi,h5,Li,u5,Yh,m5,f5,g5,Ni,_5,mp,b5,k5,v5,Ii,w5,Oi,y5,T5,$5,eo,Di,x5,vn,F5,fp,C5,R5,Xh,E5,M5,z5,Es,q5,Ms,$g,wn,zs,eu,Si,j5,tu,A5,xg,Rt,Hi,P5,Vi,L5,ou,N5,I5,O5,Bi,D5,gp,S5,H5,V5,Wi,B5,Ui,W5,U5,Q5,Lt,Qi,Z5,yn,K5,_p,J5,G5,nu,Y5,X5,e4,qs,t4,js,o4,As,Fg,Tn,Ps,su,Zi,n4,au,s4,Cg,Et,Ki,a4,ru,r4,i4,Ji,l4,bp,d4,c4,p4,Gi,h4,Yi,u4,m4,f4,ft,Xi,g4,$n,_4,kp,b4,k4,iu,v4,w4,y4,Ls,T4,Ns,$4,Is,x4,Os,F4,Ds,Rg,xn,Ss,lu,el,C4,du,R4,Eg,Mt,tl,E4,cu,M4,z4,ol,q4,vp,j4,A4,P4,nl,L4,sl,N4,I4,O4,to,al,D4,Fn,S4,wp,H4,V4,pu,B4,W4,U4,Hs,Q4,Vs,Mg,Cn,Bs,hu,rl,Z4,uu,K4,zg,zt,il,J4,mu,G4,Y4,ll,X4,yp,ev,tv,ov,dl,nv,cl,sv,av,rv,Nt,pl,iv,Rn,lv,Tp,dv,cv,fu,pv,hv,uv,Ws,mv,Us,fv,Qs,qg,En,Zs,gu,hl,gv,_u,_v,jg,qt,ul,bv,Mn,kv,bu,vv,wv,ku,yv,Tv,$v,ml,xv,$p,Fv,Cv,Rv,fl,Ev,gl,Mv,zv,qv,It,_l,jv,zn,Av,xp,Pv,Lv,vu,Nv,Iv,Ov,Ks,Dv,Js,Sv,Gs,Ag,qn,Ys,wu,bl,Hv,yu,Vv,Pg,_t,kl,Bv,Tu,Wv,Uv,vl,Qv,Fp,Zv,Kv,Jv,wl,Gv,yl,Yv,Xv,ew,Xs,tw,oo,Tl,ow,jn,nw,Cp,sw,aw,$u,rw,iw,lw,ea,dw,ta,Lg,An,oa,xu,$l,cw,Fu,pw,Ng,Pn,xl,hw,no,Fl,uw,Ln,mw,Rp,fw,gw,Cu,_w,bw,kw,na,vw,sa,Ig,Nn,aa,Ru,Cl,ww,Eu,yw,Og,bt,Rl,Tw,El,$w,Mu,xw,Fw,Cw,Ml,Rw,Ep,Ew,Mw,zw,zl,qw,ql,jw,Aw,Pw,ra,Lw,Ot,jl,Nw,In,Iw,Mp,Ow,Dw,zu,Sw,Hw,Vw,ia,Bw,la,Ww,da,Dg,On,ca,qu,Al,Uw,ju,Qw,Sg,kt,Pl,Zw,Au,Kw,Jw,Ll,Gw,zp,Yw,Xw,e3,Nl,t3,Il,o3,n3,s3,pa,a3,Dt,Ol,r3,Dn,i3,qp,l3,d3,Pu,c3,p3,h3,ha,u3,ua,m3,ma,Hg,Sn,fa,Lu,Dl,f3,Nu,g3,Vg,vt,Sl,_3,Iu,b3,k3,Hl,v3,jp,w3,y3,T3,Vl,$3,Bl,x3,F3,C3,ga,R3,so,Wl,E3,Hn,M3,Ap,z3,q3,Ou,j3,A3,P3,_a,L3,ba,Bg,Vn,ka,Du,Ul,N3,Su,I3,Wg,wt,Ql,O3,Hu,D3,S3,Zl,H3,Pp,V3,B3,W3,Kl,U3,Jl,Q3,Z3,K3,va,J3,St,Gl,G3,Bn,Y3,Lp,X3,ey,Vu,ty,oy,ny,wa,sy,ya,ay,Ta,Ug,Wn,$a,Bu,Yl,ry,Wu,iy,Qg,yt,Xl,ly,Un,dy,Uu,cy,py,Qu,hy,uy,my,ed,fy,Np,gy,_y,by,td,ky,od,vy,wy,yy,xa,Ty,Ht,nd,$y,Qn,xy,Ip,Fy,Cy,Zu,Ry,Ey,My,Fa,zy,Ca,qy,Ra,Zg,Zn,Ea,Ku,sd,jy,Ju,Ay,Kg,lt,ad,Py,Gu,Ly,Ny,rd,Iy,Op,Oy,Dy,Sy,id,Hy,ld,Vy,By,Wy,Yu,Uy,Qy,vo,Xu,dd,Zy,Ky,em,cd,Jy,Gy,tm,pd,Yy,Xy,om,hd,e6,t6,ao,ud,o6,Kn,n6,nm,s6,a6,sm,r6,i6,l6,Ma,d6,za,Jg,Jn,qa,am,md,c6,rm,p6,Gg,dt,fd,h6,im,u6,m6,gd,f6,Dp,g6,_6,b6,_d,k6,bd,v6,w6,y6,lm,T6,$6,wo,dm,kd,x6,F6,cm,vd,C6,R6,pm,wd,E6,M6,hm,yd,z6,q6,ro,Td,j6,Gn,A6,um,P6,L6,mm,N6,I6,O6,ja,D6,Aa,Yg,Yn,Pa,fm,$d,S6,gm,H6,Xg,ct,xd,V6,Fd,B6,_m,W6,U6,Q6,Cd,Z6,Sp,K6,J6,G6,Rd,Y6,Ed,X6,e7,t7,bm,o7,n7,yo,km,Md,s7,a7,vm,zd,r7,i7,wm,qd,l7,d7,ym,jd,c7,p7,io,Ad,h7,Xn,u7,Tm,m7,f7,$m,g7,_7,b7,La,k7,Na,e_,es,Ia,xm,Pd,v7,Fm,w7,t_,pt,Ld,y7,Cm,T7,$7,Nd,x7,Hp,F7,C7,R7,Id,E7,Od,M7,z7,q7,Rm,j7,A7,To,Em,Dd,P7,L7,Mm,Sd,N7,I7,zm,Hd,O7,D7,qm,Vd,S7,H7,lo,Bd,V7,ts,B7,jm,W7,U7,Am,Q7,Z7,K7,Oa,J7,Da,o_,os,Sa,Pm,Wd,G7,Lm,Y7,n_,ht,Ud,X7,Nm,eT,tT,Qd,oT,Vp,nT,sT,aT,Zd,rT,Kd,iT,lT,dT,Im,cT,pT,$o,Om,Jd,hT,uT,Dm,Gd,mT,fT,Sm,Yd,gT,_T,Hm,Xd,bT,kT,co,ec,vT,ns,wT,Vm,yT,TT,Bm,$T,xT,FT,Ha,CT,Va,s_,ss,Ba,Wm,tc,RT,Um,ET,a_,ut,oc,MT,Qm,zT,qT,nc,jT,Bp,AT,PT,LT,sc,NT,ac,IT,OT,DT,Zm,ST,HT,xo,Km,rc,VT,BT,Jm,ic,WT,UT,Gm,lc,QT,ZT,Ym,dc,KT,JT,po,cc,GT,as,YT,Xm,XT,e8,ef,t8,o8,n8,Wa,s8,Ua,r_,rs,Qa,tf,pc,a8,of,r8,i_,mt,hc,i8,is,l8,nf,d8,c8,sf,p8,h8,u8,uc,m8,Wp,f8,g8,_8,mc,b8,fc,k8,v8,w8,af,y8,T8,Fo,rf,gc,$8,x8,lf,_c,F8,C8,df,bc,R8,E8,cf,kc,M8,z8,ho,vc,q8,ls,j8,pf,A8,P8,hf,L8,N8,I8,Za,O8,Ka,l_;return p=new We({}),E=new We({}),Ze=new We({}),br=new N1({props:{pipeline:"text-classification"}}),zr=new N1({props:{pipeline:"token-classification"}}),Ir=new N1({props:{pipeline:"fill-mask"}}),Zr=new N1({props:{pipeline:"question-answering"}}),ii=new We({}),li=new W({props:{name:"class transformers.RobertaConfig",anchor:"transformers.RobertaConfig",parameters:[{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/configuration_roberta.py#L37"}}),fs=new pe({props:{anchor:"transformers.RobertaConfig.example",$$slots:{default:[kM]},$$scope:{ctx:k}}}),ci=new We({}),pi=new W({props:{name:"class transformers.RobertaTokenizer",anchor:"transformers.RobertaTokenizer",parameters:[{name:"vocab_file",val:""},{name:"merges_file",val:""},{name:"errors",val:" = 'replace'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"add_prefix_space",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.RobertaTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.RobertaTokenizer.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"},{anchor:"transformers.RobertaTokenizer.errors",description:`<strong>errors</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;replace&quot;</code>) &#x2014;
Paradigm to follow when decoding bytes to UTF-8. See
<a href="https://docs.python.org/3/library/stdtypes.html#bytes.decode" rel="nofollow">bytes.decode</a> for more information.`,name:"errors"},{anchor:"transformers.RobertaTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.RobertaTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.RobertaTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.RobertaTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.RobertaTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.RobertaTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.RobertaTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.RobertaTokenizer.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to add an initial space to the input. This allows to treat the leading word just as any
other word. (RoBERTa tokenizer detect beginning of words by the preceding space).`,name:"add_prefix_space"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/tokenization_roberta.py#L107"}}),_s=new pe({props:{anchor:"transformers.RobertaTokenizer.example",$$slots:{default:[vM]},$$scope:{ctx:k}}}),bs=new ye({props:{$$slots:{default:[wM]},$$scope:{ctx:k}}}),mi=new W({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.RobertaTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.RobertaTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.RobertaTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/tokenization_roberta.py#L344",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),gi=new W({props:{name:"get_special_tokens_mask",anchor:"transformers.RobertaTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.RobertaTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.RobertaTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.RobertaTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/tokenization_roberta.py#L369",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),bi=new W({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.RobertaTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.RobertaTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.RobertaTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/tokenization_roberta.py#L396",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ki=new W({props:{name:"save_vocabulary",anchor:"transformers.RobertaTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/tokenization_roberta.py#L315"}}),vi=new We({}),wi=new W({props:{name:"class transformers.RobertaTokenizerFast",anchor:"transformers.RobertaTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"merges_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"errors",val:" = 'replace'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"add_prefix_space",val:" = False"},{name:"trim_offsets",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.RobertaTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.RobertaTokenizerFast.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"},{anchor:"transformers.RobertaTokenizerFast.errors",description:`<strong>errors</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;replace&quot;</code>) &#x2014;
Paradigm to follow when decoding bytes to UTF-8. See
<a href="https://docs.python.org/3/library/stdtypes.html#bytes.decode" rel="nofollow">bytes.decode</a> for more information.`,name:"errors"},{anchor:"transformers.RobertaTokenizerFast.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.RobertaTokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.RobertaTokenizerFast.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.RobertaTokenizerFast.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.RobertaTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.RobertaTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.RobertaTokenizerFast.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.RobertaTokenizerFast.add_prefix_space",description:`<strong>add_prefix_space</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to add an initial space to the input. This allows to treat the leading word just as any
other word. (RoBERTa tokenizer detect beginning of words by the preceding space).`,name:"add_prefix_space"},{anchor:"transformers.RobertaTokenizerFast.trim_offsets",description:`<strong>trim_offsets</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether the post processing step should trim offsets to avoid including whitespaces.`,name:"trim_offsets"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/tokenization_roberta_fast.py#L76"}}),ys=new pe({props:{anchor:"transformers.RobertaTokenizerFast.example",$$slots:{default:[yM]},$$scope:{ctx:k}}}),Ts=new ye({props:{$$slots:{default:[TM]},$$scope:{ctx:k}}}),xi=new W({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.RobertaTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:""},{name:"token_ids_1",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/tokenization_roberta_fast.py#L279"}}),Fi=new We({}),Ci=new W({props:{name:"class transformers.RobertaModel",anchor:"transformers.RobertaModel",parameters:[{name:"config",val:""},{name:"add_pooling_layer",val:" = True"}],parametersDescription:[{anchor:"transformers.RobertaModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L691"}}),ji=new W({props:{name:"forward",anchor:"transformers.RobertaModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[torch.Tensor] = None"},{name:"encoder_attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.List[torch.FloatTensor]] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.RobertaModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RobertaModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RobertaModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0,1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
This parameter can only be used when the model is initialized with <code>type_vocab_size</code> parameter with value<blockquote>
<p>= 2. All the value in this tensor should be always &lt; type_vocab_size.</p>
</blockquote></li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RobertaModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RobertaModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RobertaModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RobertaModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RobertaModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RobertaModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RobertaModel.forward.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong>  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
the model is configured as a decoder.`,name:"encoder_hidden_states"},{anchor:"transformers.RobertaModel.forward.encoder_attention_mask",description:`<strong>encoder_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"encoder_attention_mask"},{anchor:"transformers.RobertaModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.RobertaModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L736",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
>transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
>transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Fs=new ye({props:{$$slots:{default:[$M]},$$scope:{ctx:k}}}),Cs=new pe({props:{anchor:"transformers.RobertaModel.forward.example",$$slots:{default:[xM]},$$scope:{ctx:k}}}),Ai=new We({}),Pi=new W({props:{name:"class transformers.RobertaForCausalLM",anchor:"transformers.RobertaForCausalLM",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.RobertaForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L877"}}),Di=new W({props:{name:"forward",anchor:"transformers.RobertaForCausalLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"past_key_values",val:": typing.Tuple[typing.Tuple[torch.FloatTensor]] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.RobertaForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RobertaForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RobertaForCausalLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0,1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
This parameter can only be used when the model is initialized with <code>type_vocab_size</code> parameter with value<blockquote>
<p>= 2. All the value in this tensor should be always &lt; type_vocab_size.</p>
</blockquote></li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RobertaForCausalLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RobertaForCausalLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RobertaForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RobertaForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RobertaForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RobertaForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RobertaForCausalLM.forward.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong>  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
the model is configured as a decoder.`,name:"encoder_hidden_states"},{anchor:"transformers.RobertaForCausalLM.forward.encoder_attention_mask",description:`<strong>encoder_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"encoder_attention_mask"},{anchor:"transformers.RobertaForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in
<code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are
ignored (masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.RobertaForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.RobertaForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L903",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
>transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Cross attentions weights after the attention softmax, used to compute the weighted average in the
cross-attention heads.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> tuples of length <code>config.n_layers</code>, with each tuple containing the cached key,
value states of the self-attention and the cross-attention layers if model is used in encoder-decoder
setting. Only relevant if <code>config.is_decoder = True</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
>transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Es=new ye({props:{$$slots:{default:[FM]},$$scope:{ctx:k}}}),Ms=new pe({props:{anchor:"transformers.RobertaForCausalLM.forward.example",$$slots:{default:[CM]},$$scope:{ctx:k}}}),Si=new We({}),Hi=new W({props:{name:"class transformers.RobertaForMaskedLM",anchor:"transformers.RobertaForMaskedLM",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.RobertaForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1029"}}),Qi=new W({props:{name:"forward",anchor:"transformers.RobertaForMaskedLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.RobertaForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RobertaForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RobertaForMaskedLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0,1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
This parameter can only be used when the model is initialized with <code>type_vocab_size</code> parameter with value<blockquote>
<p>= 2. All the value in this tensor should be always &lt; type_vocab_size.</p>
</blockquote></li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RobertaForMaskedLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RobertaForMaskedLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RobertaForMaskedLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RobertaForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RobertaForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RobertaForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RobertaForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.RobertaForMaskedLM.forward.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, any]</code>, optional, defaults to <em>{}</em>) &#x2014;
Used to hide legacy arguments that have been deprecated.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1058",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),qs=new ye({props:{$$slots:{default:[RM]},$$scope:{ctx:k}}}),js=new pe({props:{anchor:"transformers.RobertaForMaskedLM.forward.example",$$slots:{default:[EM]},$$scope:{ctx:k}}}),As=new pe({props:{anchor:"transformers.RobertaForMaskedLM.forward.example-2",$$slots:{default:[MM]},$$scope:{ctx:k}}}),Zi=new We({}),Ki=new W({props:{name:"class transformers.RobertaForSequenceClassification",anchor:"transformers.RobertaForSequenceClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.RobertaForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1164"}}),Xi=new W({props:{name:"forward",anchor:"transformers.RobertaForSequenceClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.RobertaForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RobertaForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RobertaForSequenceClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0,1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
This parameter can only be used when the model is initialized with <code>type_vocab_size</code> parameter with value<blockquote>
<p>= 2. All the value in this tensor should be always &lt; type_vocab_size.</p>
</blockquote></li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RobertaForSequenceClassification.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RobertaForSequenceClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RobertaForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RobertaForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RobertaForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RobertaForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RobertaForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1178",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ls=new ye({props:{$$slots:{default:[zM]},$$scope:{ctx:k}}}),Ns=new pe({props:{anchor:"transformers.RobertaForSequenceClassification.forward.example",$$slots:{default:[qM]},$$scope:{ctx:k}}}),Is=new pe({props:{anchor:"transformers.RobertaForSequenceClassification.forward.example-2",$$slots:{default:[jM]},$$scope:{ctx:k}}}),Os=new pe({props:{anchor:"transformers.RobertaForSequenceClassification.forward.example-3",$$slots:{default:[AM]},$$scope:{ctx:k}}}),Ds=new pe({props:{anchor:"transformers.RobertaForSequenceClassification.forward.example-4",$$slots:{default:[PM]},$$scope:{ctx:k}}}),el=new We({}),tl=new W({props:{name:"class transformers.RobertaForMultipleChoice",anchor:"transformers.RobertaForMultipleChoice",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.RobertaForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1264"}}),al=new W({props:{name:"forward",anchor:"transformers.RobertaForMultipleChoice.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.RobertaForMultipleChoice.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RobertaForMultipleChoice.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RobertaForMultipleChoice.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0,1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
This parameter can only be used when the model is initialized with <code>type_vocab_size</code> parameter with value<blockquote>
<p>= 2. All the value in this tensor should be always &lt; type_vocab_size.</p>
</blockquote></li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RobertaForMultipleChoice.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RobertaForMultipleChoice.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RobertaForMultipleChoice.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RobertaForMultipleChoice.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RobertaForMultipleChoice.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RobertaForMultipleChoice.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RobertaForMultipleChoice.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices-1]</code> where <code>num_choices</code> is the size of the second dimension of the input tensors. (See
<code>input_ids</code> above)`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1277",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Hs=new ye({props:{$$slots:{default:[LM]},$$scope:{ctx:k}}}),Vs=new pe({props:{anchor:"transformers.RobertaForMultipleChoice.forward.example",$$slots:{default:[NM]},$$scope:{ctx:k}}}),rl=new We({}),il=new W({props:{name:"class transformers.RobertaForTokenClassification",anchor:"transformers.RobertaForTokenClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.RobertaForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1357"}}),pl=new W({props:{name:"forward",anchor:"transformers.RobertaForTokenClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.RobertaForTokenClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RobertaForTokenClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RobertaForTokenClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0,1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
This parameter can only be used when the model is initialized with <code>type_vocab_size</code> parameter with value<blockquote>
<p>= 2. All the value in this tensor should be always &lt; type_vocab_size.</p>
</blockquote></li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RobertaForTokenClassification.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RobertaForTokenClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RobertaForTokenClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RobertaForTokenClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RobertaForTokenClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RobertaForTokenClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RobertaForTokenClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1375",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ws=new ye({props:{$$slots:{default:[IM]},$$scope:{ctx:k}}}),Us=new pe({props:{anchor:"transformers.RobertaForTokenClassification.forward.example",$$slots:{default:[OM]},$$scope:{ctx:k}}}),Qs=new pe({props:{anchor:"transformers.RobertaForTokenClassification.forward.example-2",$$slots:{default:[DM]},$$scope:{ctx:k}}}),hl=new We({}),ul=new W({props:{name:"class transformers.RobertaForQuestionAnswering",anchor:"transformers.RobertaForQuestionAnswering",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.RobertaForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1466"}}),_l=new W({props:{name:"forward",anchor:"transformers.RobertaForQuestionAnswering.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"position_ids",val:": typing.Optional[torch.LongTensor] = None"},{name:"head_mask",val:": typing.Optional[torch.FloatTensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.FloatTensor] = None"},{name:"start_positions",val:": typing.Optional[torch.LongTensor] = None"},{name:"end_positions",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.RobertaForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RobertaForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RobertaForQuestionAnswering.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0,1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
This parameter can only be used when the model is initialized with <code>type_vocab_size</code> parameter with value<blockquote>
<p>= 2. All the value in this tensor should be always &lt; type_vocab_size.</p>
</blockquote></li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RobertaForQuestionAnswering.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RobertaForQuestionAnswering.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RobertaForQuestionAnswering.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RobertaForQuestionAnswering.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RobertaForQuestionAnswering.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RobertaForQuestionAnswering.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RobertaForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.RobertaForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_roberta.py#L1480",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ks=new ye({props:{$$slots:{default:[SM]},$$scope:{ctx:k}}}),Js=new pe({props:{anchor:"transformers.RobertaForQuestionAnswering.forward.example",$$slots:{default:[HM]},$$scope:{ctx:k}}}),Gs=new pe({props:{anchor:"transformers.RobertaForQuestionAnswering.forward.example-2",$$slots:{default:[VM]},$$scope:{ctx:k}}}),bl=new We({}),kl=new W({props:{name:"class transformers.TFRobertaModel",anchor:"transformers.TFRobertaModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFRobertaModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_tf_roberta.py#L924"}}),Xs=new ye({props:{$$slots:{default:[BM]},$$scope:{ctx:k}}}),Tl=new W({props:{name:"call",anchor:"transformers.TFRobertaModel.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"position_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"encoder_hidden_states",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"encoder_attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"past_key_values",val:": typing.Union[typing.Tuple[typing.Tuple[typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor]]], NoneType] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"training",val:": typing.Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFRobertaModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFRobertaModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFRobertaModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFRobertaModel.call.position_ids",description:`<strong>position_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.TFRobertaModel.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFRobertaModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFRobertaModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFRobertaModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFRobertaModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFRobertaModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFRobertaModel.call.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong>  (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
the model is configured as a decoder.`,name:"encoder_hidden_states"},{anchor:"transformers.TFRobertaModel.call.encoder_attention_mask",description:`<strong>encoder_attention_mask</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"encoder_attention_mask"},{anchor:"transformers.TFRobertaModel.call.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple[Tuple[tf.Tensor]]</code> of length <code>config.n_layers</code>) &#x2014;
contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.
If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.TFRobertaModel.call.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>). Set to <code>False</code> during training, <code>True</code> during generation`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_tf_roberta.py#L929",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions"
>transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) further processed by a
Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
prediction (classification) objective during pretraining.</p>
<p>This output is usually <em>not</em> a good summary of the semantic content of the input, you\u2019re often better with
averaging or pooling the sequence of hidden-states for the whole input sequence.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions"
>transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions</a> or <code>tuple(tf.Tensor)</code></p>
`}}),ea=new ye({props:{$$slots:{default:[WM]},$$scope:{ctx:k}}}),ta=new pe({props:{anchor:"transformers.TFRobertaModel.call.example",$$slots:{default:[UM]},$$scope:{ctx:k}}}),$l=new We({}),xl=new W({props:{name:"class transformers.TFRobertaForCausalLM",anchor:"transformers.TFRobertaForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_tf_roberta.py#L1153"}}),Fl=new W({props:{name:"call",anchor:"transformers.TFRobertaForCausalLM.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"position_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"encoder_hidden_states",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"encoder_attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"past_key_values",val:": typing.Union[typing.Tuple[typing.Tuple[typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor]]], NoneType] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFRobertaForCausalLM.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFRobertaForCausalLM.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFRobertaForCausalLM.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFRobertaForCausalLM.call.position_ids",description:`<strong>position_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.TFRobertaForCausalLM.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFRobertaForCausalLM.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFRobertaForCausalLM.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFRobertaForCausalLM.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFRobertaForCausalLM.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFRobertaForCausalLM.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFRobertaForCausalLM.call.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong>  (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
the model is configured as a decoder.`,name:"encoder_hidden_states"},{anchor:"transformers.TFRobertaForCausalLM.call.encoder_attention_mask",description:`<strong>encoder_attention_mask</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"encoder_attention_mask"},{anchor:"transformers.TFRobertaForCausalLM.call.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple[Tuple[tf.Tensor]]</code> of length <code>config.n_layers</code>) &#x2014;
contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.
If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.TFRobertaForCausalLM.call.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>). Set to <code>False</code> during training, <code>True</code> during generation`,name:"use_cache"},{anchor:"transformers.TFRobertaForCausalLM.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> or <code>np.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the cross entropy classification loss. Indices should be in <code>[0, ..., config.vocab_size - 1]</code>.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_tf_roberta.py#L1186",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions"
>transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions"
>transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions</a> or <code>tuple(tf.Tensor)</code></p>
`}}),na=new ye({props:{$$slots:{default:[QM]},$$scope:{ctx:k}}}),sa=new pe({props:{anchor:"transformers.TFRobertaForCausalLM.call.example",$$slots:{default:[ZM]},$$scope:{ctx:k}}}),Cl=new We({}),Rl=new W({props:{name:"class transformers.TFRobertaForMaskedLM",anchor:"transformers.TFRobertaForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFRobertaForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_tf_roberta.py#L1068"}}),ra=new ye({props:{$$slots:{default:[KM]},$$scope:{ctx:k}}}),jl=new W({props:{name:"call",anchor:"transformers.TFRobertaForMaskedLM.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"position_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFRobertaForMaskedLM.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFRobertaForMaskedLM.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFRobertaForMaskedLM.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFRobertaForMaskedLM.call.position_ids",description:`<strong>position_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.TFRobertaForMaskedLM.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFRobertaForMaskedLM.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFRobertaForMaskedLM.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFRobertaForMaskedLM.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFRobertaForMaskedLM.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFRobertaForMaskedLM.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFRobertaForMaskedLM.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_tf_roberta.py#L1085",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),ia=new ye({props:{$$slots:{default:[JM]},$$scope:{ctx:k}}}),la=new pe({props:{anchor:"transformers.TFRobertaForMaskedLM.call.example",$$slots:{default:[GM]},$$scope:{ctx:k}}}),da=new pe({props:{anchor:"transformers.TFRobertaForMaskedLM.call.example-2",$$slots:{default:[YM]},$$scope:{ctx:k}}}),Al=new We({}),Pl=new W({props:{name:"class transformers.TFRobertaForSequenceClassification",anchor:"transformers.TFRobertaForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFRobertaForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_tf_roberta.py#L1333"}}),pa=new ye({props:{$$slots:{default:[XM]},$$scope:{ctx:k}}}),Ol=new W({props:{name:"call",anchor:"transformers.TFRobertaForSequenceClassification.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"position_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFRobertaForSequenceClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFRobertaForSequenceClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFRobertaForSequenceClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFRobertaForSequenceClassification.call.position_ids",description:`<strong>position_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.TFRobertaForSequenceClassification.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFRobertaForSequenceClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFRobertaForSequenceClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFRobertaForSequenceClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFRobertaForSequenceClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFRobertaForSequenceClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFRobertaForSequenceClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_tf_roberta.py#L1344",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),ha=new ye({props:{$$slots:{default:[ez]},$$scope:{ctx:k}}}),ua=new pe({props:{anchor:"transformers.TFRobertaForSequenceClassification.call.example",$$slots:{default:[tz]},$$scope:{ctx:k}}}),ma=new pe({props:{anchor:"transformers.TFRobertaForSequenceClassification.call.example-2",$$slots:{default:[oz]},$$scope:{ctx:k}}}),Dl=new We({}),Sl=new W({props:{name:"class transformers.TFRobertaForMultipleChoice",anchor:"transformers.TFRobertaForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFRobertaForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_tf_roberta.py#L1417"}}),ga=new ye({props:{$$slots:{default:[nz]},$$scope:{ctx:k}}}),Wl=new W({props:{name:"call",anchor:"transformers.TFRobertaForMultipleChoice.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"position_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFRobertaForMultipleChoice.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFRobertaForMultipleChoice.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFRobertaForMultipleChoice.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFRobertaForMultipleChoice.call.position_ids",description:`<strong>position_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.TFRobertaForMultipleChoice.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFRobertaForMultipleChoice.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFRobertaForMultipleChoice.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFRobertaForMultipleChoice.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFRobertaForMultipleChoice.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFRobertaForMultipleChoice.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFRobertaForMultipleChoice.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices]</code>
where <code>num_choices</code> is the size of the second dimension of the input tensors. (See <code>input_ids</code> above)`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_tf_roberta.py#L1441",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <em>(batch_size, )</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),_a=new ye({props:{$$slots:{default:[sz]},$$scope:{ctx:k}}}),ba=new pe({props:{anchor:"transformers.TFRobertaForMultipleChoice.call.example",$$slots:{default:[az]},$$scope:{ctx:k}}}),Ul=new We({}),Ql=new W({props:{name:"class transformers.TFRobertaForTokenClassification",anchor:"transformers.TFRobertaForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFRobertaForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_tf_roberta.py#L1538"}}),va=new ye({props:{$$slots:{default:[rz]},$$scope:{ctx:k}}}),Gl=new W({props:{name:"call",anchor:"transformers.TFRobertaForTokenClassification.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"position_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFRobertaForTokenClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFRobertaForTokenClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFRobertaForTokenClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFRobertaForTokenClassification.call.position_ids",description:`<strong>position_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.TFRobertaForTokenClassification.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFRobertaForTokenClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFRobertaForTokenClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFRobertaForTokenClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFRobertaForTokenClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFRobertaForTokenClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFRobertaForTokenClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_tf_roberta.py#L1556",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of unmasked labels, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),wa=new ye({props:{$$slots:{default:[iz]},$$scope:{ctx:k}}}),ya=new pe({props:{anchor:"transformers.TFRobertaForTokenClassification.call.example",$$slots:{default:[lz]},$$scope:{ctx:k}}}),Ta=new pe({props:{anchor:"transformers.TFRobertaForTokenClassification.call.example-2",$$slots:{default:[dz]},$$scope:{ctx:k}}}),Yl=new We({}),Xl=new W({props:{name:"class transformers.TFRobertaForQuestionAnswering",anchor:"transformers.TFRobertaForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFRobertaForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_tf_roberta.py#L1629"}}),xa=new ye({props:{$$slots:{default:[cz]},$$scope:{ctx:k}}}),nd=new W({props:{name:"call",anchor:"transformers.TFRobertaForQuestionAnswering.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"position_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"start_positions",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"end_positions",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFRobertaForQuestionAnswering.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFRobertaForQuestionAnswering.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFRobertaForQuestionAnswering.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFRobertaForQuestionAnswering.call.position_ids",description:`<strong>position_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.TFRobertaForQuestionAnswering.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFRobertaForQuestionAnswering.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFRobertaForQuestionAnswering.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFRobertaForQuestionAnswering.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFRobertaForQuestionAnswering.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFRobertaForQuestionAnswering.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFRobertaForQuestionAnswering.call.start_positions",description:`<strong>start_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.TFRobertaForQuestionAnswering.call.end_positions",description:`<strong>end_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_tf_roberta.py#L1642",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>start_positions</code> and <code>end_positions</code> are provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Fa=new ye({props:{$$slots:{default:[pz]},$$scope:{ctx:k}}}),Ca=new pe({props:{anchor:"transformers.TFRobertaForQuestionAnswering.call.example",$$slots:{default:[hz]},$$scope:{ctx:k}}}),Ra=new pe({props:{anchor:"transformers.TFRobertaForQuestionAnswering.call.example-2",$$slots:{default:[uz]},$$scope:{ctx:k}}}),sd=new We({}),ad=new W({props:{name:"class transformers.FlaxRobertaModel",anchor:"transformers.FlaxRobertaModel",parameters:[{name:"config",val:": RobertaConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"gradient_checkpointing",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxRobertaModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_flax_roberta.py#L989"}}),ud=new W({props:{name:"__call__",anchor:"transformers.FlaxRobertaModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"encoder_hidden_states",val:" = None"},{name:"encoder_attention_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"past_key_values",val:": dict = None"}],parametersDescription:[{anchor:"transformers.FlaxRobertaModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxRobertaModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxRobertaModel.__call__.token_type_ids",description:`<strong>token_type_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FlaxRobertaModel.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.FlaxRobertaModel.__call__.head_mask",description:`<strong>head_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <code>optional) -- Mask to nullify selected heads of the attention modules. Mask values selected in </code>[0, 1]\`:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlaxRobertaModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_flax_roberta.py#L815",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling"
>transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) further processed by a
Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling"
>transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ma=new ye({props:{$$slots:{default:[mz]},$$scope:{ctx:k}}}),za=new pe({props:{anchor:"transformers.FlaxRobertaModel.__call__.example",$$slots:{default:[fz]},$$scope:{ctx:k}}}),md=new We({}),fd=new W({props:{name:"class transformers.FlaxRobertaForCausalLM",anchor:"transformers.FlaxRobertaForCausalLM",parameters:[{name:"config",val:": RobertaConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"gradient_checkpointing",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxRobertaForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_flax_roberta.py#L1458"}}),Td=new W({props:{name:"__call__",anchor:"transformers.FlaxRobertaForCausalLM.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"encoder_hidden_states",val:" = None"},{name:"encoder_attention_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"past_key_values",val:": dict = None"}],parametersDescription:[{anchor:"transformers.FlaxRobertaForCausalLM.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxRobertaForCausalLM.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxRobertaForCausalLM.__call__.token_type_ids",description:`<strong>token_type_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FlaxRobertaForCausalLM.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.FlaxRobertaForCausalLM.__call__.head_mask",description:`<strong>head_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <code>optional) -- Mask to nullify selected heads of the attention modules. Mask values selected in </code>[0, 1]\`:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlaxRobertaForCausalLM.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_flax_roberta.py#L815",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions"
>transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Cross attentions weights after the attention softmax, used to compute the weighted average in the
cross-attention heads.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> tuples of length <code>config.n_layers</code>, with each tuple containing the cached key, value
states of the self-attention and the cross-attention layers if model is used in encoder-decoder setting.
Only relevant if <code>config.is_decoder = True</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions"
>transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ja=new ye({props:{$$slots:{default:[gz]},$$scope:{ctx:k}}}),Aa=new pe({props:{anchor:"transformers.FlaxRobertaForCausalLM.__call__.example",$$slots:{default:[_z]},$$scope:{ctx:k}}}),$d=new We({}),xd=new W({props:{name:"class transformers.FlaxRobertaForMaskedLM",anchor:"transformers.FlaxRobertaForMaskedLM",parameters:[{name:"config",val:": RobertaConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"gradient_checkpointing",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxRobertaForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_flax_roberta.py#L1057"}}),Ad=new W({props:{name:"__call__",anchor:"transformers.FlaxRobertaForMaskedLM.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"encoder_hidden_states",val:" = None"},{name:"encoder_attention_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"past_key_values",val:": dict = None"}],parametersDescription:[{anchor:"transformers.FlaxRobertaForMaskedLM.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxRobertaForMaskedLM.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxRobertaForMaskedLM.__call__.token_type_ids",description:`<strong>token_type_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FlaxRobertaForMaskedLM.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.FlaxRobertaForMaskedLM.__call__.head_mask",description:`<strong>head_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <code>optional) -- Mask to nullify selected heads of the attention modules. Mask values selected in </code>[0, 1]\`:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlaxRobertaForMaskedLM.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_flax_roberta.py#L815",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling"
>transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, hidden_size)</code>) \u2014 Last layer hidden-state of the first token of the sequence (classification token) further processed by a
Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling"
>transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),La=new ye({props:{$$slots:{default:[bz]},$$scope:{ctx:k}}}),Na=new pe({props:{anchor:"transformers.FlaxRobertaForMaskedLM.__call__.example",$$slots:{default:[kz]},$$scope:{ctx:k}}}),Pd=new We({}),Ld=new W({props:{name:"class transformers.FlaxRobertaForSequenceClassification",anchor:"transformers.FlaxRobertaForSequenceClassification",parameters:[{name:"config",val:": RobertaConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"gradient_checkpointing",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxRobertaForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_flax_roberta.py#L1130"}}),Bd=new W({props:{name:"__call__",anchor:"transformers.FlaxRobertaForSequenceClassification.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"encoder_hidden_states",val:" = None"},{name:"encoder_attention_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"past_key_values",val:": dict = None"}],parametersDescription:[{anchor:"transformers.FlaxRobertaForSequenceClassification.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxRobertaForSequenceClassification.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxRobertaForSequenceClassification.__call__.token_type_ids",description:`<strong>token_type_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FlaxRobertaForSequenceClassification.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.FlaxRobertaForSequenceClassification.__call__.head_mask",description:`<strong>head_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <code>optional) -- Mask to nullify selected heads of the attention modules. Mask values selected in </code>[0, 1]\`:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlaxRobertaForSequenceClassification.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_flax_roberta.py#L815",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"
>transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"
>transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Oa=new ye({props:{$$slots:{default:[vz]},$$scope:{ctx:k}}}),Da=new pe({props:{anchor:"transformers.FlaxRobertaForSequenceClassification.__call__.example",$$slots:{default:[wz]},$$scope:{ctx:k}}}),Wd=new We({}),Ud=new W({props:{name:"class transformers.FlaxRobertaForMultipleChoice",anchor:"transformers.FlaxRobertaForMultipleChoice",parameters:[{name:"config",val:": RobertaConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"gradient_checkpointing",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxRobertaForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_flax_roberta.py#L1212"}}),ec=new W({props:{name:"__call__",anchor:"transformers.FlaxRobertaForMultipleChoice.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"encoder_hidden_states",val:" = None"},{name:"encoder_attention_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"past_key_values",val:": dict = None"}],parametersDescription:[{anchor:"transformers.FlaxRobertaForMultipleChoice.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxRobertaForMultipleChoice.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxRobertaForMultipleChoice.__call__.token_type_ids",description:`<strong>token_type_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FlaxRobertaForMultipleChoice.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.FlaxRobertaForMultipleChoice.__call__.head_mask",description:`<strong>head_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <code>optional) -- Mask to nullify selected heads of the attention modules. Mask values selected in </code>[0, 1]\`:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlaxRobertaForMultipleChoice.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_flax_roberta.py#L815",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput"
>transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput"
>transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ha=new ye({props:{$$slots:{default:[yz]},$$scope:{ctx:k}}}),Va=new pe({props:{anchor:"transformers.FlaxRobertaForMultipleChoice.__call__.example",$$slots:{default:[Tz]},$$scope:{ctx:k}}}),tc=new We({}),oc=new W({props:{name:"class transformers.FlaxRobertaForTokenClassification",anchor:"transformers.FlaxRobertaForTokenClassification",parameters:[{name:"config",val:": RobertaConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"gradient_checkpointing",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxRobertaForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_flax_roberta.py#L1295"}}),cc=new W({props:{name:"__call__",anchor:"transformers.FlaxRobertaForTokenClassification.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"encoder_hidden_states",val:" = None"},{name:"encoder_attention_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"past_key_values",val:": dict = None"}],parametersDescription:[{anchor:"transformers.FlaxRobertaForTokenClassification.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxRobertaForTokenClassification.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxRobertaForTokenClassification.__call__.token_type_ids",description:`<strong>token_type_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FlaxRobertaForTokenClassification.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.FlaxRobertaForTokenClassification.__call__.head_mask",description:`<strong>head_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <code>optional) -- Mask to nullify selected heads of the attention modules. Mask values selected in </code>[0, 1]\`:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlaxRobertaForTokenClassification.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_flax_roberta.py#L815",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxTokenClassifierOutput"
>transformers.modeling_flax_outputs.FlaxTokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxTokenClassifierOutput"
>transformers.modeling_flax_outputs.FlaxTokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Wa=new ye({props:{$$slots:{default:[$z]},$$scope:{ctx:k}}}),Ua=new pe({props:{anchor:"transformers.FlaxRobertaForTokenClassification.__call__.example",$$slots:{default:[xz]},$$scope:{ctx:k}}}),pc=new We({}),hc=new W({props:{name:"class transformers.FlaxRobertaForQuestionAnswering",anchor:"transformers.FlaxRobertaForQuestionAnswering",parameters:[{name:"config",val:": RobertaConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"gradient_checkpointing",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxRobertaForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_flax_roberta.py#L1373"}}),vc=new W({props:{name:"__call__",anchor:"transformers.FlaxRobertaForQuestionAnswering.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"encoder_hidden_states",val:" = None"},{name:"encoder_attention_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"past_key_values",val:": dict = None"}],parametersDescription:[{anchor:"transformers.FlaxRobertaForQuestionAnswering.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxRobertaForQuestionAnswering.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxRobertaForQuestionAnswering.__call__.token_type_ids",description:`<strong>token_type_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FlaxRobertaForQuestionAnswering.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.FlaxRobertaForQuestionAnswering.__call__.head_mask",description:`<strong>head_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <code>optional) -- Mask to nullify selected heads of the attention modules. Mask values selected in </code>[0, 1]\`:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlaxRobertaForQuestionAnswering.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/modeling_flax_roberta.py#L815",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput"
>transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"
>RobertaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput"
>transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Za=new ye({props:{$$slots:{default:[Fz]},$$scope:{ctx:k}}}),Ka=new pe({props:{anchor:"transformers.FlaxRobertaForQuestionAnswering.__call__.example",$$slots:{default:[Cz]},$$scope:{ctx:k}}}),{c(){t=l("meta"),u=g(),c=l("h1"),h=l("a"),f=l("span"),y(p.$$.fragment),m=g(),R=l("span"),I=a("RoBERTa"),P=g(),L=l("h2"),H=l("a"),V=l("span"),y(E.$$.fragment),ie=g(),U=l("span"),Z=a("Overview"),Te=g(),G=l("p"),Ee=a("The RoBERTa model was proposed in "),be=l("a"),oe=a("RoBERTa: A Robustly Optimized BERT Pretraining Approach"),Me=a(` by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. It is based on Google\u2019s BERT model released in 2018.`),$e=g(),X=l("p"),ze=a(`It builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with
much larger mini-batches and learning rates.`),xe=g(),ee=l("p"),qe=a("The abstract from the paper is the following:"),Fe=g(),le=l("p"),N=l("em"),D=a(`Language model pretraining has led to significant performance gains but careful comparison between different
approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes,
and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication
study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and
training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every
model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results
highlight the importance of previously overlooked design choices, and raise questions about the source of recently
reported improvements. We release our models and code.`),Ce=g(),te=l("p"),je=a("Tips:"),Re=g(),Q=l("ul"),ke=l("li"),Ae=a("This implementation is the same as "),O=l("a"),Pe=a("BertModel"),ne=a(` with a tiny embeddings tweak as well as a setup
for Roberta pretrained models.`),Le=g(),ve=l("li"),se=a(`RoBERTa has the same architecture as BERT, but uses a byte-level BPE as a tokenizer (same as GPT-2) and uses a
different pretraining scheme.`),Ne=g(),de=l("li"),K=a("RoBERTa doesn\u2019t have "),we=l("code"),ae=a("token_type_ids"),Ie=a(`, you don\u2019t need to indicate which token belongs to which segment. Just
separate your segments with the separation token `),ce=l("code"),S=a("tokenizer.sep_token"),Oe=a(" (or "),B=l("code"),De=a("</s>"),Se=a(")"),w=g(),q=l("li"),Ke=l("a"),ue=a("CamemBERT"),ot=a(" is a wrapper around RoBERTa. Refer to this page for usage examples."),Ye=g(),A=l("p"),nt=a("This model was contributed by "),Ue=l("a"),st=a("julien-c"),at=a(". The original code can be found "),Qe=l("a"),J=a("here"),re=a("."),Xe=g(),He=l("h2"),Y=l("a"),Je=l("span"),y(Ze.$$.fragment),Ve=g(),Ge=l("span"),me=a("Resources"),et=g(),Pc=l("p"),I1=a("A list of official Hugging Face and community (indicated by \u{1F30E}) resources to help you get started with RoBERTa. If you\u2019re interested in submitting a resource to be included here, please feel free to open a Pull Request and we\u2019ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource."),sg=g(),y(br.$$.fragment),ag=g(),Tt=l("ul"),pn=l("li"),O1=a("A blog on "),kr=l("a"),D1=a("Getting Started with Sentiment Analysis on Twitter"),S1=a(" using RoBERTa and the "),vr=l("a"),H1=a("Inference API"),V1=a("."),B1=g(),wr=l("li"),W1=a("A blog on "),yr=l("a"),U1=a("Opinion Classification with Kili and Hugging Face AutoTrain"),Q1=a(" using RoBERTa."),Z1=g(),Tr=l("li"),K1=a("A notebook on how to "),$r=l("a"),J1=a("finetune RoBERTa for sentiment analysis"),G1=a(". \u{1F30E}"),Y1=g(),Mo=l("li"),Lc=l("a"),X1=a("RobertaForSequenceClassification"),e2=a(" is supported by this "),xr=l("a"),t2=a("example script"),o2=a(" and "),Fr=l("a"),n2=a("notebook"),s2=a("."),a2=g(),zo=l("li"),Nc=l("a"),r2=a("TFRobertaForSequenceClassification"),i2=a(" is supported by this "),Cr=l("a"),l2=a("example script"),d2=a(" and "),Rr=l("a"),c2=a("notebook"),p2=a("."),h2=g(),qo=l("li"),Ic=l("a"),u2=a("FlaxRobertaForSequenceClassification"),m2=a(" is supported by this "),Er=l("a"),f2=a("example script"),g2=a(" and "),Mr=l("a"),_2=a("notebook"),b2=a("."),rg=g(),y(zr.$$.fragment),ig=g(),Yt=l("ul"),jo=l("li"),Oc=l("a"),k2=a("RobertaForTokenClassification"),v2=a(" is supported by this "),qr=l("a"),w2=a("example script"),y2=a(" and "),jr=l("a"),T2=a("notebook"),$2=a("."),x2=g(),Ao=l("li"),Dc=l("a"),F2=a("TFRobertaForTokenClassification"),C2=a(" is supported by this "),Ar=l("a"),R2=a("example script"),E2=a(" and "),Pr=l("a"),M2=a("notebook"),z2=a("."),q2=g(),ps=l("li"),Sc=l("a"),j2=a("FlaxRobertaForTokenClassification"),A2=a(" is supported by this "),Lr=l("a"),P2=a("example script"),L2=a("."),N2=g(),Hc=l("li"),Nr=l("a"),I2=a("Token classification"),O2=a(" chapter of the \u{1F917} Hugging Face Course."),lg=g(),y(Ir.$$.fragment),dg=g(),At=l("ul"),Or=l("li"),D2=a("A blog on "),Dr=l("a"),S2=a("How to train a new language model from scratch using Transformers and Tokenizers"),H2=a(" with RoBERTa."),V2=g(),Po=l("li"),Vc=l("a"),B2=a("RobertaForMaskedLM"),W2=a(" is supported by this "),Sr=l("a"),U2=a("example script"),Q2=a(" and "),Hr=l("a"),Z2=a("notebook"),K2=a("."),J2=g(),Lo=l("li"),Bc=l("a"),G2=a("TFRobertaForMaskedLM"),Y2=a(" is supported by this "),Vr=l("a"),X2=a("example script"),eb=a(" and "),Br=l("a"),tb=a("notebook"),ob=a("."),nb=g(),No=l("li"),Wc=l("a"),sb=a("FlaxRobertaForMaskedLM"),ab=a(" is supported by this "),Wr=l("a"),rb=a("example script"),ib=a(" and "),Ur=l("a"),lb=a("notebook"),db=a("."),cb=g(),Uc=l("li"),Qr=l("a"),pb=a("Masked language modeling"),hb=a(" chapter of the \u{1F917} Hugging Face Course."),cg=g(),y(Zr.$$.fragment),pg=g(),Pt=l("ul"),Kr=l("li"),ub=a("A blog on "),Jr=l("a"),mb=a("Accelerated Inference with Optimum and Transformers Pipelines"),fb=a(" with RoBERTa for question answering."),gb=g(),Io=l("li"),Qc=l("a"),_b=a("RobertaForQuestionAnswering"),bb=a(" is supported by this "),Gr=l("a"),kb=a("example script"),vb=a(" and "),Yr=l("a"),wb=a("notebook"),yb=a("."),Tb=g(),Oo=l("li"),Zc=l("a"),$b=a("TFRobertaForQuestionAnswering"),xb=a(" is supported by this "),Xr=l("a"),Fb=a("example script"),Cb=a(" and "),ei=l("a"),Rb=a("notebook"),Eb=a("."),Mb=g(),hs=l("li"),Kc=l("a"),zb=a("FlaxRobertaForQuestionAnswering"),qb=a(" is supported by this "),ti=l("a"),jb=a("example script"),Ab=a("."),Pb=g(),Jc=l("li"),oi=l("a"),Lb=a("Question answering"),Nb=a(" chapter of the \u{1F917} Hugging Face Course."),hg=g(),Gc=l("p"),vh=l("strong"),Ib=a("Multiple choice"),ug=g(),us=l("ul"),Do=l("li"),Yc=l("a"),Ob=a("RobertaForMultipleChoice"),Db=a(" is supported by this "),ni=l("a"),Sb=a("example script"),Hb=a(" and "),si=l("a"),Vb=a("notebook"),Bb=a("."),Wb=g(),So=l("li"),Xc=l("a"),Ub=a("TFRobertaForMultipleChoice"),Qb=a(" is supported by this "),ai=l("a"),Zb=a("example script"),Kb=a(" and "),ri=l("a"),Jb=a("notebook"),Gb=a("."),mg=g(),hn=l("h2"),ms=l("a"),wh=l("span"),y(ii.$$.fragment),Yb=g(),yh=l("span"),Xb=a("RobertaConfig"),fg=g(),Ft=l("div"),y(li.$$.fragment),e0=g(),ko=l("p"),t0=a("This is the configuration class to store the configuration of a "),ep=l("a"),o0=a("RobertaModel"),n0=a(" or a "),tp=l("a"),s0=a("TFRobertaModel"),a0=a(`. It is
used to instantiate a RoBERTa model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the RoBERTa
`),di=l("a"),r0=a("roberta-base"),i0=a(" architecture."),l0=g(),un=l("p"),d0=a("Configuration objects inherit from "),op=l("a"),c0=a("PretrainedConfig"),p0=a(` and can be used to control the model outputs. Read the
documentation from `),np=l("a"),h0=a("PretrainedConfig"),u0=a(" for more information."),m0=g(),mn=l("p"),f0=a("The "),sp=l("a"),g0=a("RobertaConfig"),_0=a(" class directly inherits "),ap=l("a"),b0=a("BertConfig"),k0=a(`. It reuses the same defaults. Please check the parent
class for more information.`),v0=g(),y(fs.$$.fragment),gg=g(),fn=l("h2"),gs=l("a"),Th=l("span"),y(ci.$$.fragment),w0=g(),$h=l("span"),y0=a("RobertaTokenizer"),_g=g(),Be=l("div"),y(pi.$$.fragment),T0=g(),xh=l("p"),$0=a("Constructs a RoBERTa tokenizer, derived from the GPT-2 tokenizer, using byte-level Byte-Pair-Encoding."),x0=g(),Fh=l("p"),F0=a("This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will"),C0=g(),y(_s.$$.fragment),R0=g(),hi=l("p"),E0=a("You can get around that behavior by passing "),Ch=l("code"),M0=a("add_prefix_space=True"),z0=a(` when instantiating this tokenizer or when you
call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.`),q0=g(),y(bs.$$.fragment),j0=g(),ui=l("p"),A0=a("This tokenizer inherits from "),rp=l("a"),P0=a("PreTrainedTokenizer"),L0=a(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),N0=g(),Ho=l("div"),y(mi.$$.fragment),I0=g(),Rh=l("p"),O0=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A RoBERTa sequence has the following format:`),D0=g(),fi=l("ul"),ip=l("li"),S0=a("single sequence: "),Eh=l("code"),H0=a("<s> X </s>"),V0=g(),lp=l("li"),B0=a("pair of sequences: "),Mh=l("code"),W0=a("<s> A </s></s> B </s>"),U0=g(),ks=l("div"),y(gi.$$.fragment),Q0=g(),_i=l("p"),Z0=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),zh=l("code"),K0=a("prepare_for_model"),J0=a(" method."),G0=g(),vs=l("div"),y(bi.$$.fragment),Y0=g(),qh=l("p"),X0=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. RoBERTa does not
make use of token type ids, therefore a list of zeros is returned.`),ek=g(),dp=l("div"),y(ki.$$.fragment),bg=g(),gn=l("h2"),ws=l("a"),jh=l("span"),y(vi.$$.fragment),tk=g(),Ah=l("span"),ok=a("RobertaTokenizerFast"),kg=g(),rt=l("div"),y(wi.$$.fragment),nk=g(),yi=l("p"),sk=a("Construct a \u201Cfast\u201D RoBERTa tokenizer (backed by HuggingFace\u2019s "),Ph=l("em"),ak=a("tokenizers"),rk=a(` library), derived from the GPT-2
tokenizer, using byte-level Byte-Pair-Encoding.`),ik=g(),Lh=l("p"),lk=a("This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will"),dk=g(),y(ys.$$.fragment),ck=g(),Ti=l("p"),pk=a("You can get around that behavior by passing "),Nh=l("code"),hk=a("add_prefix_space=True"),uk=a(` when instantiating this tokenizer or when you
call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.`),mk=g(),y(Ts.$$.fragment),fk=g(),$i=l("p"),gk=a("This tokenizer inherits from "),cp=l("a"),_k=a("PreTrainedTokenizerFast"),bk=a(` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),kk=g(),pp=l("div"),y(xi.$$.fragment),vg=g(),_n=l("h2"),$s=l("a"),Ih=l("span"),y(Fi.$$.fragment),vk=g(),Oh=l("span"),wk=a("RobertaModel"),wg=g(),it=l("div"),y(Ci.$$.fragment),yk=g(),Dh=l("p"),Tk=a("The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top."),$k=g(),Ri=l("p"),xk=a("This model inherits from "),hp=l("a"),Fk=a("PreTrainedModel"),Ck=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Rk=g(),Ei=l("p"),Ek=a("This model is also a PyTorch "),Mi=l("a"),Mk=a("torch.nn.Module"),zk=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),qk=g(),zi=l("p"),jk=a(`The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of
cross-attention is added between the self-attention layers, following the architecture described in `),Sh=l("em"),Ak=a(`Attention is
all you need`),Pk=a(`_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser and Illia Polosukhin.`),Lk=g(),gt=l("p"),Nk=a("To behave as an decoder the model needs to be initialized with the "),Hh=l("code"),Ik=a("is_decoder"),Ok=a(` argument of the configuration set
to `),Vh=l("code"),Dk=a("True"),Sk=a(". To be used in a Seq2Seq model, the model needs to initialized with both "),Bh=l("code"),Hk=a("is_decoder"),Vk=a(` argument and
`),Wh=l("code"),Bk=a("add_cross_attention"),Wk=a(" set to "),Uh=l("code"),Uk=a("True"),Qk=a("; an "),Qh=l("code"),Zk=a("encoder_hidden_states"),Kk=a(" is then expected as an input to the forward pass."),Jk=g(),xs=l("p"),Gk=a(".. _"),Zh=l("em"),Yk=a("Attention is all you need"),Xk=a(": "),qi=l("a"),e5=a("https://arxiv.org/abs/1706.03762"),t5=g(),Xt=l("div"),y(ji.$$.fragment),o5=g(),bn=l("p"),n5=a("The "),up=l("a"),s5=a("RobertaModel"),a5=a(" forward method, overrides the "),Kh=l("code"),r5=a("__call__"),i5=a(" special method."),l5=g(),y(Fs.$$.fragment),d5=g(),y(Cs.$$.fragment),yg=g(),kn=l("h2"),Rs=l("a"),Jh=l("span"),y(Ai.$$.fragment),c5=g(),Gh=l("span"),p5=a("RobertaForCausalLM"),Tg=g(),Ct=l("div"),y(Pi.$$.fragment),h5=g(),Li=l("p"),u5=a("RoBERTa Model with a "),Yh=l("code"),m5=a("language modeling"),f5=a(" head on top for CLM fine-tuning."),g5=g(),Ni=l("p"),_5=a("This model inherits from "),mp=l("a"),b5=a("PreTrainedModel"),k5=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),v5=g(),Ii=l("p"),w5=a("This model is also a PyTorch "),Oi=l("a"),y5=a("torch.nn.Module"),T5=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),$5=g(),eo=l("div"),y(Di.$$.fragment),x5=g(),vn=l("p"),F5=a("The "),fp=l("a"),C5=a("RobertaForCausalLM"),R5=a(" forward method, overrides the "),Xh=l("code"),E5=a("__call__"),M5=a(" special method."),z5=g(),y(Es.$$.fragment),q5=g(),y(Ms.$$.fragment),$g=g(),wn=l("h2"),zs=l("a"),eu=l("span"),y(Si.$$.fragment),j5=g(),tu=l("span"),A5=a("RobertaForMaskedLM"),xg=g(),Rt=l("div"),y(Hi.$$.fragment),P5=g(),Vi=l("p"),L5=a("RoBERTa Model with a "),ou=l("code"),N5=a("language modeling"),I5=a(" head on top."),O5=g(),Bi=l("p"),D5=a("This model inherits from "),gp=l("a"),S5=a("PreTrainedModel"),H5=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),V5=g(),Wi=l("p"),B5=a("This model is also a PyTorch "),Ui=l("a"),W5=a("torch.nn.Module"),U5=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Q5=g(),Lt=l("div"),y(Qi.$$.fragment),Z5=g(),yn=l("p"),K5=a("The "),_p=l("a"),J5=a("RobertaForMaskedLM"),G5=a(" forward method, overrides the "),nu=l("code"),Y5=a("__call__"),X5=a(" special method."),e4=g(),y(qs.$$.fragment),t4=g(),y(js.$$.fragment),o4=g(),y(As.$$.fragment),Fg=g(),Tn=l("h2"),Ps=l("a"),su=l("span"),y(Zi.$$.fragment),n4=g(),au=l("span"),s4=a("RobertaForSequenceClassification"),Cg=g(),Et=l("div"),y(Ki.$$.fragment),a4=g(),ru=l("p"),r4=a(`RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),i4=g(),Ji=l("p"),l4=a("This model inherits from "),bp=l("a"),d4=a("PreTrainedModel"),c4=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),p4=g(),Gi=l("p"),h4=a("This model is also a PyTorch "),Yi=l("a"),u4=a("torch.nn.Module"),m4=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),f4=g(),ft=l("div"),y(Xi.$$.fragment),g4=g(),$n=l("p"),_4=a("The "),kp=l("a"),b4=a("RobertaForSequenceClassification"),k4=a(" forward method, overrides the "),iu=l("code"),v4=a("__call__"),w4=a(" special method."),y4=g(),y(Ls.$$.fragment),T4=g(),y(Ns.$$.fragment),$4=g(),y(Is.$$.fragment),x4=g(),y(Os.$$.fragment),F4=g(),y(Ds.$$.fragment),Rg=g(),xn=l("h2"),Ss=l("a"),lu=l("span"),y(el.$$.fragment),C4=g(),du=l("span"),R4=a("RobertaForMultipleChoice"),Eg=g(),Mt=l("div"),y(tl.$$.fragment),E4=g(),cu=l("p"),M4=a(`Roberta Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),z4=g(),ol=l("p"),q4=a("This model inherits from "),vp=l("a"),j4=a("PreTrainedModel"),A4=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),P4=g(),nl=l("p"),L4=a("This model is also a PyTorch "),sl=l("a"),N4=a("torch.nn.Module"),I4=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),O4=g(),to=l("div"),y(al.$$.fragment),D4=g(),Fn=l("p"),S4=a("The "),wp=l("a"),H4=a("RobertaForMultipleChoice"),V4=a(" forward method, overrides the "),pu=l("code"),B4=a("__call__"),W4=a(" special method."),U4=g(),y(Hs.$$.fragment),Q4=g(),y(Vs.$$.fragment),Mg=g(),Cn=l("h2"),Bs=l("a"),hu=l("span"),y(rl.$$.fragment),Z4=g(),uu=l("span"),K4=a("RobertaForTokenClassification"),zg=g(),zt=l("div"),y(il.$$.fragment),J4=g(),mu=l("p"),G4=a(`Roberta Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),Y4=g(),ll=l("p"),X4=a("This model inherits from "),yp=l("a"),ev=a("PreTrainedModel"),tv=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ov=g(),dl=l("p"),nv=a("This model is also a PyTorch "),cl=l("a"),sv=a("torch.nn.Module"),av=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),rv=g(),Nt=l("div"),y(pl.$$.fragment),iv=g(),Rn=l("p"),lv=a("The "),Tp=l("a"),dv=a("RobertaForTokenClassification"),cv=a(" forward method, overrides the "),fu=l("code"),pv=a("__call__"),hv=a(" special method."),uv=g(),y(Ws.$$.fragment),mv=g(),y(Us.$$.fragment),fv=g(),y(Qs.$$.fragment),qg=g(),En=l("h2"),Zs=l("a"),gu=l("span"),y(hl.$$.fragment),gv=g(),_u=l("span"),_v=a("RobertaForQuestionAnswering"),jg=g(),qt=l("div"),y(ul.$$.fragment),bv=g(),Mn=l("p"),kv=a(`Roberta Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),bu=l("code"),vv=a("span start logits"),wv=a(" and "),ku=l("code"),yv=a("span end logits"),Tv=a(")."),$v=g(),ml=l("p"),xv=a("This model inherits from "),$p=l("a"),Fv=a("PreTrainedModel"),Cv=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Rv=g(),fl=l("p"),Ev=a("This model is also a PyTorch "),gl=l("a"),Mv=a("torch.nn.Module"),zv=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),qv=g(),It=l("div"),y(_l.$$.fragment),jv=g(),zn=l("p"),Av=a("The "),xp=l("a"),Pv=a("RobertaForQuestionAnswering"),Lv=a(" forward method, overrides the "),vu=l("code"),Nv=a("__call__"),Iv=a(" special method."),Ov=g(),y(Ks.$$.fragment),Dv=g(),y(Js.$$.fragment),Sv=g(),y(Gs.$$.fragment),Ag=g(),qn=l("h2"),Ys=l("a"),wu=l("span"),y(bl.$$.fragment),Hv=g(),yu=l("span"),Vv=a("TFRobertaModel"),Pg=g(),_t=l("div"),y(kl.$$.fragment),Bv=g(),Tu=l("p"),Wv=a("The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top."),Uv=g(),vl=l("p"),Qv=a("This model inherits from "),Fp=l("a"),Zv=a("TFPreTrainedModel"),Kv=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Jv=g(),wl=l("p"),Gv=a("This model is also a "),yl=l("a"),Yv=a("tf.keras.Model"),Xv=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),ew=g(),y(Xs.$$.fragment),tw=g(),oo=l("div"),y(Tl.$$.fragment),ow=g(),jn=l("p"),nw=a("The "),Cp=l("a"),sw=a("TFRobertaModel"),aw=a(" forward method, overrides the "),$u=l("code"),rw=a("__call__"),iw=a(" special method."),lw=g(),y(ea.$$.fragment),dw=g(),y(ta.$$.fragment),Lg=g(),An=l("h2"),oa=l("a"),xu=l("span"),y($l.$$.fragment),cw=g(),Fu=l("span"),pw=a("TFRobertaForCausalLM"),Ng=g(),Pn=l("div"),y(xl.$$.fragment),hw=g(),no=l("div"),y(Fl.$$.fragment),uw=g(),Ln=l("p"),mw=a("The "),Rp=l("a"),fw=a("TFRobertaForCausalLM"),gw=a(" forward method, overrides the "),Cu=l("code"),_w=a("__call__"),bw=a(" special method."),kw=g(),y(na.$$.fragment),vw=g(),y(sa.$$.fragment),Ig=g(),Nn=l("h2"),aa=l("a"),Ru=l("span"),y(Cl.$$.fragment),ww=g(),Eu=l("span"),yw=a("TFRobertaForMaskedLM"),Og=g(),bt=l("div"),y(Rl.$$.fragment),Tw=g(),El=l("p"),$w=a("RoBERTa Model with a "),Mu=l("code"),xw=a("language modeling"),Fw=a(" head on top."),Cw=g(),Ml=l("p"),Rw=a("This model inherits from "),Ep=l("a"),Ew=a("TFPreTrainedModel"),Mw=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),zw=g(),zl=l("p"),qw=a("This model is also a "),ql=l("a"),jw=a("tf.keras.Model"),Aw=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Pw=g(),y(ra.$$.fragment),Lw=g(),Ot=l("div"),y(jl.$$.fragment),Nw=g(),In=l("p"),Iw=a("The "),Mp=l("a"),Ow=a("TFRobertaForMaskedLM"),Dw=a(" forward method, overrides the "),zu=l("code"),Sw=a("__call__"),Hw=a(" special method."),Vw=g(),y(ia.$$.fragment),Bw=g(),y(la.$$.fragment),Ww=g(),y(da.$$.fragment),Dg=g(),On=l("h2"),ca=l("a"),qu=l("span"),y(Al.$$.fragment),Uw=g(),ju=l("span"),Qw=a("TFRobertaForSequenceClassification"),Sg=g(),kt=l("div"),y(Pl.$$.fragment),Zw=g(),Au=l("p"),Kw=a(`RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),Jw=g(),Ll=l("p"),Gw=a("This model inherits from "),zp=l("a"),Yw=a("TFPreTrainedModel"),Xw=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),e3=g(),Nl=l("p"),t3=a("This model is also a "),Il=l("a"),o3=a("tf.keras.Model"),n3=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),s3=g(),y(pa.$$.fragment),a3=g(),Dt=l("div"),y(Ol.$$.fragment),r3=g(),Dn=l("p"),i3=a("The "),qp=l("a"),l3=a("TFRobertaForSequenceClassification"),d3=a(" forward method, overrides the "),Pu=l("code"),c3=a("__call__"),p3=a(" special method."),h3=g(),y(ha.$$.fragment),u3=g(),y(ua.$$.fragment),m3=g(),y(ma.$$.fragment),Hg=g(),Sn=l("h2"),fa=l("a"),Lu=l("span"),y(Dl.$$.fragment),f3=g(),Nu=l("span"),g3=a("TFRobertaForMultipleChoice"),Vg=g(),vt=l("div"),y(Sl.$$.fragment),_3=g(),Iu=l("p"),b3=a(`Roberta Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),k3=g(),Hl=l("p"),v3=a("This model inherits from "),jp=l("a"),w3=a("TFPreTrainedModel"),y3=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),T3=g(),Vl=l("p"),$3=a("This model is also a "),Bl=l("a"),x3=a("tf.keras.Model"),F3=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),C3=g(),y(ga.$$.fragment),R3=g(),so=l("div"),y(Wl.$$.fragment),E3=g(),Hn=l("p"),M3=a("The "),Ap=l("a"),z3=a("TFRobertaForMultipleChoice"),q3=a(" forward method, overrides the "),Ou=l("code"),j3=a("__call__"),A3=a(" special method."),P3=g(),y(_a.$$.fragment),L3=g(),y(ba.$$.fragment),Bg=g(),Vn=l("h2"),ka=l("a"),Du=l("span"),y(Ul.$$.fragment),N3=g(),Su=l("span"),I3=a("TFRobertaForTokenClassification"),Wg=g(),wt=l("div"),y(Ql.$$.fragment),O3=g(),Hu=l("p"),D3=a(`RoBERTa Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),S3=g(),Zl=l("p"),H3=a("This model inherits from "),Pp=l("a"),V3=a("TFPreTrainedModel"),B3=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),W3=g(),Kl=l("p"),U3=a("This model is also a "),Jl=l("a"),Q3=a("tf.keras.Model"),Z3=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),K3=g(),y(va.$$.fragment),J3=g(),St=l("div"),y(Gl.$$.fragment),G3=g(),Bn=l("p"),Y3=a("The "),Lp=l("a"),X3=a("TFRobertaForTokenClassification"),ey=a(" forward method, overrides the "),Vu=l("code"),ty=a("__call__"),oy=a(" special method."),ny=g(),y(wa.$$.fragment),sy=g(),y(ya.$$.fragment),ay=g(),y(Ta.$$.fragment),Ug=g(),Wn=l("h2"),$a=l("a"),Bu=l("span"),y(Yl.$$.fragment),ry=g(),Wu=l("span"),iy=a("TFRobertaForQuestionAnswering"),Qg=g(),yt=l("div"),y(Xl.$$.fragment),ly=g(),Un=l("p"),dy=a(`RoBERTa Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),Uu=l("code"),cy=a("span start logits"),py=a(" and "),Qu=l("code"),hy=a("span end logits"),uy=a(")."),my=g(),ed=l("p"),fy=a("This model inherits from "),Np=l("a"),gy=a("TFPreTrainedModel"),_y=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),by=g(),td=l("p"),ky=a("This model is also a "),od=l("a"),vy=a("tf.keras.Model"),wy=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),yy=g(),y(xa.$$.fragment),Ty=g(),Ht=l("div"),y(nd.$$.fragment),$y=g(),Qn=l("p"),xy=a("The "),Ip=l("a"),Fy=a("TFRobertaForQuestionAnswering"),Cy=a(" forward method, overrides the "),Zu=l("code"),Ry=a("__call__"),Ey=a(" special method."),My=g(),y(Fa.$$.fragment),zy=g(),y(Ca.$$.fragment),qy=g(),y(Ra.$$.fragment),Zg=g(),Zn=l("h2"),Ea=l("a"),Ku=l("span"),y(sd.$$.fragment),jy=g(),Ju=l("span"),Ay=a("FlaxRobertaModel"),Kg=g(),lt=l("div"),y(ad.$$.fragment),Py=g(),Gu=l("p"),Ly=a("The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top."),Ny=g(),rd=l("p"),Iy=a("This model inherits from "),Op=l("a"),Oy=a("FlaxPreTrainedModel"),Dy=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),Sy=g(),id=l("p"),Hy=a("This model is also a Flax Linen "),ld=l("a"),Vy=a("flax.linen.Module"),By=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Wy=g(),Yu=l("p"),Uy=a("Finally, this model supports inherent JAX features such as:"),Qy=g(),vo=l("ul"),Xu=l("li"),dd=l("a"),Zy=a("Just-In-Time (JIT) compilation"),Ky=g(),em=l("li"),cd=l("a"),Jy=a("Automatic Differentiation"),Gy=g(),tm=l("li"),pd=l("a"),Yy=a("Vectorization"),Xy=g(),om=l("li"),hd=l("a"),e6=a("Parallelization"),t6=g(),ao=l("div"),y(ud.$$.fragment),o6=g(),Kn=l("p"),n6=a("The "),nm=l("code"),s6=a("FlaxRobertaPreTrainedModel"),a6=a(" forward method, overrides the "),sm=l("code"),r6=a("__call__"),i6=a(" special method."),l6=g(),y(Ma.$$.fragment),d6=g(),y(za.$$.fragment),Jg=g(),Jn=l("h2"),qa=l("a"),am=l("span"),y(md.$$.fragment),c6=g(),rm=l("span"),p6=a("FlaxRobertaForCausalLM"),Gg=g(),dt=l("div"),y(fd.$$.fragment),h6=g(),im=l("p"),u6=a(`Roberta Model with a language modeling head on top (a linear layer on top of the hidden-states output) e.g for
autoregressive tasks.`),m6=g(),gd=l("p"),f6=a("This model inherits from "),Dp=l("a"),g6=a("FlaxPreTrainedModel"),_6=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),b6=g(),_d=l("p"),k6=a("This model is also a Flax Linen "),bd=l("a"),v6=a("flax.linen.Module"),w6=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),y6=g(),lm=l("p"),T6=a("Finally, this model supports inherent JAX features such as:"),$6=g(),wo=l("ul"),dm=l("li"),kd=l("a"),x6=a("Just-In-Time (JIT) compilation"),F6=g(),cm=l("li"),vd=l("a"),C6=a("Automatic Differentiation"),R6=g(),pm=l("li"),wd=l("a"),E6=a("Vectorization"),M6=g(),hm=l("li"),yd=l("a"),z6=a("Parallelization"),q6=g(),ro=l("div"),y(Td.$$.fragment),j6=g(),Gn=l("p"),A6=a("The "),um=l("code"),P6=a("FlaxRobertaPreTrainedModel"),L6=a(" forward method, overrides the "),mm=l("code"),N6=a("__call__"),I6=a(" special method."),O6=g(),y(ja.$$.fragment),D6=g(),y(Aa.$$.fragment),Yg=g(),Yn=l("h2"),Pa=l("a"),fm=l("span"),y($d.$$.fragment),S6=g(),gm=l("span"),H6=a("FlaxRobertaForMaskedLM"),Xg=g(),ct=l("div"),y(xd.$$.fragment),V6=g(),Fd=l("p"),B6=a("RoBERTa Model with a "),_m=l("code"),W6=a("language modeling"),U6=a(" head on top."),Q6=g(),Cd=l("p"),Z6=a("This model inherits from "),Sp=l("a"),K6=a("FlaxPreTrainedModel"),J6=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),G6=g(),Rd=l("p"),Y6=a("This model is also a Flax Linen "),Ed=l("a"),X6=a("flax.linen.Module"),e7=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),t7=g(),bm=l("p"),o7=a("Finally, this model supports inherent JAX features such as:"),n7=g(),yo=l("ul"),km=l("li"),Md=l("a"),s7=a("Just-In-Time (JIT) compilation"),a7=g(),vm=l("li"),zd=l("a"),r7=a("Automatic Differentiation"),i7=g(),wm=l("li"),qd=l("a"),l7=a("Vectorization"),d7=g(),ym=l("li"),jd=l("a"),c7=a("Parallelization"),p7=g(),io=l("div"),y(Ad.$$.fragment),h7=g(),Xn=l("p"),u7=a("The "),Tm=l("code"),m7=a("FlaxRobertaPreTrainedModel"),f7=a(" forward method, overrides the "),$m=l("code"),g7=a("__call__"),_7=a(" special method."),b7=g(),y(La.$$.fragment),k7=g(),y(Na.$$.fragment),e_=g(),es=l("h2"),Ia=l("a"),xm=l("span"),y(Pd.$$.fragment),v7=g(),Fm=l("span"),w7=a("FlaxRobertaForSequenceClassification"),t_=g(),pt=l("div"),y(Ld.$$.fragment),y7=g(),Cm=l("p"),T7=a(`Roberta Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),$7=g(),Nd=l("p"),x7=a("This model inherits from "),Hp=l("a"),F7=a("FlaxPreTrainedModel"),C7=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),R7=g(),Id=l("p"),E7=a("This model is also a Flax Linen "),Od=l("a"),M7=a("flax.linen.Module"),z7=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),q7=g(),Rm=l("p"),j7=a("Finally, this model supports inherent JAX features such as:"),A7=g(),To=l("ul"),Em=l("li"),Dd=l("a"),P7=a("Just-In-Time (JIT) compilation"),L7=g(),Mm=l("li"),Sd=l("a"),N7=a("Automatic Differentiation"),I7=g(),zm=l("li"),Hd=l("a"),O7=a("Vectorization"),D7=g(),qm=l("li"),Vd=l("a"),S7=a("Parallelization"),H7=g(),lo=l("div"),y(Bd.$$.fragment),V7=g(),ts=l("p"),B7=a("The "),jm=l("code"),W7=a("FlaxRobertaPreTrainedModel"),U7=a(" forward method, overrides the "),Am=l("code"),Q7=a("__call__"),Z7=a(" special method."),K7=g(),y(Oa.$$.fragment),J7=g(),y(Da.$$.fragment),o_=g(),os=l("h2"),Sa=l("a"),Pm=l("span"),y(Wd.$$.fragment),G7=g(),Lm=l("span"),Y7=a("FlaxRobertaForMultipleChoice"),n_=g(),ht=l("div"),y(Ud.$$.fragment),X7=g(),Nm=l("p"),eT=a(`Roberta Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),tT=g(),Qd=l("p"),oT=a("This model inherits from "),Vp=l("a"),nT=a("FlaxPreTrainedModel"),sT=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),aT=g(),Zd=l("p"),rT=a("This model is also a Flax Linen "),Kd=l("a"),iT=a("flax.linen.Module"),lT=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),dT=g(),Im=l("p"),cT=a("Finally, this model supports inherent JAX features such as:"),pT=g(),$o=l("ul"),Om=l("li"),Jd=l("a"),hT=a("Just-In-Time (JIT) compilation"),uT=g(),Dm=l("li"),Gd=l("a"),mT=a("Automatic Differentiation"),fT=g(),Sm=l("li"),Yd=l("a"),gT=a("Vectorization"),_T=g(),Hm=l("li"),Xd=l("a"),bT=a("Parallelization"),kT=g(),co=l("div"),y(ec.$$.fragment),vT=g(),ns=l("p"),wT=a("The "),Vm=l("code"),yT=a("FlaxRobertaPreTrainedModel"),TT=a(" forward method, overrides the "),Bm=l("code"),$T=a("__call__"),xT=a(" special method."),FT=g(),y(Ha.$$.fragment),CT=g(),y(Va.$$.fragment),s_=g(),ss=l("h2"),Ba=l("a"),Wm=l("span"),y(tc.$$.fragment),RT=g(),Um=l("span"),ET=a("FlaxRobertaForTokenClassification"),a_=g(),ut=l("div"),y(oc.$$.fragment),MT=g(),Qm=l("p"),zT=a(`Roberta Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),qT=g(),nc=l("p"),jT=a("This model inherits from "),Bp=l("a"),AT=a("FlaxPreTrainedModel"),PT=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),LT=g(),sc=l("p"),NT=a("This model is also a Flax Linen "),ac=l("a"),IT=a("flax.linen.Module"),OT=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),DT=g(),Zm=l("p"),ST=a("Finally, this model supports inherent JAX features such as:"),HT=g(),xo=l("ul"),Km=l("li"),rc=l("a"),VT=a("Just-In-Time (JIT) compilation"),BT=g(),Jm=l("li"),ic=l("a"),WT=a("Automatic Differentiation"),UT=g(),Gm=l("li"),lc=l("a"),QT=a("Vectorization"),ZT=g(),Ym=l("li"),dc=l("a"),KT=a("Parallelization"),JT=g(),po=l("div"),y(cc.$$.fragment),GT=g(),as=l("p"),YT=a("The "),Xm=l("code"),XT=a("FlaxRobertaPreTrainedModel"),e8=a(" forward method, overrides the "),ef=l("code"),t8=a("__call__"),o8=a(" special method."),n8=g(),y(Wa.$$.fragment),s8=g(),y(Ua.$$.fragment),r_=g(),rs=l("h2"),Qa=l("a"),tf=l("span"),y(pc.$$.fragment),a8=g(),of=l("span"),r8=a("FlaxRobertaForQuestionAnswering"),i_=g(),mt=l("div"),y(hc.$$.fragment),i8=g(),is=l("p"),l8=a(`Roberta Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),nf=l("code"),d8=a("span start logits"),c8=a(" and "),sf=l("code"),p8=a("span end logits"),h8=a(")."),u8=g(),uc=l("p"),m8=a("This model inherits from "),Wp=l("a"),f8=a("FlaxPreTrainedModel"),g8=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),_8=g(),mc=l("p"),b8=a("This model is also a Flax Linen "),fc=l("a"),k8=a("flax.linen.Module"),v8=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),w8=g(),af=l("p"),y8=a("Finally, this model supports inherent JAX features such as:"),T8=g(),Fo=l("ul"),rf=l("li"),gc=l("a"),$8=a("Just-In-Time (JIT) compilation"),x8=g(),lf=l("li"),_c=l("a"),F8=a("Automatic Differentiation"),C8=g(),df=l("li"),bc=l("a"),R8=a("Vectorization"),E8=g(),cf=l("li"),kc=l("a"),M8=a("Parallelization"),z8=g(),ho=l("div"),y(vc.$$.fragment),q8=g(),ls=l("p"),j8=a("The "),pf=l("code"),A8=a("FlaxRobertaPreTrainedModel"),P8=a(" forward method, overrides the "),hf=l("code"),L8=a("__call__"),N8=a(" special method."),I8=g(),y(Za.$$.fragment),O8=g(),y(Ka.$$.fragment),this.h()},l(i){const v=zR('[data-svelte="svelte-1phssyn"]',document.head);t=d(v,"META",{name:!0,content:!0}),v.forEach(o),u=_(i),c=d(i,"H1",{class:!0});var wc=s(c);h=d(wc,"A",{id:!0,class:!0,href:!0});var uf=s(h);f=d(uf,"SPAN",{});var mf=s(f);C(p.$$.fragment,mf),mf.forEach(o),uf.forEach(o),m=_(wc),R=d(wc,"SPAN",{});var ff=s(R);I=r(ff,"RoBERTa"),ff.forEach(o),wc.forEach(o),P=_(i),L=d(i,"H2",{class:!0});var yc=s(L);H=d(yc,"A",{id:!0,class:!0,href:!0});var gf=s(H);V=d(gf,"SPAN",{});var _f=s(V);C(E.$$.fragment,_f),_f.forEach(o),gf.forEach(o),ie=_(yc),U=d(yc,"SPAN",{});var bf=s(U);Z=r(bf,"Overview"),bf.forEach(o),yc.forEach(o),Te=_(i),G=d(i,"P",{});var Tc=s(G);Ee=r(Tc,"The RoBERTa model was proposed in "),be=d(Tc,"A",{href:!0,rel:!0});var kf=s(be);oe=r(kf,"RoBERTa: A Robustly Optimized BERT Pretraining Approach"),kf.forEach(o),Me=r(Tc,` by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. It is based on Google\u2019s BERT model released in 2018.`),Tc.forEach(o),$e=_(i),X=d(i,"P",{});var vf=s(X);ze=r(vf,`It builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with
much larger mini-batches and learning rates.`),vf.forEach(o),xe=_(i),ee=d(i,"P",{});var wf=s(ee);qe=r(wf,"The abstract from the paper is the following:"),wf.forEach(o),Fe=_(i),le=d(i,"P",{});var yf=s(le);N=d(yf,"EM",{});var Tf=s(N);D=r(Tf,`Language model pretraining has led to significant performance gains but careful comparison between different
approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes,
and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication
study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and
training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every
model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results
highlight the importance of previously overlooked design choices, and raise questions about the source of recently
reported improvements. We release our models and code.`),Tf.forEach(o),yf.forEach(o),Ce=_(i),te=d(i,"P",{});var $f=s(te);je=r($f,"Tips:"),$f.forEach(o),Re=_(i),Q=d(i,"UL",{});var Co=s(Q);ke=d(Co,"LI",{});var $c=s(ke);Ae=r($c,"This implementation is the same as "),O=d($c,"A",{href:!0});var xf=s(O);Pe=r(xf,"BertModel"),xf.forEach(o),ne=r($c,` with a tiny embeddings tweak as well as a setup
for Roberta pretrained models.`),$c.forEach(o),Le=_(Co),ve=d(Co,"LI",{});var Ff=s(ve);se=r(Ff,`RoBERTa has the same architecture as BERT, but uses a byte-level BPE as a tokenizer (same as GPT-2) and uses a
different pretraining scheme.`),Ff.forEach(o),Ne=_(Co),de=d(Co,"LI",{});var Ro=s(de);K=r(Ro,"RoBERTa doesn\u2019t have "),we=d(Ro,"CODE",{});var Cf=s(we);ae=r(Cf,"token_type_ids"),Cf.forEach(o),Ie=r(Ro,`, you don\u2019t need to indicate which token belongs to which segment. Just
separate your segments with the separation token `),ce=d(Ro,"CODE",{});var Rf=s(ce);S=r(Rf,"tokenizer.sep_token"),Rf.forEach(o),Oe=r(Ro," (or "),B=d(Ro,"CODE",{});var Ef=s(B);De=r(Ef,"</s>"),Ef.forEach(o),Se=r(Ro,")"),Ro.forEach(o),w=_(Co),q=d(Co,"LI",{});var Up=s(q);Ke=d(Up,"A",{href:!0});var Mf=s(Ke);ue=r(Mf,"CamemBERT"),Mf.forEach(o),ot=r(Up," is a wrapper around RoBERTa. Refer to this page for usage examples."),Up.forEach(o),Co.forEach(o),Ye=_(i),A=d(i,"P",{});var ds=s(A);nt=r(ds,"This model was contributed by "),Ue=d(ds,"A",{href:!0,rel:!0});var zf=s(Ue);st=r(zf,"julien-c"),zf.forEach(o),at=r(ds,". The original code can be found "),Qe=d(ds,"A",{href:!0,rel:!0});var qf=s(Qe);J=r(qf,"here"),qf.forEach(o),re=r(ds,"."),ds.forEach(o),Xe=_(i),He=d(i,"H2",{class:!0});var xc=s(He);Y=d(xc,"A",{id:!0,class:!0,href:!0});var jf=s(Y);Je=d(jf,"SPAN",{});var Af=s(Je);C(Ze.$$.fragment,Af),Af.forEach(o),jf.forEach(o),Ve=_(xc),Ge=d(xc,"SPAN",{});var Pf=s(Ge);me=r(Pf,"Resources"),Pf.forEach(o),xc.forEach(o),et=_(i),Pc=d(i,"P",{});var Lf=s(Pc);I1=r(Lf,"A list of official Hugging Face and community (indicated by \u{1F30E}) resources to help you get started with RoBERTa. If you\u2019re interested in submitting a resource to be included here, please feel free to open a Pull Request and we\u2019ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource."),Lf.forEach(o),sg=_(i),C(br.$$.fragment,i),ag=_(i),Tt=d(i,"UL",{});var jt=s(Tt);pn=d(jt,"LI",{});var cs=s(pn);O1=r(cs,"A blog on "),kr=d(cs,"A",{href:!0,rel:!0});var Nf=s(kr);D1=r(Nf,"Getting Started with Sentiment Analysis on Twitter"),Nf.forEach(o),S1=r(cs," using RoBERTa and the "),vr=d(cs,"A",{href:!0,rel:!0});var If=s(vr);H1=r(If,"Inference API"),If.forEach(o),V1=r(cs,"."),cs.forEach(o),B1=_(jt),wr=d(jt,"LI",{});var Fc=s(wr);W1=r(Fc,"A blog on "),yr=d(Fc,"A",{href:!0,rel:!0});var Of=s(yr);U1=r(Of,"Opinion Classification with Kili and Hugging Face AutoTrain"),Of.forEach(o),Q1=r(Fc," using RoBERTa."),Fc.forEach(o),Z1=_(jt),Tr=d(jt,"LI",{});var Cc=s(Tr);K1=r(Cc,"A notebook on how to "),$r=d(Cc,"A",{href:!0,rel:!0});var Df=s($r);J1=r(Df,"finetune RoBERTa for sentiment analysis"),Df.forEach(o),G1=r(Cc,". \u{1F30E}"),Cc.forEach(o),Y1=_(jt),Mo=d(jt,"LI",{});var Vo=s(Mo);Lc=d(Vo,"A",{href:!0});var Sf=s(Lc);X1=r(Sf,"RobertaForSequenceClassification"),Sf.forEach(o),e2=r(Vo," is supported by this "),xr=d(Vo,"A",{href:!0,rel:!0});var Hf=s(xr);t2=r(Hf,"example script"),Hf.forEach(o),o2=r(Vo," and "),Fr=d(Vo,"A",{href:!0,rel:!0});var Vf=s(Fr);n2=r(Vf,"notebook"),Vf.forEach(o),s2=r(Vo,"."),Vo.forEach(o),a2=_(jt),zo=d(jt,"LI",{});var Bo=s(zo);Nc=d(Bo,"A",{href:!0});var Bf=s(Nc);r2=r(Bf,"TFRobertaForSequenceClassification"),Bf.forEach(o),i2=r(Bo," is supported by this "),Cr=d(Bo,"A",{href:!0,rel:!0});var Wf=s(Cr);l2=r(Wf,"example script"),Wf.forEach(o),d2=r(Bo," and "),Rr=d(Bo,"A",{href:!0,rel:!0});var Uf=s(Rr);c2=r(Uf,"notebook"),Uf.forEach(o),p2=r(Bo,"."),Bo.forEach(o),h2=_(jt),qo=d(jt,"LI",{});var Wo=s(qo);Ic=d(Wo,"A",{href:!0});var Qf=s(Ic);u2=r(Qf,"FlaxRobertaForSequenceClassification"),Qf.forEach(o),m2=r(Wo," is supported by this "),Er=d(Wo,"A",{href:!0,rel:!0});var Zf=s(Er);f2=r(Zf,"example script"),Zf.forEach(o),g2=r(Wo," and "),Mr=d(Wo,"A",{href:!0,rel:!0});var Kf=s(Mr);_2=r(Kf,"notebook"),Kf.forEach(o),b2=r(Wo,"."),Wo.forEach(o),jt.forEach(o),rg=_(i),C(zr.$$.fragment,i),ig=_(i),Yt=d(i,"UL",{});var Eo=s(Yt);jo=d(Eo,"LI",{});var Uo=s(jo);Oc=d(Uo,"A",{href:!0});var Jf=s(Oc);k2=r(Jf,"RobertaForTokenClassification"),Jf.forEach(o),v2=r(Uo," is supported by this "),qr=d(Uo,"A",{href:!0,rel:!0});var Gf=s(qr);w2=r(Gf,"example script"),Gf.forEach(o),y2=r(Uo," and "),jr=d(Uo,"A",{href:!0,rel:!0});var Yf=s(jr);T2=r(Yf,"notebook"),Yf.forEach(o),$2=r(Uo,"."),Uo.forEach(o),x2=_(Eo),Ao=d(Eo,"LI",{});var Qo=s(Ao);Dc=d(Qo,"A",{href:!0});var Xf=s(Dc);F2=r(Xf,"TFRobertaForTokenClassification"),Xf.forEach(o),C2=r(Qo," is supported by this "),Ar=d(Qo,"A",{href:!0,rel:!0});var eg=s(Ar);R2=r(eg,"example script"),eg.forEach(o),E2=r(Qo," and "),Pr=d(Qo,"A",{href:!0,rel:!0});var tg=s(Pr);M2=r(tg,"notebook"),tg.forEach(o),z2=r(Qo,"."),Qo.forEach(o),q2=_(Eo),ps=d(Eo,"LI",{});var Ja=s(ps);Sc=d(Ja,"A",{href:!0});var Q8=s(Sc);j2=r(Q8,"FlaxRobertaForTokenClassification"),Q8.forEach(o),A2=r(Ja," is supported by this "),Lr=d(Ja,"A",{href:!0,rel:!0});var Z8=s(Lr);P2=r(Z8,"example script"),Z8.forEach(o),L2=r(Ja,"."),Ja.forEach(o),N2=_(Eo),Hc=d(Eo,"LI",{});var D8=s(Hc);Nr=d(D8,"A",{href:!0,rel:!0});var K8=s(Nr);I2=r(K8,"Token classification"),K8.forEach(o),O2=r(D8," chapter of the \u{1F917} Hugging Face Course."),D8.forEach(o),Eo.forEach(o),lg=_(i),C(Ir.$$.fragment,i),dg=_(i),At=d(i,"UL",{});var Zo=s(At);Or=d(Zo,"LI",{});var d_=s(Or);D2=r(d_,"A blog on "),Dr=d(d_,"A",{href:!0,rel:!0});var J8=s(Dr);S2=r(J8,"How to train a new language model from scratch using Transformers and Tokenizers"),J8.forEach(o),H2=r(d_," with RoBERTa."),d_.forEach(o),V2=_(Zo),Po=d(Zo,"LI",{});var Rc=s(Po);Vc=d(Rc,"A",{href:!0});var G8=s(Vc);B2=r(G8,"RobertaForMaskedLM"),G8.forEach(o),W2=r(Rc," is supported by this "),Sr=d(Rc,"A",{href:!0,rel:!0});var Y8=s(Sr);U2=r(Y8,"example script"),Y8.forEach(o),Q2=r(Rc," and "),Hr=d(Rc,"A",{href:!0,rel:!0});var X8=s(Hr);Z2=r(X8,"notebook"),X8.forEach(o),K2=r(Rc,"."),Rc.forEach(o),J2=_(Zo),Lo=d(Zo,"LI",{});var Ec=s(Lo);Bc=d(Ec,"A",{href:!0});var e$=s(Bc);G2=r(e$,"TFRobertaForMaskedLM"),e$.forEach(o),Y2=r(Ec," is supported by this "),Vr=d(Ec,"A",{href:!0,rel:!0});var t$=s(Vr);X2=r(t$,"example script"),t$.forEach(o),eb=r(Ec," and "),Br=d(Ec,"A",{href:!0,rel:!0});var o$=s(Br);tb=r(o$,"notebook"),o$.forEach(o),ob=r(Ec,"."),Ec.forEach(o),nb=_(Zo),No=d(Zo,"LI",{});var Mc=s(No);Wc=d(Mc,"A",{href:!0});var n$=s(Wc);sb=r(n$,"FlaxRobertaForMaskedLM"),n$.forEach(o),ab=r(Mc," is supported by this "),Wr=d(Mc,"A",{href:!0,rel:!0});var s$=s(Wr);rb=r(s$,"example script"),s$.forEach(o),ib=r(Mc," and "),Ur=d(Mc,"A",{href:!0,rel:!0});var a$=s(Ur);lb=r(a$,"notebook"),a$.forEach(o),db=r(Mc,"."),Mc.forEach(o),cb=_(Zo),Uc=d(Zo,"LI",{});var S8=s(Uc);Qr=d(S8,"A",{href:!0,rel:!0});var r$=s(Qr);pb=r(r$,"Masked language modeling"),r$.forEach(o),hb=r(S8," chapter of the \u{1F917} Hugging Face Course."),S8.forEach(o),Zo.forEach(o),cg=_(i),C(Zr.$$.fragment,i),pg=_(i),Pt=d(i,"UL",{});var Ko=s(Pt);Kr=d(Ko,"LI",{});var c_=s(Kr);ub=r(c_,"A blog on "),Jr=d(c_,"A",{href:!0,rel:!0});var i$=s(Jr);mb=r(i$,"Accelerated Inference with Optimum and Transformers Pipelines"),i$.forEach(o),fb=r(c_," with RoBERTa for question answering."),c_.forEach(o),gb=_(Ko),Io=d(Ko,"LI",{});var zc=s(Io);Qc=d(zc,"A",{href:!0});var l$=s(Qc);_b=r(l$,"RobertaForQuestionAnswering"),l$.forEach(o),bb=r(zc," is supported by this "),Gr=d(zc,"A",{href:!0,rel:!0});var d$=s(Gr);kb=r(d$,"example script"),d$.forEach(o),vb=r(zc," and "),Yr=d(zc,"A",{href:!0,rel:!0});var c$=s(Yr);wb=r(c$,"notebook"),c$.forEach(o),yb=r(zc,"."),zc.forEach(o),Tb=_(Ko),Oo=d(Ko,"LI",{});var qc=s(Oo);Zc=d(qc,"A",{href:!0});var p$=s(Zc);$b=r(p$,"TFRobertaForQuestionAnswering"),p$.forEach(o),xb=r(qc," is supported by this "),Xr=d(qc,"A",{href:!0,rel:!0});var h$=s(Xr);Fb=r(h$,"example script"),h$.forEach(o),Cb=r(qc," and "),ei=d(qc,"A",{href:!0,rel:!0});var u$=s(ei);Rb=r(u$,"notebook"),u$.forEach(o),Eb=r(qc,"."),qc.forEach(o),Mb=_(Ko),hs=d(Ko,"LI",{});var og=s(hs);Kc=d(og,"A",{href:!0});var m$=s(Kc);zb=r(m$,"FlaxRobertaForQuestionAnswering"),m$.forEach(o),qb=r(og," is supported by this "),ti=d(og,"A",{href:!0,rel:!0});var f$=s(ti);jb=r(f$,"example script"),f$.forEach(o),Ab=r(og,"."),og.forEach(o),Pb=_(Ko),Jc=d(Ko,"LI",{});var H8=s(Jc);oi=d(H8,"A",{href:!0,rel:!0});var g$=s(oi);Lb=r(g$,"Question answering"),g$.forEach(o),Nb=r(H8," chapter of the \u{1F917} Hugging Face Course."),H8.forEach(o),Ko.forEach(o),hg=_(i),Gc=d(i,"P",{});var _$=s(Gc);vh=d(_$,"STRONG",{});var b$=s(vh);Ib=r(b$,"Multiple choice"),b$.forEach(o),_$.forEach(o),ug=_(i),us=d(i,"UL",{});var p_=s(us);Do=d(p_,"LI",{});var jc=s(Do);Yc=d(jc,"A",{href:!0});var k$=s(Yc);Ob=r(k$,"RobertaForMultipleChoice"),k$.forEach(o),Db=r(jc," is supported by this "),ni=d(jc,"A",{href:!0,rel:!0});var v$=s(ni);Sb=r(v$,"example script"),v$.forEach(o),Hb=r(jc," and "),si=d(jc,"A",{href:!0,rel:!0});var w$=s(si);Vb=r(w$,"notebook"),w$.forEach(o),Bb=r(jc,"."),jc.forEach(o),Wb=_(p_),So=d(p_,"LI",{});var Ac=s(So);Xc=d(Ac,"A",{href:!0});var y$=s(Xc);Ub=r(y$,"TFRobertaForMultipleChoice"),y$.forEach(o),Qb=r(Ac," is supported by this "),ai=d(Ac,"A",{href:!0,rel:!0});var T$=s(ai);Zb=r(T$,"example script"),T$.forEach(o),Kb=r(Ac," and "),ri=d(Ac,"A",{href:!0,rel:!0});var $$=s(ri);Jb=r($$,"notebook"),$$.forEach(o),Gb=r(Ac,"."),Ac.forEach(o),p_.forEach(o),mg=_(i),hn=d(i,"H2",{class:!0});var h_=s(hn);ms=d(h_,"A",{id:!0,class:!0,href:!0});var x$=s(ms);wh=d(x$,"SPAN",{});var F$=s(wh);C(ii.$$.fragment,F$),F$.forEach(o),x$.forEach(o),Yb=_(h_),yh=d(h_,"SPAN",{});var C$=s(yh);Xb=r(C$,"RobertaConfig"),C$.forEach(o),h_.forEach(o),fg=_(i),Ft=d(i,"DIV",{class:!0});var Jo=s(Ft);C(li.$$.fragment,Jo),e0=_(Jo),ko=d(Jo,"P",{});var Ga=s(ko);t0=r(Ga,"This is the configuration class to store the configuration of a "),ep=d(Ga,"A",{href:!0});var R$=s(ep);o0=r(R$,"RobertaModel"),R$.forEach(o),n0=r(Ga," or a "),tp=d(Ga,"A",{href:!0});var E$=s(tp);s0=r(E$,"TFRobertaModel"),E$.forEach(o),a0=r(Ga,`. It is
used to instantiate a RoBERTa model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the RoBERTa
`),di=d(Ga,"A",{href:!0,rel:!0});var M$=s(di);r0=r(M$,"roberta-base"),M$.forEach(o),i0=r(Ga," architecture."),Ga.forEach(o),l0=_(Jo),un=d(Jo,"P",{});var Qp=s(un);d0=r(Qp,"Configuration objects inherit from "),op=d(Qp,"A",{href:!0});var z$=s(op);c0=r(z$,"PretrainedConfig"),z$.forEach(o),p0=r(Qp,` and can be used to control the model outputs. Read the
documentation from `),np=d(Qp,"A",{href:!0});var q$=s(np);h0=r(q$,"PretrainedConfig"),q$.forEach(o),u0=r(Qp," for more information."),Qp.forEach(o),m0=_(Jo),mn=d(Jo,"P",{});var Zp=s(mn);f0=r(Zp,"The "),sp=d(Zp,"A",{href:!0});var j$=s(sp);g0=r(j$,"RobertaConfig"),j$.forEach(o),_0=r(Zp," class directly inherits "),ap=d(Zp,"A",{href:!0});var A$=s(ap);b0=r(A$,"BertConfig"),A$.forEach(o),k0=r(Zp,`. It reuses the same defaults. Please check the parent
class for more information.`),Zp.forEach(o),v0=_(Jo),C(fs.$$.fragment,Jo),Jo.forEach(o),gg=_(i),fn=d(i,"H2",{class:!0});var u_=s(fn);gs=d(u_,"A",{id:!0,class:!0,href:!0});var P$=s(gs);Th=d(P$,"SPAN",{});var L$=s(Th);C(ci.$$.fragment,L$),L$.forEach(o),P$.forEach(o),w0=_(u_),$h=d(u_,"SPAN",{});var N$=s($h);y0=r(N$,"RobertaTokenizer"),N$.forEach(o),u_.forEach(o),_g=_(i),Be=d(i,"DIV",{class:!0});var tt=s(Be);C(pi.$$.fragment,tt),T0=_(tt),xh=d(tt,"P",{});var I$=s(xh);$0=r(I$,"Constructs a RoBERTa tokenizer, derived from the GPT-2 tokenizer, using byte-level Byte-Pair-Encoding."),I$.forEach(o),x0=_(tt),Fh=d(tt,"P",{});var O$=s(Fh);F0=r(O$,"This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will"),O$.forEach(o),C0=_(tt),C(_s.$$.fragment,tt),R0=_(tt),hi=d(tt,"P",{});var m_=s(hi);E0=r(m_,"You can get around that behavior by passing "),Ch=d(m_,"CODE",{});var D$=s(Ch);M0=r(D$,"add_prefix_space=True"),D$.forEach(o),z0=r(m_,` when instantiating this tokenizer or when you
call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.`),m_.forEach(o),q0=_(tt),C(bs.$$.fragment,tt),j0=_(tt),ui=d(tt,"P",{});var f_=s(ui);A0=r(f_,"This tokenizer inherits from "),rp=d(f_,"A",{href:!0});var S$=s(rp);P0=r(S$,"PreTrainedTokenizer"),S$.forEach(o),L0=r(f_,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),f_.forEach(o),N0=_(tt),Ho=d(tt,"DIV",{class:!0});var Kp=s(Ho);C(mi.$$.fragment,Kp),I0=_(Kp),Rh=d(Kp,"P",{});var H$=s(Rh);O0=r(H$,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A RoBERTa sequence has the following format:`),H$.forEach(o),D0=_(Kp),fi=d(Kp,"UL",{});var g_=s(fi);ip=d(g_,"LI",{});var V8=s(ip);S0=r(V8,"single sequence: "),Eh=d(V8,"CODE",{});var V$=s(Eh);H0=r(V$,"<s> X </s>"),V$.forEach(o),V8.forEach(o),V0=_(g_),lp=d(g_,"LI",{});var B8=s(lp);B0=r(B8,"pair of sequences: "),Mh=d(B8,"CODE",{});var B$=s(Mh);W0=r(B$,"<s> A </s></s> B </s>"),B$.forEach(o),B8.forEach(o),g_.forEach(o),Kp.forEach(o),U0=_(tt),ks=d(tt,"DIV",{class:!0});var __=s(ks);C(gi.$$.fragment,__),Q0=_(__),_i=d(__,"P",{});var b_=s(_i);Z0=r(b_,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),zh=d(b_,"CODE",{});var W$=s(zh);K0=r(W$,"prepare_for_model"),W$.forEach(o),J0=r(b_," method."),b_.forEach(o),__.forEach(o),G0=_(tt),vs=d(tt,"DIV",{class:!0});var k_=s(vs);C(bi.$$.fragment,k_),Y0=_(k_),qh=d(k_,"P",{});var U$=s(qh);X0=r(U$,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. RoBERTa does not
make use of token type ids, therefore a list of zeros is returned.`),U$.forEach(o),k_.forEach(o),ek=_(tt),dp=d(tt,"DIV",{class:!0});var Q$=s(dp);C(ki.$$.fragment,Q$),Q$.forEach(o),tt.forEach(o),bg=_(i),gn=d(i,"H2",{class:!0});var v_=s(gn);ws=d(v_,"A",{id:!0,class:!0,href:!0});var Z$=s(ws);jh=d(Z$,"SPAN",{});var K$=s(jh);C(vi.$$.fragment,K$),K$.forEach(o),Z$.forEach(o),tk=_(v_),Ah=d(v_,"SPAN",{});var J$=s(Ah);ok=r(J$,"RobertaTokenizerFast"),J$.forEach(o),v_.forEach(o),kg=_(i),rt=d(i,"DIV",{class:!0});var $t=s(rt);C(wi.$$.fragment,$t),nk=_($t),yi=d($t,"P",{});var w_=s(yi);sk=r(w_,"Construct a \u201Cfast\u201D RoBERTa tokenizer (backed by HuggingFace\u2019s "),Ph=d(w_,"EM",{});var G$=s(Ph);ak=r(G$,"tokenizers"),G$.forEach(o),rk=r(w_,` library), derived from the GPT-2
tokenizer, using byte-level Byte-Pair-Encoding.`),w_.forEach(o),ik=_($t),Lh=d($t,"P",{});var Y$=s(Lh);lk=r(Y$,"This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will"),Y$.forEach(o),dk=_($t),C(ys.$$.fragment,$t),ck=_($t),Ti=d($t,"P",{});var y_=s(Ti);pk=r(y_,"You can get around that behavior by passing "),Nh=d(y_,"CODE",{});var X$=s(Nh);hk=r(X$,"add_prefix_space=True"),X$.forEach(o),uk=r(y_,` when instantiating this tokenizer or when you
call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.`),y_.forEach(o),mk=_($t),C(Ts.$$.fragment,$t),fk=_($t),$i=d($t,"P",{});var T_=s($i);gk=r(T_,"This tokenizer inherits from "),cp=d(T_,"A",{href:!0});var e9=s(cp);_k=r(e9,"PreTrainedTokenizerFast"),e9.forEach(o),bk=r(T_,` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),T_.forEach(o),kk=_($t),pp=d($t,"DIV",{class:!0});var t9=s(pp);C(xi.$$.fragment,t9),t9.forEach(o),$t.forEach(o),vg=_(i),_n=d(i,"H2",{class:!0});var $_=s(_n);$s=d($_,"A",{id:!0,class:!0,href:!0});var o9=s($s);Ih=d(o9,"SPAN",{});var n9=s(Ih);C(Fi.$$.fragment,n9),n9.forEach(o),o9.forEach(o),vk=_($_),Oh=d($_,"SPAN",{});var s9=s(Oh);wk=r(s9,"RobertaModel"),s9.forEach(o),$_.forEach(o),wg=_(i),it=d(i,"DIV",{class:!0});var xt=s(it);C(Ci.$$.fragment,xt),yk=_(xt),Dh=d(xt,"P",{});var a9=s(Dh);Tk=r(a9,"The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top."),a9.forEach(o),$k=_(xt),Ri=d(xt,"P",{});var x_=s(Ri);xk=r(x_,"This model inherits from "),hp=d(x_,"A",{href:!0});var r9=s(hp);Fk=r(r9,"PreTrainedModel"),r9.forEach(o),Ck=r(x_,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),x_.forEach(o),Rk=_(xt),Ei=d(xt,"P",{});var F_=s(Ei);Ek=r(F_,"This model is also a PyTorch "),Mi=d(F_,"A",{href:!0,rel:!0});var i9=s(Mi);Mk=r(i9,"torch.nn.Module"),i9.forEach(o),zk=r(F_,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),F_.forEach(o),qk=_(xt),zi=d(xt,"P",{});var C_=s(zi);jk=r(C_,`The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of
cross-attention is added between the self-attention layers, following the architecture described in `),Sh=d(C_,"EM",{});var l9=s(Sh);Ak=r(l9,`Attention is
all you need`),l9.forEach(o),Pk=r(C_,`_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser and Illia Polosukhin.`),C_.forEach(o),Lk=_(xt),gt=d(xt,"P",{});var Vt=s(gt);Nk=r(Vt,"To behave as an decoder the model needs to be initialized with the "),Hh=d(Vt,"CODE",{});var d9=s(Hh);Ik=r(d9,"is_decoder"),d9.forEach(o),Ok=r(Vt,` argument of the configuration set
to `),Vh=d(Vt,"CODE",{});var c9=s(Vh);Dk=r(c9,"True"),c9.forEach(o),Sk=r(Vt,". To be used in a Seq2Seq model, the model needs to initialized with both "),Bh=d(Vt,"CODE",{});var p9=s(Bh);Hk=r(p9,"is_decoder"),p9.forEach(o),Vk=r(Vt,` argument and
`),Wh=d(Vt,"CODE",{});var h9=s(Wh);Bk=r(h9,"add_cross_attention"),h9.forEach(o),Wk=r(Vt," set to "),Uh=d(Vt,"CODE",{});var u9=s(Uh);Uk=r(u9,"True"),u9.forEach(o),Qk=r(Vt,"; an "),Qh=d(Vt,"CODE",{});var m9=s(Qh);Zk=r(m9,"encoder_hidden_states"),m9.forEach(o),Kk=r(Vt," is then expected as an input to the forward pass."),Vt.forEach(o),Jk=_(xt),xs=d(xt,"P",{});var ng=s(xs);Gk=r(ng,".. _"),Zh=d(ng,"EM",{});var f9=s(Zh);Yk=r(f9,"Attention is all you need"),f9.forEach(o),Xk=r(ng,": "),qi=d(ng,"A",{href:!0,rel:!0});var g9=s(qi);e5=r(g9,"https://arxiv.org/abs/1706.03762"),g9.forEach(o),ng.forEach(o),t5=_(xt),Xt=d(xt,"DIV",{class:!0});var Ya=s(Xt);C(ji.$$.fragment,Ya),o5=_(Ya),bn=d(Ya,"P",{});var Jp=s(bn);n5=r(Jp,"The "),up=d(Jp,"A",{href:!0});var _9=s(up);s5=r(_9,"RobertaModel"),_9.forEach(o),a5=r(Jp," forward method, overrides the "),Kh=d(Jp,"CODE",{});var b9=s(Kh);r5=r(b9,"__call__"),b9.forEach(o),i5=r(Jp," special method."),Jp.forEach(o),l5=_(Ya),C(Fs.$$.fragment,Ya),d5=_(Ya),C(Cs.$$.fragment,Ya),Ya.forEach(o),xt.forEach(o),yg=_(i),kn=d(i,"H2",{class:!0});var R_=s(kn);Rs=d(R_,"A",{id:!0,class:!0,href:!0});var k9=s(Rs);Jh=d(k9,"SPAN",{});var v9=s(Jh);C(Ai.$$.fragment,v9),v9.forEach(o),k9.forEach(o),c5=_(R_),Gh=d(R_,"SPAN",{});var w9=s(Gh);p5=r(w9,"RobertaForCausalLM"),w9.forEach(o),R_.forEach(o),Tg=_(i),Ct=d(i,"DIV",{class:!0});var Go=s(Ct);C(Pi.$$.fragment,Go),h5=_(Go),Li=d(Go,"P",{});var E_=s(Li);u5=r(E_,"RoBERTa Model with a "),Yh=d(E_,"CODE",{});var y9=s(Yh);m5=r(y9,"language modeling"),y9.forEach(o),f5=r(E_," head on top for CLM fine-tuning."),E_.forEach(o),g5=_(Go),Ni=d(Go,"P",{});var M_=s(Ni);_5=r(M_,"This model inherits from "),mp=d(M_,"A",{href:!0});var T9=s(mp);b5=r(T9,"PreTrainedModel"),T9.forEach(o),k5=r(M_,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),M_.forEach(o),v5=_(Go),Ii=d(Go,"P",{});var z_=s(Ii);w5=r(z_,"This model is also a PyTorch "),Oi=d(z_,"A",{href:!0,rel:!0});var $9=s(Oi);y5=r($9,"torch.nn.Module"),$9.forEach(o),T5=r(z_,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),z_.forEach(o),$5=_(Go),eo=d(Go,"DIV",{class:!0});var Xa=s(eo);C(Di.$$.fragment,Xa),x5=_(Xa),vn=d(Xa,"P",{});var Gp=s(vn);F5=r(Gp,"The "),fp=d(Gp,"A",{href:!0});var x9=s(fp);C5=r(x9,"RobertaForCausalLM"),x9.forEach(o),R5=r(Gp," forward method, overrides the "),Xh=d(Gp,"CODE",{});var F9=s(Xh);E5=r(F9,"__call__"),F9.forEach(o),M5=r(Gp," special method."),Gp.forEach(o),z5=_(Xa),C(Es.$$.fragment,Xa),q5=_(Xa),C(Ms.$$.fragment,Xa),Xa.forEach(o),Go.forEach(o),$g=_(i),wn=d(i,"H2",{class:!0});var q_=s(wn);zs=d(q_,"A",{id:!0,class:!0,href:!0});var C9=s(zs);eu=d(C9,"SPAN",{});var R9=s(eu);C(Si.$$.fragment,R9),R9.forEach(o),C9.forEach(o),j5=_(q_),tu=d(q_,"SPAN",{});var E9=s(tu);A5=r(E9,"RobertaForMaskedLM"),E9.forEach(o),q_.forEach(o),xg=_(i),Rt=d(i,"DIV",{class:!0});var Yo=s(Rt);C(Hi.$$.fragment,Yo),P5=_(Yo),Vi=d(Yo,"P",{});var j_=s(Vi);L5=r(j_,"RoBERTa Model with a "),ou=d(j_,"CODE",{});var M9=s(ou);N5=r(M9,"language modeling"),M9.forEach(o),I5=r(j_," head on top."),j_.forEach(o),O5=_(Yo),Bi=d(Yo,"P",{});var A_=s(Bi);D5=r(A_,"This model inherits from "),gp=d(A_,"A",{href:!0});var z9=s(gp);S5=r(z9,"PreTrainedModel"),z9.forEach(o),H5=r(A_,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),A_.forEach(o),V5=_(Yo),Wi=d(Yo,"P",{});var P_=s(Wi);B5=r(P_,"This model is also a PyTorch "),Ui=d(P_,"A",{href:!0,rel:!0});var q9=s(Ui);W5=r(q9,"torch.nn.Module"),q9.forEach(o),U5=r(P_,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),P_.forEach(o),Q5=_(Yo),Lt=d(Yo,"DIV",{class:!0});var Xo=s(Lt);C(Qi.$$.fragment,Xo),Z5=_(Xo),yn=d(Xo,"P",{});var Yp=s(yn);K5=r(Yp,"The "),_p=d(Yp,"A",{href:!0});var j9=s(_p);J5=r(j9,"RobertaForMaskedLM"),j9.forEach(o),G5=r(Yp," forward method, overrides the "),nu=d(Yp,"CODE",{});var A9=s(nu);Y5=r(A9,"__call__"),A9.forEach(o),X5=r(Yp," special method."),Yp.forEach(o),e4=_(Xo),C(qs.$$.fragment,Xo),t4=_(Xo),C(js.$$.fragment,Xo),o4=_(Xo),C(As.$$.fragment,Xo),Xo.forEach(o),Yo.forEach(o),Fg=_(i),Tn=d(i,"H2",{class:!0});var L_=s(Tn);Ps=d(L_,"A",{id:!0,class:!0,href:!0});var P9=s(Ps);su=d(P9,"SPAN",{});var L9=s(su);C(Zi.$$.fragment,L9),L9.forEach(o),P9.forEach(o),n4=_(L_),au=d(L_,"SPAN",{});var N9=s(au);s4=r(N9,"RobertaForSequenceClassification"),N9.forEach(o),L_.forEach(o),Cg=_(i),Et=d(i,"DIV",{class:!0});var en=s(Et);C(Ki.$$.fragment,en),a4=_(en),ru=d(en,"P",{});var I9=s(ru);r4=r(I9,`RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),I9.forEach(o),i4=_(en),Ji=d(en,"P",{});var N_=s(Ji);l4=r(N_,"This model inherits from "),bp=d(N_,"A",{href:!0});var O9=s(bp);d4=r(O9,"PreTrainedModel"),O9.forEach(o),c4=r(N_,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),N_.forEach(o),p4=_(en),Gi=d(en,"P",{});var I_=s(Gi);h4=r(I_,"This model is also a PyTorch "),Yi=d(I_,"A",{href:!0,rel:!0});var D9=s(Yi);u4=r(D9,"torch.nn.Module"),D9.forEach(o),m4=r(I_,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),I_.forEach(o),f4=_(en),ft=d(en,"DIV",{class:!0});var Bt=s(ft);C(Xi.$$.fragment,Bt),g4=_(Bt),$n=d(Bt,"P",{});var Xp=s($n);_4=r(Xp,"The "),kp=d(Xp,"A",{href:!0});var S9=s(kp);b4=r(S9,"RobertaForSequenceClassification"),S9.forEach(o),k4=r(Xp," forward method, overrides the "),iu=d(Xp,"CODE",{});var H9=s(iu);v4=r(H9,"__call__"),H9.forEach(o),w4=r(Xp," special method."),Xp.forEach(o),y4=_(Bt),C(Ls.$$.fragment,Bt),T4=_(Bt),C(Ns.$$.fragment,Bt),$4=_(Bt),C(Is.$$.fragment,Bt),x4=_(Bt),C(Os.$$.fragment,Bt),F4=_(Bt),C(Ds.$$.fragment,Bt),Bt.forEach(o),en.forEach(o),Rg=_(i),xn=d(i,"H2",{class:!0});var O_=s(xn);Ss=d(O_,"A",{id:!0,class:!0,href:!0});var V9=s(Ss);lu=d(V9,"SPAN",{});var B9=s(lu);C(el.$$.fragment,B9),B9.forEach(o),V9.forEach(o),C4=_(O_),du=d(O_,"SPAN",{});var W9=s(du);R4=r(W9,"RobertaForMultipleChoice"),W9.forEach(o),O_.forEach(o),Eg=_(i),Mt=d(i,"DIV",{class:!0});var tn=s(Mt);C(tl.$$.fragment,tn),E4=_(tn),cu=d(tn,"P",{});var U9=s(cu);M4=r(U9,`Roberta Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),U9.forEach(o),z4=_(tn),ol=d(tn,"P",{});var D_=s(ol);q4=r(D_,"This model inherits from "),vp=d(D_,"A",{href:!0});var Q9=s(vp);j4=r(Q9,"PreTrainedModel"),Q9.forEach(o),A4=r(D_,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),D_.forEach(o),P4=_(tn),nl=d(tn,"P",{});var S_=s(nl);L4=r(S_,"This model is also a PyTorch "),sl=d(S_,"A",{href:!0,rel:!0});var Z9=s(sl);N4=r(Z9,"torch.nn.Module"),Z9.forEach(o),I4=r(S_,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),S_.forEach(o),O4=_(tn),to=d(tn,"DIV",{class:!0});var er=s(to);C(al.$$.fragment,er),D4=_(er),Fn=d(er,"P",{});var eh=s(Fn);S4=r(eh,"The "),wp=d(eh,"A",{href:!0});var K9=s(wp);H4=r(K9,"RobertaForMultipleChoice"),K9.forEach(o),V4=r(eh," forward method, overrides the "),pu=d(eh,"CODE",{});var J9=s(pu);B4=r(J9,"__call__"),J9.forEach(o),W4=r(eh," special method."),eh.forEach(o),U4=_(er),C(Hs.$$.fragment,er),Q4=_(er),C(Vs.$$.fragment,er),er.forEach(o),tn.forEach(o),Mg=_(i),Cn=d(i,"H2",{class:!0});var H_=s(Cn);Bs=d(H_,"A",{id:!0,class:!0,href:!0});var G9=s(Bs);hu=d(G9,"SPAN",{});var Y9=s(hu);C(rl.$$.fragment,Y9),Y9.forEach(o),G9.forEach(o),Z4=_(H_),uu=d(H_,"SPAN",{});var X9=s(uu);K4=r(X9,"RobertaForTokenClassification"),X9.forEach(o),H_.forEach(o),zg=_(i),zt=d(i,"DIV",{class:!0});var on=s(zt);C(il.$$.fragment,on),J4=_(on),mu=d(on,"P",{});var ex=s(mu);G4=r(ex,`Roberta Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),ex.forEach(o),Y4=_(on),ll=d(on,"P",{});var V_=s(ll);X4=r(V_,"This model inherits from "),yp=d(V_,"A",{href:!0});var tx=s(yp);ev=r(tx,"PreTrainedModel"),tx.forEach(o),tv=r(V_,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),V_.forEach(o),ov=_(on),dl=d(on,"P",{});var B_=s(dl);nv=r(B_,"This model is also a PyTorch "),cl=d(B_,"A",{href:!0,rel:!0});var ox=s(cl);sv=r(ox,"torch.nn.Module"),ox.forEach(o),av=r(B_,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),B_.forEach(o),rv=_(on),Nt=d(on,"DIV",{class:!0});var nn=s(Nt);C(pl.$$.fragment,nn),iv=_(nn),Rn=d(nn,"P",{});var th=s(Rn);lv=r(th,"The "),Tp=d(th,"A",{href:!0});var nx=s(Tp);dv=r(nx,"RobertaForTokenClassification"),nx.forEach(o),cv=r(th," forward method, overrides the "),fu=d(th,"CODE",{});var sx=s(fu);pv=r(sx,"__call__"),sx.forEach(o),hv=r(th," special method."),th.forEach(o),uv=_(nn),C(Ws.$$.fragment,nn),mv=_(nn),C(Us.$$.fragment,nn),fv=_(nn),C(Qs.$$.fragment,nn),nn.forEach(o),on.forEach(o),qg=_(i),En=d(i,"H2",{class:!0});var W_=s(En);Zs=d(W_,"A",{id:!0,class:!0,href:!0});var ax=s(Zs);gu=d(ax,"SPAN",{});var rx=s(gu);C(hl.$$.fragment,rx),rx.forEach(o),ax.forEach(o),gv=_(W_),_u=d(W_,"SPAN",{});var ix=s(_u);_v=r(ix,"RobertaForQuestionAnswering"),ix.forEach(o),W_.forEach(o),jg=_(i),qt=d(i,"DIV",{class:!0});var sn=s(qt);C(ul.$$.fragment,sn),bv=_(sn),Mn=d(sn,"P",{});var oh=s(Mn);kv=r(oh,`Roberta Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),bu=d(oh,"CODE",{});var lx=s(bu);vv=r(lx,"span start logits"),lx.forEach(o),wv=r(oh," and "),ku=d(oh,"CODE",{});var dx=s(ku);yv=r(dx,"span end logits"),dx.forEach(o),Tv=r(oh,")."),oh.forEach(o),$v=_(sn),ml=d(sn,"P",{});var U_=s(ml);xv=r(U_,"This model inherits from "),$p=d(U_,"A",{href:!0});var cx=s($p);Fv=r(cx,"PreTrainedModel"),cx.forEach(o),Cv=r(U_,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),U_.forEach(o),Rv=_(sn),fl=d(sn,"P",{});var Q_=s(fl);Ev=r(Q_,"This model is also a PyTorch "),gl=d(Q_,"A",{href:!0,rel:!0});var px=s(gl);Mv=r(px,"torch.nn.Module"),px.forEach(o),zv=r(Q_,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Q_.forEach(o),qv=_(sn),It=d(sn,"DIV",{class:!0});var an=s(It);C(_l.$$.fragment,an),jv=_(an),zn=d(an,"P",{});var nh=s(zn);Av=r(nh,"The "),xp=d(nh,"A",{href:!0});var hx=s(xp);Pv=r(hx,"RobertaForQuestionAnswering"),hx.forEach(o),Lv=r(nh," forward method, overrides the "),vu=d(nh,"CODE",{});var ux=s(vu);Nv=r(ux,"__call__"),ux.forEach(o),Iv=r(nh," special method."),nh.forEach(o),Ov=_(an),C(Ks.$$.fragment,an),Dv=_(an),C(Js.$$.fragment,an),Sv=_(an),C(Gs.$$.fragment,an),an.forEach(o),sn.forEach(o),Ag=_(i),qn=d(i,"H2",{class:!0});var Z_=s(qn);Ys=d(Z_,"A",{id:!0,class:!0,href:!0});var mx=s(Ys);wu=d(mx,"SPAN",{});var fx=s(wu);C(bl.$$.fragment,fx),fx.forEach(o),mx.forEach(o),Hv=_(Z_),yu=d(Z_,"SPAN",{});var gx=s(yu);Vv=r(gx,"TFRobertaModel"),gx.forEach(o),Z_.forEach(o),Pg=_(i),_t=d(i,"DIV",{class:!0});var uo=s(_t);C(kl.$$.fragment,uo),Bv=_(uo),Tu=d(uo,"P",{});var _x=s(Tu);Wv=r(_x,"The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top."),_x.forEach(o),Uv=_(uo),vl=d(uo,"P",{});var K_=s(vl);Qv=r(K_,"This model inherits from "),Fp=d(K_,"A",{href:!0});var bx=s(Fp);Zv=r(bx,"TFPreTrainedModel"),bx.forEach(o),Kv=r(K_,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),K_.forEach(o),Jv=_(uo),wl=d(uo,"P",{});var J_=s(wl);Gv=r(J_,"This model is also a "),yl=d(J_,"A",{href:!0,rel:!0});var kx=s(yl);Yv=r(kx,"tf.keras.Model"),kx.forEach(o),Xv=r(J_,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),J_.forEach(o),ew=_(uo),C(Xs.$$.fragment,uo),tw=_(uo),oo=d(uo,"DIV",{class:!0});var tr=s(oo);C(Tl.$$.fragment,tr),ow=_(tr),jn=d(tr,"P",{});var sh=s(jn);nw=r(sh,"The "),Cp=d(sh,"A",{href:!0});var vx=s(Cp);sw=r(vx,"TFRobertaModel"),vx.forEach(o),aw=r(sh," forward method, overrides the "),$u=d(sh,"CODE",{});var wx=s($u);rw=r(wx,"__call__"),wx.forEach(o),iw=r(sh," special method."),sh.forEach(o),lw=_(tr),C(ea.$$.fragment,tr),dw=_(tr),C(ta.$$.fragment,tr),tr.forEach(o),uo.forEach(o),Lg=_(i),An=d(i,"H2",{class:!0});var G_=s(An);oa=d(G_,"A",{id:!0,class:!0,href:!0});var yx=s(oa);xu=d(yx,"SPAN",{});var Tx=s(xu);C($l.$$.fragment,Tx),Tx.forEach(o),yx.forEach(o),cw=_(G_),Fu=d(G_,"SPAN",{});var $x=s(Fu);pw=r($x,"TFRobertaForCausalLM"),$x.forEach(o),G_.forEach(o),Ng=_(i),Pn=d(i,"DIV",{class:!0});var Y_=s(Pn);C(xl.$$.fragment,Y_),hw=_(Y_),no=d(Y_,"DIV",{class:!0});var or=s(no);C(Fl.$$.fragment,or),uw=_(or),Ln=d(or,"P",{});var ah=s(Ln);mw=r(ah,"The "),Rp=d(ah,"A",{href:!0});var xx=s(Rp);fw=r(xx,"TFRobertaForCausalLM"),xx.forEach(o),gw=r(ah," forward method, overrides the "),Cu=d(ah,"CODE",{});var Fx=s(Cu);_w=r(Fx,"__call__"),Fx.forEach(o),bw=r(ah," special method."),ah.forEach(o),kw=_(or),C(na.$$.fragment,or),vw=_(or),C(sa.$$.fragment,or),or.forEach(o),Y_.forEach(o),Ig=_(i),Nn=d(i,"H2",{class:!0});var X_=s(Nn);aa=d(X_,"A",{id:!0,class:!0,href:!0});var Cx=s(aa);Ru=d(Cx,"SPAN",{});var Rx=s(Ru);C(Cl.$$.fragment,Rx),Rx.forEach(o),Cx.forEach(o),ww=_(X_),Eu=d(X_,"SPAN",{});var Ex=s(Eu);yw=r(Ex,"TFRobertaForMaskedLM"),Ex.forEach(o),X_.forEach(o),Og=_(i),bt=d(i,"DIV",{class:!0});var mo=s(bt);C(Rl.$$.fragment,mo),Tw=_(mo),El=d(mo,"P",{});var e1=s(El);$w=r(e1,"RoBERTa Model with a "),Mu=d(e1,"CODE",{});var Mx=s(Mu);xw=r(Mx,"language modeling"),Mx.forEach(o),Fw=r(e1," head on top."),e1.forEach(o),Cw=_(mo),Ml=d(mo,"P",{});var t1=s(Ml);Rw=r(t1,"This model inherits from "),Ep=d(t1,"A",{href:!0});var zx=s(Ep);Ew=r(zx,"TFPreTrainedModel"),zx.forEach(o),Mw=r(t1,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),t1.forEach(o),zw=_(mo),zl=d(mo,"P",{});var o1=s(zl);qw=r(o1,"This model is also a "),ql=d(o1,"A",{href:!0,rel:!0});var qx=s(ql);jw=r(qx,"tf.keras.Model"),qx.forEach(o),Aw=r(o1,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),o1.forEach(o),Pw=_(mo),C(ra.$$.fragment,mo),Lw=_(mo),Ot=d(mo,"DIV",{class:!0});var rn=s(Ot);C(jl.$$.fragment,rn),Nw=_(rn),In=d(rn,"P",{});var rh=s(In);Iw=r(rh,"The "),Mp=d(rh,"A",{href:!0});var jx=s(Mp);Ow=r(jx,"TFRobertaForMaskedLM"),jx.forEach(o),Dw=r(rh," forward method, overrides the "),zu=d(rh,"CODE",{});var Ax=s(zu);Sw=r(Ax,"__call__"),Ax.forEach(o),Hw=r(rh," special method."),rh.forEach(o),Vw=_(rn),C(ia.$$.fragment,rn),Bw=_(rn),C(la.$$.fragment,rn),Ww=_(rn),C(da.$$.fragment,rn),rn.forEach(o),mo.forEach(o),Dg=_(i),On=d(i,"H2",{class:!0});var n1=s(On);ca=d(n1,"A",{id:!0,class:!0,href:!0});var Px=s(ca);qu=d(Px,"SPAN",{});var Lx=s(qu);C(Al.$$.fragment,Lx),Lx.forEach(o),Px.forEach(o),Uw=_(n1),ju=d(n1,"SPAN",{});var Nx=s(ju);Qw=r(Nx,"TFRobertaForSequenceClassification"),Nx.forEach(o),n1.forEach(o),Sg=_(i),kt=d(i,"DIV",{class:!0});var fo=s(kt);C(Pl.$$.fragment,fo),Zw=_(fo),Au=d(fo,"P",{});var Ix=s(Au);Kw=r(Ix,`RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),Ix.forEach(o),Jw=_(fo),Ll=d(fo,"P",{});var s1=s(Ll);Gw=r(s1,"This model inherits from "),zp=d(s1,"A",{href:!0});var Ox=s(zp);Yw=r(Ox,"TFPreTrainedModel"),Ox.forEach(o),Xw=r(s1,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),s1.forEach(o),e3=_(fo),Nl=d(fo,"P",{});var a1=s(Nl);t3=r(a1,"This model is also a "),Il=d(a1,"A",{href:!0,rel:!0});var Dx=s(Il);o3=r(Dx,"tf.keras.Model"),Dx.forEach(o),n3=r(a1,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),a1.forEach(o),s3=_(fo),C(pa.$$.fragment,fo),a3=_(fo),Dt=d(fo,"DIV",{class:!0});var ln=s(Dt);C(Ol.$$.fragment,ln),r3=_(ln),Dn=d(ln,"P",{});var ih=s(Dn);i3=r(ih,"The "),qp=d(ih,"A",{href:!0});var Sx=s(qp);l3=r(Sx,"TFRobertaForSequenceClassification"),Sx.forEach(o),d3=r(ih," forward method, overrides the "),Pu=d(ih,"CODE",{});var Hx=s(Pu);c3=r(Hx,"__call__"),Hx.forEach(o),p3=r(ih," special method."),ih.forEach(o),h3=_(ln),C(ha.$$.fragment,ln),u3=_(ln),C(ua.$$.fragment,ln),m3=_(ln),C(ma.$$.fragment,ln),ln.forEach(o),fo.forEach(o),Hg=_(i),Sn=d(i,"H2",{class:!0});var r1=s(Sn);fa=d(r1,"A",{id:!0,class:!0,href:!0});var Vx=s(fa);Lu=d(Vx,"SPAN",{});var Bx=s(Lu);C(Dl.$$.fragment,Bx),Bx.forEach(o),Vx.forEach(o),f3=_(r1),Nu=d(r1,"SPAN",{});var Wx=s(Nu);g3=r(Wx,"TFRobertaForMultipleChoice"),Wx.forEach(o),r1.forEach(o),Vg=_(i),vt=d(i,"DIV",{class:!0});var go=s(vt);C(Sl.$$.fragment,go),_3=_(go),Iu=d(go,"P",{});var Ux=s(Iu);b3=r(Ux,`Roberta Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),Ux.forEach(o),k3=_(go),Hl=d(go,"P",{});var i1=s(Hl);v3=r(i1,"This model inherits from "),jp=d(i1,"A",{href:!0});var Qx=s(jp);w3=r(Qx,"TFPreTrainedModel"),Qx.forEach(o),y3=r(i1,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),i1.forEach(o),T3=_(go),Vl=d(go,"P",{});var l1=s(Vl);$3=r(l1,"This model is also a "),Bl=d(l1,"A",{href:!0,rel:!0});var Zx=s(Bl);x3=r(Zx,"tf.keras.Model"),Zx.forEach(o),F3=r(l1,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),l1.forEach(o),C3=_(go),C(ga.$$.fragment,go),R3=_(go),so=d(go,"DIV",{class:!0});var nr=s(so);C(Wl.$$.fragment,nr),E3=_(nr),Hn=d(nr,"P",{});var lh=s(Hn);M3=r(lh,"The "),Ap=d(lh,"A",{href:!0});var Kx=s(Ap);z3=r(Kx,"TFRobertaForMultipleChoice"),Kx.forEach(o),q3=r(lh," forward method, overrides the "),Ou=d(lh,"CODE",{});var Jx=s(Ou);j3=r(Jx,"__call__"),Jx.forEach(o),A3=r(lh," special method."),lh.forEach(o),P3=_(nr),C(_a.$$.fragment,nr),L3=_(nr),C(ba.$$.fragment,nr),nr.forEach(o),go.forEach(o),Bg=_(i),Vn=d(i,"H2",{class:!0});var d1=s(Vn);ka=d(d1,"A",{id:!0,class:!0,href:!0});var Gx=s(ka);Du=d(Gx,"SPAN",{});var Yx=s(Du);C(Ul.$$.fragment,Yx),Yx.forEach(o),Gx.forEach(o),N3=_(d1),Su=d(d1,"SPAN",{});var Xx=s(Su);I3=r(Xx,"TFRobertaForTokenClassification"),Xx.forEach(o),d1.forEach(o),Wg=_(i),wt=d(i,"DIV",{class:!0});var _o=s(wt);C(Ql.$$.fragment,_o),O3=_(_o),Hu=d(_o,"P",{});var eF=s(Hu);D3=r(eF,`RoBERTa Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),eF.forEach(o),S3=_(_o),Zl=d(_o,"P",{});var c1=s(Zl);H3=r(c1,"This model inherits from "),Pp=d(c1,"A",{href:!0});var tF=s(Pp);V3=r(tF,"TFPreTrainedModel"),tF.forEach(o),B3=r(c1,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),c1.forEach(o),W3=_(_o),Kl=d(_o,"P",{});var p1=s(Kl);U3=r(p1,"This model is also a "),Jl=d(p1,"A",{href:!0,rel:!0});var oF=s(Jl);Q3=r(oF,"tf.keras.Model"),oF.forEach(o),Z3=r(p1,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),p1.forEach(o),K3=_(_o),C(va.$$.fragment,_o),J3=_(_o),St=d(_o,"DIV",{class:!0});var dn=s(St);C(Gl.$$.fragment,dn),G3=_(dn),Bn=d(dn,"P",{});var dh=s(Bn);Y3=r(dh,"The "),Lp=d(dh,"A",{href:!0});var nF=s(Lp);X3=r(nF,"TFRobertaForTokenClassification"),nF.forEach(o),ey=r(dh," forward method, overrides the "),Vu=d(dh,"CODE",{});var sF=s(Vu);ty=r(sF,"__call__"),sF.forEach(o),oy=r(dh," special method."),dh.forEach(o),ny=_(dn),C(wa.$$.fragment,dn),sy=_(dn),C(ya.$$.fragment,dn),ay=_(dn),C(Ta.$$.fragment,dn),dn.forEach(o),_o.forEach(o),Ug=_(i),Wn=d(i,"H2",{class:!0});var h1=s(Wn);$a=d(h1,"A",{id:!0,class:!0,href:!0});var aF=s($a);Bu=d(aF,"SPAN",{});var rF=s(Bu);C(Yl.$$.fragment,rF),rF.forEach(o),aF.forEach(o),ry=_(h1),Wu=d(h1,"SPAN",{});var iF=s(Wu);iy=r(iF,"TFRobertaForQuestionAnswering"),iF.forEach(o),h1.forEach(o),Qg=_(i),yt=d(i,"DIV",{class:!0});var bo=s(yt);C(Xl.$$.fragment,bo),ly=_(bo),Un=d(bo,"P",{});var ch=s(Un);dy=r(ch,`RoBERTa Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),Uu=d(ch,"CODE",{});var lF=s(Uu);cy=r(lF,"span start logits"),lF.forEach(o),py=r(ch," and "),Qu=d(ch,"CODE",{});var dF=s(Qu);hy=r(dF,"span end logits"),dF.forEach(o),uy=r(ch,")."),ch.forEach(o),my=_(bo),ed=d(bo,"P",{});var u1=s(ed);fy=r(u1,"This model inherits from "),Np=d(u1,"A",{href:!0});var cF=s(Np);gy=r(cF,"TFPreTrainedModel"),cF.forEach(o),_y=r(u1,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),u1.forEach(o),by=_(bo),td=d(bo,"P",{});var m1=s(td);ky=r(m1,"This model is also a "),od=d(m1,"A",{href:!0,rel:!0});var pF=s(od);vy=r(pF,"tf.keras.Model"),pF.forEach(o),wy=r(m1,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),m1.forEach(o),yy=_(bo),C(xa.$$.fragment,bo),Ty=_(bo),Ht=d(bo,"DIV",{class:!0});var cn=s(Ht);C(nd.$$.fragment,cn),$y=_(cn),Qn=d(cn,"P",{});var ph=s(Qn);xy=r(ph,"The "),Ip=d(ph,"A",{href:!0});var hF=s(Ip);Fy=r(hF,"TFRobertaForQuestionAnswering"),hF.forEach(o),Cy=r(ph," forward method, overrides the "),Zu=d(ph,"CODE",{});var uF=s(Zu);Ry=r(uF,"__call__"),uF.forEach(o),Ey=r(ph," special method."),ph.forEach(o),My=_(cn),C(Fa.$$.fragment,cn),zy=_(cn),C(Ca.$$.fragment,cn),qy=_(cn),C(Ra.$$.fragment,cn),cn.forEach(o),bo.forEach(o),Zg=_(i),Zn=d(i,"H2",{class:!0});var f1=s(Zn);Ea=d(f1,"A",{id:!0,class:!0,href:!0});var mF=s(Ea);Ku=d(mF,"SPAN",{});var fF=s(Ku);C(sd.$$.fragment,fF),fF.forEach(o),mF.forEach(o),jy=_(f1),Ju=d(f1,"SPAN",{});var gF=s(Ju);Ay=r(gF,"FlaxRobertaModel"),gF.forEach(o),f1.forEach(o),Kg=_(i),lt=d(i,"DIV",{class:!0});var Wt=s(lt);C(ad.$$.fragment,Wt),Py=_(Wt),Gu=d(Wt,"P",{});var _F=s(Gu);Ly=r(_F,"The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top."),_F.forEach(o),Ny=_(Wt),rd=d(Wt,"P",{});var g1=s(rd);Iy=r(g1,"This model inherits from "),Op=d(g1,"A",{href:!0});var bF=s(Op);Oy=r(bF,"FlaxPreTrainedModel"),bF.forEach(o),Dy=r(g1,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),g1.forEach(o),Sy=_(Wt),id=d(Wt,"P",{});var _1=s(id);Hy=r(_1,"This model is also a Flax Linen "),ld=d(_1,"A",{href:!0,rel:!0});var kF=s(ld);Vy=r(kF,"flax.linen.Module"),kF.forEach(o),By=r(_1,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),_1.forEach(o),Wy=_(Wt),Yu=d(Wt,"P",{});var vF=s(Yu);Uy=r(vF,"Finally, this model supports inherent JAX features such as:"),vF.forEach(o),Qy=_(Wt),vo=d(Wt,"UL",{});var sr=s(vo);Xu=d(sr,"LI",{});var wF=s(Xu);dd=d(wF,"A",{href:!0,rel:!0});var yF=s(dd);Zy=r(yF,"Just-In-Time (JIT) compilation"),yF.forEach(o),wF.forEach(o),Ky=_(sr),em=d(sr,"LI",{});var TF=s(em);cd=d(TF,"A",{href:!0,rel:!0});var $F=s(cd);Jy=r($F,"Automatic Differentiation"),$F.forEach(o),TF.forEach(o),Gy=_(sr),tm=d(sr,"LI",{});var xF=s(tm);pd=d(xF,"A",{href:!0,rel:!0});var FF=s(pd);Yy=r(FF,"Vectorization"),FF.forEach(o),xF.forEach(o),Xy=_(sr),om=d(sr,"LI",{});var CF=s(om);hd=d(CF,"A",{href:!0,rel:!0});var RF=s(hd);e6=r(RF,"Parallelization"),RF.forEach(o),CF.forEach(o),sr.forEach(o),t6=_(Wt),ao=d(Wt,"DIV",{class:!0});var ar=s(ao);C(ud.$$.fragment,ar),o6=_(ar),Kn=d(ar,"P",{});var hh=s(Kn);n6=r(hh,"The "),nm=d(hh,"CODE",{});var EF=s(nm);s6=r(EF,"FlaxRobertaPreTrainedModel"),EF.forEach(o),a6=r(hh," forward method, overrides the "),sm=d(hh,"CODE",{});var MF=s(sm);r6=r(MF,"__call__"),MF.forEach(o),i6=r(hh," special method."),hh.forEach(o),l6=_(ar),C(Ma.$$.fragment,ar),d6=_(ar),C(za.$$.fragment,ar),ar.forEach(o),Wt.forEach(o),Jg=_(i),Jn=d(i,"H2",{class:!0});var b1=s(Jn);qa=d(b1,"A",{id:!0,class:!0,href:!0});var zF=s(qa);am=d(zF,"SPAN",{});var qF=s(am);C(md.$$.fragment,qF),qF.forEach(o),zF.forEach(o),c6=_(b1),rm=d(b1,"SPAN",{});var jF=s(rm);p6=r(jF,"FlaxRobertaForCausalLM"),jF.forEach(o),b1.forEach(o),Gg=_(i),dt=d(i,"DIV",{class:!0});var Ut=s(dt);C(fd.$$.fragment,Ut),h6=_(Ut),im=d(Ut,"P",{});var AF=s(im);u6=r(AF,`Roberta Model with a language modeling head on top (a linear layer on top of the hidden-states output) e.g for
autoregressive tasks.`),AF.forEach(o),m6=_(Ut),gd=d(Ut,"P",{});var k1=s(gd);f6=r(k1,"This model inherits from "),Dp=d(k1,"A",{href:!0});var PF=s(Dp);g6=r(PF,"FlaxPreTrainedModel"),PF.forEach(o),_6=r(k1,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),k1.forEach(o),b6=_(Ut),_d=d(Ut,"P",{});var v1=s(_d);k6=r(v1,"This model is also a Flax Linen "),bd=d(v1,"A",{href:!0,rel:!0});var LF=s(bd);v6=r(LF,"flax.linen.Module"),LF.forEach(o),w6=r(v1,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),v1.forEach(o),y6=_(Ut),lm=d(Ut,"P",{});var NF=s(lm);T6=r(NF,"Finally, this model supports inherent JAX features such as:"),NF.forEach(o),$6=_(Ut),wo=d(Ut,"UL",{});var rr=s(wo);dm=d(rr,"LI",{});var IF=s(dm);kd=d(IF,"A",{href:!0,rel:!0});var OF=s(kd);x6=r(OF,"Just-In-Time (JIT) compilation"),OF.forEach(o),IF.forEach(o),F6=_(rr),cm=d(rr,"LI",{});var DF=s(cm);vd=d(DF,"A",{href:!0,rel:!0});var SF=s(vd);C6=r(SF,"Automatic Differentiation"),SF.forEach(o),DF.forEach(o),R6=_(rr),pm=d(rr,"LI",{});var HF=s(pm);wd=d(HF,"A",{href:!0,rel:!0});var VF=s(wd);E6=r(VF,"Vectorization"),VF.forEach(o),HF.forEach(o),M6=_(rr),hm=d(rr,"LI",{});var BF=s(hm);yd=d(BF,"A",{href:!0,rel:!0});var WF=s(yd);z6=r(WF,"Parallelization"),WF.forEach(o),BF.forEach(o),rr.forEach(o),q6=_(Ut),ro=d(Ut,"DIV",{class:!0});var ir=s(ro);C(Td.$$.fragment,ir),j6=_(ir),Gn=d(ir,"P",{});var uh=s(Gn);A6=r(uh,"The "),um=d(uh,"CODE",{});var UF=s(um);P6=r(UF,"FlaxRobertaPreTrainedModel"),UF.forEach(o),L6=r(uh," forward method, overrides the "),mm=d(uh,"CODE",{});var QF=s(mm);N6=r(QF,"__call__"),QF.forEach(o),I6=r(uh," special method."),uh.forEach(o),O6=_(ir),C(ja.$$.fragment,ir),D6=_(ir),C(Aa.$$.fragment,ir),ir.forEach(o),Ut.forEach(o),Yg=_(i),Yn=d(i,"H2",{class:!0});var w1=s(Yn);Pa=d(w1,"A",{id:!0,class:!0,href:!0});var ZF=s(Pa);fm=d(ZF,"SPAN",{});var KF=s(fm);C($d.$$.fragment,KF),KF.forEach(o),ZF.forEach(o),S6=_(w1),gm=d(w1,"SPAN",{});var JF=s(gm);H6=r(JF,"FlaxRobertaForMaskedLM"),JF.forEach(o),w1.forEach(o),Xg=_(i),ct=d(i,"DIV",{class:!0});var Qt=s(ct);C(xd.$$.fragment,Qt),V6=_(Qt),Fd=d(Qt,"P",{});var y1=s(Fd);B6=r(y1,"RoBERTa Model with a "),_m=d(y1,"CODE",{});var GF=s(_m);W6=r(GF,"language modeling"),GF.forEach(o),U6=r(y1," head on top."),y1.forEach(o),Q6=_(Qt),Cd=d(Qt,"P",{});var T1=s(Cd);Z6=r(T1,"This model inherits from "),Sp=d(T1,"A",{href:!0});var YF=s(Sp);K6=r(YF,"FlaxPreTrainedModel"),YF.forEach(o),J6=r(T1,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),T1.forEach(o),G6=_(Qt),Rd=d(Qt,"P",{});var $1=s(Rd);Y6=r($1,"This model is also a Flax Linen "),Ed=d($1,"A",{href:!0,rel:!0});var XF=s(Ed);X6=r(XF,"flax.linen.Module"),XF.forEach(o),e7=r($1,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),$1.forEach(o),t7=_(Qt),bm=d(Qt,"P",{});var eC=s(bm);o7=r(eC,"Finally, this model supports inherent JAX features such as:"),eC.forEach(o),n7=_(Qt),yo=d(Qt,"UL",{});var lr=s(yo);km=d(lr,"LI",{});var tC=s(km);Md=d(tC,"A",{href:!0,rel:!0});var oC=s(Md);s7=r(oC,"Just-In-Time (JIT) compilation"),oC.forEach(o),tC.forEach(o),a7=_(lr),vm=d(lr,"LI",{});var nC=s(vm);zd=d(nC,"A",{href:!0,rel:!0});var sC=s(zd);r7=r(sC,"Automatic Differentiation"),sC.forEach(o),nC.forEach(o),i7=_(lr),wm=d(lr,"LI",{});var aC=s(wm);qd=d(aC,"A",{href:!0,rel:!0});var rC=s(qd);l7=r(rC,"Vectorization"),rC.forEach(o),aC.forEach(o),d7=_(lr),ym=d(lr,"LI",{});var iC=s(ym);jd=d(iC,"A",{href:!0,rel:!0});var lC=s(jd);c7=r(lC,"Parallelization"),lC.forEach(o),iC.forEach(o),lr.forEach(o),p7=_(Qt),io=d(Qt,"DIV",{class:!0});var dr=s(io);C(Ad.$$.fragment,dr),h7=_(dr),Xn=d(dr,"P",{});var mh=s(Xn);u7=r(mh,"The "),Tm=d(mh,"CODE",{});var dC=s(Tm);m7=r(dC,"FlaxRobertaPreTrainedModel"),dC.forEach(o),f7=r(mh," forward method, overrides the "),$m=d(mh,"CODE",{});var cC=s($m);g7=r(cC,"__call__"),cC.forEach(o),_7=r(mh," special method."),mh.forEach(o),b7=_(dr),C(La.$$.fragment,dr),k7=_(dr),C(Na.$$.fragment,dr),dr.forEach(o),Qt.forEach(o),e_=_(i),es=d(i,"H2",{class:!0});var x1=s(es);Ia=d(x1,"A",{id:!0,class:!0,href:!0});var pC=s(Ia);xm=d(pC,"SPAN",{});var hC=s(xm);C(Pd.$$.fragment,hC),hC.forEach(o),pC.forEach(o),v7=_(x1),Fm=d(x1,"SPAN",{});var uC=s(Fm);w7=r(uC,"FlaxRobertaForSequenceClassification"),uC.forEach(o),x1.forEach(o),t_=_(i),pt=d(i,"DIV",{class:!0});var Zt=s(pt);C(Ld.$$.fragment,Zt),y7=_(Zt),Cm=d(Zt,"P",{});var mC=s(Cm);T7=r(mC,`Roberta Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),mC.forEach(o),$7=_(Zt),Nd=d(Zt,"P",{});var F1=s(Nd);x7=r(F1,"This model inherits from "),Hp=d(F1,"A",{href:!0});var fC=s(Hp);F7=r(fC,"FlaxPreTrainedModel"),fC.forEach(o),C7=r(F1,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),F1.forEach(o),R7=_(Zt),Id=d(Zt,"P",{});var C1=s(Id);E7=r(C1,"This model is also a Flax Linen "),Od=d(C1,"A",{href:!0,rel:!0});var gC=s(Od);M7=r(gC,"flax.linen.Module"),gC.forEach(o),z7=r(C1,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),C1.forEach(o),q7=_(Zt),Rm=d(Zt,"P",{});var _C=s(Rm);j7=r(_C,"Finally, this model supports inherent JAX features such as:"),_C.forEach(o),A7=_(Zt),To=d(Zt,"UL",{});var cr=s(To);Em=d(cr,"LI",{});var bC=s(Em);Dd=d(bC,"A",{href:!0,rel:!0});var kC=s(Dd);P7=r(kC,"Just-In-Time (JIT) compilation"),kC.forEach(o),bC.forEach(o),L7=_(cr),Mm=d(cr,"LI",{});var vC=s(Mm);Sd=d(vC,"A",{href:!0,rel:!0});var wC=s(Sd);N7=r(wC,"Automatic Differentiation"),wC.forEach(o),vC.forEach(o),I7=_(cr),zm=d(cr,"LI",{});var yC=s(zm);Hd=d(yC,"A",{href:!0,rel:!0});var TC=s(Hd);O7=r(TC,"Vectorization"),TC.forEach(o),yC.forEach(o),D7=_(cr),qm=d(cr,"LI",{});var $C=s(qm);Vd=d($C,"A",{href:!0,rel:!0});var xC=s(Vd);S7=r(xC,"Parallelization"),xC.forEach(o),$C.forEach(o),cr.forEach(o),H7=_(Zt),lo=d(Zt,"DIV",{class:!0});var pr=s(lo);C(Bd.$$.fragment,pr),V7=_(pr),ts=d(pr,"P",{});var fh=s(ts);B7=r(fh,"The "),jm=d(fh,"CODE",{});var FC=s(jm);W7=r(FC,"FlaxRobertaPreTrainedModel"),FC.forEach(o),U7=r(fh," forward method, overrides the "),Am=d(fh,"CODE",{});var CC=s(Am);Q7=r(CC,"__call__"),CC.forEach(o),Z7=r(fh," special method."),fh.forEach(o),K7=_(pr),C(Oa.$$.fragment,pr),J7=_(pr),C(Da.$$.fragment,pr),pr.forEach(o),Zt.forEach(o),o_=_(i),os=d(i,"H2",{class:!0});var R1=s(os);Sa=d(R1,"A",{id:!0,class:!0,href:!0});var RC=s(Sa);Pm=d(RC,"SPAN",{});var EC=s(Pm);C(Wd.$$.fragment,EC),EC.forEach(o),RC.forEach(o),G7=_(R1),Lm=d(R1,"SPAN",{});var MC=s(Lm);Y7=r(MC,"FlaxRobertaForMultipleChoice"),MC.forEach(o),R1.forEach(o),n_=_(i),ht=d(i,"DIV",{class:!0});var Kt=s(ht);C(Ud.$$.fragment,Kt),X7=_(Kt),Nm=d(Kt,"P",{});var zC=s(Nm);eT=r(zC,`Roberta Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),zC.forEach(o),tT=_(Kt),Qd=d(Kt,"P",{});var E1=s(Qd);oT=r(E1,"This model inherits from "),Vp=d(E1,"A",{href:!0});var qC=s(Vp);nT=r(qC,"FlaxPreTrainedModel"),qC.forEach(o),sT=r(E1,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),E1.forEach(o),aT=_(Kt),Zd=d(Kt,"P",{});var M1=s(Zd);rT=r(M1,"This model is also a Flax Linen "),Kd=d(M1,"A",{href:!0,rel:!0});var jC=s(Kd);iT=r(jC,"flax.linen.Module"),jC.forEach(o),lT=r(M1,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),M1.forEach(o),dT=_(Kt),Im=d(Kt,"P",{});var AC=s(Im);cT=r(AC,"Finally, this model supports inherent JAX features such as:"),AC.forEach(o),pT=_(Kt),$o=d(Kt,"UL",{});var hr=s($o);Om=d(hr,"LI",{});var PC=s(Om);Jd=d(PC,"A",{href:!0,rel:!0});var LC=s(Jd);hT=r(LC,"Just-In-Time (JIT) compilation"),LC.forEach(o),PC.forEach(o),uT=_(hr),Dm=d(hr,"LI",{});var NC=s(Dm);Gd=d(NC,"A",{href:!0,rel:!0});var IC=s(Gd);mT=r(IC,"Automatic Differentiation"),IC.forEach(o),NC.forEach(o),fT=_(hr),Sm=d(hr,"LI",{});var OC=s(Sm);Yd=d(OC,"A",{href:!0,rel:!0});var DC=s(Yd);gT=r(DC,"Vectorization"),DC.forEach(o),OC.forEach(o),_T=_(hr),Hm=d(hr,"LI",{});var SC=s(Hm);Xd=d(SC,"A",{href:!0,rel:!0});var HC=s(Xd);bT=r(HC,"Parallelization"),HC.forEach(o),SC.forEach(o),hr.forEach(o),kT=_(Kt),co=d(Kt,"DIV",{class:!0});var ur=s(co);C(ec.$$.fragment,ur),vT=_(ur),ns=d(ur,"P",{});var gh=s(ns);wT=r(gh,"The "),Vm=d(gh,"CODE",{});var VC=s(Vm);yT=r(VC,"FlaxRobertaPreTrainedModel"),VC.forEach(o),TT=r(gh," forward method, overrides the "),Bm=d(gh,"CODE",{});var BC=s(Bm);$T=r(BC,"__call__"),BC.forEach(o),xT=r(gh," special method."),gh.forEach(o),FT=_(ur),C(Ha.$$.fragment,ur),CT=_(ur),C(Va.$$.fragment,ur),ur.forEach(o),Kt.forEach(o),s_=_(i),ss=d(i,"H2",{class:!0});var z1=s(ss);Ba=d(z1,"A",{id:!0,class:!0,href:!0});var WC=s(Ba);Wm=d(WC,"SPAN",{});var UC=s(Wm);C(tc.$$.fragment,UC),UC.forEach(o),WC.forEach(o),RT=_(z1),Um=d(z1,"SPAN",{});var QC=s(Um);ET=r(QC,"FlaxRobertaForTokenClassification"),QC.forEach(o),z1.forEach(o),a_=_(i),ut=d(i,"DIV",{class:!0});var Jt=s(ut);C(oc.$$.fragment,Jt),MT=_(Jt),Qm=d(Jt,"P",{});var ZC=s(Qm);zT=r(ZC,`Roberta Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),ZC.forEach(o),qT=_(Jt),nc=d(Jt,"P",{});var q1=s(nc);jT=r(q1,"This model inherits from "),Bp=d(q1,"A",{href:!0});var KC=s(Bp);AT=r(KC,"FlaxPreTrainedModel"),KC.forEach(o),PT=r(q1,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),q1.forEach(o),LT=_(Jt),sc=d(Jt,"P",{});var j1=s(sc);NT=r(j1,"This model is also a Flax Linen "),ac=d(j1,"A",{href:!0,rel:!0});var JC=s(ac);IT=r(JC,"flax.linen.Module"),JC.forEach(o),OT=r(j1,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),j1.forEach(o),DT=_(Jt),Zm=d(Jt,"P",{});var GC=s(Zm);ST=r(GC,"Finally, this model supports inherent JAX features such as:"),GC.forEach(o),HT=_(Jt),xo=d(Jt,"UL",{});var mr=s(xo);Km=d(mr,"LI",{});var YC=s(Km);rc=d(YC,"A",{href:!0,rel:!0});var XC=s(rc);VT=r(XC,"Just-In-Time (JIT) compilation"),XC.forEach(o),YC.forEach(o),BT=_(mr),Jm=d(mr,"LI",{});var eR=s(Jm);ic=d(eR,"A",{href:!0,rel:!0});var tR=s(ic);WT=r(tR,"Automatic Differentiation"),tR.forEach(o),eR.forEach(o),UT=_(mr),Gm=d(mr,"LI",{});var oR=s(Gm);lc=d(oR,"A",{href:!0,rel:!0});var nR=s(lc);QT=r(nR,"Vectorization"),nR.forEach(o),oR.forEach(o),ZT=_(mr),Ym=d(mr,"LI",{});var sR=s(Ym);dc=d(sR,"A",{href:!0,rel:!0});var aR=s(dc);KT=r(aR,"Parallelization"),aR.forEach(o),sR.forEach(o),mr.forEach(o),JT=_(Jt),po=d(Jt,"DIV",{class:!0});var fr=s(po);C(cc.$$.fragment,fr),GT=_(fr),as=d(fr,"P",{});var _h=s(as);YT=r(_h,"The "),Xm=d(_h,"CODE",{});var rR=s(Xm);XT=r(rR,"FlaxRobertaPreTrainedModel"),rR.forEach(o),e8=r(_h," forward method, overrides the "),ef=d(_h,"CODE",{});var iR=s(ef);t8=r(iR,"__call__"),iR.forEach(o),o8=r(_h," special method."),_h.forEach(o),n8=_(fr),C(Wa.$$.fragment,fr),s8=_(fr),C(Ua.$$.fragment,fr),fr.forEach(o),Jt.forEach(o),r_=_(i),rs=d(i,"H2",{class:!0});var A1=s(rs);Qa=d(A1,"A",{id:!0,class:!0,href:!0});var lR=s(Qa);tf=d(lR,"SPAN",{});var dR=s(tf);C(pc.$$.fragment,dR),dR.forEach(o),lR.forEach(o),a8=_(A1),of=d(A1,"SPAN",{});var cR=s(of);r8=r(cR,"FlaxRobertaForQuestionAnswering"),cR.forEach(o),A1.forEach(o),i_=_(i),mt=d(i,"DIV",{class:!0});var Gt=s(mt);C(hc.$$.fragment,Gt),i8=_(Gt),is=d(Gt,"P",{});var bh=s(is);l8=r(bh,`Roberta Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),nf=d(bh,"CODE",{});var pR=s(nf);d8=r(pR,"span start logits"),pR.forEach(o),c8=r(bh," and "),sf=d(bh,"CODE",{});var hR=s(sf);p8=r(hR,"span end logits"),hR.forEach(o),h8=r(bh,")."),bh.forEach(o),u8=_(Gt),uc=d(Gt,"P",{});var P1=s(uc);m8=r(P1,"This model inherits from "),Wp=d(P1,"A",{href:!0});var uR=s(Wp);f8=r(uR,"FlaxPreTrainedModel"),uR.forEach(o),g8=r(P1,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),P1.forEach(o),_8=_(Gt),mc=d(Gt,"P",{});var L1=s(mc);b8=r(L1,"This model is also a Flax Linen "),fc=d(L1,"A",{href:!0,rel:!0});var mR=s(fc);k8=r(mR,"flax.linen.Module"),mR.forEach(o),v8=r(L1,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),L1.forEach(o),w8=_(Gt),af=d(Gt,"P",{});var fR=s(af);y8=r(fR,"Finally, this model supports inherent JAX features such as:"),fR.forEach(o),T8=_(Gt),Fo=d(Gt,"UL",{});var gr=s(Fo);rf=d(gr,"LI",{});var gR=s(rf);gc=d(gR,"A",{href:!0,rel:!0});var _R=s(gc);$8=r(_R,"Just-In-Time (JIT) compilation"),_R.forEach(o),gR.forEach(o),x8=_(gr),lf=d(gr,"LI",{});var bR=s(lf);_c=d(bR,"A",{href:!0,rel:!0});var kR=s(_c);F8=r(kR,"Automatic Differentiation"),kR.forEach(o),bR.forEach(o),C8=_(gr),df=d(gr,"LI",{});var vR=s(df);bc=d(vR,"A",{href:!0,rel:!0});var wR=s(bc);R8=r(wR,"Vectorization"),wR.forEach(o),vR.forEach(o),E8=_(gr),cf=d(gr,"LI",{});var yR=s(cf);kc=d(yR,"A",{href:!0,rel:!0});var TR=s(kc);M8=r(TR,"Parallelization"),TR.forEach(o),yR.forEach(o),gr.forEach(o),z8=_(Gt),ho=d(Gt,"DIV",{class:!0});var _r=s(ho);C(vc.$$.fragment,_r),q8=_(_r),ls=d(_r,"P",{});var kh=s(ls);j8=r(kh,"The "),pf=d(kh,"CODE",{});var $R=s(pf);A8=r($R,"FlaxRobertaPreTrainedModel"),$R.forEach(o),P8=r(kh," forward method, overrides the "),hf=d(kh,"CODE",{});var xR=s(hf);L8=r(xR,"__call__"),xR.forEach(o),N8=r(kh," special method."),kh.forEach(o),I8=_(_r),C(Za.$$.fragment,_r),O8=_(_r),C(Ka.$$.fragment,_r),_r.forEach(o),Gt.forEach(o),this.h()},h(){n(t,"name","hf:doc:metadata"),n(t,"content",JSON.stringify(Ez)),n(h,"id","roberta"),n(h,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(h,"href","#roberta"),n(c,"class","relative group"),n(H,"id","overview"),n(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(H,"href","#overview"),n(L,"class","relative group"),n(be,"href","https://arxiv.org/abs/1907.11692"),n(be,"rel","nofollow"),n(O,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertModel"),n(Ke,"href","camembert"),n(Ue,"href","https://huggingface.co/julien-c"),n(Ue,"rel","nofollow"),n(Qe,"href","https://github.com/pytorch/fairseq/tree/master/examples/roberta"),n(Qe,"rel","nofollow"),n(Y,"id","resources"),n(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(Y,"href","#resources"),n(He,"class","relative group"),n(kr,"href","https://huggingface.co/blog/sentiment-analysis-twitter"),n(kr,"rel","nofollow"),n(vr,"href","https://huggingface.co/inference-api"),n(vr,"rel","nofollow"),n(yr,"href","https://huggingface.co/blog/opinion-classification-with-kili"),n(yr,"rel","nofollow"),n($r,"href","https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb"),n($r,"rel","nofollow"),n(Lc,"href","/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),n(xr,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification"),n(xr,"rel","nofollow"),n(Fr,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb"),n(Fr,"rel","nofollow"),n(Nc,"href","/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),n(Cr,"href","https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification"),n(Cr,"rel","nofollow"),n(Rr,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb"),n(Rr,"rel","nofollow"),n(Ic,"href","/docs/transformers/main/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification"),n(Er,"href","https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification"),n(Er,"rel","nofollow"),n(Mr,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb"),n(Mr,"rel","nofollow"),n(Oc,"href","/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForTokenClassification"),n(qr,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification"),n(qr,"rel","nofollow"),n(jr,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb"),n(jr,"rel","nofollow"),n(Dc,"href","/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),n(Ar,"href","https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification"),n(Ar,"rel","nofollow"),n(Pr,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb"),n(Pr,"rel","nofollow"),n(Sc,"href","/docs/transformers/main/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification"),n(Lr,"href","https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification"),n(Lr,"rel","nofollow"),n(Nr,"href","https://huggingface.co/course/chapter7/2?fw=pt"),n(Nr,"rel","nofollow"),n(Dr,"href","https://huggingface.co/blog/how-to-train"),n(Dr,"rel","nofollow"),n(Vc,"href","/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForMaskedLM"),n(Sr,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling"),n(Sr,"rel","nofollow"),n(Hr,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb"),n(Hr,"rel","nofollow"),n(Bc,"href","/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),n(Vr,"href","https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy"),n(Vr,"rel","nofollow"),n(Br,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb"),n(Br,"rel","nofollow"),n(Wc,"href","/docs/transformers/main/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),n(Wr,"href","https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#masked-language-modeling"),n(Wr,"rel","nofollow"),n(Ur,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb"),n(Ur,"rel","nofollow"),n(Qr,"href","https://huggingface.co/course/chapter7/3?fw=pt"),n(Qr,"rel","nofollow"),n(Jr,"href","https://huggingface.co/blog/optimum-inference"),n(Jr,"rel","nofollow"),n(Qc,"href","/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),n(Gr,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering"),n(Gr,"rel","nofollow"),n(Yr,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb"),n(Yr,"rel","nofollow"),n(Zc,"href","/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),n(Xr,"href","https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering"),n(Xr,"rel","nofollow"),n(ei,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb"),n(ei,"rel","nofollow"),n(Kc,"href","/docs/transformers/main/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering"),n(ti,"href","https://github.com/huggingface/transformers/tree/main/examples/flax/question-answering"),n(ti,"rel","nofollow"),n(oi,"href","https://huggingface.co/course/chapter7/7?fw=pt"),n(oi,"rel","nofollow"),n(Yc,"href","/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),n(ni,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice"),n(ni,"rel","nofollow"),n(si,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb"),n(si,"rel","nofollow"),n(Xc,"href","/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),n(ai,"href","https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice"),n(ai,"rel","nofollow"),n(ri,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb"),n(ri,"rel","nofollow"),n(ms,"id","transformers.RobertaConfig"),n(ms,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(ms,"href","#transformers.RobertaConfig"),n(hn,"class","relative group"),n(ep,"href","/docs/transformers/main/en/model_doc/roberta#transformers.RobertaModel"),n(tp,"href","/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaModel"),n(di,"href","https://huggingface.co/roberta-base"),n(di,"rel","nofollow"),n(op,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),n(np,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),n(sp,"href","/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig"),n(ap,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertConfig"),n(Ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(gs,"id","transformers.RobertaTokenizer"),n(gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(gs,"href","#transformers.RobertaTokenizer"),n(fn,"class","relative group"),n(rp,"href","/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),n(Ho,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(ks,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(vs,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(dp,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(Be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(ws,"id","transformers.RobertaTokenizerFast"),n(ws,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(ws,"href","#transformers.RobertaTokenizerFast"),n(gn,"class","relative group"),n(cp,"href","/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),n(pp,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(rt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n($s,"id","transformers.RobertaModel"),n($s,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n($s,"href","#transformers.RobertaModel"),n(_n,"class","relative group"),n(hp,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),n(Mi,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),n(Mi,"rel","nofollow"),n(qi,"href","https://arxiv.org/abs/1706.03762"),n(qi,"rel","nofollow"),n(up,"href","/docs/transformers/main/en/model_doc/roberta#transformers.RobertaModel"),n(Xt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(it,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(Rs,"id","transformers.RobertaForCausalLM"),n(Rs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(Rs,"href","#transformers.RobertaForCausalLM"),n(kn,"class","relative group"),n(mp,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),n(Oi,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),n(Oi,"rel","nofollow"),n(fp,"href","/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForCausalLM"),n(eo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(Ct,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(zs,"id","transformers.RobertaForMaskedLM"),n(zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(zs,"href","#transformers.RobertaForMaskedLM"),n(wn,"class","relative group"),n(gp,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),n(Ui,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),n(Ui,"rel","nofollow"),n(_p,"href","/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForMaskedLM"),n(Lt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(Rt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(Ps,"id","transformers.RobertaForSequenceClassification"),n(Ps,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(Ps,"href","#transformers.RobertaForSequenceClassification"),n(Tn,"class","relative group"),n(bp,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),n(Yi,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),n(Yi,"rel","nofollow"),n(kp,"href","/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),n(ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(Et,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(Ss,"id","transformers.RobertaForMultipleChoice"),n(Ss,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(Ss,"href","#transformers.RobertaForMultipleChoice"),n(xn,"class","relative group"),n(vp,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),n(sl,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),n(sl,"rel","nofollow"),n(wp,"href","/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),n(to,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(Mt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(Bs,"id","transformers.RobertaForTokenClassification"),n(Bs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(Bs,"href","#transformers.RobertaForTokenClassification"),n(Cn,"class","relative group"),n(yp,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),n(cl,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),n(cl,"rel","nofollow"),n(Tp,"href","/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForTokenClassification"),n(Nt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(zt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(Zs,"id","transformers.RobertaForQuestionAnswering"),n(Zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(Zs,"href","#transformers.RobertaForQuestionAnswering"),n(En,"class","relative group"),n($p,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),n(gl,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),n(gl,"rel","nofollow"),n(xp,"href","/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),n(It,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(qt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(Ys,"id","transformers.TFRobertaModel"),n(Ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(Ys,"href","#transformers.TFRobertaModel"),n(qn,"class","relative group"),n(Fp,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),n(yl,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),n(yl,"rel","nofollow"),n(Cp,"href","/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaModel"),n(oo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(_t,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(oa,"id","transformers.TFRobertaForCausalLM"),n(oa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(oa,"href","#transformers.TFRobertaForCausalLM"),n(An,"class","relative group"),n(Rp,"href","/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForCausalLM"),n(no,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(Pn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(aa,"id","transformers.TFRobertaForMaskedLM"),n(aa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(aa,"href","#transformers.TFRobertaForMaskedLM"),n(Nn,"class","relative group"),n(Ep,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),n(ql,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),n(ql,"rel","nofollow"),n(Mp,"href","/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),n(Ot,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(bt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(ca,"id","transformers.TFRobertaForSequenceClassification"),n(ca,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(ca,"href","#transformers.TFRobertaForSequenceClassification"),n(On,"class","relative group"),n(zp,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),n(Il,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),n(Il,"rel","nofollow"),n(qp,"href","/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),n(Dt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(kt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(fa,"id","transformers.TFRobertaForMultipleChoice"),n(fa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(fa,"href","#transformers.TFRobertaForMultipleChoice"),n(Sn,"class","relative group"),n(jp,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),n(Bl,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),n(Bl,"rel","nofollow"),n(Ap,"href","/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),n(so,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(vt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(ka,"id","transformers.TFRobertaForTokenClassification"),n(ka,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(ka,"href","#transformers.TFRobertaForTokenClassification"),n(Vn,"class","relative group"),n(Pp,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),n(Jl,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),n(Jl,"rel","nofollow"),n(Lp,"href","/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),n(St,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(wt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n($a,"id","transformers.TFRobertaForQuestionAnswering"),n($a,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n($a,"href","#transformers.TFRobertaForQuestionAnswering"),n(Wn,"class","relative group"),n(Np,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),n(od,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),n(od,"rel","nofollow"),n(Ip,"href","/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),n(Ht,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(yt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(Ea,"id","transformers.FlaxRobertaModel"),n(Ea,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(Ea,"href","#transformers.FlaxRobertaModel"),n(Zn,"class","relative group"),n(Op,"href","/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel"),n(ld,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),n(ld,"rel","nofollow"),n(dd,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),n(dd,"rel","nofollow"),n(cd,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),n(cd,"rel","nofollow"),n(pd,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),n(pd,"rel","nofollow"),n(hd,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),n(hd,"rel","nofollow"),n(ao,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(lt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(qa,"id","transformers.FlaxRobertaForCausalLM"),n(qa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(qa,"href","#transformers.FlaxRobertaForCausalLM"),n(Jn,"class","relative group"),n(Dp,"href","/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel"),n(bd,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),n(bd,"rel","nofollow"),n(kd,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),n(kd,"rel","nofollow"),n(vd,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),n(vd,"rel","nofollow"),n(wd,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),n(wd,"rel","nofollow"),n(yd,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),n(yd,"rel","nofollow"),n(ro,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(dt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(Pa,"id","transformers.FlaxRobertaForMaskedLM"),n(Pa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(Pa,"href","#transformers.FlaxRobertaForMaskedLM"),n(Yn,"class","relative group"),n(Sp,"href","/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel"),n(Ed,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),n(Ed,"rel","nofollow"),n(Md,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),n(Md,"rel","nofollow"),n(zd,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),n(zd,"rel","nofollow"),n(qd,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),n(qd,"rel","nofollow"),n(jd,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),n(jd,"rel","nofollow"),n(io,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(ct,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(Ia,"id","transformers.FlaxRobertaForSequenceClassification"),n(Ia,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(Ia,"href","#transformers.FlaxRobertaForSequenceClassification"),n(es,"class","relative group"),n(Hp,"href","/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel"),n(Od,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),n(Od,"rel","nofollow"),n(Dd,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),n(Dd,"rel","nofollow"),n(Sd,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),n(Sd,"rel","nofollow"),n(Hd,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),n(Hd,"rel","nofollow"),n(Vd,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),n(Vd,"rel","nofollow"),n(lo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(Sa,"id","transformers.FlaxRobertaForMultipleChoice"),n(Sa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(Sa,"href","#transformers.FlaxRobertaForMultipleChoice"),n(os,"class","relative group"),n(Vp,"href","/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel"),n(Kd,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),n(Kd,"rel","nofollow"),n(Jd,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),n(Jd,"rel","nofollow"),n(Gd,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),n(Gd,"rel","nofollow"),n(Yd,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),n(Yd,"rel","nofollow"),n(Xd,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),n(Xd,"rel","nofollow"),n(co,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(ht,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(Ba,"id","transformers.FlaxRobertaForTokenClassification"),n(Ba,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(Ba,"href","#transformers.FlaxRobertaForTokenClassification"),n(ss,"class","relative group"),n(Bp,"href","/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel"),n(ac,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),n(ac,"rel","nofollow"),n(rc,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),n(rc,"rel","nofollow"),n(ic,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),n(ic,"rel","nofollow"),n(lc,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),n(lc,"rel","nofollow"),n(dc,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),n(dc,"rel","nofollow"),n(po,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(Qa,"id","transformers.FlaxRobertaForQuestionAnswering"),n(Qa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(Qa,"href","#transformers.FlaxRobertaForQuestionAnswering"),n(rs,"class","relative group"),n(Wp,"href","/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel"),n(fc,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),n(fc,"rel","nofollow"),n(gc,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),n(gc,"rel","nofollow"),n(_c,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),n(_c,"rel","nofollow"),n(bc,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),n(bc,"rel","nofollow"),n(kc,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),n(kc,"rel","nofollow"),n(ho,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),n(mt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(i,v){e(document.head,t),b(i,u,v),b(i,c,v),e(c,h),e(h,f),T(p,f,null),e(c,m),e(c,R),e(R,I),b(i,P,v),b(i,L,v),e(L,H),e(H,V),T(E,V,null),e(L,ie),e(L,U),e(U,Z),b(i,Te,v),b(i,G,v),e(G,Ee),e(G,be),e(be,oe),e(G,Me),b(i,$e,v),b(i,X,v),e(X,ze),b(i,xe,v),b(i,ee,v),e(ee,qe),b(i,Fe,v),b(i,le,v),e(le,N),e(N,D),b(i,Ce,v),b(i,te,v),e(te,je),b(i,Re,v),b(i,Q,v),e(Q,ke),e(ke,Ae),e(ke,O),e(O,Pe),e(ke,ne),e(Q,Le),e(Q,ve),e(ve,se),e(Q,Ne),e(Q,de),e(de,K),e(de,we),e(we,ae),e(de,Ie),e(de,ce),e(ce,S),e(de,Oe),e(de,B),e(B,De),e(de,Se),e(Q,w),e(Q,q),e(q,Ke),e(Ke,ue),e(q,ot),b(i,Ye,v),b(i,A,v),e(A,nt),e(A,Ue),e(Ue,st),e(A,at),e(A,Qe),e(Qe,J),e(A,re),b(i,Xe,v),b(i,He,v),e(He,Y),e(Y,Je),T(Ze,Je,null),e(He,Ve),e(He,Ge),e(Ge,me),b(i,et,v),b(i,Pc,v),e(Pc,I1),b(i,sg,v),T(br,i,v),b(i,ag,v),b(i,Tt,v),e(Tt,pn),e(pn,O1),e(pn,kr),e(kr,D1),e(pn,S1),e(pn,vr),e(vr,H1),e(pn,V1),e(Tt,B1),e(Tt,wr),e(wr,W1),e(wr,yr),e(yr,U1),e(wr,Q1),e(Tt,Z1),e(Tt,Tr),e(Tr,K1),e(Tr,$r),e($r,J1),e(Tr,G1),e(Tt,Y1),e(Tt,Mo),e(Mo,Lc),e(Lc,X1),e(Mo,e2),e(Mo,xr),e(xr,t2),e(Mo,o2),e(Mo,Fr),e(Fr,n2),e(Mo,s2),e(Tt,a2),e(Tt,zo),e(zo,Nc),e(Nc,r2),e(zo,i2),e(zo,Cr),e(Cr,l2),e(zo,d2),e(zo,Rr),e(Rr,c2),e(zo,p2),e(Tt,h2),e(Tt,qo),e(qo,Ic),e(Ic,u2),e(qo,m2),e(qo,Er),e(Er,f2),e(qo,g2),e(qo,Mr),e(Mr,_2),e(qo,b2),b(i,rg,v),T(zr,i,v),b(i,ig,v),b(i,Yt,v),e(Yt,jo),e(jo,Oc),e(Oc,k2),e(jo,v2),e(jo,qr),e(qr,w2),e(jo,y2),e(jo,jr),e(jr,T2),e(jo,$2),e(Yt,x2),e(Yt,Ao),e(Ao,Dc),e(Dc,F2),e(Ao,C2),e(Ao,Ar),e(Ar,R2),e(Ao,E2),e(Ao,Pr),e(Pr,M2),e(Ao,z2),e(Yt,q2),e(Yt,ps),e(ps,Sc),e(Sc,j2),e(ps,A2),e(ps,Lr),e(Lr,P2),e(ps,L2),e(Yt,N2),e(Yt,Hc),e(Hc,Nr),e(Nr,I2),e(Hc,O2),b(i,lg,v),T(Ir,i,v),b(i,dg,v),b(i,At,v),e(At,Or),e(Or,D2),e(Or,Dr),e(Dr,S2),e(Or,H2),e(At,V2),e(At,Po),e(Po,Vc),e(Vc,B2),e(Po,W2),e(Po,Sr),e(Sr,U2),e(Po,Q2),e(Po,Hr),e(Hr,Z2),e(Po,K2),e(At,J2),e(At,Lo),e(Lo,Bc),e(Bc,G2),e(Lo,Y2),e(Lo,Vr),e(Vr,X2),e(Lo,eb),e(Lo,Br),e(Br,tb),e(Lo,ob),e(At,nb),e(At,No),e(No,Wc),e(Wc,sb),e(No,ab),e(No,Wr),e(Wr,rb),e(No,ib),e(No,Ur),e(Ur,lb),e(No,db),e(At,cb),e(At,Uc),e(Uc,Qr),e(Qr,pb),e(Uc,hb),b(i,cg,v),T(Zr,i,v),b(i,pg,v),b(i,Pt,v),e(Pt,Kr),e(Kr,ub),e(Kr,Jr),e(Jr,mb),e(Kr,fb),e(Pt,gb),e(Pt,Io),e(Io,Qc),e(Qc,_b),e(Io,bb),e(Io,Gr),e(Gr,kb),e(Io,vb),e(Io,Yr),e(Yr,wb),e(Io,yb),e(Pt,Tb),e(Pt,Oo),e(Oo,Zc),e(Zc,$b),e(Oo,xb),e(Oo,Xr),e(Xr,Fb),e(Oo,Cb),e(Oo,ei),e(ei,Rb),e(Oo,Eb),e(Pt,Mb),e(Pt,hs),e(hs,Kc),e(Kc,zb),e(hs,qb),e(hs,ti),e(ti,jb),e(hs,Ab),e(Pt,Pb),e(Pt,Jc),e(Jc,oi),e(oi,Lb),e(Jc,Nb),b(i,hg,v),b(i,Gc,v),e(Gc,vh),e(vh,Ib),b(i,ug,v),b(i,us,v),e(us,Do),e(Do,Yc),e(Yc,Ob),e(Do,Db),e(Do,ni),e(ni,Sb),e(Do,Hb),e(Do,si),e(si,Vb),e(Do,Bb),e(us,Wb),e(us,So),e(So,Xc),e(Xc,Ub),e(So,Qb),e(So,ai),e(ai,Zb),e(So,Kb),e(So,ri),e(ri,Jb),e(So,Gb),b(i,mg,v),b(i,hn,v),e(hn,ms),e(ms,wh),T(ii,wh,null),e(hn,Yb),e(hn,yh),e(yh,Xb),b(i,fg,v),b(i,Ft,v),T(li,Ft,null),e(Ft,e0),e(Ft,ko),e(ko,t0),e(ko,ep),e(ep,o0),e(ko,n0),e(ko,tp),e(tp,s0),e(ko,a0),e(ko,di),e(di,r0),e(ko,i0),e(Ft,l0),e(Ft,un),e(un,d0),e(un,op),e(op,c0),e(un,p0),e(un,np),e(np,h0),e(un,u0),e(Ft,m0),e(Ft,mn),e(mn,f0),e(mn,sp),e(sp,g0),e(mn,_0),e(mn,ap),e(ap,b0),e(mn,k0),e(Ft,v0),T(fs,Ft,null),b(i,gg,v),b(i,fn,v),e(fn,gs),e(gs,Th),T(ci,Th,null),e(fn,w0),e(fn,$h),e($h,y0),b(i,_g,v),b(i,Be,v),T(pi,Be,null),e(Be,T0),e(Be,xh),e(xh,$0),e(Be,x0),e(Be,Fh),e(Fh,F0),e(Be,C0),T(_s,Be,null),e(Be,R0),e(Be,hi),e(hi,E0),e(hi,Ch),e(Ch,M0),e(hi,z0),e(Be,q0),T(bs,Be,null),e(Be,j0),e(Be,ui),e(ui,A0),e(ui,rp),e(rp,P0),e(ui,L0),e(Be,N0),e(Be,Ho),T(mi,Ho,null),e(Ho,I0),e(Ho,Rh),e(Rh,O0),e(Ho,D0),e(Ho,fi),e(fi,ip),e(ip,S0),e(ip,Eh),e(Eh,H0),e(fi,V0),e(fi,lp),e(lp,B0),e(lp,Mh),e(Mh,W0),e(Be,U0),e(Be,ks),T(gi,ks,null),e(ks,Q0),e(ks,_i),e(_i,Z0),e(_i,zh),e(zh,K0),e(_i,J0),e(Be,G0),e(Be,vs),T(bi,vs,null),e(vs,Y0),e(vs,qh),e(qh,X0),e(Be,ek),e(Be,dp),T(ki,dp,null),b(i,bg,v),b(i,gn,v),e(gn,ws),e(ws,jh),T(vi,jh,null),e(gn,tk),e(gn,Ah),e(Ah,ok),b(i,kg,v),b(i,rt,v),T(wi,rt,null),e(rt,nk),e(rt,yi),e(yi,sk),e(yi,Ph),e(Ph,ak),e(yi,rk),e(rt,ik),e(rt,Lh),e(Lh,lk),e(rt,dk),T(ys,rt,null),e(rt,ck),e(rt,Ti),e(Ti,pk),e(Ti,Nh),e(Nh,hk),e(Ti,uk),e(rt,mk),T(Ts,rt,null),e(rt,fk),e(rt,$i),e($i,gk),e($i,cp),e(cp,_k),e($i,bk),e(rt,kk),e(rt,pp),T(xi,pp,null),b(i,vg,v),b(i,_n,v),e(_n,$s),e($s,Ih),T(Fi,Ih,null),e(_n,vk),e(_n,Oh),e(Oh,wk),b(i,wg,v),b(i,it,v),T(Ci,it,null),e(it,yk),e(it,Dh),e(Dh,Tk),e(it,$k),e(it,Ri),e(Ri,xk),e(Ri,hp),e(hp,Fk),e(Ri,Ck),e(it,Rk),e(it,Ei),e(Ei,Ek),e(Ei,Mi),e(Mi,Mk),e(Ei,zk),e(it,qk),e(it,zi),e(zi,jk),e(zi,Sh),e(Sh,Ak),e(zi,Pk),e(it,Lk),e(it,gt),e(gt,Nk),e(gt,Hh),e(Hh,Ik),e(gt,Ok),e(gt,Vh),e(Vh,Dk),e(gt,Sk),e(gt,Bh),e(Bh,Hk),e(gt,Vk),e(gt,Wh),e(Wh,Bk),e(gt,Wk),e(gt,Uh),e(Uh,Uk),e(gt,Qk),e(gt,Qh),e(Qh,Zk),e(gt,Kk),e(it,Jk),e(it,xs),e(xs,Gk),e(xs,Zh),e(Zh,Yk),e(xs,Xk),e(xs,qi),e(qi,e5),e(it,t5),e(it,Xt),T(ji,Xt,null),e(Xt,o5),e(Xt,bn),e(bn,n5),e(bn,up),e(up,s5),e(bn,a5),e(bn,Kh),e(Kh,r5),e(bn,i5),e(Xt,l5),T(Fs,Xt,null),e(Xt,d5),T(Cs,Xt,null),b(i,yg,v),b(i,kn,v),e(kn,Rs),e(Rs,Jh),T(Ai,Jh,null),e(kn,c5),e(kn,Gh),e(Gh,p5),b(i,Tg,v),b(i,Ct,v),T(Pi,Ct,null),e(Ct,h5),e(Ct,Li),e(Li,u5),e(Li,Yh),e(Yh,m5),e(Li,f5),e(Ct,g5),e(Ct,Ni),e(Ni,_5),e(Ni,mp),e(mp,b5),e(Ni,k5),e(Ct,v5),e(Ct,Ii),e(Ii,w5),e(Ii,Oi),e(Oi,y5),e(Ii,T5),e(Ct,$5),e(Ct,eo),T(Di,eo,null),e(eo,x5),e(eo,vn),e(vn,F5),e(vn,fp),e(fp,C5),e(vn,R5),e(vn,Xh),e(Xh,E5),e(vn,M5),e(eo,z5),T(Es,eo,null),e(eo,q5),T(Ms,eo,null),b(i,$g,v),b(i,wn,v),e(wn,zs),e(zs,eu),T(Si,eu,null),e(wn,j5),e(wn,tu),e(tu,A5),b(i,xg,v),b(i,Rt,v),T(Hi,Rt,null),e(Rt,P5),e(Rt,Vi),e(Vi,L5),e(Vi,ou),e(ou,N5),e(Vi,I5),e(Rt,O5),e(Rt,Bi),e(Bi,D5),e(Bi,gp),e(gp,S5),e(Bi,H5),e(Rt,V5),e(Rt,Wi),e(Wi,B5),e(Wi,Ui),e(Ui,W5),e(Wi,U5),e(Rt,Q5),e(Rt,Lt),T(Qi,Lt,null),e(Lt,Z5),e(Lt,yn),e(yn,K5),e(yn,_p),e(_p,J5),e(yn,G5),e(yn,nu),e(nu,Y5),e(yn,X5),e(Lt,e4),T(qs,Lt,null),e(Lt,t4),T(js,Lt,null),e(Lt,o4),T(As,Lt,null),b(i,Fg,v),b(i,Tn,v),e(Tn,Ps),e(Ps,su),T(Zi,su,null),e(Tn,n4),e(Tn,au),e(au,s4),b(i,Cg,v),b(i,Et,v),T(Ki,Et,null),e(Et,a4),e(Et,ru),e(ru,r4),e(Et,i4),e(Et,Ji),e(Ji,l4),e(Ji,bp),e(bp,d4),e(Ji,c4),e(Et,p4),e(Et,Gi),e(Gi,h4),e(Gi,Yi),e(Yi,u4),e(Gi,m4),e(Et,f4),e(Et,ft),T(Xi,ft,null),e(ft,g4),e(ft,$n),e($n,_4),e($n,kp),e(kp,b4),e($n,k4),e($n,iu),e(iu,v4),e($n,w4),e(ft,y4),T(Ls,ft,null),e(ft,T4),T(Ns,ft,null),e(ft,$4),T(Is,ft,null),e(ft,x4),T(Os,ft,null),e(ft,F4),T(Ds,ft,null),b(i,Rg,v),b(i,xn,v),e(xn,Ss),e(Ss,lu),T(el,lu,null),e(xn,C4),e(xn,du),e(du,R4),b(i,Eg,v),b(i,Mt,v),T(tl,Mt,null),e(Mt,E4),e(Mt,cu),e(cu,M4),e(Mt,z4),e(Mt,ol),e(ol,q4),e(ol,vp),e(vp,j4),e(ol,A4),e(Mt,P4),e(Mt,nl),e(nl,L4),e(nl,sl),e(sl,N4),e(nl,I4),e(Mt,O4),e(Mt,to),T(al,to,null),e(to,D4),e(to,Fn),e(Fn,S4),e(Fn,wp),e(wp,H4),e(Fn,V4),e(Fn,pu),e(pu,B4),e(Fn,W4),e(to,U4),T(Hs,to,null),e(to,Q4),T(Vs,to,null),b(i,Mg,v),b(i,Cn,v),e(Cn,Bs),e(Bs,hu),T(rl,hu,null),e(Cn,Z4),e(Cn,uu),e(uu,K4),b(i,zg,v),b(i,zt,v),T(il,zt,null),e(zt,J4),e(zt,mu),e(mu,G4),e(zt,Y4),e(zt,ll),e(ll,X4),e(ll,yp),e(yp,ev),e(ll,tv),e(zt,ov),e(zt,dl),e(dl,nv),e(dl,cl),e(cl,sv),e(dl,av),e(zt,rv),e(zt,Nt),T(pl,Nt,null),e(Nt,iv),e(Nt,Rn),e(Rn,lv),e(Rn,Tp),e(Tp,dv),e(Rn,cv),e(Rn,fu),e(fu,pv),e(Rn,hv),e(Nt,uv),T(Ws,Nt,null),e(Nt,mv),T(Us,Nt,null),e(Nt,fv),T(Qs,Nt,null),b(i,qg,v),b(i,En,v),e(En,Zs),e(Zs,gu),T(hl,gu,null),e(En,gv),e(En,_u),e(_u,_v),b(i,jg,v),b(i,qt,v),T(ul,qt,null),e(qt,bv),e(qt,Mn),e(Mn,kv),e(Mn,bu),e(bu,vv),e(Mn,wv),e(Mn,ku),e(ku,yv),e(Mn,Tv),e(qt,$v),e(qt,ml),e(ml,xv),e(ml,$p),e($p,Fv),e(ml,Cv),e(qt,Rv),e(qt,fl),e(fl,Ev),e(fl,gl),e(gl,Mv),e(fl,zv),e(qt,qv),e(qt,It),T(_l,It,null),e(It,jv),e(It,zn),e(zn,Av),e(zn,xp),e(xp,Pv),e(zn,Lv),e(zn,vu),e(vu,Nv),e(zn,Iv),e(It,Ov),T(Ks,It,null),e(It,Dv),T(Js,It,null),e(It,Sv),T(Gs,It,null),b(i,Ag,v),b(i,qn,v),e(qn,Ys),e(Ys,wu),T(bl,wu,null),e(qn,Hv),e(qn,yu),e(yu,Vv),b(i,Pg,v),b(i,_t,v),T(kl,_t,null),e(_t,Bv),e(_t,Tu),e(Tu,Wv),e(_t,Uv),e(_t,vl),e(vl,Qv),e(vl,Fp),e(Fp,Zv),e(vl,Kv),e(_t,Jv),e(_t,wl),e(wl,Gv),e(wl,yl),e(yl,Yv),e(wl,Xv),e(_t,ew),T(Xs,_t,null),e(_t,tw),e(_t,oo),T(Tl,oo,null),e(oo,ow),e(oo,jn),e(jn,nw),e(jn,Cp),e(Cp,sw),e(jn,aw),e(jn,$u),e($u,rw),e(jn,iw),e(oo,lw),T(ea,oo,null),e(oo,dw),T(ta,oo,null),b(i,Lg,v),b(i,An,v),e(An,oa),e(oa,xu),T($l,xu,null),e(An,cw),e(An,Fu),e(Fu,pw),b(i,Ng,v),b(i,Pn,v),T(xl,Pn,null),e(Pn,hw),e(Pn,no),T(Fl,no,null),e(no,uw),e(no,Ln),e(Ln,mw),e(Ln,Rp),e(Rp,fw),e(Ln,gw),e(Ln,Cu),e(Cu,_w),e(Ln,bw),e(no,kw),T(na,no,null),e(no,vw),T(sa,no,null),b(i,Ig,v),b(i,Nn,v),e(Nn,aa),e(aa,Ru),T(Cl,Ru,null),e(Nn,ww),e(Nn,Eu),e(Eu,yw),b(i,Og,v),b(i,bt,v),T(Rl,bt,null),e(bt,Tw),e(bt,El),e(El,$w),e(El,Mu),e(Mu,xw),e(El,Fw),e(bt,Cw),e(bt,Ml),e(Ml,Rw),e(Ml,Ep),e(Ep,Ew),e(Ml,Mw),e(bt,zw),e(bt,zl),e(zl,qw),e(zl,ql),e(ql,jw),e(zl,Aw),e(bt,Pw),T(ra,bt,null),e(bt,Lw),e(bt,Ot),T(jl,Ot,null),e(Ot,Nw),e(Ot,In),e(In,Iw),e(In,Mp),e(Mp,Ow),e(In,Dw),e(In,zu),e(zu,Sw),e(In,Hw),e(Ot,Vw),T(ia,Ot,null),e(Ot,Bw),T(la,Ot,null),e(Ot,Ww),T(da,Ot,null),b(i,Dg,v),b(i,On,v),e(On,ca),e(ca,qu),T(Al,qu,null),e(On,Uw),e(On,ju),e(ju,Qw),b(i,Sg,v),b(i,kt,v),T(Pl,kt,null),e(kt,Zw),e(kt,Au),e(Au,Kw),e(kt,Jw),e(kt,Ll),e(Ll,Gw),e(Ll,zp),e(zp,Yw),e(Ll,Xw),e(kt,e3),e(kt,Nl),e(Nl,t3),e(Nl,Il),e(Il,o3),e(Nl,n3),e(kt,s3),T(pa,kt,null),e(kt,a3),e(kt,Dt),T(Ol,Dt,null),e(Dt,r3),e(Dt,Dn),e(Dn,i3),e(Dn,qp),e(qp,l3),e(Dn,d3),e(Dn,Pu),e(Pu,c3),e(Dn,p3),e(Dt,h3),T(ha,Dt,null),e(Dt,u3),T(ua,Dt,null),e(Dt,m3),T(ma,Dt,null),b(i,Hg,v),b(i,Sn,v),e(Sn,fa),e(fa,Lu),T(Dl,Lu,null),e(Sn,f3),e(Sn,Nu),e(Nu,g3),b(i,Vg,v),b(i,vt,v),T(Sl,vt,null),e(vt,_3),e(vt,Iu),e(Iu,b3),e(vt,k3),e(vt,Hl),e(Hl,v3),e(Hl,jp),e(jp,w3),e(Hl,y3),e(vt,T3),e(vt,Vl),e(Vl,$3),e(Vl,Bl),e(Bl,x3),e(Vl,F3),e(vt,C3),T(ga,vt,null),e(vt,R3),e(vt,so),T(Wl,so,null),e(so,E3),e(so,Hn),e(Hn,M3),e(Hn,Ap),e(Ap,z3),e(Hn,q3),e(Hn,Ou),e(Ou,j3),e(Hn,A3),e(so,P3),T(_a,so,null),e(so,L3),T(ba,so,null),b(i,Bg,v),b(i,Vn,v),e(Vn,ka),e(ka,Du),T(Ul,Du,null),e(Vn,N3),e(Vn,Su),e(Su,I3),b(i,Wg,v),b(i,wt,v),T(Ql,wt,null),e(wt,O3),e(wt,Hu),e(Hu,D3),e(wt,S3),e(wt,Zl),e(Zl,H3),e(Zl,Pp),e(Pp,V3),e(Zl,B3),e(wt,W3),e(wt,Kl),e(Kl,U3),e(Kl,Jl),e(Jl,Q3),e(Kl,Z3),e(wt,K3),T(va,wt,null),e(wt,J3),e(wt,St),T(Gl,St,null),e(St,G3),e(St,Bn),e(Bn,Y3),e(Bn,Lp),e(Lp,X3),e(Bn,ey),e(Bn,Vu),e(Vu,ty),e(Bn,oy),e(St,ny),T(wa,St,null),e(St,sy),T(ya,St,null),e(St,ay),T(Ta,St,null),b(i,Ug,v),b(i,Wn,v),e(Wn,$a),e($a,Bu),T(Yl,Bu,null),e(Wn,ry),e(Wn,Wu),e(Wu,iy),b(i,Qg,v),b(i,yt,v),T(Xl,yt,null),e(yt,ly),e(yt,Un),e(Un,dy),e(Un,Uu),e(Uu,cy),e(Un,py),e(Un,Qu),e(Qu,hy),e(Un,uy),e(yt,my),e(yt,ed),e(ed,fy),e(ed,Np),e(Np,gy),e(ed,_y),e(yt,by),e(yt,td),e(td,ky),e(td,od),e(od,vy),e(td,wy),e(yt,yy),T(xa,yt,null),e(yt,Ty),e(yt,Ht),T(nd,Ht,null),e(Ht,$y),e(Ht,Qn),e(Qn,xy),e(Qn,Ip),e(Ip,Fy),e(Qn,Cy),e(Qn,Zu),e(Zu,Ry),e(Qn,Ey),e(Ht,My),T(Fa,Ht,null),e(Ht,zy),T(Ca,Ht,null),e(Ht,qy),T(Ra,Ht,null),b(i,Zg,v),b(i,Zn,v),e(Zn,Ea),e(Ea,Ku),T(sd,Ku,null),e(Zn,jy),e(Zn,Ju),e(Ju,Ay),b(i,Kg,v),b(i,lt,v),T(ad,lt,null),e(lt,Py),e(lt,Gu),e(Gu,Ly),e(lt,Ny),e(lt,rd),e(rd,Iy),e(rd,Op),e(Op,Oy),e(rd,Dy),e(lt,Sy),e(lt,id),e(id,Hy),e(id,ld),e(ld,Vy),e(id,By),e(lt,Wy),e(lt,Yu),e(Yu,Uy),e(lt,Qy),e(lt,vo),e(vo,Xu),e(Xu,dd),e(dd,Zy),e(vo,Ky),e(vo,em),e(em,cd),e(cd,Jy),e(vo,Gy),e(vo,tm),e(tm,pd),e(pd,Yy),e(vo,Xy),e(vo,om),e(om,hd),e(hd,e6),e(lt,t6),e(lt,ao),T(ud,ao,null),e(ao,o6),e(ao,Kn),e(Kn,n6),e(Kn,nm),e(nm,s6),e(Kn,a6),e(Kn,sm),e(sm,r6),e(Kn,i6),e(ao,l6),T(Ma,ao,null),e(ao,d6),T(za,ao,null),b(i,Jg,v),b(i,Jn,v),e(Jn,qa),e(qa,am),T(md,am,null),e(Jn,c6),e(Jn,rm),e(rm,p6),b(i,Gg,v),b(i,dt,v),T(fd,dt,null),e(dt,h6),e(dt,im),e(im,u6),e(dt,m6),e(dt,gd),e(gd,f6),e(gd,Dp),e(Dp,g6),e(gd,_6),e(dt,b6),e(dt,_d),e(_d,k6),e(_d,bd),e(bd,v6),e(_d,w6),e(dt,y6),e(dt,lm),e(lm,T6),e(dt,$6),e(dt,wo),e(wo,dm),e(dm,kd),e(kd,x6),e(wo,F6),e(wo,cm),e(cm,vd),e(vd,C6),e(wo,R6),e(wo,pm),e(pm,wd),e(wd,E6),e(wo,M6),e(wo,hm),e(hm,yd),e(yd,z6),e(dt,q6),e(dt,ro),T(Td,ro,null),e(ro,j6),e(ro,Gn),e(Gn,A6),e(Gn,um),e(um,P6),e(Gn,L6),e(Gn,mm),e(mm,N6),e(Gn,I6),e(ro,O6),T(ja,ro,null),e(ro,D6),T(Aa,ro,null),b(i,Yg,v),b(i,Yn,v),e(Yn,Pa),e(Pa,fm),T($d,fm,null),e(Yn,S6),e(Yn,gm),e(gm,H6),b(i,Xg,v),b(i,ct,v),T(xd,ct,null),e(ct,V6),e(ct,Fd),e(Fd,B6),e(Fd,_m),e(_m,W6),e(Fd,U6),e(ct,Q6),e(ct,Cd),e(Cd,Z6),e(Cd,Sp),e(Sp,K6),e(Cd,J6),e(ct,G6),e(ct,Rd),e(Rd,Y6),e(Rd,Ed),e(Ed,X6),e(Rd,e7),e(ct,t7),e(ct,bm),e(bm,o7),e(ct,n7),e(ct,yo),e(yo,km),e(km,Md),e(Md,s7),e(yo,a7),e(yo,vm),e(vm,zd),e(zd,r7),e(yo,i7),e(yo,wm),e(wm,qd),e(qd,l7),e(yo,d7),e(yo,ym),e(ym,jd),e(jd,c7),e(ct,p7),e(ct,io),T(Ad,io,null),e(io,h7),e(io,Xn),e(Xn,u7),e(Xn,Tm),e(Tm,m7),e(Xn,f7),e(Xn,$m),e($m,g7),e(Xn,_7),e(io,b7),T(La,io,null),e(io,k7),T(Na,io,null),b(i,e_,v),b(i,es,v),e(es,Ia),e(Ia,xm),T(Pd,xm,null),e(es,v7),e(es,Fm),e(Fm,w7),b(i,t_,v),b(i,pt,v),T(Ld,pt,null),e(pt,y7),e(pt,Cm),e(Cm,T7),e(pt,$7),e(pt,Nd),e(Nd,x7),e(Nd,Hp),e(Hp,F7),e(Nd,C7),e(pt,R7),e(pt,Id),e(Id,E7),e(Id,Od),e(Od,M7),e(Id,z7),e(pt,q7),e(pt,Rm),e(Rm,j7),e(pt,A7),e(pt,To),e(To,Em),e(Em,Dd),e(Dd,P7),e(To,L7),e(To,Mm),e(Mm,Sd),e(Sd,N7),e(To,I7),e(To,zm),e(zm,Hd),e(Hd,O7),e(To,D7),e(To,qm),e(qm,Vd),e(Vd,S7),e(pt,H7),e(pt,lo),T(Bd,lo,null),e(lo,V7),e(lo,ts),e(ts,B7),e(ts,jm),e(jm,W7),e(ts,U7),e(ts,Am),e(Am,Q7),e(ts,Z7),e(lo,K7),T(Oa,lo,null),e(lo,J7),T(Da,lo,null),b(i,o_,v),b(i,os,v),e(os,Sa),e(Sa,Pm),T(Wd,Pm,null),e(os,G7),e(os,Lm),e(Lm,Y7),b(i,n_,v),b(i,ht,v),T(Ud,ht,null),e(ht,X7),e(ht,Nm),e(Nm,eT),e(ht,tT),e(ht,Qd),e(Qd,oT),e(Qd,Vp),e(Vp,nT),e(Qd,sT),e(ht,aT),e(ht,Zd),e(Zd,rT),e(Zd,Kd),e(Kd,iT),e(Zd,lT),e(ht,dT),e(ht,Im),e(Im,cT),e(ht,pT),e(ht,$o),e($o,Om),e(Om,Jd),e(Jd,hT),e($o,uT),e($o,Dm),e(Dm,Gd),e(Gd,mT),e($o,fT),e($o,Sm),e(Sm,Yd),e(Yd,gT),e($o,_T),e($o,Hm),e(Hm,Xd),e(Xd,bT),e(ht,kT),e(ht,co),T(ec,co,null),e(co,vT),e(co,ns),e(ns,wT),e(ns,Vm),e(Vm,yT),e(ns,TT),e(ns,Bm),e(Bm,$T),e(ns,xT),e(co,FT),T(Ha,co,null),e(co,CT),T(Va,co,null),b(i,s_,v),b(i,ss,v),e(ss,Ba),e(Ba,Wm),T(tc,Wm,null),e(ss,RT),e(ss,Um),e(Um,ET),b(i,a_,v),b(i,ut,v),T(oc,ut,null),e(ut,MT),e(ut,Qm),e(Qm,zT),e(ut,qT),e(ut,nc),e(nc,jT),e(nc,Bp),e(Bp,AT),e(nc,PT),e(ut,LT),e(ut,sc),e(sc,NT),e(sc,ac),e(ac,IT),e(sc,OT),e(ut,DT),e(ut,Zm),e(Zm,ST),e(ut,HT),e(ut,xo),e(xo,Km),e(Km,rc),e(rc,VT),e(xo,BT),e(xo,Jm),e(Jm,ic),e(ic,WT),e(xo,UT),e(xo,Gm),e(Gm,lc),e(lc,QT),e(xo,ZT),e(xo,Ym),e(Ym,dc),e(dc,KT),e(ut,JT),e(ut,po),T(cc,po,null),e(po,GT),e(po,as),e(as,YT),e(as,Xm),e(Xm,XT),e(as,e8),e(as,ef),e(ef,t8),e(as,o8),e(po,n8),T(Wa,po,null),e(po,s8),T(Ua,po,null),b(i,r_,v),b(i,rs,v),e(rs,Qa),e(Qa,tf),T(pc,tf,null),e(rs,a8),e(rs,of),e(of,r8),b(i,i_,v),b(i,mt,v),T(hc,mt,null),e(mt,i8),e(mt,is),e(is,l8),e(is,nf),e(nf,d8),e(is,c8),e(is,sf),e(sf,p8),e(is,h8),e(mt,u8),e(mt,uc),e(uc,m8),e(uc,Wp),e(Wp,f8),e(uc,g8),e(mt,_8),e(mt,mc),e(mc,b8),e(mc,fc),e(fc,k8),e(mc,v8),e(mt,w8),e(mt,af),e(af,y8),e(mt,T8),e(mt,Fo),e(Fo,rf),e(rf,gc),e(gc,$8),e(Fo,x8),e(Fo,lf),e(lf,_c),e(_c,F8),e(Fo,C8),e(Fo,df),e(df,bc),e(bc,R8),e(Fo,E8),e(Fo,cf),e(cf,kc),e(kc,M8),e(mt,z8),e(mt,ho),T(vc,ho,null),e(ho,q8),e(ho,ls),e(ls,j8),e(ls,pf),e(pf,A8),e(ls,P8),e(ls,hf),e(hf,L8),e(ls,N8),e(ho,I8),T(Za,ho,null),e(ho,O8),T(Ka,ho,null),l_=!0},p(i,[v]){const wc={};v&2&&(wc.$$scope={dirty:v,ctx:i}),fs.$set(wc);const uf={};v&2&&(uf.$$scope={dirty:v,ctx:i}),_s.$set(uf);const mf={};v&2&&(mf.$$scope={dirty:v,ctx:i}),bs.$set(mf);const ff={};v&2&&(ff.$$scope={dirty:v,ctx:i}),ys.$set(ff);const yc={};v&2&&(yc.$$scope={dirty:v,ctx:i}),Ts.$set(yc);const gf={};v&2&&(gf.$$scope={dirty:v,ctx:i}),Fs.$set(gf);const _f={};v&2&&(_f.$$scope={dirty:v,ctx:i}),Cs.$set(_f);const bf={};v&2&&(bf.$$scope={dirty:v,ctx:i}),Es.$set(bf);const Tc={};v&2&&(Tc.$$scope={dirty:v,ctx:i}),Ms.$set(Tc);const kf={};v&2&&(kf.$$scope={dirty:v,ctx:i}),qs.$set(kf);const vf={};v&2&&(vf.$$scope={dirty:v,ctx:i}),js.$set(vf);const wf={};v&2&&(wf.$$scope={dirty:v,ctx:i}),As.$set(wf);const yf={};v&2&&(yf.$$scope={dirty:v,ctx:i}),Ls.$set(yf);const Tf={};v&2&&(Tf.$$scope={dirty:v,ctx:i}),Ns.$set(Tf);const $f={};v&2&&($f.$$scope={dirty:v,ctx:i}),Is.$set($f);const Co={};v&2&&(Co.$$scope={dirty:v,ctx:i}),Os.$set(Co);const $c={};v&2&&($c.$$scope={dirty:v,ctx:i}),Ds.$set($c);const xf={};v&2&&(xf.$$scope={dirty:v,ctx:i}),Hs.$set(xf);const Ff={};v&2&&(Ff.$$scope={dirty:v,ctx:i}),Vs.$set(Ff);const Ro={};v&2&&(Ro.$$scope={dirty:v,ctx:i}),Ws.$set(Ro);const Cf={};v&2&&(Cf.$$scope={dirty:v,ctx:i}),Us.$set(Cf);const Rf={};v&2&&(Rf.$$scope={dirty:v,ctx:i}),Qs.$set(Rf);const Ef={};v&2&&(Ef.$$scope={dirty:v,ctx:i}),Ks.$set(Ef);const Up={};v&2&&(Up.$$scope={dirty:v,ctx:i}),Js.$set(Up);const Mf={};v&2&&(Mf.$$scope={dirty:v,ctx:i}),Gs.$set(Mf);const ds={};v&2&&(ds.$$scope={dirty:v,ctx:i}),Xs.$set(ds);const zf={};v&2&&(zf.$$scope={dirty:v,ctx:i}),ea.$set(zf);const qf={};v&2&&(qf.$$scope={dirty:v,ctx:i}),ta.$set(qf);const xc={};v&2&&(xc.$$scope={dirty:v,ctx:i}),na.$set(xc);const jf={};v&2&&(jf.$$scope={dirty:v,ctx:i}),sa.$set(jf);const Af={};v&2&&(Af.$$scope={dirty:v,ctx:i}),ra.$set(Af);const Pf={};v&2&&(Pf.$$scope={dirty:v,ctx:i}),ia.$set(Pf);const Lf={};v&2&&(Lf.$$scope={dirty:v,ctx:i}),la.$set(Lf);const jt={};v&2&&(jt.$$scope={dirty:v,ctx:i}),da.$set(jt);const cs={};v&2&&(cs.$$scope={dirty:v,ctx:i}),pa.$set(cs);const Nf={};v&2&&(Nf.$$scope={dirty:v,ctx:i}),ha.$set(Nf);const If={};v&2&&(If.$$scope={dirty:v,ctx:i}),ua.$set(If);const Fc={};v&2&&(Fc.$$scope={dirty:v,ctx:i}),ma.$set(Fc);const Of={};v&2&&(Of.$$scope={dirty:v,ctx:i}),ga.$set(Of);const Cc={};v&2&&(Cc.$$scope={dirty:v,ctx:i}),_a.$set(Cc);const Df={};v&2&&(Df.$$scope={dirty:v,ctx:i}),ba.$set(Df);const Vo={};v&2&&(Vo.$$scope={dirty:v,ctx:i}),va.$set(Vo);const Sf={};v&2&&(Sf.$$scope={dirty:v,ctx:i}),wa.$set(Sf);const Hf={};v&2&&(Hf.$$scope={dirty:v,ctx:i}),ya.$set(Hf);const Vf={};v&2&&(Vf.$$scope={dirty:v,ctx:i}),Ta.$set(Vf);const Bo={};v&2&&(Bo.$$scope={dirty:v,ctx:i}),xa.$set(Bo);const Bf={};v&2&&(Bf.$$scope={dirty:v,ctx:i}),Fa.$set(Bf);const Wf={};v&2&&(Wf.$$scope={dirty:v,ctx:i}),Ca.$set(Wf);const Uf={};v&2&&(Uf.$$scope={dirty:v,ctx:i}),Ra.$set(Uf);const Wo={};v&2&&(Wo.$$scope={dirty:v,ctx:i}),Ma.$set(Wo);const Qf={};v&2&&(Qf.$$scope={dirty:v,ctx:i}),za.$set(Qf);const Zf={};v&2&&(Zf.$$scope={dirty:v,ctx:i}),ja.$set(Zf);const Kf={};v&2&&(Kf.$$scope={dirty:v,ctx:i}),Aa.$set(Kf);const Eo={};v&2&&(Eo.$$scope={dirty:v,ctx:i}),La.$set(Eo);const Uo={};v&2&&(Uo.$$scope={dirty:v,ctx:i}),Na.$set(Uo);const Jf={};v&2&&(Jf.$$scope={dirty:v,ctx:i}),Oa.$set(Jf);const Gf={};v&2&&(Gf.$$scope={dirty:v,ctx:i}),Da.$set(Gf);const Yf={};v&2&&(Yf.$$scope={dirty:v,ctx:i}),Ha.$set(Yf);const Qo={};v&2&&(Qo.$$scope={dirty:v,ctx:i}),Va.$set(Qo);const Xf={};v&2&&(Xf.$$scope={dirty:v,ctx:i}),Wa.$set(Xf);const eg={};v&2&&(eg.$$scope={dirty:v,ctx:i}),Ua.$set(eg);const tg={};v&2&&(tg.$$scope={dirty:v,ctx:i}),Za.$set(tg);const Ja={};v&2&&(Ja.$$scope={dirty:v,ctx:i}),Ka.$set(Ja)},i(i){l_||(F(p.$$.fragment,i),F(E.$$.fragment,i),F(Ze.$$.fragment,i),F(br.$$.fragment,i),F(zr.$$.fragment,i),F(Ir.$$.fragment,i),F(Zr.$$.fragment,i),F(ii.$$.fragment,i),F(li.$$.fragment,i),F(fs.$$.fragment,i),F(ci.$$.fragment,i),F(pi.$$.fragment,i),F(_s.$$.fragment,i),F(bs.$$.fragment,i),F(mi.$$.fragment,i),F(gi.$$.fragment,i),F(bi.$$.fragment,i),F(ki.$$.fragment,i),F(vi.$$.fragment,i),F(wi.$$.fragment,i),F(ys.$$.fragment,i),F(Ts.$$.fragment,i),F(xi.$$.fragment,i),F(Fi.$$.fragment,i),F(Ci.$$.fragment,i),F(ji.$$.fragment,i),F(Fs.$$.fragment,i),F(Cs.$$.fragment,i),F(Ai.$$.fragment,i),F(Pi.$$.fragment,i),F(Di.$$.fragment,i),F(Es.$$.fragment,i),F(Ms.$$.fragment,i),F(Si.$$.fragment,i),F(Hi.$$.fragment,i),F(Qi.$$.fragment,i),F(qs.$$.fragment,i),F(js.$$.fragment,i),F(As.$$.fragment,i),F(Zi.$$.fragment,i),F(Ki.$$.fragment,i),F(Xi.$$.fragment,i),F(Ls.$$.fragment,i),F(Ns.$$.fragment,i),F(Is.$$.fragment,i),F(Os.$$.fragment,i),F(Ds.$$.fragment,i),F(el.$$.fragment,i),F(tl.$$.fragment,i),F(al.$$.fragment,i),F(Hs.$$.fragment,i),F(Vs.$$.fragment,i),F(rl.$$.fragment,i),F(il.$$.fragment,i),F(pl.$$.fragment,i),F(Ws.$$.fragment,i),F(Us.$$.fragment,i),F(Qs.$$.fragment,i),F(hl.$$.fragment,i),F(ul.$$.fragment,i),F(_l.$$.fragment,i),F(Ks.$$.fragment,i),F(Js.$$.fragment,i),F(Gs.$$.fragment,i),F(bl.$$.fragment,i),F(kl.$$.fragment,i),F(Xs.$$.fragment,i),F(Tl.$$.fragment,i),F(ea.$$.fragment,i),F(ta.$$.fragment,i),F($l.$$.fragment,i),F(xl.$$.fragment,i),F(Fl.$$.fragment,i),F(na.$$.fragment,i),F(sa.$$.fragment,i),F(Cl.$$.fragment,i),F(Rl.$$.fragment,i),F(ra.$$.fragment,i),F(jl.$$.fragment,i),F(ia.$$.fragment,i),F(la.$$.fragment,i),F(da.$$.fragment,i),F(Al.$$.fragment,i),F(Pl.$$.fragment,i),F(pa.$$.fragment,i),F(Ol.$$.fragment,i),F(ha.$$.fragment,i),F(ua.$$.fragment,i),F(ma.$$.fragment,i),F(Dl.$$.fragment,i),F(Sl.$$.fragment,i),F(ga.$$.fragment,i),F(Wl.$$.fragment,i),F(_a.$$.fragment,i),F(ba.$$.fragment,i),F(Ul.$$.fragment,i),F(Ql.$$.fragment,i),F(va.$$.fragment,i),F(Gl.$$.fragment,i),F(wa.$$.fragment,i),F(ya.$$.fragment,i),F(Ta.$$.fragment,i),F(Yl.$$.fragment,i),F(Xl.$$.fragment,i),F(xa.$$.fragment,i),F(nd.$$.fragment,i),F(Fa.$$.fragment,i),F(Ca.$$.fragment,i),F(Ra.$$.fragment,i),F(sd.$$.fragment,i),F(ad.$$.fragment,i),F(ud.$$.fragment,i),F(Ma.$$.fragment,i),F(za.$$.fragment,i),F(md.$$.fragment,i),F(fd.$$.fragment,i),F(Td.$$.fragment,i),F(ja.$$.fragment,i),F(Aa.$$.fragment,i),F($d.$$.fragment,i),F(xd.$$.fragment,i),F(Ad.$$.fragment,i),F(La.$$.fragment,i),F(Na.$$.fragment,i),F(Pd.$$.fragment,i),F(Ld.$$.fragment,i),F(Bd.$$.fragment,i),F(Oa.$$.fragment,i),F(Da.$$.fragment,i),F(Wd.$$.fragment,i),F(Ud.$$.fragment,i),F(ec.$$.fragment,i),F(Ha.$$.fragment,i),F(Va.$$.fragment,i),F(tc.$$.fragment,i),F(oc.$$.fragment,i),F(cc.$$.fragment,i),F(Wa.$$.fragment,i),F(Ua.$$.fragment,i),F(pc.$$.fragment,i),F(hc.$$.fragment,i),F(vc.$$.fragment,i),F(Za.$$.fragment,i),F(Ka.$$.fragment,i),l_=!0)},o(i){$(p.$$.fragment,i),$(E.$$.fragment,i),$(Ze.$$.fragment,i),$(br.$$.fragment,i),$(zr.$$.fragment,i),$(Ir.$$.fragment,i),$(Zr.$$.fragment,i),$(ii.$$.fragment,i),$(li.$$.fragment,i),$(fs.$$.fragment,i),$(ci.$$.fragment,i),$(pi.$$.fragment,i),$(_s.$$.fragment,i),$(bs.$$.fragment,i),$(mi.$$.fragment,i),$(gi.$$.fragment,i),$(bi.$$.fragment,i),$(ki.$$.fragment,i),$(vi.$$.fragment,i),$(wi.$$.fragment,i),$(ys.$$.fragment,i),$(Ts.$$.fragment,i),$(xi.$$.fragment,i),$(Fi.$$.fragment,i),$(Ci.$$.fragment,i),$(ji.$$.fragment,i),$(Fs.$$.fragment,i),$(Cs.$$.fragment,i),$(Ai.$$.fragment,i),$(Pi.$$.fragment,i),$(Di.$$.fragment,i),$(Es.$$.fragment,i),$(Ms.$$.fragment,i),$(Si.$$.fragment,i),$(Hi.$$.fragment,i),$(Qi.$$.fragment,i),$(qs.$$.fragment,i),$(js.$$.fragment,i),$(As.$$.fragment,i),$(Zi.$$.fragment,i),$(Ki.$$.fragment,i),$(Xi.$$.fragment,i),$(Ls.$$.fragment,i),$(Ns.$$.fragment,i),$(Is.$$.fragment,i),$(Os.$$.fragment,i),$(Ds.$$.fragment,i),$(el.$$.fragment,i),$(tl.$$.fragment,i),$(al.$$.fragment,i),$(Hs.$$.fragment,i),$(Vs.$$.fragment,i),$(rl.$$.fragment,i),$(il.$$.fragment,i),$(pl.$$.fragment,i),$(Ws.$$.fragment,i),$(Us.$$.fragment,i),$(Qs.$$.fragment,i),$(hl.$$.fragment,i),$(ul.$$.fragment,i),$(_l.$$.fragment,i),$(Ks.$$.fragment,i),$(Js.$$.fragment,i),$(Gs.$$.fragment,i),$(bl.$$.fragment,i),$(kl.$$.fragment,i),$(Xs.$$.fragment,i),$(Tl.$$.fragment,i),$(ea.$$.fragment,i),$(ta.$$.fragment,i),$($l.$$.fragment,i),$(xl.$$.fragment,i),$(Fl.$$.fragment,i),$(na.$$.fragment,i),$(sa.$$.fragment,i),$(Cl.$$.fragment,i),$(Rl.$$.fragment,i),$(ra.$$.fragment,i),$(jl.$$.fragment,i),$(ia.$$.fragment,i),$(la.$$.fragment,i),$(da.$$.fragment,i),$(Al.$$.fragment,i),$(Pl.$$.fragment,i),$(pa.$$.fragment,i),$(Ol.$$.fragment,i),$(ha.$$.fragment,i),$(ua.$$.fragment,i),$(ma.$$.fragment,i),$(Dl.$$.fragment,i),$(Sl.$$.fragment,i),$(ga.$$.fragment,i),$(Wl.$$.fragment,i),$(_a.$$.fragment,i),$(ba.$$.fragment,i),$(Ul.$$.fragment,i),$(Ql.$$.fragment,i),$(va.$$.fragment,i),$(Gl.$$.fragment,i),$(wa.$$.fragment,i),$(ya.$$.fragment,i),$(Ta.$$.fragment,i),$(Yl.$$.fragment,i),$(Xl.$$.fragment,i),$(xa.$$.fragment,i),$(nd.$$.fragment,i),$(Fa.$$.fragment,i),$(Ca.$$.fragment,i),$(Ra.$$.fragment,i),$(sd.$$.fragment,i),$(ad.$$.fragment,i),$(ud.$$.fragment,i),$(Ma.$$.fragment,i),$(za.$$.fragment,i),$(md.$$.fragment,i),$(fd.$$.fragment,i),$(Td.$$.fragment,i),$(ja.$$.fragment,i),$(Aa.$$.fragment,i),$($d.$$.fragment,i),$(xd.$$.fragment,i),$(Ad.$$.fragment,i),$(La.$$.fragment,i),$(Na.$$.fragment,i),$(Pd.$$.fragment,i),$(Ld.$$.fragment,i),$(Bd.$$.fragment,i),$(Oa.$$.fragment,i),$(Da.$$.fragment,i),$(Wd.$$.fragment,i),$(Ud.$$.fragment,i),$(ec.$$.fragment,i),$(Ha.$$.fragment,i),$(Va.$$.fragment,i),$(tc.$$.fragment,i),$(oc.$$.fragment,i),$(cc.$$.fragment,i),$(Wa.$$.fragment,i),$(Ua.$$.fragment,i),$(pc.$$.fragment,i),$(hc.$$.fragment,i),$(vc.$$.fragment,i),$(Za.$$.fragment,i),$(Ka.$$.fragment,i),l_=!1},d(i){o(t),i&&o(u),i&&o(c),x(p),i&&o(P),i&&o(L),x(E),i&&o(Te),i&&o(G),i&&o($e),i&&o(X),i&&o(xe),i&&o(ee),i&&o(Fe),i&&o(le),i&&o(Ce),i&&o(te),i&&o(Re),i&&o(Q),i&&o(Ye),i&&o(A),i&&o(Xe),i&&o(He),x(Ze),i&&o(et),i&&o(Pc),i&&o(sg),x(br,i),i&&o(ag),i&&o(Tt),i&&o(rg),x(zr,i),i&&o(ig),i&&o(Yt),i&&o(lg),x(Ir,i),i&&o(dg),i&&o(At),i&&o(cg),x(Zr,i),i&&o(pg),i&&o(Pt),i&&o(hg),i&&o(Gc),i&&o(ug),i&&o(us),i&&o(mg),i&&o(hn),x(ii),i&&o(fg),i&&o(Ft),x(li),x(fs),i&&o(gg),i&&o(fn),x(ci),i&&o(_g),i&&o(Be),x(pi),x(_s),x(bs),x(mi),x(gi),x(bi),x(ki),i&&o(bg),i&&o(gn),x(vi),i&&o(kg),i&&o(rt),x(wi),x(ys),x(Ts),x(xi),i&&o(vg),i&&o(_n),x(Fi),i&&o(wg),i&&o(it),x(Ci),x(ji),x(Fs),x(Cs),i&&o(yg),i&&o(kn),x(Ai),i&&o(Tg),i&&o(Ct),x(Pi),x(Di),x(Es),x(Ms),i&&o($g),i&&o(wn),x(Si),i&&o(xg),i&&o(Rt),x(Hi),x(Qi),x(qs),x(js),x(As),i&&o(Fg),i&&o(Tn),x(Zi),i&&o(Cg),i&&o(Et),x(Ki),x(Xi),x(Ls),x(Ns),x(Is),x(Os),x(Ds),i&&o(Rg),i&&o(xn),x(el),i&&o(Eg),i&&o(Mt),x(tl),x(al),x(Hs),x(Vs),i&&o(Mg),i&&o(Cn),x(rl),i&&o(zg),i&&o(zt),x(il),x(pl),x(Ws),x(Us),x(Qs),i&&o(qg),i&&o(En),x(hl),i&&o(jg),i&&o(qt),x(ul),x(_l),x(Ks),x(Js),x(Gs),i&&o(Ag),i&&o(qn),x(bl),i&&o(Pg),i&&o(_t),x(kl),x(Xs),x(Tl),x(ea),x(ta),i&&o(Lg),i&&o(An),x($l),i&&o(Ng),i&&o(Pn),x(xl),x(Fl),x(na),x(sa),i&&o(Ig),i&&o(Nn),x(Cl),i&&o(Og),i&&o(bt),x(Rl),x(ra),x(jl),x(ia),x(la),x(da),i&&o(Dg),i&&o(On),x(Al),i&&o(Sg),i&&o(kt),x(Pl),x(pa),x(Ol),x(ha),x(ua),x(ma),i&&o(Hg),i&&o(Sn),x(Dl),i&&o(Vg),i&&o(vt),x(Sl),x(ga),x(Wl),x(_a),x(ba),i&&o(Bg),i&&o(Vn),x(Ul),i&&o(Wg),i&&o(wt),x(Ql),x(va),x(Gl),x(wa),x(ya),x(Ta),i&&o(Ug),i&&o(Wn),x(Yl),i&&o(Qg),i&&o(yt),x(Xl),x(xa),x(nd),x(Fa),x(Ca),x(Ra),i&&o(Zg),i&&o(Zn),x(sd),i&&o(Kg),i&&o(lt),x(ad),x(ud),x(Ma),x(za),i&&o(Jg),i&&o(Jn),x(md),i&&o(Gg),i&&o(dt),x(fd),x(Td),x(ja),x(Aa),i&&o(Yg),i&&o(Yn),x($d),i&&o(Xg),i&&o(ct),x(xd),x(Ad),x(La),x(Na),i&&o(e_),i&&o(es),x(Pd),i&&o(t_),i&&o(pt),x(Ld),x(Bd),x(Oa),x(Da),i&&o(o_),i&&o(os),x(Wd),i&&o(n_),i&&o(ht),x(Ud),x(ec),x(Ha),x(Va),i&&o(s_),i&&o(ss),x(tc),i&&o(a_),i&&o(ut),x(oc),x(cc),x(Wa),x(Ua),i&&o(r_),i&&o(rs),x(pc),i&&o(i_),i&&o(mt),x(hc),x(vc),x(Za),x(Ka)}}}const Ez={local:"roberta",sections:[{local:"overview",title:"Overview"},{local:"resources",title:"Resources"},{local:"transformers.RobertaConfig",title:"RobertaConfig"},{local:"transformers.RobertaTokenizer",title:"RobertaTokenizer"},{local:"transformers.RobertaTokenizerFast",title:"RobertaTokenizerFast"},{local:"transformers.RobertaModel",title:"RobertaModel"},{local:"transformers.RobertaForCausalLM",title:"RobertaForCausalLM"},{local:"transformers.RobertaForMaskedLM",title:"RobertaForMaskedLM"},{local:"transformers.RobertaForSequenceClassification",title:"RobertaForSequenceClassification"},{local:"transformers.RobertaForMultipleChoice",title:"RobertaForMultipleChoice"},{local:"transformers.RobertaForTokenClassification",title:"RobertaForTokenClassification"},{local:"transformers.RobertaForQuestionAnswering",title:"RobertaForQuestionAnswering"},{local:"transformers.TFRobertaModel",title:"TFRobertaModel"},{local:"transformers.TFRobertaForCausalLM",title:"TFRobertaForCausalLM"},{local:"transformers.TFRobertaForMaskedLM",title:"TFRobertaForMaskedLM"},{local:"transformers.TFRobertaForSequenceClassification",title:"TFRobertaForSequenceClassification"},{local:"transformers.TFRobertaForMultipleChoice",title:"TFRobertaForMultipleChoice"},{local:"transformers.TFRobertaForTokenClassification",title:"TFRobertaForTokenClassification"},{local:"transformers.TFRobertaForQuestionAnswering",title:"TFRobertaForQuestionAnswering"},{local:"transformers.FlaxRobertaModel",title:"FlaxRobertaModel"},{local:"transformers.FlaxRobertaForCausalLM",title:"FlaxRobertaForCausalLM"},{local:"transformers.FlaxRobertaForMaskedLM",title:"FlaxRobertaForMaskedLM"},{local:"transformers.FlaxRobertaForSequenceClassification",title:"FlaxRobertaForSequenceClassification"},{local:"transformers.FlaxRobertaForMultipleChoice",title:"FlaxRobertaForMultipleChoice"},{local:"transformers.FlaxRobertaForTokenClassification",title:"FlaxRobertaForTokenClassification"},{local:"transformers.FlaxRobertaForQuestionAnswering",title:"FlaxRobertaForQuestionAnswering"}],title:"RoBERTa"};function Mz(k){return qR(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Nz extends fe{constructor(t){super();ge(this,t,Mz,Rz,_e,{})}}export{Nz as default,Ez as metadata};
