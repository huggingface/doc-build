import{S as Jm,i as Zm,s as Ym,e as n,k as d,w as f,t as s,M as eh,c as r,d as o,m as l,a,x as u,h as i,b as c,F as e,g as p,y as g,q as _,o as k,B as v,v as th}from"../../chunks/vendor-6b77c823.js";import{T as Rr}from"../../chunks/Tip-39098574.js";import{D as z}from"../../chunks/Docstring-af1d0ae0.js";import{C as Xe}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as J}from"../../chunks/IconCopyLink-7a11ce68.js";function oh(O){let h,y,w,T,$;return{c(){h=n("p"),y=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),w=n("code"),T=s("Module"),$=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var b=a(h);y=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),w=r(b,"CODE",{});var q=a(w);T=i(q,"Module"),q.forEach(o),$=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(R,b){p(R,h,b),e(h,y),e(h,w),e(w,T),e(h,$)},d(R){R&&o(h)}}}function nh(O){let h,y,w,T,$;return{c(){h=n("p"),y=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),w=n("code"),T=s("Module"),$=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var b=a(h);y=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),w=r(b,"CODE",{});var q=a(w);T=i(q,"Module"),q.forEach(o),$=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(R,b){p(R,h,b),e(h,y),e(h,w),e(w,T),e(h,$)},d(R){R&&o(h)}}}function rh(O){let h,y,w,T,$;return{c(){h=n("p"),y=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),w=n("code"),T=s("Module"),$=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var b=a(h);y=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),w=r(b,"CODE",{});var q=a(w);T=i(q,"Module"),q.forEach(o),$=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(R,b){p(R,h,b),e(h,y),e(h,w),e(w,T),e(h,$)},d(R){R&&o(h)}}}function ah(O){let h,y,w,T,$;return{c(){h=n("p"),y=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),w=n("code"),T=s("Module"),$=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var b=a(h);y=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),w=r(b,"CODE",{});var q=a(w);T=i(q,"Module"),q.forEach(o),$=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(R,b){p(R,h,b),e(h,y),e(h,w),e(w,T),e(h,$)},d(R){R&&o(h)}}}function sh(O){let h,y,w,T,$;return{c(){h=n("p"),y=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),w=n("code"),T=s("Module"),$=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var b=a(h);y=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),w=r(b,"CODE",{});var q=a(w);T=i(q,"Module"),q.forEach(o),$=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(R,b){p(R,h,b),e(h,y),e(h,w),e(w,T),e(h,$)},d(R){R&&o(h)}}}function ih(O){let h,y,w,T,$,R,b,q,ya,Tr,se,ye,rn,Ge,Ea,an,za,$r,Ee,qa,Je,xa,Aa,yr,po,Pa,Er,fo,sn,ja,zr,Z,La,Ze,Ma,Fa,Ye,Sa,Ca,qr,ie,ze,dn,et,Ia,ln,Da,xr,x,tt,Na,cn,Oa,Wa,L,mn,uo,Ka,Qa,hn,go,Ba,Ha,pn,_o,Va,Ua,fn,ko,Xa,Ga,un,vo,Ja,Za,gn,wo,Ya,es,ot,ts,nt,os,ns,rs,de,as,bo,ss,is,Ro,ds,ls,cs,_n,ms,hs,rt,Ar,le,qe,kn,at,ps,vn,fs,Pr,E,st,us,wn,gs,_s,xe,To,ks,vs,$o,ws,bs,Rs,it,Ts,yo,$s,ys,Es,Y,dt,zs,bn,qs,xs,lt,Eo,As,Rn,Ps,js,zo,Ls,Tn,Ms,Fs,Ae,ct,Ss,mt,Cs,$n,Is,Ds,Ns,W,ht,Os,yn,Ws,Ks,pt,Qs,ce,Bs,En,Hs,Vs,zn,Us,Xs,Gs,qo,ft,Js,P,ut,Zs,gt,Ys,qn,ei,ti,oi,me,xn,ni,ri,_t,ai,An,si,ii,di,kt,li,Pn,ci,mi,hi,vt,xo,pi,jn,fi,ui,Ao,gi,Ln,_i,ki,Mn,vi,wi,wt,jr,he,Pe,Fn,bt,bi,Sn,Ri,Lr,M,Rt,Ti,Tt,$i,Cn,yi,Ei,zi,je,Po,qi,xi,jo,Ai,Pi,ji,$t,Li,Lo,Mi,Fi,Si,j,yt,Ci,Et,Ii,In,Di,Ni,Oi,pe,Dn,Wi,Ki,zt,Qi,Nn,Bi,Hi,Vi,qt,Ui,On,Xi,Gi,Ji,xt,Mo,Zi,Wn,Yi,ed,Fo,td,Kn,od,nd,Qn,rd,ad,At,Mr,fe,Le,Bn,Pt,sd,Hn,id,Fr,H,jt,dd,Vn,ld,cd,Me,Lt,md,Un,hd,Sr,ue,Fe,Xn,Mt,pd,Gn,fd,Cr,V,Ft,ud,St,gd,Ct,_d,kd,vd,F,It,wd,ge,bd,So,Rd,Td,Jn,$d,yd,Ed,Se,zd,Zn,qd,xd,Dt,Ir,_e,Ce,Yn,Nt,Ad,er,Pd,Dr,U,Ot,jd,Wt,Ld,Kt,Md,Fd,Sd,S,Qt,Cd,ke,Id,Co,Dd,Nd,tr,Od,Wd,Kd,Ie,Qd,or,Bd,Hd,Bt,Nr,ve,De,nr,Ht,Vd,rr,Ud,Or,X,Vt,Xd,Ut,Gd,Xt,Jd,Zd,Yd,C,Gt,el,we,tl,Io,ol,nl,ar,rl,al,sl,Ne,il,sr,dl,ll,Jt,Wr,be,Oe,ir,Zt,cl,dr,ml,Kr,G,Yt,hl,eo,pl,to,fl,ul,gl,ee,oo,_l,Re,kl,Do,vl,wl,lr,bl,Rl,Tl,We,Qr,Te,Ke,cr,no,$l,mr,yl,Br,N,ro,El,Qe,hr,zl,ql,ao,xl,Al,Pl,Be,so,jl,io,Ll,pr,Ml,Fl,Sl,I,lo,Cl,$e,Il,No,Dl,Nl,fr,Ol,Wl,Kl,He,Ql,ur,Bl,Hl,co,Hr;return R=new J({}),Ge=new J({}),et=new J({}),tt=new z({props:{name:"class transformers.RealmConfig",anchor:"transformers.RealmConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"retriever_proj_size",val:" = 128"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"num_candidates",val:" = 8"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu_new'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"span_hidden_size",val:" = 256"},{name:"max_span_width",val:" = 10"},{name:"reader_layer_norm_eps",val:" = 0.001"},{name:"reader_beam_size",val:" = 5"},{name:"reader_seq_len",val:" = 320"},{name:"num_block_records",val:" = 13353718"},{name:"searcher_beam_size",val:" = 5000"},{name:"searcher_seq_len",val:" = 64"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/configuration_realm.py#L36",parametersDescription:[{anchor:"transformers.RealmConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the REALM model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmEmbedder">RealmEmbedder</a>, <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmScorer">RealmScorer</a>, <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmKnowledgeAugEncoder">RealmKnowledgeAugEncoder</a>, or
<a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmReader">RealmReader</a>.`,name:"vocab_size"},{anchor:"transformers.RealmConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimension of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.RealmConfig.retriever_proj_size",description:`<strong>retriever_proj_size</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Dimension of the retriever(embedder) projection.`,name:"retriever_proj_size"},{anchor:"transformers.RealmConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.RealmConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.RealmConfig.num_candidates",description:`<strong>num_candidates</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of candidates inputted to the RealmScorer or RealmKnowledgeAugEncoder.`,name:"num_candidates"},{anchor:"transformers.RealmConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimension of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.RealmConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu_new&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.RealmConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.RealmConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.RealmConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.RealmConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmEmbedder">RealmEmbedder</a>, <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmScorer">RealmScorer</a>,
<a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmKnowledgeAugEncoder">RealmKnowledgeAugEncoder</a>, or <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmReader">RealmReader</a>.`,name:"type_vocab_size"},{anchor:"transformers.RealmConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.RealmConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.RealmConfig.span_hidden_size",description:`<strong>span_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimension of the reader&#x2019;s spans.`,name:"span_hidden_size"},{anchor:"transformers.RealmConfig.max_span_width",description:`<strong>max_span_width</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
Max span width of the reader.`,name:"max_span_width"},{anchor:"transformers.RealmConfig.reader_layer_norm_eps",description:`<strong>reader_layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-3) &#x2014;
The epsilon used by the reader&#x2019;s layer normalization layers.`,name:"reader_layer_norm_eps"},{anchor:"transformers.RealmConfig.reader_beam_size",description:`<strong>reader_beam_size</strong> (<code>int</code>, <em>optional</em>, defaults to 5) &#x2014;
Beam size of the reader.`,name:"reader_beam_size"},{anchor:"transformers.RealmConfig.reader_seq_len",description:`<strong>reader_seq_len</strong> (<code>int</code>, <em>optional</em>, defaults to 288+32) &#x2014;
Maximum sequence length of the reader.`,name:"reader_seq_len"},{anchor:"transformers.RealmConfig.num_block_records",description:`<strong>num_block_records</strong> (<code>int</code>, <em>optional</em>, defaults to 13353718) &#x2014;
Number of block records.`,name:"num_block_records"},{anchor:"transformers.RealmConfig.searcher_beam_size",description:`<strong>searcher_beam_size</strong> (<code>int</code>, <em>optional</em>, defaults to 5000) &#x2014;
Beam size of the searcher. Note that when eval mode is enabled, <em>searcher_beam_size</em> will be the same as
<em>reader_beam_size</em>.`,name:"searcher_beam_size"},{anchor:"transformers.RealmConfig.searcher_seq_len",description:`<strong>searcher_seq_len</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Maximum sequence length of the searcher.`,name:"searcher_seq_len"}]}}),rt=new Xe({props:{code:`from transformers import RealmEmbedder, RealmConfig

# Initializing a REALM realm-cc-news-pretrained-* style configuration
configuration = RealmConfig()

# Initializing a model from the google/realm-cc-news-pretrained-embedder style configuration
model = RealmEmbedder(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmEmbedder, RealmConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a REALM realm-cc-news-pretrained-* style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = RealmConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the google/realm-cc-news-pretrained-embedder style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmEmbedder(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),at=new J({}),st=new z({props:{name:"class transformers.RealmTokenizer",anchor:"transformers.RealmTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/tokenization_realm.py#L87",parametersDescription:[{anchor:"transformers.RealmTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.RealmTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.RealmTokenizer.do_basic_tokenize",description:`<strong>do_basic_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to do basic tokenization before WordPiece.`,name:"do_basic_tokenize"},{anchor:"transformers.RealmTokenizer.never_split",description:`<strong>never_split</strong> (<code>Iterable</code>, <em>optional</em>) &#x2014;
Collection of tokens which will never be split during tokenization. Only has an effect when
<code>do_basic_tokenize=True</code>`,name:"never_split"},{anchor:"transformers.RealmTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.RealmTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.RealmTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.RealmTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.RealmTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.RealmTokenizer.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters.</p>
<p>This should likely be deactivated for Japanese (see this
<a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">issue</a>).
strip_accents &#x2014; (<code>bool</code>, <em>optional</em>):
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"tokenize_chinese_chars"}]}}),dt=new z({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.RealmTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/tokenization_realm.py#L294",parametersDescription:[{anchor:"transformers.RealmTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.RealmTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ct=new z({props:{name:"get_special_tokens_mask",anchor:"transformers.RealmTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/tokenization_realm.py#L319",parametersDescription:[{anchor:"transformers.RealmTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.RealmTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.RealmTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ht=new z({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.RealmTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/tokenization_realm.py#L347",parametersDescription:[{anchor:"transformers.RealmTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.RealmTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),pt=new Xe({props:{code:`0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),ft=new z({props:{name:"save_vocabulary",anchor:"transformers.RealmTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/tokenization_realm.py#L376"}}),ut=new z({props:{name:"batch_encode_candidates",anchor:"transformers.RealmTokenizer.batch_encode_candidates",parameters:[{name:"text",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/tokenization_realm.py#L221",parametersDescription:[{anchor:"transformers.RealmTokenizer.batch_encode_candidates.text",description:`<strong>text</strong> (<code>List[List[str]]</code>) &#x2014;
The batch of sequences to be encoded. Each sequence must be in this format: (batch_size,
num_candidates, text).`,name:"text"},{anchor:"transformers.RealmTokenizer.batch_encode_candidates.text_pair",description:`<strong>text_pair</strong> (<code>List[List[str]]</code>, <em>optional</em>) &#x2014;
The batch of sequences to be encoded. Each sequence must be in this format: (batch_size,
num_candidates, text).
**kwargs &#x2014;
Keyword arguments of the <strong>call</strong> method.`,name:"text_pair"}],returnDescription:`
<p>Encoded text or text pair.</p>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a></p>
`}}),wt=new Xe({props:{code:`from transformers import RealmTokenizer

# batch_size = 2, num_candidates = 2
text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-encoder")
tokenized_text = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># batch_size = 2, num_candidates = 2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = [[<span class="hljs-string">&quot;Hello world!&quot;</span>, <span class="hljs-string">&quot;Nice to meet you!&quot;</span>], [<span class="hljs-string">&quot;The cute cat.&quot;</span>, <span class="hljs-string">&quot;The adorable dog.&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-encoder&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_text = tokenizer.batch_encode_candidates(text, max_length=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`}}),bt=new J({}),Rt=new z({props:{name:"class transformers.RealmTokenizerFast",anchor:"transformers.RealmTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/tokenization_realm_fast.py#L78",parametersDescription:[{anchor:"transformers.RealmTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.RealmTokenizerFast.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.RealmTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.RealmTokenizerFast.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.RealmTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.RealmTokenizerFast.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.RealmTokenizerFast.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.RealmTokenizerFast.clean_text",description:`<strong>clean_text</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to clean the text before tokenization by removing any control characters and replacing all
whitespaces by the classic one.`,name:"clean_text"},{anchor:"transformers.RealmTokenizerFast.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see <a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">this
issue</a>).`,name:"tokenize_chinese_chars"},{anchor:"transformers.RealmTokenizerFast.strip_accents",description:`<strong>strip_accents</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"strip_accents"},{anchor:"transformers.RealmTokenizerFast.wordpieces_prefix",description:`<strong>wordpieces_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;##&quot;</code>) &#x2014;
The prefix for subwords.`,name:"wordpieces_prefix"}]}}),yt=new z({props:{name:"batch_encode_candidates",anchor:"transformers.RealmTokenizerFast.batch_encode_candidates",parameters:[{name:"text",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/tokenization_realm_fast.py#L169",parametersDescription:[{anchor:"transformers.RealmTokenizerFast.batch_encode_candidates.text",description:`<strong>text</strong> (<code>List[List[str]]</code>) &#x2014;
The batch of sequences to be encoded. Each sequence must be in this format: (batch_size,
num_candidates, text).`,name:"text"},{anchor:"transformers.RealmTokenizerFast.batch_encode_candidates.text_pair",description:`<strong>text_pair</strong> (<code>List[List[str]]</code>, <em>optional</em>) &#x2014;
The batch of sequences to be encoded. Each sequence must be in this format: (batch_size,
num_candidates, text).
**kwargs &#x2014;
Keyword arguments of the <strong>call</strong> method.`,name:"text_pair"}],returnDescription:`
<p>Encoded text or text pair.</p>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a></p>
`}}),At=new Xe({props:{code:`from transformers import RealmTokenizerFast

# batch_size = 2, num_candidates = 2
text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

tokenizer = RealmTokenizerFast.from_pretrained("google/realm-cc-news-pretrained-encoder")
tokenized_text = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># batch_size = 2, num_candidates = 2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = [[<span class="hljs-string">&quot;Hello world!&quot;</span>, <span class="hljs-string">&quot;Nice to meet you!&quot;</span>], [<span class="hljs-string">&quot;The cute cat.&quot;</span>, <span class="hljs-string">&quot;The adorable dog.&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizerFast.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-encoder&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_text = tokenizer.batch_encode_candidates(text, max_length=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`}}),Pt=new J({}),jt=new z({props:{name:"class transformers.RealmRetriever",anchor:"transformers.RealmRetriever",parameters:[{name:"block_records",val:""},{name:"tokenizer",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/retrieval_realm.py#L73",parametersDescription:[{anchor:"transformers.RealmRetriever.block_records",description:`<strong>block_records</strong> (<code>np.ndarray</code>) &#x2014;
A numpy array which cantains evidence texts.`,name:"block_records"},{anchor:"transformers.RealmRetriever.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>) &#x2014;
The tokenizer to encode retrieved texts.`,name:"tokenizer"}]}}),Lt=new z({props:{name:"block_has_answer",anchor:"transformers.RealmRetriever.block_has_answer",parameters:[{name:"concat_inputs",val:""},{name:"answer_ids",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/retrieval_realm.py#L130"}}),Mt=new J({}),Ft=new z({props:{name:"class transformers.RealmEmbedder",anchor:"transformers.RealmEmbedder",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1140",parametersDescription:[{anchor:"transformers.RealmEmbedder.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),It=new z({props:{name:"forward",anchor:"transformers.RealmEmbedder.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1154",parametersDescription:[{anchor:"transformers.RealmEmbedder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmEmbedder.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmEmbedder.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmEmbedder.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RealmEmbedder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RealmEmbedder.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RealmEmbedder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RealmEmbedder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RealmEmbedder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <code>transformers.models.realm.modeling_realm.RealmEmbedderOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>projected_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.retriever_proj_size)</code>) \u2014 Projected score.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.realm.modeling_realm.RealmEmbedderOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Se=new Rr({props:{$$slots:{default:[oh]},$$scope:{ctx:O}}}),Dt=new Xe({props:{code:`from transformers import RealmTokenizer, RealmEmbedder
import torch

tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-embedder")
model = RealmEmbedder.from_pretrained("google/realm-cc-news-pretrained-embedder")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

projected_score = outputs.projected_score`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizer, RealmEmbedder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-embedder&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmEmbedder.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-embedder&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>projected_score = outputs.projected_score`}}),Nt=new J({}),Ot=new z({props:{name:"class transformers.RealmScorer",anchor:"transformers.RealmScorer",parameters:[{name:"config",val:""},{name:"query_embedder",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1220",parametersDescription:[{anchor:"transformers.RealmScorer.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.RealmScorer.query_embedder",description:`<strong>query_embedder</strong> (<a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmEmbedder">RealmEmbedder</a>) &#x2014;
Embedder for input sequences. If not specified, it will use the same embedder as candidate sequences.`,name:"query_embedder"}]}}),Qt=new z({props:{name:"forward",anchor:"transformers.RealmScorer.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"candidate_input_ids",val:" = None"},{name:"candidate_attention_mask",val:" = None"},{name:"candidate_token_type_ids",val:" = None"},{name:"candidate_inputs_embeds",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1236",parametersDescription:[{anchor:"transformers.RealmScorer.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmScorer.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmScorer.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmScorer.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RealmScorer.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RealmScorer.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RealmScorer.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RealmScorer.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RealmScorer.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RealmScorer.forward.candidate_input_ids",description:`<strong>candidate_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>) &#x2014;
Indices of candidate input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"candidate_input_ids"},{anchor:"transformers.RealmScorer.forward.candidate_attention_mask",description:`<strong>candidate_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"candidate_attention_mask"},{anchor:"transformers.RealmScorer.forward.candidate_token_type_ids",description:`<strong>candidate_token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"candidate_token_type_ids"},{anchor:"transformers.RealmScorer.forward.candidate_inputs_embeds",description:`<strong>candidate_inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size * num_candidates, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>candidate_input_ids</code> you can choose to directly pass an embedded
representation. This is useful if you want more control over how to convert <em>candidate_input_ids</em> indices
into associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"candidate_inputs_embeds"}],returnDescription:`
<p>A <code>transformers.models.realm.modeling_realm.RealmScorerOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li><strong>relevance_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_candidates)</code>) \u2014 The relevance score of document candidates (before softmax).</li>
<li><strong>query_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.retriever_proj_size)</code>) \u2014 Query score derived from the query embedder.</li>
<li><strong>candidate_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_candidates, config.retriever_proj_size)</code>) \u2014 Candidate score derived from the embedder.</li>
</ul>
`,returnType:`
<p><code>transformers.models.realm.modeling_realm.RealmScorerOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ie=new Rr({props:{$$slots:{default:[nh]},$$scope:{ctx:O}}}),Bt=new Xe({props:{code:`import torch
from transformers import RealmTokenizer, RealmScorer

tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-scorer")
model = RealmScorer.from_pretrained("google/realm-cc-news-pretrained-scorer", num_candidates=2)

# batch_size = 2, num_candidates = 2
input_texts = ["How are you?", "What is the item in the picture?"]
candidates_texts = [["Hello world!", "Nice to meet you!"], ["A cute cat.", "An adorable dog."]]

inputs = tokenizer(input_texts, return_tensors="pt")
candidates_inputs = tokenizer.batch_encode_candidates(candidates_texts, max_length=10, return_tensors="pt")

outputs = model(
    **inputs,
    candidate_input_ids=candidates_inputs.input_ids,
    candidate_attention_mask=candidates_inputs.attention_mask,
    candidate_token_type_ids=candidates_inputs.token_type_ids,
)
relevance_score = outputs.relevance_score`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizer, RealmScorer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-scorer&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmScorer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-scorer&quot;</span>, num_candidates=<span class="hljs-number">2</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># batch_size = 2, num_candidates = 2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_texts = [<span class="hljs-string">&quot;How are you?&quot;</span>, <span class="hljs-string">&quot;What is the item in the picture?&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>candidates_texts = [[<span class="hljs-string">&quot;Hello world!&quot;</span>, <span class="hljs-string">&quot;Nice to meet you!&quot;</span>], [<span class="hljs-string">&quot;A cute cat.&quot;</span>, <span class="hljs-string">&quot;An adorable dog.&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(input_texts, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>candidates_inputs = tokenizer.batch_encode_candidates(candidates_texts, max_length=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(
<span class="hljs-meta">... </span>    **inputs,
<span class="hljs-meta">... </span>    candidate_input_ids=candidates_inputs.input_ids,
<span class="hljs-meta">... </span>    candidate_attention_mask=candidates_inputs.attention_mask,
<span class="hljs-meta">... </span>    candidate_token_type_ids=candidates_inputs.token_type_ids,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_score = outputs.relevance_score`}}),Ht=new J({}),Vt=new z({props:{name:"class transformers.RealmKnowledgeAugEncoder",anchor:"transformers.RealmKnowledgeAugEncoder",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1367",parametersDescription:[{anchor:"transformers.RealmKnowledgeAugEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Gt=new z({props:{name:"forward",anchor:"transformers.RealmKnowledgeAugEncoder.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"relevance_score",val:" = None"},{name:"labels",val:" = None"},{name:"mlm_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1386",parametersDescription:[{anchor:"transformers.RealmKnowledgeAugEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_candidates, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.relevance_score",description:`<strong>relevance_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_candidates)</code>, <em>optional</em>) &#x2014;
Relevance score derived from RealmScorer, must be specified if you want to compute the masked language
modeling loss.`,name:"relevance_score"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.mlm_mask",description:`<strong>mlm_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid calculating joint loss on certain positions. If not specified, the loss will not be masked.
Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"mlm_mask"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ne=new Rr({props:{$$slots:{default:[rh]},$$scope:{ctx:O}}}),Jt=new Xe({props:{code:`import torch
from transformers import RealmTokenizer, RealmKnowledgeAugEncoder

tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-encoder")
model = RealmKnowledgeAugEncoder.from_pretrained(
    "google/realm-cc-news-pretrained-encoder", num_candidates=2
)

# batch_size = 2, num_candidates = 2
text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

inputs = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizer, RealmKnowledgeAugEncoder

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-encoder&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmKnowledgeAugEncoder.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google/realm-cc-news-pretrained-encoder&quot;</span>, num_candidates=<span class="hljs-number">2</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># batch_size = 2, num_candidates = 2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = [[<span class="hljs-string">&quot;Hello world!&quot;</span>, <span class="hljs-string">&quot;Nice to meet you!&quot;</span>], [<span class="hljs-string">&quot;The cute cat.&quot;</span>, <span class="hljs-string">&quot;The adorable dog.&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer.batch_encode_candidates(text, max_length=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Zt=new J({}),Yt=new z({props:{name:"class transformers.RealmReader",anchor:"transformers.RealmReader",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1515",parametersDescription:[{anchor:"transformers.RealmReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),oo=new z({props:{name:"forward",anchor:"transformers.RealmReader.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"relevance_score",val:" = None"},{name:"block_mask",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"has_answers",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1529",parametersDescription:[{anchor:"transformers.RealmReader.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(reader_beam_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmReader.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(reader_beam_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmReader.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(reader_beam_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmReader.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(reader_beam_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RealmReader.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RealmReader.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(reader_beam_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RealmReader.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RealmReader.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RealmReader.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RealmReader.forward.relevance_score",description:`<strong>relevance_score</strong> (<code>torch.FloatTensor</code> of shape <code>(searcher_beam_size,)</code>, <em>optional</em>) &#x2014;
Relevance score, which must be specified if you want to compute the logits and marginal log loss.`,name:"relevance_score"},{anchor:"transformers.RealmReader.forward.block_mask",description:`<strong>block_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(searcher_beam_size, sequence_length)</code>, <em>optional</em>) &#x2014;
The mask of the evidence block, which must be specified if you want to compute the logits and marginal log
loss.`,name:"block_mask"},{anchor:"transformers.RealmReader.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(searcher_beam_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.RealmReader.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(searcher_beam_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"},{anchor:"transformers.RealmReader.forward.has_answers",description:`<strong>has_answers</strong> (<code>torch.BoolTensor</code> of shape <code>(searcher_beam_size,)</code>, <em>optional</em>) &#x2014;
Whether or not the evidence block has answer(s).`,name:"has_answers"}],returnDescription:`
<p>A <code>transformers.models.realm.modeling_realm.RealmReaderOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>start_positions</code>, <code>end_positions</code>, <code>has_answers</code> are provided) \u2014 Total loss.</p>
</li>
<li>
<p><strong>retriever_loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>start_positions</code>, <code>end_positions</code>, <code>has_answers</code> are provided) \u2014 Retriever loss.</p>
</li>
<li>
<p><strong>reader_loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>start_positions</code>, <code>end_positions</code>, <code>has_answers</code> are provided) \u2014 Reader loss.</p>
</li>
<li>
<p><strong>retriever_correct</strong> (<code>torch.BoolTensor</code> of shape <code>(config.searcher_beam_size,)</code>, <em>optional</em>) \u2014 Whether or not an evidence block contains answer.</p>
</li>
<li>
<p><strong>reader_correct</strong> (<code>torch.BoolTensor</code> of shape <code>(config.reader_beam_size, num_candidates)</code>, <em>optional</em>) \u2014 Whether or not a span candidate contains answer.</p>
</li>
<li>
<p><strong>block_idx</strong> (<code>torch.LongTensor</code> of shape <code>()</code>) \u2014 The index of the retrieved evidence block in which the predicted answer is most likely.</p>
</li>
<li>
<p><strong>candidate</strong> (<code>torch.LongTensor</code> of shape <code>()</code>) \u2014 The index of the retrieved span candidates in which the predicted answer is most likely.</p>
</li>
<li>
<p><strong>start_pos</strong> (<code>torch.IntTensor</code> of shape <code>()</code>) \u2014 Predicted answer starting position in <em>RealmReader</em>\u2019s inputs.</p>
</li>
<li>
<p><strong>end_pos:</strong> (<code>torch.IntTensor</code> of shape <code>()</code>) \u2014 Predicted answer ending position in <em>RealmReader</em>\u2019s inputs.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.realm.modeling_realm.RealmReaderOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),We=new Rr({props:{$$slots:{default:[ah]},$$scope:{ctx:O}}}),no=new J({}),ro=new z({props:{name:"class transformers.RealmForOpenQA",anchor:"transformers.RealmForOpenQA",parameters:[{name:"config",val:""},{name:"retriever",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1722",parametersDescription:[{anchor:"transformers.RealmForOpenQA.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),so=new z({props:{name:"block_embedding_to",anchor:"transformers.RealmForOpenQA.block_embedding_to",parameters:[{name:"device",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1745",parametersDescription:[{anchor:"transformers.RealmForOpenQA.block_embedding_to.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>) &#x2014;
The device to which <code>self.block_emb</code> will be sent.`,name:"device"}]}}),lo=new z({props:{name:"forward",anchor:"transformers.RealmForOpenQA.forward",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"answer_ids",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1755",parametersDescription:[{anchor:"transformers.RealmForOpenQA.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(1, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmForOpenQA.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(1, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmForOpenQA.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(1, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token (should not be used in this model by design).</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmForOpenQA.forward.answer_ids",description:`<strong>answer_ids</strong> (<code>list</code> of shape <code>(num_answers, answer_length)</code>, <em>optional</em>) &#x2014;
Answer ids for computing the marginal log-likelihood loss. Indices should be in <code>[-1, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-1</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"answer_ids"},{anchor:"transformers.RealmForOpenQA.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <code>transformers.models.realm.modeling_realm.RealmForOpenQAOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li><strong>reader_output</strong> (<code>dict</code>) \u2014 Reader output.</li>
<li><strong>predicted_answer_ids</strong> (<code>torch.LongTensor</code> of shape <code>(answer_sequence_length)</code>) \u2014 Predicted answer ids.</li>
</ul>
`,returnType:`
<p><code>transformers.models.realm.modeling_realm.RealmForOpenQAOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),He=new Rr({props:{$$slots:{default:[sh]},$$scope:{ctx:O}}}),co=new Xe({props:{code:`import torch
from transformers import RealmForOpenQA, RealmRetriever, RealmTokenizer

retriever = RealmRetriever.from_pretrained("google/realm-orqa-nq-openqa")
tokenizer = RealmTokenizer.from_pretrained("google/realm-orqa-nq-openqa")
model = RealmForOpenQA.from_pretrained("google/realm-orqa-nq-openqa", retriever=retriever)

question = "Who is the pioneer in modern computer science?"
question_ids = tokenizer([question], return_tensors="pt")
answer_ids = tokenizer(
    ["alan mathison turing"],
    add_special_tokens=False,
    return_token_type_ids=False,
    return_attention_mask=False,
).input_ids

reader_output, predicted_answer_ids = model(**question_ids, answer_ids=answer_ids, return_dict=False)
predicted_answer = tokenizer.decode(predicted_answer_ids)
loss = reader_output.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmForOpenQA, RealmRetriever, RealmTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>retriever = RealmRetriever.from_pretrained(<span class="hljs-string">&quot;google/realm-orqa-nq-openqa&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-orqa-nq-openqa&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmForOpenQA.from_pretrained(<span class="hljs-string">&quot;google/realm-orqa-nq-openqa&quot;</span>, retriever=retriever)

<span class="hljs-meta">&gt;&gt;&gt; </span>question = <span class="hljs-string">&quot;Who is the pioneer in modern computer science?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>question_ids = tokenizer([question], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>answer_ids = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;alan mathison turing&quot;</span>],
<span class="hljs-meta">... </span>    add_special_tokens=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    return_token_type_ids=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    return_attention_mask=<span class="hljs-literal">False</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>reader_output, predicted_answer_ids = model(**question_ids, answer_ids=answer_ids, return_dict=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_answer = tokenizer.decode(predicted_answer_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = reader_output.loss`}}),{c(){h=n("meta"),y=d(),w=n("h1"),T=n("a"),$=n("span"),f(R.$$.fragment),b=d(),q=n("span"),ya=s("REALM"),Tr=d(),se=n("h2"),ye=n("a"),rn=n("span"),f(Ge.$$.fragment),Ea=d(),an=n("span"),za=s("Overview"),$r=d(),Ee=n("p"),qa=s("The REALM model was proposed in "),Je=n("a"),xa=s("REALM: Retrieval-Augmented Language Model Pre-Training"),Aa=s(` by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang. It\u2019s a
retrieval-augmented language model that firstly retrieves documents from a textual knowledge corpus and then
utilizes retrieved documents to process question answering tasks.`),yr=d(),po=n("p"),Pa=s("The abstract from the paper is the following:"),Er=d(),fo=n("p"),sn=n("em"),ja=s(`Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks
such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network,
requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we
augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend
over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the
first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language
modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We
demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the
challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both
explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous
methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as
interpretability and modularity.`),zr=d(),Z=n("p"),La=s("This model was contributed by "),Ze=n("a"),Ma=s("qqaatw"),Fa=s(`. The original code can be found
`),Ye=n("a"),Sa=s("here"),Ca=s("."),qr=d(),ie=n("h2"),ze=n("a"),dn=n("span"),f(et.$$.fragment),Ia=d(),ln=n("span"),Da=s("RealmConfig"),xr=d(),x=n("div"),f(tt.$$.fragment),Na=d(),cn=n("p"),Oa=s("This is the configuration class to store the configuration of"),Wa=d(),L=n("ol"),mn=n("li"),uo=n("a"),Ka=s("RealmEmbedder"),Qa=d(),hn=n("li"),go=n("a"),Ba=s("RealmScorer"),Ha=d(),pn=n("li"),_o=n("a"),Va=s("RealmKnowledgeAugEncoder"),Ua=d(),fn=n("li"),ko=n("a"),Xa=s("RealmRetriever"),Ga=d(),un=n("li"),vo=n("a"),Ja=s("RealmReader"),Za=d(),gn=n("li"),wo=n("a"),Ya=s("RealmForOpenQA"),es=d(),ot=n("p"),ts=s(`It is used to instantiate an REALM model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the REALM
`),nt=n("a"),os=s("realm-cc-news-pretrained"),ns=s(" architecture."),rs=d(),de=n("p"),as=s("Configuration objects inherit from "),bo=n("a"),ss=s("PretrainedConfig"),is=s(` and can be used to control the model outputs. Read the
documentation from `),Ro=n("a"),ds=s("PretrainedConfig"),ls=s(" for more information."),cs=d(),_n=n("p"),ms=s("Example:"),hs=d(),f(rt.$$.fragment),Ar=d(),le=n("h2"),qe=n("a"),kn=n("span"),f(at.$$.fragment),ps=d(),vn=n("span"),fs=s("RealmTokenizer"),Pr=d(),E=n("div"),f(st.$$.fragment),us=d(),wn=n("p"),gs=s("Construct a REALM tokenizer."),_s=d(),xe=n("p"),To=n("a"),ks=s("RealmTokenizer"),vs=s(" is identical to "),$o=n("a"),ws=s("BertTokenizer"),bs=s(` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),Rs=d(),it=n("p"),Ts=s("This tokenizer inherits from "),yo=n("a"),$s=s("PreTrainedTokenizer"),ys=s(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Es=d(),Y=n("div"),f(dt.$$.fragment),zs=d(),bn=n("p"),qs=s(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A REALM sequence has the following format:`),xs=d(),lt=n("ul"),Eo=n("li"),As=s("single sequence: "),Rn=n("code"),Ps=s("[CLS] X [SEP]"),js=d(),zo=n("li"),Ls=s("pair of sequences: "),Tn=n("code"),Ms=s("[CLS] A [SEP] B [SEP]"),Fs=d(),Ae=n("div"),f(ct.$$.fragment),Ss=d(),mt=n("p"),Cs=s(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),$n=n("code"),Is=s("prepare_for_model"),Ds=s(" method."),Ns=d(),W=n("div"),f(ht.$$.fragment),Os=d(),yn=n("p"),Ws=s(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A REALM sequence
pair mask has the following format:`),Ks=d(),f(pt.$$.fragment),Qs=d(),ce=n("p"),Bs=s("If "),En=n("code"),Hs=s("token_ids_1"),Vs=s(" is "),zn=n("code"),Us=s("None"),Xs=s(", this method only returns the first portion of the mask (0s)."),Gs=d(),qo=n("div"),f(ft.$$.fragment),Js=d(),P=n("div"),f(ut.$$.fragment),Zs=d(),gt=n("p"),Ys=s("Encode a batch of text or text pair. This method is similar to regular "),qn=n("strong"),ei=s("call"),ti=s(` method but has the following
differences:`),oi=d(),me=n("ol"),xn=n("li"),ni=s("Handle additional num_candidate axis. (batch_size, num_candidates, text)"),ri=d(),_t=n("li"),ai=s("Always pad the sequences to "),An=n("em"),si=s("max_length"),ii=s("."),di=d(),kt=n("li"),li=s("Must specify "),Pn=n("em"),ci=s("max_length"),mi=s(" in order to stack packs of candidates into a batch."),hi=d(),vt=n("ul"),xo=n("li"),pi=s("single sequence: "),jn=n("code"),fi=s("[CLS] X [SEP]"),ui=d(),Ao=n("li"),gi=s("pair of sequences: "),Ln=n("code"),_i=s("[CLS] A [SEP] B [SEP]"),ki=d(),Mn=n("p"),vi=s("Example:"),wi=d(),f(wt.$$.fragment),jr=d(),he=n("h2"),Pe=n("a"),Fn=n("span"),f(bt.$$.fragment),bi=d(),Sn=n("span"),Ri=s("RealmTokenizerFast"),Lr=d(),M=n("div"),f(Rt.$$.fragment),Ti=d(),Tt=n("p"),$i=s("Construct a \u201Cfast\u201D REALM tokenizer (backed by HuggingFace\u2019s "),Cn=n("em"),yi=s("tokenizers"),Ei=s(" library). Based on WordPiece."),zi=d(),je=n("p"),Po=n("a"),qi=s("RealmTokenizerFast"),xi=s(" is identical to "),jo=n("a"),Ai=s("BertTokenizerFast"),Pi=s(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),ji=d(),$t=n("p"),Li=s("This tokenizer inherits from "),Lo=n("a"),Mi=s("PreTrainedTokenizerFast"),Fi=s(` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),Si=d(),j=n("div"),f(yt.$$.fragment),Ci=d(),Et=n("p"),Ii=s("Encode a batch of text or text pair. This method is similar to regular "),In=n("strong"),Di=s("call"),Ni=s(` method but has the following
differences:`),Oi=d(),pe=n("ol"),Dn=n("li"),Wi=s("Handle additional num_candidate axis. (batch_size, num_candidates, text)"),Ki=d(),zt=n("li"),Qi=s("Always pad the sequences to "),Nn=n("em"),Bi=s("max_length"),Hi=s("."),Vi=d(),qt=n("li"),Ui=s("Must specify "),On=n("em"),Xi=s("max_length"),Gi=s(" in order to stack packs of candidates into a batch."),Ji=d(),xt=n("ul"),Mo=n("li"),Zi=s("single sequence: "),Wn=n("code"),Yi=s("[CLS] X [SEP]"),ed=d(),Fo=n("li"),td=s("pair of sequences: "),Kn=n("code"),od=s("[CLS] A [SEP] B [SEP]"),nd=d(),Qn=n("p"),rd=s("Example:"),ad=d(),f(At.$$.fragment),Mr=d(),fe=n("h2"),Le=n("a"),Bn=n("span"),f(Pt.$$.fragment),sd=d(),Hn=n("span"),id=s("RealmRetriever"),Fr=d(),H=n("div"),f(jt.$$.fragment),dd=d(),Vn=n("p"),ld=s(`The retriever of REALM outputting the retrieved evidence block and whether the block has answers as well as answer
positions.\u201D`),cd=d(),Me=n("div"),f(Lt.$$.fragment),md=d(),Un=n("p"),hd=s("check if retrieved_blocks has answers."),Sr=d(),ue=n("h2"),Fe=n("a"),Xn=n("span"),f(Mt.$$.fragment),pd=d(),Gn=n("span"),fd=s("RealmEmbedder"),Cr=d(),V=n("div"),f(Ft.$$.fragment),ud=d(),St=n("p"),gd=s(`The embedder of REALM outputting projected score that will be used to calculate relevance score.
This model is a PyTorch `),Ct=n("a"),_d=s("torch.nn.Module"),kd=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),vd=d(),F=n("div"),f(It.$$.fragment),wd=d(),ge=n("p"),bd=s("The "),So=n("a"),Rd=s("RealmEmbedder"),Td=s(" forward method, overrides the "),Jn=n("code"),$d=s("__call__"),yd=s(" special method."),Ed=d(),f(Se.$$.fragment),zd=d(),Zn=n("p"),qd=s("Example:"),xd=d(),f(Dt.$$.fragment),Ir=d(),_e=n("h2"),Ce=n("a"),Yn=n("span"),f(Nt.$$.fragment),Ad=d(),er=n("span"),Pd=s("RealmScorer"),Dr=d(),U=n("div"),f(Ot.$$.fragment),jd=d(),Wt=n("p"),Ld=s(`The scorer of REALM outputting relevance scores representing the score of document candidates (before softmax).
This model is a PyTorch `),Kt=n("a"),Md=s("torch.nn.Module"),Fd=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Sd=d(),S=n("div"),f(Qt.$$.fragment),Cd=d(),ke=n("p"),Id=s("The "),Co=n("a"),Dd=s("RealmScorer"),Nd=s(" forward method, overrides the "),tr=n("code"),Od=s("__call__"),Wd=s(" special method."),Kd=d(),f(Ie.$$.fragment),Qd=d(),or=n("p"),Bd=s("Example:"),Hd=d(),f(Bt.$$.fragment),Nr=d(),ve=n("h2"),De=n("a"),nr=n("span"),f(Ht.$$.fragment),Vd=d(),rr=n("span"),Ud=s("RealmKnowledgeAugEncoder"),Or=d(),X=n("div"),f(Vt.$$.fragment),Xd=d(),Ut=n("p"),Gd=s(`The knowledge-augmented encoder of REALM outputting masked language model logits and marginal log-likelihood loss.
This model is a PyTorch `),Xt=n("a"),Jd=s("torch.nn.Module"),Zd=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Yd=d(),C=n("div"),f(Gt.$$.fragment),el=d(),we=n("p"),tl=s("The "),Io=n("a"),ol=s("RealmKnowledgeAugEncoder"),nl=s(" forward method, overrides the "),ar=n("code"),rl=s("__call__"),al=s(" special method."),sl=d(),f(Ne.$$.fragment),il=d(),sr=n("p"),dl=s("Example:"),ll=d(),f(Jt.$$.fragment),Wr=d(),be=n("h2"),Oe=n("a"),ir=n("span"),f(Zt.$$.fragment),cl=d(),dr=n("span"),ml=s("RealmReader"),Kr=d(),G=n("div"),f(Yt.$$.fragment),hl=d(),eo=n("p"),pl=s(`The reader of REALM.
This model is a PyTorch `),to=n("a"),fl=s("torch.nn.Module"),ul=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),gl=d(),ee=n("div"),f(oo.$$.fragment),_l=d(),Re=n("p"),kl=s("The "),Do=n("a"),vl=s("RealmReader"),wl=s(" forward method, overrides the "),lr=n("code"),bl=s("__call__"),Rl=s(" special method."),Tl=d(),f(We.$$.fragment),Qr=d(),Te=n("h2"),Ke=n("a"),cr=n("span"),f(no.$$.fragment),$l=d(),mr=n("span"),yl=s("RealmForOpenQA"),Br=d(),N=n("div"),f(ro.$$.fragment),El=d(),Qe=n("p"),hr=n("code"),zl=s("RealmForOpenQA"),ql=s(` for end-to-end open domain question answering.
This model is a PyTorch `),ao=n("a"),xl=s("torch.nn.Module"),Al=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Pl=d(),Be=n("div"),f(so.$$.fragment),jl=d(),io=n("p"),Ll=s("Send "),pr=n("code"),Ml=s("self.block_emb"),Fl=s(" to a specific device."),Sl=d(),I=n("div"),f(lo.$$.fragment),Cl=d(),$e=n("p"),Il=s("The "),No=n("a"),Dl=s("RealmForOpenQA"),Nl=s(" forward method, overrides the "),fr=n("code"),Ol=s("__call__"),Wl=s(" special method."),Kl=d(),f(He.$$.fragment),Ql=d(),ur=n("p"),Bl=s("Example:"),Hl=d(),f(co.$$.fragment),this.h()},l(t){const m=eh('[data-svelte="svelte-1phssyn"]',document.head);h=r(m,"META",{name:!0,content:!0}),m.forEach(o),y=l(t),w=r(t,"H1",{class:!0});var mo=a(w);T=r(mo,"A",{id:!0,class:!0,href:!0});var gr=a(T);$=r(gr,"SPAN",{});var _r=a($);u(R.$$.fragment,_r),_r.forEach(o),gr.forEach(o),b=l(mo),q=r(mo,"SPAN",{});var kr=a(q);ya=i(kr,"REALM"),kr.forEach(o),mo.forEach(o),Tr=l(t),se=r(t,"H2",{class:!0});var ho=a(se);ye=r(ho,"A",{id:!0,class:!0,href:!0});var Yl=a(ye);rn=r(Yl,"SPAN",{});var ec=a(rn);u(Ge.$$.fragment,ec),ec.forEach(o),Yl.forEach(o),Ea=l(ho),an=r(ho,"SPAN",{});var tc=a(an);za=i(tc,"Overview"),tc.forEach(o),ho.forEach(o),$r=l(t),Ee=r(t,"P",{});var Vr=a(Ee);qa=i(Vr,"The REALM model was proposed in "),Je=r(Vr,"A",{href:!0,rel:!0});var oc=a(Je);xa=i(oc,"REALM: Retrieval-Augmented Language Model Pre-Training"),oc.forEach(o),Aa=i(Vr,` by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang. It\u2019s a
retrieval-augmented language model that firstly retrieves documents from a textual knowledge corpus and then
utilizes retrieved documents to process question answering tasks.`),Vr.forEach(o),yr=l(t),po=r(t,"P",{});var nc=a(po);Pa=i(nc,"The abstract from the paper is the following:"),nc.forEach(o),Er=l(t),fo=r(t,"P",{});var rc=a(fo);sn=r(rc,"EM",{});var ac=a(sn);ja=i(ac,`Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks
such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network,
requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we
augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend
over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the
first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language
modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We
demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the
challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both
explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous
methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as
interpretability and modularity.`),ac.forEach(o),rc.forEach(o),zr=l(t),Z=r(t,"P",{});var Oo=a(Z);La=i(Oo,"This model was contributed by "),Ze=r(Oo,"A",{href:!0,rel:!0});var sc=a(Ze);Ma=i(sc,"qqaatw"),sc.forEach(o),Fa=i(Oo,`. The original code can be found
`),Ye=r(Oo,"A",{href:!0,rel:!0});var ic=a(Ye);Sa=i(ic,"here"),ic.forEach(o),Ca=i(Oo,"."),Oo.forEach(o),qr=l(t),ie=r(t,"H2",{class:!0});var Ur=a(ie);ze=r(Ur,"A",{id:!0,class:!0,href:!0});var dc=a(ze);dn=r(dc,"SPAN",{});var lc=a(dn);u(et.$$.fragment,lc),lc.forEach(o),dc.forEach(o),Ia=l(Ur),ln=r(Ur,"SPAN",{});var cc=a(ln);Da=i(cc,"RealmConfig"),cc.forEach(o),Ur.forEach(o),xr=l(t),x=r(t,"DIV",{class:!0});var D=a(x);u(tt.$$.fragment,D),Na=l(D),cn=r(D,"P",{});var mc=a(cn);Oa=i(mc,"This is the configuration class to store the configuration of"),mc.forEach(o),Wa=l(D),L=r(D,"OL",{});var K=a(L);mn=r(K,"LI",{});var hc=a(mn);uo=r(hc,"A",{href:!0});var pc=a(uo);Ka=i(pc,"RealmEmbedder"),pc.forEach(o),hc.forEach(o),Qa=l(K),hn=r(K,"LI",{});var fc=a(hn);go=r(fc,"A",{href:!0});var uc=a(go);Ba=i(uc,"RealmScorer"),uc.forEach(o),fc.forEach(o),Ha=l(K),pn=r(K,"LI",{});var gc=a(pn);_o=r(gc,"A",{href:!0});var _c=a(_o);Va=i(_c,"RealmKnowledgeAugEncoder"),_c.forEach(o),gc.forEach(o),Ua=l(K),fn=r(K,"LI",{});var kc=a(fn);ko=r(kc,"A",{href:!0});var vc=a(ko);Xa=i(vc,"RealmRetriever"),vc.forEach(o),kc.forEach(o),Ga=l(K),un=r(K,"LI",{});var wc=a(un);vo=r(wc,"A",{href:!0});var bc=a(vo);Ja=i(bc,"RealmReader"),bc.forEach(o),wc.forEach(o),Za=l(K),gn=r(K,"LI",{});var Rc=a(gn);wo=r(Rc,"A",{href:!0});var Tc=a(wo);Ya=i(Tc,"RealmForOpenQA"),Tc.forEach(o),Rc.forEach(o),K.forEach(o),es=l(D),ot=r(D,"P",{});var Xr=a(ot);ts=i(Xr,`It is used to instantiate an REALM model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the REALM
`),nt=r(Xr,"A",{href:!0,rel:!0});var $c=a(nt);os=i($c,"realm-cc-news-pretrained"),$c.forEach(o),ns=i(Xr," architecture."),Xr.forEach(o),rs=l(D),de=r(D,"P",{});var Wo=a(de);as=i(Wo,"Configuration objects inherit from "),bo=r(Wo,"A",{href:!0});var yc=a(bo);ss=i(yc,"PretrainedConfig"),yc.forEach(o),is=i(Wo,` and can be used to control the model outputs. Read the
documentation from `),Ro=r(Wo,"A",{href:!0});var Ec=a(Ro);ds=i(Ec,"PretrainedConfig"),Ec.forEach(o),ls=i(Wo," for more information."),Wo.forEach(o),cs=l(D),_n=r(D,"P",{});var zc=a(_n);ms=i(zc,"Example:"),zc.forEach(o),hs=l(D),u(rt.$$.fragment,D),D.forEach(o),Ar=l(t),le=r(t,"H2",{class:!0});var Gr=a(le);qe=r(Gr,"A",{id:!0,class:!0,href:!0});var qc=a(qe);kn=r(qc,"SPAN",{});var xc=a(kn);u(at.$$.fragment,xc),xc.forEach(o),qc.forEach(o),ps=l(Gr),vn=r(Gr,"SPAN",{});var Ac=a(vn);fs=i(Ac,"RealmTokenizer"),Ac.forEach(o),Gr.forEach(o),Pr=l(t),E=r(t,"DIV",{class:!0});var A=a(E);u(st.$$.fragment,A),us=l(A),wn=r(A,"P",{});var Pc=a(wn);gs=i(Pc,"Construct a REALM tokenizer."),Pc.forEach(o),_s=l(A),xe=r(A,"P",{});var vr=a(xe);To=r(vr,"A",{href:!0});var jc=a(To);ks=i(jc,"RealmTokenizer"),jc.forEach(o),vs=i(vr," is identical to "),$o=r(vr,"A",{href:!0});var Lc=a($o);ws=i(Lc,"BertTokenizer"),Lc.forEach(o),bs=i(vr,` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),vr.forEach(o),Rs=l(A),it=r(A,"P",{});var Jr=a(it);Ts=i(Jr,"This tokenizer inherits from "),yo=r(Jr,"A",{href:!0});var Mc=a(yo);$s=i(Mc,"PreTrainedTokenizer"),Mc.forEach(o),ys=i(Jr,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Jr.forEach(o),Es=l(A),Y=r(A,"DIV",{class:!0});var Ko=a(Y);u(dt.$$.fragment,Ko),zs=l(Ko),bn=r(Ko,"P",{});var Fc=a(bn);qs=i(Fc,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A REALM sequence has the following format:`),Fc.forEach(o),xs=l(Ko),lt=r(Ko,"UL",{});var Zr=a(lt);Eo=r(Zr,"LI",{});var Vl=a(Eo);As=i(Vl,"single sequence: "),Rn=r(Vl,"CODE",{});var Sc=a(Rn);Ps=i(Sc,"[CLS] X [SEP]"),Sc.forEach(o),Vl.forEach(o),js=l(Zr),zo=r(Zr,"LI",{});var Ul=a(zo);Ls=i(Ul,"pair of sequences: "),Tn=r(Ul,"CODE",{});var Cc=a(Tn);Ms=i(Cc,"[CLS] A [SEP] B [SEP]"),Cc.forEach(o),Ul.forEach(o),Zr.forEach(o),Ko.forEach(o),Fs=l(A),Ae=r(A,"DIV",{class:!0});var Yr=a(Ae);u(ct.$$.fragment,Yr),Ss=l(Yr),mt=r(Yr,"P",{});var ea=a(mt);Cs=i(ea,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),$n=r(ea,"CODE",{});var Ic=a($n);Is=i(Ic,"prepare_for_model"),Ic.forEach(o),Ds=i(ea," method."),ea.forEach(o),Yr.forEach(o),Ns=l(A),W=r(A,"DIV",{class:!0});var Ve=a(W);u(ht.$$.fragment,Ve),Os=l(Ve),yn=r(Ve,"P",{});var Dc=a(yn);Ws=i(Dc,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A REALM sequence
pair mask has the following format:`),Dc.forEach(o),Ks=l(Ve),u(pt.$$.fragment,Ve),Qs=l(Ve),ce=r(Ve,"P",{});var Qo=a(ce);Bs=i(Qo,"If "),En=r(Qo,"CODE",{});var Nc=a(En);Hs=i(Nc,"token_ids_1"),Nc.forEach(o),Vs=i(Qo," is "),zn=r(Qo,"CODE",{});var Oc=a(zn);Us=i(Oc,"None"),Oc.forEach(o),Xs=i(Qo,", this method only returns the first portion of the mask (0s)."),Qo.forEach(o),Ve.forEach(o),Gs=l(A),qo=r(A,"DIV",{class:!0});var Wc=a(qo);u(ft.$$.fragment,Wc),Wc.forEach(o),Js=l(A),P=r(A,"DIV",{class:!0});var Q=a(P);u(ut.$$.fragment,Q),Zs=l(Q),gt=r(Q,"P",{});var ta=a(gt);Ys=i(ta,"Encode a batch of text or text pair. This method is similar to regular "),qn=r(ta,"STRONG",{});var Kc=a(qn);ei=i(Kc,"call"),Kc.forEach(o),ti=i(ta,` method but has the following
differences:`),ta.forEach(o),oi=l(Q),me=r(Q,"OL",{});var Bo=a(me);xn=r(Bo,"LI",{});var Qc=a(xn);ni=i(Qc,"Handle additional num_candidate axis. (batch_size, num_candidates, text)"),Qc.forEach(o),ri=l(Bo),_t=r(Bo,"LI",{});var oa=a(_t);ai=i(oa,"Always pad the sequences to "),An=r(oa,"EM",{});var Bc=a(An);si=i(Bc,"max_length"),Bc.forEach(o),ii=i(oa,"."),oa.forEach(o),di=l(Bo),kt=r(Bo,"LI",{});var na=a(kt);li=i(na,"Must specify "),Pn=r(na,"EM",{});var Hc=a(Pn);ci=i(Hc,"max_length"),Hc.forEach(o),mi=i(na," in order to stack packs of candidates into a batch."),na.forEach(o),Bo.forEach(o),hi=l(Q),vt=r(Q,"UL",{});var ra=a(vt);xo=r(ra,"LI",{});var Xl=a(xo);pi=i(Xl,"single sequence: "),jn=r(Xl,"CODE",{});var Vc=a(jn);fi=i(Vc,"[CLS] X [SEP]"),Vc.forEach(o),Xl.forEach(o),ui=l(ra),Ao=r(ra,"LI",{});var Gl=a(Ao);gi=i(Gl,"pair of sequences: "),Ln=r(Gl,"CODE",{});var Uc=a(Ln);_i=i(Uc,"[CLS] A [SEP] B [SEP]"),Uc.forEach(o),Gl.forEach(o),ra.forEach(o),ki=l(Q),Mn=r(Q,"P",{});var Xc=a(Mn);vi=i(Xc,"Example:"),Xc.forEach(o),wi=l(Q),u(wt.$$.fragment,Q),Q.forEach(o),A.forEach(o),jr=l(t),he=r(t,"H2",{class:!0});var aa=a(he);Pe=r(aa,"A",{id:!0,class:!0,href:!0});var Gc=a(Pe);Fn=r(Gc,"SPAN",{});var Jc=a(Fn);u(bt.$$.fragment,Jc),Jc.forEach(o),Gc.forEach(o),bi=l(aa),Sn=r(aa,"SPAN",{});var Zc=a(Sn);Ri=i(Zc,"RealmTokenizerFast"),Zc.forEach(o),aa.forEach(o),Lr=l(t),M=r(t,"DIV",{class:!0});var te=a(M);u(Rt.$$.fragment,te),Ti=l(te),Tt=r(te,"P",{});var sa=a(Tt);$i=i(sa,"Construct a \u201Cfast\u201D REALM tokenizer (backed by HuggingFace\u2019s "),Cn=r(sa,"EM",{});var Yc=a(Cn);yi=i(Yc,"tokenizers"),Yc.forEach(o),Ei=i(sa," library). Based on WordPiece."),sa.forEach(o),zi=l(te),je=r(te,"P",{});var wr=a(je);Po=r(wr,"A",{href:!0});var em=a(Po);qi=i(em,"RealmTokenizerFast"),em.forEach(o),xi=i(wr," is identical to "),jo=r(wr,"A",{href:!0});var tm=a(jo);Ai=i(tm,"BertTokenizerFast"),tm.forEach(o),Pi=i(wr,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),wr.forEach(o),ji=l(te),$t=r(te,"P",{});var ia=a($t);Li=i(ia,"This tokenizer inherits from "),Lo=r(ia,"A",{href:!0});var om=a(Lo);Mi=i(om,"PreTrainedTokenizerFast"),om.forEach(o),Fi=i(ia,` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),ia.forEach(o),Si=l(te),j=r(te,"DIV",{class:!0});var B=a(j);u(yt.$$.fragment,B),Ci=l(B),Et=r(B,"P",{});var da=a(Et);Ii=i(da,"Encode a batch of text or text pair. This method is similar to regular "),In=r(da,"STRONG",{});var nm=a(In);Di=i(nm,"call"),nm.forEach(o),Ni=i(da,` method but has the following
differences:`),da.forEach(o),Oi=l(B),pe=r(B,"OL",{});var Ho=a(pe);Dn=r(Ho,"LI",{});var rm=a(Dn);Wi=i(rm,"Handle additional num_candidate axis. (batch_size, num_candidates, text)"),rm.forEach(o),Ki=l(Ho),zt=r(Ho,"LI",{});var la=a(zt);Qi=i(la,"Always pad the sequences to "),Nn=r(la,"EM",{});var am=a(Nn);Bi=i(am,"max_length"),am.forEach(o),Hi=i(la,"."),la.forEach(o),Vi=l(Ho),qt=r(Ho,"LI",{});var ca=a(qt);Ui=i(ca,"Must specify "),On=r(ca,"EM",{});var sm=a(On);Xi=i(sm,"max_length"),sm.forEach(o),Gi=i(ca," in order to stack packs of candidates into a batch."),ca.forEach(o),Ho.forEach(o),Ji=l(B),xt=r(B,"UL",{});var ma=a(xt);Mo=r(ma,"LI",{});var Jl=a(Mo);Zi=i(Jl,"single sequence: "),Wn=r(Jl,"CODE",{});var im=a(Wn);Yi=i(im,"[CLS] X [SEP]"),im.forEach(o),Jl.forEach(o),ed=l(ma),Fo=r(ma,"LI",{});var Zl=a(Fo);td=i(Zl,"pair of sequences: "),Kn=r(Zl,"CODE",{});var dm=a(Kn);od=i(dm,"[CLS] A [SEP] B [SEP]"),dm.forEach(o),Zl.forEach(o),ma.forEach(o),nd=l(B),Qn=r(B,"P",{});var lm=a(Qn);rd=i(lm,"Example:"),lm.forEach(o),ad=l(B),u(At.$$.fragment,B),B.forEach(o),te.forEach(o),Mr=l(t),fe=r(t,"H2",{class:!0});var ha=a(fe);Le=r(ha,"A",{id:!0,class:!0,href:!0});var cm=a(Le);Bn=r(cm,"SPAN",{});var mm=a(Bn);u(Pt.$$.fragment,mm),mm.forEach(o),cm.forEach(o),sd=l(ha),Hn=r(ha,"SPAN",{});var hm=a(Hn);id=i(hm,"RealmRetriever"),hm.forEach(o),ha.forEach(o),Fr=l(t),H=r(t,"DIV",{class:!0});var Vo=a(H);u(jt.$$.fragment,Vo),dd=l(Vo),Vn=r(Vo,"P",{});var pm=a(Vn);ld=i(pm,`The retriever of REALM outputting the retrieved evidence block and whether the block has answers as well as answer
positions.\u201D`),pm.forEach(o),cd=l(Vo),Me=r(Vo,"DIV",{class:!0});var pa=a(Me);u(Lt.$$.fragment,pa),md=l(pa),Un=r(pa,"P",{});var fm=a(Un);hd=i(fm,"check if retrieved_blocks has answers."),fm.forEach(o),pa.forEach(o),Vo.forEach(o),Sr=l(t),ue=r(t,"H2",{class:!0});var fa=a(ue);Fe=r(fa,"A",{id:!0,class:!0,href:!0});var um=a(Fe);Xn=r(um,"SPAN",{});var gm=a(Xn);u(Mt.$$.fragment,gm),gm.forEach(o),um.forEach(o),pd=l(fa),Gn=r(fa,"SPAN",{});var _m=a(Gn);fd=i(_m,"RealmEmbedder"),_m.forEach(o),fa.forEach(o),Cr=l(t),V=r(t,"DIV",{class:!0});var Uo=a(V);u(Ft.$$.fragment,Uo),ud=l(Uo),St=r(Uo,"P",{});var ua=a(St);gd=i(ua,`The embedder of REALM outputting projected score that will be used to calculate relevance score.
This model is a PyTorch `),Ct=r(ua,"A",{href:!0,rel:!0});var km=a(Ct);_d=i(km,"torch.nn.Module"),km.forEach(o),kd=i(ua,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ua.forEach(o),vd=l(Uo),F=r(Uo,"DIV",{class:!0});var oe=a(F);u(It.$$.fragment,oe),wd=l(oe),ge=r(oe,"P",{});var Xo=a(ge);bd=i(Xo,"The "),So=r(Xo,"A",{href:!0});var vm=a(So);Rd=i(vm,"RealmEmbedder"),vm.forEach(o),Td=i(Xo," forward method, overrides the "),Jn=r(Xo,"CODE",{});var wm=a(Jn);$d=i(wm,"__call__"),wm.forEach(o),yd=i(Xo," special method."),Xo.forEach(o),Ed=l(oe),u(Se.$$.fragment,oe),zd=l(oe),Zn=r(oe,"P",{});var bm=a(Zn);qd=i(bm,"Example:"),bm.forEach(o),xd=l(oe),u(Dt.$$.fragment,oe),oe.forEach(o),Uo.forEach(o),Ir=l(t),_e=r(t,"H2",{class:!0});var ga=a(_e);Ce=r(ga,"A",{id:!0,class:!0,href:!0});var Rm=a(Ce);Yn=r(Rm,"SPAN",{});var Tm=a(Yn);u(Nt.$$.fragment,Tm),Tm.forEach(o),Rm.forEach(o),Ad=l(ga),er=r(ga,"SPAN",{});var $m=a(er);Pd=i($m,"RealmScorer"),$m.forEach(o),ga.forEach(o),Dr=l(t),U=r(t,"DIV",{class:!0});var Go=a(U);u(Ot.$$.fragment,Go),jd=l(Go),Wt=r(Go,"P",{});var _a=a(Wt);Ld=i(_a,`The scorer of REALM outputting relevance scores representing the score of document candidates (before softmax).
This model is a PyTorch `),Kt=r(_a,"A",{href:!0,rel:!0});var ym=a(Kt);Md=i(ym,"torch.nn.Module"),ym.forEach(o),Fd=i(_a,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),_a.forEach(o),Sd=l(Go),S=r(Go,"DIV",{class:!0});var ne=a(S);u(Qt.$$.fragment,ne),Cd=l(ne),ke=r(ne,"P",{});var Jo=a(ke);Id=i(Jo,"The "),Co=r(Jo,"A",{href:!0});var Em=a(Co);Dd=i(Em,"RealmScorer"),Em.forEach(o),Nd=i(Jo," forward method, overrides the "),tr=r(Jo,"CODE",{});var zm=a(tr);Od=i(zm,"__call__"),zm.forEach(o),Wd=i(Jo," special method."),Jo.forEach(o),Kd=l(ne),u(Ie.$$.fragment,ne),Qd=l(ne),or=r(ne,"P",{});var qm=a(or);Bd=i(qm,"Example:"),qm.forEach(o),Hd=l(ne),u(Bt.$$.fragment,ne),ne.forEach(o),Go.forEach(o),Nr=l(t),ve=r(t,"H2",{class:!0});var ka=a(ve);De=r(ka,"A",{id:!0,class:!0,href:!0});var xm=a(De);nr=r(xm,"SPAN",{});var Am=a(nr);u(Ht.$$.fragment,Am),Am.forEach(o),xm.forEach(o),Vd=l(ka),rr=r(ka,"SPAN",{});var Pm=a(rr);Ud=i(Pm,"RealmKnowledgeAugEncoder"),Pm.forEach(o),ka.forEach(o),Or=l(t),X=r(t,"DIV",{class:!0});var Zo=a(X);u(Vt.$$.fragment,Zo),Xd=l(Zo),Ut=r(Zo,"P",{});var va=a(Ut);Gd=i(va,`The knowledge-augmented encoder of REALM outputting masked language model logits and marginal log-likelihood loss.
This model is a PyTorch `),Xt=r(va,"A",{href:!0,rel:!0});var jm=a(Xt);Jd=i(jm,"torch.nn.Module"),jm.forEach(o),Zd=i(va,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),va.forEach(o),Yd=l(Zo),C=r(Zo,"DIV",{class:!0});var re=a(C);u(Gt.$$.fragment,re),el=l(re),we=r(re,"P",{});var Yo=a(we);tl=i(Yo,"The "),Io=r(Yo,"A",{href:!0});var Lm=a(Io);ol=i(Lm,"RealmKnowledgeAugEncoder"),Lm.forEach(o),nl=i(Yo," forward method, overrides the "),ar=r(Yo,"CODE",{});var Mm=a(ar);rl=i(Mm,"__call__"),Mm.forEach(o),al=i(Yo," special method."),Yo.forEach(o),sl=l(re),u(Ne.$$.fragment,re),il=l(re),sr=r(re,"P",{});var Fm=a(sr);dl=i(Fm,"Example:"),Fm.forEach(o),ll=l(re),u(Jt.$$.fragment,re),re.forEach(o),Zo.forEach(o),Wr=l(t),be=r(t,"H2",{class:!0});var wa=a(be);Oe=r(wa,"A",{id:!0,class:!0,href:!0});var Sm=a(Oe);ir=r(Sm,"SPAN",{});var Cm=a(ir);u(Zt.$$.fragment,Cm),Cm.forEach(o),Sm.forEach(o),cl=l(wa),dr=r(wa,"SPAN",{});var Im=a(dr);ml=i(Im,"RealmReader"),Im.forEach(o),wa.forEach(o),Kr=l(t),G=r(t,"DIV",{class:!0});var en=a(G);u(Yt.$$.fragment,en),hl=l(en),eo=r(en,"P",{});var ba=a(eo);pl=i(ba,`The reader of REALM.
This model is a PyTorch `),to=r(ba,"A",{href:!0,rel:!0});var Dm=a(to);fl=i(Dm,"torch.nn.Module"),Dm.forEach(o),ul=i(ba,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ba.forEach(o),gl=l(en),ee=r(en,"DIV",{class:!0});var tn=a(ee);u(oo.$$.fragment,tn),_l=l(tn),Re=r(tn,"P",{});var on=a(Re);kl=i(on,"The "),Do=r(on,"A",{href:!0});var Nm=a(Do);vl=i(Nm,"RealmReader"),Nm.forEach(o),wl=i(on," forward method, overrides the "),lr=r(on,"CODE",{});var Om=a(lr);bl=i(Om,"__call__"),Om.forEach(o),Rl=i(on," special method."),on.forEach(o),Tl=l(tn),u(We.$$.fragment,tn),tn.forEach(o),en.forEach(o),Qr=l(t),Te=r(t,"H2",{class:!0});var Ra=a(Te);Ke=r(Ra,"A",{id:!0,class:!0,href:!0});var Wm=a(Ke);cr=r(Wm,"SPAN",{});var Km=a(cr);u(no.$$.fragment,Km),Km.forEach(o),Wm.forEach(o),$l=l(Ra),mr=r(Ra,"SPAN",{});var Qm=a(mr);yl=i(Qm,"RealmForOpenQA"),Qm.forEach(o),Ra.forEach(o),Br=l(t),N=r(t,"DIV",{class:!0});var Ue=a(N);u(ro.$$.fragment,Ue),El=l(Ue),Qe=r(Ue,"P",{});var br=a(Qe);hr=r(br,"CODE",{});var Bm=a(hr);zl=i(Bm,"RealmForOpenQA"),Bm.forEach(o),ql=i(br,` for end-to-end open domain question answering.
This model is a PyTorch `),ao=r(br,"A",{href:!0,rel:!0});var Hm=a(ao);xl=i(Hm,"torch.nn.Module"),Hm.forEach(o),Al=i(br,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),br.forEach(o),Pl=l(Ue),Be=r(Ue,"DIV",{class:!0});var Ta=a(Be);u(so.$$.fragment,Ta),jl=l(Ta),io=r(Ta,"P",{});var $a=a(io);Ll=i($a,"Send "),pr=r($a,"CODE",{});var Vm=a(pr);Ml=i(Vm,"self.block_emb"),Vm.forEach(o),Fl=i($a," to a specific device."),$a.forEach(o),Ta.forEach(o),Sl=l(Ue),I=r(Ue,"DIV",{class:!0});var ae=a(I);u(lo.$$.fragment,ae),Cl=l(ae),$e=r(ae,"P",{});var nn=a($e);Il=i(nn,"The "),No=r(nn,"A",{href:!0});var Um=a(No);Dl=i(Um,"RealmForOpenQA"),Um.forEach(o),Nl=i(nn," forward method, overrides the "),fr=r(nn,"CODE",{});var Xm=a(fr);Ol=i(Xm,"__call__"),Xm.forEach(o),Wl=i(nn," special method."),nn.forEach(o),Kl=l(ae),u(He.$$.fragment,ae),Ql=l(ae),ur=r(ae,"P",{});var Gm=a(ur);Bl=i(Gm,"Example:"),Gm.forEach(o),Hl=l(ae),u(co.$$.fragment,ae),ae.forEach(o),Ue.forEach(o),this.h()},h(){c(h,"name","hf:doc:metadata"),c(h,"content",JSON.stringify(dh)),c(T,"id","realm"),c(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(T,"href","#realm"),c(w,"class","relative group"),c(ye,"id","overview"),c(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ye,"href","#overview"),c(se,"class","relative group"),c(Je,"href","https://arxiv.org/abs/2002.08909"),c(Je,"rel","nofollow"),c(Ze,"href","https://huggingface.co/qqaatw"),c(Ze,"rel","nofollow"),c(Ye,"href","https://github.com/google-research/language/tree/master/language/realm"),c(Ye,"rel","nofollow"),c(ze,"id","transformers.RealmConfig"),c(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ze,"href","#transformers.RealmConfig"),c(ie,"class","relative group"),c(uo,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmEmbedder"),c(go,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmScorer"),c(_o,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmKnowledgeAugEncoder"),c(ko,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmRetriever"),c(vo,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmReader"),c(wo,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmForOpenQA"),c(nt,"href","https://huggingface.co/google/realm-cc-news-pretrained-embedder"),c(nt,"rel","nofollow"),c(bo,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(Ro,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(x,"class","docstring"),c(qe,"id","transformers.RealmTokenizer"),c(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qe,"href","#transformers.RealmTokenizer"),c(le,"class","relative group"),c(To,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmTokenizer"),c($o,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),c(yo,"href","/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(Y,"class","docstring"),c(Ae,"class","docstring"),c(W,"class","docstring"),c(qo,"class","docstring"),c(P,"class","docstring"),c(E,"class","docstring"),c(Pe,"id","transformers.RealmTokenizerFast"),c(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Pe,"href","#transformers.RealmTokenizerFast"),c(he,"class","relative group"),c(Po,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmTokenizerFast"),c(jo,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),c(Lo,"href","/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),c(j,"class","docstring"),c(M,"class","docstring"),c(Le,"id","transformers.RealmRetriever"),c(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Le,"href","#transformers.RealmRetriever"),c(fe,"class","relative group"),c(Me,"class","docstring"),c(H,"class","docstring"),c(Fe,"id","transformers.RealmEmbedder"),c(Fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fe,"href","#transformers.RealmEmbedder"),c(ue,"class","relative group"),c(Ct,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ct,"rel","nofollow"),c(So,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmEmbedder"),c(F,"class","docstring"),c(V,"class","docstring"),c(Ce,"id","transformers.RealmScorer"),c(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ce,"href","#transformers.RealmScorer"),c(_e,"class","relative group"),c(Kt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Kt,"rel","nofollow"),c(Co,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmScorer"),c(S,"class","docstring"),c(U,"class","docstring"),c(De,"id","transformers.RealmKnowledgeAugEncoder"),c(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(De,"href","#transformers.RealmKnowledgeAugEncoder"),c(ve,"class","relative group"),c(Xt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Xt,"rel","nofollow"),c(Io,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmKnowledgeAugEncoder"),c(C,"class","docstring"),c(X,"class","docstring"),c(Oe,"id","transformers.RealmReader"),c(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Oe,"href","#transformers.RealmReader"),c(be,"class","relative group"),c(to,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(to,"rel","nofollow"),c(Do,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmReader"),c(ee,"class","docstring"),c(G,"class","docstring"),c(Ke,"id","transformers.RealmForOpenQA"),c(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ke,"href","#transformers.RealmForOpenQA"),c(Te,"class","relative group"),c(ao,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(ao,"rel","nofollow"),c(Be,"class","docstring"),c(No,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmForOpenQA"),c(I,"class","docstring"),c(N,"class","docstring")},m(t,m){e(document.head,h),p(t,y,m),p(t,w,m),e(w,T),e(T,$),g(R,$,null),e(w,b),e(w,q),e(q,ya),p(t,Tr,m),p(t,se,m),e(se,ye),e(ye,rn),g(Ge,rn,null),e(se,Ea),e(se,an),e(an,za),p(t,$r,m),p(t,Ee,m),e(Ee,qa),e(Ee,Je),e(Je,xa),e(Ee,Aa),p(t,yr,m),p(t,po,m),e(po,Pa),p(t,Er,m),p(t,fo,m),e(fo,sn),e(sn,ja),p(t,zr,m),p(t,Z,m),e(Z,La),e(Z,Ze),e(Ze,Ma),e(Z,Fa),e(Z,Ye),e(Ye,Sa),e(Z,Ca),p(t,qr,m),p(t,ie,m),e(ie,ze),e(ze,dn),g(et,dn,null),e(ie,Ia),e(ie,ln),e(ln,Da),p(t,xr,m),p(t,x,m),g(tt,x,null),e(x,Na),e(x,cn),e(cn,Oa),e(x,Wa),e(x,L),e(L,mn),e(mn,uo),e(uo,Ka),e(L,Qa),e(L,hn),e(hn,go),e(go,Ba),e(L,Ha),e(L,pn),e(pn,_o),e(_o,Va),e(L,Ua),e(L,fn),e(fn,ko),e(ko,Xa),e(L,Ga),e(L,un),e(un,vo),e(vo,Ja),e(L,Za),e(L,gn),e(gn,wo),e(wo,Ya),e(x,es),e(x,ot),e(ot,ts),e(ot,nt),e(nt,os),e(ot,ns),e(x,rs),e(x,de),e(de,as),e(de,bo),e(bo,ss),e(de,is),e(de,Ro),e(Ro,ds),e(de,ls),e(x,cs),e(x,_n),e(_n,ms),e(x,hs),g(rt,x,null),p(t,Ar,m),p(t,le,m),e(le,qe),e(qe,kn),g(at,kn,null),e(le,ps),e(le,vn),e(vn,fs),p(t,Pr,m),p(t,E,m),g(st,E,null),e(E,us),e(E,wn),e(wn,gs),e(E,_s),e(E,xe),e(xe,To),e(To,ks),e(xe,vs),e(xe,$o),e($o,ws),e(xe,bs),e(E,Rs),e(E,it),e(it,Ts),e(it,yo),e(yo,$s),e(it,ys),e(E,Es),e(E,Y),g(dt,Y,null),e(Y,zs),e(Y,bn),e(bn,qs),e(Y,xs),e(Y,lt),e(lt,Eo),e(Eo,As),e(Eo,Rn),e(Rn,Ps),e(lt,js),e(lt,zo),e(zo,Ls),e(zo,Tn),e(Tn,Ms),e(E,Fs),e(E,Ae),g(ct,Ae,null),e(Ae,Ss),e(Ae,mt),e(mt,Cs),e(mt,$n),e($n,Is),e(mt,Ds),e(E,Ns),e(E,W),g(ht,W,null),e(W,Os),e(W,yn),e(yn,Ws),e(W,Ks),g(pt,W,null),e(W,Qs),e(W,ce),e(ce,Bs),e(ce,En),e(En,Hs),e(ce,Vs),e(ce,zn),e(zn,Us),e(ce,Xs),e(E,Gs),e(E,qo),g(ft,qo,null),e(E,Js),e(E,P),g(ut,P,null),e(P,Zs),e(P,gt),e(gt,Ys),e(gt,qn),e(qn,ei),e(gt,ti),e(P,oi),e(P,me),e(me,xn),e(xn,ni),e(me,ri),e(me,_t),e(_t,ai),e(_t,An),e(An,si),e(_t,ii),e(me,di),e(me,kt),e(kt,li),e(kt,Pn),e(Pn,ci),e(kt,mi),e(P,hi),e(P,vt),e(vt,xo),e(xo,pi),e(xo,jn),e(jn,fi),e(vt,ui),e(vt,Ao),e(Ao,gi),e(Ao,Ln),e(Ln,_i),e(P,ki),e(P,Mn),e(Mn,vi),e(P,wi),g(wt,P,null),p(t,jr,m),p(t,he,m),e(he,Pe),e(Pe,Fn),g(bt,Fn,null),e(he,bi),e(he,Sn),e(Sn,Ri),p(t,Lr,m),p(t,M,m),g(Rt,M,null),e(M,Ti),e(M,Tt),e(Tt,$i),e(Tt,Cn),e(Cn,yi),e(Tt,Ei),e(M,zi),e(M,je),e(je,Po),e(Po,qi),e(je,xi),e(je,jo),e(jo,Ai),e(je,Pi),e(M,ji),e(M,$t),e($t,Li),e($t,Lo),e(Lo,Mi),e($t,Fi),e(M,Si),e(M,j),g(yt,j,null),e(j,Ci),e(j,Et),e(Et,Ii),e(Et,In),e(In,Di),e(Et,Ni),e(j,Oi),e(j,pe),e(pe,Dn),e(Dn,Wi),e(pe,Ki),e(pe,zt),e(zt,Qi),e(zt,Nn),e(Nn,Bi),e(zt,Hi),e(pe,Vi),e(pe,qt),e(qt,Ui),e(qt,On),e(On,Xi),e(qt,Gi),e(j,Ji),e(j,xt),e(xt,Mo),e(Mo,Zi),e(Mo,Wn),e(Wn,Yi),e(xt,ed),e(xt,Fo),e(Fo,td),e(Fo,Kn),e(Kn,od),e(j,nd),e(j,Qn),e(Qn,rd),e(j,ad),g(At,j,null),p(t,Mr,m),p(t,fe,m),e(fe,Le),e(Le,Bn),g(Pt,Bn,null),e(fe,sd),e(fe,Hn),e(Hn,id),p(t,Fr,m),p(t,H,m),g(jt,H,null),e(H,dd),e(H,Vn),e(Vn,ld),e(H,cd),e(H,Me),g(Lt,Me,null),e(Me,md),e(Me,Un),e(Un,hd),p(t,Sr,m),p(t,ue,m),e(ue,Fe),e(Fe,Xn),g(Mt,Xn,null),e(ue,pd),e(ue,Gn),e(Gn,fd),p(t,Cr,m),p(t,V,m),g(Ft,V,null),e(V,ud),e(V,St),e(St,gd),e(St,Ct),e(Ct,_d),e(St,kd),e(V,vd),e(V,F),g(It,F,null),e(F,wd),e(F,ge),e(ge,bd),e(ge,So),e(So,Rd),e(ge,Td),e(ge,Jn),e(Jn,$d),e(ge,yd),e(F,Ed),g(Se,F,null),e(F,zd),e(F,Zn),e(Zn,qd),e(F,xd),g(Dt,F,null),p(t,Ir,m),p(t,_e,m),e(_e,Ce),e(Ce,Yn),g(Nt,Yn,null),e(_e,Ad),e(_e,er),e(er,Pd),p(t,Dr,m),p(t,U,m),g(Ot,U,null),e(U,jd),e(U,Wt),e(Wt,Ld),e(Wt,Kt),e(Kt,Md),e(Wt,Fd),e(U,Sd),e(U,S),g(Qt,S,null),e(S,Cd),e(S,ke),e(ke,Id),e(ke,Co),e(Co,Dd),e(ke,Nd),e(ke,tr),e(tr,Od),e(ke,Wd),e(S,Kd),g(Ie,S,null),e(S,Qd),e(S,or),e(or,Bd),e(S,Hd),g(Bt,S,null),p(t,Nr,m),p(t,ve,m),e(ve,De),e(De,nr),g(Ht,nr,null),e(ve,Vd),e(ve,rr),e(rr,Ud),p(t,Or,m),p(t,X,m),g(Vt,X,null),e(X,Xd),e(X,Ut),e(Ut,Gd),e(Ut,Xt),e(Xt,Jd),e(Ut,Zd),e(X,Yd),e(X,C),g(Gt,C,null),e(C,el),e(C,we),e(we,tl),e(we,Io),e(Io,ol),e(we,nl),e(we,ar),e(ar,rl),e(we,al),e(C,sl),g(Ne,C,null),e(C,il),e(C,sr),e(sr,dl),e(C,ll),g(Jt,C,null),p(t,Wr,m),p(t,be,m),e(be,Oe),e(Oe,ir),g(Zt,ir,null),e(be,cl),e(be,dr),e(dr,ml),p(t,Kr,m),p(t,G,m),g(Yt,G,null),e(G,hl),e(G,eo),e(eo,pl),e(eo,to),e(to,fl),e(eo,ul),e(G,gl),e(G,ee),g(oo,ee,null),e(ee,_l),e(ee,Re),e(Re,kl),e(Re,Do),e(Do,vl),e(Re,wl),e(Re,lr),e(lr,bl),e(Re,Rl),e(ee,Tl),g(We,ee,null),p(t,Qr,m),p(t,Te,m),e(Te,Ke),e(Ke,cr),g(no,cr,null),e(Te,$l),e(Te,mr),e(mr,yl),p(t,Br,m),p(t,N,m),g(ro,N,null),e(N,El),e(N,Qe),e(Qe,hr),e(hr,zl),e(Qe,ql),e(Qe,ao),e(ao,xl),e(Qe,Al),e(N,Pl),e(N,Be),g(so,Be,null),e(Be,jl),e(Be,io),e(io,Ll),e(io,pr),e(pr,Ml),e(io,Fl),e(N,Sl),e(N,I),g(lo,I,null),e(I,Cl),e(I,$e),e($e,Il),e($e,No),e(No,Dl),e($e,Nl),e($e,fr),e(fr,Ol),e($e,Wl),e(I,Kl),g(He,I,null),e(I,Ql),e(I,ur),e(ur,Bl),e(I,Hl),g(co,I,null),Hr=!0},p(t,[m]){const mo={};m&2&&(mo.$$scope={dirty:m,ctx:t}),Se.$set(mo);const gr={};m&2&&(gr.$$scope={dirty:m,ctx:t}),Ie.$set(gr);const _r={};m&2&&(_r.$$scope={dirty:m,ctx:t}),Ne.$set(_r);const kr={};m&2&&(kr.$$scope={dirty:m,ctx:t}),We.$set(kr);const ho={};m&2&&(ho.$$scope={dirty:m,ctx:t}),He.$set(ho)},i(t){Hr||(_(R.$$.fragment,t),_(Ge.$$.fragment,t),_(et.$$.fragment,t),_(tt.$$.fragment,t),_(rt.$$.fragment,t),_(at.$$.fragment,t),_(st.$$.fragment,t),_(dt.$$.fragment,t),_(ct.$$.fragment,t),_(ht.$$.fragment,t),_(pt.$$.fragment,t),_(ft.$$.fragment,t),_(ut.$$.fragment,t),_(wt.$$.fragment,t),_(bt.$$.fragment,t),_(Rt.$$.fragment,t),_(yt.$$.fragment,t),_(At.$$.fragment,t),_(Pt.$$.fragment,t),_(jt.$$.fragment,t),_(Lt.$$.fragment,t),_(Mt.$$.fragment,t),_(Ft.$$.fragment,t),_(It.$$.fragment,t),_(Se.$$.fragment,t),_(Dt.$$.fragment,t),_(Nt.$$.fragment,t),_(Ot.$$.fragment,t),_(Qt.$$.fragment,t),_(Ie.$$.fragment,t),_(Bt.$$.fragment,t),_(Ht.$$.fragment,t),_(Vt.$$.fragment,t),_(Gt.$$.fragment,t),_(Ne.$$.fragment,t),_(Jt.$$.fragment,t),_(Zt.$$.fragment,t),_(Yt.$$.fragment,t),_(oo.$$.fragment,t),_(We.$$.fragment,t),_(no.$$.fragment,t),_(ro.$$.fragment,t),_(so.$$.fragment,t),_(lo.$$.fragment,t),_(He.$$.fragment,t),_(co.$$.fragment,t),Hr=!0)},o(t){k(R.$$.fragment,t),k(Ge.$$.fragment,t),k(et.$$.fragment,t),k(tt.$$.fragment,t),k(rt.$$.fragment,t),k(at.$$.fragment,t),k(st.$$.fragment,t),k(dt.$$.fragment,t),k(ct.$$.fragment,t),k(ht.$$.fragment,t),k(pt.$$.fragment,t),k(ft.$$.fragment,t),k(ut.$$.fragment,t),k(wt.$$.fragment,t),k(bt.$$.fragment,t),k(Rt.$$.fragment,t),k(yt.$$.fragment,t),k(At.$$.fragment,t),k(Pt.$$.fragment,t),k(jt.$$.fragment,t),k(Lt.$$.fragment,t),k(Mt.$$.fragment,t),k(Ft.$$.fragment,t),k(It.$$.fragment,t),k(Se.$$.fragment,t),k(Dt.$$.fragment,t),k(Nt.$$.fragment,t),k(Ot.$$.fragment,t),k(Qt.$$.fragment,t),k(Ie.$$.fragment,t),k(Bt.$$.fragment,t),k(Ht.$$.fragment,t),k(Vt.$$.fragment,t),k(Gt.$$.fragment,t),k(Ne.$$.fragment,t),k(Jt.$$.fragment,t),k(Zt.$$.fragment,t),k(Yt.$$.fragment,t),k(oo.$$.fragment,t),k(We.$$.fragment,t),k(no.$$.fragment,t),k(ro.$$.fragment,t),k(so.$$.fragment,t),k(lo.$$.fragment,t),k(He.$$.fragment,t),k(co.$$.fragment,t),Hr=!1},d(t){o(h),t&&o(y),t&&o(w),v(R),t&&o(Tr),t&&o(se),v(Ge),t&&o($r),t&&o(Ee),t&&o(yr),t&&o(po),t&&o(Er),t&&o(fo),t&&o(zr),t&&o(Z),t&&o(qr),t&&o(ie),v(et),t&&o(xr),t&&o(x),v(tt),v(rt),t&&o(Ar),t&&o(le),v(at),t&&o(Pr),t&&o(E),v(st),v(dt),v(ct),v(ht),v(pt),v(ft),v(ut),v(wt),t&&o(jr),t&&o(he),v(bt),t&&o(Lr),t&&o(M),v(Rt),v(yt),v(At),t&&o(Mr),t&&o(fe),v(Pt),t&&o(Fr),t&&o(H),v(jt),v(Lt),t&&o(Sr),t&&o(ue),v(Mt),t&&o(Cr),t&&o(V),v(Ft),v(It),v(Se),v(Dt),t&&o(Ir),t&&o(_e),v(Nt),t&&o(Dr),t&&o(U),v(Ot),v(Qt),v(Ie),v(Bt),t&&o(Nr),t&&o(ve),v(Ht),t&&o(Or),t&&o(X),v(Vt),v(Gt),v(Ne),v(Jt),t&&o(Wr),t&&o(be),v(Zt),t&&o(Kr),t&&o(G),v(Yt),v(oo),v(We),t&&o(Qr),t&&o(Te),v(no),t&&o(Br),t&&o(N),v(ro),v(so),v(lo),v(He),v(co)}}}const dh={local:"realm",sections:[{local:"overview",title:"Overview"},{local:"transformers.RealmConfig",title:"RealmConfig"},{local:"transformers.RealmTokenizer",title:"RealmTokenizer"},{local:"transformers.RealmTokenizerFast",title:"RealmTokenizerFast"},{local:"transformers.RealmRetriever",title:"RealmRetriever"},{local:"transformers.RealmEmbedder",title:"RealmEmbedder"},{local:"transformers.RealmScorer",title:"RealmScorer"},{local:"transformers.RealmKnowledgeAugEncoder",title:"RealmKnowledgeAugEncoder"},{local:"transformers.RealmReader",title:"RealmReader"},{local:"transformers.RealmForOpenQA",title:"RealmForOpenQA"}],title:"REALM"};function lh(O){return th(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class uh extends Jm{constructor(h){super();Zm(this,h,lh,ih,Ym,{})}}export{uh as default,dh as metadata};
