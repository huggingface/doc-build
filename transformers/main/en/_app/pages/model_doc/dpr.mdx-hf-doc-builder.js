import{S as uf,i as ff,s as mf,e as s,k as l,w,t as n,M as gf,c as a,d as t,m as c,a as i,x as k,h as r,b as p,G as e,g as f,y as P,q as $,o as E,B as R,v as _f,L as Bo}from"../../chunks/vendor-hf-doc-builder.js";import{T as Xt}from"../../chunks/Tip-hf-doc-builder.js";import{D as U}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Ho}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Ee}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as So}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function vf(C){let h,T,_,g,b;return g=new Ho({props:{code:`from transformers import DPRConfig, DPRContextEncoder

# Initializing a DPR facebook/dpr-ctx_encoder-single-nq-base style configuration
configuration = DPRConfig()

# Initializing a model (with random weights) from the facebook/dpr-ctx_encoder-single-nq-base style configuration
model = DPRContextEncoder(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRConfig, DPRContextEncoder

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a DPR facebook/dpr-ctx_encoder-single-nq-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = DPRConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the facebook/dpr-ctx_encoder-single-nq-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRContextEncoder(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),{c(){h=s("p"),T=n("Example:"),_=l(),w(g.$$.fragment)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Example:"),u.forEach(t),_=c(d),k(g.$$.fragment,d)},m(d,u){f(d,h,u),e(h,T),f(d,_,u),P(g,d,u),b=!0},p:Bo,i(d){b||($(g.$$.fragment,d),b=!0)},o(d){E(g.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(_),R(g,d)}}}function bf(C){let h,T,_,g,b;return g=new Ho({props:{code:"[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>",highlighted:'[CLS] <span class="hljs-tag">&lt;<span class="hljs-name">question</span> <span class="hljs-attr">token</span> <span class="hljs-attr">ids</span>&gt;</span> [SEP] <span class="hljs-tag">&lt;<span class="hljs-name">titles</span> <span class="hljs-attr">ids</span>&gt;</span> [SEP] <span class="hljs-tag">&lt;<span class="hljs-name">texts</span> <span class="hljs-attr">ids</span>&gt;</span>'}}),{c(){h=s("p"),T=n("with the format:"),_=l(),w(g.$$.fragment)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"with the format:"),u.forEach(t),_=c(d),k(g.$$.fragment,d)},m(d,u){f(d,h,u),e(h,T),f(d,_,u),P(g,d,u),b=!0},p:Bo,i(d){b||($(g.$$.fragment,d),b=!0)},o(d){E(g.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(_),R(g,d)}}}function Tf(C){let h,T,_,g,b;return{c(){h=s("p"),T=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=s("code"),g=n("Module"),b=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=a(u,"CODE",{});var q=i(_);g=r(q,"Module"),q.forEach(t),b=r(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(d,u){f(d,h,u),e(h,T),e(h,_),e(_,g),e(h,b)},d(d){d&&t(h)}}}function wf(C){let h,T,_,g,b;return g=new Ho({props:{code:`from transformers import DPRContextEncoder, DPRContextEncoderTokenizer

tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
model = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="pt")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRContextEncoder, DPRContextEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRContextEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRContextEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),{c(){h=s("p"),T=n("Examples:"),_=l(),w(g.$$.fragment)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Examples:"),u.forEach(t),_=c(d),k(g.$$.fragment,d)},m(d,u){f(d,h,u),e(h,T),f(d,_,u),P(g,d,u),b=!0},p:Bo,i(d){b||($(g.$$.fragment,d),b=!0)},o(d){E(g.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(_),R(g,d)}}}function kf(C){let h,T,_,g,b;return{c(){h=s("p"),T=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=s("code"),g=n("Module"),b=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=a(u,"CODE",{});var q=i(_);g=r(q,"Module"),q.forEach(t),b=r(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(d,u){f(d,h,u),e(h,T),e(h,_),e(_,g),e(h,b)},d(d){d&&t(h)}}}function Pf(C){let h,T,_,g,b;return g=new Ho({props:{code:`from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer

tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
model = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="pt")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRQuestionEncoder, DPRQuestionEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRQuestionEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),{c(){h=s("p"),T=n("Examples:"),_=l(),w(g.$$.fragment)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Examples:"),u.forEach(t),_=c(d),k(g.$$.fragment,d)},m(d,u){f(d,h,u),e(h,T),f(d,_,u),P(g,d,u),b=!0},p:Bo,i(d){b||($(g.$$.fragment,d),b=!0)},o(d){E(g.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(_),R(g,d)}}}function $f(C){let h,T,_,g,b;return{c(){h=s("p"),T=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=s("code"),g=n("Module"),b=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=a(u,"CODE",{});var q=i(_);g=r(q,"Module"),q.forEach(t),b=r(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(d,u){f(d,h,u),e(h,T),e(h,_),e(_,g),e(h,b)},d(d){d&&t(h)}}}function Ef(C){let h,T,_,g,b;return g=new Ho({props:{code:`from transformers import DPRReader, DPRReaderTokenizer

tokenizer = DPRReaderTokenizer.from_pretrained("facebook/dpr-reader-single-nq-base")
model = DPRReader.from_pretrained("facebook/dpr-reader-single-nq-base")
encoded_inputs = tokenizer(
    questions=["What is love ?"],
    titles=["Haddaway"],
    texts=["'What Is Love' is a song recorded by the artist Haddaway"],
    return_tensors="pt",
)
outputs = model(**encoded_inputs)
start_logits = outputs.start_logits
end_logits = outputs.end_logits
relevance_logits = outputs.relevance_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRReader, DPRReaderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRReaderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRReader.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(
<span class="hljs-meta">... </span>    questions=[<span class="hljs-string">&quot;What is love ?&quot;</span>],
<span class="hljs-meta">... </span>    titles=[<span class="hljs-string">&quot;Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    texts=[<span class="hljs-string">&quot;&#x27;What Is Love&#x27; is a song recorded by the artist Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**encoded_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_logits = outputs.relevance_logits`}}),{c(){h=s("p"),T=n("Examples:"),_=l(),w(g.$$.fragment)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Examples:"),u.forEach(t),_=c(d),k(g.$$.fragment,d)},m(d,u){f(d,h,u),e(h,T),f(d,_,u),P(g,d,u),b=!0},p:Bo,i(d){b||($(g.$$.fragment,d),b=!0)},o(d){E(g.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(_),R(g,d)}}}function Rf(C){let h,T,_,g,b,d,u,q,Re,be,Q,te,oe,y,ye,H,De,Te,L,xe,se,V,ze,we,S,qe,ke,B,he,Ce,ae,z,j,ie,Y,Fe,de,X,Ae,Pe,F,ne,K,le,je,W,Oe,$e,A,ce,G,re,J,O,Ne,N,Ie,Qe;return{c(){h=s("p"),T=n("TensorFlow models and layers in "),_=s("code"),g=n("transformers"),b=n(" accept two formats as input:"),d=l(),u=s("ul"),q=s("li"),Re=n("having all inputs as keyword arguments (like PyTorch models), or"),be=l(),Q=s("li"),te=n("having all inputs as a list, tuple or dict in the first positional argument."),oe=l(),y=s("p"),ye=n(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),H=s("code"),De=n("model.fit()"),Te=n(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),L=s("code"),xe=n("model.fit()"),se=n(` supports! If, however, you want to use the second
format outside of Keras methods like `),V=s("code"),ze=n("fit()"),we=n(" and "),S=s("code"),qe=n("predict()"),ke=n(`, such as when creating your own layers or models with
the Keras `),B=s("code"),he=n("Functional"),Ce=n(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),ae=l(),z=s("ul"),j=s("li"),ie=n("a single Tensor with "),Y=s("code"),Fe=n("input_ids"),de=n(" only and nothing else: "),X=s("code"),Ae=n("model(input_ids)"),Pe=l(),F=s("li"),ne=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),K=s("code"),le=n("model([input_ids, attention_mask])"),je=n(" or "),W=s("code"),Oe=n("model([input_ids, attention_mask, token_type_ids])"),$e=l(),A=s("li"),ce=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),G=s("code"),re=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),J=l(),O=s("p"),Ne=n(`Note that when creating models and layers with
`),N=s("a"),Ie=n("subclassing"),Qe=n(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(v){h=a(v,"P",{});var D=i(h);T=r(D,"TensorFlow models and layers in "),_=a(D,"CODE",{});var Xe=i(_);g=r(Xe,"transformers"),Xe.forEach(t),b=r(D," accept two formats as input:"),D.forEach(t),d=c(v),u=a(v,"UL",{});var Z=i(u);q=a(Z,"LI",{});var Ge=i(q);Re=r(Ge,"having all inputs as keyword arguments (like PyTorch models), or"),Ge.forEach(t),be=c(Z),Q=a(Z,"LI",{});var Je=i(Q);te=r(Je,"having all inputs as a list, tuple or dict in the first positional argument."),Je.forEach(t),Z.forEach(t),oe=c(v),y=a(v,"P",{});var x=i(y);ye=r(x,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),H=a(x,"CODE",{});var Ze=i(H);De=r(Ze,"model.fit()"),Ze.forEach(t),Te=r(x,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),L=a(x,"CODE",{});var et=i(L);xe=r(et,"model.fit()"),et.forEach(t),se=r(x,` supports! If, however, you want to use the second
format outside of Keras methods like `),V=a(x,"CODE",{});var Le=i(V);ze=r(Le,"fit()"),Le.forEach(t),we=r(x," and "),S=a(x,"CODE",{});var tt=i(S);qe=r(tt,"predict()"),tt.forEach(t),ke=r(x,`, such as when creating your own layers or models with
the Keras `),B=a(x,"CODE",{});var ot=i(B);he=r(ot,"Functional"),ot.forEach(t),Ce=r(x,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),x.forEach(t),ae=c(v),z=a(v,"UL",{});var I=i(z);j=a(I,"LI",{});var ee=i(j);ie=r(ee,"a single Tensor with "),Y=a(ee,"CODE",{});var nt=i(Y);Fe=r(nt,"input_ids"),nt.forEach(t),de=r(ee," only and nothing else: "),X=a(ee,"CODE",{});var rt=i(X);Ae=r(rt,"model(input_ids)"),rt.forEach(t),ee.forEach(t),Pe=c(I),F=a(I,"LI",{});var M=i(F);ne=r(M,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),K=a(M,"CODE",{});var st=i(K);le=r(st,"model([input_ids, attention_mask])"),st.forEach(t),je=r(M," or "),W=a(M,"CODE",{});var Me=i(W);Oe=r(Me,"model([input_ids, attention_mask, token_type_ids])"),Me.forEach(t),M.forEach(t),$e=c(I),A=a(I,"LI",{});var Se=i(A);ce=r(Se,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),G=a(Se,"CODE",{});var at=i(G);re=r(at,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),at.forEach(t),Se.forEach(t),I.forEach(t),J=c(v),O=a(v,"P",{});var pe=i(O);Ne=r(pe,`Note that when creating models and layers with
`),N=a(pe,"A",{href:!0,rel:!0});var ue=i(N);Ie=r(ue,"subclassing"),ue.forEach(t),Qe=r(pe,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),pe.forEach(t),this.h()},h(){p(N,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),p(N,"rel","nofollow")},m(v,D){f(v,h,D),e(h,T),e(h,_),e(_,g),e(h,b),f(v,d,D),f(v,u,D),e(u,q),e(q,Re),e(u,be),e(u,Q),e(Q,te),f(v,oe,D),f(v,y,D),e(y,ye),e(y,H),e(H,De),e(y,Te),e(y,L),e(L,xe),e(y,se),e(y,V),e(V,ze),e(y,we),e(y,S),e(S,qe),e(y,ke),e(y,B),e(B,he),e(y,Ce),f(v,ae,D),f(v,z,D),e(z,j),e(j,ie),e(j,Y),e(Y,Fe),e(j,de),e(j,X),e(X,Ae),e(z,Pe),e(z,F),e(F,ne),e(F,K),e(K,le),e(F,je),e(F,W),e(W,Oe),e(z,$e),e(z,A),e(A,ce),e(A,G),e(G,re),f(v,J,D),f(v,O,D),e(O,Ne),e(O,N),e(N,Ie),e(O,Qe)},d(v){v&&t(h),v&&t(d),v&&t(u),v&&t(oe),v&&t(y),v&&t(ae),v&&t(z),v&&t(J),v&&t(O)}}}function yf(C){let h,T,_,g,b;return{c(){h=s("p"),T=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=s("code"),g=n("Module"),b=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=a(u,"CODE",{});var q=i(_);g=r(q,"Module"),q.forEach(t),b=r(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(d,u){f(d,h,u),e(h,T),e(h,_),e(_,g),e(h,b)},d(d){d&&t(h)}}}function Df(C){let h,T,_,g,b;return g=new Ho({props:{code:`from transformers import TFDPRContextEncoder, DPRContextEncoderTokenizer

tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
model = TFDPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base", from_pt=True)
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="tf")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRContextEncoder, DPRContextEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRContextEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRContextEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),{c(){h=s("p"),T=n("Examples:"),_=l(),w(g.$$.fragment)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Examples:"),u.forEach(t),_=c(d),k(g.$$.fragment,d)},m(d,u){f(d,h,u),e(h,T),f(d,_,u),P(g,d,u),b=!0},p:Bo,i(d){b||($(g.$$.fragment,d),b=!0)},o(d){E(g.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(_),R(g,d)}}}function xf(C){let h,T,_,g,b,d,u,q,Re,be,Q,te,oe,y,ye,H,De,Te,L,xe,se,V,ze,we,S,qe,ke,B,he,Ce,ae,z,j,ie,Y,Fe,de,X,Ae,Pe,F,ne,K,le,je,W,Oe,$e,A,ce,G,re,J,O,Ne,N,Ie,Qe;return{c(){h=s("p"),T=n("TensorFlow models and layers in "),_=s("code"),g=n("transformers"),b=n(" accept two formats as input:"),d=l(),u=s("ul"),q=s("li"),Re=n("having all inputs as keyword arguments (like PyTorch models), or"),be=l(),Q=s("li"),te=n("having all inputs as a list, tuple or dict in the first positional argument."),oe=l(),y=s("p"),ye=n(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),H=s("code"),De=n("model.fit()"),Te=n(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),L=s("code"),xe=n("model.fit()"),se=n(` supports! If, however, you want to use the second
format outside of Keras methods like `),V=s("code"),ze=n("fit()"),we=n(" and "),S=s("code"),qe=n("predict()"),ke=n(`, such as when creating your own layers or models with
the Keras `),B=s("code"),he=n("Functional"),Ce=n(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),ae=l(),z=s("ul"),j=s("li"),ie=n("a single Tensor with "),Y=s("code"),Fe=n("input_ids"),de=n(" only and nothing else: "),X=s("code"),Ae=n("model(input_ids)"),Pe=l(),F=s("li"),ne=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),K=s("code"),le=n("model([input_ids, attention_mask])"),je=n(" or "),W=s("code"),Oe=n("model([input_ids, attention_mask, token_type_ids])"),$e=l(),A=s("li"),ce=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),G=s("code"),re=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),J=l(),O=s("p"),Ne=n(`Note that when creating models and layers with
`),N=s("a"),Ie=n("subclassing"),Qe=n(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(v){h=a(v,"P",{});var D=i(h);T=r(D,"TensorFlow models and layers in "),_=a(D,"CODE",{});var Xe=i(_);g=r(Xe,"transformers"),Xe.forEach(t),b=r(D," accept two formats as input:"),D.forEach(t),d=c(v),u=a(v,"UL",{});var Z=i(u);q=a(Z,"LI",{});var Ge=i(q);Re=r(Ge,"having all inputs as keyword arguments (like PyTorch models), or"),Ge.forEach(t),be=c(Z),Q=a(Z,"LI",{});var Je=i(Q);te=r(Je,"having all inputs as a list, tuple or dict in the first positional argument."),Je.forEach(t),Z.forEach(t),oe=c(v),y=a(v,"P",{});var x=i(y);ye=r(x,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),H=a(x,"CODE",{});var Ze=i(H);De=r(Ze,"model.fit()"),Ze.forEach(t),Te=r(x,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),L=a(x,"CODE",{});var et=i(L);xe=r(et,"model.fit()"),et.forEach(t),se=r(x,` supports! If, however, you want to use the second
format outside of Keras methods like `),V=a(x,"CODE",{});var Le=i(V);ze=r(Le,"fit()"),Le.forEach(t),we=r(x," and "),S=a(x,"CODE",{});var tt=i(S);qe=r(tt,"predict()"),tt.forEach(t),ke=r(x,`, such as when creating your own layers or models with
the Keras `),B=a(x,"CODE",{});var ot=i(B);he=r(ot,"Functional"),ot.forEach(t),Ce=r(x,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),x.forEach(t),ae=c(v),z=a(v,"UL",{});var I=i(z);j=a(I,"LI",{});var ee=i(j);ie=r(ee,"a single Tensor with "),Y=a(ee,"CODE",{});var nt=i(Y);Fe=r(nt,"input_ids"),nt.forEach(t),de=r(ee," only and nothing else: "),X=a(ee,"CODE",{});var rt=i(X);Ae=r(rt,"model(input_ids)"),rt.forEach(t),ee.forEach(t),Pe=c(I),F=a(I,"LI",{});var M=i(F);ne=r(M,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),K=a(M,"CODE",{});var st=i(K);le=r(st,"model([input_ids, attention_mask])"),st.forEach(t),je=r(M," or "),W=a(M,"CODE",{});var Me=i(W);Oe=r(Me,"model([input_ids, attention_mask, token_type_ids])"),Me.forEach(t),M.forEach(t),$e=c(I),A=a(I,"LI",{});var Se=i(A);ce=r(Se,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),G=a(Se,"CODE",{});var at=i(G);re=r(at,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),at.forEach(t),Se.forEach(t),I.forEach(t),J=c(v),O=a(v,"P",{});var pe=i(O);Ne=r(pe,`Note that when creating models and layers with
`),N=a(pe,"A",{href:!0,rel:!0});var ue=i(N);Ie=r(ue,"subclassing"),ue.forEach(t),Qe=r(pe,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),pe.forEach(t),this.h()},h(){p(N,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),p(N,"rel","nofollow")},m(v,D){f(v,h,D),e(h,T),e(h,_),e(_,g),e(h,b),f(v,d,D),f(v,u,D),e(u,q),e(q,Re),e(u,be),e(u,Q),e(Q,te),f(v,oe,D),f(v,y,D),e(y,ye),e(y,H),e(H,De),e(y,Te),e(y,L),e(L,xe),e(y,se),e(y,V),e(V,ze),e(y,we),e(y,S),e(S,qe),e(y,ke),e(y,B),e(B,he),e(y,Ce),f(v,ae,D),f(v,z,D),e(z,j),e(j,ie),e(j,Y),e(Y,Fe),e(j,de),e(j,X),e(X,Ae),e(z,Pe),e(z,F),e(F,ne),e(F,K),e(K,le),e(F,je),e(F,W),e(W,Oe),e(z,$e),e(z,A),e(A,ce),e(A,G),e(G,re),f(v,J,D),f(v,O,D),e(O,Ne),e(O,N),e(N,Ie),e(O,Qe)},d(v){v&&t(h),v&&t(d),v&&t(u),v&&t(oe),v&&t(y),v&&t(ae),v&&t(z),v&&t(J),v&&t(O)}}}function zf(C){let h,T,_,g,b;return{c(){h=s("p"),T=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=s("code"),g=n("Module"),b=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=a(u,"CODE",{});var q=i(_);g=r(q,"Module"),q.forEach(t),b=r(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(d,u){f(d,h,u),e(h,T),e(h,_),e(_,g),e(h,b)},d(d){d&&t(h)}}}function qf(C){let h,T,_,g,b;return g=new Ho({props:{code:`from transformers import TFDPRQuestionEncoder, DPRQuestionEncoderTokenizer

tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
model = TFDPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base", from_pt=True)
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="tf")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRQuestionEncoder, DPRQuestionEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRQuestionEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),{c(){h=s("p"),T=n("Examples:"),_=l(),w(g.$$.fragment)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Examples:"),u.forEach(t),_=c(d),k(g.$$.fragment,d)},m(d,u){f(d,h,u),e(h,T),f(d,_,u),P(g,d,u),b=!0},p:Bo,i(d){b||($(g.$$.fragment,d),b=!0)},o(d){E(g.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(_),R(g,d)}}}function Cf(C){let h,T,_,g,b,d,u,q,Re,be,Q,te,oe,y,ye,H,De,Te,L,xe,se,V,ze,we,S,qe,ke,B,he,Ce,ae,z,j,ie,Y,Fe,de,X,Ae,Pe,F,ne,K,le,je,W,Oe,$e,A,ce,G,re,J,O,Ne,N,Ie,Qe;return{c(){h=s("p"),T=n("TensorFlow models and layers in "),_=s("code"),g=n("transformers"),b=n(" accept two formats as input:"),d=l(),u=s("ul"),q=s("li"),Re=n("having all inputs as keyword arguments (like PyTorch models), or"),be=l(),Q=s("li"),te=n("having all inputs as a list, tuple or dict in the first positional argument."),oe=l(),y=s("p"),ye=n(`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),H=s("code"),De=n("model.fit()"),Te=n(` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),L=s("code"),xe=n("model.fit()"),se=n(` supports! If, however, you want to use the second
format outside of Keras methods like `),V=s("code"),ze=n("fit()"),we=n(" and "),S=s("code"),qe=n("predict()"),ke=n(`, such as when creating your own layers or models with
the Keras `),B=s("code"),he=n("Functional"),Ce=n(` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),ae=l(),z=s("ul"),j=s("li"),ie=n("a single Tensor with "),Y=s("code"),Fe=n("input_ids"),de=n(" only and nothing else: "),X=s("code"),Ae=n("model(input_ids)"),Pe=l(),F=s("li"),ne=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),K=s("code"),le=n("model([input_ids, attention_mask])"),je=n(" or "),W=s("code"),Oe=n("model([input_ids, attention_mask, token_type_ids])"),$e=l(),A=s("li"),ce=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),G=s("code"),re=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),J=l(),O=s("p"),Ne=n(`Note that when creating models and layers with
`),N=s("a"),Ie=n("subclassing"),Qe=n(` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),this.h()},l(v){h=a(v,"P",{});var D=i(h);T=r(D,"TensorFlow models and layers in "),_=a(D,"CODE",{});var Xe=i(_);g=r(Xe,"transformers"),Xe.forEach(t),b=r(D," accept two formats as input:"),D.forEach(t),d=c(v),u=a(v,"UL",{});var Z=i(u);q=a(Z,"LI",{});var Ge=i(q);Re=r(Ge,"having all inputs as keyword arguments (like PyTorch models), or"),Ge.forEach(t),be=c(Z),Q=a(Z,"LI",{});var Je=i(Q);te=r(Je,"having all inputs as a list, tuple or dict in the first positional argument."),Je.forEach(t),Z.forEach(t),oe=c(v),y=a(v,"P",{});var x=i(y);ye=r(x,`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like `),H=a(x,"CODE",{});var Ze=i(H);De=r(Ze,"model.fit()"),Ze.forEach(t),Te=r(x,` things should \u201Cjust work\u201D for you - just
pass your inputs and labels in any format that `),L=a(x,"CODE",{});var et=i(L);xe=r(et,"model.fit()"),et.forEach(t),se=r(x,` supports! If, however, you want to use the second
format outside of Keras methods like `),V=a(x,"CODE",{});var Le=i(V);ze=r(Le,"fit()"),Le.forEach(t),we=r(x," and "),S=a(x,"CODE",{});var tt=i(S);qe=r(tt,"predict()"),tt.forEach(t),ke=r(x,`, such as when creating your own layers or models with
the Keras `),B=a(x,"CODE",{});var ot=i(B);he=r(ot,"Functional"),ot.forEach(t),Ce=r(x,` API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`),x.forEach(t),ae=c(v),z=a(v,"UL",{});var I=i(z);j=a(I,"LI",{});var ee=i(j);ie=r(ee,"a single Tensor with "),Y=a(ee,"CODE",{});var nt=i(Y);Fe=r(nt,"input_ids"),nt.forEach(t),de=r(ee," only and nothing else: "),X=a(ee,"CODE",{});var rt=i(X);Ae=r(rt,"model(input_ids)"),rt.forEach(t),ee.forEach(t),Pe=c(I),F=a(I,"LI",{});var M=i(F);ne=r(M,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),K=a(M,"CODE",{});var st=i(K);le=r(st,"model([input_ids, attention_mask])"),st.forEach(t),je=r(M," or "),W=a(M,"CODE",{});var Me=i(W);Oe=r(Me,"model([input_ids, attention_mask, token_type_ids])"),Me.forEach(t),M.forEach(t),$e=c(I),A=a(I,"LI",{});var Se=i(A);ce=r(Se,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),G=a(Se,"CODE",{});var at=i(G);re=r(at,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),at.forEach(t),Se.forEach(t),I.forEach(t),J=c(v),O=a(v,"P",{});var pe=i(O);Ne=r(pe,`Note that when creating models and layers with
`),N=a(pe,"A",{href:!0,rel:!0});var ue=i(N);Ie=r(ue,"subclassing"),ue.forEach(t),Qe=r(pe,` then you don\u2019t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`),pe.forEach(t),this.h()},h(){p(N,"href","https://keras.io/guides/making_new_layers_and_models_via_subclassing/"),p(N,"rel","nofollow")},m(v,D){f(v,h,D),e(h,T),e(h,_),e(_,g),e(h,b),f(v,d,D),f(v,u,D),e(u,q),e(q,Re),e(u,be),e(u,Q),e(Q,te),f(v,oe,D),f(v,y,D),e(y,ye),e(y,H),e(H,De),e(y,Te),e(y,L),e(L,xe),e(y,se),e(y,V),e(V,ze),e(y,we),e(y,S),e(S,qe),e(y,ke),e(y,B),e(B,he),e(y,Ce),f(v,ae,D),f(v,z,D),e(z,j),e(j,ie),e(j,Y),e(Y,Fe),e(j,de),e(j,X),e(X,Ae),e(z,Pe),e(z,F),e(F,ne),e(F,K),e(K,le),e(F,je),e(F,W),e(W,Oe),e(z,$e),e(z,A),e(A,ce),e(A,G),e(G,re),f(v,J,D),f(v,O,D),e(O,Ne),e(O,N),e(N,Ie),e(O,Qe)},d(v){v&&t(h),v&&t(d),v&&t(u),v&&t(oe),v&&t(y),v&&t(ae),v&&t(z),v&&t(J),v&&t(O)}}}function Ff(C){let h,T,_,g,b;return{c(){h=s("p"),T=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),_=s("code"),g=n("Module"),b=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),_=a(u,"CODE",{});var q=i(_);g=r(q,"Module"),q.forEach(t),b=r(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(d,u){f(d,h,u),e(h,T),e(h,_),e(_,g),e(h,b)},d(d){d&&t(h)}}}function Af(C){let h,T,_,g,b;return g=new Ho({props:{code:`from transformers import TFDPRReader, DPRReaderTokenizer

tokenizer = DPRReaderTokenizer.from_pretrained("facebook/dpr-reader-single-nq-base")
model = TFDPRReader.from_pretrained("facebook/dpr-reader-single-nq-base", from_pt=True)
encoded_inputs = tokenizer(
    questions=["What is love ?"],
    titles=["Haddaway"],
    texts=["'What Is Love' is a song recorded by the artist Haddaway"],
    return_tensors="tf",
)
outputs = model(encoded_inputs)
start_logits = outputs.start_logits
end_logits = outputs.end_logits
relevance_logits = outputs.relevance_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRReader, DPRReaderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRReaderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRReader.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(
<span class="hljs-meta">... </span>    questions=[<span class="hljs-string">&quot;What is love ?&quot;</span>],
<span class="hljs-meta">... </span>    titles=[<span class="hljs-string">&quot;Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    texts=[<span class="hljs-string">&quot;&#x27;What Is Love&#x27; is a song recorded by the artist Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(encoded_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_logits = outputs.relevance_logits`}}),{c(){h=s("p"),T=n("Examples:"),_=l(),w(g.$$.fragment)},l(d){h=a(d,"P",{});var u=i(h);T=r(u,"Examples:"),u.forEach(t),_=c(d),k(g.$$.fragment,d)},m(d,u){f(d,h,u),e(h,T),f(d,_,u),P(g,d,u),b=!0},p:Bo,i(d){b||($(g.$$.fragment,d),b=!0)},o(d){E(g.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(_),R(g,d)}}}function jf(C){let h,T,_,g,b,d,u,q,Re,be,Q,te,oe,y,ye,H,De,Te,L,xe,se,V,ze,we,S,qe,ke,B,he,Ce,ae,z,j,ie,Y,Fe,de,X,Ae,Pe,F,ne,K,le,je,W,Oe,$e,A,ce,G,re,J,O,Ne,N,Ie,Qe,v,D,Xe,Z,Ge,Je,x,Ze,et,Le,tt,ot,I,ee,nt,rt,M,st,Me,Se,at,pe,ue,ca,yt,Gt,Xr,Ko,Ri,Gr,yi,pa,We,Wo,Di,Jr,xi,zi,Jt,lr,qi,Ci,cr,Fi,Ai,ji,Uo,Oi,pr,Ni,Ii,ha,Dt,Zt,Zr,Vo,Qi,es,Li,ua,Ue,Yo,Mi,Xo,Si,ts,Bi,Hi,Ki,eo,hr,Wi,Ui,ur,Vi,Yi,Xi,Go,Gi,fr,Ji,Zi,fa,xt,to,os,Jo,ed,ns,td,ma,Ve,Zo,od,rs,nd,rd,oo,mr,sd,ad,gr,id,dd,ld,en,cd,_r,pd,hd,ga,zt,no,ss,tn,ud,as,fd,_a,Ye,on,md,nn,gd,is,_d,vd,bd,ro,vr,Td,wd,br,kd,Pd,$d,rn,Ed,Tr,Rd,yd,va,qt,so,ds,sn,Dd,ls,xd,ba,fe,an,zd,cs,qd,Cd,bt,wr,Fd,Ad,kr,jd,Od,Pr,Nd,Id,Qd,dn,Ld,$r,Md,Sd,Bd,Tt,Hd,ps,Kd,Wd,hs,Ud,Vd,us,Yd,Xd,ao,Ta,Ct,io,fs,ln,Gd,ms,Jd,wa,me,cn,Zd,pn,el,gs,tl,ol,nl,wt,Er,rl,sl,Rr,al,il,yr,dl,ll,cl,hn,pl,Dr,hl,ul,fl,vt,ml,_s,gl,_l,vs,vl,bl,bs,Tl,wl,kl,Ts,Pl,ka,Ft,lo,ws,un,$l,ks,El,Pa,At,fn,Rl,mn,yl,xr,Dl,xl,$a,jt,gn,zl,_n,ql,zr,Cl,Fl,Ea,Ot,vn,Al,bn,jl,qr,Ol,Nl,Ra,Nt,co,Ps,Tn,Il,$s,Ql,ya,Be,wn,Ll,Es,Ml,Sl,kn,Bl,Cr,Hl,Kl,Wl,Pn,Ul,$n,Vl,Yl,Xl,it,En,Gl,It,Jl,Fr,Zl,ec,Rs,tc,oc,nc,po,rc,ho,Da,Qt,uo,ys,Rn,sc,Ds,ac,xa,He,yn,ic,xs,dc,lc,Dn,cc,Ar,pc,hc,uc,xn,fc,zn,mc,gc,_c,dt,qn,vc,Lt,bc,jr,Tc,wc,zs,kc,Pc,$c,fo,Ec,mo,za,Mt,go,qs,Cn,Rc,Cs,yc,qa,Ke,Fn,Dc,Fs,xc,zc,An,qc,Or,Cc,Fc,Ac,jn,jc,On,Oc,Nc,Ic,lt,Nn,Qc,St,Lc,Nr,Mc,Sc,As,Bc,Hc,Kc,_o,Wc,vo,Ca,Bt,bo,js,In,Uc,Os,Vc,Fa,ge,Qn,Yc,Ns,Xc,Gc,Ln,Jc,Ir,Zc,ep,tp,Mn,op,Sn,np,rp,sp,To,ap,ct,Bn,ip,Ht,dp,Qr,lp,cp,Is,pp,hp,up,wo,fp,ko,Aa,Kt,Po,Qs,Hn,mp,Ls,gp,ja,_e,Kn,_p,Ms,vp,bp,Wn,Tp,Lr,wp,kp,Pp,Un,$p,Vn,Ep,Rp,yp,$o,Dp,pt,Yn,xp,Wt,zp,Mr,qp,Cp,Ss,Fp,Ap,jp,Eo,Op,Ro,Oa,Ut,yo,Bs,Xn,Np,Hs,Ip,Na,ve,Gn,Qp,Ks,Lp,Mp,Jn,Sp,Sr,Bp,Hp,Kp,Zn,Wp,er,Up,Vp,Yp,Do,Xp,ht,tr,Gp,Vt,Jp,Br,Zp,eh,Ws,th,oh,nh,xo,rh,zo,Ia;return d=new Ee({}),y=new Ee({}),le=new Ee({}),ce=new U({props:{name:"class transformers.DPRConfig",anchor:"transformers.DPRConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"projection_dim",val:": int = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the DPR model. Defines the different tokens that can be represented by the <em>inputs_ids</em>
passed to the forward method of <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"vocab_size"},{anchor:"transformers.DPRConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.DPRConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.DPRConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.DPRConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.DPRConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.DPRConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.DPRConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.DPRConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.DPRConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <em>token_type_ids</em> passed into <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.DPRConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DPRConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.DPRConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.DPRConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Dimension of the projection for the context and question encoders. If it is set to zero (default), then no
projection is done.`,name:"projection_dim"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/configuration_dpr.py#L45"}}),ue=new So({props:{anchor:"transformers.DPRConfig.example",$$slots:{default:[vf]},$$scope:{ctx:C}}}),Ko=new Ee({}),Wo=new U({props:{name:"class transformers.DPRContextEncoderTokenizer",anchor:"transformers.DPRContextEncoderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/tokenization_dpr.py#L113"}}),Vo=new Ee({}),Yo=new U({props:{name:"class transformers.DPRContextEncoderTokenizerFast",anchor:"transformers.DPRContextEncoderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/tokenization_dpr_fast.py#L114"}}),Jo=new Ee({}),Zo=new U({props:{name:"class transformers.DPRQuestionEncoderTokenizer",anchor:"transformers.DPRQuestionEncoderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/tokenization_dpr.py#L129"}}),tn=new Ee({}),on=new U({props:{name:"class transformers.DPRQuestionEncoderTokenizerFast",anchor:"transformers.DPRQuestionEncoderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/tokenization_dpr_fast.py#L131"}}),sn=new Ee({}),an=new U({props:{name:"class transformers.DPRReaderTokenizer",anchor:"transformers.DPRReaderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRReaderTokenizer.questions",description:`<strong>questions</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code>[questions] * n_passages</code>. Otherwise you have to specify as many questions as in
<code>titles</code> or <code>texts</code>.`,name:"questions"},{anchor:"transformers.DPRReaderTokenizer.titles",description:`<strong>titles</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages titles to be encoded. This can be a string or a list of strings if there are several passages.`,name:"titles"},{anchor:"transformers.DPRReaderTokenizer.texts",description:`<strong>texts</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages texts to be encoded. This can be a string or a list of strings if there are several passages.`,name:"texts"},{anchor:"transformers.DPRReaderTokenizer.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DPRReaderTokenizer.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to
the maximum acceptable input length for the model if that argument is not provided. This will truncate
token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch
of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the first
sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the
second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.DPRReaderTokenizer.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.DPRReaderTokenizer.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DPRReaderTokenizer.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/tokenization_dpr.py#L396",returnDescription:`
<p>A dictionary with the following keys:</p>
<ul>
<li><code>input_ids</code>: List of token ids to be fed to a model.</li>
<li><code>attention_mask</code>: List of indices specifying which tokens should be attended to by the model.</li>
</ul>
`,returnType:`
<p><code>Dict[str, List[List[int]]]</code></p>
`}}),ao=new So({props:{anchor:"transformers.DPRReaderTokenizer.example",$$slots:{default:[bf]},$$scope:{ctx:C}}}),ln=new Ee({}),cn=new U({props:{name:"class transformers.DPRReaderTokenizerFast",anchor:"transformers.DPRReaderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRReaderTokenizerFast.questions",description:`<strong>questions</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code>[questions] * n_passages</code>. Otherwise you have to specify as many questions as in
<code>titles</code> or <code>texts</code>.`,name:"questions"},{anchor:"transformers.DPRReaderTokenizerFast.titles",description:`<strong>titles</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages titles to be encoded. This can be a string or a list of strings if there are several passages.`,name:"titles"},{anchor:"transformers.DPRReaderTokenizerFast.texts",description:`<strong>texts</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages texts to be encoded. This can be a string or a list of strings if there are several passages.`,name:"texts"},{anchor:"transformers.DPRReaderTokenizerFast.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DPRReaderTokenizerFast.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to
the maximum acceptable input length for the model if that argument is not provided. This will truncate
token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch
of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the first
sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the
second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.DPRReaderTokenizerFast.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.DPRReaderTokenizerFast.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DPRReaderTokenizerFast.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/tokenization_dpr_fast.py#L394",returnDescription:`
<p>A dictionary with the following keys:</p>
<ul>
<li><code>input_ids</code>: List of token ids to be fed to a model.</li>
<li><code>attention_mask</code>: List of indices specifying which tokens should be attended to by the model.</li>
</ul>
`,returnType:`
<p><code>Dict[str, List[List[int]]]</code></p>
`}}),un=new Ee({}),fn=new U({props:{name:"class transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",parameters:[{name:"pooler_output",val:": FloatTensor"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) &#x2014;
The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.`,name:"pooler_output"},{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L62"}}),gn=new U({props:{name:"class transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput",anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput",parameters:[{name:"pooler_output",val:": FloatTensor"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) &#x2014;
The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.`,name:"pooler_output"},{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L90"}}),vn=new U({props:{name:"class transformers.DPRReaderOutput",anchor:"transformers.DPRReaderOutput",parameters:[{name:"start_logits",val:": FloatTensor"},{name:"end_logits",val:": FloatTensor = None"},{name:"relevance_logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.DPRReaderOutput.start_logits",description:`<strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) &#x2014;
Logits of the start index of the span for each passage.`,name:"start_logits"},{anchor:"transformers.DPRReaderOutput.end_logits",description:`<strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) &#x2014;
Logits of the end index of the span for each passage.`,name:"end_logits"},{anchor:"transformers.DPRReaderOutput.relevance_logits",description:`<strong>relevance_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, )</code>) &#x2014;
Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.`,name:"relevance_logits"},{anchor:"transformers.DPRReaderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.DPRReaderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L118"}}),Tn=new Ee({}),wn=new U({props:{name:"class transformers.DPRContextEncoder",anchor:"transformers.DPRContextEncoder",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRContextEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L448"}}),En=new U({props:{name:"forward",anchor:"transformers.DPRContextEncoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRContextEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L456",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),po=new Xt({props:{$$slots:{default:[Tf]},$$scope:{ctx:C}}}),ho=new So({props:{anchor:"transformers.DPRContextEncoder.forward.example",$$slots:{default:[wf]},$$scope:{ctx:C}}}),Rn=new Ee({}),yn=new U({props:{name:"class transformers.DPRQuestionEncoder",anchor:"transformers.DPRQuestionEncoder",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRQuestionEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L529"}}),qn=new U({props:{name:"forward",anchor:"transformers.DPRQuestionEncoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRQuestionEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L537",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),fo=new Xt({props:{$$slots:{default:[kf]},$$scope:{ctx:C}}}),mo=new So({props:{anchor:"transformers.DPRQuestionEncoder.forward.example",$$slots:{default:[Pf]},$$scope:{ctx:C}}}),Cn=new Ee({}),Fn=new U({props:{name:"class transformers.DPRReader",anchor:"transformers.DPRReader",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L610"}}),Nn=new U({props:{name:"forward",anchor:"transformers.DPRReader.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": bool = None"},{name:"output_hidden_states",val:": bool = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRReader.forward.input_ids",description:`<strong>input_ids</strong> (<code>Tuple[torch.LongTensor]</code> of shapes <code>(n_passages, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. It has to be a sequence triplet with 1) the question
and 2) the passages titles and 3) the passages texts To match pretraining, DPR <code>input_ids</code> sequence should
be formatted with [CLS] and [SEP] with the format:</p>
<p><code>[CLS] &lt;question token ids&gt; [SEP] &lt;titles ids&gt; [SEP] &lt;texts ids&gt;</code></p>
<p>DPR is a model with absolute position embeddings so it&#x2019;s usually advised to pad the inputs on the right
rather than the left.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRReaderTokenizer">DPRReaderTokenizer</a>. See this class documentation for more details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DPRReader.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DPRReader.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DPRReader.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DPRReader.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DPRReader.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L618",returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRReaderOutput"
>transformers.models.dpr.modeling_dpr.DPRReaderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the start index of the span for each passage.</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the end index of the span for each passage.</p>
</li>
<li>
<p><strong>relevance_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, )</code>) \u2014 Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRReaderOutput"
>transformers.models.dpr.modeling_dpr.DPRReaderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),_o=new Xt({props:{$$slots:{default:[$f]},$$scope:{ctx:C}}}),vo=new So({props:{anchor:"transformers.DPRReader.forward.example",$$slots:{default:[Ef]},$$scope:{ctx:C}}}),In=new Ee({}),Qn=new U({props:{name:"class transformers.TFDPRContextEncoder",anchor:"transformers.TFDPRContextEncoder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRContextEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_tf_dpr.py#L540"}}),To=new Xt({props:{$$slots:{default:[Rf]},$$scope:{ctx:C}}}),Bn=new U({props:{name:"call",anchor:"transformers.TFDPRContextEncoder.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRContextEncoder.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_tf_dpr.py#L552",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),wo=new Xt({props:{$$slots:{default:[yf]},$$scope:{ctx:C}}}),ko=new So({props:{anchor:"transformers.TFDPRContextEncoder.call.example",$$slots:{default:[Df]},$$scope:{ctx:C}}}),Hn=new Ee({}),Kn=new U({props:{name:"class transformers.TFDPRQuestionEncoder",anchor:"transformers.TFDPRQuestionEncoder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRQuestionEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_tf_dpr.py#L627"}}),$o=new Xt({props:{$$slots:{default:[xf]},$$scope:{ctx:C}}}),Yn=new U({props:{name:"call",anchor:"transformers.TFDPRQuestionEncoder.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRQuestionEncoder.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_tf_dpr.py#L639",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),Eo=new Xt({props:{$$slots:{default:[zf]},$$scope:{ctx:C}}}),Ro=new So({props:{anchor:"transformers.TFDPRQuestionEncoder.call.example",$$slots:{default:[qf]},$$scope:{ctx:C}}}),Xn=new Ee({}),Gn=new U({props:{name:"class transformers.TFDPRReader",anchor:"transformers.TFDPRReader",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_tf_dpr.py#L713"}}),Do=new Xt({props:{$$slots:{default:[Cf]},$$scope:{ctx:C}}}),tr=new U({props:{name:"call",anchor:"transformers.TFDPRReader.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:": bool = None"},{name:"output_hidden_states",val:": bool = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRReader.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shapes <code>(n_passages, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. It has to be a sequence triplet with 1) the question
and 2) the passages titles and 3) the passages texts To match pretraining, DPR <code>input_ids</code> sequence should
be formatted with [CLS] and [SEP] with the format:</p>
<p><code>[CLS] &lt;question token ids&gt; [SEP] &lt;titles ids&gt; [SEP] &lt;texts ids&gt;</code></p>
<p>DPR is a model with absolute position embeddings so it&#x2019;s usually advised to pad the inputs on the right
rather than the left.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRReaderTokenizer">DPRReaderTokenizer</a>. See this class documentation for more details.`,name:"input_ids"},{anchor:"transformers.TFDPRReader.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDPRReader.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(n_passages, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDPRReader.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDPRReader.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDPRReader.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/dpr/modeling_tf_dpr.py#L725",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the start index of the span for each passage.</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the end index of the span for each passage.</p>
</li>
<li>
<p><strong>relevance_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, )</code>) \u2014 Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),xo=new Xt({props:{$$slots:{default:[Ff]},$$scope:{ctx:C}}}),zo=new So({props:{anchor:"transformers.TFDPRReader.call.example",$$slots:{default:[Af]},$$scope:{ctx:C}}}),{c(){h=s("meta"),T=l(),_=s("h1"),g=s("a"),b=s("span"),w(d.$$.fragment),u=l(),q=s("span"),Re=n("DPR"),be=l(),Q=s("h2"),te=s("a"),oe=s("span"),w(y.$$.fragment),ye=l(),H=s("span"),De=n("Overview"),Te=l(),L=s("p"),xe=n(`Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. It was
introduced in `),se=s("a"),V=n("Dense Passage Retrieval for Open-Domain Question Answering"),ze=n(` by
Vladimir Karpukhin, Barlas O\u011Fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.`),we=l(),S=s("p"),qe=n("The abstract from the paper is the following:"),ke=l(),B=s("p"),he=s("em"),Ce=n(`Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional
sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can
be practically implemented using dense representations alone, where embeddings are learned from a small number of
questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,
our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage
retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.`),ae=l(),z=s("p"),j=n("This model was contributed by "),ie=s("a"),Y=n("lhoestq"),Fe=n(". The original code can be found "),de=s("a"),X=n("here"),Ae=n("."),Pe=l(),F=s("h2"),ne=s("a"),K=s("span"),w(le.$$.fragment),je=l(),W=s("span"),Oe=n("DPRConfig"),$e=l(),A=s("div"),w(ce.$$.fragment),G=l(),re=s("p"),J=s("a"),O=n("DPRConfig"),Ne=n(" is the configuration class to store the configuration of a "),N=s("em"),Ie=n("DPRModel"),Qe=n("."),v=l(),D=s("p"),Xe=n("This is the configuration class to store the configuration of a "),Z=s("a"),Ge=n("DPRContextEncoder"),Je=n(", "),x=s("a"),Ze=n("DPRQuestionEncoder"),et=n(`, or a
`),Le=s("a"),tt=n("DPRReader"),ot=n(`. It is used to instantiate the components of the DPR model according to the specified arguments,
defining the model component architectures. Instantiating a configuration with the defaults will yield a similar
configuration to that of the DPRContextEncoder
`),I=s("a"),ee=n("facebook/dpr-ctx_encoder-single-nq-base"),nt=n(`
architecture.`),rt=l(),M=s("p"),st=n("This class is a subclass of "),Me=s("a"),Se=n("BertConfig"),at=n(". Please check the superclass for the documentation of all kwargs."),pe=l(),w(ue.$$.fragment),ca=l(),yt=s("h2"),Gt=s("a"),Xr=s("span"),w(Ko.$$.fragment),Ri=l(),Gr=s("span"),yi=n("DPRContextEncoderTokenizer"),pa=l(),We=s("div"),w(Wo.$$.fragment),Di=l(),Jr=s("p"),xi=n("Construct a DPRContextEncoder tokenizer."),zi=l(),Jt=s("p"),lr=s("a"),qi=n("DPRContextEncoderTokenizer"),Ci=n(" is identical to "),cr=s("a"),Fi=n("BertTokenizer"),Ai=n(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),ji=l(),Uo=s("p"),Oi=n("Refer to superclass "),pr=s("a"),Ni=n("BertTokenizer"),Ii=n(" for usage examples and documentation concerning parameters."),ha=l(),Dt=s("h2"),Zt=s("a"),Zr=s("span"),w(Vo.$$.fragment),Qi=l(),es=s("span"),Li=n("DPRContextEncoderTokenizerFast"),ua=l(),Ue=s("div"),w(Yo.$$.fragment),Mi=l(),Xo=s("p"),Si=n("Construct a \u201Cfast\u201D DPRContextEncoder tokenizer (backed by HuggingFace\u2019s "),ts=s("em"),Bi=n("tokenizers"),Hi=n(" library)."),Ki=l(),eo=s("p"),hr=s("a"),Wi=n("DPRContextEncoderTokenizerFast"),Ui=n(" is identical to "),ur=s("a"),Vi=n("BertTokenizerFast"),Yi=n(` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Xi=l(),Go=s("p"),Gi=n("Refer to superclass "),fr=s("a"),Ji=n("BertTokenizerFast"),Zi=n(" for usage examples and documentation concerning parameters."),fa=l(),xt=s("h2"),to=s("a"),os=s("span"),w(Jo.$$.fragment),ed=l(),ns=s("span"),td=n("DPRQuestionEncoderTokenizer"),ma=l(),Ve=s("div"),w(Zo.$$.fragment),od=l(),rs=s("p"),nd=n("Constructs a DPRQuestionEncoder tokenizer."),rd=l(),oo=s("p"),mr=s("a"),sd=n("DPRQuestionEncoderTokenizer"),ad=n(" is identical to "),gr=s("a"),id=n("BertTokenizer"),dd=n(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),ld=l(),en=s("p"),cd=n("Refer to superclass "),_r=s("a"),pd=n("BertTokenizer"),hd=n(" for usage examples and documentation concerning parameters."),ga=l(),zt=s("h2"),no=s("a"),ss=s("span"),w(tn.$$.fragment),ud=l(),as=s("span"),fd=n("DPRQuestionEncoderTokenizerFast"),_a=l(),Ye=s("div"),w(on.$$.fragment),md=l(),nn=s("p"),gd=n("Constructs a \u201Cfast\u201D DPRQuestionEncoder tokenizer (backed by HuggingFace\u2019s "),is=s("em"),_d=n("tokenizers"),vd=n(" library)."),bd=l(),ro=s("p"),vr=s("a"),Td=n("DPRQuestionEncoderTokenizerFast"),wd=n(" is identical to "),br=s("a"),kd=n("BertTokenizerFast"),Pd=n(` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),$d=l(),rn=s("p"),Ed=n("Refer to superclass "),Tr=s("a"),Rd=n("BertTokenizerFast"),yd=n(" for usage examples and documentation concerning parameters."),va=l(),qt=s("h2"),so=s("a"),ds=s("span"),w(sn.$$.fragment),Dd=l(),ls=s("span"),xd=n("DPRReaderTokenizer"),ba=l(),fe=s("div"),w(an.$$.fragment),zd=l(),cs=s("p"),qd=n("Construct a DPRReader tokenizer."),Cd=l(),bt=s("p"),wr=s("a"),Fd=n("DPRReaderTokenizer"),Ad=n(" is almost identical to "),kr=s("a"),jd=n("BertTokenizer"),Od=n(` and runs end-to-end tokenization: punctuation
splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts that are
combined to be fed to the `),Pr=s("a"),Nd=n("DPRReader"),Id=n(" model."),Qd=l(),dn=s("p"),Ld=n("Refer to superclass "),$r=s("a"),Md=n("BertTokenizer"),Sd=n(" for usage examples and documentation concerning parameters."),Bd=l(),Tt=s("p"),Hd=n("Return a dictionary with the token ids of the input strings and other information to give to "),ps=s("code"),Kd=n(".decode_best_spans"),Wd=n(`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),hs=s("code"),Ud=n("input_ids"),Vd=n(" is a matrix of size "),us=s("code"),Yd=n("(n_passages, sequence_length)"),Xd=l(),w(ao.$$.fragment),Ta=l(),Ct=s("h2"),io=s("a"),fs=s("span"),w(ln.$$.fragment),Gd=l(),ms=s("span"),Jd=n("DPRReaderTokenizerFast"),wa=l(),me=s("div"),w(cn.$$.fragment),Zd=l(),pn=s("p"),el=n("Constructs a \u201Cfast\u201D DPRReader tokenizer (backed by HuggingFace\u2019s "),gs=s("em"),tl=n("tokenizers"),ol=n(" library)."),nl=l(),wt=s("p"),Er=s("a"),rl=n("DPRReaderTokenizerFast"),sl=n(" is almost identical to "),Rr=s("a"),al=n("BertTokenizerFast"),il=n(` and runs end-to-end tokenization:
punctuation splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts
that are combined to be fed to the `),yr=s("a"),dl=n("DPRReader"),ll=n(" model."),cl=l(),hn=s("p"),pl=n("Refer to superclass "),Dr=s("a"),hl=n("BertTokenizerFast"),ul=n(" for usage examples and documentation concerning parameters."),fl=l(),vt=s("p"),ml=n("Return a dictionary with the token ids of the input strings and other information to give to "),_s=s("code"),gl=n(".decode_best_spans"),_l=n(`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),vs=s("code"),vl=n("input_ids"),bl=n(" is a matrix of size "),bs=s("code"),Tl=n("(n_passages, sequence_length)"),wl=n(`
with the format:`),kl=l(),Ts=s("p"),Pl=n("[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>"),ka=l(),Ft=s("h2"),lo=s("a"),ws=s("span"),w(un.$$.fragment),$l=l(),ks=s("span"),El=n("DPR specific outputs"),Pa=l(),At=s("div"),w(fn.$$.fragment),Rl=l(),mn=s("p"),yl=n("Class for outputs of "),xr=s("a"),Dl=n("DPRQuestionEncoder"),xl=n("."),$a=l(),jt=s("div"),w(gn.$$.fragment),zl=l(),_n=s("p"),ql=n("Class for outputs of "),zr=s("a"),Cl=n("DPRQuestionEncoder"),Fl=n("."),Ea=l(),Ot=s("div"),w(vn.$$.fragment),Al=l(),bn=s("p"),jl=n("Class for outputs of "),qr=s("a"),Ol=n("DPRQuestionEncoder"),Nl=n("."),Ra=l(),Nt=s("h2"),co=s("a"),Ps=s("span"),w(Tn.$$.fragment),Il=l(),$s=s("span"),Ql=n("DPRContextEncoder"),ya=l(),Be=s("div"),w(wn.$$.fragment),Ll=l(),Es=s("p"),Ml=n("The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Sl=l(),kn=s("p"),Bl=n("This model inherits from "),Cr=s("a"),Hl=n("PreTrainedModel"),Kl=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Wl=l(),Pn=s("p"),Ul=n("This model is also a PyTorch "),$n=s("a"),Vl=n("torch.nn.Module"),Yl=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Xl=l(),it=s("div"),w(En.$$.fragment),Gl=l(),It=s("p"),Jl=n("The "),Fr=s("a"),Zl=n("DPRContextEncoder"),ec=n(" forward method, overrides the "),Rs=s("code"),tc=n("__call__"),oc=n(" special method."),nc=l(),w(po.$$.fragment),rc=l(),w(ho.$$.fragment),Da=l(),Qt=s("h2"),uo=s("a"),ys=s("span"),w(Rn.$$.fragment),sc=l(),Ds=s("span"),ac=n("DPRQuestionEncoder"),xa=l(),He=s("div"),w(yn.$$.fragment),ic=l(),xs=s("p"),dc=n("The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),lc=l(),Dn=s("p"),cc=n("This model inherits from "),Ar=s("a"),pc=n("PreTrainedModel"),hc=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),uc=l(),xn=s("p"),fc=n("This model is also a PyTorch "),zn=s("a"),mc=n("torch.nn.Module"),gc=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),_c=l(),dt=s("div"),w(qn.$$.fragment),vc=l(),Lt=s("p"),bc=n("The "),jr=s("a"),Tc=n("DPRQuestionEncoder"),wc=n(" forward method, overrides the "),zs=s("code"),kc=n("__call__"),Pc=n(" special method."),$c=l(),w(fo.$$.fragment),Ec=l(),w(mo.$$.fragment),za=l(),Mt=s("h2"),go=s("a"),qs=s("span"),w(Cn.$$.fragment),Rc=l(),Cs=s("span"),yc=n("DPRReader"),qa=l(),Ke=s("div"),w(Fn.$$.fragment),Dc=l(),Fs=s("p"),xc=n("The bare DPRReader transformer outputting span predictions."),zc=l(),An=s("p"),qc=n("This model inherits from "),Or=s("a"),Cc=n("PreTrainedModel"),Fc=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ac=l(),jn=s("p"),jc=n("This model is also a PyTorch "),On=s("a"),Oc=n("torch.nn.Module"),Nc=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ic=l(),lt=s("div"),w(Nn.$$.fragment),Qc=l(),St=s("p"),Lc=n("The "),Nr=s("a"),Mc=n("DPRReader"),Sc=n(" forward method, overrides the "),As=s("code"),Bc=n("__call__"),Hc=n(" special method."),Kc=l(),w(_o.$$.fragment),Wc=l(),w(vo.$$.fragment),Ca=l(),Bt=s("h2"),bo=s("a"),js=s("span"),w(In.$$.fragment),Uc=l(),Os=s("span"),Vc=n("TFDPRContextEncoder"),Fa=l(),ge=s("div"),w(Qn.$$.fragment),Yc=l(),Ns=s("p"),Xc=n("The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Gc=l(),Ln=s("p"),Jc=n("This model inherits from "),Ir=s("a"),Zc=n("TFPreTrainedModel"),ep=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),tp=l(),Mn=s("p"),op=n("This model is also a Tensorflow "),Sn=s("a"),np=n("tf.keras.Model"),rp=n(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),sp=l(),w(To.$$.fragment),ap=l(),ct=s("div"),w(Bn.$$.fragment),ip=l(),Ht=s("p"),dp=n("The "),Qr=s("a"),lp=n("TFDPRContextEncoder"),cp=n(" forward method, overrides the "),Is=s("code"),pp=n("__call__"),hp=n(" special method."),up=l(),w(wo.$$.fragment),fp=l(),w(ko.$$.fragment),Aa=l(),Kt=s("h2"),Po=s("a"),Qs=s("span"),w(Hn.$$.fragment),mp=l(),Ls=s("span"),gp=n("TFDPRQuestionEncoder"),ja=l(),_e=s("div"),w(Kn.$$.fragment),_p=l(),Ms=s("p"),vp=n("The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),bp=l(),Wn=s("p"),Tp=n("This model inherits from "),Lr=s("a"),wp=n("TFPreTrainedModel"),kp=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Pp=l(),Un=s("p"),$p=n("This model is also a Tensorflow "),Vn=s("a"),Ep=n("tf.keras.Model"),Rp=n(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),yp=l(),w($o.$$.fragment),Dp=l(),pt=s("div"),w(Yn.$$.fragment),xp=l(),Wt=s("p"),zp=n("The "),Mr=s("a"),qp=n("TFDPRQuestionEncoder"),Cp=n(" forward method, overrides the "),Ss=s("code"),Fp=n("__call__"),Ap=n(" special method."),jp=l(),w(Eo.$$.fragment),Op=l(),w(Ro.$$.fragment),Oa=l(),Ut=s("h2"),yo=s("a"),Bs=s("span"),w(Xn.$$.fragment),Np=l(),Hs=s("span"),Ip=n("TFDPRReader"),Na=l(),ve=s("div"),w(Gn.$$.fragment),Qp=l(),Ks=s("p"),Lp=n("The bare DPRReader transformer outputting span predictions."),Mp=l(),Jn=s("p"),Sp=n("This model inherits from "),Sr=s("a"),Bp=n("TFPreTrainedModel"),Hp=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Kp=l(),Zn=s("p"),Wp=n("This model is also a Tensorflow "),er=s("a"),Up=n("tf.keras.Model"),Vp=n(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),Yp=l(),w(Do.$$.fragment),Xp=l(),ht=s("div"),w(tr.$$.fragment),Gp=l(),Vt=s("p"),Jp=n("The "),Br=s("a"),Zp=n("TFDPRReader"),eh=n(" forward method, overrides the "),Ws=s("code"),th=n("__call__"),oh=n(" special method."),nh=l(),w(xo.$$.fragment),rh=l(),w(zo.$$.fragment),this.h()},l(o){const m=gf('[data-svelte="svelte-1phssyn"]',document.head);h=a(m,"META",{name:!0,content:!0}),m.forEach(t),T=c(o),_=a(o,"H1",{class:!0});var or=i(_);g=a(or,"A",{id:!0,class:!0,href:!0});var Us=i(g);b=a(Us,"SPAN",{});var Vs=i(b);k(d.$$.fragment,Vs),Vs.forEach(t),Us.forEach(t),u=c(or),q=a(or,"SPAN",{});var Ys=i(q);Re=r(Ys,"DPR"),Ys.forEach(t),or.forEach(t),be=c(o),Q=a(o,"H2",{class:!0});var nr=i(Q);te=a(nr,"A",{id:!0,class:!0,href:!0});var Xs=i(te);oe=a(Xs,"SPAN",{});var Gs=i(oe);k(y.$$.fragment,Gs),Gs.forEach(t),Xs.forEach(t),ye=c(nr),H=a(nr,"SPAN",{});var Js=i(H);De=r(Js,"Overview"),Js.forEach(t),nr.forEach(t),Te=c(o),L=a(o,"P",{});var rr=i(L);xe=r(rr,`Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. It was
introduced in `),se=a(rr,"A",{href:!0,rel:!0});var Zs=i(se);V=r(Zs,"Dense Passage Retrieval for Open-Domain Question Answering"),Zs.forEach(t),ze=r(rr,` by
Vladimir Karpukhin, Barlas O\u011Fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.`),rr.forEach(t),we=c(o),S=a(o,"P",{});var ea=i(S);qe=r(ea,"The abstract from the paper is the following:"),ea.forEach(t),ke=c(o),B=a(o,"P",{});var ta=i(B);he=a(ta,"EM",{});var oa=i(he);Ce=r(oa,`Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional
sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can
be practically implemented using dense representations alone, where embeddings are learned from a small number of
questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,
our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage
retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.`),oa.forEach(t),ta.forEach(t),ae=c(o),z=a(o,"P",{});var Yt=i(z);j=r(Yt,"This model was contributed by "),ie=a(Yt,"A",{href:!0,rel:!0});var na=i(ie);Y=r(na,"lhoestq"),na.forEach(t),Fe=r(Yt,". The original code can be found "),de=a(Yt,"A",{href:!0,rel:!0});var ra=i(de);X=r(ra,"here"),ra.forEach(t),Ae=r(Yt,"."),Yt.forEach(t),Pe=c(o),F=a(o,"H2",{class:!0});var sr=i(F);ne=a(sr,"A",{id:!0,class:!0,href:!0});var sh=i(ne);K=a(sh,"SPAN",{});var ah=i(K);k(le.$$.fragment,ah),ah.forEach(t),sh.forEach(t),je=c(sr),W=a(sr,"SPAN",{});var ih=i(W);Oe=r(ih,"DPRConfig"),ih.forEach(t),sr.forEach(t),$e=c(o),A=a(o,"DIV",{class:!0});var kt=i(A);k(ce.$$.fragment,kt),G=c(kt),re=a(kt,"P",{});var sa=i(re);J=a(sa,"A",{href:!0});var dh=i(J);O=r(dh,"DPRConfig"),dh.forEach(t),Ne=r(sa," is the configuration class to store the configuration of a "),N=a(sa,"EM",{});var lh=i(N);Ie=r(lh,"DPRModel"),lh.forEach(t),Qe=r(sa,"."),sa.forEach(t),v=c(kt),D=a(kt,"P",{});var Pt=i(D);Xe=r(Pt,"This is the configuration class to store the configuration of a "),Z=a(Pt,"A",{href:!0});var ch=i(Z);Ge=r(ch,"DPRContextEncoder"),ch.forEach(t),Je=r(Pt,", "),x=a(Pt,"A",{href:!0});var ph=i(x);Ze=r(ph,"DPRQuestionEncoder"),ph.forEach(t),et=r(Pt,`, or a
`),Le=a(Pt,"A",{href:!0});var hh=i(Le);tt=r(hh,"DPRReader"),hh.forEach(t),ot=r(Pt,`. It is used to instantiate the components of the DPR model according to the specified arguments,
defining the model component architectures. Instantiating a configuration with the defaults will yield a similar
configuration to that of the DPRContextEncoder
`),I=a(Pt,"A",{href:!0,rel:!0});var uh=i(I);ee=r(uh,"facebook/dpr-ctx_encoder-single-nq-base"),uh.forEach(t),nt=r(Pt,`
architecture.`),Pt.forEach(t),rt=c(kt),M=a(kt,"P",{});var Qa=i(M);st=r(Qa,"This class is a subclass of "),Me=a(Qa,"A",{href:!0});var fh=i(Me);Se=r(fh,"BertConfig"),fh.forEach(t),at=r(Qa,". Please check the superclass for the documentation of all kwargs."),Qa.forEach(t),pe=c(kt),k(ue.$$.fragment,kt),kt.forEach(t),ca=c(o),yt=a(o,"H2",{class:!0});var La=i(yt);Gt=a(La,"A",{id:!0,class:!0,href:!0});var mh=i(Gt);Xr=a(mh,"SPAN",{});var gh=i(Xr);k(Ko.$$.fragment,gh),gh.forEach(t),mh.forEach(t),Ri=c(La),Gr=a(La,"SPAN",{});var _h=i(Gr);yi=r(_h,"DPRContextEncoderTokenizer"),_h.forEach(t),La.forEach(t),pa=c(o),We=a(o,"DIV",{class:!0});var qo=i(We);k(Wo.$$.fragment,qo),Di=c(qo),Jr=a(qo,"P",{});var vh=i(Jr);xi=r(vh,"Construct a DPRContextEncoder tokenizer."),vh.forEach(t),zi=c(qo),Jt=a(qo,"P",{});var aa=i(Jt);lr=a(aa,"A",{href:!0});var bh=i(lr);qi=r(bh,"DPRContextEncoderTokenizer"),bh.forEach(t),Ci=r(aa," is identical to "),cr=a(aa,"A",{href:!0});var Th=i(cr);Fi=r(Th,"BertTokenizer"),Th.forEach(t),Ai=r(aa,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),aa.forEach(t),ji=c(qo),Uo=a(qo,"P",{});var Ma=i(Uo);Oi=r(Ma,"Refer to superclass "),pr=a(Ma,"A",{href:!0});var wh=i(pr);Ni=r(wh,"BertTokenizer"),wh.forEach(t),Ii=r(Ma," for usage examples and documentation concerning parameters."),Ma.forEach(t),qo.forEach(t),ha=c(o),Dt=a(o,"H2",{class:!0});var Sa=i(Dt);Zt=a(Sa,"A",{id:!0,class:!0,href:!0});var kh=i(Zt);Zr=a(kh,"SPAN",{});var Ph=i(Zr);k(Vo.$$.fragment,Ph),Ph.forEach(t),kh.forEach(t),Qi=c(Sa),es=a(Sa,"SPAN",{});var $h=i(es);Li=r($h,"DPRContextEncoderTokenizerFast"),$h.forEach(t),Sa.forEach(t),ua=c(o),Ue=a(o,"DIV",{class:!0});var Co=i(Ue);k(Yo.$$.fragment,Co),Mi=c(Co),Xo=a(Co,"P",{});var Ba=i(Xo);Si=r(Ba,"Construct a \u201Cfast\u201D DPRContextEncoder tokenizer (backed by HuggingFace\u2019s "),ts=a(Ba,"EM",{});var Eh=i(ts);Bi=r(Eh,"tokenizers"),Eh.forEach(t),Hi=r(Ba," library)."),Ba.forEach(t),Ki=c(Co),eo=a(Co,"P",{});var ia=i(eo);hr=a(ia,"A",{href:!0});var Rh=i(hr);Wi=r(Rh,"DPRContextEncoderTokenizerFast"),Rh.forEach(t),Ui=r(ia," is identical to "),ur=a(ia,"A",{href:!0});var yh=i(ur);Vi=r(yh,"BertTokenizerFast"),yh.forEach(t),Yi=r(ia,` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),ia.forEach(t),Xi=c(Co),Go=a(Co,"P",{});var Ha=i(Go);Gi=r(Ha,"Refer to superclass "),fr=a(Ha,"A",{href:!0});var Dh=i(fr);Ji=r(Dh,"BertTokenizerFast"),Dh.forEach(t),Zi=r(Ha," for usage examples and documentation concerning parameters."),Ha.forEach(t),Co.forEach(t),fa=c(o),xt=a(o,"H2",{class:!0});var Ka=i(xt);to=a(Ka,"A",{id:!0,class:!0,href:!0});var xh=i(to);os=a(xh,"SPAN",{});var zh=i(os);k(Jo.$$.fragment,zh),zh.forEach(t),xh.forEach(t),ed=c(Ka),ns=a(Ka,"SPAN",{});var qh=i(ns);td=r(qh,"DPRQuestionEncoderTokenizer"),qh.forEach(t),Ka.forEach(t),ma=c(o),Ve=a(o,"DIV",{class:!0});var Fo=i(Ve);k(Zo.$$.fragment,Fo),od=c(Fo),rs=a(Fo,"P",{});var Ch=i(rs);nd=r(Ch,"Constructs a DPRQuestionEncoder tokenizer."),Ch.forEach(t),rd=c(Fo),oo=a(Fo,"P",{});var da=i(oo);mr=a(da,"A",{href:!0});var Fh=i(mr);sd=r(Fh,"DPRQuestionEncoderTokenizer"),Fh.forEach(t),ad=r(da," is identical to "),gr=a(da,"A",{href:!0});var Ah=i(gr);id=r(Ah,"BertTokenizer"),Ah.forEach(t),dd=r(da,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),da.forEach(t),ld=c(Fo),en=a(Fo,"P",{});var Wa=i(en);cd=r(Wa,"Refer to superclass "),_r=a(Wa,"A",{href:!0});var jh=i(_r);pd=r(jh,"BertTokenizer"),jh.forEach(t),hd=r(Wa," for usage examples and documentation concerning parameters."),Wa.forEach(t),Fo.forEach(t),ga=c(o),zt=a(o,"H2",{class:!0});var Ua=i(zt);no=a(Ua,"A",{id:!0,class:!0,href:!0});var Oh=i(no);ss=a(Oh,"SPAN",{});var Nh=i(ss);k(tn.$$.fragment,Nh),Nh.forEach(t),Oh.forEach(t),ud=c(Ua),as=a(Ua,"SPAN",{});var Ih=i(as);fd=r(Ih,"DPRQuestionEncoderTokenizerFast"),Ih.forEach(t),Ua.forEach(t),_a=c(o),Ye=a(o,"DIV",{class:!0});var Ao=i(Ye);k(on.$$.fragment,Ao),md=c(Ao),nn=a(Ao,"P",{});var Va=i(nn);gd=r(Va,"Constructs a \u201Cfast\u201D DPRQuestionEncoder tokenizer (backed by HuggingFace\u2019s "),is=a(Va,"EM",{});var Qh=i(is);_d=r(Qh,"tokenizers"),Qh.forEach(t),vd=r(Va," library)."),Va.forEach(t),bd=c(Ao),ro=a(Ao,"P",{});var la=i(ro);vr=a(la,"A",{href:!0});var Lh=i(vr);Td=r(Lh,"DPRQuestionEncoderTokenizerFast"),Lh.forEach(t),wd=r(la," is identical to "),br=a(la,"A",{href:!0});var Mh=i(br);kd=r(Mh,"BertTokenizerFast"),Mh.forEach(t),Pd=r(la,` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),la.forEach(t),$d=c(Ao),rn=a(Ao,"P",{});var Ya=i(rn);Ed=r(Ya,"Refer to superclass "),Tr=a(Ya,"A",{href:!0});var Sh=i(Tr);Rd=r(Sh,"BertTokenizerFast"),Sh.forEach(t),yd=r(Ya," for usage examples and documentation concerning parameters."),Ya.forEach(t),Ao.forEach(t),va=c(o),qt=a(o,"H2",{class:!0});var Xa=i(qt);so=a(Xa,"A",{id:!0,class:!0,href:!0});var Bh=i(so);ds=a(Bh,"SPAN",{});var Hh=i(ds);k(sn.$$.fragment,Hh),Hh.forEach(t),Bh.forEach(t),Dd=c(Xa),ls=a(Xa,"SPAN",{});var Kh=i(ls);xd=r(Kh,"DPRReaderTokenizer"),Kh.forEach(t),Xa.forEach(t),ba=c(o),fe=a(o,"DIV",{class:!0});var ut=i(fe);k(an.$$.fragment,ut),zd=c(ut),cs=a(ut,"P",{});var Wh=i(cs);qd=r(Wh,"Construct a DPRReader tokenizer."),Wh.forEach(t),Cd=c(ut),bt=a(ut,"P",{});var ar=i(bt);wr=a(ar,"A",{href:!0});var Uh=i(wr);Fd=r(Uh,"DPRReaderTokenizer"),Uh.forEach(t),Ad=r(ar," is almost identical to "),kr=a(ar,"A",{href:!0});var Vh=i(kr);jd=r(Vh,"BertTokenizer"),Vh.forEach(t),Od=r(ar,` and runs end-to-end tokenization: punctuation
splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts that are
combined to be fed to the `),Pr=a(ar,"A",{href:!0});var Yh=i(Pr);Nd=r(Yh,"DPRReader"),Yh.forEach(t),Id=r(ar," model."),ar.forEach(t),Qd=c(ut),dn=a(ut,"P",{});var Ga=i(dn);Ld=r(Ga,"Refer to superclass "),$r=a(Ga,"A",{href:!0});var Xh=i($r);Md=r(Xh,"BertTokenizer"),Xh.forEach(t),Sd=r(Ga," for usage examples and documentation concerning parameters."),Ga.forEach(t),Bd=c(ut),Tt=a(ut,"P",{});var ir=i(Tt);Hd=r(ir,"Return a dictionary with the token ids of the input strings and other information to give to "),ps=a(ir,"CODE",{});var Gh=i(ps);Kd=r(Gh,".decode_best_spans"),Gh.forEach(t),Wd=r(ir,`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),hs=a(ir,"CODE",{});var Jh=i(hs);Ud=r(Jh,"input_ids"),Jh.forEach(t),Vd=r(ir," is a matrix of size "),us=a(ir,"CODE",{});var Zh=i(us);Yd=r(Zh,"(n_passages, sequence_length)"),Zh.forEach(t),ir.forEach(t),Xd=c(ut),k(ao.$$.fragment,ut),ut.forEach(t),Ta=c(o),Ct=a(o,"H2",{class:!0});var Ja=i(Ct);io=a(Ja,"A",{id:!0,class:!0,href:!0});var eu=i(io);fs=a(eu,"SPAN",{});var tu=i(fs);k(ln.$$.fragment,tu),tu.forEach(t),eu.forEach(t),Gd=c(Ja),ms=a(Ja,"SPAN",{});var ou=i(ms);Jd=r(ou,"DPRReaderTokenizerFast"),ou.forEach(t),Ja.forEach(t),wa=c(o),me=a(o,"DIV",{class:!0});var ft=i(me);k(cn.$$.fragment,ft),Zd=c(ft),pn=a(ft,"P",{});var Za=i(pn);el=r(Za,"Constructs a \u201Cfast\u201D DPRReader tokenizer (backed by HuggingFace\u2019s "),gs=a(Za,"EM",{});var nu=i(gs);tl=r(nu,"tokenizers"),nu.forEach(t),ol=r(Za," library)."),Za.forEach(t),nl=c(ft),wt=a(ft,"P",{});var dr=i(wt);Er=a(dr,"A",{href:!0});var ru=i(Er);rl=r(ru,"DPRReaderTokenizerFast"),ru.forEach(t),sl=r(dr," is almost identical to "),Rr=a(dr,"A",{href:!0});var su=i(Rr);al=r(su,"BertTokenizerFast"),su.forEach(t),il=r(dr,` and runs end-to-end tokenization:
punctuation splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts
that are combined to be fed to the `),yr=a(dr,"A",{href:!0});var au=i(yr);dl=r(au,"DPRReader"),au.forEach(t),ll=r(dr," model."),dr.forEach(t),cl=c(ft),hn=a(ft,"P",{});var ei=i(hn);pl=r(ei,"Refer to superclass "),Dr=a(ei,"A",{href:!0});var iu=i(Dr);hl=r(iu,"BertTokenizerFast"),iu.forEach(t),ul=r(ei," for usage examples and documentation concerning parameters."),ei.forEach(t),fl=c(ft),vt=a(ft,"P",{});var jo=i(vt);ml=r(jo,"Return a dictionary with the token ids of the input strings and other information to give to "),_s=a(jo,"CODE",{});var du=i(_s);gl=r(du,".decode_best_spans"),du.forEach(t),_l=r(jo,`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),vs=a(jo,"CODE",{});var lu=i(vs);vl=r(lu,"input_ids"),lu.forEach(t),bl=r(jo," is a matrix of size "),bs=a(jo,"CODE",{});var cu=i(bs);Tl=r(cu,"(n_passages, sequence_length)"),cu.forEach(t),wl=r(jo,`
with the format:`),jo.forEach(t),kl=c(ft),Ts=a(ft,"P",{});var pu=i(Ts);Pl=r(pu,"[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>"),pu.forEach(t),ft.forEach(t),ka=c(o),Ft=a(o,"H2",{class:!0});var ti=i(Ft);lo=a(ti,"A",{id:!0,class:!0,href:!0});var hu=i(lo);ws=a(hu,"SPAN",{});var uu=i(ws);k(un.$$.fragment,uu),uu.forEach(t),hu.forEach(t),$l=c(ti),ks=a(ti,"SPAN",{});var fu=i(ks);El=r(fu,"DPR specific outputs"),fu.forEach(t),ti.forEach(t),Pa=c(o),At=a(o,"DIV",{class:!0});var oi=i(At);k(fn.$$.fragment,oi),Rl=c(oi),mn=a(oi,"P",{});var ni=i(mn);yl=r(ni,"Class for outputs of "),xr=a(ni,"A",{href:!0});var mu=i(xr);Dl=r(mu,"DPRQuestionEncoder"),mu.forEach(t),xl=r(ni,"."),ni.forEach(t),oi.forEach(t),$a=c(o),jt=a(o,"DIV",{class:!0});var ri=i(jt);k(gn.$$.fragment,ri),zl=c(ri),_n=a(ri,"P",{});var si=i(_n);ql=r(si,"Class for outputs of "),zr=a(si,"A",{href:!0});var gu=i(zr);Cl=r(gu,"DPRQuestionEncoder"),gu.forEach(t),Fl=r(si,"."),si.forEach(t),ri.forEach(t),Ea=c(o),Ot=a(o,"DIV",{class:!0});var ai=i(Ot);k(vn.$$.fragment,ai),Al=c(ai),bn=a(ai,"P",{});var ii=i(bn);jl=r(ii,"Class for outputs of "),qr=a(ii,"A",{href:!0});var _u=i(qr);Ol=r(_u,"DPRQuestionEncoder"),_u.forEach(t),Nl=r(ii,"."),ii.forEach(t),ai.forEach(t),Ra=c(o),Nt=a(o,"H2",{class:!0});var di=i(Nt);co=a(di,"A",{id:!0,class:!0,href:!0});var vu=i(co);Ps=a(vu,"SPAN",{});var bu=i(Ps);k(Tn.$$.fragment,bu),bu.forEach(t),vu.forEach(t),Il=c(di),$s=a(di,"SPAN",{});var Tu=i($s);Ql=r(Tu,"DPRContextEncoder"),Tu.forEach(t),di.forEach(t),ya=c(o),Be=a(o,"DIV",{class:!0});var $t=i(Be);k(wn.$$.fragment,$t),Ll=c($t),Es=a($t,"P",{});var wu=i(Es);Ml=r(wu,"The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),wu.forEach(t),Sl=c($t),kn=a($t,"P",{});var li=i(kn);Bl=r(li,"This model inherits from "),Cr=a(li,"A",{href:!0});var ku=i(Cr);Hl=r(ku,"PreTrainedModel"),ku.forEach(t),Kl=r(li,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),li.forEach(t),Wl=c($t),Pn=a($t,"P",{});var ci=i(Pn);Ul=r(ci,"This model is also a PyTorch "),$n=a(ci,"A",{href:!0,rel:!0});var Pu=i($n);Vl=r(Pu,"torch.nn.Module"),Pu.forEach(t),Yl=r(ci,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ci.forEach(t),Xl=c($t),it=a($t,"DIV",{class:!0});var Oo=i(it);k(En.$$.fragment,Oo),Gl=c(Oo),It=a(Oo,"P",{});var Hr=i(It);Jl=r(Hr,"The "),Fr=a(Hr,"A",{href:!0});var $u=i(Fr);Zl=r($u,"DPRContextEncoder"),$u.forEach(t),ec=r(Hr," forward method, overrides the "),Rs=a(Hr,"CODE",{});var Eu=i(Rs);tc=r(Eu,"__call__"),Eu.forEach(t),oc=r(Hr," special method."),Hr.forEach(t),nc=c(Oo),k(po.$$.fragment,Oo),rc=c(Oo),k(ho.$$.fragment,Oo),Oo.forEach(t),$t.forEach(t),Da=c(o),Qt=a(o,"H2",{class:!0});var pi=i(Qt);uo=a(pi,"A",{id:!0,class:!0,href:!0});var Ru=i(uo);ys=a(Ru,"SPAN",{});var yu=i(ys);k(Rn.$$.fragment,yu),yu.forEach(t),Ru.forEach(t),sc=c(pi),Ds=a(pi,"SPAN",{});var Du=i(Ds);ac=r(Du,"DPRQuestionEncoder"),Du.forEach(t),pi.forEach(t),xa=c(o),He=a(o,"DIV",{class:!0});var Et=i(He);k(yn.$$.fragment,Et),ic=c(Et),xs=a(Et,"P",{});var xu=i(xs);dc=r(xu,"The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),xu.forEach(t),lc=c(Et),Dn=a(Et,"P",{});var hi=i(Dn);cc=r(hi,"This model inherits from "),Ar=a(hi,"A",{href:!0});var zu=i(Ar);pc=r(zu,"PreTrainedModel"),zu.forEach(t),hc=r(hi,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),hi.forEach(t),uc=c(Et),xn=a(Et,"P",{});var ui=i(xn);fc=r(ui,"This model is also a PyTorch "),zn=a(ui,"A",{href:!0,rel:!0});var qu=i(zn);mc=r(qu,"torch.nn.Module"),qu.forEach(t),gc=r(ui,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ui.forEach(t),_c=c(Et),dt=a(Et,"DIV",{class:!0});var No=i(dt);k(qn.$$.fragment,No),vc=c(No),Lt=a(No,"P",{});var Kr=i(Lt);bc=r(Kr,"The "),jr=a(Kr,"A",{href:!0});var Cu=i(jr);Tc=r(Cu,"DPRQuestionEncoder"),Cu.forEach(t),wc=r(Kr," forward method, overrides the "),zs=a(Kr,"CODE",{});var Fu=i(zs);kc=r(Fu,"__call__"),Fu.forEach(t),Pc=r(Kr," special method."),Kr.forEach(t),$c=c(No),k(fo.$$.fragment,No),Ec=c(No),k(mo.$$.fragment,No),No.forEach(t),Et.forEach(t),za=c(o),Mt=a(o,"H2",{class:!0});var fi=i(Mt);go=a(fi,"A",{id:!0,class:!0,href:!0});var Au=i(go);qs=a(Au,"SPAN",{});var ju=i(qs);k(Cn.$$.fragment,ju),ju.forEach(t),Au.forEach(t),Rc=c(fi),Cs=a(fi,"SPAN",{});var Ou=i(Cs);yc=r(Ou,"DPRReader"),Ou.forEach(t),fi.forEach(t),qa=c(o),Ke=a(o,"DIV",{class:!0});var Rt=i(Ke);k(Fn.$$.fragment,Rt),Dc=c(Rt),Fs=a(Rt,"P",{});var Nu=i(Fs);xc=r(Nu,"The bare DPRReader transformer outputting span predictions."),Nu.forEach(t),zc=c(Rt),An=a(Rt,"P",{});var mi=i(An);qc=r(mi,"This model inherits from "),Or=a(mi,"A",{href:!0});var Iu=i(Or);Cc=r(Iu,"PreTrainedModel"),Iu.forEach(t),Fc=r(mi,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),mi.forEach(t),Ac=c(Rt),jn=a(Rt,"P",{});var gi=i(jn);jc=r(gi,"This model is also a PyTorch "),On=a(gi,"A",{href:!0,rel:!0});var Qu=i(On);Oc=r(Qu,"torch.nn.Module"),Qu.forEach(t),Nc=r(gi,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),gi.forEach(t),Ic=c(Rt),lt=a(Rt,"DIV",{class:!0});var Io=i(lt);k(Nn.$$.fragment,Io),Qc=c(Io),St=a(Io,"P",{});var Wr=i(St);Lc=r(Wr,"The "),Nr=a(Wr,"A",{href:!0});var Lu=i(Nr);Mc=r(Lu,"DPRReader"),Lu.forEach(t),Sc=r(Wr," forward method, overrides the "),As=a(Wr,"CODE",{});var Mu=i(As);Bc=r(Mu,"__call__"),Mu.forEach(t),Hc=r(Wr," special method."),Wr.forEach(t),Kc=c(Io),k(_o.$$.fragment,Io),Wc=c(Io),k(vo.$$.fragment,Io),Io.forEach(t),Rt.forEach(t),Ca=c(o),Bt=a(o,"H2",{class:!0});var _i=i(Bt);bo=a(_i,"A",{id:!0,class:!0,href:!0});var Su=i(bo);js=a(Su,"SPAN",{});var Bu=i(js);k(In.$$.fragment,Bu),Bu.forEach(t),Su.forEach(t),Uc=c(_i),Os=a(_i,"SPAN",{});var Hu=i(Os);Vc=r(Hu,"TFDPRContextEncoder"),Hu.forEach(t),_i.forEach(t),Fa=c(o),ge=a(o,"DIV",{class:!0});var mt=i(ge);k(Qn.$$.fragment,mt),Yc=c(mt),Ns=a(mt,"P",{});var Ku=i(Ns);Xc=r(Ku,"The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Ku.forEach(t),Gc=c(mt),Ln=a(mt,"P",{});var vi=i(Ln);Jc=r(vi,"This model inherits from "),Ir=a(vi,"A",{href:!0});var Wu=i(Ir);Zc=r(Wu,"TFPreTrainedModel"),Wu.forEach(t),ep=r(vi,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),vi.forEach(t),tp=c(mt),Mn=a(mt,"P",{});var bi=i(Mn);op=r(bi,"This model is also a Tensorflow "),Sn=a(bi,"A",{href:!0,rel:!0});var Uu=i(Sn);np=r(Uu,"tf.keras.Model"),Uu.forEach(t),rp=r(bi,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),bi.forEach(t),sp=c(mt),k(To.$$.fragment,mt),ap=c(mt),ct=a(mt,"DIV",{class:!0});var Qo=i(ct);k(Bn.$$.fragment,Qo),ip=c(Qo),Ht=a(Qo,"P",{});var Ur=i(Ht);dp=r(Ur,"The "),Qr=a(Ur,"A",{href:!0});var Vu=i(Qr);lp=r(Vu,"TFDPRContextEncoder"),Vu.forEach(t),cp=r(Ur," forward method, overrides the "),Is=a(Ur,"CODE",{});var Yu=i(Is);pp=r(Yu,"__call__"),Yu.forEach(t),hp=r(Ur," special method."),Ur.forEach(t),up=c(Qo),k(wo.$$.fragment,Qo),fp=c(Qo),k(ko.$$.fragment,Qo),Qo.forEach(t),mt.forEach(t),Aa=c(o),Kt=a(o,"H2",{class:!0});var Ti=i(Kt);Po=a(Ti,"A",{id:!0,class:!0,href:!0});var Xu=i(Po);Qs=a(Xu,"SPAN",{});var Gu=i(Qs);k(Hn.$$.fragment,Gu),Gu.forEach(t),Xu.forEach(t),mp=c(Ti),Ls=a(Ti,"SPAN",{});var Ju=i(Ls);gp=r(Ju,"TFDPRQuestionEncoder"),Ju.forEach(t),Ti.forEach(t),ja=c(o),_e=a(o,"DIV",{class:!0});var gt=i(_e);k(Kn.$$.fragment,gt),_p=c(gt),Ms=a(gt,"P",{});var Zu=i(Ms);vp=r(Zu,"The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),Zu.forEach(t),bp=c(gt),Wn=a(gt,"P",{});var wi=i(Wn);Tp=r(wi,"This model inherits from "),Lr=a(wi,"A",{href:!0});var ef=i(Lr);wp=r(ef,"TFPreTrainedModel"),ef.forEach(t),kp=r(wi,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),wi.forEach(t),Pp=c(gt),Un=a(gt,"P",{});var ki=i(Un);$p=r(ki,"This model is also a Tensorflow "),Vn=a(ki,"A",{href:!0,rel:!0});var tf=i(Vn);Ep=r(tf,"tf.keras.Model"),tf.forEach(t),Rp=r(ki,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),ki.forEach(t),yp=c(gt),k($o.$$.fragment,gt),Dp=c(gt),pt=a(gt,"DIV",{class:!0});var Lo=i(pt);k(Yn.$$.fragment,Lo),xp=c(Lo),Wt=a(Lo,"P",{});var Vr=i(Wt);zp=r(Vr,"The "),Mr=a(Vr,"A",{href:!0});var of=i(Mr);qp=r(of,"TFDPRQuestionEncoder"),of.forEach(t),Cp=r(Vr," forward method, overrides the "),Ss=a(Vr,"CODE",{});var nf=i(Ss);Fp=r(nf,"__call__"),nf.forEach(t),Ap=r(Vr," special method."),Vr.forEach(t),jp=c(Lo),k(Eo.$$.fragment,Lo),Op=c(Lo),k(Ro.$$.fragment,Lo),Lo.forEach(t),gt.forEach(t),Oa=c(o),Ut=a(o,"H2",{class:!0});var Pi=i(Ut);yo=a(Pi,"A",{id:!0,class:!0,href:!0});var rf=i(yo);Bs=a(rf,"SPAN",{});var sf=i(Bs);k(Xn.$$.fragment,sf),sf.forEach(t),rf.forEach(t),Np=c(Pi),Hs=a(Pi,"SPAN",{});var af=i(Hs);Ip=r(af,"TFDPRReader"),af.forEach(t),Pi.forEach(t),Na=c(o),ve=a(o,"DIV",{class:!0});var _t=i(ve);k(Gn.$$.fragment,_t),Qp=c(_t),Ks=a(_t,"P",{});var df=i(Ks);Lp=r(df,"The bare DPRReader transformer outputting span predictions."),df.forEach(t),Mp=c(_t),Jn=a(_t,"P",{});var $i=i(Jn);Sp=r($i,"This model inherits from "),Sr=a($i,"A",{href:!0});var lf=i(Sr);Bp=r(lf,"TFPreTrainedModel"),lf.forEach(t),Hp=r($i,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),$i.forEach(t),Kp=c(_t),Zn=a(_t,"P",{});var Ei=i(Zn);Wp=r(Ei,"This model is also a Tensorflow "),er=a(Ei,"A",{href:!0,rel:!0});var cf=i(er);Up=r(cf,"tf.keras.Model"),cf.forEach(t),Vp=r(Ei,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),Ei.forEach(t),Yp=c(_t),k(Do.$$.fragment,_t),Xp=c(_t),ht=a(_t,"DIV",{class:!0});var Mo=i(ht);k(tr.$$.fragment,Mo),Gp=c(Mo),Vt=a(Mo,"P",{});var Yr=i(Vt);Jp=r(Yr,"The "),Br=a(Yr,"A",{href:!0});var pf=i(Br);Zp=r(pf,"TFDPRReader"),pf.forEach(t),eh=r(Yr," forward method, overrides the "),Ws=a(Yr,"CODE",{});var hf=i(Ws);th=r(hf,"__call__"),hf.forEach(t),oh=r(Yr," special method."),Yr.forEach(t),nh=c(Mo),k(xo.$$.fragment,Mo),rh=c(Mo),k(zo.$$.fragment,Mo),Mo.forEach(t),_t.forEach(t),this.h()},h(){p(h,"name","hf:doc:metadata"),p(h,"content",JSON.stringify(Of)),p(g,"id","dpr"),p(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(g,"href","#dpr"),p(_,"class","relative group"),p(te,"id","overview"),p(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(te,"href","#overview"),p(Q,"class","relative group"),p(se,"href","https://arxiv.org/abs/2004.04906"),p(se,"rel","nofollow"),p(ie,"href","https://huggingface.co/lhoestq"),p(ie,"rel","nofollow"),p(de,"href","https://github.com/facebookresearch/DPR"),p(de,"rel","nofollow"),p(ne,"id","transformers.DPRConfig"),p(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ne,"href","#transformers.DPRConfig"),p(F,"class","relative group"),p(J,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig"),p(Z,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRContextEncoder"),p(x,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoder"),p(Le,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRReader"),p(I,"href","https://huggingface.co/facebook/dpr-ctx_encoder-single-nq-base"),p(I,"rel","nofollow"),p(Me,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertConfig"),p(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Gt,"id","transformers.DPRContextEncoderTokenizer"),p(Gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Gt,"href","#transformers.DPRContextEncoderTokenizer"),p(yt,"class","relative group"),p(lr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRContextEncoderTokenizer"),p(cr,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),p(pr,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),p(We,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Zt,"id","transformers.DPRContextEncoderTokenizerFast"),p(Zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Zt,"href","#transformers.DPRContextEncoderTokenizerFast"),p(Dt,"class","relative group"),p(hr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRContextEncoderTokenizerFast"),p(ur,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),p(fr,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),p(Ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(to,"id","transformers.DPRQuestionEncoderTokenizer"),p(to,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(to,"href","#transformers.DPRQuestionEncoderTokenizer"),p(xt,"class","relative group"),p(mr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),p(gr,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),p(_r,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),p(Ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(no,"id","transformers.DPRQuestionEncoderTokenizerFast"),p(no,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(no,"href","#transformers.DPRQuestionEncoderTokenizerFast"),p(zt,"class","relative group"),p(vr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),p(br,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),p(Tr,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),p(Ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(so,"id","transformers.DPRReaderTokenizer"),p(so,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(so,"href","#transformers.DPRReaderTokenizer"),p(qt,"class","relative group"),p(wr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRReaderTokenizer"),p(kr,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),p(Pr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRReader"),p($r,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),p(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(io,"id","transformers.DPRReaderTokenizerFast"),p(io,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(io,"href","#transformers.DPRReaderTokenizerFast"),p(Ct,"class","relative group"),p(Er,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRReaderTokenizerFast"),p(Rr,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),p(yr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRReader"),p(Dr,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),p(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(lo,"id","transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"),p(lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(lo,"href","#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"),p(Ft,"class","relative group"),p(xr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoder"),p(At,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(zr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoder"),p(jt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(qr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoder"),p(Ot,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(co,"id","transformers.DPRContextEncoder"),p(co,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(co,"href","#transformers.DPRContextEncoder"),p(Nt,"class","relative group"),p(Cr,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),p($n,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),p($n,"rel","nofollow"),p(Fr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRContextEncoder"),p(it,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(uo,"id","transformers.DPRQuestionEncoder"),p(uo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(uo,"href","#transformers.DPRQuestionEncoder"),p(Qt,"class","relative group"),p(Ar,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),p(zn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),p(zn,"rel","nofollow"),p(jr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoder"),p(dt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(He,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(go,"id","transformers.DPRReader"),p(go,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(go,"href","#transformers.DPRReader"),p(Mt,"class","relative group"),p(Or,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),p(On,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),p(On,"rel","nofollow"),p(Nr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.DPRReader"),p(lt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(bo,"id","transformers.TFDPRContextEncoder"),p(bo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(bo,"href","#transformers.TFDPRContextEncoder"),p(Bt,"class","relative group"),p(Ir,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),p(Sn,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),p(Sn,"rel","nofollow"),p(Qr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.TFDPRContextEncoder"),p(ct,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Po,"id","transformers.TFDPRQuestionEncoder"),p(Po,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Po,"href","#transformers.TFDPRQuestionEncoder"),p(Kt,"class","relative group"),p(Lr,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),p(Vn,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),p(Vn,"rel","nofollow"),p(Mr,"href","/docs/transformers/main/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),p(pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(_e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(yo,"id","transformers.TFDPRReader"),p(yo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(yo,"href","#transformers.TFDPRReader"),p(Ut,"class","relative group"),p(Sr,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),p(er,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),p(er,"rel","nofollow"),p(Br,"href","/docs/transformers/main/en/model_doc/dpr#transformers.TFDPRReader"),p(ht,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,m){e(document.head,h),f(o,T,m),f(o,_,m),e(_,g),e(g,b),P(d,b,null),e(_,u),e(_,q),e(q,Re),f(o,be,m),f(o,Q,m),e(Q,te),e(te,oe),P(y,oe,null),e(Q,ye),e(Q,H),e(H,De),f(o,Te,m),f(o,L,m),e(L,xe),e(L,se),e(se,V),e(L,ze),f(o,we,m),f(o,S,m),e(S,qe),f(o,ke,m),f(o,B,m),e(B,he),e(he,Ce),f(o,ae,m),f(o,z,m),e(z,j),e(z,ie),e(ie,Y),e(z,Fe),e(z,de),e(de,X),e(z,Ae),f(o,Pe,m),f(o,F,m),e(F,ne),e(ne,K),P(le,K,null),e(F,je),e(F,W),e(W,Oe),f(o,$e,m),f(o,A,m),P(ce,A,null),e(A,G),e(A,re),e(re,J),e(J,O),e(re,Ne),e(re,N),e(N,Ie),e(re,Qe),e(A,v),e(A,D),e(D,Xe),e(D,Z),e(Z,Ge),e(D,Je),e(D,x),e(x,Ze),e(D,et),e(D,Le),e(Le,tt),e(D,ot),e(D,I),e(I,ee),e(D,nt),e(A,rt),e(A,M),e(M,st),e(M,Me),e(Me,Se),e(M,at),e(A,pe),P(ue,A,null),f(o,ca,m),f(o,yt,m),e(yt,Gt),e(Gt,Xr),P(Ko,Xr,null),e(yt,Ri),e(yt,Gr),e(Gr,yi),f(o,pa,m),f(o,We,m),P(Wo,We,null),e(We,Di),e(We,Jr),e(Jr,xi),e(We,zi),e(We,Jt),e(Jt,lr),e(lr,qi),e(Jt,Ci),e(Jt,cr),e(cr,Fi),e(Jt,Ai),e(We,ji),e(We,Uo),e(Uo,Oi),e(Uo,pr),e(pr,Ni),e(Uo,Ii),f(o,ha,m),f(o,Dt,m),e(Dt,Zt),e(Zt,Zr),P(Vo,Zr,null),e(Dt,Qi),e(Dt,es),e(es,Li),f(o,ua,m),f(o,Ue,m),P(Yo,Ue,null),e(Ue,Mi),e(Ue,Xo),e(Xo,Si),e(Xo,ts),e(ts,Bi),e(Xo,Hi),e(Ue,Ki),e(Ue,eo),e(eo,hr),e(hr,Wi),e(eo,Ui),e(eo,ur),e(ur,Vi),e(eo,Yi),e(Ue,Xi),e(Ue,Go),e(Go,Gi),e(Go,fr),e(fr,Ji),e(Go,Zi),f(o,fa,m),f(o,xt,m),e(xt,to),e(to,os),P(Jo,os,null),e(xt,ed),e(xt,ns),e(ns,td),f(o,ma,m),f(o,Ve,m),P(Zo,Ve,null),e(Ve,od),e(Ve,rs),e(rs,nd),e(Ve,rd),e(Ve,oo),e(oo,mr),e(mr,sd),e(oo,ad),e(oo,gr),e(gr,id),e(oo,dd),e(Ve,ld),e(Ve,en),e(en,cd),e(en,_r),e(_r,pd),e(en,hd),f(o,ga,m),f(o,zt,m),e(zt,no),e(no,ss),P(tn,ss,null),e(zt,ud),e(zt,as),e(as,fd),f(o,_a,m),f(o,Ye,m),P(on,Ye,null),e(Ye,md),e(Ye,nn),e(nn,gd),e(nn,is),e(is,_d),e(nn,vd),e(Ye,bd),e(Ye,ro),e(ro,vr),e(vr,Td),e(ro,wd),e(ro,br),e(br,kd),e(ro,Pd),e(Ye,$d),e(Ye,rn),e(rn,Ed),e(rn,Tr),e(Tr,Rd),e(rn,yd),f(o,va,m),f(o,qt,m),e(qt,so),e(so,ds),P(sn,ds,null),e(qt,Dd),e(qt,ls),e(ls,xd),f(o,ba,m),f(o,fe,m),P(an,fe,null),e(fe,zd),e(fe,cs),e(cs,qd),e(fe,Cd),e(fe,bt),e(bt,wr),e(wr,Fd),e(bt,Ad),e(bt,kr),e(kr,jd),e(bt,Od),e(bt,Pr),e(Pr,Nd),e(bt,Id),e(fe,Qd),e(fe,dn),e(dn,Ld),e(dn,$r),e($r,Md),e(dn,Sd),e(fe,Bd),e(fe,Tt),e(Tt,Hd),e(Tt,ps),e(ps,Kd),e(Tt,Wd),e(Tt,hs),e(hs,Ud),e(Tt,Vd),e(Tt,us),e(us,Yd),e(fe,Xd),P(ao,fe,null),f(o,Ta,m),f(o,Ct,m),e(Ct,io),e(io,fs),P(ln,fs,null),e(Ct,Gd),e(Ct,ms),e(ms,Jd),f(o,wa,m),f(o,me,m),P(cn,me,null),e(me,Zd),e(me,pn),e(pn,el),e(pn,gs),e(gs,tl),e(pn,ol),e(me,nl),e(me,wt),e(wt,Er),e(Er,rl),e(wt,sl),e(wt,Rr),e(Rr,al),e(wt,il),e(wt,yr),e(yr,dl),e(wt,ll),e(me,cl),e(me,hn),e(hn,pl),e(hn,Dr),e(Dr,hl),e(hn,ul),e(me,fl),e(me,vt),e(vt,ml),e(vt,_s),e(_s,gl),e(vt,_l),e(vt,vs),e(vs,vl),e(vt,bl),e(vt,bs),e(bs,Tl),e(vt,wl),e(me,kl),e(me,Ts),e(Ts,Pl),f(o,ka,m),f(o,Ft,m),e(Ft,lo),e(lo,ws),P(un,ws,null),e(Ft,$l),e(Ft,ks),e(ks,El),f(o,Pa,m),f(o,At,m),P(fn,At,null),e(At,Rl),e(At,mn),e(mn,yl),e(mn,xr),e(xr,Dl),e(mn,xl),f(o,$a,m),f(o,jt,m),P(gn,jt,null),e(jt,zl),e(jt,_n),e(_n,ql),e(_n,zr),e(zr,Cl),e(_n,Fl),f(o,Ea,m),f(o,Ot,m),P(vn,Ot,null),e(Ot,Al),e(Ot,bn),e(bn,jl),e(bn,qr),e(qr,Ol),e(bn,Nl),f(o,Ra,m),f(o,Nt,m),e(Nt,co),e(co,Ps),P(Tn,Ps,null),e(Nt,Il),e(Nt,$s),e($s,Ql),f(o,ya,m),f(o,Be,m),P(wn,Be,null),e(Be,Ll),e(Be,Es),e(Es,Ml),e(Be,Sl),e(Be,kn),e(kn,Bl),e(kn,Cr),e(Cr,Hl),e(kn,Kl),e(Be,Wl),e(Be,Pn),e(Pn,Ul),e(Pn,$n),e($n,Vl),e(Pn,Yl),e(Be,Xl),e(Be,it),P(En,it,null),e(it,Gl),e(it,It),e(It,Jl),e(It,Fr),e(Fr,Zl),e(It,ec),e(It,Rs),e(Rs,tc),e(It,oc),e(it,nc),P(po,it,null),e(it,rc),P(ho,it,null),f(o,Da,m),f(o,Qt,m),e(Qt,uo),e(uo,ys),P(Rn,ys,null),e(Qt,sc),e(Qt,Ds),e(Ds,ac),f(o,xa,m),f(o,He,m),P(yn,He,null),e(He,ic),e(He,xs),e(xs,dc),e(He,lc),e(He,Dn),e(Dn,cc),e(Dn,Ar),e(Ar,pc),e(Dn,hc),e(He,uc),e(He,xn),e(xn,fc),e(xn,zn),e(zn,mc),e(xn,gc),e(He,_c),e(He,dt),P(qn,dt,null),e(dt,vc),e(dt,Lt),e(Lt,bc),e(Lt,jr),e(jr,Tc),e(Lt,wc),e(Lt,zs),e(zs,kc),e(Lt,Pc),e(dt,$c),P(fo,dt,null),e(dt,Ec),P(mo,dt,null),f(o,za,m),f(o,Mt,m),e(Mt,go),e(go,qs),P(Cn,qs,null),e(Mt,Rc),e(Mt,Cs),e(Cs,yc),f(o,qa,m),f(o,Ke,m),P(Fn,Ke,null),e(Ke,Dc),e(Ke,Fs),e(Fs,xc),e(Ke,zc),e(Ke,An),e(An,qc),e(An,Or),e(Or,Cc),e(An,Fc),e(Ke,Ac),e(Ke,jn),e(jn,jc),e(jn,On),e(On,Oc),e(jn,Nc),e(Ke,Ic),e(Ke,lt),P(Nn,lt,null),e(lt,Qc),e(lt,St),e(St,Lc),e(St,Nr),e(Nr,Mc),e(St,Sc),e(St,As),e(As,Bc),e(St,Hc),e(lt,Kc),P(_o,lt,null),e(lt,Wc),P(vo,lt,null),f(o,Ca,m),f(o,Bt,m),e(Bt,bo),e(bo,js),P(In,js,null),e(Bt,Uc),e(Bt,Os),e(Os,Vc),f(o,Fa,m),f(o,ge,m),P(Qn,ge,null),e(ge,Yc),e(ge,Ns),e(Ns,Xc),e(ge,Gc),e(ge,Ln),e(Ln,Jc),e(Ln,Ir),e(Ir,Zc),e(Ln,ep),e(ge,tp),e(ge,Mn),e(Mn,op),e(Mn,Sn),e(Sn,np),e(Mn,rp),e(ge,sp),P(To,ge,null),e(ge,ap),e(ge,ct),P(Bn,ct,null),e(ct,ip),e(ct,Ht),e(Ht,dp),e(Ht,Qr),e(Qr,lp),e(Ht,cp),e(Ht,Is),e(Is,pp),e(Ht,hp),e(ct,up),P(wo,ct,null),e(ct,fp),P(ko,ct,null),f(o,Aa,m),f(o,Kt,m),e(Kt,Po),e(Po,Qs),P(Hn,Qs,null),e(Kt,mp),e(Kt,Ls),e(Ls,gp),f(o,ja,m),f(o,_e,m),P(Kn,_e,null),e(_e,_p),e(_e,Ms),e(Ms,vp),e(_e,bp),e(_e,Wn),e(Wn,Tp),e(Wn,Lr),e(Lr,wp),e(Wn,kp),e(_e,Pp),e(_e,Un),e(Un,$p),e(Un,Vn),e(Vn,Ep),e(Un,Rp),e(_e,yp),P($o,_e,null),e(_e,Dp),e(_e,pt),P(Yn,pt,null),e(pt,xp),e(pt,Wt),e(Wt,zp),e(Wt,Mr),e(Mr,qp),e(Wt,Cp),e(Wt,Ss),e(Ss,Fp),e(Wt,Ap),e(pt,jp),P(Eo,pt,null),e(pt,Op),P(Ro,pt,null),f(o,Oa,m),f(o,Ut,m),e(Ut,yo),e(yo,Bs),P(Xn,Bs,null),e(Ut,Np),e(Ut,Hs),e(Hs,Ip),f(o,Na,m),f(o,ve,m),P(Gn,ve,null),e(ve,Qp),e(ve,Ks),e(Ks,Lp),e(ve,Mp),e(ve,Jn),e(Jn,Sp),e(Jn,Sr),e(Sr,Bp),e(Jn,Hp),e(ve,Kp),e(ve,Zn),e(Zn,Wp),e(Zn,er),e(er,Up),e(Zn,Vp),e(ve,Yp),P(Do,ve,null),e(ve,Xp),e(ve,ht),P(tr,ht,null),e(ht,Gp),e(ht,Vt),e(Vt,Jp),e(Vt,Br),e(Br,Zp),e(Vt,eh),e(Vt,Ws),e(Ws,th),e(Vt,oh),e(ht,nh),P(xo,ht,null),e(ht,rh),P(zo,ht,null),Ia=!0},p(o,[m]){const or={};m&2&&(or.$$scope={dirty:m,ctx:o}),ue.$set(or);const Us={};m&2&&(Us.$$scope={dirty:m,ctx:o}),ao.$set(Us);const Vs={};m&2&&(Vs.$$scope={dirty:m,ctx:o}),po.$set(Vs);const Ys={};m&2&&(Ys.$$scope={dirty:m,ctx:o}),ho.$set(Ys);const nr={};m&2&&(nr.$$scope={dirty:m,ctx:o}),fo.$set(nr);const Xs={};m&2&&(Xs.$$scope={dirty:m,ctx:o}),mo.$set(Xs);const Gs={};m&2&&(Gs.$$scope={dirty:m,ctx:o}),_o.$set(Gs);const Js={};m&2&&(Js.$$scope={dirty:m,ctx:o}),vo.$set(Js);const rr={};m&2&&(rr.$$scope={dirty:m,ctx:o}),To.$set(rr);const Zs={};m&2&&(Zs.$$scope={dirty:m,ctx:o}),wo.$set(Zs);const ea={};m&2&&(ea.$$scope={dirty:m,ctx:o}),ko.$set(ea);const ta={};m&2&&(ta.$$scope={dirty:m,ctx:o}),$o.$set(ta);const oa={};m&2&&(oa.$$scope={dirty:m,ctx:o}),Eo.$set(oa);const Yt={};m&2&&(Yt.$$scope={dirty:m,ctx:o}),Ro.$set(Yt);const na={};m&2&&(na.$$scope={dirty:m,ctx:o}),Do.$set(na);const ra={};m&2&&(ra.$$scope={dirty:m,ctx:o}),xo.$set(ra);const sr={};m&2&&(sr.$$scope={dirty:m,ctx:o}),zo.$set(sr)},i(o){Ia||($(d.$$.fragment,o),$(y.$$.fragment,o),$(le.$$.fragment,o),$(ce.$$.fragment,o),$(ue.$$.fragment,o),$(Ko.$$.fragment,o),$(Wo.$$.fragment,o),$(Vo.$$.fragment,o),$(Yo.$$.fragment,o),$(Jo.$$.fragment,o),$(Zo.$$.fragment,o),$(tn.$$.fragment,o),$(on.$$.fragment,o),$(sn.$$.fragment,o),$(an.$$.fragment,o),$(ao.$$.fragment,o),$(ln.$$.fragment,o),$(cn.$$.fragment,o),$(un.$$.fragment,o),$(fn.$$.fragment,o),$(gn.$$.fragment,o),$(vn.$$.fragment,o),$(Tn.$$.fragment,o),$(wn.$$.fragment,o),$(En.$$.fragment,o),$(po.$$.fragment,o),$(ho.$$.fragment,o),$(Rn.$$.fragment,o),$(yn.$$.fragment,o),$(qn.$$.fragment,o),$(fo.$$.fragment,o),$(mo.$$.fragment,o),$(Cn.$$.fragment,o),$(Fn.$$.fragment,o),$(Nn.$$.fragment,o),$(_o.$$.fragment,o),$(vo.$$.fragment,o),$(In.$$.fragment,o),$(Qn.$$.fragment,o),$(To.$$.fragment,o),$(Bn.$$.fragment,o),$(wo.$$.fragment,o),$(ko.$$.fragment,o),$(Hn.$$.fragment,o),$(Kn.$$.fragment,o),$($o.$$.fragment,o),$(Yn.$$.fragment,o),$(Eo.$$.fragment,o),$(Ro.$$.fragment,o),$(Xn.$$.fragment,o),$(Gn.$$.fragment,o),$(Do.$$.fragment,o),$(tr.$$.fragment,o),$(xo.$$.fragment,o),$(zo.$$.fragment,o),Ia=!0)},o(o){E(d.$$.fragment,o),E(y.$$.fragment,o),E(le.$$.fragment,o),E(ce.$$.fragment,o),E(ue.$$.fragment,o),E(Ko.$$.fragment,o),E(Wo.$$.fragment,o),E(Vo.$$.fragment,o),E(Yo.$$.fragment,o),E(Jo.$$.fragment,o),E(Zo.$$.fragment,o),E(tn.$$.fragment,o),E(on.$$.fragment,o),E(sn.$$.fragment,o),E(an.$$.fragment,o),E(ao.$$.fragment,o),E(ln.$$.fragment,o),E(cn.$$.fragment,o),E(un.$$.fragment,o),E(fn.$$.fragment,o),E(gn.$$.fragment,o),E(vn.$$.fragment,o),E(Tn.$$.fragment,o),E(wn.$$.fragment,o),E(En.$$.fragment,o),E(po.$$.fragment,o),E(ho.$$.fragment,o),E(Rn.$$.fragment,o),E(yn.$$.fragment,o),E(qn.$$.fragment,o),E(fo.$$.fragment,o),E(mo.$$.fragment,o),E(Cn.$$.fragment,o),E(Fn.$$.fragment,o),E(Nn.$$.fragment,o),E(_o.$$.fragment,o),E(vo.$$.fragment,o),E(In.$$.fragment,o),E(Qn.$$.fragment,o),E(To.$$.fragment,o),E(Bn.$$.fragment,o),E(wo.$$.fragment,o),E(ko.$$.fragment,o),E(Hn.$$.fragment,o),E(Kn.$$.fragment,o),E($o.$$.fragment,o),E(Yn.$$.fragment,o),E(Eo.$$.fragment,o),E(Ro.$$.fragment,o),E(Xn.$$.fragment,o),E(Gn.$$.fragment,o),E(Do.$$.fragment,o),E(tr.$$.fragment,o),E(xo.$$.fragment,o),E(zo.$$.fragment,o),Ia=!1},d(o){t(h),o&&t(T),o&&t(_),R(d),o&&t(be),o&&t(Q),R(y),o&&t(Te),o&&t(L),o&&t(we),o&&t(S),o&&t(ke),o&&t(B),o&&t(ae),o&&t(z),o&&t(Pe),o&&t(F),R(le),o&&t($e),o&&t(A),R(ce),R(ue),o&&t(ca),o&&t(yt),R(Ko),o&&t(pa),o&&t(We),R(Wo),o&&t(ha),o&&t(Dt),R(Vo),o&&t(ua),o&&t(Ue),R(Yo),o&&t(fa),o&&t(xt),R(Jo),o&&t(ma),o&&t(Ve),R(Zo),o&&t(ga),o&&t(zt),R(tn),o&&t(_a),o&&t(Ye),R(on),o&&t(va),o&&t(qt),R(sn),o&&t(ba),o&&t(fe),R(an),R(ao),o&&t(Ta),o&&t(Ct),R(ln),o&&t(wa),o&&t(me),R(cn),o&&t(ka),o&&t(Ft),R(un),o&&t(Pa),o&&t(At),R(fn),o&&t($a),o&&t(jt),R(gn),o&&t(Ea),o&&t(Ot),R(vn),o&&t(Ra),o&&t(Nt),R(Tn),o&&t(ya),o&&t(Be),R(wn),R(En),R(po),R(ho),o&&t(Da),o&&t(Qt),R(Rn),o&&t(xa),o&&t(He),R(yn),R(qn),R(fo),R(mo),o&&t(za),o&&t(Mt),R(Cn),o&&t(qa),o&&t(Ke),R(Fn),R(Nn),R(_o),R(vo),o&&t(Ca),o&&t(Bt),R(In),o&&t(Fa),o&&t(ge),R(Qn),R(To),R(Bn),R(wo),R(ko),o&&t(Aa),o&&t(Kt),R(Hn),o&&t(ja),o&&t(_e),R(Kn),R($o),R(Yn),R(Eo),R(Ro),o&&t(Oa),o&&t(Ut),R(Xn),o&&t(Na),o&&t(ve),R(Gn),R(Do),R(tr),R(xo),R(zo)}}}const Of={local:"dpr",sections:[{local:"overview",title:"Overview"},{local:"transformers.DPRConfig",title:"DPRConfig"},{local:"transformers.DPRContextEncoderTokenizer",title:"DPRContextEncoderTokenizer"},{local:"transformers.DPRContextEncoderTokenizerFast",title:"DPRContextEncoderTokenizerFast"},{local:"transformers.DPRQuestionEncoderTokenizer",title:"DPRQuestionEncoderTokenizer"},{local:"transformers.DPRQuestionEncoderTokenizerFast",title:"DPRQuestionEncoderTokenizerFast"},{local:"transformers.DPRReaderTokenizer",title:"DPRReaderTokenizer"},{local:"transformers.DPRReaderTokenizerFast",title:"DPRReaderTokenizerFast"},{local:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",title:"DPR specific outputs"},{local:"transformers.DPRContextEncoder",title:"DPRContextEncoder"},{local:"transformers.DPRQuestionEncoder",title:"DPRQuestionEncoder"},{local:"transformers.DPRReader",title:"DPRReader"},{local:"transformers.TFDPRContextEncoder",title:"TFDPRContextEncoder"},{local:"transformers.TFDPRQuestionEncoder",title:"TFDPRQuestionEncoder"},{local:"transformers.TFDPRReader",title:"TFDPRReader"}],title:"DPR"};function Nf(C){return _f(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Hf extends uf{constructor(h){super();ff(this,h,Nf,jf,mf,{})}}export{Hf as default,Of as metadata};
