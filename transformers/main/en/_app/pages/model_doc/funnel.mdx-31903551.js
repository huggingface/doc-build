import{S as x$,i as L$,s as O$,e as r,k as l,w as F,t,M as D$,c as a,d as n,m as d,a as i,x as v,h as o,b as c,F as e,g as h,y,q as w,o as b,B as $,v as A$}from"../../chunks/vendor-6b77c823.js";import{T as ze}from"../../chunks/Tip-39098574.js";import{D as X}from"../../chunks/Docstring-abef54e3.js";import{C as qe}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as Ce}from"../../chunks/IconCopyLink-7a11ce68.js";function N$(V){let p,z,g,_,k;return{c(){p=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var m=i(p);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,p,m),e(p,z),e(p,g),e(g,_),e(p,k)},d(T){T&&n(p)}}}function I$(V){let p,z,g,_,k;return{c(){p=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var m=i(p);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,p,m),e(p,z),e(p,g),e(g,_),e(p,k)},d(T){T&&n(p)}}}function S$(V){let p,z,g,_,k;return{c(){p=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var m=i(p);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,p,m),e(p,z),e(p,g),e(g,_),e(p,k)},d(T){T&&n(p)}}}function B$(V){let p,z,g,_,k;return{c(){p=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var m=i(p);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,p,m),e(p,z),e(p,g),e(g,_),e(p,k)},d(T){T&&n(p)}}}function W$(V){let p,z,g,_,k;return{c(){p=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var m=i(p);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,p,m),e(p,z),e(p,g),e(g,_),e(p,k)},d(T){T&&n(p)}}}function Q$(V){let p,z,g,_,k;return{c(){p=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var m=i(p);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,p,m),e(p,z),e(p,g),e(g,_),e(p,k)},d(T){T&&n(p)}}}function U$(V){let p,z,g,_,k;return{c(){p=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var m=i(p);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,p,m),e(p,z),e(p,g),e(g,_),e(p,k)},d(T){T&&n(p)}}}function R$(V){let p,z,g,_,k;return{c(){p=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var m=i(p);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,p,m),e(p,z),e(p,g),e(g,_),e(p,k)},d(T){T&&n(p)}}}function H$(V){let p,z,g,_,k,T,m,M,ce,K,q,J,A,ne,ue,N,pe,ie,Y,L,te,G,P,j,oe,W,le,se,I,he,de,C,fe,B,ee,ae,Q,me,S,O,re,U,ge;return{c(){p=r("p"),z=t("TF 2.0 models accepts two formats as inputs:"),g=l(),_=r("ul"),k=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),M=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),W=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var E=i(p);z=o(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(n),g=d(u),_=a(u,"UL",{});var Z=i(_);k=a(Z,"LI",{});var Te=i(k);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),m=d(Z),M=a(Z,"LI",{});var ye=i(M);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var ke=i(A);ne=o(ke,"tf.keras.Model.fit"),ke.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);pe=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var R=i(j);oe=o(R,"a single Tensor with "),W=a(R,"CODE",{});var $e=i(W);le=o($e,"input_ids"),$e.forEach(n),se=o(R," only and nothing else: "),I=a(R,"CODE",{});var Fe=i(I);he=o(Fe,"model(inputs_ids)"),Fe.forEach(n),R.forEach(n),de=d(x),C=a(x,"LI",{});var H=i(C);fe=o(H,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(H,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(H," or "),Q=a(H,"CODE",{});var ve=i(Q);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),H.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=a(_e,"CODE",{});var Me=i(U);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,E){h(u,p,E),e(p,z),h(u,g,E),h(u,_,E),e(_,k),e(k,T),e(_,m),e(_,M),e(M,ce),h(u,K,E),h(u,q,E),e(q,J),e(q,A),e(A,ne),e(q,ue),e(q,N),e(N,pe),e(q,ie),h(u,Y,E),h(u,L,E),e(L,te),h(u,G,E),h(u,P,E),e(P,j),e(j,oe),e(j,W),e(W,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,S),e(P,O),e(O,re),e(O,U),e(U,ge)},d(u){u&&n(p),u&&n(g),u&&n(_),u&&n(K),u&&n(q),u&&n(Y),u&&n(L),u&&n(G),u&&n(P)}}}function V$(V){let p,z,g,_,k;return{c(){p=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var m=i(p);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,p,m),e(p,z),e(p,g),e(g,_),e(p,k)},d(T){T&&n(p)}}}function Y$(V){let p,z,g,_,k,T,m,M,ce,K,q,J,A,ne,ue,N,pe,ie,Y,L,te,G,P,j,oe,W,le,se,I,he,de,C,fe,B,ee,ae,Q,me,S,O,re,U,ge;return{c(){p=r("p"),z=t("TF 2.0 models accepts two formats as inputs:"),g=l(),_=r("ul"),k=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),M=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),W=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var E=i(p);z=o(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(n),g=d(u),_=a(u,"UL",{});var Z=i(_);k=a(Z,"LI",{});var Te=i(k);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),m=d(Z),M=a(Z,"LI",{});var ye=i(M);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var ke=i(A);ne=o(ke,"tf.keras.Model.fit"),ke.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);pe=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var R=i(j);oe=o(R,"a single Tensor with "),W=a(R,"CODE",{});var $e=i(W);le=o($e,"input_ids"),$e.forEach(n),se=o(R," only and nothing else: "),I=a(R,"CODE",{});var Fe=i(I);he=o(Fe,"model(inputs_ids)"),Fe.forEach(n),R.forEach(n),de=d(x),C=a(x,"LI",{});var H=i(C);fe=o(H,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(H,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(H," or "),Q=a(H,"CODE",{});var ve=i(Q);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),H.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=a(_e,"CODE",{});var Me=i(U);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,E){h(u,p,E),e(p,z),h(u,g,E),h(u,_,E),e(_,k),e(k,T),e(_,m),e(_,M),e(M,ce),h(u,K,E),h(u,q,E),e(q,J),e(q,A),e(A,ne),e(q,ue),e(q,N),e(N,pe),e(q,ie),h(u,Y,E),h(u,L,E),e(L,te),h(u,G,E),h(u,P,E),e(P,j),e(j,oe),e(j,W),e(W,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,S),e(P,O),e(O,re),e(O,U),e(U,ge)},d(u){u&&n(p),u&&n(g),u&&n(_),u&&n(K),u&&n(q),u&&n(Y),u&&n(L),u&&n(G),u&&n(P)}}}function K$(V){let p,z,g,_,k;return{c(){p=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var m=i(p);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,p,m),e(p,z),e(p,g),e(g,_),e(p,k)},d(T){T&&n(p)}}}function G$(V){let p,z,g,_,k,T,m,M,ce,K,q,J,A,ne,ue,N,pe,ie,Y,L,te,G,P,j,oe,W,le,se,I,he,de,C,fe,B,ee,ae,Q,me,S,O,re,U,ge;return{c(){p=r("p"),z=t("TF 2.0 models accepts two formats as inputs:"),g=l(),_=r("ul"),k=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),M=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),W=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var E=i(p);z=o(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(n),g=d(u),_=a(u,"UL",{});var Z=i(_);k=a(Z,"LI",{});var Te=i(k);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),m=d(Z),M=a(Z,"LI",{});var ye=i(M);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var ke=i(A);ne=o(ke,"tf.keras.Model.fit"),ke.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);pe=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var R=i(j);oe=o(R,"a single Tensor with "),W=a(R,"CODE",{});var $e=i(W);le=o($e,"input_ids"),$e.forEach(n),se=o(R," only and nothing else: "),I=a(R,"CODE",{});var Fe=i(I);he=o(Fe,"model(inputs_ids)"),Fe.forEach(n),R.forEach(n),de=d(x),C=a(x,"LI",{});var H=i(C);fe=o(H,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(H,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(H," or "),Q=a(H,"CODE",{});var ve=i(Q);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),H.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=a(_e,"CODE",{});var Me=i(U);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,E){h(u,p,E),e(p,z),h(u,g,E),h(u,_,E),e(_,k),e(k,T),e(_,m),e(_,M),e(M,ce),h(u,K,E),h(u,q,E),e(q,J),e(q,A),e(A,ne),e(q,ue),e(q,N),e(N,pe),e(q,ie),h(u,Y,E),h(u,L,E),e(L,te),h(u,G,E),h(u,P,E),e(P,j),e(j,oe),e(j,W),e(W,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,S),e(P,O),e(O,re),e(O,U),e(U,ge)},d(u){u&&n(p),u&&n(g),u&&n(_),u&&n(K),u&&n(q),u&&n(Y),u&&n(L),u&&n(G),u&&n(P)}}}function Z$(V){let p,z,g,_,k;return{c(){p=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var m=i(p);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,p,m),e(p,z),e(p,g),e(g,_),e(p,k)},d(T){T&&n(p)}}}function X$(V){let p,z,g,_,k,T,m,M,ce,K,q,J,A,ne,ue,N,pe,ie,Y,L,te,G,P,j,oe,W,le,se,I,he,de,C,fe,B,ee,ae,Q,me,S,O,re,U,ge;return{c(){p=r("p"),z=t("TF 2.0 models accepts two formats as inputs:"),g=l(),_=r("ul"),k=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),M=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),W=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var E=i(p);z=o(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(n),g=d(u),_=a(u,"UL",{});var Z=i(_);k=a(Z,"LI",{});var Te=i(k);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),m=d(Z),M=a(Z,"LI",{});var ye=i(M);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var ke=i(A);ne=o(ke,"tf.keras.Model.fit"),ke.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);pe=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var R=i(j);oe=o(R,"a single Tensor with "),W=a(R,"CODE",{});var $e=i(W);le=o($e,"input_ids"),$e.forEach(n),se=o(R," only and nothing else: "),I=a(R,"CODE",{});var Fe=i(I);he=o(Fe,"model(inputs_ids)"),Fe.forEach(n),R.forEach(n),de=d(x),C=a(x,"LI",{});var H=i(C);fe=o(H,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(H,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(H," or "),Q=a(H,"CODE",{});var ve=i(Q);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),H.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=a(_e,"CODE",{});var Me=i(U);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,E){h(u,p,E),e(p,z),h(u,g,E),h(u,_,E),e(_,k),e(k,T),e(_,m),e(_,M),e(M,ce),h(u,K,E),h(u,q,E),e(q,J),e(q,A),e(A,ne),e(q,ue),e(q,N),e(N,pe),e(q,ie),h(u,Y,E),h(u,L,E),e(L,te),h(u,G,E),h(u,P,E),e(P,j),e(j,oe),e(j,W),e(W,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,S),e(P,O),e(O,re),e(O,U),e(U,ge)},d(u){u&&n(p),u&&n(g),u&&n(_),u&&n(K),u&&n(q),u&&n(Y),u&&n(L),u&&n(G),u&&n(P)}}}function J$(V){let p,z,g,_,k;return{c(){p=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var m=i(p);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,p,m),e(p,z),e(p,g),e(g,_),e(p,k)},d(T){T&&n(p)}}}function e2(V){let p,z,g,_,k,T,m,M,ce,K,q,J,A,ne,ue,N,pe,ie,Y,L,te,G,P,j,oe,W,le,se,I,he,de,C,fe,B,ee,ae,Q,me,S,O,re,U,ge;return{c(){p=r("p"),z=t("TF 2.0 models accepts two formats as inputs:"),g=l(),_=r("ul"),k=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),M=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),W=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var E=i(p);z=o(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(n),g=d(u),_=a(u,"UL",{});var Z=i(_);k=a(Z,"LI",{});var Te=i(k);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),m=d(Z),M=a(Z,"LI",{});var ye=i(M);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var ke=i(A);ne=o(ke,"tf.keras.Model.fit"),ke.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);pe=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var R=i(j);oe=o(R,"a single Tensor with "),W=a(R,"CODE",{});var $e=i(W);le=o($e,"input_ids"),$e.forEach(n),se=o(R," only and nothing else: "),I=a(R,"CODE",{});var Fe=i(I);he=o(Fe,"model(inputs_ids)"),Fe.forEach(n),R.forEach(n),de=d(x),C=a(x,"LI",{});var H=i(C);fe=o(H,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(H,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(H," or "),Q=a(H,"CODE",{});var ve=i(Q);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),H.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=a(_e,"CODE",{});var Me=i(U);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,E){h(u,p,E),e(p,z),h(u,g,E),h(u,_,E),e(_,k),e(k,T),e(_,m),e(_,M),e(M,ce),h(u,K,E),h(u,q,E),e(q,J),e(q,A),e(A,ne),e(q,ue),e(q,N),e(N,pe),e(q,ie),h(u,Y,E),h(u,L,E),e(L,te),h(u,G,E),h(u,P,E),e(P,j),e(j,oe),e(j,W),e(W,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,S),e(P,O),e(O,re),e(O,U),e(U,ge)},d(u){u&&n(p),u&&n(g),u&&n(_),u&&n(K),u&&n(q),u&&n(Y),u&&n(L),u&&n(G),u&&n(P)}}}function n2(V){let p,z,g,_,k;return{c(){p=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var m=i(p);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,p,m),e(p,z),e(p,g),e(g,_),e(p,k)},d(T){T&&n(p)}}}function t2(V){let p,z,g,_,k,T,m,M,ce,K,q,J,A,ne,ue,N,pe,ie,Y,L,te,G,P,j,oe,W,le,se,I,he,de,C,fe,B,ee,ae,Q,me,S,O,re,U,ge;return{c(){p=r("p"),z=t("TF 2.0 models accepts two formats as inputs:"),g=l(),_=r("ul"),k=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),M=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),W=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var E=i(p);z=o(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(n),g=d(u),_=a(u,"UL",{});var Z=i(_);k=a(Z,"LI",{});var Te=i(k);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),m=d(Z),M=a(Z,"LI",{});var ye=i(M);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var ke=i(A);ne=o(ke,"tf.keras.Model.fit"),ke.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);pe=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var R=i(j);oe=o(R,"a single Tensor with "),W=a(R,"CODE",{});var $e=i(W);le=o($e,"input_ids"),$e.forEach(n),se=o(R," only and nothing else: "),I=a(R,"CODE",{});var Fe=i(I);he=o(Fe,"model(inputs_ids)"),Fe.forEach(n),R.forEach(n),de=d(x),C=a(x,"LI",{});var H=i(C);fe=o(H,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(H,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(H," or "),Q=a(H,"CODE",{});var ve=i(Q);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),H.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=a(_e,"CODE",{});var Me=i(U);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,E){h(u,p,E),e(p,z),h(u,g,E),h(u,_,E),e(_,k),e(k,T),e(_,m),e(_,M),e(M,ce),h(u,K,E),h(u,q,E),e(q,J),e(q,A),e(A,ne),e(q,ue),e(q,N),e(N,pe),e(q,ie),h(u,Y,E),h(u,L,E),e(L,te),h(u,G,E),h(u,P,E),e(P,j),e(j,oe),e(j,W),e(W,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,S),e(P,O),e(O,re),e(O,U),e(U,ge)},d(u){u&&n(p),u&&n(g),u&&n(_),u&&n(K),u&&n(q),u&&n(Y),u&&n(L),u&&n(G),u&&n(P)}}}function o2(V){let p,z,g,_,k;return{c(){p=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var m=i(p);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,p,m),e(p,z),e(p,g),e(g,_),e(p,k)},d(T){T&&n(p)}}}function s2(V){let p,z,g,_,k,T,m,M,ce,K,q,J,A,ne,ue,N,pe,ie,Y,L,te,G,P,j,oe,W,le,se,I,he,de,C,fe,B,ee,ae,Q,me,S,O,re,U,ge;return{c(){p=r("p"),z=t("TF 2.0 models accepts two formats as inputs:"),g=l(),_=r("ul"),k=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),M=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),W=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var E=i(p);z=o(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(n),g=d(u),_=a(u,"UL",{});var Z=i(_);k=a(Z,"LI",{});var Te=i(k);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),m=d(Z),M=a(Z,"LI",{});var ye=i(M);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var ke=i(A);ne=o(ke,"tf.keras.Model.fit"),ke.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);pe=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var R=i(j);oe=o(R,"a single Tensor with "),W=a(R,"CODE",{});var $e=i(W);le=o($e,"input_ids"),$e.forEach(n),se=o(R," only and nothing else: "),I=a(R,"CODE",{});var Fe=i(I);he=o(Fe,"model(inputs_ids)"),Fe.forEach(n),R.forEach(n),de=d(x),C=a(x,"LI",{});var H=i(C);fe=o(H,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(H,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(H," or "),Q=a(H,"CODE",{});var ve=i(Q);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),H.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=a(_e,"CODE",{});var Me=i(U);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,E){h(u,p,E),e(p,z),h(u,g,E),h(u,_,E),e(_,k),e(k,T),e(_,m),e(_,M),e(M,ce),h(u,K,E),h(u,q,E),e(q,J),e(q,A),e(A,ne),e(q,ue),e(q,N),e(N,pe),e(q,ie),h(u,Y,E),h(u,L,E),e(L,te),h(u,G,E),h(u,P,E),e(P,j),e(j,oe),e(j,W),e(W,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,S),e(P,O),e(O,re),e(O,U),e(U,ge)},d(u){u&&n(p),u&&n(g),u&&n(_),u&&n(K),u&&n(q),u&&n(Y),u&&n(L),u&&n(G),u&&n(P)}}}function r2(V){let p,z,g,_,k;return{c(){p=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var m=i(p);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,p,m),e(p,z),e(p,g),e(g,_),e(p,k)},d(T){T&&n(p)}}}function a2(V){let p,z,g,_,k,T,m,M,ce,K,q,J,A,ne,ue,N,pe,ie,Y,L,te,G,P,j,oe,W,le,se,I,he,de,C,fe,B,ee,ae,Q,me,S,O,re,U,ge;return{c(){p=r("p"),z=t("TF 2.0 models accepts two formats as inputs:"),g=l(),_=r("ul"),k=r("li"),T=t("having all inputs as keyword arguments (like PyTorch models), or"),m=l(),M=r("li"),ce=t("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),q=r("p"),J=t("This second option is useful when using "),A=r("code"),ne=t("tf.keras.Model.fit"),ue=t(` method which currently requires having all the
tensors in the first argument of the model call function: `),N=r("code"),pe=t("model(inputs)"),ie=t("."),Y=l(),L=r("p"),te=t(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),G=l(),P=r("ul"),j=r("li"),oe=t("a single Tensor with "),W=r("code"),le=t("input_ids"),se=t(" only and nothing else: "),I=r("code"),he=t("model(inputs_ids)"),de=l(),C=r("li"),fe=t(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),ee=t("model([input_ids, attention_mask])"),ae=t(" or "),Q=r("code"),me=t("model([input_ids, attention_mask, token_type_ids])"),S=l(),O=r("li"),re=t(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=r("code"),ge=t('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=a(u,"P",{});var E=i(p);z=o(E,"TF 2.0 models accepts two formats as inputs:"),E.forEach(n),g=d(u),_=a(u,"UL",{});var Z=i(_);k=a(Z,"LI",{});var Te=i(k);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(n),m=d(Z),M=a(Z,"LI",{});var ye=i(M);ce=o(ye,"having all inputs as a list, tuple or dict in the first positional arguments."),ye.forEach(n),Z.forEach(n),K=d(u),q=a(u,"P",{});var D=i(q);J=o(D,"This second option is useful when using "),A=a(D,"CODE",{});var ke=i(A);ne=o(ke,"tf.keras.Model.fit"),ke.forEach(n),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),N=a(D,"CODE",{});var we=i(N);pe=o(we,"model(inputs)"),we.forEach(n),ie=o(D,"."),D.forEach(n),Y=d(u),L=a(u,"P",{});var be=i(L);te=o(be,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),be.forEach(n),G=d(u),P=a(u,"UL",{});var x=i(P);j=a(x,"LI",{});var R=i(j);oe=o(R,"a single Tensor with "),W=a(R,"CODE",{});var $e=i(W);le=o($e,"input_ids"),$e.forEach(n),se=o(R," only and nothing else: "),I=a(R,"CODE",{});var Fe=i(I);he=o(Fe,"model(inputs_ids)"),Fe.forEach(n),R.forEach(n),de=d(x),C=a(x,"LI",{});var H=i(C);fe=o(H,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(H,"CODE",{});var Ee=i(B);ee=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(n),ae=o(H," or "),Q=a(H,"CODE",{});var ve=i(Q);me=o(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(n),H.forEach(n),S=d(x),O=a(x,"LI",{});var _e=i(O);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=a(_e,"CODE",{});var Me=i(U);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(n),_e.forEach(n),x.forEach(n)},m(u,E){h(u,p,E),e(p,z),h(u,g,E),h(u,_,E),e(_,k),e(k,T),e(_,m),e(_,M),e(M,ce),h(u,K,E),h(u,q,E),e(q,J),e(q,A),e(A,ne),e(q,ue),e(q,N),e(N,pe),e(q,ie),h(u,Y,E),h(u,L,E),e(L,te),h(u,G,E),h(u,P,E),e(P,j),e(j,oe),e(j,W),e(W,le),e(j,se),e(j,I),e(I,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,ee),e(C,ae),e(C,Q),e(Q,me),e(P,S),e(P,O),e(O,re),e(O,U),e(U,ge)},d(u){u&&n(p),u&&n(g),u&&n(_),u&&n(K),u&&n(q),u&&n(Y),u&&n(L),u&&n(G),u&&n(P)}}}function i2(V){let p,z,g,_,k;return{c(){p=r("p"),z=t("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=r("code"),_=t("Module"),k=t(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=a(T,"P",{});var m=i(p);z=o(m,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=a(m,"CODE",{});var M=i(g);_=o(M,"Module"),M.forEach(n),k=o(m,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),m.forEach(n)},m(T,m){h(T,p,m),e(p,z),e(p,g),e(g,_),e(p,k)},d(T){T&&n(p)}}}function l2(V){let p,z,g,_,k,T,m,M,ce,K,q,J,A,ne,ue,N,pe,ie,Y,L,te,G,P,j,oe,W,le,se,I,he,de,C,fe,B,ee,ae,Q,me,S,O,re,U,ge,u,E,Z,Te,ye,D,ke,we,be,x,R,$e,Fe,H,Ee,ve,_e,Me,Ja,Up,Rp,Cc,Dn,Hp,No,Vp,Yp,Io,Kp,Gp,jc,Zn,Bt,gl,So,Zp,_l,Xp,xc,Ln,Bo,Jp,On,eh,ei,nh,th,ni,oh,sh,Wo,rh,ah,ih,Xn,lh,ti,dh,ch,oi,uh,ph,Lc,Jn,Wt,Tl,Qo,hh,kl,fh,Oc,je,Uo,mh,Fl,gh,_h,Qt,si,Th,kh,ri,Fh,vh,yh,Ro,wh,ai,bh,$h,Eh,An,Ho,Mh,vl,zh,qh,Vo,ii,Ph,yl,Ch,jh,li,xh,wl,Lh,Oh,Ut,Yo,Dh,Ko,Ah,bl,Nh,Ih,Sh,yn,Go,Bh,$l,Wh,Qh,Zo,Uh,et,Rh,El,Hh,Vh,Ml,Yh,Kh,Gh,di,Xo,Dc,nt,Rt,zl,Jo,Zh,ql,Xh,Ac,en,es,Jh,ns,ef,Pl,nf,tf,of,Ht,ci,sf,rf,ui,af,lf,df,ts,cf,pi,uf,pf,hf,wn,os,ff,Cl,mf,gf,ss,_f,tt,Tf,jl,kf,Ff,xl,vf,yf,Nc,ot,Vt,Ll,rs,wf,Ol,bf,Ic,st,as,$f,is,Ef,hi,Mf,zf,Sc,rt,ls,qf,ds,Pf,fi,Cf,jf,Bc,at,Yt,Dl,cs,xf,Al,Lf,Wc,Qe,us,Of,Nl,Df,Af,ps,Nf,hs,If,Sf,Bf,fs,Wf,mi,Qf,Uf,Rf,ms,Hf,gs,Vf,Yf,Kf,nn,_s,Gf,it,Zf,gi,Xf,Jf,Il,em,nm,tm,Kt,om,Sl,sm,rm,Ts,Qc,lt,Gt,Bl,ks,am,Wl,im,Uc,Ue,Fs,lm,Ql,dm,cm,vs,um,ys,pm,hm,fm,ws,mm,_i,gm,_m,Tm,bs,km,$s,Fm,vm,ym,tn,Es,wm,dt,bm,Ti,$m,Em,Ul,Mm,zm,qm,Zt,Pm,Rl,Cm,jm,Ms,Rc,ct,Xt,Hl,zs,xm,Vl,Lm,Hc,ut,qs,Om,on,Ps,Dm,pt,Am,ki,Nm,Im,Yl,Sm,Bm,Wm,Jt,Qm,Kl,Um,Rm,Cs,Vc,ht,eo,Gl,js,Hm,Zl,Vm,Yc,Re,xs,Ym,Ls,Km,Xl,Gm,Zm,Xm,Os,Jm,Ds,eg,ng,tg,As,og,Fi,sg,rg,ag,Ns,ig,Is,lg,dg,cg,Ge,Ss,ug,ft,pg,vi,hg,fg,Jl,mg,gg,_g,no,Tg,ed,kg,Fg,Bs,vg,Ws,Kc,mt,to,nd,Qs,yg,td,wg,Gc,He,Us,bg,od,$g,Eg,Rs,Mg,Hs,zg,qg,Pg,Vs,Cg,yi,jg,xg,Lg,Ys,Og,Ks,Dg,Ag,Ng,Pe,Gs,Ig,gt,Sg,wi,Bg,Wg,sd,Qg,Ug,Rg,oo,Hg,rd,Vg,Yg,Zs,Kg,Xs,Gg,ad,Zg,Xg,Js,Jg,er,Zc,_t,so,id,nr,e_,ld,n_,Xc,Ve,tr,t_,dd,o_,s_,or,r_,sr,a_,i_,l_,rr,d_,bi,c_,u_,p_,ar,h_,ir,f_,m_,g_,sn,lr,__,Tt,T_,$i,k_,F_,cd,v_,y_,w_,ro,b_,ud,$_,E_,dr,Jc,kt,ao,pd,cr,M_,hd,z_,eu,Ye,ur,q_,fd,P_,C_,pr,j_,hr,x_,L_,O_,fr,D_,Ei,A_,N_,I_,mr,S_,gr,B_,W_,Q_,Ze,_r,U_,Ft,R_,Mi,H_,V_,md,Y_,K_,G_,io,Z_,gd,X_,J_,Tr,eT,kr,nu,vt,lo,_d,Fr,nT,Td,tT,tu,Ke,vr,oT,yt,sT,kd,rT,aT,Fd,iT,lT,dT,yr,cT,wr,uT,pT,hT,br,fT,zi,mT,gT,_T,$r,TT,Er,kT,FT,vT,Xe,Mr,yT,wt,wT,qi,bT,$T,vd,ET,MT,zT,co,qT,yd,PT,CT,zr,jT,qr,ou,bt,uo,wd,Pr,xT,bd,LT,su,Le,Cr,OT,$d,DT,AT,jr,NT,xr,IT,ST,BT,Lr,WT,Pi,QT,UT,RT,Or,HT,Dr,VT,YT,KT,po,GT,rn,Ar,ZT,$t,XT,Ci,JT,ek,Ed,nk,tk,ok,ho,sk,Md,rk,ak,Nr,ru,Et,fo,zd,Ir,ik,qd,lk,au,Oe,Sr,dk,Pd,ck,uk,Br,pk,Wr,hk,fk,mk,Qr,gk,ji,_k,Tk,kk,Ur,Fk,Rr,vk,yk,wk,mo,bk,an,Hr,$k,Mt,Ek,xi,Mk,zk,Cd,qk,Pk,Ck,go,jk,jd,xk,Lk,Vr,iu,zt,_o,xd,Yr,Ok,Ld,Dk,lu,De,Kr,Ak,Od,Nk,Ik,Gr,Sk,Zr,Bk,Wk,Qk,Xr,Uk,Li,Rk,Hk,Vk,Jr,Yk,ea,Kk,Gk,Zk,To,Xk,ln,na,Jk,qt,eF,Oi,nF,tF,Dd,oF,sF,rF,ko,aF,Ad,iF,lF,ta,du,Pt,Fo,Nd,oa,dF,Id,cF,cu,Ae,sa,uF,ra,pF,Sd,hF,fF,mF,aa,gF,ia,_F,TF,kF,la,FF,Di,vF,yF,wF,da,bF,ca,$F,EF,MF,vo,zF,dn,ua,qF,Ct,PF,Ai,CF,jF,Bd,xF,LF,OF,yo,DF,Wd,AF,NF,pa,uu,jt,wo,Qd,ha,IF,Ud,SF,pu,Ne,fa,BF,Rd,WF,QF,ma,UF,ga,RF,HF,VF,_a,YF,Ni,KF,GF,ZF,Ta,XF,ka,JF,ev,nv,bo,tv,cn,Fa,ov,xt,sv,Ii,rv,av,Hd,iv,lv,dv,$o,cv,Vd,uv,pv,va,hu,Lt,Eo,Yd,ya,hv,Kd,fv,fu,Ie,wa,mv,Gd,gv,_v,ba,Tv,$a,kv,Fv,vv,Ea,yv,Si,wv,bv,$v,Ma,Ev,za,Mv,zv,qv,Mo,Pv,un,qa,Cv,Ot,jv,Bi,xv,Lv,Zd,Ov,Dv,Av,zo,Nv,Xd,Iv,Sv,Pa,mu,Dt,qo,Jd,Ca,Bv,ec,Wv,gu,Se,ja,Qv,nc,Uv,Rv,xa,Hv,La,Vv,Yv,Kv,Oa,Gv,Wi,Zv,Xv,Jv,Da,ey,Aa,ny,ty,oy,Po,sy,pn,Na,ry,At,ay,Qi,iy,ly,tc,dy,cy,uy,Co,py,oc,hy,fy,Ia,_u,Nt,jo,sc,Sa,my,rc,gy,Tu,Be,Ba,_y,It,Ty,ac,ky,Fy,ic,vy,yy,wy,Wa,by,Qa,$y,Ey,My,Ua,zy,Ui,qy,Py,Cy,Ra,jy,Ha,xy,Ly,Oy,xo,Dy,hn,Va,Ay,St,Ny,Ri,Iy,Sy,lc,By,Wy,Qy,Lo,Uy,dc,Ry,Hy,Ya,ku;return T=new Ce({}),ne=new Ce({}),So=new Ce({}),Bo=new X({props:{name:"class transformers.FunnelConfig",anchor:"transformers.FunnelConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"block_sizes",val:" = [4, 4, 4]"},{name:"block_repeats",val:" = None"},{name:"num_decoder_layers",val:" = 2"},{name:"d_model",val:" = 768"},{name:"n_head",val:" = 12"},{name:"d_head",val:" = 64"},{name:"d_inner",val:" = 3072"},{name:"hidden_act",val:" = 'gelu_new'"},{name:"hidden_dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.1"},{name:"activation_dropout",val:" = 0.0"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 3"},{name:"initializer_range",val:" = 0.1"},{name:"initializer_std",val:" = None"},{name:"layer_norm_eps",val:" = 1e-09"},{name:"pooling_type",val:" = 'mean'"},{name:"attention_type",val:" = 'relative_shift'"},{name:"separate_cls",val:" = True"},{name:"truncate_seq",val:" = True"},{name:"pool_q_only",val:" = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/configuration_funnel.py#L37",parametersDescription:[{anchor:"transformers.FunnelConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the Funnel transformer. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a>.`,name:"vocab_size"},{anchor:"transformers.FunnelConfig.block_sizes",description:`<strong>block_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[4, 4, 4]</code>) &#x2014;
The sizes of the blocks used in the model.`,name:"block_sizes"},{anchor:"transformers.FunnelConfig.block_repeats",description:`<strong>block_repeats</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If passed along, each layer of each block is repeated the number of times indicated.`,name:"block_repeats"},{anchor:"transformers.FunnelConfig.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The number of layers in the decoder (when not using the base model).`,name:"num_decoder_layers"},{anchor:"transformers.FunnelConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the model&#x2019;s hidden states.`,name:"d_model"},{anchor:"transformers.FunnelConfig.n_head",description:`<strong>n_head</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"n_head"},{anchor:"transformers.FunnelConfig.d_head",description:`<strong>d_head</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality of the model&#x2019;s heads.`,name:"d_head"},{anchor:"transformers.FunnelConfig.d_inner",description:`<strong>d_inner</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Inner dimension in the feed-forward blocks.`,name:"d_inner"},{anchor:"transformers.FunnelConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>callable</code>, <em>optional</em>, defaults to <code>&quot;gelu_new&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FunnelConfig.hidden_dropout",description:`<strong>hidden_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout"},{anchor:"transformers.FunnelConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.FunnelConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability used between the two layers of the feed-forward blocks.`,name:"activation_dropout"},{anchor:"transformers.FunnelConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.FunnelConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.FunnelConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The upper bound of the <em>uniform initializer</em> for initializing all weight matrices in attention layers.`,name:"initializer_range"},{anchor:"transformers.FunnelConfig.initializer_std",description:`<strong>initializer_std</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The standard deviation of the <em>normal initializer</em> for initializing the embedding matrix and the weight of
linear layers. Will default to 1 for the embedding matrix and the value given by Xavier initialization for
linear layers.`,name:"initializer_std"},{anchor:"transformers.FunnelConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-9) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FunnelConfig.pooling_type",description:`<strong>pooling_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;mean&quot;</code>) &#x2014;
Possible values are <code>&quot;mean&quot;</code> or <code>&quot;max&quot;</code>. The way pooling is performed at the beginning of each block.`,name:"pooling_type"},{anchor:"transformers.FunnelConfig.attention_type",description:`<strong>attention_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;relative_shift&quot;</code>) &#x2014;
Possible values are <code>&quot;relative_shift&quot;</code> or <code>&quot;factorized&quot;</code>. The former is faster on CPU/GPU while the latter
is faster on TPU.`,name:"attention_type"},{anchor:"transformers.FunnelConfig.separate_cls",description:`<strong>separate_cls</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to separate the cls token when applying pooling.`,name:"separate_cls"},{anchor:"transformers.FunnelConfig.truncate_seq",description:`<strong>truncate_seq</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
When using <code>separate_cls</code>, whether or not to truncate the last token when pooling, to avoid getting a
sequence length that is not a multiple of 2.`,name:"truncate_seq"},{anchor:"transformers.FunnelConfig.pool_q_only",description:`<strong>pool_q_only</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to apply the pooling only to the query or to query, key and values for the attention layers.`,name:"pool_q_only"}]}}),Qo=new Ce({}),Uo=new X({props:{name:"class transformers.FunnelTokenizer",anchor:"transformers.FunnelTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/tokenization_funnel.py#L58"}}),Ho=new X({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/tokenization_bert.py#L248",parametersDescription:[{anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Yo=new X({props:{name:"get_special_tokens_mask",anchor:"transformers.BertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/tokenization_bert.py#L273",parametersDescription:[{anchor:"transformers.BertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Go=new X({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/tokenization_funnel.py#L108",parametersDescription:[{anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Zo=new qe({props:{code:`2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`2<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),Xo=new X({props:{name:"save_vocabulary",anchor:"transformers.BertTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/tokenization_bert.py#L330"}}),Jo=new Ce({}),es=new X({props:{name:"class transformers.FunnelTokenizerFast",anchor:"transformers.FunnelTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"clean_text",val:" = True"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"wordpieces_prefix",val:" = '##'"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/tokenization_funnel_fast.py#L71"}}),os=new X({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/tokenization_funnel_fast.py#L124",parametersDescription:[{anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ss=new qe({props:{code:`2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`2<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),rs=new Ce({}),as=new X({props:{name:"class transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L834",parametersDescription:[{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.loss",description:`<strong>loss</strong> (<em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) &#x2014;
Total loss of the ELECTRA-style objective.`,name:"loss"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Prediction scores of the head (scores for each token before SoftMax).`,name:"logits"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),ls=new X({props:{name:"class transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput",anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput",parameters:[{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L980",parametersDescription:[{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Prediction scores of the head (scores for each token before SoftMax).`,name:"logits"},{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),cs=new Ce({}),us=new X({props:{name:"class transformers.FunnelBaseModel",anchor:"transformers.FunnelBaseModel",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L927",parametersDescription:[{anchor:"transformers.FunnelBaseModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),_s=new X({props:{name:"forward",anchor:"transformers.FunnelBaseModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"position_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L943",parametersDescription:[{anchor:"transformers.FunnelBaseModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelBaseModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelBaseModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelBaseModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelBaseModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelBaseModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelBaseModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Kt=new ze({props:{$$slots:{default:[N$]},$$scope:{ctx:V}}}),Ts=new qe({props:{code:`from transformers import FunnelTokenizer, FunnelBaseModel
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelBaseModel.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelBaseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelBaseModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),ks=new Ce({}),Fs=new X({props:{name:"class transformers.FunnelModel",anchor:"transformers.FunnelModel",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1004",parametersDescription:[{anchor:"transformers.FunnelModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Es=new X({props:{name:"forward",anchor:"transformers.FunnelModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1021",parametersDescription:[{anchor:"transformers.FunnelModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Zt=new ze({props:{$$slots:{default:[I$]},$$scope:{ctx:V}}}),Ms=new qe({props:{code:`from transformers import FunnelTokenizer, FunnelModel
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelModel.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),zs=new Ce({}),qs=new X({props:{name:"class transformers.FunnelForPreTraining",anchor:"transformers.FunnelForPreTraining",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1112"}}),Ps=new X({props:{name:"forward",anchor:"transformers.FunnelForPreTraining.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1121",parametersDescription:[{anchor:"transformers.FunnelForPreTraining.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForPreTraining.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForPreTraining.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForPreTraining.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForPreTraining.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForPreTraining.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForPreTraining.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForPreTraining.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the ELECTRA-style loss. Input should be a sequence of tokens (see <code>input_ids</code>
docstring) Indices should be in <code>[0, 1]</code>:</p>
<ul>
<li>0 indicates the token is an original token,</li>
<li>1 indicates the token was replaced.</li>
</ul>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) \u2014 Total loss of the ELECTRA-style objective.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Prediction scores of the head (scores for each token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Jt=new ze({props:{$$slots:{default:[S$]},$$scope:{ctx:V}}}),Cs=new qe({props:{code:`from transformers import FunnelTokenizer, FunnelForPreTraining
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForPreTraining.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
logits = model(**inputs).logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForPreTraining.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits`}}),js=new Ce({}),xs=new X({props:{name:"class transformers.FunnelForMaskedLM",anchor:"transformers.FunnelForMaskedLM",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1195",parametersDescription:[{anchor:"transformers.FunnelForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ss=new X({props:{name:"forward",anchor:"transformers.FunnelForMaskedLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1211",parametersDescription:[{anchor:"transformers.FunnelForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForMaskedLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForMaskedLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),no=new ze({props:{$$slots:{default:[B$]},$$scope:{ctx:V}}}),Bs=new qe({props:{code:`from transformers import FunnelTokenizer, FunnelForMaskedLM
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForMaskedLM.from_pretrained("funnel-transformer/small")

inputs = tokenizer("The capital of France is <mask>.", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

# retrieve index of <mask>
mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]

predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)
tokenizer.decode(predicted_token_id)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is &lt;mask&gt;.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve index of &lt;mask&gt;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[<span class="hljs-number">0</span>].nonzero(as_tuple=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_id = logits[<span class="hljs-number">0</span>, mask_token_index].argmax(axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predicted_token_id)
`}}),Ws=new qe({props:{code:`labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]
# mask labels of non-<mask> tokens
labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

outputs = model(**inputs, labels=labels)
round(outputs.loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># mask labels of non-&lt;mask&gt; tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -<span class="hljs-number">100</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(outputs.loss.item(), <span class="hljs-number">2</span>)
`}}),Qs=new Ce({}),Us=new X({props:{name:"class transformers.FunnelForSequenceClassification",anchor:"transformers.FunnelForSequenceClassification",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1275",parametersDescription:[{anchor:"transformers.FunnelForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Gs=new X({props:{name:"forward",anchor:"transformers.FunnelForSequenceClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1286",parametersDescription:[{anchor:"transformers.FunnelForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForSequenceClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),oo=new ze({props:{$$slots:{default:[W$]},$$scope:{ctx:V}}}),Zs=new qe({props:{code:`import torch
from transformers import FunnelTokenizer, FunnelForSequenceClassification

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
`}}),Xs=new qe({props:{code:`# To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`
num_labels = len(model.config.id2label)
model = FunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base", num_labels=num_labels)

labels = torch.tensor(1)
loss = model(**inputs, labels=labels).loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>, num_labels=num_labels)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),Js=new qe({props:{code:`import torch
from transformers import FunnelTokenizer, FunnelForSequenceClassification

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base", problem_type="multi_label_classification")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
`}}),er=new qe({props:{code:`# To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`
num_labels = len(model.config.id2label)
model = FunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base", num_labels=num_labels)

num_labels = len(model.config.id2label)
labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(
    torch.float
)
loss = model(**inputs, labels=labels).loss
loss.backward()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>, num_labels=num_labels)

<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(
<span class="hljs-meta">... </span>    torch.<span class="hljs-built_in">float</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span>loss.backward()`}}),nr=new Ce({}),tr=new X({props:{name:"class transformers.FunnelForMultipleChoice",anchor:"transformers.FunnelForMultipleChoice",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1368",parametersDescription:[{anchor:"transformers.FunnelForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),lr=new X({props:{name:"forward",anchor:"transformers.FunnelForMultipleChoice.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1377",parametersDescription:[{anchor:"transformers.FunnelForMultipleChoice.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForMultipleChoice.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForMultipleChoice.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForMultipleChoice.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForMultipleChoice.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForMultipleChoice.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForMultipleChoice.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForMultipleChoice.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices-1]</code> where <code>num_choices</code> is the size of the second dimension of the input tensors. (See
<code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ro=new ze({props:{$$slots:{default:[Q$]},$$scope:{ctx:V}}}),dr=new qe({props:{code:`from transformers import FunnelTokenizer, FunnelForMultipleChoice
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForMultipleChoice.from_pretrained("funnel-transformer/small-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."
labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="pt", padding=True)
outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1

# the linear classifier still needs to be trained
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># choice0 is correct (according to Wikipedia ;)), batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v.unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}, labels=labels)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),cr=new Ce({}),ur=new X({props:{name:"class transformers.FunnelForTokenClassification",anchor:"transformers.FunnelForTokenClassification",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1452",parametersDescription:[{anchor:"transformers.FunnelForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),_r=new X({props:{name:"forward",anchor:"transformers.FunnelForTokenClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1464",parametersDescription:[{anchor:"transformers.FunnelForTokenClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForTokenClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForTokenClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForTokenClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForTokenClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForTokenClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForTokenClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForTokenClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),io=new ze({props:{$$slots:{default:[U$]},$$scope:{ctx:V}}}),Tr=new qe({props:{code:`from transformers import FunnelTokenizer, FunnelForTokenClassification
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForTokenClassification.from_pretrained("funnel-transformer/small")

inputs = tokenizer(
    "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="pt"
)

with torch.no_grad():
    logits = model(**inputs).logits

predicted_token_class_ids = logits.argmax(-1)

# Note that tokens are classified rather then input words which means that
# there might be more predicted token classes than words.
# Multiple token classes might account for the same word
predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]
predicted_tokens_classes
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;HuggingFace is a company based in Paris and New York&quot;</span>, add_special_tokens=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_class_ids = logits.argmax(-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Note that tokens are classified rather then input words which means that</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># there might be more predicted token classes than words.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multiple token classes might account for the same word</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes = [model.config.id2label[t.item()] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> predicted_token_class_ids[<span class="hljs-number">0</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes
`}}),kr=new qe({props:{code:`labels = predicted_token_class_ids
loss = model(**inputs, labels=labels).loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = predicted_token_class_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),Fr=new Ce({}),vr=new X({props:{name:"class transformers.FunnelForQuestionAnswering",anchor:"transformers.FunnelForQuestionAnswering",parameters:[{name:"config",val:": FunnelConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1526",parametersDescription:[{anchor:"transformers.FunnelForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Mr=new X({props:{name:"forward",anchor:"transformers.FunnelForQuestionAnswering.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"start_positions",val:": typing.Optional[torch.Tensor] = None"},{name:"end_positions",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_funnel.py#L1537",parametersDescription:[{anchor:"transformers.FunnelForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForQuestionAnswering.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForQuestionAnswering.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForQuestionAnswering.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForQuestionAnswering.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForQuestionAnswering.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.FunnelForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),co=new ze({props:{$$slots:{default:[R$]},$$scope:{ctx:V}}}),zr=new qe({props:{code:`from transformers import FunnelTokenizer, FunnelForQuestionAnswering
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForQuestionAnswering.from_pretrained("funnel-transformer/small")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

inputs = tokenizer(question, text, return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs)

answer_start_index = outputs.start_logits.argmax()
answer_end_index = outputs.end_logits.argmax()

predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
tokenizer.decode(predict_answer_tokens)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>answer_start_index = outputs.start_logits.argmax()
<span class="hljs-meta">&gt;&gt;&gt; </span>answer_end_index = outputs.end_logits.argmax()

<span class="hljs-meta">&gt;&gt;&gt; </span>predict_answer_tokens = inputs.input_ids[<span class="hljs-number">0</span>, answer_start_index : answer_end_index + <span class="hljs-number">1</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predict_answer_tokens)
`}}),qr=new qe({props:{code:`# target is "nice puppet"
target_start_index, target_end_index = torch.tensor([14]), torch.tensor([15])

outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
loss = outputs.loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target is &quot;nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_start_index, target_end_index = torch.tensor([<span class="hljs-number">14</span>]), torch.tensor([<span class="hljs-number">15</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),Pr=new Ce({}),Cr=new X({props:{name:"class transformers.TFFunnelBaseModel",anchor:"transformers.TFFunnelBaseModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1097",parametersDescription:[{anchor:"transformers.TFFunnelBaseModel.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),po=new ze({props:{$$slots:{default:[H$]},$$scope:{ctx:V}}}),Ar=new X({props:{name:"call",anchor:"transformers.TFFunnelBaseModel.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1102",parametersDescription:[{anchor:"transformers.TFFunnelBaseModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelBaseModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelBaseModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelBaseModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelBaseModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelBaseModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelBaseModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelBaseModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),ho=new ze({props:{$$slots:{default:[V$]},$$scope:{ctx:V}}}),Nr=new qe({props:{code:`from transformers import FunnelTokenizer, TFFunnelBaseModel
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelBaseModel.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelBaseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelBaseModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Ir=new Ce({}),Sr=new X({props:{name:"class transformers.TFFunnelModel",anchor:"transformers.TFFunnelModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1145",parametersDescription:[{anchor:"transformers.TFFunnelModel.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),mo=new ze({props:{$$slots:{default:[Y$]},$$scope:{ctx:V}}}),Hr=new X({props:{name:"call",anchor:"transformers.TFFunnelModel.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1150",parametersDescription:[{anchor:"transformers.TFFunnelModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),go=new ze({props:{$$slots:{default:[K$]},$$scope:{ctx:V}}}),Vr=new qe({props:{code:`from transformers import FunnelTokenizer, TFFunnelModel
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelModel.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Yr=new Ce({}),Kr=new X({props:{name:"class transformers.TFFunnelForPreTraining",anchor:"transformers.TFFunnelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1196",parametersDescription:[{anchor:"transformers.TFFunnelForPreTraining.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),To=new ze({props:{$$slots:{default:[G$]},$$scope:{ctx:V}}}),na=new X({props:{name:"call",anchor:"transformers.TFFunnelForPreTraining.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1203",parametersDescription:[{anchor:"transformers.TFFunnelForPreTraining.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForPreTraining.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForPreTraining.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForPreTraining.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForPreTraining.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForPreTraining.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForPreTraining.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForPreTraining.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Prediction scores of the head (scores for each token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),ko=new ze({props:{$$slots:{default:[Z$]},$$scope:{ctx:V}}}),ta=new qe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForPreTraining
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForPreTraining.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
logits = model(inputs).logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForPreTraining.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(inputs).logits`}}),oa=new Ce({}),sa=new X({props:{name:"class transformers.TFFunnelForMaskedLM",anchor:"transformers.TFFunnelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1263",parametersDescription:[{anchor:"transformers.TFFunnelForMaskedLM.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),vo=new ze({props:{$$slots:{default:[X$]},$$scope:{ctx:V}}}),ua=new X({props:{name:"call",anchor:"transformers.TFFunnelForMaskedLM.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1277",parametersDescription:[{anchor:"transformers.TFFunnelForMaskedLM.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForMaskedLM.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForMaskedLM.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForMaskedLM.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForMaskedLM.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForMaskedLM.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForMaskedLM.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForMaskedLM.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForMaskedLM.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),yo=new ze({props:{$$slots:{default:[J$]},$$scope:{ctx:V}}}),pa=new qe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForMaskedLM
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForMaskedLM.from_pretrained("funnel-transformer/small")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="tf")
inputs["labels"] = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ha=new Ce({}),fa=new X({props:{name:"class transformers.TFFunnelForSequenceClassification",anchor:"transformers.TFFunnelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1345",parametersDescription:[{anchor:"transformers.TFFunnelForSequenceClassification.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),bo=new ze({props:{$$slots:{default:[e2]},$$scope:{ctx:V}}}),Fa=new X({props:{name:"call",anchor:"transformers.TFFunnelForSequenceClassification.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1353",parametersDescription:[{anchor:"transformers.TFFunnelForSequenceClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForSequenceClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForSequenceClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForSequenceClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForSequenceClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForSequenceClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForSequenceClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForSequenceClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForSequenceClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),$o=new ze({props:{$$slots:{default:[n2]},$$scope:{ctx:V}}}),va=new qe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForSequenceClassification
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
inputs["labels"] = tf.reshape(tf.constant(1), (-1, 1))  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(tf.constant(<span class="hljs-number">1</span>), (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ya=new Ce({}),wa=new X({props:{name:"class transformers.TFFunnelForMultipleChoice",anchor:"transformers.TFFunnelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1422",parametersDescription:[{anchor:"transformers.TFFunnelForMultipleChoice.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Mo=new ze({props:{$$slots:{default:[t2]},$$scope:{ctx:V}}}),qa=new X({props:{name:"call",anchor:"transformers.TFFunnelForMultipleChoice.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1439",parametersDescription:[{anchor:"transformers.TFFunnelForMultipleChoice.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForMultipleChoice.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForMultipleChoice.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForMultipleChoice.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForMultipleChoice.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForMultipleChoice.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForMultipleChoice.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForMultipleChoice.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForMultipleChoice.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices]</code>
where <code>num_choices</code> is the size of the second dimension of the input tensors. (See <code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <em>(batch_size, )</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),zo=new ze({props:{$$slots:{default:[o2]},$$scope:{ctx:V}}}),Pa=new qe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForMultipleChoice
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelForMultipleChoice.from_pretrained("funnel-transformer/small-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="tf", padding=True)
inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}
outputs = model(inputs)  # batch size is 1

# the linear classifier still needs to be trained
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;tf&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {k: tf.expand_dims(v, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ca=new Ce({}),ja=new X({props:{name:"class transformers.TFFunnelForTokenClassification",anchor:"transformers.TFFunnelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1539",parametersDescription:[{anchor:"transformers.TFFunnelForTokenClassification.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Po=new ze({props:{$$slots:{default:[s2]},$$scope:{ctx:V}}}),Na=new X({props:{name:"call",anchor:"transformers.TFFunnelForTokenClassification.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1550",parametersDescription:[{anchor:"transformers.TFFunnelForTokenClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForTokenClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForTokenClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForTokenClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForTokenClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForTokenClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForTokenClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForTokenClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForTokenClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of unmasked labels, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Co=new ze({props:{$$slots:{default:[r2]},$$scope:{ctx:V}}}),Ia=new qe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForTokenClassification
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForTokenClassification.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
input_ids = inputs["input_ids"]
inputs["labels"] = tf.reshape(
    tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids))
)  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(
<span class="hljs-meta">... </span>    tf.constant([<span class="hljs-number">1</span>] * tf.size(input_ids).numpy()), (-<span class="hljs-number">1</span>, tf.size(input_ids))
<span class="hljs-meta">&gt;&gt;&gt; </span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Sa=new Ce({}),Ba=new X({props:{name:"class transformers.TFFunnelForQuestionAnswering",anchor:"transformers.TFFunnelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1618",parametersDescription:[{anchor:"transformers.TFFunnelForQuestionAnswering.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),xo=new ze({props:{$$slots:{default:[a2]},$$scope:{ctx:V}}}),Va=new X({props:{name:"call",anchor:"transformers.TFFunnelForQuestionAnswering.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"token_type_ids",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"start_positions",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"end_positions",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": bool = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/funnel/modeling_tf_funnel.py#L1628",parametersDescription:[{anchor:"transformers.TFFunnelForQuestionAnswering.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.start_positions",description:`<strong>start_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.end_positions",description:`<strong>end_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>start_positions</code> and <code>end_positions</code> are provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Lo=new ze({props:{$$slots:{default:[i2]},$$scope:{ctx:V}}}),Ya=new qe({props:{code:`from transformers import FunnelTokenizer, TFFunnelForQuestionAnswering
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForQuestionAnswering.from_pretrained("funnel-transformer/small")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
input_dict = tokenizer(question, text, return_tensors="tf")
outputs = model(input_dict)
start_logits = outputs.start_logits
end_logits = outputs.end_logits

all_tokens = tokenizer.convert_ids_to_tokens(input_dict["input_ids"].numpy()[0])
answer = " ".join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0] + 1])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_dict = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_dict)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits

<span class="hljs-meta">&gt;&gt;&gt; </span>all_tokens = tokenizer.convert_ids_to_tokens(input_dict[<span class="hljs-string">&quot;input_ids&quot;</span>].numpy()[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>answer = <span class="hljs-string">&quot; &quot;</span>.join(all_tokens[tf.math.argmax(start_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] : tf.math.argmax(end_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] + <span class="hljs-number">1</span>])`}}),{c(){p=r("meta"),z=l(),g=r("h1"),_=r("a"),k=r("span"),F(T.$$.fragment),m=l(),M=r("span"),ce=t("Funnel Transformer"),K=l(),q=r("h2"),J=r("a"),A=r("span"),F(ne.$$.fragment),ue=l(),N=r("span"),pe=t("Overview"),ie=l(),Y=r("p"),L=t("The Funnel Transformer model was proposed in the paper "),te=r("a"),G=t(`Funnel-Transformer: Filtering out Sequential Redundancy for
Efficient Language Processing`),P=t(`. It is a bidirectional transformer model, like
BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks
(CNN) in computer vision.`),j=l(),oe=r("p"),W=t("The abstract from the paper is the following:"),le=l(),se=r("p"),I=r("em"),he=t(`With the success of language pretraining, it is highly desirable to develop more efficient architectures of good
scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the
much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only
require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which
gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More
importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further
improve the model capacity. In addition, to perform token-level predictions as required by common pretraining
objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence
via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on
a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading
comprehension.`),de=l(),C=r("p"),fe=t("Tips:"),B=l(),ee=r("ul"),ae=r("li"),Q=t(`Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers.
The base model therefore has a final sequence length that is a quarter of the original one. This model can be used
directly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other
tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same
sequence length as the input.`),me=l(),S=r("li"),O=t(`The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be
used for `),re=r("a"),U=t("FunnelModel"),ge=t(", "),u=r("a"),E=t("FunnelForPreTraining"),Z=t(`,
`),Te=r("a"),ye=t("FunnelForMaskedLM"),D=t(", "),ke=r("a"),we=t("FunnelForTokenClassification"),be=t(` and
class:`),x=r("em"),R=t("~transformers.FunnelForQuestionAnswering"),$e=t(`. The second ones should be used for
`),Fe=r("a"),H=t("FunnelBaseModel"),Ee=t(", "),ve=r("a"),_e=t("FunnelForSequenceClassification"),Me=t(` and
`),Ja=r("a"),Up=t("FunnelForMultipleChoice"),Rp=t("."),Cc=l(),Dn=r("p"),Hp=t("This model was contributed by "),No=r("a"),Vp=t("sgugger"),Yp=t(". The original code can be found "),Io=r("a"),Kp=t("here"),Gp=t("."),jc=l(),Zn=r("h2"),Bt=r("a"),gl=r("span"),F(So.$$.fragment),Zp=l(),_l=r("span"),Xp=t("FunnelConfig"),xc=l(),Ln=r("div"),F(Bo.$$.fragment),Jp=l(),On=r("p"),eh=t("This is the configuration class to store the configuration of a "),ei=r("a"),nh=t("FunnelModel"),th=t(" or a "),ni=r("a"),oh=t("TFBertModel"),sh=t(`. It is used to
instantiate a Funnel Transformer model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Funnel
Transformer `),Wo=r("a"),rh=t("funnel-transformer/small"),ah=t(" architecture."),ih=l(),Xn=r("p"),lh=t("Configuration objects inherit from "),ti=r("a"),dh=t("PretrainedConfig"),ch=t(` and can be used to control the model outputs. Read the
documentation from `),oi=r("a"),uh=t("PretrainedConfig"),ph=t(" for more information."),Lc=l(),Jn=r("h2"),Wt=r("a"),Tl=r("span"),F(Qo.$$.fragment),hh=l(),kl=r("span"),fh=t("FunnelTokenizer"),Oc=l(),je=r("div"),F(Uo.$$.fragment),mh=l(),Fl=r("p"),gh=t("Construct a Funnel Transformer tokenizer."),_h=l(),Qt=r("p"),si=r("a"),Th=t("FunnelTokenizer"),kh=t(" is identical to "),ri=r("a"),Fh=t("BertTokenizer"),vh=t(` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),yh=l(),Ro=r("p"),wh=t("Refer to superclass "),ai=r("a"),bh=t("BertTokenizer"),$h=t(" for usage examples and documentation concerning parameters."),Eh=l(),An=r("div"),F(Ho.$$.fragment),Mh=l(),vl=r("p"),zh=t(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),qh=l(),Vo=r("ul"),ii=r("li"),Ph=t("single sequence: "),yl=r("code"),Ch=t("[CLS] X [SEP]"),jh=l(),li=r("li"),xh=t("pair of sequences: "),wl=r("code"),Lh=t("[CLS] A [SEP] B [SEP]"),Oh=l(),Ut=r("div"),F(Yo.$$.fragment),Dh=l(),Ko=r("p"),Ah=t(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),bl=r("code"),Nh=t("prepare_for_model"),Ih=t(" method."),Sh=l(),yn=r("div"),F(Go.$$.fragment),Bh=l(),$l=r("p"),Wh=t(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),Qh=l(),F(Zo.$$.fragment),Uh=l(),et=r("p"),Rh=t("If "),El=r("code"),Hh=t("token_ids_1"),Vh=t(" is "),Ml=r("code"),Yh=t("None"),Kh=t(", this method only returns the first portion of the mask (0s)."),Gh=l(),di=r("div"),F(Xo.$$.fragment),Dc=l(),nt=r("h2"),Rt=r("a"),zl=r("span"),F(Jo.$$.fragment),Zh=l(),ql=r("span"),Xh=t("FunnelTokenizerFast"),Ac=l(),en=r("div"),F(es.$$.fragment),Jh=l(),ns=r("p"),ef=t("Construct a \u201Cfast\u201D Funnel Transformer tokenizer (backed by HuggingFace\u2019s "),Pl=r("em"),nf=t("tokenizers"),tf=t(" library)."),of=l(),Ht=r("p"),ci=r("a"),sf=t("FunnelTokenizerFast"),rf=t(" is identical to "),ui=r("a"),af=t("BertTokenizerFast"),lf=t(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),df=l(),ts=r("p"),cf=t("Refer to superclass "),pi=r("a"),uf=t("BertTokenizerFast"),pf=t(" for usage examples and documentation concerning parameters."),hf=l(),wn=r("div"),F(os.$$.fragment),ff=l(),Cl=r("p"),mf=t(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),gf=l(),F(ss.$$.fragment),_f=l(),tt=r("p"),Tf=t("If "),jl=r("code"),kf=t("token_ids_1"),Ff=t(" is "),xl=r("code"),vf=t("None"),yf=t(", this method only returns the first portion of the mask (0s)."),Nc=l(),ot=r("h2"),Vt=r("a"),Ll=r("span"),F(rs.$$.fragment),wf=l(),Ol=r("span"),bf=t("Funnel specific outputs"),Ic=l(),st=r("div"),F(as.$$.fragment),$f=l(),is=r("p"),Ef=t("Output type of "),hi=r("a"),Mf=t("FunnelForPreTraining"),zf=t("."),Sc=l(),rt=r("div"),F(ls.$$.fragment),qf=l(),ds=r("p"),Pf=t("Output type of "),fi=r("a"),Cf=t("FunnelForPreTraining"),jf=t("."),Bc=l(),at=r("h2"),Yt=r("a"),Dl=r("span"),F(cs.$$.fragment),xf=l(),Al=r("span"),Lf=t("FunnelBaseModel"),Wc=l(),Qe=r("div"),F(us.$$.fragment),Of=l(),Nl=r("p"),Df=t(`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),Af=l(),ps=r("p"),Nf=t("The Funnel Transformer model was proposed in "),hs=r("a"),If=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Sf=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Bf=l(),fs=r("p"),Wf=t("This model inherits from "),mi=r("a"),Qf=t("PreTrainedModel"),Uf=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Rf=l(),ms=r("p"),Hf=t("This model is also a PyTorch "),gs=r("a"),Vf=t("torch.nn.Module"),Yf=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Kf=l(),nn=r("div"),F(_s.$$.fragment),Gf=l(),it=r("p"),Zf=t("The "),gi=r("a"),Xf=t("FunnelBaseModel"),Jf=t(" forward method, overrides the "),Il=r("code"),em=t("__call__"),nm=t(" special method."),tm=l(),F(Kt.$$.fragment),om=l(),Sl=r("p"),sm=t("Example:"),rm=l(),F(Ts.$$.fragment),Qc=l(),lt=r("h2"),Gt=r("a"),Bl=r("span"),F(ks.$$.fragment),am=l(),Wl=r("span"),im=t("FunnelModel"),Uc=l(),Ue=r("div"),F(Fs.$$.fragment),lm=l(),Ql=r("p"),dm=t("The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),cm=l(),vs=r("p"),um=t("The Funnel Transformer model was proposed in "),ys=r("a"),pm=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),hm=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),fm=l(),ws=r("p"),mm=t("This model inherits from "),_i=r("a"),gm=t("PreTrainedModel"),_m=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Tm=l(),bs=r("p"),km=t("This model is also a PyTorch "),$s=r("a"),Fm=t("torch.nn.Module"),vm=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ym=l(),tn=r("div"),F(Es.$$.fragment),wm=l(),dt=r("p"),bm=t("The "),Ti=r("a"),$m=t("FunnelModel"),Em=t(" forward method, overrides the "),Ul=r("code"),Mm=t("__call__"),zm=t(" special method."),qm=l(),F(Zt.$$.fragment),Pm=l(),Rl=r("p"),Cm=t("Example:"),jm=l(),F(Ms.$$.fragment),Rc=l(),ct=r("h2"),Xt=r("a"),Hl=r("span"),F(zs.$$.fragment),xm=l(),Vl=r("span"),Lm=t("FunnelModelForPreTraining"),Hc=l(),ut=r("div"),F(qs.$$.fragment),Om=l(),on=r("div"),F(Ps.$$.fragment),Dm=l(),pt=r("p"),Am=t("The "),ki=r("a"),Nm=t("FunnelForPreTraining"),Im=t(" forward method, overrides the "),Yl=r("code"),Sm=t("__call__"),Bm=t(" special method."),Wm=l(),F(Jt.$$.fragment),Qm=l(),Kl=r("p"),Um=t("Examples:"),Rm=l(),F(Cs.$$.fragment),Vc=l(),ht=r("h2"),eo=r("a"),Gl=r("span"),F(js.$$.fragment),Hm=l(),Zl=r("span"),Vm=t("FunnelForMaskedLM"),Yc=l(),Re=r("div"),F(xs.$$.fragment),Ym=l(),Ls=r("p"),Km=t("Funnel Transformer Model with a "),Xl=r("code"),Gm=t("language modeling"),Zm=t(" head on top."),Xm=l(),Os=r("p"),Jm=t("The Funnel Transformer model was proposed in "),Ds=r("a"),eg=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),ng=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),tg=l(),As=r("p"),og=t("This model inherits from "),Fi=r("a"),sg=t("PreTrainedModel"),rg=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ag=l(),Ns=r("p"),ig=t("This model is also a PyTorch "),Is=r("a"),lg=t("torch.nn.Module"),dg=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),cg=l(),Ge=r("div"),F(Ss.$$.fragment),ug=l(),ft=r("p"),pg=t("The "),vi=r("a"),hg=t("FunnelForMaskedLM"),fg=t(" forward method, overrides the "),Jl=r("code"),mg=t("__call__"),gg=t(" special method."),_g=l(),F(no.$$.fragment),Tg=l(),ed=r("p"),kg=t("Example:"),Fg=l(),F(Bs.$$.fragment),vg=l(),F(Ws.$$.fragment),Kc=l(),mt=r("h2"),to=r("a"),nd=r("span"),F(Qs.$$.fragment),yg=l(),td=r("span"),wg=t("FunnelForSequenceClassification"),Gc=l(),He=r("div"),F(Us.$$.fragment),bg=l(),od=r("p"),$g=t(`Funnel Transformer Model with a sequence classification/regression head on top (two linear layer on top of the
first timestep of the last hidden state) e.g. for GLUE tasks.`),Eg=l(),Rs=r("p"),Mg=t("The Funnel Transformer model was proposed in "),Hs=r("a"),zg=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),qg=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Pg=l(),Vs=r("p"),Cg=t("This model inherits from "),yi=r("a"),jg=t("PreTrainedModel"),xg=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Lg=l(),Ys=r("p"),Og=t("This model is also a PyTorch "),Ks=r("a"),Dg=t("torch.nn.Module"),Ag=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ng=l(),Pe=r("div"),F(Gs.$$.fragment),Ig=l(),gt=r("p"),Sg=t("The "),wi=r("a"),Bg=t("FunnelForSequenceClassification"),Wg=t(" forward method, overrides the "),sd=r("code"),Qg=t("__call__"),Ug=t(" special method."),Rg=l(),F(oo.$$.fragment),Hg=l(),rd=r("p"),Vg=t("Example of single-label classification:"),Yg=l(),F(Zs.$$.fragment),Kg=l(),F(Xs.$$.fragment),Gg=l(),ad=r("p"),Zg=t("Example of multi-label classification:"),Xg=l(),F(Js.$$.fragment),Jg=l(),F(er.$$.fragment),Zc=l(),_t=r("h2"),so=r("a"),id=r("span"),F(nr.$$.fragment),e_=l(),ld=r("span"),n_=t("FunnelForMultipleChoice"),Xc=l(),Ve=r("div"),F(tr.$$.fragment),t_=l(),dd=r("p"),o_=t(`Funnel Transformer Model with a multiple choice classification head on top (two linear layer on top of the first
timestep of the last hidden state, and a softmax) e.g. for RocStories/SWAG tasks.`),s_=l(),or=r("p"),r_=t("The Funnel Transformer model was proposed in "),sr=r("a"),a_=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),i_=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),l_=l(),rr=r("p"),d_=t("This model inherits from "),bi=r("a"),c_=t("PreTrainedModel"),u_=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),p_=l(),ar=r("p"),h_=t("This model is also a PyTorch "),ir=r("a"),f_=t("torch.nn.Module"),m_=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),g_=l(),sn=r("div"),F(lr.$$.fragment),__=l(),Tt=r("p"),T_=t("The "),$i=r("a"),k_=t("FunnelForMultipleChoice"),F_=t(" forward method, overrides the "),cd=r("code"),v_=t("__call__"),y_=t(" special method."),w_=l(),F(ro.$$.fragment),b_=l(),ud=r("p"),$_=t("Example:"),E_=l(),F(dr.$$.fragment),Jc=l(),kt=r("h2"),ao=r("a"),pd=r("span"),F(cr.$$.fragment),M_=l(),hd=r("span"),z_=t("FunnelForTokenClassification"),eu=l(),Ye=r("div"),F(ur.$$.fragment),q_=l(),fd=r("p"),P_=t(`Funnel Transformer Model with a token classification head on top (a linear layer on top of the hidden-states
output) e.g. for Named-Entity-Recognition (NER) tasks.`),C_=l(),pr=r("p"),j_=t("The Funnel Transformer model was proposed in "),hr=r("a"),x_=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),L_=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),O_=l(),fr=r("p"),D_=t("This model inherits from "),Ei=r("a"),A_=t("PreTrainedModel"),N_=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),I_=l(),mr=r("p"),S_=t("This model is also a PyTorch "),gr=r("a"),B_=t("torch.nn.Module"),W_=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Q_=l(),Ze=r("div"),F(_r.$$.fragment),U_=l(),Ft=r("p"),R_=t("The "),Mi=r("a"),H_=t("FunnelForTokenClassification"),V_=t(" forward method, overrides the "),md=r("code"),Y_=t("__call__"),K_=t(" special method."),G_=l(),F(io.$$.fragment),Z_=l(),gd=r("p"),X_=t("Example:"),J_=l(),F(Tr.$$.fragment),eT=l(),F(kr.$$.fragment),nu=l(),vt=r("h2"),lo=r("a"),_d=r("span"),F(Fr.$$.fragment),nT=l(),Td=r("span"),tT=t("FunnelForQuestionAnswering"),tu=l(),Ke=r("div"),F(vr.$$.fragment),oT=l(),yt=r("p"),sT=t(`Funnel Transformer Model with a span classification head on top for extractive question-answering tasks like SQuAD
(a linear layer on top of the hidden-states output to compute `),kd=r("code"),rT=t("span start logits"),aT=t(" and "),Fd=r("code"),iT=t("span end logits"),lT=t(")."),dT=l(),yr=r("p"),cT=t("The Funnel Transformer model was proposed in "),wr=r("a"),uT=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),pT=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),hT=l(),br=r("p"),fT=t("This model inherits from "),zi=r("a"),mT=t("PreTrainedModel"),gT=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),_T=l(),$r=r("p"),TT=t("This model is also a PyTorch "),Er=r("a"),kT=t("torch.nn.Module"),FT=t(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),vT=l(),Xe=r("div"),F(Mr.$$.fragment),yT=l(),wt=r("p"),wT=t("The "),qi=r("a"),bT=t("FunnelForQuestionAnswering"),$T=t(" forward method, overrides the "),vd=r("code"),ET=t("__call__"),MT=t(" special method."),zT=l(),F(co.$$.fragment),qT=l(),yd=r("p"),PT=t("Example:"),CT=l(),F(zr.$$.fragment),jT=l(),F(qr.$$.fragment),ou=l(),bt=r("h2"),uo=r("a"),wd=r("span"),F(Pr.$$.fragment),xT=l(),bd=r("span"),LT=t("TFFunnelBaseModel"),su=l(),Le=r("div"),F(Cr.$$.fragment),OT=l(),$d=r("p"),DT=t(`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),AT=l(),jr=r("p"),NT=t("The Funnel Transformer model was proposed in "),xr=r("a"),IT=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),ST=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),BT=l(),Lr=r("p"),WT=t("This model inherits from "),Pi=r("a"),QT=t("TFPreTrainedModel"),UT=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),RT=l(),Or=r("p"),HT=t("This model is also a "),Dr=r("a"),VT=t("tf.keras.Model"),YT=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),KT=l(),F(po.$$.fragment),GT=l(),rn=r("div"),F(Ar.$$.fragment),ZT=l(),$t=r("p"),XT=t("The "),Ci=r("a"),JT=t("TFFunnelBaseModel"),ek=t(" forward method, overrides the "),Ed=r("code"),nk=t("__call__"),tk=t(" special method."),ok=l(),F(ho.$$.fragment),sk=l(),Md=r("p"),rk=t("Example:"),ak=l(),F(Nr.$$.fragment),ru=l(),Et=r("h2"),fo=r("a"),zd=r("span"),F(Ir.$$.fragment),ik=l(),qd=r("span"),lk=t("TFFunnelModel"),au=l(),Oe=r("div"),F(Sr.$$.fragment),dk=l(),Pd=r("p"),ck=t("The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),uk=l(),Br=r("p"),pk=t("The Funnel Transformer model was proposed in "),Wr=r("a"),hk=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),fk=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),mk=l(),Qr=r("p"),gk=t("This model inherits from "),ji=r("a"),_k=t("TFPreTrainedModel"),Tk=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),kk=l(),Ur=r("p"),Fk=t("This model is also a "),Rr=r("a"),vk=t("tf.keras.Model"),yk=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),wk=l(),F(mo.$$.fragment),bk=l(),an=r("div"),F(Hr.$$.fragment),$k=l(),Mt=r("p"),Ek=t("The "),xi=r("a"),Mk=t("TFFunnelModel"),zk=t(" forward method, overrides the "),Cd=r("code"),qk=t("__call__"),Pk=t(" special method."),Ck=l(),F(go.$$.fragment),jk=l(),jd=r("p"),xk=t("Example:"),Lk=l(),F(Vr.$$.fragment),iu=l(),zt=r("h2"),_o=r("a"),xd=r("span"),F(Yr.$$.fragment),Ok=l(),Ld=r("span"),Dk=t("TFFunnelModelForPreTraining"),lu=l(),De=r("div"),F(Kr.$$.fragment),Ak=l(),Od=r("p"),Nk=t("Funnel model with a binary classification head on top as used during pretraining for identifying generated tokens."),Ik=l(),Gr=r("p"),Sk=t("The Funnel Transformer model was proposed in "),Zr=r("a"),Bk=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Wk=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Qk=l(),Xr=r("p"),Uk=t("This model inherits from "),Li=r("a"),Rk=t("TFPreTrainedModel"),Hk=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Vk=l(),Jr=r("p"),Yk=t("This model is also a "),ea=r("a"),Kk=t("tf.keras.Model"),Gk=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Zk=l(),F(To.$$.fragment),Xk=l(),ln=r("div"),F(na.$$.fragment),Jk=l(),qt=r("p"),eF=t("The "),Oi=r("a"),nF=t("TFFunnelForPreTraining"),tF=t(" forward method, overrides the "),Dd=r("code"),oF=t("__call__"),sF=t(" special method."),rF=l(),F(ko.$$.fragment),aF=l(),Ad=r("p"),iF=t("Examples:"),lF=l(),F(ta.$$.fragment),du=l(),Pt=r("h2"),Fo=r("a"),Nd=r("span"),F(oa.$$.fragment),dF=l(),Id=r("span"),cF=t("TFFunnelForMaskedLM"),cu=l(),Ae=r("div"),F(sa.$$.fragment),uF=l(),ra=r("p"),pF=t("Funnel Model with a "),Sd=r("code"),hF=t("language modeling"),fF=t(" head on top."),mF=l(),aa=r("p"),gF=t("The Funnel Transformer model was proposed in "),ia=r("a"),_F=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),TF=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),kF=l(),la=r("p"),FF=t("This model inherits from "),Di=r("a"),vF=t("TFPreTrainedModel"),yF=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),wF=l(),da=r("p"),bF=t("This model is also a "),ca=r("a"),$F=t("tf.keras.Model"),EF=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),MF=l(),F(vo.$$.fragment),zF=l(),dn=r("div"),F(ua.$$.fragment),qF=l(),Ct=r("p"),PF=t("The "),Ai=r("a"),CF=t("TFFunnelForMaskedLM"),jF=t(" forward method, overrides the "),Bd=r("code"),xF=t("__call__"),LF=t(" special method."),OF=l(),F(yo.$$.fragment),DF=l(),Wd=r("p"),AF=t("Example:"),NF=l(),F(pa.$$.fragment),uu=l(),jt=r("h2"),wo=r("a"),Qd=r("span"),F(ha.$$.fragment),IF=l(),Ud=r("span"),SF=t("TFFunnelForSequenceClassification"),pu=l(),Ne=r("div"),F(fa.$$.fragment),BF=l(),Rd=r("p"),WF=t(`Funnel Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
output) e.g. for GLUE tasks.`),QF=l(),ma=r("p"),UF=t("The Funnel Transformer model was proposed in "),ga=r("a"),RF=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),HF=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),VF=l(),_a=r("p"),YF=t("This model inherits from "),Ni=r("a"),KF=t("TFPreTrainedModel"),GF=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ZF=l(),Ta=r("p"),XF=t("This model is also a "),ka=r("a"),JF=t("tf.keras.Model"),ev=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),nv=l(),F(bo.$$.fragment),tv=l(),cn=r("div"),F(Fa.$$.fragment),ov=l(),xt=r("p"),sv=t("The "),Ii=r("a"),rv=t("TFFunnelForSequenceClassification"),av=t(" forward method, overrides the "),Hd=r("code"),iv=t("__call__"),lv=t(" special method."),dv=l(),F($o.$$.fragment),cv=l(),Vd=r("p"),uv=t("Example:"),pv=l(),F(va.$$.fragment),hu=l(),Lt=r("h2"),Eo=r("a"),Yd=r("span"),F(ya.$$.fragment),hv=l(),Kd=r("span"),fv=t("TFFunnelForMultipleChoice"),fu=l(),Ie=r("div"),F(wa.$$.fragment),mv=l(),Gd=r("p"),gv=t(`Funnel Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),_v=l(),ba=r("p"),Tv=t("The Funnel Transformer model was proposed in "),$a=r("a"),kv=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Fv=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),vv=l(),Ea=r("p"),yv=t("This model inherits from "),Si=r("a"),wv=t("TFPreTrainedModel"),bv=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),$v=l(),Ma=r("p"),Ev=t("This model is also a "),za=r("a"),Mv=t("tf.keras.Model"),zv=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),qv=l(),F(Mo.$$.fragment),Pv=l(),un=r("div"),F(qa.$$.fragment),Cv=l(),Ot=r("p"),jv=t("The "),Bi=r("a"),xv=t("TFFunnelForMultipleChoice"),Lv=t(" forward method, overrides the "),Zd=r("code"),Ov=t("__call__"),Dv=t(" special method."),Av=l(),F(zo.$$.fragment),Nv=l(),Xd=r("p"),Iv=t("Example:"),Sv=l(),F(Pa.$$.fragment),mu=l(),Dt=r("h2"),qo=r("a"),Jd=r("span"),F(Ca.$$.fragment),Bv=l(),ec=r("span"),Wv=t("TFFunnelForTokenClassification"),gu=l(),Se=r("div"),F(ja.$$.fragment),Qv=l(),nc=r("p"),Uv=t(`Funnel Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),Rv=l(),xa=r("p"),Hv=t("The Funnel Transformer model was proposed in "),La=r("a"),Vv=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Yv=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Kv=l(),Oa=r("p"),Gv=t("This model inherits from "),Wi=r("a"),Zv=t("TFPreTrainedModel"),Xv=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Jv=l(),Da=r("p"),ey=t("This model is also a "),Aa=r("a"),ny=t("tf.keras.Model"),ty=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),oy=l(),F(Po.$$.fragment),sy=l(),pn=r("div"),F(Na.$$.fragment),ry=l(),At=r("p"),ay=t("The "),Qi=r("a"),iy=t("TFFunnelForTokenClassification"),ly=t(" forward method, overrides the "),tc=r("code"),dy=t("__call__"),cy=t(" special method."),uy=l(),F(Co.$$.fragment),py=l(),oc=r("p"),hy=t("Example:"),fy=l(),F(Ia.$$.fragment),_u=l(),Nt=r("h2"),jo=r("a"),sc=r("span"),F(Sa.$$.fragment),my=l(),rc=r("span"),gy=t("TFFunnelForQuestionAnswering"),Tu=l(),Be=r("div"),F(Ba.$$.fragment),_y=l(),It=r("p"),Ty=t(`Funnel Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),ac=r("code"),ky=t("span start logits"),Fy=t(" and "),ic=r("code"),vy=t("span end logits"),yy=t(")."),wy=l(),Wa=r("p"),by=t("The Funnel Transformer model was proposed in "),Qa=r("a"),$y=t(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Ey=t(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),My=l(),Ua=r("p"),zy=t("This model inherits from "),Ui=r("a"),qy=t("TFPreTrainedModel"),Py=t(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Cy=l(),Ra=r("p"),jy=t("This model is also a "),Ha=r("a"),xy=t("tf.keras.Model"),Ly=t(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Oy=l(),F(xo.$$.fragment),Dy=l(),hn=r("div"),F(Va.$$.fragment),Ay=l(),St=r("p"),Ny=t("The "),Ri=r("a"),Iy=t("TFFunnelForQuestionAnswering"),Sy=t(" forward method, overrides the "),lc=r("code"),By=t("__call__"),Wy=t(" special method."),Qy=l(),F(Lo.$$.fragment),Uy=l(),dc=r("p"),Ry=t("Example:"),Hy=l(),F(Ya.$$.fragment),this.h()},l(s){const f=D$('[data-svelte="svelte-1phssyn"]',document.head);p=a(f,"META",{name:!0,content:!0}),f.forEach(n),z=d(s),g=a(s,"H1",{class:!0});var Ka=i(g);_=a(Ka,"A",{id:!0,class:!0,href:!0});var cc=i(_);k=a(cc,"SPAN",{});var uc=i(k);v(T.$$.fragment,uc),uc.forEach(n),cc.forEach(n),m=d(Ka),M=a(Ka,"SPAN",{});var pc=i(M);ce=o(pc,"Funnel Transformer"),pc.forEach(n),Ka.forEach(n),K=d(s),q=a(s,"H2",{class:!0});var Ga=i(q);J=a(Ga,"A",{id:!0,class:!0,href:!0});var hc=i(J);A=a(hc,"SPAN",{});var fc=i(A);v(ne.$$.fragment,fc),fc.forEach(n),hc.forEach(n),ue=d(Ga),N=a(Ga,"SPAN",{});var mc=i(N);pe=o(mc,"Overview"),mc.forEach(n),Ga.forEach(n),ie=d(s),Y=a(s,"P",{});var Za=i(Y);L=o(Za,"The Funnel Transformer model was proposed in the paper "),te=a(Za,"A",{href:!0,rel:!0});var gc=i(te);G=o(gc,`Funnel-Transformer: Filtering out Sequential Redundancy for
Efficient Language Processing`),gc.forEach(n),P=o(Za,`. It is a bidirectional transformer model, like
BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks
(CNN) in computer vision.`),Za.forEach(n),j=d(s),oe=a(s,"P",{});var _c=i(oe);W=o(_c,"The abstract from the paper is the following:"),_c.forEach(n),le=d(s),se=a(s,"P",{});var Tc=i(se);I=a(Tc,"EM",{});var kc=i(I);he=o(kc,`With the success of language pretraining, it is highly desirable to develop more efficient architectures of good
scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the
much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only
require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which
gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More
importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further
improve the model capacity. In addition, to perform token-level predictions as required by common pretraining
objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence
via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on
a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading
comprehension.`),kc.forEach(n),Tc.forEach(n),de=d(s),C=a(s,"P",{});var Fc=i(C);fe=o(Fc,"Tips:"),Fc.forEach(n),B=d(s),ee=a(s,"UL",{});var Xa=i(ee);ae=a(Xa,"LI",{});var vc=i(ae);Q=o(vc,`Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers.
The base model therefore has a final sequence length that is a quarter of the original one. This model can be used
directly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other
tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same
sequence length as the input.`),vc.forEach(n),me=d(Xa),S=a(Xa,"LI",{});var xe=i(S);O=o(xe,`The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be
used for `),re=a(xe,"A",{href:!0});var yc=i(re);U=o(yc,"FunnelModel"),yc.forEach(n),ge=o(xe,", "),u=a(xe,"A",{href:!0});var wc=i(u);E=o(wc,"FunnelForPreTraining"),wc.forEach(n),Z=o(xe,`,
`),Te=a(xe,"A",{href:!0});var bc=i(Te);ye=o(bc,"FunnelForMaskedLM"),bc.forEach(n),D=o(xe,", "),ke=a(xe,"A",{href:!0});var $c=i(ke);we=o($c,"FunnelForTokenClassification"),$c.forEach(n),be=o(xe,` and
class:`),x=a(xe,"EM",{});var Ec=i(x);R=o(Ec,"~transformers.FunnelForQuestionAnswering"),Ec.forEach(n),$e=o(xe,`. The second ones should be used for
`),Fe=a(xe,"A",{href:!0});var Mc=i(Fe);H=o(Mc,"FunnelBaseModel"),Mc.forEach(n),Ee=o(xe,", "),ve=a(xe,"A",{href:!0});var zc=i(ve);_e=o(zc,"FunnelForSequenceClassification"),zc.forEach(n),Me=o(xe,` and
`),Ja=a(xe,"A",{href:!0});var Ky=i(Ja);Up=o(Ky,"FunnelForMultipleChoice"),Ky.forEach(n),Rp=o(xe,"."),xe.forEach(n),Xa.forEach(n),Cc=d(s),Dn=a(s,"P",{});var Hi=i(Dn);Hp=o(Hi,"This model was contributed by "),No=a(Hi,"A",{href:!0,rel:!0});var Gy=i(No);Vp=o(Gy,"sgugger"),Gy.forEach(n),Yp=o(Hi,". The original code can be found "),Io=a(Hi,"A",{href:!0,rel:!0});var Zy=i(Io);Kp=o(Zy,"here"),Zy.forEach(n),Gp=o(Hi,"."),Hi.forEach(n),jc=d(s),Zn=a(s,"H2",{class:!0});var Fu=i(Zn);Bt=a(Fu,"A",{id:!0,class:!0,href:!0});var Xy=i(Bt);gl=a(Xy,"SPAN",{});var Jy=i(gl);v(So.$$.fragment,Jy),Jy.forEach(n),Xy.forEach(n),Zp=d(Fu),_l=a(Fu,"SPAN",{});var ew=i(_l);Xp=o(ew,"FunnelConfig"),ew.forEach(n),Fu.forEach(n),xc=d(s),Ln=a(s,"DIV",{class:!0});var Vi=i(Ln);v(Bo.$$.fragment,Vi),Jp=d(Vi),On=a(Vi,"P",{});var Oo=i(On);eh=o(Oo,"This is the configuration class to store the configuration of a "),ei=a(Oo,"A",{href:!0});var nw=i(ei);nh=o(nw,"FunnelModel"),nw.forEach(n),th=o(Oo," or a "),ni=a(Oo,"A",{href:!0});var tw=i(ni);oh=o(tw,"TFBertModel"),tw.forEach(n),sh=o(Oo,`. It is used to
instantiate a Funnel Transformer model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Funnel
Transformer `),Wo=a(Oo,"A",{href:!0,rel:!0});var ow=i(Wo);rh=o(ow,"funnel-transformer/small"),ow.forEach(n),ah=o(Oo," architecture."),Oo.forEach(n),ih=d(Vi),Xn=a(Vi,"P",{});var Yi=i(Xn);lh=o(Yi,"Configuration objects inherit from "),ti=a(Yi,"A",{href:!0});var sw=i(ti);dh=o(sw,"PretrainedConfig"),sw.forEach(n),ch=o(Yi,` and can be used to control the model outputs. Read the
documentation from `),oi=a(Yi,"A",{href:!0});var rw=i(oi);uh=o(rw,"PretrainedConfig"),rw.forEach(n),ph=o(Yi," for more information."),Yi.forEach(n),Vi.forEach(n),Lc=d(s),Jn=a(s,"H2",{class:!0});var vu=i(Jn);Wt=a(vu,"A",{id:!0,class:!0,href:!0});var aw=i(Wt);Tl=a(aw,"SPAN",{});var iw=i(Tl);v(Qo.$$.fragment,iw),iw.forEach(n),aw.forEach(n),hh=d(vu),kl=a(vu,"SPAN",{});var lw=i(kl);fh=o(lw,"FunnelTokenizer"),lw.forEach(n),vu.forEach(n),Oc=d(s),je=a(s,"DIV",{class:!0});var Je=i(je);v(Uo.$$.fragment,Je),mh=d(Je),Fl=a(Je,"P",{});var dw=i(Fl);gh=o(dw,"Construct a Funnel Transformer tokenizer."),dw.forEach(n),_h=d(Je),Qt=a(Je,"P",{});var qc=i(Qt);si=a(qc,"A",{href:!0});var cw=i(si);Th=o(cw,"FunnelTokenizer"),cw.forEach(n),kh=o(qc," is identical to "),ri=a(qc,"A",{href:!0});var uw=i(ri);Fh=o(uw,"BertTokenizer"),uw.forEach(n),vh=o(qc,` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),qc.forEach(n),yh=d(Je),Ro=a(Je,"P",{});var yu=i(Ro);wh=o(yu,"Refer to superclass "),ai=a(yu,"A",{href:!0});var pw=i(ai);bh=o(pw,"BertTokenizer"),pw.forEach(n),$h=o(yu," for usage examples and documentation concerning parameters."),yu.forEach(n),Eh=d(Je),An=a(Je,"DIV",{class:!0});var Ki=i(An);v(Ho.$$.fragment,Ki),Mh=d(Ki),vl=a(Ki,"P",{});var hw=i(vl);zh=o(hw,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),hw.forEach(n),qh=d(Ki),Vo=a(Ki,"UL",{});var wu=i(Vo);ii=a(wu,"LI",{});var Vy=i(ii);Ph=o(Vy,"single sequence: "),yl=a(Vy,"CODE",{});var fw=i(yl);Ch=o(fw,"[CLS] X [SEP]"),fw.forEach(n),Vy.forEach(n),jh=d(wu),li=a(wu,"LI",{});var Yy=i(li);xh=o(Yy,"pair of sequences: "),wl=a(Yy,"CODE",{});var mw=i(wl);Lh=o(mw,"[CLS] A [SEP] B [SEP]"),mw.forEach(n),Yy.forEach(n),wu.forEach(n),Ki.forEach(n),Oh=d(Je),Ut=a(Je,"DIV",{class:!0});var bu=i(Ut);v(Yo.$$.fragment,bu),Dh=d(bu),Ko=a(bu,"P",{});var $u=i(Ko);Ah=o($u,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),bl=a($u,"CODE",{});var gw=i(bl);Nh=o(gw,"prepare_for_model"),gw.forEach(n),Ih=o($u," method."),$u.forEach(n),bu.forEach(n),Sh=d(Je),yn=a(Je,"DIV",{class:!0});var Do=i(yn);v(Go.$$.fragment,Do),Bh=d(Do),$l=a(Do,"P",{});var _w=i($l);Wh=o(_w,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),_w.forEach(n),Qh=d(Do),v(Zo.$$.fragment,Do),Uh=d(Do),et=a(Do,"P",{});var Gi=i(et);Rh=o(Gi,"If "),El=a(Gi,"CODE",{});var Tw=i(El);Hh=o(Tw,"token_ids_1"),Tw.forEach(n),Vh=o(Gi," is "),Ml=a(Gi,"CODE",{});var kw=i(Ml);Yh=o(kw,"None"),kw.forEach(n),Kh=o(Gi,", this method only returns the first portion of the mask (0s)."),Gi.forEach(n),Do.forEach(n),Gh=d(Je),di=a(Je,"DIV",{class:!0});var Fw=i(di);v(Xo.$$.fragment,Fw),Fw.forEach(n),Je.forEach(n),Dc=d(s),nt=a(s,"H2",{class:!0});var Eu=i(nt);Rt=a(Eu,"A",{id:!0,class:!0,href:!0});var vw=i(Rt);zl=a(vw,"SPAN",{});var yw=i(zl);v(Jo.$$.fragment,yw),yw.forEach(n),vw.forEach(n),Zh=d(Eu),ql=a(Eu,"SPAN",{});var ww=i(ql);Xh=o(ww,"FunnelTokenizerFast"),ww.forEach(n),Eu.forEach(n),Ac=d(s),en=a(s,"DIV",{class:!0});var Nn=i(en);v(es.$$.fragment,Nn),Jh=d(Nn),ns=a(Nn,"P",{});var Mu=i(ns);ef=o(Mu,"Construct a \u201Cfast\u201D Funnel Transformer tokenizer (backed by HuggingFace\u2019s "),Pl=a(Mu,"EM",{});var bw=i(Pl);nf=o(bw,"tokenizers"),bw.forEach(n),tf=o(Mu," library)."),Mu.forEach(n),of=d(Nn),Ht=a(Nn,"P",{});var Pc=i(Ht);ci=a(Pc,"A",{href:!0});var $w=i(ci);sf=o($w,"FunnelTokenizerFast"),$w.forEach(n),rf=o(Pc," is identical to "),ui=a(Pc,"A",{href:!0});var Ew=i(ui);af=o(Ew,"BertTokenizerFast"),Ew.forEach(n),lf=o(Pc,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Pc.forEach(n),df=d(Nn),ts=a(Nn,"P",{});var zu=i(ts);cf=o(zu,"Refer to superclass "),pi=a(zu,"A",{href:!0});var Mw=i(pi);uf=o(Mw,"BertTokenizerFast"),Mw.forEach(n),pf=o(zu," for usage examples and documentation concerning parameters."),zu.forEach(n),hf=d(Nn),wn=a(Nn,"DIV",{class:!0});var Ao=i(wn);v(os.$$.fragment,Ao),ff=d(Ao),Cl=a(Ao,"P",{});var zw=i(Cl);mf=o(zw,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),zw.forEach(n),gf=d(Ao),v(ss.$$.fragment,Ao),_f=d(Ao),tt=a(Ao,"P",{});var Zi=i(tt);Tf=o(Zi,"If "),jl=a(Zi,"CODE",{});var qw=i(jl);kf=o(qw,"token_ids_1"),qw.forEach(n),Ff=o(Zi," is "),xl=a(Zi,"CODE",{});var Pw=i(xl);vf=o(Pw,"None"),Pw.forEach(n),yf=o(Zi,", this method only returns the first portion of the mask (0s)."),Zi.forEach(n),Ao.forEach(n),Nn.forEach(n),Nc=d(s),ot=a(s,"H2",{class:!0});var qu=i(ot);Vt=a(qu,"A",{id:!0,class:!0,href:!0});var Cw=i(Vt);Ll=a(Cw,"SPAN",{});var jw=i(Ll);v(rs.$$.fragment,jw),jw.forEach(n),Cw.forEach(n),wf=d(qu),Ol=a(qu,"SPAN",{});var xw=i(Ol);bf=o(xw,"Funnel specific outputs"),xw.forEach(n),qu.forEach(n),Ic=d(s),st=a(s,"DIV",{class:!0});var Pu=i(st);v(as.$$.fragment,Pu),$f=d(Pu),is=a(Pu,"P",{});var Cu=i(is);Ef=o(Cu,"Output type of "),hi=a(Cu,"A",{href:!0});var Lw=i(hi);Mf=o(Lw,"FunnelForPreTraining"),Lw.forEach(n),zf=o(Cu,"."),Cu.forEach(n),Pu.forEach(n),Sc=d(s),rt=a(s,"DIV",{class:!0});var ju=i(rt);v(ls.$$.fragment,ju),qf=d(ju),ds=a(ju,"P",{});var xu=i(ds);Pf=o(xu,"Output type of "),fi=a(xu,"A",{href:!0});var Ow=i(fi);Cf=o(Ow,"FunnelForPreTraining"),Ow.forEach(n),jf=o(xu,"."),xu.forEach(n),ju.forEach(n),Bc=d(s),at=a(s,"H2",{class:!0});var Lu=i(at);Yt=a(Lu,"A",{id:!0,class:!0,href:!0});var Dw=i(Yt);Dl=a(Dw,"SPAN",{});var Aw=i(Dl);v(cs.$$.fragment,Aw),Aw.forEach(n),Dw.forEach(n),xf=d(Lu),Al=a(Lu,"SPAN",{});var Nw=i(Al);Lf=o(Nw,"FunnelBaseModel"),Nw.forEach(n),Lu.forEach(n),Wc=d(s),Qe=a(s,"DIV",{class:!0});var bn=i(Qe);v(us.$$.fragment,bn),Of=d(bn),Nl=a(bn,"P",{});var Iw=i(Nl);Df=o(Iw,`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),Iw.forEach(n),Af=d(bn),ps=a(bn,"P",{});var Ou=i(ps);Nf=o(Ou,"The Funnel Transformer model was proposed in "),hs=a(Ou,"A",{href:!0,rel:!0});var Sw=i(hs);If=o(Sw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Sw.forEach(n),Sf=o(Ou," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Ou.forEach(n),Bf=d(bn),fs=a(bn,"P",{});var Du=i(fs);Wf=o(Du,"This model inherits from "),mi=a(Du,"A",{href:!0});var Bw=i(mi);Qf=o(Bw,"PreTrainedModel"),Bw.forEach(n),Uf=o(Du,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Du.forEach(n),Rf=d(bn),ms=a(bn,"P",{});var Au=i(ms);Hf=o(Au,"This model is also a PyTorch "),gs=a(Au,"A",{href:!0,rel:!0});var Ww=i(gs);Vf=o(Ww,"torch.nn.Module"),Ww.forEach(n),Yf=o(Au,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Au.forEach(n),Kf=d(bn),nn=a(bn,"DIV",{class:!0});var In=i(nn);v(_s.$$.fragment,In),Gf=d(In),it=a(In,"P",{});var Xi=i(it);Zf=o(Xi,"The "),gi=a(Xi,"A",{href:!0});var Qw=i(gi);Xf=o(Qw,"FunnelBaseModel"),Qw.forEach(n),Jf=o(Xi," forward method, overrides the "),Il=a(Xi,"CODE",{});var Uw=i(Il);em=o(Uw,"__call__"),Uw.forEach(n),nm=o(Xi," special method."),Xi.forEach(n),tm=d(In),v(Kt.$$.fragment,In),om=d(In),Sl=a(In,"P",{});var Rw=i(Sl);sm=o(Rw,"Example:"),Rw.forEach(n),rm=d(In),v(Ts.$$.fragment,In),In.forEach(n),bn.forEach(n),Qc=d(s),lt=a(s,"H2",{class:!0});var Nu=i(lt);Gt=a(Nu,"A",{id:!0,class:!0,href:!0});var Hw=i(Gt);Bl=a(Hw,"SPAN",{});var Vw=i(Bl);v(ks.$$.fragment,Vw),Vw.forEach(n),Hw.forEach(n),am=d(Nu),Wl=a(Nu,"SPAN",{});var Yw=i(Wl);im=o(Yw,"FunnelModel"),Yw.forEach(n),Nu.forEach(n),Uc=d(s),Ue=a(s,"DIV",{class:!0});var $n=i(Ue);v(Fs.$$.fragment,$n),lm=d($n),Ql=a($n,"P",{});var Kw=i(Ql);dm=o(Kw,"The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),Kw.forEach(n),cm=d($n),vs=a($n,"P",{});var Iu=i(vs);um=o(Iu,"The Funnel Transformer model was proposed in "),ys=a(Iu,"A",{href:!0,rel:!0});var Gw=i(ys);pm=o(Gw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Gw.forEach(n),hm=o(Iu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Iu.forEach(n),fm=d($n),ws=a($n,"P",{});var Su=i(ws);mm=o(Su,"This model inherits from "),_i=a(Su,"A",{href:!0});var Zw=i(_i);gm=o(Zw,"PreTrainedModel"),Zw.forEach(n),_m=o(Su,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Su.forEach(n),Tm=d($n),bs=a($n,"P",{});var Bu=i(bs);km=o(Bu,"This model is also a PyTorch "),$s=a(Bu,"A",{href:!0,rel:!0});var Xw=i($s);Fm=o(Xw,"torch.nn.Module"),Xw.forEach(n),vm=o(Bu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Bu.forEach(n),ym=d($n),tn=a($n,"DIV",{class:!0});var Sn=i(tn);v(Es.$$.fragment,Sn),wm=d(Sn),dt=a(Sn,"P",{});var Ji=i(dt);bm=o(Ji,"The "),Ti=a(Ji,"A",{href:!0});var Jw=i(Ti);$m=o(Jw,"FunnelModel"),Jw.forEach(n),Em=o(Ji," forward method, overrides the "),Ul=a(Ji,"CODE",{});var eb=i(Ul);Mm=o(eb,"__call__"),eb.forEach(n),zm=o(Ji," special method."),Ji.forEach(n),qm=d(Sn),v(Zt.$$.fragment,Sn),Pm=d(Sn),Rl=a(Sn,"P",{});var nb=i(Rl);Cm=o(nb,"Example:"),nb.forEach(n),jm=d(Sn),v(Ms.$$.fragment,Sn),Sn.forEach(n),$n.forEach(n),Rc=d(s),ct=a(s,"H2",{class:!0});var Wu=i(ct);Xt=a(Wu,"A",{id:!0,class:!0,href:!0});var tb=i(Xt);Hl=a(tb,"SPAN",{});var ob=i(Hl);v(zs.$$.fragment,ob),ob.forEach(n),tb.forEach(n),xm=d(Wu),Vl=a(Wu,"SPAN",{});var sb=i(Vl);Lm=o(sb,"FunnelModelForPreTraining"),sb.forEach(n),Wu.forEach(n),Hc=d(s),ut=a(s,"DIV",{class:!0});var Qu=i(ut);v(qs.$$.fragment,Qu),Om=d(Qu),on=a(Qu,"DIV",{class:!0});var Bn=i(on);v(Ps.$$.fragment,Bn),Dm=d(Bn),pt=a(Bn,"P",{});var el=i(pt);Am=o(el,"The "),ki=a(el,"A",{href:!0});var rb=i(ki);Nm=o(rb,"FunnelForPreTraining"),rb.forEach(n),Im=o(el," forward method, overrides the "),Yl=a(el,"CODE",{});var ab=i(Yl);Sm=o(ab,"__call__"),ab.forEach(n),Bm=o(el," special method."),el.forEach(n),Wm=d(Bn),v(Jt.$$.fragment,Bn),Qm=d(Bn),Kl=a(Bn,"P",{});var ib=i(Kl);Um=o(ib,"Examples:"),ib.forEach(n),Rm=d(Bn),v(Cs.$$.fragment,Bn),Bn.forEach(n),Qu.forEach(n),Vc=d(s),ht=a(s,"H2",{class:!0});var Uu=i(ht);eo=a(Uu,"A",{id:!0,class:!0,href:!0});var lb=i(eo);Gl=a(lb,"SPAN",{});var db=i(Gl);v(js.$$.fragment,db),db.forEach(n),lb.forEach(n),Hm=d(Uu),Zl=a(Uu,"SPAN",{});var cb=i(Zl);Vm=o(cb,"FunnelForMaskedLM"),cb.forEach(n),Uu.forEach(n),Yc=d(s),Re=a(s,"DIV",{class:!0});var En=i(Re);v(xs.$$.fragment,En),Ym=d(En),Ls=a(En,"P",{});var Ru=i(Ls);Km=o(Ru,"Funnel Transformer Model with a "),Xl=a(Ru,"CODE",{});var ub=i(Xl);Gm=o(ub,"language modeling"),ub.forEach(n),Zm=o(Ru," head on top."),Ru.forEach(n),Xm=d(En),Os=a(En,"P",{});var Hu=i(Os);Jm=o(Hu,"The Funnel Transformer model was proposed in "),Ds=a(Hu,"A",{href:!0,rel:!0});var pb=i(Ds);eg=o(pb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),pb.forEach(n),ng=o(Hu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Hu.forEach(n),tg=d(En),As=a(En,"P",{});var Vu=i(As);og=o(Vu,"This model inherits from "),Fi=a(Vu,"A",{href:!0});var hb=i(Fi);sg=o(hb,"PreTrainedModel"),hb.forEach(n),rg=o(Vu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Vu.forEach(n),ag=d(En),Ns=a(En,"P",{});var Yu=i(Ns);ig=o(Yu,"This model is also a PyTorch "),Is=a(Yu,"A",{href:!0,rel:!0});var fb=i(Is);lg=o(fb,"torch.nn.Module"),fb.forEach(n),dg=o(Yu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Yu.forEach(n),cg=d(En),Ge=a(En,"DIV",{class:!0});var Mn=i(Ge);v(Ss.$$.fragment,Mn),ug=d(Mn),ft=a(Mn,"P",{});var nl=i(ft);pg=o(nl,"The "),vi=a(nl,"A",{href:!0});var mb=i(vi);hg=o(mb,"FunnelForMaskedLM"),mb.forEach(n),fg=o(nl," forward method, overrides the "),Jl=a(nl,"CODE",{});var gb=i(Jl);mg=o(gb,"__call__"),gb.forEach(n),gg=o(nl," special method."),nl.forEach(n),_g=d(Mn),v(no.$$.fragment,Mn),Tg=d(Mn),ed=a(Mn,"P",{});var _b=i(ed);kg=o(_b,"Example:"),_b.forEach(n),Fg=d(Mn),v(Bs.$$.fragment,Mn),vg=d(Mn),v(Ws.$$.fragment,Mn),Mn.forEach(n),En.forEach(n),Kc=d(s),mt=a(s,"H2",{class:!0});var Ku=i(mt);to=a(Ku,"A",{id:!0,class:!0,href:!0});var Tb=i(to);nd=a(Tb,"SPAN",{});var kb=i(nd);v(Qs.$$.fragment,kb),kb.forEach(n),Tb.forEach(n),yg=d(Ku),td=a(Ku,"SPAN",{});var Fb=i(td);wg=o(Fb,"FunnelForSequenceClassification"),Fb.forEach(n),Ku.forEach(n),Gc=d(s),He=a(s,"DIV",{class:!0});var zn=i(He);v(Us.$$.fragment,zn),bg=d(zn),od=a(zn,"P",{});var vb=i(od);$g=o(vb,`Funnel Transformer Model with a sequence classification/regression head on top (two linear layer on top of the
first timestep of the last hidden state) e.g. for GLUE tasks.`),vb.forEach(n),Eg=d(zn),Rs=a(zn,"P",{});var Gu=i(Rs);Mg=o(Gu,"The Funnel Transformer model was proposed in "),Hs=a(Gu,"A",{href:!0,rel:!0});var yb=i(Hs);zg=o(yb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),yb.forEach(n),qg=o(Gu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Gu.forEach(n),Pg=d(zn),Vs=a(zn,"P",{});var Zu=i(Vs);Cg=o(Zu,"This model inherits from "),yi=a(Zu,"A",{href:!0});var wb=i(yi);jg=o(wb,"PreTrainedModel"),wb.forEach(n),xg=o(Zu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Zu.forEach(n),Lg=d(zn),Ys=a(zn,"P",{});var Xu=i(Ys);Og=o(Xu,"This model is also a PyTorch "),Ks=a(Xu,"A",{href:!0,rel:!0});var bb=i(Ks);Dg=o(bb,"torch.nn.Module"),bb.forEach(n),Ag=o(Xu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Xu.forEach(n),Ng=d(zn),Pe=a(zn,"DIV",{class:!0});var We=i(Pe);v(Gs.$$.fragment,We),Ig=d(We),gt=a(We,"P",{});var tl=i(gt);Sg=o(tl,"The "),wi=a(tl,"A",{href:!0});var $b=i(wi);Bg=o($b,"FunnelForSequenceClassification"),$b.forEach(n),Wg=o(tl," forward method, overrides the "),sd=a(tl,"CODE",{});var Eb=i(sd);Qg=o(Eb,"__call__"),Eb.forEach(n),Ug=o(tl," special method."),tl.forEach(n),Rg=d(We),v(oo.$$.fragment,We),Hg=d(We),rd=a(We,"P",{});var Mb=i(rd);Vg=o(Mb,"Example of single-label classification:"),Mb.forEach(n),Yg=d(We),v(Zs.$$.fragment,We),Kg=d(We),v(Xs.$$.fragment,We),Gg=d(We),ad=a(We,"P",{});var zb=i(ad);Zg=o(zb,"Example of multi-label classification:"),zb.forEach(n),Xg=d(We),v(Js.$$.fragment,We),Jg=d(We),v(er.$$.fragment,We),We.forEach(n),zn.forEach(n),Zc=d(s),_t=a(s,"H2",{class:!0});var Ju=i(_t);so=a(Ju,"A",{id:!0,class:!0,href:!0});var qb=i(so);id=a(qb,"SPAN",{});var Pb=i(id);v(nr.$$.fragment,Pb),Pb.forEach(n),qb.forEach(n),e_=d(Ju),ld=a(Ju,"SPAN",{});var Cb=i(ld);n_=o(Cb,"FunnelForMultipleChoice"),Cb.forEach(n),Ju.forEach(n),Xc=d(s),Ve=a(s,"DIV",{class:!0});var qn=i(Ve);v(tr.$$.fragment,qn),t_=d(qn),dd=a(qn,"P",{});var jb=i(dd);o_=o(jb,`Funnel Transformer Model with a multiple choice classification head on top (two linear layer on top of the first
timestep of the last hidden state, and a softmax) e.g. for RocStories/SWAG tasks.`),jb.forEach(n),s_=d(qn),or=a(qn,"P",{});var ep=i(or);r_=o(ep,"The Funnel Transformer model was proposed in "),sr=a(ep,"A",{href:!0,rel:!0});var xb=i(sr);a_=o(xb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),xb.forEach(n),i_=o(ep," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),ep.forEach(n),l_=d(qn),rr=a(qn,"P",{});var np=i(rr);d_=o(np,"This model inherits from "),bi=a(np,"A",{href:!0});var Lb=i(bi);c_=o(Lb,"PreTrainedModel"),Lb.forEach(n),u_=o(np,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),np.forEach(n),p_=d(qn),ar=a(qn,"P",{});var tp=i(ar);h_=o(tp,"This model is also a PyTorch "),ir=a(tp,"A",{href:!0,rel:!0});var Ob=i(ir);f_=o(Ob,"torch.nn.Module"),Ob.forEach(n),m_=o(tp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),tp.forEach(n),g_=d(qn),sn=a(qn,"DIV",{class:!0});var Wn=i(sn);v(lr.$$.fragment,Wn),__=d(Wn),Tt=a(Wn,"P",{});var ol=i(Tt);T_=o(ol,"The "),$i=a(ol,"A",{href:!0});var Db=i($i);k_=o(Db,"FunnelForMultipleChoice"),Db.forEach(n),F_=o(ol," forward method, overrides the "),cd=a(ol,"CODE",{});var Ab=i(cd);v_=o(Ab,"__call__"),Ab.forEach(n),y_=o(ol," special method."),ol.forEach(n),w_=d(Wn),v(ro.$$.fragment,Wn),b_=d(Wn),ud=a(Wn,"P",{});var Nb=i(ud);$_=o(Nb,"Example:"),Nb.forEach(n),E_=d(Wn),v(dr.$$.fragment,Wn),Wn.forEach(n),qn.forEach(n),Jc=d(s),kt=a(s,"H2",{class:!0});var op=i(kt);ao=a(op,"A",{id:!0,class:!0,href:!0});var Ib=i(ao);pd=a(Ib,"SPAN",{});var Sb=i(pd);v(cr.$$.fragment,Sb),Sb.forEach(n),Ib.forEach(n),M_=d(op),hd=a(op,"SPAN",{});var Bb=i(hd);z_=o(Bb,"FunnelForTokenClassification"),Bb.forEach(n),op.forEach(n),eu=d(s),Ye=a(s,"DIV",{class:!0});var Pn=i(Ye);v(ur.$$.fragment,Pn),q_=d(Pn),fd=a(Pn,"P",{});var Wb=i(fd);P_=o(Wb,`Funnel Transformer Model with a token classification head on top (a linear layer on top of the hidden-states
output) e.g. for Named-Entity-Recognition (NER) tasks.`),Wb.forEach(n),C_=d(Pn),pr=a(Pn,"P",{});var sp=i(pr);j_=o(sp,"The Funnel Transformer model was proposed in "),hr=a(sp,"A",{href:!0,rel:!0});var Qb=i(hr);x_=o(Qb,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Qb.forEach(n),L_=o(sp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),sp.forEach(n),O_=d(Pn),fr=a(Pn,"P",{});var rp=i(fr);D_=o(rp,"This model inherits from "),Ei=a(rp,"A",{href:!0});var Ub=i(Ei);A_=o(Ub,"PreTrainedModel"),Ub.forEach(n),N_=o(rp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),rp.forEach(n),I_=d(Pn),mr=a(Pn,"P",{});var ap=i(mr);S_=o(ap,"This model is also a PyTorch "),gr=a(ap,"A",{href:!0,rel:!0});var Rb=i(gr);B_=o(Rb,"torch.nn.Module"),Rb.forEach(n),W_=o(ap,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ap.forEach(n),Q_=d(Pn),Ze=a(Pn,"DIV",{class:!0});var Cn=i(Ze);v(_r.$$.fragment,Cn),U_=d(Cn),Ft=a(Cn,"P",{});var sl=i(Ft);R_=o(sl,"The "),Mi=a(sl,"A",{href:!0});var Hb=i(Mi);H_=o(Hb,"FunnelForTokenClassification"),Hb.forEach(n),V_=o(sl," forward method, overrides the "),md=a(sl,"CODE",{});var Vb=i(md);Y_=o(Vb,"__call__"),Vb.forEach(n),K_=o(sl," special method."),sl.forEach(n),G_=d(Cn),v(io.$$.fragment,Cn),Z_=d(Cn),gd=a(Cn,"P",{});var Yb=i(gd);X_=o(Yb,"Example:"),Yb.forEach(n),J_=d(Cn),v(Tr.$$.fragment,Cn),eT=d(Cn),v(kr.$$.fragment,Cn),Cn.forEach(n),Pn.forEach(n),nu=d(s),vt=a(s,"H2",{class:!0});var ip=i(vt);lo=a(ip,"A",{id:!0,class:!0,href:!0});var Kb=i(lo);_d=a(Kb,"SPAN",{});var Gb=i(_d);v(Fr.$$.fragment,Gb),Gb.forEach(n),Kb.forEach(n),nT=d(ip),Td=a(ip,"SPAN",{});var Zb=i(Td);tT=o(Zb,"FunnelForQuestionAnswering"),Zb.forEach(n),ip.forEach(n),tu=d(s),Ke=a(s,"DIV",{class:!0});var jn=i(Ke);v(vr.$$.fragment,jn),oT=d(jn),yt=a(jn,"P",{});var rl=i(yt);sT=o(rl,`Funnel Transformer Model with a span classification head on top for extractive question-answering tasks like SQuAD
(a linear layer on top of the hidden-states output to compute `),kd=a(rl,"CODE",{});var Xb=i(kd);rT=o(Xb,"span start logits"),Xb.forEach(n),aT=o(rl," and "),Fd=a(rl,"CODE",{});var Jb=i(Fd);iT=o(Jb,"span end logits"),Jb.forEach(n),lT=o(rl,")."),rl.forEach(n),dT=d(jn),yr=a(jn,"P",{});var lp=i(yr);cT=o(lp,"The Funnel Transformer model was proposed in "),wr=a(lp,"A",{href:!0,rel:!0});var e1=i(wr);uT=o(e1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),e1.forEach(n),pT=o(lp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),lp.forEach(n),hT=d(jn),br=a(jn,"P",{});var dp=i(br);fT=o(dp,"This model inherits from "),zi=a(dp,"A",{href:!0});var n1=i(zi);mT=o(n1,"PreTrainedModel"),n1.forEach(n),gT=o(dp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),dp.forEach(n),_T=d(jn),$r=a(jn,"P",{});var cp=i($r);TT=o(cp,"This model is also a PyTorch "),Er=a(cp,"A",{href:!0,rel:!0});var t1=i(Er);kT=o(t1,"torch.nn.Module"),t1.forEach(n),FT=o(cp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),cp.forEach(n),vT=d(jn),Xe=a(jn,"DIV",{class:!0});var xn=i(Xe);v(Mr.$$.fragment,xn),yT=d(xn),wt=a(xn,"P",{});var al=i(wt);wT=o(al,"The "),qi=a(al,"A",{href:!0});var o1=i(qi);bT=o(o1,"FunnelForQuestionAnswering"),o1.forEach(n),$T=o(al," forward method, overrides the "),vd=a(al,"CODE",{});var s1=i(vd);ET=o(s1,"__call__"),s1.forEach(n),MT=o(al," special method."),al.forEach(n),zT=d(xn),v(co.$$.fragment,xn),qT=d(xn),yd=a(xn,"P",{});var r1=i(yd);PT=o(r1,"Example:"),r1.forEach(n),CT=d(xn),v(zr.$$.fragment,xn),jT=d(xn),v(qr.$$.fragment,xn),xn.forEach(n),jn.forEach(n),ou=d(s),bt=a(s,"H2",{class:!0});var up=i(bt);uo=a(up,"A",{id:!0,class:!0,href:!0});var a1=i(uo);wd=a(a1,"SPAN",{});var i1=i(wd);v(Pr.$$.fragment,i1),i1.forEach(n),a1.forEach(n),xT=d(up),bd=a(up,"SPAN",{});var l1=i(bd);LT=o(l1,"TFFunnelBaseModel"),l1.forEach(n),up.forEach(n),su=d(s),Le=a(s,"DIV",{class:!0});var fn=i(Le);v(Cr.$$.fragment,fn),OT=d(fn),$d=a(fn,"P",{});var d1=i($d);DT=o(d1,`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),d1.forEach(n),AT=d(fn),jr=a(fn,"P",{});var pp=i(jr);NT=o(pp,"The Funnel Transformer model was proposed in "),xr=a(pp,"A",{href:!0,rel:!0});var c1=i(xr);IT=o(c1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),c1.forEach(n),ST=o(pp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),pp.forEach(n),BT=d(fn),Lr=a(fn,"P",{});var hp=i(Lr);WT=o(hp,"This model inherits from "),Pi=a(hp,"A",{href:!0});var u1=i(Pi);QT=o(u1,"TFPreTrainedModel"),u1.forEach(n),UT=o(hp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),hp.forEach(n),RT=d(fn),Or=a(fn,"P",{});var fp=i(Or);HT=o(fp,"This model is also a "),Dr=a(fp,"A",{href:!0,rel:!0});var p1=i(Dr);VT=o(p1,"tf.keras.Model"),p1.forEach(n),YT=o(fp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),fp.forEach(n),KT=d(fn),v(po.$$.fragment,fn),GT=d(fn),rn=a(fn,"DIV",{class:!0});var Qn=i(rn);v(Ar.$$.fragment,Qn),ZT=d(Qn),$t=a(Qn,"P",{});var il=i($t);XT=o(il,"The "),Ci=a(il,"A",{href:!0});var h1=i(Ci);JT=o(h1,"TFFunnelBaseModel"),h1.forEach(n),ek=o(il," forward method, overrides the "),Ed=a(il,"CODE",{});var f1=i(Ed);nk=o(f1,"__call__"),f1.forEach(n),tk=o(il," special method."),il.forEach(n),ok=d(Qn),v(ho.$$.fragment,Qn),sk=d(Qn),Md=a(Qn,"P",{});var m1=i(Md);rk=o(m1,"Example:"),m1.forEach(n),ak=d(Qn),v(Nr.$$.fragment,Qn),Qn.forEach(n),fn.forEach(n),ru=d(s),Et=a(s,"H2",{class:!0});var mp=i(Et);fo=a(mp,"A",{id:!0,class:!0,href:!0});var g1=i(fo);zd=a(g1,"SPAN",{});var _1=i(zd);v(Ir.$$.fragment,_1),_1.forEach(n),g1.forEach(n),ik=d(mp),qd=a(mp,"SPAN",{});var T1=i(qd);lk=o(T1,"TFFunnelModel"),T1.forEach(n),mp.forEach(n),au=d(s),Oe=a(s,"DIV",{class:!0});var mn=i(Oe);v(Sr.$$.fragment,mn),dk=d(mn),Pd=a(mn,"P",{});var k1=i(Pd);ck=o(k1,"The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),k1.forEach(n),uk=d(mn),Br=a(mn,"P",{});var gp=i(Br);pk=o(gp,"The Funnel Transformer model was proposed in "),Wr=a(gp,"A",{href:!0,rel:!0});var F1=i(Wr);hk=o(F1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),F1.forEach(n),fk=o(gp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),gp.forEach(n),mk=d(mn),Qr=a(mn,"P",{});var _p=i(Qr);gk=o(_p,"This model inherits from "),ji=a(_p,"A",{href:!0});var v1=i(ji);_k=o(v1,"TFPreTrainedModel"),v1.forEach(n),Tk=o(_p,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),_p.forEach(n),kk=d(mn),Ur=a(mn,"P",{});var Tp=i(Ur);Fk=o(Tp,"This model is also a "),Rr=a(Tp,"A",{href:!0,rel:!0});var y1=i(Rr);vk=o(y1,"tf.keras.Model"),y1.forEach(n),yk=o(Tp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Tp.forEach(n),wk=d(mn),v(mo.$$.fragment,mn),bk=d(mn),an=a(mn,"DIV",{class:!0});var Un=i(an);v(Hr.$$.fragment,Un),$k=d(Un),Mt=a(Un,"P",{});var ll=i(Mt);Ek=o(ll,"The "),xi=a(ll,"A",{href:!0});var w1=i(xi);Mk=o(w1,"TFFunnelModel"),w1.forEach(n),zk=o(ll," forward method, overrides the "),Cd=a(ll,"CODE",{});var b1=i(Cd);qk=o(b1,"__call__"),b1.forEach(n),Pk=o(ll," special method."),ll.forEach(n),Ck=d(Un),v(go.$$.fragment,Un),jk=d(Un),jd=a(Un,"P",{});var $1=i(jd);xk=o($1,"Example:"),$1.forEach(n),Lk=d(Un),v(Vr.$$.fragment,Un),Un.forEach(n),mn.forEach(n),iu=d(s),zt=a(s,"H2",{class:!0});var kp=i(zt);_o=a(kp,"A",{id:!0,class:!0,href:!0});var E1=i(_o);xd=a(E1,"SPAN",{});var M1=i(xd);v(Yr.$$.fragment,M1),M1.forEach(n),E1.forEach(n),Ok=d(kp),Ld=a(kp,"SPAN",{});var z1=i(Ld);Dk=o(z1,"TFFunnelModelForPreTraining"),z1.forEach(n),kp.forEach(n),lu=d(s),De=a(s,"DIV",{class:!0});var gn=i(De);v(Kr.$$.fragment,gn),Ak=d(gn),Od=a(gn,"P",{});var q1=i(Od);Nk=o(q1,"Funnel model with a binary classification head on top as used during pretraining for identifying generated tokens."),q1.forEach(n),Ik=d(gn),Gr=a(gn,"P",{});var Fp=i(Gr);Sk=o(Fp,"The Funnel Transformer model was proposed in "),Zr=a(Fp,"A",{href:!0,rel:!0});var P1=i(Zr);Bk=o(P1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),P1.forEach(n),Wk=o(Fp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Fp.forEach(n),Qk=d(gn),Xr=a(gn,"P",{});var vp=i(Xr);Uk=o(vp,"This model inherits from "),Li=a(vp,"A",{href:!0});var C1=i(Li);Rk=o(C1,"TFPreTrainedModel"),C1.forEach(n),Hk=o(vp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),vp.forEach(n),Vk=d(gn),Jr=a(gn,"P",{});var yp=i(Jr);Yk=o(yp,"This model is also a "),ea=a(yp,"A",{href:!0,rel:!0});var j1=i(ea);Kk=o(j1,"tf.keras.Model"),j1.forEach(n),Gk=o(yp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),yp.forEach(n),Zk=d(gn),v(To.$$.fragment,gn),Xk=d(gn),ln=a(gn,"DIV",{class:!0});var Rn=i(ln);v(na.$$.fragment,Rn),Jk=d(Rn),qt=a(Rn,"P",{});var dl=i(qt);eF=o(dl,"The "),Oi=a(dl,"A",{href:!0});var x1=i(Oi);nF=o(x1,"TFFunnelForPreTraining"),x1.forEach(n),tF=o(dl," forward method, overrides the "),Dd=a(dl,"CODE",{});var L1=i(Dd);oF=o(L1,"__call__"),L1.forEach(n),sF=o(dl," special method."),dl.forEach(n),rF=d(Rn),v(ko.$$.fragment,Rn),aF=d(Rn),Ad=a(Rn,"P",{});var O1=i(Ad);iF=o(O1,"Examples:"),O1.forEach(n),lF=d(Rn),v(ta.$$.fragment,Rn),Rn.forEach(n),gn.forEach(n),du=d(s),Pt=a(s,"H2",{class:!0});var wp=i(Pt);Fo=a(wp,"A",{id:!0,class:!0,href:!0});var D1=i(Fo);Nd=a(D1,"SPAN",{});var A1=i(Nd);v(oa.$$.fragment,A1),A1.forEach(n),D1.forEach(n),dF=d(wp),Id=a(wp,"SPAN",{});var N1=i(Id);cF=o(N1,"TFFunnelForMaskedLM"),N1.forEach(n),wp.forEach(n),cu=d(s),Ae=a(s,"DIV",{class:!0});var _n=i(Ae);v(sa.$$.fragment,_n),uF=d(_n),ra=a(_n,"P",{});var bp=i(ra);pF=o(bp,"Funnel Model with a "),Sd=a(bp,"CODE",{});var I1=i(Sd);hF=o(I1,"language modeling"),I1.forEach(n),fF=o(bp," head on top."),bp.forEach(n),mF=d(_n),aa=a(_n,"P",{});var $p=i(aa);gF=o($p,"The Funnel Transformer model was proposed in "),ia=a($p,"A",{href:!0,rel:!0});var S1=i(ia);_F=o(S1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),S1.forEach(n),TF=o($p," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),$p.forEach(n),kF=d(_n),la=a(_n,"P",{});var Ep=i(la);FF=o(Ep,"This model inherits from "),Di=a(Ep,"A",{href:!0});var B1=i(Di);vF=o(B1,"TFPreTrainedModel"),B1.forEach(n),yF=o(Ep,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ep.forEach(n),wF=d(_n),da=a(_n,"P",{});var Mp=i(da);bF=o(Mp,"This model is also a "),ca=a(Mp,"A",{href:!0,rel:!0});var W1=i(ca);$F=o(W1,"tf.keras.Model"),W1.forEach(n),EF=o(Mp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Mp.forEach(n),MF=d(_n),v(vo.$$.fragment,_n),zF=d(_n),dn=a(_n,"DIV",{class:!0});var Hn=i(dn);v(ua.$$.fragment,Hn),qF=d(Hn),Ct=a(Hn,"P",{});var cl=i(Ct);PF=o(cl,"The "),Ai=a(cl,"A",{href:!0});var Q1=i(Ai);CF=o(Q1,"TFFunnelForMaskedLM"),Q1.forEach(n),jF=o(cl," forward method, overrides the "),Bd=a(cl,"CODE",{});var U1=i(Bd);xF=o(U1,"__call__"),U1.forEach(n),LF=o(cl," special method."),cl.forEach(n),OF=d(Hn),v(yo.$$.fragment,Hn),DF=d(Hn),Wd=a(Hn,"P",{});var R1=i(Wd);AF=o(R1,"Example:"),R1.forEach(n),NF=d(Hn),v(pa.$$.fragment,Hn),Hn.forEach(n),_n.forEach(n),uu=d(s),jt=a(s,"H2",{class:!0});var zp=i(jt);wo=a(zp,"A",{id:!0,class:!0,href:!0});var H1=i(wo);Qd=a(H1,"SPAN",{});var V1=i(Qd);v(ha.$$.fragment,V1),V1.forEach(n),H1.forEach(n),IF=d(zp),Ud=a(zp,"SPAN",{});var Y1=i(Ud);SF=o(Y1,"TFFunnelForSequenceClassification"),Y1.forEach(n),zp.forEach(n),pu=d(s),Ne=a(s,"DIV",{class:!0});var Tn=i(Ne);v(fa.$$.fragment,Tn),BF=d(Tn),Rd=a(Tn,"P",{});var K1=i(Rd);WF=o(K1,`Funnel Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
output) e.g. for GLUE tasks.`),K1.forEach(n),QF=d(Tn),ma=a(Tn,"P",{});var qp=i(ma);UF=o(qp,"The Funnel Transformer model was proposed in "),ga=a(qp,"A",{href:!0,rel:!0});var G1=i(ga);RF=o(G1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),G1.forEach(n),HF=o(qp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),qp.forEach(n),VF=d(Tn),_a=a(Tn,"P",{});var Pp=i(_a);YF=o(Pp,"This model inherits from "),Ni=a(Pp,"A",{href:!0});var Z1=i(Ni);KF=o(Z1,"TFPreTrainedModel"),Z1.forEach(n),GF=o(Pp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Pp.forEach(n),ZF=d(Tn),Ta=a(Tn,"P",{});var Cp=i(Ta);XF=o(Cp,"This model is also a "),ka=a(Cp,"A",{href:!0,rel:!0});var X1=i(ka);JF=o(X1,"tf.keras.Model"),X1.forEach(n),ev=o(Cp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Cp.forEach(n),nv=d(Tn),v(bo.$$.fragment,Tn),tv=d(Tn),cn=a(Tn,"DIV",{class:!0});var Vn=i(cn);v(Fa.$$.fragment,Vn),ov=d(Vn),xt=a(Vn,"P",{});var ul=i(xt);sv=o(ul,"The "),Ii=a(ul,"A",{href:!0});var J1=i(Ii);rv=o(J1,"TFFunnelForSequenceClassification"),J1.forEach(n),av=o(ul," forward method, overrides the "),Hd=a(ul,"CODE",{});var e$=i(Hd);iv=o(e$,"__call__"),e$.forEach(n),lv=o(ul," special method."),ul.forEach(n),dv=d(Vn),v($o.$$.fragment,Vn),cv=d(Vn),Vd=a(Vn,"P",{});var n$=i(Vd);uv=o(n$,"Example:"),n$.forEach(n),pv=d(Vn),v(va.$$.fragment,Vn),Vn.forEach(n),Tn.forEach(n),hu=d(s),Lt=a(s,"H2",{class:!0});var jp=i(Lt);Eo=a(jp,"A",{id:!0,class:!0,href:!0});var t$=i(Eo);Yd=a(t$,"SPAN",{});var o$=i(Yd);v(ya.$$.fragment,o$),o$.forEach(n),t$.forEach(n),hv=d(jp),Kd=a(jp,"SPAN",{});var s$=i(Kd);fv=o(s$,"TFFunnelForMultipleChoice"),s$.forEach(n),jp.forEach(n),fu=d(s),Ie=a(s,"DIV",{class:!0});var kn=i(Ie);v(wa.$$.fragment,kn),mv=d(kn),Gd=a(kn,"P",{});var r$=i(Gd);gv=o(r$,`Funnel Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),r$.forEach(n),_v=d(kn),ba=a(kn,"P",{});var xp=i(ba);Tv=o(xp,"The Funnel Transformer model was proposed in "),$a=a(xp,"A",{href:!0,rel:!0});var a$=i($a);kv=o(a$,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),a$.forEach(n),Fv=o(xp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),xp.forEach(n),vv=d(kn),Ea=a(kn,"P",{});var Lp=i(Ea);yv=o(Lp,"This model inherits from "),Si=a(Lp,"A",{href:!0});var i$=i(Si);wv=o(i$,"TFPreTrainedModel"),i$.forEach(n),bv=o(Lp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Lp.forEach(n),$v=d(kn),Ma=a(kn,"P",{});var Op=i(Ma);Ev=o(Op,"This model is also a "),za=a(Op,"A",{href:!0,rel:!0});var l$=i(za);Mv=o(l$,"tf.keras.Model"),l$.forEach(n),zv=o(Op,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Op.forEach(n),qv=d(kn),v(Mo.$$.fragment,kn),Pv=d(kn),un=a(kn,"DIV",{class:!0});var Yn=i(un);v(qa.$$.fragment,Yn),Cv=d(Yn),Ot=a(Yn,"P",{});var pl=i(Ot);jv=o(pl,"The "),Bi=a(pl,"A",{href:!0});var d$=i(Bi);xv=o(d$,"TFFunnelForMultipleChoice"),d$.forEach(n),Lv=o(pl," forward method, overrides the "),Zd=a(pl,"CODE",{});var c$=i(Zd);Ov=o(c$,"__call__"),c$.forEach(n),Dv=o(pl," special method."),pl.forEach(n),Av=d(Yn),v(zo.$$.fragment,Yn),Nv=d(Yn),Xd=a(Yn,"P",{});var u$=i(Xd);Iv=o(u$,"Example:"),u$.forEach(n),Sv=d(Yn),v(Pa.$$.fragment,Yn),Yn.forEach(n),kn.forEach(n),mu=d(s),Dt=a(s,"H2",{class:!0});var Dp=i(Dt);qo=a(Dp,"A",{id:!0,class:!0,href:!0});var p$=i(qo);Jd=a(p$,"SPAN",{});var h$=i(Jd);v(Ca.$$.fragment,h$),h$.forEach(n),p$.forEach(n),Bv=d(Dp),ec=a(Dp,"SPAN",{});var f$=i(ec);Wv=o(f$,"TFFunnelForTokenClassification"),f$.forEach(n),Dp.forEach(n),gu=d(s),Se=a(s,"DIV",{class:!0});var Fn=i(Se);v(ja.$$.fragment,Fn),Qv=d(Fn),nc=a(Fn,"P",{});var m$=i(nc);Uv=o(m$,`Funnel Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),m$.forEach(n),Rv=d(Fn),xa=a(Fn,"P",{});var Ap=i(xa);Hv=o(Ap,"The Funnel Transformer model was proposed in "),La=a(Ap,"A",{href:!0,rel:!0});var g$=i(La);Vv=o(g$,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),g$.forEach(n),Yv=o(Ap," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Ap.forEach(n),Kv=d(Fn),Oa=a(Fn,"P",{});var Np=i(Oa);Gv=o(Np,"This model inherits from "),Wi=a(Np,"A",{href:!0});var _$=i(Wi);Zv=o(_$,"TFPreTrainedModel"),_$.forEach(n),Xv=o(Np,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Np.forEach(n),Jv=d(Fn),Da=a(Fn,"P",{});var Ip=i(Da);ey=o(Ip,"This model is also a "),Aa=a(Ip,"A",{href:!0,rel:!0});var T$=i(Aa);ny=o(T$,"tf.keras.Model"),T$.forEach(n),ty=o(Ip,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Ip.forEach(n),oy=d(Fn),v(Po.$$.fragment,Fn),sy=d(Fn),pn=a(Fn,"DIV",{class:!0});var Kn=i(pn);v(Na.$$.fragment,Kn),ry=d(Kn),At=a(Kn,"P",{});var hl=i(At);ay=o(hl,"The "),Qi=a(hl,"A",{href:!0});var k$=i(Qi);iy=o(k$,"TFFunnelForTokenClassification"),k$.forEach(n),ly=o(hl," forward method, overrides the "),tc=a(hl,"CODE",{});var F$=i(tc);dy=o(F$,"__call__"),F$.forEach(n),cy=o(hl," special method."),hl.forEach(n),uy=d(Kn),v(Co.$$.fragment,Kn),py=d(Kn),oc=a(Kn,"P",{});var v$=i(oc);hy=o(v$,"Example:"),v$.forEach(n),fy=d(Kn),v(Ia.$$.fragment,Kn),Kn.forEach(n),Fn.forEach(n),_u=d(s),Nt=a(s,"H2",{class:!0});var Sp=i(Nt);jo=a(Sp,"A",{id:!0,class:!0,href:!0});var y$=i(jo);sc=a(y$,"SPAN",{});var w$=i(sc);v(Sa.$$.fragment,w$),w$.forEach(n),y$.forEach(n),my=d(Sp),rc=a(Sp,"SPAN",{});var b$=i(rc);gy=o(b$,"TFFunnelForQuestionAnswering"),b$.forEach(n),Sp.forEach(n),Tu=d(s),Be=a(s,"DIV",{class:!0});var vn=i(Be);v(Ba.$$.fragment,vn),_y=d(vn),It=a(vn,"P",{});var fl=i(It);Ty=o(fl,`Funnel Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),ac=a(fl,"CODE",{});var $$=i(ac);ky=o($$,"span start logits"),$$.forEach(n),Fy=o(fl," and "),ic=a(fl,"CODE",{});var E$=i(ic);vy=o(E$,"span end logits"),E$.forEach(n),yy=o(fl,")."),fl.forEach(n),wy=d(vn),Wa=a(vn,"P",{});var Bp=i(Wa);by=o(Bp,"The Funnel Transformer model was proposed in "),Qa=a(Bp,"A",{href:!0,rel:!0});var M$=i(Qa);$y=o(M$,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),M$.forEach(n),Ey=o(Bp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Bp.forEach(n),My=d(vn),Ua=a(vn,"P",{});var Wp=i(Ua);zy=o(Wp,"This model inherits from "),Ui=a(Wp,"A",{href:!0});var z$=i(Ui);qy=o(z$,"TFPreTrainedModel"),z$.forEach(n),Py=o(Wp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Wp.forEach(n),Cy=d(vn),Ra=a(vn,"P",{});var Qp=i(Ra);jy=o(Qp,"This model is also a "),Ha=a(Qp,"A",{href:!0,rel:!0});var q$=i(Ha);xy=o(q$,"tf.keras.Model"),q$.forEach(n),Ly=o(Qp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Qp.forEach(n),Oy=d(vn),v(xo.$$.fragment,vn),Dy=d(vn),hn=a(vn,"DIV",{class:!0});var Gn=i(hn);v(Va.$$.fragment,Gn),Ay=d(Gn),St=a(Gn,"P",{});var ml=i(St);Ny=o(ml,"The "),Ri=a(ml,"A",{href:!0});var P$=i(Ri);Iy=o(P$,"TFFunnelForQuestionAnswering"),P$.forEach(n),Sy=o(ml," forward method, overrides the "),lc=a(ml,"CODE",{});var C$=i(lc);By=o(C$,"__call__"),C$.forEach(n),Wy=o(ml," special method."),ml.forEach(n),Qy=d(Gn),v(Lo.$$.fragment,Gn),Uy=d(Gn),dc=a(Gn,"P",{});var j$=i(dc);Ry=o(j$,"Example:"),j$.forEach(n),Hy=d(Gn),v(Ya.$$.fragment,Gn),Gn.forEach(n),vn.forEach(n),this.h()},h(){c(p,"name","hf:doc:metadata"),c(p,"content",JSON.stringify(d2)),c(_,"id","funnel-transformer"),c(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_,"href","#funnel-transformer"),c(g,"class","relative group"),c(J,"id","overview"),c(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J,"href","#overview"),c(q,"class","relative group"),c(te,"href","https://arxiv.org/abs/2006.03236"),c(te,"rel","nofollow"),c(re,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelModel"),c(u,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(Te,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(ke,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(Fe,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelBaseModel"),c(ve,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(Ja,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(No,"href","https://huggingface.co/sgugger"),c(No,"rel","nofollow"),c(Io,"href","https://github.com/laiguokun/Funnel-Transformer"),c(Io,"rel","nofollow"),c(Bt,"id","transformers.FunnelConfig"),c(Bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bt,"href","#transformers.FunnelConfig"),c(Zn,"class","relative group"),c(ei,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelModel"),c(ni,"href","/docs/transformers/main/en/model_doc/bert#transformers.TFBertModel"),c(Wo,"href","https://huggingface.co/funnel-transformer/small"),c(Wo,"rel","nofollow"),c(ti,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(oi,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(Ln,"class","docstring"),c(Wt,"id","transformers.FunnelTokenizer"),c(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Wt,"href","#transformers.FunnelTokenizer"),c(Jn,"class","relative group"),c(si,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizer"),c(ri,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),c(ai,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),c(An,"class","docstring"),c(Ut,"class","docstring"),c(yn,"class","docstring"),c(di,"class","docstring"),c(je,"class","docstring"),c(Rt,"id","transformers.FunnelTokenizerFast"),c(Rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Rt,"href","#transformers.FunnelTokenizerFast"),c(nt,"class","relative group"),c(ci,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(ui,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),c(pi,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),c(wn,"class","docstring"),c(en,"class","docstring"),c(Vt,"id","transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"),c(Vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Vt,"href","#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"),c(ot,"class","relative group"),c(hi,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(st,"class","docstring"),c(fi,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(rt,"class","docstring"),c(Yt,"id","transformers.FunnelBaseModel"),c(Yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Yt,"href","#transformers.FunnelBaseModel"),c(at,"class","relative group"),c(hs,"href","https://arxiv.org/abs/2006.03236"),c(hs,"rel","nofollow"),c(mi,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(gs,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(gs,"rel","nofollow"),c(gi,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelBaseModel"),c(nn,"class","docstring"),c(Qe,"class","docstring"),c(Gt,"id","transformers.FunnelModel"),c(Gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Gt,"href","#transformers.FunnelModel"),c(lt,"class","relative group"),c(ys,"href","https://arxiv.org/abs/2006.03236"),c(ys,"rel","nofollow"),c(_i,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c($s,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c($s,"rel","nofollow"),c(Ti,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelModel"),c(tn,"class","docstring"),c(Ue,"class","docstring"),c(Xt,"id","transformers.FunnelForPreTraining"),c(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Xt,"href","#transformers.FunnelForPreTraining"),c(ct,"class","relative group"),c(ki,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(on,"class","docstring"),c(ut,"class","docstring"),c(eo,"id","transformers.FunnelForMaskedLM"),c(eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(eo,"href","#transformers.FunnelForMaskedLM"),c(ht,"class","relative group"),c(Ds,"href","https://arxiv.org/abs/2006.03236"),c(Ds,"rel","nofollow"),c(Fi,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(Is,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Is,"rel","nofollow"),c(vi,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(Ge,"class","docstring"),c(Re,"class","docstring"),c(to,"id","transformers.FunnelForSequenceClassification"),c(to,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(to,"href","#transformers.FunnelForSequenceClassification"),c(mt,"class","relative group"),c(Hs,"href","https://arxiv.org/abs/2006.03236"),c(Hs,"rel","nofollow"),c(yi,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(Ks,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ks,"rel","nofollow"),c(wi,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(Pe,"class","docstring"),c(He,"class","docstring"),c(so,"id","transformers.FunnelForMultipleChoice"),c(so,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(so,"href","#transformers.FunnelForMultipleChoice"),c(_t,"class","relative group"),c(sr,"href","https://arxiv.org/abs/2006.03236"),c(sr,"rel","nofollow"),c(bi,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(ir,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(ir,"rel","nofollow"),c($i,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(sn,"class","docstring"),c(Ve,"class","docstring"),c(ao,"id","transformers.FunnelForTokenClassification"),c(ao,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ao,"href","#transformers.FunnelForTokenClassification"),c(kt,"class","relative group"),c(hr,"href","https://arxiv.org/abs/2006.03236"),c(hr,"rel","nofollow"),c(Ei,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(gr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(gr,"rel","nofollow"),c(Mi,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(Ze,"class","docstring"),c(Ye,"class","docstring"),c(lo,"id","transformers.FunnelForQuestionAnswering"),c(lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lo,"href","#transformers.FunnelForQuestionAnswering"),c(vt,"class","relative group"),c(wr,"href","https://arxiv.org/abs/2006.03236"),c(wr,"rel","nofollow"),c(zi,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(Er,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Er,"rel","nofollow"),c(qi,"href","/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(Xe,"class","docstring"),c(Ke,"class","docstring"),c(uo,"id","transformers.TFFunnelBaseModel"),c(uo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(uo,"href","#transformers.TFFunnelBaseModel"),c(bt,"class","relative group"),c(xr,"href","https://arxiv.org/abs/2006.03236"),c(xr,"rel","nofollow"),c(Pi,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(Dr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Dr,"rel","nofollow"),c(Ci,"href","/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(rn,"class","docstring"),c(Le,"class","docstring"),c(fo,"id","transformers.TFFunnelModel"),c(fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fo,"href","#transformers.TFFunnelModel"),c(Et,"class","relative group"),c(Wr,"href","https://arxiv.org/abs/2006.03236"),c(Wr,"rel","nofollow"),c(ji,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(Rr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Rr,"rel","nofollow"),c(xi,"href","/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelModel"),c(an,"class","docstring"),c(Oe,"class","docstring"),c(_o,"id","transformers.TFFunnelForPreTraining"),c(_o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_o,"href","#transformers.TFFunnelForPreTraining"),c(zt,"class","relative group"),c(Zr,"href","https://arxiv.org/abs/2006.03236"),c(Zr,"rel","nofollow"),c(Li,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(ea,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ea,"rel","nofollow"),c(Oi,"href","/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(ln,"class","docstring"),c(De,"class","docstring"),c(Fo,"id","transformers.TFFunnelForMaskedLM"),c(Fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fo,"href","#transformers.TFFunnelForMaskedLM"),c(Pt,"class","relative group"),c(ia,"href","https://arxiv.org/abs/2006.03236"),c(ia,"rel","nofollow"),c(Di,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(ca,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ca,"rel","nofollow"),c(Ai,"href","/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(dn,"class","docstring"),c(Ae,"class","docstring"),c(wo,"id","transformers.TFFunnelForSequenceClassification"),c(wo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(wo,"href","#transformers.TFFunnelForSequenceClassification"),c(jt,"class","relative group"),c(ga,"href","https://arxiv.org/abs/2006.03236"),c(ga,"rel","nofollow"),c(Ni,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(ka,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ka,"rel","nofollow"),c(Ii,"href","/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(cn,"class","docstring"),c(Ne,"class","docstring"),c(Eo,"id","transformers.TFFunnelForMultipleChoice"),c(Eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Eo,"href","#transformers.TFFunnelForMultipleChoice"),c(Lt,"class","relative group"),c($a,"href","https://arxiv.org/abs/2006.03236"),c($a,"rel","nofollow"),c(Si,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(za,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(za,"rel","nofollow"),c(Bi,"href","/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(un,"class","docstring"),c(Ie,"class","docstring"),c(qo,"id","transformers.TFFunnelForTokenClassification"),c(qo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qo,"href","#transformers.TFFunnelForTokenClassification"),c(Dt,"class","relative group"),c(La,"href","https://arxiv.org/abs/2006.03236"),c(La,"rel","nofollow"),c(Wi,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(Aa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Aa,"rel","nofollow"),c(Qi,"href","/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(pn,"class","docstring"),c(Se,"class","docstring"),c(jo,"id","transformers.TFFunnelForQuestionAnswering"),c(jo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(jo,"href","#transformers.TFFunnelForQuestionAnswering"),c(Nt,"class","relative group"),c(Qa,"href","https://arxiv.org/abs/2006.03236"),c(Qa,"rel","nofollow"),c(Ui,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(Ha,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Ha,"rel","nofollow"),c(Ri,"href","/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(hn,"class","docstring"),c(Be,"class","docstring")},m(s,f){e(document.head,p),h(s,z,f),h(s,g,f),e(g,_),e(_,k),y(T,k,null),e(g,m),e(g,M),e(M,ce),h(s,K,f),h(s,q,f),e(q,J),e(J,A),y(ne,A,null),e(q,ue),e(q,N),e(N,pe),h(s,ie,f),h(s,Y,f),e(Y,L),e(Y,te),e(te,G),e(Y,P),h(s,j,f),h(s,oe,f),e(oe,W),h(s,le,f),h(s,se,f),e(se,I),e(I,he),h(s,de,f),h(s,C,f),e(C,fe),h(s,B,f),h(s,ee,f),e(ee,ae),e(ae,Q),e(ee,me),e(ee,S),e(S,O),e(S,re),e(re,U),e(S,ge),e(S,u),e(u,E),e(S,Z),e(S,Te),e(Te,ye),e(S,D),e(S,ke),e(ke,we),e(S,be),e(S,x),e(x,R),e(S,$e),e(S,Fe),e(Fe,H),e(S,Ee),e(S,ve),e(ve,_e),e(S,Me),e(S,Ja),e(Ja,Up),e(S,Rp),h(s,Cc,f),h(s,Dn,f),e(Dn,Hp),e(Dn,No),e(No,Vp),e(Dn,Yp),e(Dn,Io),e(Io,Kp),e(Dn,Gp),h(s,jc,f),h(s,Zn,f),e(Zn,Bt),e(Bt,gl),y(So,gl,null),e(Zn,Zp),e(Zn,_l),e(_l,Xp),h(s,xc,f),h(s,Ln,f),y(Bo,Ln,null),e(Ln,Jp),e(Ln,On),e(On,eh),e(On,ei),e(ei,nh),e(On,th),e(On,ni),e(ni,oh),e(On,sh),e(On,Wo),e(Wo,rh),e(On,ah),e(Ln,ih),e(Ln,Xn),e(Xn,lh),e(Xn,ti),e(ti,dh),e(Xn,ch),e(Xn,oi),e(oi,uh),e(Xn,ph),h(s,Lc,f),h(s,Jn,f),e(Jn,Wt),e(Wt,Tl),y(Qo,Tl,null),e(Jn,hh),e(Jn,kl),e(kl,fh),h(s,Oc,f),h(s,je,f),y(Uo,je,null),e(je,mh),e(je,Fl),e(Fl,gh),e(je,_h),e(je,Qt),e(Qt,si),e(si,Th),e(Qt,kh),e(Qt,ri),e(ri,Fh),e(Qt,vh),e(je,yh),e(je,Ro),e(Ro,wh),e(Ro,ai),e(ai,bh),e(Ro,$h),e(je,Eh),e(je,An),y(Ho,An,null),e(An,Mh),e(An,vl),e(vl,zh),e(An,qh),e(An,Vo),e(Vo,ii),e(ii,Ph),e(ii,yl),e(yl,Ch),e(Vo,jh),e(Vo,li),e(li,xh),e(li,wl),e(wl,Lh),e(je,Oh),e(je,Ut),y(Yo,Ut,null),e(Ut,Dh),e(Ut,Ko),e(Ko,Ah),e(Ko,bl),e(bl,Nh),e(Ko,Ih),e(je,Sh),e(je,yn),y(Go,yn,null),e(yn,Bh),e(yn,$l),e($l,Wh),e(yn,Qh),y(Zo,yn,null),e(yn,Uh),e(yn,et),e(et,Rh),e(et,El),e(El,Hh),e(et,Vh),e(et,Ml),e(Ml,Yh),e(et,Kh),e(je,Gh),e(je,di),y(Xo,di,null),h(s,Dc,f),h(s,nt,f),e(nt,Rt),e(Rt,zl),y(Jo,zl,null),e(nt,Zh),e(nt,ql),e(ql,Xh),h(s,Ac,f),h(s,en,f),y(es,en,null),e(en,Jh),e(en,ns),e(ns,ef),e(ns,Pl),e(Pl,nf),e(ns,tf),e(en,of),e(en,Ht),e(Ht,ci),e(ci,sf),e(Ht,rf),e(Ht,ui),e(ui,af),e(Ht,lf),e(en,df),e(en,ts),e(ts,cf),e(ts,pi),e(pi,uf),e(ts,pf),e(en,hf),e(en,wn),y(os,wn,null),e(wn,ff),e(wn,Cl),e(Cl,mf),e(wn,gf),y(ss,wn,null),e(wn,_f),e(wn,tt),e(tt,Tf),e(tt,jl),e(jl,kf),e(tt,Ff),e(tt,xl),e(xl,vf),e(tt,yf),h(s,Nc,f),h(s,ot,f),e(ot,Vt),e(Vt,Ll),y(rs,Ll,null),e(ot,wf),e(ot,Ol),e(Ol,bf),h(s,Ic,f),h(s,st,f),y(as,st,null),e(st,$f),e(st,is),e(is,Ef),e(is,hi),e(hi,Mf),e(is,zf),h(s,Sc,f),h(s,rt,f),y(ls,rt,null),e(rt,qf),e(rt,ds),e(ds,Pf),e(ds,fi),e(fi,Cf),e(ds,jf),h(s,Bc,f),h(s,at,f),e(at,Yt),e(Yt,Dl),y(cs,Dl,null),e(at,xf),e(at,Al),e(Al,Lf),h(s,Wc,f),h(s,Qe,f),y(us,Qe,null),e(Qe,Of),e(Qe,Nl),e(Nl,Df),e(Qe,Af),e(Qe,ps),e(ps,Nf),e(ps,hs),e(hs,If),e(ps,Sf),e(Qe,Bf),e(Qe,fs),e(fs,Wf),e(fs,mi),e(mi,Qf),e(fs,Uf),e(Qe,Rf),e(Qe,ms),e(ms,Hf),e(ms,gs),e(gs,Vf),e(ms,Yf),e(Qe,Kf),e(Qe,nn),y(_s,nn,null),e(nn,Gf),e(nn,it),e(it,Zf),e(it,gi),e(gi,Xf),e(it,Jf),e(it,Il),e(Il,em),e(it,nm),e(nn,tm),y(Kt,nn,null),e(nn,om),e(nn,Sl),e(Sl,sm),e(nn,rm),y(Ts,nn,null),h(s,Qc,f),h(s,lt,f),e(lt,Gt),e(Gt,Bl),y(ks,Bl,null),e(lt,am),e(lt,Wl),e(Wl,im),h(s,Uc,f),h(s,Ue,f),y(Fs,Ue,null),e(Ue,lm),e(Ue,Ql),e(Ql,dm),e(Ue,cm),e(Ue,vs),e(vs,um),e(vs,ys),e(ys,pm),e(vs,hm),e(Ue,fm),e(Ue,ws),e(ws,mm),e(ws,_i),e(_i,gm),e(ws,_m),e(Ue,Tm),e(Ue,bs),e(bs,km),e(bs,$s),e($s,Fm),e(bs,vm),e(Ue,ym),e(Ue,tn),y(Es,tn,null),e(tn,wm),e(tn,dt),e(dt,bm),e(dt,Ti),e(Ti,$m),e(dt,Em),e(dt,Ul),e(Ul,Mm),e(dt,zm),e(tn,qm),y(Zt,tn,null),e(tn,Pm),e(tn,Rl),e(Rl,Cm),e(tn,jm),y(Ms,tn,null),h(s,Rc,f),h(s,ct,f),e(ct,Xt),e(Xt,Hl),y(zs,Hl,null),e(ct,xm),e(ct,Vl),e(Vl,Lm),h(s,Hc,f),h(s,ut,f),y(qs,ut,null),e(ut,Om),e(ut,on),y(Ps,on,null),e(on,Dm),e(on,pt),e(pt,Am),e(pt,ki),e(ki,Nm),e(pt,Im),e(pt,Yl),e(Yl,Sm),e(pt,Bm),e(on,Wm),y(Jt,on,null),e(on,Qm),e(on,Kl),e(Kl,Um),e(on,Rm),y(Cs,on,null),h(s,Vc,f),h(s,ht,f),e(ht,eo),e(eo,Gl),y(js,Gl,null),e(ht,Hm),e(ht,Zl),e(Zl,Vm),h(s,Yc,f),h(s,Re,f),y(xs,Re,null),e(Re,Ym),e(Re,Ls),e(Ls,Km),e(Ls,Xl),e(Xl,Gm),e(Ls,Zm),e(Re,Xm),e(Re,Os),e(Os,Jm),e(Os,Ds),e(Ds,eg),e(Os,ng),e(Re,tg),e(Re,As),e(As,og),e(As,Fi),e(Fi,sg),e(As,rg),e(Re,ag),e(Re,Ns),e(Ns,ig),e(Ns,Is),e(Is,lg),e(Ns,dg),e(Re,cg),e(Re,Ge),y(Ss,Ge,null),e(Ge,ug),e(Ge,ft),e(ft,pg),e(ft,vi),e(vi,hg),e(ft,fg),e(ft,Jl),e(Jl,mg),e(ft,gg),e(Ge,_g),y(no,Ge,null),e(Ge,Tg),e(Ge,ed),e(ed,kg),e(Ge,Fg),y(Bs,Ge,null),e(Ge,vg),y(Ws,Ge,null),h(s,Kc,f),h(s,mt,f),e(mt,to),e(to,nd),y(Qs,nd,null),e(mt,yg),e(mt,td),e(td,wg),h(s,Gc,f),h(s,He,f),y(Us,He,null),e(He,bg),e(He,od),e(od,$g),e(He,Eg),e(He,Rs),e(Rs,Mg),e(Rs,Hs),e(Hs,zg),e(Rs,qg),e(He,Pg),e(He,Vs),e(Vs,Cg),e(Vs,yi),e(yi,jg),e(Vs,xg),e(He,Lg),e(He,Ys),e(Ys,Og),e(Ys,Ks),e(Ks,Dg),e(Ys,Ag),e(He,Ng),e(He,Pe),y(Gs,Pe,null),e(Pe,Ig),e(Pe,gt),e(gt,Sg),e(gt,wi),e(wi,Bg),e(gt,Wg),e(gt,sd),e(sd,Qg),e(gt,Ug),e(Pe,Rg),y(oo,Pe,null),e(Pe,Hg),e(Pe,rd),e(rd,Vg),e(Pe,Yg),y(Zs,Pe,null),e(Pe,Kg),y(Xs,Pe,null),e(Pe,Gg),e(Pe,ad),e(ad,Zg),e(Pe,Xg),y(Js,Pe,null),e(Pe,Jg),y(er,Pe,null),h(s,Zc,f),h(s,_t,f),e(_t,so),e(so,id),y(nr,id,null),e(_t,e_),e(_t,ld),e(ld,n_),h(s,Xc,f),h(s,Ve,f),y(tr,Ve,null),e(Ve,t_),e(Ve,dd),e(dd,o_),e(Ve,s_),e(Ve,or),e(or,r_),e(or,sr),e(sr,a_),e(or,i_),e(Ve,l_),e(Ve,rr),e(rr,d_),e(rr,bi),e(bi,c_),e(rr,u_),e(Ve,p_),e(Ve,ar),e(ar,h_),e(ar,ir),e(ir,f_),e(ar,m_),e(Ve,g_),e(Ve,sn),y(lr,sn,null),e(sn,__),e(sn,Tt),e(Tt,T_),e(Tt,$i),e($i,k_),e(Tt,F_),e(Tt,cd),e(cd,v_),e(Tt,y_),e(sn,w_),y(ro,sn,null),e(sn,b_),e(sn,ud),e(ud,$_),e(sn,E_),y(dr,sn,null),h(s,Jc,f),h(s,kt,f),e(kt,ao),e(ao,pd),y(cr,pd,null),e(kt,M_),e(kt,hd),e(hd,z_),h(s,eu,f),h(s,Ye,f),y(ur,Ye,null),e(Ye,q_),e(Ye,fd),e(fd,P_),e(Ye,C_),e(Ye,pr),e(pr,j_),e(pr,hr),e(hr,x_),e(pr,L_),e(Ye,O_),e(Ye,fr),e(fr,D_),e(fr,Ei),e(Ei,A_),e(fr,N_),e(Ye,I_),e(Ye,mr),e(mr,S_),e(mr,gr),e(gr,B_),e(mr,W_),e(Ye,Q_),e(Ye,Ze),y(_r,Ze,null),e(Ze,U_),e(Ze,Ft),e(Ft,R_),e(Ft,Mi),e(Mi,H_),e(Ft,V_),e(Ft,md),e(md,Y_),e(Ft,K_),e(Ze,G_),y(io,Ze,null),e(Ze,Z_),e(Ze,gd),e(gd,X_),e(Ze,J_),y(Tr,Ze,null),e(Ze,eT),y(kr,Ze,null),h(s,nu,f),h(s,vt,f),e(vt,lo),e(lo,_d),y(Fr,_d,null),e(vt,nT),e(vt,Td),e(Td,tT),h(s,tu,f),h(s,Ke,f),y(vr,Ke,null),e(Ke,oT),e(Ke,yt),e(yt,sT),e(yt,kd),e(kd,rT),e(yt,aT),e(yt,Fd),e(Fd,iT),e(yt,lT),e(Ke,dT),e(Ke,yr),e(yr,cT),e(yr,wr),e(wr,uT),e(yr,pT),e(Ke,hT),e(Ke,br),e(br,fT),e(br,zi),e(zi,mT),e(br,gT),e(Ke,_T),e(Ke,$r),e($r,TT),e($r,Er),e(Er,kT),e($r,FT),e(Ke,vT),e(Ke,Xe),y(Mr,Xe,null),e(Xe,yT),e(Xe,wt),e(wt,wT),e(wt,qi),e(qi,bT),e(wt,$T),e(wt,vd),e(vd,ET),e(wt,MT),e(Xe,zT),y(co,Xe,null),e(Xe,qT),e(Xe,yd),e(yd,PT),e(Xe,CT),y(zr,Xe,null),e(Xe,jT),y(qr,Xe,null),h(s,ou,f),h(s,bt,f),e(bt,uo),e(uo,wd),y(Pr,wd,null),e(bt,xT),e(bt,bd),e(bd,LT),h(s,su,f),h(s,Le,f),y(Cr,Le,null),e(Le,OT),e(Le,$d),e($d,DT),e(Le,AT),e(Le,jr),e(jr,NT),e(jr,xr),e(xr,IT),e(jr,ST),e(Le,BT),e(Le,Lr),e(Lr,WT),e(Lr,Pi),e(Pi,QT),e(Lr,UT),e(Le,RT),e(Le,Or),e(Or,HT),e(Or,Dr),e(Dr,VT),e(Or,YT),e(Le,KT),y(po,Le,null),e(Le,GT),e(Le,rn),y(Ar,rn,null),e(rn,ZT),e(rn,$t),e($t,XT),e($t,Ci),e(Ci,JT),e($t,ek),e($t,Ed),e(Ed,nk),e($t,tk),e(rn,ok),y(ho,rn,null),e(rn,sk),e(rn,Md),e(Md,rk),e(rn,ak),y(Nr,rn,null),h(s,ru,f),h(s,Et,f),e(Et,fo),e(fo,zd),y(Ir,zd,null),e(Et,ik),e(Et,qd),e(qd,lk),h(s,au,f),h(s,Oe,f),y(Sr,Oe,null),e(Oe,dk),e(Oe,Pd),e(Pd,ck),e(Oe,uk),e(Oe,Br),e(Br,pk),e(Br,Wr),e(Wr,hk),e(Br,fk),e(Oe,mk),e(Oe,Qr),e(Qr,gk),e(Qr,ji),e(ji,_k),e(Qr,Tk),e(Oe,kk),e(Oe,Ur),e(Ur,Fk),e(Ur,Rr),e(Rr,vk),e(Ur,yk),e(Oe,wk),y(mo,Oe,null),e(Oe,bk),e(Oe,an),y(Hr,an,null),e(an,$k),e(an,Mt),e(Mt,Ek),e(Mt,xi),e(xi,Mk),e(Mt,zk),e(Mt,Cd),e(Cd,qk),e(Mt,Pk),e(an,Ck),y(go,an,null),e(an,jk),e(an,jd),e(jd,xk),e(an,Lk),y(Vr,an,null),h(s,iu,f),h(s,zt,f),e(zt,_o),e(_o,xd),y(Yr,xd,null),e(zt,Ok),e(zt,Ld),e(Ld,Dk),h(s,lu,f),h(s,De,f),y(Kr,De,null),e(De,Ak),e(De,Od),e(Od,Nk),e(De,Ik),e(De,Gr),e(Gr,Sk),e(Gr,Zr),e(Zr,Bk),e(Gr,Wk),e(De,Qk),e(De,Xr),e(Xr,Uk),e(Xr,Li),e(Li,Rk),e(Xr,Hk),e(De,Vk),e(De,Jr),e(Jr,Yk),e(Jr,ea),e(ea,Kk),e(Jr,Gk),e(De,Zk),y(To,De,null),e(De,Xk),e(De,ln),y(na,ln,null),e(ln,Jk),e(ln,qt),e(qt,eF),e(qt,Oi),e(Oi,nF),e(qt,tF),e(qt,Dd),e(Dd,oF),e(qt,sF),e(ln,rF),y(ko,ln,null),e(ln,aF),e(ln,Ad),e(Ad,iF),e(ln,lF),y(ta,ln,null),h(s,du,f),h(s,Pt,f),e(Pt,Fo),e(Fo,Nd),y(oa,Nd,null),e(Pt,dF),e(Pt,Id),e(Id,cF),h(s,cu,f),h(s,Ae,f),y(sa,Ae,null),e(Ae,uF),e(Ae,ra),e(ra,pF),e(ra,Sd),e(Sd,hF),e(ra,fF),e(Ae,mF),e(Ae,aa),e(aa,gF),e(aa,ia),e(ia,_F),e(aa,TF),e(Ae,kF),e(Ae,la),e(la,FF),e(la,Di),e(Di,vF),e(la,yF),e(Ae,wF),e(Ae,da),e(da,bF),e(da,ca),e(ca,$F),e(da,EF),e(Ae,MF),y(vo,Ae,null),e(Ae,zF),e(Ae,dn),y(ua,dn,null),e(dn,qF),e(dn,Ct),e(Ct,PF),e(Ct,Ai),e(Ai,CF),e(Ct,jF),e(Ct,Bd),e(Bd,xF),e(Ct,LF),e(dn,OF),y(yo,dn,null),e(dn,DF),e(dn,Wd),e(Wd,AF),e(dn,NF),y(pa,dn,null),h(s,uu,f),h(s,jt,f),e(jt,wo),e(wo,Qd),y(ha,Qd,null),e(jt,IF),e(jt,Ud),e(Ud,SF),h(s,pu,f),h(s,Ne,f),y(fa,Ne,null),e(Ne,BF),e(Ne,Rd),e(Rd,WF),e(Ne,QF),e(Ne,ma),e(ma,UF),e(ma,ga),e(ga,RF),e(ma,HF),e(Ne,VF),e(Ne,_a),e(_a,YF),e(_a,Ni),e(Ni,KF),e(_a,GF),e(Ne,ZF),e(Ne,Ta),e(Ta,XF),e(Ta,ka),e(ka,JF),e(Ta,ev),e(Ne,nv),y(bo,Ne,null),e(Ne,tv),e(Ne,cn),y(Fa,cn,null),e(cn,ov),e(cn,xt),e(xt,sv),e(xt,Ii),e(Ii,rv),e(xt,av),e(xt,Hd),e(Hd,iv),e(xt,lv),e(cn,dv),y($o,cn,null),e(cn,cv),e(cn,Vd),e(Vd,uv),e(cn,pv),y(va,cn,null),h(s,hu,f),h(s,Lt,f),e(Lt,Eo),e(Eo,Yd),y(ya,Yd,null),e(Lt,hv),e(Lt,Kd),e(Kd,fv),h(s,fu,f),h(s,Ie,f),y(wa,Ie,null),e(Ie,mv),e(Ie,Gd),e(Gd,gv),e(Ie,_v),e(Ie,ba),e(ba,Tv),e(ba,$a),e($a,kv),e(ba,Fv),e(Ie,vv),e(Ie,Ea),e(Ea,yv),e(Ea,Si),e(Si,wv),e(Ea,bv),e(Ie,$v),e(Ie,Ma),e(Ma,Ev),e(Ma,za),e(za,Mv),e(Ma,zv),e(Ie,qv),y(Mo,Ie,null),e(Ie,Pv),e(Ie,un),y(qa,un,null),e(un,Cv),e(un,Ot),e(Ot,jv),e(Ot,Bi),e(Bi,xv),e(Ot,Lv),e(Ot,Zd),e(Zd,Ov),e(Ot,Dv),e(un,Av),y(zo,un,null),e(un,Nv),e(un,Xd),e(Xd,Iv),e(un,Sv),y(Pa,un,null),h(s,mu,f),h(s,Dt,f),e(Dt,qo),e(qo,Jd),y(Ca,Jd,null),e(Dt,Bv),e(Dt,ec),e(ec,Wv),h(s,gu,f),h(s,Se,f),y(ja,Se,null),e(Se,Qv),e(Se,nc),e(nc,Uv),e(Se,Rv),e(Se,xa),e(xa,Hv),e(xa,La),e(La,Vv),e(xa,Yv),e(Se,Kv),e(Se,Oa),e(Oa,Gv),e(Oa,Wi),e(Wi,Zv),e(Oa,Xv),e(Se,Jv),e(Se,Da),e(Da,ey),e(Da,Aa),e(Aa,ny),e(Da,ty),e(Se,oy),y(Po,Se,null),e(Se,sy),e(Se,pn),y(Na,pn,null),e(pn,ry),e(pn,At),e(At,ay),e(At,Qi),e(Qi,iy),e(At,ly),e(At,tc),e(tc,dy),e(At,cy),e(pn,uy),y(Co,pn,null),e(pn,py),e(pn,oc),e(oc,hy),e(pn,fy),y(Ia,pn,null),h(s,_u,f),h(s,Nt,f),e(Nt,jo),e(jo,sc),y(Sa,sc,null),e(Nt,my),e(Nt,rc),e(rc,gy),h(s,Tu,f),h(s,Be,f),y(Ba,Be,null),e(Be,_y),e(Be,It),e(It,Ty),e(It,ac),e(ac,ky),e(It,Fy),e(It,ic),e(ic,vy),e(It,yy),e(Be,wy),e(Be,Wa),e(Wa,by),e(Wa,Qa),e(Qa,$y),e(Wa,Ey),e(Be,My),e(Be,Ua),e(Ua,zy),e(Ua,Ui),e(Ui,qy),e(Ua,Py),e(Be,Cy),e(Be,Ra),e(Ra,jy),e(Ra,Ha),e(Ha,xy),e(Ra,Ly),e(Be,Oy),y(xo,Be,null),e(Be,Dy),e(Be,hn),y(Va,hn,null),e(hn,Ay),e(hn,St),e(St,Ny),e(St,Ri),e(Ri,Iy),e(St,Sy),e(St,lc),e(lc,By),e(St,Wy),e(hn,Qy),y(Lo,hn,null),e(hn,Uy),e(hn,dc),e(dc,Ry),e(hn,Hy),y(Ya,hn,null),ku=!0},p(s,[f]){const Ka={};f&2&&(Ka.$$scope={dirty:f,ctx:s}),Kt.$set(Ka);const cc={};f&2&&(cc.$$scope={dirty:f,ctx:s}),Zt.$set(cc);const uc={};f&2&&(uc.$$scope={dirty:f,ctx:s}),Jt.$set(uc);const pc={};f&2&&(pc.$$scope={dirty:f,ctx:s}),no.$set(pc);const Ga={};f&2&&(Ga.$$scope={dirty:f,ctx:s}),oo.$set(Ga);const hc={};f&2&&(hc.$$scope={dirty:f,ctx:s}),ro.$set(hc);const fc={};f&2&&(fc.$$scope={dirty:f,ctx:s}),io.$set(fc);const mc={};f&2&&(mc.$$scope={dirty:f,ctx:s}),co.$set(mc);const Za={};f&2&&(Za.$$scope={dirty:f,ctx:s}),po.$set(Za);const gc={};f&2&&(gc.$$scope={dirty:f,ctx:s}),ho.$set(gc);const _c={};f&2&&(_c.$$scope={dirty:f,ctx:s}),mo.$set(_c);const Tc={};f&2&&(Tc.$$scope={dirty:f,ctx:s}),go.$set(Tc);const kc={};f&2&&(kc.$$scope={dirty:f,ctx:s}),To.$set(kc);const Fc={};f&2&&(Fc.$$scope={dirty:f,ctx:s}),ko.$set(Fc);const Xa={};f&2&&(Xa.$$scope={dirty:f,ctx:s}),vo.$set(Xa);const vc={};f&2&&(vc.$$scope={dirty:f,ctx:s}),yo.$set(vc);const xe={};f&2&&(xe.$$scope={dirty:f,ctx:s}),bo.$set(xe);const yc={};f&2&&(yc.$$scope={dirty:f,ctx:s}),$o.$set(yc);const wc={};f&2&&(wc.$$scope={dirty:f,ctx:s}),Mo.$set(wc);const bc={};f&2&&(bc.$$scope={dirty:f,ctx:s}),zo.$set(bc);const $c={};f&2&&($c.$$scope={dirty:f,ctx:s}),Po.$set($c);const Ec={};f&2&&(Ec.$$scope={dirty:f,ctx:s}),Co.$set(Ec);const Mc={};f&2&&(Mc.$$scope={dirty:f,ctx:s}),xo.$set(Mc);const zc={};f&2&&(zc.$$scope={dirty:f,ctx:s}),Lo.$set(zc)},i(s){ku||(w(T.$$.fragment,s),w(ne.$$.fragment,s),w(So.$$.fragment,s),w(Bo.$$.fragment,s),w(Qo.$$.fragment,s),w(Uo.$$.fragment,s),w(Ho.$$.fragment,s),w(Yo.$$.fragment,s),w(Go.$$.fragment,s),w(Zo.$$.fragment,s),w(Xo.$$.fragment,s),w(Jo.$$.fragment,s),w(es.$$.fragment,s),w(os.$$.fragment,s),w(ss.$$.fragment,s),w(rs.$$.fragment,s),w(as.$$.fragment,s),w(ls.$$.fragment,s),w(cs.$$.fragment,s),w(us.$$.fragment,s),w(_s.$$.fragment,s),w(Kt.$$.fragment,s),w(Ts.$$.fragment,s),w(ks.$$.fragment,s),w(Fs.$$.fragment,s),w(Es.$$.fragment,s),w(Zt.$$.fragment,s),w(Ms.$$.fragment,s),w(zs.$$.fragment,s),w(qs.$$.fragment,s),w(Ps.$$.fragment,s),w(Jt.$$.fragment,s),w(Cs.$$.fragment,s),w(js.$$.fragment,s),w(xs.$$.fragment,s),w(Ss.$$.fragment,s),w(no.$$.fragment,s),w(Bs.$$.fragment,s),w(Ws.$$.fragment,s),w(Qs.$$.fragment,s),w(Us.$$.fragment,s),w(Gs.$$.fragment,s),w(oo.$$.fragment,s),w(Zs.$$.fragment,s),w(Xs.$$.fragment,s),w(Js.$$.fragment,s),w(er.$$.fragment,s),w(nr.$$.fragment,s),w(tr.$$.fragment,s),w(lr.$$.fragment,s),w(ro.$$.fragment,s),w(dr.$$.fragment,s),w(cr.$$.fragment,s),w(ur.$$.fragment,s),w(_r.$$.fragment,s),w(io.$$.fragment,s),w(Tr.$$.fragment,s),w(kr.$$.fragment,s),w(Fr.$$.fragment,s),w(vr.$$.fragment,s),w(Mr.$$.fragment,s),w(co.$$.fragment,s),w(zr.$$.fragment,s),w(qr.$$.fragment,s),w(Pr.$$.fragment,s),w(Cr.$$.fragment,s),w(po.$$.fragment,s),w(Ar.$$.fragment,s),w(ho.$$.fragment,s),w(Nr.$$.fragment,s),w(Ir.$$.fragment,s),w(Sr.$$.fragment,s),w(mo.$$.fragment,s),w(Hr.$$.fragment,s),w(go.$$.fragment,s),w(Vr.$$.fragment,s),w(Yr.$$.fragment,s),w(Kr.$$.fragment,s),w(To.$$.fragment,s),w(na.$$.fragment,s),w(ko.$$.fragment,s),w(ta.$$.fragment,s),w(oa.$$.fragment,s),w(sa.$$.fragment,s),w(vo.$$.fragment,s),w(ua.$$.fragment,s),w(yo.$$.fragment,s),w(pa.$$.fragment,s),w(ha.$$.fragment,s),w(fa.$$.fragment,s),w(bo.$$.fragment,s),w(Fa.$$.fragment,s),w($o.$$.fragment,s),w(va.$$.fragment,s),w(ya.$$.fragment,s),w(wa.$$.fragment,s),w(Mo.$$.fragment,s),w(qa.$$.fragment,s),w(zo.$$.fragment,s),w(Pa.$$.fragment,s),w(Ca.$$.fragment,s),w(ja.$$.fragment,s),w(Po.$$.fragment,s),w(Na.$$.fragment,s),w(Co.$$.fragment,s),w(Ia.$$.fragment,s),w(Sa.$$.fragment,s),w(Ba.$$.fragment,s),w(xo.$$.fragment,s),w(Va.$$.fragment,s),w(Lo.$$.fragment,s),w(Ya.$$.fragment,s),ku=!0)},o(s){b(T.$$.fragment,s),b(ne.$$.fragment,s),b(So.$$.fragment,s),b(Bo.$$.fragment,s),b(Qo.$$.fragment,s),b(Uo.$$.fragment,s),b(Ho.$$.fragment,s),b(Yo.$$.fragment,s),b(Go.$$.fragment,s),b(Zo.$$.fragment,s),b(Xo.$$.fragment,s),b(Jo.$$.fragment,s),b(es.$$.fragment,s),b(os.$$.fragment,s),b(ss.$$.fragment,s),b(rs.$$.fragment,s),b(as.$$.fragment,s),b(ls.$$.fragment,s),b(cs.$$.fragment,s),b(us.$$.fragment,s),b(_s.$$.fragment,s),b(Kt.$$.fragment,s),b(Ts.$$.fragment,s),b(ks.$$.fragment,s),b(Fs.$$.fragment,s),b(Es.$$.fragment,s),b(Zt.$$.fragment,s),b(Ms.$$.fragment,s),b(zs.$$.fragment,s),b(qs.$$.fragment,s),b(Ps.$$.fragment,s),b(Jt.$$.fragment,s),b(Cs.$$.fragment,s),b(js.$$.fragment,s),b(xs.$$.fragment,s),b(Ss.$$.fragment,s),b(no.$$.fragment,s),b(Bs.$$.fragment,s),b(Ws.$$.fragment,s),b(Qs.$$.fragment,s),b(Us.$$.fragment,s),b(Gs.$$.fragment,s),b(oo.$$.fragment,s),b(Zs.$$.fragment,s),b(Xs.$$.fragment,s),b(Js.$$.fragment,s),b(er.$$.fragment,s),b(nr.$$.fragment,s),b(tr.$$.fragment,s),b(lr.$$.fragment,s),b(ro.$$.fragment,s),b(dr.$$.fragment,s),b(cr.$$.fragment,s),b(ur.$$.fragment,s),b(_r.$$.fragment,s),b(io.$$.fragment,s),b(Tr.$$.fragment,s),b(kr.$$.fragment,s),b(Fr.$$.fragment,s),b(vr.$$.fragment,s),b(Mr.$$.fragment,s),b(co.$$.fragment,s),b(zr.$$.fragment,s),b(qr.$$.fragment,s),b(Pr.$$.fragment,s),b(Cr.$$.fragment,s),b(po.$$.fragment,s),b(Ar.$$.fragment,s),b(ho.$$.fragment,s),b(Nr.$$.fragment,s),b(Ir.$$.fragment,s),b(Sr.$$.fragment,s),b(mo.$$.fragment,s),b(Hr.$$.fragment,s),b(go.$$.fragment,s),b(Vr.$$.fragment,s),b(Yr.$$.fragment,s),b(Kr.$$.fragment,s),b(To.$$.fragment,s),b(na.$$.fragment,s),b(ko.$$.fragment,s),b(ta.$$.fragment,s),b(oa.$$.fragment,s),b(sa.$$.fragment,s),b(vo.$$.fragment,s),b(ua.$$.fragment,s),b(yo.$$.fragment,s),b(pa.$$.fragment,s),b(ha.$$.fragment,s),b(fa.$$.fragment,s),b(bo.$$.fragment,s),b(Fa.$$.fragment,s),b($o.$$.fragment,s),b(va.$$.fragment,s),b(ya.$$.fragment,s),b(wa.$$.fragment,s),b(Mo.$$.fragment,s),b(qa.$$.fragment,s),b(zo.$$.fragment,s),b(Pa.$$.fragment,s),b(Ca.$$.fragment,s),b(ja.$$.fragment,s),b(Po.$$.fragment,s),b(Na.$$.fragment,s),b(Co.$$.fragment,s),b(Ia.$$.fragment,s),b(Sa.$$.fragment,s),b(Ba.$$.fragment,s),b(xo.$$.fragment,s),b(Va.$$.fragment,s),b(Lo.$$.fragment,s),b(Ya.$$.fragment,s),ku=!1},d(s){n(p),s&&n(z),s&&n(g),$(T),s&&n(K),s&&n(q),$(ne),s&&n(ie),s&&n(Y),s&&n(j),s&&n(oe),s&&n(le),s&&n(se),s&&n(de),s&&n(C),s&&n(B),s&&n(ee),s&&n(Cc),s&&n(Dn),s&&n(jc),s&&n(Zn),$(So),s&&n(xc),s&&n(Ln),$(Bo),s&&n(Lc),s&&n(Jn),$(Qo),s&&n(Oc),s&&n(je),$(Uo),$(Ho),$(Yo),$(Go),$(Zo),$(Xo),s&&n(Dc),s&&n(nt),$(Jo),s&&n(Ac),s&&n(en),$(es),$(os),$(ss),s&&n(Nc),s&&n(ot),$(rs),s&&n(Ic),s&&n(st),$(as),s&&n(Sc),s&&n(rt),$(ls),s&&n(Bc),s&&n(at),$(cs),s&&n(Wc),s&&n(Qe),$(us),$(_s),$(Kt),$(Ts),s&&n(Qc),s&&n(lt),$(ks),s&&n(Uc),s&&n(Ue),$(Fs),$(Es),$(Zt),$(Ms),s&&n(Rc),s&&n(ct),$(zs),s&&n(Hc),s&&n(ut),$(qs),$(Ps),$(Jt),$(Cs),s&&n(Vc),s&&n(ht),$(js),s&&n(Yc),s&&n(Re),$(xs),$(Ss),$(no),$(Bs),$(Ws),s&&n(Kc),s&&n(mt),$(Qs),s&&n(Gc),s&&n(He),$(Us),$(Gs),$(oo),$(Zs),$(Xs),$(Js),$(er),s&&n(Zc),s&&n(_t),$(nr),s&&n(Xc),s&&n(Ve),$(tr),$(lr),$(ro),$(dr),s&&n(Jc),s&&n(kt),$(cr),s&&n(eu),s&&n(Ye),$(ur),$(_r),$(io),$(Tr),$(kr),s&&n(nu),s&&n(vt),$(Fr),s&&n(tu),s&&n(Ke),$(vr),$(Mr),$(co),$(zr),$(qr),s&&n(ou),s&&n(bt),$(Pr),s&&n(su),s&&n(Le),$(Cr),$(po),$(Ar),$(ho),$(Nr),s&&n(ru),s&&n(Et),$(Ir),s&&n(au),s&&n(Oe),$(Sr),$(mo),$(Hr),$(go),$(Vr),s&&n(iu),s&&n(zt),$(Yr),s&&n(lu),s&&n(De),$(Kr),$(To),$(na),$(ko),$(ta),s&&n(du),s&&n(Pt),$(oa),s&&n(cu),s&&n(Ae),$(sa),$(vo),$(ua),$(yo),$(pa),s&&n(uu),s&&n(jt),$(ha),s&&n(pu),s&&n(Ne),$(fa),$(bo),$(Fa),$($o),$(va),s&&n(hu),s&&n(Lt),$(ya),s&&n(fu),s&&n(Ie),$(wa),$(Mo),$(qa),$(zo),$(Pa),s&&n(mu),s&&n(Dt),$(Ca),s&&n(gu),s&&n(Se),$(ja),$(Po),$(Na),$(Co),$(Ia),s&&n(_u),s&&n(Nt),$(Sa),s&&n(Tu),s&&n(Be),$(Ba),$(xo),$(Va),$(Lo),$(Ya)}}}const d2={local:"funnel-transformer",sections:[{local:"overview",title:"Overview"},{local:"transformers.FunnelConfig",title:"FunnelConfig"},{local:"transformers.FunnelTokenizer",title:"FunnelTokenizer"},{local:"transformers.FunnelTokenizerFast",title:"FunnelTokenizerFast"},{local:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",title:"Funnel specific outputs"},{local:"transformers.FunnelBaseModel",title:"FunnelBaseModel"},{local:"transformers.FunnelModel",title:"FunnelModel"},{local:"transformers.FunnelForPreTraining",title:"FunnelModelForPreTraining"},{local:"transformers.FunnelForMaskedLM",title:"FunnelForMaskedLM"},{local:"transformers.FunnelForSequenceClassification",title:"FunnelForSequenceClassification"},{local:"transformers.FunnelForMultipleChoice",title:"FunnelForMultipleChoice"},{local:"transformers.FunnelForTokenClassification",title:"FunnelForTokenClassification"},{local:"transformers.FunnelForQuestionAnswering",title:"FunnelForQuestionAnswering"},{local:"transformers.TFFunnelBaseModel",title:"TFFunnelBaseModel"},{local:"transformers.TFFunnelModel",title:"TFFunnelModel"},{local:"transformers.TFFunnelForPreTraining",title:"TFFunnelModelForPreTraining"},{local:"transformers.TFFunnelForMaskedLM",title:"TFFunnelForMaskedLM"},{local:"transformers.TFFunnelForSequenceClassification",title:"TFFunnelForSequenceClassification"},{local:"transformers.TFFunnelForMultipleChoice",title:"TFFunnelForMultipleChoice"},{local:"transformers.TFFunnelForTokenClassification",title:"TFFunnelForTokenClassification"},{local:"transformers.TFFunnelForQuestionAnswering",title:"TFFunnelForQuestionAnswering"}],title:"Funnel Transformer"};function c2(V){return A$(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class g2 extends x${constructor(p){super();L$(this,p,c2,l2,O$,{})}}export{g2 as default,d2 as metadata};
