import{S as Cp,i as Ip,s as Op,e as l,k as h,w as y,t as o,M as Np,c as i,d as s,m as d,a as p,x as b,h as n,b as $,F as t,g as u,y as E,q as A,o as T,B as j,v as Dp,L as ze}from"../chunks/vendor-6b77c823.js";import{T as Cs}from"../chunks/Tip-39098574.js";import{Y as Sp}from"../chunks/Youtube-5c6e11e6.js";import{I as ht}from"../chunks/IconCopyLink-7a11ce68.js";import{C as W}from"../chunks/CodeBlock-3a8b25a8.js";import{D as Hp}from"../chunks/DocNotebookDropdown-f2b55cd8.js";import{F as Ss,M as _e}from"../chunks/Markdown-9acf6d91.js";function Lp(P){let a,m;return{c(){a=l("p"),m=o(`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`)},l(r){a=i(r,"P",{});var c=p(a);m=n(c,`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`),c.forEach(s)},m(r,c){u(r,a,c),t(a,m)},d(r){r&&s(a)}}}function Wp(P){let a,m,r,c,_,v,z,C;return{c(){a=l("p"),m=o("For more details about the "),r=l("a"),c=o("pipeline()"),_=o(" and associated tasks, refer to the documentation "),v=l("a"),z=o("here"),C=o("."),this.h()},l(w){a=i(w,"P",{});var M=p(a);m=n(M,"For more details about the "),r=i(M,"A",{href:!0});var I=p(r);c=n(I,"pipeline()"),I.forEach(s),_=n(M," and associated tasks, refer to the documentation "),v=i(M,"A",{href:!0});var O=p(v);z=n(O,"here"),O.forEach(s),C=n(M,"."),M.forEach(s),this.h()},h(){$(r,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),$(v,"href","./main_classes/pipelines")},m(w,M){u(w,a,M),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,z),t(a,C)},d(w){w&&s(a)}}}function Up(P){let a,m;return a=new W({props:{code:"pip install torch",highlighted:"pip install torch"}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p:ze,i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function Rp(P){let a,m;return a=new _e({props:{$$slots:{default:[Up]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function Gp(P){let a,m;return a=new W({props:{code:"pip install tensorflow",highlighted:"pip install tensorflow"}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p:ze,i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function Yp(P){let a,m;return a=new _e({props:{$$slots:{default:[Gp]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function Jp(P){let a,m,r,c,_,v,z,C,w,M,I,O,D,L;return D=new W({props:{code:`from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),{c(){a=l("p"),m=o("Use the "),r=l("a"),c=o("AutoModelForSequenceClassification"),_=o(" and "),v=l("a"),z=o("AutoTokenizer"),C=o(" to load the pretrained model and it\u2019s associated tokenizer (more on an "),w=l("code"),M=o("AutoClass"),I=o(" below):"),O=h(),y(D.$$.fragment),this.h()},l(x){a=i(x,"P",{});var S=p(a);m=n(S,"Use the "),r=i(S,"A",{href:!0});var g=p(r);c=n(g,"AutoModelForSequenceClassification"),g.forEach(s),_=n(S," and "),v=i(S,"A",{href:!0});var F=p(v);z=n(F,"AutoTokenizer"),F.forEach(s),C=n(S," to load the pretrained model and it\u2019s associated tokenizer (more on an "),w=i(S,"CODE",{});var R=p(w);M=n(R,"AutoClass"),R.forEach(s),I=n(S," below):"),S.forEach(s),O=d(x),b(D.$$.fragment,x),this.h()},h(){$(r,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),$(v,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer")},m(x,S){u(x,a,S),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,z),t(a,C),t(a,w),t(w,M),t(a,I),u(x,O,S),E(D,x,S),L=!0},p:ze,i(x){L||(A(D.$$.fragment,x),L=!0)},o(x){T(D.$$.fragment,x),L=!1},d(x){x&&s(a),x&&s(O),j(D,x)}}}function Qp(P){let a,m;return a=new _e({props:{$$slots:{default:[Jp]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function Bp(P){let a,m,r,c,_,v,z,C,w,M,I,O,D,L;return D=new W({props:{code:`from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),{c(){a=l("p"),m=o("Use the "),r=l("a"),c=o("TFAutoModelForSequenceClassification"),_=o(" and "),v=l("a"),z=o("AutoTokenizer"),C=o(" to load the pretrained model and it\u2019s associated tokenizer (more on an "),w=l("code"),M=o("TFAutoClass"),I=o(" below):"),O=h(),y(D.$$.fragment),this.h()},l(x){a=i(x,"P",{});var S=p(a);m=n(S,"Use the "),r=i(S,"A",{href:!0});var g=p(r);c=n(g,"TFAutoModelForSequenceClassification"),g.forEach(s),_=n(S," and "),v=i(S,"A",{href:!0});var F=p(v);z=n(F,"AutoTokenizer"),F.forEach(s),C=n(S," to load the pretrained model and it\u2019s associated tokenizer (more on an "),w=i(S,"CODE",{});var R=p(w);M=n(R,"TFAutoClass"),R.forEach(s),I=n(S," below):"),S.forEach(s),O=d(x),b(D.$$.fragment,x),this.h()},h(){$(r,"href","/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification"),$(v,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer")},m(x,S){u(x,a,S),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,z),t(a,C),t(a,w),t(w,M),t(a,I),u(x,O,S),E(D,x,S),L=!0},p:ze,i(x){L||(A(D.$$.fragment,x),L=!0)},o(x){T(D.$$.fragment,x),L=!1},d(x){x&&s(a),x&&s(O),j(D,x)}}}function Vp(P){let a,m;return a=new _e({props:{$$slots:{default:[Bp]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function Kp(P){let a,m;return a=new W({props:{code:`pt_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p:ze,i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function Zp(P){let a,m;return a=new _e({props:{$$slots:{default:[Kp]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function Xp(P){let a,m;return a=new W({props:{code:`tf_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p:ze,i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function ef(P){let a,m;return a=new _e({props:{$$slots:{default:[Xp]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function tf(P){let a,m,r,c,_,v,z,C;return{c(){a=l("p"),m=o("See the "),r=l("a"),c=o("task summary"),_=o(" for which "),v=l("a"),z=o("AutoModel"),C=o(" class to use for which task."),this.h()},l(w){a=i(w,"P",{});var M=p(a);m=n(M,"See the "),r=i(M,"A",{href:!0});var I=p(r);c=n(I,"task summary"),I.forEach(s),_=n(M," for which "),v=i(M,"A",{href:!0});var O=p(v);z=n(O,"AutoModel"),O.forEach(s),C=n(M," class to use for which task."),M.forEach(s),this.h()},h(){$(r,"href","./task_summary"),$(v,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoModel")},m(w,M){u(w,a,M),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,z),t(a,C)},d(w){w&&s(a)}}}function sf(P){let a,m,r,c,_,v,z,C,w,M,I,O,D,L,x,S,g,F,R,U,Q,J,se,B,G,ee,V,K,ce,re,de,oe,te,ne,$e,q,N,le;return S=new W({props:{code:`from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`}}),F=new Cs({props:{$$slots:{default:[tf]},$$scope:{ctx:P}}}),ee=new W({props:{code:"pt_outputs = pt_model(**pt_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)'}}),N=new W({props:{code:`from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">0.0021</span>, <span class="hljs-number">0.0018</span>, <span class="hljs-number">0.0115</span>, <span class="hljs-number">0.2121</span>, <span class="hljs-number">0.7725</span>],
        [<span class="hljs-number">0.2084</span>, <span class="hljs-number">0.1826</span>, <span class="hljs-number">0.1969</span>, <span class="hljs-number">0.1755</span>, <span class="hljs-number">0.2365</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)`}}),{c(){a=l("p"),m=o("\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=l("a"),c=o("AutoModel"),_=o(" like you would load an "),v=l("a"),z=o("AutoTokenizer"),C=o(". The only difference is selecting the correct "),w=l("a"),M=o("AutoModel"),I=o(" for the task. Since you are doing text - or sequence - classification, load "),O=l("a"),D=o("AutoModelForSequenceClassification"),L=o(":"),x=h(),y(S.$$.fragment),g=h(),y(F.$$.fragment),R=h(),U=l("p"),Q=o("Now you can pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding "),J=l("code"),se=o("**"),B=o(":"),G=h(),y(ee.$$.fragment),V=h(),K=l("p"),ce=o("The model outputs the final activations in the "),re=l("code"),de=o("logits"),oe=o(" attribute. Apply the softmax function to the "),te=l("code"),ne=o("logits"),$e=o(" to retrieve the probabilities:"),q=h(),y(N.$$.fragment),this.h()},l(k){a=i(k,"P",{});var H=p(a);m=n(H,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=i(H,"A",{href:!0});var ie=p(r);c=n(ie,"AutoModel"),ie.forEach(s),_=n(H," like you would load an "),v=i(H,"A",{href:!0});var Pe=p(v);z=n(Pe,"AutoTokenizer"),Pe.forEach(s),C=n(H,". The only difference is selecting the correct "),w=i(H,"A",{href:!0});var he=p(w);M=n(he,"AutoModel"),he.forEach(s),I=n(H," for the task. Since you are doing text - or sequence - classification, load "),O=i(H,"A",{href:!0});var ge=p(O);D=n(ge,"AutoModelForSequenceClassification"),ge.forEach(s),L=n(H,":"),H.forEach(s),x=d(k),b(S.$$.fragment,k),g=d(k),b(F.$$.fragment,k),R=d(k),U=i(k,"P",{});var pe=p(U);Q=n(pe,"Now you can pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding "),J=i(pe,"CODE",{});var De=p(J);se=n(De,"**"),De.forEach(s),B=n(pe,":"),pe.forEach(s),G=d(k),b(ee.$$.fragment,k),V=d(k),K=i(k,"P",{});var ve=p(K);ce=n(ve,"The model outputs the final activations in the "),re=i(ve,"CODE",{});var Qt=p(re);de=n(Qt,"logits"),Qt.forEach(s),oe=n(ve," attribute. Apply the softmax function to the "),te=i(ve,"CODE",{});var dt=p(te);ne=n(dt,"logits"),dt.forEach(s),$e=n(ve," to retrieve the probabilities:"),ve.forEach(s),q=d(k),b(N.$$.fragment,k),this.h()},h(){$(r,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoModel"),$(v,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer"),$(w,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoModel"),$(O,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification")},m(k,H){u(k,a,H),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,z),t(a,C),t(a,w),t(w,M),t(a,I),t(a,O),t(O,D),t(a,L),u(k,x,H),E(S,k,H),u(k,g,H),E(F,k,H),u(k,R,H),u(k,U,H),t(U,Q),t(U,J),t(J,se),t(U,B),u(k,G,H),E(ee,k,H),u(k,V,H),u(k,K,H),t(K,ce),t(K,re),t(re,de),t(K,oe),t(K,te),t(te,ne),t(K,$e),u(k,q,H),E(N,k,H),le=!0},p(k,H){const ie={};H&2&&(ie.$$scope={dirty:H,ctx:k}),F.$set(ie)},i(k){le||(A(S.$$.fragment,k),A(F.$$.fragment,k),A(ee.$$.fragment,k),A(N.$$.fragment,k),le=!0)},o(k){T(S.$$.fragment,k),T(F.$$.fragment,k),T(ee.$$.fragment,k),T(N.$$.fragment,k),le=!1},d(k){k&&s(a),k&&s(x),j(S,k),k&&s(g),j(F,k),k&&s(R),k&&s(U),k&&s(G),j(ee,k),k&&s(V),k&&s(K),k&&s(q),j(N,k)}}}function af(P){let a,m;return a=new _e({props:{$$slots:{default:[sf]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function rf(P){let a,m,r,c,_,v,z,C;return{c(){a=l("p"),m=o("See the "),r=l("a"),c=o("task summary"),_=o(" for which "),v=l("a"),z=o("AutoModel"),C=o(" class to use for which task."),this.h()},l(w){a=i(w,"P",{});var M=p(a);m=n(M,"See the "),r=i(M,"A",{href:!0});var I=p(r);c=n(I,"task summary"),I.forEach(s),_=n(M," for which "),v=i(M,"A",{href:!0});var O=p(v);z=n(O,"AutoModel"),O.forEach(s),C=n(M," class to use for which task."),M.forEach(s),this.h()},h(){$(r,"href","./task_summary"),$(v,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoModel")},m(w,M){u(w,a,M),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,z),t(a,C)},d(w){w&&s(a)}}}function of(P){let a,m,r,c,_,v,z,C,w,M,I,O,D,L,x,S,g,F,R,U,Q,J,se,B,G,ee,V,K,ce,re,de,oe,te,ne,$e;return S=new W({props:{code:`from transformers import TFAutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`}}),F=new Cs({props:{$$slots:{default:[rf]},$$scope:{ctx:P}}}),se=new W({props:{code:"tf_outputs = tf_model(tf_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)'}}),ne=new W({props:{code:`import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
tf_predictions`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions`}}),{c(){a=l("p"),m=o("\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=l("a"),c=o("TFAutoModel"),_=o(" like you would load an "),v=l("a"),z=o("AutoTokenizer"),C=o(". The only difference is selecting the correct "),w=l("a"),M=o("TFAutoModel"),I=o(" for the task. Since you are doing text - or sequence - classification, load "),O=l("a"),D=o("TFAutoModelForSequenceClassification"),L=o(":"),x=h(),y(S.$$.fragment),g=h(),y(F.$$.fragment),R=h(),U=l("p"),Q=o("Now you can pass your preprocessed batch of inputs directly to the model by passing the dictionary keys directly to the tensors:"),J=h(),y(se.$$.fragment),B=h(),G=l("p"),ee=o("The model outputs the final activations in the "),V=l("code"),K=o("logits"),ce=o(" attribute. Apply the softmax function to the "),re=l("code"),de=o("logits"),oe=o(" to retrieve the probabilities:"),te=h(),y(ne.$$.fragment),this.h()},l(q){a=i(q,"P",{});var N=p(a);m=n(N,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),r=i(N,"A",{href:!0});var le=p(r);c=n(le,"TFAutoModel"),le.forEach(s),_=n(N," like you would load an "),v=i(N,"A",{href:!0});var k=p(v);z=n(k,"AutoTokenizer"),k.forEach(s),C=n(N,". The only difference is selecting the correct "),w=i(N,"A",{href:!0});var H=p(w);M=n(H,"TFAutoModel"),H.forEach(s),I=n(N," for the task. Since you are doing text - or sequence - classification, load "),O=i(N,"A",{href:!0});var ie=p(O);D=n(ie,"TFAutoModelForSequenceClassification"),ie.forEach(s),L=n(N,":"),N.forEach(s),x=d(q),b(S.$$.fragment,q),g=d(q),b(F.$$.fragment,q),R=d(q),U=i(q,"P",{});var Pe=p(U);Q=n(Pe,"Now you can pass your preprocessed batch of inputs directly to the model by passing the dictionary keys directly to the tensors:"),Pe.forEach(s),J=d(q),b(se.$$.fragment,q),B=d(q),G=i(q,"P",{});var he=p(G);ee=n(he,"The model outputs the final activations in the "),V=i(he,"CODE",{});var ge=p(V);K=n(ge,"logits"),ge.forEach(s),ce=n(he," attribute. Apply the softmax function to the "),re=i(he,"CODE",{});var pe=p(re);de=n(pe,"logits"),pe.forEach(s),oe=n(he," to retrieve the probabilities:"),he.forEach(s),te=d(q),b(ne.$$.fragment,q),this.h()},h(){$(r,"href","/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModel"),$(v,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer"),$(w,"href","/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModel"),$(O,"href","/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification")},m(q,N){u(q,a,N),t(a,m),t(a,r),t(r,c),t(a,_),t(a,v),t(v,z),t(a,C),t(a,w),t(w,M),t(a,I),t(a,O),t(O,D),t(a,L),u(q,x,N),E(S,q,N),u(q,g,N),E(F,q,N),u(q,R,N),u(q,U,N),t(U,Q),u(q,J,N),E(se,q,N),u(q,B,N),u(q,G,N),t(G,ee),t(G,V),t(V,K),t(G,ce),t(G,re),t(re,de),t(G,oe),u(q,te,N),E(ne,q,N),$e=!0},p(q,N){const le={};N&2&&(le.$$scope={dirty:N,ctx:q}),F.$set(le)},i(q){$e||(A(S.$$.fragment,q),A(F.$$.fragment,q),A(se.$$.fragment,q),A(ne.$$.fragment,q),$e=!0)},o(q){T(S.$$.fragment,q),T(F.$$.fragment,q),T(se.$$.fragment,q),T(ne.$$.fragment,q),$e=!1},d(q){q&&s(a),q&&s(x),j(S,q),q&&s(g),j(F,q),q&&s(R),q&&s(U),q&&s(J),j(se,q),q&&s(B),q&&s(G),q&&s(te),j(ne,q)}}}function nf(P){let a,m;return a=new _e({props:{$$slots:{default:[of]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function lf(P){let a,m,r,c,_;return{c(){a=l("p"),m=o("All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),r=l("em"),c=o("before"),_=o(` the final activation
function (like softmax) because the final activation function is often fused with the loss.`)},l(v){a=i(v,"P",{});var z=p(a);m=n(z,"All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),r=i(z,"EM",{});var C=p(r);c=n(C,"before"),C.forEach(s),_=n(z,` the final activation
function (like softmax) because the final activation function is often fused with the loss.`),z.forEach(s)},m(v,z){u(v,a,z),t(a,m),t(a,r),t(r,c),t(a,_)},d(v){v&&s(a)}}}function pf(P){let a,m,r,c,_;return{c(){a=l("p"),m=o(`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),r=l("code"),c=o("None"),_=o(" are ignored.")},l(v){a=i(v,"P",{});var z=p(a);m=n(z,`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),r=i(z,"CODE",{});var C=p(r);c=n(C,"None"),C.forEach(s),_=n(z," are ignored."),z.forEach(s)},m(v,z){u(v,a,z),t(a,m),t(a,r),t(r,c),t(a,_)},d(v){v&&s(a)}}}function ff(P){let a,m,r,c,_,v,z,C,w,M,I,O,D,L,x,S;return z=new W({props:{code:`pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)`}}),x=new W({props:{code:'pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)'}}),{c(){a=l("p"),m=o("Once your model is fine-tuned, you can save it with its tokenizer using "),r=l("a"),c=o("PreTrainedModel.save_pretrained()"),_=o(":"),v=h(),y(z.$$.fragment),C=h(),w=l("p"),M=o("When you are ready to use the model again, reload it with "),I=l("a"),O=o("PreTrainedModel.from_pretrained()"),D=o(":"),L=h(),y(x.$$.fragment),this.h()},l(g){a=i(g,"P",{});var F=p(a);m=n(F,"Once your model is fine-tuned, you can save it with its tokenizer using "),r=i(F,"A",{href:!0});var R=p(r);c=n(R,"PreTrainedModel.save_pretrained()"),R.forEach(s),_=n(F,":"),F.forEach(s),v=d(g),b(z.$$.fragment,g),C=d(g),w=i(g,"P",{});var U=p(w);M=n(U,"When you are ready to use the model again, reload it with "),I=i(U,"A",{href:!0});var Q=p(I);O=n(Q,"PreTrainedModel.from_pretrained()"),Q.forEach(s),D=n(U,":"),U.forEach(s),L=d(g),b(x.$$.fragment,g),this.h()},h(){$(r,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),$(I,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained")},m(g,F){u(g,a,F),t(a,m),t(a,r),t(r,c),t(a,_),u(g,v,F),E(z,g,F),u(g,C,F),u(g,w,F),t(w,M),t(w,I),t(I,O),t(w,D),u(g,L,F),E(x,g,F),S=!0},p:ze,i(g){S||(A(z.$$.fragment,g),A(x.$$.fragment,g),S=!0)},o(g){T(z.$$.fragment,g),T(x.$$.fragment,g),S=!1},d(g){g&&s(a),g&&s(v),j(z,g),g&&s(C),g&&s(w),g&&s(L),j(x,g)}}}function uf(P){let a,m;return a=new _e({props:{$$slots:{default:[ff]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function mf(P){let a,m,r,c,_,v,z,C,w,M,I,O,D,L,x,S;return z=new W({props:{code:`tf_save_directory = "./tf_save_pretrained"
tokenizer.save_pretrained(tf_save_directory)
tf_model.save_pretrained(tf_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`}}),x=new W({props:{code:'tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)'}}),{c(){a=l("p"),m=o("Once your model is fine-tuned, you can save it with its tokenizer using "),r=l("a"),c=o("TFPreTrainedModel.save_pretrained()"),_=o(":"),v=h(),y(z.$$.fragment),C=h(),w=l("p"),M=o("When you are ready to use the model again, reload it with "),I=l("a"),O=o("TFPreTrainedModel.from_pretrained()"),D=o(":"),L=h(),y(x.$$.fragment),this.h()},l(g){a=i(g,"P",{});var F=p(a);m=n(F,"Once your model is fine-tuned, you can save it with its tokenizer using "),r=i(F,"A",{href:!0});var R=p(r);c=n(R,"TFPreTrainedModel.save_pretrained()"),R.forEach(s),_=n(F,":"),F.forEach(s),v=d(g),b(z.$$.fragment,g),C=d(g),w=i(g,"P",{});var U=p(w);M=n(U,"When you are ready to use the model again, reload it with "),I=i(U,"A",{href:!0});var Q=p(I);O=n(Q,"TFPreTrainedModel.from_pretrained()"),Q.forEach(s),D=n(U,":"),U.forEach(s),L=d(g),b(x.$$.fragment,g),this.h()},h(){$(r,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained"),$(I,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained")},m(g,F){u(g,a,F),t(a,m),t(a,r),t(r,c),t(a,_),u(g,v,F),E(z,g,F),u(g,C,F),u(g,w,F),t(w,M),t(w,I),t(I,O),t(w,D),u(g,L,F),E(x,g,F),S=!0},p:ze,i(g){S||(A(z.$$.fragment,g),A(x.$$.fragment,g),S=!0)},o(g){T(z.$$.fragment,g),T(x.$$.fragment,g),S=!1},d(g){g&&s(a),g&&s(v),j(z,g),g&&s(C),g&&s(w),g&&s(L),j(x,g)}}}function cf(P){let a,m;return a=new _e({props:{$$slots:{default:[mf]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function hf(P){let a,m;return a=new W({props:{code:`from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p:ze,i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function df(P){let a,m;return a=new _e({props:{$$slots:{default:[hf]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function $f(P){let a,m;return a=new W({props:{code:`from transformers import TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p:ze,i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function _f(P){let a,m;return a=new _e({props:{$$slots:{default:[$f]},$$scope:{ctx:P}}}),{c(){y(a.$$.fragment)},l(r){b(a.$$.fragment,r)},m(r,c){E(a,r,c),m=!0},p(r,c){const _={};c&2&&(_.$$scope={dirty:c,ctx:r}),a.$set(_)},i(r){m||(A(a.$$.fragment,r),m=!0)},o(r){T(a.$$.fragment,r),m=!1},d(r){j(a,r)}}}function gf(P){let a,m,r,c,_,v,z,C,w,M,I,O,D,L,x,S,g,F,R,U,Q,J,se,B,G,ee,V,K,ce,re,de,oe,te,ne,$e,q,N,le,k,H,ie,Pe,he,ge,pe,De,ve,Qt,dt,Y,Is,eo,to,Os,so,ao,Ns,ro,oo,Ds,no,lo,Hs,io,po,Ls,fo,uo,Ws,mo,co,Us,ho,qa,$t,Rs,$o,_o,za,we,Gs,go,vo,Ys,wo,ko,Js,yo,Pa,_t,Qs,bo,Eo,Fa,He,Bs,Ao,To,Vs,jo,Ma,Le,Sa,Fe,We,Ks,gt,xo,Zs,qo,Ca,Ue,zo,Bt,Po,Fo,Ia,Vt,Mo,Oa,Re,Na,Ge,So,Kt,Co,Io,Da,vt,Ha,ke,Oo,wt,No,Do,Xs,Ho,Lo,La,kt,Wa,Ye,Wo,Zt,Uo,Ro,Ua,yt,Ra,ye,Go,Xt,Yo,Jo,bt,Qo,Bo,Ga,Et,Ya,Je,Vo,es,Ko,Zo,Ja,At,Qa,be,Xo,Tt,en,tn,jt,sn,an,Ba,xt,Va,Qe,rn,ea,on,nn,Ka,qt,Za,Be,ln,ta,pn,fn,Xa,zt,er,Ve,un,ts,mn,cn,tr,Me,Ke,sa,Pt,hn,aa,dn,sr,fe,$n,ss,_n,gn,Ft,vn,wn,as,kn,yn,Mt,bn,En,ar,St,rr,Ze,or,Ee,An,rs,Tn,jn,ra,xn,qn,nr,Ct,lr,Ae,zn,os,Pn,Fn,ns,Mn,Sn,ir,Se,Xe,oa,It,Cn,na,In,pr,Ot,fr,Z,On,ls,Nn,Dn,is,Hn,Ln,ps,Wn,Un,fs,Rn,Gn,la,Yn,Jn,us,Qn,Bn,ur,Te,Vn,ia,Kn,Zn,ms,Xn,el,mr,Ce,et,pa,Nt,tl,fa,sl,cr,je,al,ua,rl,ol,cs,nl,ll,hr,tt,il,hs,pl,fl,dr,Dt,$r,st,ul,ma,ml,cl,_r,ds,hl,gr,Ht,vr,$s,dl,wr,at,_s,gs,$l,_l,gl,vs,ws,vl,wl,kr,rt,kl,ks,yl,bl,yr,ot,br,nt,El,ys,Al,Tl,Er,Ie,lt,ca,Lt,jl,ha,xl,Ar,it,Tr,pt,jr,X,ql,Wt,da,zl,Pl,Ut,$a,Fl,Ml,bs,Sl,Cl,_a,Il,Ol,Rt,Nl,Dl,Es,Hl,Ll,xr,ft,qr,Oe,ut,ga,Gt,Wl,va,Ul,zr,mt,Pr,xe,Rl,wa,Gl,Yl,ka,Jl,Ql,Fr,ct,Mr;return v=new ht({}),I=new Hp({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/quicktour.ipynb"}]}}),J=new Cs({props:{$$slots:{default:[Lp]},$$scope:{ctx:P}}}),V=new ht({}),N=new Sp({props:{id:"tiZFewofSLM"}}),Le=new Cs({props:{$$slots:{default:[Wp]},$$scope:{ctx:P}}}),gt=new ht({}),Re=new Ss({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Yp],pytorch:[Rp]},$$scope:{ctx:P}}}),vt=new W({props:{code:`from transformers import pipeline

classifier = pipeline("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),kt=new W({props:{code:'classifier("We are very happy to show you the \u{1F917} Transformers library.")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9998</span>}]`}}),yt=new W({props:{code:`results = classifier(["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">{result[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, with score: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>`}}),Et=new W({props:{code:"pip install datasets ",highlighted:"pip install datasets "}}),At=new W({props:{code:`import torch
from transformers import pipeline

speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)`}}),xt=new W({props:{code:`from datasets import load_dataset, Audio

dataset = load_dataset("PolyAI/minds14", name="en-US", split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),qt=new W({props:{code:'dataset = dataset.cast_column("audio", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))'}}),zt=new W({props:{code:`result = speech_recognizer(dataset[:4]["audio"])
print([d["text"] for d in result])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>result = speech_recognizer(dataset[:<span class="hljs-number">4</span>][<span class="hljs-string">&quot;audio&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>([d[<span class="hljs-string">&quot;text&quot;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> result])
[<span class="hljs-string">&#x27;I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT&#x27;</span>, <span class="hljs-string">&quot;FONDERING HOW I&#x27;D SET UP A JOIN TO HET WITH MY WIFE AND WHERE THE AP MIGHT BE&quot;</span>, <span class="hljs-string">&quot;I I&#x27;D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I&#x27;M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AND I&#x27;M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS&quot;</span>, <span class="hljs-string">&#x27;HOW DO I TURN A JOIN A COUNT&#x27;</span>]`}}),Pt=new ht({}),St=new W({props:{code:'model_name = "nlptown/bert-base-multilingual-uncased-sentiment"',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>'}}),Ze=new Ss({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Vp],pytorch:[Qp]},$$scope:{ctx:P}}}),Ct=new W({props:{code:`classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;5 stars&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.7273</span>}]`}}),It=new ht({}),Ot=new Sp({props:{id:"AhChOFRegn4"}}),Nt=new ht({}),Dt=new W({props:{code:`from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),Ht=new W({props:{code:`encoding = tokenizer("We are very happy to show you the \u{1F917} Transformers library.")
print(encoding)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">11312</span>, <span class="hljs-number">10320</span>, <span class="hljs-number">12495</span>, <span class="hljs-number">19308</span>, <span class="hljs-number">10114</span>, <span class="hljs-number">11391</span>, <span class="hljs-number">10855</span>, <span class="hljs-number">10103</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">13299</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),ot=new Ss({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[ef],pytorch:[Zp]},$$scope:{ctx:P}}}),Lt=new ht({}),it=new Ss({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[nf],pytorch:[af]},$$scope:{ctx:P}}}),pt=new Cs({props:{$$slots:{default:[lf]},$$scope:{ctx:P}}}),ft=new Cs({props:{$$slots:{default:[pf]},$$scope:{ctx:P}}}),Gt=new ht({}),mt=new Ss({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[cf],pytorch:[uf]},$$scope:{ctx:P}}}),ct=new Ss({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[_f],pytorch:[df]},$$scope:{ctx:P}}}),{c(){a=l("meta"),m=h(),r=l("h1"),c=l("a"),_=l("span"),y(v.$$.fragment),z=h(),C=l("span"),w=o("Quick tour"),M=h(),y(I.$$.fragment),O=h(),D=l("p"),L=o("Get up and running with \u{1F917} Transformers! Start using the "),x=l("a"),S=o("pipeline()"),g=o(" for rapid inference, and quickly load a pretrained model and tokenizer with an "),F=l("a"),R=o("AutoClass"),U=o(" to solve your text, vision or audio task."),Q=h(),y(J.$$.fragment),se=h(),B=l("h2"),G=l("a"),ee=l("span"),y(V.$$.fragment),K=h(),ce=l("span"),re=o("Pipeline"),de=h(),oe=l("p"),te=l("a"),ne=o("pipeline()"),$e=o(" is the easiest way to use a pretrained model for a given task."),q=h(),y(N.$$.fragment),le=h(),k=l("p"),H=o("The "),ie=l("a"),Pe=o("pipeline()"),he=o(" supports many common tasks out-of-the-box:"),ge=h(),pe=l("p"),De=l("strong"),ve=o("Text"),Qt=o(":"),dt=h(),Y=l("ul"),Is=l("li"),eo=o("Sentiment analysis: classify the polarity of a given text."),to=h(),Os=l("li"),so=o("Text generation (in English): generate text from a given input."),ao=h(),Ns=l("li"),ro=o("Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),oo=h(),Ds=l("li"),no=o("Question answering: extract the answer from the context, given some context and a question."),lo=h(),Hs=l("li"),io=o("Fill-mask: fill in the blank given a text with masked words."),po=h(),Ls=l("li"),fo=o("Summarization: generate a summary of a long sequence of text or document."),uo=h(),Ws=l("li"),mo=o("Translation: translate text into another language."),co=h(),Us=l("li"),ho=o("Feature extraction: create a tensor representation of the text."),qa=h(),$t=l("p"),Rs=l("strong"),$o=o("Image"),_o=o(":"),za=h(),we=l("ul"),Gs=l("li"),go=o("Image classification: classify an image."),vo=h(),Ys=l("li"),wo=o("Image segmentation: classify every pixel in an image."),ko=h(),Js=l("li"),yo=o("Object detection: detect objects within an image."),Pa=h(),_t=l("p"),Qs=l("strong"),bo=o("Audio"),Eo=o(":"),Fa=h(),He=l("ul"),Bs=l("li"),Ao=o("Audio classification: assign a label to a given segment of audio."),To=h(),Vs=l("li"),jo=o("Automatic speech recognition (ASR): transcribe audio data into text."),Ma=h(),y(Le.$$.fragment),Sa=h(),Fe=l("h3"),We=l("a"),Ks=l("span"),y(gt.$$.fragment),xo=h(),Zs=l("span"),qo=o("Pipeline usage"),Ca=h(),Ue=l("p"),zo=o("In the following example, you will use the "),Bt=l("a"),Po=o("pipeline()"),Fo=o(" for sentiment analysis."),Ia=h(),Vt=l("p"),Mo=o("Install the following dependencies if you haven\u2019t already:"),Oa=h(),y(Re.$$.fragment),Na=h(),Ge=l("p"),So=o("Import "),Kt=l("a"),Co=o("pipeline()"),Io=o(" and specify the task you want to complete:"),Da=h(),y(vt.$$.fragment),Ha=h(),ke=l("p"),Oo=o("The pipeline downloads and caches a default "),wt=l("a"),No=o("pretrained model"),Do=o(" and tokenizer for sentiment analysis. Now you can use the "),Xs=l("code"),Ho=o("classifier"),Lo=o(" on your target text:"),La=h(),y(kt.$$.fragment),Wa=h(),Ye=l("p"),Wo=o("For more than one sentence, pass a list of sentences to the "),Zt=l("a"),Uo=o("pipeline()"),Ro=o(" which returns a list of dictionaries:"),Ua=h(),y(yt.$$.fragment),Ra=h(),ye=l("p"),Go=o("The "),Xt=l("a"),Yo=o("pipeline()"),Jo=o(" can also iterate over an entire dataset. Start by installing the "),bt=l("a"),Qo=o("\u{1F917} Datasets"),Bo=o(" library:"),Ga=h(),y(Et.$$.fragment),Ya=h(),Je=l("p"),Vo=o("Create a "),es=l("a"),Ko=o("pipeline()"),Zo=o(" with the task you want to solve for and the model you want to use."),Ja=h(),y(At.$$.fragment),Qa=h(),be=l("p"),Xo=o("Next, load a dataset (see the \u{1F917} Datasets "),Tt=l("a"),en=o("Quick Start"),tn=o(" for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),jt=l("a"),sn=o("MInDS-14"),an=o(" dataset:"),Ba=h(),y(xt.$$.fragment),Va=h(),Qe=l("p"),rn=o(`We need to make sure that the sampling rate of the dataset matches the sampling
rate `),ea=l("code"),on=o("facebook/wav2vec2-base-960h"),nn=o(" was trained on."),Ka=h(),y(qt.$$.fragment),Za=h(),Be=l("p"),ln=o("Audio files are automatically loaded and resampled when calling the "),ta=l("code"),pn=o('"audio"'),fn=o(` column.
Let\u2019s extract the raw waveform arrays of the first 4 samples and pass it as a list to the pipeline:`),Xa=h(),y(zt.$$.fragment),er=h(),Ve=l("p"),un=o("For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the "),ts=l("a"),mn=o("pipeline documentation"),cn=o(" for more information."),tr=h(),Me=l("h3"),Ke=l("a"),sa=l("span"),y(Pt.$$.fragment),hn=h(),aa=l("span"),dn=o("Use another model and tokenizer in the pipeline"),sr=h(),fe=l("p"),$n=o("The "),ss=l("a"),_n=o("pipeline()"),gn=o(" can accommodate any model from the "),Ft=l("a"),vn=o("Model Hub"),wn=o(", making it easy to adapt the "),as=l("a"),kn=o("pipeline()"),yn=o(" for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),Mt=l("a"),bn=o("BERT model"),En=o(" fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),ar=h(),y(St.$$.fragment),rr=h(),y(Ze.$$.fragment),or=h(),Ee=l("p"),An=o("Then you can specify the model and tokenizer in the "),rs=l("a"),Tn=o("pipeline()"),jn=o(", and apply the "),ra=l("code"),xn=o("classifier"),qn=o(" on your target text:"),nr=h(),y(Ct.$$.fragment),lr=h(),Ae=l("p"),zn=o("If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),os=l("a"),Pn=o("fine-tuning tutorial"),Fn=o(" to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),ns=l("a"),Mn=o("here"),Sn=o(") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),ir=h(),Se=l("h2"),Xe=l("a"),oa=l("span"),y(It.$$.fragment),Cn=h(),na=l("span"),In=o("AutoClass"),pr=h(),y(Ot.$$.fragment),fr=h(),Z=l("p"),On=o("Under the hood, the "),ls=l("a"),Nn=o("AutoModelForSequenceClassification"),Dn=o(" and "),is=l("a"),Hn=o("AutoTokenizer"),Ln=o(" classes work together to power the "),ps=l("a"),Wn=o("pipeline()"),Un=o(". An "),fs=l("a"),Rn=o("AutoClass"),Gn=o(" is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),la=l("code"),Yn=o("AutoClass"),Jn=o(" for your task and it\u2019s associated tokenizer with "),us=l("a"),Qn=o("AutoTokenizer"),Bn=o("."),ur=h(),Te=l("p"),Vn=o("Let\u2019s return to our example and see how you can use the "),ia=l("code"),Kn=o("AutoClass"),Zn=o(" to replicate the results of the "),ms=l("a"),Xn=o("pipeline()"),el=o("."),mr=h(),Ce=l("h3"),et=l("a"),pa=l("span"),y(Nt.$$.fragment),tl=h(),fa=l("span"),sl=o("AutoTokenizer"),cr=h(),je=l("p"),al=o("A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),ua=l("em"),rl=o("tokens"),ol=o(". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),cs=l("a"),nl=o("here"),ll=o("). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),hr=h(),tt=l("p"),il=o("Load a tokenizer with "),hs=l("a"),pl=o("AutoTokenizer"),fl=o(":"),dr=h(),y(Dt.$$.fragment),$r=h(),st=l("p"),ul=o("Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),ma=l("em"),ml=o("vocabulary"),cl=o("."),_r=h(),ds=l("p"),hl=o("Pass your text to the tokenizer:"),gr=h(),y(Ht.$$.fragment),vr=h(),$s=l("p"),dl=o("The tokenizer will return a dictionary containing:"),wr=h(),at=l("ul"),_s=l("li"),gs=l("a"),$l=o("input_ids"),_l=o(": numerical representions of your tokens."),gl=h(),vs=l("li"),ws=l("a"),vl=o("atttention_mask"),wl=o(": indicates which tokens should be attended to."),kr=h(),rt=l("p"),kl=o("Just like the "),ks=l("a"),yl=o("pipeline()"),bl=o(", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),yr=h(),y(ot.$$.fragment),br=h(),nt=l("p"),El=o("Read the "),ys=l("a"),Al=o("preprocessing"),Tl=o(" tutorial for more details about tokenization."),Er=h(),Ie=l("h3"),lt=l("a"),ca=l("span"),y(Lt.$$.fragment),jl=h(),ha=l("span"),xl=o("AutoModel"),Ar=h(),y(it.$$.fragment),Tr=h(),y(pt.$$.fragment),jr=h(),X=l("p"),ql=o("Models are a standard "),Wt=l("a"),da=l("code"),zl=o("torch.nn.Module"),Pl=o(" or a "),Ut=l("a"),$a=l("code"),Fl=o("tf.keras.Model"),Ml=o(" so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),bs=l("a"),Sl=o("Trainer"),Cl=o(" class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),_a=l("code"),Il=o("fit"),Ol=o(" method from "),Rt=l("a"),Nl=o("Keras"),Dl=o(". Refer to the "),Es=l("a"),Hl=o("training tutorial"),Ll=o(" for more details."),xr=h(),y(ft.$$.fragment),qr=h(),Oe=l("h3"),ut=l("a"),ga=l("span"),y(Gt.$$.fragment),Wl=h(),va=l("span"),Ul=o("Save a model"),zr=h(),y(mt.$$.fragment),Pr=h(),xe=l("p"),Rl=o("One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),wa=l("code"),Gl=o("from_pt"),Yl=o(" or "),ka=l("code"),Jl=o("from_tf"),Ql=o(" parameter can convert the model from one framework to the other:"),Fr=h(),y(ct.$$.fragment),this.h()},l(e){const f=Np('[data-svelte="svelte-1phssyn"]',document.head);a=i(f,"META",{name:!0,content:!0}),f.forEach(s),m=d(e),r=i(e,"H1",{class:!0});var Yt=p(r);c=i(Yt,"A",{id:!0,class:!0,href:!0});var ya=p(c);_=i(ya,"SPAN",{});var ba=p(_);b(v.$$.fragment,ba),ba.forEach(s),ya.forEach(s),z=d(Yt),C=i(Yt,"SPAN",{});var Ea=p(C);w=n(Ea,"Quick tour"),Ea.forEach(s),Yt.forEach(s),M=d(e),b(I.$$.fragment,e),O=d(e),D=i(e,"P",{});var Ne=p(D);L=n(Ne,"Get up and running with \u{1F917} Transformers! Start using the "),x=i(Ne,"A",{href:!0});var Aa=p(x);S=n(Aa,"pipeline()"),Aa.forEach(s),g=n(Ne," for rapid inference, and quickly load a pretrained model and tokenizer with an "),F=i(Ne,"A",{href:!0});var Ta=p(F);R=n(Ta,"AutoClass"),Ta.forEach(s),U=n(Ne," to solve your text, vision or audio task."),Ne.forEach(s),Q=d(e),b(J.$$.fragment,e),se=d(e),B=i(e,"H2",{class:!0});var Jt=p(B);G=i(Jt,"A",{id:!0,class:!0,href:!0});var ja=p(G);ee=i(ja,"SPAN",{});var xa=p(ee);b(V.$$.fragment,xa),xa.forEach(s),ja.forEach(s),K=d(Jt),ce=i(Jt,"SPAN",{});var ti=p(ce);re=n(ti,"Pipeline"),ti.forEach(s),Jt.forEach(s),de=d(e),oe=i(e,"P",{});var Bl=p(oe);te=i(Bl,"A",{href:!0});var si=p(te);ne=n(si,"pipeline()"),si.forEach(s),$e=n(Bl," is the easiest way to use a pretrained model for a given task."),Bl.forEach(s),q=d(e),b(N.$$.fragment,e),le=d(e),k=i(e,"P",{});var Sr=p(k);H=n(Sr,"The "),ie=i(Sr,"A",{href:!0});var ai=p(ie);Pe=n(ai,"pipeline()"),ai.forEach(s),he=n(Sr," supports many common tasks out-of-the-box:"),Sr.forEach(s),ge=d(e),pe=i(e,"P",{});var Vl=p(pe);De=i(Vl,"STRONG",{});var ri=p(De);ve=n(ri,"Text"),ri.forEach(s),Qt=n(Vl,":"),Vl.forEach(s),dt=d(e),Y=i(e,"UL",{});var ae=p(Y);Is=i(ae,"LI",{});var oi=p(Is);eo=n(oi,"Sentiment analysis: classify the polarity of a given text."),oi.forEach(s),to=d(ae),Os=i(ae,"LI",{});var ni=p(Os);so=n(ni,"Text generation (in English): generate text from a given input."),ni.forEach(s),ao=d(ae),Ns=i(ae,"LI",{});var li=p(Ns);ro=n(li,"Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),li.forEach(s),oo=d(ae),Ds=i(ae,"LI",{});var ii=p(Ds);no=n(ii,"Question answering: extract the answer from the context, given some context and a question."),ii.forEach(s),lo=d(ae),Hs=i(ae,"LI",{});var pi=p(Hs);io=n(pi,"Fill-mask: fill in the blank given a text with masked words."),pi.forEach(s),po=d(ae),Ls=i(ae,"LI",{});var fi=p(Ls);fo=n(fi,"Summarization: generate a summary of a long sequence of text or document."),fi.forEach(s),uo=d(ae),Ws=i(ae,"LI",{});var ui=p(Ws);mo=n(ui,"Translation: translate text into another language."),ui.forEach(s),co=d(ae),Us=i(ae,"LI",{});var mi=p(Us);ho=n(mi,"Feature extraction: create a tensor representation of the text."),mi.forEach(s),ae.forEach(s),qa=d(e),$t=i(e,"P",{});var Kl=p($t);Rs=i(Kl,"STRONG",{});var ci=p(Rs);$o=n(ci,"Image"),ci.forEach(s),_o=n(Kl,":"),Kl.forEach(s),za=d(e),we=i(e,"UL",{});var As=p(we);Gs=i(As,"LI",{});var hi=p(Gs);go=n(hi,"Image classification: classify an image."),hi.forEach(s),vo=d(As),Ys=i(As,"LI",{});var di=p(Ys);wo=n(di,"Image segmentation: classify every pixel in an image."),di.forEach(s),ko=d(As),Js=i(As,"LI",{});var $i=p(Js);yo=n($i,"Object detection: detect objects within an image."),$i.forEach(s),As.forEach(s),Pa=d(e),_t=i(e,"P",{});var Zl=p(_t);Qs=i(Zl,"STRONG",{});var _i=p(Qs);bo=n(_i,"Audio"),_i.forEach(s),Eo=n(Zl,":"),Zl.forEach(s),Fa=d(e),He=i(e,"UL",{});var Cr=p(He);Bs=i(Cr,"LI",{});var gi=p(Bs);Ao=n(gi,"Audio classification: assign a label to a given segment of audio."),gi.forEach(s),To=d(Cr),Vs=i(Cr,"LI",{});var vi=p(Vs);jo=n(vi,"Automatic speech recognition (ASR): transcribe audio data into text."),vi.forEach(s),Cr.forEach(s),Ma=d(e),b(Le.$$.fragment,e),Sa=d(e),Fe=i(e,"H3",{class:!0});var Ir=p(Fe);We=i(Ir,"A",{id:!0,class:!0,href:!0});var wi=p(We);Ks=i(wi,"SPAN",{});var ki=p(Ks);b(gt.$$.fragment,ki),ki.forEach(s),wi.forEach(s),xo=d(Ir),Zs=i(Ir,"SPAN",{});var yi=p(Zs);qo=n(yi,"Pipeline usage"),yi.forEach(s),Ir.forEach(s),Ca=d(e),Ue=i(e,"P",{});var Or=p(Ue);zo=n(Or,"In the following example, you will use the "),Bt=i(Or,"A",{href:!0});var bi=p(Bt);Po=n(bi,"pipeline()"),bi.forEach(s),Fo=n(Or," for sentiment analysis."),Or.forEach(s),Ia=d(e),Vt=i(e,"P",{});var Ei=p(Vt);Mo=n(Ei,"Install the following dependencies if you haven\u2019t already:"),Ei.forEach(s),Oa=d(e),b(Re.$$.fragment,e),Na=d(e),Ge=i(e,"P",{});var Nr=p(Ge);So=n(Nr,"Import "),Kt=i(Nr,"A",{href:!0});var Ai=p(Kt);Co=n(Ai,"pipeline()"),Ai.forEach(s),Io=n(Nr," and specify the task you want to complete:"),Nr.forEach(s),Da=d(e),b(vt.$$.fragment,e),Ha=d(e),ke=i(e,"P",{});var Ts=p(ke);Oo=n(Ts,"The pipeline downloads and caches a default "),wt=i(Ts,"A",{href:!0,rel:!0});var Ti=p(wt);No=n(Ti,"pretrained model"),Ti.forEach(s),Do=n(Ts," and tokenizer for sentiment analysis. Now you can use the "),Xs=i(Ts,"CODE",{});var ji=p(Xs);Ho=n(ji,"classifier"),ji.forEach(s),Lo=n(Ts," on your target text:"),Ts.forEach(s),La=d(e),b(kt.$$.fragment,e),Wa=d(e),Ye=i(e,"P",{});var Dr=p(Ye);Wo=n(Dr,"For more than one sentence, pass a list of sentences to the "),Zt=i(Dr,"A",{href:!0});var xi=p(Zt);Uo=n(xi,"pipeline()"),xi.forEach(s),Ro=n(Dr," which returns a list of dictionaries:"),Dr.forEach(s),Ua=d(e),b(yt.$$.fragment,e),Ra=d(e),ye=i(e,"P",{});var js=p(ye);Go=n(js,"The "),Xt=i(js,"A",{href:!0});var qi=p(Xt);Yo=n(qi,"pipeline()"),qi.forEach(s),Jo=n(js," can also iterate over an entire dataset. Start by installing the "),bt=i(js,"A",{href:!0,rel:!0});var zi=p(bt);Qo=n(zi,"\u{1F917} Datasets"),zi.forEach(s),Bo=n(js," library:"),js.forEach(s),Ga=d(e),b(Et.$$.fragment,e),Ya=d(e),Je=i(e,"P",{});var Hr=p(Je);Vo=n(Hr,"Create a "),es=i(Hr,"A",{href:!0});var Pi=p(es);Ko=n(Pi,"pipeline()"),Pi.forEach(s),Zo=n(Hr," with the task you want to solve for and the model you want to use."),Hr.forEach(s),Ja=d(e),b(At.$$.fragment,e),Qa=d(e),be=i(e,"P",{});var xs=p(be);Xo=n(xs,"Next, load a dataset (see the \u{1F917} Datasets "),Tt=i(xs,"A",{href:!0,rel:!0});var Fi=p(Tt);en=n(Fi,"Quick Start"),Fi.forEach(s),tn=n(xs," for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),jt=i(xs,"A",{href:!0,rel:!0});var Mi=p(jt);sn=n(Mi,"MInDS-14"),Mi.forEach(s),an=n(xs," dataset:"),xs.forEach(s),Ba=d(e),b(xt.$$.fragment,e),Va=d(e),Qe=i(e,"P",{});var Lr=p(Qe);rn=n(Lr,`We need to make sure that the sampling rate of the dataset matches the sampling
rate `),ea=i(Lr,"CODE",{});var Si=p(ea);on=n(Si,"facebook/wav2vec2-base-960h"),Si.forEach(s),nn=n(Lr," was trained on."),Lr.forEach(s),Ka=d(e),b(qt.$$.fragment,e),Za=d(e),Be=i(e,"P",{});var Wr=p(Be);ln=n(Wr,"Audio files are automatically loaded and resampled when calling the "),ta=i(Wr,"CODE",{});var Ci=p(ta);pn=n(Ci,'"audio"'),Ci.forEach(s),fn=n(Wr,` column.
Let\u2019s extract the raw waveform arrays of the first 4 samples and pass it as a list to the pipeline:`),Wr.forEach(s),Xa=d(e),b(zt.$$.fragment,e),er=d(e),Ve=i(e,"P",{});var Ur=p(Ve);un=n(Ur,"For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the "),ts=i(Ur,"A",{href:!0});var Ii=p(ts);mn=n(Ii,"pipeline documentation"),Ii.forEach(s),cn=n(Ur," for more information."),Ur.forEach(s),tr=d(e),Me=i(e,"H3",{class:!0});var Rr=p(Me);Ke=i(Rr,"A",{id:!0,class:!0,href:!0});var Oi=p(Ke);sa=i(Oi,"SPAN",{});var Ni=p(sa);b(Pt.$$.fragment,Ni),Ni.forEach(s),Oi.forEach(s),hn=d(Rr),aa=i(Rr,"SPAN",{});var Di=p(aa);dn=n(Di,"Use another model and tokenizer in the pipeline"),Di.forEach(s),Rr.forEach(s),sr=d(e),fe=i(e,"P",{});var qe=p(fe);$n=n(qe,"The "),ss=i(qe,"A",{href:!0});var Hi=p(ss);_n=n(Hi,"pipeline()"),Hi.forEach(s),gn=n(qe," can accommodate any model from the "),Ft=i(qe,"A",{href:!0,rel:!0});var Li=p(Ft);vn=n(Li,"Model Hub"),Li.forEach(s),wn=n(qe,", making it easy to adapt the "),as=i(qe,"A",{href:!0});var Wi=p(as);kn=n(Wi,"pipeline()"),Wi.forEach(s),yn=n(qe," for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),Mt=i(qe,"A",{href:!0,rel:!0});var Ui=p(Mt);bn=n(Ui,"BERT model"),Ui.forEach(s),En=n(qe," fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),qe.forEach(s),ar=d(e),b(St.$$.fragment,e),rr=d(e),b(Ze.$$.fragment,e),or=d(e),Ee=i(e,"P",{});var qs=p(Ee);An=n(qs,"Then you can specify the model and tokenizer in the "),rs=i(qs,"A",{href:!0});var Ri=p(rs);Tn=n(Ri,"pipeline()"),Ri.forEach(s),jn=n(qs,", and apply the "),ra=i(qs,"CODE",{});var Gi=p(ra);xn=n(Gi,"classifier"),Gi.forEach(s),qn=n(qs," on your target text:"),qs.forEach(s),nr=d(e),b(Ct.$$.fragment,e),lr=d(e),Ae=i(e,"P",{});var zs=p(Ae);zn=n(zs,"If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),os=i(zs,"A",{href:!0});var Yi=p(os);Pn=n(Yi,"fine-tuning tutorial"),Yi.forEach(s),Fn=n(zs," to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),ns=i(zs,"A",{href:!0});var Ji=p(ns);Mn=n(Ji,"here"),Ji.forEach(s),Sn=n(zs,") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),zs.forEach(s),ir=d(e),Se=i(e,"H2",{class:!0});var Gr=p(Se);Xe=i(Gr,"A",{id:!0,class:!0,href:!0});var Qi=p(Xe);oa=i(Qi,"SPAN",{});var Bi=p(oa);b(It.$$.fragment,Bi),Bi.forEach(s),Qi.forEach(s),Cn=d(Gr),na=i(Gr,"SPAN",{});var Vi=p(na);In=n(Vi,"AutoClass"),Vi.forEach(s),Gr.forEach(s),pr=d(e),b(Ot.$$.fragment,e),fr=d(e),Z=i(e,"P",{});var ue=p(Z);On=n(ue,"Under the hood, the "),ls=i(ue,"A",{href:!0});var Ki=p(ls);Nn=n(Ki,"AutoModelForSequenceClassification"),Ki.forEach(s),Dn=n(ue," and "),is=i(ue,"A",{href:!0});var Zi=p(is);Hn=n(Zi,"AutoTokenizer"),Zi.forEach(s),Ln=n(ue," classes work together to power the "),ps=i(ue,"A",{href:!0});var Xi=p(ps);Wn=n(Xi,"pipeline()"),Xi.forEach(s),Un=n(ue,". An "),fs=i(ue,"A",{href:!0});var ep=p(fs);Rn=n(ep,"AutoClass"),ep.forEach(s),Gn=n(ue," is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),la=i(ue,"CODE",{});var tp=p(la);Yn=n(tp,"AutoClass"),tp.forEach(s),Jn=n(ue," for your task and it\u2019s associated tokenizer with "),us=i(ue,"A",{href:!0});var sp=p(us);Qn=n(sp,"AutoTokenizer"),sp.forEach(s),Bn=n(ue,"."),ue.forEach(s),ur=d(e),Te=i(e,"P",{});var Ps=p(Te);Vn=n(Ps,"Let\u2019s return to our example and see how you can use the "),ia=i(Ps,"CODE",{});var ap=p(ia);Kn=n(ap,"AutoClass"),ap.forEach(s),Zn=n(Ps," to replicate the results of the "),ms=i(Ps,"A",{href:!0});var rp=p(ms);Xn=n(rp,"pipeline()"),rp.forEach(s),el=n(Ps,"."),Ps.forEach(s),mr=d(e),Ce=i(e,"H3",{class:!0});var Yr=p(Ce);et=i(Yr,"A",{id:!0,class:!0,href:!0});var op=p(et);pa=i(op,"SPAN",{});var np=p(pa);b(Nt.$$.fragment,np),np.forEach(s),op.forEach(s),tl=d(Yr),fa=i(Yr,"SPAN",{});var lp=p(fa);sl=n(lp,"AutoTokenizer"),lp.forEach(s),Yr.forEach(s),cr=d(e),je=i(e,"P",{});var Fs=p(je);al=n(Fs,"A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),ua=i(Fs,"EM",{});var ip=p(ua);rl=n(ip,"tokens"),ip.forEach(s),ol=n(Fs,". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),cs=i(Fs,"A",{href:!0});var pp=p(cs);nl=n(pp,"here"),pp.forEach(s),ll=n(Fs,"). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),Fs.forEach(s),hr=d(e),tt=i(e,"P",{});var Jr=p(tt);il=n(Jr,"Load a tokenizer with "),hs=i(Jr,"A",{href:!0});var fp=p(hs);pl=n(fp,"AutoTokenizer"),fp.forEach(s),fl=n(Jr,":"),Jr.forEach(s),dr=d(e),b(Dt.$$.fragment,e),$r=d(e),st=i(e,"P",{});var Qr=p(st);ul=n(Qr,"Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),ma=i(Qr,"EM",{});var up=p(ma);ml=n(up,"vocabulary"),up.forEach(s),cl=n(Qr,"."),Qr.forEach(s),_r=d(e),ds=i(e,"P",{});var mp=p(ds);hl=n(mp,"Pass your text to the tokenizer:"),mp.forEach(s),gr=d(e),b(Ht.$$.fragment,e),vr=d(e),$s=i(e,"P",{});var cp=p($s);dl=n(cp,"The tokenizer will return a dictionary containing:"),cp.forEach(s),wr=d(e),at=i(e,"UL",{});var Br=p(at);_s=i(Br,"LI",{});var Xl=p(_s);gs=i(Xl,"A",{href:!0});var hp=p(gs);$l=n(hp,"input_ids"),hp.forEach(s),_l=n(Xl,": numerical representions of your tokens."),Xl.forEach(s),gl=d(Br),vs=i(Br,"LI",{});var ei=p(vs);ws=i(ei,"A",{href:!0});var dp=p(ws);vl=n(dp,"atttention_mask"),dp.forEach(s),wl=n(ei,": indicates which tokens should be attended to."),ei.forEach(s),Br.forEach(s),kr=d(e),rt=i(e,"P",{});var Vr=p(rt);kl=n(Vr,"Just like the "),ks=i(Vr,"A",{href:!0});var $p=p(ks);yl=n($p,"pipeline()"),$p.forEach(s),bl=n(Vr,", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),Vr.forEach(s),yr=d(e),b(ot.$$.fragment,e),br=d(e),nt=i(e,"P",{});var Kr=p(nt);El=n(Kr,"Read the "),ys=i(Kr,"A",{href:!0});var _p=p(ys);Al=n(_p,"preprocessing"),_p.forEach(s),Tl=n(Kr," tutorial for more details about tokenization."),Kr.forEach(s),Er=d(e),Ie=i(e,"H3",{class:!0});var Zr=p(Ie);lt=i(Zr,"A",{id:!0,class:!0,href:!0});var gp=p(lt);ca=i(gp,"SPAN",{});var vp=p(ca);b(Lt.$$.fragment,vp),vp.forEach(s),gp.forEach(s),jl=d(Zr),ha=i(Zr,"SPAN",{});var wp=p(ha);xl=n(wp,"AutoModel"),wp.forEach(s),Zr.forEach(s),Ar=d(e),b(it.$$.fragment,e),Tr=d(e),b(pt.$$.fragment,e),jr=d(e),X=i(e,"P",{});var me=p(X);ql=n(me,"Models are a standard "),Wt=i(me,"A",{href:!0,rel:!0});var kp=p(Wt);da=i(kp,"CODE",{});var yp=p(da);zl=n(yp,"torch.nn.Module"),yp.forEach(s),kp.forEach(s),Pl=n(me," or a "),Ut=i(me,"A",{href:!0,rel:!0});var bp=p(Ut);$a=i(bp,"CODE",{});var Ep=p($a);Fl=n(Ep,"tf.keras.Model"),Ep.forEach(s),bp.forEach(s),Ml=n(me," so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),bs=i(me,"A",{href:!0});var Ap=p(bs);Sl=n(Ap,"Trainer"),Ap.forEach(s),Cl=n(me," class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),_a=i(me,"CODE",{});var Tp=p(_a);Il=n(Tp,"fit"),Tp.forEach(s),Ol=n(me," method from "),Rt=i(me,"A",{href:!0,rel:!0});var jp=p(Rt);Nl=n(jp,"Keras"),jp.forEach(s),Dl=n(me,". Refer to the "),Es=i(me,"A",{href:!0});var xp=p(Es);Hl=n(xp,"training tutorial"),xp.forEach(s),Ll=n(me," for more details."),me.forEach(s),xr=d(e),b(ft.$$.fragment,e),qr=d(e),Oe=i(e,"H3",{class:!0});var Xr=p(Oe);ut=i(Xr,"A",{id:!0,class:!0,href:!0});var qp=p(ut);ga=i(qp,"SPAN",{});var zp=p(ga);b(Gt.$$.fragment,zp),zp.forEach(s),qp.forEach(s),Wl=d(Xr),va=i(Xr,"SPAN",{});var Pp=p(va);Ul=n(Pp,"Save a model"),Pp.forEach(s),Xr.forEach(s),zr=d(e),b(mt.$$.fragment,e),Pr=d(e),xe=i(e,"P",{});var Ms=p(xe);Rl=n(Ms,"One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),wa=i(Ms,"CODE",{});var Fp=p(wa);Gl=n(Fp,"from_pt"),Fp.forEach(s),Yl=n(Ms," or "),ka=i(Ms,"CODE",{});var Mp=p(ka);Jl=n(Mp,"from_tf"),Mp.forEach(s),Ql=n(Ms," parameter can convert the model from one framework to the other:"),Ms.forEach(s),Fr=d(e),b(ct.$$.fragment,e),this.h()},h(){$(a,"name","hf:doc:metadata"),$(a,"content",JSON.stringify(vf)),$(c,"id","quick-tour"),$(c,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(c,"href","#quick-tour"),$(r,"class","relative group"),$(x,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),$(F,"href","./model_doc/auto"),$(G,"id","pipeline"),$(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(G,"href","#pipeline"),$(B,"class","relative group"),$(te,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),$(ie,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),$(We,"id","pipeline-usage"),$(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(We,"href","#pipeline-usage"),$(Fe,"class","relative group"),$(Bt,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),$(Kt,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),$(wt,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),$(wt,"rel","nofollow"),$(Zt,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),$(Xt,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),$(bt,"href","https://huggingface.co/docs/datasets/"),$(bt,"rel","nofollow"),$(es,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),$(Tt,"href","https://huggingface.co/docs/datasets/quickstart.html"),$(Tt,"rel","nofollow"),$(jt,"href","https://huggingface.co/datasets/PolyAI/minds14"),$(jt,"rel","nofollow"),$(ts,"href","./main_classes/pipelines"),$(Ke,"id","use-another-model-and-tokenizer-in-the-pipeline"),$(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Ke,"href","#use-another-model-and-tokenizer-in-the-pipeline"),$(Me,"class","relative group"),$(ss,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),$(Ft,"href","https://huggingface.co/models"),$(Ft,"rel","nofollow"),$(as,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),$(Mt,"href","https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"),$(Mt,"rel","nofollow"),$(rs,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),$(os,"href","./training"),$(ns,"href","./model_sharing"),$(Xe,"id","autoclass"),$(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(Xe,"href","#autoclass"),$(Se,"class","relative group"),$(ls,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),$(is,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer"),$(ps,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),$(fs,"href","./model_doc/auto"),$(us,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer"),$(ms,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),$(et,"id","autotokenizer"),$(et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(et,"href","#autotokenizer"),$(Ce,"class","relative group"),$(cs,"href","./tokenizer_summary"),$(hs,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer"),$(gs,"href","./glossary#input-ids"),$(ws,"href",".glossary#attention-mask"),$(ks,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),$(ys,"href","./preprocessing"),$(lt,"id","automodel"),$(lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(lt,"href","#automodel"),$(Ie,"class","relative group"),$(Wt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),$(Wt,"rel","nofollow"),$(Ut,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),$(Ut,"rel","nofollow"),$(bs,"href","/docs/transformers/main/en/main_classes/trainer#transformers.Trainer"),$(Rt,"href","https://keras.io/"),$(Rt,"rel","nofollow"),$(Es,"href","./training"),$(ut,"id","save-a-model"),$(ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),$(ut,"href","#save-a-model"),$(Oe,"class","relative group")},m(e,f){t(document.head,a),u(e,m,f),u(e,r,f),t(r,c),t(c,_),E(v,_,null),t(r,z),t(r,C),t(C,w),u(e,M,f),E(I,e,f),u(e,O,f),u(e,D,f),t(D,L),t(D,x),t(x,S),t(D,g),t(D,F),t(F,R),t(D,U),u(e,Q,f),E(J,e,f),u(e,se,f),u(e,B,f),t(B,G),t(G,ee),E(V,ee,null),t(B,K),t(B,ce),t(ce,re),u(e,de,f),u(e,oe,f),t(oe,te),t(te,ne),t(oe,$e),u(e,q,f),E(N,e,f),u(e,le,f),u(e,k,f),t(k,H),t(k,ie),t(ie,Pe),t(k,he),u(e,ge,f),u(e,pe,f),t(pe,De),t(De,ve),t(pe,Qt),u(e,dt,f),u(e,Y,f),t(Y,Is),t(Is,eo),t(Y,to),t(Y,Os),t(Os,so),t(Y,ao),t(Y,Ns),t(Ns,ro),t(Y,oo),t(Y,Ds),t(Ds,no),t(Y,lo),t(Y,Hs),t(Hs,io),t(Y,po),t(Y,Ls),t(Ls,fo),t(Y,uo),t(Y,Ws),t(Ws,mo),t(Y,co),t(Y,Us),t(Us,ho),u(e,qa,f),u(e,$t,f),t($t,Rs),t(Rs,$o),t($t,_o),u(e,za,f),u(e,we,f),t(we,Gs),t(Gs,go),t(we,vo),t(we,Ys),t(Ys,wo),t(we,ko),t(we,Js),t(Js,yo),u(e,Pa,f),u(e,_t,f),t(_t,Qs),t(Qs,bo),t(_t,Eo),u(e,Fa,f),u(e,He,f),t(He,Bs),t(Bs,Ao),t(He,To),t(He,Vs),t(Vs,jo),u(e,Ma,f),E(Le,e,f),u(e,Sa,f),u(e,Fe,f),t(Fe,We),t(We,Ks),E(gt,Ks,null),t(Fe,xo),t(Fe,Zs),t(Zs,qo),u(e,Ca,f),u(e,Ue,f),t(Ue,zo),t(Ue,Bt),t(Bt,Po),t(Ue,Fo),u(e,Ia,f),u(e,Vt,f),t(Vt,Mo),u(e,Oa,f),E(Re,e,f),u(e,Na,f),u(e,Ge,f),t(Ge,So),t(Ge,Kt),t(Kt,Co),t(Ge,Io),u(e,Da,f),E(vt,e,f),u(e,Ha,f),u(e,ke,f),t(ke,Oo),t(ke,wt),t(wt,No),t(ke,Do),t(ke,Xs),t(Xs,Ho),t(ke,Lo),u(e,La,f),E(kt,e,f),u(e,Wa,f),u(e,Ye,f),t(Ye,Wo),t(Ye,Zt),t(Zt,Uo),t(Ye,Ro),u(e,Ua,f),E(yt,e,f),u(e,Ra,f),u(e,ye,f),t(ye,Go),t(ye,Xt),t(Xt,Yo),t(ye,Jo),t(ye,bt),t(bt,Qo),t(ye,Bo),u(e,Ga,f),E(Et,e,f),u(e,Ya,f),u(e,Je,f),t(Je,Vo),t(Je,es),t(es,Ko),t(Je,Zo),u(e,Ja,f),E(At,e,f),u(e,Qa,f),u(e,be,f),t(be,Xo),t(be,Tt),t(Tt,en),t(be,tn),t(be,jt),t(jt,sn),t(be,an),u(e,Ba,f),E(xt,e,f),u(e,Va,f),u(e,Qe,f),t(Qe,rn),t(Qe,ea),t(ea,on),t(Qe,nn),u(e,Ka,f),E(qt,e,f),u(e,Za,f),u(e,Be,f),t(Be,ln),t(Be,ta),t(ta,pn),t(Be,fn),u(e,Xa,f),E(zt,e,f),u(e,er,f),u(e,Ve,f),t(Ve,un),t(Ve,ts),t(ts,mn),t(Ve,cn),u(e,tr,f),u(e,Me,f),t(Me,Ke),t(Ke,sa),E(Pt,sa,null),t(Me,hn),t(Me,aa),t(aa,dn),u(e,sr,f),u(e,fe,f),t(fe,$n),t(fe,ss),t(ss,_n),t(fe,gn),t(fe,Ft),t(Ft,vn),t(fe,wn),t(fe,as),t(as,kn),t(fe,yn),t(fe,Mt),t(Mt,bn),t(fe,En),u(e,ar,f),E(St,e,f),u(e,rr,f),E(Ze,e,f),u(e,or,f),u(e,Ee,f),t(Ee,An),t(Ee,rs),t(rs,Tn),t(Ee,jn),t(Ee,ra),t(ra,xn),t(Ee,qn),u(e,nr,f),E(Ct,e,f),u(e,lr,f),u(e,Ae,f),t(Ae,zn),t(Ae,os),t(os,Pn),t(Ae,Fn),t(Ae,ns),t(ns,Mn),t(Ae,Sn),u(e,ir,f),u(e,Se,f),t(Se,Xe),t(Xe,oa),E(It,oa,null),t(Se,Cn),t(Se,na),t(na,In),u(e,pr,f),E(Ot,e,f),u(e,fr,f),u(e,Z,f),t(Z,On),t(Z,ls),t(ls,Nn),t(Z,Dn),t(Z,is),t(is,Hn),t(Z,Ln),t(Z,ps),t(ps,Wn),t(Z,Un),t(Z,fs),t(fs,Rn),t(Z,Gn),t(Z,la),t(la,Yn),t(Z,Jn),t(Z,us),t(us,Qn),t(Z,Bn),u(e,ur,f),u(e,Te,f),t(Te,Vn),t(Te,ia),t(ia,Kn),t(Te,Zn),t(Te,ms),t(ms,Xn),t(Te,el),u(e,mr,f),u(e,Ce,f),t(Ce,et),t(et,pa),E(Nt,pa,null),t(Ce,tl),t(Ce,fa),t(fa,sl),u(e,cr,f),u(e,je,f),t(je,al),t(je,ua),t(ua,rl),t(je,ol),t(je,cs),t(cs,nl),t(je,ll),u(e,hr,f),u(e,tt,f),t(tt,il),t(tt,hs),t(hs,pl),t(tt,fl),u(e,dr,f),E(Dt,e,f),u(e,$r,f),u(e,st,f),t(st,ul),t(st,ma),t(ma,ml),t(st,cl),u(e,_r,f),u(e,ds,f),t(ds,hl),u(e,gr,f),E(Ht,e,f),u(e,vr,f),u(e,$s,f),t($s,dl),u(e,wr,f),u(e,at,f),t(at,_s),t(_s,gs),t(gs,$l),t(_s,_l),t(at,gl),t(at,vs),t(vs,ws),t(ws,vl),t(vs,wl),u(e,kr,f),u(e,rt,f),t(rt,kl),t(rt,ks),t(ks,yl),t(rt,bl),u(e,yr,f),E(ot,e,f),u(e,br,f),u(e,nt,f),t(nt,El),t(nt,ys),t(ys,Al),t(nt,Tl),u(e,Er,f),u(e,Ie,f),t(Ie,lt),t(lt,ca),E(Lt,ca,null),t(Ie,jl),t(Ie,ha),t(ha,xl),u(e,Ar,f),E(it,e,f),u(e,Tr,f),E(pt,e,f),u(e,jr,f),u(e,X,f),t(X,ql),t(X,Wt),t(Wt,da),t(da,zl),t(X,Pl),t(X,Ut),t(Ut,$a),t($a,Fl),t(X,Ml),t(X,bs),t(bs,Sl),t(X,Cl),t(X,_a),t(_a,Il),t(X,Ol),t(X,Rt),t(Rt,Nl),t(X,Dl),t(X,Es),t(Es,Hl),t(X,Ll),u(e,xr,f),E(ft,e,f),u(e,qr,f),u(e,Oe,f),t(Oe,ut),t(ut,ga),E(Gt,ga,null),t(Oe,Wl),t(Oe,va),t(va,Ul),u(e,zr,f),E(mt,e,f),u(e,Pr,f),u(e,xe,f),t(xe,Rl),t(xe,wa),t(wa,Gl),t(xe,Yl),t(xe,ka),t(ka,Jl),t(xe,Ql),u(e,Fr,f),E(ct,e,f),Mr=!0},p(e,[f]){const Yt={};f&2&&(Yt.$$scope={dirty:f,ctx:e}),J.$set(Yt);const ya={};f&2&&(ya.$$scope={dirty:f,ctx:e}),Le.$set(ya);const ba={};f&2&&(ba.$$scope={dirty:f,ctx:e}),Re.$set(ba);const Ea={};f&2&&(Ea.$$scope={dirty:f,ctx:e}),Ze.$set(Ea);const Ne={};f&2&&(Ne.$$scope={dirty:f,ctx:e}),ot.$set(Ne);const Aa={};f&2&&(Aa.$$scope={dirty:f,ctx:e}),it.$set(Aa);const Ta={};f&2&&(Ta.$$scope={dirty:f,ctx:e}),pt.$set(Ta);const Jt={};f&2&&(Jt.$$scope={dirty:f,ctx:e}),ft.$set(Jt);const ja={};f&2&&(ja.$$scope={dirty:f,ctx:e}),mt.$set(ja);const xa={};f&2&&(xa.$$scope={dirty:f,ctx:e}),ct.$set(xa)},i(e){Mr||(A(v.$$.fragment,e),A(I.$$.fragment,e),A(J.$$.fragment,e),A(V.$$.fragment,e),A(N.$$.fragment,e),A(Le.$$.fragment,e),A(gt.$$.fragment,e),A(Re.$$.fragment,e),A(vt.$$.fragment,e),A(kt.$$.fragment,e),A(yt.$$.fragment,e),A(Et.$$.fragment,e),A(At.$$.fragment,e),A(xt.$$.fragment,e),A(qt.$$.fragment,e),A(zt.$$.fragment,e),A(Pt.$$.fragment,e),A(St.$$.fragment,e),A(Ze.$$.fragment,e),A(Ct.$$.fragment,e),A(It.$$.fragment,e),A(Ot.$$.fragment,e),A(Nt.$$.fragment,e),A(Dt.$$.fragment,e),A(Ht.$$.fragment,e),A(ot.$$.fragment,e),A(Lt.$$.fragment,e),A(it.$$.fragment,e),A(pt.$$.fragment,e),A(ft.$$.fragment,e),A(Gt.$$.fragment,e),A(mt.$$.fragment,e),A(ct.$$.fragment,e),Mr=!0)},o(e){T(v.$$.fragment,e),T(I.$$.fragment,e),T(J.$$.fragment,e),T(V.$$.fragment,e),T(N.$$.fragment,e),T(Le.$$.fragment,e),T(gt.$$.fragment,e),T(Re.$$.fragment,e),T(vt.$$.fragment,e),T(kt.$$.fragment,e),T(yt.$$.fragment,e),T(Et.$$.fragment,e),T(At.$$.fragment,e),T(xt.$$.fragment,e),T(qt.$$.fragment,e),T(zt.$$.fragment,e),T(Pt.$$.fragment,e),T(St.$$.fragment,e),T(Ze.$$.fragment,e),T(Ct.$$.fragment,e),T(It.$$.fragment,e),T(Ot.$$.fragment,e),T(Nt.$$.fragment,e),T(Dt.$$.fragment,e),T(Ht.$$.fragment,e),T(ot.$$.fragment,e),T(Lt.$$.fragment,e),T(it.$$.fragment,e),T(pt.$$.fragment,e),T(ft.$$.fragment,e),T(Gt.$$.fragment,e),T(mt.$$.fragment,e),T(ct.$$.fragment,e),Mr=!1},d(e){s(a),e&&s(m),e&&s(r),j(v),e&&s(M),j(I,e),e&&s(O),e&&s(D),e&&s(Q),j(J,e),e&&s(se),e&&s(B),j(V),e&&s(de),e&&s(oe),e&&s(q),j(N,e),e&&s(le),e&&s(k),e&&s(ge),e&&s(pe),e&&s(dt),e&&s(Y),e&&s(qa),e&&s($t),e&&s(za),e&&s(we),e&&s(Pa),e&&s(_t),e&&s(Fa),e&&s(He),e&&s(Ma),j(Le,e),e&&s(Sa),e&&s(Fe),j(gt),e&&s(Ca),e&&s(Ue),e&&s(Ia),e&&s(Vt),e&&s(Oa),j(Re,e),e&&s(Na),e&&s(Ge),e&&s(Da),j(vt,e),e&&s(Ha),e&&s(ke),e&&s(La),j(kt,e),e&&s(Wa),e&&s(Ye),e&&s(Ua),j(yt,e),e&&s(Ra),e&&s(ye),e&&s(Ga),j(Et,e),e&&s(Ya),e&&s(Je),e&&s(Ja),j(At,e),e&&s(Qa),e&&s(be),e&&s(Ba),j(xt,e),e&&s(Va),e&&s(Qe),e&&s(Ka),j(qt,e),e&&s(Za),e&&s(Be),e&&s(Xa),j(zt,e),e&&s(er),e&&s(Ve),e&&s(tr),e&&s(Me),j(Pt),e&&s(sr),e&&s(fe),e&&s(ar),j(St,e),e&&s(rr),j(Ze,e),e&&s(or),e&&s(Ee),e&&s(nr),j(Ct,e),e&&s(lr),e&&s(Ae),e&&s(ir),e&&s(Se),j(It),e&&s(pr),j(Ot,e),e&&s(fr),e&&s(Z),e&&s(ur),e&&s(Te),e&&s(mr),e&&s(Ce),j(Nt),e&&s(cr),e&&s(je),e&&s(hr),e&&s(tt),e&&s(dr),j(Dt,e),e&&s($r),e&&s(st),e&&s(_r),e&&s(ds),e&&s(gr),j(Ht,e),e&&s(vr),e&&s($s),e&&s(wr),e&&s(at),e&&s(kr),e&&s(rt),e&&s(yr),j(ot,e),e&&s(br),e&&s(nt),e&&s(Er),e&&s(Ie),j(Lt),e&&s(Ar),j(it,e),e&&s(Tr),j(pt,e),e&&s(jr),e&&s(X),e&&s(xr),j(ft,e),e&&s(qr),e&&s(Oe),j(Gt),e&&s(zr),j(mt,e),e&&s(Pr),e&&s(xe),e&&s(Fr),j(ct,e)}}}const vf={local:"quick-tour",sections:[{local:"pipeline",sections:[{local:"pipeline-usage",title:"Pipeline usage"},{local:"use-another-model-and-tokenizer-in-the-pipeline",title:"Use another model and tokenizer in the pipeline"}],title:"Pipeline"},{local:"autoclass",sections:[{local:"autotokenizer",title:"AutoTokenizer"},{local:"automodel",title:"AutoModel"},{local:"save-a-model",title:"Save a model"}],title:"AutoClass"}],title:"Quick tour"};function wf(P){return Dp(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class xf extends Cp{constructor(a){super();Ip(this,a,wf,gf,Op,{})}}export{xf as default,vf as metadata};
