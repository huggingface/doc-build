import{S as qt,i as jt,s as Vt,e as o,k as h,w as B,t as l,M as Lt,c as a,d as t,m as c,a as r,x as q,h as p,b as f,G as n,g as s,y as j,L as Nt,q as V,o as L,B as N,v as zt}from"../chunks/vendor-hf-doc-builder.js";import{I as Fe}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as ft}from"../chunks/CodeBlock-hf-doc-builder.js";function Ht(ht){let u,le,m,w,W,g,Me,Y,Se,pe,z,Be,fe,d,b,Z,A,qe,ee,je,he,H,Ve,ce,G,Le,ue,v,Ne,X,ze,He,me,_,x,te,I,Ge,ne,De,de,D,Re,_e,R,Je,Pe,T,we,J,Oe,be,C,ve,O,Ke,xe,k,Ee,E,Qe,U,We,Ye,ye,P,y,ie,F,Ze,oe,et,$e,M,tt,S,nt,ge,K,Q,it,$,ot,ae,at,rt,re,st,Ae;return g=new Fe({}),A=new Fe({}),I=new Fe({}),T=new ft({props:{code:"pip install intel_extension_for_pytorch==1.10.100+cpu -f https://software.intel.com/ipex-whl-stable",highlighted:'<span class="hljs-attribute">pip</span> install intel_extension_for_pytorch==<span class="hljs-number">1</span>.<span class="hljs-number">10</span>.<span class="hljs-number">100</span>+cpu -f https://software.intel.com/ipex-whl-stable'}}),C=new ft({props:{code:"pip install intel_extension_for_pytorch==1.11.200+cpu -f https://software.intel.com/ipex-whl-stable",highlighted:'<span class="hljs-attribute">pip</span> install intel_extension_for_pytorch==<span class="hljs-number">1</span>.<span class="hljs-number">11</span>.<span class="hljs-number">200</span>+cpu -f https://software.intel.com/ipex-whl-stable'}}),k=new ft({props:{code:"pip install intel_extension_for_pytorch==1.12.300+cpu -f https://software.intel.com/ipex-whl-stable",highlighted:'<span class="hljs-attribute">pip</span> install intel_extension_for_pytorch==<span class="hljs-number">1</span>.<span class="hljs-number">12</span>.<span class="hljs-number">300</span>+cpu -f https://software.intel.com/ipex-whl-stable'}}),F=new Fe({}),{c(){u=o("meta"),le=h(),m=o("h1"),w=o("a"),W=o("span"),B(g.$$.fragment),Me=h(),Y=o("span"),Se=l("Efficient Training on CPU"),pe=h(),z=o("p"),Be=l("This guide focuses on training large models efficiently on CPU."),fe=h(),d=o("h2"),b=o("a"),Z=o("span"),B(A.$$.fragment),qe=h(),ee=o("span"),je=l("Mixed precision with IPEX"),he=h(),H=o("p"),Ve=l("IPEX is optimized for CPUs with AVX-512 or above, and functionally works for CPUs with only AVX2. So, it is expected to bring performance benefit for Intel CPU generations with AVX-512 or above while CPUs with only AVX2 (e.g., AMD CPUs or older Intel CPUs) might result in a better performance under IPEX, but not guaranteed. IPEX provides performance optimizations for CPU training with both Float32 and BFloat16. The usage of BFloat16 is the main focus of the following sections."),ce=h(),G=o("p"),Le=l("Low precision data type BFloat16 has been natively supported on the 3rd Generation Xeon\xAE Scalable Processors (aka Cooper Lake) with AVX512 instruction set and will be supported on the next generation of Intel\xAE Xeon\xAE Scalable Processors with Intel\xAE Advanced Matrix Extensions (Intel\xAE AMX) instruction set with further boosted performance. The Auto Mixed Precision for CPU backend has been enabled since PyTorch-1.10. At the same time, the support of Auto Mixed Precision with BFloat16 for CPU and BFloat16 optimization of operators has been massively enabled in Intel\xAE Extension for PyTorch, and partially upstreamed to PyTorch master branch. Users can get better performance and user experience with IPEX Auto Mixed Precision."),ue=h(),v=o("p"),Ne=l("Check more detailed information for "),X=o("a"),ze=l("Auto Mixed Precision"),He=l("."),me=h(),_=o("h3"),x=o("a"),te=o("span"),B(I.$$.fragment),Ge=h(),ne=o("span"),De=l("IPEX installation:"),de=h(),D=o("p"),Re=l("IPEX release is following PyTorch, to install via pip:"),_e=h(),R=o("p"),Je=l("For PyTorch-1.10:"),Pe=h(),B(T.$$.fragment),we=h(),J=o("p"),Oe=l("For PyTorch-1.11:"),be=h(),B(C.$$.fragment),ve=h(),O=o("p"),Ke=l("For PyTorch-1.12:"),xe=h(),B(k.$$.fragment),Ee=h(),E=o("p"),Qe=l("Check more approaches for "),U=o("a"),We=l("IPEX installation"),Ye=l("."),ye=h(),P=o("h3"),y=o("a"),ie=o("span"),B(F.$$.fragment),Ze=h(),oe=o("span"),et=l("Usage in Trainer"),$e=l("\n\nTo enable auto mixed precision with IPEX in Trainer, users should add `use_ipex`, `bf16` and `no_cuda` in training command arguments.\n"),M=o("p"),tt=l("Take an example of the use cases on "),S=o("a"),nt=l("Transformers question-answering"),ge=h(),K=o("ul"),Q=o("li"),it=l("Training with IPEX using BF16 auto mixed precision on CPU:"),$=o("pre"),ot=l(` python run_qa.py \\
--model_name_or_path bert-base-uncased \\
--dataset_name squad \\
--do_train \\
--do_eval \\
--per_device_train_batch_size 12 \\
--learning_rate 3e-5 \\
--num_train_epochs 2 \\
--max_seq_length 384 \\
--doc_stride 128 \\
--output_dir /tmp/debug_squad/ \\
`),ae=o("b"),at=l("--use_ipex \\"),rt=l(`
`),re=o("b"),st=l("--bf16 --no_cuda"),this.h()},l(e){const i=Lt('[data-svelte="svelte-1phssyn"]',document.head);u=a(i,"META",{name:!0,content:!0}),i.forEach(t),le=c(e),m=a(e,"H1",{class:!0});var Xe=r(m);w=a(Xe,"A",{id:!0,class:!0,href:!0});var ct=r(w);W=a(ct,"SPAN",{});var ut=r(W);q(g.$$.fragment,ut),ut.forEach(t),ct.forEach(t),Me=c(Xe),Y=a(Xe,"SPAN",{});var mt=r(Y);Se=p(mt,"Efficient Training on CPU"),mt.forEach(t),Xe.forEach(t),pe=c(e),z=a(e,"P",{});var dt=r(z);Be=p(dt,"This guide focuses on training large models efficiently on CPU."),dt.forEach(t),fe=c(e),d=a(e,"H2",{class:!0});var Ie=r(d);b=a(Ie,"A",{id:!0,class:!0,href:!0});var _t=r(b);Z=a(_t,"SPAN",{});var Pt=r(Z);q(A.$$.fragment,Pt),Pt.forEach(t),_t.forEach(t),qe=c(Ie),ee=a(Ie,"SPAN",{});var wt=r(ee);je=p(wt,"Mixed precision with IPEX"),wt.forEach(t),Ie.forEach(t),he=c(e),H=a(e,"P",{});var bt=r(H);Ve=p(bt,"IPEX is optimized for CPUs with AVX-512 or above, and functionally works for CPUs with only AVX2. So, it is expected to bring performance benefit for Intel CPU generations with AVX-512 or above while CPUs with only AVX2 (e.g., AMD CPUs or older Intel CPUs) might result in a better performance under IPEX, but not guaranteed. IPEX provides performance optimizations for CPU training with both Float32 and BFloat16. The usage of BFloat16 is the main focus of the following sections."),bt.forEach(t),ce=c(e),G=a(e,"P",{});var vt=r(G);Le=p(vt,"Low precision data type BFloat16 has been natively supported on the 3rd Generation Xeon\xAE Scalable Processors (aka Cooper Lake) with AVX512 instruction set and will be supported on the next generation of Intel\xAE Xeon\xAE Scalable Processors with Intel\xAE Advanced Matrix Extensions (Intel\xAE AMX) instruction set with further boosted performance. The Auto Mixed Precision for CPU backend has been enabled since PyTorch-1.10. At the same time, the support of Auto Mixed Precision with BFloat16 for CPU and BFloat16 optimization of operators has been massively enabled in Intel\xAE Extension for PyTorch, and partially upstreamed to PyTorch master branch. Users can get better performance and user experience with IPEX Auto Mixed Precision."),vt.forEach(t),ue=c(e),v=a(e,"P",{});var Te=r(v);Ne=p(Te,"Check more detailed information for "),X=a(Te,"A",{href:!0,rel:!0});var xt=r(X);ze=p(xt,"Auto Mixed Precision"),xt.forEach(t),He=p(Te,"."),Te.forEach(t),me=c(e),_=a(e,"H3",{class:!0});var Ce=r(_);x=a(Ce,"A",{id:!0,class:!0,href:!0});var Et=r(x);te=a(Et,"SPAN",{});var yt=r(te);q(I.$$.fragment,yt),yt.forEach(t),Et.forEach(t),Ge=c(Ce),ne=a(Ce,"SPAN",{});var $t=r(ne);De=p($t,"IPEX installation:"),$t.forEach(t),Ce.forEach(t),de=c(e),D=a(e,"P",{});var gt=r(D);Re=p(gt,"IPEX release is following PyTorch, to install via pip:"),gt.forEach(t),_e=c(e),R=a(e,"P",{});var At=r(R);Je=p(At,"For PyTorch-1.10:"),At.forEach(t),Pe=c(e),q(T.$$.fragment,e),we=c(e),J=a(e,"P",{});var Xt=r(J);Oe=p(Xt,"For PyTorch-1.11:"),Xt.forEach(t),be=c(e),q(C.$$.fragment,e),ve=c(e),O=a(e,"P",{});var It=r(O);Ke=p(It,"For PyTorch-1.12:"),It.forEach(t),xe=c(e),q(k.$$.fragment,e),Ee=c(e),E=a(e,"P",{});var ke=r(E);Qe=p(ke,"Check more approaches for "),U=a(ke,"A",{href:!0,rel:!0});var Tt=r(U);We=p(Tt,"IPEX installation"),Tt.forEach(t),Ye=p(ke,"."),ke.forEach(t),ye=c(e),P=a(e,"H3",{class:!0});var Ue=r(P);y=a(Ue,"A",{id:!0,class:!0,href:!0});var Ct=r(y);ie=a(Ct,"SPAN",{});var kt=r(ie);q(F.$$.fragment,kt),kt.forEach(t),Ct.forEach(t),Ze=c(Ue),oe=a(Ue,"SPAN",{});var Ut=r(oe);et=p(Ut,"Usage in Trainer"),Ut.forEach(t),Ue.forEach(t),$e=p(e,"\n\nTo enable auto mixed precision with IPEX in Trainer, users should add `use_ipex`, `bf16` and `no_cuda` in training command arguments.\n"),M=a(e,"P",{});var lt=r(M);tt=p(lt,"Take an example of the use cases on "),S=a(lt,"A",{href:!0,rel:!0});var Ft=r(S);nt=p(Ft,"Transformers question-answering"),Ft.forEach(t),lt.forEach(t),ge=c(e),K=a(e,"UL",{});var Mt=r(K);Q=a(Mt,"LI",{});var pt=r(Q);it=p(pt,"Training with IPEX using BF16 auto mixed precision on CPU:"),$=a(pt,"PRE",{});var se=r($);ot=p(se,` python run_qa.py \\
--model_name_or_path bert-base-uncased \\
--dataset_name squad \\
--do_train \\
--do_eval \\
--per_device_train_batch_size 12 \\
--learning_rate 3e-5 \\
--num_train_epochs 2 \\
--max_seq_length 384 \\
--doc_stride 128 \\
--output_dir /tmp/debug_squad/ \\
`),ae=a(se,"B",{});var St=r(ae);at=p(St,"--use_ipex \\"),St.forEach(t),rt=p(se,`
`),re=a(se,"B",{});var Bt=r(re);st=p(Bt,"--bf16 --no_cuda"),Bt.forEach(t),se.forEach(t),pt.forEach(t),Mt.forEach(t),this.h()},h(){f(u,"name","hf:doc:metadata"),f(u,"content",JSON.stringify(Gt)),f(w,"id","efficient-training-on-cpu"),f(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(w,"href","#efficient-training-on-cpu"),f(m,"class","relative group"),f(b,"id","mixed-precision-with-ipex"),f(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(b,"href","#mixed-precision-with-ipex"),f(d,"class","relative group"),f(X,"href","https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features/amp.html"),f(X,"rel","nofollow"),f(x,"id","ipex-installation"),f(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(x,"href","#ipex-installation"),f(_,"class","relative group"),f(U,"href","https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/installation.html"),f(U,"rel","nofollow"),f(y,"id","usage-in-trainer"),f(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(y,"href","#usage-in-trainer"),f(P,"class","relative group"),f(S,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering"),f(S,"rel","nofollow")},m(e,i){n(document.head,u),s(e,le,i),s(e,m,i),n(m,w),n(w,W),j(g,W,null),n(m,Me),n(m,Y),n(Y,Se),s(e,pe,i),s(e,z,i),n(z,Be),s(e,fe,i),s(e,d,i),n(d,b),n(b,Z),j(A,Z,null),n(d,qe),n(d,ee),n(ee,je),s(e,he,i),s(e,H,i),n(H,Ve),s(e,ce,i),s(e,G,i),n(G,Le),s(e,ue,i),s(e,v,i),n(v,Ne),n(v,X),n(X,ze),n(v,He),s(e,me,i),s(e,_,i),n(_,x),n(x,te),j(I,te,null),n(_,Ge),n(_,ne),n(ne,De),s(e,de,i),s(e,D,i),n(D,Re),s(e,_e,i),s(e,R,i),n(R,Je),s(e,Pe,i),j(T,e,i),s(e,we,i),s(e,J,i),n(J,Oe),s(e,be,i),j(C,e,i),s(e,ve,i),s(e,O,i),n(O,Ke),s(e,xe,i),j(k,e,i),s(e,Ee,i),s(e,E,i),n(E,Qe),n(E,U),n(U,We),n(E,Ye),s(e,ye,i),s(e,P,i),n(P,y),n(y,ie),j(F,ie,null),n(P,Ze),n(P,oe),n(oe,et),s(e,$e,i),s(e,M,i),n(M,tt),n(M,S),n(S,nt),s(e,ge,i),s(e,K,i),n(K,Q),n(Q,it),n(Q,$),n($,ot),n($,ae),n(ae,at),n($,rt),n($,re),n(re,st),Ae=!0},p:Nt,i(e){Ae||(V(g.$$.fragment,e),V(A.$$.fragment,e),V(I.$$.fragment,e),V(T.$$.fragment,e),V(C.$$.fragment,e),V(k.$$.fragment,e),V(F.$$.fragment,e),Ae=!0)},o(e){L(g.$$.fragment,e),L(A.$$.fragment,e),L(I.$$.fragment,e),L(T.$$.fragment,e),L(C.$$.fragment,e),L(k.$$.fragment,e),L(F.$$.fragment,e),Ae=!1},d(e){t(u),e&&t(le),e&&t(m),N(g),e&&t(pe),e&&t(z),e&&t(fe),e&&t(d),N(A),e&&t(he),e&&t(H),e&&t(ce),e&&t(G),e&&t(ue),e&&t(v),e&&t(me),e&&t(_),N(I),e&&t(de),e&&t(D),e&&t(_e),e&&t(R),e&&t(Pe),N(T,e),e&&t(we),e&&t(J),e&&t(be),N(C,e),e&&t(ve),e&&t(O),e&&t(xe),N(k,e),e&&t(Ee),e&&t(E),e&&t(ye),e&&t(P),N(F),e&&t($e),e&&t(M),e&&t(ge),e&&t(K)}}}const Gt={local:"efficient-training-on-cpu",sections:[{local:"mixed-precision-with-ipex",sections:[{local:"ipex-installation",title:"IPEX installation:"},{local:"usage-in-trainer",title:"Usage in Trainer"}],title:"Mixed precision with IPEX"}],title:"Efficient Training on CPU"};function Dt(ht){return zt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Kt extends qt{constructor(u){super();jt(this,u,Dt,Ht,Vt,{})}}export{Kt as default,Gt as metadata};
