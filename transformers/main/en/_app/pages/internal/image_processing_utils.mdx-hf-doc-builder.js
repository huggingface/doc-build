import{S as Lo,i as Oo,s as Ao,e as a,k as c,w as x,t as s,M as No,c as o,d as r,m as l,a as n,x as v,h as i,b as p,G as e,g as _,y as b,q as y,o as $,B as E,v as So,L as jo}from"../../chunks/vendor-hf-doc-builder.js";import{T as Po}from"../../chunks/Tip-hf-doc-builder.js";import{D as I}from"../../chunks/Docstring-hf-doc-builder.js";import{C as qo}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Ha}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as zo}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Co(L){let m,k,h,u,T;return{c(){m=a("p"),k=s("Passing "),h=a("code"),u=s("use_auth_token=True"),T=s(" is required when you want to use a private model.")},l(d){m=o(d,"P",{});var w=n(m);k=i(w,"Passing "),h=o(w,"CODE",{});var H=n(h);u=i(H,"use_auth_token=True"),H.forEach(r),T=i(w," is required when you want to use a private model."),w.forEach(r)},m(d,w){_(d,m,w),e(m,k),e(m,h),e(h,u),e(m,T)},d(d){d&&r(m)}}}function Vo(L){let m,k,h,u,T;return u=new qo({props:{code:`# We can't instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let's show the examples on a
# derived class: *Wav2Vec2FeatureExtractor*
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h"
)  # Download feature_extraction_config from huggingface.co and cache.
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "./test/saved_model/"
)  # E.g. feature_extractor (or model) was saved using *save_pretrained('./test/saved_model/')*
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("./test/saved_model/preprocessor_config.json")
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h", return_attention_mask=False, foo=False
)
assert feature_extractor.return_attention_mask is False
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h", return_attention_mask=False, foo=False, return_unused_kwargs=True
)
assert feature_extractor.return_attention_mask is False
assert unused_kwargs == {"foo": False}`,highlighted:`<span class="hljs-comment"># We can&#x27;t instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let&#x27;s show the examples on a</span>
<span class="hljs-comment"># derived class: *Wav2Vec2FeatureExtractor*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>
)  <span class="hljs-comment"># Download feature_extraction_config from huggingface.co and cache.</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;./test/saved_model/&quot;</span>
)  <span class="hljs-comment"># E.g. feature_extractor (or model) was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/preprocessor_config.json&quot;</span>)
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
<span class="hljs-keyword">assert</span> unused_kwargs == {<span class="hljs-string">&quot;foo&quot;</span>: <span class="hljs-literal">False</span>}`}}),{c(){m=a("p"),k=s("Examples:"),h=c(),x(u.$$.fragment)},l(d){m=o(d,"P",{});var w=n(m);k=i(w,"Examples:"),w.forEach(r),h=l(d),v(u.$$.fragment,d)},m(d,w){_(d,m,w),e(m,k),_(d,h,w),b(u,d,w),T=!0},p:jo,i(d){T||(y(u.$$.fragment,d),T=!0)},o(d){$(u.$$.fragment,d),T=!1},d(d){d&&r(m),d&&r(h),E(u,d)}}}function Wo(L){let m,k,h,u,T;return u=new qo({props:{code:`from transformers import AutoFeatureExtractor

feature extractor = AutoFeatureExtractor.from_pretrained("bert-base-cased")

# Push the feature extractor to your namespace with the name "my-finetuned-bert".
feature extractor.push_to_hub("my-finetuned-bert")

# Push the feature extractor to an organization with the name "my-finetuned-bert".
feature extractor.push_to_hub("huggingface/my-finetuned-bert")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

feature extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-comment"># Push the feature extractor to your namespace with the name &quot;my-finetuned-bert&quot;.</span>
feature extractor.push_to_hub(<span class="hljs-string">&quot;my-finetuned-bert&quot;</span>)

<span class="hljs-comment"># Push the feature extractor to an organization with the name &quot;my-finetuned-bert&quot;.</span>
feature extractor.push_to_hub(<span class="hljs-string">&quot;huggingface/my-finetuned-bert&quot;</span>)`}}),{c(){m=a("p"),k=s("Examples:"),h=c(),x(u.$$.fragment)},l(d){m=o(d,"P",{});var w=n(m);k=i(w,"Examples:"),w.forEach(r),h=l(d),v(u.$$.fragment,d)},m(d,w){_(d,m,w),e(m,k),_(d,h,w),b(u,d,w),T=!0},p:jo,i(d){T||(y(u.$$.fragment,d),T=!0)},o(d){$(u.$$.fragment,d),T=!1},d(d){d&&r(m),d&&r(h),E(u,d)}}}function Uo(L){let m,k;return{c(){m=a("p"),k=s("This API is experimental and may have some slight breaking changes in the next releases.")},l(h){m=o(h,"P",{});var u=n(m);k=i(u,"This API is experimental and may have some slight breaking changes in the next releases."),u.forEach(r)},m(h,u){_(h,m,u),e(m,k)},d(h){h&&r(m)}}}function Ro(L){let m,k,h,u,T,d,w,H,er,Ft,Oe,tr,kt,Ae,rr,Tt,N,G,Ke,pe,ar,Qe,or,It,S,fe,nr,M,sr,Xe,ir,cr,Ze,lr,dr,et,mr,pr,Dt,P,ue,fr,z,ur,tt,hr,gr,rt,_r,xr,at,vr,br,yr,ot,$r,Mt,C,he,Er,V,wr,nt,Fr,kr,st,Tr,Ir,Pt,W,ge,Dr,U,Mr,it,Pr,zr,ct,jr,qr,zt,R,_e,Lr,xe,Or,lt,Ar,Nr,jt,J,Y,dt,ve,Sr,mt,Cr,qt,g,be,Vr,pt,Wr,Ur,K,ye,Rr,$e,Jr,Ne,Br,Hr,Gr,Q,Ee,Yr,we,Kr,Se,Qr,Xr,Zr,D,Fe,ea,j,ta,Ce,ra,aa,ft,oa,na,Ve,sa,ia,ca,X,la,Z,da,ee,ke,ma,q,pa,ut,fa,ua,We,ha,ga,ht,_a,xa,va,O,Te,ba,Ie,ya,gt,$a,Ea,wa,te,Fa,A,De,ka,Me,Ta,_t,Ia,Da,Ma,re,Pa,ae,Pe,za,B,ja,xt,qa,La,Ue,Oa,Aa,Na,oe,ze,Sa,vt,Ca,Va,ne,je,Wa,bt,Ua,Ra,se,qe,Ja,yt,Ba,Lt;return d=new Ha({}),pe=new Ha({}),fe=new I({props:{name:"transformers.image_transforms.center_crop",anchor:"transformers.image_transforms.center_crop",parameters:[{name:"image",val:": ndarray"},{name:"size",val:": typing.Tuple[int, int]"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"return_numpy",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.image_transforms.center_crop.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to crop.`,name:"image"},{anchor:"transformers.image_transforms.center_crop.size",description:`<strong>size</strong> (<code>Tuple[int, int]</code>) &#x2014;
The target size for the cropped image.`,name:"size"},{anchor:"transformers.image_transforms.center_crop.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.
If unset, will use the inferred format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.image_transforms.center_crop.return_numpy",description:`<strong>return_numpy</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the cropped image as a numpy array. Used for backwards compatibility with the
previous ImageFeatureExtractionMixin method.<ul>
<li>Unset: will return the same type as the input image.</li>
<li><code>True</code>: will return a numpy array.</li>
<li><code>False</code>: will return a <code>PIL.Image.Image</code> object.</li>
</ul>`,name:"return_numpy"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_transforms.py#L322",returnDescription:`
<p>The cropped image.</p>
`,returnType:`
<p><code>np.ndarray</code></p>
`}}),ue=new I({props:{name:"transformers.image_transforms.normalize",anchor:"transformers.image_transforms.normalize",parameters:[{name:"image",val:": ndarray"},{name:"mean",val:": typing.Union[float, typing.Iterable[float]]"},{name:"std",val:": typing.Union[float, typing.Iterable[float]]"},{name:"data_format",val:": typing.Optional[transformers.image_utils.ChannelDimension] = None"}],parametersDescription:[{anchor:"transformers.image_transforms.normalize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to normalize.`,name:"image"},{anchor:"transformers.image_transforms.normalize.mean",description:`<strong>mean</strong> (<code>float</code> or <code>Iterable[float]</code>) &#x2014;
The mean to use for normalization.`,name:"mean"},{anchor:"transformers.image_transforms.normalize.std",description:`<strong>std</strong> (<code>float</code> or <code>Iterable[float]</code>) &#x2014;
The standard deviation to use for normalization.`,name:"std"},{anchor:"transformers.image_transforms.normalize.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the output image. If <code>None</code>, will use the inferred format from the input.`,name:"data_format"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_transforms.py#L266"}}),he=new I({props:{name:"transformers.rescale",anchor:"transformers.rescale",parameters:[{name:"image",val:": ndarray"},{name:"scale",val:": float"},{name:"data_format",val:": typing.Optional[transformers.image_utils.ChannelDimension] = None"},{name:"dtype",val:" = <class 'numpy.float32'>"}],parametersDescription:[{anchor:"transformers.rescale.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to rescale.`,name:"image"},{anchor:"transformers.rescale.scale",description:`<strong>scale</strong> (<code>float</code>) &#x2014;
The scale to use for rescaling the image.`,name:"scale"},{anchor:"transformers.rescale.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the image. If not provided, it will be the same as the input image.`,name:"data_format"},{anchor:"transformers.rescale.dtype",description:`<strong>dtype</strong> (<code>np.dtype</code>, <em>optional</em>, defaults to <code>np.float32</code>) &#x2014;
The dtype of the output image. Defaults to <code>np.float32</code>. Used for backwards compatibility with feature
extractors.`,name:"dtype"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_transforms.py#L80",returnDescription:`
<p>The rescaled image.</p>
`,returnType:`
<p><code>np.ndarray</code></p>
`}}),ge=new I({props:{name:"transformers.resize",anchor:"transformers.resize",parameters:[{name:"image",val:""},{name:"size",val:": typing.Tuple[int, int]"},{name:"resample",val:" = <Resampling.BILINEAR: 2>"},{name:"data_format",val:": typing.Optional[transformers.image_utils.ChannelDimension] = None"},{name:"return_numpy",val:": bool = True"}],parametersDescription:[{anchor:"transformers.resize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.resize.size",description:`<strong>size</strong> (<code>Tuple[int, int]</code>) &#x2014;
The size to use for resizing the image.`,name:"size"},{anchor:"transformers.resize.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.Resampling.BILINEAR</code>) &#x2014;
The filter to user for resampling.`,name:"resample"},{anchor:"transformers.resize.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the output image. If <code>None</code>, will use the inferred format from the input.`,name:"data_format"},{anchor:"transformers.resize.return_numpy",description:`<strong>return_numpy</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return the resized image as a numpy array. If False a <code>PIL.Image.Image</code> object is
returned.`,name:"return_numpy"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_transforms.py#L217",returnDescription:`
<p>The resized image.</p>
`,returnType:`
<p><code>np.ndarray</code></p>
`}}),_e=new I({props:{name:"transformers.to_pil_image",anchor:"transformers.to_pil_image",parameters:[{name:"image",val:": typing.Union[numpy.ndarray, PIL.Image.Image, ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor'), ForwardRef('jnp.Tensor')]"},{name:"do_rescale",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.to_pil_image.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>numpy.ndarray</code> or <code>torch.Tensor</code> or <code>tf.Tensor</code>) &#x2014;
The image to convert to the <code>PIL.Image</code> format.`,name:"image"},{anchor:"transformers.to_pil_image.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values integers between 0 and 255). Will default
to <code>True</code> if the image type is a floating type, <code>False</code> otherwise.`,name:"do_rescale"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_transforms.py#L110",returnDescription:`
<p>The converted image.</p>
`,returnType:`
<p><code>PIL.Image.Image</code></p>
`}}),ve=new Ha({}),be=new I({props:{name:"class transformers.FeatureExtractionMixin",anchor:"transformers.FeatureExtractionMixin",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L200"}}),ye=new I({props:{name:"from_dict",anchor:"transformers.FeatureExtractionMixin.from_dict",parameters:[{name:"feature_extractor_dict",val:": typing.Dict[str, typing.Any]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.from_dict.feature_extractor_dict",description:`<strong>feature_extractor_dict</strong> (<code>Dict[str, Any]</code>) &#x2014;
Dictionary that will be used to instantiate the feature extractor object. Such a dictionary can be
retrieved from a pretrained checkpoint by leveraging the
<a href="/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin.to_dict">to_dict()</a> method.`,name:"feature_extractor_dict"},{anchor:"transformers.FeatureExtractionMixin.from_dict.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>) &#x2014;
Additional parameters from which to initialize the feature extractor object.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L445",returnDescription:`
<p>The feature extractor object instantiated from those
parameters.</p>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"
>FeatureExtractionMixin</a></p>
`}}),Ee=new I({props:{name:"from_json_file",anchor:"transformers.FeatureExtractionMixin.from_json_file",parameters:[{name:"json_file",val:": typing.Union[str, os.PathLike]"}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.from_json_file.json_file",description:`<strong>json_file</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Path to the JSON file containing the parameters.`,name:"json_file"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L494",returnDescription:`
<p>The feature_extractor
object instantiated from that JSON file.</p>
`,returnType:`
<p>A feature extractor of type <a
  href="/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"
>FeatureExtractionMixin</a></p>
`}}),Fe=new I({props:{name:"from_pretrained",anchor:"transformers.FeatureExtractionMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L224",returnDescription:`
<p>A feature extractor of type <a
  href="/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"
>FeatureExtractionMixin</a>.</p>
`}}),X=new Po({props:{$$slots:{default:[Co]},$$scope:{ctx:L}}}),Z=new zo({props:{anchor:"transformers.FeatureExtractionMixin.from_pretrained.example",$$slots:{default:[Vo]},$$scope:{ctx:L}}}),ke=new I({props:{name:"get_feature_extractor_dict",anchor:"transformers.FeatureExtractionMixin.get_feature_extractor_dict",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.get_feature_extractor_dict.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
The identifier of the pre-trained checkpoint from which we want the dictionary of parameters.`,name:"pretrained_model_name_or_path"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L352",returnDescription:`
<p>The dictionary(ies) that will be used to instantiate the feature extractor object.</p>
`,returnType:`
<p><code>Tuple[Dict, Dict]</code></p>
`}}),Te=new I({props:{name:"push_to_hub",anchor:"transformers.FeatureExtractionMixin.push_to_hub",parameters:[{name:"repo_id",val:": str"},{name:"use_temp_dir",val:": typing.Optional[bool] = None"},{name:"commit_message",val:": typing.Optional[str] = None"},{name:"private",val:": typing.Optional[bool] = None"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"max_shard_size",val:": typing.Union[int, str, NoneType] = '10GB'"},{name:"create_pr",val:": bool = False"},{name:"**deprecated_kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.push_to_hub.repo_id",description:`<strong>repo_id</strong> (<code>str</code>) &#x2014;
The name of the repository you want to push your feature extractor to. It should contain your organization name
when pushing to a given organization.`,name:"repo_id"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.use_temp_dir",description:`<strong>use_temp_dir</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.
Will default to <code>True</code> if there is no directory named like <code>repo_id</code>, <code>False</code> otherwise.`,name:"use_temp_dir"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.commit_message",description:`<strong>commit_message</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Message to commit while pushing. Will default to <code>&quot;Upload feature extractor&quot;</code>.`,name:"commit_message"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.private",description:`<strong>private</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not the repository created should be private (requires a paying subscription).`,name:"private"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.use_auth_token",description:`<strong>use_auth_token</strong> (<code>bool</code> or <code>str</code>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>). Will default to <code>True</code> if <code>repo_url</code>
is not specified.`,name:"use_auth_token"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.max_shard_size",description:`<strong>max_shard_size</strong> (<code>int</code> or <code>str</code>, <em>optional</em>, defaults to <code>&quot;10GB&quot;</code>) &#x2014;
Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard
will then be each of size lower than this size. If expressed as a string, needs to be digits followed
by a unit (like <code>&quot;5MB&quot;</code>).`,name:"max_shard_size"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.create_pr",description:`<strong>create_pr</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to create a PR with the uploaded files or directly commit.`,name:"create_pr"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/utils/hub.py#L712"}}),te=new zo({props:{anchor:"transformers.FeatureExtractionMixin.push_to_hub.example",$$slots:{default:[Wo]},$$scope:{ctx:L}}}),De=new I({props:{name:"register_for_auto_class",anchor:"transformers.FeatureExtractionMixin.register_for_auto_class",parameters:[{name:"auto_class",val:" = 'AutoFeatureExtractor'"}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.register_for_auto_class.auto_class",description:`<strong>auto_class</strong> (<code>str</code> or <code>type</code>, <em>optional</em>, defaults to <code>&quot;AutoFeatureExtractor&quot;</code>) &#x2014;
The auto class to register this new feature extractor with.`,name:"auto_class"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L548"}}),re=new Po({props:{warning:!0,$$slots:{default:[Uo]},$$scope:{ctx:L}}}),Pe=new I({props:{name:"save_pretrained",anchor:"transformers.FeatureExtractionMixin.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike]"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the feature extractor JSON file will be saved (will be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.FeatureExtractionMixin.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).
kwargs &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin.push_to_hub">push_to_hub()</a> method.`,name:"push_to_hub"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L308"}}),ze=new I({props:{name:"to_dict",anchor:"transformers.FeatureExtractionMixin.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L482",returnDescription:`
<p>Dictionary of all the attributes that make up this feature extractor instance.</p>
`,returnType:`
<p><code>Dict[str, Any]</code></p>
`}}),je=new I({props:{name:"to_json_file",anchor:"transformers.FeatureExtractionMixin.to_json_file",parameters:[{name:"json_file_path",val:": typing.Union[str, os.PathLike]"}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.to_json_file.json_file_path",description:`<strong>json_file_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Path to the JSON file in which this feature_extractor instance&#x2019;s parameters will be saved.`,name:"json_file_path"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L534"}}),qe=new I({props:{name:"to_json_string",anchor:"transformers.FeatureExtractionMixin.to_json_string",parameters:[],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L513",returnDescription:`
<p>String containing all the attributes that make up this feature_extractor instance in JSON format.</p>
`,returnType:`
<p><code>str</code></p>
`}}),{c(){m=a("meta"),k=c(),h=a("h1"),u=a("a"),T=a("span"),x(d.$$.fragment),w=c(),H=a("span"),er=s("Utilities for Image Processors"),Ft=c(),Oe=a("p"),tr=s(`This page lists all the utility functions used by the image processors, mainly the functional
transformations used to process the images.`),kt=c(),Ae=a("p"),rr=s("Most of those are only useful if you are studying the code of the image processors in the library."),Tt=c(),N=a("h2"),G=a("a"),Ke=a("span"),x(pe.$$.fragment),ar=c(),Qe=a("span"),or=s("Image Transformations"),It=c(),S=a("div"),x(fe.$$.fragment),nr=c(),M=a("p"),sr=s("Crops the "),Xe=a("code"),ir=s("image"),cr=s(" to the specified "),Ze=a("code"),lr=s("size"),dr=s(` using a center crop. Note that if the image is too small to be cropped to
the size given, it will be padded (so the returned result will always be of size `),et=a("code"),mr=s("size"),pr=s(")."),Dt=c(),P=a("div"),x(ue.$$.fragment),fr=c(),z=a("p"),ur=s("Normalizes "),tt=a("code"),hr=s("image"),gr=s(" using the mean and standard deviation specified by "),rt=a("code"),_r=s("mean"),xr=s(" and "),at=a("code"),vr=s("std"),br=s("."),yr=c(),ot=a("p"),$r=s("image = (image - mean) / std"),Mt=c(),C=a("div"),x(he.$$.fragment),Er=c(),V=a("p"),wr=s("Rescales "),nt=a("code"),Fr=s("image"),kr=s(" by "),st=a("code"),Tr=s("scale"),Ir=s("."),Pt=c(),W=a("div"),x(ge.$$.fragment),Dr=c(),U=a("p"),Mr=s("Resizes "),it=a("code"),Pr=s("image"),zr=s(" to (h, w) specified by "),ct=a("code"),jr=s("size"),qr=s(" using the PIL library."),zt=c(),R=a("div"),x(_e.$$.fragment),Lr=c(),xe=a("p"),Or=s("Converts "),lt=a("code"),Ar=s("image"),Nr=s(` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),jt=c(),J=a("h2"),Y=a("a"),dt=a("span"),x(ve.$$.fragment),Sr=c(),mt=a("span"),Cr=s("ImageProcessorMixin"),qt=c(),g=a("div"),x(be.$$.fragment),Vr=c(),pt=a("p"),Wr=s(`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),Ur=c(),K=a("div"),x(ye.$$.fragment),Rr=c(),$e=a("p"),Jr=s("Instantiates a type of "),Ne=a("a"),Br=s("FeatureExtractionMixin"),Hr=s(` from a Python dictionary of
parameters.`),Gr=c(),Q=a("div"),x(Ee.$$.fragment),Yr=c(),we=a("p"),Kr=s("Instantiates a feature extractor of type "),Se=a("a"),Qr=s("FeatureExtractionMixin"),Xr=s(` from the path to
a JSON file of parameters.`),Zr=c(),D=a("div"),x(Fe.$$.fragment),ea=c(),j=a("p"),ta=s("Instantiate a type of "),Ce=a("a"),ra=s("FeatureExtractionMixin"),aa=s(" from a feature extractor, "),ft=a("em"),oa=s("e.g."),na=s(` a
derived class of `),Ve=a("a"),sa=s("SequenceFeatureExtractor"),ia=s("."),ca=c(),x(X.$$.fragment),la=c(),x(Z.$$.fragment),da=c(),ee=a("div"),x(ke.$$.fragment),ma=c(),q=a("p"),pa=s("From a "),ut=a("code"),fa=s("pretrained_model_name_or_path"),ua=s(`, resolve to a dictionary of parameters, to be used for instantiating a
feature extractor of type `),We=a("a"),ha=s("FeatureExtractionMixin"),ga=s(" using "),ht=a("code"),_a=s("from_dict"),xa=s("."),va=c(),O=a("div"),x(Te.$$.fragment),ba=c(),Ie=a("p"),ya=s(`Upload the feature extractor file to the \u{1F917} Model Hub while synchronizing a local clone of the repo in
`),gt=a("code"),$a=s("repo_path_or_name"),Ea=s("."),wa=c(),x(te.$$.fragment),Fa=c(),A=a("div"),x(De.$$.fragment),ka=c(),Me=a("p"),Ta=s(`Register this class with a given auto class. This should only be used for custom feature extractors as the ones
in the library are already mapped with `),_t=a("code"),Ia=s("AutoFeatureExtractor"),Da=s("."),Ma=c(),x(re.$$.fragment),Pa=c(),ae=a("div"),x(Pe.$$.fragment),za=c(),B=a("p"),ja=s("Save a feature_extractor object to the directory "),xt=a("code"),qa=s("save_directory"),La=s(`, so that it can be re-loaded using the
`),Ue=a("a"),Oa=s("from_pretrained()"),Aa=s(" class method."),Na=c(),oe=a("div"),x(ze.$$.fragment),Sa=c(),vt=a("p"),Ca=s("Serializes this instance to a Python dictionary."),Va=c(),ne=a("div"),x(je.$$.fragment),Wa=c(),bt=a("p"),Ua=s("Save this instance to a JSON file."),Ra=c(),se=a("div"),x(qe.$$.fragment),Ja=c(),yt=a("p"),Ba=s("Serializes this instance to a JSON string."),this.h()},l(t){const f=No('[data-svelte="svelte-1phssyn"]',document.head);m=o(f,"META",{name:!0,content:!0}),f.forEach(r),k=l(t),h=o(t,"H1",{class:!0});var Le=n(h);u=o(Le,"A",{id:!0,class:!0,href:!0});var $t=n(u);T=o($t,"SPAN",{});var Et=n(T);v(d.$$.fragment,Et),Et.forEach(r),$t.forEach(r),w=l(Le),H=o(Le,"SPAN",{});var wt=n(H);er=i(wt,"Utilities for Image Processors"),wt.forEach(r),Le.forEach(r),Ft=l(t),Oe=o(t,"P",{});var Ga=n(Oe);tr=i(Ga,`This page lists all the utility functions used by the image processors, mainly the functional
transformations used to process the images.`),Ga.forEach(r),kt=l(t),Ae=o(t,"P",{});var Ya=n(Ae);rr=i(Ya,"Most of those are only useful if you are studying the code of the image processors in the library."),Ya.forEach(r),Tt=l(t),N=o(t,"H2",{class:!0});var Ot=n(N);G=o(Ot,"A",{id:!0,class:!0,href:!0});var Ka=n(G);Ke=o(Ka,"SPAN",{});var Qa=n(Ke);v(pe.$$.fragment,Qa),Qa.forEach(r),Ka.forEach(r),ar=l(Ot),Qe=o(Ot,"SPAN",{});var Xa=n(Qe);or=i(Xa,"Image Transformations"),Xa.forEach(r),Ot.forEach(r),It=l(t),S=o(t,"DIV",{class:!0});var At=n(S);v(fe.$$.fragment,At),nr=l(At),M=o(At,"P",{});var ie=n(M);sr=i(ie,"Crops the "),Xe=o(ie,"CODE",{});var Za=n(Xe);ir=i(Za,"image"),Za.forEach(r),cr=i(ie," to the specified "),Ze=o(ie,"CODE",{});var eo=n(Ze);lr=i(eo,"size"),eo.forEach(r),dr=i(ie,` using a center crop. Note that if the image is too small to be cropped to
the size given, it will be padded (so the returned result will always be of size `),et=o(ie,"CODE",{});var to=n(et);mr=i(to,"size"),to.forEach(r),pr=i(ie,")."),ie.forEach(r),At.forEach(r),Dt=l(t),P=o(t,"DIV",{class:!0});var Re=n(P);v(ue.$$.fragment,Re),fr=l(Re),z=o(Re,"P",{});var ce=n(z);ur=i(ce,"Normalizes "),tt=o(ce,"CODE",{});var ro=n(tt);hr=i(ro,"image"),ro.forEach(r),gr=i(ce," using the mean and standard deviation specified by "),rt=o(ce,"CODE",{});var ao=n(rt);_r=i(ao,"mean"),ao.forEach(r),xr=i(ce," and "),at=o(ce,"CODE",{});var oo=n(at);vr=i(oo,"std"),oo.forEach(r),br=i(ce,"."),ce.forEach(r),yr=l(Re),ot=o(Re,"P",{});var no=n(ot);$r=i(no,"image = (image - mean) / std"),no.forEach(r),Re.forEach(r),Mt=l(t),C=o(t,"DIV",{class:!0});var Nt=n(C);v(he.$$.fragment,Nt),Er=l(Nt),V=o(Nt,"P",{});var Je=n(V);wr=i(Je,"Rescales "),nt=o(Je,"CODE",{});var so=n(nt);Fr=i(so,"image"),so.forEach(r),kr=i(Je," by "),st=o(Je,"CODE",{});var io=n(st);Tr=i(io,"scale"),io.forEach(r),Ir=i(Je,"."),Je.forEach(r),Nt.forEach(r),Pt=l(t),W=o(t,"DIV",{class:!0});var St=n(W);v(ge.$$.fragment,St),Dr=l(St),U=o(St,"P",{});var Be=n(U);Mr=i(Be,"Resizes "),it=o(Be,"CODE",{});var co=n(it);Pr=i(co,"image"),co.forEach(r),zr=i(Be," to (h, w) specified by "),ct=o(Be,"CODE",{});var lo=n(ct);jr=i(lo,"size"),lo.forEach(r),qr=i(Be," using the PIL library."),Be.forEach(r),St.forEach(r),zt=l(t),R=o(t,"DIV",{class:!0});var Ct=n(R);v(_e.$$.fragment,Ct),Lr=l(Ct),xe=o(Ct,"P",{});var Vt=n(xe);Or=i(Vt,"Converts "),lt=o(Vt,"CODE",{});var mo=n(lt);Ar=i(mo,"image"),mo.forEach(r),Nr=i(Vt,` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),Vt.forEach(r),Ct.forEach(r),jt=l(t),J=o(t,"H2",{class:!0});var Wt=n(J);Y=o(Wt,"A",{id:!0,class:!0,href:!0});var po=n(Y);dt=o(po,"SPAN",{});var fo=n(dt);v(ve.$$.fragment,fo),fo.forEach(r),po.forEach(r),Sr=l(Wt),mt=o(Wt,"SPAN",{});var uo=n(mt);Cr=i(uo,"ImageProcessorMixin"),uo.forEach(r),Wt.forEach(r),qt=l(t),g=o(t,"DIV",{class:!0});var F=n(g);v(be.$$.fragment,F),Vr=l(F),pt=o(F,"P",{});var ho=n(pt);Wr=i(ho,`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),ho.forEach(r),Ur=l(F),K=o(F,"DIV",{class:!0});var Ut=n(K);v(ye.$$.fragment,Ut),Rr=l(Ut),$e=o(Ut,"P",{});var Rt=n($e);Jr=i(Rt,"Instantiates a type of "),Ne=o(Rt,"A",{href:!0});var go=n(Ne);Br=i(go,"FeatureExtractionMixin"),go.forEach(r),Hr=i(Rt,` from a Python dictionary of
parameters.`),Rt.forEach(r),Ut.forEach(r),Gr=l(F),Q=o(F,"DIV",{class:!0});var Jt=n(Q);v(Ee.$$.fragment,Jt),Yr=l(Jt),we=o(Jt,"P",{});var Bt=n(we);Kr=i(Bt,"Instantiates a feature extractor of type "),Se=o(Bt,"A",{href:!0});var _o=n(Se);Qr=i(_o,"FeatureExtractionMixin"),_o.forEach(r),Xr=i(Bt,` from the path to
a JSON file of parameters.`),Bt.forEach(r),Jt.forEach(r),Zr=l(F),D=o(F,"DIV",{class:!0});var le=n(D);v(Fe.$$.fragment,le),ea=l(le),j=o(le,"P",{});var de=n(j);ta=i(de,"Instantiate a type of "),Ce=o(de,"A",{href:!0});var xo=n(Ce);ra=i(xo,"FeatureExtractionMixin"),xo.forEach(r),aa=i(de," from a feature extractor, "),ft=o(de,"EM",{});var vo=n(ft);oa=i(vo,"e.g."),vo.forEach(r),na=i(de,` a
derived class of `),Ve=o(de,"A",{href:!0});var bo=n(Ve);sa=i(bo,"SequenceFeatureExtractor"),bo.forEach(r),ia=i(de,"."),de.forEach(r),ca=l(le),v(X.$$.fragment,le),la=l(le),v(Z.$$.fragment,le),le.forEach(r),da=l(F),ee=o(F,"DIV",{class:!0});var Ht=n(ee);v(ke.$$.fragment,Ht),ma=l(Ht),q=o(Ht,"P",{});var me=n(q);pa=i(me,"From a "),ut=o(me,"CODE",{});var yo=n(ut);fa=i(yo,"pretrained_model_name_or_path"),yo.forEach(r),ua=i(me,`, resolve to a dictionary of parameters, to be used for instantiating a
feature extractor of type `),We=o(me,"A",{href:!0});var $o=n(We);ha=i($o,"FeatureExtractionMixin"),$o.forEach(r),ga=i(me," using "),ht=o(me,"CODE",{});var Eo=n(ht);_a=i(Eo,"from_dict"),Eo.forEach(r),xa=i(me,"."),me.forEach(r),Ht.forEach(r),va=l(F),O=o(F,"DIV",{class:!0});var He=n(O);v(Te.$$.fragment,He),ba=l(He),Ie=o(He,"P",{});var Gt=n(Ie);ya=i(Gt,`Upload the feature extractor file to the \u{1F917} Model Hub while synchronizing a local clone of the repo in
`),gt=o(Gt,"CODE",{});var wo=n(gt);$a=i(wo,"repo_path_or_name"),wo.forEach(r),Ea=i(Gt,"."),Gt.forEach(r),wa=l(He),v(te.$$.fragment,He),He.forEach(r),Fa=l(F),A=o(F,"DIV",{class:!0});var Ge=n(A);v(De.$$.fragment,Ge),ka=l(Ge),Me=o(Ge,"P",{});var Yt=n(Me);Ta=i(Yt,`Register this class with a given auto class. This should only be used for custom feature extractors as the ones
in the library are already mapped with `),_t=o(Yt,"CODE",{});var Fo=n(_t);Ia=i(Fo,"AutoFeatureExtractor"),Fo.forEach(r),Da=i(Yt,"."),Yt.forEach(r),Ma=l(Ge),v(re.$$.fragment,Ge),Ge.forEach(r),Pa=l(F),ae=o(F,"DIV",{class:!0});var Kt=n(ae);v(Pe.$$.fragment,Kt),za=l(Kt),B=o(Kt,"P",{});var Ye=n(B);ja=i(Ye,"Save a feature_extractor object to the directory "),xt=o(Ye,"CODE",{});var ko=n(xt);qa=i(ko,"save_directory"),ko.forEach(r),La=i(Ye,`, so that it can be re-loaded using the
`),Ue=o(Ye,"A",{href:!0});var To=n(Ue);Oa=i(To,"from_pretrained()"),To.forEach(r),Aa=i(Ye," class method."),Ye.forEach(r),Kt.forEach(r),Na=l(F),oe=o(F,"DIV",{class:!0});var Qt=n(oe);v(ze.$$.fragment,Qt),Sa=l(Qt),vt=o(Qt,"P",{});var Io=n(vt);Ca=i(Io,"Serializes this instance to a Python dictionary."),Io.forEach(r),Qt.forEach(r),Va=l(F),ne=o(F,"DIV",{class:!0});var Xt=n(ne);v(je.$$.fragment,Xt),Wa=l(Xt),bt=o(Xt,"P",{});var Do=n(bt);Ua=i(Do,"Save this instance to a JSON file."),Do.forEach(r),Xt.forEach(r),Ra=l(F),se=o(F,"DIV",{class:!0});var Zt=n(se);v(qe.$$.fragment,Zt),Ja=l(Zt),yt=o(Zt,"P",{});var Mo=n(yt);Ba=i(Mo,"Serializes this instance to a JSON string."),Mo.forEach(r),Zt.forEach(r),F.forEach(r),this.h()},h(){p(m,"name","hf:doc:metadata"),p(m,"content",JSON.stringify(Jo)),p(u,"id","utilities-for-image-processors"),p(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(u,"href","#utilities-for-image-processors"),p(h,"class","relative group"),p(G,"id","transformers.image_transforms.center_crop"),p(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(G,"href","#transformers.image_transforms.center_crop"),p(N,"class","relative group"),p(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Y,"id","transformers.FeatureExtractionMixin"),p(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Y,"href","#transformers.FeatureExtractionMixin"),p(J,"class","relative group"),p(Ne,"href","/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"),p(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Se,"href","/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"),p(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Ce,"href","/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"),p(Ve,"href","/docs/transformers/main/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor"),p(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(We,"href","/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"),p(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Ue,"href","/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin.from_pretrained"),p(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(g,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,f){e(document.head,m),_(t,k,f),_(t,h,f),e(h,u),e(u,T),b(d,T,null),e(h,w),e(h,H),e(H,er),_(t,Ft,f),_(t,Oe,f),e(Oe,tr),_(t,kt,f),_(t,Ae,f),e(Ae,rr),_(t,Tt,f),_(t,N,f),e(N,G),e(G,Ke),b(pe,Ke,null),e(N,ar),e(N,Qe),e(Qe,or),_(t,It,f),_(t,S,f),b(fe,S,null),e(S,nr),e(S,M),e(M,sr),e(M,Xe),e(Xe,ir),e(M,cr),e(M,Ze),e(Ze,lr),e(M,dr),e(M,et),e(et,mr),e(M,pr),_(t,Dt,f),_(t,P,f),b(ue,P,null),e(P,fr),e(P,z),e(z,ur),e(z,tt),e(tt,hr),e(z,gr),e(z,rt),e(rt,_r),e(z,xr),e(z,at),e(at,vr),e(z,br),e(P,yr),e(P,ot),e(ot,$r),_(t,Mt,f),_(t,C,f),b(he,C,null),e(C,Er),e(C,V),e(V,wr),e(V,nt),e(nt,Fr),e(V,kr),e(V,st),e(st,Tr),e(V,Ir),_(t,Pt,f),_(t,W,f),b(ge,W,null),e(W,Dr),e(W,U),e(U,Mr),e(U,it),e(it,Pr),e(U,zr),e(U,ct),e(ct,jr),e(U,qr),_(t,zt,f),_(t,R,f),b(_e,R,null),e(R,Lr),e(R,xe),e(xe,Or),e(xe,lt),e(lt,Ar),e(xe,Nr),_(t,jt,f),_(t,J,f),e(J,Y),e(Y,dt),b(ve,dt,null),e(J,Sr),e(J,mt),e(mt,Cr),_(t,qt,f),_(t,g,f),b(be,g,null),e(g,Vr),e(g,pt),e(pt,Wr),e(g,Ur),e(g,K),b(ye,K,null),e(K,Rr),e(K,$e),e($e,Jr),e($e,Ne),e(Ne,Br),e($e,Hr),e(g,Gr),e(g,Q),b(Ee,Q,null),e(Q,Yr),e(Q,we),e(we,Kr),e(we,Se),e(Se,Qr),e(we,Xr),e(g,Zr),e(g,D),b(Fe,D,null),e(D,ea),e(D,j),e(j,ta),e(j,Ce),e(Ce,ra),e(j,aa),e(j,ft),e(ft,oa),e(j,na),e(j,Ve),e(Ve,sa),e(j,ia),e(D,ca),b(X,D,null),e(D,la),b(Z,D,null),e(g,da),e(g,ee),b(ke,ee,null),e(ee,ma),e(ee,q),e(q,pa),e(q,ut),e(ut,fa),e(q,ua),e(q,We),e(We,ha),e(q,ga),e(q,ht),e(ht,_a),e(q,xa),e(g,va),e(g,O),b(Te,O,null),e(O,ba),e(O,Ie),e(Ie,ya),e(Ie,gt),e(gt,$a),e(Ie,Ea),e(O,wa),b(te,O,null),e(g,Fa),e(g,A),b(De,A,null),e(A,ka),e(A,Me),e(Me,Ta),e(Me,_t),e(_t,Ia),e(Me,Da),e(A,Ma),b(re,A,null),e(g,Pa),e(g,ae),b(Pe,ae,null),e(ae,za),e(ae,B),e(B,ja),e(B,xt),e(xt,qa),e(B,La),e(B,Ue),e(Ue,Oa),e(B,Aa),e(g,Na),e(g,oe),b(ze,oe,null),e(oe,Sa),e(oe,vt),e(vt,Ca),e(g,Va),e(g,ne),b(je,ne,null),e(ne,Wa),e(ne,bt),e(bt,Ua),e(g,Ra),e(g,se),b(qe,se,null),e(se,Ja),e(se,yt),e(yt,Ba),Lt=!0},p(t,[f]){const Le={};f&2&&(Le.$$scope={dirty:f,ctx:t}),X.$set(Le);const $t={};f&2&&($t.$$scope={dirty:f,ctx:t}),Z.$set($t);const Et={};f&2&&(Et.$$scope={dirty:f,ctx:t}),te.$set(Et);const wt={};f&2&&(wt.$$scope={dirty:f,ctx:t}),re.$set(wt)},i(t){Lt||(y(d.$$.fragment,t),y(pe.$$.fragment,t),y(fe.$$.fragment,t),y(ue.$$.fragment,t),y(he.$$.fragment,t),y(ge.$$.fragment,t),y(_e.$$.fragment,t),y(ve.$$.fragment,t),y(be.$$.fragment,t),y(ye.$$.fragment,t),y(Ee.$$.fragment,t),y(Fe.$$.fragment,t),y(X.$$.fragment,t),y(Z.$$.fragment,t),y(ke.$$.fragment,t),y(Te.$$.fragment,t),y(te.$$.fragment,t),y(De.$$.fragment,t),y(re.$$.fragment,t),y(Pe.$$.fragment,t),y(ze.$$.fragment,t),y(je.$$.fragment,t),y(qe.$$.fragment,t),Lt=!0)},o(t){$(d.$$.fragment,t),$(pe.$$.fragment,t),$(fe.$$.fragment,t),$(ue.$$.fragment,t),$(he.$$.fragment,t),$(ge.$$.fragment,t),$(_e.$$.fragment,t),$(ve.$$.fragment,t),$(be.$$.fragment,t),$(ye.$$.fragment,t),$(Ee.$$.fragment,t),$(Fe.$$.fragment,t),$(X.$$.fragment,t),$(Z.$$.fragment,t),$(ke.$$.fragment,t),$(Te.$$.fragment,t),$(te.$$.fragment,t),$(De.$$.fragment,t),$(re.$$.fragment,t),$(Pe.$$.fragment,t),$(ze.$$.fragment,t),$(je.$$.fragment,t),$(qe.$$.fragment,t),Lt=!1},d(t){r(m),t&&r(k),t&&r(h),E(d),t&&r(Ft),t&&r(Oe),t&&r(kt),t&&r(Ae),t&&r(Tt),t&&r(N),E(pe),t&&r(It),t&&r(S),E(fe),t&&r(Dt),t&&r(P),E(ue),t&&r(Mt),t&&r(C),E(he),t&&r(Pt),t&&r(W),E(ge),t&&r(zt),t&&r(R),E(_e),t&&r(jt),t&&r(J),E(ve),t&&r(qt),t&&r(g),E(be),E(ye),E(Ee),E(Fe),E(X),E(Z),E(ke),E(Te),E(te),E(De),E(re),E(Pe),E(ze),E(je),E(qe)}}}const Jo={local:"utilities-for-image-processors",sections:[{local:"transformers.image_transforms.center_crop",title:"Image Transformations"},{local:"transformers.FeatureExtractionMixin",title:"ImageProcessorMixin"}],title:"Utilities for Image Processors"};function Bo(L){return So(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Zo extends Lo{constructor(m){super();Oo(this,m,Bo,Ro,Ao,{})}}export{Zo as default,Jo as metadata};
