import{S as la,i as da,s as pa,e as t,k as m,w as _,t as n,M as ga,c as s,d as o,m as c,a,x as b,h as i,b as p,G as e,g,y as v,q as y,o as $,B as x,v as fa,L as ma}from"../../chunks/vendor-hf-doc-builder.js";import{T as ha}from"../../chunks/Tip-hf-doc-builder.js";import{D as w}from"../../chunks/Docstring-hf-doc-builder.js";import{C as ca}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as vs}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as ia}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function ua(Y){let f,D,I,h,T;return h=new ca({props:{code:`# We can't instantiate directly the base class *ImageProcessingMixin* so let's show the examples on a
# derived class: *CLIPImageProcessor*
image_processor = CLIPImageProcessor.from_pretrained(
    "openai/clip-vit-base-patch32"
)  # Download image_processing_config from huggingface.co and cache.
image_processor = CLIPImageProcessor.from_pretrained(
    "./test/saved_model/"
)  # E.g. image processor (or model) was saved using *save_pretrained('./test/saved_model/')*
image_processor = CLIPImageProcessor.from_pretrained("./test/saved_model/preprocessor_config.json")
image_processor = CLIPImageProcessor.from_pretrained(
    "openai/clip-vit-base-patch32", do_normalize=False, foo=False
)
assert image_processor.do_normalize is False
image_processor, unused_kwargs = CLIPImageProcessor.from_pretrained(
    "openai/clip-vit-base-patch32", do_normalize=False, foo=False, return_unused_kwargs=True
)
assert image_processor.do_normalize is False
assert unused_kwargs == {"foo": False}`,highlighted:`<span class="hljs-comment"># We can&#x27;t instantiate directly the base class *ImageProcessingMixin* so let&#x27;s show the examples on a</span>
<span class="hljs-comment"># derived class: *CLIPImageProcessor*</span>
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>
)  <span class="hljs-comment"># Download image_processing_config from huggingface.co and cache.</span>
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;./test/saved_model/&quot;</span>
)  <span class="hljs-comment"># E.g. image processor (or model) was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*</span>
image_processor = CLIPImageProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/preprocessor_config.json&quot;</span>)
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>, do_normalize=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>
)
<span class="hljs-keyword">assert</span> image_processor.do_normalize <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
image_processor, unused_kwargs = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>, do_normalize=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
)
<span class="hljs-keyword">assert</span> image_processor.do_normalize <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
<span class="hljs-keyword">assert</span> unused_kwargs == {<span class="hljs-string">&quot;foo&quot;</span>: <span class="hljs-literal">False</span>}`}}),{c(){f=t("p"),D=n("Examples:"),I=m(),_(h.$$.fragment)},l(d){f=s(d,"P",{});var E=a(f);D=i(E,"Examples:"),E.forEach(o),I=c(d),b(h.$$.fragment,d)},m(d,E){g(d,f,E),e(f,D),g(d,I,E),v(h,d,E),T=!0},p:ma,i(d){T||(y(h.$$.fragment,d),T=!0)},o(d){$(h.$$.fragment,d),T=!1},d(d){d&&o(f),d&&o(I),x(h,d)}}}function _a(Y){let f,D,I,h,T;return h=new ca({props:{code:`from transformers import AutoImageProcessor

image processor = AutoImageProcessor.from_pretrained("bert-base-cased")

# Push the image processor to your namespace with the name "my-finetuned-bert".
image processor.push_to_hub("my-finetuned-bert")

# Push the image processor to an organization with the name "my-finetuned-bert".
image processor.push_to_hub("huggingface/my-finetuned-bert")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor

image processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-comment"># Push the image processor to your namespace with the name &quot;my-finetuned-bert&quot;.</span>
image processor.push_to_hub(<span class="hljs-string">&quot;my-finetuned-bert&quot;</span>)

<span class="hljs-comment"># Push the image processor to an organization with the name &quot;my-finetuned-bert&quot;.</span>
image processor.push_to_hub(<span class="hljs-string">&quot;huggingface/my-finetuned-bert&quot;</span>)`}}),{c(){f=t("p"),D=n("Examples:"),I=m(),_(h.$$.fragment)},l(d){f=s(d,"P",{});var E=a(f);D=i(E,"Examples:"),E.forEach(o),I=c(d),b(h.$$.fragment,d)},m(d,E){g(d,f,E),e(f,D),g(d,I,E),v(h,d,E),T=!0},p:ma,i(d){T||(y(h.$$.fragment,d),T=!0)},o(d){$(h.$$.fragment,d),T=!1},d(d){d&&o(f),d&&o(I),x(h,d)}}}function ba(Y){let f,D;return{c(){f=t("p"),D=n("This API is experimental and may have some slight breaking changes in the next releases.")},l(I){f=s(I,"P",{});var h=a(f);D=i(h,"This API is experimental and may have some slight breaking changes in the next releases."),h.forEach(o)},m(I,h){g(I,f,h),e(f,D)},d(I){I&&o(f)}}}function va(Y){let f,D,I,h,T,d,E,rr,vo,Or,Fe,yo,Nr,Ue,$o,Ar,A,K,or,de,xo,tr,Io,Sr,S,pe,Po,k,wo,sr,Eo,Do,ar,To,ko,nr,Mo,zo,Fr,M,ge,Lo,ir,jo,Co,mr,qo,Ur,z,fe,Oo,cr,No,Ao,lr,So,Vr,F,he,Fo,dr,Uo,Rr,L,ue,Vo,j,Ro,pr,Wo,Jo,gr,Bo,Ho,fr,Go,Yo,Ko,hr,Qo,Wr,U,_e,Xo,ur,Zo,Jr,V,be,et,R,rt,_r,ot,tt,br,st,at,Br,W,ve,nt,J,it,vr,mt,ct,yr,lt,dt,Hr,B,ye,pt,$e,gt,$r,ft,ht,Gr,H,Q,xr,xe,ut,Ir,_t,Yr,u,Ie,bt,Pr,vt,yt,X,Pe,$t,we,xt,Ve,It,Pt,wt,Z,Ee,Et,De,Dt,Re,Tt,kt,Mt,q,Te,zt,ke,Lt,We,jt,Ct,qt,ee,Ot,re,Me,Nt,C,At,wr,St,Ft,Er,Ut,Vt,Dr,Rt,Wt,Jt,O,ze,Bt,Le,Ht,Tr,Gt,Yt,Kt,oe,Qt,N,je,Xt,Ce,Zt,kr,es,rs,os,te,ts,se,qe,ss,G,as,Mr,ns,is,Je,ms,cs,ls,ae,Oe,ds,zr,ps,gs,ne,Ne,fs,Lr,hs,us,ie,Ae,_s,jr,bs,Kr;return d=new vs({}),de=new vs({}),pe=new w({props:{name:"transformers.image_transforms.center_crop",anchor:"transformers.image_transforms.center_crop",parameters:[{name:"image",val:": ndarray"},{name:"size",val:": typing.Tuple[int, int]"},{name:"data_format",val:": typing.Union[str, transformers.image_utils.ChannelDimension, NoneType] = None"},{name:"return_numpy",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.image_transforms.center_crop.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to crop.`,name:"image"},{anchor:"transformers.image_transforms.center_crop.size",description:`<strong>size</strong> (<code>Tuple[int, int]</code>) &#x2014;
The target size for the cropped image.`,name:"size"},{anchor:"transformers.image_transforms.center_crop.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.
If unset, will use the inferred format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.image_transforms.center_crop.return_numpy",description:`<strong>return_numpy</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the cropped image as a numpy array. Used for backwards compatibility with the
previous ImageFeatureExtractionMixin method.<ul>
<li>Unset: will return the same type as the input image.</li>
<li><code>True</code>: will return a numpy array.</li>
<li><code>False</code>: will return a <code>PIL.Image.Image</code> object.</li>
</ul>`,name:"return_numpy"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_transforms.py#L334",returnDescription:`
<p>The cropped image.</p>
`,returnType:`
<p><code>np.ndarray</code></p>
`}}),ge=new w({props:{name:"transformers.image_transforms.center_to_corners_format",anchor:"transformers.image_transforms.center_to_corners_format",parameters:[{name:"bboxes_center",val:": TensorType"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_transforms.py#L460"}}),fe=new w({props:{name:"transformers.image_transforms.corners_to_center_format",anchor:"transformers.image_transforms.corners_to_center_format",parameters:[{name:"bboxes_corners",val:": TensorType"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_transforms.py#L520"}}),he=new w({props:{name:"transformers.image_transforms.id_to_rgb",anchor:"transformers.image_transforms.id_to_rgb",parameters:[{name:"id_map",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_transforms.py#L554"}}),ue=new w({props:{name:"transformers.image_transforms.normalize",anchor:"transformers.image_transforms.normalize",parameters:[{name:"image",val:": ndarray"},{name:"mean",val:": typing.Union[float, typing.Iterable[float]]"},{name:"std",val:": typing.Union[float, typing.Iterable[float]]"},{name:"data_format",val:": typing.Optional[transformers.image_utils.ChannelDimension] = None"}],parametersDescription:[{anchor:"transformers.image_transforms.normalize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to normalize.`,name:"image"},{anchor:"transformers.image_transforms.normalize.mean",description:`<strong>mean</strong> (<code>float</code> or <code>Iterable[float]</code>) &#x2014;
The mean to use for normalization.`,name:"mean"},{anchor:"transformers.image_transforms.normalize.std",description:`<strong>std</strong> (<code>float</code> or <code>Iterable[float]</code>) &#x2014;
The standard deviation to use for normalization.`,name:"std"},{anchor:"transformers.image_transforms.normalize.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the output image. If <code>None</code>, will use the inferred format from the input.`,name:"data_format"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_transforms.py#L273"}}),_e=new w({props:{name:"transformers.image_transforms.rgb_to_id",anchor:"transformers.image_transforms.rgb_to_id",parameters:[{name:"color",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_transforms.py#L543"}}),be=new w({props:{name:"transformers.rescale",anchor:"transformers.rescale",parameters:[{name:"image",val:": ndarray"},{name:"scale",val:": float"},{name:"data_format",val:": typing.Optional[transformers.image_utils.ChannelDimension] = None"},{name:"dtype",val:" = <class 'numpy.float32'>"}],parametersDescription:[{anchor:"transformers.rescale.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to rescale.`,name:"image"},{anchor:"transformers.rescale.scale",description:`<strong>scale</strong> (<code>float</code>) &#x2014;
The scale to use for rescaling the image.`,name:"scale"},{anchor:"transformers.rescale.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the image. If not provided, it will be the same as the input image.`,name:"data_format"},{anchor:"transformers.rescale.dtype",description:`<strong>dtype</strong> (<code>np.dtype</code>, <em>optional</em>, defaults to <code>np.float32</code>) &#x2014;
The dtype of the output image. Defaults to <code>np.float32</code>. Used for backwards compatibility with feature
extractors.`,name:"dtype"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_transforms.py#L81",returnDescription:`
<p>The rescaled image.</p>
`,returnType:`
<p><code>np.ndarray</code></p>
`}}),ve=new w({props:{name:"transformers.resize",anchor:"transformers.resize",parameters:[{name:"image",val:""},{name:"size",val:": typing.Tuple[int, int]"},{name:"resample",val:" = <Resampling.BILINEAR: 2>"},{name:"data_format",val:": typing.Optional[transformers.image_utils.ChannelDimension] = None"},{name:"return_numpy",val:": bool = True"}],parametersDescription:[{anchor:"transformers.resize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.resize.size",description:`<strong>size</strong> (<code>Tuple[int, int]</code>) &#x2014;
The size to use for resizing the image.`,name:"size"},{anchor:"transformers.resize.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PILImageResampling.BILINEAR</code>) &#x2014;
The filter to user for resampling.`,name:"resample"},{anchor:"transformers.resize.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the output image. If <code>None</code>, will use the inferred format from the input.`,name:"data_format"},{anchor:"transformers.resize.return_numpy",description:`<strong>return_numpy</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return the resized image as a numpy array. If False a <code>PIL.Image.Image</code> object is
returned.`,name:"return_numpy"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_transforms.py#L221",returnDescription:`
<p>The resized image.</p>
`,returnType:`
<p><code>np.ndarray</code></p>
`}}),ye=new w({props:{name:"transformers.to_pil_image",anchor:"transformers.to_pil_image",parameters:[{name:"image",val:": typing.Union[numpy.ndarray, PIL.Image.Image, ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor'), ForwardRef('jnp.ndarray')]"},{name:"do_rescale",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.to_pil_image.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>numpy.ndarray</code> or <code>torch.Tensor</code> or <code>tf.Tensor</code>) &#x2014;
The image to convert to the <code>PIL.Image</code> format.`,name:"image"},{anchor:"transformers.to_pil_image.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values integers between 0 and 255). Will default
to <code>True</code> if the image type is a floating type, <code>False</code> otherwise.`,name:"do_rescale"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_transforms.py#L111",returnDescription:`
<p>The converted image.</p>
`,returnType:`
<p><code>PIL.Image.Image</code></p>
`}}),xe=new vs({}),Ie=new w({props:{name:"class transformers.ImageProcessingMixin",anchor:"transformers.ImageProcessingMixin",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_processing_utils.py#L58"}}),Pe=new w({props:{name:"from_dict",anchor:"transformers.ImageProcessingMixin.from_dict",parameters:[{name:"image_processor_dict",val:": typing.Dict[str, typing.Any]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.from_dict.image_processor_dict",description:`<strong>image_processor_dict</strong> (<code>Dict[str, Any]</code>) &#x2014;
Dictionary that will be used to instantiate the image processor object. Such a dictionary can be
retrieved from a pretrained checkpoint by leveraging the
<a href="/docs/transformers/main/en/internal/image_processing_utils#transformers.ImageProcessingMixin.to_dict">to_dict()</a> method.`,name:"image_processor_dict"},{anchor:"transformers.ImageProcessingMixin.from_dict.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>) &#x2014;
Additional parameters from which to initialize the image processor object.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_processing_utils.py#L302",returnDescription:`
<p>The image processor object instantiated from those
parameters.</p>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/internal/image_processing_utils#transformers.ImageProcessingMixin"
>ImageProcessingMixin</a></p>
`}}),Ee=new w({props:{name:"from_json_file",anchor:"transformers.ImageProcessingMixin.from_json_file",parameters:[{name:"json_file",val:": typing.Union[str, os.PathLike]"}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.from_json_file.json_file",description:`<strong>json_file</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Path to the JSON file containing the parameters.`,name:"json_file"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_processing_utils.py#L350",returnDescription:`
<p>The image_processor object
instantiated from that JSON file.</p>
`,returnType:`
<p>A image processor of type <a
  href="/docs/transformers/main/en/internal/image_processing_utils#transformers.ImageProcessingMixin"
>ImageProcessingMixin</a></p>
`}}),Te=new w({props:{name:"from_pretrained",anchor:"transformers.ImageProcessingMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained image_processor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a image processor file saved using the
<a href="/docs/transformers/main/en/internal/image_processing_utils#transformers.ImageProcessingMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved image processor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model image processor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the image processor files and override the cached versions if
they exist.`,name:"force_download"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, or not specified, will use
the token generated when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_processing_utils.py#L82",returnDescription:`
<p>A image processor of type <a
  href="/docs/transformers/main/en/internal/image_processing_utils#transformers.ImageProcessingMixin"
>ImageProcessingMixin</a>.</p>
`}}),ee=new ia({props:{anchor:"transformers.ImageProcessingMixin.from_pretrained.example",$$slots:{default:[ua]},$$scope:{ctx:Y}}}),Me=new w({props:{name:"get_image_processor_dict",anchor:"transformers.ImageProcessingMixin.get_image_processor_dict",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.get_image_processor_dict.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
The identifier of the pre-trained checkpoint from which we want the dictionary of parameters.`,name:"pretrained_model_name_or_path"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_processing_utils.py#L209",returnDescription:`
<p>The dictionary(ies) that will be used to instantiate the image processor object.</p>
`,returnType:`
<p><code>Tuple[Dict, Dict]</code></p>
`}}),ze=new w({props:{name:"push_to_hub",anchor:"transformers.ImageProcessingMixin.push_to_hub",parameters:[{name:"repo_id",val:": str"},{name:"use_temp_dir",val:": typing.Optional[bool] = None"},{name:"commit_message",val:": typing.Optional[str] = None"},{name:"private",val:": typing.Optional[bool] = None"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"max_shard_size",val:": typing.Union[int, str, NoneType] = '10GB'"},{name:"create_pr",val:": bool = False"},{name:"**deprecated_kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.push_to_hub.repo_id",description:`<strong>repo_id</strong> (<code>str</code>) &#x2014;
The name of the repository you want to push your image processor to. It should contain your organization name
when pushing to a given organization.`,name:"repo_id"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.use_temp_dir",description:`<strong>use_temp_dir</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.
Will default to <code>True</code> if there is no directory named like <code>repo_id</code>, <code>False</code> otherwise.`,name:"use_temp_dir"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.commit_message",description:`<strong>commit_message</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Message to commit while pushing. Will default to <code>&quot;Upload image processor&quot;</code>.`,name:"commit_message"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.private",description:`<strong>private</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not the repository created should be private.`,name:"private"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.use_auth_token",description:`<strong>use_auth_token</strong> (<code>bool</code> or <code>str</code>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>). Will default to <code>True</code> if <code>repo_url</code>
is not specified.`,name:"use_auth_token"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.max_shard_size",description:`<strong>max_shard_size</strong> (<code>int</code> or <code>str</code>, <em>optional</em>, defaults to <code>&quot;10GB&quot;</code>) &#x2014;
Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard
will then be each of size lower than this size. If expressed as a string, needs to be digits followed
by a unit (like <code>&quot;5MB&quot;</code>).`,name:"max_shard_size"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.create_pr",description:`<strong>create_pr</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to create a PR with the uploaded files or directly commit.`,name:"create_pr"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/utils/hub.py#L712"}}),oe=new ia({props:{anchor:"transformers.ImageProcessingMixin.push_to_hub.example",$$slots:{default:[_a]},$$scope:{ctx:Y}}}),je=new w({props:{name:"register_for_auto_class",anchor:"transformers.ImageProcessingMixin.register_for_auto_class",parameters:[{name:"auto_class",val:" = 'AutoImageProcessor'"}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.register_for_auto_class.auto_class",description:`<strong>auto_class</strong> (<code>str</code> or <code>type</code>, <em>optional</em>, defaults to <code>&quot;AutoImageProcessor &quot;</code>) &#x2014;
The auto class to register this new image processor with.`,name:"auto_class"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_processing_utils.py#L404"}}),te=new ha({props:{warning:!0,$$slots:{default:[ba]},$$scope:{ctx:Y}}}),qe=new w({props:{name:"save_pretrained",anchor:"transformers.ImageProcessingMixin.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike]"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the image processor JSON file will be saved (will be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.ImageProcessingMixin.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).
kwargs &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/main/en/internal/image_processing_utils#transformers.ImageProcessingMixin.push_to_hub">push_to_hub()</a> method.`,name:"push_to_hub"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_processing_utils.py#L165"}}),Oe=new w({props:{name:"to_dict",anchor:"transformers.ImageProcessingMixin.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_processing_utils.py#L338",returnDescription:`
<p>Dictionary of all the attributes that make up this image processor instance.</p>
`,returnType:`
<p><code>Dict[str, Any]</code></p>
`}}),Ne=new w({props:{name:"to_json_file",anchor:"transformers.ImageProcessingMixin.to_json_file",parameters:[{name:"json_file_path",val:": typing.Union[str, os.PathLike]"}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.to_json_file.json_file_path",description:`<strong>json_file_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Path to the JSON file in which this image_processor instance&#x2019;s parameters will be saved.`,name:"json_file_path"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_processing_utils.py#L390"}}),Ae=new w({props:{name:"to_json_string",anchor:"transformers.ImageProcessingMixin.to_json_string",parameters:[],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_processing_utils.py#L369",returnDescription:`
<p>String containing all the attributes that make up this feature_extractor instance in JSON format.</p>
`,returnType:`
<p><code>str</code></p>
`}}),{c(){f=t("meta"),D=m(),I=t("h1"),h=t("a"),T=t("span"),_(d.$$.fragment),E=m(),rr=t("span"),vo=n("Utilities for Image Processors"),Or=m(),Fe=t("p"),yo=n(`This page lists all the utility functions used by the image processors, mainly the functional
transformations used to process the images.`),Nr=m(),Ue=t("p"),$o=n("Most of those are only useful if you are studying the code of the image processors in the library."),Ar=m(),A=t("h2"),K=t("a"),or=t("span"),_(de.$$.fragment),xo=m(),tr=t("span"),Io=n("Image Transformations"),Sr=m(),S=t("div"),_(pe.$$.fragment),Po=m(),k=t("p"),wo=n("Crops the "),sr=t("code"),Eo=n("image"),Do=n(" to the specified "),ar=t("code"),To=n("size"),ko=n(` using a center crop. Note that if the image is too small to be cropped to
the size given, it will be padded (so the returned result will always be of size `),nr=t("code"),Mo=n("size"),zo=n(")."),Fr=m(),M=t("div"),_(ge.$$.fragment),Lo=m(),ir=t("p"),jo=n("Converts bounding boxes from center format to corners format."),Co=m(),mr=t("p"),qo=n(`center format: contains the coordinate for the center of the box and its width, height dimensions
(center_x, center_y, width, height)
corners format: contains the coodinates for the top-left and bottom-right corners of the box
(top_left_x, top_left_y, bottom_right_x, bottom_right_y)`),Ur=m(),z=t("div"),_(fe.$$.fragment),Oo=m(),cr=t("p"),No=n("Converts bounding boxes from corners format to center format."),Ao=m(),lr=t("p"),So=n(`corners format: contains the coodinates for the top-left and bottom-right corners of the box
(top_left_x, top_left_y, bottom_right_x, bottom_right_y)
center format: contains the coordinate for the center of the box and its the width, height dimensions
(center_x, center_y, width, height)`),Vr=m(),F=t("div"),_(he.$$.fragment),Fo=m(),dr=t("p"),Uo=n("Converts unique ID to RGB color."),Rr=m(),L=t("div"),_(ue.$$.fragment),Vo=m(),j=t("p"),Ro=n("Normalizes "),pr=t("code"),Wo=n("image"),Jo=n(" using the mean and standard deviation specified by "),gr=t("code"),Bo=n("mean"),Ho=n(" and "),fr=t("code"),Go=n("std"),Yo=n("."),Ko=m(),hr=t("p"),Qo=n("image = (image - mean) / std"),Wr=m(),U=t("div"),_(_e.$$.fragment),Xo=m(),ur=t("p"),Zo=n("Converts RGB color to unique ID."),Jr=m(),V=t("div"),_(be.$$.fragment),et=m(),R=t("p"),rt=n("Rescales "),_r=t("code"),ot=n("image"),tt=n(" by "),br=t("code"),st=n("scale"),at=n("."),Br=m(),W=t("div"),_(ve.$$.fragment),nt=m(),J=t("p"),it=n("Resizes "),vr=t("code"),mt=n("image"),ct=n(" to (h, w) specified by "),yr=t("code"),lt=n("size"),dt=n(" using the PIL library."),Hr=m(),B=t("div"),_(ye.$$.fragment),pt=m(),$e=t("p"),gt=n("Converts "),$r=t("code"),ft=n("image"),ht=n(` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),Gr=m(),H=t("h2"),Q=t("a"),xr=t("span"),_(xe.$$.fragment),ut=m(),Ir=t("span"),_t=n("ImageProcessingMixin"),Yr=m(),u=t("div"),_(Ie.$$.fragment),bt=m(),Pr=t("p"),vt=n(`This is an image processor mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),yt=m(),X=t("div"),_(Pe.$$.fragment),$t=m(),we=t("p"),xt=n("Instantiates a type of "),Ve=t("a"),It=n("ImageProcessingMixin"),Pt=n(" from a Python dictionary of parameters."),wt=m(),Z=t("div"),_(Ee.$$.fragment),Et=m(),De=t("p"),Dt=n("Instantiates a image processor of type "),Re=t("a"),Tt=n("ImageProcessingMixin"),kt=n(` from the path to a JSON
file of parameters.`),Mt=m(),q=t("div"),_(Te.$$.fragment),zt=m(),ke=t("p"),Lt=n("Instantiate a type of "),We=t("a"),jt=n("ImageProcessingMixin"),Ct=n(" from an image processor."),qt=m(),_(ee.$$.fragment),Ot=m(),re=t("div"),_(Me.$$.fragment),Nt=m(),C=t("p"),At=n("From a "),wr=t("code"),St=n("pretrained_model_name_or_path"),Ft=n(`, resolve to a dictionary of parameters, to be used for instantiating a
image processor of type `),Er=t("code"),Ut=n("~image_processor_utils.ImageProcessingMixin"),Vt=n(" using "),Dr=t("code"),Rt=n("from_dict"),Wt=n("."),Jt=m(),O=t("div"),_(ze.$$.fragment),Bt=m(),Le=t("p"),Ht=n(`Upload the image processor file to the \u{1F917} Model Hub while synchronizing a local clone of the repo in
`),Tr=t("code"),Gt=n("repo_path_or_name"),Yt=n("."),Kt=m(),_(oe.$$.fragment),Qt=m(),N=t("div"),_(je.$$.fragment),Xt=m(),Ce=t("p"),Zt=n(`Register this class with a given auto class. This should only be used for custom image processors as the ones
in the library are already mapped with `),kr=t("code"),es=n("AutoImageProcessor "),rs=n("."),os=m(),_(te.$$.fragment),ts=m(),se=t("div"),_(qe.$$.fragment),ss=m(),G=t("p"),as=n("Save an image processor object to the directory "),Mr=t("code"),ns=n("save_directory"),is=n(`, so that it can be re-loaded using the
`),Je=t("a"),ms=n("from_pretrained()"),cs=n(" class method."),ls=m(),ae=t("div"),_(Oe.$$.fragment),ds=m(),zr=t("p"),ps=n("Serializes this instance to a Python dictionary."),gs=m(),ne=t("div"),_(Ne.$$.fragment),fs=m(),Lr=t("p"),hs=n("Save this instance to a JSON file."),us=m(),ie=t("div"),_(Ae.$$.fragment),_s=m(),jr=t("p"),bs=n("Serializes this instance to a JSON string."),this.h()},l(r){const l=ga('[data-svelte="svelte-1phssyn"]',document.head);f=s(l,"META",{name:!0,content:!0}),l.forEach(o),D=c(r),I=s(r,"H1",{class:!0});var Se=a(I);h=s(Se,"A",{id:!0,class:!0,href:!0});var Cr=a(h);T=s(Cr,"SPAN",{});var qr=a(T);b(d.$$.fragment,qr),qr.forEach(o),Cr.forEach(o),E=c(Se),rr=s(Se,"SPAN",{});var ys=a(rr);vo=i(ys,"Utilities for Image Processors"),ys.forEach(o),Se.forEach(o),Or=c(r),Fe=s(r,"P",{});var $s=a(Fe);yo=i($s,`This page lists all the utility functions used by the image processors, mainly the functional
transformations used to process the images.`),$s.forEach(o),Nr=c(r),Ue=s(r,"P",{});var xs=a(Ue);$o=i(xs,"Most of those are only useful if you are studying the code of the image processors in the library."),xs.forEach(o),Ar=c(r),A=s(r,"H2",{class:!0});var Qr=a(A);K=s(Qr,"A",{id:!0,class:!0,href:!0});var Is=a(K);or=s(Is,"SPAN",{});var Ps=a(or);b(de.$$.fragment,Ps),Ps.forEach(o),Is.forEach(o),xo=c(Qr),tr=s(Qr,"SPAN",{});var ws=a(tr);Io=i(ws,"Image Transformations"),ws.forEach(o),Qr.forEach(o),Sr=c(r),S=s(r,"DIV",{class:!0});var Xr=a(S);b(pe.$$.fragment,Xr),Po=c(Xr),k=s(Xr,"P",{});var me=a(k);wo=i(me,"Crops the "),sr=s(me,"CODE",{});var Es=a(sr);Eo=i(Es,"image"),Es.forEach(o),Do=i(me," to the specified "),ar=s(me,"CODE",{});var Ds=a(ar);To=i(Ds,"size"),Ds.forEach(o),ko=i(me,` using a center crop. Note that if the image is too small to be cropped to
the size given, it will be padded (so the returned result will always be of size `),nr=s(me,"CODE",{});var Ts=a(nr);Mo=i(Ts,"size"),Ts.forEach(o),zo=i(me,")."),me.forEach(o),Xr.forEach(o),Fr=c(r),M=s(r,"DIV",{class:!0});var Be=a(M);b(ge.$$.fragment,Be),Lo=c(Be),ir=s(Be,"P",{});var ks=a(ir);jo=i(ks,"Converts bounding boxes from center format to corners format."),ks.forEach(o),Co=c(Be),mr=s(Be,"P",{});var Ms=a(mr);qo=i(Ms,`center format: contains the coordinate for the center of the box and its width, height dimensions
(center_x, center_y, width, height)
corners format: contains the coodinates for the top-left and bottom-right corners of the box
(top_left_x, top_left_y, bottom_right_x, bottom_right_y)`),Ms.forEach(o),Be.forEach(o),Ur=c(r),z=s(r,"DIV",{class:!0});var He=a(z);b(fe.$$.fragment,He),Oo=c(He),cr=s(He,"P",{});var zs=a(cr);No=i(zs,"Converts bounding boxes from corners format to center format."),zs.forEach(o),Ao=c(He),lr=s(He,"P",{});var Ls=a(lr);So=i(Ls,`corners format: contains the coodinates for the top-left and bottom-right corners of the box
(top_left_x, top_left_y, bottom_right_x, bottom_right_y)
center format: contains the coordinate for the center of the box and its the width, height dimensions
(center_x, center_y, width, height)`),Ls.forEach(o),He.forEach(o),Vr=c(r),F=s(r,"DIV",{class:!0});var Zr=a(F);b(he.$$.fragment,Zr),Fo=c(Zr),dr=s(Zr,"P",{});var js=a(dr);Uo=i(js,"Converts unique ID to RGB color."),js.forEach(o),Zr.forEach(o),Rr=c(r),L=s(r,"DIV",{class:!0});var Ge=a(L);b(ue.$$.fragment,Ge),Vo=c(Ge),j=s(Ge,"P",{});var ce=a(j);Ro=i(ce,"Normalizes "),pr=s(ce,"CODE",{});var Cs=a(pr);Wo=i(Cs,"image"),Cs.forEach(o),Jo=i(ce," using the mean and standard deviation specified by "),gr=s(ce,"CODE",{});var qs=a(gr);Bo=i(qs,"mean"),qs.forEach(o),Ho=i(ce," and "),fr=s(ce,"CODE",{});var Os=a(fr);Go=i(Os,"std"),Os.forEach(o),Yo=i(ce,"."),ce.forEach(o),Ko=c(Ge),hr=s(Ge,"P",{});var Ns=a(hr);Qo=i(Ns,"image = (image - mean) / std"),Ns.forEach(o),Ge.forEach(o),Wr=c(r),U=s(r,"DIV",{class:!0});var eo=a(U);b(_e.$$.fragment,eo),Xo=c(eo),ur=s(eo,"P",{});var As=a(ur);Zo=i(As,"Converts RGB color to unique ID."),As.forEach(o),eo.forEach(o),Jr=c(r),V=s(r,"DIV",{class:!0});var ro=a(V);b(be.$$.fragment,ro),et=c(ro),R=s(ro,"P",{});var Ye=a(R);rt=i(Ye,"Rescales "),_r=s(Ye,"CODE",{});var Ss=a(_r);ot=i(Ss,"image"),Ss.forEach(o),tt=i(Ye," by "),br=s(Ye,"CODE",{});var Fs=a(br);st=i(Fs,"scale"),Fs.forEach(o),at=i(Ye,"."),Ye.forEach(o),ro.forEach(o),Br=c(r),W=s(r,"DIV",{class:!0});var oo=a(W);b(ve.$$.fragment,oo),nt=c(oo),J=s(oo,"P",{});var Ke=a(J);it=i(Ke,"Resizes "),vr=s(Ke,"CODE",{});var Us=a(vr);mt=i(Us,"image"),Us.forEach(o),ct=i(Ke," to (h, w) specified by "),yr=s(Ke,"CODE",{});var Vs=a(yr);lt=i(Vs,"size"),Vs.forEach(o),dt=i(Ke," using the PIL library."),Ke.forEach(o),oo.forEach(o),Hr=c(r),B=s(r,"DIV",{class:!0});var to=a(B);b(ye.$$.fragment,to),pt=c(to),$e=s(to,"P",{});var so=a($e);gt=i(so,"Converts "),$r=s(so,"CODE",{});var Rs=a($r);ft=i(Rs,"image"),Rs.forEach(o),ht=i(so,` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),so.forEach(o),to.forEach(o),Gr=c(r),H=s(r,"H2",{class:!0});var ao=a(H);Q=s(ao,"A",{id:!0,class:!0,href:!0});var Ws=a(Q);xr=s(Ws,"SPAN",{});var Js=a(xr);b(xe.$$.fragment,Js),Js.forEach(o),Ws.forEach(o),ut=c(ao),Ir=s(ao,"SPAN",{});var Bs=a(Ir);_t=i(Bs,"ImageProcessingMixin"),Bs.forEach(o),ao.forEach(o),Yr=c(r),u=s(r,"DIV",{class:!0});var P=a(u);b(Ie.$$.fragment,P),bt=c(P),Pr=s(P,"P",{});var Hs=a(Pr);vt=i(Hs,`This is an image processor mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),Hs.forEach(o),yt=c(P),X=s(P,"DIV",{class:!0});var no=a(X);b(Pe.$$.fragment,no),$t=c(no),we=s(no,"P",{});var io=a(we);xt=i(io,"Instantiates a type of "),Ve=s(io,"A",{href:!0});var Gs=a(Ve);It=i(Gs,"ImageProcessingMixin"),Gs.forEach(o),Pt=i(io," from a Python dictionary of parameters."),io.forEach(o),no.forEach(o),wt=c(P),Z=s(P,"DIV",{class:!0});var mo=a(Z);b(Ee.$$.fragment,mo),Et=c(mo),De=s(mo,"P",{});var co=a(De);Dt=i(co,"Instantiates a image processor of type "),Re=s(co,"A",{href:!0});var Ys=a(Re);Tt=i(Ys,"ImageProcessingMixin"),Ys.forEach(o),kt=i(co,` from the path to a JSON
file of parameters.`),co.forEach(o),mo.forEach(o),Mt=c(P),q=s(P,"DIV",{class:!0});var Qe=a(q);b(Te.$$.fragment,Qe),zt=c(Qe),ke=s(Qe,"P",{});var lo=a(ke);Lt=i(lo,"Instantiate a type of "),We=s(lo,"A",{href:!0});var Ks=a(We);jt=i(Ks,"ImageProcessingMixin"),Ks.forEach(o),Ct=i(lo," from an image processor."),lo.forEach(o),qt=c(Qe),b(ee.$$.fragment,Qe),Qe.forEach(o),Ot=c(P),re=s(P,"DIV",{class:!0});var po=a(re);b(Me.$$.fragment,po),Nt=c(po),C=s(po,"P",{});var le=a(C);At=i(le,"From a "),wr=s(le,"CODE",{});var Qs=a(wr);St=i(Qs,"pretrained_model_name_or_path"),Qs.forEach(o),Ft=i(le,`, resolve to a dictionary of parameters, to be used for instantiating a
image processor of type `),Er=s(le,"CODE",{});var Xs=a(Er);Ut=i(Xs,"~image_processor_utils.ImageProcessingMixin"),Xs.forEach(o),Vt=i(le," using "),Dr=s(le,"CODE",{});var Zs=a(Dr);Rt=i(Zs,"from_dict"),Zs.forEach(o),Wt=i(le,"."),le.forEach(o),po.forEach(o),Jt=c(P),O=s(P,"DIV",{class:!0});var Xe=a(O);b(ze.$$.fragment,Xe),Bt=c(Xe),Le=s(Xe,"P",{});var go=a(Le);Ht=i(go,`Upload the image processor file to the \u{1F917} Model Hub while synchronizing a local clone of the repo in
`),Tr=s(go,"CODE",{});var ea=a(Tr);Gt=i(ea,"repo_path_or_name"),ea.forEach(o),Yt=i(go,"."),go.forEach(o),Kt=c(Xe),b(oe.$$.fragment,Xe),Xe.forEach(o),Qt=c(P),N=s(P,"DIV",{class:!0});var Ze=a(N);b(je.$$.fragment,Ze),Xt=c(Ze),Ce=s(Ze,"P",{});var fo=a(Ce);Zt=i(fo,`Register this class with a given auto class. This should only be used for custom image processors as the ones
in the library are already mapped with `),kr=s(fo,"CODE",{});var ra=a(kr);es=i(ra,"AutoImageProcessor "),ra.forEach(o),rs=i(fo,"."),fo.forEach(o),os=c(Ze),b(te.$$.fragment,Ze),Ze.forEach(o),ts=c(P),se=s(P,"DIV",{class:!0});var ho=a(se);b(qe.$$.fragment,ho),ss=c(ho),G=s(ho,"P",{});var er=a(G);as=i(er,"Save an image processor object to the directory "),Mr=s(er,"CODE",{});var oa=a(Mr);ns=i(oa,"save_directory"),oa.forEach(o),is=i(er,`, so that it can be re-loaded using the
`),Je=s(er,"A",{href:!0});var ta=a(Je);ms=i(ta,"from_pretrained()"),ta.forEach(o),cs=i(er," class method."),er.forEach(o),ho.forEach(o),ls=c(P),ae=s(P,"DIV",{class:!0});var uo=a(ae);b(Oe.$$.fragment,uo),ds=c(uo),zr=s(uo,"P",{});var sa=a(zr);ps=i(sa,"Serializes this instance to a Python dictionary."),sa.forEach(o),uo.forEach(o),gs=c(P),ne=s(P,"DIV",{class:!0});var _o=a(ne);b(Ne.$$.fragment,_o),fs=c(_o),Lr=s(_o,"P",{});var aa=a(Lr);hs=i(aa,"Save this instance to a JSON file."),aa.forEach(o),_o.forEach(o),us=c(P),ie=s(P,"DIV",{class:!0});var bo=a(ie);b(Ae.$$.fragment,bo),_s=c(bo),jr=s(bo,"P",{});var na=a(jr);bs=i(na,"Serializes this instance to a JSON string."),na.forEach(o),bo.forEach(o),P.forEach(o),this.h()},h(){p(f,"name","hf:doc:metadata"),p(f,"content",JSON.stringify(ya)),p(h,"id","utilities-for-image-processors"),p(h,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(h,"href","#utilities-for-image-processors"),p(I,"class","relative group"),p(K,"id","transformers.image_transforms.center_crop"),p(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(K,"href","#transformers.image_transforms.center_crop"),p(A,"class","relative group"),p(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Q,"id","transformers.ImageProcessingMixin"),p(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Q,"href","#transformers.ImageProcessingMixin"),p(H,"class","relative group"),p(Ve,"href","/docs/transformers/main/en/internal/image_processing_utils#transformers.ImageProcessingMixin"),p(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Re,"href","/docs/transformers/main/en/internal/image_processing_utils#transformers.ImageProcessingMixin"),p(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(We,"href","/docs/transformers/main/en/internal/image_processing_utils#transformers.ImageProcessingMixin"),p(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Je,"href","/docs/transformers/main/en/internal/image_processing_utils#transformers.ImageProcessingMixin.from_pretrained"),p(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(u,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(r,l){e(document.head,f),g(r,D,l),g(r,I,l),e(I,h),e(h,T),v(d,T,null),e(I,E),e(I,rr),e(rr,vo),g(r,Or,l),g(r,Fe,l),e(Fe,yo),g(r,Nr,l),g(r,Ue,l),e(Ue,$o),g(r,Ar,l),g(r,A,l),e(A,K),e(K,or),v(de,or,null),e(A,xo),e(A,tr),e(tr,Io),g(r,Sr,l),g(r,S,l),v(pe,S,null),e(S,Po),e(S,k),e(k,wo),e(k,sr),e(sr,Eo),e(k,Do),e(k,ar),e(ar,To),e(k,ko),e(k,nr),e(nr,Mo),e(k,zo),g(r,Fr,l),g(r,M,l),v(ge,M,null),e(M,Lo),e(M,ir),e(ir,jo),e(M,Co),e(M,mr),e(mr,qo),g(r,Ur,l),g(r,z,l),v(fe,z,null),e(z,Oo),e(z,cr),e(cr,No),e(z,Ao),e(z,lr),e(lr,So),g(r,Vr,l),g(r,F,l),v(he,F,null),e(F,Fo),e(F,dr),e(dr,Uo),g(r,Rr,l),g(r,L,l),v(ue,L,null),e(L,Vo),e(L,j),e(j,Ro),e(j,pr),e(pr,Wo),e(j,Jo),e(j,gr),e(gr,Bo),e(j,Ho),e(j,fr),e(fr,Go),e(j,Yo),e(L,Ko),e(L,hr),e(hr,Qo),g(r,Wr,l),g(r,U,l),v(_e,U,null),e(U,Xo),e(U,ur),e(ur,Zo),g(r,Jr,l),g(r,V,l),v(be,V,null),e(V,et),e(V,R),e(R,rt),e(R,_r),e(_r,ot),e(R,tt),e(R,br),e(br,st),e(R,at),g(r,Br,l),g(r,W,l),v(ve,W,null),e(W,nt),e(W,J),e(J,it),e(J,vr),e(vr,mt),e(J,ct),e(J,yr),e(yr,lt),e(J,dt),g(r,Hr,l),g(r,B,l),v(ye,B,null),e(B,pt),e(B,$e),e($e,gt),e($e,$r),e($r,ft),e($e,ht),g(r,Gr,l),g(r,H,l),e(H,Q),e(Q,xr),v(xe,xr,null),e(H,ut),e(H,Ir),e(Ir,_t),g(r,Yr,l),g(r,u,l),v(Ie,u,null),e(u,bt),e(u,Pr),e(Pr,vt),e(u,yt),e(u,X),v(Pe,X,null),e(X,$t),e(X,we),e(we,xt),e(we,Ve),e(Ve,It),e(we,Pt),e(u,wt),e(u,Z),v(Ee,Z,null),e(Z,Et),e(Z,De),e(De,Dt),e(De,Re),e(Re,Tt),e(De,kt),e(u,Mt),e(u,q),v(Te,q,null),e(q,zt),e(q,ke),e(ke,Lt),e(ke,We),e(We,jt),e(ke,Ct),e(q,qt),v(ee,q,null),e(u,Ot),e(u,re),v(Me,re,null),e(re,Nt),e(re,C),e(C,At),e(C,wr),e(wr,St),e(C,Ft),e(C,Er),e(Er,Ut),e(C,Vt),e(C,Dr),e(Dr,Rt),e(C,Wt),e(u,Jt),e(u,O),v(ze,O,null),e(O,Bt),e(O,Le),e(Le,Ht),e(Le,Tr),e(Tr,Gt),e(Le,Yt),e(O,Kt),v(oe,O,null),e(u,Qt),e(u,N),v(je,N,null),e(N,Xt),e(N,Ce),e(Ce,Zt),e(Ce,kr),e(kr,es),e(Ce,rs),e(N,os),v(te,N,null),e(u,ts),e(u,se),v(qe,se,null),e(se,ss),e(se,G),e(G,as),e(G,Mr),e(Mr,ns),e(G,is),e(G,Je),e(Je,ms),e(G,cs),e(u,ls),e(u,ae),v(Oe,ae,null),e(ae,ds),e(ae,zr),e(zr,ps),e(u,gs),e(u,ne),v(Ne,ne,null),e(ne,fs),e(ne,Lr),e(Lr,hs),e(u,us),e(u,ie),v(Ae,ie,null),e(ie,_s),e(ie,jr),e(jr,bs),Kr=!0},p(r,[l]){const Se={};l&2&&(Se.$$scope={dirty:l,ctx:r}),ee.$set(Se);const Cr={};l&2&&(Cr.$$scope={dirty:l,ctx:r}),oe.$set(Cr);const qr={};l&2&&(qr.$$scope={dirty:l,ctx:r}),te.$set(qr)},i(r){Kr||(y(d.$$.fragment,r),y(de.$$.fragment,r),y(pe.$$.fragment,r),y(ge.$$.fragment,r),y(fe.$$.fragment,r),y(he.$$.fragment,r),y(ue.$$.fragment,r),y(_e.$$.fragment,r),y(be.$$.fragment,r),y(ve.$$.fragment,r),y(ye.$$.fragment,r),y(xe.$$.fragment,r),y(Ie.$$.fragment,r),y(Pe.$$.fragment,r),y(Ee.$$.fragment,r),y(Te.$$.fragment,r),y(ee.$$.fragment,r),y(Me.$$.fragment,r),y(ze.$$.fragment,r),y(oe.$$.fragment,r),y(je.$$.fragment,r),y(te.$$.fragment,r),y(qe.$$.fragment,r),y(Oe.$$.fragment,r),y(Ne.$$.fragment,r),y(Ae.$$.fragment,r),Kr=!0)},o(r){$(d.$$.fragment,r),$(de.$$.fragment,r),$(pe.$$.fragment,r),$(ge.$$.fragment,r),$(fe.$$.fragment,r),$(he.$$.fragment,r),$(ue.$$.fragment,r),$(_e.$$.fragment,r),$(be.$$.fragment,r),$(ve.$$.fragment,r),$(ye.$$.fragment,r),$(xe.$$.fragment,r),$(Ie.$$.fragment,r),$(Pe.$$.fragment,r),$(Ee.$$.fragment,r),$(Te.$$.fragment,r),$(ee.$$.fragment,r),$(Me.$$.fragment,r),$(ze.$$.fragment,r),$(oe.$$.fragment,r),$(je.$$.fragment,r),$(te.$$.fragment,r),$(qe.$$.fragment,r),$(Oe.$$.fragment,r),$(Ne.$$.fragment,r),$(Ae.$$.fragment,r),Kr=!1},d(r){o(f),r&&o(D),r&&o(I),x(d),r&&o(Or),r&&o(Fe),r&&o(Nr),r&&o(Ue),r&&o(Ar),r&&o(A),x(de),r&&o(Sr),r&&o(S),x(pe),r&&o(Fr),r&&o(M),x(ge),r&&o(Ur),r&&o(z),x(fe),r&&o(Vr),r&&o(F),x(he),r&&o(Rr),r&&o(L),x(ue),r&&o(Wr),r&&o(U),x(_e),r&&o(Jr),r&&o(V),x(be),r&&o(Br),r&&o(W),x(ve),r&&o(Hr),r&&o(B),x(ye),r&&o(Gr),r&&o(H),x(xe),r&&o(Yr),r&&o(u),x(Ie),x(Pe),x(Ee),x(Te),x(ee),x(Me),x(ze),x(oe),x(je),x(te),x(qe),x(Oe),x(Ne),x(Ae)}}}const ya={local:"utilities-for-image-processors",sections:[{local:"transformers.image_transforms.center_crop",title:"Image Transformations"},{local:"transformers.ImageProcessingMixin",title:"ImageProcessingMixin"}],title:"Utilities for Image Processors"};function $a(Y){return fa(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ta extends la{constructor(f){super();da(this,f,$a,va,pa,{})}}export{Ta as default,ya as metadata};
