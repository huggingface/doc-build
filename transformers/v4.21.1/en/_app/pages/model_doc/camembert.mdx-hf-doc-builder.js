import{S as Rg,i as qg,s as Ag,e as o,k as d,w as F,t as s,M as Dg,c as r,d as t,m,a,x as M,h as n,b as c,G as e,g as h,y as P,q as L,o as R,B as q,v as zg}from"../../chunks/vendor-hf-doc-builder.js";import{T as sa}from"../../chunks/Tip-hf-doc-builder.js";import{D as $e}from"../../chunks/Docstring-hf-doc-builder.js";import{I as Me}from"../../chunks/IconCopyLink-hf-doc-builder.js";function xg(xe){let k,re,U,_,A,G,le,D,de,Q,u,W,z,X,me,x,ce,ae,H,w,j,V,g,b,Z,O,se,ee,I,he,ne,v,fe,S,te,J,B,oe,pe,E,ie,C,ue;return{c(){k=o("p"),re=s("TF 2.0 models accepts two formats as inputs:"),U=d(),_=o("ul"),A=o("li"),G=s("having all inputs as keyword arguments (like PyTorch models), or"),le=d(),D=o("li"),de=s("having all inputs as a list, tuple or dict in the first positional arguments."),Q=d(),u=o("p"),W=s("This second option is useful when using "),z=o("code"),X=s("tf.keras.Model.fit"),me=s(` method which currently requires having all the
tensors in the first argument of the model call function: `),x=o("code"),ce=s("model(inputs)"),ae=s("."),H=d(),w=o("p"),j=s(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=d(),g=o("ul"),b=o("li"),Z=s("a single Tensor with "),O=o("code"),se=s("input_ids"),ee=s(" only and nothing else: "),I=o("code"),he=s("model(inputs_ids)"),ne=d(),v=o("li"),fe=s(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=o("code"),te=s("model([input_ids, attention_mask])"),J=s(" or "),B=o("code"),oe=s("model([input_ids, attention_mask, token_type_ids])"),pe=d(),E=o("li"),ie=s(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),C=o("code"),ue=s('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(l){k=r(l,"P",{});var p=a(k);re=n(p,"TF 2.0 models accepts two formats as inputs:"),p.forEach(t),U=m(l),_=r(l,"UL",{});var K=a(_);A=r(K,"LI",{});var be=a(A);G=n(be,"having all inputs as keyword arguments (like PyTorch models), or"),be.forEach(t),le=m(K),D=r(K,"LI",{});var Ee=a(D);de=n(Ee,"having all inputs as a list, tuple or dict in the first positional arguments."),Ee.forEach(t),K.forEach(t),Q=m(l),u=r(l,"P",{});var y=a(u);W=n(y,"This second option is useful when using "),z=r(y,"CODE",{});var Te=a(z);X=n(Te,"tf.keras.Model.fit"),Te.forEach(t),me=n(y,` method which currently requires having all the
tensors in the first argument of the model call function: `),x=r(y,"CODE",{});var ge=a(x);ce=n(ge,"model(inputs)"),ge.forEach(t),ae=n(y,"."),y.forEach(t),H=m(l),w=r(l,"P",{});var ve=a(w);j=n(ve,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ve.forEach(t),V=m(l),g=r(l,"UL",{});var T=a(g);b=r(T,"LI",{});var $=a(b);Z=n($,"a single Tensor with "),O=r($,"CODE",{});var Ce=a(O);se=n(Ce,"input_ids"),Ce.forEach(t),ee=n($," only and nothing else: "),I=r($,"CODE",{});var ke=a(I);he=n(ke,"model(inputs_ids)"),ke.forEach(t),$.forEach(t),ne=m(T),v=r(T,"LI",{});var N=a(v);fe=n(N,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=r(N,"CODE",{});var we=a(S);te=n(we,"model([input_ids, attention_mask])"),we.forEach(t),J=n(N," or "),B=r(N,"CODE",{});var _e=a(B);oe=n(_e,"model([input_ids, attention_mask, token_type_ids])"),_e.forEach(t),N.forEach(t),pe=m(T),E=r(T,"LI",{});var Y=a(E);ie=n(Y,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),C=r(Y,"CODE",{});var ye=a(C);ue=n(ye,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ye.forEach(t),Y.forEach(t),T.forEach(t)},m(l,p){h(l,k,p),e(k,re),h(l,U,p),h(l,_,p),e(_,A),e(A,G),e(_,le),e(_,D),e(D,de),h(l,Q,p),h(l,u,p),e(u,W),e(u,z),e(z,X),e(u,me),e(u,x),e(x,ce),e(u,ae),h(l,H,p),h(l,w,p),e(w,j),h(l,V,p),h(l,g,p),e(g,b),e(b,Z),e(b,O),e(O,se),e(b,ee),e(b,I),e(I,he),e(g,ne),e(g,v),e(v,fe),e(v,S),e(S,te),e(v,J),e(v,B),e(B,oe),e(g,pe),e(g,E),e(E,ie),e(E,C),e(C,ue)},d(l){l&&t(k),l&&t(U),l&&t(_),l&&t(Q),l&&t(u),l&&t(H),l&&t(w),l&&t(V),l&&t(g)}}}function Ig(xe){let k,re,U,_,A,G,le,D,de,Q,u,W,z,X,me,x,ce,ae,H,w,j,V,g,b,Z,O,se,ee,I,he,ne,v,fe,S,te,J,B,oe,pe,E,ie,C,ue;return{c(){k=o("p"),re=s("TF 2.0 models accepts two formats as inputs:"),U=d(),_=o("ul"),A=o("li"),G=s("having all inputs as keyword arguments (like PyTorch models), or"),le=d(),D=o("li"),de=s("having all inputs as a list, tuple or dict in the first positional arguments."),Q=d(),u=o("p"),W=s("This second option is useful when using "),z=o("code"),X=s("tf.keras.Model.fit"),me=s(` method which currently requires having all the
tensors in the first argument of the model call function: `),x=o("code"),ce=s("model(inputs)"),ae=s("."),H=d(),w=o("p"),j=s(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=d(),g=o("ul"),b=o("li"),Z=s("a single Tensor with "),O=o("code"),se=s("input_ids"),ee=s(" only and nothing else: "),I=o("code"),he=s("model(inputs_ids)"),ne=d(),v=o("li"),fe=s(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=o("code"),te=s("model([input_ids, attention_mask])"),J=s(" or "),B=o("code"),oe=s("model([input_ids, attention_mask, token_type_ids])"),pe=d(),E=o("li"),ie=s(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),C=o("code"),ue=s('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(l){k=r(l,"P",{});var p=a(k);re=n(p,"TF 2.0 models accepts two formats as inputs:"),p.forEach(t),U=m(l),_=r(l,"UL",{});var K=a(_);A=r(K,"LI",{});var be=a(A);G=n(be,"having all inputs as keyword arguments (like PyTorch models), or"),be.forEach(t),le=m(K),D=r(K,"LI",{});var Ee=a(D);de=n(Ee,"having all inputs as a list, tuple or dict in the first positional arguments."),Ee.forEach(t),K.forEach(t),Q=m(l),u=r(l,"P",{});var y=a(u);W=n(y,"This second option is useful when using "),z=r(y,"CODE",{});var Te=a(z);X=n(Te,"tf.keras.Model.fit"),Te.forEach(t),me=n(y,` method which currently requires having all the
tensors in the first argument of the model call function: `),x=r(y,"CODE",{});var ge=a(x);ce=n(ge,"model(inputs)"),ge.forEach(t),ae=n(y,"."),y.forEach(t),H=m(l),w=r(l,"P",{});var ve=a(w);j=n(ve,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ve.forEach(t),V=m(l),g=r(l,"UL",{});var T=a(g);b=r(T,"LI",{});var $=a(b);Z=n($,"a single Tensor with "),O=r($,"CODE",{});var Ce=a(O);se=n(Ce,"input_ids"),Ce.forEach(t),ee=n($," only and nothing else: "),I=r($,"CODE",{});var ke=a(I);he=n(ke,"model(inputs_ids)"),ke.forEach(t),$.forEach(t),ne=m(T),v=r(T,"LI",{});var N=a(v);fe=n(N,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=r(N,"CODE",{});var we=a(S);te=n(we,"model([input_ids, attention_mask])"),we.forEach(t),J=n(N," or "),B=r(N,"CODE",{});var _e=a(B);oe=n(_e,"model([input_ids, attention_mask, token_type_ids])"),_e.forEach(t),N.forEach(t),pe=m(T),E=r(T,"LI",{});var Y=a(E);ie=n(Y,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),C=r(Y,"CODE",{});var ye=a(C);ue=n(ye,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ye.forEach(t),Y.forEach(t),T.forEach(t)},m(l,p){h(l,k,p),e(k,re),h(l,U,p),h(l,_,p),e(_,A),e(A,G),e(_,le),e(_,D),e(D,de),h(l,Q,p),h(l,u,p),e(u,W),e(u,z),e(z,X),e(u,me),e(u,x),e(x,ce),e(u,ae),h(l,H,p),h(l,w,p),e(w,j),h(l,V,p),h(l,g,p),e(g,b),e(b,Z),e(b,O),e(O,se),e(b,ee),e(b,I),e(I,he),e(g,ne),e(g,v),e(v,fe),e(v,S),e(S,te),e(v,J),e(v,B),e(B,oe),e(g,pe),e(g,E),e(E,ie),e(E,C),e(C,ue)},d(l){l&&t(k),l&&t(U),l&&t(_),l&&t(Q),l&&t(u),l&&t(H),l&&t(w),l&&t(V),l&&t(g)}}}function Sg(xe){let k,re,U,_,A,G,le,D,de,Q,u,W,z,X,me,x,ce,ae,H,w,j,V,g,b,Z,O,se,ee,I,he,ne,v,fe,S,te,J,B,oe,pe,E,ie,C,ue;return{c(){k=o("p"),re=s("TF 2.0 models accepts two formats as inputs:"),U=d(),_=o("ul"),A=o("li"),G=s("having all inputs as keyword arguments (like PyTorch models), or"),le=d(),D=o("li"),de=s("having all inputs as a list, tuple or dict in the first positional arguments."),Q=d(),u=o("p"),W=s("This second option is useful when using "),z=o("code"),X=s("tf.keras.Model.fit"),me=s(` method which currently requires having all the
tensors in the first argument of the model call function: `),x=o("code"),ce=s("model(inputs)"),ae=s("."),H=d(),w=o("p"),j=s(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=d(),g=o("ul"),b=o("li"),Z=s("a single Tensor with "),O=o("code"),se=s("input_ids"),ee=s(" only and nothing else: "),I=o("code"),he=s("model(inputs_ids)"),ne=d(),v=o("li"),fe=s(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=o("code"),te=s("model([input_ids, attention_mask])"),J=s(" or "),B=o("code"),oe=s("model([input_ids, attention_mask, token_type_ids])"),pe=d(),E=o("li"),ie=s(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),C=o("code"),ue=s('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(l){k=r(l,"P",{});var p=a(k);re=n(p,"TF 2.0 models accepts two formats as inputs:"),p.forEach(t),U=m(l),_=r(l,"UL",{});var K=a(_);A=r(K,"LI",{});var be=a(A);G=n(be,"having all inputs as keyword arguments (like PyTorch models), or"),be.forEach(t),le=m(K),D=r(K,"LI",{});var Ee=a(D);de=n(Ee,"having all inputs as a list, tuple or dict in the first positional arguments."),Ee.forEach(t),K.forEach(t),Q=m(l),u=r(l,"P",{});var y=a(u);W=n(y,"This second option is useful when using "),z=r(y,"CODE",{});var Te=a(z);X=n(Te,"tf.keras.Model.fit"),Te.forEach(t),me=n(y,` method which currently requires having all the
tensors in the first argument of the model call function: `),x=r(y,"CODE",{});var ge=a(x);ce=n(ge,"model(inputs)"),ge.forEach(t),ae=n(y,"."),y.forEach(t),H=m(l),w=r(l,"P",{});var ve=a(w);j=n(ve,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ve.forEach(t),V=m(l),g=r(l,"UL",{});var T=a(g);b=r(T,"LI",{});var $=a(b);Z=n($,"a single Tensor with "),O=r($,"CODE",{});var Ce=a(O);se=n(Ce,"input_ids"),Ce.forEach(t),ee=n($," only and nothing else: "),I=r($,"CODE",{});var ke=a(I);he=n(ke,"model(inputs_ids)"),ke.forEach(t),$.forEach(t),ne=m(T),v=r(T,"LI",{});var N=a(v);fe=n(N,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=r(N,"CODE",{});var we=a(S);te=n(we,"model([input_ids, attention_mask])"),we.forEach(t),J=n(N," or "),B=r(N,"CODE",{});var _e=a(B);oe=n(_e,"model([input_ids, attention_mask, token_type_ids])"),_e.forEach(t),N.forEach(t),pe=m(T),E=r(T,"LI",{});var Y=a(E);ie=n(Y,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),C=r(Y,"CODE",{});var ye=a(C);ue=n(ye,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ye.forEach(t),Y.forEach(t),T.forEach(t)},m(l,p){h(l,k,p),e(k,re),h(l,U,p),h(l,_,p),e(_,A),e(A,G),e(_,le),e(_,D),e(D,de),h(l,Q,p),h(l,u,p),e(u,W),e(u,z),e(z,X),e(u,me),e(u,x),e(x,ce),e(u,ae),h(l,H,p),h(l,w,p),e(w,j),h(l,V,p),h(l,g,p),e(g,b),e(b,Z),e(b,O),e(O,se),e(b,ee),e(b,I),e(I,he),e(g,ne),e(g,v),e(v,fe),e(v,S),e(S,te),e(v,J),e(v,B),e(B,oe),e(g,pe),e(g,E),e(E,ie),e(E,C),e(C,ue)},d(l){l&&t(k),l&&t(U),l&&t(_),l&&t(Q),l&&t(u),l&&t(H),l&&t(w),l&&t(V),l&&t(g)}}}function Og(xe){let k,re,U,_,A,G,le,D,de,Q,u,W,z,X,me,x,ce,ae,H,w,j,V,g,b,Z,O,se,ee,I,he,ne,v,fe,S,te,J,B,oe,pe,E,ie,C,ue;return{c(){k=o("p"),re=s("TF 2.0 models accepts two formats as inputs:"),U=d(),_=o("ul"),A=o("li"),G=s("having all inputs as keyword arguments (like PyTorch models), or"),le=d(),D=o("li"),de=s("having all inputs as a list, tuple or dict in the first positional arguments."),Q=d(),u=o("p"),W=s("This second option is useful when using "),z=o("code"),X=s("tf.keras.Model.fit"),me=s(` method which currently requires having all the
tensors in the first argument of the model call function: `),x=o("code"),ce=s("model(inputs)"),ae=s("."),H=d(),w=o("p"),j=s(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=d(),g=o("ul"),b=o("li"),Z=s("a single Tensor with "),O=o("code"),se=s("input_ids"),ee=s(" only and nothing else: "),I=o("code"),he=s("model(inputs_ids)"),ne=d(),v=o("li"),fe=s(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=o("code"),te=s("model([input_ids, attention_mask])"),J=s(" or "),B=o("code"),oe=s("model([input_ids, attention_mask, token_type_ids])"),pe=d(),E=o("li"),ie=s(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),C=o("code"),ue=s('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(l){k=r(l,"P",{});var p=a(k);re=n(p,"TF 2.0 models accepts two formats as inputs:"),p.forEach(t),U=m(l),_=r(l,"UL",{});var K=a(_);A=r(K,"LI",{});var be=a(A);G=n(be,"having all inputs as keyword arguments (like PyTorch models), or"),be.forEach(t),le=m(K),D=r(K,"LI",{});var Ee=a(D);de=n(Ee,"having all inputs as a list, tuple or dict in the first positional arguments."),Ee.forEach(t),K.forEach(t),Q=m(l),u=r(l,"P",{});var y=a(u);W=n(y,"This second option is useful when using "),z=r(y,"CODE",{});var Te=a(z);X=n(Te,"tf.keras.Model.fit"),Te.forEach(t),me=n(y,` method which currently requires having all the
tensors in the first argument of the model call function: `),x=r(y,"CODE",{});var ge=a(x);ce=n(ge,"model(inputs)"),ge.forEach(t),ae=n(y,"."),y.forEach(t),H=m(l),w=r(l,"P",{});var ve=a(w);j=n(ve,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ve.forEach(t),V=m(l),g=r(l,"UL",{});var T=a(g);b=r(T,"LI",{});var $=a(b);Z=n($,"a single Tensor with "),O=r($,"CODE",{});var Ce=a(O);se=n(Ce,"input_ids"),Ce.forEach(t),ee=n($," only and nothing else: "),I=r($,"CODE",{});var ke=a(I);he=n(ke,"model(inputs_ids)"),ke.forEach(t),$.forEach(t),ne=m(T),v=r(T,"LI",{});var N=a(v);fe=n(N,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=r(N,"CODE",{});var we=a(S);te=n(we,"model([input_ids, attention_mask])"),we.forEach(t),J=n(N," or "),B=r(N,"CODE",{});var _e=a(B);oe=n(_e,"model([input_ids, attention_mask, token_type_ids])"),_e.forEach(t),N.forEach(t),pe=m(T),E=r(T,"LI",{});var Y=a(E);ie=n(Y,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),C=r(Y,"CODE",{});var ye=a(C);ue=n(ye,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ye.forEach(t),Y.forEach(t),T.forEach(t)},m(l,p){h(l,k,p),e(k,re),h(l,U,p),h(l,_,p),e(_,A),e(A,G),e(_,le),e(_,D),e(D,de),h(l,Q,p),h(l,u,p),e(u,W),e(u,z),e(z,X),e(u,me),e(u,x),e(x,ce),e(u,ae),h(l,H,p),h(l,w,p),e(w,j),h(l,V,p),h(l,g,p),e(g,b),e(b,Z),e(b,O),e(O,se),e(b,ee),e(b,I),e(I,he),e(g,ne),e(g,v),e(v,fe),e(v,S),e(S,te),e(v,J),e(v,B),e(B,oe),e(g,pe),e(g,E),e(E,ie),e(E,C),e(C,ue)},d(l){l&&t(k),l&&t(U),l&&t(_),l&&t(Q),l&&t(u),l&&t(H),l&&t(w),l&&t(V),l&&t(g)}}}function Bg(xe){let k,re,U,_,A,G,le,D,de,Q,u,W,z,X,me,x,ce,ae,H,w,j,V,g,b,Z,O,se,ee,I,he,ne,v,fe,S,te,J,B,oe,pe,E,ie,C,ue;return{c(){k=o("p"),re=s("TF 2.0 models accepts two formats as inputs:"),U=d(),_=o("ul"),A=o("li"),G=s("having all inputs as keyword arguments (like PyTorch models), or"),le=d(),D=o("li"),de=s("having all inputs as a list, tuple or dict in the first positional arguments."),Q=d(),u=o("p"),W=s("This second option is useful when using "),z=o("code"),X=s("tf.keras.Model.fit"),me=s(` method which currently requires having all the
tensors in the first argument of the model call function: `),x=o("code"),ce=s("model(inputs)"),ae=s("."),H=d(),w=o("p"),j=s(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=d(),g=o("ul"),b=o("li"),Z=s("a single Tensor with "),O=o("code"),se=s("input_ids"),ee=s(" only and nothing else: "),I=o("code"),he=s("model(inputs_ids)"),ne=d(),v=o("li"),fe=s(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=o("code"),te=s("model([input_ids, attention_mask])"),J=s(" or "),B=o("code"),oe=s("model([input_ids, attention_mask, token_type_ids])"),pe=d(),E=o("li"),ie=s(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),C=o("code"),ue=s('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(l){k=r(l,"P",{});var p=a(k);re=n(p,"TF 2.0 models accepts two formats as inputs:"),p.forEach(t),U=m(l),_=r(l,"UL",{});var K=a(_);A=r(K,"LI",{});var be=a(A);G=n(be,"having all inputs as keyword arguments (like PyTorch models), or"),be.forEach(t),le=m(K),D=r(K,"LI",{});var Ee=a(D);de=n(Ee,"having all inputs as a list, tuple or dict in the first positional arguments."),Ee.forEach(t),K.forEach(t),Q=m(l),u=r(l,"P",{});var y=a(u);W=n(y,"This second option is useful when using "),z=r(y,"CODE",{});var Te=a(z);X=n(Te,"tf.keras.Model.fit"),Te.forEach(t),me=n(y,` method which currently requires having all the
tensors in the first argument of the model call function: `),x=r(y,"CODE",{});var ge=a(x);ce=n(ge,"model(inputs)"),ge.forEach(t),ae=n(y,"."),y.forEach(t),H=m(l),w=r(l,"P",{});var ve=a(w);j=n(ve,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ve.forEach(t),V=m(l),g=r(l,"UL",{});var T=a(g);b=r(T,"LI",{});var $=a(b);Z=n($,"a single Tensor with "),O=r($,"CODE",{});var Ce=a(O);se=n(Ce,"input_ids"),Ce.forEach(t),ee=n($," only and nothing else: "),I=r($,"CODE",{});var ke=a(I);he=n(ke,"model(inputs_ids)"),ke.forEach(t),$.forEach(t),ne=m(T),v=r(T,"LI",{});var N=a(v);fe=n(N,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=r(N,"CODE",{});var we=a(S);te=n(we,"model([input_ids, attention_mask])"),we.forEach(t),J=n(N," or "),B=r(N,"CODE",{});var _e=a(B);oe=n(_e,"model([input_ids, attention_mask, token_type_ids])"),_e.forEach(t),N.forEach(t),pe=m(T),E=r(T,"LI",{});var Y=a(E);ie=n(Y,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),C=r(Y,"CODE",{});var ye=a(C);ue=n(ye,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ye.forEach(t),Y.forEach(t),T.forEach(t)},m(l,p){h(l,k,p),e(k,re),h(l,U,p),h(l,_,p),e(_,A),e(A,G),e(_,le),e(_,D),e(D,de),h(l,Q,p),h(l,u,p),e(u,W),e(u,z),e(z,X),e(u,me),e(u,x),e(x,ce),e(u,ae),h(l,H,p),h(l,w,p),e(w,j),h(l,V,p),h(l,g,p),e(g,b),e(b,Z),e(b,O),e(O,se),e(b,ee),e(b,I),e(I,he),e(g,ne),e(g,v),e(v,fe),e(v,S),e(S,te),e(v,J),e(v,B),e(B,oe),e(g,pe),e(g,E),e(E,ie),e(E,C),e(C,ue)},d(l){l&&t(k),l&&t(U),l&&t(_),l&&t(Q),l&&t(u),l&&t(H),l&&t(w),l&&t(V),l&&t(g)}}}function Ng(xe){let k,re,U,_,A,G,le,D,de,Q,u,W,z,X,me,x,ce,ae,H,w,j,V,g,b,Z,O,se,ee,I,he,ne,v,fe,S,te,J,B,oe,pe,E,ie,C,ue;return{c(){k=o("p"),re=s("TF 2.0 models accepts two formats as inputs:"),U=d(),_=o("ul"),A=o("li"),G=s("having all inputs as keyword arguments (like PyTorch models), or"),le=d(),D=o("li"),de=s("having all inputs as a list, tuple or dict in the first positional arguments."),Q=d(),u=o("p"),W=s("This second option is useful when using "),z=o("code"),X=s("tf.keras.Model.fit"),me=s(` method which currently requires having all the
tensors in the first argument of the model call function: `),x=o("code"),ce=s("model(inputs)"),ae=s("."),H=d(),w=o("p"),j=s(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=d(),g=o("ul"),b=o("li"),Z=s("a single Tensor with "),O=o("code"),se=s("input_ids"),ee=s(" only and nothing else: "),I=o("code"),he=s("model(inputs_ids)"),ne=d(),v=o("li"),fe=s(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=o("code"),te=s("model([input_ids, attention_mask])"),J=s(" or "),B=o("code"),oe=s("model([input_ids, attention_mask, token_type_ids])"),pe=d(),E=o("li"),ie=s(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),C=o("code"),ue=s('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(l){k=r(l,"P",{});var p=a(k);re=n(p,"TF 2.0 models accepts two formats as inputs:"),p.forEach(t),U=m(l),_=r(l,"UL",{});var K=a(_);A=r(K,"LI",{});var be=a(A);G=n(be,"having all inputs as keyword arguments (like PyTorch models), or"),be.forEach(t),le=m(K),D=r(K,"LI",{});var Ee=a(D);de=n(Ee,"having all inputs as a list, tuple or dict in the first positional arguments."),Ee.forEach(t),K.forEach(t),Q=m(l),u=r(l,"P",{});var y=a(u);W=n(y,"This second option is useful when using "),z=r(y,"CODE",{});var Te=a(z);X=n(Te,"tf.keras.Model.fit"),Te.forEach(t),me=n(y,` method which currently requires having all the
tensors in the first argument of the model call function: `),x=r(y,"CODE",{});var ge=a(x);ce=n(ge,"model(inputs)"),ge.forEach(t),ae=n(y,"."),y.forEach(t),H=m(l),w=r(l,"P",{});var ve=a(w);j=n(ve,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ve.forEach(t),V=m(l),g=r(l,"UL",{});var T=a(g);b=r(T,"LI",{});var $=a(b);Z=n($,"a single Tensor with "),O=r($,"CODE",{});var Ce=a(O);se=n(Ce,"input_ids"),Ce.forEach(t),ee=n($," only and nothing else: "),I=r($,"CODE",{});var ke=a(I);he=n(ke,"model(inputs_ids)"),ke.forEach(t),$.forEach(t),ne=m(T),v=r(T,"LI",{});var N=a(v);fe=n(N,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=r(N,"CODE",{});var we=a(S);te=n(we,"model([input_ids, attention_mask])"),we.forEach(t),J=n(N," or "),B=r(N,"CODE",{});var _e=a(B);oe=n(_e,"model([input_ids, attention_mask, token_type_ids])"),_e.forEach(t),N.forEach(t),pe=m(T),E=r(T,"LI",{});var Y=a(E);ie=n(Y,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),C=r(Y,"CODE",{});var ye=a(C);ue=n(ye,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ye.forEach(t),Y.forEach(t),T.forEach(t)},m(l,p){h(l,k,p),e(k,re),h(l,U,p),h(l,_,p),e(_,A),e(A,G),e(_,le),e(_,D),e(D,de),h(l,Q,p),h(l,u,p),e(u,W),e(u,z),e(z,X),e(u,me),e(u,x),e(x,ce),e(u,ae),h(l,H,p),h(l,w,p),e(w,j),h(l,V,p),h(l,g,p),e(g,b),e(b,Z),e(b,O),e(O,se),e(b,ee),e(b,I),e(I,he),e(g,ne),e(g,v),e(v,fe),e(v,S),e(S,te),e(v,J),e(v,B),e(B,oe),e(g,pe),e(g,E),e(E,ie),e(E,C),e(C,ue)},d(l){l&&t(k),l&&t(U),l&&t(_),l&&t(Q),l&&t(u),l&&t(H),l&&t(w),l&&t(V),l&&t(g)}}}function Ug(xe){let k,re,U,_,A,G,le,D,de,Q,u,W,z,X,me,x,ce,ae,H,w,j,V,g,b,Z,O,se,ee,I,he,ne,v,fe,S,te,J,B,oe,pe,E,ie,C,ue;return{c(){k=o("p"),re=s("TF 2.0 models accepts two formats as inputs:"),U=d(),_=o("ul"),A=o("li"),G=s("having all inputs as keyword arguments (like PyTorch models), or"),le=d(),D=o("li"),de=s("having all inputs as a list, tuple or dict in the first positional arguments."),Q=d(),u=o("p"),W=s("This second option is useful when using "),z=o("code"),X=s("tf.keras.Model.fit"),me=s(` method which currently requires having all the
tensors in the first argument of the model call function: `),x=o("code"),ce=s("model(inputs)"),ae=s("."),H=d(),w=o("p"),j=s(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=d(),g=o("ul"),b=o("li"),Z=s("a single Tensor with "),O=o("code"),se=s("input_ids"),ee=s(" only and nothing else: "),I=o("code"),he=s("model(inputs_ids)"),ne=d(),v=o("li"),fe=s(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=o("code"),te=s("model([input_ids, attention_mask])"),J=s(" or "),B=o("code"),oe=s("model([input_ids, attention_mask, token_type_ids])"),pe=d(),E=o("li"),ie=s(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),C=o("code"),ue=s('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(l){k=r(l,"P",{});var p=a(k);re=n(p,"TF 2.0 models accepts two formats as inputs:"),p.forEach(t),U=m(l),_=r(l,"UL",{});var K=a(_);A=r(K,"LI",{});var be=a(A);G=n(be,"having all inputs as keyword arguments (like PyTorch models), or"),be.forEach(t),le=m(K),D=r(K,"LI",{});var Ee=a(D);de=n(Ee,"having all inputs as a list, tuple or dict in the first positional arguments."),Ee.forEach(t),K.forEach(t),Q=m(l),u=r(l,"P",{});var y=a(u);W=n(y,"This second option is useful when using "),z=r(y,"CODE",{});var Te=a(z);X=n(Te,"tf.keras.Model.fit"),Te.forEach(t),me=n(y,` method which currently requires having all the
tensors in the first argument of the model call function: `),x=r(y,"CODE",{});var ge=a(x);ce=n(ge,"model(inputs)"),ge.forEach(t),ae=n(y,"."),y.forEach(t),H=m(l),w=r(l,"P",{});var ve=a(w);j=n(ve,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ve.forEach(t),V=m(l),g=r(l,"UL",{});var T=a(g);b=r(T,"LI",{});var $=a(b);Z=n($,"a single Tensor with "),O=r($,"CODE",{});var Ce=a(O);se=n(Ce,"input_ids"),Ce.forEach(t),ee=n($," only and nothing else: "),I=r($,"CODE",{});var ke=a(I);he=n(ke,"model(inputs_ids)"),ke.forEach(t),$.forEach(t),ne=m(T),v=r(T,"LI",{});var N=a(v);fe=n(N,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=r(N,"CODE",{});var we=a(S);te=n(we,"model([input_ids, attention_mask])"),we.forEach(t),J=n(N," or "),B=r(N,"CODE",{});var _e=a(B);oe=n(_e,"model([input_ids, attention_mask, token_type_ids])"),_e.forEach(t),N.forEach(t),pe=m(T),E=r(T,"LI",{});var Y=a(E);ie=n(Y,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),C=r(Y,"CODE",{});var ye=a(C);ue=n(ye,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ye.forEach(t),Y.forEach(t),T.forEach(t)},m(l,p){h(l,k,p),e(k,re),h(l,U,p),h(l,_,p),e(_,A),e(A,G),e(_,le),e(_,D),e(D,de),h(l,Q,p),h(l,u,p),e(u,W),e(u,z),e(z,X),e(u,me),e(u,x),e(x,ce),e(u,ae),h(l,H,p),h(l,w,p),e(w,j),h(l,V,p),h(l,g,p),e(g,b),e(b,Z),e(b,O),e(O,se),e(b,ee),e(b,I),e(I,he),e(g,ne),e(g,v),e(v,fe),e(v,S),e(S,te),e(v,J),e(v,B),e(B,oe),e(g,pe),e(g,E),e(E,ie),e(E,C),e(C,ue)},d(l){l&&t(k),l&&t(U),l&&t(_),l&&t(Q),l&&t(u),l&&t(H),l&&t(w),l&&t(V),l&&t(g)}}}function Hg(xe){let k,re,U,_,A,G,le,D,de,Q,u,W,z,X,me,x,ce,ae,H,w,j,V,g,b,Z,O,se,ee,I,he,ne,v,fe,S,te,J,B,oe,pe,E,ie,C,ue,l,p,K,be,Ee,y,Te,ge,ve,T,$,Ce,ke,N,we,_e,Y,ye,ht,il,na,ll,dl,ao,ml,cl,ln,ft,Lt,Ja,so,hl,Ya,fl,dn,Fe,no,pl,et,ul,ia,gl,_l,la,vl,bl,io,kl,Tl,wl,lo,El,da,Cl,yl,$l,tt,mo,Fl,Za,Ml,Pl,co,ma,Ll,es,Rl,ql,ca,Al,ts,Dl,zl,Rt,ho,xl,fo,Il,os,Sl,Ol,Bl,qt,po,Nl,rs,Ul,Hl,ha,uo,mn,pt,At,as,go,Ql,ss,Vl,cn,Ie,_o,Kl,Ke,Wl,ns,Gl,Xl,fa,jl,Jl,pa,Yl,Zl,vo,ed,td,od,bo,rd,ua,ad,sd,nd,ot,ko,id,is,ld,dd,To,ga,md,ls,cd,hd,_a,fd,ds,pd,ud,Dt,wo,gd,ms,_d,hn,ut,zt,cs,Eo,vd,hs,bd,fn,Se,Co,kd,fs,Td,wd,yo,Ed,va,Cd,yd,$d,$o,Fd,Fo,Md,Pd,Ld,Mo,Rd,ba,qd,Ad,pn,gt,xt,ps,Po,Dd,us,zd,un,Oe,Lo,xd,Ro,Id,gs,Sd,Od,Bd,qo,Nd,ka,Ud,Hd,Qd,Ao,Vd,Do,Kd,Wd,Gd,zo,Xd,Ta,jd,Jd,gn,_t,It,_s,xo,Yd,vs,Zd,_n,Be,Io,em,So,tm,bs,om,rm,am,Oo,sm,wa,nm,im,lm,Bo,dm,No,mm,cm,hm,Uo,fm,Ea,pm,um,vn,vt,St,ks,Ho,gm,Ts,_m,bn,Ne,Qo,vm,ws,bm,km,Vo,Tm,Ca,wm,Em,Cm,Ko,ym,Wo,$m,Fm,Mm,Go,Pm,ya,Lm,Rm,kn,bt,Ot,Es,Xo,qm,Cs,Am,Tn,Ue,jo,Dm,ys,zm,xm,Jo,Im,$a,Sm,Om,Bm,Yo,Nm,Zo,Um,Hm,Qm,er,Vm,Fa,Km,Wm,wn,kt,Bt,$s,tr,Gm,Fs,Xm,En,He,or,jm,Ms,Jm,Ym,rr,Zm,Ma,ec,tc,oc,ar,rc,sr,ac,sc,nc,nr,ic,Pa,lc,dc,Cn,Tt,Nt,Ps,ir,mc,Ls,cc,yn,Qe,lr,hc,Ut,fc,Rs,pc,uc,qs,gc,_c,dr,vc,La,bc,kc,Tc,mr,wc,cr,Ec,Cc,yc,hr,$c,Ra,Fc,Mc,$n,wt,Ht,As,fr,Pc,Ds,Lc,Fn,Pe,pr,Rc,zs,qc,Ac,ur,Dc,qa,zc,xc,Ic,gr,Sc,_r,Oc,Bc,Nc,Qt,Uc,vr,Hc,Aa,Qc,Vc,Mn,Et,Vt,xs,br,Kc,Is,Wc,Pn,Le,kr,Gc,Tr,Xc,Ss,jc,Jc,Yc,wr,Zc,Da,eh,th,oh,Er,rh,Cr,ah,sh,nh,Kt,ih,yr,lh,za,dh,mh,Ln,Ct,Wt,Os,$r,ch,Bs,hh,Rn,Re,Fr,fh,Mr,ph,Ns,uh,gh,_h,Pr,vh,xa,bh,kh,Th,Lr,wh,Rr,Eh,Ch,yh,Gt,$h,qr,Fh,Ia,Mh,Ph,qn,yt,Xt,Us,Ar,Lh,Hs,Rh,An,qe,Dr,qh,Qs,Ah,Dh,zr,zh,Sa,xh,Ih,Sh,xr,Oh,Ir,Bh,Nh,Uh,jt,Hh,Sr,Qh,Oa,Vh,Kh,Dn,$t,Jt,Vs,Or,Wh,Ks,Gh,zn,Ae,Br,Xh,Ws,jh,Jh,Nr,Yh,Ba,Zh,ef,tf,Ur,of,Hr,rf,af,sf,Yt,nf,Qr,lf,Na,df,mf,xn,Ft,Zt,Gs,Vr,cf,Xs,hf,In,De,Kr,ff,js,pf,uf,Wr,gf,Ua,_f,vf,bf,Gr,kf,Xr,Tf,wf,Ef,eo,Cf,jr,yf,Ha,$f,Ff,Sn,Mt,to,Js,Jr,Mf,Ys,Pf,On,ze,Yr,Lf,Pt,Rf,Zs,qf,Af,en,Df,zf,xf,Zr,If,Qa,Sf,Of,Bf,ea,Nf,ta,Uf,Hf,Qf,oo,Vf,oa,Kf,Va,Wf,Gf,Bn;return G=new Me({}),X=new Me({}),$=new Me({}),Y=new $e({props:{name:"class transformers.CamembertConfig",anchor:"transformers.CamembertConfig",parameters:[{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/configuration_camembert.py#L39"}}),so=new Me({}),no=new $e({props:{name:"class transformers.CamembertTokenizer",anchor:"transformers.CamembertTokenizer",parameters:[{name:"vocab_file",val:""},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"additional_special_tokens",val:" = ['<s>NOTUSED', '</s>NOTUSED']"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CamembertTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.CamembertTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.CamembertTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.CamembertTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.CamembertTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.CamembertTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.CamembertTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.CamembertTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.CamembertTokenizer.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>, defaults to <code>[&quot;&lt;s&gt;NOTUSED&quot;, &quot;&lt;/s&gt;NOTUSED&quot;]</code>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"},{anchor:"transformers.CamembertTokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"},{anchor:"transformers.CamembertTokenizer.sp_model",description:`<strong>sp_model</strong> (<code>SentencePieceProcessor</code>) &#x2014;
The <em>SentencePiece</em> processor that is used for every conversion (string, tokens and IDs).`,name:"sp_model"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/tokenization_camembert.py#L45"}}),mo=new $e({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.CamembertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.CamembertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.CamembertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/tokenization_camembert.py#L161",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ho=new $e({props:{name:"get_special_tokens_mask",anchor:"transformers.CamembertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.CamembertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.CamembertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.CamembertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/tokenization_camembert.py#L187",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),po=new $e({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.CamembertTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.CamembertTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.CamembertTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/tokenization_camembert.py#L214",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),uo=new $e({props:{name:"save_vocabulary",anchor:"transformers.CamembertTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/tokenization_camembert.py#L283"}}),go=new Me({}),_o=new $e({props:{name:"class transformers.CamembertTokenizerFast",anchor:"transformers.CamembertTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"additional_special_tokens",val:" = ['<s>NOTUSED', '</s>NOTUSED']"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CamembertTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.CamembertTokenizerFast.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.CamembertTokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.CamembertTokenizerFast.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.CamembertTokenizerFast.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.CamembertTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.CamembertTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.CamembertTokenizerFast.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.CamembertTokenizerFast.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>, defaults to <code>[&quot;&lt;s&gt;NOTUSED&quot;, &quot;&lt;/s&gt;NOTUSED&quot;]</code>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/tokenization_camembert_fast.py#L53"}}),ko=new $e({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.CamembertTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.CamembertTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.CamembertTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/tokenization_camembert_fast.py#L145",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),wo=new $e({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.CamembertTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.CamembertTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.CamembertTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/tokenization_camembert_fast.py#L171",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Eo=new Me({}),Co=new $e({props:{name:"class transformers.CamembertModel",anchor:"transformers.CamembertModel",parameters:[{name:"config",val:""},{name:"add_pooling_layer",val:" = True"}],parametersDescription:[{anchor:"transformers.CamembertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.21.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/modeling_camembert.py#L63"}}),Po=new Me({}),Lo=new $e({props:{name:"class transformers.CamembertForCausalLM",anchor:"transformers.CamembertForCausalLM",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.CamembertForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.21.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/modeling_camembert.py#L152"}}),xo=new Me({}),Io=new $e({props:{name:"class transformers.CamembertForMaskedLM",anchor:"transformers.CamembertForMaskedLM",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.CamembertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.21.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/modeling_camembert.py#L76"}}),Ho=new Me({}),Qo=new $e({props:{name:"class transformers.CamembertForSequenceClassification",anchor:"transformers.CamembertForSequenceClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.CamembertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.21.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/modeling_camembert.py#L92"}}),Xo=new Me({}),jo=new $e({props:{name:"class transformers.CamembertForMultipleChoice",anchor:"transformers.CamembertForMultipleChoice",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.CamembertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.21.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/modeling_camembert.py#L108"}}),tr=new Me({}),or=new $e({props:{name:"class transformers.CamembertForTokenClassification",anchor:"transformers.CamembertForTokenClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.CamembertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.21.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/modeling_camembert.py#L124"}}),ir=new Me({}),lr=new $e({props:{name:"class transformers.CamembertForQuestionAnswering",anchor:"transformers.CamembertForQuestionAnswering",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.CamembertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.21.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/modeling_camembert.py#L140"}}),fr=new Me({}),pr=new $e({props:{name:"class transformers.TFCamembertModel",anchor:"transformers.TFCamembertModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFCamembertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.21.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/modeling_tf_camembert.py#L80"}}),Qt=new sa({props:{$$slots:{default:[xg]},$$scope:{ctx:xe}}}),br=new Me({}),kr=new $e({props:{name:"class transformers.TFCamembertForCausalLM",anchor:"transformers.TFCamembertForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFCamembertForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.21.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/modeling_tf_camembert.py#L169"}}),Kt=new sa({props:{$$slots:{default:[Ig]},$$scope:{ctx:xe}}}),$r=new Me({}),Fr=new $e({props:{name:"class transformers.TFCamembertForMaskedLM",anchor:"transformers.TFCamembertForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFCamembertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.21.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/modeling_tf_camembert.py#L93"}}),Gt=new sa({props:{$$slots:{default:[Sg]},$$scope:{ctx:xe}}}),Ar=new Me({}),Dr=new $e({props:{name:"class transformers.TFCamembertForSequenceClassification",anchor:"transformers.TFCamembertForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFCamembertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.21.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/modeling_tf_camembert.py#L109"}}),jt=new sa({props:{$$slots:{default:[Og]},$$scope:{ctx:xe}}}),Or=new Me({}),Br=new $e({props:{name:"class transformers.TFCamembertForMultipleChoice",anchor:"transformers.TFCamembertForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFCamembertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.21.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/modeling_tf_camembert.py#L141"}}),Yt=new sa({props:{$$slots:{default:[Bg]},$$scope:{ctx:xe}}}),Vr=new Me({}),Kr=new $e({props:{name:"class transformers.TFCamembertForTokenClassification",anchor:"transformers.TFCamembertForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFCamembertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.21.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/modeling_tf_camembert.py#L125"}}),eo=new sa({props:{$$slots:{default:[Ng]},$$scope:{ctx:xe}}}),Jr=new Me({}),Yr=new $e({props:{name:"class transformers.TFCamembertForQuestionAnswering",anchor:"transformers.TFCamembertForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFCamembertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.21.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/models/camembert/modeling_tf_camembert.py#L157"}}),oo=new sa({props:{$$slots:{default:[Ug]},$$scope:{ctx:xe}}}),{c(){k=o("meta"),re=d(),U=o("h1"),_=o("a"),A=o("span"),F(G.$$.fragment),le=d(),D=o("span"),de=s("CamemBERT"),Q=d(),u=o("h2"),W=o("a"),z=o("span"),F(X.$$.fragment),me=d(),x=o("span"),ce=s("Overview"),ae=d(),H=o("p"),w=s("The CamemBERT model was proposed in "),j=o("a"),V=s("CamemBERT: a Tasty French Language Model"),g=s(` by
Louis Martin, Benjamin Muller, Pedro Javier Ortiz Su\xE1rez, Yoann Dupont, Laurent Romary, \xC9ric Villemonte de la
Clergerie, Djam\xE9 Seddah, and Beno\xEEt Sagot. It is based on Facebook\u2019s RoBERTa model released in 2019. It is a model
trained on 138GB of French text.`),b=d(),Z=o("p"),O=s("The abstract from the paper is the following:"),se=d(),ee=o("p"),I=o("em"),he=s(`Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available
models have either been trained on English data or on the concatenation of data in multiple languages. This makes
practical use of such models \u2014in all languages except English\u2014 very limited. Aiming to address this issue for French,
we release CamemBERT, a French version of the Bi-directional Encoders for Transformers (BERT). We measure the
performance of CamemBERT compared to multilingual models in multiple downstream tasks, namely part-of-speech tagging,
dependency parsing, named-entity recognition, and natural language inference. CamemBERT improves the state of the art
for most of the tasks considered. We release the pretrained model for CamemBERT hoping to foster research and
downstream applications for French NLP.`),ne=d(),v=o("p"),fe=s("Tips:"),S=d(),te=o("ul"),J=o("li"),B=s("This implementation is the same as RoBERTa. Refer to the "),oe=o("a"),pe=s("documentation of RoBERTa"),E=s(` for usage examples
as well as the information relative to the inputs and outputs.`),ie=d(),C=o("p"),ue=s("This model was contributed by "),l=o("a"),p=s("camembert"),K=s(". The original code can be found "),be=o("a"),Ee=s("here"),y=s("."),Te=d(),ge=o("h2"),ve=o("a"),T=o("span"),F($.$$.fragment),Ce=d(),ke=o("span"),N=s("CamembertConfig"),we=d(),_e=o("div"),F(Y.$$.fragment),ye=d(),ht=o("p"),il=s("This class overrides "),na=o("a"),ll=s("RobertaConfig"),dl=s(`. Please check the superclass for the appropriate documentation alongside
usage examples. Instantiating a configuration with the defaults will yield a similar configuration to that of the
Camembert `),ao=o("a"),ml=s("camembert-base"),cl=s(" architecture."),ln=d(),ft=o("h2"),Lt=o("a"),Ja=o("span"),F(so.$$.fragment),hl=d(),Ya=o("span"),fl=s("CamembertTokenizer"),dn=d(),Fe=o("div"),F(no.$$.fragment),pl=d(),et=o("p"),ul=s("Adapted from "),ia=o("a"),gl=s("RobertaTokenizer"),_l=s(" and "),la=o("a"),vl=s("XLNetTokenizer"),bl=s(`. Construct a CamemBERT tokenizer. Based on
`),io=o("a"),kl=s("SentencePiece"),Tl=s("."),wl=d(),lo=o("p"),El=s("This tokenizer inherits from "),da=o("a"),Cl=s("PreTrainedTokenizer"),yl=s(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),$l=d(),tt=o("div"),F(mo.$$.fragment),Fl=d(),Za=o("p"),Ml=s(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An CamemBERT sequence has the following format:`),Pl=d(),co=o("ul"),ma=o("li"),Ll=s("single sequence: "),es=o("code"),Rl=s("<s> X </s>"),ql=d(),ca=o("li"),Al=s("pair of sequences: "),ts=o("code"),Dl=s("<s> A </s></s> B </s>"),zl=d(),Rt=o("div"),F(ho.$$.fragment),xl=d(),fo=o("p"),Il=s(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),os=o("code"),Sl=s("prepare_for_model"),Ol=s(" method."),Bl=d(),qt=o("div"),F(po.$$.fragment),Nl=d(),rs=o("p"),Ul=s(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. CamemBERT, like
RoBERTa, does not make use of token type ids, therefore a list of zeros is returned.`),Hl=d(),ha=o("div"),F(uo.$$.fragment),mn=d(),pt=o("h2"),At=o("a"),as=o("span"),F(go.$$.fragment),Ql=d(),ss=o("span"),Vl=s("CamembertTokenizerFast"),cn=d(),Ie=o("div"),F(_o.$$.fragment),Kl=d(),Ke=o("p"),Wl=s("Construct a \u201Cfast\u201D CamemBERT tokenizer (backed by HuggingFace\u2019s "),ns=o("em"),Gl=s("tokenizers"),Xl=s(` library). Adapted from
`),fa=o("a"),jl=s("RobertaTokenizer"),Jl=s(" and "),pa=o("a"),Yl=s("XLNetTokenizer"),Zl=s(`. Based on
`),vo=o("a"),ed=s("BPE"),td=s("."),od=d(),bo=o("p"),rd=s("This tokenizer inherits from "),ua=o("a"),ad=s("PreTrainedTokenizerFast"),sd=s(` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),nd=d(),ot=o("div"),F(ko.$$.fragment),id=d(),is=o("p"),ld=s(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An CamemBERT sequence has the following format:`),dd=d(),To=o("ul"),ga=o("li"),md=s("single sequence: "),ls=o("code"),cd=s("<s> X </s>"),hd=d(),_a=o("li"),fd=s("pair of sequences: "),ds=o("code"),pd=s("<s> A </s></s> B </s>"),ud=d(),Dt=o("div"),F(wo.$$.fragment),gd=d(),ms=o("p"),_d=s(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. CamemBERT, like
RoBERTa, does not make use of token type ids, therefore a list of zeros is returned.`),hn=d(),ut=o("h2"),zt=o("a"),cs=o("span"),F(Eo.$$.fragment),vd=d(),hs=o("span"),bd=s("CamembertModel"),fn=d(),Se=o("div"),F(Co.$$.fragment),kd=d(),fs=o("p"),Td=s("The bare CamemBERT Model transformer outputting raw hidden-states without any specific head on top."),wd=d(),yo=o("p"),Ed=s("This model inherits from "),va=o("a"),Cd=s("PreTrainedModel"),yd=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),$d=d(),$o=o("p"),Fd=s("This model is also a PyTorch "),Fo=o("a"),Md=s("torch.nn.Module"),Pd=s(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ld=d(),Mo=o("p"),Rd=s("This class overrides "),ba=o("a"),qd=s("RobertaModel"),Ad=s(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),pn=d(),gt=o("h2"),xt=o("a"),ps=o("span"),F(Po.$$.fragment),Dd=d(),us=o("span"),zd=s("CamembertForCausalLM"),un=d(),Oe=o("div"),F(Lo.$$.fragment),xd=d(),Ro=o("p"),Id=s("CamemBERT Model with a "),gs=o("code"),Sd=s("language modeling"),Od=s(" head on top for CLM fine-tuning."),Bd=d(),qo=o("p"),Nd=s("This model inherits from "),ka=o("a"),Ud=s("PreTrainedModel"),Hd=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Qd=d(),Ao=o("p"),Vd=s("This model is also a PyTorch "),Do=o("a"),Kd=s("torch.nn.Module"),Wd=s(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Gd=d(),zo=o("p"),Xd=s("This class overrides "),Ta=o("a"),jd=s("RobertaForCausalLM"),Jd=s(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),gn=d(),_t=o("h2"),It=o("a"),_s=o("span"),F(xo.$$.fragment),Yd=d(),vs=o("span"),Zd=s("CamembertForMaskedLM"),_n=d(),Be=o("div"),F(Io.$$.fragment),em=d(),So=o("p"),tm=s("CamemBERT Model with a "),bs=o("code"),om=s("language modeling"),rm=s(" head on top."),am=d(),Oo=o("p"),sm=s("This model inherits from "),wa=o("a"),nm=s("PreTrainedModel"),im=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),lm=d(),Bo=o("p"),dm=s("This model is also a PyTorch "),No=o("a"),mm=s("torch.nn.Module"),cm=s(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),hm=d(),Uo=o("p"),fm=s("This class overrides "),Ea=o("a"),pm=s("RobertaForMaskedLM"),um=s(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),vn=d(),vt=o("h2"),St=o("a"),ks=o("span"),F(Ho.$$.fragment),gm=d(),Ts=o("span"),_m=s("CamembertForSequenceClassification"),bn=d(),Ne=o("div"),F(Qo.$$.fragment),vm=d(),ws=o("p"),bm=s(`CamemBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),km=d(),Vo=o("p"),Tm=s("This model inherits from "),Ca=o("a"),wm=s("PreTrainedModel"),Em=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Cm=d(),Ko=o("p"),ym=s("This model is also a PyTorch "),Wo=o("a"),$m=s("torch.nn.Module"),Fm=s(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Mm=d(),Go=o("p"),Pm=s("This class overrides "),ya=o("a"),Lm=s("RobertaForSequenceClassification"),Rm=s(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),kn=d(),bt=o("h2"),Ot=o("a"),Es=o("span"),F(Xo.$$.fragment),qm=d(),Cs=o("span"),Am=s("CamembertForMultipleChoice"),Tn=d(),Ue=o("div"),F(jo.$$.fragment),Dm=d(),ys=o("p"),zm=s(`CamemBERT Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),xm=d(),Jo=o("p"),Im=s("This model inherits from "),$a=o("a"),Sm=s("PreTrainedModel"),Om=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Bm=d(),Yo=o("p"),Nm=s("This model is also a PyTorch "),Zo=o("a"),Um=s("torch.nn.Module"),Hm=s(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Qm=d(),er=o("p"),Vm=s("This class overrides "),Fa=o("a"),Km=s("RobertaForMultipleChoice"),Wm=s(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),wn=d(),kt=o("h2"),Bt=o("a"),$s=o("span"),F(tr.$$.fragment),Gm=d(),Fs=o("span"),Xm=s("CamembertForTokenClassification"),En=d(),He=o("div"),F(or.$$.fragment),jm=d(),Ms=o("p"),Jm=s(`CamemBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),Ym=d(),rr=o("p"),Zm=s("This model inherits from "),Ma=o("a"),ec=s("PreTrainedModel"),tc=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),oc=d(),ar=o("p"),rc=s("This model is also a PyTorch "),sr=o("a"),ac=s("torch.nn.Module"),sc=s(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),nc=d(),nr=o("p"),ic=s("This class overrides "),Pa=o("a"),lc=s("RobertaForTokenClassification"),dc=s(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Cn=d(),Tt=o("h2"),Nt=o("a"),Ps=o("span"),F(ir.$$.fragment),mc=d(),Ls=o("span"),cc=s("CamembertForQuestionAnswering"),yn=d(),Qe=o("div"),F(lr.$$.fragment),hc=d(),Ut=o("p"),fc=s(`CamemBERT Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),Rs=o("code"),pc=s("span start logits"),uc=s(" and "),qs=o("code"),gc=s("span end logits"),_c=d(),dr=o("p"),vc=s("This model inherits from "),La=o("a"),bc=s("PreTrainedModel"),kc=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Tc=d(),mr=o("p"),wc=s("This model is also a PyTorch "),cr=o("a"),Ec=s("torch.nn.Module"),Cc=s(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),yc=d(),hr=o("p"),$c=s("This class overrides "),Ra=o("a"),Fc=s("RobertaForQuestionAnswering"),Mc=s(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),$n=d(),wt=o("h2"),Ht=o("a"),As=o("span"),F(fr.$$.fragment),Pc=d(),Ds=o("span"),Lc=s("TFCamembertModel"),Fn=d(),Pe=o("div"),F(pr.$$.fragment),Rc=d(),zs=o("p"),qc=s("The bare CamemBERT Model transformer outputting raw hidden-states without any specific head on top."),Ac=d(),ur=o("p"),Dc=s("This model inherits from "),qa=o("a"),zc=s("TFPreTrainedModel"),xc=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ic=d(),gr=o("p"),Sc=s("This model is also a "),_r=o("a"),Oc=s("tf.keras.Model"),Bc=s(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Nc=d(),F(Qt.$$.fragment),Uc=d(),vr=o("p"),Hc=s("This class overrides "),Aa=o("a"),Qc=s("TFRobertaModel"),Vc=s(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Mn=d(),Et=o("h2"),Vt=o("a"),xs=o("span"),F(br.$$.fragment),Kc=d(),Is=o("span"),Wc=s("TFCamembertForCasualLM"),Pn=d(),Le=o("div"),F(kr.$$.fragment),Gc=d(),Tr=o("p"),Xc=s("CamemBERT Model with a "),Ss=o("code"),jc=s("language modeling"),Jc=s(" head on top for CLM fine-tuning."),Yc=d(),wr=o("p"),Zc=s("This model inherits from "),Da=o("a"),eh=s("TFPreTrainedModel"),th=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),oh=d(),Er=o("p"),rh=s("This model is also a "),Cr=o("a"),ah=s("tf.keras.Model"),sh=s(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),nh=d(),F(Kt.$$.fragment),ih=d(),yr=o("p"),lh=s("This class overrides "),za=o("a"),dh=s("TFRobertaForCausalLM"),mh=s(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Ln=d(),Ct=o("h2"),Wt=o("a"),Os=o("span"),F($r.$$.fragment),ch=d(),Bs=o("span"),hh=s("TFCamembertForMaskedLM"),Rn=d(),Re=o("div"),F(Fr.$$.fragment),fh=d(),Mr=o("p"),ph=s("CamemBERT Model with a "),Ns=o("code"),uh=s("language modeling"),gh=s(" head on top."),_h=d(),Pr=o("p"),vh=s("This model inherits from "),xa=o("a"),bh=s("TFPreTrainedModel"),kh=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Th=d(),Lr=o("p"),wh=s("This model is also a "),Rr=o("a"),Eh=s("tf.keras.Model"),Ch=s(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),yh=d(),F(Gt.$$.fragment),$h=d(),qr=o("p"),Fh=s("This class overrides "),Ia=o("a"),Mh=s("TFRobertaForMaskedLM"),Ph=s(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),qn=d(),yt=o("h2"),Xt=o("a"),Us=o("span"),F(Ar.$$.fragment),Lh=d(),Hs=o("span"),Rh=s("TFCamembertForSequenceClassification"),An=d(),qe=o("div"),F(Dr.$$.fragment),qh=d(),Qs=o("p"),Ah=s(`CamemBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),Dh=d(),zr=o("p"),zh=s("This model inherits from "),Sa=o("a"),xh=s("TFPreTrainedModel"),Ih=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Sh=d(),xr=o("p"),Oh=s("This model is also a "),Ir=o("a"),Bh=s("tf.keras.Model"),Nh=s(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Uh=d(),F(jt.$$.fragment),Hh=d(),Sr=o("p"),Qh=s("This class overrides "),Oa=o("a"),Vh=s("TFRobertaForSequenceClassification"),Kh=s(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Dn=d(),$t=o("h2"),Jt=o("a"),Vs=o("span"),F(Or.$$.fragment),Wh=d(),Ks=o("span"),Gh=s("TFCamembertForMultipleChoice"),zn=d(),Ae=o("div"),F(Br.$$.fragment),Xh=d(),Ws=o("p"),jh=s(`CamemBERT Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),Jh=d(),Nr=o("p"),Yh=s("This model inherits from "),Ba=o("a"),Zh=s("TFPreTrainedModel"),ef=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),tf=d(),Ur=o("p"),of=s("This model is also a "),Hr=o("a"),rf=s("tf.keras.Model"),af=s(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),sf=d(),F(Yt.$$.fragment),nf=d(),Qr=o("p"),lf=s("This class overrides "),Na=o("a"),df=s("TFRobertaForMultipleChoice"),mf=s(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),xn=d(),Ft=o("h2"),Zt=o("a"),Gs=o("span"),F(Vr.$$.fragment),cf=d(),Xs=o("span"),hf=s("TFCamembertForTokenClassification"),In=d(),De=o("div"),F(Kr.$$.fragment),ff=d(),js=o("p"),pf=s(`CamemBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),uf=d(),Wr=o("p"),gf=s("This model inherits from "),Ua=o("a"),_f=s("TFPreTrainedModel"),vf=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),bf=d(),Gr=o("p"),kf=s("This model is also a "),Xr=o("a"),Tf=s("tf.keras.Model"),wf=s(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Ef=d(),F(eo.$$.fragment),Cf=d(),jr=o("p"),yf=s("This class overrides "),Ha=o("a"),$f=s("TFRobertaForTokenClassification"),Ff=s(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Sn=d(),Mt=o("h2"),to=o("a"),Js=o("span"),F(Jr.$$.fragment),Mf=d(),Ys=o("span"),Pf=s("TFCamembertForQuestionAnswering"),On=d(),ze=o("div"),F(Yr.$$.fragment),Lf=d(),Pt=o("p"),Rf=s(`CamemBERT Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),Zs=o("code"),qf=s("span start logits"),Af=s(" and "),en=o("code"),Df=s("span end logits"),zf=s(")."),xf=d(),Zr=o("p"),If=s("This model inherits from "),Qa=o("a"),Sf=s("TFPreTrainedModel"),Of=s(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Bf=d(),ea=o("p"),Nf=s("This model is also a "),ta=o("a"),Uf=s("tf.keras.Model"),Hf=s(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Qf=d(),F(oo.$$.fragment),Vf=d(),oa=o("p"),Kf=s("This class overrides "),Va=o("a"),Wf=s("TFRobertaForQuestionAnswering"),Gf=s(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),this.h()},l(i){const f=Dg('[data-svelte="svelte-1phssyn"]',document.head);k=r(f,"META",{name:!0,content:!0}),f.forEach(t),re=m(i),U=r(i,"H1",{class:!0});var ra=a(U);_=r(ra,"A",{id:!0,class:!0,href:!0});var tn=a(_);A=r(tn,"SPAN",{});var on=a(A);M(G.$$.fragment,on),on.forEach(t),tn.forEach(t),le=m(ra),D=r(ra,"SPAN",{});var rn=a(D);de=n(rn,"CamemBERT"),rn.forEach(t),ra.forEach(t),Q=m(i),u=r(i,"H2",{class:!0});var aa=a(u);W=r(aa,"A",{id:!0,class:!0,href:!0});var an=a(W);z=r(an,"SPAN",{});var sn=a(z);M(X.$$.fragment,sn),sn.forEach(t),an.forEach(t),me=m(aa),x=r(aa,"SPAN",{});var Zf=a(x);ce=n(Zf,"Overview"),Zf.forEach(t),aa.forEach(t),ae=m(i),H=r(i,"P",{});var Nn=a(H);w=n(Nn,"The CamemBERT model was proposed in "),j=r(Nn,"A",{href:!0,rel:!0});var ep=a(j);V=n(ep,"CamemBERT: a Tasty French Language Model"),ep.forEach(t),g=n(Nn,` by
Louis Martin, Benjamin Muller, Pedro Javier Ortiz Su\xE1rez, Yoann Dupont, Laurent Romary, \xC9ric Villemonte de la
Clergerie, Djam\xE9 Seddah, and Beno\xEEt Sagot. It is based on Facebook\u2019s RoBERTa model released in 2019. It is a model
trained on 138GB of French text.`),Nn.forEach(t),b=m(i),Z=r(i,"P",{});var tp=a(Z);O=n(tp,"The abstract from the paper is the following:"),tp.forEach(t),se=m(i),ee=r(i,"P",{});var op=a(ee);I=r(op,"EM",{});var rp=a(I);he=n(rp,`Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available
models have either been trained on English data or on the concatenation of data in multiple languages. This makes
practical use of such models \u2014in all languages except English\u2014 very limited. Aiming to address this issue for French,
we release CamemBERT, a French version of the Bi-directional Encoders for Transformers (BERT). We measure the
performance of CamemBERT compared to multilingual models in multiple downstream tasks, namely part-of-speech tagging,
dependency parsing, named-entity recognition, and natural language inference. CamemBERT improves the state of the art
for most of the tasks considered. We release the pretrained model for CamemBERT hoping to foster research and
downstream applications for French NLP.`),rp.forEach(t),op.forEach(t),ne=m(i),v=r(i,"P",{});var ap=a(v);fe=n(ap,"Tips:"),ap.forEach(t),S=m(i),te=r(i,"UL",{});var sp=a(te);J=r(sp,"LI",{});var Un=a(J);B=n(Un,"This implementation is the same as RoBERTa. Refer to the "),oe=r(Un,"A",{href:!0});var np=a(oe);pe=n(np,"documentation of RoBERTa"),np.forEach(t),E=n(Un,` for usage examples
as well as the information relative to the inputs and outputs.`),Un.forEach(t),sp.forEach(t),ie=m(i),C=r(i,"P",{});var Ka=a(C);ue=n(Ka,"This model was contributed by "),l=r(Ka,"A",{href:!0,rel:!0});var ip=a(l);p=n(ip,"camembert"),ip.forEach(t),K=n(Ka,". The original code can be found "),be=r(Ka,"A",{href:!0,rel:!0});var lp=a(be);Ee=n(lp,"here"),lp.forEach(t),y=n(Ka,"."),Ka.forEach(t),Te=m(i),ge=r(i,"H2",{class:!0});var Hn=a(ge);ve=r(Hn,"A",{id:!0,class:!0,href:!0});var dp=a(ve);T=r(dp,"SPAN",{});var mp=a(T);M($.$$.fragment,mp),mp.forEach(t),dp.forEach(t),Ce=m(Hn),ke=r(Hn,"SPAN",{});var cp=a(ke);N=n(cp,"CamembertConfig"),cp.forEach(t),Hn.forEach(t),we=m(i),_e=r(i,"DIV",{class:!0});var Qn=a(_e);M(Y.$$.fragment,Qn),ye=m(Qn),ht=r(Qn,"P",{});var Wa=a(ht);il=n(Wa,"This class overrides "),na=r(Wa,"A",{href:!0});var hp=a(na);ll=n(hp,"RobertaConfig"),hp.forEach(t),dl=n(Wa,`. Please check the superclass for the appropriate documentation alongside
usage examples. Instantiating a configuration with the defaults will yield a similar configuration to that of the
Camembert `),ao=r(Wa,"A",{href:!0,rel:!0});var fp=a(ao);ml=n(fp,"camembert-base"),fp.forEach(t),cl=n(Wa," architecture."),Wa.forEach(t),Qn.forEach(t),ln=m(i),ft=r(i,"H2",{class:!0});var Vn=a(ft);Lt=r(Vn,"A",{id:!0,class:!0,href:!0});var pp=a(Lt);Ja=r(pp,"SPAN",{});var up=a(Ja);M(so.$$.fragment,up),up.forEach(t),pp.forEach(t),hl=m(Vn),Ya=r(Vn,"SPAN",{});var gp=a(Ya);fl=n(gp,"CamembertTokenizer"),gp.forEach(t),Vn.forEach(t),dn=m(i),Fe=r(i,"DIV",{class:!0});var Ve=a(Fe);M(no.$$.fragment,Ve),pl=m(Ve),et=r(Ve,"P",{});var ro=a(et);ul=n(ro,"Adapted from "),ia=r(ro,"A",{href:!0});var _p=a(ia);gl=n(_p,"RobertaTokenizer"),_p.forEach(t),_l=n(ro," and "),la=r(ro,"A",{href:!0});var vp=a(la);vl=n(vp,"XLNetTokenizer"),vp.forEach(t),bl=n(ro,`. Construct a CamemBERT tokenizer. Based on
`),io=r(ro,"A",{href:!0,rel:!0});var bp=a(io);kl=n(bp,"SentencePiece"),bp.forEach(t),Tl=n(ro,"."),ro.forEach(t),wl=m(Ve),lo=r(Ve,"P",{});var Kn=a(lo);El=n(Kn,"This tokenizer inherits from "),da=r(Kn,"A",{href:!0});var kp=a(da);Cl=n(kp,"PreTrainedTokenizer"),kp.forEach(t),yl=n(Kn,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Kn.forEach(t),$l=m(Ve),tt=r(Ve,"DIV",{class:!0});var Ga=a(tt);M(mo.$$.fragment,Ga),Fl=m(Ga),Za=r(Ga,"P",{});var Tp=a(Za);Ml=n(Tp,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An CamemBERT sequence has the following format:`),Tp.forEach(t),Pl=m(Ga),co=r(Ga,"UL",{});var Wn=a(co);ma=r(Wn,"LI",{});var Xf=a(ma);Ll=n(Xf,"single sequence: "),es=r(Xf,"CODE",{});var wp=a(es);Rl=n(wp,"<s> X </s>"),wp.forEach(t),Xf.forEach(t),ql=m(Wn),ca=r(Wn,"LI",{});var jf=a(ca);Al=n(jf,"pair of sequences: "),ts=r(jf,"CODE",{});var Ep=a(ts);Dl=n(Ep,"<s> A </s></s> B </s>"),Ep.forEach(t),jf.forEach(t),Wn.forEach(t),Ga.forEach(t),zl=m(Ve),Rt=r(Ve,"DIV",{class:!0});var Gn=a(Rt);M(ho.$$.fragment,Gn),xl=m(Gn),fo=r(Gn,"P",{});var Xn=a(fo);Il=n(Xn,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),os=r(Xn,"CODE",{});var Cp=a(os);Sl=n(Cp,"prepare_for_model"),Cp.forEach(t),Ol=n(Xn," method."),Xn.forEach(t),Gn.forEach(t),Bl=m(Ve),qt=r(Ve,"DIV",{class:!0});var jn=a(qt);M(po.$$.fragment,jn),Nl=m(jn),rs=r(jn,"P",{});var yp=a(rs);Ul=n(yp,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. CamemBERT, like
RoBERTa, does not make use of token type ids, therefore a list of zeros is returned.`),yp.forEach(t),jn.forEach(t),Hl=m(Ve),ha=r(Ve,"DIV",{class:!0});var $p=a(ha);M(uo.$$.fragment,$p),$p.forEach(t),Ve.forEach(t),mn=m(i),pt=r(i,"H2",{class:!0});var Jn=a(pt);At=r(Jn,"A",{id:!0,class:!0,href:!0});var Fp=a(At);as=r(Fp,"SPAN",{});var Mp=a(as);M(go.$$.fragment,Mp),Mp.forEach(t),Fp.forEach(t),Ql=m(Jn),ss=r(Jn,"SPAN",{});var Pp=a(ss);Vl=n(Pp,"CamembertTokenizerFast"),Pp.forEach(t),Jn.forEach(t),cn=m(i),Ie=r(i,"DIV",{class:!0});var rt=a(Ie);M(_o.$$.fragment,rt),Kl=m(rt),Ke=r(rt,"P",{});var at=a(Ke);Wl=n(at,"Construct a \u201Cfast\u201D CamemBERT tokenizer (backed by HuggingFace\u2019s "),ns=r(at,"EM",{});var Lp=a(ns);Gl=n(Lp,"tokenizers"),Lp.forEach(t),Xl=n(at,` library). Adapted from
`),fa=r(at,"A",{href:!0});var Rp=a(fa);jl=n(Rp,"RobertaTokenizer"),Rp.forEach(t),Jl=n(at," and "),pa=r(at,"A",{href:!0});var qp=a(pa);Yl=n(qp,"XLNetTokenizer"),qp.forEach(t),Zl=n(at,`. Based on
`),vo=r(at,"A",{href:!0,rel:!0});var Ap=a(vo);ed=n(Ap,"BPE"),Ap.forEach(t),td=n(at,"."),at.forEach(t),od=m(rt),bo=r(rt,"P",{});var Yn=a(bo);rd=n(Yn,"This tokenizer inherits from "),ua=r(Yn,"A",{href:!0});var Dp=a(ua);ad=n(Dp,"PreTrainedTokenizerFast"),Dp.forEach(t),sd=n(Yn,` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),Yn.forEach(t),nd=m(rt),ot=r(rt,"DIV",{class:!0});var Xa=a(ot);M(ko.$$.fragment,Xa),id=m(Xa),is=r(Xa,"P",{});var zp=a(is);ld=n(zp,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An CamemBERT sequence has the following format:`),zp.forEach(t),dd=m(Xa),To=r(Xa,"UL",{});var Zn=a(To);ga=r(Zn,"LI",{});var Jf=a(ga);md=n(Jf,"single sequence: "),ls=r(Jf,"CODE",{});var xp=a(ls);cd=n(xp,"<s> X </s>"),xp.forEach(t),Jf.forEach(t),hd=m(Zn),_a=r(Zn,"LI",{});var Yf=a(_a);fd=n(Yf,"pair of sequences: "),ds=r(Yf,"CODE",{});var Ip=a(ds);pd=n(Ip,"<s> A </s></s> B </s>"),Ip.forEach(t),Yf.forEach(t),Zn.forEach(t),Xa.forEach(t),ud=m(rt),Dt=r(rt,"DIV",{class:!0});var ei=a(Dt);M(wo.$$.fragment,ei),gd=m(ei),ms=r(ei,"P",{});var Sp=a(ms);_d=n(Sp,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. CamemBERT, like
RoBERTa, does not make use of token type ids, therefore a list of zeros is returned.`),Sp.forEach(t),ei.forEach(t),rt.forEach(t),hn=m(i),ut=r(i,"H2",{class:!0});var ti=a(ut);zt=r(ti,"A",{id:!0,class:!0,href:!0});var Op=a(zt);cs=r(Op,"SPAN",{});var Bp=a(cs);M(Eo.$$.fragment,Bp),Bp.forEach(t),Op.forEach(t),vd=m(ti),hs=r(ti,"SPAN",{});var Np=a(hs);bd=n(Np,"CamembertModel"),Np.forEach(t),ti.forEach(t),fn=m(i),Se=r(i,"DIV",{class:!0});var st=a(Se);M(Co.$$.fragment,st),kd=m(st),fs=r(st,"P",{});var Up=a(fs);Td=n(Up,"The bare CamemBERT Model transformer outputting raw hidden-states without any specific head on top."),Up.forEach(t),wd=m(st),yo=r(st,"P",{});var oi=a(yo);Ed=n(oi,"This model inherits from "),va=r(oi,"A",{href:!0});var Hp=a(va);Cd=n(Hp,"PreTrainedModel"),Hp.forEach(t),yd=n(oi,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),oi.forEach(t),$d=m(st),$o=r(st,"P",{});var ri=a($o);Fd=n(ri,"This model is also a PyTorch "),Fo=r(ri,"A",{href:!0,rel:!0});var Qp=a(Fo);Md=n(Qp,"torch.nn.Module"),Qp.forEach(t),Pd=n(ri,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ri.forEach(t),Ld=m(st),Mo=r(st,"P",{});var ai=a(Mo);Rd=n(ai,"This class overrides "),ba=r(ai,"A",{href:!0});var Vp=a(ba);qd=n(Vp,"RobertaModel"),Vp.forEach(t),Ad=n(ai,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),ai.forEach(t),st.forEach(t),pn=m(i),gt=r(i,"H2",{class:!0});var si=a(gt);xt=r(si,"A",{id:!0,class:!0,href:!0});var Kp=a(xt);ps=r(Kp,"SPAN",{});var Wp=a(ps);M(Po.$$.fragment,Wp),Wp.forEach(t),Kp.forEach(t),Dd=m(si),us=r(si,"SPAN",{});var Gp=a(us);zd=n(Gp,"CamembertForCausalLM"),Gp.forEach(t),si.forEach(t),un=m(i),Oe=r(i,"DIV",{class:!0});var nt=a(Oe);M(Lo.$$.fragment,nt),xd=m(nt),Ro=r(nt,"P",{});var ni=a(Ro);Id=n(ni,"CamemBERT Model with a "),gs=r(ni,"CODE",{});var Xp=a(gs);Sd=n(Xp,"language modeling"),Xp.forEach(t),Od=n(ni," head on top for CLM fine-tuning."),ni.forEach(t),Bd=m(nt),qo=r(nt,"P",{});var ii=a(qo);Nd=n(ii,"This model inherits from "),ka=r(ii,"A",{href:!0});var jp=a(ka);Ud=n(jp,"PreTrainedModel"),jp.forEach(t),Hd=n(ii,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ii.forEach(t),Qd=m(nt),Ao=r(nt,"P",{});var li=a(Ao);Vd=n(li,"This model is also a PyTorch "),Do=r(li,"A",{href:!0,rel:!0});var Jp=a(Do);Kd=n(Jp,"torch.nn.Module"),Jp.forEach(t),Wd=n(li,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),li.forEach(t),Gd=m(nt),zo=r(nt,"P",{});var di=a(zo);Xd=n(di,"This class overrides "),Ta=r(di,"A",{href:!0});var Yp=a(Ta);jd=n(Yp,"RobertaForCausalLM"),Yp.forEach(t),Jd=n(di,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),di.forEach(t),nt.forEach(t),gn=m(i),_t=r(i,"H2",{class:!0});var mi=a(_t);It=r(mi,"A",{id:!0,class:!0,href:!0});var Zp=a(It);_s=r(Zp,"SPAN",{});var eu=a(_s);M(xo.$$.fragment,eu),eu.forEach(t),Zp.forEach(t),Yd=m(mi),vs=r(mi,"SPAN",{});var tu=a(vs);Zd=n(tu,"CamembertForMaskedLM"),tu.forEach(t),mi.forEach(t),_n=m(i),Be=r(i,"DIV",{class:!0});var it=a(Be);M(Io.$$.fragment,it),em=m(it),So=r(it,"P",{});var ci=a(So);tm=n(ci,"CamemBERT Model with a "),bs=r(ci,"CODE",{});var ou=a(bs);om=n(ou,"language modeling"),ou.forEach(t),rm=n(ci," head on top."),ci.forEach(t),am=m(it),Oo=r(it,"P",{});var hi=a(Oo);sm=n(hi,"This model inherits from "),wa=r(hi,"A",{href:!0});var ru=a(wa);nm=n(ru,"PreTrainedModel"),ru.forEach(t),im=n(hi,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),hi.forEach(t),lm=m(it),Bo=r(it,"P",{});var fi=a(Bo);dm=n(fi,"This model is also a PyTorch "),No=r(fi,"A",{href:!0,rel:!0});var au=a(No);mm=n(au,"torch.nn.Module"),au.forEach(t),cm=n(fi,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),fi.forEach(t),hm=m(it),Uo=r(it,"P",{});var pi=a(Uo);fm=n(pi,"This class overrides "),Ea=r(pi,"A",{href:!0});var su=a(Ea);pm=n(su,"RobertaForMaskedLM"),su.forEach(t),um=n(pi,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),pi.forEach(t),it.forEach(t),vn=m(i),vt=r(i,"H2",{class:!0});var ui=a(vt);St=r(ui,"A",{id:!0,class:!0,href:!0});var nu=a(St);ks=r(nu,"SPAN",{});var iu=a(ks);M(Ho.$$.fragment,iu),iu.forEach(t),nu.forEach(t),gm=m(ui),Ts=r(ui,"SPAN",{});var lu=a(Ts);_m=n(lu,"CamembertForSequenceClassification"),lu.forEach(t),ui.forEach(t),bn=m(i),Ne=r(i,"DIV",{class:!0});var lt=a(Ne);M(Qo.$$.fragment,lt),vm=m(lt),ws=r(lt,"P",{});var du=a(ws);bm=n(du,`CamemBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),du.forEach(t),km=m(lt),Vo=r(lt,"P",{});var gi=a(Vo);Tm=n(gi,"This model inherits from "),Ca=r(gi,"A",{href:!0});var mu=a(Ca);wm=n(mu,"PreTrainedModel"),mu.forEach(t),Em=n(gi,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),gi.forEach(t),Cm=m(lt),Ko=r(lt,"P",{});var _i=a(Ko);ym=n(_i,"This model is also a PyTorch "),Wo=r(_i,"A",{href:!0,rel:!0});var cu=a(Wo);$m=n(cu,"torch.nn.Module"),cu.forEach(t),Fm=n(_i,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),_i.forEach(t),Mm=m(lt),Go=r(lt,"P",{});var vi=a(Go);Pm=n(vi,"This class overrides "),ya=r(vi,"A",{href:!0});var hu=a(ya);Lm=n(hu,"RobertaForSequenceClassification"),hu.forEach(t),Rm=n(vi,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),vi.forEach(t),lt.forEach(t),kn=m(i),bt=r(i,"H2",{class:!0});var bi=a(bt);Ot=r(bi,"A",{id:!0,class:!0,href:!0});var fu=a(Ot);Es=r(fu,"SPAN",{});var pu=a(Es);M(Xo.$$.fragment,pu),pu.forEach(t),fu.forEach(t),qm=m(bi),Cs=r(bi,"SPAN",{});var uu=a(Cs);Am=n(uu,"CamembertForMultipleChoice"),uu.forEach(t),bi.forEach(t),Tn=m(i),Ue=r(i,"DIV",{class:!0});var dt=a(Ue);M(jo.$$.fragment,dt),Dm=m(dt),ys=r(dt,"P",{});var gu=a(ys);zm=n(gu,`CamemBERT Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),gu.forEach(t),xm=m(dt),Jo=r(dt,"P",{});var ki=a(Jo);Im=n(ki,"This model inherits from "),$a=r(ki,"A",{href:!0});var _u=a($a);Sm=n(_u,"PreTrainedModel"),_u.forEach(t),Om=n(ki,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ki.forEach(t),Bm=m(dt),Yo=r(dt,"P",{});var Ti=a(Yo);Nm=n(Ti,"This model is also a PyTorch "),Zo=r(Ti,"A",{href:!0,rel:!0});var vu=a(Zo);Um=n(vu,"torch.nn.Module"),vu.forEach(t),Hm=n(Ti,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ti.forEach(t),Qm=m(dt),er=r(dt,"P",{});var wi=a(er);Vm=n(wi,"This class overrides "),Fa=r(wi,"A",{href:!0});var bu=a(Fa);Km=n(bu,"RobertaForMultipleChoice"),bu.forEach(t),Wm=n(wi,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),wi.forEach(t),dt.forEach(t),wn=m(i),kt=r(i,"H2",{class:!0});var Ei=a(kt);Bt=r(Ei,"A",{id:!0,class:!0,href:!0});var ku=a(Bt);$s=r(ku,"SPAN",{});var Tu=a($s);M(tr.$$.fragment,Tu),Tu.forEach(t),ku.forEach(t),Gm=m(Ei),Fs=r(Ei,"SPAN",{});var wu=a(Fs);Xm=n(wu,"CamembertForTokenClassification"),wu.forEach(t),Ei.forEach(t),En=m(i),He=r(i,"DIV",{class:!0});var mt=a(He);M(or.$$.fragment,mt),jm=m(mt),Ms=r(mt,"P",{});var Eu=a(Ms);Jm=n(Eu,`CamemBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),Eu.forEach(t),Ym=m(mt),rr=r(mt,"P",{});var Ci=a(rr);Zm=n(Ci,"This model inherits from "),Ma=r(Ci,"A",{href:!0});var Cu=a(Ma);ec=n(Cu,"PreTrainedModel"),Cu.forEach(t),tc=n(Ci,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ci.forEach(t),oc=m(mt),ar=r(mt,"P",{});var yi=a(ar);rc=n(yi,"This model is also a PyTorch "),sr=r(yi,"A",{href:!0,rel:!0});var yu=a(sr);ac=n(yu,"torch.nn.Module"),yu.forEach(t),sc=n(yi,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),yi.forEach(t),nc=m(mt),nr=r(mt,"P",{});var $i=a(nr);ic=n($i,"This class overrides "),Pa=r($i,"A",{href:!0});var $u=a(Pa);lc=n($u,"RobertaForTokenClassification"),$u.forEach(t),dc=n($i,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),$i.forEach(t),mt.forEach(t),Cn=m(i),Tt=r(i,"H2",{class:!0});var Fi=a(Tt);Nt=r(Fi,"A",{id:!0,class:!0,href:!0});var Fu=a(Nt);Ps=r(Fu,"SPAN",{});var Mu=a(Ps);M(ir.$$.fragment,Mu),Mu.forEach(t),Fu.forEach(t),mc=m(Fi),Ls=r(Fi,"SPAN",{});var Pu=a(Ls);cc=n(Pu,"CamembertForQuestionAnswering"),Pu.forEach(t),Fi.forEach(t),yn=m(i),Qe=r(i,"DIV",{class:!0});var ct=a(Qe);M(lr.$$.fragment,ct),hc=m(ct),Ut=r(ct,"P",{});var nn=a(Ut);fc=n(nn,`CamemBERT Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),Rs=r(nn,"CODE",{});var Lu=a(Rs);pc=n(Lu,"span start logits"),Lu.forEach(t),uc=n(nn," and "),qs=r(nn,"CODE",{});var Ru=a(qs);gc=n(Ru,"span end logits"),Ru.forEach(t),nn.forEach(t),_c=m(ct),dr=r(ct,"P",{});var Mi=a(dr);vc=n(Mi,"This model inherits from "),La=r(Mi,"A",{href:!0});var qu=a(La);bc=n(qu,"PreTrainedModel"),qu.forEach(t),kc=n(Mi,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Mi.forEach(t),Tc=m(ct),mr=r(ct,"P",{});var Pi=a(mr);wc=n(Pi,"This model is also a PyTorch "),cr=r(Pi,"A",{href:!0,rel:!0});var Au=a(cr);Ec=n(Au,"torch.nn.Module"),Au.forEach(t),Cc=n(Pi,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Pi.forEach(t),yc=m(ct),hr=r(ct,"P",{});var Li=a(hr);$c=n(Li,"This class overrides "),Ra=r(Li,"A",{href:!0});var Du=a(Ra);Fc=n(Du,"RobertaForQuestionAnswering"),Du.forEach(t),Mc=n(Li,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Li.forEach(t),ct.forEach(t),$n=m(i),wt=r(i,"H2",{class:!0});var Ri=a(wt);Ht=r(Ri,"A",{id:!0,class:!0,href:!0});var zu=a(Ht);As=r(zu,"SPAN",{});var xu=a(As);M(fr.$$.fragment,xu),xu.forEach(t),zu.forEach(t),Pc=m(Ri),Ds=r(Ri,"SPAN",{});var Iu=a(Ds);Lc=n(Iu,"TFCamembertModel"),Iu.forEach(t),Ri.forEach(t),Fn=m(i),Pe=r(i,"DIV",{class:!0});var We=a(Pe);M(pr.$$.fragment,We),Rc=m(We),zs=r(We,"P",{});var Su=a(zs);qc=n(Su,"The bare CamemBERT Model transformer outputting raw hidden-states without any specific head on top."),Su.forEach(t),Ac=m(We),ur=r(We,"P",{});var qi=a(ur);Dc=n(qi,"This model inherits from "),qa=r(qi,"A",{href:!0});var Ou=a(qa);zc=n(Ou,"TFPreTrainedModel"),Ou.forEach(t),xc=n(qi,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),qi.forEach(t),Ic=m(We),gr=r(We,"P",{});var Ai=a(gr);Sc=n(Ai,"This model is also a "),_r=r(Ai,"A",{href:!0,rel:!0});var Bu=a(_r);Oc=n(Bu,"tf.keras.Model"),Bu.forEach(t),Bc=n(Ai,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Ai.forEach(t),Nc=m(We),M(Qt.$$.fragment,We),Uc=m(We),vr=r(We,"P",{});var Di=a(vr);Hc=n(Di,"This class overrides "),Aa=r(Di,"A",{href:!0});var Nu=a(Aa);Qc=n(Nu,"TFRobertaModel"),Nu.forEach(t),Vc=n(Di,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Di.forEach(t),We.forEach(t),Mn=m(i),Et=r(i,"H2",{class:!0});var zi=a(Et);Vt=r(zi,"A",{id:!0,class:!0,href:!0});var Uu=a(Vt);xs=r(Uu,"SPAN",{});var Hu=a(xs);M(br.$$.fragment,Hu),Hu.forEach(t),Uu.forEach(t),Kc=m(zi),Is=r(zi,"SPAN",{});var Qu=a(Is);Wc=n(Qu,"TFCamembertForCasualLM"),Qu.forEach(t),zi.forEach(t),Pn=m(i),Le=r(i,"DIV",{class:!0});var Ge=a(Le);M(kr.$$.fragment,Ge),Gc=m(Ge),Tr=r(Ge,"P",{});var xi=a(Tr);Xc=n(xi,"CamemBERT Model with a "),Ss=r(xi,"CODE",{});var Vu=a(Ss);jc=n(Vu,"language modeling"),Vu.forEach(t),Jc=n(xi," head on top for CLM fine-tuning."),xi.forEach(t),Yc=m(Ge),wr=r(Ge,"P",{});var Ii=a(wr);Zc=n(Ii,"This model inherits from "),Da=r(Ii,"A",{href:!0});var Ku=a(Da);eh=n(Ku,"TFPreTrainedModel"),Ku.forEach(t),th=n(Ii,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ii.forEach(t),oh=m(Ge),Er=r(Ge,"P",{});var Si=a(Er);rh=n(Si,"This model is also a "),Cr=r(Si,"A",{href:!0,rel:!0});var Wu=a(Cr);ah=n(Wu,"tf.keras.Model"),Wu.forEach(t),sh=n(Si,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Si.forEach(t),nh=m(Ge),M(Kt.$$.fragment,Ge),ih=m(Ge),yr=r(Ge,"P",{});var Oi=a(yr);lh=n(Oi,"This class overrides "),za=r(Oi,"A",{href:!0});var Gu=a(za);dh=n(Gu,"TFRobertaForCausalLM"),Gu.forEach(t),mh=n(Oi,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Oi.forEach(t),Ge.forEach(t),Ln=m(i),Ct=r(i,"H2",{class:!0});var Bi=a(Ct);Wt=r(Bi,"A",{id:!0,class:!0,href:!0});var Xu=a(Wt);Os=r(Xu,"SPAN",{});var ju=a(Os);M($r.$$.fragment,ju),ju.forEach(t),Xu.forEach(t),ch=m(Bi),Bs=r(Bi,"SPAN",{});var Ju=a(Bs);hh=n(Ju,"TFCamembertForMaskedLM"),Ju.forEach(t),Bi.forEach(t),Rn=m(i),Re=r(i,"DIV",{class:!0});var Xe=a(Re);M(Fr.$$.fragment,Xe),fh=m(Xe),Mr=r(Xe,"P",{});var Ni=a(Mr);ph=n(Ni,"CamemBERT Model with a "),Ns=r(Ni,"CODE",{});var Yu=a(Ns);uh=n(Yu,"language modeling"),Yu.forEach(t),gh=n(Ni," head on top."),Ni.forEach(t),_h=m(Xe),Pr=r(Xe,"P",{});var Ui=a(Pr);vh=n(Ui,"This model inherits from "),xa=r(Ui,"A",{href:!0});var Zu=a(xa);bh=n(Zu,"TFPreTrainedModel"),Zu.forEach(t),kh=n(Ui,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ui.forEach(t),Th=m(Xe),Lr=r(Xe,"P",{});var Hi=a(Lr);wh=n(Hi,"This model is also a "),Rr=r(Hi,"A",{href:!0,rel:!0});var eg=a(Rr);Eh=n(eg,"tf.keras.Model"),eg.forEach(t),Ch=n(Hi,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Hi.forEach(t),yh=m(Xe),M(Gt.$$.fragment,Xe),$h=m(Xe),qr=r(Xe,"P",{});var Qi=a(qr);Fh=n(Qi,"This class overrides "),Ia=r(Qi,"A",{href:!0});var tg=a(Ia);Mh=n(tg,"TFRobertaForMaskedLM"),tg.forEach(t),Ph=n(Qi,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Qi.forEach(t),Xe.forEach(t),qn=m(i),yt=r(i,"H2",{class:!0});var Vi=a(yt);Xt=r(Vi,"A",{id:!0,class:!0,href:!0});var og=a(Xt);Us=r(og,"SPAN",{});var rg=a(Us);M(Ar.$$.fragment,rg),rg.forEach(t),og.forEach(t),Lh=m(Vi),Hs=r(Vi,"SPAN",{});var ag=a(Hs);Rh=n(ag,"TFCamembertForSequenceClassification"),ag.forEach(t),Vi.forEach(t),An=m(i),qe=r(i,"DIV",{class:!0});var je=a(qe);M(Dr.$$.fragment,je),qh=m(je),Qs=r(je,"P",{});var sg=a(Qs);Ah=n(sg,`CamemBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),sg.forEach(t),Dh=m(je),zr=r(je,"P",{});var Ki=a(zr);zh=n(Ki,"This model inherits from "),Sa=r(Ki,"A",{href:!0});var ng=a(Sa);xh=n(ng,"TFPreTrainedModel"),ng.forEach(t),Ih=n(Ki,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ki.forEach(t),Sh=m(je),xr=r(je,"P",{});var Wi=a(xr);Oh=n(Wi,"This model is also a "),Ir=r(Wi,"A",{href:!0,rel:!0});var ig=a(Ir);Bh=n(ig,"tf.keras.Model"),ig.forEach(t),Nh=n(Wi,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Wi.forEach(t),Uh=m(je),M(jt.$$.fragment,je),Hh=m(je),Sr=r(je,"P",{});var Gi=a(Sr);Qh=n(Gi,"This class overrides "),Oa=r(Gi,"A",{href:!0});var lg=a(Oa);Vh=n(lg,"TFRobertaForSequenceClassification"),lg.forEach(t),Kh=n(Gi,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Gi.forEach(t),je.forEach(t),Dn=m(i),$t=r(i,"H2",{class:!0});var Xi=a($t);Jt=r(Xi,"A",{id:!0,class:!0,href:!0});var dg=a(Jt);Vs=r(dg,"SPAN",{});var mg=a(Vs);M(Or.$$.fragment,mg),mg.forEach(t),dg.forEach(t),Wh=m(Xi),Ks=r(Xi,"SPAN",{});var cg=a(Ks);Gh=n(cg,"TFCamembertForMultipleChoice"),cg.forEach(t),Xi.forEach(t),zn=m(i),Ae=r(i,"DIV",{class:!0});var Je=a(Ae);M(Br.$$.fragment,Je),Xh=m(Je),Ws=r(Je,"P",{});var hg=a(Ws);jh=n(hg,`CamemBERT Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),hg.forEach(t),Jh=m(Je),Nr=r(Je,"P",{});var ji=a(Nr);Yh=n(ji,"This model inherits from "),Ba=r(ji,"A",{href:!0});var fg=a(Ba);Zh=n(fg,"TFPreTrainedModel"),fg.forEach(t),ef=n(ji,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ji.forEach(t),tf=m(Je),Ur=r(Je,"P",{});var Ji=a(Ur);of=n(Ji,"This model is also a "),Hr=r(Ji,"A",{href:!0,rel:!0});var pg=a(Hr);rf=n(pg,"tf.keras.Model"),pg.forEach(t),af=n(Ji,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Ji.forEach(t),sf=m(Je),M(Yt.$$.fragment,Je),nf=m(Je),Qr=r(Je,"P",{});var Yi=a(Qr);lf=n(Yi,"This class overrides "),Na=r(Yi,"A",{href:!0});var ug=a(Na);df=n(ug,"TFRobertaForMultipleChoice"),ug.forEach(t),mf=n(Yi,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Yi.forEach(t),Je.forEach(t),xn=m(i),Ft=r(i,"H2",{class:!0});var Zi=a(Ft);Zt=r(Zi,"A",{id:!0,class:!0,href:!0});var gg=a(Zt);Gs=r(gg,"SPAN",{});var _g=a(Gs);M(Vr.$$.fragment,_g),_g.forEach(t),gg.forEach(t),cf=m(Zi),Xs=r(Zi,"SPAN",{});var vg=a(Xs);hf=n(vg,"TFCamembertForTokenClassification"),vg.forEach(t),Zi.forEach(t),In=m(i),De=r(i,"DIV",{class:!0});var Ye=a(De);M(Kr.$$.fragment,Ye),ff=m(Ye),js=r(Ye,"P",{});var bg=a(js);pf=n(bg,`CamemBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),bg.forEach(t),uf=m(Ye),Wr=r(Ye,"P",{});var el=a(Wr);gf=n(el,"This model inherits from "),Ua=r(el,"A",{href:!0});var kg=a(Ua);_f=n(kg,"TFPreTrainedModel"),kg.forEach(t),vf=n(el,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),el.forEach(t),bf=m(Ye),Gr=r(Ye,"P",{});var tl=a(Gr);kf=n(tl,"This model is also a "),Xr=r(tl,"A",{href:!0,rel:!0});var Tg=a(Xr);Tf=n(Tg,"tf.keras.Model"),Tg.forEach(t),wf=n(tl,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),tl.forEach(t),Ef=m(Ye),M(eo.$$.fragment,Ye),Cf=m(Ye),jr=r(Ye,"P",{});var ol=a(jr);yf=n(ol,"This class overrides "),Ha=r(ol,"A",{href:!0});var wg=a(Ha);$f=n(wg,"TFRobertaForTokenClassification"),wg.forEach(t),Ff=n(ol,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),ol.forEach(t),Ye.forEach(t),Sn=m(i),Mt=r(i,"H2",{class:!0});var rl=a(Mt);to=r(rl,"A",{id:!0,class:!0,href:!0});var Eg=a(to);Js=r(Eg,"SPAN",{});var Cg=a(Js);M(Jr.$$.fragment,Cg),Cg.forEach(t),Eg.forEach(t),Mf=m(rl),Ys=r(rl,"SPAN",{});var yg=a(Ys);Pf=n(yg,"TFCamembertForQuestionAnswering"),yg.forEach(t),rl.forEach(t),On=m(i),ze=r(i,"DIV",{class:!0});var Ze=a(ze);M(Yr.$$.fragment,Ze),Lf=m(Ze),Pt=r(Ze,"P",{});var ja=a(Pt);Rf=n(ja,`CamemBERT Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),Zs=r(ja,"CODE",{});var $g=a(Zs);qf=n($g,"span start logits"),$g.forEach(t),Af=n(ja," and "),en=r(ja,"CODE",{});var Fg=a(en);Df=n(Fg,"span end logits"),Fg.forEach(t),zf=n(ja,")."),ja.forEach(t),xf=m(Ze),Zr=r(Ze,"P",{});var al=a(Zr);If=n(al,"This model inherits from "),Qa=r(al,"A",{href:!0});var Mg=a(Qa);Sf=n(Mg,"TFPreTrainedModel"),Mg.forEach(t),Of=n(al,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),al.forEach(t),Bf=m(Ze),ea=r(Ze,"P",{});var sl=a(ea);Nf=n(sl,"This model is also a "),ta=r(sl,"A",{href:!0,rel:!0});var Pg=a(ta);Uf=n(Pg,"tf.keras.Model"),Pg.forEach(t),Hf=n(sl,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),sl.forEach(t),Qf=m(Ze),M(oo.$$.fragment,Ze),Vf=m(Ze),oa=r(Ze,"P",{});var nl=a(oa);Kf=n(nl,"This class overrides "),Va=r(nl,"A",{href:!0});var Lg=a(Va);Wf=n(Lg,"TFRobertaForQuestionAnswering"),Lg.forEach(t),Gf=n(nl,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),nl.forEach(t),Ze.forEach(t),this.h()},h(){c(k,"name","hf:doc:metadata"),c(k,"content",JSON.stringify(Qg)),c(_,"id","camembert"),c(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_,"href","#camembert"),c(U,"class","relative group"),c(W,"id","overview"),c(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(W,"href","#overview"),c(u,"class","relative group"),c(j,"href","https://arxiv.org/abs/1911.03894"),c(j,"rel","nofollow"),c(oe,"href","roberta"),c(l,"href","https://huggingface.co/camembert"),c(l,"rel","nofollow"),c(be,"href","https://camembert-model.fr/"),c(be,"rel","nofollow"),c(ve,"id","transformers.CamembertConfig"),c(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ve,"href","#transformers.CamembertConfig"),c(ge,"class","relative group"),c(na,"href","/docs/transformers/v4.21.1/en/model_doc/roberta#transformers.RobertaConfig"),c(ao,"href","https://huggingface.co/camembert-base"),c(ao,"rel","nofollow"),c(_e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Lt,"id","transformers.CamembertTokenizer"),c(Lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Lt,"href","#transformers.CamembertTokenizer"),c(ft,"class","relative group"),c(ia,"href","/docs/transformers/v4.21.1/en/model_doc/roberta#transformers.RobertaTokenizer"),c(la,"href","/docs/transformers/v4.21.1/en/model_doc/xlnet#transformers.XLNetTokenizer"),c(io,"href","https://github.com/google/sentencepiece"),c(io,"rel","nofollow"),c(da,"href","/docs/transformers/v4.21.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(tt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Rt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(qt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(ha,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(At,"id","transformers.CamembertTokenizerFast"),c(At,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(At,"href","#transformers.CamembertTokenizerFast"),c(pt,"class","relative group"),c(fa,"href","/docs/transformers/v4.21.1/en/model_doc/roberta#transformers.RobertaTokenizer"),c(pa,"href","/docs/transformers/v4.21.1/en/model_doc/xlnet#transformers.XLNetTokenizer"),c(vo,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models"),c(vo,"rel","nofollow"),c(ua,"href","/docs/transformers/v4.21.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),c(ot,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Dt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(zt,"id","transformers.CamembertModel"),c(zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zt,"href","#transformers.CamembertModel"),c(ut,"class","relative group"),c(va,"href","/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel"),c(Fo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Fo,"rel","nofollow"),c(ba,"href","/docs/transformers/v4.21.1/en/model_doc/roberta#transformers.RobertaModel"),c(Se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(xt,"id","transformers.CamembertForCausalLM"),c(xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(xt,"href","#transformers.CamembertForCausalLM"),c(gt,"class","relative group"),c(ka,"href","/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel"),c(Do,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Do,"rel","nofollow"),c(Ta,"href","/docs/transformers/v4.21.1/en/model_doc/roberta#transformers.RobertaForCausalLM"),c(Oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(It,"id","transformers.CamembertForMaskedLM"),c(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(It,"href","#transformers.CamembertForMaskedLM"),c(_t,"class","relative group"),c(wa,"href","/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel"),c(No,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(No,"rel","nofollow"),c(Ea,"href","/docs/transformers/v4.21.1/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(Be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(St,"id","transformers.CamembertForSequenceClassification"),c(St,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(St,"href","#transformers.CamembertForSequenceClassification"),c(vt,"class","relative group"),c(Ca,"href","/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel"),c(Wo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Wo,"rel","nofollow"),c(ya,"href","/docs/transformers/v4.21.1/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),c(Ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Ot,"id","transformers.CamembertForMultipleChoice"),c(Ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ot,"href","#transformers.CamembertForMultipleChoice"),c(bt,"class","relative group"),c($a,"href","/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel"),c(Zo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Zo,"rel","nofollow"),c(Fa,"href","/docs/transformers/v4.21.1/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),c(Ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Bt,"id","transformers.CamembertForTokenClassification"),c(Bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bt,"href","#transformers.CamembertForTokenClassification"),c(kt,"class","relative group"),c(Ma,"href","/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel"),c(sr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(sr,"rel","nofollow"),c(Pa,"href","/docs/transformers/v4.21.1/en/model_doc/roberta#transformers.RobertaForTokenClassification"),c(He,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Nt,"id","transformers.CamembertForQuestionAnswering"),c(Nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Nt,"href","#transformers.CamembertForQuestionAnswering"),c(Tt,"class","relative group"),c(La,"href","/docs/transformers/v4.21.1/en/main_classes/model#transformers.PreTrainedModel"),c(cr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(cr,"rel","nofollow"),c(Ra,"href","/docs/transformers/v4.21.1/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),c(Qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Ht,"id","transformers.TFCamembertModel"),c(Ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ht,"href","#transformers.TFCamembertModel"),c(wt,"class","relative group"),c(qa,"href","/docs/transformers/v4.21.1/en/main_classes/model#transformers.TFPreTrainedModel"),c(_r,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(_r,"rel","nofollow"),c(Aa,"href","/docs/transformers/v4.21.1/en/model_doc/roberta#transformers.TFRobertaModel"),c(Pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Vt,"id","transformers.TFCamembertForCausalLM"),c(Vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Vt,"href","#transformers.TFCamembertForCausalLM"),c(Et,"class","relative group"),c(Da,"href","/docs/transformers/v4.21.1/en/main_classes/model#transformers.TFPreTrainedModel"),c(Cr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Cr,"rel","nofollow"),c(za,"href","/docs/transformers/v4.21.1/en/model_doc/roberta#transformers.TFRobertaForCausalLM"),c(Le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Wt,"id","transformers.TFCamembertForMaskedLM"),c(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Wt,"href","#transformers.TFCamembertForMaskedLM"),c(Ct,"class","relative group"),c(xa,"href","/docs/transformers/v4.21.1/en/main_classes/model#transformers.TFPreTrainedModel"),c(Rr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Rr,"rel","nofollow"),c(Ia,"href","/docs/transformers/v4.21.1/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(Re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Xt,"id","transformers.TFCamembertForSequenceClassification"),c(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Xt,"href","#transformers.TFCamembertForSequenceClassification"),c(yt,"class","relative group"),c(Sa,"href","/docs/transformers/v4.21.1/en/main_classes/model#transformers.TFPreTrainedModel"),c(Ir,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Ir,"rel","nofollow"),c(Oa,"href","/docs/transformers/v4.21.1/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),c(qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Jt,"id","transformers.TFCamembertForMultipleChoice"),c(Jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Jt,"href","#transformers.TFCamembertForMultipleChoice"),c($t,"class","relative group"),c(Ba,"href","/docs/transformers/v4.21.1/en/main_classes/model#transformers.TFPreTrainedModel"),c(Hr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Hr,"rel","nofollow"),c(Na,"href","/docs/transformers/v4.21.1/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),c(Ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(Zt,"id","transformers.TFCamembertForTokenClassification"),c(Zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Zt,"href","#transformers.TFCamembertForTokenClassification"),c(Ft,"class","relative group"),c(Ua,"href","/docs/transformers/v4.21.1/en/main_classes/model#transformers.TFPreTrainedModel"),c(Xr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Xr,"rel","nofollow"),c(Ha,"href","/docs/transformers/v4.21.1/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),c(De,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),c(to,"id","transformers.TFCamembertForQuestionAnswering"),c(to,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(to,"href","#transformers.TFCamembertForQuestionAnswering"),c(Mt,"class","relative group"),c(Qa,"href","/docs/transformers/v4.21.1/en/main_classes/model#transformers.TFPreTrainedModel"),c(ta,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ta,"rel","nofollow"),c(Va,"href","/docs/transformers/v4.21.1/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),c(ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(i,f){e(document.head,k),h(i,re,f),h(i,U,f),e(U,_),e(_,A),P(G,A,null),e(U,le),e(U,D),e(D,de),h(i,Q,f),h(i,u,f),e(u,W),e(W,z),P(X,z,null),e(u,me),e(u,x),e(x,ce),h(i,ae,f),h(i,H,f),e(H,w),e(H,j),e(j,V),e(H,g),h(i,b,f),h(i,Z,f),e(Z,O),h(i,se,f),h(i,ee,f),e(ee,I),e(I,he),h(i,ne,f),h(i,v,f),e(v,fe),h(i,S,f),h(i,te,f),e(te,J),e(J,B),e(J,oe),e(oe,pe),e(J,E),h(i,ie,f),h(i,C,f),e(C,ue),e(C,l),e(l,p),e(C,K),e(C,be),e(be,Ee),e(C,y),h(i,Te,f),h(i,ge,f),e(ge,ve),e(ve,T),P($,T,null),e(ge,Ce),e(ge,ke),e(ke,N),h(i,we,f),h(i,_e,f),P(Y,_e,null),e(_e,ye),e(_e,ht),e(ht,il),e(ht,na),e(na,ll),e(ht,dl),e(ht,ao),e(ao,ml),e(ht,cl),h(i,ln,f),h(i,ft,f),e(ft,Lt),e(Lt,Ja),P(so,Ja,null),e(ft,hl),e(ft,Ya),e(Ya,fl),h(i,dn,f),h(i,Fe,f),P(no,Fe,null),e(Fe,pl),e(Fe,et),e(et,ul),e(et,ia),e(ia,gl),e(et,_l),e(et,la),e(la,vl),e(et,bl),e(et,io),e(io,kl),e(et,Tl),e(Fe,wl),e(Fe,lo),e(lo,El),e(lo,da),e(da,Cl),e(lo,yl),e(Fe,$l),e(Fe,tt),P(mo,tt,null),e(tt,Fl),e(tt,Za),e(Za,Ml),e(tt,Pl),e(tt,co),e(co,ma),e(ma,Ll),e(ma,es),e(es,Rl),e(co,ql),e(co,ca),e(ca,Al),e(ca,ts),e(ts,Dl),e(Fe,zl),e(Fe,Rt),P(ho,Rt,null),e(Rt,xl),e(Rt,fo),e(fo,Il),e(fo,os),e(os,Sl),e(fo,Ol),e(Fe,Bl),e(Fe,qt),P(po,qt,null),e(qt,Nl),e(qt,rs),e(rs,Ul),e(Fe,Hl),e(Fe,ha),P(uo,ha,null),h(i,mn,f),h(i,pt,f),e(pt,At),e(At,as),P(go,as,null),e(pt,Ql),e(pt,ss),e(ss,Vl),h(i,cn,f),h(i,Ie,f),P(_o,Ie,null),e(Ie,Kl),e(Ie,Ke),e(Ke,Wl),e(Ke,ns),e(ns,Gl),e(Ke,Xl),e(Ke,fa),e(fa,jl),e(Ke,Jl),e(Ke,pa),e(pa,Yl),e(Ke,Zl),e(Ke,vo),e(vo,ed),e(Ke,td),e(Ie,od),e(Ie,bo),e(bo,rd),e(bo,ua),e(ua,ad),e(bo,sd),e(Ie,nd),e(Ie,ot),P(ko,ot,null),e(ot,id),e(ot,is),e(is,ld),e(ot,dd),e(ot,To),e(To,ga),e(ga,md),e(ga,ls),e(ls,cd),e(To,hd),e(To,_a),e(_a,fd),e(_a,ds),e(ds,pd),e(Ie,ud),e(Ie,Dt),P(wo,Dt,null),e(Dt,gd),e(Dt,ms),e(ms,_d),h(i,hn,f),h(i,ut,f),e(ut,zt),e(zt,cs),P(Eo,cs,null),e(ut,vd),e(ut,hs),e(hs,bd),h(i,fn,f),h(i,Se,f),P(Co,Se,null),e(Se,kd),e(Se,fs),e(fs,Td),e(Se,wd),e(Se,yo),e(yo,Ed),e(yo,va),e(va,Cd),e(yo,yd),e(Se,$d),e(Se,$o),e($o,Fd),e($o,Fo),e(Fo,Md),e($o,Pd),e(Se,Ld),e(Se,Mo),e(Mo,Rd),e(Mo,ba),e(ba,qd),e(Mo,Ad),h(i,pn,f),h(i,gt,f),e(gt,xt),e(xt,ps),P(Po,ps,null),e(gt,Dd),e(gt,us),e(us,zd),h(i,un,f),h(i,Oe,f),P(Lo,Oe,null),e(Oe,xd),e(Oe,Ro),e(Ro,Id),e(Ro,gs),e(gs,Sd),e(Ro,Od),e(Oe,Bd),e(Oe,qo),e(qo,Nd),e(qo,ka),e(ka,Ud),e(qo,Hd),e(Oe,Qd),e(Oe,Ao),e(Ao,Vd),e(Ao,Do),e(Do,Kd),e(Ao,Wd),e(Oe,Gd),e(Oe,zo),e(zo,Xd),e(zo,Ta),e(Ta,jd),e(zo,Jd),h(i,gn,f),h(i,_t,f),e(_t,It),e(It,_s),P(xo,_s,null),e(_t,Yd),e(_t,vs),e(vs,Zd),h(i,_n,f),h(i,Be,f),P(Io,Be,null),e(Be,em),e(Be,So),e(So,tm),e(So,bs),e(bs,om),e(So,rm),e(Be,am),e(Be,Oo),e(Oo,sm),e(Oo,wa),e(wa,nm),e(Oo,im),e(Be,lm),e(Be,Bo),e(Bo,dm),e(Bo,No),e(No,mm),e(Bo,cm),e(Be,hm),e(Be,Uo),e(Uo,fm),e(Uo,Ea),e(Ea,pm),e(Uo,um),h(i,vn,f),h(i,vt,f),e(vt,St),e(St,ks),P(Ho,ks,null),e(vt,gm),e(vt,Ts),e(Ts,_m),h(i,bn,f),h(i,Ne,f),P(Qo,Ne,null),e(Ne,vm),e(Ne,ws),e(ws,bm),e(Ne,km),e(Ne,Vo),e(Vo,Tm),e(Vo,Ca),e(Ca,wm),e(Vo,Em),e(Ne,Cm),e(Ne,Ko),e(Ko,ym),e(Ko,Wo),e(Wo,$m),e(Ko,Fm),e(Ne,Mm),e(Ne,Go),e(Go,Pm),e(Go,ya),e(ya,Lm),e(Go,Rm),h(i,kn,f),h(i,bt,f),e(bt,Ot),e(Ot,Es),P(Xo,Es,null),e(bt,qm),e(bt,Cs),e(Cs,Am),h(i,Tn,f),h(i,Ue,f),P(jo,Ue,null),e(Ue,Dm),e(Ue,ys),e(ys,zm),e(Ue,xm),e(Ue,Jo),e(Jo,Im),e(Jo,$a),e($a,Sm),e(Jo,Om),e(Ue,Bm),e(Ue,Yo),e(Yo,Nm),e(Yo,Zo),e(Zo,Um),e(Yo,Hm),e(Ue,Qm),e(Ue,er),e(er,Vm),e(er,Fa),e(Fa,Km),e(er,Wm),h(i,wn,f),h(i,kt,f),e(kt,Bt),e(Bt,$s),P(tr,$s,null),e(kt,Gm),e(kt,Fs),e(Fs,Xm),h(i,En,f),h(i,He,f),P(or,He,null),e(He,jm),e(He,Ms),e(Ms,Jm),e(He,Ym),e(He,rr),e(rr,Zm),e(rr,Ma),e(Ma,ec),e(rr,tc),e(He,oc),e(He,ar),e(ar,rc),e(ar,sr),e(sr,ac),e(ar,sc),e(He,nc),e(He,nr),e(nr,ic),e(nr,Pa),e(Pa,lc),e(nr,dc),h(i,Cn,f),h(i,Tt,f),e(Tt,Nt),e(Nt,Ps),P(ir,Ps,null),e(Tt,mc),e(Tt,Ls),e(Ls,cc),h(i,yn,f),h(i,Qe,f),P(lr,Qe,null),e(Qe,hc),e(Qe,Ut),e(Ut,fc),e(Ut,Rs),e(Rs,pc),e(Ut,uc),e(Ut,qs),e(qs,gc),e(Qe,_c),e(Qe,dr),e(dr,vc),e(dr,La),e(La,bc),e(dr,kc),e(Qe,Tc),e(Qe,mr),e(mr,wc),e(mr,cr),e(cr,Ec),e(mr,Cc),e(Qe,yc),e(Qe,hr),e(hr,$c),e(hr,Ra),e(Ra,Fc),e(hr,Mc),h(i,$n,f),h(i,wt,f),e(wt,Ht),e(Ht,As),P(fr,As,null),e(wt,Pc),e(wt,Ds),e(Ds,Lc),h(i,Fn,f),h(i,Pe,f),P(pr,Pe,null),e(Pe,Rc),e(Pe,zs),e(zs,qc),e(Pe,Ac),e(Pe,ur),e(ur,Dc),e(ur,qa),e(qa,zc),e(ur,xc),e(Pe,Ic),e(Pe,gr),e(gr,Sc),e(gr,_r),e(_r,Oc),e(gr,Bc),e(Pe,Nc),P(Qt,Pe,null),e(Pe,Uc),e(Pe,vr),e(vr,Hc),e(vr,Aa),e(Aa,Qc),e(vr,Vc),h(i,Mn,f),h(i,Et,f),e(Et,Vt),e(Vt,xs),P(br,xs,null),e(Et,Kc),e(Et,Is),e(Is,Wc),h(i,Pn,f),h(i,Le,f),P(kr,Le,null),e(Le,Gc),e(Le,Tr),e(Tr,Xc),e(Tr,Ss),e(Ss,jc),e(Tr,Jc),e(Le,Yc),e(Le,wr),e(wr,Zc),e(wr,Da),e(Da,eh),e(wr,th),e(Le,oh),e(Le,Er),e(Er,rh),e(Er,Cr),e(Cr,ah),e(Er,sh),e(Le,nh),P(Kt,Le,null),e(Le,ih),e(Le,yr),e(yr,lh),e(yr,za),e(za,dh),e(yr,mh),h(i,Ln,f),h(i,Ct,f),e(Ct,Wt),e(Wt,Os),P($r,Os,null),e(Ct,ch),e(Ct,Bs),e(Bs,hh),h(i,Rn,f),h(i,Re,f),P(Fr,Re,null),e(Re,fh),e(Re,Mr),e(Mr,ph),e(Mr,Ns),e(Ns,uh),e(Mr,gh),e(Re,_h),e(Re,Pr),e(Pr,vh),e(Pr,xa),e(xa,bh),e(Pr,kh),e(Re,Th),e(Re,Lr),e(Lr,wh),e(Lr,Rr),e(Rr,Eh),e(Lr,Ch),e(Re,yh),P(Gt,Re,null),e(Re,$h),e(Re,qr),e(qr,Fh),e(qr,Ia),e(Ia,Mh),e(qr,Ph),h(i,qn,f),h(i,yt,f),e(yt,Xt),e(Xt,Us),P(Ar,Us,null),e(yt,Lh),e(yt,Hs),e(Hs,Rh),h(i,An,f),h(i,qe,f),P(Dr,qe,null),e(qe,qh),e(qe,Qs),e(Qs,Ah),e(qe,Dh),e(qe,zr),e(zr,zh),e(zr,Sa),e(Sa,xh),e(zr,Ih),e(qe,Sh),e(qe,xr),e(xr,Oh),e(xr,Ir),e(Ir,Bh),e(xr,Nh),e(qe,Uh),P(jt,qe,null),e(qe,Hh),e(qe,Sr),e(Sr,Qh),e(Sr,Oa),e(Oa,Vh),e(Sr,Kh),h(i,Dn,f),h(i,$t,f),e($t,Jt),e(Jt,Vs),P(Or,Vs,null),e($t,Wh),e($t,Ks),e(Ks,Gh),h(i,zn,f),h(i,Ae,f),P(Br,Ae,null),e(Ae,Xh),e(Ae,Ws),e(Ws,jh),e(Ae,Jh),e(Ae,Nr),e(Nr,Yh),e(Nr,Ba),e(Ba,Zh),e(Nr,ef),e(Ae,tf),e(Ae,Ur),e(Ur,of),e(Ur,Hr),e(Hr,rf),e(Ur,af),e(Ae,sf),P(Yt,Ae,null),e(Ae,nf),e(Ae,Qr),e(Qr,lf),e(Qr,Na),e(Na,df),e(Qr,mf),h(i,xn,f),h(i,Ft,f),e(Ft,Zt),e(Zt,Gs),P(Vr,Gs,null),e(Ft,cf),e(Ft,Xs),e(Xs,hf),h(i,In,f),h(i,De,f),P(Kr,De,null),e(De,ff),e(De,js),e(js,pf),e(De,uf),e(De,Wr),e(Wr,gf),e(Wr,Ua),e(Ua,_f),e(Wr,vf),e(De,bf),e(De,Gr),e(Gr,kf),e(Gr,Xr),e(Xr,Tf),e(Gr,wf),e(De,Ef),P(eo,De,null),e(De,Cf),e(De,jr),e(jr,yf),e(jr,Ha),e(Ha,$f),e(jr,Ff),h(i,Sn,f),h(i,Mt,f),e(Mt,to),e(to,Js),P(Jr,Js,null),e(Mt,Mf),e(Mt,Ys),e(Ys,Pf),h(i,On,f),h(i,ze,f),P(Yr,ze,null),e(ze,Lf),e(ze,Pt),e(Pt,Rf),e(Pt,Zs),e(Zs,qf),e(Pt,Af),e(Pt,en),e(en,Df),e(Pt,zf),e(ze,xf),e(ze,Zr),e(Zr,If),e(Zr,Qa),e(Qa,Sf),e(Zr,Of),e(ze,Bf),e(ze,ea),e(ea,Nf),e(ea,ta),e(ta,Uf),e(ea,Hf),e(ze,Qf),P(oo,ze,null),e(ze,Vf),e(ze,oa),e(oa,Kf),e(oa,Va),e(Va,Wf),e(oa,Gf),Bn=!0},p(i,[f]){const ra={};f&2&&(ra.$$scope={dirty:f,ctx:i}),Qt.$set(ra);const tn={};f&2&&(tn.$$scope={dirty:f,ctx:i}),Kt.$set(tn);const on={};f&2&&(on.$$scope={dirty:f,ctx:i}),Gt.$set(on);const rn={};f&2&&(rn.$$scope={dirty:f,ctx:i}),jt.$set(rn);const aa={};f&2&&(aa.$$scope={dirty:f,ctx:i}),Yt.$set(aa);const an={};f&2&&(an.$$scope={dirty:f,ctx:i}),eo.$set(an);const sn={};f&2&&(sn.$$scope={dirty:f,ctx:i}),oo.$set(sn)},i(i){Bn||(L(G.$$.fragment,i),L(X.$$.fragment,i),L($.$$.fragment,i),L(Y.$$.fragment,i),L(so.$$.fragment,i),L(no.$$.fragment,i),L(mo.$$.fragment,i),L(ho.$$.fragment,i),L(po.$$.fragment,i),L(uo.$$.fragment,i),L(go.$$.fragment,i),L(_o.$$.fragment,i),L(ko.$$.fragment,i),L(wo.$$.fragment,i),L(Eo.$$.fragment,i),L(Co.$$.fragment,i),L(Po.$$.fragment,i),L(Lo.$$.fragment,i),L(xo.$$.fragment,i),L(Io.$$.fragment,i),L(Ho.$$.fragment,i),L(Qo.$$.fragment,i),L(Xo.$$.fragment,i),L(jo.$$.fragment,i),L(tr.$$.fragment,i),L(or.$$.fragment,i),L(ir.$$.fragment,i),L(lr.$$.fragment,i),L(fr.$$.fragment,i),L(pr.$$.fragment,i),L(Qt.$$.fragment,i),L(br.$$.fragment,i),L(kr.$$.fragment,i),L(Kt.$$.fragment,i),L($r.$$.fragment,i),L(Fr.$$.fragment,i),L(Gt.$$.fragment,i),L(Ar.$$.fragment,i),L(Dr.$$.fragment,i),L(jt.$$.fragment,i),L(Or.$$.fragment,i),L(Br.$$.fragment,i),L(Yt.$$.fragment,i),L(Vr.$$.fragment,i),L(Kr.$$.fragment,i),L(eo.$$.fragment,i),L(Jr.$$.fragment,i),L(Yr.$$.fragment,i),L(oo.$$.fragment,i),Bn=!0)},o(i){R(G.$$.fragment,i),R(X.$$.fragment,i),R($.$$.fragment,i),R(Y.$$.fragment,i),R(so.$$.fragment,i),R(no.$$.fragment,i),R(mo.$$.fragment,i),R(ho.$$.fragment,i),R(po.$$.fragment,i),R(uo.$$.fragment,i),R(go.$$.fragment,i),R(_o.$$.fragment,i),R(ko.$$.fragment,i),R(wo.$$.fragment,i),R(Eo.$$.fragment,i),R(Co.$$.fragment,i),R(Po.$$.fragment,i),R(Lo.$$.fragment,i),R(xo.$$.fragment,i),R(Io.$$.fragment,i),R(Ho.$$.fragment,i),R(Qo.$$.fragment,i),R(Xo.$$.fragment,i),R(jo.$$.fragment,i),R(tr.$$.fragment,i),R(or.$$.fragment,i),R(ir.$$.fragment,i),R(lr.$$.fragment,i),R(fr.$$.fragment,i),R(pr.$$.fragment,i),R(Qt.$$.fragment,i),R(br.$$.fragment,i),R(kr.$$.fragment,i),R(Kt.$$.fragment,i),R($r.$$.fragment,i),R(Fr.$$.fragment,i),R(Gt.$$.fragment,i),R(Ar.$$.fragment,i),R(Dr.$$.fragment,i),R(jt.$$.fragment,i),R(Or.$$.fragment,i),R(Br.$$.fragment,i),R(Yt.$$.fragment,i),R(Vr.$$.fragment,i),R(Kr.$$.fragment,i),R(eo.$$.fragment,i),R(Jr.$$.fragment,i),R(Yr.$$.fragment,i),R(oo.$$.fragment,i),Bn=!1},d(i){t(k),i&&t(re),i&&t(U),q(G),i&&t(Q),i&&t(u),q(X),i&&t(ae),i&&t(H),i&&t(b),i&&t(Z),i&&t(se),i&&t(ee),i&&t(ne),i&&t(v),i&&t(S),i&&t(te),i&&t(ie),i&&t(C),i&&t(Te),i&&t(ge),q($),i&&t(we),i&&t(_e),q(Y),i&&t(ln),i&&t(ft),q(so),i&&t(dn),i&&t(Fe),q(no),q(mo),q(ho),q(po),q(uo),i&&t(mn),i&&t(pt),q(go),i&&t(cn),i&&t(Ie),q(_o),q(ko),q(wo),i&&t(hn),i&&t(ut),q(Eo),i&&t(fn),i&&t(Se),q(Co),i&&t(pn),i&&t(gt),q(Po),i&&t(un),i&&t(Oe),q(Lo),i&&t(gn),i&&t(_t),q(xo),i&&t(_n),i&&t(Be),q(Io),i&&t(vn),i&&t(vt),q(Ho),i&&t(bn),i&&t(Ne),q(Qo),i&&t(kn),i&&t(bt),q(Xo),i&&t(Tn),i&&t(Ue),q(jo),i&&t(wn),i&&t(kt),q(tr),i&&t(En),i&&t(He),q(or),i&&t(Cn),i&&t(Tt),q(ir),i&&t(yn),i&&t(Qe),q(lr),i&&t($n),i&&t(wt),q(fr),i&&t(Fn),i&&t(Pe),q(pr),q(Qt),i&&t(Mn),i&&t(Et),q(br),i&&t(Pn),i&&t(Le),q(kr),q(Kt),i&&t(Ln),i&&t(Ct),q($r),i&&t(Rn),i&&t(Re),q(Fr),q(Gt),i&&t(qn),i&&t(yt),q(Ar),i&&t(An),i&&t(qe),q(Dr),q(jt),i&&t(Dn),i&&t($t),q(Or),i&&t(zn),i&&t(Ae),q(Br),q(Yt),i&&t(xn),i&&t(Ft),q(Vr),i&&t(In),i&&t(De),q(Kr),q(eo),i&&t(Sn),i&&t(Mt),q(Jr),i&&t(On),i&&t(ze),q(Yr),q(oo)}}}const Qg={local:"camembert",sections:[{local:"overview",title:"Overview"},{local:"transformers.CamembertConfig",title:"CamembertConfig"},{local:"transformers.CamembertTokenizer",title:"CamembertTokenizer"},{local:"transformers.CamembertTokenizerFast",title:"CamembertTokenizerFast"},{local:"transformers.CamembertModel",title:"CamembertModel"},{local:"transformers.CamembertForCausalLM",title:"CamembertForCausalLM"},{local:"transformers.CamembertForMaskedLM",title:"CamembertForMaskedLM"},{local:"transformers.CamembertForSequenceClassification",title:"CamembertForSequenceClassification"},{local:"transformers.CamembertForMultipleChoice",title:"CamembertForMultipleChoice"},{local:"transformers.CamembertForTokenClassification",title:"CamembertForTokenClassification"},{local:"transformers.CamembertForQuestionAnswering",title:"CamembertForQuestionAnswering"},{local:"transformers.TFCamembertModel",title:"TFCamembertModel"},{local:"transformers.TFCamembertForCausalLM",title:"TFCamembertForCasualLM"},{local:"transformers.TFCamembertForMaskedLM",title:"TFCamembertForMaskedLM"},{local:"transformers.TFCamembertForSequenceClassification",title:"TFCamembertForSequenceClassification"},{local:"transformers.TFCamembertForMultipleChoice",title:"TFCamembertForMultipleChoice"},{local:"transformers.TFCamembertForTokenClassification",title:"TFCamembertForTokenClassification"},{local:"transformers.TFCamembertForQuestionAnswering",title:"TFCamembertForQuestionAnswering"}],title:"CamemBERT"};function Vg(xe){return zg(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class jg extends Rg{constructor(k){super();qg(this,k,Vg,Hg,Ag,{})}}export{jg as default,Qg as metadata};
