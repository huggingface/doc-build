import{S as Un,i as jn,s as Kn,e as r,k as f,w as m,t as s,M as Mn,c as n,d as t,m as h,a,x as u,h as l,b as c,G as o,g as p,y as d,q as v,o as T,B as E,v as Yn}from"../chunks/vendor-hf-doc-builder.js";import{T as qn}from"../chunks/Tip-hf-doc-builder.js";import{I as K}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as he}from"../chunks/CodeBlock-hf-doc-builder.js";function Jn(Pt){let _,M,P,g,H,w,k,L,x,Y,O;return{c(){_=r("p"),M=s("Since 2.3.0 the conversion script is now part of the transformers CLI ("),P=r("strong"),g=s("transformers-cli"),H=s(`) available in any
transformers >= 2.3.0 installation.`),w=f(),k=r("p"),L=s("The documentation below reflects the "),x=r("strong"),Y=s("transformers-cli convert"),O=s(" command format.")},l(A){_=n(A,"P",{});var b=a(_);M=l(b,"Since 2.3.0 the conversion script is now part of the transformers CLI ("),P=n(b,"STRONG",{});var Ge=a(P);g=l(Ge,"transformers-cli"),Ge.forEach(t),H=l(b,`) available in any
transformers >= 2.3.0 installation.`),b.forEach(t),w=h(A),k=n(A,"P",{});var q=a(k);L=l(q,"The documentation below reflects the "),x=n(q,"STRONG",{});var _e=a(x);Y=l(_e,"transformers-cli convert"),_e.forEach(t),O=l(q," command format."),q.forEach(t)},m(A,b){p(A,_,b),o(_,M),o(_,P),o(P,g),o(_,H),p(A,w,b),p(A,k,b),o(k,L),o(k,x),o(x,Y),o(k,O)},d(A){A&&t(_),A&&t(w),A&&t(k)}}}function zn(Pt){let _,M,P,g,H,w,k,L,x,Y,O,A,b,Ge,q,_e,J,At,S,z,qe,me,$o,Je,Po,bt,I,Ao,ue,bo,yo,de,go,wo,yt,$,ko,ze,Oo,No,Qe,Io,Co,Ve,Ro,Ho,Xe,Lo,xo,ve,So,Fo,gt,y,Do,We,Bo,Go,Ze,Xo,Uo,et,jo,Ko,tt,Mo,Yo,wt,Q,qo,ot,Jo,zo,kt,V,Qo,rt,Vo,Wo,Ot,Te,Nt,W,Zo,Ee,er,tr,It,F,Z,nt,$e,or,at,rr,Ct,ee,nr,Pe,ar,sr,Rt,C,lr,st,ir,pr,lt,cr,fr,Ht,te,hr,it,_r,mr,Lt,Ae,xt,oe,ur,be,dr,vr,St,D,re,pt,ye,Tr,ct,Er,Ft,ne,$r,ge,Pr,Ar,Dt,we,Bt,B,ae,ft,ke,br,ht,yr,Gt,se,gr,Oe,wr,kr,Xt,Ne,Ut,G,le,_t,Ie,Or,mt,Nr,jt,ie,Ir,Ce,Cr,Rr,Kt,Re,Mt,X,pe,ut,He,Hr,dt,Lr,Yt,Ue,xr,qt,Le,Jt,U,ce,vt,xe,Sr,Tt,Fr,zt,je,Dr,Qt,Se,Vt,j,fe,Et,Fe,Br,$t,Gr,Wt,Ke,Xr,Zt,De,eo;return w=new K({}),J=new qn({props:{$$slots:{default:[Jn]},$$scope:{ctx:Pt}}}),me=new K({}),Te=new he({props:{code:`export BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12

transformers-cli convert --model_type bert \\
  --tf_checkpoint $BERT_BASE_DIR/bert_model.ckpt \\
  --config $BERT_BASE_DIR/bert_config.json \\
  --pytorch_dump_output $BERT_BASE_DIR/pytorch_model.bin`,highlighted:`<span class="hljs-built_in">export</span> BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12

transformers-cli convert --model_type bert \\
  --tf_checkpoint <span class="hljs-variable">$BERT_BASE_DIR</span>/bert_model.ckpt \\
  --config <span class="hljs-variable">$BERT_BASE_DIR</span>/bert_config.json \\
  --pytorch_dump_output <span class="hljs-variable">$BERT_BASE_DIR</span>/pytorch_model.bin`}}),$e=new K({}),Ae=new he({props:{code:`export ALBERT_BASE_DIR=/path/to/albert/albert_base

transformers-cli convert --model_type albert \\
  --tf_checkpoint $ALBERT_BASE_DIR/model.ckpt-best \\
  --config $ALBERT_BASE_DIR/albert_config.json \\
  --pytorch_dump_output $ALBERT_BASE_DIR/pytorch_model.bin`,highlighted:`<span class="hljs-built_in">export</span> ALBERT_BASE_DIR=/path/to/albert/albert_base

transformers-cli convert --model_type albert \\
  --tf_checkpoint <span class="hljs-variable">$ALBERT_BASE_DIR</span>/model.ckpt-best \\
  --config <span class="hljs-variable">$ALBERT_BASE_DIR</span>/albert_config.json \\
  --pytorch_dump_output <span class="hljs-variable">$ALBERT_BASE_DIR</span>/pytorch_model.bin`}}),ye=new K({}),we=new he({props:{code:`export OPENAI_GPT_CHECKPOINT_FOLDER_PATH=/path/to/openai/pretrained/numpy/weights

transformers-cli convert --model_type gpt \\
  --tf_checkpoint $OPENAI_GPT_CHECKPOINT_FOLDER_PATH \\
  --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\
  [--config OPENAI_GPT_CONFIG] \\
  [--finetuning_task_name OPENAI_GPT_FINETUNED_TASK] \\`,highlighted:`<span class="hljs-built_in">export</span> OPENAI_GPT_CHECKPOINT_FOLDER_PATH=/path/to/openai/pretrained/numpy/weights

transformers-cli convert --model_type gpt \\
  --tf_checkpoint <span class="hljs-variable">$OPENAI_GPT_CHECKPOINT_FOLDER_PATH</span> \\
  --pytorch_dump_output <span class="hljs-variable">$PYTORCH_DUMP_OUTPUT</span> \\
  [--config OPENAI_GPT_CONFIG] \\
  [--finetuning_task_name OPENAI_GPT_FINETUNED_TASK] \\`}}),ke=new K({}),Ne=new he({props:{code:`export OPENAI_GPT2_CHECKPOINT_PATH=/path/to/gpt2/pretrained/weights

transformers-cli convert --model_type gpt2 \\
  --tf_checkpoint $OPENAI_GPT2_CHECKPOINT_PATH \\
  --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\
  [--config OPENAI_GPT2_CONFIG] \\
  [--finetuning_task_name OPENAI_GPT2_FINETUNED_TASK]`,highlighted:`<span class="hljs-built_in">export</span> OPENAI_GPT2_CHECKPOINT_PATH=/path/to/gpt2/pretrained/weights

transformers-cli convert --model_type gpt2 \\
  --tf_checkpoint <span class="hljs-variable">$OPENAI_GPT2_CHECKPOINT_PATH</span> \\
  --pytorch_dump_output <span class="hljs-variable">$PYTORCH_DUMP_OUTPUT</span> \\
  [--config OPENAI_GPT2_CONFIG] \\
  [--finetuning_task_name OPENAI_GPT2_FINETUNED_TASK]`}}),Ie=new K({}),Re=new he({props:{code:`export TRANSFO_XL_CHECKPOINT_FOLDER_PATH=/path/to/transfo/xl/checkpoint

transformers-cli convert --model_type transfo_xl \\
  --tf_checkpoint $TRANSFO_XL_CHECKPOINT_FOLDER_PATH \\
  --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\
  [--config TRANSFO_XL_CONFIG] \\
  [--finetuning_task_name TRANSFO_XL_FINETUNED_TASK]`,highlighted:`<span class="hljs-built_in">export</span> TRANSFO_XL_CHECKPOINT_FOLDER_PATH=/path/to/transfo/xl/checkpoint

transformers-cli convert --model_type transfo_xl \\
  --tf_checkpoint <span class="hljs-variable">$TRANSFO_XL_CHECKPOINT_FOLDER_PATH</span> \\
  --pytorch_dump_output <span class="hljs-variable">$PYTORCH_DUMP_OUTPUT</span> \\
  [--config TRANSFO_XL_CONFIG] \\
  [--finetuning_task_name TRANSFO_XL_FINETUNED_TASK]`}}),He=new K({}),Le=new he({props:{code:`export TRANSFO_XL_CHECKPOINT_PATH=/path/to/xlnet/checkpoint
export TRANSFO_XL_CONFIG_PATH=/path/to/xlnet/config

transformers-cli convert --model_type xlnet \\
  --tf_checkpoint $TRANSFO_XL_CHECKPOINT_PATH \\
  --config $TRANSFO_XL_CONFIG_PATH \\
  --pytorch_dump_output $PYTORCH_DUMP_OUTPUT \\
  [--finetuning_task_name XLNET_FINETUNED_TASK] \\`,highlighted:`<span class="hljs-built_in">export</span> TRANSFO_XL_CHECKPOINT_PATH=/path/to/xlnet/checkpoint
<span class="hljs-built_in">export</span> TRANSFO_XL_CONFIG_PATH=/path/to/xlnet/config

transformers-cli convert --model_type xlnet \\
  --tf_checkpoint <span class="hljs-variable">$TRANSFO_XL_CHECKPOINT_PATH</span> \\
  --config <span class="hljs-variable">$TRANSFO_XL_CONFIG_PATH</span> \\
  --pytorch_dump_output <span class="hljs-variable">$PYTORCH_DUMP_OUTPUT</span> \\
  [--finetuning_task_name XLNET_FINETUNED_TASK] \\`}}),xe=new K({}),Se=new he({props:{code:`export XLM_CHECKPOINT_PATH=/path/to/xlm/checkpoint

transformers-cli convert --model_type xlm \\
  --tf_checkpoint $XLM_CHECKPOINT_PATH \\
  --pytorch_dump_output $PYTORCH_DUMP_OUTPUT
 [--config XML_CONFIG] \\
 [--finetuning_task_name XML_FINETUNED_TASK]`,highlighted:`<span class="hljs-built_in">export</span> XLM_CHECKPOINT_PATH=/path/to/xlm/checkpoint

transformers-cli convert --model_type xlm \\
  --tf_checkpoint <span class="hljs-variable">$XLM_CHECKPOINT_PATH</span> \\
  --pytorch_dump_output <span class="hljs-variable">$PYTORCH_DUMP_OUTPUT</span>
 [--config XML_CONFIG] \\
 [--finetuning_task_name XML_FINETUNED_TASK]`}}),Fe=new K({}),De=new he({props:{code:`export T5=/path/to/t5/uncased_L-12_H-768_A-12

transformers-cli convert --model_type t5 \\
  --tf_checkpoint $T5/t5_model.ckpt \\
  --config $T5/t5_config.json \\
  --pytorch_dump_output $T5/pytorch_model.bin`,highlighted:`<span class="hljs-built_in">export</span> T5=/path/to/t5/uncased_L-12_H-768_A-12

transformers-cli convert --model_type t5 \\
  --tf_checkpoint <span class="hljs-variable">$T5</span>/t5_model.ckpt \\
  --config <span class="hljs-variable">$T5</span>/t5_config.json \\
  --pytorch_dump_output <span class="hljs-variable">$T5</span>/pytorch_model.bin`}}),{c(){_=r("meta"),M=f(),P=r("h1"),g=r("a"),H=r("span"),m(w.$$.fragment),k=f(),L=r("span"),x=s("Converting Tensorflow Checkpoints"),Y=f(),O=r("p"),A=s(`A command-line interface is provided to convert original Bert/GPT/GPT-2/Transformer-XL/XLNet/XLM checkpoints to models
that can be loaded using the `),b=r("code"),Ge=s("from_pretrained"),q=s(" methods of the library."),_e=f(),m(J.$$.fragment),At=f(),S=r("h2"),z=r("a"),qe=r("span"),m(me.$$.fragment),$o=f(),Je=r("span"),Po=s("BERT"),bt=f(),I=r("p"),Ao=s("You can convert any TensorFlow checkpoint for BERT (in particular "),ue=r("a"),bo=s("the pre-trained models released by Google"),yo=s(`) in a PyTorch save file by using the
`),de=r("a"),go=s("convert_bert_original_tf_checkpoint_to_pytorch.py"),wo=s(" script."),yt=f(),$=r("p"),ko=s("This CLI takes as input a TensorFlow checkpoint (three files starting with "),ze=r("code"),Oo=s("bert_model.ckpt"),No=s(`) and the associated
configuration file (`),Qe=r("code"),Io=s("bert_config.json"),Co=s(`), and creates a PyTorch model for this configuration, loads the weights from
the TensorFlow checkpoint in the PyTorch model and saves the resulting model in a standard PyTorch save file that can
be imported using `),Ve=r("code"),Ro=s("from_pretrained()"),Ho=s(" (see example in "),Xe=r("a"),Lo=s("quicktour"),xo=s(" , "),ve=r("a"),So=s("run_glue.py"),Fo=s(" )."),gt=f(),y=r("p"),Do=s("You only need to run this conversion script "),We=r("strong"),Bo=s("once"),Go=s(` to get a PyTorch model. You can then disregard the TensorFlow
checkpoint (the three files starting with `),Ze=r("code"),Xo=s("bert_model.ckpt"),Uo=s(`) but be sure to keep the configuration file (\\
`),et=r("code"),jo=s("bert_config.json"),Ko=s(") and the vocabulary file ("),tt=r("code"),Mo=s("vocab.txt"),Yo=s(") as these are needed for the PyTorch model too."),wt=f(),Q=r("p"),qo=s("To run this specific conversion script you will need to have TensorFlow and PyTorch installed ("),ot=r("code"),Jo=s("pip install tensorflow"),zo=s("). The rest of the repository only requires PyTorch."),kt=f(),V=r("p"),Qo=s("Here is an example of the conversion process for a pre-trained "),rt=r("code"),Vo=s("BERT-Base Uncased"),Wo=s(" model:"),Ot=f(),m(Te.$$.fragment),Nt=f(),W=r("p"),Zo=s("You can download Google\u2019s pre-trained models for the conversion "),Ee=r("a"),er=s("here"),tr=s("."),It=f(),F=r("h2"),Z=r("a"),nt=r("span"),m($e.$$.fragment),or=f(),at=r("span"),rr=s("ALBERT"),Ct=f(),ee=r("p"),nr=s(`Convert TensorFlow model checkpoints of ALBERT to PyTorch using the
`),Pe=r("a"),ar=s("convert_albert_original_tf_checkpoint_to_pytorch.py"),sr=s(" script."),Rt=f(),C=r("p"),lr=s("The CLI takes as input a TensorFlow checkpoint (three files starting with "),st=r("code"),ir=s("model.ckpt-best"),pr=s(`) and the accompanying
configuration file (`),lt=r("code"),cr=s("albert_config.json"),fr=s(`), then creates and saves a PyTorch model. To run this conversion you will
need to have TensorFlow and PyTorch installed.`),Ht=f(),te=r("p"),hr=s("Here is an example of the conversion process for the pre-trained "),it=r("code"),_r=s("ALBERT Base"),mr=s(" model:"),Lt=f(),m(Ae.$$.fragment),xt=f(),oe=r("p"),ur=s("You can download Google\u2019s pre-trained models for the conversion "),be=r("a"),dr=s("here"),vr=s("."),St=f(),D=r("h2"),re=r("a"),pt=r("span"),m(ye.$$.fragment),Tr=f(),ct=r("span"),Er=s("OpenAI GPT"),Ft=f(),ne=r("p"),$r=s(`Here is an example of the conversion process for a pre-trained OpenAI GPT model, assuming that your NumPy checkpoint
save as the same format than OpenAI pretrained model (see `),ge=r("a"),Pr=s("here"),Ar=s(`\\
)`),Dt=f(),m(we.$$.fragment),Bt=f(),B=r("h2"),ae=r("a"),ft=r("span"),m(ke.$$.fragment),br=f(),ht=r("span"),yr=s("OpenAI GPT-2"),Gt=f(),se=r("p"),gr=s("Here is an example of the conversion process for a pre-trained OpenAI GPT-2 model (see "),Oe=r("a"),wr=s("here"),kr=s(")"),Xt=f(),m(Ne.$$.fragment),Ut=f(),G=r("h2"),le=r("a"),_t=r("span"),m(Ie.$$.fragment),Or=f(),mt=r("span"),Nr=s("Transformer-XL"),jt=f(),ie=r("p"),Ir=s("Here is an example of the conversion process for a pre-trained Transformer-XL model (see "),Ce=r("a"),Cr=s("here"),Rr=s(")"),Kt=f(),m(Re.$$.fragment),Mt=f(),X=r("h2"),pe=r("a"),ut=r("span"),m(He.$$.fragment),Hr=f(),dt=r("span"),Lr=s("XLNet"),Yt=f(),Ue=r("p"),xr=s("Here is an example of the conversion process for a pre-trained XLNet model:"),qt=f(),m(Le.$$.fragment),Jt=f(),U=r("h2"),ce=r("a"),vt=r("span"),m(xe.$$.fragment),Sr=f(),Tt=r("span"),Fr=s("XLM"),zt=f(),je=r("p"),Dr=s("Here is an example of the conversion process for a pre-trained XLM model:"),Qt=f(),m(Se.$$.fragment),Vt=f(),j=r("h2"),fe=r("a"),Et=r("span"),m(Fe.$$.fragment),Br=f(),$t=r("span"),Gr=s("T5"),Wt=f(),Ke=r("p"),Xr=s("Here is an example of the conversion process for a pre-trained T5 model:"),Zt=f(),m(De.$$.fragment),this.h()},l(e){const i=Mn('[data-svelte="svelte-1phssyn"]',document.head);_=n(i,"META",{name:!0,content:!0}),i.forEach(t),M=h(e),P=n(e,"H1",{class:!0});var Be=a(P);g=n(Be,"A",{id:!0,class:!0,href:!0});var Ur=a(g);H=n(Ur,"SPAN",{});var jr=a(H);u(w.$$.fragment,jr),jr.forEach(t),Ur.forEach(t),k=h(Be),L=n(Be,"SPAN",{});var Kr=a(L);x=l(Kr,"Converting Tensorflow Checkpoints"),Kr.forEach(t),Be.forEach(t),Y=h(e),O=n(e,"P",{});var to=a(O);A=l(to,`A command-line interface is provided to convert original Bert/GPT/GPT-2/Transformer-XL/XLNet/XLM checkpoints to models
that can be loaded using the `),b=n(to,"CODE",{});var Mr=a(b);Ge=l(Mr,"from_pretrained"),Mr.forEach(t),q=l(to," methods of the library."),to.forEach(t),_e=h(e),u(J.$$.fragment,e),At=h(e),S=n(e,"H2",{class:!0});var oo=a(S);z=n(oo,"A",{id:!0,class:!0,href:!0});var Yr=a(z);qe=n(Yr,"SPAN",{});var qr=a(qe);u(me.$$.fragment,qr),qr.forEach(t),Yr.forEach(t),$o=h(oo),Je=n(oo,"SPAN",{});var Jr=a(Je);Po=l(Jr,"BERT"),Jr.forEach(t),oo.forEach(t),bt=h(e),I=n(e,"P",{});var Me=a(I);Ao=l(Me,"You can convert any TensorFlow checkpoint for BERT (in particular "),ue=n(Me,"A",{href:!0,rel:!0});var zr=a(ue);bo=l(zr,"the pre-trained models released by Google"),zr.forEach(t),yo=l(Me,`) in a PyTorch save file by using the
`),de=n(Me,"A",{href:!0,rel:!0});var Qr=a(de);go=l(Qr,"convert_bert_original_tf_checkpoint_to_pytorch.py"),Qr.forEach(t),wo=l(Me," script."),Me.forEach(t),yt=h(e),$=n(e,"P",{});var N=a($);ko=l(N,"This CLI takes as input a TensorFlow checkpoint (three files starting with "),ze=n(N,"CODE",{});var Vr=a(ze);Oo=l(Vr,"bert_model.ckpt"),Vr.forEach(t),No=l(N,`) and the associated
configuration file (`),Qe=n(N,"CODE",{});var Wr=a(Qe);Io=l(Wr,"bert_config.json"),Wr.forEach(t),Co=l(N,`), and creates a PyTorch model for this configuration, loads the weights from
the TensorFlow checkpoint in the PyTorch model and saves the resulting model in a standard PyTorch save file that can
be imported using `),Ve=n(N,"CODE",{});var Zr=a(Ve);Ro=l(Zr,"from_pretrained()"),Zr.forEach(t),Ho=l(N," (see example in "),Xe=n(N,"A",{href:!0});var en=a(Xe);Lo=l(en,"quicktour"),en.forEach(t),xo=l(N," , "),ve=n(N,"A",{href:!0,rel:!0});var tn=a(ve);So=l(tn,"run_glue.py"),tn.forEach(t),Fo=l(N," )."),N.forEach(t),gt=h(e),y=n(e,"P",{});var R=a(y);Do=l(R,"You only need to run this conversion script "),We=n(R,"STRONG",{});var on=a(We);Bo=l(on,"once"),on.forEach(t),Go=l(R,` to get a PyTorch model. You can then disregard the TensorFlow
checkpoint (the three files starting with `),Ze=n(R,"CODE",{});var rn=a(Ze);Xo=l(rn,"bert_model.ckpt"),rn.forEach(t),Uo=l(R,`) but be sure to keep the configuration file (\\
`),et=n(R,"CODE",{});var nn=a(et);jo=l(nn,"bert_config.json"),nn.forEach(t),Ko=l(R,") and the vocabulary file ("),tt=n(R,"CODE",{});var an=a(tt);Mo=l(an,"vocab.txt"),an.forEach(t),Yo=l(R,") as these are needed for the PyTorch model too."),R.forEach(t),wt=h(e),Q=n(e,"P",{});var ro=a(Q);qo=l(ro,"To run this specific conversion script you will need to have TensorFlow and PyTorch installed ("),ot=n(ro,"CODE",{});var sn=a(ot);Jo=l(sn,"pip install tensorflow"),sn.forEach(t),zo=l(ro,"). The rest of the repository only requires PyTorch."),ro.forEach(t),kt=h(e),V=n(e,"P",{});var no=a(V);Qo=l(no,"Here is an example of the conversion process for a pre-trained "),rt=n(no,"CODE",{});var ln=a(rt);Vo=l(ln,"BERT-Base Uncased"),ln.forEach(t),Wo=l(no," model:"),no.forEach(t),Ot=h(e),u(Te.$$.fragment,e),Nt=h(e),W=n(e,"P",{});var ao=a(W);Zo=l(ao,"You can download Google\u2019s pre-trained models for the conversion "),Ee=n(ao,"A",{href:!0,rel:!0});var pn=a(Ee);er=l(pn,"here"),pn.forEach(t),tr=l(ao,"."),ao.forEach(t),It=h(e),F=n(e,"H2",{class:!0});var so=a(F);Z=n(so,"A",{id:!0,class:!0,href:!0});var cn=a(Z);nt=n(cn,"SPAN",{});var fn=a(nt);u($e.$$.fragment,fn),fn.forEach(t),cn.forEach(t),or=h(so),at=n(so,"SPAN",{});var hn=a(at);rr=l(hn,"ALBERT"),hn.forEach(t),so.forEach(t),Ct=h(e),ee=n(e,"P",{});var lo=a(ee);nr=l(lo,`Convert TensorFlow model checkpoints of ALBERT to PyTorch using the
`),Pe=n(lo,"A",{href:!0,rel:!0});var _n=a(Pe);ar=l(_n,"convert_albert_original_tf_checkpoint_to_pytorch.py"),_n.forEach(t),sr=l(lo," script."),lo.forEach(t),Rt=h(e),C=n(e,"P",{});var Ye=a(C);lr=l(Ye,"The CLI takes as input a TensorFlow checkpoint (three files starting with "),st=n(Ye,"CODE",{});var mn=a(st);ir=l(mn,"model.ckpt-best"),mn.forEach(t),pr=l(Ye,`) and the accompanying
configuration file (`),lt=n(Ye,"CODE",{});var un=a(lt);cr=l(un,"albert_config.json"),un.forEach(t),fr=l(Ye,`), then creates and saves a PyTorch model. To run this conversion you will
need to have TensorFlow and PyTorch installed.`),Ye.forEach(t),Ht=h(e),te=n(e,"P",{});var io=a(te);hr=l(io,"Here is an example of the conversion process for the pre-trained "),it=n(io,"CODE",{});var dn=a(it);_r=l(dn,"ALBERT Base"),dn.forEach(t),mr=l(io," model:"),io.forEach(t),Lt=h(e),u(Ae.$$.fragment,e),xt=h(e),oe=n(e,"P",{});var po=a(oe);ur=l(po,"You can download Google\u2019s pre-trained models for the conversion "),be=n(po,"A",{href:!0,rel:!0});var vn=a(be);dr=l(vn,"here"),vn.forEach(t),vr=l(po,"."),po.forEach(t),St=h(e),D=n(e,"H2",{class:!0});var co=a(D);re=n(co,"A",{id:!0,class:!0,href:!0});var Tn=a(re);pt=n(Tn,"SPAN",{});var En=a(pt);u(ye.$$.fragment,En),En.forEach(t),Tn.forEach(t),Tr=h(co),ct=n(co,"SPAN",{});var $n=a(ct);Er=l($n,"OpenAI GPT"),$n.forEach(t),co.forEach(t),Ft=h(e),ne=n(e,"P",{});var fo=a(ne);$r=l(fo,`Here is an example of the conversion process for a pre-trained OpenAI GPT model, assuming that your NumPy checkpoint
save as the same format than OpenAI pretrained model (see `),ge=n(fo,"A",{href:!0,rel:!0});var Pn=a(ge);Pr=l(Pn,"here"),Pn.forEach(t),Ar=l(fo,`\\
)`),fo.forEach(t),Dt=h(e),u(we.$$.fragment,e),Bt=h(e),B=n(e,"H2",{class:!0});var ho=a(B);ae=n(ho,"A",{id:!0,class:!0,href:!0});var An=a(ae);ft=n(An,"SPAN",{});var bn=a(ft);u(ke.$$.fragment,bn),bn.forEach(t),An.forEach(t),br=h(ho),ht=n(ho,"SPAN",{});var yn=a(ht);yr=l(yn,"OpenAI GPT-2"),yn.forEach(t),ho.forEach(t),Gt=h(e),se=n(e,"P",{});var _o=a(se);gr=l(_o,"Here is an example of the conversion process for a pre-trained OpenAI GPT-2 model (see "),Oe=n(_o,"A",{href:!0,rel:!0});var gn=a(Oe);wr=l(gn,"here"),gn.forEach(t),kr=l(_o,")"),_o.forEach(t),Xt=h(e),u(Ne.$$.fragment,e),Ut=h(e),G=n(e,"H2",{class:!0});var mo=a(G);le=n(mo,"A",{id:!0,class:!0,href:!0});var wn=a(le);_t=n(wn,"SPAN",{});var kn=a(_t);u(Ie.$$.fragment,kn),kn.forEach(t),wn.forEach(t),Or=h(mo),mt=n(mo,"SPAN",{});var On=a(mt);Nr=l(On,"Transformer-XL"),On.forEach(t),mo.forEach(t),jt=h(e),ie=n(e,"P",{});var uo=a(ie);Ir=l(uo,"Here is an example of the conversion process for a pre-trained Transformer-XL model (see "),Ce=n(uo,"A",{href:!0,rel:!0});var Nn=a(Ce);Cr=l(Nn,"here"),Nn.forEach(t),Rr=l(uo,")"),uo.forEach(t),Kt=h(e),u(Re.$$.fragment,e),Mt=h(e),X=n(e,"H2",{class:!0});var vo=a(X);pe=n(vo,"A",{id:!0,class:!0,href:!0});var In=a(pe);ut=n(In,"SPAN",{});var Cn=a(ut);u(He.$$.fragment,Cn),Cn.forEach(t),In.forEach(t),Hr=h(vo),dt=n(vo,"SPAN",{});var Rn=a(dt);Lr=l(Rn,"XLNet"),Rn.forEach(t),vo.forEach(t),Yt=h(e),Ue=n(e,"P",{});var Hn=a(Ue);xr=l(Hn,"Here is an example of the conversion process for a pre-trained XLNet model:"),Hn.forEach(t),qt=h(e),u(Le.$$.fragment,e),Jt=h(e),U=n(e,"H2",{class:!0});var To=a(U);ce=n(To,"A",{id:!0,class:!0,href:!0});var Ln=a(ce);vt=n(Ln,"SPAN",{});var xn=a(vt);u(xe.$$.fragment,xn),xn.forEach(t),Ln.forEach(t),Sr=h(To),Tt=n(To,"SPAN",{});var Sn=a(Tt);Fr=l(Sn,"XLM"),Sn.forEach(t),To.forEach(t),zt=h(e),je=n(e,"P",{});var Fn=a(je);Dr=l(Fn,"Here is an example of the conversion process for a pre-trained XLM model:"),Fn.forEach(t),Qt=h(e),u(Se.$$.fragment,e),Vt=h(e),j=n(e,"H2",{class:!0});var Eo=a(j);fe=n(Eo,"A",{id:!0,class:!0,href:!0});var Dn=a(fe);Et=n(Dn,"SPAN",{});var Bn=a(Et);u(Fe.$$.fragment,Bn),Bn.forEach(t),Dn.forEach(t),Br=h(Eo),$t=n(Eo,"SPAN",{});var Gn=a($t);Gr=l(Gn,"T5"),Gn.forEach(t),Eo.forEach(t),Wt=h(e),Ke=n(e,"P",{});var Xn=a(Ke);Xr=l(Xn,"Here is an example of the conversion process for a pre-trained T5 model:"),Xn.forEach(t),Zt=h(e),u(De.$$.fragment,e),this.h()},h(){c(_,"name","hf:doc:metadata"),c(_,"content",JSON.stringify(Qn)),c(g,"id","converting-tensorflow-checkpoints"),c(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g,"href","#converting-tensorflow-checkpoints"),c(P,"class","relative group"),c(z,"id","bert"),c(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(z,"href","#bert"),c(S,"class","relative group"),c(ue,"href","https://github.com/google-research/bert#pre-trained-models"),c(ue,"rel","nofollow"),c(de,"href","https://github.com/huggingface/transformers/tree/main/src/transformers/models/bert/convert_bert_original_tf_checkpoint_to_pytorch.py"),c(de,"rel","nofollow"),c(Xe,"href","quicktour"),c(ve,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification/run_glue.py"),c(ve,"rel","nofollow"),c(Ee,"href","https://github.com/google-research/bert#pre-trained-models"),c(Ee,"rel","nofollow"),c(Z,"id","albert"),c(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Z,"href","#albert"),c(F,"class","relative group"),c(Pe,"href","https://github.com/huggingface/transformers/tree/main/src/transformers/models/albert/convert_albert_original_tf_checkpoint_to_pytorch.py"),c(Pe,"rel","nofollow"),c(be,"href","https://github.com/google-research/albert#pre-trained-models"),c(be,"rel","nofollow"),c(re,"id","openai-gpt"),c(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(re,"href","#openai-gpt"),c(D,"class","relative group"),c(ge,"href","https://github.com/openai/finetune-transformer-lm"),c(ge,"rel","nofollow"),c(ae,"id","openai-gpt2"),c(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ae,"href","#openai-gpt2"),c(B,"class","relative group"),c(Oe,"href","https://github.com/openai/gpt-2"),c(Oe,"rel","nofollow"),c(le,"id","transformerxl"),c(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(le,"href","#transformerxl"),c(G,"class","relative group"),c(Ce,"href","https://github.com/kimiyoung/transformer-xl/tree/master/tf#obtain-and-evaluate-pretrained-sota-models"),c(Ce,"rel","nofollow"),c(pe,"id","xlnet"),c(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pe,"href","#xlnet"),c(X,"class","relative group"),c(ce,"id","xlm"),c(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ce,"href","#xlm"),c(U,"class","relative group"),c(fe,"id","t5"),c(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fe,"href","#t5"),c(j,"class","relative group")},m(e,i){o(document.head,_),p(e,M,i),p(e,P,i),o(P,g),o(g,H),d(w,H,null),o(P,k),o(P,L),o(L,x),p(e,Y,i),p(e,O,i),o(O,A),o(O,b),o(b,Ge),o(O,q),p(e,_e,i),d(J,e,i),p(e,At,i),p(e,S,i),o(S,z),o(z,qe),d(me,qe,null),o(S,$o),o(S,Je),o(Je,Po),p(e,bt,i),p(e,I,i),o(I,Ao),o(I,ue),o(ue,bo),o(I,yo),o(I,de),o(de,go),o(I,wo),p(e,yt,i),p(e,$,i),o($,ko),o($,ze),o(ze,Oo),o($,No),o($,Qe),o(Qe,Io),o($,Co),o($,Ve),o(Ve,Ro),o($,Ho),o($,Xe),o(Xe,Lo),o($,xo),o($,ve),o(ve,So),o($,Fo),p(e,gt,i),p(e,y,i),o(y,Do),o(y,We),o(We,Bo),o(y,Go),o(y,Ze),o(Ze,Xo),o(y,Uo),o(y,et),o(et,jo),o(y,Ko),o(y,tt),o(tt,Mo),o(y,Yo),p(e,wt,i),p(e,Q,i),o(Q,qo),o(Q,ot),o(ot,Jo),o(Q,zo),p(e,kt,i),p(e,V,i),o(V,Qo),o(V,rt),o(rt,Vo),o(V,Wo),p(e,Ot,i),d(Te,e,i),p(e,Nt,i),p(e,W,i),o(W,Zo),o(W,Ee),o(Ee,er),o(W,tr),p(e,It,i),p(e,F,i),o(F,Z),o(Z,nt),d($e,nt,null),o(F,or),o(F,at),o(at,rr),p(e,Ct,i),p(e,ee,i),o(ee,nr),o(ee,Pe),o(Pe,ar),o(ee,sr),p(e,Rt,i),p(e,C,i),o(C,lr),o(C,st),o(st,ir),o(C,pr),o(C,lt),o(lt,cr),o(C,fr),p(e,Ht,i),p(e,te,i),o(te,hr),o(te,it),o(it,_r),o(te,mr),p(e,Lt,i),d(Ae,e,i),p(e,xt,i),p(e,oe,i),o(oe,ur),o(oe,be),o(be,dr),o(oe,vr),p(e,St,i),p(e,D,i),o(D,re),o(re,pt),d(ye,pt,null),o(D,Tr),o(D,ct),o(ct,Er),p(e,Ft,i),p(e,ne,i),o(ne,$r),o(ne,ge),o(ge,Pr),o(ne,Ar),p(e,Dt,i),d(we,e,i),p(e,Bt,i),p(e,B,i),o(B,ae),o(ae,ft),d(ke,ft,null),o(B,br),o(B,ht),o(ht,yr),p(e,Gt,i),p(e,se,i),o(se,gr),o(se,Oe),o(Oe,wr),o(se,kr),p(e,Xt,i),d(Ne,e,i),p(e,Ut,i),p(e,G,i),o(G,le),o(le,_t),d(Ie,_t,null),o(G,Or),o(G,mt),o(mt,Nr),p(e,jt,i),p(e,ie,i),o(ie,Ir),o(ie,Ce),o(Ce,Cr),o(ie,Rr),p(e,Kt,i),d(Re,e,i),p(e,Mt,i),p(e,X,i),o(X,pe),o(pe,ut),d(He,ut,null),o(X,Hr),o(X,dt),o(dt,Lr),p(e,Yt,i),p(e,Ue,i),o(Ue,xr),p(e,qt,i),d(Le,e,i),p(e,Jt,i),p(e,U,i),o(U,ce),o(ce,vt),d(xe,vt,null),o(U,Sr),o(U,Tt),o(Tt,Fr),p(e,zt,i),p(e,je,i),o(je,Dr),p(e,Qt,i),d(Se,e,i),p(e,Vt,i),p(e,j,i),o(j,fe),o(fe,Et),d(Fe,Et,null),o(j,Br),o(j,$t),o($t,Gr),p(e,Wt,i),p(e,Ke,i),o(Ke,Xr),p(e,Zt,i),d(De,e,i),eo=!0},p(e,[i]){const Be={};i&2&&(Be.$$scope={dirty:i,ctx:e}),J.$set(Be)},i(e){eo||(v(w.$$.fragment,e),v(J.$$.fragment,e),v(me.$$.fragment,e),v(Te.$$.fragment,e),v($e.$$.fragment,e),v(Ae.$$.fragment,e),v(ye.$$.fragment,e),v(we.$$.fragment,e),v(ke.$$.fragment,e),v(Ne.$$.fragment,e),v(Ie.$$.fragment,e),v(Re.$$.fragment,e),v(He.$$.fragment,e),v(Le.$$.fragment,e),v(xe.$$.fragment,e),v(Se.$$.fragment,e),v(Fe.$$.fragment,e),v(De.$$.fragment,e),eo=!0)},o(e){T(w.$$.fragment,e),T(J.$$.fragment,e),T(me.$$.fragment,e),T(Te.$$.fragment,e),T($e.$$.fragment,e),T(Ae.$$.fragment,e),T(ye.$$.fragment,e),T(we.$$.fragment,e),T(ke.$$.fragment,e),T(Ne.$$.fragment,e),T(Ie.$$.fragment,e),T(Re.$$.fragment,e),T(He.$$.fragment,e),T(Le.$$.fragment,e),T(xe.$$.fragment,e),T(Se.$$.fragment,e),T(Fe.$$.fragment,e),T(De.$$.fragment,e),eo=!1},d(e){t(_),e&&t(M),e&&t(P),E(w),e&&t(Y),e&&t(O),e&&t(_e),E(J,e),e&&t(At),e&&t(S),E(me),e&&t(bt),e&&t(I),e&&t(yt),e&&t($),e&&t(gt),e&&t(y),e&&t(wt),e&&t(Q),e&&t(kt),e&&t(V),e&&t(Ot),E(Te,e),e&&t(Nt),e&&t(W),e&&t(It),e&&t(F),E($e),e&&t(Ct),e&&t(ee),e&&t(Rt),e&&t(C),e&&t(Ht),e&&t(te),e&&t(Lt),E(Ae,e),e&&t(xt),e&&t(oe),e&&t(St),e&&t(D),E(ye),e&&t(Ft),e&&t(ne),e&&t(Dt),E(we,e),e&&t(Bt),e&&t(B),E(ke),e&&t(Gt),e&&t(se),e&&t(Xt),E(Ne,e),e&&t(Ut),e&&t(G),E(Ie),e&&t(jt),e&&t(ie),e&&t(Kt),E(Re,e),e&&t(Mt),e&&t(X),E(He),e&&t(Yt),e&&t(Ue),e&&t(qt),E(Le,e),e&&t(Jt),e&&t(U),E(xe),e&&t(zt),e&&t(je),e&&t(Qt),E(Se,e),e&&t(Vt),e&&t(j),E(Fe),e&&t(Wt),e&&t(Ke),e&&t(Zt),E(De,e)}}}const Qn={local:"converting-tensorflow-checkpoints",sections:[{local:"bert",title:"BERT"},{local:"albert",title:"ALBERT"},{local:"openai-gpt",title:"OpenAI GPT"},{local:"openai-gpt2",title:"OpenAI GPT-2"},{local:"transformerxl",title:"Transformer-XL"},{local:"xlnet",title:"XLNet"},{local:"xlm",title:"XLM"},{local:"t5",title:"T5"}],title:"Converting Tensorflow Checkpoints"};function Vn(Pt){return Yn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class oa extends Un{constructor(_){super();jn(this,_,Vn,zn,Kn,{})}}export{oa as default,Qn as metadata};
