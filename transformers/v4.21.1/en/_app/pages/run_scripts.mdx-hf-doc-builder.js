import{S as Gu,i as Bu,s as Ju,e as a,k as h,w as E,t as s,M as Wu,c as o,d as r,m,a as i,x as b,h as l,b as p,G as t,g as u,y as x,q as z,o as T,B as k,v as Xu,L as ii}from"../chunks/vendor-hf-doc-builder.js";import{I as re}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as M}from"../chunks/CodeBlock-hf-doc-builder.js";import{F as Hu,M as si}from"../chunks/Markdown-hf-doc-builder.js";function Ku(H){let f,w,c,v,q,$,O,j,I,F,A,S,P,C,N,L,R,y,g,G;return g=new M({props:{code:`python examples/pytorch/summarization/run_summarization.py \\
    --model_name_or_path t5-small \\
    --do_train \\
    --do_eval \\
    --dataset_name cnn_dailymail \\
    --dataset_config "3.0.0" \\
    --source_prefix "summarize: " \\
    --output_dir /tmp/tst-summarization \\
    --per_device_train_batch_size=4 \\
    --per_device_eval_batch_size=4 \\
    --overwrite_output_dir \\
    --predict_with_generate`,highlighted:`python examples/pytorch/summarization/run_summarization.py \\
    --model_name_or_path t5-small \\
    --do_train \\
    --do_eval \\
    --dataset_name cnn_dailymail \\
    --dataset_config <span class="hljs-string">&quot;3.0.0&quot;</span> \\
    --source_prefix <span class="hljs-string">&quot;summarize: &quot;</span> \\
    --output_dir /tmp/tst-summarization \\
    --per_device_train_batch_size=4 \\
    --per_device_eval_batch_size=4 \\
    --overwrite_output_dir \\
    --predict_with_generate`}}),{c(){f=a("p"),w=s("The example script downloads and preprocesses a dataset from the \u{1F917} "),c=a("a"),v=s("Datasets"),q=s(" library. Then the script fine-tunes a dataset with the "),$=a("a"),O=s("Trainer"),j=s(" on an architecture that supports summarization. The following example shows how to fine-tune "),I=a("a"),F=s("T5-small"),A=s(" on the "),S=a("a"),P=s("CNN/DailyMail"),C=s(" dataset. The T5 model requires an additional "),N=a("code"),L=s("source_prefix"),R=s(" argument due to how it was trained. This prompt lets T5 know this is a summarization task."),y=h(),E(g.$$.fragment),this.h()},l(D){f=o(D,"P",{});var U=i(f);w=l(U,"The example script downloads and preprocesses a dataset from the \u{1F917} "),c=o(U,"A",{href:!0,rel:!0});var J=i(c);v=l(J,"Datasets"),J.forEach(r),q=l(U," library. Then the script fine-tunes a dataset with the "),$=o(U,"A",{href:!0,rel:!0});var yt=i($);O=l(yt,"Trainer"),yt.forEach(r),j=l(U," on an architecture that supports summarization. The following example shows how to fine-tune "),I=o(U,"A",{href:!0,rel:!0});var De=i(I);F=l(De,"T5-small"),De.forEach(r),A=l(U," on the "),S=o(U,"A",{href:!0,rel:!0});var B=i(S);P=l(B,"CNN/DailyMail"),B.forEach(r),C=l(U," dataset. The T5 model requires an additional "),N=o(U,"CODE",{});var $t=i(N);L=l($t,"source_prefix"),$t.forEach(r),R=l(U," argument due to how it was trained. This prompt lets T5 know this is a summarization task."),U.forEach(r),y=m(D),b(g.$$.fragment,D),this.h()},h(){p(c,"href","https://huggingface.co/docs/datasets/"),p(c,"rel","nofollow"),p($,"href","https://huggingface.co/docs/transformers/main_classes/trainer"),p($,"rel","nofollow"),p(I,"href","https://huggingface.co/t5-small"),p(I,"rel","nofollow"),p(S,"href","https://huggingface.co/datasets/cnn_dailymail"),p(S,"rel","nofollow")},m(D,U){u(D,f,U),t(f,w),t(f,c),t(c,v),t(f,q),t(f,$),t($,O),t(f,j),t(f,I),t(I,F),t(f,A),t(f,S),t(S,P),t(f,C),t(f,N),t(N,L),t(f,R),u(D,y,U),x(g,D,U),G=!0},p:ii,i(D){G||(z(g.$$.fragment,D),G=!0)},o(D){T(g.$$.fragment,D),G=!1},d(D){D&&r(f),D&&r(y),k(g,D)}}}function Yu(H){let f,w;return f=new si({props:{$$slots:{default:[Ku]},$$scope:{ctx:H}}}),{c(){E(f.$$.fragment)},l(c){b(f.$$.fragment,c)},m(c,v){x(f,c,v),w=!0},p(c,v){const q={};v&2&&(q.$$scope={dirty:v,ctx:c}),f.$set(q)},i(c){w||(z(f.$$.fragment,c),w=!0)},o(c){T(f.$$.fragment,c),w=!1},d(c){k(f,c)}}}function Vu(H){let f,w,c,v,q,$,O,j,I,F,A,S,P,C,N,L,R;return L=new M({props:{code:`python examples/tensorflow/summarization/run_summarization.py  \\
    --model_name_or_path t5-small \\
    --dataset_name cnn_dailymail \\
    --dataset_config "3.0.0" \\
    --output_dir /tmp/tst-summarization  \\
    --per_device_train_batch_size 8 \\
    --per_device_eval_batch_size 16 \\
    --num_train_epochs 3 \\
    --do_train \\
    --do_eval`,highlighted:`python examples/tensorflow/summarization/run_summarization.py  \\
    --model_name_or_path t5-small \\
    --dataset_name cnn_dailymail \\
    --dataset_config <span class="hljs-string">&quot;3.0.0&quot;</span> \\
    --output_dir /tmp/tst-summarization  \\
    --per_device_train_batch_size 8 \\
    --per_device_eval_batch_size 16 \\
    --num_train_epochs 3 \\
    --do_train \\
    --do_eval`}}),{c(){f=a("p"),w=s("The example script downloads and preprocesses a dataset from the \u{1F917} "),c=a("a"),v=s("Datasets"),q=s(" library. Then the script fine-tunes a dataset using Keras on an architecture that supports summarization. The following example shows how to fine-tune "),$=a("a"),O=s("T5-small"),j=s(" on the "),I=a("a"),F=s("CNN/DailyMail"),A=s(" dataset. The T5 model requires an additional "),S=a("code"),P=s("source_prefix"),C=s(" argument due to how it was trained. This prompt lets T5 know this is a summarization task."),N=h(),E(L.$$.fragment),this.h()},l(y){f=o(y,"P",{});var g=i(f);w=l(g,"The example script downloads and preprocesses a dataset from the \u{1F917} "),c=o(g,"A",{href:!0,rel:!0});var G=i(c);v=l(G,"Datasets"),G.forEach(r),q=l(g," library. Then the script fine-tunes a dataset using Keras on an architecture that supports summarization. The following example shows how to fine-tune "),$=o(g,"A",{href:!0,rel:!0});var D=i($);O=l(D,"T5-small"),D.forEach(r),j=l(g," on the "),I=o(g,"A",{href:!0,rel:!0});var U=i(I);F=l(U,"CNN/DailyMail"),U.forEach(r),A=l(g," dataset. The T5 model requires an additional "),S=o(g,"CODE",{});var J=i(S);P=l(J,"source_prefix"),J.forEach(r),C=l(g," argument due to how it was trained. This prompt lets T5 know this is a summarization task."),g.forEach(r),N=m(y),b(L.$$.fragment,y),this.h()},h(){p(c,"href","https://huggingface.co/docs/datasets/"),p(c,"rel","nofollow"),p($,"href","https://huggingface.co/t5-small"),p($,"rel","nofollow"),p(I,"href","https://huggingface.co/datasets/cnn_dailymail"),p(I,"rel","nofollow")},m(y,g){u(y,f,g),t(f,w),t(f,c),t(c,v),t(f,q),t(f,$),t($,O),t(f,j),t(f,I),t(I,F),t(f,A),t(f,S),t(S,P),t(f,C),u(y,N,g),x(L,y,g),R=!0},p:ii,i(y){R||(z(L.$$.fragment,y),R=!0)},o(y){T(L.$$.fragment,y),R=!1},d(y){y&&r(f),y&&r(N),k(L,y)}}}function Qu(H){let f,w;return f=new si({props:{$$slots:{default:[Vu]},$$scope:{ctx:H}}}),{c(){E(f.$$.fragment)},l(c){b(f.$$.fragment,c)},m(c,v){x(f,c,v),w=!0},p(c,v){const q={};v&2&&(q.$$scope={dirty:v,ctx:c}),f.$set(q)},i(c){w||(z(f.$$.fragment,c),w=!0)},o(c){T(f.$$.fragment,c),w=!1},d(c){k(f,c)}}}function Zu(H){let f,w,c,v,q,$,O,j,I,F,A,S,P,C,N,L,R;return L=new M({props:{code:`python xla_spawn.py --num_cores 8 \\
    summarization/run_summarization.py \\
    --model_name_or_path t5-small \\
    --do_train \\
    --do_eval \\
    --dataset_name cnn_dailymail \\
    --dataset_config "3.0.0" \\
    --source_prefix "summarize: " \\
    --output_dir /tmp/tst-summarization \\
    --per_device_train_batch_size=4 \\
    --per_device_eval_batch_size=4 \\
    --overwrite_output_dir \\
    --predict_with_generate`,highlighted:`python xla_spawn.py --num_cores 8 \\
    summarization/run_summarization.py \\
    --model_name_or_path t5-small \\
    --do_train \\
    --do_eval \\
    --dataset_name cnn_dailymail \\
    --dataset_config <span class="hljs-string">&quot;3.0.0&quot;</span> \\
    --source_prefix <span class="hljs-string">&quot;summarize: &quot;</span> \\
    --output_dir /tmp/tst-summarization \\
    --per_device_train_batch_size=4 \\
    --per_device_eval_batch_size=4 \\
    --overwrite_output_dir \\
    --predict_with_generate`}}),{c(){f=a("p"),w=s("Tensor Processing Units (TPUs) are specifically designed to accelerate performance. PyTorch supports TPUs with the "),c=a("a"),v=s("XLA"),q=s(" deep learning compiler (see "),$=a("a"),O=s("here"),j=s(" for more details). To use a TPU, launch the "),I=a("code"),F=s("xla_spawn.py"),A=s(" script and use the "),S=a("code"),P=s("num_cores"),C=s(" argument to set the number of TPU cores you want to use."),N=h(),E(L.$$.fragment),this.h()},l(y){f=o(y,"P",{});var g=i(f);w=l(g,"Tensor Processing Units (TPUs) are specifically designed to accelerate performance. PyTorch supports TPUs with the "),c=o(g,"A",{href:!0,rel:!0});var G=i(c);v=l(G,"XLA"),G.forEach(r),q=l(g," deep learning compiler (see "),$=o(g,"A",{href:!0,rel:!0});var D=i($);O=l(D,"here"),D.forEach(r),j=l(g," for more details). To use a TPU, launch the "),I=o(g,"CODE",{});var U=i(I);F=l(U,"xla_spawn.py"),U.forEach(r),A=l(g," script and use the "),S=o(g,"CODE",{});var J=i(S);P=l(J,"num_cores"),J.forEach(r),C=l(g," argument to set the number of TPU cores you want to use."),g.forEach(r),N=m(y),b(L.$$.fragment,y),this.h()},h(){p(c,"href","https://www.tensorflow.org/xla"),p(c,"rel","nofollow"),p($,"href","https://github.com/pytorch/xla/blob/master/README.md"),p($,"rel","nofollow")},m(y,g){u(y,f,g),t(f,w),t(f,c),t(c,v),t(f,q),t(f,$),t($,O),t(f,j),t(f,I),t(I,F),t(f,A),t(f,S),t(S,P),t(f,C),u(y,N,g),x(L,y,g),R=!0},p:ii,i(y){R||(z(L.$$.fragment,y),R=!0)},o(y){T(L.$$.fragment,y),R=!1},d(y){y&&r(f),y&&r(N),k(L,y)}}}function eh(H){let f,w;return f=new si({props:{$$slots:{default:[Zu]},$$scope:{ctx:H}}}),{c(){E(f.$$.fragment)},l(c){b(f.$$.fragment,c)},m(c,v){x(f,c,v),w=!0},p(c,v){const q={};v&2&&(q.$$scope={dirty:v,ctx:c}),f.$set(q)},i(c){w||(z(f.$$.fragment,c),w=!0)},o(c){T(f.$$.fragment,c),w=!1},d(c){k(f,c)}}}function th(H){let f,w,c,v,q,$,O,j,I,F,A,S;return A=new M({props:{code:`python run_summarization.py  \\
    --tpu name_of_tpu_resource \\
    --model_name_or_path t5-small \\
    --dataset_name cnn_dailymail \\
    --dataset_config "3.0.0" \\
    --output_dir /tmp/tst-summarization  \\
    --per_device_train_batch_size 8 \\
    --per_device_eval_batch_size 16 \\
    --num_train_epochs 3 \\
    --do_train \\
    --do_eval`,highlighted:`python run_summarization.py  \\
    --tpu name_of_tpu_resource \\
    --model_name_or_path t5-small \\
    --dataset_name cnn_dailymail \\
    --dataset_config <span class="hljs-string">&quot;3.0.0&quot;</span> \\
    --output_dir /tmp/tst-summarization  \\
    --per_device_train_batch_size 8 \\
    --per_device_eval_batch_size 16 \\
    --num_train_epochs 3 \\
    --do_train \\
    --do_eval`}}),{c(){f=a("p"),w=s("Tensor Processing Units (TPUs) are specifically designed to accelerate performance. TensorFlow scripts utilize a "),c=a("a"),v=a("code"),q=s("TPUStrategy"),$=s(" for training on TPUs. To use a TPU, pass the name of the TPU resource to the "),O=a("code"),j=s("tpu"),I=s(" argument."),F=h(),E(A.$$.fragment),this.h()},l(P){f=o(P,"P",{});var C=i(f);w=l(C,"Tensor Processing Units (TPUs) are specifically designed to accelerate performance. TensorFlow scripts utilize a "),c=o(C,"A",{href:!0,rel:!0});var N=i(c);v=o(N,"CODE",{});var L=i(v);q=l(L,"TPUStrategy"),L.forEach(r),N.forEach(r),$=l(C," for training on TPUs. To use a TPU, pass the name of the TPU resource to the "),O=o(C,"CODE",{});var R=i(O);j=l(R,"tpu"),R.forEach(r),I=l(C," argument."),C.forEach(r),F=m(P),b(A.$$.fragment,P),this.h()},h(){p(c,"href","https://www.tensorflow.org/guide/distributed_training#tpustrategy"),p(c,"rel","nofollow")},m(P,C){u(P,f,C),t(f,w),t(f,c),t(c,v),t(v,q),t(f,$),t(f,O),t(O,j),t(f,I),u(P,F,C),x(A,P,C),S=!0},p:ii,i(P){S||(z(A.$$.fragment,P),S=!0)},o(P){T(A.$$.fragment,P),S=!1},d(P){P&&r(f),P&&r(F),k(A,P)}}}function rh(H){let f,w;return f=new si({props:{$$slots:{default:[th]},$$scope:{ctx:H}}}),{c(){E(f.$$.fragment)},l(c){b(f.$$.fragment,c)},m(c,v){x(f,c,v),w=!0},p(c,v){const q={};v&2&&(q.$$scope={dirty:v,ctx:c}),f.$set(q)},i(c){w||(z(f.$$.fragment,c),w=!0)},o(c){T(f.$$.fragment,c),w=!1},d(c){k(f,c)}}}function ah(H){let f,w,c,v,q,$,O,j,I,F,A,S,P,C,N,L,R,y,g,G,D,U,J,yt,De,B,$t,Ne,li,ni,Oe,pi,ui,Da,Et,hi,Na,K,mi,je,fi,ci,Fe,_i,di,Oa,Y,vi,Re,gi,wi,Me,yi,$i,ja,ae,me,_r,He,Ei,dr,bi,Fa,fe,xi,vr,zi,Ti,Ra,Ge,Ma,bt,ki,Ha,ce,gr,Ai,Pi,_,wr,xt,qi,Li,yr,zt,Ui,Ii,$r,Tt,Si,Ci,Er,kt,Di,Ni,br,At,Oi,ji,xr,Pt,Fi,Ri,zr,qt,Mi,Hi,Tr,Lt,Gi,Bi,kr,Ut,Ji,Wi,Ar,It,Xi,Ki,Pr,St,Yi,Vi,qr,Ct,Qi,Zi,Lr,Dt,es,ts,Ur,Nt,rs,as,Ir,Ot,os,is,Sr,jt,ss,ls,Cr,Ft,ns,ps,Dr,Rt,us,hs,Nr,Mt,ms,fs,Or,Ht,cs,_s,jr,Gt,ds,vs,Fr,Bt,gs,ws,Rr,Jt,ys,$s,Mr,Wt,Es,bs,Hr,Xt,xs,zs,Gr,Kt,Ts,ks,Br,Yt,As,Ga,Vt,Ps,Ba,Be,Ja,Qt,qs,Wa,Je,Xa,oe,_e,Jr,We,Ls,Wr,Us,Ka,de,Ya,ie,ve,Xr,Xe,Is,Kr,Ss,Va,ge,Cs,Ke,Ds,Ns,Qa,we,Ye,Os,Yr,js,Fs,Rs,Ve,Ms,Vr,Hs,Gs,Za,Qe,eo,ye,Bs,Ze,Qr,Js,Ws,to,se,$e,Zr,et,Xs,ea,Ks,ro,Ee,ao,le,be,ta,tt,Ys,ra,Vs,oo,xe,Qs,rt,Zs,el,io,ze,aa,tl,rl,at,so,W,al,oa,ol,il,ia,sl,ll,sa,nl,pl,lo,ot,no,Zt,ul,po,it,uo,er,hl,ho,st,mo,ne,Te,la,lt,ml,na,fl,fo,tr,cl,co,V,ke,pa,_l,dl,ua,vl,gl,wl,rr,ha,yl,$l,El,ar,ma,bl,xl,_o,or,zl,vo,nt,go,pe,Ae,fa,pt,Tl,ca,kl,wo,ir,Al,yo,Q,_a,da,Pl,ql,va,ga,Ll,Ul,wa,ya,Il,$o,ut,Eo,Z,Sl,$a,Cl,Dl,Ea,Nl,Ol,bo,ht,xo,ue,Pe,ba,mt,jl,xa,Fl,zo,sr,Rl,To,X,Ml,za,Hl,Gl,Ta,Bl,Jl,ka,Wl,Xl,ko,ft,Ao,qe,Kl,Aa,Yl,Vl,Po,ct,qo,he,Le,Pa,_t,Ql,qa,Zl,Lo,Ue,en,dt,tn,rn,Uo,vt,Io,ee,an,La,on,sn,Ua,ln,nn,So,Ie,pn,Ia,un,hn,Co,lr,mn,Do,gt,No;return $=new re({}),He=new re({}),Ge=new M({props:{code:`git clone https://github.com/huggingface/transformers
cd transformers
pip install .`,highlighted:`git <span class="hljs-built_in">clone</span> https://github.com/huggingface/transformers
<span class="hljs-built_in">cd</span> transformers
pip install .`}}),Be=new M({props:{code:"git checkout tags/v3.5.1",highlighted:"git checkout tags/v3.5.1"}}),Je=new M({props:{code:"pip install -r requirements.txt",highlighted:"pip install -r requirements.txt"}}),We=new re({}),de=new Hu({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Qu],pytorch:[Yu]},$$scope:{ctx:H}}}),Xe=new re({}),Qe=new M({props:{code:`python -m torch.distributed.launch \\
    --nproc_per_node 8 pytorch/summarization/run_summarization.py \\
    --fp16 \\
    --model_name_or_path t5-small \\
    --do_train \\
    --do_eval \\
    --dataset_name cnn_dailymail \\
    --dataset_config "3.0.0" \\
    --source_prefix "summarize: " \\
    --output_dir /tmp/tst-summarization \\
    --per_device_train_batch_size=4 \\
    --per_device_eval_batch_size=4 \\
    --overwrite_output_dir \\
    --predict_with_generate`,highlighted:`python -m torch.distributed.launch \\
    --nproc_per_node 8 pytorch/summarization/run_summarization.py \\
    --fp16 \\
    --model_name_or_path t5-small \\
    --do_train \\
    --do_eval \\
    --dataset_name cnn_dailymail \\
    --dataset_config <span class="hljs-string">&quot;3.0.0&quot;</span> \\
    --source_prefix <span class="hljs-string">&quot;summarize: &quot;</span> \\
    --output_dir /tmp/tst-summarization \\
    --per_device_train_batch_size=4 \\
    --per_device_eval_batch_size=4 \\
    --overwrite_output_dir \\
    --predict_with_generate`}}),et=new re({}),Ee=new Hu({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[rh],pytorch:[eh]},$$scope:{ctx:H}}}),tt=new re({}),at=new M({props:{code:"pip install git+https://github.com/huggingface/accelerate",highlighted:"pip install git+https://github.com/huggingface/accelerate"}}),ot=new M({props:{code:"accelerate config",highlighted:"accelerate config"}}),it=new M({props:{code:"accelerate test",highlighted:'accelerate <span class="hljs-built_in">test</span>'}}),st=new M({props:{code:`accelerate launch run_summarization_no_trainer.py \\
    --model_name_or_path t5-small \\
    --dataset_name cnn_dailymail \\
    --dataset_config "3.0.0" \\
    --source_prefix "summarize: " \\
    --output_dir ~/tmp/tst-summarization`,highlighted:`accelerate launch run_summarization_no_trainer.py \\
    --model_name_or_path t5-small \\
    --dataset_name cnn_dailymail \\
    --dataset_config <span class="hljs-string">&quot;3.0.0&quot;</span> \\
    --source_prefix <span class="hljs-string">&quot;summarize: &quot;</span> \\
    --output_dir ~/tmp/tst-summarization`}}),lt=new re({}),nt=new M({props:{code:`python examples/pytorch/summarization/run_summarization.py \\
    --model_name_or_path t5-small \\
    --do_train \\
    --do_eval \\
    --train_file path_to_csv_or_jsonlines_file \\
    --validation_file path_to_csv_or_jsonlines_file \\
    --text_column text_column_name \\
    --summary_column summary_column_name \\
    --source_prefix "summarize: " \\
    --output_dir /tmp/tst-summarization \\
    --overwrite_output_dir \\
    --per_device_train_batch_size=4 \\
    --per_device_eval_batch_size=4 \\
    --predict_with_generate`,highlighted:`python examples/pytorch/summarization/run_summarization.py \\
    --model_name_or_path t5-small \\
    --do_train \\
    --do_eval \\
    --train_file path_to_csv_or_jsonlines_file \\
    --validation_file path_to_csv_or_jsonlines_file \\
    --text_column text_column_name \\
    --summary_column summary_column_name \\
    --source_prefix <span class="hljs-string">&quot;summarize: &quot;</span> \\
    --output_dir /tmp/tst-summarization \\
    --overwrite_output_dir \\
    --per_device_train_batch_size=4 \\
    --per_device_eval_batch_size=4 \\
    --predict_with_generate`}}),pt=new re({}),ut=new M({props:{code:`python examples/pytorch/summarization/run_summarization.py \\
    --model_name_or_path t5-small \\
    --max_train_samples 50 \\
    --max_eval_samples 50 \\
    --max_predict_samples 50 \\
    --do_train \\
    --do_eval \\
    --dataset_name cnn_dailymail \\
    --dataset_config "3.0.0" \\
    --source_prefix "summarize: " \\
    --output_dir /tmp/tst-summarization \\
    --per_device_train_batch_size=4 \\
    --per_device_eval_batch_size=4 \\
    --overwrite_output_dir \\
    --predict_with_generate`,highlighted:`python examples/pytorch/summarization/run_summarization.py \\
    --model_name_or_path t5-small \\
    --max_train_samples 50 \\
    --max_eval_samples 50 \\
    --max_predict_samples 50 \\
    --do_train \\
    --do_eval \\
    --dataset_name cnn_dailymail \\
    --dataset_config <span class="hljs-string">&quot;3.0.0&quot;</span> \\
    --source_prefix <span class="hljs-string">&quot;summarize: &quot;</span> \\
    --output_dir /tmp/tst-summarization \\
    --per_device_train_batch_size=4 \\
    --per_device_eval_batch_size=4 \\
    --overwrite_output_dir \\
    --predict_with_generate`}}),ht=new M({props:{code:"examples/pytorch/summarization/run_summarization.py -h",highlighted:"examples/pytorch/summarization/run_summarization.py -h"}}),mt=new re({}),ft=new M({props:{code:`python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path t5-small \\
    --do_train \\
    --do_eval \\
    --dataset_name cnn_dailymail \\
    --dataset_config "3.0.0" \\
    --source_prefix "summarize: " \\
    --output_dir /tmp/tst-summarization \\
    --per_device_train_batch_size=4 \\
    --per_device_eval_batch_size=4 \\
    --output_dir previous_output_dir \\
    --predict_with_generate`,highlighted:`python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path t5-small \\
    --do_train \\
    --do_eval \\
    --dataset_name cnn_dailymail \\
    --dataset_config <span class="hljs-string">&quot;3.0.0&quot;</span> \\
    --source_prefix <span class="hljs-string">&quot;summarize: &quot;</span> \\
    --output_dir /tmp/tst-summarization \\
    --per_device_train_batch_size=4 \\
    --per_device_eval_batch_size=4 \\
    --output_dir previous_output_dir \\
    --predict_with_generate`}}),ct=new M({props:{code:`python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path t5-small \\
    --do_train \\
    --do_eval \\
    --dataset_name cnn_dailymail \\
    --dataset_config "3.0.0" \\
    --source_prefix "summarize: " \\
    --output_dir /tmp/tst-summarization \\
    --per_device_train_batch_size=4 \\
    --per_device_eval_batch_size=4 \\
    --overwrite_output_dir \\
    --resume_from_checkpoint path_to_specific_checkpoint \\
    --predict_with_generate`,highlighted:`python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path t5-small \\
    --do_train \\
    --do_eval \\
    --dataset_name cnn_dailymail \\
    --dataset_config <span class="hljs-string">&quot;3.0.0&quot;</span> \\
    --source_prefix <span class="hljs-string">&quot;summarize: &quot;</span> \\
    --output_dir /tmp/tst-summarization \\
    --per_device_train_batch_size=4 \\
    --per_device_eval_batch_size=4 \\
    --overwrite_output_dir \\
    --resume_from_checkpoint path_to_specific_checkpoint \\
    --predict_with_generate`}}),_t=new re({}),vt=new M({props:{code:"huggingface-cli login",highlighted:"huggingface-cli login"}}),gt=new M({props:{code:`python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path t5-small \\
    --do_train \\
    --do_eval \\
    --dataset_name cnn_dailymail \\
    --dataset_config "3.0.0" \\
    --source_prefix "summarize: " \\
    --push_to_hub \\
    --push_to_hub_model_id finetuned-t5-cnn_dailymail \\
    --output_dir /tmp/tst-summarization \\
    --per_device_train_batch_size=4 \\
    --per_device_eval_batch_size=4 \\
    --overwrite_output_dir \\
    --predict_with_generate`,highlighted:`python examples/pytorch/summarization/run_summarization.py
    --model_name_or_path t5-small \\
    --do_train \\
    --do_eval \\
    --dataset_name cnn_dailymail \\
    --dataset_config <span class="hljs-string">&quot;3.0.0&quot;</span> \\
    --source_prefix <span class="hljs-string">&quot;summarize: &quot;</span> \\
    --push_to_hub \\
    --push_to_hub_model_id finetuned-t5-cnn_dailymail \\
    --output_dir /tmp/tst-summarization \\
    --per_device_train_batch_size=4 \\
    --per_device_eval_batch_size=4 \\
    --overwrite_output_dir \\
    --predict_with_generate`}}),{c(){f=a("meta"),w=h(),c=a("h1"),v=a("a"),q=a("span"),E($.$$.fragment),O=h(),j=a("span"),I=s("Train with a script"),F=h(),A=a("p"),S=s("Along with the \u{1F917} Transformers "),P=a("a"),C=s("notebooks"),N=s(", there are also example scripts demonstrating how to train a model for a task with "),L=a("a"),R=s("PyTorch"),y=s(", "),g=a("a"),G=s("TensorFlow"),D=s(", or "),U=a("a"),J=s("JAX/Flax"),yt=s("."),De=h(),B=a("p"),$t=s("You will also find scripts we\u2019ve used in our "),Ne=a("a"),li=s("research projects"),ni=s(" and "),Oe=a("a"),pi=s("legacy examples"),ui=s(" which are mostly community contributed. These scripts are not actively maintained and require a specific version of \u{1F917} Transformers that will most likely be incompatible with the latest version of the library."),Da=h(),Et=a("p"),hi=s("The example scripts are not expected to work out-of-the-box on every problem, and you may need to adapt the script to the problem you\u2019re trying to solve. To help you with this, most of the scripts fully expose how data is preprocessed, allowing you to edit it as necessary for your use case."),Na=h(),K=a("p"),mi=s("For any feature you\u2019d like to implement in an example script, please discuss it on the "),je=a("a"),fi=s("forum"),ci=s(" or in an "),Fe=a("a"),_i=s("issue"),di=s(" before submitting a Pull Request. While we welcome bug fixes, it is unlikely we will merge a Pull Request that adds more functionality at the cost of readability."),Oa=h(),Y=a("p"),vi=s("This guide will show you how to run an example summarization training script in "),Re=a("a"),gi=s("PyTorch"),wi=s(" and "),Me=a("a"),yi=s("TensorFlow"),$i=s(". All examples are expected to work with both frameworks unless otherwise specified."),ja=h(),ae=a("h2"),me=a("a"),_r=a("span"),E(He.$$.fragment),Ei=h(),dr=a("span"),bi=s("Setup"),Fa=h(),fe=a("p"),xi=s("To successfully run the latest version of the example scripts, you have to "),vr=a("strong"),zi=s("install \u{1F917} Transformers from source"),Ti=s(" in a new virtual environment:"),Ra=h(),E(Ge.$$.fragment),Ma=h(),bt=a("p"),ki=s("For older versions of the example scripts, click on the toggle below:"),Ha=h(),ce=a("details"),gr=a("summary"),Ai=s("Examples for older versions of \u{1F917} Transformers"),Pi=h(),_=a("ul"),wr=a("li"),xt=a("a"),qi=s("v4.5.1"),Li=h(),yr=a("li"),zt=a("a"),Ui=s("v4.4.2"),Ii=h(),$r=a("li"),Tt=a("a"),Si=s("v4.3.3"),Ci=h(),Er=a("li"),kt=a("a"),Di=s("v4.2.2"),Ni=h(),br=a("li"),At=a("a"),Oi=s("v4.1.1"),ji=h(),xr=a("li"),Pt=a("a"),Fi=s("v4.0.1"),Ri=h(),zr=a("li"),qt=a("a"),Mi=s("v3.5.1"),Hi=h(),Tr=a("li"),Lt=a("a"),Gi=s("v3.4.0"),Bi=h(),kr=a("li"),Ut=a("a"),Ji=s("v3.3.1"),Wi=h(),Ar=a("li"),It=a("a"),Xi=s("v3.2.0"),Ki=h(),Pr=a("li"),St=a("a"),Yi=s("v3.1.0"),Vi=h(),qr=a("li"),Ct=a("a"),Qi=s("v3.0.2"),Zi=h(),Lr=a("li"),Dt=a("a"),es=s("v2.11.0"),ts=h(),Ur=a("li"),Nt=a("a"),rs=s("v2.10.0"),as=h(),Ir=a("li"),Ot=a("a"),os=s("v2.9.1"),is=h(),Sr=a("li"),jt=a("a"),ss=s("v2.8.0"),ls=h(),Cr=a("li"),Ft=a("a"),ns=s("v2.7.0"),ps=h(),Dr=a("li"),Rt=a("a"),us=s("v2.6.0"),hs=h(),Nr=a("li"),Mt=a("a"),ms=s("v2.5.1"),fs=h(),Or=a("li"),Ht=a("a"),cs=s("v2.4.0"),_s=h(),jr=a("li"),Gt=a("a"),ds=s("v2.3.0"),vs=h(),Fr=a("li"),Bt=a("a"),gs=s("v2.2.0"),ws=h(),Rr=a("li"),Jt=a("a"),ys=s("v2.1.1"),$s=h(),Mr=a("li"),Wt=a("a"),Es=s("v2.0.0"),bs=h(),Hr=a("li"),Xt=a("a"),xs=s("v1.2.0"),zs=h(),Gr=a("li"),Kt=a("a"),Ts=s("v1.1.0"),ks=h(),Br=a("li"),Yt=a("a"),As=s("v1.0.0"),Ga=h(),Vt=a("p"),Ps=s("Then switch your current clone of \u{1F917} Transformers to a specific version, like v3.5.1 for example:"),Ba=h(),E(Be.$$.fragment),Ja=h(),Qt=a("p"),qs=s("After you\u2019ve setup the correct library version, navigate to the example folder of your choice and install the example specific requirements:"),Wa=h(),E(Je.$$.fragment),Xa=h(),oe=a("h2"),_e=a("a"),Jr=a("span"),E(We.$$.fragment),Ls=h(),Wr=a("span"),Us=s("Run a script"),Ka=h(),E(de.$$.fragment),Ya=h(),ie=a("h2"),ve=a("a"),Xr=a("span"),E(Xe.$$.fragment),Is=h(),Kr=a("span"),Ss=s("Distributed training and mixed precision"),Va=h(),ge=a("p"),Cs=s("The "),Ke=a("a"),Ds=s("Trainer"),Ns=s(" supports distributed training and mixed precision, which means you can also use it in a script. To enable both of these features:"),Qa=h(),we=a("ul"),Ye=a("li"),Os=s("Add the "),Yr=a("code"),js=s("fp16"),Fs=s(" argument to enable mixed precision."),Rs=h(),Ve=a("li"),Ms=s("Set the number of GPUs to use with the "),Vr=a("code"),Hs=s("nproc_per_node"),Gs=s(" argument."),Za=h(),E(Qe.$$.fragment),eo=h(),ye=a("p"),Bs=s("TensorFlow scripts utilize a "),Ze=a("a"),Qr=a("code"),Js=s("MirroredStrategy"),Ws=s(" for distributed training, and you don\u2019t need to add any additional arguments to the training script. The TensorFlow script will use multiple GPUs by default if they are available."),to=h(),se=a("h2"),$e=a("a"),Zr=a("span"),E(et.$$.fragment),Xs=h(),ea=a("span"),Ks=s("Run a script on a TPU"),ro=h(),E(Ee.$$.fragment),ao=h(),le=a("h2"),be=a("a"),ta=a("span"),E(tt.$$.fragment),Ys=h(),ra=a("span"),Vs=s("Run a script with \u{1F917} Accelerate"),oo=h(),xe=a("p"),Qs=s("\u{1F917} "),rt=a("a"),Zs=s("Accelerate"),el=s(" is a PyTorch-only library that offers a unified method for training a model on several types of setups (CPU-only, multiple GPUs, TPUs) while maintaining complete visibility into the PyTorch training loop. Make sure you have \u{1F917} Accelerate installed if you don\u2019t already have it:"),io=h(),ze=a("blockquote"),aa=a("p"),tl=s("Note: As Accelerate is rapidly developing, the git version of accelerate must be installed to run the scripts"),rl=h(),E(at.$$.fragment),so=h(),W=a("p"),al=s("Instead of the "),oa=a("code"),ol=s("run_summarization.py"),il=s(" script, you need to use the "),ia=a("code"),sl=s("run_summarization_no_trainer.py"),ll=s(" script. \u{1F917} Accelerate supported scripts will have a "),sa=a("code"),nl=s("task_no_trainer.py"),pl=s(" file in the folder. Begin by running the following command to create and save a configuration file:"),lo=h(),E(ot.$$.fragment),no=h(),Zt=a("p"),ul=s("Test your setup to make sure it is configured correctly:"),po=h(),E(it.$$.fragment),uo=h(),er=a("p"),hl=s("Now you are ready to launch the training:"),ho=h(),E(st.$$.fragment),mo=h(),ne=a("h2"),Te=a("a"),la=a("span"),E(lt.$$.fragment),ml=h(),na=a("span"),fl=s("Use a custom dataset"),fo=h(),tr=a("p"),cl=s("The summarization script supports custom datasets as long as they are a CSV or JSON Line file. When you use your own dataset, you need to specify several additional arguments:"),co=h(),V=a("ul"),ke=a("li"),pa=a("code"),_l=s("train_file"),dl=s(" and "),ua=a("code"),vl=s("validation_file"),gl=s(" specify the path to your training and validation files."),wl=h(),rr=a("li"),ha=a("code"),yl=s("text_column"),$l=s(" is the input text to summarize."),El=h(),ar=a("li"),ma=a("code"),bl=s("summary_column"),xl=s(" is the target text to output."),_o=h(),or=a("p"),zl=s("A summarization script using a custom dataset would look like this:"),vo=h(),E(nt.$$.fragment),go=h(),pe=a("h2"),Ae=a("a"),fa=a("span"),E(pt.$$.fragment),Tl=h(),ca=a("span"),kl=s("Test a script"),wo=h(),ir=a("p"),Al=s("It is often a good idea to run your script on a smaller number of dataset examples to ensure everything works as expected before committing to an entire dataset which may take hours to complete. Use the following arguments to truncate the dataset to a maximum number of samples:"),yo=h(),Q=a("ul"),_a=a("li"),da=a("code"),Pl=s("max_train_samples"),ql=h(),va=a("li"),ga=a("code"),Ll=s("max_eval_samples"),Ul=h(),wa=a("li"),ya=a("code"),Il=s("max_predict_samples"),$o=h(),E(ut.$$.fragment),Eo=h(),Z=a("p"),Sl=s("Not all example scripts support the "),$a=a("code"),Cl=s("max_predict_samples"),Dl=s(" argument. If you aren\u2019t sure whether your script supports this argument, add the "),Ea=a("code"),Nl=s("-h"),Ol=s(" argument to check:"),bo=h(),E(ht.$$.fragment),xo=h(),ue=a("h2"),Pe=a("a"),ba=a("span"),E(mt.$$.fragment),jl=h(),xa=a("span"),Fl=s("Resume training from checkpoint"),zo=h(),sr=a("p"),Rl=s("Another helpful option to enable is resuming training from a previous checkpoint. This will ensure you can pick up where you left off without starting over if your training gets interrupted. There are two methods to resume training from a checkpoint."),To=h(),X=a("p"),Ml=s("The first method uses the "),za=a("code"),Hl=s("output_dir previous_output_dir"),Gl=s(" argument to resume training from the latest checkpoint stored in "),Ta=a("code"),Bl=s("output_dir"),Jl=s(". In this case, you should remove "),ka=a("code"),Wl=s("overwrite_output_dir"),Xl=s(":"),ko=h(),E(ft.$$.fragment),Ao=h(),qe=a("p"),Kl=s("The second method uses the "),Aa=a("code"),Yl=s("resume_from_checkpoint path_to_specific_checkpoint"),Vl=s(" argument to resume training from a specific checkpoint folder."),Po=h(),E(ct.$$.fragment),qo=h(),he=a("h2"),Le=a("a"),Pa=a("span"),E(_t.$$.fragment),Ql=h(),qa=a("span"),Zl=s("Share your model"),Lo=h(),Ue=a("p"),en=s("All scripts can upload your final model to the "),dt=a("a"),tn=s("Model Hub"),rn=s(". Make sure you are logged into Hugging Face before you begin:"),Uo=h(),E(vt.$$.fragment),Io=h(),ee=a("p"),an=s("Then add the "),La=a("code"),on=s("push_to_hub"),sn=s(" argument to the script. This argument will create a repository with your Hugging Face username and the folder name specified in "),Ua=a("code"),ln=s("output_dir"),nn=s("."),So=h(),Ie=a("p"),pn=s("To give your repository a specific name, use the "),Ia=a("code"),un=s("push_to_hub_model_id"),hn=s(" argument to add it. The repository will be automatically listed under your namespace."),Co=h(),lr=a("p"),mn=s("The following example shows how to upload a model with a specific repository name:"),Do=h(),E(gt.$$.fragment),this.h()},l(e){const n=Wu('[data-svelte="svelte-1phssyn"]',document.head);f=o(n,"META",{name:!0,content:!0}),n.forEach(r),w=m(e),c=o(e,"H1",{class:!0});var wt=i(c);v=o(wt,"A",{id:!0,class:!0,href:!0});var Sa=i(v);q=o(Sa,"SPAN",{});var _n=i(q);b($.$$.fragment,_n),_n.forEach(r),Sa.forEach(r),O=m(wt),j=o(wt,"SPAN",{});var dn=i(j);I=l(dn,"Train with a script"),dn.forEach(r),wt.forEach(r),F=m(e),A=o(e,"P",{});var te=i(A);S=l(te,"Along with the \u{1F917} Transformers "),P=o(te,"A",{href:!0});var vn=i(P);C=l(vn,"notebooks"),vn.forEach(r),N=l(te,", there are also example scripts demonstrating how to train a model for a task with "),L=o(te,"A",{href:!0,rel:!0});var gn=i(L);R=l(gn,"PyTorch"),gn.forEach(r),y=l(te,", "),g=o(te,"A",{href:!0,rel:!0});var wn=i(g);G=l(wn,"TensorFlow"),wn.forEach(r),D=l(te,", or "),U=o(te,"A",{href:!0,rel:!0});var yn=i(U);J=l(yn,"JAX/Flax"),yn.forEach(r),yt=l(te,"."),te.forEach(r),De=m(e),B=o(e,"P",{});var nr=i(B);$t=l(nr,"You will also find scripts we\u2019ve used in our "),Ne=o(nr,"A",{href:!0,rel:!0});var $n=i(Ne);li=l($n,"research projects"),$n.forEach(r),ni=l(nr," and "),Oe=o(nr,"A",{href:!0,rel:!0});var En=i(Oe);pi=l(En,"legacy examples"),En.forEach(r),ui=l(nr," which are mostly community contributed. These scripts are not actively maintained and require a specific version of \u{1F917} Transformers that will most likely be incompatible with the latest version of the library."),nr.forEach(r),Da=m(e),Et=o(e,"P",{});var bn=i(Et);hi=l(bn,"The example scripts are not expected to work out-of-the-box on every problem, and you may need to adapt the script to the problem you\u2019re trying to solve. To help you with this, most of the scripts fully expose how data is preprocessed, allowing you to edit it as necessary for your use case."),bn.forEach(r),Na=m(e),K=o(e,"P",{});var pr=i(K);mi=l(pr,"For any feature you\u2019d like to implement in an example script, please discuss it on the "),je=o(pr,"A",{href:!0,rel:!0});var xn=i(je);fi=l(xn,"forum"),xn.forEach(r),ci=l(pr," or in an "),Fe=o(pr,"A",{href:!0,rel:!0});var zn=i(Fe);_i=l(zn,"issue"),zn.forEach(r),di=l(pr," before submitting a Pull Request. While we welcome bug fixes, it is unlikely we will merge a Pull Request that adds more functionality at the cost of readability."),pr.forEach(r),Oa=m(e),Y=o(e,"P",{});var ur=i(Y);vi=l(ur,"This guide will show you how to run an example summarization training script in "),Re=o(ur,"A",{href:!0,rel:!0});var Tn=i(Re);gi=l(Tn,"PyTorch"),Tn.forEach(r),wi=l(ur," and "),Me=o(ur,"A",{href:!0,rel:!0});var kn=i(Me);yi=l(kn,"TensorFlow"),kn.forEach(r),$i=l(ur,". All examples are expected to work with both frameworks unless otherwise specified."),ur.forEach(r),ja=m(e),ae=o(e,"H2",{class:!0});var Oo=i(ae);me=o(Oo,"A",{id:!0,class:!0,href:!0});var An=i(me);_r=o(An,"SPAN",{});var Pn=i(_r);b(He.$$.fragment,Pn),Pn.forEach(r),An.forEach(r),Ei=m(Oo),dr=o(Oo,"SPAN",{});var qn=i(dr);bi=l(qn,"Setup"),qn.forEach(r),Oo.forEach(r),Fa=m(e),fe=o(e,"P",{});var jo=i(fe);xi=l(jo,"To successfully run the latest version of the example scripts, you have to "),vr=o(jo,"STRONG",{});var Ln=i(vr);zi=l(Ln,"install \u{1F917} Transformers from source"),Ln.forEach(r),Ti=l(jo," in a new virtual environment:"),jo.forEach(r),Ra=m(e),b(Ge.$$.fragment,e),Ma=m(e),bt=o(e,"P",{});var Un=i(bt);ki=l(Un,"For older versions of the example scripts, click on the toggle below:"),Un.forEach(r),Ha=m(e),ce=o(e,"DETAILS",{});var Fo=i(ce);gr=o(Fo,"SUMMARY",{});var In=i(gr);Ai=l(In,"Examples for older versions of \u{1F917} Transformers"),In.forEach(r),Pi=m(Fo),_=o(Fo,"UL",{});var d=i(_);wr=o(d,"LI",{});var Sn=i(wr);xt=o(Sn,"A",{href:!0});var Cn=i(xt);qi=l(Cn,"v4.5.1"),Cn.forEach(r),Sn.forEach(r),Li=m(d),yr=o(d,"LI",{});var Dn=i(yr);zt=o(Dn,"A",{href:!0});var Nn=i(zt);Ui=l(Nn,"v4.4.2"),Nn.forEach(r),Dn.forEach(r),Ii=m(d),$r=o(d,"LI",{});var On=i($r);Tt=o(On,"A",{href:!0});var jn=i(Tt);Si=l(jn,"v4.3.3"),jn.forEach(r),On.forEach(r),Ci=m(d),Er=o(d,"LI",{});var Fn=i(Er);kt=o(Fn,"A",{href:!0});var Rn=i(kt);Di=l(Rn,"v4.2.2"),Rn.forEach(r),Fn.forEach(r),Ni=m(d),br=o(d,"LI",{});var Mn=i(br);At=o(Mn,"A",{href:!0});var Hn=i(At);Oi=l(Hn,"v4.1.1"),Hn.forEach(r),Mn.forEach(r),ji=m(d),xr=o(d,"LI",{});var Gn=i(xr);Pt=o(Gn,"A",{href:!0});var Bn=i(Pt);Fi=l(Bn,"v4.0.1"),Bn.forEach(r),Gn.forEach(r),Ri=m(d),zr=o(d,"LI",{});var Jn=i(zr);qt=o(Jn,"A",{href:!0});var Wn=i(qt);Mi=l(Wn,"v3.5.1"),Wn.forEach(r),Jn.forEach(r),Hi=m(d),Tr=o(d,"LI",{});var Xn=i(Tr);Lt=o(Xn,"A",{href:!0});var Kn=i(Lt);Gi=l(Kn,"v3.4.0"),Kn.forEach(r),Xn.forEach(r),Bi=m(d),kr=o(d,"LI",{});var Yn=i(kr);Ut=o(Yn,"A",{href:!0});var Vn=i(Ut);Ji=l(Vn,"v3.3.1"),Vn.forEach(r),Yn.forEach(r),Wi=m(d),Ar=o(d,"LI",{});var Qn=i(Ar);It=o(Qn,"A",{href:!0});var Zn=i(It);Xi=l(Zn,"v3.2.0"),Zn.forEach(r),Qn.forEach(r),Ki=m(d),Pr=o(d,"LI",{});var ep=i(Pr);St=o(ep,"A",{href:!0});var tp=i(St);Yi=l(tp,"v3.1.0"),tp.forEach(r),ep.forEach(r),Vi=m(d),qr=o(d,"LI",{});var rp=i(qr);Ct=o(rp,"A",{href:!0});var ap=i(Ct);Qi=l(ap,"v3.0.2"),ap.forEach(r),rp.forEach(r),Zi=m(d),Lr=o(d,"LI",{});var op=i(Lr);Dt=o(op,"A",{href:!0});var ip=i(Dt);es=l(ip,"v2.11.0"),ip.forEach(r),op.forEach(r),ts=m(d),Ur=o(d,"LI",{});var sp=i(Ur);Nt=o(sp,"A",{href:!0});var lp=i(Nt);rs=l(lp,"v2.10.0"),lp.forEach(r),sp.forEach(r),as=m(d),Ir=o(d,"LI",{});var np=i(Ir);Ot=o(np,"A",{href:!0});var pp=i(Ot);os=l(pp,"v2.9.1"),pp.forEach(r),np.forEach(r),is=m(d),Sr=o(d,"LI",{});var up=i(Sr);jt=o(up,"A",{href:!0});var hp=i(jt);ss=l(hp,"v2.8.0"),hp.forEach(r),up.forEach(r),ls=m(d),Cr=o(d,"LI",{});var mp=i(Cr);Ft=o(mp,"A",{href:!0});var fp=i(Ft);ns=l(fp,"v2.7.0"),fp.forEach(r),mp.forEach(r),ps=m(d),Dr=o(d,"LI",{});var cp=i(Dr);Rt=o(cp,"A",{href:!0});var _p=i(Rt);us=l(_p,"v2.6.0"),_p.forEach(r),cp.forEach(r),hs=m(d),Nr=o(d,"LI",{});var dp=i(Nr);Mt=o(dp,"A",{href:!0});var vp=i(Mt);ms=l(vp,"v2.5.1"),vp.forEach(r),dp.forEach(r),fs=m(d),Or=o(d,"LI",{});var gp=i(Or);Ht=o(gp,"A",{href:!0});var wp=i(Ht);cs=l(wp,"v2.4.0"),wp.forEach(r),gp.forEach(r),_s=m(d),jr=o(d,"LI",{});var yp=i(jr);Gt=o(yp,"A",{href:!0});var $p=i(Gt);ds=l($p,"v2.3.0"),$p.forEach(r),yp.forEach(r),vs=m(d),Fr=o(d,"LI",{});var Ep=i(Fr);Bt=o(Ep,"A",{href:!0});var bp=i(Bt);gs=l(bp,"v2.2.0"),bp.forEach(r),Ep.forEach(r),ws=m(d),Rr=o(d,"LI",{});var xp=i(Rr);Jt=o(xp,"A",{href:!0});var zp=i(Jt);ys=l(zp,"v2.1.1"),zp.forEach(r),xp.forEach(r),$s=m(d),Mr=o(d,"LI",{});var Tp=i(Mr);Wt=o(Tp,"A",{href:!0});var kp=i(Wt);Es=l(kp,"v2.0.0"),kp.forEach(r),Tp.forEach(r),bs=m(d),Hr=o(d,"LI",{});var Ap=i(Hr);Xt=o(Ap,"A",{href:!0});var Pp=i(Xt);xs=l(Pp,"v1.2.0"),Pp.forEach(r),Ap.forEach(r),zs=m(d),Gr=o(d,"LI",{});var qp=i(Gr);Kt=o(qp,"A",{href:!0});var Lp=i(Kt);Ts=l(Lp,"v1.1.0"),Lp.forEach(r),qp.forEach(r),ks=m(d),Br=o(d,"LI",{});var Up=i(Br);Yt=o(Up,"A",{href:!0});var Ip=i(Yt);As=l(Ip,"v1.0.0"),Ip.forEach(r),Up.forEach(r),d.forEach(r),Fo.forEach(r),Ga=m(e),Vt=o(e,"P",{});var Sp=i(Vt);Ps=l(Sp,"Then switch your current clone of \u{1F917} Transformers to a specific version, like v3.5.1 for example:"),Sp.forEach(r),Ba=m(e),b(Be.$$.fragment,e),Ja=m(e),Qt=o(e,"P",{});var Cp=i(Qt);qs=l(Cp,"After you\u2019ve setup the correct library version, navigate to the example folder of your choice and install the example specific requirements:"),Cp.forEach(r),Wa=m(e),b(Je.$$.fragment,e),Xa=m(e),oe=o(e,"H2",{class:!0});var Ro=i(oe);_e=o(Ro,"A",{id:!0,class:!0,href:!0});var Dp=i(_e);Jr=o(Dp,"SPAN",{});var Np=i(Jr);b(We.$$.fragment,Np),Np.forEach(r),Dp.forEach(r),Ls=m(Ro),Wr=o(Ro,"SPAN",{});var Op=i(Wr);Us=l(Op,"Run a script"),Op.forEach(r),Ro.forEach(r),Ka=m(e),b(de.$$.fragment,e),Ya=m(e),ie=o(e,"H2",{class:!0});var Mo=i(ie);ve=o(Mo,"A",{id:!0,class:!0,href:!0});var jp=i(ve);Xr=o(jp,"SPAN",{});var Fp=i(Xr);b(Xe.$$.fragment,Fp),Fp.forEach(r),jp.forEach(r),Is=m(Mo),Kr=o(Mo,"SPAN",{});var Rp=i(Kr);Ss=l(Rp,"Distributed training and mixed precision"),Rp.forEach(r),Mo.forEach(r),Va=m(e),ge=o(e,"P",{});var Ho=i(ge);Cs=l(Ho,"The "),Ke=o(Ho,"A",{href:!0,rel:!0});var Mp=i(Ke);Ds=l(Mp,"Trainer"),Mp.forEach(r),Ns=l(Ho," supports distributed training and mixed precision, which means you can also use it in a script. To enable both of these features:"),Ho.forEach(r),Qa=m(e),we=o(e,"UL",{});var Go=i(we);Ye=o(Go,"LI",{});var Bo=i(Ye);Os=l(Bo,"Add the "),Yr=o(Bo,"CODE",{});var Hp=i(Yr);js=l(Hp,"fp16"),Hp.forEach(r),Fs=l(Bo," argument to enable mixed precision."),Bo.forEach(r),Rs=m(Go),Ve=o(Go,"LI",{});var Jo=i(Ve);Ms=l(Jo,"Set the number of GPUs to use with the "),Vr=o(Jo,"CODE",{});var Gp=i(Vr);Hs=l(Gp,"nproc_per_node"),Gp.forEach(r),Gs=l(Jo," argument."),Jo.forEach(r),Go.forEach(r),Za=m(e),b(Qe.$$.fragment,e),eo=m(e),ye=o(e,"P",{});var Wo=i(ye);Bs=l(Wo,"TensorFlow scripts utilize a "),Ze=o(Wo,"A",{href:!0,rel:!0});var Bp=i(Ze);Qr=o(Bp,"CODE",{});var Jp=i(Qr);Js=l(Jp,"MirroredStrategy"),Jp.forEach(r),Bp.forEach(r),Ws=l(Wo," for distributed training, and you don\u2019t need to add any additional arguments to the training script. The TensorFlow script will use multiple GPUs by default if they are available."),Wo.forEach(r),to=m(e),se=o(e,"H2",{class:!0});var Xo=i(se);$e=o(Xo,"A",{id:!0,class:!0,href:!0});var Wp=i($e);Zr=o(Wp,"SPAN",{});var Xp=i(Zr);b(et.$$.fragment,Xp),Xp.forEach(r),Wp.forEach(r),Xs=m(Xo),ea=o(Xo,"SPAN",{});var Kp=i(ea);Ks=l(Kp,"Run a script on a TPU"),Kp.forEach(r),Xo.forEach(r),ro=m(e),b(Ee.$$.fragment,e),ao=m(e),le=o(e,"H2",{class:!0});var Ko=i(le);be=o(Ko,"A",{id:!0,class:!0,href:!0});var Yp=i(be);ta=o(Yp,"SPAN",{});var Vp=i(ta);b(tt.$$.fragment,Vp),Vp.forEach(r),Yp.forEach(r),Ys=m(Ko),ra=o(Ko,"SPAN",{});var Qp=i(ra);Vs=l(Qp,"Run a script with \u{1F917} Accelerate"),Qp.forEach(r),Ko.forEach(r),oo=m(e),xe=o(e,"P",{});var Yo=i(xe);Qs=l(Yo,"\u{1F917} "),rt=o(Yo,"A",{href:!0,rel:!0});var Zp=i(rt);Zs=l(Zp,"Accelerate"),Zp.forEach(r),el=l(Yo," is a PyTorch-only library that offers a unified method for training a model on several types of setups (CPU-only, multiple GPUs, TPUs) while maintaining complete visibility into the PyTorch training loop. Make sure you have \u{1F917} Accelerate installed if you don\u2019t already have it:"),Yo.forEach(r),io=m(e),ze=o(e,"BLOCKQUOTE",{});var Vo=i(ze);aa=o(Vo,"P",{});var eu=i(aa);tl=l(eu,"Note: As Accelerate is rapidly developing, the git version of accelerate must be installed to run the scripts"),eu.forEach(r),rl=m(Vo),b(at.$$.fragment,Vo),Vo.forEach(r),so=m(e),W=o(e,"P",{});var Se=i(W);al=l(Se,"Instead of the "),oa=o(Se,"CODE",{});var tu=i(oa);ol=l(tu,"run_summarization.py"),tu.forEach(r),il=l(Se," script, you need to use the "),ia=o(Se,"CODE",{});var ru=i(ia);sl=l(ru,"run_summarization_no_trainer.py"),ru.forEach(r),ll=l(Se," script. \u{1F917} Accelerate supported scripts will have a "),sa=o(Se,"CODE",{});var au=i(sa);nl=l(au,"task_no_trainer.py"),au.forEach(r),pl=l(Se," file in the folder. Begin by running the following command to create and save a configuration file:"),Se.forEach(r),lo=m(e),b(ot.$$.fragment,e),no=m(e),Zt=o(e,"P",{});var ou=i(Zt);ul=l(ou,"Test your setup to make sure it is configured correctly:"),ou.forEach(r),po=m(e),b(it.$$.fragment,e),uo=m(e),er=o(e,"P",{});var iu=i(er);hl=l(iu,"Now you are ready to launch the training:"),iu.forEach(r),ho=m(e),b(st.$$.fragment,e),mo=m(e),ne=o(e,"H2",{class:!0});var Qo=i(ne);Te=o(Qo,"A",{id:!0,class:!0,href:!0});var su=i(Te);la=o(su,"SPAN",{});var lu=i(la);b(lt.$$.fragment,lu),lu.forEach(r),su.forEach(r),ml=m(Qo),na=o(Qo,"SPAN",{});var nu=i(na);fl=l(nu,"Use a custom dataset"),nu.forEach(r),Qo.forEach(r),fo=m(e),tr=o(e,"P",{});var pu=i(tr);cl=l(pu,"The summarization script supports custom datasets as long as they are a CSV or JSON Line file. When you use your own dataset, you need to specify several additional arguments:"),pu.forEach(r),co=m(e),V=o(e,"UL",{});var hr=i(V);ke=o(hr,"LI",{});var Ca=i(ke);pa=o(Ca,"CODE",{});var uu=i(pa);_l=l(uu,"train_file"),uu.forEach(r),dl=l(Ca," and "),ua=o(Ca,"CODE",{});var hu=i(ua);vl=l(hu,"validation_file"),hu.forEach(r),gl=l(Ca," specify the path to your training and validation files."),Ca.forEach(r),wl=m(hr),rr=o(hr,"LI",{});var fn=i(rr);ha=o(fn,"CODE",{});var mu=i(ha);yl=l(mu,"text_column"),mu.forEach(r),$l=l(fn," is the input text to summarize."),fn.forEach(r),El=m(hr),ar=o(hr,"LI",{});var cn=i(ar);ma=o(cn,"CODE",{});var fu=i(ma);bl=l(fu,"summary_column"),fu.forEach(r),xl=l(cn," is the target text to output."),cn.forEach(r),hr.forEach(r),_o=m(e),or=o(e,"P",{});var cu=i(or);zl=l(cu,"A summarization script using a custom dataset would look like this:"),cu.forEach(r),vo=m(e),b(nt.$$.fragment,e),go=m(e),pe=o(e,"H2",{class:!0});var Zo=i(pe);Ae=o(Zo,"A",{id:!0,class:!0,href:!0});var _u=i(Ae);fa=o(_u,"SPAN",{});var du=i(fa);b(pt.$$.fragment,du),du.forEach(r),_u.forEach(r),Tl=m(Zo),ca=o(Zo,"SPAN",{});var vu=i(ca);kl=l(vu,"Test a script"),vu.forEach(r),Zo.forEach(r),wo=m(e),ir=o(e,"P",{});var gu=i(ir);Al=l(gu,"It is often a good idea to run your script on a smaller number of dataset examples to ensure everything works as expected before committing to an entire dataset which may take hours to complete. Use the following arguments to truncate the dataset to a maximum number of samples:"),gu.forEach(r),yo=m(e),Q=o(e,"UL",{});var mr=i(Q);_a=o(mr,"LI",{});var wu=i(_a);da=o(wu,"CODE",{});var yu=i(da);Pl=l(yu,"max_train_samples"),yu.forEach(r),wu.forEach(r),ql=m(mr),va=o(mr,"LI",{});var $u=i(va);ga=o($u,"CODE",{});var Eu=i(ga);Ll=l(Eu,"max_eval_samples"),Eu.forEach(r),$u.forEach(r),Ul=m(mr),wa=o(mr,"LI",{});var bu=i(wa);ya=o(bu,"CODE",{});var xu=i(ya);Il=l(xu,"max_predict_samples"),xu.forEach(r),bu.forEach(r),mr.forEach(r),$o=m(e),b(ut.$$.fragment,e),Eo=m(e),Z=o(e,"P",{});var fr=i(Z);Sl=l(fr,"Not all example scripts support the "),$a=o(fr,"CODE",{});var zu=i($a);Cl=l(zu,"max_predict_samples"),zu.forEach(r),Dl=l(fr," argument. If you aren\u2019t sure whether your script supports this argument, add the "),Ea=o(fr,"CODE",{});var Tu=i(Ea);Nl=l(Tu,"-h"),Tu.forEach(r),Ol=l(fr," argument to check:"),fr.forEach(r),bo=m(e),b(ht.$$.fragment,e),xo=m(e),ue=o(e,"H2",{class:!0});var ei=i(ue);Pe=o(ei,"A",{id:!0,class:!0,href:!0});var ku=i(Pe);ba=o(ku,"SPAN",{});var Au=i(ba);b(mt.$$.fragment,Au),Au.forEach(r),ku.forEach(r),jl=m(ei),xa=o(ei,"SPAN",{});var Pu=i(xa);Fl=l(Pu,"Resume training from checkpoint"),Pu.forEach(r),ei.forEach(r),zo=m(e),sr=o(e,"P",{});var qu=i(sr);Rl=l(qu,"Another helpful option to enable is resuming training from a previous checkpoint. This will ensure you can pick up where you left off without starting over if your training gets interrupted. There are two methods to resume training from a checkpoint."),qu.forEach(r),To=m(e),X=o(e,"P",{});var Ce=i(X);Ml=l(Ce,"The first method uses the "),za=o(Ce,"CODE",{});var Lu=i(za);Hl=l(Lu,"output_dir previous_output_dir"),Lu.forEach(r),Gl=l(Ce," argument to resume training from the latest checkpoint stored in "),Ta=o(Ce,"CODE",{});var Uu=i(Ta);Bl=l(Uu,"output_dir"),Uu.forEach(r),Jl=l(Ce,". In this case, you should remove "),ka=o(Ce,"CODE",{});var Iu=i(ka);Wl=l(Iu,"overwrite_output_dir"),Iu.forEach(r),Xl=l(Ce,":"),Ce.forEach(r),ko=m(e),b(ft.$$.fragment,e),Ao=m(e),qe=o(e,"P",{});var ti=i(qe);Kl=l(ti,"The second method uses the "),Aa=o(ti,"CODE",{});var Su=i(Aa);Yl=l(Su,"resume_from_checkpoint path_to_specific_checkpoint"),Su.forEach(r),Vl=l(ti," argument to resume training from a specific checkpoint folder."),ti.forEach(r),Po=m(e),b(ct.$$.fragment,e),qo=m(e),he=o(e,"H2",{class:!0});var ri=i(he);Le=o(ri,"A",{id:!0,class:!0,href:!0});var Cu=i(Le);Pa=o(Cu,"SPAN",{});var Du=i(Pa);b(_t.$$.fragment,Du),Du.forEach(r),Cu.forEach(r),Ql=m(ri),qa=o(ri,"SPAN",{});var Nu=i(qa);Zl=l(Nu,"Share your model"),Nu.forEach(r),ri.forEach(r),Lo=m(e),Ue=o(e,"P",{});var ai=i(Ue);en=l(ai,"All scripts can upload your final model to the "),dt=o(ai,"A",{href:!0,rel:!0});var Ou=i(dt);tn=l(Ou,"Model Hub"),Ou.forEach(r),rn=l(ai,". Make sure you are logged into Hugging Face before you begin:"),ai.forEach(r),Uo=m(e),b(vt.$$.fragment,e),Io=m(e),ee=o(e,"P",{});var cr=i(ee);an=l(cr,"Then add the "),La=o(cr,"CODE",{});var ju=i(La);on=l(ju,"push_to_hub"),ju.forEach(r),sn=l(cr," argument to the script. This argument will create a repository with your Hugging Face username and the folder name specified in "),Ua=o(cr,"CODE",{});var Fu=i(Ua);ln=l(Fu,"output_dir"),Fu.forEach(r),nn=l(cr,"."),cr.forEach(r),So=m(e),Ie=o(e,"P",{});var oi=i(Ie);pn=l(oi,"To give your repository a specific name, use the "),Ia=o(oi,"CODE",{});var Ru=i(Ia);un=l(Ru,"push_to_hub_model_id"),Ru.forEach(r),hn=l(oi," argument to add it. The repository will be automatically listed under your namespace."),oi.forEach(r),Co=m(e),lr=o(e,"P",{});var Mu=i(lr);mn=l(Mu,"The following example shows how to upload a model with a specific repository name:"),Mu.forEach(r),Do=m(e),b(gt.$$.fragment,e),this.h()},h(){p(f,"name","hf:doc:metadata"),p(f,"content",JSON.stringify(oh)),p(v,"id","train-with-a-script"),p(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(v,"href","#train-with-a-script"),p(c,"class","relative group"),p(P,"href","./noteboks/README"),p(L,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch"),p(L,"rel","nofollow"),p(g,"href","https://github.com/huggingface/transformers/tree/main/examples/tensorflow"),p(g,"rel","nofollow"),p(U,"href","https://github.com/huggingface/transformers/tree/main/examples/flax"),p(U,"rel","nofollow"),p(Ne,"href","https://github.com/huggingface/transformers/tree/main/examples/research_projects"),p(Ne,"rel","nofollow"),p(Oe,"href","https://github.com/huggingface/transformers/tree/main/examples/legacy"),p(Oe,"rel","nofollow"),p(je,"href","https://discuss.huggingface.co/"),p(je,"rel","nofollow"),p(Fe,"href","https://github.com/huggingface/transformers/issues"),p(Fe,"rel","nofollow"),p(Re,"href","https://github.com/huggingface/transformers/tree/main/examples/pytorch/summarization"),p(Re,"rel","nofollow"),p(Me,"href","https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization"),p(Me,"rel","nofollow"),p(me,"id","setup"),p(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(me,"href","#setup"),p(ae,"class","relative group"),p(xt,"href","https://github.com/huggingface/transformers/tree/v4.5.1/examples"),p(zt,"href","https://github.com/huggingface/transformers/tree/v4.4.2/examples"),p(Tt,"href","https://github.com/huggingface/transformers/tree/v4.3.3/examples"),p(kt,"href","https://github.com/huggingface/transformers/tree/v4.2.2/examples"),p(At,"href","https://github.com/huggingface/transformers/tree/v4.1.1/examples"),p(Pt,"href","https://github.com/huggingface/transformers/tree/v4.0.1/examples"),p(qt,"href","https://github.com/huggingface/transformers/tree/v3.5.1/examples"),p(Lt,"href","https://github.com/huggingface/transformers/tree/v3.4.0/examples"),p(Ut,"href","https://github.com/huggingface/transformers/tree/v3.3.1/examples"),p(It,"href","https://github.com/huggingface/transformers/tree/v3.2.0/examples"),p(St,"href","https://github.com/huggingface/transformers/tree/v3.1.0/examples"),p(Ct,"href","https://github.com/huggingface/transformers/tree/v3.0.2/examples"),p(Dt,"href","https://github.com/huggingface/transformers/tree/v2.11.0/examples"),p(Nt,"href","https://github.com/huggingface/transformers/tree/v2.10.0/examples"),p(Ot,"href","https://github.com/huggingface/transformers/tree/v2.9.1/examples"),p(jt,"href","https://github.com/huggingface/transformers/tree/v2.8.0/examples"),p(Ft,"href","https://github.com/huggingface/transformers/tree/v2.7.0/examples"),p(Rt,"href","https://github.com/huggingface/transformers/tree/v2.6.0/examples"),p(Mt,"href","https://github.com/huggingface/transformers/tree/v2.5.1/examples"),p(Ht,"href","https://github.com/huggingface/transformers/tree/v2.4.0/examples"),p(Gt,"href","https://github.com/huggingface/transformers/tree/v2.3.0/examples"),p(Bt,"href","https://github.com/huggingface/transformers/tree/v2.2.0/examples"),p(Jt,"href","https://github.com/huggingface/transformers/tree/v2.1.0/examples"),p(Wt,"href","https://github.com/huggingface/transformers/tree/v2.0.0/examples"),p(Xt,"href","https://github.com/huggingface/transformers/tree/v1.2.0/examples"),p(Kt,"href","https://github.com/huggingface/transformers/tree/v1.1.0/examples"),p(Yt,"href","https://github.com/huggingface/transformers/tree/v1.0.0/examples"),p(_e,"id","run-a-script"),p(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(_e,"href","#run-a-script"),p(oe,"class","relative group"),p(ve,"id","distributed-training-and-mixed-precision"),p(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ve,"href","#distributed-training-and-mixed-precision"),p(ie,"class","relative group"),p(Ke,"href","https://huggingface.co/docs/transformers/main_classes/trainer"),p(Ke,"rel","nofollow"),p(Ze,"href","https://www.tensorflow.org/guide/distributed_training#mirroredstrategy"),p(Ze,"rel","nofollow"),p($e,"id","run-a-script-on-a-tpu"),p($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p($e,"href","#run-a-script-on-a-tpu"),p(se,"class","relative group"),p(be,"id","run-a-script-with-accelerate"),p(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(be,"href","#run-a-script-with-accelerate"),p(le,"class","relative group"),p(rt,"href","https://huggingface.co/docs/accelerate/index.html"),p(rt,"rel","nofollow"),p(Te,"id","use-a-custom-dataset"),p(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Te,"href","#use-a-custom-dataset"),p(ne,"class","relative group"),p(Ae,"id","test-a-script"),p(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ae,"href","#test-a-script"),p(pe,"class","relative group"),p(Pe,"id","resume-training-from-checkpoint"),p(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Pe,"href","#resume-training-from-checkpoint"),p(ue,"class","relative group"),p(Le,"id","share-your-model"),p(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Le,"href","#share-your-model"),p(he,"class","relative group"),p(dt,"href","https://huggingface.co/models"),p(dt,"rel","nofollow")},m(e,n){t(document.head,f),u(e,w,n),u(e,c,n),t(c,v),t(v,q),x($,q,null),t(c,O),t(c,j),t(j,I),u(e,F,n),u(e,A,n),t(A,S),t(A,P),t(P,C),t(A,N),t(A,L),t(L,R),t(A,y),t(A,g),t(g,G),t(A,D),t(A,U),t(U,J),t(A,yt),u(e,De,n),u(e,B,n),t(B,$t),t(B,Ne),t(Ne,li),t(B,ni),t(B,Oe),t(Oe,pi),t(B,ui),u(e,Da,n),u(e,Et,n),t(Et,hi),u(e,Na,n),u(e,K,n),t(K,mi),t(K,je),t(je,fi),t(K,ci),t(K,Fe),t(Fe,_i),t(K,di),u(e,Oa,n),u(e,Y,n),t(Y,vi),t(Y,Re),t(Re,gi),t(Y,wi),t(Y,Me),t(Me,yi),t(Y,$i),u(e,ja,n),u(e,ae,n),t(ae,me),t(me,_r),x(He,_r,null),t(ae,Ei),t(ae,dr),t(dr,bi),u(e,Fa,n),u(e,fe,n),t(fe,xi),t(fe,vr),t(vr,zi),t(fe,Ti),u(e,Ra,n),x(Ge,e,n),u(e,Ma,n),u(e,bt,n),t(bt,ki),u(e,Ha,n),u(e,ce,n),t(ce,gr),t(gr,Ai),t(ce,Pi),t(ce,_),t(_,wr),t(wr,xt),t(xt,qi),t(_,Li),t(_,yr),t(yr,zt),t(zt,Ui),t(_,Ii),t(_,$r),t($r,Tt),t(Tt,Si),t(_,Ci),t(_,Er),t(Er,kt),t(kt,Di),t(_,Ni),t(_,br),t(br,At),t(At,Oi),t(_,ji),t(_,xr),t(xr,Pt),t(Pt,Fi),t(_,Ri),t(_,zr),t(zr,qt),t(qt,Mi),t(_,Hi),t(_,Tr),t(Tr,Lt),t(Lt,Gi),t(_,Bi),t(_,kr),t(kr,Ut),t(Ut,Ji),t(_,Wi),t(_,Ar),t(Ar,It),t(It,Xi),t(_,Ki),t(_,Pr),t(Pr,St),t(St,Yi),t(_,Vi),t(_,qr),t(qr,Ct),t(Ct,Qi),t(_,Zi),t(_,Lr),t(Lr,Dt),t(Dt,es),t(_,ts),t(_,Ur),t(Ur,Nt),t(Nt,rs),t(_,as),t(_,Ir),t(Ir,Ot),t(Ot,os),t(_,is),t(_,Sr),t(Sr,jt),t(jt,ss),t(_,ls),t(_,Cr),t(Cr,Ft),t(Ft,ns),t(_,ps),t(_,Dr),t(Dr,Rt),t(Rt,us),t(_,hs),t(_,Nr),t(Nr,Mt),t(Mt,ms),t(_,fs),t(_,Or),t(Or,Ht),t(Ht,cs),t(_,_s),t(_,jr),t(jr,Gt),t(Gt,ds),t(_,vs),t(_,Fr),t(Fr,Bt),t(Bt,gs),t(_,ws),t(_,Rr),t(Rr,Jt),t(Jt,ys),t(_,$s),t(_,Mr),t(Mr,Wt),t(Wt,Es),t(_,bs),t(_,Hr),t(Hr,Xt),t(Xt,xs),t(_,zs),t(_,Gr),t(Gr,Kt),t(Kt,Ts),t(_,ks),t(_,Br),t(Br,Yt),t(Yt,As),u(e,Ga,n),u(e,Vt,n),t(Vt,Ps),u(e,Ba,n),x(Be,e,n),u(e,Ja,n),u(e,Qt,n),t(Qt,qs),u(e,Wa,n),x(Je,e,n),u(e,Xa,n),u(e,oe,n),t(oe,_e),t(_e,Jr),x(We,Jr,null),t(oe,Ls),t(oe,Wr),t(Wr,Us),u(e,Ka,n),x(de,e,n),u(e,Ya,n),u(e,ie,n),t(ie,ve),t(ve,Xr),x(Xe,Xr,null),t(ie,Is),t(ie,Kr),t(Kr,Ss),u(e,Va,n),u(e,ge,n),t(ge,Cs),t(ge,Ke),t(Ke,Ds),t(ge,Ns),u(e,Qa,n),u(e,we,n),t(we,Ye),t(Ye,Os),t(Ye,Yr),t(Yr,js),t(Ye,Fs),t(we,Rs),t(we,Ve),t(Ve,Ms),t(Ve,Vr),t(Vr,Hs),t(Ve,Gs),u(e,Za,n),x(Qe,e,n),u(e,eo,n),u(e,ye,n),t(ye,Bs),t(ye,Ze),t(Ze,Qr),t(Qr,Js),t(ye,Ws),u(e,to,n),u(e,se,n),t(se,$e),t($e,Zr),x(et,Zr,null),t(se,Xs),t(se,ea),t(ea,Ks),u(e,ro,n),x(Ee,e,n),u(e,ao,n),u(e,le,n),t(le,be),t(be,ta),x(tt,ta,null),t(le,Ys),t(le,ra),t(ra,Vs),u(e,oo,n),u(e,xe,n),t(xe,Qs),t(xe,rt),t(rt,Zs),t(xe,el),u(e,io,n),u(e,ze,n),t(ze,aa),t(aa,tl),t(ze,rl),x(at,ze,null),u(e,so,n),u(e,W,n),t(W,al),t(W,oa),t(oa,ol),t(W,il),t(W,ia),t(ia,sl),t(W,ll),t(W,sa),t(sa,nl),t(W,pl),u(e,lo,n),x(ot,e,n),u(e,no,n),u(e,Zt,n),t(Zt,ul),u(e,po,n),x(it,e,n),u(e,uo,n),u(e,er,n),t(er,hl),u(e,ho,n),x(st,e,n),u(e,mo,n),u(e,ne,n),t(ne,Te),t(Te,la),x(lt,la,null),t(ne,ml),t(ne,na),t(na,fl),u(e,fo,n),u(e,tr,n),t(tr,cl),u(e,co,n),u(e,V,n),t(V,ke),t(ke,pa),t(pa,_l),t(ke,dl),t(ke,ua),t(ua,vl),t(ke,gl),t(V,wl),t(V,rr),t(rr,ha),t(ha,yl),t(rr,$l),t(V,El),t(V,ar),t(ar,ma),t(ma,bl),t(ar,xl),u(e,_o,n),u(e,or,n),t(or,zl),u(e,vo,n),x(nt,e,n),u(e,go,n),u(e,pe,n),t(pe,Ae),t(Ae,fa),x(pt,fa,null),t(pe,Tl),t(pe,ca),t(ca,kl),u(e,wo,n),u(e,ir,n),t(ir,Al),u(e,yo,n),u(e,Q,n),t(Q,_a),t(_a,da),t(da,Pl),t(Q,ql),t(Q,va),t(va,ga),t(ga,Ll),t(Q,Ul),t(Q,wa),t(wa,ya),t(ya,Il),u(e,$o,n),x(ut,e,n),u(e,Eo,n),u(e,Z,n),t(Z,Sl),t(Z,$a),t($a,Cl),t(Z,Dl),t(Z,Ea),t(Ea,Nl),t(Z,Ol),u(e,bo,n),x(ht,e,n),u(e,xo,n),u(e,ue,n),t(ue,Pe),t(Pe,ba),x(mt,ba,null),t(ue,jl),t(ue,xa),t(xa,Fl),u(e,zo,n),u(e,sr,n),t(sr,Rl),u(e,To,n),u(e,X,n),t(X,Ml),t(X,za),t(za,Hl),t(X,Gl),t(X,Ta),t(Ta,Bl),t(X,Jl),t(X,ka),t(ka,Wl),t(X,Xl),u(e,ko,n),x(ft,e,n),u(e,Ao,n),u(e,qe,n),t(qe,Kl),t(qe,Aa),t(Aa,Yl),t(qe,Vl),u(e,Po,n),x(ct,e,n),u(e,qo,n),u(e,he,n),t(he,Le),t(Le,Pa),x(_t,Pa,null),t(he,Ql),t(he,qa),t(qa,Zl),u(e,Lo,n),u(e,Ue,n),t(Ue,en),t(Ue,dt),t(dt,tn),t(Ue,rn),u(e,Uo,n),x(vt,e,n),u(e,Io,n),u(e,ee,n),t(ee,an),t(ee,La),t(La,on),t(ee,sn),t(ee,Ua),t(Ua,ln),t(ee,nn),u(e,So,n),u(e,Ie,n),t(Ie,pn),t(Ie,Ia),t(Ia,un),t(Ie,hn),u(e,Co,n),u(e,lr,n),t(lr,mn),u(e,Do,n),x(gt,e,n),No=!0},p(e,[n]){const wt={};n&2&&(wt.$$scope={dirty:n,ctx:e}),de.$set(wt);const Sa={};n&2&&(Sa.$$scope={dirty:n,ctx:e}),Ee.$set(Sa)},i(e){No||(z($.$$.fragment,e),z(He.$$.fragment,e),z(Ge.$$.fragment,e),z(Be.$$.fragment,e),z(Je.$$.fragment,e),z(We.$$.fragment,e),z(de.$$.fragment,e),z(Xe.$$.fragment,e),z(Qe.$$.fragment,e),z(et.$$.fragment,e),z(Ee.$$.fragment,e),z(tt.$$.fragment,e),z(at.$$.fragment,e),z(ot.$$.fragment,e),z(it.$$.fragment,e),z(st.$$.fragment,e),z(lt.$$.fragment,e),z(nt.$$.fragment,e),z(pt.$$.fragment,e),z(ut.$$.fragment,e),z(ht.$$.fragment,e),z(mt.$$.fragment,e),z(ft.$$.fragment,e),z(ct.$$.fragment,e),z(_t.$$.fragment,e),z(vt.$$.fragment,e),z(gt.$$.fragment,e),No=!0)},o(e){T($.$$.fragment,e),T(He.$$.fragment,e),T(Ge.$$.fragment,e),T(Be.$$.fragment,e),T(Je.$$.fragment,e),T(We.$$.fragment,e),T(de.$$.fragment,e),T(Xe.$$.fragment,e),T(Qe.$$.fragment,e),T(et.$$.fragment,e),T(Ee.$$.fragment,e),T(tt.$$.fragment,e),T(at.$$.fragment,e),T(ot.$$.fragment,e),T(it.$$.fragment,e),T(st.$$.fragment,e),T(lt.$$.fragment,e),T(nt.$$.fragment,e),T(pt.$$.fragment,e),T(ut.$$.fragment,e),T(ht.$$.fragment,e),T(mt.$$.fragment,e),T(ft.$$.fragment,e),T(ct.$$.fragment,e),T(_t.$$.fragment,e),T(vt.$$.fragment,e),T(gt.$$.fragment,e),No=!1},d(e){r(f),e&&r(w),e&&r(c),k($),e&&r(F),e&&r(A),e&&r(De),e&&r(B),e&&r(Da),e&&r(Et),e&&r(Na),e&&r(K),e&&r(Oa),e&&r(Y),e&&r(ja),e&&r(ae),k(He),e&&r(Fa),e&&r(fe),e&&r(Ra),k(Ge,e),e&&r(Ma),e&&r(bt),e&&r(Ha),e&&r(ce),e&&r(Ga),e&&r(Vt),e&&r(Ba),k(Be,e),e&&r(Ja),e&&r(Qt),e&&r(Wa),k(Je,e),e&&r(Xa),e&&r(oe),k(We),e&&r(Ka),k(de,e),e&&r(Ya),e&&r(ie),k(Xe),e&&r(Va),e&&r(ge),e&&r(Qa),e&&r(we),e&&r(Za),k(Qe,e),e&&r(eo),e&&r(ye),e&&r(to),e&&r(se),k(et),e&&r(ro),k(Ee,e),e&&r(ao),e&&r(le),k(tt),e&&r(oo),e&&r(xe),e&&r(io),e&&r(ze),k(at),e&&r(so),e&&r(W),e&&r(lo),k(ot,e),e&&r(no),e&&r(Zt),e&&r(po),k(it,e),e&&r(uo),e&&r(er),e&&r(ho),k(st,e),e&&r(mo),e&&r(ne),k(lt),e&&r(fo),e&&r(tr),e&&r(co),e&&r(V),e&&r(_o),e&&r(or),e&&r(vo),k(nt,e),e&&r(go),e&&r(pe),k(pt),e&&r(wo),e&&r(ir),e&&r(yo),e&&r(Q),e&&r($o),k(ut,e),e&&r(Eo),e&&r(Z),e&&r(bo),k(ht,e),e&&r(xo),e&&r(ue),k(mt),e&&r(zo),e&&r(sr),e&&r(To),e&&r(X),e&&r(ko),k(ft,e),e&&r(Ao),e&&r(qe),e&&r(Po),k(ct,e),e&&r(qo),e&&r(he),k(_t),e&&r(Lo),e&&r(Ue),e&&r(Uo),k(vt,e),e&&r(Io),e&&r(ee),e&&r(So),e&&r(Ie),e&&r(Co),e&&r(lr),e&&r(Do),k(gt,e)}}}const oh={local:"train-with-a-script",sections:[{local:"setup",title:"Setup"},{local:"run-a-script",title:"Run a script"},{local:"distributed-training-and-mixed-precision",title:"Distributed training and mixed precision"},{local:"run-a-script-on-a-tpu",title:"Run a script on a TPU"},{local:"run-a-script-with-accelerate",title:"Run a script with \u{1F917} Accelerate"},{local:"use-a-custom-dataset",title:"Use a custom dataset"},{local:"test-a-script",title:"Test a script"},{local:"resume-training-from-checkpoint",title:"Resume training from checkpoint"},{local:"share-your-model",title:"Share your model"}],title:"Train with a script"};function ih(H){return Xu(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class uh extends Gu{constructor(f){super();Bu(this,f,ih,ah,Ju,{})}}export{uh as default,oh as metadata};
