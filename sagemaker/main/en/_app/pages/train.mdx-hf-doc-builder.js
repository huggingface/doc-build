import{S as Lf,i as zf,s as Rf,e as o,k as h,w as u,t as s,M as Uf,c as n,d as a,m,a as i,x as f,h as r,b as c,N as Gf,G as t,g as p,y as d,L as Wf,q as g,o as _,B as v,v as Bf}from"../chunks/vendor-hf-doc-builder.js";import{I as P}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as O}from"../chunks/CodeBlock-hf-doc-builder.js";function Xf(Uh){let X,$r,J,pe,Da,Ke,kn,Ca,xn,Sr,T,Gh,jr,ce,$n,Pa,Sn,jn,Ar,y,Vt,Yt,An,Tn,Mn,Qt,Zt,Dn,Cn,Pn,ea,ta,On,qn,In,aa,he,Nn,Oa,Hn,Fn,Ln,zn,sa,ra,Rn,Un,Gn,oa,na,Wn,Bn,Xn,ia,la,Jn,Kn,Vn,pa,ca,Yn,Qn,Zn,ha,ma,ei,ti,Tr,K,me,qa,Ve,ai,Ia,si,Mr,ue,ri,Ye,oi,ni,Dr,ua,ii,Cr,L,Na,Qe,li,pi,Ha,Ze,ci,hi,Fa,mi,Pr,fe,ui,et,fi,di,Or,de,gi,La,_i,vi,qr,tt,Ir,fa,za,yi,Nr,da,Ei,Hr,at,Fr,ga,V,bi,Ra,wi,ki,Ua,xi,$i,Lr,_a,Ga,Si,zr,va,ji,Rr,st,Ur,Y,ge,Wa,rt,Ai,Ba,Ti,Gr,_e,Mi,ot,Di,Ci,Wr,z,Xa,R,Ja,Pi,Oi,Ka,qi,Ii,Va,Ni,Hi,Fi,Ya,ya,Qa,Li,zi,Ri,Za,$,es,Ui,Gi,ts,Wi,Bi,as,Xi,Ji,ss,Ki,Vi,rs,Yi,Qi,os,Zi,el,Br,q,tl,ns,al,sl,Ea,rl,ol,is,nl,il,Xr,nt,Jr,ba,M,ll,ls,pl,cl,ps,hl,ml,cs,ul,fl,hs,dl,gl,Kr,ve,_l,it,vl,yl,Vr,Q,ye,ms,lt,El,us,bl,Yr,E,wl,fs,kl,xl,ds,$l,Sl,gs,jl,Al,_s,Tl,Ml,pt,Dl,Cl,vs,Pl,Ol,ys,ql,Il,Es,Nl,Hl,Qr,b,Fl,ct,Ll,zl,ht,Rl,Ul,bs,Gl,Wl,ws,Bl,Xl,ks,Jl,Kl,xs,Vl,Yl,D,Ql,$s,Zl,ep,Ss,tp,ap,js,sp,rp,As,op,np,Zr,Z,Ee,Ts,mt,ip,Ms,lp,eo,be,pp,ut,cp,hp,to,U,wa,Ds,mp,up,fp,we,Cs,dp,gp,ft,_p,vp,yp,ke,Ps,Ep,bp,dt,wp,kp,ao,j,xp,Os,$p,Sp,qs,jp,Ap,Is,Tp,Mp,Ns,Dp,Cp,so,gt,ro,I,Pp,Hs,Op,qp,Fs,Ip,Np,Ls,Hp,Fp,oo,ee,xe,zs,_t,Lp,Rs,zp,no,N,Rp,Us,Up,Gp,Gs,Wp,Bp,Ws,Xp,Jp,io,$e,vt,Kp,Bs,Vp,Yp,Qp,Se,Xs,Zp,ec,yt,tc,ac,lo,je,sc,Js,rc,oc,po,Et,co,Ae,nc,Ks,ic,lc,ho,bt,mo,te,Te,Vs,wt,pc,Ys,cc,uo,Me,hc,kt,mc,uc,fo,xt,go,ae,De,Qs,$t,fc,Zs,dc,_o,ka,gc,vo,se,Ce,er,St,_c,tr,vc,yo,Pe,yc,jt,Ec,bc,Eo,At,bo,Oe,wc,Tt,kc,xc,wo,re,qe,ar,Mt,$c,sr,Sc,ko,Ie,jc,Dt,Ac,Tc,xo,Ct,$o,Ne,Mc,Pt,Dc,Cc,So,oe,He,rr,Ot,Pc,or,Oc,jo,Fe,qc,qt,Ic,Nc,Ao,xa,ne,Hc,It,Fc,Lc,nr,zc,Rc,To,H,Uc,ir,Gc,Wc,lr,Bc,Xc,pr,Jc,Kc,Mo,Nt,Do,Le,Vc,Ht,Yc,Qc,Co,ie,ze,cr,Ft,Zc,hr,eh,Po,F,th,Lt,ah,sh,mr,rh,oh,ur,nh,ih,Oo,w,lh,fr,ph,ch,zt,hh,mh,dr,uh,fh,gr,dh,gh,_r,_h,vh,vr,yh,Eh,qo,$a,Rt,bh,yr,wh,kh,Io,Ut,No,le,Re,Er,Gt,xh,br,$h,Ho,Wt,Bt,Sh,jh,Fo,Xt,Lo,Ue,Ah,Jt,Th,Mh,zo;return Ke=new P({}),Ve=new P({}),tt=new O({props:{code:"pip install sagemaker --upgrade",highlighted:"pip install sagemaker --upgrade"}}),at=new O({props:{code:`import sagemaker
sess = sagemaker.Session()
role = sagemaker.get_execution_role()`,highlighted:`<span class="hljs-keyword">import</span> sagemaker
sess = sagemaker.Session()
role = sagemaker.get_execution_role()`}}),st=new O({props:{code:`import sagemaker
import boto3

iam_client = boto3.client('iam')
role = iam_client.get_role(RoleName='role-name-of-your-iam-role-with-right-permissions')['Role']['Arn']
sess = sagemaker.Session()`,highlighted:`<span class="hljs-keyword">import</span> sagemaker
<span class="hljs-keyword">import</span> boto3

iam_client = boto3.client(<span class="hljs-string">&#x27;iam&#x27;</span>)
role = iam_client.get_role(RoleName=<span class="hljs-string">&#x27;role-name-of-your-iam-role-with-right-permissions&#x27;</span>)[<span class="hljs-string">&#x27;Role&#x27;</span>][<span class="hljs-string">&#x27;Arn&#x27;</span>]
sess = sagemaker.Session()`}}),rt=new P({}),nt=new O({props:{code:`import transformers
import datasets
import argparse
import os

if __name__ == "__main__":

    parser = argparse.ArgumentParser()

    # hyperparameters sent by the client are passed as command-line arguments to the script
    parser.add_argument("--epochs", type=int, default=3)
    parser.add_argument("--per_device_train_batch_size", type=int, default=32)
    parser.add_argument("--model_name_or_path", type=str)

    # data, model, and output directories
    parser.add_argument("--model-dir", type=str, default=os.environ["SM_MODEL_DIR"])
    parser.add_argument("--training_dir", type=str, default=os.environ["SM_CHANNEL_TRAIN"])
    parser.add_argument("--test_dir", type=str, default=os.environ["SM_CHANNEL_TEST"])`,highlighted:`<span class="hljs-keyword">import</span> transformers
<span class="hljs-keyword">import</span> datasets
<span class="hljs-keyword">import</span> argparse
<span class="hljs-keyword">import</span> os

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:

    parser = argparse.ArgumentParser()

    <span class="hljs-comment"># hyperparameters sent by the client are passed as command-line arguments to the script</span>
    parser.add_argument(<span class="hljs-string">&quot;--epochs&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">3</span>)
    parser.add_argument(<span class="hljs-string">&quot;--per_device_train_batch_size&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">int</span>, default=<span class="hljs-number">32</span>)
    parser.add_argument(<span class="hljs-string">&quot;--model_name_or_path&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>)

    <span class="hljs-comment"># data, model, and output directories</span>
    parser.add_argument(<span class="hljs-string">&quot;--model-dir&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=os.environ[<span class="hljs-string">&quot;SM_MODEL_DIR&quot;</span>])
    parser.add_argument(<span class="hljs-string">&quot;--training_dir&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=os.environ[<span class="hljs-string">&quot;SM_CHANNEL_TRAIN&quot;</span>])
    parser.add_argument(<span class="hljs-string">&quot;--test_dir&quot;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, default=os.environ[<span class="hljs-string">&quot;SM_CHANNEL_TEST&quot;</span>])`}}),lt=new P({}),mt=new P({}),gt=new O({props:{code:`from sagemaker.huggingface import HuggingFace


# hyperparameters which are passed to the training job
hyperparameters={'epochs': 1,
                 'per_device_train_batch_size': 32,
                 'model_name_or_path': 'distilbert-base-uncased'
                 }

# create the Estimator
huggingface_estimator = HuggingFace(
        entry_point='train.py',
        source_dir='./scripts',
        instance_type='ml.p3.2xlarge',
        instance_count=1,
        role=role,
        transformers_version='4.4',
        pytorch_version='1.6',
        py_version='py36',
        hyperparameters = hyperparameters
)`,highlighted:`<span class="hljs-keyword">from</span> sagemaker.huggingface <span class="hljs-keyword">import</span> HuggingFace


<span class="hljs-comment"># hyperparameters which are passed to the training job</span>
hyperparameters={<span class="hljs-string">&#x27;epochs&#x27;</span>: <span class="hljs-number">1</span>,
                 <span class="hljs-string">&#x27;per_device_train_batch_size&#x27;</span>: <span class="hljs-number">32</span>,
                 <span class="hljs-string">&#x27;model_name_or_path&#x27;</span>: <span class="hljs-string">&#x27;distilbert-base-uncased&#x27;</span>
                 }

<span class="hljs-comment"># create the Estimator</span>
huggingface_estimator = HuggingFace(
        entry_point=<span class="hljs-string">&#x27;train.py&#x27;</span>,
        source_dir=<span class="hljs-string">&#x27;./scripts&#x27;</span>,
        instance_type=<span class="hljs-string">&#x27;ml.p3.2xlarge&#x27;</span>,
        instance_count=<span class="hljs-number">1</span>,
        role=role,
        transformers_version=<span class="hljs-string">&#x27;4.4&#x27;</span>,
        pytorch_version=<span class="hljs-string">&#x27;1.6&#x27;</span>,
        py_version=<span class="hljs-string">&#x27;py36&#x27;</span>,
        hyperparameters = hyperparameters
)`}}),_t=new P({}),Et=new O({props:{code:`huggingface_estimator.fit(
  {'train': 's3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/train',
   'test': 's3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/test'}
)`,highlighted:`huggingface_estimator.fit(
  {<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;s3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/train&#x27;</span>,
   <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;s3://sagemaker-us-east-1-558105141721/samples/datasets/imdb/test&#x27;</span>}
)`}}),bt=new O({props:{code:"/opt/conda/bin/python train.py --epochs 1 --model_name_or_path distilbert-base-uncased --per_device_train_batch_size 32",highlighted:"/opt/conda/bin/python train.py --epochs 1 --model_name_or_path distilbert-base-uncased --per_device_train_batch_size 32"}}),wt=new P({}),xt=new O({props:{code:`from sagemaker.s3 import S3Downloader

S3Downloader.download(
    s3_uri=huggingface_estimator.model_data, # S3 URI where the trained model is located
    local_path='.',                          # local path where *.targ.gz is saved
    sagemaker_session=sess                   # SageMaker session used for training the model
)`,highlighted:`<span class="hljs-keyword">from</span> sagemaker.s3 <span class="hljs-keyword">import</span> S3Downloader

S3Downloader.download(
    s3_uri=huggingface_estimator.model_data, <span class="hljs-comment"># S3 URI where the trained model is located</span>
    local_path=<span class="hljs-string">&#x27;.&#x27;</span>,                          <span class="hljs-comment"># local path where *.targ.gz is saved</span>
    sagemaker_session=sess                   <span class="hljs-comment"># SageMaker session used for training the model</span>
)`}}),$t=new P({}),St=new P({}),At=new O({props:{code:`# configuration for running training on smdistributed data parallel
distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}

# create the Estimator
huggingface_estimator = HuggingFace(
        entry_point='train.py',
        source_dir='./scripts',
        instance_type='ml.p3dn.24xlarge',
        instance_count=2,
        role=role,
        transformers_version='4.4.2',
        pytorch_version='1.6.0',
        py_version='py36',
        hyperparameters = hyperparameters
        distribution = distribution
)`,highlighted:`<span class="hljs-comment"># configuration for running training on smdistributed data parallel</span>
distribution = {<span class="hljs-string">&#x27;smdistributed&#x27;</span>:{<span class="hljs-string">&#x27;dataparallel&#x27;</span>:{ <span class="hljs-string">&#x27;enabled&#x27;</span>: <span class="hljs-literal">True</span> }}}

<span class="hljs-comment"># create the Estimator</span>
huggingface_estimator = HuggingFace(
        entry_point=<span class="hljs-string">&#x27;train.py&#x27;</span>,
        source_dir=<span class="hljs-string">&#x27;./scripts&#x27;</span>,
        instance_type=<span class="hljs-string">&#x27;ml.p3dn.24xlarge&#x27;</span>,
        instance_count=<span class="hljs-number">2</span>,
        role=role,
        transformers_version=<span class="hljs-string">&#x27;4.4.2&#x27;</span>,
        pytorch_version=<span class="hljs-string">&#x27;1.6.0&#x27;</span>,
        py_version=<span class="hljs-string">&#x27;py36&#x27;</span>,
        hyperparameters = hyperparameters
        distribution = distribution
)`}}),Mt=new P({}),Ct=new O({props:{code:`# configuration for running training on smdistributed model parallel
mpi_options = {
    "enabled" : True,
    "processes_per_host" : 8
}

smp_options = {
    "enabled":True,
    "parameters": {
        "microbatches": 4,
        "placement_strategy": "spread",
        "pipeline": "interleaved",
        "optimize": "speed",
        "partitions": 4,
        "ddp": True,
    }
}

distribution={
    "smdistributed": {"modelparallel": smp_options},
    "mpi": mpi_options
}

 # create the Estimator
huggingface_estimator = HuggingFace(
        entry_point='train.py',
        source_dir='./scripts',
        instance_type='ml.p3dn.24xlarge',
        instance_count=2,
        role=role,
        transformers_version='4.4.2',
        pytorch_version='1.6.0',
        py_version='py36',
        hyperparameters = hyperparameters,
        distribution = distribution
)`,highlighted:`<span class="hljs-comment"># configuration for running training on smdistributed model parallel</span>
mpi_options = {
    <span class="hljs-string">&quot;enabled&quot;</span> : <span class="hljs-literal">True</span>,
    <span class="hljs-string">&quot;processes_per_host&quot;</span> : <span class="hljs-number">8</span>
}

smp_options = {
    <span class="hljs-string">&quot;enabled&quot;</span>:<span class="hljs-literal">True</span>,
    <span class="hljs-string">&quot;parameters&quot;</span>: {
        <span class="hljs-string">&quot;microbatches&quot;</span>: <span class="hljs-number">4</span>,
        <span class="hljs-string">&quot;placement_strategy&quot;</span>: <span class="hljs-string">&quot;spread&quot;</span>,
        <span class="hljs-string">&quot;pipeline&quot;</span>: <span class="hljs-string">&quot;interleaved&quot;</span>,
        <span class="hljs-string">&quot;optimize&quot;</span>: <span class="hljs-string">&quot;speed&quot;</span>,
        <span class="hljs-string">&quot;partitions&quot;</span>: <span class="hljs-number">4</span>,
        <span class="hljs-string">&quot;ddp&quot;</span>: <span class="hljs-literal">True</span>,
    }
}

distribution={
    <span class="hljs-string">&quot;smdistributed&quot;</span>: {<span class="hljs-string">&quot;modelparallel&quot;</span>: smp_options},
    <span class="hljs-string">&quot;mpi&quot;</span>: mpi_options
}

 <span class="hljs-comment"># create the Estimator</span>
huggingface_estimator = HuggingFace(
        entry_point=<span class="hljs-string">&#x27;train.py&#x27;</span>,
        source_dir=<span class="hljs-string">&#x27;./scripts&#x27;</span>,
        instance_type=<span class="hljs-string">&#x27;ml.p3dn.24xlarge&#x27;</span>,
        instance_count=<span class="hljs-number">2</span>,
        role=role,
        transformers_version=<span class="hljs-string">&#x27;4.4.2&#x27;</span>,
        pytorch_version=<span class="hljs-string">&#x27;1.6.0&#x27;</span>,
        py_version=<span class="hljs-string">&#x27;py36&#x27;</span>,
        hyperparameters = hyperparameters,
        distribution = distribution
)`}}),Ot=new P({}),Nt=new O({props:{code:`# hyperparameters which are passed to the training job
hyperparameters={'epochs': 1,
                 'train_batch_size': 32,
                 'model_name':'distilbert-base-uncased',
                 'output_dir':'/opt/ml/checkpoints'
                 }

# create the Estimator
huggingface_estimator = HuggingFace(
        entry_point='train.py',
        source_dir='./scripts',
        instance_type='ml.p3.2xlarge',
        instance_count=1,
	    checkpoint_s3_uri=f's3://{sess.default_bucket()}/checkpoints'
        use_spot_instances=True,
        # max_wait should be equal to or greater than max_run in seconds
        max_wait=3600,
        max_run=1000,
        role=role,
        transformers_version='4.4',
        pytorch_version='1.6',
        py_version='py36',
        hyperparameters = hyperparameters
)

# Training seconds: 874
# Billable seconds: 262
# Managed Spot Training savings: 70.0%`,highlighted:`<span class="hljs-comment"># hyperparameters which are passed to the training job</span>
hyperparameters={<span class="hljs-string">&#x27;epochs&#x27;</span>: <span class="hljs-number">1</span>,
                 <span class="hljs-string">&#x27;train_batch_size&#x27;</span>: <span class="hljs-number">32</span>,
                 <span class="hljs-string">&#x27;model_name&#x27;</span>:<span class="hljs-string">&#x27;distilbert-base-uncased&#x27;</span>,
                 <span class="hljs-string">&#x27;output_dir&#x27;</span>:<span class="hljs-string">&#x27;/opt/ml/checkpoints&#x27;</span>
                 }

<span class="hljs-comment"># create the Estimator</span>
huggingface_estimator = HuggingFace(
        entry_point=<span class="hljs-string">&#x27;train.py&#x27;</span>,
        source_dir=<span class="hljs-string">&#x27;./scripts&#x27;</span>,
        instance_type=<span class="hljs-string">&#x27;ml.p3.2xlarge&#x27;</span>,
        instance_count=<span class="hljs-number">1</span>,
	    checkpoint_s3_uri=<span class="hljs-string">f&#x27;s3://<span class="hljs-subst">{sess.default_bucket()}</span>/checkpoints&#x27;</span>
        use_spot_instances=<span class="hljs-literal">True</span>,
        <span class="hljs-comment"># max_wait should be equal to or greater than max_run in seconds</span>
        max_wait=<span class="hljs-number">3600</span>,
        max_run=<span class="hljs-number">1000</span>,
        role=role,
        transformers_version=<span class="hljs-string">&#x27;4.4&#x27;</span>,
        pytorch_version=<span class="hljs-string">&#x27;1.6&#x27;</span>,
        py_version=<span class="hljs-string">&#x27;py36&#x27;</span>,
        hyperparameters = hyperparameters
)

<span class="hljs-comment"># Training seconds: 874</span>
<span class="hljs-comment"># Billable seconds: 262</span>
<span class="hljs-comment"># Managed Spot Training savings: 70.0%</span>`}}),Ft=new P({}),Ut=new O({props:{code:`# configure git settings
git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.4.2'} # v4.4.2 refers to the transformers_version you use in the estimator

 # create the Estimator
huggingface_estimator = HuggingFace(
        entry_point='run_glue.py',
        source_dir='./examples/pytorch/text-classification',
        git_config=git_config,
        instance_type='ml.p3.2xlarge',
        instance_count=1,
        role=role,
        transformers_version='4.4',
        pytorch_version='1.6',
        py_version='py36',
        hyperparameters=hyperparameters
)`,highlighted:`<span class="hljs-comment"># configure git settings</span>
git_config = {<span class="hljs-string">&#x27;repo&#x27;</span>: <span class="hljs-string">&#x27;https://github.com/huggingface/transformers.git&#x27;</span>,<span class="hljs-string">&#x27;branch&#x27;</span>: <span class="hljs-string">&#x27;v4.4.2&#x27;</span>} <span class="hljs-comment"># v4.4.2 refers to the transformers_version you use in the estimator</span>

 <span class="hljs-comment"># create the Estimator</span>
huggingface_estimator = HuggingFace(
        entry_point=<span class="hljs-string">&#x27;run_glue.py&#x27;</span>,
        source_dir=<span class="hljs-string">&#x27;./examples/pytorch/text-classification&#x27;</span>,
        git_config=git_config,
        instance_type=<span class="hljs-string">&#x27;ml.p3.2xlarge&#x27;</span>,
        instance_count=<span class="hljs-number">1</span>,
        role=role,
        transformers_version=<span class="hljs-string">&#x27;4.4&#x27;</span>,
        pytorch_version=<span class="hljs-string">&#x27;1.6&#x27;</span>,
        py_version=<span class="hljs-string">&#x27;py36&#x27;</span>,
        hyperparameters=hyperparameters
)`}}),Gt=new P({}),Xt=new O({props:{code:`# define metrics definitions
metric_definitions = [
    {"Name": "train_runtime", "Regex": "train_runtime.*=\\D*(.*?)$"},
    {"Name": "eval_accuracy", "Regex": "eval_accuracy.*=\\D*(.*?)$"},
    {"Name": "eval_loss", "Regex": "eval_loss.*=\\D*(.*?)$"},
]

# create the Estimator
huggingface_estimator = HuggingFace(
        entry_point='train.py',
        source_dir='./scripts',
        instance_type='ml.p3.2xlarge',
        instance_count=1,
        role=role,
        transformers_version='4.4',
        pytorch_version='1.6',
        py_version='py36',
        metric_definitions=metric_definitions,
        hyperparameters = hyperparameters)`,highlighted:`<span class="hljs-comment"># define metrics definitions</span>
metric_definitions = [
    {<span class="hljs-string">&quot;Name&quot;</span>: <span class="hljs-string">&quot;train_runtime&quot;</span>, <span class="hljs-string">&quot;Regex&quot;</span>: <span class="hljs-string">&quot;train_runtime.*=\\D*(.*?)$&quot;</span>},
    {<span class="hljs-string">&quot;Name&quot;</span>: <span class="hljs-string">&quot;eval_accuracy&quot;</span>, <span class="hljs-string">&quot;Regex&quot;</span>: <span class="hljs-string">&quot;eval_accuracy.*=\\D*(.*?)$&quot;</span>},
    {<span class="hljs-string">&quot;Name&quot;</span>: <span class="hljs-string">&quot;eval_loss&quot;</span>, <span class="hljs-string">&quot;Regex&quot;</span>: <span class="hljs-string">&quot;eval_loss.*=\\D*(.*?)$&quot;</span>},
]

<span class="hljs-comment"># create the Estimator</span>
huggingface_estimator = HuggingFace(
        entry_point=<span class="hljs-string">&#x27;train.py&#x27;</span>,
        source_dir=<span class="hljs-string">&#x27;./scripts&#x27;</span>,
        instance_type=<span class="hljs-string">&#x27;ml.p3.2xlarge&#x27;</span>,
        instance_count=<span class="hljs-number">1</span>,
        role=role,
        transformers_version=<span class="hljs-string">&#x27;4.4&#x27;</span>,
        pytorch_version=<span class="hljs-string">&#x27;1.6&#x27;</span>,
        py_version=<span class="hljs-string">&#x27;py36&#x27;</span>,
        metric_definitions=metric_definitions,
        hyperparameters = hyperparameters)`}}),{c(){X=o("meta"),$r=h(),J=o("h1"),pe=o("a"),Da=o("span"),u(Ke.$$.fragment),kn=h(),Ca=o("span"),xn=s("Run training on Amazon SageMaker"),Sr=h(),T=o("iframe"),jr=h(),ce=o("p"),$n=s("This guide will show you how to train a \u{1F917} Transformers model with the "),Pa=o("code"),Sn=s("HuggingFace"),jn=s(" SageMaker Python SDK. Learn how to:"),Ar=h(),y=o("ul"),Vt=o("li"),Yt=o("a"),An=s("Install and setup your training environment"),Tn=s("."),Mn=h(),Qt=o("li"),Zt=o("a"),Dn=s("Prepare a training script"),Cn=s("."),Pn=h(),ea=o("li"),ta=o("a"),On=s("Create a Hugging Face Estimator"),qn=s("."),In=h(),aa=o("li"),he=o("a"),Nn=s("Run training with the "),Oa=o("code"),Hn=s("fit"),Fn=s(" method"),Ln=s("."),zn=h(),sa=o("li"),ra=o("a"),Rn=s("Access your trained model"),Un=s("."),Gn=h(),oa=o("li"),na=o("a"),Wn=s("Perform distributed training"),Bn=s("."),Xn=h(),ia=o("li"),la=o("a"),Jn=s("Create a spot instance"),Kn=s("."),Vn=h(),pa=o("li"),ca=o("a"),Yn=s("Load a training script from a GitHub repository"),Qn=s("."),Zn=h(),ha=o("li"),ma=o("a"),ei=s("Collect training metrics"),ti=s("."),Tr=h(),K=o("h2"),me=o("a"),qa=o("span"),u(Ve.$$.fragment),ai=h(),Ia=o("span"),si=s("Installation and setup"),Mr=h(),ue=o("p"),ri=s("Before you can train a \u{1F917} Transformers model with SageMaker, you need to sign up for an AWS account. If you don\u2019t have an AWS account yet, learn more "),Ye=o("a"),oi=s("here"),ni=s("."),Dr=h(),ua=o("p"),ii=s("Once you have an AWS account, get started using one of the following:"),Cr=h(),L=o("ul"),Na=o("li"),Qe=o("a"),li=s("SageMaker Studio"),pi=h(),Ha=o("li"),Ze=o("a"),ci=s("SageMaker notebook instance"),hi=h(),Fa=o("li"),mi=s("Local environment"),Pr=h(),fe=o("p"),ui=s("To start training locally, you need to setup an appropriate "),et=o("a"),fi=s("IAM role"),di=s("."),Or=h(),de=o("p"),gi=s("Upgrade to the latest "),La=o("code"),_i=s("sagemaker"),vi=s(" version:"),qr=h(),u(tt.$$.fragment),Ir=h(),fa=o("p"),za=o("strong"),yi=s("SageMaker environment"),Nr=h(),da=o("p"),Ei=s("Setup your SageMaker environment as shown below:"),Hr=h(),u(at.$$.fragment),Fr=h(),ga=o("p"),V=o("em"),bi=s("Note: The execution role is only available when running a notebook within SageMaker. If you run "),Ra=o("code"),wi=s("get_execution_role"),ki=s(" in a notebook not on SageMaker, expect a "),Ua=o("code"),xi=s("region"),$i=s(" error."),Lr=h(),_a=o("p"),Ga=o("strong"),Si=s("Local environment"),zr=h(),va=o("p"),ji=s("Setup your local environment as shown below:"),Rr=h(),u(st.$$.fragment),Ur=h(),Y=o("h2"),ge=o("a"),Wa=o("span"),u(rt.$$.fragment),Ai=h(),Ba=o("span"),Ti=s("Prepare a \u{1F917} Transformers fine-tuning script"),Gr=h(),_e=o("p"),Mi=s("Our training script is very similar to a training script you might run outside of SageMaker. However, you can access useful properties about the training environment through various environment variables (see "),ot=o("a"),Di=s("here"),Ci=s(" for a complete list), such as:"),Wr=h(),z=o("ul"),Xa=o("li"),R=o("p"),Ja=o("code"),Pi=s("SM_MODEL_DIR"),Oi=s(": A string representing the path to which the training job writes the model artifacts. After training, artifacts in this directory are uploaded to S3 for model hosting. "),Ka=o("code"),qi=s("SM_MODEL_DIR"),Ii=s(" is always set to "),Va=o("code"),Ni=s("/opt/ml/model"),Hi=s("."),Fi=h(),Ya=o("li"),ya=o("p"),Qa=o("code"),Li=s("SM_NUM_GPUS"),zi=s(": An integer representing the number of GPUs available to the host."),Ri=h(),Za=o("li"),$=o("p"),es=o("code"),Ui=s("SM_CHANNEL_XXXX:"),Gi=s(" A string representing the path to the directory that contains the input data for the specified channel. For example, when you specify "),ts=o("code"),Wi=s("train"),Bi=s(" and "),as=o("code"),Xi=s("test"),Ji=s(" in the Hugging Face Estimator "),ss=o("code"),Ki=s("fit"),Vi=s(" method, the environment variables are set to "),rs=o("code"),Yi=s("SM_CHANNEL_TRAIN"),Qi=s(" and "),os=o("code"),Zi=s("SM_CHANNEL_TEST"),el=s("."),Br=h(),q=o("p"),tl=s("The "),ns=o("code"),al=s("hyperparameters"),sl=s(" defined in the "),Ea=o("a"),rl=s("Hugging Face Estimator"),ol=s(" are passed as named arguments and processed by "),is=o("code"),nl=s("ArgumentParser()"),il=s("."),Xr=h(),u(nt.$$.fragment),Jr=h(),ba=o("p"),M=o("em"),ll=s("Note that SageMaker doesn\u2019t support argparse actions. For example, if you want to use a boolean hyperparameter, specify "),ls=o("code"),pl=s("type"),cl=s(" as "),ps=o("code"),hl=s("bool"),ml=s(" in your script and provide an explicit "),cs=o("code"),ul=s("True"),fl=s(" or "),hs=o("code"),dl=s("False"),gl=s(" value."),Kr=h(),ve=o("p"),_l=s("Look "),it=o("a"),vl=s("here"),yl=s(" for a complete example of a \u{1F917} Transformers training script."),Vr=h(),Q=o("h2"),ye=o("a"),ms=o("span"),u(lt.$$.fragment),El=h(),us=o("span"),bl=s("Training Output Management"),Yr=h(),E=o("p"),wl=s("If "),fs=o("code"),kl=s("output_dir"),xl=s(" in the "),ds=o("code"),$l=s("TrainingArguments"),Sl=s(" is set to \u2018/opt/ml/model\u2019 the Trainer saves all training artifacts, including logs, checkpoints, and models. Amazon SageMaker archives the whole \u2018/opt/ml/model\u2019 directory as "),gs=o("code"),jl=s("model.tar.gz"),Al=s(" and uploads it at the end of the training job to Amazon S3. Depending on your Hyperparameters and "),_s=o("code"),Tl=s("TrainingArguments"),Ml=s(` this could lead to a large artifact (> 5GB), which can slow down deployment for Amazon SageMaker Inference.
You can control how checkpoints, logs, and artifacts are saved by customization the `),pt=o("a"),Dl=s("TrainingArguments"),Cl=s(". For example by providing "),vs=o("code"),Pl=s("save_total_limit"),Ol=s(" as "),ys=o("code"),ql=s("TrainingArgument"),Il=s(" you can control the limit of the total amount of checkpoints. Deletes the older checkpoints in "),Es=o("code"),Nl=s("output_dir"),Hl=s(" if new ones are saved and the maximum limit is reached."),Qr=h(),b=o("p"),Fl=s("In addition to the options already mentioned above, there is another option to save the training artifacts during the training session. Amazon SageMaker supports "),ct=o("a"),Ll=s("Checkpointing"),zl=s(", which allows you to continuously save your artifacts during training to Amazon S3 rather than at the end of your training. To enable "),ht=o("a"),Rl=s("Checkpointing"),Ul=s(" you need to provide the "),bs=o("code"),Gl=s("checkpoint_s3_uri"),Wl=s(" parameter pointing to an Amazon S3 location in the "),ws=o("code"),Bl=s("HuggingFace"),Xl=s(" estimator and set "),ks=o("code"),Jl=s("output_dir"),Kl=s(" to "),xs=o("code"),Vl=s("/opt/ml/checkpoints"),Yl=s(`.
`),D=o("em"),Ql=s("Note: If you set "),$s=o("code"),Zl=s("output_dir"),ep=s(" to "),Ss=o("code"),tp=s("/opt/ml/checkpoints"),ap=s(" make sure to call "),js=o("code"),sp=s('trainer.save_model("/opt/ml/model")'),rp=s(" or model.save_pretrained(\u201C/opt/ml/model\u201D)/"),As=o("code"),op=s('tokenizer.save_pretrained("/opt/ml/model")'),np=s(" at the end of your training to be able to deploy your model seamlessly to Amazon SageMaker for Inference."),Zr=h(),Z=o("h2"),Ee=o("a"),Ts=o("span"),u(mt.$$.fragment),ip=h(),Ms=o("span"),lp=s("Create a Hugging Face Estimator"),eo=h(),be=o("p"),pp=s("Run \u{1F917} Transformers training scripts on SageMaker by creating a "),ut=o("a"),cp=s("Hugging Face Estimator"),hp=s(". The Estimator handles end-to-end SageMaker training. There are several parameters you should define in the Estimator:"),to=h(),U=o("ol"),wa=o("li"),Ds=o("code"),mp=s("entry_point"),up=s(" specifies which fine-tuning script to use."),fp=h(),we=o("li"),Cs=o("code"),dp=s("instance_type"),gp=s(" specifies an Amazon instance to launch. Refer "),ft=o("a"),_p=s("here"),vp=s(" for a complete list of instance types."),yp=h(),ke=o("li"),Ps=o("code"),Ep=s("hyperparameters"),bp=s(" specifies training hyperparameters. View additional available hyperparameters "),dt=o("a"),wp=s("here"),kp=s("."),ao=h(),j=o("p"),xp=s("The following code sample shows how to train with a custom script "),Os=o("code"),$p=s("train.py"),Sp=s(" with three hyperparameters ("),qs=o("code"),jp=s("epochs"),Ap=s(", "),Is=o("code"),Tp=s("per_device_train_batch_size"),Mp=s(", and "),Ns=o("code"),Dp=s("model_name_or_path"),Cp=s("):"),so=h(),u(gt.$$.fragment),ro=h(),I=o("p"),Pp=s("If you are running a "),Hs=o("code"),Op=s("TrainingJob"),qp=s(" locally, define "),Fs=o("code"),Ip=s("instance_type='local'"),Np=s(" or "),Ls=o("code"),Hp=s("instance_type='local-gpu'"),Fp=s(" for GPU usage. Note that this will not work with SageMaker Studio."),oo=h(),ee=o("h2"),xe=o("a"),zs=o("span"),u(_t.$$.fragment),Lp=h(),Rs=o("span"),zp=s("Execute training"),no=h(),N=o("p"),Rp=s("Start your "),Us=o("code"),Up=s("TrainingJob"),Gp=s(" by calling "),Gs=o("code"),Wp=s("fit"),Bp=s(" on a Hugging Face Estimator. Specify your input training data in "),Ws=o("code"),Xp=s("fit"),Jp=s(". The input training data can be a:"),io=h(),$e=o("ul"),vt=o("li"),Kp=s("S3 URI such as "),Bs=o("code"),Vp=s("s3://my-bucket/my-training-data"),Yp=s("."),Qp=h(),Se=o("li"),Xs=o("code"),Zp=s("FileSystemInput"),ec=s(" for Amazon Elastic File System or FSx for Lustre. See "),yt=o("a"),tc=s("here"),ac=s(" for more details about using these file systems as input."),lo=h(),je=o("p"),sc=s("Call "),Js=o("code"),rc=s("fit"),oc=s(" to begin training:"),po=h(),u(Et.$$.fragment),co=h(),Ae=o("p"),nc=s("SageMaker starts and manages all the required EC2 instances and initiates the "),Ks=o("code"),ic=s("TrainingJob"),lc=s(" by running:"),ho=h(),u(bt.$$.fragment),mo=h(),te=o("h2"),Te=o("a"),Vs=o("span"),u(wt.$$.fragment),pc=h(),Ys=o("span"),cc=s("Access trained model"),uo=h(),Me=o("p"),hc=s("Once training is complete, you can access your model through the "),kt=o("a"),mc=s("AWS console"),uc=s(" or download it directly from S3."),fo=h(),u(xt.$$.fragment),go=h(),ae=o("h2"),De=o("a"),Qs=o("span"),u($t.$$.fragment),fc=h(),Zs=o("span"),dc=s("Distributed training"),_o=h(),ka=o("p"),gc=s("SageMaker provides two strategies for distributed training: data parallelism and model parallelism. Data parallelism splits a training set across several GPUs, while model parallelism splits a model across several GPUs."),vo=h(),se=o("h3"),Ce=o("a"),er=o("span"),u(St.$$.fragment),_c=h(),tr=o("span"),vc=s("Data parallelism"),yo=h(),Pe=o("p"),yc=s("The Hugging Face "),jt=o("a"),Ec=s("Trainer"),bc=s(" supports SageMaker\u2019s data parallelism library. If your training script uses the Trainer API, you only need to define the distribution parameter in the Hugging Face Estimator:"),Eo=h(),u(At.$$.fragment),bo=h(),Oe=o("p"),wc=s("\u{1F4D3} Open the "),Tt=o("a"),kc=s("notebook"),xc=s(" for an example of how to run the data parallelism library with TensorFlow."),wo=h(),re=o("h3"),qe=o("a"),ar=o("span"),u(Mt.$$.fragment),$c=h(),sr=o("span"),Sc=s("Model parallelism"),ko=h(),Ie=o("p"),jc=s("The Hugging Face [Trainer] also supports SageMaker\u2019s model parallelism library. If your training script uses the Trainer API, you only need to define the distribution parameter in the Hugging Face Estimator (see "),Dt=o("a"),Ac=s("here"),Tc=s(" for more detailed information about using model parallelism):"),xo=h(),u(Ct.$$.fragment),$o=h(),Ne=o("p"),Mc=s("\u{1F4D3} Open the "),Pt=o("a"),Dc=s("notebook"),Cc=s(" for an example of how to run the model parallelism library."),So=h(),oe=o("h2"),He=o("a"),rr=o("span"),u(Ot.$$.fragment),Pc=h(),or=o("span"),Oc=s("Spot instances"),jo=h(),Fe=o("p"),qc=s("The Hugging Face extension for the SageMaker Python SDK means we can benefit from "),qt=o("a"),Ic=s("fully-managed EC2 spot instances"),Nc=s(". This can help you save up to 90% of training costs!"),Ao=h(),xa=o("p"),ne=o("em"),Hc=s("Note: Unless your training job completes quickly, we recommend you use "),It=o("a"),Fc=s("checkpointing"),Lc=s(" with managed spot training. In this case, you need to define the "),nr=o("code"),zc=s("checkpoint_s3_uri"),Rc=s("."),To=h(),H=o("p"),Uc=s("Set "),ir=o("code"),Gc=s("use_spot_instances=True"),Wc=s(" and define your "),lr=o("code"),Bc=s("max_wait"),Xc=s(" and "),pr=o("code"),Jc=s("max_run"),Kc=s(" time in the Estimator to use spot instances:"),Mo=h(),u(Nt.$$.fragment),Do=h(),Le=o("p"),Vc=s("\u{1F4D3} Open the "),Ht=o("a"),Yc=s("notebook"),Qc=s(" for an example of how to use spot instances."),Co=h(),ie=o("h2"),ze=o("a"),cr=o("span"),u(Ft.$$.fragment),Zc=h(),hr=o("span"),eh=s("Git repository"),Po=h(),F=o("p"),th=s("The Hugging Face Estimator can load a training script "),Lt=o("a"),ah=s("stored in a GitHub repository"),sh=s(". Provide the relative path to the training script in "),mr=o("code"),rh=s("entry_point"),oh=s(" and the relative path to the directory in "),ur=o("code"),nh=s("source_dir"),ih=s("."),Oo=h(),w=o("p"),lh=s("If you are using "),fr=o("code"),ph=s("git_config"),ch=s(" to run the "),zt=o("a"),hh=s("\u{1F917} Transformers example scripts"),mh=s(", you need to configure the correct "),dr=o("code"),uh=s("'branch'"),fh=s(" in "),gr=o("code"),dh=s("transformers_version"),gh=s(" (e.g. if you use "),_r=o("code"),_h=s("transformers_version='4.4.2"),vh=s(" you have to use "),vr=o("code"),yh=s("'branch':'v4.4.2'"),Eh=s(")."),qo=h(),$a=o("p"),Rt=o("em"),bh=s("Tip: Save your model to S3 by setting "),yr=o("code"),wh=s("output_dir=/opt/ml/model"),kh=s(" in the hyperparameter of your training script."),Io=h(),u(Ut.$$.fragment),No=h(),le=o("h2"),Re=o("a"),Er=o("span"),u(Gt.$$.fragment),xh=h(),br=o("span"),$h=s("SageMaker metrics"),Ho=h(),Wt=o("p"),Bt=o("a"),Sh=s("SageMaker metrics"),jh=s(" automatically parses training job logs for metrics and sends them to CloudWatch. If you want SageMaker to parse the logs, you must specify the metric\u2019s name and a regular expression for SageMaker to use to find the metric."),Fo=h(),u(Xt.$$.fragment),Lo=h(),Ue=o("p"),Ah=s("\u{1F4D3} Open the "),Jt=o("a"),Th=s("notebook"),Mh=s(" for an example of how to capture metrics in SageMaker."),this.h()},l(e){const l=Uf('[data-svelte="svelte-1phssyn"]',document.head);X=n(l,"META",{name:!0,content:!0}),l.forEach(a),$r=m(e),J=n(e,"H1",{class:!0});var Ro=i(J);pe=n(Ro,"A",{id:!0,class:!0,href:!0});var Wh=i(pe);Da=n(Wh,"SPAN",{});var Bh=i(Da);f(Ke.$$.fragment,Bh),Bh.forEach(a),Wh.forEach(a),kn=m(Ro),Ca=n(Ro,"SPAN",{});var Xh=i(Ca);xn=r(Xh,"Run training on Amazon SageMaker"),Xh.forEach(a),Ro.forEach(a),Sr=m(e),T=n(e,"IFRAME",{width:!0,height:!0,src:!0,title:!0,frameborder:!0,allow:!0}),i(T).forEach(a),jr=m(e),ce=n(e,"P",{});var Uo=i(ce);$n=r(Uo,"This guide will show you how to train a \u{1F917} Transformers model with the "),Pa=n(Uo,"CODE",{});var Jh=i(Pa);Sn=r(Jh,"HuggingFace"),Jh.forEach(a),jn=r(Uo," SageMaker Python SDK. Learn how to:"),Uo.forEach(a),Ar=m(e),y=n(e,"UL",{});var k=i(y);Vt=n(k,"LI",{});var Dh=i(Vt);Yt=n(Dh,"A",{href:!0});var Kh=i(Yt);An=r(Kh,"Install and setup your training environment"),Kh.forEach(a),Tn=r(Dh,"."),Dh.forEach(a),Mn=m(k),Qt=n(k,"LI",{});var Ch=i(Qt);Zt=n(Ch,"A",{href:!0});var Vh=i(Zt);Dn=r(Vh,"Prepare a training script"),Vh.forEach(a),Cn=r(Ch,"."),Ch.forEach(a),Pn=m(k),ea=n(k,"LI",{});var Ph=i(ea);ta=n(Ph,"A",{href:!0});var Yh=i(ta);On=r(Yh,"Create a Hugging Face Estimator"),Yh.forEach(a),qn=r(Ph,"."),Ph.forEach(a),In=m(k),aa=n(k,"LI",{});var Oh=i(aa);he=n(Oh,"A",{href:!0});var Go=i(he);Nn=r(Go,"Run training with the "),Oa=n(Go,"CODE",{});var Qh=i(Oa);Hn=r(Qh,"fit"),Qh.forEach(a),Fn=r(Go," method"),Go.forEach(a),Ln=r(Oh,"."),Oh.forEach(a),zn=m(k),sa=n(k,"LI",{});var qh=i(sa);ra=n(qh,"A",{href:!0});var Zh=i(ra);Rn=r(Zh,"Access your trained model"),Zh.forEach(a),Un=r(qh,"."),qh.forEach(a),Gn=m(k),oa=n(k,"LI",{});var Ih=i(oa);na=n(Ih,"A",{href:!0});var em=i(na);Wn=r(em,"Perform distributed training"),em.forEach(a),Bn=r(Ih,"."),Ih.forEach(a),Xn=m(k),ia=n(k,"LI",{});var Nh=i(ia);la=n(Nh,"A",{href:!0});var tm=i(la);Jn=r(tm,"Create a spot instance"),tm.forEach(a),Kn=r(Nh,"."),Nh.forEach(a),Vn=m(k),pa=n(k,"LI",{});var Hh=i(pa);ca=n(Hh,"A",{href:!0});var am=i(ca);Yn=r(am,"Load a training script from a GitHub repository"),am.forEach(a),Qn=r(Hh,"."),Hh.forEach(a),Zn=m(k),ha=n(k,"LI",{});var Fh=i(ha);ma=n(Fh,"A",{href:!0});var sm=i(ma);ei=r(sm,"Collect training metrics"),sm.forEach(a),ti=r(Fh,"."),Fh.forEach(a),k.forEach(a),Tr=m(e),K=n(e,"H2",{class:!0});var Wo=i(K);me=n(Wo,"A",{id:!0,class:!0,href:!0});var rm=i(me);qa=n(rm,"SPAN",{});var om=i(qa);f(Ve.$$.fragment,om),om.forEach(a),rm.forEach(a),ai=m(Wo),Ia=n(Wo,"SPAN",{});var nm=i(Ia);si=r(nm,"Installation and setup"),nm.forEach(a),Wo.forEach(a),Mr=m(e),ue=n(e,"P",{});var Bo=i(ue);ri=r(Bo,"Before you can train a \u{1F917} Transformers model with SageMaker, you need to sign up for an AWS account. If you don\u2019t have an AWS account yet, learn more "),Ye=n(Bo,"A",{href:!0,rel:!0});var im=i(Ye);oi=r(im,"here"),im.forEach(a),ni=r(Bo,"."),Bo.forEach(a),Dr=m(e),ua=n(e,"P",{});var lm=i(ua);ii=r(lm,"Once you have an AWS account, get started using one of the following:"),lm.forEach(a),Cr=m(e),L=n(e,"UL",{});var Sa=i(L);Na=n(Sa,"LI",{});var pm=i(Na);Qe=n(pm,"A",{href:!0,rel:!0});var cm=i(Qe);li=r(cm,"SageMaker Studio"),cm.forEach(a),pm.forEach(a),pi=m(Sa),Ha=n(Sa,"LI",{});var hm=i(Ha);Ze=n(hm,"A",{href:!0,rel:!0});var mm=i(Ze);ci=r(mm,"SageMaker notebook instance"),mm.forEach(a),hm.forEach(a),hi=m(Sa),Fa=n(Sa,"LI",{});var um=i(Fa);mi=r(um,"Local environment"),um.forEach(a),Sa.forEach(a),Pr=m(e),fe=n(e,"P",{});var Xo=i(fe);ui=r(Xo,"To start training locally, you need to setup an appropriate "),et=n(Xo,"A",{href:!0,rel:!0});var fm=i(et);fi=r(fm,"IAM role"),fm.forEach(a),di=r(Xo,"."),Xo.forEach(a),Or=m(e),de=n(e,"P",{});var Jo=i(de);gi=r(Jo,"Upgrade to the latest "),La=n(Jo,"CODE",{});var dm=i(La);_i=r(dm,"sagemaker"),dm.forEach(a),vi=r(Jo," version:"),Jo.forEach(a),qr=m(e),f(tt.$$.fragment,e),Ir=m(e),fa=n(e,"P",{});var gm=i(fa);za=n(gm,"STRONG",{});var _m=i(za);yi=r(_m,"SageMaker environment"),_m.forEach(a),gm.forEach(a),Nr=m(e),da=n(e,"P",{});var vm=i(da);Ei=r(vm,"Setup your SageMaker environment as shown below:"),vm.forEach(a),Hr=m(e),f(at.$$.fragment,e),Fr=m(e),ga=n(e,"P",{});var ym=i(ga);V=n(ym,"EM",{});var ja=i(V);bi=r(ja,"Note: The execution role is only available when running a notebook within SageMaker. If you run "),Ra=n(ja,"CODE",{});var Em=i(Ra);wi=r(Em,"get_execution_role"),Em.forEach(a),ki=r(ja," in a notebook not on SageMaker, expect a "),Ua=n(ja,"CODE",{});var bm=i(Ua);xi=r(bm,"region"),bm.forEach(a),$i=r(ja," error."),ja.forEach(a),ym.forEach(a),Lr=m(e),_a=n(e,"P",{});var wm=i(_a);Ga=n(wm,"STRONG",{});var km=i(Ga);Si=r(km,"Local environment"),km.forEach(a),wm.forEach(a),zr=m(e),va=n(e,"P",{});var xm=i(va);ji=r(xm,"Setup your local environment as shown below:"),xm.forEach(a),Rr=m(e),f(st.$$.fragment,e),Ur=m(e),Y=n(e,"H2",{class:!0});var Ko=i(Y);ge=n(Ko,"A",{id:!0,class:!0,href:!0});var $m=i(ge);Wa=n($m,"SPAN",{});var Sm=i(Wa);f(rt.$$.fragment,Sm),Sm.forEach(a),$m.forEach(a),Ai=m(Ko),Ba=n(Ko,"SPAN",{});var jm=i(Ba);Ti=r(jm,"Prepare a \u{1F917} Transformers fine-tuning script"),jm.forEach(a),Ko.forEach(a),Gr=m(e),_e=n(e,"P",{});var Vo=i(_e);Mi=r(Vo,"Our training script is very similar to a training script you might run outside of SageMaker. However, you can access useful properties about the training environment through various environment variables (see "),ot=n(Vo,"A",{href:!0,rel:!0});var Am=i(ot);Di=r(Am,"here"),Am.forEach(a),Ci=r(Vo," for a complete list), such as:"),Vo.forEach(a),Wr=m(e),z=n(e,"UL",{});var Aa=i(z);Xa=n(Aa,"LI",{});var Tm=i(Xa);R=n(Tm,"P",{});var Kt=i(R);Ja=n(Kt,"CODE",{});var Mm=i(Ja);Pi=r(Mm,"SM_MODEL_DIR"),Mm.forEach(a),Oi=r(Kt,": A string representing the path to which the training job writes the model artifacts. After training, artifacts in this directory are uploaded to S3 for model hosting. "),Ka=n(Kt,"CODE",{});var Dm=i(Ka);qi=r(Dm,"SM_MODEL_DIR"),Dm.forEach(a),Ii=r(Kt," is always set to "),Va=n(Kt,"CODE",{});var Cm=i(Va);Ni=r(Cm,"/opt/ml/model"),Cm.forEach(a),Hi=r(Kt,"."),Kt.forEach(a),Tm.forEach(a),Fi=m(Aa),Ya=n(Aa,"LI",{});var Pm=i(Ya);ya=n(Pm,"P",{});var Lh=i(ya);Qa=n(Lh,"CODE",{});var Om=i(Qa);Li=r(Om,"SM_NUM_GPUS"),Om.forEach(a),zi=r(Lh,": An integer representing the number of GPUs available to the host."),Lh.forEach(a),Pm.forEach(a),Ri=m(Aa),Za=n(Aa,"LI",{});var qm=i(Za);$=n(qm,"P",{});var C=i($);es=n(C,"CODE",{});var Im=i(es);Ui=r(Im,"SM_CHANNEL_XXXX:"),Im.forEach(a),Gi=r(C," A string representing the path to the directory that contains the input data for the specified channel. For example, when you specify "),ts=n(C,"CODE",{});var Nm=i(ts);Wi=r(Nm,"train"),Nm.forEach(a),Bi=r(C," and "),as=n(C,"CODE",{});var Hm=i(as);Xi=r(Hm,"test"),Hm.forEach(a),Ji=r(C," in the Hugging Face Estimator "),ss=n(C,"CODE",{});var Fm=i(ss);Ki=r(Fm,"fit"),Fm.forEach(a),Vi=r(C," method, the environment variables are set to "),rs=n(C,"CODE",{});var Lm=i(rs);Yi=r(Lm,"SM_CHANNEL_TRAIN"),Lm.forEach(a),Qi=r(C," and "),os=n(C,"CODE",{});var zm=i(os);Zi=r(zm,"SM_CHANNEL_TEST"),zm.forEach(a),el=r(C,"."),C.forEach(a),qm.forEach(a),Aa.forEach(a),Br=m(e),q=n(e,"P",{});var Ge=i(q);tl=r(Ge,"The "),ns=n(Ge,"CODE",{});var Rm=i(ns);al=r(Rm,"hyperparameters"),Rm.forEach(a),sl=r(Ge," defined in the "),Ea=n(Ge,"A",{href:!0});var Um=i(Ea);rl=r(Um,"Hugging Face Estimator"),Um.forEach(a),ol=r(Ge," are passed as named arguments and processed by "),is=n(Ge,"CODE",{});var Gm=i(is);nl=r(Gm,"ArgumentParser()"),Gm.forEach(a),il=r(Ge,"."),Ge.forEach(a),Xr=m(e),f(nt.$$.fragment,e),Jr=m(e),ba=n(e,"P",{});var Wm=i(ba);M=n(Wm,"EM",{});var G=i(M);ll=r(G,"Note that SageMaker doesn\u2019t support argparse actions. For example, if you want to use a boolean hyperparameter, specify "),ls=n(G,"CODE",{});var Bm=i(ls);pl=r(Bm,"type"),Bm.forEach(a),cl=r(G," as "),ps=n(G,"CODE",{});var Xm=i(ps);hl=r(Xm,"bool"),Xm.forEach(a),ml=r(G," in your script and provide an explicit "),cs=n(G,"CODE",{});var Jm=i(cs);ul=r(Jm,"True"),Jm.forEach(a),fl=r(G," or "),hs=n(G,"CODE",{});var Km=i(hs);dl=r(Km,"False"),Km.forEach(a),gl=r(G," value."),G.forEach(a),Wm.forEach(a),Kr=m(e),ve=n(e,"P",{});var Yo=i(ve);_l=r(Yo,"Look "),it=n(Yo,"A",{href:!0,rel:!0});var Vm=i(it);vl=r(Vm,"here"),Vm.forEach(a),yl=r(Yo," for a complete example of a \u{1F917} Transformers training script."),Yo.forEach(a),Vr=m(e),Q=n(e,"H2",{class:!0});var Qo=i(Q);ye=n(Qo,"A",{id:!0,class:!0,href:!0});var Ym=i(ye);ms=n(Ym,"SPAN",{});var Qm=i(ms);f(lt.$$.fragment,Qm),Qm.forEach(a),Ym.forEach(a),El=m(Qo),us=n(Qo,"SPAN",{});var Zm=i(us);bl=r(Zm,"Training Output Management"),Zm.forEach(a),Qo.forEach(a),Yr=m(e),E=n(e,"P",{});var x=i(E);wl=r(x,"If "),fs=n(x,"CODE",{});var eu=i(fs);kl=r(eu,"output_dir"),eu.forEach(a),xl=r(x," in the "),ds=n(x,"CODE",{});var tu=i(ds);$l=r(tu,"TrainingArguments"),tu.forEach(a),Sl=r(x," is set to \u2018/opt/ml/model\u2019 the Trainer saves all training artifacts, including logs, checkpoints, and models. Amazon SageMaker archives the whole \u2018/opt/ml/model\u2019 directory as "),gs=n(x,"CODE",{});var au=i(gs);jl=r(au,"model.tar.gz"),au.forEach(a),Al=r(x," and uploads it at the end of the training job to Amazon S3. Depending on your Hyperparameters and "),_s=n(x,"CODE",{});var su=i(_s);Tl=r(su,"TrainingArguments"),su.forEach(a),Ml=r(x,` this could lead to a large artifact (> 5GB), which can slow down deployment for Amazon SageMaker Inference.
You can control how checkpoints, logs, and artifacts are saved by customization the `),pt=n(x,"A",{href:!0,rel:!0});var ru=i(pt);Dl=r(ru,"TrainingArguments"),ru.forEach(a),Cl=r(x,". For example by providing "),vs=n(x,"CODE",{});var ou=i(vs);Pl=r(ou,"save_total_limit"),ou.forEach(a),Ol=r(x," as "),ys=n(x,"CODE",{});var nu=i(ys);ql=r(nu,"TrainingArgument"),nu.forEach(a),Il=r(x," you can control the limit of the total amount of checkpoints. Deletes the older checkpoints in "),Es=n(x,"CODE",{});var iu=i(Es);Nl=r(iu,"output_dir"),iu.forEach(a),Hl=r(x," if new ones are saved and the maximum limit is reached."),x.forEach(a),Qr=m(e),b=n(e,"P",{});var S=i(b);Fl=r(S,"In addition to the options already mentioned above, there is another option to save the training artifacts during the training session. Amazon SageMaker supports "),ct=n(S,"A",{href:!0,rel:!0});var lu=i(ct);Ll=r(lu,"Checkpointing"),lu.forEach(a),zl=r(S,", which allows you to continuously save your artifacts during training to Amazon S3 rather than at the end of your training. To enable "),ht=n(S,"A",{href:!0,rel:!0});var pu=i(ht);Rl=r(pu,"Checkpointing"),pu.forEach(a),Ul=r(S," you need to provide the "),bs=n(S,"CODE",{});var cu=i(bs);Gl=r(cu,"checkpoint_s3_uri"),cu.forEach(a),Wl=r(S," parameter pointing to an Amazon S3 location in the "),ws=n(S,"CODE",{});var hu=i(ws);Bl=r(hu,"HuggingFace"),hu.forEach(a),Xl=r(S," estimator and set "),ks=n(S,"CODE",{});var mu=i(ks);Jl=r(mu,"output_dir"),mu.forEach(a),Kl=r(S," to "),xs=n(S,"CODE",{});var uu=i(xs);Vl=r(uu,"/opt/ml/checkpoints"),uu.forEach(a),Yl=r(S,`.
`),D=n(S,"EM",{});var W=i(D);Ql=r(W,"Note: If you set "),$s=n(W,"CODE",{});var fu=i($s);Zl=r(fu,"output_dir"),fu.forEach(a),ep=r(W," to "),Ss=n(W,"CODE",{});var du=i(Ss);tp=r(du,"/opt/ml/checkpoints"),du.forEach(a),ap=r(W," make sure to call "),js=n(W,"CODE",{});var gu=i(js);sp=r(gu,'trainer.save_model("/opt/ml/model")'),gu.forEach(a),rp=r(W," or model.save_pretrained(\u201C/opt/ml/model\u201D)/"),As=n(W,"CODE",{});var _u=i(As);op=r(_u,'tokenizer.save_pretrained("/opt/ml/model")'),_u.forEach(a),np=r(W," at the end of your training to be able to deploy your model seamlessly to Amazon SageMaker for Inference."),W.forEach(a),S.forEach(a),Zr=m(e),Z=n(e,"H2",{class:!0});var Zo=i(Z);Ee=n(Zo,"A",{id:!0,class:!0,href:!0});var vu=i(Ee);Ts=n(vu,"SPAN",{});var yu=i(Ts);f(mt.$$.fragment,yu),yu.forEach(a),vu.forEach(a),ip=m(Zo),Ms=n(Zo,"SPAN",{});var Eu=i(Ms);lp=r(Eu,"Create a Hugging Face Estimator"),Eu.forEach(a),Zo.forEach(a),eo=m(e),be=n(e,"P",{});var en=i(be);pp=r(en,"Run \u{1F917} Transformers training scripts on SageMaker by creating a "),ut=n(en,"A",{href:!0,rel:!0});var bu=i(ut);cp=r(bu,"Hugging Face Estimator"),bu.forEach(a),hp=r(en,". The Estimator handles end-to-end SageMaker training. There are several parameters you should define in the Estimator:"),en.forEach(a),to=m(e),U=n(e,"OL",{});var Ta=i(U);wa=n(Ta,"LI",{});var zh=i(wa);Ds=n(zh,"CODE",{});var wu=i(Ds);mp=r(wu,"entry_point"),wu.forEach(a),up=r(zh," specifies which fine-tuning script to use."),zh.forEach(a),fp=m(Ta),we=n(Ta,"LI",{});var wr=i(we);Cs=n(wr,"CODE",{});var ku=i(Cs);dp=r(ku,"instance_type"),ku.forEach(a),gp=r(wr," specifies an Amazon instance to launch. Refer "),ft=n(wr,"A",{href:!0,rel:!0});var xu=i(ft);_p=r(xu,"here"),xu.forEach(a),vp=r(wr," for a complete list of instance types."),wr.forEach(a),yp=m(Ta),ke=n(Ta,"LI",{});var kr=i(ke);Ps=n(kr,"CODE",{});var $u=i(Ps);Ep=r($u,"hyperparameters"),$u.forEach(a),bp=r(kr," specifies training hyperparameters. View additional available hyperparameters "),dt=n(kr,"A",{href:!0,rel:!0});var Su=i(dt);wp=r(Su,"here"),Su.forEach(a),kp=r(kr,"."),kr.forEach(a),Ta.forEach(a),ao=m(e),j=n(e,"P",{});var B=i(j);xp=r(B,"The following code sample shows how to train with a custom script "),Os=n(B,"CODE",{});var ju=i(Os);$p=r(ju,"train.py"),ju.forEach(a),Sp=r(B," with three hyperparameters ("),qs=n(B,"CODE",{});var Au=i(qs);jp=r(Au,"epochs"),Au.forEach(a),Ap=r(B,", "),Is=n(B,"CODE",{});var Tu=i(Is);Tp=r(Tu,"per_device_train_batch_size"),Tu.forEach(a),Mp=r(B,", and "),Ns=n(B,"CODE",{});var Mu=i(Ns);Dp=r(Mu,"model_name_or_path"),Mu.forEach(a),Cp=r(B,"):"),B.forEach(a),so=m(e),f(gt.$$.fragment,e),ro=m(e),I=n(e,"P",{});var We=i(I);Pp=r(We,"If you are running a "),Hs=n(We,"CODE",{});var Du=i(Hs);Op=r(Du,"TrainingJob"),Du.forEach(a),qp=r(We," locally, define "),Fs=n(We,"CODE",{});var Cu=i(Fs);Ip=r(Cu,"instance_type='local'"),Cu.forEach(a),Np=r(We," or "),Ls=n(We,"CODE",{});var Pu=i(Ls);Hp=r(Pu,"instance_type='local-gpu'"),Pu.forEach(a),Fp=r(We," for GPU usage. Note that this will not work with SageMaker Studio."),We.forEach(a),oo=m(e),ee=n(e,"H2",{class:!0});var tn=i(ee);xe=n(tn,"A",{id:!0,class:!0,href:!0});var Ou=i(xe);zs=n(Ou,"SPAN",{});var qu=i(zs);f(_t.$$.fragment,qu),qu.forEach(a),Ou.forEach(a),Lp=m(tn),Rs=n(tn,"SPAN",{});var Iu=i(Rs);zp=r(Iu,"Execute training"),Iu.forEach(a),tn.forEach(a),no=m(e),N=n(e,"P",{});var Be=i(N);Rp=r(Be,"Start your "),Us=n(Be,"CODE",{});var Nu=i(Us);Up=r(Nu,"TrainingJob"),Nu.forEach(a),Gp=r(Be," by calling "),Gs=n(Be,"CODE",{});var Hu=i(Gs);Wp=r(Hu,"fit"),Hu.forEach(a),Bp=r(Be," on a Hugging Face Estimator. Specify your input training data in "),Ws=n(Be,"CODE",{});var Fu=i(Ws);Xp=r(Fu,"fit"),Fu.forEach(a),Jp=r(Be,". The input training data can be a:"),Be.forEach(a),io=m(e),$e=n(e,"UL",{});var an=i($e);vt=n(an,"LI",{});var sn=i(vt);Kp=r(sn,"S3 URI such as "),Bs=n(sn,"CODE",{});var Lu=i(Bs);Vp=r(Lu,"s3://my-bucket/my-training-data"),Lu.forEach(a),Yp=r(sn,"."),sn.forEach(a),Qp=m(an),Se=n(an,"LI",{});var xr=i(Se);Xs=n(xr,"CODE",{});var zu=i(Xs);Zp=r(zu,"FileSystemInput"),zu.forEach(a),ec=r(xr," for Amazon Elastic File System or FSx for Lustre. See "),yt=n(xr,"A",{href:!0,rel:!0});var Ru=i(yt);tc=r(Ru,"here"),Ru.forEach(a),ac=r(xr," for more details about using these file systems as input."),xr.forEach(a),an.forEach(a),lo=m(e),je=n(e,"P",{});var rn=i(je);sc=r(rn,"Call "),Js=n(rn,"CODE",{});var Uu=i(Js);rc=r(Uu,"fit"),Uu.forEach(a),oc=r(rn," to begin training:"),rn.forEach(a),po=m(e),f(Et.$$.fragment,e),co=m(e),Ae=n(e,"P",{});var on=i(Ae);nc=r(on,"SageMaker starts and manages all the required EC2 instances and initiates the "),Ks=n(on,"CODE",{});var Gu=i(Ks);ic=r(Gu,"TrainingJob"),Gu.forEach(a),lc=r(on," by running:"),on.forEach(a),ho=m(e),f(bt.$$.fragment,e),mo=m(e),te=n(e,"H2",{class:!0});var nn=i(te);Te=n(nn,"A",{id:!0,class:!0,href:!0});var Wu=i(Te);Vs=n(Wu,"SPAN",{});var Bu=i(Vs);f(wt.$$.fragment,Bu),Bu.forEach(a),Wu.forEach(a),pc=m(nn),Ys=n(nn,"SPAN",{});var Xu=i(Ys);cc=r(Xu,"Access trained model"),Xu.forEach(a),nn.forEach(a),uo=m(e),Me=n(e,"P",{});var ln=i(Me);hc=r(ln,"Once training is complete, you can access your model through the "),kt=n(ln,"A",{href:!0,rel:!0});var Ju=i(kt);mc=r(Ju,"AWS console"),Ju.forEach(a),uc=r(ln," or download it directly from S3."),ln.forEach(a),fo=m(e),f(xt.$$.fragment,e),go=m(e),ae=n(e,"H2",{class:!0});var pn=i(ae);De=n(pn,"A",{id:!0,class:!0,href:!0});var Ku=i(De);Qs=n(Ku,"SPAN",{});var Vu=i(Qs);f($t.$$.fragment,Vu),Vu.forEach(a),Ku.forEach(a),fc=m(pn),Zs=n(pn,"SPAN",{});var Yu=i(Zs);dc=r(Yu,"Distributed training"),Yu.forEach(a),pn.forEach(a),_o=m(e),ka=n(e,"P",{});var Qu=i(ka);gc=r(Qu,"SageMaker provides two strategies for distributed training: data parallelism and model parallelism. Data parallelism splits a training set across several GPUs, while model parallelism splits a model across several GPUs."),Qu.forEach(a),vo=m(e),se=n(e,"H3",{class:!0});var cn=i(se);Ce=n(cn,"A",{id:!0,class:!0,href:!0});var Zu=i(Ce);er=n(Zu,"SPAN",{});var ef=i(er);f(St.$$.fragment,ef),ef.forEach(a),Zu.forEach(a),_c=m(cn),tr=n(cn,"SPAN",{});var tf=i(tr);vc=r(tf,"Data parallelism"),tf.forEach(a),cn.forEach(a),yo=m(e),Pe=n(e,"P",{});var hn=i(Pe);yc=r(hn,"The Hugging Face "),jt=n(hn,"A",{href:!0,rel:!0});var af=i(jt);Ec=r(af,"Trainer"),af.forEach(a),bc=r(hn," supports SageMaker\u2019s data parallelism library. If your training script uses the Trainer API, you only need to define the distribution parameter in the Hugging Face Estimator:"),hn.forEach(a),Eo=m(e),f(At.$$.fragment,e),bo=m(e),Oe=n(e,"P",{});var mn=i(Oe);wc=r(mn,"\u{1F4D3} Open the "),Tt=n(mn,"A",{href:!0,rel:!0});var sf=i(Tt);kc=r(sf,"notebook"),sf.forEach(a),xc=r(mn," for an example of how to run the data parallelism library with TensorFlow."),mn.forEach(a),wo=m(e),re=n(e,"H3",{class:!0});var un=i(re);qe=n(un,"A",{id:!0,class:!0,href:!0});var rf=i(qe);ar=n(rf,"SPAN",{});var of=i(ar);f(Mt.$$.fragment,of),of.forEach(a),rf.forEach(a),$c=m(un),sr=n(un,"SPAN",{});var nf=i(sr);Sc=r(nf,"Model parallelism"),nf.forEach(a),un.forEach(a),ko=m(e),Ie=n(e,"P",{});var fn=i(Ie);jc=r(fn,"The Hugging Face [Trainer] also supports SageMaker\u2019s model parallelism library. If your training script uses the Trainer API, you only need to define the distribution parameter in the Hugging Face Estimator (see "),Dt=n(fn,"A",{href:!0,rel:!0});var lf=i(Dt);Ac=r(lf,"here"),lf.forEach(a),Tc=r(fn," for more detailed information about using model parallelism):"),fn.forEach(a),xo=m(e),f(Ct.$$.fragment,e),$o=m(e),Ne=n(e,"P",{});var dn=i(Ne);Mc=r(dn,"\u{1F4D3} Open the "),Pt=n(dn,"A",{href:!0,rel:!0});var pf=i(Pt);Dc=r(pf,"notebook"),pf.forEach(a),Cc=r(dn," for an example of how to run the model parallelism library."),dn.forEach(a),So=m(e),oe=n(e,"H2",{class:!0});var gn=i(oe);He=n(gn,"A",{id:!0,class:!0,href:!0});var cf=i(He);rr=n(cf,"SPAN",{});var hf=i(rr);f(Ot.$$.fragment,hf),hf.forEach(a),cf.forEach(a),Pc=m(gn),or=n(gn,"SPAN",{});var mf=i(or);Oc=r(mf,"Spot instances"),mf.forEach(a),gn.forEach(a),jo=m(e),Fe=n(e,"P",{});var _n=i(Fe);qc=r(_n,"The Hugging Face extension for the SageMaker Python SDK means we can benefit from "),qt=n(_n,"A",{href:!0,rel:!0});var uf=i(qt);Ic=r(uf,"fully-managed EC2 spot instances"),uf.forEach(a),Nc=r(_n,". This can help you save up to 90% of training costs!"),_n.forEach(a),Ao=m(e),xa=n(e,"P",{});var ff=i(xa);ne=n(ff,"EM",{});var Ma=i(ne);Hc=r(Ma,"Note: Unless your training job completes quickly, we recommend you use "),It=n(Ma,"A",{href:!0,rel:!0});var df=i(It);Fc=r(df,"checkpointing"),df.forEach(a),Lc=r(Ma," with managed spot training. In this case, you need to define the "),nr=n(Ma,"CODE",{});var gf=i(nr);zc=r(gf,"checkpoint_s3_uri"),gf.forEach(a),Rc=r(Ma,"."),Ma.forEach(a),ff.forEach(a),To=m(e),H=n(e,"P",{});var Xe=i(H);Uc=r(Xe,"Set "),ir=n(Xe,"CODE",{});var _f=i(ir);Gc=r(_f,"use_spot_instances=True"),_f.forEach(a),Wc=r(Xe," and define your "),lr=n(Xe,"CODE",{});var vf=i(lr);Bc=r(vf,"max_wait"),vf.forEach(a),Xc=r(Xe," and "),pr=n(Xe,"CODE",{});var yf=i(pr);Jc=r(yf,"max_run"),yf.forEach(a),Kc=r(Xe," time in the Estimator to use spot instances:"),Xe.forEach(a),Mo=m(e),f(Nt.$$.fragment,e),Do=m(e),Le=n(e,"P",{});var vn=i(Le);Vc=r(vn,"\u{1F4D3} Open the "),Ht=n(vn,"A",{href:!0,rel:!0});var Ef=i(Ht);Yc=r(Ef,"notebook"),Ef.forEach(a),Qc=r(vn," for an example of how to use spot instances."),vn.forEach(a),Co=m(e),ie=n(e,"H2",{class:!0});var yn=i(ie);ze=n(yn,"A",{id:!0,class:!0,href:!0});var bf=i(ze);cr=n(bf,"SPAN",{});var wf=i(cr);f(Ft.$$.fragment,wf),wf.forEach(a),bf.forEach(a),Zc=m(yn),hr=n(yn,"SPAN",{});var kf=i(hr);eh=r(kf,"Git repository"),kf.forEach(a),yn.forEach(a),Po=m(e),F=n(e,"P",{});var Je=i(F);th=r(Je,"The Hugging Face Estimator can load a training script "),Lt=n(Je,"A",{href:!0,rel:!0});var xf=i(Lt);ah=r(xf,"stored in a GitHub repository"),xf.forEach(a),sh=r(Je,". Provide the relative path to the training script in "),mr=n(Je,"CODE",{});var $f=i(mr);rh=r($f,"entry_point"),$f.forEach(a),oh=r(Je," and the relative path to the directory in "),ur=n(Je,"CODE",{});var Sf=i(ur);nh=r(Sf,"source_dir"),Sf.forEach(a),ih=r(Je,"."),Je.forEach(a),Oo=m(e),w=n(e,"P",{});var A=i(w);lh=r(A,"If you are using "),fr=n(A,"CODE",{});var jf=i(fr);ph=r(jf,"git_config"),jf.forEach(a),ch=r(A," to run the "),zt=n(A,"A",{href:!0,rel:!0});var Af=i(zt);hh=r(Af,"\u{1F917} Transformers example scripts"),Af.forEach(a),mh=r(A,", you need to configure the correct "),dr=n(A,"CODE",{});var Tf=i(dr);uh=r(Tf,"'branch'"),Tf.forEach(a),fh=r(A," in "),gr=n(A,"CODE",{});var Mf=i(gr);dh=r(Mf,"transformers_version"),Mf.forEach(a),gh=r(A," (e.g. if you use "),_r=n(A,"CODE",{});var Df=i(_r);_h=r(Df,"transformers_version='4.4.2"),Df.forEach(a),vh=r(A," you have to use "),vr=n(A,"CODE",{});var Cf=i(vr);yh=r(Cf,"'branch':'v4.4.2'"),Cf.forEach(a),Eh=r(A,")."),A.forEach(a),qo=m(e),$a=n(e,"P",{});var Pf=i($a);Rt=n(Pf,"EM",{});var En=i(Rt);bh=r(En,"Tip: Save your model to S3 by setting "),yr=n(En,"CODE",{});var Of=i(yr);wh=r(Of,"output_dir=/opt/ml/model"),Of.forEach(a),kh=r(En," in the hyperparameter of your training script."),En.forEach(a),Pf.forEach(a),Io=m(e),f(Ut.$$.fragment,e),No=m(e),le=n(e,"H2",{class:!0});var bn=i(le);Re=n(bn,"A",{id:!0,class:!0,href:!0});var qf=i(Re);Er=n(qf,"SPAN",{});var If=i(Er);f(Gt.$$.fragment,If),If.forEach(a),qf.forEach(a),xh=m(bn),br=n(bn,"SPAN",{});var Nf=i(br);$h=r(Nf,"SageMaker metrics"),Nf.forEach(a),bn.forEach(a),Ho=m(e),Wt=n(e,"P",{});var Rh=i(Wt);Bt=n(Rh,"A",{href:!0,rel:!0});var Hf=i(Bt);Sh=r(Hf,"SageMaker metrics"),Hf.forEach(a),jh=r(Rh," automatically parses training job logs for metrics and sends them to CloudWatch. If you want SageMaker to parse the logs, you must specify the metric\u2019s name and a regular expression for SageMaker to use to find the metric."),Rh.forEach(a),Fo=m(e),f(Xt.$$.fragment,e),Lo=m(e),Ue=n(e,"P",{});var wn=i(Ue);Ah=r(wn,"\u{1F4D3} Open the "),Jt=n(wn,"A",{href:!0,rel:!0});var Ff=i(Jt);Th=r(Ff,"notebook"),Ff.forEach(a),Mh=r(wn," for an example of how to capture metrics in SageMaker."),wn.forEach(a),this.h()},h(){c(X,"name","hf:doc:metadata"),c(X,"content",JSON.stringify(Jf)),c(pe,"id","run-training-on-amazon-sagemaker"),c(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pe,"href","#run-training-on-amazon-sagemaker"),c(J,"class","relative group"),c(T,"width","700"),c(T,"height","394"),Gf(T.src,Gh="https://www.youtube.com/embed/ok3hetb42gU")||c(T,"src",Gh),c(T,"title","YouTube video player"),c(T,"frameborder","0"),c(T,"allow","accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"),T.allowFullscreen=!0,c(Yt,"href","#installation-and-setup"),c(Zt,"href","#prepare-a-transformers-fine-tuning-script"),c(ta,"href","#create-a-hugging-face-estimator"),c(he,"href","#execute-training"),c(ra,"href","#access-trained-model"),c(na,"href","#distributed-training"),c(la,"href","#spot-instances"),c(ca,"href","#git-repository"),c(ma,"href","#sagemaker-metrics"),c(me,"id","installation-and-setup"),c(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(me,"href","#installation-and-setup"),c(K,"class","relative group"),c(Ye,"href","https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html"),c(Ye,"rel","nofollow"),c(Qe,"href","https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.html"),c(Qe,"rel","nofollow"),c(Ze,"href","https://docs.aws.amazon.com/sagemaker/latest/dg/gs-console.html"),c(Ze,"rel","nofollow"),c(et,"href","https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html"),c(et,"rel","nofollow"),c(ge,"id","prepare-a-transformers-finetuning-script"),c(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ge,"href","#prepare-a-transformers-finetuning-script"),c(Y,"class","relative group"),c(ot,"href","https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md"),c(ot,"rel","nofollow"),c(Ea,"href","#create-an-huggingface-estimator"),c(it,"href","https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/scripts/train.py"),c(it,"rel","nofollow"),c(ye,"id","training-output-management"),c(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ye,"href","#training-output-management"),c(Q,"class","relative group"),c(pt,"href","https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments"),c(pt,"rel","nofollow"),c(ct,"href","https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html"),c(ct,"rel","nofollow"),c(ht,"href","https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html"),c(ht,"rel","nofollow"),c(Ee,"id","create-a-hugging-face-estimator"),c(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ee,"href","#create-a-hugging-face-estimator"),c(Z,"class","relative group"),c(ut,"href","https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#huggingface-estimator"),c(ut,"rel","nofollow"),c(ft,"href","https://aws.amazon.com/sagemaker/pricing/"),c(ft,"rel","nofollow"),c(dt,"href","https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/scripts/train.py"),c(dt,"rel","nofollow"),c(xe,"id","execute-training"),c(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(xe,"href","#execute-training"),c(ee,"class","relative group"),c(yt,"href","https://sagemaker.readthedocs.io/en/stable/overview.html?highlight=FileSystemInput#use-file-systems-as-training-inputs"),c(yt,"rel","nofollow"),c(Te,"id","access-trained-model"),c(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Te,"href","#access-trained-model"),c(te,"class","relative group"),c(kt,"href","https://console.aws.amazon.com/console/home?nc2=h_ct&src=header-signin"),c(kt,"rel","nofollow"),c(De,"id","distributed-training"),c(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(De,"href","#distributed-training"),c(ae,"class","relative group"),c(Ce,"id","data-parallelism"),c(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ce,"href","#data-parallelism"),c(se,"class","relative group"),c(jt,"href","https://huggingface.co/transformers/main_classes/trainer.html"),c(jt,"rel","nofollow"),c(Tt,"href","https://github.com/huggingface/notebooks/blob/main/sagemaker/07_tensorflow_distributed_training_data_parallelism/sagemaker-notebook.ipynb"),c(Tt,"rel","nofollow"),c(qe,"id","model-parallelism"),c(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qe,"href","#model-parallelism"),c(re,"class","relative group"),c(Dt,"href","https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html?highlight=modelparallel#required-sagemaker-python-sdk-parameters"),c(Dt,"rel","nofollow"),c(Pt,"href","https://github.com/huggingface/notebooks/blob/main/sagemaker/04_distributed_training_model_parallelism/sagemaker-notebook.ipynb"),c(Pt,"rel","nofollow"),c(He,"id","spot-instances"),c(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(He,"href","#spot-instances"),c(oe,"class","relative group"),c(qt,"href","https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html"),c(qt,"rel","nofollow"),c(It,"href","https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html"),c(It,"rel","nofollow"),c(Ht,"href","https://github.com/huggingface/notebooks/blob/main/sagemaker/05_spot_instances/sagemaker-notebook.ipynb"),c(Ht,"rel","nofollow"),c(ze,"id","git-repository"),c(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ze,"href","#git-repository"),c(ie,"class","relative group"),c(Lt,"href","https://sagemaker.readthedocs.io/en/stable/overview.html#use-scripts-stored-in-a-git-repository"),c(Lt,"rel","nofollow"),c(zt,"href","https://github.com/huggingface/transformers/tree/main/examples"),c(zt,"rel","nofollow"),c(Re,"id","sagemaker-metrics"),c(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Re,"href","#sagemaker-metrics"),c(le,"class","relative group"),c(Bt,"href","https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html#define-train-metrics"),c(Bt,"rel","nofollow"),c(Jt,"href","https://github.com/huggingface/notebooks/blob/main/sagemaker/06_sagemaker_metrics/sagemaker-notebook.ipynb"),c(Jt,"rel","nofollow")},m(e,l){t(document.head,X),p(e,$r,l),p(e,J,l),t(J,pe),t(pe,Da),d(Ke,Da,null),t(J,kn),t(J,Ca),t(Ca,xn),p(e,Sr,l),p(e,T,l),p(e,jr,l),p(e,ce,l),t(ce,$n),t(ce,Pa),t(Pa,Sn),t(ce,jn),p(e,Ar,l),p(e,y,l),t(y,Vt),t(Vt,Yt),t(Yt,An),t(Vt,Tn),t(y,Mn),t(y,Qt),t(Qt,Zt),t(Zt,Dn),t(Qt,Cn),t(y,Pn),t(y,ea),t(ea,ta),t(ta,On),t(ea,qn),t(y,In),t(y,aa),t(aa,he),t(he,Nn),t(he,Oa),t(Oa,Hn),t(he,Fn),t(aa,Ln),t(y,zn),t(y,sa),t(sa,ra),t(ra,Rn),t(sa,Un),t(y,Gn),t(y,oa),t(oa,na),t(na,Wn),t(oa,Bn),t(y,Xn),t(y,ia),t(ia,la),t(la,Jn),t(ia,Kn),t(y,Vn),t(y,pa),t(pa,ca),t(ca,Yn),t(pa,Qn),t(y,Zn),t(y,ha),t(ha,ma),t(ma,ei),t(ha,ti),p(e,Tr,l),p(e,K,l),t(K,me),t(me,qa),d(Ve,qa,null),t(K,ai),t(K,Ia),t(Ia,si),p(e,Mr,l),p(e,ue,l),t(ue,ri),t(ue,Ye),t(Ye,oi),t(ue,ni),p(e,Dr,l),p(e,ua,l),t(ua,ii),p(e,Cr,l),p(e,L,l),t(L,Na),t(Na,Qe),t(Qe,li),t(L,pi),t(L,Ha),t(Ha,Ze),t(Ze,ci),t(L,hi),t(L,Fa),t(Fa,mi),p(e,Pr,l),p(e,fe,l),t(fe,ui),t(fe,et),t(et,fi),t(fe,di),p(e,Or,l),p(e,de,l),t(de,gi),t(de,La),t(La,_i),t(de,vi),p(e,qr,l),d(tt,e,l),p(e,Ir,l),p(e,fa,l),t(fa,za),t(za,yi),p(e,Nr,l),p(e,da,l),t(da,Ei),p(e,Hr,l),d(at,e,l),p(e,Fr,l),p(e,ga,l),t(ga,V),t(V,bi),t(V,Ra),t(Ra,wi),t(V,ki),t(V,Ua),t(Ua,xi),t(V,$i),p(e,Lr,l),p(e,_a,l),t(_a,Ga),t(Ga,Si),p(e,zr,l),p(e,va,l),t(va,ji),p(e,Rr,l),d(st,e,l),p(e,Ur,l),p(e,Y,l),t(Y,ge),t(ge,Wa),d(rt,Wa,null),t(Y,Ai),t(Y,Ba),t(Ba,Ti),p(e,Gr,l),p(e,_e,l),t(_e,Mi),t(_e,ot),t(ot,Di),t(_e,Ci),p(e,Wr,l),p(e,z,l),t(z,Xa),t(Xa,R),t(R,Ja),t(Ja,Pi),t(R,Oi),t(R,Ka),t(Ka,qi),t(R,Ii),t(R,Va),t(Va,Ni),t(R,Hi),t(z,Fi),t(z,Ya),t(Ya,ya),t(ya,Qa),t(Qa,Li),t(ya,zi),t(z,Ri),t(z,Za),t(Za,$),t($,es),t(es,Ui),t($,Gi),t($,ts),t(ts,Wi),t($,Bi),t($,as),t(as,Xi),t($,Ji),t($,ss),t(ss,Ki),t($,Vi),t($,rs),t(rs,Yi),t($,Qi),t($,os),t(os,Zi),t($,el),p(e,Br,l),p(e,q,l),t(q,tl),t(q,ns),t(ns,al),t(q,sl),t(q,Ea),t(Ea,rl),t(q,ol),t(q,is),t(is,nl),t(q,il),p(e,Xr,l),d(nt,e,l),p(e,Jr,l),p(e,ba,l),t(ba,M),t(M,ll),t(M,ls),t(ls,pl),t(M,cl),t(M,ps),t(ps,hl),t(M,ml),t(M,cs),t(cs,ul),t(M,fl),t(M,hs),t(hs,dl),t(M,gl),p(e,Kr,l),p(e,ve,l),t(ve,_l),t(ve,it),t(it,vl),t(ve,yl),p(e,Vr,l),p(e,Q,l),t(Q,ye),t(ye,ms),d(lt,ms,null),t(Q,El),t(Q,us),t(us,bl),p(e,Yr,l),p(e,E,l),t(E,wl),t(E,fs),t(fs,kl),t(E,xl),t(E,ds),t(ds,$l),t(E,Sl),t(E,gs),t(gs,jl),t(E,Al),t(E,_s),t(_s,Tl),t(E,Ml),t(E,pt),t(pt,Dl),t(E,Cl),t(E,vs),t(vs,Pl),t(E,Ol),t(E,ys),t(ys,ql),t(E,Il),t(E,Es),t(Es,Nl),t(E,Hl),p(e,Qr,l),p(e,b,l),t(b,Fl),t(b,ct),t(ct,Ll),t(b,zl),t(b,ht),t(ht,Rl),t(b,Ul),t(b,bs),t(bs,Gl),t(b,Wl),t(b,ws),t(ws,Bl),t(b,Xl),t(b,ks),t(ks,Jl),t(b,Kl),t(b,xs),t(xs,Vl),t(b,Yl),t(b,D),t(D,Ql),t(D,$s),t($s,Zl),t(D,ep),t(D,Ss),t(Ss,tp),t(D,ap),t(D,js),t(js,sp),t(D,rp),t(D,As),t(As,op),t(D,np),p(e,Zr,l),p(e,Z,l),t(Z,Ee),t(Ee,Ts),d(mt,Ts,null),t(Z,ip),t(Z,Ms),t(Ms,lp),p(e,eo,l),p(e,be,l),t(be,pp),t(be,ut),t(ut,cp),t(be,hp),p(e,to,l),p(e,U,l),t(U,wa),t(wa,Ds),t(Ds,mp),t(wa,up),t(U,fp),t(U,we),t(we,Cs),t(Cs,dp),t(we,gp),t(we,ft),t(ft,_p),t(we,vp),t(U,yp),t(U,ke),t(ke,Ps),t(Ps,Ep),t(ke,bp),t(ke,dt),t(dt,wp),t(ke,kp),p(e,ao,l),p(e,j,l),t(j,xp),t(j,Os),t(Os,$p),t(j,Sp),t(j,qs),t(qs,jp),t(j,Ap),t(j,Is),t(Is,Tp),t(j,Mp),t(j,Ns),t(Ns,Dp),t(j,Cp),p(e,so,l),d(gt,e,l),p(e,ro,l),p(e,I,l),t(I,Pp),t(I,Hs),t(Hs,Op),t(I,qp),t(I,Fs),t(Fs,Ip),t(I,Np),t(I,Ls),t(Ls,Hp),t(I,Fp),p(e,oo,l),p(e,ee,l),t(ee,xe),t(xe,zs),d(_t,zs,null),t(ee,Lp),t(ee,Rs),t(Rs,zp),p(e,no,l),p(e,N,l),t(N,Rp),t(N,Us),t(Us,Up),t(N,Gp),t(N,Gs),t(Gs,Wp),t(N,Bp),t(N,Ws),t(Ws,Xp),t(N,Jp),p(e,io,l),p(e,$e,l),t($e,vt),t(vt,Kp),t(vt,Bs),t(Bs,Vp),t(vt,Yp),t($e,Qp),t($e,Se),t(Se,Xs),t(Xs,Zp),t(Se,ec),t(Se,yt),t(yt,tc),t(Se,ac),p(e,lo,l),p(e,je,l),t(je,sc),t(je,Js),t(Js,rc),t(je,oc),p(e,po,l),d(Et,e,l),p(e,co,l),p(e,Ae,l),t(Ae,nc),t(Ae,Ks),t(Ks,ic),t(Ae,lc),p(e,ho,l),d(bt,e,l),p(e,mo,l),p(e,te,l),t(te,Te),t(Te,Vs),d(wt,Vs,null),t(te,pc),t(te,Ys),t(Ys,cc),p(e,uo,l),p(e,Me,l),t(Me,hc),t(Me,kt),t(kt,mc),t(Me,uc),p(e,fo,l),d(xt,e,l),p(e,go,l),p(e,ae,l),t(ae,De),t(De,Qs),d($t,Qs,null),t(ae,fc),t(ae,Zs),t(Zs,dc),p(e,_o,l),p(e,ka,l),t(ka,gc),p(e,vo,l),p(e,se,l),t(se,Ce),t(Ce,er),d(St,er,null),t(se,_c),t(se,tr),t(tr,vc),p(e,yo,l),p(e,Pe,l),t(Pe,yc),t(Pe,jt),t(jt,Ec),t(Pe,bc),p(e,Eo,l),d(At,e,l),p(e,bo,l),p(e,Oe,l),t(Oe,wc),t(Oe,Tt),t(Tt,kc),t(Oe,xc),p(e,wo,l),p(e,re,l),t(re,qe),t(qe,ar),d(Mt,ar,null),t(re,$c),t(re,sr),t(sr,Sc),p(e,ko,l),p(e,Ie,l),t(Ie,jc),t(Ie,Dt),t(Dt,Ac),t(Ie,Tc),p(e,xo,l),d(Ct,e,l),p(e,$o,l),p(e,Ne,l),t(Ne,Mc),t(Ne,Pt),t(Pt,Dc),t(Ne,Cc),p(e,So,l),p(e,oe,l),t(oe,He),t(He,rr),d(Ot,rr,null),t(oe,Pc),t(oe,or),t(or,Oc),p(e,jo,l),p(e,Fe,l),t(Fe,qc),t(Fe,qt),t(qt,Ic),t(Fe,Nc),p(e,Ao,l),p(e,xa,l),t(xa,ne),t(ne,Hc),t(ne,It),t(It,Fc),t(ne,Lc),t(ne,nr),t(nr,zc),t(ne,Rc),p(e,To,l),p(e,H,l),t(H,Uc),t(H,ir),t(ir,Gc),t(H,Wc),t(H,lr),t(lr,Bc),t(H,Xc),t(H,pr),t(pr,Jc),t(H,Kc),p(e,Mo,l),d(Nt,e,l),p(e,Do,l),p(e,Le,l),t(Le,Vc),t(Le,Ht),t(Ht,Yc),t(Le,Qc),p(e,Co,l),p(e,ie,l),t(ie,ze),t(ze,cr),d(Ft,cr,null),t(ie,Zc),t(ie,hr),t(hr,eh),p(e,Po,l),p(e,F,l),t(F,th),t(F,Lt),t(Lt,ah),t(F,sh),t(F,mr),t(mr,rh),t(F,oh),t(F,ur),t(ur,nh),t(F,ih),p(e,Oo,l),p(e,w,l),t(w,lh),t(w,fr),t(fr,ph),t(w,ch),t(w,zt),t(zt,hh),t(w,mh),t(w,dr),t(dr,uh),t(w,fh),t(w,gr),t(gr,dh),t(w,gh),t(w,_r),t(_r,_h),t(w,vh),t(w,vr),t(vr,yh),t(w,Eh),p(e,qo,l),p(e,$a,l),t($a,Rt),t(Rt,bh),t(Rt,yr),t(yr,wh),t(Rt,kh),p(e,Io,l),d(Ut,e,l),p(e,No,l),p(e,le,l),t(le,Re),t(Re,Er),d(Gt,Er,null),t(le,xh),t(le,br),t(br,$h),p(e,Ho,l),p(e,Wt,l),t(Wt,Bt),t(Bt,Sh),t(Wt,jh),p(e,Fo,l),d(Xt,e,l),p(e,Lo,l),p(e,Ue,l),t(Ue,Ah),t(Ue,Jt),t(Jt,Th),t(Ue,Mh),zo=!0},p:Wf,i(e){zo||(g(Ke.$$.fragment,e),g(Ve.$$.fragment,e),g(tt.$$.fragment,e),g(at.$$.fragment,e),g(st.$$.fragment,e),g(rt.$$.fragment,e),g(nt.$$.fragment,e),g(lt.$$.fragment,e),g(mt.$$.fragment,e),g(gt.$$.fragment,e),g(_t.$$.fragment,e),g(Et.$$.fragment,e),g(bt.$$.fragment,e),g(wt.$$.fragment,e),g(xt.$$.fragment,e),g($t.$$.fragment,e),g(St.$$.fragment,e),g(At.$$.fragment,e),g(Mt.$$.fragment,e),g(Ct.$$.fragment,e),g(Ot.$$.fragment,e),g(Nt.$$.fragment,e),g(Ft.$$.fragment,e),g(Ut.$$.fragment,e),g(Gt.$$.fragment,e),g(Xt.$$.fragment,e),zo=!0)},o(e){_(Ke.$$.fragment,e),_(Ve.$$.fragment,e),_(tt.$$.fragment,e),_(at.$$.fragment,e),_(st.$$.fragment,e),_(rt.$$.fragment,e),_(nt.$$.fragment,e),_(lt.$$.fragment,e),_(mt.$$.fragment,e),_(gt.$$.fragment,e),_(_t.$$.fragment,e),_(Et.$$.fragment,e),_(bt.$$.fragment,e),_(wt.$$.fragment,e),_(xt.$$.fragment,e),_($t.$$.fragment,e),_(St.$$.fragment,e),_(At.$$.fragment,e),_(Mt.$$.fragment,e),_(Ct.$$.fragment,e),_(Ot.$$.fragment,e),_(Nt.$$.fragment,e),_(Ft.$$.fragment,e),_(Ut.$$.fragment,e),_(Gt.$$.fragment,e),_(Xt.$$.fragment,e),zo=!1},d(e){a(X),e&&a($r),e&&a(J),v(Ke),e&&a(Sr),e&&a(T),e&&a(jr),e&&a(ce),e&&a(Ar),e&&a(y),e&&a(Tr),e&&a(K),v(Ve),e&&a(Mr),e&&a(ue),e&&a(Dr),e&&a(ua),e&&a(Cr),e&&a(L),e&&a(Pr),e&&a(fe),e&&a(Or),e&&a(de),e&&a(qr),v(tt,e),e&&a(Ir),e&&a(fa),e&&a(Nr),e&&a(da),e&&a(Hr),v(at,e),e&&a(Fr),e&&a(ga),e&&a(Lr),e&&a(_a),e&&a(zr),e&&a(va),e&&a(Rr),v(st,e),e&&a(Ur),e&&a(Y),v(rt),e&&a(Gr),e&&a(_e),e&&a(Wr),e&&a(z),e&&a(Br),e&&a(q),e&&a(Xr),v(nt,e),e&&a(Jr),e&&a(ba),e&&a(Kr),e&&a(ve),e&&a(Vr),e&&a(Q),v(lt),e&&a(Yr),e&&a(E),e&&a(Qr),e&&a(b),e&&a(Zr),e&&a(Z),v(mt),e&&a(eo),e&&a(be),e&&a(to),e&&a(U),e&&a(ao),e&&a(j),e&&a(so),v(gt,e),e&&a(ro),e&&a(I),e&&a(oo),e&&a(ee),v(_t),e&&a(no),e&&a(N),e&&a(io),e&&a($e),e&&a(lo),e&&a(je),e&&a(po),v(Et,e),e&&a(co),e&&a(Ae),e&&a(ho),v(bt,e),e&&a(mo),e&&a(te),v(wt),e&&a(uo),e&&a(Me),e&&a(fo),v(xt,e),e&&a(go),e&&a(ae),v($t),e&&a(_o),e&&a(ka),e&&a(vo),e&&a(se),v(St),e&&a(yo),e&&a(Pe),e&&a(Eo),v(At,e),e&&a(bo),e&&a(Oe),e&&a(wo),e&&a(re),v(Mt),e&&a(ko),e&&a(Ie),e&&a(xo),v(Ct,e),e&&a($o),e&&a(Ne),e&&a(So),e&&a(oe),v(Ot),e&&a(jo),e&&a(Fe),e&&a(Ao),e&&a(xa),e&&a(To),e&&a(H),e&&a(Mo),v(Nt,e),e&&a(Do),e&&a(Le),e&&a(Co),e&&a(ie),v(Ft),e&&a(Po),e&&a(F),e&&a(Oo),e&&a(w),e&&a(qo),e&&a($a),e&&a(Io),v(Ut,e),e&&a(No),e&&a(le),v(Gt),e&&a(Ho),e&&a(Wt),e&&a(Fo),v(Xt,e),e&&a(Lo),e&&a(Ue)}}}const Jf={local:"run-training-on-amazon-sagemaker",sections:[{local:"installation-and-setup",title:"Installation and setup"},{local:"prepare-a-transformers-finetuning-script",title:"Prepare a \u{1F917} Transformers fine-tuning script"},{local:"training-output-management",title:"Training Output Management"},{local:"create-a-hugging-face-estimator",title:"Create a Hugging Face Estimator"},{local:"execute-training",title:"Execute training"},{local:"access-trained-model",title:"Access trained model"},{local:"distributed-training",sections:[{local:"data-parallelism",title:"Data parallelism"},{local:"model-parallelism",title:"Model parallelism"}],title:"Distributed training"},{local:"spot-instances",title:"Spot instances"},{local:"git-repository",title:"Git repository"},{local:"sagemaker-metrics",title:"SageMaker metrics"}],title:"Run training on Amazon SageMaker"};function Kf(Uh){return Bf(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Zf extends Lf{constructor(X){super();zf(this,X,Kf,Xf,Rf,{})}}export{Zf as default,Jf as metadata};
