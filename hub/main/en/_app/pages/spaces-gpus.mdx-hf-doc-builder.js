import{S as Gs,i as Cs,s as qs,e as o,k as u,w as v,t as n,N as Is,c as r,d as t,m as f,a as l,x as g,h as c,b as p,P as pt,G as s,g as i,y,q as w,o as _,B as $,v as Js}from"../chunks/vendor-hf-doc-builder.js";import{T as Ns}from"../chunks/Tip-hf-doc-builder.js";import{I as Ue}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as ie}from"../chunks/CodeBlock-hf-doc-builder.js";function Xs(Se){let h;return{c(){h=n("Longer-term, we would also like to expose non-GPU hardware, like HPU, IPU or TPU. If you have a specific AI hardware you'd like to run on, please let us know (website at huggingface.co).")},l(b){h=c(b,"Longer-term, we would also like to expose non-GPU hardware, like HPU, IPU or TPU. If you have a specific AI hardware you'd like to run on, please let us know (website at huggingface.co).")},m(b,P){i(b,h,P)},d(b){b&&t(h)}}}function Ms(Se){let h,b,P,T,ne,Y,ut,ce,ft,Te,D,ht,pe,dt,mt,De,x,ee,es,vt,te,ts,Ge,G,Ce,se,gt,qe,E,ae,ss,yt,oe,as,Ie,A,C,ue,H,wt,fe,_t,Je,re,$t,Ne,j,q,he,L,Pt,de,bt,Xe,I,kt,me,xt,Et,Me,F,Oe,J,At,ve,jt,Ut,Ye,z,He,m,St,ge,Tt,Dt,ye,Gt,Ct,we,qt,It,Le,B,Fe,U,N,_e,V,Jt,$e,Nt,ze,X,Xt,Pe,Mt,Ot,Be,R,Ve,le,Yt,Re,K,Ke,S,M,be,Q,Ht,ke,Lt,Qe,d,Ft,xe,zt,Bt,Ee,Vt,Rt,Ae,Kt,Qt,je,Wt,Zt,We,W,Ze;return Y=new Ue({}),G=new Ns({props:{$$slots:{default:[Xs]},$$scope:{ctx:Se}}}),H=new Ue({}),L=new Ue({}),F=new ie({props:{code:`--extra-index-url https://download.pytorch.org/whl/cu113
torch`,highlighted:`--extra-index-url https:<span class="hljs-regexp">//</span>download.pytorch.org<span class="hljs-regexp">/whl/</span>cu113
torch`}}),z=new ie({props:{code:`import torch
print(f"Is CUDA available: {torch.cuda.is_available()}")
# True
print(f"CUDA device: {torch.cuda.get_device_name(torch.cuda.current_device())}")
# Tesla T4`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Is CUDA available: <span class="hljs-subst">{torch.cuda.is_available()}</span>&quot;</span>)
<span class="hljs-comment"># True</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;CUDA device: <span class="hljs-subst">{torch.cuda.get_device_name(torch.cuda.current_device())}</span>&quot;</span>)
<span class="hljs-comment"># Tesla T4</span>`}}),B=new ie({props:{code:`model = load_pytorch_model()
model = model.to("cuda")`,highlighted:`model = load_pytorch_model()
model = model.to(<span class="hljs-string">&quot;cuda&quot;</span>)`}}),V=new Ue({}),R=new ie({props:{code:`-f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
jax[cuda11_cudnn805]`,highlighted:`-f https:<span class="hljs-regexp">//</span>storage.googleapis.com<span class="hljs-regexp">/jax-releases/</span>jax_cuda_releases.html
jax[cuda11_cudnn805]`}}),K=new ie({props:{code:`import jax

print(f"JAX devices: {jax.devices()}")
# JAX devices: [StreamExecutorGpuDevice(id=0, process_index=0)]
print(f"JAX device type: {jax.devices()[0].device_kind}")
# JAX device type: Tesla T4`,highlighted:`<span class="hljs-keyword">import</span> jax

<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;JAX devices: <span class="hljs-subst">{jax.devices()}</span>&quot;</span>)
<span class="hljs-comment"># JAX devices: [StreamExecutorGpuDevice(id=0, process_index=0)]</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;JAX device type: <span class="hljs-subst">{jax.devices()[<span class="hljs-number">0</span>].device_kind}</span>&quot;</span>)
<span class="hljs-comment"># JAX device type: Tesla T4</span>`}}),Q=new Ue({}),W=new ie({props:{code:`import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))
# [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-built_in">print</span>(tf.config.list_physical_devices(<span class="hljs-string">&#x27;GPU&#x27;</span>))
<span class="hljs-comment"># [PhysicalDevice(name=&#x27;/physical_device:GPU:0&#x27;, device_type=&#x27;GPU&#x27;)]</span>`}}),{c(){h=o("meta"),b=u(),P=o("h1"),T=o("a"),ne=o("span"),v(Y.$$.fragment),ut=u(),ce=o("span"),ft=n("Using GPU Spaces"),Te=u(),D=o("p"),ht=n("You can upgrade your Space to use a GPU accelerator using the "),pe=o("em"),dt=n("Settings"),mt=n(" button in the top navigation bar of the Space. You can even request a free upgrade if you are building a cool demo for a side project!"),De=u(),x=o("div"),ee=o("img"),vt=u(),te=o("img"),Ge=u(),v(G.$$.fragment),Ce=u(),se=o("p"),gt=n("As soon as your Space is running on GPU you can see which hardware it\u2019s running on directly from this badge:"),qe=u(),E=o("div"),ae=o("img"),yt=u(),oe=o("img"),Ie=u(),A=o("h2"),C=o("a"),ue=o("span"),v(H.$$.fragment),wt=u(),fe=o("span"),_t=n("Framework specific requirements"),Je=u(),re=o("p"),$t=n("Most Spaces should run out of the box after a GPU upgrade, but sometimes you\u2019ll need to install CUDA versions of the machine learning frameworks you use. Please, follow this guide to ensure your Space takes advantage of the improved hardware."),Ne=u(),j=o("h3"),q=o("a"),he=o("span"),v(L.$$.fragment),Pt=u(),de=o("span"),bt=n("PyTorch"),Xe=u(),I=o("p"),kt=n("You\u2019ll need to install a version of PyTorch compatible with the built-in CUDA drivers. Adding the following two lines to your "),me=o("code"),xt=n("requirements.txt"),Et=n(" file should work:"),Me=u(),v(F.$$.fragment),Oe=u(),J=o("p"),At=n("You can verify whether the installation was successful by running the following code in your "),ve=o("code"),jt=n("app.py"),Ut=n(" and checking the output in your Space logs:"),Ye=u(),v(z.$$.fragment),He=u(),m=o("p"),St=n("Many frameworks automatically use the GPU if one is available. This is the case for the Pipelines in \u{1F917} "),ge=o("code"),Tt=n("transformers"),Dt=n(", "),ye=o("code"),Gt=n("fastai"),Ct=n(" and many others. In other cases, or if you use PyTorch directly, you may need to move your models and data to the GPU to ensure computation is done on the accelerator and not on the CPU. You can use PyTorch\u2019s "),we=o("code"),qt=n(".to()"),It=n(" syntax, for example:"),Le=u(),v(B.$$.fragment),Fe=u(),U=o("h3"),N=o("a"),_e=o("span"),v(V.$$.fragment),Jt=u(),$e=o("span"),Nt=n("JAX"),ze=u(),X=o("p"),Xt=n("If you use JAX, you need to specify the binary that is compatible with the CUDA and cuDNN versions installed in your GPU Space. Please, add the following two lines to your "),Pe=o("code"),Mt=n("requirements.txt"),Ot=n(" file:"),Be=u(),v(R.$$.fragment),Ve=u(),le=o("p"),Yt=n("After that, you can verify the installation by printing the output from the following code and checking it in your Space logs."),Re=u(),v(K.$$.fragment),Ke=u(),S=o("h3"),M=o("a"),be=o("span"),v(Q.$$.fragment),Ht=u(),ke=o("span"),Lt=n("Tensorflow"),Qe=u(),d=o("p"),Ft=n("The default "),xe=o("code"),zt=n("tensorflow"),Bt=n(" installation should recognize the CUDA device. Just add "),Ee=o("code"),Vt=n("tensorflow"),Rt=n(" to your "),Ae=o("code"),Kt=n("requirements.txt"),Qt=n(" file and use the following code in your "),je=o("code"),Wt=n("app.py"),Zt=n(" to verify in your Space logs."),We=u(),v(W.$$.fragment),this.h()},l(e){const a=Is('[data-svelte="svelte-1phssyn"]',document.head);h=r(a,"META",{name:!0,content:!0}),a.forEach(t),b=f(e),P=r(e,"H1",{class:!0});var Z=l(P);T=r(Z,"A",{id:!0,class:!0,href:!0});var os=l(T);ne=r(os,"SPAN",{});var rs=l(ne);g(Y.$$.fragment,rs),rs.forEach(t),os.forEach(t),ut=f(Z),ce=r(Z,"SPAN",{});var ls=l(ce);ft=c(ls,"Using GPU Spaces"),ls.forEach(t),Z.forEach(t),Te=f(e),D=r(e,"P",{});var et=l(D);ht=c(et,"You can upgrade your Space to use a GPU accelerator using the "),pe=r(et,"EM",{});var is=l(pe);dt=c(is,"Settings"),is.forEach(t),mt=c(et," button in the top navigation bar of the Space. You can even request a free upgrade if you are building a cool demo for a side project!"),et.forEach(t),De=f(e),x=r(e,"DIV",{class:!0});var tt=l(x);ee=r(tt,"IMG",{class:!0,src:!0}),vt=f(tt),te=r(tt,"IMG",{class:!0,src:!0}),tt.forEach(t),Ge=f(e),g(G.$$.fragment,e),Ce=f(e),se=r(e,"P",{});var ns=l(se);gt=c(ns,"As soon as your Space is running on GPU you can see which hardware it\u2019s running on directly from this badge:"),ns.forEach(t),qe=f(e),E=r(e,"DIV",{class:!0});var st=l(E);ae=r(st,"IMG",{class:!0,src:!0}),yt=f(st),oe=r(st,"IMG",{class:!0,src:!0}),st.forEach(t),Ie=f(e),A=r(e,"H2",{class:!0});var at=l(A);C=r(at,"A",{id:!0,class:!0,href:!0});var cs=l(C);ue=r(cs,"SPAN",{});var ps=l(ue);g(H.$$.fragment,ps),ps.forEach(t),cs.forEach(t),wt=f(at),fe=r(at,"SPAN",{});var us=l(fe);_t=c(us,"Framework specific requirements"),us.forEach(t),at.forEach(t),Je=f(e),re=r(e,"P",{});var fs=l(re);$t=c(fs,"Most Spaces should run out of the box after a GPU upgrade, but sometimes you\u2019ll need to install CUDA versions of the machine learning frameworks you use. Please, follow this guide to ensure your Space takes advantage of the improved hardware."),fs.forEach(t),Ne=f(e),j=r(e,"H3",{class:!0});var ot=l(j);q=r(ot,"A",{id:!0,class:!0,href:!0});var hs=l(q);he=r(hs,"SPAN",{});var ds=l(he);g(L.$$.fragment,ds),ds.forEach(t),hs.forEach(t),Pt=f(ot),de=r(ot,"SPAN",{});var ms=l(de);bt=c(ms,"PyTorch"),ms.forEach(t),ot.forEach(t),Xe=f(e),I=r(e,"P",{});var rt=l(I);kt=c(rt,"You\u2019ll need to install a version of PyTorch compatible with the built-in CUDA drivers. Adding the following two lines to your "),me=r(rt,"CODE",{});var vs=l(me);xt=c(vs,"requirements.txt"),vs.forEach(t),Et=c(rt," file should work:"),rt.forEach(t),Me=f(e),g(F.$$.fragment,e),Oe=f(e),J=r(e,"P",{});var lt=l(J);At=c(lt,"You can verify whether the installation was successful by running the following code in your "),ve=r(lt,"CODE",{});var gs=l(ve);jt=c(gs,"app.py"),gs.forEach(t),Ut=c(lt," and checking the output in your Space logs:"),lt.forEach(t),Ye=f(e),g(z.$$.fragment,e),He=f(e),m=r(e,"P",{});var O=l(m);St=c(O,"Many frameworks automatically use the GPU if one is available. This is the case for the Pipelines in \u{1F917} "),ge=r(O,"CODE",{});var ys=l(ge);Tt=c(ys,"transformers"),ys.forEach(t),Dt=c(O,", "),ye=r(O,"CODE",{});var ws=l(ye);Gt=c(ws,"fastai"),ws.forEach(t),Ct=c(O," and many others. In other cases, or if you use PyTorch directly, you may need to move your models and data to the GPU to ensure computation is done on the accelerator and not on the CPU. You can use PyTorch\u2019s "),we=r(O,"CODE",{});var _s=l(we);qt=c(_s,".to()"),_s.forEach(t),It=c(O," syntax, for example:"),O.forEach(t),Le=f(e),g(B.$$.fragment,e),Fe=f(e),U=r(e,"H3",{class:!0});var it=l(U);N=r(it,"A",{id:!0,class:!0,href:!0});var $s=l(N);_e=r($s,"SPAN",{});var Ps=l(_e);g(V.$$.fragment,Ps),Ps.forEach(t),$s.forEach(t),Jt=f(it),$e=r(it,"SPAN",{});var bs=l($e);Nt=c(bs,"JAX"),bs.forEach(t),it.forEach(t),ze=f(e),X=r(e,"P",{});var nt=l(X);Xt=c(nt,"If you use JAX, you need to specify the binary that is compatible with the CUDA and cuDNN versions installed in your GPU Space. Please, add the following two lines to your "),Pe=r(nt,"CODE",{});var ks=l(Pe);Mt=c(ks,"requirements.txt"),ks.forEach(t),Ot=c(nt," file:"),nt.forEach(t),Be=f(e),g(R.$$.fragment,e),Ve=f(e),le=r(e,"P",{});var xs=l(le);Yt=c(xs,"After that, you can verify the installation by printing the output from the following code and checking it in your Space logs."),xs.forEach(t),Re=f(e),g(K.$$.fragment,e),Ke=f(e),S=r(e,"H3",{class:!0});var ct=l(S);M=r(ct,"A",{id:!0,class:!0,href:!0});var Es=l(M);be=r(Es,"SPAN",{});var As=l(be);g(Q.$$.fragment,As),As.forEach(t),Es.forEach(t),Ht=f(ct),ke=r(ct,"SPAN",{});var js=l(ke);Lt=c(js,"Tensorflow"),js.forEach(t),ct.forEach(t),Qe=f(e),d=r(e,"P",{});var k=l(d);Ft=c(k,"The default "),xe=r(k,"CODE",{});var Us=l(xe);zt=c(Us,"tensorflow"),Us.forEach(t),Bt=c(k," installation should recognize the CUDA device. Just add "),Ee=r(k,"CODE",{});var Ss=l(Ee);Vt=c(Ss,"tensorflow"),Ss.forEach(t),Rt=c(k," to your "),Ae=r(k,"CODE",{});var Ts=l(Ae);Kt=c(Ts,"requirements.txt"),Ts.forEach(t),Qt=c(k," file and use the following code in your "),je=r(k,"CODE",{});var Ds=l(je);Wt=c(Ds,"app.py"),Ds.forEach(t),Zt=c(k," to verify in your Space logs."),k.forEach(t),We=f(e),g(W.$$.fragment,e),this.h()},h(){p(h,"name","hf:doc:metadata"),p(h,"content",JSON.stringify(Os)),p(T,"id","using-gpu-spaces"),p(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(T,"href","#using-gpu-spaces"),p(P,"class","relative group"),p(ee,"class","block dark:hidden"),pt(ee.src,es="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/spaces-gpu-settings.png")||p(ee,"src",es),p(te,"class","hidden dark:block"),pt(te.src,ts="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/spaces-gpu-settings-dark.png")||p(te,"src",ts),p(x,"class","flex justify-center"),p(ae,"class","block dark:hidden"),pt(ae.src,ss="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/spaces-running-badge.png")||p(ae,"src",ss),p(oe,"class","hidden dark:block"),pt(oe.src,as="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/spaces-running-badge-dark.png")||p(oe,"src",as),p(E,"class","flex justify-center"),p(C,"id","frameworks"),p(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(C,"href","#frameworks"),p(A,"class","relative group"),p(q,"id","pytorch"),p(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(q,"href","#pytorch"),p(j,"class","relative group"),p(N,"id","jax"),p(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(N,"href","#jax"),p(U,"class","relative group"),p(M,"id","tensorflow"),p(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(M,"href","#tensorflow"),p(S,"class","relative group")},m(e,a){s(document.head,h),i(e,b,a),i(e,P,a),s(P,T),s(T,ne),y(Y,ne,null),s(P,ut),s(P,ce),s(ce,ft),i(e,Te,a),i(e,D,a),s(D,ht),s(D,pe),s(pe,dt),s(D,mt),i(e,De,a),i(e,x,a),s(x,ee),s(x,vt),s(x,te),i(e,Ge,a),y(G,e,a),i(e,Ce,a),i(e,se,a),s(se,gt),i(e,qe,a),i(e,E,a),s(E,ae),s(E,yt),s(E,oe),i(e,Ie,a),i(e,A,a),s(A,C),s(C,ue),y(H,ue,null),s(A,wt),s(A,fe),s(fe,_t),i(e,Je,a),i(e,re,a),s(re,$t),i(e,Ne,a),i(e,j,a),s(j,q),s(q,he),y(L,he,null),s(j,Pt),s(j,de),s(de,bt),i(e,Xe,a),i(e,I,a),s(I,kt),s(I,me),s(me,xt),s(I,Et),i(e,Me,a),y(F,e,a),i(e,Oe,a),i(e,J,a),s(J,At),s(J,ve),s(ve,jt),s(J,Ut),i(e,Ye,a),y(z,e,a),i(e,He,a),i(e,m,a),s(m,St),s(m,ge),s(ge,Tt),s(m,Dt),s(m,ye),s(ye,Gt),s(m,Ct),s(m,we),s(we,qt),s(m,It),i(e,Le,a),y(B,e,a),i(e,Fe,a),i(e,U,a),s(U,N),s(N,_e),y(V,_e,null),s(U,Jt),s(U,$e),s($e,Nt),i(e,ze,a),i(e,X,a),s(X,Xt),s(X,Pe),s(Pe,Mt),s(X,Ot),i(e,Be,a),y(R,e,a),i(e,Ve,a),i(e,le,a),s(le,Yt),i(e,Re,a),y(K,e,a),i(e,Ke,a),i(e,S,a),s(S,M),s(M,be),y(Q,be,null),s(S,Ht),s(S,ke),s(ke,Lt),i(e,Qe,a),i(e,d,a),s(d,Ft),s(d,xe),s(xe,zt),s(d,Bt),s(d,Ee),s(Ee,Vt),s(d,Rt),s(d,Ae),s(Ae,Kt),s(d,Qt),s(d,je),s(je,Wt),s(d,Zt),i(e,We,a),y(W,e,a),Ze=!0},p(e,[a]){const Z={};a&2&&(Z.$$scope={dirty:a,ctx:e}),G.$set(Z)},i(e){Ze||(w(Y.$$.fragment,e),w(G.$$.fragment,e),w(H.$$.fragment,e),w(L.$$.fragment,e),w(F.$$.fragment,e),w(z.$$.fragment,e),w(B.$$.fragment,e),w(V.$$.fragment,e),w(R.$$.fragment,e),w(K.$$.fragment,e),w(Q.$$.fragment,e),w(W.$$.fragment,e),Ze=!0)},o(e){_(Y.$$.fragment,e),_(G.$$.fragment,e),_(H.$$.fragment,e),_(L.$$.fragment,e),_(F.$$.fragment,e),_(z.$$.fragment,e),_(B.$$.fragment,e),_(V.$$.fragment,e),_(R.$$.fragment,e),_(K.$$.fragment,e),_(Q.$$.fragment,e),_(W.$$.fragment,e),Ze=!1},d(e){t(h),e&&t(b),e&&t(P),$(Y),e&&t(Te),e&&t(D),e&&t(De),e&&t(x),e&&t(Ge),$(G,e),e&&t(Ce),e&&t(se),e&&t(qe),e&&t(E),e&&t(Ie),e&&t(A),$(H),e&&t(Je),e&&t(re),e&&t(Ne),e&&t(j),$(L),e&&t(Xe),e&&t(I),e&&t(Me),$(F,e),e&&t(Oe),e&&t(J),e&&t(Ye),$(z,e),e&&t(He),e&&t(m),e&&t(Le),$(B,e),e&&t(Fe),e&&t(U),$(V),e&&t(ze),e&&t(X),e&&t(Be),$(R,e),e&&t(Ve),e&&t(le),e&&t(Re),$(K,e),e&&t(Ke),e&&t(S),$(Q),e&&t(Qe),e&&t(d),e&&t(We),$(W,e)}}}const Os={local:"using-gpu-spaces",sections:[{local:"frameworks",sections:[{local:"pytorch",title:"PyTorch"},{local:"jax",title:"JAX"},{local:"tensorflow",title:"Tensorflow"}],title:"Framework specific requirements"}],title:"Using GPU Spaces"};function Ys(Se){return Js(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Bs extends Gs{constructor(h){super();Cs(this,h,Ys,Ms,qs,{})}}export{Bs as default,Os as metadata};
