import{S as oc,i as lc,s as dc,e as n,k as p,w as $,t as m,M as ic,c as o,d as a,m as c,a as l,x as b,h as g,b as j,G as e,g as D,y as v,q as x,o as w,B as E,v as pc,L as B}from"../../chunks/vendor-hf-doc-builder.js";import{D as k}from"../../chunks/Docstring-hf-doc-builder.js";import{C as N}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as cc}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as S}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function mc(y){let d,_,f,r,u;return r=new N({props:{code:`from datasets import load_dataset_builder
builder = load_dataset_builder('rotten_tomatoes')
ds = builder.download_and_prepare()
ds = builder.as_dataset(split='train')
ds`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.download_and_prepare()
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.as_dataset(split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds
Dataset({
    features: [<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
    num_rows: <span class="hljs-number">8530</span>
})`}}),{c(){d=n("p"),_=m("Example:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function gc(y){let d,_,f,r,u;return r=new N({props:{code:`from datasets import load_dataset_builder
builder = load_dataset_builder('rotten_tomatoes')
ds = builder.download_and_prepare()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.download_and_prepare()`}}),{c(){d=n("p"),_=m("Example:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function fc(y){let d,_,f,r,u;return r=new N({props:{code:`from datasets import load_dataset_builder
ds_builder = load_dataset_builder('rotten_tomatoes')
ds_builder.get_all_exported_dataset_infos()
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder.get_all_exported_dataset_infos()
{<span class="hljs-string">&#x27;default&#x27;</span>: DatasetInfo(description=<span class="hljs-string">&quot;Movie Review Dataset.
a dataset of containing 5,331 positive and 5,331 negative processed
s from Rotten Tomatoes movie reviews. This data was first used in Bo
 Lillian Lee, \`\`Seeing stars: Exploiting class relationships for
t categorization with respect to rating scales.&#x27;&#x27;, Proceedings of the
5.
ion=&#x27;@InProceedings{Pang+Lee:05a,
 =       {Bo Pang and Lillian Lee},
=        {Seeing stars: Exploiting class relationships for sentiment
          categorization with respect to rating scales},
tle =    {Proceedings of the ACL},
         2005

age=&#x27;http://www.cs.cornell.edu/people/pabo/movie-review-data/&#x27;, license=&#x27;&#x27;, features={&#x27;text&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;label&#x27;: ClassLabel(num_classes=2, names=[&#x27;neg&#x27;, &#x27;pos&#x27;], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input=&#x27;&#x27;, output=&#x27;&#x27;), task_templates=[TextClassification(task=&#x27;text-classification&#x27;, text_column=&#x27;text&#x27;, label_column=&#x27;label&#x27;)], builder_name=&#x27;rotten_tomatoes_movie_review&#x27;, config_name=&#x27;default&#x27;, version=1.0.0, splits={&#x27;train&#x27;: SplitInfo(name=&#x27;train&#x27;, num_bytes=1074810, num_examples=8530, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;validation&#x27;: SplitInfo(name=&#x27;validation&#x27;, num_bytes=134679, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;test&#x27;: SplitInfo(name=&#x27;test&#x27;, num_bytes=135972, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;)}, download_checksums={&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;: {&#x27;num_bytes&#x27;: 487770, &#x27;checksum&#x27;: &#x27;a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9&#x27;}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)}</span>`}}),{c(){d=n("p"),_=m("Example:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function uc(y){let d,_,f,r,u;return r=new N({props:{code:`from datasets import load_dataset_builder
ds_builder = load_dataset_builder('rotten_tomatoes')
ds_builder.get_exported_dataset_info()
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder.get_exported_dataset_info()
DatasetInfo(description=<span class="hljs-string">&quot;Movie Review Dataset.
a dataset of containing 5,331 positive and 5,331 negative processed
s from Rotten Tomatoes movie reviews. This data was first used in Bo
 Lillian Lee, \`\`Seeing stars: Exploiting class relationships for
t categorization with respect to rating scales.&#x27;&#x27;, Proceedings of the
5.
ion=&#x27;@InProceedings{Pang+Lee:05a,
 =       {Bo Pang and Lillian Lee},
=        {Seeing stars: Exploiting class relationships for sentiment
          categorization with respect to rating scales},
tle =    {Proceedings of the ACL},
         2005

age=&#x27;http://www.cs.cornell.edu/people/pabo/movie-review-data/&#x27;, license=&#x27;&#x27;, features={&#x27;text&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;label&#x27;: ClassLabel(num_classes=2, names=[&#x27;neg&#x27;, &#x27;pos&#x27;], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input=&#x27;&#x27;, output=&#x27;&#x27;), task_templates=[TextClassification(task=&#x27;text-classification&#x27;, text_column=&#x27;text&#x27;, label_column=&#x27;label&#x27;)], builder_name=&#x27;rotten_tomatoes_movie_review&#x27;, config_name=&#x27;default&#x27;, version=1.0.0, splits={&#x27;train&#x27;: SplitInfo(name=&#x27;train&#x27;, num_bytes=1074810, num_examples=8530, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;validation&#x27;: SplitInfo(name=&#x27;validation&#x27;, num_bytes=134679, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;test&#x27;: SplitInfo(name=&#x27;test&#x27;, num_bytes=135972, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;)}, download_checksums={&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;: {&#x27;num_bytes&#x27;: 487770, &#x27;checksum&#x27;: &#x27;a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9&#x27;}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)</span>`}}),{c(){d=n("p"),_=m("Example:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function _c(y){let d,_,f,r,u;return r=new N({props:{code:"downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)'}}),{c(){d=n("p"),_=m("Example:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function hc(y){let d,_,f,r,u;return r=new N({props:{code:"extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))",highlighted:'<span class="hljs-attr">extracted_paths</span> = dl_manager.extract(dl_manager.download(url_or_urls))'}}),{c(){d=n("p"),_=m("Is roughly equivalent to:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Is roughly equivalent to:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function $c(y){let d,_,f,r,u;return r=new N({props:{code:"downloaded_files = dl_manager.download_custom('s3://my-bucket/data.zip', custom_download_for_my_private_bucket)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download_custom(<span class="hljs-string">&#x27;s3://my-bucket/data.zip&#x27;</span>, custom_download_for_my_private_bucket)'}}),{c(){d=n("p"),_=m("Example:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function bc(y){let d,_,f,r,u;return r=new N({props:{code:`downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
extracted_files = dl_manager.extract(downloaded_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>extracted_files = dl_manager.extract(downloaded_files)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function vc(y){let d,_,f,r,u;return r=new N({props:{code:`archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
files = dl_manager.iter_archive(archive)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>archive = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_archive(archive)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function xc(y){let d,_,f,r,u;return r=new N({props:{code:`files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')
files = dl_manager.iter_files(files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.download_and_extract(<span class="hljs-string">&#x27;https://huggingface.co/datasets/beans/resolve/main/data/train.zip&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_files(files)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function wc(y){let d,_,f,r,u;return r=new N({props:{code:"downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)'}}),{c(){d=n("p"),_=m("Example:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function Ec(y){let d,_,f,r,u;return r=new N({props:{code:"extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))",highlighted:'<span class="hljs-attr">extracted_paths</span> = dl_manager.extract(dl_manager.download(url_or_urls))'}}),{c(){d=n("p"),_=m("Is roughly equivalent to:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Is roughly equivalent to:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function Dc(y){let d,_,f,r,u;return r=new N({props:{code:`downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
extracted_files = dl_manager.extract(downloaded_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>extracted_files = dl_manager.extract(downloaded_files)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function yc(y){let d,_,f,r,u;return r=new N({props:{code:`archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
files = dl_manager.iter_archive(archive)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>archive = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_archive(archive)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function jc(y){let d,_,f,r,u;return r=new N({props:{code:`files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')
files = dl_manager.iter_files(files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.download_and_extract(<span class="hljs-string">&#x27;https://huggingface.co/datasets/beans/resolve/main/data/train.zip&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_files(files)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function kc(y){let d,_,f,r,u;return r=new N({props:{code:`datasets.SplitGenerator(
    name=datasets.Split.TRAIN,
    gen_kwargs={"split_key": "train", "files": dl_manager.download_and_extract(url)},
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TRAIN,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and_extract(url)},
<span class="hljs-meta">... </span>)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function Tc(y){let d,_,f,r,u;return r=new N({props:{code:`datasets.SplitGenerator(
    name=datasets.Split.TRAIN,
    gen_kwargs={"split_key": "train", "files": dl_manager.download_and extract(url)},
),
datasets.SplitGenerator(
    name=datasets.Split.VALIDATION,
    gen_kwargs={"split_key": "validation", "files": dl_manager.download_and extract(url)},
),
datasets.SplitGenerator(
    name=datasets.Split.TEST,
    gen_kwargs={"split_key": "test", "files": dl_manager.download_and extract(url)},
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TRAIN,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and extract(url)},
<span class="hljs-meta">... </span>),
<span class="hljs-meta">... </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.VALIDATION,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;validation&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and extract(url)},
<span class="hljs-meta">... </span>),
<span class="hljs-meta">... </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TEST,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;test&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and extract(url)},
<span class="hljs-meta">... </span>)`}}),{c(){d=n("p"),_=m("Example:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function Ic(y){let d,_,f,r,u;return r=new N({props:{code:`Each descriptor can be composed with other using addition or slice. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[0:25]) + datasets.Split.TEST

The resulting split will correspond to 25% of the train split merged with
100% of the test split.`,highlighted:`Each descriptor can be composed <span class="hljs-keyword">with</span> other using addition <span class="hljs-keyword">or</span> <span class="hljs-built_in">slice</span>. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">0</span>:<span class="hljs-number">25</span>]) + datasets.Split.TEST

The resulting split will correspond to <span class="hljs-number">25</span>% of the train split merged <span class="hljs-keyword">with</span>
<span class="hljs-number">100</span>% of the test split.`}}),{c(){d=n("p"),_=m("Example:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function Sc(y){let d,_,f,r,u;return r=new N({props:{code:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[75:])
)  # Error
split = datasets.Split.TEST + datasets.Split.ALL  # Error`,highlighted:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">75</span>:])
)  <span class="hljs-comment"># Error</span>
split = datasets.Split.TEST + datasets.Split.ALL  <span class="hljs-comment"># Error</span>`}}),{c(){d=n("p"),_=m("A split cannot be added twice, so the following will fail:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"A split cannot be added twice, so the following will fail:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function Bc(y){let d,_,f,r,u;return r=new N({props:{code:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
datasets.Split.TEST.subsplit(datasets.percent[:50])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:50])`,highlighted:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
datasets.Split.TEST.subsplit(datasets.percent[:<span class="hljs-number">50</span>])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),{c(){d=n("p"),_=m("The slices can be applied only one time. So the following are valid:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"The slices can be applied only one time. So the following are valid:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function Nc(y){let d,_,f,r,u;return r=new N({props:{code:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:25]).subsplit(datasets.percent[:25])
split = (train.subsplit(datasets.percent[:25]) + test).subsplit(datasets.percent[:50])`,highlighted:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]).subsplit(datasets.percent[:<span class="hljs-number">25</span>])
split = (train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) + test).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),{c(){d=n("p"),_=m("But not:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"But not:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function Rc(y){let d,_,f,r,u;return r=new N({props:{code:`# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec('test[:33%]'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction('test', to=33, unit='%'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%'))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]+train[1:-1]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%]+train[1:-1]'))
ds = datasets.load_dataset('mnist', split=(
datasets.ReadInstruction('test', to=33, unit='%') +
datasets.ReadInstruction('train', from_=1, to=-1, unit='abs')))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%](pct1_dropremainder)')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%](pct1_dropremainder)'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%', rounding="pct1_dropremainder"))

# 10-fold validation:
tests = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', from_=k, to=k+10, unit='%')
for k in range(0, 100, 10)])
trains = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')
for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(<span class="hljs-string">&#x27;test[:33%]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=(
datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) +
datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">1</span>, to=-<span class="hljs-number">1</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>)))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&quot;pct1_dropremainder&quot;</span>))

<span class="hljs-comment"># 10-fold validation:</span>
tests = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
trains = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}),{c(){d=n("p"),_=m("Examples:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Examples:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function Pc(y){let d,_,f,r,u;return r=new N({props:{code:'VERSION = datasets.Version("1.0.0")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>VERSION = datasets.Version(<span class="hljs-string">&quot;1.0.0&quot;</span>)'}}),{c(){d=n("p"),_=m("Example:"),f=p(),$(r.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(r.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(r,t,i),u=!0},p:B,i(t){u||(x(r.$$.fragment,t),u=!0)},o(t){w(r.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(r,t)}}}function Cc(y){let d,_,f,r,u,t,i,Wa,cn,Nr,Y,mn,aa,gn,fn,sa,un,_n,Rr,T,mt,hn,Xa,$n,bn,ra,Ja,vn,xn,wn,be,na,Ka,En,Dn,yn,oa,la,jn,kn,Tn,Re,da,In,Sn,Ya,Bn,Nn,Rn,W,Qa,Pn,Cn,Za,Ln,On,es,An,Vn,ts,qn,Mn,Fn,Q,gt,zn,as,Un,Gn,Pe,Hn,Z,ft,Wn,ss,Xn,Jn,Ce,Kn,ee,ut,Yn,rs,Qn,Zn,Le,eo,te,_t,to,ns,ao,so,Oe,ro,Ae,ht,no,os,oo,Pr,J,$t,lo,ls,io,po,ae,ds,co,mo,is,go,fo,ps,uo,_o,Cr,ve,bt,ho,cs,$o,Lr,xe,vt,bo,ms,vo,Or,z,xt,xo,wt,wo,ia,Eo,Do,yo,Et,jo,pa,ko,To,Io,se,Dt,So,gs,Bo,No,we,fs,Ro,Po,us,Co,Lo,_s,Oo,Ar,R,yt,Ao,re,jt,Vo,hs,qo,Mo,Ve,Fo,ne,kt,zo,$s,Uo,Go,qe,Ho,oe,Tt,Wo,It,Xo,bs,Jo,Ko,Yo,Me,Qo,le,St,Zo,vs,el,tl,Fe,al,de,Bt,sl,xs,rl,nl,ze,ol,ie,Nt,ll,ws,dl,il,Ue,pl,Ge,Rt,cl,Es,ml,Vr,C,Pt,gl,U,fl,Ds,ul,_l,ys,hl,$l,js,bl,vl,ks,xl,wl,El,pe,Ct,Dl,Ts,yl,jl,He,kl,ce,Lt,Tl,Is,Il,Sl,We,Bl,me,Ot,Nl,Ss,Rl,Pl,Xe,Cl,ge,At,Ll,Bs,Ol,Al,Je,Vl,fe,Vt,ql,Ns,Ml,Fl,Ke,qr,V,qt,zl,ca,Rs,Ul,Gl,Hl,Mt,Wl,Ps,Xl,Jl,Kl,Cs,Yl,Ql,Ft,Ls,Ee,Mr,Zl,Os,ed,td,As,ad,sd,De,ye,ma,Vs,rd,nd,od,qs,ld,dd,Ms,id,pd,je,Fs,zs,cd,md,Us,gd,fd,Gs,ud,_d,ke,Hs,Ws,hd,$d,Xs,bd,vd,Js,xd,Fr,G,zt,wd,Ks,Ed,Dd,Te,yd,Ys,jd,kd,Qs,Td,Id,Sd,Ye,zr,L,Ut,Bd,ga,Zs,Nd,Rd,Pd,er,Cd,Ld,K,fa,tr,Od,Ad,Vd,ua,ar,qd,Md,Fd,_a,sr,zd,Ud,Gd,ha,rr,Hd,Wd,Xd,$a,Jd,nr,Kd,Yd,Gt,Qd,or,Zd,ei,ti,Qe,Ur,P,Ht,ai,lr,si,ri,Ze,ni,dr,oi,li,et,di,ir,ii,pi,tt,ci,at,Gr,Ie,Wt,mi,pr,gi,Hr,q,Xt,fi,cr,ui,_i,st,hi,rt,Jt,$i,mr,bi,vi,ue,Kt,xi,gr,wi,Ei,fr,Di,Wr,Se,Yt,yi,ur,ji,Xr,H,Qt,ki,_r,Ti,Ii,nt,Si,ot,Zt,Bi,hr,Ni,Jr;return t=new cc({}),mt=new k({props:{name:"class datasets.DatasetBuilder",anchor:"datasets.DatasetBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"repo_id",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L177"}}),gt=new k({props:{name:"as_dataset",anchor:"datasets.DatasetBuilder.as_dataset",parameters:[{name:"split",val:": typing.Optional[datasets.splits.Split] = None"},{name:"run_post_process",val:" = True"},{name:"ignore_verifications",val:" = False"},{name:"in_memory",val:" = False"}],parametersDescription:[{anchor:"datasets.DatasetBuilder.as_dataset.split",description:"<strong>split</strong> (<code>datasets.Split</code>) &#x2014; Which subset of the data to return.",name:"split"},{anchor:"datasets.DatasetBuilder.as_dataset.run_post_process",description:`<strong>run_post_process</strong> (bool, default=True) &#x2014; Whether to run post-processing dataset transforms and/or add
indexes.`,name:"run_post_process"},{anchor:"datasets.DatasetBuilder.as_dataset.ignore_verifications",description:`<strong>ignore_verifications</strong> (bool, default=False) &#x2014; Whether to ignore the verifications of the
downloaded/processed dataset information (checksums/size/splits/&#x2026;).`,name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.as_dataset.in_memory",description:"<strong>in_memory</strong> (bool, default=False) &#x2014; Whether to copy the data in-memory.",name:"in_memory"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L779",returnDescription:`
<p>datasets.Dataset</p>
`}}),Pe=new S({props:{anchor:"datasets.DatasetBuilder.as_dataset.example",$$slots:{default:[mc]},$$scope:{ctx:y}}}),ft=new k({props:{name:"download_and_prepare",anchor:"datasets.DatasetBuilder.download_and_prepare",parameters:[{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"download_mode",val:": typing.Optional[datasets.utils.download_manager.DownloadMode] = None"},{name:"ignore_verifications",val:": bool = False"},{name:"try_from_hf_gcs",val:": bool = True"},{name:"dl_manager",val:": typing.Optional[datasets.utils.download_manager.DownloadManager] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"**download_and_prepare_kwargs",val:""}],parametersDescription:[{anchor:"datasets.DatasetBuilder.download_and_prepare.download_config",description:'<strong>download_config</strong> (<a href="/docs/datasets/master/en/package_reference/builder_classes#datasets.DownloadConfig">DownloadConfig</a>, optional) &#x2014; specific download configuration parameters.',name:"download_config"},{anchor:"datasets.DatasetBuilder.download_and_prepare.download_mode",description:'<strong>download_mode</strong> (<a href="/docs/datasets/master/en/package_reference/builder_classes#datasets.DownloadMode">DownloadMode</a>, optional) &#x2014; select the download/generate mode - Default to <code>REUSE_DATASET_IF_EXISTS</code>',name:"download_mode"},{anchor:"datasets.DatasetBuilder.download_and_prepare.ignore_verifications",description:"<strong>ignore_verifications</strong> (<code>bool</code>) &#x2014; Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/&#x2026;)",name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.download_and_prepare.try_from_hf_gcs",description:"<strong>try_from_hf_gcs</strong> (<code>bool</code>) &#x2014; If True, it will try to download the already prepared dataset from the Hf google cloud storage",name:"try_from_hf_gcs"},{anchor:"datasets.DatasetBuilder.download_and_prepare.dl_manager",description:'<strong>dl_manager</strong> (<a href="/docs/datasets/master/en/package_reference/builder_classes#datasets.DownloadManager">DownloadManager</a>, optional) &#x2014; specific Download Manger to use',name:"dl_manager"},{anchor:"datasets.DatasetBuilder.download_and_prepare.base_path",description:`<strong>base_path</strong> (<code>str</code>, optional) &#x2014; base path for relative paths that are used to download files. This can be a remote url.
If not specified, the value of the <em>base_path</em> attribute (<em>self.base_path</em>) will be used instead.`,name:"base_path"},{anchor:"datasets.DatasetBuilder.download_and_prepare.use_auth_token",description:`<strong>use_auth_token</strong> (<code>Union[str, bool]</code>, optional) &#x2014; Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.
If True, will get token from ~/.huggingface.`,name:"use_auth_token"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L512"}}),Ce=new S({props:{anchor:"datasets.DatasetBuilder.download_and_prepare.example",$$slots:{default:[gc]},$$scope:{ctx:y}}}),ut=new k({props:{name:"get_all_exported_dataset_infos",anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos",parameters:[],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L324"}}),Le=new S({props:{anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos.example",$$slots:{default:[fc]},$$scope:{ctx:y}}}),_t=new k({props:{name:"get_exported_dataset_info",anchor:"datasets.DatasetBuilder.get_exported_dataset_info",parameters:[],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L342"}}),Oe=new S({props:{anchor:"datasets.DatasetBuilder.get_exported_dataset_info.example",$$slots:{default:[uc]},$$scope:{ctx:y}}}),ht=new k({props:{name:"get_imported_module_dir",anchor:"datasets.DatasetBuilder.get_imported_module_dir",parameters:[],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L507"}}),$t=new k({props:{name:"class datasets.GeneratorBasedBuilder",anchor:"datasets.GeneratorBasedBuilder",parameters:[{name:"*args",val:""},{name:"writer_batch_size",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L1066"}}),bt=new k({props:{name:"class datasets.BeamBasedBuilder",anchor:"datasets.BeamBasedBuilder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L1226"}}),vt=new k({props:{name:"class datasets.ArrowBasedBuilder",anchor:"datasets.ArrowBasedBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"repo_id",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L1164"}}),xt=new k({props:{name:"class datasets.BuilderConfig",anchor:"datasets.BuilderConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = '0.0.0'"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"datasets.BuilderConfig.name",description:"<strong>name</strong> (<code>str</code>, default <code>&quot;default&quot;</code>) &#x2014;",name:"name"},{anchor:"datasets.BuilderConfig.version",description:'<strong>version</strong> (<a href="/docs/datasets/master/en/package_reference/builder_classes#datasets.Version">Version</a> or <code>str</code>, optional) &#x2014;',name:"version"},{anchor:"datasets.BuilderConfig.data_dir",description:"<strong>data_dir</strong> (<code>str</code>, optional) &#x2014;",name:"data_dir"},{anchor:"datasets.BuilderConfig.data_files",description:"<strong>data_files</strong> (<code>str</code> or <code>Sequence</code> or <code>Mapping</code>, optional) &#x2014; Path(s) to source data file(s).",name:"data_files"},{anchor:"datasets.BuilderConfig.description",description:"<strong>description</strong> (<code>str</code>, optional) &#x2014;",name:"description"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L81"}}),Dt=new k({props:{name:"create_config_id",anchor:"datasets.BuilderConfig.create_config_id",parameters:[{name:"config_kwargs",val:": dict"},{name:"custom_features",val:": typing.Optional[datasets.features.features.Features] = None"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L120"}}),yt=new k({props:{name:"class datasets.DownloadManager",anchor:"datasets.DownloadManager",parameters:[{name:"dataset_name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"record_checksums",val:" = True"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L141"}}),jt=new k({props:{name:"download",anchor:"datasets.DownloadManager.download",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L265",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Ve=new S({props:{anchor:"datasets.DownloadManager.download.example",$$slots:{default:[_c]},$$scope:{ctx:y}}}),kt=new k({props:{name:"download_and_extract",anchor:"datasets.DownloadManager.download_and_extract",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L400",returnDescription:`
<p><code>str</code>, extracted paths of given URL(s).</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),qe=new S({props:{anchor:"datasets.DownloadManager.download_and_extract.example",$$slots:{default:[hc]},$$scope:{ctx:y}}}),Tt=new k({props:{name:"download_custom",anchor:"datasets.DownloadManager.download_custom",parameters:[{name:"url_or_urls",val:""},{name:"custom_download",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L218",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Me=new S({props:{anchor:"datasets.DownloadManager.download_custom.example",$$slots:{default:[$c]},$$scope:{ctx:y}}}),St=new k({props:{name:"extract",anchor:"datasets.DownloadManager.extract",parameters:[{name:"path_or_paths",val:""},{name:"num_proc",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L363",returnDescription:`
<p><code>str</code>, The extracted paths matching the given input
path_or_paths.</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Fe=new S({props:{anchor:"datasets.DownloadManager.extract.example",$$slots:{default:[bc]},$$scope:{ctx:y}}}),Bt=new k({props:{name:"iter_archive",anchor:"datasets.DownloadManager.iter_archive",parameters:[{name:"path_or_buf",val:": typing.Union[str, _io.BufferedReader]"}],parametersDescription:[{anchor:"datasets.DownloadManager.iter_archive.path_or_buf",description:"<strong>path_or_buf</strong> (<code>str</code> or <code>io.BufferedReader</code>) &#x2014; Archive path or archive binary file object.",name:"path_or_buf"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L322"}}),ze=new S({props:{anchor:"datasets.DownloadManager.iter_archive.example",$$slots:{default:[vc]},$$scope:{ctx:y}}}),Nt=new k({props:{name:"iter_files",anchor:"datasets.DownloadManager.iter_files",parameters:[{name:"paths",val:": typing.Union[str, typing.List[str]]"}],parametersDescription:[{anchor:"datasets.DownloadManager.iter_files.paths",description:"<strong>paths</strong> (<code>str</code> or <code>list</code> of <code>str</code>) &#x2014; Root paths.",name:"paths"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L345"}}),Ue=new S({props:{anchor:"datasets.DownloadManager.iter_files.example",$$slots:{default:[xc]},$$scope:{ctx:y}}}),Rt=new k({props:{name:"ship_files_with_pipeline",anchor:"datasets.DownloadManager.ship_files_with_pipeline",parameters:[{name:"downloaded_path_or_paths",val:""},{name:"pipeline",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L183"}}),Pt=new k({props:{name:"class datasets.StreamingDownloadManager",anchor:"datasets.StreamingDownloadManager",parameters:[{name:"dataset_name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"base_path",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/streaming_download_manager.py#L766"}}),Ct=new k({props:{name:"download",anchor:"datasets.StreamingDownloadManager.download",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/streaming_download_manager.py#L792",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),He=new S({props:{anchor:"datasets.StreamingDownloadManager.download.example",$$slots:{default:[wc]},$$scope:{ctx:y}}}),Lt=new k({props:{name:"download_and_extract",anchor:"datasets.StreamingDownloadManager.download_and_extract",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/streaming_download_manager.py#L860",returnDescription:`
<p><code>str</code>, extracted paths of given URL(s).</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),We=new S({props:{anchor:"datasets.StreamingDownloadManager.download_and_extract.example",$$slots:{default:[Ec]},$$scope:{ctx:y}}}),Ot=new k({props:{name:"extract",anchor:"datasets.StreamingDownloadManager.extract",parameters:[{name:"path_or_paths",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/streaming_download_manager.py#L819",returnDescription:`
<p><code>str</code>, The extracted paths matching the given input
path_or_paths.</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Xe=new S({props:{anchor:"datasets.StreamingDownloadManager.extract.example",$$slots:{default:[Dc]},$$scope:{ctx:y}}}),At=new k({props:{name:"iter_archive",anchor:"datasets.StreamingDownloadManager.iter_archive",parameters:[{name:"urlpath_or_buf",val:": typing.Union[str, _io.BufferedReader]"}],parametersDescription:[{anchor:"datasets.StreamingDownloadManager.iter_archive.urlpath_or_buf",description:"<strong>urlpath_or_buf</strong> (<code>str</code> or <code>io.BufferedReader</code>) &#x2014; Archive path or archive binary file object.",name:"urlpath_or_buf"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/streaming_download_manager.py#L878"}}),Je=new S({props:{anchor:"datasets.StreamingDownloadManager.iter_archive.example",$$slots:{default:[yc]},$$scope:{ctx:y}}}),Vt=new k({props:{name:"iter_files",anchor:"datasets.StreamingDownloadManager.iter_files",parameters:[{name:"urlpaths",val:": typing.Union[str, typing.List[str]]"}],parametersDescription:[{anchor:"datasets.StreamingDownloadManager.iter_files.urlpaths",description:"<strong>urlpaths</strong> (<code>str</code> or <code>list</code> of <code>str</code>) &#x2014; Root paths.",name:"urlpaths"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/streaming_download_manager.py#L901"}}),Ke=new S({props:{anchor:"datasets.StreamingDownloadManager.iter_files.example",$$slots:{default:[jc]},$$scope:{ctx:y}}}),qt=new k({props:{name:"class datasets.DownloadMode",anchor:"datasets.DownloadMode",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L44"}}),zt=new k({props:{name:"class datasets.SplitGenerator",anchor:"datasets.SplitGenerator",parameters:[{name:"name",val:": str"},{name:"gen_kwargs",val:": typing.Dict = <factory>"}],parametersDescription:[{anchor:"datasets.SplitGenerator.name",description:`<strong>name</strong> (str) &#x2014; Name of the Split for which the generator will
create the examples.
**gen_kwargs &#x2014; Keyword arguments to forward to the <code>DatasetBuilder._generate_examples</code> method
of the builder.`,name:"name"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/splits.py#L566"}}),Ye=new S({props:{anchor:"datasets.SplitGenerator.example",$$slots:{default:[kc]},$$scope:{ctx:y}}}),Ut=new k({props:{name:"class datasets.Split",anchor:"datasets.Split",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/splits.py#L387"}}),Qe=new S({props:{anchor:"datasets.Split.example",$$slots:{default:[Tc]},$$scope:{ctx:y}}}),Ht=new k({props:{name:"class datasets.NamedSplit",anchor:"datasets.NamedSplit",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/splits.py#L303"}}),Ze=new S({props:{anchor:"datasets.NamedSplit.example",$$slots:{default:[Ic]},$$scope:{ctx:y}}}),et=new S({props:{anchor:"datasets.NamedSplit.example-2",$$slots:{default:[Sc]},$$scope:{ctx:y}}}),tt=new S({props:{anchor:"datasets.NamedSplit.example-3",$$slots:{default:[Bc]},$$scope:{ctx:y}}}),at=new S({props:{anchor:"datasets.NamedSplit.example-4",$$slots:{default:[Nc]},$$scope:{ctx:y}}}),Wt=new k({props:{name:"class datasets.NamedSplitAll",anchor:"datasets.NamedSplitAll",parameters:[],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/splits.py#L372"}}),Xt=new k({props:{name:"class datasets.ReadInstruction",anchor:"datasets.ReadInstruction",parameters:[{name:"split_name",val:""},{name:"rounding",val:" = None"},{name:"from_",val:" = None"},{name:"to",val:" = None"},{name:"unit",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_reader.py#L456"}}),st=new S({props:{anchor:"datasets.ReadInstruction.example",$$slots:{default:[Rc]},$$scope:{ctx:y}}}),Jt=new k({props:{name:"from_spec",anchor:"datasets.ReadInstruction.from_spec",parameters:[{name:"spec",val:""}],parametersDescription:[{anchor:"datasets.ReadInstruction.from_spec.spec",description:`<strong>spec</strong> (str) &#x2014; split(s) + optional slice(s) to read + optional rounding
if percents are used as the slicing unit. A slice can be specified,
using absolute numbers (int) or percentages (int). E.g.
<code>test</code>: test split.
<code>test + validation</code>: test split + validation split.
<code>test[10:]</code>: test split, minus its first 10 records.
<code>test[:10%]</code>: first 10% records of test split.
<code>test[:20%](pct1_dropremainder)</code>: first 10% records, rounded with
the <code>pct1_dropremainder</code> rounding.
<code>test[:-5%]+train[40%:60%]</code>: first 95% of test + middle 20% of
train.`,name:"spec"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_reader.py#L536",returnDescription:`
<p>ReadInstruction instance.</p>
`}}),Kt=new k({props:{name:"to_absolute",anchor:"datasets.ReadInstruction.to_absolute",parameters:[{name:"name2len",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_reader.py#L604",returnDescription:`
<p>list of _AbsoluteInstruction instances (corresponds to the + in spec).</p>
`}}),Yt=new k({props:{name:"class datasets.DownloadConfig",anchor:"datasets.DownloadConfig",parameters:[{name:"cache_dir",val:": typing.Union[pathlib.Path, str, NoneType] = None"},{name:"force_download",val:": bool = False"},{name:"resume_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"proxies",val:": typing.Optional[typing.Dict] = None"},{name:"user_agent",val:": typing.Optional[str] = None"},{name:"extract_compressed_file",val:": bool = False"},{name:"force_extract",val:": bool = False"},{name:"delete_extracted",val:": bool = False"},{name:"use_etag",val:": bool = True"},{name:"num_proc",val:": typing.Optional[int] = None"},{name:"max_retries",val:": int = 1"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"ignore_url_params",val:": bool = False"},{name:"download_desc",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"datasets.DownloadConfig.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>Path</code>, optional) &#x2014; Specify a cache directory to save the file to (overwrite the
default cache dir).`,name:"cache_dir"},{anchor:"datasets.DownloadConfig.force_download",description:`<strong>force_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, re-dowload the file even if it&#x2019;s already cached in
the cache dir.`,name:"force_download"},{anchor:"datasets.DownloadConfig.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, resume the download if incompletly recieved file is
found.`,name:"resume_download"},{anchor:"datasets.DownloadConfig.proxies",description:"<strong>proxies</strong> (<code>dict</code>, optional) &#x2014;",name:"proxies"},{anchor:"datasets.DownloadConfig.user_agent",description:`<strong>user_agent</strong> (<code>str</code>, optional) &#x2014; Optional string or dict that will be appended to the user-agent on remote
requests.`,name:"user_agent"},{anchor:"datasets.DownloadConfig.extract_compressed_file",description:`<strong>extract_compressed_file</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True and the path point to a zip or tar file,
extract the compressed file in a folder along the archive.`,name:"extract_compressed_file"},{anchor:"datasets.DownloadConfig.force_extract",description:`<strong>force_extract</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True when extract_compressed_file is True and the archive
was already extracted, re-extract the archive and override the folder where it was extracted.`,name:"force_extract"},{anchor:"datasets.DownloadConfig.delete_extracted",description:"<strong>delete_extracted</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to delete (or keep) the extracted files.",name:"delete_extracted"},{anchor:"datasets.DownloadConfig.use_etag",description:"<strong>use_etag</strong> (<code>bool</code>, default <code>True</code>) &#x2014; Whether to use the ETag HTTP response header to validate the cached files.",name:"use_etag"},{anchor:"datasets.DownloadConfig.num_proc",description:"<strong>num_proc</strong> (<code>int</code>, optional) &#x2014; The number of processes to launch to download the files in parallel.",name:"num_proc"},{anchor:"datasets.DownloadConfig.max_retries",description:"<strong>max_retries</strong> (<code>int</code>, default <code>1</code>) &#x2014; The number of times to retry an HTTP request if it fails.",name:"max_retries"},{anchor:"datasets.DownloadConfig.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, optional) &#x2014; Optional string or boolean to use as Bearer token
for remote files on the Datasets Hub. If True, will get token from ~/.huggingface.`,name:"use_auth_token"},{anchor:"datasets.DownloadConfig.ignore_url_params",description:`<strong>ignore_url_params</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to strip all query parameters and #fragments from
the download URL before using it for caching the file.`,name:"ignore_url_params"},{anchor:"datasets.DownloadConfig.download_desc",description:"<strong>download_desc</strong> (<code>str</code>, optional) &#x2014; A description to be displayed alongside with the progress bar while downloading the files.",name:"download_desc"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/file_utils.py#L153"}}),Qt=new k({props:{name:"class datasets.Version",anchor:"datasets.Version",parameters:[{name:"version_str",val:": str"},{name:"description",val:": typing.Optional[str] = None"},{name:"major",val:": typing.Union[str, int, NoneType] = None"},{name:"minor",val:": typing.Union[str, int, NoneType] = None"},{name:"patch",val:": typing.Union[str, int, NoneType] = None"}],parametersDescription:[{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.major",description:"<strong>major</strong> (<code>str</code>) &#x2014;",name:"major"},{anchor:"datasets.Version.minor",description:"<strong>minor</strong> (<code>str</code>) &#x2014;",name:"minor"},{anchor:"datasets.Version.patch",description:"<strong>patch</strong> (<code>str</code>) &#x2014;",name:"patch"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/version.py#L30"}}),nt=new S({props:{anchor:"datasets.Version.example",$$slots:{default:[Pc]},$$scope:{ctx:y}}}),Zt=new k({props:{name:"match",anchor:"datasets.Version.match",parameters:[{name:"other_version",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/version.py#L98"}}),{c(){d=n("meta"),_=p(),f=n("h1"),r=n("a"),u=n("span"),$(t.$$.fragment),i=p(),Wa=n("span"),cn=m("Builder classes"),Nr=p(),Y=n("p"),mn=m("\u{1F917} Datasets relies on two main classes during the dataset building process: "),aa=n("a"),gn=m("DatasetBuilder"),fn=m(" and "),sa=n("a"),un=m("BuilderConfig"),_n=m("."),Rr=p(),T=n("div"),$(mt.$$.fragment),hn=p(),Xa=n("p"),$n=m("Abstract base class for all datasets."),bn=p(),ra=n("p"),Ja=n("em"),vn=m("DatasetBuilder"),xn=m(" has 3 key methods:"),wn=p(),be=n("ul"),na=n("li"),Ka=n("code"),En=m("datasets.DatasetBuilder.info"),Dn=m(`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),yn=p(),oa=n("li"),la=n("a"),jn=m("datasets.DatasetBuilder.download_and_prepare()"),kn=m(`: Downloads the source data
and writes it to disk.`),Tn=p(),Re=n("li"),da=n("a"),In=m("datasets.DatasetBuilder.as_dataset()"),Sn=m(": Generates a "),Ya=n("em"),Bn=m("Dataset"),Nn=m("."),Rn=p(),W=n("p"),Qa=n("strong"),Pn=m("Configuration"),Cn=m(": Some "),Za=n("em"),Ln=m("DatasetBuilder"),On=m(`s expose multiple variants of the
dataset by defining a `),es=n("em"),An=m("datasets.BuilderConfig"),Vn=m(` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),ts=n("code"),qn=m("datasets.DatasetBuilder.builder_configs()"),Mn=m("."),Fn=p(),Q=n("div"),$(gt.$$.fragment),zn=p(),as=n("p"),Un=m("Return a Dataset for the specified split."),Gn=p(),$(Pe.$$.fragment),Hn=p(),Z=n("div"),$(ft.$$.fragment),Wn=p(),ss=n("p"),Xn=m("Downloads and prepares dataset for reading."),Jn=p(),$(Ce.$$.fragment),Kn=p(),ee=n("div"),$(ut.$$.fragment),Yn=p(),rs=n("p"),Qn=m("Empty dict if doesn\u2019t exist"),Zn=p(),$(Le.$$.fragment),eo=p(),te=n("div"),$(_t.$$.fragment),to=p(),ns=n("p"),ao=m("Empty DatasetInfo if doesn\u2019t exist"),so=p(),$(Oe.$$.fragment),ro=p(),Ae=n("div"),$(ht.$$.fragment),no=p(),os=n("p"),oo=m("Return the path of the module of this class or subclass."),Pr=p(),J=n("div"),$($t.$$.fragment),lo=p(),ls=n("p"),io=m("Base class for datasets with data generation based on dict generators."),po=p(),ae=n("p"),ds=n("code"),co=m("GeneratorBasedBuilder"),mo=m(` is a convenience class that abstracts away much
of the data writing and reading of `),is=n("code"),go=m("DatasetBuilder"),fo=m(`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),ps=n("code"),uo=m("_split_generators"),_o=m("). See the method docstrings for details."),Cr=p(),ve=n("div"),$(bt.$$.fragment),ho=p(),cs=n("p"),$o=m("Beam based Builder."),Lr=p(),xe=n("div"),$(vt.$$.fragment),bo=p(),ms=n("p"),vo=m("Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),Or=p(),z=n("div"),$(xt.$$.fragment),xo=p(),wt=n("p"),wo=m("Base class for "),ia=n("a"),Eo=m("DatasetBuilder"),Do=m(" data configuration."),yo=p(),Et=n("p"),jo=m(`DatasetBuilder subclasses with data configuration options should subclass
`),pa=n("a"),ko=m("BuilderConfig"),To=m(" and add their own properties."),Io=p(),se=n("div"),$(Dt.$$.fragment),So=p(),gs=n("p"),Bo=m(`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),No=p(),we=n("ul"),fs=n("li"),Ro=m("the config kwargs that can be used to overwrite attributes"),Po=p(),us=n("li"),Co=m("the custom features used to write the dataset"),Lo=p(),_s=n("li"),Oo=m(`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),Ar=p(),R=n("div"),$(yt.$$.fragment),Ao=p(),re=n("div"),$(jt.$$.fragment),Vo=p(),hs=n("p"),qo=m("Download given url(s)."),Mo=p(),$(Ve.$$.fragment),Fo=p(),ne=n("div"),$(kt.$$.fragment),zo=p(),$s=n("p"),Uo=m("Download and extract given url_or_urls."),Go=p(),$(qe.$$.fragment),Ho=p(),oe=n("div"),$(Tt.$$.fragment),Wo=p(),It=n("p"),Xo=m("Download given urls(s) by calling "),bs=n("code"),Jo=m("custom_download"),Ko=m("."),Yo=p(),$(Me.$$.fragment),Qo=p(),le=n("div"),$(St.$$.fragment),Zo=p(),vs=n("p"),el=m("Extract given path(s)."),tl=p(),$(Fe.$$.fragment),al=p(),de=n("div"),$(Bt.$$.fragment),sl=p(),xs=n("p"),rl=m("Iterate over files within an archive."),nl=p(),$(ze.$$.fragment),ol=p(),ie=n("div"),$(Nt.$$.fragment),ll=p(),ws=n("p"),dl=m("Iterate over file paths."),il=p(),$(Ue.$$.fragment),pl=p(),Ge=n("div"),$(Rt.$$.fragment),cl=p(),Es=n("p"),ml=m("Ship the files using Beam FileSystems to the pipeline temp dir."),Vr=p(),C=n("div"),$(Pt.$$.fragment),gl=p(),U=n("p"),fl=m(`Download manager that uses the \u201D::\u201D separator to navigate through (possibly remote) compressed archives.
Contrary to the regular DownloadManager, the `),Ds=n("code"),ul=m("download"),_l=m(" and "),ys=n("code"),hl=m("extract"),$l=m(` methods don\u2019t actually download nor extract
data, but they rather return the path or url that could be opened using the `),js=n("code"),bl=m("xopen"),vl=m(` function which extends the
builtin `),ks=n("code"),xl=m("open"),wl=m(" function to stream data from remote files."),El=p(),pe=n("div"),$(Ct.$$.fragment),Dl=p(),Ts=n("p"),yl=m("Download given url(s)."),jl=p(),$(He.$$.fragment),kl=p(),ce=n("div"),$(Lt.$$.fragment),Tl=p(),Is=n("p"),Il=m("Download and extract given url_or_urls."),Sl=p(),$(We.$$.fragment),Bl=p(),me=n("div"),$(Ot.$$.fragment),Nl=p(),Ss=n("p"),Rl=m("Extract given path(s)."),Pl=p(),$(Xe.$$.fragment),Cl=p(),ge=n("div"),$(At.$$.fragment),Ll=p(),Bs=n("p"),Ol=m("Iterate over files within an archive."),Al=p(),$(Je.$$.fragment),Vl=p(),fe=n("div"),$(Vt.$$.fragment),ql=p(),Ns=n("p"),Ml=m("Iterate over files."),Fl=p(),$(Ke.$$.fragment),qr=p(),V=n("div"),$(qt.$$.fragment),zl=p(),ca=n("p"),Rs=n("code"),Ul=m("Enum"),Gl=m(" for how to treat pre-existing downloads and data."),Hl=p(),Mt=n("p"),Wl=m("The default mode is "),Ps=n("code"),Xl=m("REUSE_DATASET_IF_EXISTS"),Jl=m(`, which will reuse both
raw downloads and the prepared dataset if they exist.`),Kl=p(),Cs=n("p"),Yl=m("The generations modes:"),Ql=p(),Ft=n("table"),Ls=n("thead"),Ee=n("tr"),Mr=n("th"),Zl=p(),Os=n("th"),ed=m("Downloads"),td=p(),As=n("th"),ad=m("Dataset"),sd=p(),De=n("tbody"),ye=n("tr"),ma=n("td"),Vs=n("code"),rd=m("REUSE_DATASET_IF_EXISTS"),nd=m(" (default)"),od=p(),qs=n("td"),ld=m("Reuse"),dd=p(),Ms=n("td"),id=m("Reuse"),pd=p(),je=n("tr"),Fs=n("td"),zs=n("code"),cd=m("REUSE_CACHE_IF_EXISTS"),md=p(),Us=n("td"),gd=m("Reuse"),fd=p(),Gs=n("td"),ud=m("Fresh"),_d=p(),ke=n("tr"),Hs=n("td"),Ws=n("code"),hd=m("FORCE_REDOWNLOAD"),$d=p(),Xs=n("td"),bd=m("Fresh"),vd=p(),Js=n("td"),xd=m("Fresh"),Fr=p(),G=n("div"),$(zt.$$.fragment),wd=p(),Ks=n("p"),Ed=m("Defines the split information for the generator."),Dd=p(),Te=n("p"),yd=m(`This should be used as returned value of
`),Ys=n("code"),jd=m("GeneratorBasedBuilder._split_generators()"),kd=m(`.
See `),Qs=n("code"),Td=m("GeneratorBasedBuilder._split_generators()"),Id=m(` for more info and example
of usage.`),Sd=p(),$(Ye.$$.fragment),zr=p(),L=n("div"),$(Ut.$$.fragment),Bd=p(),ga=n("p"),Zs=n("code"),Nd=m("Enum"),Rd=m(" for dataset splits."),Pd=p(),er=n("p"),Cd=m(`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),Ld=p(),K=n("ul"),fa=n("li"),tr=n("code"),Od=m("TRAIN"),Ad=m(": the training data."),Vd=p(),ua=n("li"),ar=n("code"),qd=m("VALIDATION"),Md=m(`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Fd=p(),_a=n("li"),sr=n("code"),zd=m("TEST"),Ud=m(`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Gd=p(),ha=n("li"),rr=n("code"),Hd=m("ALL"),Wd=m(": the union of all defined dataset splits."),Xd=p(),$a=n("p"),Jd=m("Note: All splits, including compositions inherit from "),nr=n("code"),Kd=m("datasets.SplitBase"),Yd=p(),Gt=n("p"),Qd=m("See the :doc:"),or=n("code"),Zd=m("guide on splits </loading>"),ei=m(" for more information."),ti=p(),$(Qe.$$.fragment),Ur=p(),P=n("div"),$(Ht.$$.fragment),ai=p(),lr=n("p"),si=m("Descriptor corresponding to a named split (train, test, \u2026)."),ri=p(),$(Ze.$$.fragment),ni=p(),dr=n("p"),oi=m("Warning:"),li=p(),$(et.$$.fragment),di=p(),ir=n("p"),ii=m("Warning:"),pi=p(),$(tt.$$.fragment),ci=p(),$(at.$$.fragment),Gr=p(),Ie=n("div"),$(Wt.$$.fragment),mi=p(),pr=n("p"),gi=m("Split corresponding to the union of all defined dataset splits."),Hr=p(),q=n("div"),$(Xt.$$.fragment),fi=p(),cr=n("p"),ui=m("Reading instruction for a dataset."),_i=p(),$(st.$$.fragment),hi=p(),rt=n("div"),$(Jt.$$.fragment),$i=p(),mr=n("p"),bi=m("Creates a ReadInstruction instance out of a string spec."),vi=p(),ue=n("div"),$(Kt.$$.fragment),xi=p(),gr=n("p"),wi=m("Translate instruction into a list of absolute instructions."),Ei=p(),fr=n("p"),Di=m("Those absolute instructions are then to be added together."),Wr=p(),Se=n("div"),$(Yt.$$.fragment),yi=p(),ur=n("p"),ji=m("Configuration for our cached path manager."),Xr=p(),H=n("div"),$(Qt.$$.fragment),ki=p(),_r=n("p"),Ti=m("Dataset version MAJOR.MINOR.PATCH."),Ii=p(),$(nt.$$.fragment),Si=p(),ot=n("div"),$(Zt.$$.fragment),Bi=p(),hr=n("p"),Ni=m("Returns True if other_version matches."),this.h()},l(s){const h=ic('[data-svelte="svelte-1phssyn"]',document.head);d=o(h,"META",{name:!0,content:!0}),h.forEach(a),_=c(s),f=o(s,"H1",{class:!0});var ea=l(f);r=o(ea,"A",{id:!0,class:!0,href:!0});var $r=l(r);u=o($r,"SPAN",{});var br=l(u);b(t.$$.fragment,br),br.forEach(a),$r.forEach(a),i=c(ea),Wa=o(ea,"SPAN",{});var vr=l(Wa);cn=g(vr,"Builder classes"),vr.forEach(a),ea.forEach(a),Nr=c(s),Y=o(s,"P",{});var Be=l(Y);mn=g(Be,"\u{1F917} Datasets relies on two main classes during the dataset building process: "),aa=o(Be,"A",{href:!0});var xr=l(aa);gn=g(xr,"DatasetBuilder"),xr.forEach(a),fn=g(Be," and "),sa=o(Be,"A",{href:!0});var wr=l(sa);un=g(wr,"BuilderConfig"),wr.forEach(a),_n=g(Be,"."),Be.forEach(a),Rr=c(s),T=o(s,"DIV",{class:!0});var I=l(T);b(mt.$$.fragment,I),hn=c(I),Xa=o(I,"P",{});var Er=l(Xa);$n=g(Er,"Abstract base class for all datasets."),Er.forEach(a),bn=c(I),ra=o(I,"P",{});var ba=l(ra);Ja=o(ba,"EM",{});var Dr=l(Ja);vn=g(Dr,"DatasetBuilder"),Dr.forEach(a),xn=g(ba," has 3 key methods:"),ba.forEach(a),wn=c(I),be=o(I,"UL",{});var Ne=l(be);na=o(Ne,"LI",{});var va=l(na);Ka=o(va,"CODE",{});var yr=l(Ka);En=g(yr,"datasets.DatasetBuilder.info"),yr.forEach(a),Dn=g(va,`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),va.forEach(a),yn=c(Ne),oa=o(Ne,"LI",{});var xa=l(oa);la=o(xa,"A",{href:!0});var jr=l(la);jn=g(jr,"datasets.DatasetBuilder.download_and_prepare()"),jr.forEach(a),kn=g(xa,`: Downloads the source data
and writes it to disk.`),xa.forEach(a),Tn=c(Ne),Re=o(Ne,"LI",{});var lt=l(Re);da=o(lt,"A",{href:!0});var kr=l(da);In=g(kr,"datasets.DatasetBuilder.as_dataset()"),kr.forEach(a),Sn=g(lt,": Generates a "),Ya=o(lt,"EM",{});var Tr=l(Ya);Bn=g(Tr,"Dataset"),Tr.forEach(a),Nn=g(lt,"."),lt.forEach(a),Ne.forEach(a),Rn=c(I),W=o(I,"P",{});var X=l(W);Qa=o(X,"STRONG",{});var Ir=l(Qa);Pn=g(Ir,"Configuration"),Ir.forEach(a),Cn=g(X,": Some "),Za=o(X,"EM",{});var Sr=l(Za);Ln=g(Sr,"DatasetBuilder"),Sr.forEach(a),On=g(X,`s expose multiple variants of the
dataset by defining a `),es=o(X,"EM",{});var Br=l(es);An=g(Br,"datasets.BuilderConfig"),Br.forEach(a),Vn=g(X,` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),ts=o(X,"CODE",{});var Mi=l(ts);qn=g(Mi,"datasets.DatasetBuilder.builder_configs()"),Mi.forEach(a),Mn=g(X,"."),X.forEach(a),Fn=c(I),Q=o(I,"DIV",{class:!0});var wa=l(Q);b(gt.$$.fragment,wa),zn=c(wa),as=o(wa,"P",{});var Fi=l(as);Un=g(Fi,"Return a Dataset for the specified split."),Fi.forEach(a),Gn=c(wa),b(Pe.$$.fragment,wa),wa.forEach(a),Hn=c(I),Z=o(I,"DIV",{class:!0});var Ea=l(Z);b(ft.$$.fragment,Ea),Wn=c(Ea),ss=o(Ea,"P",{});var zi=l(ss);Xn=g(zi,"Downloads and prepares dataset for reading."),zi.forEach(a),Jn=c(Ea),b(Ce.$$.fragment,Ea),Ea.forEach(a),Kn=c(I),ee=o(I,"DIV",{class:!0});var Da=l(ee);b(ut.$$.fragment,Da),Yn=c(Da),rs=o(Da,"P",{});var Ui=l(rs);Qn=g(Ui,"Empty dict if doesn\u2019t exist"),Ui.forEach(a),Zn=c(Da),b(Le.$$.fragment,Da),Da.forEach(a),eo=c(I),te=o(I,"DIV",{class:!0});var ya=l(te);b(_t.$$.fragment,ya),to=c(ya),ns=o(ya,"P",{});var Gi=l(ns);ao=g(Gi,"Empty DatasetInfo if doesn\u2019t exist"),Gi.forEach(a),so=c(ya),b(Oe.$$.fragment,ya),ya.forEach(a),ro=c(I),Ae=o(I,"DIV",{class:!0});var Kr=l(Ae);b(ht.$$.fragment,Kr),no=c(Kr),os=o(Kr,"P",{});var Hi=l(os);oo=g(Hi,"Return the path of the module of this class or subclass."),Hi.forEach(a),Kr.forEach(a),I.forEach(a),Pr=c(s),J=o(s,"DIV",{class:!0});var ja=l(J);b($t.$$.fragment,ja),lo=c(ja),ls=o(ja,"P",{});var Wi=l(ls);io=g(Wi,"Base class for datasets with data generation based on dict generators."),Wi.forEach(a),po=c(ja),ae=o(ja,"P",{});var ta=l(ae);ds=o(ta,"CODE",{});var Xi=l(ds);co=g(Xi,"GeneratorBasedBuilder"),Xi.forEach(a),mo=g(ta,` is a convenience class that abstracts away much
of the data writing and reading of `),is=o(ta,"CODE",{});var Ji=l(is);go=g(Ji,"DatasetBuilder"),Ji.forEach(a),fo=g(ta,`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),ps=o(ta,"CODE",{});var Ki=l(ps);uo=g(Ki,"_split_generators"),Ki.forEach(a),_o=g(ta,"). See the method docstrings for details."),ta.forEach(a),ja.forEach(a),Cr=c(s),ve=o(s,"DIV",{class:!0});var Yr=l(ve);b(bt.$$.fragment,Yr),ho=c(Yr),cs=o(Yr,"P",{});var Yi=l(cs);$o=g(Yi,"Beam based Builder."),Yi.forEach(a),Yr.forEach(a),Lr=c(s),xe=o(s,"DIV",{class:!0});var Qr=l(xe);b(vt.$$.fragment,Qr),bo=c(Qr),ms=o(Qr,"P",{});var Qi=l(ms);vo=g(Qi,"Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),Qi.forEach(a),Qr.forEach(a),Or=c(s),z=o(s,"DIV",{class:!0});var dt=l(z);b(xt.$$.fragment,dt),xo=c(dt),wt=o(dt,"P",{});var Zr=l(wt);wo=g(Zr,"Base class for "),ia=o(Zr,"A",{href:!0});var Zi=l(ia);Eo=g(Zi,"DatasetBuilder"),Zi.forEach(a),Do=g(Zr," data configuration."),Zr.forEach(a),yo=c(dt),Et=o(dt,"P",{});var en=l(Et);jo=g(en,`DatasetBuilder subclasses with data configuration options should subclass
`),pa=o(en,"A",{href:!0});var ep=l(pa);ko=g(ep,"BuilderConfig"),ep.forEach(a),To=g(en," and add their own properties."),en.forEach(a),Io=c(dt),se=o(dt,"DIV",{class:!0});var ka=l(se);b(Dt.$$.fragment,ka),So=c(ka),gs=o(ka,"P",{});var tp=l(gs);Bo=g(tp,`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),tp.forEach(a),No=c(ka),we=o(ka,"UL",{});var Ta=l(we);fs=o(Ta,"LI",{});var ap=l(fs);Ro=g(ap,"the config kwargs that can be used to overwrite attributes"),ap.forEach(a),Po=c(Ta),us=o(Ta,"LI",{});var sp=l(us);Co=g(sp,"the custom features used to write the dataset"),sp.forEach(a),Lo=c(Ta),_s=o(Ta,"LI",{});var rp=l(_s);Oo=g(rp,`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),rp.forEach(a),Ta.forEach(a),ka.forEach(a),dt.forEach(a),Ar=c(s),R=o(s,"DIV",{class:!0});var O=l(R);b(yt.$$.fragment,O),Ao=c(O),re=o(O,"DIV",{class:!0});var Ia=l(re);b(jt.$$.fragment,Ia),Vo=c(Ia),hs=o(Ia,"P",{});var np=l(hs);qo=g(np,"Download given url(s)."),np.forEach(a),Mo=c(Ia),b(Ve.$$.fragment,Ia),Ia.forEach(a),Fo=c(O),ne=o(O,"DIV",{class:!0});var Sa=l(ne);b(kt.$$.fragment,Sa),zo=c(Sa),$s=o(Sa,"P",{});var op=l($s);Uo=g(op,"Download and extract given url_or_urls."),op.forEach(a),Go=c(Sa),b(qe.$$.fragment,Sa),Sa.forEach(a),Ho=c(O),oe=o(O,"DIV",{class:!0});var Ba=l(oe);b(Tt.$$.fragment,Ba),Wo=c(Ba),It=o(Ba,"P",{});var tn=l(It);Xo=g(tn,"Download given urls(s) by calling "),bs=o(tn,"CODE",{});var lp=l(bs);Jo=g(lp,"custom_download"),lp.forEach(a),Ko=g(tn,"."),tn.forEach(a),Yo=c(Ba),b(Me.$$.fragment,Ba),Ba.forEach(a),Qo=c(O),le=o(O,"DIV",{class:!0});var Na=l(le);b(St.$$.fragment,Na),Zo=c(Na),vs=o(Na,"P",{});var dp=l(vs);el=g(dp,"Extract given path(s)."),dp.forEach(a),tl=c(Na),b(Fe.$$.fragment,Na),Na.forEach(a),al=c(O),de=o(O,"DIV",{class:!0});var Ra=l(de);b(Bt.$$.fragment,Ra),sl=c(Ra),xs=o(Ra,"P",{});var ip=l(xs);rl=g(ip,"Iterate over files within an archive."),ip.forEach(a),nl=c(Ra),b(ze.$$.fragment,Ra),Ra.forEach(a),ol=c(O),ie=o(O,"DIV",{class:!0});var Pa=l(ie);b(Nt.$$.fragment,Pa),ll=c(Pa),ws=o(Pa,"P",{});var pp=l(ws);dl=g(pp,"Iterate over file paths."),pp.forEach(a),il=c(Pa),b(Ue.$$.fragment,Pa),Pa.forEach(a),pl=c(O),Ge=o(O,"DIV",{class:!0});var an=l(Ge);b(Rt.$$.fragment,an),cl=c(an),Es=o(an,"P",{});var cp=l(Es);ml=g(cp,"Ship the files using Beam FileSystems to the pipeline temp dir."),cp.forEach(a),an.forEach(a),O.forEach(a),Vr=c(s),C=o(s,"DIV",{class:!0});var M=l(C);b(Pt.$$.fragment,M),gl=c(M),U=o(M,"P",{});var _e=l(U);fl=g(_e,`Download manager that uses the \u201D::\u201D separator to navigate through (possibly remote) compressed archives.
Contrary to the regular DownloadManager, the `),Ds=o(_e,"CODE",{});var mp=l(Ds);ul=g(mp,"download"),mp.forEach(a),_l=g(_e," and "),ys=o(_e,"CODE",{});var gp=l(ys);hl=g(gp,"extract"),gp.forEach(a),$l=g(_e,` methods don\u2019t actually download nor extract
data, but they rather return the path or url that could be opened using the `),js=o(_e,"CODE",{});var fp=l(js);bl=g(fp,"xopen"),fp.forEach(a),vl=g(_e,` function which extends the
builtin `),ks=o(_e,"CODE",{});var up=l(ks);xl=g(up,"open"),up.forEach(a),wl=g(_e," function to stream data from remote files."),_e.forEach(a),El=c(M),pe=o(M,"DIV",{class:!0});var Ca=l(pe);b(Ct.$$.fragment,Ca),Dl=c(Ca),Ts=o(Ca,"P",{});var _p=l(Ts);yl=g(_p,"Download given url(s)."),_p.forEach(a),jl=c(Ca),b(He.$$.fragment,Ca),Ca.forEach(a),kl=c(M),ce=o(M,"DIV",{class:!0});var La=l(ce);b(Lt.$$.fragment,La),Tl=c(La),Is=o(La,"P",{});var hp=l(Is);Il=g(hp,"Download and extract given url_or_urls."),hp.forEach(a),Sl=c(La),b(We.$$.fragment,La),La.forEach(a),Bl=c(M),me=o(M,"DIV",{class:!0});var Oa=l(me);b(Ot.$$.fragment,Oa),Nl=c(Oa),Ss=o(Oa,"P",{});var $p=l(Ss);Rl=g($p,"Extract given path(s)."),$p.forEach(a),Pl=c(Oa),b(Xe.$$.fragment,Oa),Oa.forEach(a),Cl=c(M),ge=o(M,"DIV",{class:!0});var Aa=l(ge);b(At.$$.fragment,Aa),Ll=c(Aa),Bs=o(Aa,"P",{});var bp=l(Bs);Ol=g(bp,"Iterate over files within an archive."),bp.forEach(a),Al=c(Aa),b(Je.$$.fragment,Aa),Aa.forEach(a),Vl=c(M),fe=o(M,"DIV",{class:!0});var Va=l(fe);b(Vt.$$.fragment,Va),ql=c(Va),Ns=o(Va,"P",{});var vp=l(Ns);Ml=g(vp,"Iterate over files."),vp.forEach(a),Fl=c(Va),b(Ke.$$.fragment,Va),Va.forEach(a),M.forEach(a),qr=c(s),V=o(s,"DIV",{class:!0});var he=l(V);b(qt.$$.fragment,he),zl=c(he),ca=o(he,"P",{});var Ri=l(ca);Rs=o(Ri,"CODE",{});var xp=l(Rs);Ul=g(xp,"Enum"),xp.forEach(a),Gl=g(Ri," for how to treat pre-existing downloads and data."),Ri.forEach(a),Hl=c(he),Mt=o(he,"P",{});var sn=l(Mt);Wl=g(sn,"The default mode is "),Ps=o(sn,"CODE",{});var wp=l(Ps);Xl=g(wp,"REUSE_DATASET_IF_EXISTS"),wp.forEach(a),Jl=g(sn,`, which will reuse both
raw downloads and the prepared dataset if they exist.`),sn.forEach(a),Kl=c(he),Cs=o(he,"P",{});var Ep=l(Cs);Yl=g(Ep,"The generations modes:"),Ep.forEach(a),Ql=c(he),Ft=o(he,"TABLE",{});var rn=l(Ft);Ls=o(rn,"THEAD",{});var Dp=l(Ls);Ee=o(Dp,"TR",{});var qa=l(Ee);Mr=o(qa,"TH",{}),l(Mr).forEach(a),Zl=c(qa),Os=o(qa,"TH",{});var yp=l(Os);ed=g(yp,"Downloads"),yp.forEach(a),td=c(qa),As=o(qa,"TH",{});var jp=l(As);ad=g(jp,"Dataset"),jp.forEach(a),qa.forEach(a),Dp.forEach(a),sd=c(rn),De=o(rn,"TBODY",{});var Ma=l(De);ye=o(Ma,"TR",{});var Fa=l(ye);ma=o(Fa,"TD",{});var Pi=l(ma);Vs=o(Pi,"CODE",{});var kp=l(Vs);rd=g(kp,"REUSE_DATASET_IF_EXISTS"),kp.forEach(a),nd=g(Pi," (default)"),Pi.forEach(a),od=c(Fa),qs=o(Fa,"TD",{});var Tp=l(qs);ld=g(Tp,"Reuse"),Tp.forEach(a),dd=c(Fa),Ms=o(Fa,"TD",{});var Ip=l(Ms);id=g(Ip,"Reuse"),Ip.forEach(a),Fa.forEach(a),pd=c(Ma),je=o(Ma,"TR",{});var za=l(je);Fs=o(za,"TD",{});var Sp=l(Fs);zs=o(Sp,"CODE",{});var Bp=l(zs);cd=g(Bp,"REUSE_CACHE_IF_EXISTS"),Bp.forEach(a),Sp.forEach(a),md=c(za),Us=o(za,"TD",{});var Np=l(Us);gd=g(Np,"Reuse"),Np.forEach(a),fd=c(za),Gs=o(za,"TD",{});var Rp=l(Gs);ud=g(Rp,"Fresh"),Rp.forEach(a),za.forEach(a),_d=c(Ma),ke=o(Ma,"TR",{});var Ua=l(ke);Hs=o(Ua,"TD",{});var Pp=l(Hs);Ws=o(Pp,"CODE",{});var Cp=l(Ws);hd=g(Cp,"FORCE_REDOWNLOAD"),Cp.forEach(a),Pp.forEach(a),$d=c(Ua),Xs=o(Ua,"TD",{});var Lp=l(Xs);bd=g(Lp,"Fresh"),Lp.forEach(a),vd=c(Ua),Js=o(Ua,"TD",{});var Op=l(Js);xd=g(Op,"Fresh"),Op.forEach(a),Ua.forEach(a),Ma.forEach(a),rn.forEach(a),he.forEach(a),Fr=c(s),G=o(s,"DIV",{class:!0});var it=l(G);b(zt.$$.fragment,it),wd=c(it),Ks=o(it,"P",{});var Ap=l(Ks);Ed=g(Ap,"Defines the split information for the generator."),Ap.forEach(a),Dd=c(it),Te=o(it,"P",{});var Ga=l(Te);yd=g(Ga,`This should be used as returned value of
`),Ys=o(Ga,"CODE",{});var Vp=l(Ys);jd=g(Vp,"GeneratorBasedBuilder._split_generators()"),Vp.forEach(a),kd=g(Ga,`.
See `),Qs=o(Ga,"CODE",{});var qp=l(Qs);Td=g(qp,"GeneratorBasedBuilder._split_generators()"),qp.forEach(a),Id=g(Ga,` for more info and example
of usage.`),Ga.forEach(a),Sd=c(it),b(Ye.$$.fragment,it),it.forEach(a),zr=c(s),L=o(s,"DIV",{class:!0});var F=l(L);b(Ut.$$.fragment,F),Bd=c(F),ga=o(F,"P",{});var Ci=l(ga);Zs=o(Ci,"CODE",{});var Mp=l(Zs);Nd=g(Mp,"Enum"),Mp.forEach(a),Rd=g(Ci," for dataset splits."),Ci.forEach(a),Pd=c(F),er=o(F,"P",{});var Fp=l(er);Cd=g(Fp,`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),Fp.forEach(a),Ld=c(F),K=o(F,"UL",{});var pt=l(K);fa=o(pt,"LI",{});var Li=l(fa);tr=o(Li,"CODE",{});var zp=l(tr);Od=g(zp,"TRAIN"),zp.forEach(a),Ad=g(Li,": the training data."),Li.forEach(a),Vd=c(pt),ua=o(pt,"LI",{});var Oi=l(ua);ar=o(Oi,"CODE",{});var Up=l(ar);qd=g(Up,"VALIDATION"),Up.forEach(a),Md=g(Oi,`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Oi.forEach(a),Fd=c(pt),_a=o(pt,"LI",{});var Ai=l(_a);sr=o(Ai,"CODE",{});var Gp=l(sr);zd=g(Gp,"TEST"),Gp.forEach(a),Ud=g(Ai,`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Ai.forEach(a),Gd=c(pt),ha=o(pt,"LI",{});var Vi=l(ha);rr=o(Vi,"CODE",{});var Hp=l(rr);Hd=g(Hp,"ALL"),Hp.forEach(a),Wd=g(Vi,": the union of all defined dataset splits."),Vi.forEach(a),pt.forEach(a),Xd=c(F),$a=o(F,"P",{});var qi=l($a);Jd=g(qi,"Note: All splits, including compositions inherit from "),nr=o(qi,"CODE",{});var Wp=l(nr);Kd=g(Wp,"datasets.SplitBase"),Wp.forEach(a),qi.forEach(a),Yd=c(F),Gt=o(F,"P",{});var nn=l(Gt);Qd=g(nn,"See the :doc:"),or=o(nn,"CODE",{});var Xp=l(or);Zd=g(Xp,"guide on splits </loading>"),Xp.forEach(a),ei=g(nn," for more information."),nn.forEach(a),ti=c(F),b(Qe.$$.fragment,F),F.forEach(a),Ur=c(s),P=o(s,"DIV",{class:!0});var A=l(P);b(Ht.$$.fragment,A),ai=c(A),lr=o(A,"P",{});var Jp=l(lr);si=g(Jp,"Descriptor corresponding to a named split (train, test, \u2026)."),Jp.forEach(a),ri=c(A),b(Ze.$$.fragment,A),ni=c(A),dr=o(A,"P",{});var Kp=l(dr);oi=g(Kp,"Warning:"),Kp.forEach(a),li=c(A),b(et.$$.fragment,A),di=c(A),ir=o(A,"P",{});var Yp=l(ir);ii=g(Yp,"Warning:"),Yp.forEach(a),pi=c(A),b(tt.$$.fragment,A),ci=c(A),b(at.$$.fragment,A),A.forEach(a),Gr=c(s),Ie=o(s,"DIV",{class:!0});var on=l(Ie);b(Wt.$$.fragment,on),mi=c(on),pr=o(on,"P",{});var Qp=l(pr);gi=g(Qp,"Split corresponding to the union of all defined dataset splits."),Qp.forEach(a),on.forEach(a),Hr=c(s),q=o(s,"DIV",{class:!0});var $e=l(q);b(Xt.$$.fragment,$e),fi=c($e),cr=o($e,"P",{});var Zp=l(cr);ui=g(Zp,"Reading instruction for a dataset."),Zp.forEach(a),_i=c($e),b(st.$$.fragment,$e),hi=c($e),rt=o($e,"DIV",{class:!0});var ln=l(rt);b(Jt.$$.fragment,ln),$i=c(ln),mr=o(ln,"P",{});var ec=l(mr);bi=g(ec,"Creates a ReadInstruction instance out of a string spec."),ec.forEach(a),ln.forEach(a),vi=c($e),ue=o($e,"DIV",{class:!0});var Ha=l(ue);b(Kt.$$.fragment,Ha),xi=c(Ha),gr=o(Ha,"P",{});var tc=l(gr);wi=g(tc,"Translate instruction into a list of absolute instructions."),tc.forEach(a),Ei=c(Ha),fr=o(Ha,"P",{});var ac=l(fr);Di=g(ac,"Those absolute instructions are then to be added together."),ac.forEach(a),Ha.forEach(a),$e.forEach(a),Wr=c(s),Se=o(s,"DIV",{class:!0});var dn=l(Se);b(Yt.$$.fragment,dn),yi=c(dn),ur=o(dn,"P",{});var sc=l(ur);ji=g(sc,"Configuration for our cached path manager."),sc.forEach(a),dn.forEach(a),Xr=c(s),H=o(s,"DIV",{class:!0});var ct=l(H);b(Qt.$$.fragment,ct),ki=c(ct),_r=o(ct,"P",{});var rc=l(_r);Ti=g(rc,"Dataset version MAJOR.MINOR.PATCH."),rc.forEach(a),Ii=c(ct),b(nt.$$.fragment,ct),Si=c(ct),ot=o(ct,"DIV",{class:!0});var pn=l(ot);b(Zt.$$.fragment,pn),Bi=c(pn),hr=o(pn,"P",{});var nc=l(hr);Ni=g(nc,"Returns True if other_version matches."),nc.forEach(a),pn.forEach(a),ct.forEach(a),this.h()},h(){j(d,"name","hf:doc:metadata"),j(d,"content",JSON.stringify(Lc)),j(r,"id","datasets.DatasetBuilder"),j(r,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(r,"href","#datasets.DatasetBuilder"),j(f,"class","relative group"),j(aa,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.DatasetBuilder"),j(sa,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.BuilderConfig"),j(la,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.DatasetBuilder.download_and_prepare"),j(da,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.DatasetBuilder.as_dataset"),j(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(Ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(ia,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.DatasetBuilder"),j(pa,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.BuilderConfig"),j(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(Ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(rt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(Se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(ot,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(s,h){e(document.head,d),D(s,_,h),D(s,f,h),e(f,r),e(r,u),v(t,u,null),e(f,i),e(f,Wa),e(Wa,cn),D(s,Nr,h),D(s,Y,h),e(Y,mn),e(Y,aa),e(aa,gn),e(Y,fn),e(Y,sa),e(sa,un),e(Y,_n),D(s,Rr,h),D(s,T,h),v(mt,T,null),e(T,hn),e(T,Xa),e(Xa,$n),e(T,bn),e(T,ra),e(ra,Ja),e(Ja,vn),e(ra,xn),e(T,wn),e(T,be),e(be,na),e(na,Ka),e(Ka,En),e(na,Dn),e(be,yn),e(be,oa),e(oa,la),e(la,jn),e(oa,kn),e(be,Tn),e(be,Re),e(Re,da),e(da,In),e(Re,Sn),e(Re,Ya),e(Ya,Bn),e(Re,Nn),e(T,Rn),e(T,W),e(W,Qa),e(Qa,Pn),e(W,Cn),e(W,Za),e(Za,Ln),e(W,On),e(W,es),e(es,An),e(W,Vn),e(W,ts),e(ts,qn),e(W,Mn),e(T,Fn),e(T,Q),v(gt,Q,null),e(Q,zn),e(Q,as),e(as,Un),e(Q,Gn),v(Pe,Q,null),e(T,Hn),e(T,Z),v(ft,Z,null),e(Z,Wn),e(Z,ss),e(ss,Xn),e(Z,Jn),v(Ce,Z,null),e(T,Kn),e(T,ee),v(ut,ee,null),e(ee,Yn),e(ee,rs),e(rs,Qn),e(ee,Zn),v(Le,ee,null),e(T,eo),e(T,te),v(_t,te,null),e(te,to),e(te,ns),e(ns,ao),e(te,so),v(Oe,te,null),e(T,ro),e(T,Ae),v(ht,Ae,null),e(Ae,no),e(Ae,os),e(os,oo),D(s,Pr,h),D(s,J,h),v($t,J,null),e(J,lo),e(J,ls),e(ls,io),e(J,po),e(J,ae),e(ae,ds),e(ds,co),e(ae,mo),e(ae,is),e(is,go),e(ae,fo),e(ae,ps),e(ps,uo),e(ae,_o),D(s,Cr,h),D(s,ve,h),v(bt,ve,null),e(ve,ho),e(ve,cs),e(cs,$o),D(s,Lr,h),D(s,xe,h),v(vt,xe,null),e(xe,bo),e(xe,ms),e(ms,vo),D(s,Or,h),D(s,z,h),v(xt,z,null),e(z,xo),e(z,wt),e(wt,wo),e(wt,ia),e(ia,Eo),e(wt,Do),e(z,yo),e(z,Et),e(Et,jo),e(Et,pa),e(pa,ko),e(Et,To),e(z,Io),e(z,se),v(Dt,se,null),e(se,So),e(se,gs),e(gs,Bo),e(se,No),e(se,we),e(we,fs),e(fs,Ro),e(we,Po),e(we,us),e(us,Co),e(we,Lo),e(we,_s),e(_s,Oo),D(s,Ar,h),D(s,R,h),v(yt,R,null),e(R,Ao),e(R,re),v(jt,re,null),e(re,Vo),e(re,hs),e(hs,qo),e(re,Mo),v(Ve,re,null),e(R,Fo),e(R,ne),v(kt,ne,null),e(ne,zo),e(ne,$s),e($s,Uo),e(ne,Go),v(qe,ne,null),e(R,Ho),e(R,oe),v(Tt,oe,null),e(oe,Wo),e(oe,It),e(It,Xo),e(It,bs),e(bs,Jo),e(It,Ko),e(oe,Yo),v(Me,oe,null),e(R,Qo),e(R,le),v(St,le,null),e(le,Zo),e(le,vs),e(vs,el),e(le,tl),v(Fe,le,null),e(R,al),e(R,de),v(Bt,de,null),e(de,sl),e(de,xs),e(xs,rl),e(de,nl),v(ze,de,null),e(R,ol),e(R,ie),v(Nt,ie,null),e(ie,ll),e(ie,ws),e(ws,dl),e(ie,il),v(Ue,ie,null),e(R,pl),e(R,Ge),v(Rt,Ge,null),e(Ge,cl),e(Ge,Es),e(Es,ml),D(s,Vr,h),D(s,C,h),v(Pt,C,null),e(C,gl),e(C,U),e(U,fl),e(U,Ds),e(Ds,ul),e(U,_l),e(U,ys),e(ys,hl),e(U,$l),e(U,js),e(js,bl),e(U,vl),e(U,ks),e(ks,xl),e(U,wl),e(C,El),e(C,pe),v(Ct,pe,null),e(pe,Dl),e(pe,Ts),e(Ts,yl),e(pe,jl),v(He,pe,null),e(C,kl),e(C,ce),v(Lt,ce,null),e(ce,Tl),e(ce,Is),e(Is,Il),e(ce,Sl),v(We,ce,null),e(C,Bl),e(C,me),v(Ot,me,null),e(me,Nl),e(me,Ss),e(Ss,Rl),e(me,Pl),v(Xe,me,null),e(C,Cl),e(C,ge),v(At,ge,null),e(ge,Ll),e(ge,Bs),e(Bs,Ol),e(ge,Al),v(Je,ge,null),e(C,Vl),e(C,fe),v(Vt,fe,null),e(fe,ql),e(fe,Ns),e(Ns,Ml),e(fe,Fl),v(Ke,fe,null),D(s,qr,h),D(s,V,h),v(qt,V,null),e(V,zl),e(V,ca),e(ca,Rs),e(Rs,Ul),e(ca,Gl),e(V,Hl),e(V,Mt),e(Mt,Wl),e(Mt,Ps),e(Ps,Xl),e(Mt,Jl),e(V,Kl),e(V,Cs),e(Cs,Yl),e(V,Ql),e(V,Ft),e(Ft,Ls),e(Ls,Ee),e(Ee,Mr),e(Ee,Zl),e(Ee,Os),e(Os,ed),e(Ee,td),e(Ee,As),e(As,ad),e(Ft,sd),e(Ft,De),e(De,ye),e(ye,ma),e(ma,Vs),e(Vs,rd),e(ma,nd),e(ye,od),e(ye,qs),e(qs,ld),e(ye,dd),e(ye,Ms),e(Ms,id),e(De,pd),e(De,je),e(je,Fs),e(Fs,zs),e(zs,cd),e(je,md),e(je,Us),e(Us,gd),e(je,fd),e(je,Gs),e(Gs,ud),e(De,_d),e(De,ke),e(ke,Hs),e(Hs,Ws),e(Ws,hd),e(ke,$d),e(ke,Xs),e(Xs,bd),e(ke,vd),e(ke,Js),e(Js,xd),D(s,Fr,h),D(s,G,h),v(zt,G,null),e(G,wd),e(G,Ks),e(Ks,Ed),e(G,Dd),e(G,Te),e(Te,yd),e(Te,Ys),e(Ys,jd),e(Te,kd),e(Te,Qs),e(Qs,Td),e(Te,Id),e(G,Sd),v(Ye,G,null),D(s,zr,h),D(s,L,h),v(Ut,L,null),e(L,Bd),e(L,ga),e(ga,Zs),e(Zs,Nd),e(ga,Rd),e(L,Pd),e(L,er),e(er,Cd),e(L,Ld),e(L,K),e(K,fa),e(fa,tr),e(tr,Od),e(fa,Ad),e(K,Vd),e(K,ua),e(ua,ar),e(ar,qd),e(ua,Md),e(K,Fd),e(K,_a),e(_a,sr),e(sr,zd),e(_a,Ud),e(K,Gd),e(K,ha),e(ha,rr),e(rr,Hd),e(ha,Wd),e(L,Xd),e(L,$a),e($a,Jd),e($a,nr),e(nr,Kd),e(L,Yd),e(L,Gt),e(Gt,Qd),e(Gt,or),e(or,Zd),e(Gt,ei),e(L,ti),v(Qe,L,null),D(s,Ur,h),D(s,P,h),v(Ht,P,null),e(P,ai),e(P,lr),e(lr,si),e(P,ri),v(Ze,P,null),e(P,ni),e(P,dr),e(dr,oi),e(P,li),v(et,P,null),e(P,di),e(P,ir),e(ir,ii),e(P,pi),v(tt,P,null),e(P,ci),v(at,P,null),D(s,Gr,h),D(s,Ie,h),v(Wt,Ie,null),e(Ie,mi),e(Ie,pr),e(pr,gi),D(s,Hr,h),D(s,q,h),v(Xt,q,null),e(q,fi),e(q,cr),e(cr,ui),e(q,_i),v(st,q,null),e(q,hi),e(q,rt),v(Jt,rt,null),e(rt,$i),e(rt,mr),e(mr,bi),e(q,vi),e(q,ue),v(Kt,ue,null),e(ue,xi),e(ue,gr),e(gr,wi),e(ue,Ei),e(ue,fr),e(fr,Di),D(s,Wr,h),D(s,Se,h),v(Yt,Se,null),e(Se,yi),e(Se,ur),e(ur,ji),D(s,Xr,h),D(s,H,h),v(Qt,H,null),e(H,ki),e(H,_r),e(_r,Ti),e(H,Ii),v(nt,H,null),e(H,Si),e(H,ot),v(Zt,ot,null),e(ot,Bi),e(ot,hr),e(hr,Ni),Jr=!0},p(s,[h]){const ea={};h&2&&(ea.$$scope={dirty:h,ctx:s}),Pe.$set(ea);const $r={};h&2&&($r.$$scope={dirty:h,ctx:s}),Ce.$set($r);const br={};h&2&&(br.$$scope={dirty:h,ctx:s}),Le.$set(br);const vr={};h&2&&(vr.$$scope={dirty:h,ctx:s}),Oe.$set(vr);const Be={};h&2&&(Be.$$scope={dirty:h,ctx:s}),Ve.$set(Be);const xr={};h&2&&(xr.$$scope={dirty:h,ctx:s}),qe.$set(xr);const wr={};h&2&&(wr.$$scope={dirty:h,ctx:s}),Me.$set(wr);const I={};h&2&&(I.$$scope={dirty:h,ctx:s}),Fe.$set(I);const Er={};h&2&&(Er.$$scope={dirty:h,ctx:s}),ze.$set(Er);const ba={};h&2&&(ba.$$scope={dirty:h,ctx:s}),Ue.$set(ba);const Dr={};h&2&&(Dr.$$scope={dirty:h,ctx:s}),He.$set(Dr);const Ne={};h&2&&(Ne.$$scope={dirty:h,ctx:s}),We.$set(Ne);const va={};h&2&&(va.$$scope={dirty:h,ctx:s}),Xe.$set(va);const yr={};h&2&&(yr.$$scope={dirty:h,ctx:s}),Je.$set(yr);const xa={};h&2&&(xa.$$scope={dirty:h,ctx:s}),Ke.$set(xa);const jr={};h&2&&(jr.$$scope={dirty:h,ctx:s}),Ye.$set(jr);const lt={};h&2&&(lt.$$scope={dirty:h,ctx:s}),Qe.$set(lt);const kr={};h&2&&(kr.$$scope={dirty:h,ctx:s}),Ze.$set(kr);const Tr={};h&2&&(Tr.$$scope={dirty:h,ctx:s}),et.$set(Tr);const X={};h&2&&(X.$$scope={dirty:h,ctx:s}),tt.$set(X);const Ir={};h&2&&(Ir.$$scope={dirty:h,ctx:s}),at.$set(Ir);const Sr={};h&2&&(Sr.$$scope={dirty:h,ctx:s}),st.$set(Sr);const Br={};h&2&&(Br.$$scope={dirty:h,ctx:s}),nt.$set(Br)},i(s){Jr||(x(t.$$.fragment,s),x(mt.$$.fragment,s),x(gt.$$.fragment,s),x(Pe.$$.fragment,s),x(ft.$$.fragment,s),x(Ce.$$.fragment,s),x(ut.$$.fragment,s),x(Le.$$.fragment,s),x(_t.$$.fragment,s),x(Oe.$$.fragment,s),x(ht.$$.fragment,s),x($t.$$.fragment,s),x(bt.$$.fragment,s),x(vt.$$.fragment,s),x(xt.$$.fragment,s),x(Dt.$$.fragment,s),x(yt.$$.fragment,s),x(jt.$$.fragment,s),x(Ve.$$.fragment,s),x(kt.$$.fragment,s),x(qe.$$.fragment,s),x(Tt.$$.fragment,s),x(Me.$$.fragment,s),x(St.$$.fragment,s),x(Fe.$$.fragment,s),x(Bt.$$.fragment,s),x(ze.$$.fragment,s),x(Nt.$$.fragment,s),x(Ue.$$.fragment,s),x(Rt.$$.fragment,s),x(Pt.$$.fragment,s),x(Ct.$$.fragment,s),x(He.$$.fragment,s),x(Lt.$$.fragment,s),x(We.$$.fragment,s),x(Ot.$$.fragment,s),x(Xe.$$.fragment,s),x(At.$$.fragment,s),x(Je.$$.fragment,s),x(Vt.$$.fragment,s),x(Ke.$$.fragment,s),x(qt.$$.fragment,s),x(zt.$$.fragment,s),x(Ye.$$.fragment,s),x(Ut.$$.fragment,s),x(Qe.$$.fragment,s),x(Ht.$$.fragment,s),x(Ze.$$.fragment,s),x(et.$$.fragment,s),x(tt.$$.fragment,s),x(at.$$.fragment,s),x(Wt.$$.fragment,s),x(Xt.$$.fragment,s),x(st.$$.fragment,s),x(Jt.$$.fragment,s),x(Kt.$$.fragment,s),x(Yt.$$.fragment,s),x(Qt.$$.fragment,s),x(nt.$$.fragment,s),x(Zt.$$.fragment,s),Jr=!0)},o(s){w(t.$$.fragment,s),w(mt.$$.fragment,s),w(gt.$$.fragment,s),w(Pe.$$.fragment,s),w(ft.$$.fragment,s),w(Ce.$$.fragment,s),w(ut.$$.fragment,s),w(Le.$$.fragment,s),w(_t.$$.fragment,s),w(Oe.$$.fragment,s),w(ht.$$.fragment,s),w($t.$$.fragment,s),w(bt.$$.fragment,s),w(vt.$$.fragment,s),w(xt.$$.fragment,s),w(Dt.$$.fragment,s),w(yt.$$.fragment,s),w(jt.$$.fragment,s),w(Ve.$$.fragment,s),w(kt.$$.fragment,s),w(qe.$$.fragment,s),w(Tt.$$.fragment,s),w(Me.$$.fragment,s),w(St.$$.fragment,s),w(Fe.$$.fragment,s),w(Bt.$$.fragment,s),w(ze.$$.fragment,s),w(Nt.$$.fragment,s),w(Ue.$$.fragment,s),w(Rt.$$.fragment,s),w(Pt.$$.fragment,s),w(Ct.$$.fragment,s),w(He.$$.fragment,s),w(Lt.$$.fragment,s),w(We.$$.fragment,s),w(Ot.$$.fragment,s),w(Xe.$$.fragment,s),w(At.$$.fragment,s),w(Je.$$.fragment,s),w(Vt.$$.fragment,s),w(Ke.$$.fragment,s),w(qt.$$.fragment,s),w(zt.$$.fragment,s),w(Ye.$$.fragment,s),w(Ut.$$.fragment,s),w(Qe.$$.fragment,s),w(Ht.$$.fragment,s),w(Ze.$$.fragment,s),w(et.$$.fragment,s),w(tt.$$.fragment,s),w(at.$$.fragment,s),w(Wt.$$.fragment,s),w(Xt.$$.fragment,s),w(st.$$.fragment,s),w(Jt.$$.fragment,s),w(Kt.$$.fragment,s),w(Yt.$$.fragment,s),w(Qt.$$.fragment,s),w(nt.$$.fragment,s),w(Zt.$$.fragment,s),Jr=!1},d(s){a(d),s&&a(_),s&&a(f),E(t),s&&a(Nr),s&&a(Y),s&&a(Rr),s&&a(T),E(mt),E(gt),E(Pe),E(ft),E(Ce),E(ut),E(Le),E(_t),E(Oe),E(ht),s&&a(Pr),s&&a(J),E($t),s&&a(Cr),s&&a(ve),E(bt),s&&a(Lr),s&&a(xe),E(vt),s&&a(Or),s&&a(z),E(xt),E(Dt),s&&a(Ar),s&&a(R),E(yt),E(jt),E(Ve),E(kt),E(qe),E(Tt),E(Me),E(St),E(Fe),E(Bt),E(ze),E(Nt),E(Ue),E(Rt),s&&a(Vr),s&&a(C),E(Pt),E(Ct),E(He),E(Lt),E(We),E(Ot),E(Xe),E(At),E(Je),E(Vt),E(Ke),s&&a(qr),s&&a(V),E(qt),s&&a(Fr),s&&a(G),E(zt),E(Ye),s&&a(zr),s&&a(L),E(Ut),E(Qe),s&&a(Ur),s&&a(P),E(Ht),E(Ze),E(et),E(tt),E(at),s&&a(Gr),s&&a(Ie),E(Wt),s&&a(Hr),s&&a(q),E(Xt),E(st),E(Jt),E(Kt),s&&a(Wr),s&&a(Se),E(Yt),s&&a(Xr),s&&a(H),E(Qt),E(nt),E(Zt)}}}const Lc={local:"datasets.DatasetBuilder",title:"Builder classes"};function Oc(y){return pc(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class zc extends oc{constructor(d){super();lc(this,d,Oc,Cc,dc,{})}}export{zc as default,Lc as metadata};
