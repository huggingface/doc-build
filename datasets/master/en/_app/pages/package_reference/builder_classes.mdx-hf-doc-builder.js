import{S as tm,i as am,s as sm,e as r,k as p,w as v,t as m,M as rm,c as n,d as a,m as c,a as o,x as b,h as f,b as y,G as e,g as $,y as x,q as w,o as E,B as D,v as nm,L as B}from"../../chunks/vendor-hf-doc-builder.js";import{D as j}from"../../chunks/Docstring-hf-doc-builder.js";import{C as N}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Zr}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as I}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function om(k){let d,h,g,l,u;return l=new N({props:{code:`from datasets import load_dataset_builder
builder = load_dataset_builder('rotten_tomatoes')
ds = builder.download_and_prepare()
ds = builder.as_dataset(split='train')
ds`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.download_and_prepare()
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.as_dataset(split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds
Dataset({
    features: [<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
    num_rows: <span class="hljs-number">8530</span>
})`}}),{c(){d=r("p"),h=m("Example:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"Example:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function lm(k){let d,h,g,l,u;return l=new N({props:{code:`from datasets import load_dataset_builder
builder = load_dataset_builder('rotten_tomatoes')
ds = builder.download_and_prepare()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.download_and_prepare()`}}),{c(){d=r("p"),h=m("Example:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"Example:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function dm(k){let d,h,g,l,u;return l=new N({props:{code:`from datasets import load_dataset_builder
ds_builder = load_dataset_builder('rotten_tomatoes')
ds_builder.get_all_exported_dataset_infos()
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder.get_all_exported_dataset_infos()
{<span class="hljs-string">&#x27;default&#x27;</span>: DatasetInfo(description=<span class="hljs-string">&quot;Movie Review Dataset.
a dataset of containing 5,331 positive and 5,331 negative processed
s from Rotten Tomatoes movie reviews. This data was first used in Bo
 Lillian Lee, \`\`Seeing stars: Exploiting class relationships for
t categorization with respect to rating scales.&#x27;&#x27;, Proceedings of the
5.
ion=&#x27;@InProceedings{Pang+Lee:05a,
 =       {Bo Pang and Lillian Lee},
=        {Seeing stars: Exploiting class relationships for sentiment
          categorization with respect to rating scales},
tle =    {Proceedings of the ACL},
         2005

age=&#x27;http://www.cs.cornell.edu/people/pabo/movie-review-data/&#x27;, license=&#x27;&#x27;, features={&#x27;text&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;label&#x27;: ClassLabel(num_classes=2, names=[&#x27;neg&#x27;, &#x27;pos&#x27;], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input=&#x27;&#x27;, output=&#x27;&#x27;), task_templates=[TextClassification(task=&#x27;text-classification&#x27;, text_column=&#x27;text&#x27;, label_column=&#x27;label&#x27;)], builder_name=&#x27;rotten_tomatoes_movie_review&#x27;, config_name=&#x27;default&#x27;, version=1.0.0, splits={&#x27;train&#x27;: SplitInfo(name=&#x27;train&#x27;, num_bytes=1074810, num_examples=8530, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;validation&#x27;: SplitInfo(name=&#x27;validation&#x27;, num_bytes=134679, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;test&#x27;: SplitInfo(name=&#x27;test&#x27;, num_bytes=135972, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;)}, download_checksums={&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;: {&#x27;num_bytes&#x27;: 487770, &#x27;checksum&#x27;: &#x27;a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9&#x27;}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)}</span>`}}),{c(){d=r("p"),h=m("Example:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"Example:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function im(k){let d,h,g,l,u;return l=new N({props:{code:`from datasets import load_dataset_builder
ds_builder = load_dataset_builder('rotten_tomatoes')
ds_builder.get_exported_dataset_info()
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder.get_exported_dataset_info()
DatasetInfo(description=<span class="hljs-string">&quot;Movie Review Dataset.
a dataset of containing 5,331 positive and 5,331 negative processed
s from Rotten Tomatoes movie reviews. This data was first used in Bo
 Lillian Lee, \`\`Seeing stars: Exploiting class relationships for
t categorization with respect to rating scales.&#x27;&#x27;, Proceedings of the
5.
ion=&#x27;@InProceedings{Pang+Lee:05a,
 =       {Bo Pang and Lillian Lee},
=        {Seeing stars: Exploiting class relationships for sentiment
          categorization with respect to rating scales},
tle =    {Proceedings of the ACL},
         2005

age=&#x27;http://www.cs.cornell.edu/people/pabo/movie-review-data/&#x27;, license=&#x27;&#x27;, features={&#x27;text&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;label&#x27;: ClassLabel(num_classes=2, names=[&#x27;neg&#x27;, &#x27;pos&#x27;], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input=&#x27;&#x27;, output=&#x27;&#x27;), task_templates=[TextClassification(task=&#x27;text-classification&#x27;, text_column=&#x27;text&#x27;, label_column=&#x27;label&#x27;)], builder_name=&#x27;rotten_tomatoes_movie_review&#x27;, config_name=&#x27;default&#x27;, version=1.0.0, splits={&#x27;train&#x27;: SplitInfo(name=&#x27;train&#x27;, num_bytes=1074810, num_examples=8530, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;validation&#x27;: SplitInfo(name=&#x27;validation&#x27;, num_bytes=134679, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;test&#x27;: SplitInfo(name=&#x27;test&#x27;, num_bytes=135972, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;)}, download_checksums={&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;: {&#x27;num_bytes&#x27;: 487770, &#x27;checksum&#x27;: &#x27;a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9&#x27;}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)</span>`}}),{c(){d=r("p"),h=m("Example:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"Example:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function pm(k){let d,h,g,l,u;return l=new N({props:{code:"downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)'}}),{c(){d=r("p"),h=m("Example:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"Example:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function cm(k){let d,h,g,l,u;return l=new N({props:{code:"extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))",highlighted:'<span class="hljs-attr">extracted_paths</span> = dl_manager.extract(dl_manager.download(url_or_urls))'}}),{c(){d=r("p"),h=m("Is roughly equivalent to:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"Is roughly equivalent to:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function mm(k){let d,h,g,l,u;return l=new N({props:{code:"downloaded_files = dl_manager.download_custom('s3://my-bucket/data.zip', custom_download_for_my_private_bucket)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download_custom(<span class="hljs-string">&#x27;s3://my-bucket/data.zip&#x27;</span>, custom_download_for_my_private_bucket)'}}),{c(){d=r("p"),h=m("Example:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"Example:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function fm(k){let d,h,g,l,u;return l=new N({props:{code:`downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
extracted_files = dl_manager.extract(downloaded_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>extracted_files = dl_manager.extract(downloaded_files)`}}),{c(){d=r("p"),h=m("Example:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"Example:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function gm(k){let d,h,g,l,u;return l=new N({props:{code:`archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
files = dl_manager.iter_archive(archive)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>archive = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_archive(archive)`}}),{c(){d=r("p"),h=m("Example:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"Example:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function um(k){let d,h,g,l,u;return l=new N({props:{code:`files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')
files = dl_manager.iter_files(files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.download_and_extract(<span class="hljs-string">&#x27;https://huggingface.co/datasets/beans/resolve/main/data/train.zip&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_files(files)`}}),{c(){d=r("p"),h=m("Example:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"Example:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function hm(k){let d,h,g,l,u;return l=new N({props:{code:"downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)'}}),{c(){d=r("p"),h=m("Example:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"Example:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function _m(k){let d,h,g,l,u;return l=new N({props:{code:"extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))",highlighted:'<span class="hljs-attr">extracted_paths</span> = dl_manager.extract(dl_manager.download(url_or_urls))'}}),{c(){d=r("p"),h=m("Is roughly equivalent to:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"Is roughly equivalent to:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function $m(k){let d,h,g,l,u;return l=new N({props:{code:`downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
extracted_files = dl_manager.extract(downloaded_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>extracted_files = dl_manager.extract(downloaded_files)`}}),{c(){d=r("p"),h=m("Example:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"Example:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function vm(k){let d,h,g,l,u;return l=new N({props:{code:`archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
files = dl_manager.iter_archive(archive)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>archive = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_archive(archive)`}}),{c(){d=r("p"),h=m("Example:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"Example:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function bm(k){let d,h,g,l,u;return l=new N({props:{code:`files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')
files = dl_manager.iter_files(files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.download_and_extract(<span class="hljs-string">&#x27;https://huggingface.co/datasets/beans/resolve/main/data/train.zip&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_files(files)`}}),{c(){d=r("p"),h=m("Example:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"Example:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function xm(k){let d,h,g,l,u;return l=new N({props:{code:`datasets.SplitGenerator(
    name=datasets.Split.TRAIN,
    gen_kwargs={"split_key": "train", "files": dl_manager.download_and_extract(url)},
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TRAIN,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and_extract(url)},
<span class="hljs-meta">... </span>)`}}),{c(){d=r("p"),h=m("Example:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"Example:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function wm(k){let d,h,g,l,u;return l=new N({props:{code:`datasets.SplitGenerator(
    name=datasets.Split.TRAIN,
    gen_kwargs={"split_key": "train", "files": dl_manager.download_and extract(url)},
),
datasets.SplitGenerator(
    name=datasets.Split.VALIDATION,
    gen_kwargs={"split_key": "validation", "files": dl_manager.download_and extract(url)},
),
datasets.SplitGenerator(
    name=datasets.Split.TEST,
    gen_kwargs={"split_key": "test", "files": dl_manager.download_and extract(url)},
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TRAIN,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and extract(url)},
<span class="hljs-meta">... </span>),
<span class="hljs-meta">... </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.VALIDATION,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;validation&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and extract(url)},
<span class="hljs-meta">... </span>),
<span class="hljs-meta">... </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TEST,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;test&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and extract(url)},
<span class="hljs-meta">... </span>)`}}),{c(){d=r("p"),h=m("Example:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"Example:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function Em(k){let d,h,g,l,u;return l=new N({props:{code:`Each descriptor can be composed with other using addition or slice. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[0:25]) + datasets.Split.TEST

The resulting split will correspond to 25% of the train split merged with
100% of the test split.`,highlighted:`Each descriptor can be composed <span class="hljs-keyword">with</span> other using addition <span class="hljs-keyword">or</span> <span class="hljs-built_in">slice</span>. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">0</span>:<span class="hljs-number">25</span>]) + datasets.Split.TEST

The resulting split will correspond to <span class="hljs-number">25</span>% of the train split merged <span class="hljs-keyword">with</span>
<span class="hljs-number">100</span>% of the test split.`}}),{c(){d=r("p"),h=m("Example:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"Example:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function Dm(k){let d,h,g,l,u;return l=new N({props:{code:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[75:])
)  # Error
split = datasets.Split.TEST + datasets.Split.ALL  # Error`,highlighted:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">75</span>:])
)  <span class="hljs-comment"># Error</span>
split = datasets.Split.TEST + datasets.Split.ALL  <span class="hljs-comment"># Error</span>`}}),{c(){d=r("p"),h=m("A split cannot be added twice, so the following will fail:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"A split cannot be added twice, so the following will fail:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function ym(k){let d,h,g,l,u;return l=new N({props:{code:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
datasets.Split.TEST.subsplit(datasets.percent[:50])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:50])`,highlighted:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
datasets.Split.TEST.subsplit(datasets.percent[:<span class="hljs-number">50</span>])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),{c(){d=r("p"),h=m("The slices can be applied only one time. So the following are valid:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"The slices can be applied only one time. So the following are valid:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function km(k){let d,h,g,l,u;return l=new N({props:{code:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:25]).subsplit(datasets.percent[:25])
split = (train.subsplit(datasets.percent[:25]) + test).subsplit(datasets.percent[:50])`,highlighted:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]).subsplit(datasets.percent[:<span class="hljs-number">25</span>])
split = (train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) + test).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),{c(){d=r("p"),h=m("But not:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"But not:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function jm(k){let d,h,g,l,u;return l=new N({props:{code:`# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec('test[:33%]'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction('test', to=33, unit='%'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%'))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]+train[1:-1]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%]+train[1:-1]'))
ds = datasets.load_dataset('mnist', split=(
datasets.ReadInstruction('test', to=33, unit='%') +
datasets.ReadInstruction('train', from_=1, to=-1, unit='abs')))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%](pct1_dropremainder)')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%](pct1_dropremainder)'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%', rounding="pct1_dropremainder"))

# 10-fold validation:
tests = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', from_=k, to=k+10, unit='%')
for k in range(0, 100, 10)])
trains = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')
for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(<span class="hljs-string">&#x27;test[:33%]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=(
datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) +
datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">1</span>, to=-<span class="hljs-number">1</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>)))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&quot;pct1_dropremainder&quot;</span>))

<span class="hljs-comment"># 10-fold validation:</span>
tests = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
trains = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}),{c(){d=r("p"),h=m("Examples:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"Examples:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function Sm(k){let d,h,g,l,u;return l=new N({props:{code:'VERSION = datasets.Version("1.0.0")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>VERSION = datasets.Version(<span class="hljs-string">&quot;1.0.0&quot;</span>)'}}),{c(){d=r("p"),h=m("Example:"),g=p(),v(l.$$.fragment)},l(t){d=n(t,"P",{});var i=o(d);h=f(i,"Example:"),i.forEach(a),g=c(t),b(l.$$.fragment,t)},m(t,i){$(t,d,i),e(d,h),$(t,g,i),x(l,t,i),u=!0},p:B,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){E(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(g),D(l,t)}}}function Tm(k){let d,h,g,l,u,t,i,os,Mn,en,$e,Le,ls,bt,qn,ds,Fn,tn,K,zn,ua,Un,Gn,ha,Hn,Wn,an,S,xt,Xn,is,Jn,Kn,_a,ps,Yn,Qn,Zn,ve,$a,cs,eo,to,ao,va,ba,so,ro,no,Oe,xa,oo,lo,ms,io,po,co,W,fs,mo,fo,gs,go,uo,us,ho,_o,hs,$o,vo,bo,Y,wt,xo,_s,wo,Eo,Ve,Do,Q,Et,yo,$s,ko,jo,Me,So,Z,Dt,To,vs,Io,Bo,qe,No,ee,yt,Po,bs,Ro,Ao,Fe,Co,ze,kt,Lo,xs,Oo,sn,X,jt,Vo,ws,Mo,qo,te,Es,Fo,zo,Ds,Uo,Go,ys,Ho,Wo,rn,be,St,Xo,ks,Jo,nn,xe,Tt,Ko,js,Yo,on,z,It,Qo,Bt,Zo,wa,el,tl,al,Nt,sl,Ea,rl,nl,ol,ae,Pt,ll,Ss,dl,il,we,Ts,pl,cl,Is,ml,fl,Bs,gl,ln,Ee,Ue,Ns,Rt,ul,Ps,hl,dn,P,At,_l,se,Ct,$l,Rs,vl,bl,Ge,xl,re,Lt,wl,As,El,Dl,He,yl,ne,Ot,kl,Vt,jl,Cs,Sl,Tl,Il,We,Bl,oe,Mt,Nl,Ls,Pl,Rl,Xe,Al,le,qt,Cl,Os,Ll,Ol,Je,Vl,de,Ft,Ml,Vs,ql,Fl,Ke,zl,Ye,zt,Ul,Ms,Gl,pn,A,Ut,Hl,U,Wl,qs,Xl,Jl,Fs,Kl,Yl,zs,Ql,Zl,Us,ed,td,ad,ie,Gt,sd,Gs,rd,nd,Qe,od,pe,Ht,ld,Hs,dd,id,Ze,pd,ce,Wt,cd,Ws,md,fd,et,gd,me,Xt,ud,Xs,hd,_d,tt,$d,fe,Jt,vd,Js,bd,xd,at,cn,De,Kt,wd,Ks,Ed,mn,V,Yt,Dd,Da,Ys,yd,kd,jd,Qt,Sd,Qs,Td,Id,Bd,Zs,Nd,Pd,Zt,er,ye,fn,Rd,tr,Ad,Cd,ar,Ld,Od,ke,je,ya,sr,Vd,Md,qd,rr,Fd,zd,nr,Ud,Gd,Se,or,lr,Hd,Wd,dr,Xd,Jd,ir,Kd,Yd,Te,pr,cr,Qd,Zd,mr,ei,ti,fr,ai,gn,Ie,st,gr,ea,si,ur,ri,un,G,ta,ni,hr,oi,li,Be,di,_r,ii,pi,$r,ci,mi,fi,rt,hn,C,aa,gi,ka,vr,ui,hi,_i,br,$i,vi,J,ja,xr,bi,xi,wi,Sa,wr,Ei,Di,yi,Ta,Er,ki,ji,Si,Ia,Dr,Ti,Ii,Bi,Ba,Ni,yr,Pi,Ri,sa,Ai,kr,Ci,Li,Oi,nt,_n,R,ra,Vi,jr,Mi,qi,ot,Fi,Sr,zi,Ui,lt,Gi,Tr,Hi,Wi,dt,Xi,it,$n,Ne,na,Ji,Ir,Ki,vn,M,oa,Yi,Br,Qi,Zi,pt,ep,ct,la,tp,Nr,ap,sp,ge,da,rp,Pr,np,op,Rr,lp,bn,Pe,mt,Ar,ia,dp,Cr,ip,xn,H,pa,pp,Lr,cp,mp,ft,fp,gt,ca,gp,Or,up,wn;return t=new Zr({}),bt=new Zr({}),xt=new j({props:{name:"class datasets.DatasetBuilder",anchor:"datasets.DatasetBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"repo_id",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L178"}}),wt=new j({props:{name:"as_dataset",anchor:"datasets.DatasetBuilder.as_dataset",parameters:[{name:"split",val:": typing.Optional[datasets.splits.Split] = None"},{name:"run_post_process",val:" = True"},{name:"ignore_verifications",val:" = False"},{name:"in_memory",val:" = False"}],parametersDescription:[{anchor:"datasets.DatasetBuilder.as_dataset.split",description:"<strong>split</strong> (<code>datasets.Split</code>) &#x2014; Which subset of the data to return.",name:"split"},{anchor:"datasets.DatasetBuilder.as_dataset.run_post_process",description:`<strong>run_post_process</strong> (bool, default=True) &#x2014; Whether to run post-processing dataset transforms and/or add
indexes.`,name:"run_post_process"},{anchor:"datasets.DatasetBuilder.as_dataset.ignore_verifications",description:`<strong>ignore_verifications</strong> (bool, default=False) &#x2014; Whether to ignore the verifications of the
downloaded/processed dataset information (checksums/size/splits/&#x2026;).`,name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.as_dataset.in_memory",description:"<strong>in_memory</strong> (bool, default=False) &#x2014; Whether to copy the data in-memory.",name:"in_memory"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L780",returnDescription:`
<p>datasets.Dataset</p>
`}}),Ve=new I({props:{anchor:"datasets.DatasetBuilder.as_dataset.example",$$slots:{default:[om]},$$scope:{ctx:k}}}),Et=new j({props:{name:"download_and_prepare",anchor:"datasets.DatasetBuilder.download_and_prepare",parameters:[{name:"download_config",val:": typing.Optional[datasets.download.download_config.DownloadConfig] = None"},{name:"download_mode",val:": typing.Optional[datasets.download.download_manager.DownloadMode] = None"},{name:"ignore_verifications",val:": bool = False"},{name:"try_from_hf_gcs",val:": bool = True"},{name:"dl_manager",val:": typing.Optional[datasets.download.download_manager.DownloadManager] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"**download_and_prepare_kwargs",val:""}],parametersDescription:[{anchor:"datasets.DatasetBuilder.download_and_prepare.download_config",description:'<strong>download_config</strong> (<a href="/docs/datasets/master/en/package_reference/builder_classes#datasets.DownloadConfig">DownloadConfig</a>, optional) &#x2014; specific download configuration parameters.',name:"download_config"},{anchor:"datasets.DatasetBuilder.download_and_prepare.download_mode",description:'<strong>download_mode</strong> (<a href="/docs/datasets/master/en/package_reference/builder_classes#datasets.DownloadMode">DownloadMode</a>, optional) &#x2014; select the download/generate mode - Default to <code>REUSE_DATASET_IF_EXISTS</code>',name:"download_mode"},{anchor:"datasets.DatasetBuilder.download_and_prepare.ignore_verifications",description:"<strong>ignore_verifications</strong> (<code>bool</code>) &#x2014; Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/&#x2026;)",name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.download_and_prepare.try_from_hf_gcs",description:"<strong>try_from_hf_gcs</strong> (<code>bool</code>) &#x2014; If True, it will try to download the already prepared dataset from the Hf google cloud storage",name:"try_from_hf_gcs"},{anchor:"datasets.DatasetBuilder.download_and_prepare.dl_manager",description:'<strong>dl_manager</strong> (<a href="/docs/datasets/master/en/package_reference/builder_classes#datasets.DownloadManager">DownloadManager</a>, optional) &#x2014; specific Download Manger to use',name:"dl_manager"},{anchor:"datasets.DatasetBuilder.download_and_prepare.base_path",description:`<strong>base_path</strong> (<code>str</code>, optional) &#x2014; base path for relative paths that are used to download files. This can be a remote url.
If not specified, the value of the <em>base_path</em> attribute (<em>self.base_path</em>) will be used instead.`,name:"base_path"},{anchor:"datasets.DatasetBuilder.download_and_prepare.use_auth_token",description:`<strong>use_auth_token</strong> (<code>Union[str, bool]</code>, optional) &#x2014; Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.
If True, will get token from ~/.huggingface.`,name:"use_auth_token"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L513"}}),Me=new I({props:{anchor:"datasets.DatasetBuilder.download_and_prepare.example",$$slots:{default:[lm]},$$scope:{ctx:k}}}),Dt=new j({props:{name:"get_all_exported_dataset_infos",anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos",parameters:[],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L325"}}),qe=new I({props:{anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos.example",$$slots:{default:[dm]},$$scope:{ctx:k}}}),yt=new j({props:{name:"get_exported_dataset_info",anchor:"datasets.DatasetBuilder.get_exported_dataset_info",parameters:[],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L343"}}),Fe=new I({props:{anchor:"datasets.DatasetBuilder.get_exported_dataset_info.example",$$slots:{default:[im]},$$scope:{ctx:k}}}),kt=new j({props:{name:"get_imported_module_dir",anchor:"datasets.DatasetBuilder.get_imported_module_dir",parameters:[],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L508"}}),jt=new j({props:{name:"class datasets.GeneratorBasedBuilder",anchor:"datasets.GeneratorBasedBuilder",parameters:[{name:"*args",val:""},{name:"writer_batch_size",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L1067"}}),St=new j({props:{name:"class datasets.BeamBasedBuilder",anchor:"datasets.BeamBasedBuilder",parameters:[{name:"*args",val:""},{name:"beam_runner",val:" = None"},{name:"beam_options",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L1227"}}),Tt=new j({props:{name:"class datasets.ArrowBasedBuilder",anchor:"datasets.ArrowBasedBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"repo_id",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L1165"}}),It=new j({props:{name:"class datasets.BuilderConfig",anchor:"datasets.BuilderConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = '0.0.0'"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"datasets.BuilderConfig.name",description:"<strong>name</strong> (<code>str</code>, default <code>&quot;default&quot;</code>) &#x2014;",name:"name"},{anchor:"datasets.BuilderConfig.version",description:'<strong>version</strong> (<a href="/docs/datasets/master/en/package_reference/builder_classes#datasets.Version">Version</a> or <code>str</code>, optional) &#x2014;',name:"version"},{anchor:"datasets.BuilderConfig.data_dir",description:"<strong>data_dir</strong> (<code>str</code>, optional) &#x2014;",name:"data_dir"},{anchor:"datasets.BuilderConfig.data_files",description:"<strong>data_files</strong> (<code>str</code> or <code>Sequence</code> or <code>Mapping</code>, optional) &#x2014; Path(s) to source data file(s).",name:"data_files"},{anchor:"datasets.BuilderConfig.description",description:"<strong>description</strong> (<code>str</code>, optional) &#x2014;",name:"description"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L82"}}),Pt=new j({props:{name:"create_config_id",anchor:"datasets.BuilderConfig.create_config_id",parameters:[{name:"config_kwargs",val:": dict"},{name:"custom_features",val:": typing.Optional[datasets.features.features.Features] = None"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L121"}}),Rt=new Zr({}),At=new j({props:{name:"class datasets.DownloadManager",anchor:"datasets.DownloadManager",parameters:[{name:"dataset_name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.download.download_config.DownloadConfig] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"record_checksums",val:" = True"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/download/download_manager.py#L136"}}),Ct=new j({props:{name:"download",anchor:"datasets.DownloadManager.download",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/download/download_manager.py#L268",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Ge=new I({props:{anchor:"datasets.DownloadManager.download.example",$$slots:{default:[pm]},$$scope:{ctx:k}}}),Lt=new j({props:{name:"download_and_extract",anchor:"datasets.DownloadManager.download_and_extract",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/download/download_manager.py#L403",returnDescription:`
<p><code>str</code>, extracted paths of given URL(s).</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),He=new I({props:{anchor:"datasets.DownloadManager.download_and_extract.example",$$slots:{default:[cm]},$$scope:{ctx:k}}}),Ot=new j({props:{name:"download_custom",anchor:"datasets.DownloadManager.download_custom",parameters:[{name:"url_or_urls",val:""},{name:"custom_download",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/download/download_manager.py#L221",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),We=new I({props:{anchor:"datasets.DownloadManager.download_custom.example",$$slots:{default:[mm]},$$scope:{ctx:k}}}),Mt=new j({props:{name:"extract",anchor:"datasets.DownloadManager.extract",parameters:[{name:"path_or_paths",val:""},{name:"num_proc",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/download/download_manager.py#L366",returnDescription:`
<p><code>str</code>, The extracted paths matching the given input
path_or_paths.</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Xe=new I({props:{anchor:"datasets.DownloadManager.extract.example",$$slots:{default:[fm]},$$scope:{ctx:k}}}),qt=new j({props:{name:"iter_archive",anchor:"datasets.DownloadManager.iter_archive",parameters:[{name:"path_or_buf",val:": typing.Union[str, _io.BufferedReader]"}],parametersDescription:[{anchor:"datasets.DownloadManager.iter_archive.path_or_buf",description:"<strong>path_or_buf</strong> (<code>str</code> or <code>io.BufferedReader</code>) &#x2014; Archive path or archive binary file object.",name:"path_or_buf"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/download/download_manager.py#L325"}}),Je=new I({props:{anchor:"datasets.DownloadManager.iter_archive.example",$$slots:{default:[gm]},$$scope:{ctx:k}}}),Ft=new j({props:{name:"iter_files",anchor:"datasets.DownloadManager.iter_files",parameters:[{name:"paths",val:": typing.Union[str, typing.List[str]]"}],parametersDescription:[{anchor:"datasets.DownloadManager.iter_files.paths",description:"<strong>paths</strong> (<code>str</code> or <code>list</code> of <code>str</code>) &#x2014; Root paths.",name:"paths"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/download/download_manager.py#L348"}}),Ke=new I({props:{anchor:"datasets.DownloadManager.iter_files.example",$$slots:{default:[um]},$$scope:{ctx:k}}}),zt=new j({props:{name:"ship_files_with_pipeline",anchor:"datasets.DownloadManager.ship_files_with_pipeline",parameters:[{name:"downloaded_path_or_paths",val:""},{name:"pipeline",val:""}],parametersDescription:[{anchor:"datasets.DownloadManager.ship_files_with_pipeline.downloaded_path_or_paths",description:`<strong>downloaded_path_or_paths</strong> (<code>str</code> or <code>list[str]</code> or <code>dict[str, str]</code>) &#x2014; Nested structure containing the
downloaded path(s).`,name:"downloaded_path_or_paths"},{anchor:"datasets.DownloadManager.ship_files_with_pipeline.pipeline",description:"<strong>pipeline</strong> (<code>utils.beam_utils.BeamPipeline</code>) &#x2014; Apache Beam Pipeline.",name:"pipeline"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/download/download_manager.py#L178",returnDescription:`
<p><code>str</code> or <code>list[str]</code> or <code>dict[str, str]</code></p>
`}}),Ut=new j({props:{name:"class datasets.StreamingDownloadManager",anchor:"datasets.StreamingDownloadManager",parameters:[{name:"dataset_name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.download.download_config.DownloadConfig] = None"},{name:"base_path",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/download/streaming_download_manager.py#L767"}}),Gt=new j({props:{name:"download",anchor:"datasets.StreamingDownloadManager.download",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/download/streaming_download_manager.py#L793",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Qe=new I({props:{anchor:"datasets.StreamingDownloadManager.download.example",$$slots:{default:[hm]},$$scope:{ctx:k}}}),Ht=new j({props:{name:"download_and_extract",anchor:"datasets.StreamingDownloadManager.download_and_extract",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/download/streaming_download_manager.py#L861",returnDescription:`
<p><code>str</code>, extracted paths of given URL(s).</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Ze=new I({props:{anchor:"datasets.StreamingDownloadManager.download_and_extract.example",$$slots:{default:[_m]},$$scope:{ctx:k}}}),Wt=new j({props:{name:"extract",anchor:"datasets.StreamingDownloadManager.extract",parameters:[{name:"path_or_paths",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/download/streaming_download_manager.py#L820",returnDescription:`
<p><code>str</code>, The extracted paths matching the given input
path_or_paths.</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),et=new I({props:{anchor:"datasets.StreamingDownloadManager.extract.example",$$slots:{default:[$m]},$$scope:{ctx:k}}}),Xt=new j({props:{name:"iter_archive",anchor:"datasets.StreamingDownloadManager.iter_archive",parameters:[{name:"urlpath_or_buf",val:": typing.Union[str, _io.BufferedReader]"}],parametersDescription:[{anchor:"datasets.StreamingDownloadManager.iter_archive.urlpath_or_buf",description:"<strong>urlpath_or_buf</strong> (<code>str</code> or <code>io.BufferedReader</code>) &#x2014; Archive path or archive binary file object.",name:"urlpath_or_buf"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/download/streaming_download_manager.py#L879"}}),tt=new I({props:{anchor:"datasets.StreamingDownloadManager.iter_archive.example",$$slots:{default:[vm]},$$scope:{ctx:k}}}),Jt=new j({props:{name:"iter_files",anchor:"datasets.StreamingDownloadManager.iter_files",parameters:[{name:"urlpaths",val:": typing.Union[str, typing.List[str]]"}],parametersDescription:[{anchor:"datasets.StreamingDownloadManager.iter_files.urlpaths",description:"<strong>urlpaths</strong> (<code>str</code> or <code>list</code> of <code>str</code>) &#x2014; Root paths.",name:"urlpaths"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/download/streaming_download_manager.py#L902"}}),at=new I({props:{anchor:"datasets.StreamingDownloadManager.iter_files.example",$$slots:{default:[bm]},$$scope:{ctx:k}}}),Kt=new j({props:{name:"class datasets.DownloadConfig",anchor:"datasets.DownloadConfig",parameters:[{name:"cache_dir",val:": typing.Union[str, pathlib.Path, NoneType] = None"},{name:"force_download",val:": bool = False"},{name:"resume_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"proxies",val:": typing.Optional[typing.Dict] = None"},{name:"user_agent",val:": typing.Optional[str] = None"},{name:"extract_compressed_file",val:": bool = False"},{name:"force_extract",val:": bool = False"},{name:"delete_extracted",val:": bool = False"},{name:"use_etag",val:": bool = True"},{name:"num_proc",val:": typing.Optional[int] = None"},{name:"max_retries",val:": int = 1"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"ignore_url_params",val:": bool = False"},{name:"download_desc",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"datasets.DownloadConfig.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>Path</code>, optional) &#x2014; Specify a cache directory to save the file to (overwrite the
default cache dir).`,name:"cache_dir"},{anchor:"datasets.DownloadConfig.force_download",description:`<strong>force_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, re-dowload the file even if it&#x2019;s already cached in
the cache dir.`,name:"force_download"},{anchor:"datasets.DownloadConfig.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, resume the download if incompletly recieved file is
found.`,name:"resume_download"},{anchor:"datasets.DownloadConfig.proxies",description:"<strong>proxies</strong> (<code>dict</code>, optional) &#x2014;",name:"proxies"},{anchor:"datasets.DownloadConfig.user_agent",description:`<strong>user_agent</strong> (<code>str</code>, optional) &#x2014; Optional string or dict that will be appended to the user-agent on remote
requests.`,name:"user_agent"},{anchor:"datasets.DownloadConfig.extract_compressed_file",description:`<strong>extract_compressed_file</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True and the path point to a zip or tar file,
extract the compressed file in a folder along the archive.`,name:"extract_compressed_file"},{anchor:"datasets.DownloadConfig.force_extract",description:`<strong>force_extract</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True when extract_compressed_file is True and the archive
was already extracted, re-extract the archive and override the folder where it was extracted.`,name:"force_extract"},{anchor:"datasets.DownloadConfig.delete_extracted",description:"<strong>delete_extracted</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to delete (or keep) the extracted files.",name:"delete_extracted"},{anchor:"datasets.DownloadConfig.use_etag",description:"<strong>use_etag</strong> (<code>bool</code>, default <code>True</code>) &#x2014; Whether to use the ETag HTTP response header to validate the cached files.",name:"use_etag"},{anchor:"datasets.DownloadConfig.num_proc",description:"<strong>num_proc</strong> (<code>int</code>, optional) &#x2014; The number of processes to launch to download the files in parallel.",name:"num_proc"},{anchor:"datasets.DownloadConfig.max_retries",description:"<strong>max_retries</strong> (<code>int</code>, default <code>1</code>) &#x2014; The number of times to retry an HTTP request if it fails.",name:"max_retries"},{anchor:"datasets.DownloadConfig.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, optional) &#x2014; Optional string or boolean to use as Bearer token
for remote files on the Datasets Hub. If True, will get token from ~/.huggingface.`,name:"use_auth_token"},{anchor:"datasets.DownloadConfig.ignore_url_params",description:`<strong>ignore_url_params</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to strip all query parameters and #fragments from
the download URL before using it for caching the file.`,name:"ignore_url_params"},{anchor:"datasets.DownloadConfig.download_desc",description:"<strong>download_desc</strong> (<code>str</code>, optional) &#x2014; A description to be displayed alongside with the progress bar while downloading the files.",name:"download_desc"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/download/download_config.py#L8"}}),Yt=new j({props:{name:"class datasets.DownloadMode",anchor:"datasets.DownloadMode",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/download/download_manager.py#L39"}}),ea=new Zr({}),ta=new j({props:{name:"class datasets.SplitGenerator",anchor:"datasets.SplitGenerator",parameters:[{name:"name",val:": str"},{name:"gen_kwargs",val:": typing.Dict = <factory>"}],parametersDescription:[{anchor:"datasets.SplitGenerator.name",description:`<strong>name</strong> (str) &#x2014; Name of the Split for which the generator will
create the examples.
**gen_kwargs &#x2014; Keyword arguments to forward to the <code>DatasetBuilder._generate_examples</code> method
of the builder.`,name:"name"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/splits.py#L566"}}),rt=new I({props:{anchor:"datasets.SplitGenerator.example",$$slots:{default:[xm]},$$scope:{ctx:k}}}),aa=new j({props:{name:"class datasets.Split",anchor:"datasets.Split",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/splits.py#L387"}}),nt=new I({props:{anchor:"datasets.Split.example",$$slots:{default:[wm]},$$scope:{ctx:k}}}),ra=new j({props:{name:"class datasets.NamedSplit",anchor:"datasets.NamedSplit",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/splits.py#L303"}}),ot=new I({props:{anchor:"datasets.NamedSplit.example",$$slots:{default:[Em]},$$scope:{ctx:k}}}),lt=new I({props:{anchor:"datasets.NamedSplit.example-2",$$slots:{default:[Dm]},$$scope:{ctx:k}}}),dt=new I({props:{anchor:"datasets.NamedSplit.example-3",$$slots:{default:[ym]},$$scope:{ctx:k}}}),it=new I({props:{anchor:"datasets.NamedSplit.example-4",$$slots:{default:[km]},$$scope:{ctx:k}}}),na=new j({props:{name:"class datasets.NamedSplitAll",anchor:"datasets.NamedSplitAll",parameters:[],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/splits.py#L372"}}),oa=new j({props:{name:"class datasets.ReadInstruction",anchor:"datasets.ReadInstruction",parameters:[{name:"split_name",val:""},{name:"rounding",val:" = None"},{name:"from_",val:" = None"},{name:"to",val:" = None"},{name:"unit",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_reader.py#L457"}}),pt=new I({props:{anchor:"datasets.ReadInstruction.example",$$slots:{default:[jm]},$$scope:{ctx:k}}}),la=new j({props:{name:"from_spec",anchor:"datasets.ReadInstruction.from_spec",parameters:[{name:"spec",val:""}],parametersDescription:[{anchor:"datasets.ReadInstruction.from_spec.spec",description:`<strong>spec</strong> (str) &#x2014; split(s) + optional slice(s) to read + optional rounding
if percents are used as the slicing unit. A slice can be specified,
using absolute numbers (int) or percentages (int). E.g.
<code>test</code>: test split.
<code>test + validation</code>: test split + validation split.
<code>test[10:]</code>: test split, minus its first 10 records.
<code>test[:10%]</code>: first 10% records of test split.
<code>test[:20%](pct1_dropremainder)</code>: first 10% records, rounded with
the <code>pct1_dropremainder</code> rounding.
<code>test[:-5%]+train[40%:60%]</code>: first 95% of test + middle 20% of
train.`,name:"spec"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_reader.py#L537",returnDescription:`
<p>ReadInstruction instance.</p>
`}}),da=new j({props:{name:"to_absolute",anchor:"datasets.ReadInstruction.to_absolute",parameters:[{name:"name2len",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_reader.py#L605",returnDescription:`
<p>list of _AbsoluteInstruction instances (corresponds to the + in spec).</p>
`}}),ia=new Zr({}),pa=new j({props:{name:"class datasets.Version",anchor:"datasets.Version",parameters:[{name:"version_str",val:": str"},{name:"description",val:": typing.Optional[str] = None"},{name:"major",val:": typing.Union[str, int, NoneType] = None"},{name:"minor",val:": typing.Union[str, int, NoneType] = None"},{name:"patch",val:": typing.Union[str, int, NoneType] = None"}],parametersDescription:[{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.major",description:"<strong>major</strong> (<code>str</code>) &#x2014;",name:"major"},{anchor:"datasets.Version.minor",description:"<strong>minor</strong> (<code>str</code>) &#x2014;",name:"minor"},{anchor:"datasets.Version.patch",description:"<strong>patch</strong> (<code>str</code>) &#x2014;",name:"patch"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/version.py#L30"}}),ft=new I({props:{anchor:"datasets.Version.example",$$slots:{default:[Sm]},$$scope:{ctx:k}}}),ca=new j({props:{name:"match",anchor:"datasets.Version.match",parameters:[{name:"other_version",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/version.py#L101"}}),{c(){d=r("meta"),h=p(),g=r("h1"),l=r("a"),u=r("span"),v(t.$$.fragment),i=p(),os=r("span"),Mn=m("Builder classes"),en=p(),$e=r("h2"),Le=r("a"),ls=r("span"),v(bt.$$.fragment),qn=p(),ds=r("span"),Fn=m("Builders"),tn=p(),K=r("p"),zn=m("\u{1F917} Datasets relies on two main classes during the dataset building process: "),ua=r("a"),Un=m("DatasetBuilder"),Gn=m(" and "),ha=r("a"),Hn=m("BuilderConfig"),Wn=m("."),an=p(),S=r("div"),v(xt.$$.fragment),Xn=p(),is=r("p"),Jn=m("Abstract base class for all datasets."),Kn=p(),_a=r("p"),ps=r("em"),Yn=m("DatasetBuilder"),Qn=m(" has 3 key methods:"),Zn=p(),ve=r("ul"),$a=r("li"),cs=r("code"),eo=m("datasets.DatasetBuilder.info"),to=m(`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),ao=p(),va=r("li"),ba=r("a"),so=m("datasets.DatasetBuilder.download_and_prepare()"),ro=m(`: Downloads the source data
and writes it to disk.`),no=p(),Oe=r("li"),xa=r("a"),oo=m("datasets.DatasetBuilder.as_dataset()"),lo=m(": Generates a "),ms=r("em"),io=m("Dataset"),po=m("."),co=p(),W=r("p"),fs=r("strong"),mo=m("Configuration"),fo=m(": Some "),gs=r("em"),go=m("DatasetBuilder"),uo=m(`s expose multiple variants of the
dataset by defining a `),us=r("em"),ho=m("datasets.BuilderConfig"),_o=m(` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),hs=r("code"),$o=m("datasets.DatasetBuilder.builder_configs()"),vo=m("."),bo=p(),Y=r("div"),v(wt.$$.fragment),xo=p(),_s=r("p"),wo=m("Return a Dataset for the specified split."),Eo=p(),v(Ve.$$.fragment),Do=p(),Q=r("div"),v(Et.$$.fragment),yo=p(),$s=r("p"),ko=m("Downloads and prepares dataset for reading."),jo=p(),v(Me.$$.fragment),So=p(),Z=r("div"),v(Dt.$$.fragment),To=p(),vs=r("p"),Io=m("Empty dict if doesn\u2019t exist"),Bo=p(),v(qe.$$.fragment),No=p(),ee=r("div"),v(yt.$$.fragment),Po=p(),bs=r("p"),Ro=m("Empty DatasetInfo if doesn\u2019t exist"),Ao=p(),v(Fe.$$.fragment),Co=p(),ze=r("div"),v(kt.$$.fragment),Lo=p(),xs=r("p"),Oo=m("Return the path of the module of this class or subclass."),sn=p(),X=r("div"),v(jt.$$.fragment),Vo=p(),ws=r("p"),Mo=m("Base class for datasets with data generation based on dict generators."),qo=p(),te=r("p"),Es=r("code"),Fo=m("GeneratorBasedBuilder"),zo=m(` is a convenience class that abstracts away much
of the data writing and reading of `),Ds=r("code"),Uo=m("DatasetBuilder"),Go=m(`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),ys=r("code"),Ho=m("_split_generators"),Wo=m("). See the method docstrings for details."),rn=p(),be=r("div"),v(St.$$.fragment),Xo=p(),ks=r("p"),Jo=m("Beam based Builder."),nn=p(),xe=r("div"),v(Tt.$$.fragment),Ko=p(),js=r("p"),Yo=m("Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),on=p(),z=r("div"),v(It.$$.fragment),Qo=p(),Bt=r("p"),Zo=m("Base class for "),wa=r("a"),el=m("DatasetBuilder"),tl=m(" data configuration."),al=p(),Nt=r("p"),sl=m(`DatasetBuilder subclasses with data configuration options should subclass
`),Ea=r("a"),rl=m("BuilderConfig"),nl=m(" and add their own properties."),ol=p(),ae=r("div"),v(Pt.$$.fragment),ll=p(),Ss=r("p"),dl=m(`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),il=p(),we=r("ul"),Ts=r("li"),pl=m("the config kwargs that can be used to overwrite attributes"),cl=p(),Is=r("li"),ml=m("the custom features used to write the dataset"),fl=p(),Bs=r("li"),gl=m(`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),ln=p(),Ee=r("h2"),Ue=r("a"),Ns=r("span"),v(Rt.$$.fragment),ul=p(),Ps=r("span"),hl=m("Download"),dn=p(),P=r("div"),v(At.$$.fragment),_l=p(),se=r("div"),v(Ct.$$.fragment),$l=p(),Rs=r("p"),vl=m("Download given url(s)."),bl=p(),v(Ge.$$.fragment),xl=p(),re=r("div"),v(Lt.$$.fragment),wl=p(),As=r("p"),El=m("Download and extract given url_or_urls."),Dl=p(),v(He.$$.fragment),yl=p(),ne=r("div"),v(Ot.$$.fragment),kl=p(),Vt=r("p"),jl=m("Download given urls(s) by calling "),Cs=r("code"),Sl=m("custom_download"),Tl=m("."),Il=p(),v(We.$$.fragment),Bl=p(),oe=r("div"),v(Mt.$$.fragment),Nl=p(),Ls=r("p"),Pl=m("Extract given path(s)."),Rl=p(),v(Xe.$$.fragment),Al=p(),le=r("div"),v(qt.$$.fragment),Cl=p(),Os=r("p"),Ll=m("Iterate over files within an archive."),Ol=p(),v(Je.$$.fragment),Vl=p(),de=r("div"),v(Ft.$$.fragment),Ml=p(),Vs=r("p"),ql=m("Iterate over file paths."),Fl=p(),v(Ke.$$.fragment),zl=p(),Ye=r("div"),v(zt.$$.fragment),Ul=p(),Ms=r("p"),Gl=m("Ship the files using Beam FileSystems to the pipeline temp dir."),pn=p(),A=r("div"),v(Ut.$$.fragment),Hl=p(),U=r("p"),Wl=m(`Download manager that uses the \u201D::\u201D separator to navigate through (possibly remote) compressed archives.
Contrary to the regular DownloadManager, the `),qs=r("code"),Xl=m("download"),Jl=m(" and "),Fs=r("code"),Kl=m("extract"),Yl=m(` methods don\u2019t actually download nor extract
data, but they rather return the path or url that could be opened using the `),zs=r("code"),Ql=m("xopen"),Zl=m(` function which extends the
builtin `),Us=r("code"),ed=m("open"),td=m(" function to stream data from remote files."),ad=p(),ie=r("div"),v(Gt.$$.fragment),sd=p(),Gs=r("p"),rd=m("Download given url(s)."),nd=p(),v(Qe.$$.fragment),od=p(),pe=r("div"),v(Ht.$$.fragment),ld=p(),Hs=r("p"),dd=m("Download and extract given url_or_urls."),id=p(),v(Ze.$$.fragment),pd=p(),ce=r("div"),v(Wt.$$.fragment),cd=p(),Ws=r("p"),md=m("Extract given path(s)."),fd=p(),v(et.$$.fragment),gd=p(),me=r("div"),v(Xt.$$.fragment),ud=p(),Xs=r("p"),hd=m("Iterate over files within an archive."),_d=p(),v(tt.$$.fragment),$d=p(),fe=r("div"),v(Jt.$$.fragment),vd=p(),Js=r("p"),bd=m("Iterate over files."),xd=p(),v(at.$$.fragment),cn=p(),De=r("div"),v(Kt.$$.fragment),wd=p(),Ks=r("p"),Ed=m("Configuration for our cached path manager."),mn=p(),V=r("div"),v(Yt.$$.fragment),Dd=p(),Da=r("p"),Ys=r("code"),yd=m("Enum"),kd=m(" for how to treat pre-existing downloads and data."),jd=p(),Qt=r("p"),Sd=m("The default mode is "),Qs=r("code"),Td=m("REUSE_DATASET_IF_EXISTS"),Id=m(`, which will reuse both
raw downloads and the prepared dataset if they exist.`),Bd=p(),Zs=r("p"),Nd=m("The generations modes:"),Pd=p(),Zt=r("table"),er=r("thead"),ye=r("tr"),fn=r("th"),Rd=p(),tr=r("th"),Ad=m("Downloads"),Cd=p(),ar=r("th"),Ld=m("Dataset"),Od=p(),ke=r("tbody"),je=r("tr"),ya=r("td"),sr=r("code"),Vd=m("REUSE_DATASET_IF_EXISTS"),Md=m(" (default)"),qd=p(),rr=r("td"),Fd=m("Reuse"),zd=p(),nr=r("td"),Ud=m("Reuse"),Gd=p(),Se=r("tr"),or=r("td"),lr=r("code"),Hd=m("REUSE_CACHE_IF_EXISTS"),Wd=p(),dr=r("td"),Xd=m("Reuse"),Jd=p(),ir=r("td"),Kd=m("Fresh"),Yd=p(),Te=r("tr"),pr=r("td"),cr=r("code"),Qd=m("FORCE_REDOWNLOAD"),Zd=p(),mr=r("td"),ei=m("Fresh"),ti=p(),fr=r("td"),ai=m("Fresh"),gn=p(),Ie=r("h2"),st=r("a"),gr=r("span"),v(ea.$$.fragment),si=p(),ur=r("span"),ri=m("Splits"),un=p(),G=r("div"),v(ta.$$.fragment),ni=p(),hr=r("p"),oi=m("Defines the split information for the generator."),li=p(),Be=r("p"),di=m(`This should be used as returned value of
`),_r=r("code"),ii=m("GeneratorBasedBuilder._split_generators()"),pi=m(`.
See `),$r=r("code"),ci=m("GeneratorBasedBuilder._split_generators()"),mi=m(` for more info and example
of usage.`),fi=p(),v(rt.$$.fragment),hn=p(),C=r("div"),v(aa.$$.fragment),gi=p(),ka=r("p"),vr=r("code"),ui=m("Enum"),hi=m(" for dataset splits."),_i=p(),br=r("p"),$i=m(`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),vi=p(),J=r("ul"),ja=r("li"),xr=r("code"),bi=m("TRAIN"),xi=m(": the training data."),wi=p(),Sa=r("li"),wr=r("code"),Ei=m("VALIDATION"),Di=m(`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),yi=p(),Ta=r("li"),Er=r("code"),ki=m("TEST"),ji=m(`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Si=p(),Ia=r("li"),Dr=r("code"),Ti=m("ALL"),Ii=m(": the union of all defined dataset splits."),Bi=p(),Ba=r("p"),Ni=m("Note: All splits, including compositions inherit from "),yr=r("code"),Pi=m("datasets.SplitBase"),Ri=p(),sa=r("p"),Ai=m("See the :doc:"),kr=r("code"),Ci=m("guide on splits </loading>"),Li=m(" for more information."),Oi=p(),v(nt.$$.fragment),_n=p(),R=r("div"),v(ra.$$.fragment),Vi=p(),jr=r("p"),Mi=m("Descriptor corresponding to a named split (train, test, \u2026)."),qi=p(),v(ot.$$.fragment),Fi=p(),Sr=r("p"),zi=m("Warning:"),Ui=p(),v(lt.$$.fragment),Gi=p(),Tr=r("p"),Hi=m("Warning:"),Wi=p(),v(dt.$$.fragment),Xi=p(),v(it.$$.fragment),$n=p(),Ne=r("div"),v(na.$$.fragment),Ji=p(),Ir=r("p"),Ki=m("Split corresponding to the union of all defined dataset splits."),vn=p(),M=r("div"),v(oa.$$.fragment),Yi=p(),Br=r("p"),Qi=m("Reading instruction for a dataset."),Zi=p(),v(pt.$$.fragment),ep=p(),ct=r("div"),v(la.$$.fragment),tp=p(),Nr=r("p"),ap=m("Creates a ReadInstruction instance out of a string spec."),sp=p(),ge=r("div"),v(da.$$.fragment),rp=p(),Pr=r("p"),np=m("Translate instruction into a list of absolute instructions."),op=p(),Rr=r("p"),lp=m("Those absolute instructions are then to be added together."),bn=p(),Pe=r("h2"),mt=r("a"),Ar=r("span"),v(ia.$$.fragment),dp=p(),Cr=r("span"),ip=m("Version"),xn=p(),H=r("div"),v(pa.$$.fragment),pp=p(),Lr=r("p"),cp=m("Dataset version MAJOR.MINOR.PATCH."),mp=p(),v(ft.$$.fragment),fp=p(),gt=r("div"),v(ca.$$.fragment),gp=p(),Or=r("p"),up=m("Returns True if other_version matches."),this.h()},l(s){const _=rm('[data-svelte="svelte-1phssyn"]',document.head);d=n(_,"META",{name:!0,content:!0}),_.forEach(a),h=c(s),g=n(s,"H1",{class:!0});var ma=o(g);l=n(ma,"A",{id:!0,class:!0,href:!0});var Vr=o(l);u=n(Vr,"SPAN",{});var Mr=o(u);b(t.$$.fragment,Mr),Mr.forEach(a),Vr.forEach(a),i=c(ma),os=n(ma,"SPAN",{});var qr=o(os);Mn=f(qr,"Builder classes"),qr.forEach(a),ma.forEach(a),en=c(s),$e=n(s,"H2",{class:!0});var fa=o($e);Le=n(fa,"A",{id:!0,class:!0,href:!0});var Fr=o(Le);ls=n(Fr,"SPAN",{});var zr=o(ls);b(bt.$$.fragment,zr),zr.forEach(a),Fr.forEach(a),qn=c(fa),ds=n(fa,"SPAN",{});var Ur=o(ds);Fn=f(Ur,"Builders"),Ur.forEach(a),fa.forEach(a),tn=c(s),K=n(s,"P",{});var Re=o(K);zn=f(Re,"\u{1F917} Datasets relies on two main classes during the dataset building process: "),ua=n(Re,"A",{href:!0});var Gr=o(ua);Un=f(Gr,"DatasetBuilder"),Gr.forEach(a),Gn=f(Re," and "),ha=n(Re,"A",{href:!0});var Hr=o(ha);Hn=f(Hr,"BuilderConfig"),Hr.forEach(a),Wn=f(Re,"."),Re.forEach(a),an=c(s),S=n(s,"DIV",{class:!0});var T=o(S);b(xt.$$.fragment,T),Xn=c(T),is=n(T,"P",{});var Wr=o(is);Jn=f(Wr,"Abstract base class for all datasets."),Wr.forEach(a),Kn=c(T),_a=n(T,"P",{});var Na=o(_a);ps=n(Na,"EM",{});var Xr=o(ps);Yn=f(Xr,"DatasetBuilder"),Xr.forEach(a),Qn=f(Na," has 3 key methods:"),Na.forEach(a),Zn=c(T),ve=n(T,"UL",{});var Ae=o(ve);$a=n(Ae,"LI",{});var Pa=o($a);cs=n(Pa,"CODE",{});var Jr=o(cs);eo=f(Jr,"datasets.DatasetBuilder.info"),Jr.forEach(a),to=f(Pa,`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),Pa.forEach(a),ao=c(Ae),va=n(Ae,"LI",{});var Ra=o(va);ba=n(Ra,"A",{href:!0});var Kr=o(ba);so=f(Kr,"datasets.DatasetBuilder.download_and_prepare()"),Kr.forEach(a),ro=f(Ra,`: Downloads the source data
and writes it to disk.`),Ra.forEach(a),no=c(Ae),Oe=n(Ae,"LI",{});var ut=o(Oe);xa=n(ut,"A",{href:!0});var Yr=o(xa);oo=f(Yr,"datasets.DatasetBuilder.as_dataset()"),Yr.forEach(a),lo=f(ut,": Generates a "),ms=n(ut,"EM",{});var Qr=o(ms);io=f(Qr,"Dataset"),Qr.forEach(a),po=f(ut,"."),ut.forEach(a),Ae.forEach(a),co=c(T),W=n(T,"P",{});var Ce=o(W);fs=n(Ce,"STRONG",{});var Dp=o(fs);mo=f(Dp,"Configuration"),Dp.forEach(a),fo=f(Ce,": Some "),gs=n(Ce,"EM",{});var yp=o(gs);go=f(yp,"DatasetBuilder"),yp.forEach(a),uo=f(Ce,`s expose multiple variants of the
dataset by defining a `),us=n(Ce,"EM",{});var kp=o(us);ho=f(kp,"datasets.BuilderConfig"),kp.forEach(a),_o=f(Ce,` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),hs=n(Ce,"CODE",{});var jp=o(hs);$o=f(jp,"datasets.DatasetBuilder.builder_configs()"),jp.forEach(a),vo=f(Ce,"."),Ce.forEach(a),bo=c(T),Y=n(T,"DIV",{class:!0});var Aa=o(Y);b(wt.$$.fragment,Aa),xo=c(Aa),_s=n(Aa,"P",{});var Sp=o(_s);wo=f(Sp,"Return a Dataset for the specified split."),Sp.forEach(a),Eo=c(Aa),b(Ve.$$.fragment,Aa),Aa.forEach(a),Do=c(T),Q=n(T,"DIV",{class:!0});var Ca=o(Q);b(Et.$$.fragment,Ca),yo=c(Ca),$s=n(Ca,"P",{});var Tp=o($s);ko=f(Tp,"Downloads and prepares dataset for reading."),Tp.forEach(a),jo=c(Ca),b(Me.$$.fragment,Ca),Ca.forEach(a),So=c(T),Z=n(T,"DIV",{class:!0});var La=o(Z);b(Dt.$$.fragment,La),To=c(La),vs=n(La,"P",{});var Ip=o(vs);Io=f(Ip,"Empty dict if doesn\u2019t exist"),Ip.forEach(a),Bo=c(La),b(qe.$$.fragment,La),La.forEach(a),No=c(T),ee=n(T,"DIV",{class:!0});var Oa=o(ee);b(yt.$$.fragment,Oa),Po=c(Oa),bs=n(Oa,"P",{});var Bp=o(bs);Ro=f(Bp,"Empty DatasetInfo if doesn\u2019t exist"),Bp.forEach(a),Ao=c(Oa),b(Fe.$$.fragment,Oa),Oa.forEach(a),Co=c(T),ze=n(T,"DIV",{class:!0});var En=o(ze);b(kt.$$.fragment,En),Lo=c(En),xs=n(En,"P",{});var Np=o(xs);Oo=f(Np,"Return the path of the module of this class or subclass."),Np.forEach(a),En.forEach(a),T.forEach(a),sn=c(s),X=n(s,"DIV",{class:!0});var Va=o(X);b(jt.$$.fragment,Va),Vo=c(Va),ws=n(Va,"P",{});var Pp=o(ws);Mo=f(Pp,"Base class for datasets with data generation based on dict generators."),Pp.forEach(a),qo=c(Va),te=n(Va,"P",{});var ga=o(te);Es=n(ga,"CODE",{});var Rp=o(Es);Fo=f(Rp,"GeneratorBasedBuilder"),Rp.forEach(a),zo=f(ga,` is a convenience class that abstracts away much
of the data writing and reading of `),Ds=n(ga,"CODE",{});var Ap=o(Ds);Uo=f(Ap,"DatasetBuilder"),Ap.forEach(a),Go=f(ga,`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),ys=n(ga,"CODE",{});var Cp=o(ys);Ho=f(Cp,"_split_generators"),Cp.forEach(a),Wo=f(ga,"). See the method docstrings for details."),ga.forEach(a),Va.forEach(a),rn=c(s),be=n(s,"DIV",{class:!0});var Dn=o(be);b(St.$$.fragment,Dn),Xo=c(Dn),ks=n(Dn,"P",{});var Lp=o(ks);Jo=f(Lp,"Beam based Builder."),Lp.forEach(a),Dn.forEach(a),nn=c(s),xe=n(s,"DIV",{class:!0});var yn=o(xe);b(Tt.$$.fragment,yn),Ko=c(yn),js=n(yn,"P",{});var Op=o(js);Yo=f(Op,"Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),Op.forEach(a),yn.forEach(a),on=c(s),z=n(s,"DIV",{class:!0});var ht=o(z);b(It.$$.fragment,ht),Qo=c(ht),Bt=n(ht,"P",{});var kn=o(Bt);Zo=f(kn,"Base class for "),wa=n(kn,"A",{href:!0});var Vp=o(wa);el=f(Vp,"DatasetBuilder"),Vp.forEach(a),tl=f(kn," data configuration."),kn.forEach(a),al=c(ht),Nt=n(ht,"P",{});var jn=o(Nt);sl=f(jn,`DatasetBuilder subclasses with data configuration options should subclass
`),Ea=n(jn,"A",{href:!0});var Mp=o(Ea);rl=f(Mp,"BuilderConfig"),Mp.forEach(a),nl=f(jn," and add their own properties."),jn.forEach(a),ol=c(ht),ae=n(ht,"DIV",{class:!0});var Ma=o(ae);b(Pt.$$.fragment,Ma),ll=c(Ma),Ss=n(Ma,"P",{});var qp=o(Ss);dl=f(qp,`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),qp.forEach(a),il=c(Ma),we=n(Ma,"UL",{});var qa=o(we);Ts=n(qa,"LI",{});var Fp=o(Ts);pl=f(Fp,"the config kwargs that can be used to overwrite attributes"),Fp.forEach(a),cl=c(qa),Is=n(qa,"LI",{});var zp=o(Is);ml=f(zp,"the custom features used to write the dataset"),zp.forEach(a),fl=c(qa),Bs=n(qa,"LI",{});var Up=o(Bs);gl=f(Up,`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),Up.forEach(a),qa.forEach(a),Ma.forEach(a),ht.forEach(a),ln=c(s),Ee=n(s,"H2",{class:!0});var Sn=o(Ee);Ue=n(Sn,"A",{id:!0,class:!0,href:!0});var Gp=o(Ue);Ns=n(Gp,"SPAN",{});var Hp=o(Ns);b(Rt.$$.fragment,Hp),Hp.forEach(a),Gp.forEach(a),ul=c(Sn),Ps=n(Sn,"SPAN",{});var Wp=o(Ps);hl=f(Wp,"Download"),Wp.forEach(a),Sn.forEach(a),dn=c(s),P=n(s,"DIV",{class:!0});var L=o(P);b(At.$$.fragment,L),_l=c(L),se=n(L,"DIV",{class:!0});var Fa=o(se);b(Ct.$$.fragment,Fa),$l=c(Fa),Rs=n(Fa,"P",{});var Xp=o(Rs);vl=f(Xp,"Download given url(s)."),Xp.forEach(a),bl=c(Fa),b(Ge.$$.fragment,Fa),Fa.forEach(a),xl=c(L),re=n(L,"DIV",{class:!0});var za=o(re);b(Lt.$$.fragment,za),wl=c(za),As=n(za,"P",{});var Jp=o(As);El=f(Jp,"Download and extract given url_or_urls."),Jp.forEach(a),Dl=c(za),b(He.$$.fragment,za),za.forEach(a),yl=c(L),ne=n(L,"DIV",{class:!0});var Ua=o(ne);b(Ot.$$.fragment,Ua),kl=c(Ua),Vt=n(Ua,"P",{});var Tn=o(Vt);jl=f(Tn,"Download given urls(s) by calling "),Cs=n(Tn,"CODE",{});var Kp=o(Cs);Sl=f(Kp,"custom_download"),Kp.forEach(a),Tl=f(Tn,"."),Tn.forEach(a),Il=c(Ua),b(We.$$.fragment,Ua),Ua.forEach(a),Bl=c(L),oe=n(L,"DIV",{class:!0});var Ga=o(oe);b(Mt.$$.fragment,Ga),Nl=c(Ga),Ls=n(Ga,"P",{});var Yp=o(Ls);Pl=f(Yp,"Extract given path(s)."),Yp.forEach(a),Rl=c(Ga),b(Xe.$$.fragment,Ga),Ga.forEach(a),Al=c(L),le=n(L,"DIV",{class:!0});var Ha=o(le);b(qt.$$.fragment,Ha),Cl=c(Ha),Os=n(Ha,"P",{});var Qp=o(Os);Ll=f(Qp,"Iterate over files within an archive."),Qp.forEach(a),Ol=c(Ha),b(Je.$$.fragment,Ha),Ha.forEach(a),Vl=c(L),de=n(L,"DIV",{class:!0});var Wa=o(de);b(Ft.$$.fragment,Wa),Ml=c(Wa),Vs=n(Wa,"P",{});var Zp=o(Vs);ql=f(Zp,"Iterate over file paths."),Zp.forEach(a),Fl=c(Wa),b(Ke.$$.fragment,Wa),Wa.forEach(a),zl=c(L),Ye=n(L,"DIV",{class:!0});var In=o(Ye);b(zt.$$.fragment,In),Ul=c(In),Ms=n(In,"P",{});var ec=o(Ms);Gl=f(ec,"Ship the files using Beam FileSystems to the pipeline temp dir."),ec.forEach(a),In.forEach(a),L.forEach(a),pn=c(s),A=n(s,"DIV",{class:!0});var q=o(A);b(Ut.$$.fragment,q),Hl=c(q),U=n(q,"P",{});var ue=o(U);Wl=f(ue,`Download manager that uses the \u201D::\u201D separator to navigate through (possibly remote) compressed archives.
Contrary to the regular DownloadManager, the `),qs=n(ue,"CODE",{});var tc=o(qs);Xl=f(tc,"download"),tc.forEach(a),Jl=f(ue," and "),Fs=n(ue,"CODE",{});var ac=o(Fs);Kl=f(ac,"extract"),ac.forEach(a),Yl=f(ue,` methods don\u2019t actually download nor extract
data, but they rather return the path or url that could be opened using the `),zs=n(ue,"CODE",{});var sc=o(zs);Ql=f(sc,"xopen"),sc.forEach(a),Zl=f(ue,` function which extends the
builtin `),Us=n(ue,"CODE",{});var rc=o(Us);ed=f(rc,"open"),rc.forEach(a),td=f(ue," function to stream data from remote files."),ue.forEach(a),ad=c(q),ie=n(q,"DIV",{class:!0});var Xa=o(ie);b(Gt.$$.fragment,Xa),sd=c(Xa),Gs=n(Xa,"P",{});var nc=o(Gs);rd=f(nc,"Download given url(s)."),nc.forEach(a),nd=c(Xa),b(Qe.$$.fragment,Xa),Xa.forEach(a),od=c(q),pe=n(q,"DIV",{class:!0});var Ja=o(pe);b(Ht.$$.fragment,Ja),ld=c(Ja),Hs=n(Ja,"P",{});var oc=o(Hs);dd=f(oc,"Download and extract given url_or_urls."),oc.forEach(a),id=c(Ja),b(Ze.$$.fragment,Ja),Ja.forEach(a),pd=c(q),ce=n(q,"DIV",{class:!0});var Ka=o(ce);b(Wt.$$.fragment,Ka),cd=c(Ka),Ws=n(Ka,"P",{});var lc=o(Ws);md=f(lc,"Extract given path(s)."),lc.forEach(a),fd=c(Ka),b(et.$$.fragment,Ka),Ka.forEach(a),gd=c(q),me=n(q,"DIV",{class:!0});var Ya=o(me);b(Xt.$$.fragment,Ya),ud=c(Ya),Xs=n(Ya,"P",{});var dc=o(Xs);hd=f(dc,"Iterate over files within an archive."),dc.forEach(a),_d=c(Ya),b(tt.$$.fragment,Ya),Ya.forEach(a),$d=c(q),fe=n(q,"DIV",{class:!0});var Qa=o(fe);b(Jt.$$.fragment,Qa),vd=c(Qa),Js=n(Qa,"P",{});var ic=o(Js);bd=f(ic,"Iterate over files."),ic.forEach(a),xd=c(Qa),b(at.$$.fragment,Qa),Qa.forEach(a),q.forEach(a),cn=c(s),De=n(s,"DIV",{class:!0});var Bn=o(De);b(Kt.$$.fragment,Bn),wd=c(Bn),Ks=n(Bn,"P",{});var pc=o(Ks);Ed=f(pc,"Configuration for our cached path manager."),pc.forEach(a),Bn.forEach(a),mn=c(s),V=n(s,"DIV",{class:!0});var he=o(V);b(Yt.$$.fragment,he),Dd=c(he),Da=n(he,"P",{});var hp=o(Da);Ys=n(hp,"CODE",{});var cc=o(Ys);yd=f(cc,"Enum"),cc.forEach(a),kd=f(hp," for how to treat pre-existing downloads and data."),hp.forEach(a),jd=c(he),Qt=n(he,"P",{});var Nn=o(Qt);Sd=f(Nn,"The default mode is "),Qs=n(Nn,"CODE",{});var mc=o(Qs);Td=f(mc,"REUSE_DATASET_IF_EXISTS"),mc.forEach(a),Id=f(Nn,`, which will reuse both
raw downloads and the prepared dataset if they exist.`),Nn.forEach(a),Bd=c(he),Zs=n(he,"P",{});var fc=o(Zs);Nd=f(fc,"The generations modes:"),fc.forEach(a),Pd=c(he),Zt=n(he,"TABLE",{});var Pn=o(Zt);er=n(Pn,"THEAD",{});var gc=o(er);ye=n(gc,"TR",{});var Za=o(ye);fn=n(Za,"TH",{}),o(fn).forEach(a),Rd=c(Za),tr=n(Za,"TH",{});var uc=o(tr);Ad=f(uc,"Downloads"),uc.forEach(a),Cd=c(Za),ar=n(Za,"TH",{});var hc=o(ar);Ld=f(hc,"Dataset"),hc.forEach(a),Za.forEach(a),gc.forEach(a),Od=c(Pn),ke=n(Pn,"TBODY",{});var es=o(ke);je=n(es,"TR",{});var ts=o(je);ya=n(ts,"TD",{});var _p=o(ya);sr=n(_p,"CODE",{});var _c=o(sr);Vd=f(_c,"REUSE_DATASET_IF_EXISTS"),_c.forEach(a),Md=f(_p," (default)"),_p.forEach(a),qd=c(ts),rr=n(ts,"TD",{});var $c=o(rr);Fd=f($c,"Reuse"),$c.forEach(a),zd=c(ts),nr=n(ts,"TD",{});var vc=o(nr);Ud=f(vc,"Reuse"),vc.forEach(a),ts.forEach(a),Gd=c(es),Se=n(es,"TR",{});var as=o(Se);or=n(as,"TD",{});var bc=o(or);lr=n(bc,"CODE",{});var xc=o(lr);Hd=f(xc,"REUSE_CACHE_IF_EXISTS"),xc.forEach(a),bc.forEach(a),Wd=c(as),dr=n(as,"TD",{});var wc=o(dr);Xd=f(wc,"Reuse"),wc.forEach(a),Jd=c(as),ir=n(as,"TD",{});var Ec=o(ir);Kd=f(Ec,"Fresh"),Ec.forEach(a),as.forEach(a),Yd=c(es),Te=n(es,"TR",{});var ss=o(Te);pr=n(ss,"TD",{});var Dc=o(pr);cr=n(Dc,"CODE",{});var yc=o(cr);Qd=f(yc,"FORCE_REDOWNLOAD"),yc.forEach(a),Dc.forEach(a),Zd=c(ss),mr=n(ss,"TD",{});var kc=o(mr);ei=f(kc,"Fresh"),kc.forEach(a),ti=c(ss),fr=n(ss,"TD",{});var jc=o(fr);ai=f(jc,"Fresh"),jc.forEach(a),ss.forEach(a),es.forEach(a),Pn.forEach(a),he.forEach(a),gn=c(s),Ie=n(s,"H2",{class:!0});var Rn=o(Ie);st=n(Rn,"A",{id:!0,class:!0,href:!0});var Sc=o(st);gr=n(Sc,"SPAN",{});var Tc=o(gr);b(ea.$$.fragment,Tc),Tc.forEach(a),Sc.forEach(a),si=c(Rn),ur=n(Rn,"SPAN",{});var Ic=o(ur);ri=f(Ic,"Splits"),Ic.forEach(a),Rn.forEach(a),un=c(s),G=n(s,"DIV",{class:!0});var _t=o(G);b(ta.$$.fragment,_t),ni=c(_t),hr=n(_t,"P",{});var Bc=o(hr);oi=f(Bc,"Defines the split information for the generator."),Bc.forEach(a),li=c(_t),Be=n(_t,"P",{});var rs=o(Be);di=f(rs,`This should be used as returned value of
`),_r=n(rs,"CODE",{});var Nc=o(_r);ii=f(Nc,"GeneratorBasedBuilder._split_generators()"),Nc.forEach(a),pi=f(rs,`.
See `),$r=n(rs,"CODE",{});var Pc=o($r);ci=f(Pc,"GeneratorBasedBuilder._split_generators()"),Pc.forEach(a),mi=f(rs,` for more info and example
of usage.`),rs.forEach(a),fi=c(_t),b(rt.$$.fragment,_t),_t.forEach(a),hn=c(s),C=n(s,"DIV",{class:!0});var F=o(C);b(aa.$$.fragment,F),gi=c(F),ka=n(F,"P",{});var $p=o(ka);vr=n($p,"CODE",{});var Rc=o(vr);ui=f(Rc,"Enum"),Rc.forEach(a),hi=f($p," for dataset splits."),$p.forEach(a),_i=c(F),br=n(F,"P",{});var Ac=o(br);$i=f(Ac,`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),Ac.forEach(a),vi=c(F),J=n(F,"UL",{});var $t=o(J);ja=n($t,"LI",{});var vp=o(ja);xr=n(vp,"CODE",{});var Cc=o(xr);bi=f(Cc,"TRAIN"),Cc.forEach(a),xi=f(vp,": the training data."),vp.forEach(a),wi=c($t),Sa=n($t,"LI",{});var bp=o(Sa);wr=n(bp,"CODE",{});var Lc=o(wr);Ei=f(Lc,"VALIDATION"),Lc.forEach(a),Di=f(bp,`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),bp.forEach(a),yi=c($t),Ta=n($t,"LI",{});var xp=o(Ta);Er=n(xp,"CODE",{});var Oc=o(Er);ki=f(Oc,"TEST"),Oc.forEach(a),ji=f(xp,`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),xp.forEach(a),Si=c($t),Ia=n($t,"LI",{});var wp=o(Ia);Dr=n(wp,"CODE",{});var Vc=o(Dr);Ti=f(Vc,"ALL"),Vc.forEach(a),Ii=f(wp,": the union of all defined dataset splits."),wp.forEach(a),$t.forEach(a),Bi=c(F),Ba=n(F,"P",{});var Ep=o(Ba);Ni=f(Ep,"Note: All splits, including compositions inherit from "),yr=n(Ep,"CODE",{});var Mc=o(yr);Pi=f(Mc,"datasets.SplitBase"),Mc.forEach(a),Ep.forEach(a),Ri=c(F),sa=n(F,"P",{});var An=o(sa);Ai=f(An,"See the :doc:"),kr=n(An,"CODE",{});var qc=o(kr);Ci=f(qc,"guide on splits </loading>"),qc.forEach(a),Li=f(An," for more information."),An.forEach(a),Oi=c(F),b(nt.$$.fragment,F),F.forEach(a),_n=c(s),R=n(s,"DIV",{class:!0});var O=o(R);b(ra.$$.fragment,O),Vi=c(O),jr=n(O,"P",{});var Fc=o(jr);Mi=f(Fc,"Descriptor corresponding to a named split (train, test, \u2026)."),Fc.forEach(a),qi=c(O),b(ot.$$.fragment,O),Fi=c(O),Sr=n(O,"P",{});var zc=o(Sr);zi=f(zc,"Warning:"),zc.forEach(a),Ui=c(O),b(lt.$$.fragment,O),Gi=c(O),Tr=n(O,"P",{});var Uc=o(Tr);Hi=f(Uc,"Warning:"),Uc.forEach(a),Wi=c(O),b(dt.$$.fragment,O),Xi=c(O),b(it.$$.fragment,O),O.forEach(a),$n=c(s),Ne=n(s,"DIV",{class:!0});var Cn=o(Ne);b(na.$$.fragment,Cn),Ji=c(Cn),Ir=n(Cn,"P",{});var Gc=o(Ir);Ki=f(Gc,"Split corresponding to the union of all defined dataset splits."),Gc.forEach(a),Cn.forEach(a),vn=c(s),M=n(s,"DIV",{class:!0});var _e=o(M);b(oa.$$.fragment,_e),Yi=c(_e),Br=n(_e,"P",{});var Hc=o(Br);Qi=f(Hc,"Reading instruction for a dataset."),Hc.forEach(a),Zi=c(_e),b(pt.$$.fragment,_e),ep=c(_e),ct=n(_e,"DIV",{class:!0});var Ln=o(ct);b(la.$$.fragment,Ln),tp=c(Ln),Nr=n(Ln,"P",{});var Wc=o(Nr);ap=f(Wc,"Creates a ReadInstruction instance out of a string spec."),Wc.forEach(a),Ln.forEach(a),sp=c(_e),ge=n(_e,"DIV",{class:!0});var ns=o(ge);b(da.$$.fragment,ns),rp=c(ns),Pr=n(ns,"P",{});var Xc=o(Pr);np=f(Xc,"Translate instruction into a list of absolute instructions."),Xc.forEach(a),op=c(ns),Rr=n(ns,"P",{});var Jc=o(Rr);lp=f(Jc,"Those absolute instructions are then to be added together."),Jc.forEach(a),ns.forEach(a),_e.forEach(a),bn=c(s),Pe=n(s,"H2",{class:!0});var On=o(Pe);mt=n(On,"A",{id:!0,class:!0,href:!0});var Kc=o(mt);Ar=n(Kc,"SPAN",{});var Yc=o(Ar);b(ia.$$.fragment,Yc),Yc.forEach(a),Kc.forEach(a),dp=c(On),Cr=n(On,"SPAN",{});var Qc=o(Cr);ip=f(Qc,"Version"),Qc.forEach(a),On.forEach(a),xn=c(s),H=n(s,"DIV",{class:!0});var vt=o(H);b(pa.$$.fragment,vt),pp=c(vt),Lr=n(vt,"P",{});var Zc=o(Lr);cp=f(Zc,"Dataset version MAJOR.MINOR.PATCH."),Zc.forEach(a),mp=c(vt),b(ft.$$.fragment,vt),fp=c(vt),gt=n(vt,"DIV",{class:!0});var Vn=o(gt);b(ca.$$.fragment,Vn),gp=c(Vn),Or=n(Vn,"P",{});var em=o(Or);up=f(em,"Returns True if other_version matches."),em.forEach(a),Vn.forEach(a),vt.forEach(a),this.h()},h(){y(d,"name","hf:doc:metadata"),y(d,"content",JSON.stringify(Im)),y(l,"id","builder-classes"),y(l,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(l,"href","#builder-classes"),y(g,"class","relative group"),y(Le,"id","datasets.DatasetBuilder"),y(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(Le,"href","#datasets.DatasetBuilder"),y($e,"class","relative group"),y(ua,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.DatasetBuilder"),y(ha,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.BuilderConfig"),y(ba,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.DatasetBuilder.download_and_prepare"),y(xa,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.DatasetBuilder.as_dataset"),y(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(wa,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.DatasetBuilder"),y(Ea,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.BuilderConfig"),y(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(Ue,"id","datasets.DownloadManager"),y(Ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(Ue,"href","#datasets.DownloadManager"),y(Ee,"class","relative group"),y(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(Ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(De,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(st,"id","datasets.SplitGenerator"),y(st,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(st,"href","#datasets.SplitGenerator"),y(Ie,"class","relative group"),y(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(Ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ct,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(mt,"id","datasets.Version"),y(mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),y(mt,"href","#datasets.Version"),y(Pe,"class","relative group"),y(gt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(s,_){e(document.head,d),$(s,h,_),$(s,g,_),e(g,l),e(l,u),x(t,u,null),e(g,i),e(g,os),e(os,Mn),$(s,en,_),$(s,$e,_),e($e,Le),e(Le,ls),x(bt,ls,null),e($e,qn),e($e,ds),e(ds,Fn),$(s,tn,_),$(s,K,_),e(K,zn),e(K,ua),e(ua,Un),e(K,Gn),e(K,ha),e(ha,Hn),e(K,Wn),$(s,an,_),$(s,S,_),x(xt,S,null),e(S,Xn),e(S,is),e(is,Jn),e(S,Kn),e(S,_a),e(_a,ps),e(ps,Yn),e(_a,Qn),e(S,Zn),e(S,ve),e(ve,$a),e($a,cs),e(cs,eo),e($a,to),e(ve,ao),e(ve,va),e(va,ba),e(ba,so),e(va,ro),e(ve,no),e(ve,Oe),e(Oe,xa),e(xa,oo),e(Oe,lo),e(Oe,ms),e(ms,io),e(Oe,po),e(S,co),e(S,W),e(W,fs),e(fs,mo),e(W,fo),e(W,gs),e(gs,go),e(W,uo),e(W,us),e(us,ho),e(W,_o),e(W,hs),e(hs,$o),e(W,vo),e(S,bo),e(S,Y),x(wt,Y,null),e(Y,xo),e(Y,_s),e(_s,wo),e(Y,Eo),x(Ve,Y,null),e(S,Do),e(S,Q),x(Et,Q,null),e(Q,yo),e(Q,$s),e($s,ko),e(Q,jo),x(Me,Q,null),e(S,So),e(S,Z),x(Dt,Z,null),e(Z,To),e(Z,vs),e(vs,Io),e(Z,Bo),x(qe,Z,null),e(S,No),e(S,ee),x(yt,ee,null),e(ee,Po),e(ee,bs),e(bs,Ro),e(ee,Ao),x(Fe,ee,null),e(S,Co),e(S,ze),x(kt,ze,null),e(ze,Lo),e(ze,xs),e(xs,Oo),$(s,sn,_),$(s,X,_),x(jt,X,null),e(X,Vo),e(X,ws),e(ws,Mo),e(X,qo),e(X,te),e(te,Es),e(Es,Fo),e(te,zo),e(te,Ds),e(Ds,Uo),e(te,Go),e(te,ys),e(ys,Ho),e(te,Wo),$(s,rn,_),$(s,be,_),x(St,be,null),e(be,Xo),e(be,ks),e(ks,Jo),$(s,nn,_),$(s,xe,_),x(Tt,xe,null),e(xe,Ko),e(xe,js),e(js,Yo),$(s,on,_),$(s,z,_),x(It,z,null),e(z,Qo),e(z,Bt),e(Bt,Zo),e(Bt,wa),e(wa,el),e(Bt,tl),e(z,al),e(z,Nt),e(Nt,sl),e(Nt,Ea),e(Ea,rl),e(Nt,nl),e(z,ol),e(z,ae),x(Pt,ae,null),e(ae,ll),e(ae,Ss),e(Ss,dl),e(ae,il),e(ae,we),e(we,Ts),e(Ts,pl),e(we,cl),e(we,Is),e(Is,ml),e(we,fl),e(we,Bs),e(Bs,gl),$(s,ln,_),$(s,Ee,_),e(Ee,Ue),e(Ue,Ns),x(Rt,Ns,null),e(Ee,ul),e(Ee,Ps),e(Ps,hl),$(s,dn,_),$(s,P,_),x(At,P,null),e(P,_l),e(P,se),x(Ct,se,null),e(se,$l),e(se,Rs),e(Rs,vl),e(se,bl),x(Ge,se,null),e(P,xl),e(P,re),x(Lt,re,null),e(re,wl),e(re,As),e(As,El),e(re,Dl),x(He,re,null),e(P,yl),e(P,ne),x(Ot,ne,null),e(ne,kl),e(ne,Vt),e(Vt,jl),e(Vt,Cs),e(Cs,Sl),e(Vt,Tl),e(ne,Il),x(We,ne,null),e(P,Bl),e(P,oe),x(Mt,oe,null),e(oe,Nl),e(oe,Ls),e(Ls,Pl),e(oe,Rl),x(Xe,oe,null),e(P,Al),e(P,le),x(qt,le,null),e(le,Cl),e(le,Os),e(Os,Ll),e(le,Ol),x(Je,le,null),e(P,Vl),e(P,de),x(Ft,de,null),e(de,Ml),e(de,Vs),e(Vs,ql),e(de,Fl),x(Ke,de,null),e(P,zl),e(P,Ye),x(zt,Ye,null),e(Ye,Ul),e(Ye,Ms),e(Ms,Gl),$(s,pn,_),$(s,A,_),x(Ut,A,null),e(A,Hl),e(A,U),e(U,Wl),e(U,qs),e(qs,Xl),e(U,Jl),e(U,Fs),e(Fs,Kl),e(U,Yl),e(U,zs),e(zs,Ql),e(U,Zl),e(U,Us),e(Us,ed),e(U,td),e(A,ad),e(A,ie),x(Gt,ie,null),e(ie,sd),e(ie,Gs),e(Gs,rd),e(ie,nd),x(Qe,ie,null),e(A,od),e(A,pe),x(Ht,pe,null),e(pe,ld),e(pe,Hs),e(Hs,dd),e(pe,id),x(Ze,pe,null),e(A,pd),e(A,ce),x(Wt,ce,null),e(ce,cd),e(ce,Ws),e(Ws,md),e(ce,fd),x(et,ce,null),e(A,gd),e(A,me),x(Xt,me,null),e(me,ud),e(me,Xs),e(Xs,hd),e(me,_d),x(tt,me,null),e(A,$d),e(A,fe),x(Jt,fe,null),e(fe,vd),e(fe,Js),e(Js,bd),e(fe,xd),x(at,fe,null),$(s,cn,_),$(s,De,_),x(Kt,De,null),e(De,wd),e(De,Ks),e(Ks,Ed),$(s,mn,_),$(s,V,_),x(Yt,V,null),e(V,Dd),e(V,Da),e(Da,Ys),e(Ys,yd),e(Da,kd),e(V,jd),e(V,Qt),e(Qt,Sd),e(Qt,Qs),e(Qs,Td),e(Qt,Id),e(V,Bd),e(V,Zs),e(Zs,Nd),e(V,Pd),e(V,Zt),e(Zt,er),e(er,ye),e(ye,fn),e(ye,Rd),e(ye,tr),e(tr,Ad),e(ye,Cd),e(ye,ar),e(ar,Ld),e(Zt,Od),e(Zt,ke),e(ke,je),e(je,ya),e(ya,sr),e(sr,Vd),e(ya,Md),e(je,qd),e(je,rr),e(rr,Fd),e(je,zd),e(je,nr),e(nr,Ud),e(ke,Gd),e(ke,Se),e(Se,or),e(or,lr),e(lr,Hd),e(Se,Wd),e(Se,dr),e(dr,Xd),e(Se,Jd),e(Se,ir),e(ir,Kd),e(ke,Yd),e(ke,Te),e(Te,pr),e(pr,cr),e(cr,Qd),e(Te,Zd),e(Te,mr),e(mr,ei),e(Te,ti),e(Te,fr),e(fr,ai),$(s,gn,_),$(s,Ie,_),e(Ie,st),e(st,gr),x(ea,gr,null),e(Ie,si),e(Ie,ur),e(ur,ri),$(s,un,_),$(s,G,_),x(ta,G,null),e(G,ni),e(G,hr),e(hr,oi),e(G,li),e(G,Be),e(Be,di),e(Be,_r),e(_r,ii),e(Be,pi),e(Be,$r),e($r,ci),e(Be,mi),e(G,fi),x(rt,G,null),$(s,hn,_),$(s,C,_),x(aa,C,null),e(C,gi),e(C,ka),e(ka,vr),e(vr,ui),e(ka,hi),e(C,_i),e(C,br),e(br,$i),e(C,vi),e(C,J),e(J,ja),e(ja,xr),e(xr,bi),e(ja,xi),e(J,wi),e(J,Sa),e(Sa,wr),e(wr,Ei),e(Sa,Di),e(J,yi),e(J,Ta),e(Ta,Er),e(Er,ki),e(Ta,ji),e(J,Si),e(J,Ia),e(Ia,Dr),e(Dr,Ti),e(Ia,Ii),e(C,Bi),e(C,Ba),e(Ba,Ni),e(Ba,yr),e(yr,Pi),e(C,Ri),e(C,sa),e(sa,Ai),e(sa,kr),e(kr,Ci),e(sa,Li),e(C,Oi),x(nt,C,null),$(s,_n,_),$(s,R,_),x(ra,R,null),e(R,Vi),e(R,jr),e(jr,Mi),e(R,qi),x(ot,R,null),e(R,Fi),e(R,Sr),e(Sr,zi),e(R,Ui),x(lt,R,null),e(R,Gi),e(R,Tr),e(Tr,Hi),e(R,Wi),x(dt,R,null),e(R,Xi),x(it,R,null),$(s,$n,_),$(s,Ne,_),x(na,Ne,null),e(Ne,Ji),e(Ne,Ir),e(Ir,Ki),$(s,vn,_),$(s,M,_),x(oa,M,null),e(M,Yi),e(M,Br),e(Br,Qi),e(M,Zi),x(pt,M,null),e(M,ep),e(M,ct),x(la,ct,null),e(ct,tp),e(ct,Nr),e(Nr,ap),e(M,sp),e(M,ge),x(da,ge,null),e(ge,rp),e(ge,Pr),e(Pr,np),e(ge,op),e(ge,Rr),e(Rr,lp),$(s,bn,_),$(s,Pe,_),e(Pe,mt),e(mt,Ar),x(ia,Ar,null),e(Pe,dp),e(Pe,Cr),e(Cr,ip),$(s,xn,_),$(s,H,_),x(pa,H,null),e(H,pp),e(H,Lr),e(Lr,cp),e(H,mp),x(ft,H,null),e(H,fp),e(H,gt),x(ca,gt,null),e(gt,gp),e(gt,Or),e(Or,up),wn=!0},p(s,[_]){const ma={};_&2&&(ma.$$scope={dirty:_,ctx:s}),Ve.$set(ma);const Vr={};_&2&&(Vr.$$scope={dirty:_,ctx:s}),Me.$set(Vr);const Mr={};_&2&&(Mr.$$scope={dirty:_,ctx:s}),qe.$set(Mr);const qr={};_&2&&(qr.$$scope={dirty:_,ctx:s}),Fe.$set(qr);const fa={};_&2&&(fa.$$scope={dirty:_,ctx:s}),Ge.$set(fa);const Fr={};_&2&&(Fr.$$scope={dirty:_,ctx:s}),He.$set(Fr);const zr={};_&2&&(zr.$$scope={dirty:_,ctx:s}),We.$set(zr);const Ur={};_&2&&(Ur.$$scope={dirty:_,ctx:s}),Xe.$set(Ur);const Re={};_&2&&(Re.$$scope={dirty:_,ctx:s}),Je.$set(Re);const Gr={};_&2&&(Gr.$$scope={dirty:_,ctx:s}),Ke.$set(Gr);const Hr={};_&2&&(Hr.$$scope={dirty:_,ctx:s}),Qe.$set(Hr);const T={};_&2&&(T.$$scope={dirty:_,ctx:s}),Ze.$set(T);const Wr={};_&2&&(Wr.$$scope={dirty:_,ctx:s}),et.$set(Wr);const Na={};_&2&&(Na.$$scope={dirty:_,ctx:s}),tt.$set(Na);const Xr={};_&2&&(Xr.$$scope={dirty:_,ctx:s}),at.$set(Xr);const Ae={};_&2&&(Ae.$$scope={dirty:_,ctx:s}),rt.$set(Ae);const Pa={};_&2&&(Pa.$$scope={dirty:_,ctx:s}),nt.$set(Pa);const Jr={};_&2&&(Jr.$$scope={dirty:_,ctx:s}),ot.$set(Jr);const Ra={};_&2&&(Ra.$$scope={dirty:_,ctx:s}),lt.$set(Ra);const Kr={};_&2&&(Kr.$$scope={dirty:_,ctx:s}),dt.$set(Kr);const ut={};_&2&&(ut.$$scope={dirty:_,ctx:s}),it.$set(ut);const Yr={};_&2&&(Yr.$$scope={dirty:_,ctx:s}),pt.$set(Yr);const Qr={};_&2&&(Qr.$$scope={dirty:_,ctx:s}),ft.$set(Qr)},i(s){wn||(w(t.$$.fragment,s),w(bt.$$.fragment,s),w(xt.$$.fragment,s),w(wt.$$.fragment,s),w(Ve.$$.fragment,s),w(Et.$$.fragment,s),w(Me.$$.fragment,s),w(Dt.$$.fragment,s),w(qe.$$.fragment,s),w(yt.$$.fragment,s),w(Fe.$$.fragment,s),w(kt.$$.fragment,s),w(jt.$$.fragment,s),w(St.$$.fragment,s),w(Tt.$$.fragment,s),w(It.$$.fragment,s),w(Pt.$$.fragment,s),w(Rt.$$.fragment,s),w(At.$$.fragment,s),w(Ct.$$.fragment,s),w(Ge.$$.fragment,s),w(Lt.$$.fragment,s),w(He.$$.fragment,s),w(Ot.$$.fragment,s),w(We.$$.fragment,s),w(Mt.$$.fragment,s),w(Xe.$$.fragment,s),w(qt.$$.fragment,s),w(Je.$$.fragment,s),w(Ft.$$.fragment,s),w(Ke.$$.fragment,s),w(zt.$$.fragment,s),w(Ut.$$.fragment,s),w(Gt.$$.fragment,s),w(Qe.$$.fragment,s),w(Ht.$$.fragment,s),w(Ze.$$.fragment,s),w(Wt.$$.fragment,s),w(et.$$.fragment,s),w(Xt.$$.fragment,s),w(tt.$$.fragment,s),w(Jt.$$.fragment,s),w(at.$$.fragment,s),w(Kt.$$.fragment,s),w(Yt.$$.fragment,s),w(ea.$$.fragment,s),w(ta.$$.fragment,s),w(rt.$$.fragment,s),w(aa.$$.fragment,s),w(nt.$$.fragment,s),w(ra.$$.fragment,s),w(ot.$$.fragment,s),w(lt.$$.fragment,s),w(dt.$$.fragment,s),w(it.$$.fragment,s),w(na.$$.fragment,s),w(oa.$$.fragment,s),w(pt.$$.fragment,s),w(la.$$.fragment,s),w(da.$$.fragment,s),w(ia.$$.fragment,s),w(pa.$$.fragment,s),w(ft.$$.fragment,s),w(ca.$$.fragment,s),wn=!0)},o(s){E(t.$$.fragment,s),E(bt.$$.fragment,s),E(xt.$$.fragment,s),E(wt.$$.fragment,s),E(Ve.$$.fragment,s),E(Et.$$.fragment,s),E(Me.$$.fragment,s),E(Dt.$$.fragment,s),E(qe.$$.fragment,s),E(yt.$$.fragment,s),E(Fe.$$.fragment,s),E(kt.$$.fragment,s),E(jt.$$.fragment,s),E(St.$$.fragment,s),E(Tt.$$.fragment,s),E(It.$$.fragment,s),E(Pt.$$.fragment,s),E(Rt.$$.fragment,s),E(At.$$.fragment,s),E(Ct.$$.fragment,s),E(Ge.$$.fragment,s),E(Lt.$$.fragment,s),E(He.$$.fragment,s),E(Ot.$$.fragment,s),E(We.$$.fragment,s),E(Mt.$$.fragment,s),E(Xe.$$.fragment,s),E(qt.$$.fragment,s),E(Je.$$.fragment,s),E(Ft.$$.fragment,s),E(Ke.$$.fragment,s),E(zt.$$.fragment,s),E(Ut.$$.fragment,s),E(Gt.$$.fragment,s),E(Qe.$$.fragment,s),E(Ht.$$.fragment,s),E(Ze.$$.fragment,s),E(Wt.$$.fragment,s),E(et.$$.fragment,s),E(Xt.$$.fragment,s),E(tt.$$.fragment,s),E(Jt.$$.fragment,s),E(at.$$.fragment,s),E(Kt.$$.fragment,s),E(Yt.$$.fragment,s),E(ea.$$.fragment,s),E(ta.$$.fragment,s),E(rt.$$.fragment,s),E(aa.$$.fragment,s),E(nt.$$.fragment,s),E(ra.$$.fragment,s),E(ot.$$.fragment,s),E(lt.$$.fragment,s),E(dt.$$.fragment,s),E(it.$$.fragment,s),E(na.$$.fragment,s),E(oa.$$.fragment,s),E(pt.$$.fragment,s),E(la.$$.fragment,s),E(da.$$.fragment,s),E(ia.$$.fragment,s),E(pa.$$.fragment,s),E(ft.$$.fragment,s),E(ca.$$.fragment,s),wn=!1},d(s){a(d),s&&a(h),s&&a(g),D(t),s&&a(en),s&&a($e),D(bt),s&&a(tn),s&&a(K),s&&a(an),s&&a(S),D(xt),D(wt),D(Ve),D(Et),D(Me),D(Dt),D(qe),D(yt),D(Fe),D(kt),s&&a(sn),s&&a(X),D(jt),s&&a(rn),s&&a(be),D(St),s&&a(nn),s&&a(xe),D(Tt),s&&a(on),s&&a(z),D(It),D(Pt),s&&a(ln),s&&a(Ee),D(Rt),s&&a(dn),s&&a(P),D(At),D(Ct),D(Ge),D(Lt),D(He),D(Ot),D(We),D(Mt),D(Xe),D(qt),D(Je),D(Ft),D(Ke),D(zt),s&&a(pn),s&&a(A),D(Ut),D(Gt),D(Qe),D(Ht),D(Ze),D(Wt),D(et),D(Xt),D(tt),D(Jt),D(at),s&&a(cn),s&&a(De),D(Kt),s&&a(mn),s&&a(V),D(Yt),s&&a(gn),s&&a(Ie),D(ea),s&&a(un),s&&a(G),D(ta),D(rt),s&&a(hn),s&&a(C),D(aa),D(nt),s&&a(_n),s&&a(R),D(ra),D(ot),D(lt),D(dt),D(it),s&&a($n),s&&a(Ne),D(na),s&&a(vn),s&&a(M),D(oa),D(pt),D(la),D(da),s&&a(bn),s&&a(Pe),D(ia),s&&a(xn),s&&a(H),D(pa),D(ft),D(ca)}}}const Im={local:"builder-classes",sections:[{local:"datasets.DatasetBuilder",title:"Builders"},{local:"datasets.DownloadManager",title:"Download"},{local:"datasets.SplitGenerator",title:"Splits"},{local:"datasets.Version",title:"Version"}],title:"Builder classes"};function Bm(k){return nm(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Lm extends tm{constructor(d){super();am(this,d,Bm,Tm,sm,{})}}export{Lm as default,Im as metadata};
