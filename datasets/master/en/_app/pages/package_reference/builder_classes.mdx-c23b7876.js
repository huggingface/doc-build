import{S as Sl,i as Nl,s as jl,e as s,k as d,w as p,t as o,M as Rl,c as n,d as t,m as l,a as r,x as f,h as i,b as c,F as e,g as v,y as m,L as Cl,q as u,o as g,B as h}from"../../chunks/vendor-e67aec41.js";import{D as b}from"../../chunks/Docstring-ebff01a5.js";import{C as Ht}from"../../chunks/CodeBlock-e2bcf023.js";import{I as kl}from"../../chunks/IconCopyLink-ffd7f84e.js";function Ol(en){let F,ft,N,k,Gt,Ee,tn,Wt,an,ms,M,sn,mt,nn,rn,ut,on,dn,us,$,De,ln,Xt,cn,pn,gt,zt,fn,mn,un,W,ht,Jt,gn,hn,_n,_t,vt,vn,bn,$n,ne,bt,wn,En,Kt,Dn,xn,yn,O,Yt,Tn,In,Qt,Bn,Sn,Zt,Nn,jn,ea,Rn,Cn,re,xe,kn,ta,On,An,oe,ye,Ln,aa,Pn,Vn,ie,Te,Fn,sa,Mn,qn,de,Ie,Un,na,Hn,Gn,le,Be,Wn,ra,Xn,gs,A,Se,zn,oa,Jn,Kn,q,ia,Yn,Qn,da,Zn,er,la,tr,ar,hs,X,Ne,sr,ca,nr,_s,z,je,rr,pa,or,vs,S,Re,ir,Ce,dr,$t,lr,cr,pr,ke,fr,wt,mr,ur,gr,U,Oe,hr,fa,_r,vr,J,ma,br,$r,ua,wr,Er,ga,Dr,bs,E,Ae,xr,ce,Le,yr,ha,Tr,Ir,j,Pe,Br,_a,Sr,Nr,va,jr,Rr,Ve,Cr,pe,Fe,kr,Me,Or,ba,Ar,Lr,Pr,fe,qe,Vr,$a,Fr,Mr,me,Ue,qr,wa,Ur,Hr,ue,He,Gr,Ea,Wr,Xr,ge,Ge,zr,Da,Jr,$s,B,We,Kr,Et,xa,Yr,Qr,Zr,Xe,eo,ya,to,ao,so,Ta,no,ro,ze,Ia,K,ws,oo,Ba,io,lo,Sa,co,po,Y,Q,Dt,Na,fo,mo,uo,ja,go,ho,Ra,_o,vo,Z,Ca,ka,bo,$o,Oa,wo,Eo,Aa,Do,xo,ee,La,Pa,yo,To,Va,Io,Bo,Fa,So,Es,L,Je,No,Ma,jo,Ro,te,Co,qa,ko,Oo,Ua,Ao,Lo,Ds,y,Ke,Po,xt,Ha,Vo,Fo,Mo,Ga,qo,Uo,P,yt,Wa,Ho,Go,Wo,Tt,Xa,Xo,zo,Jo,It,za,Ko,Yo,Qo,Bt,Ja,Zo,ei,ti,St,ai,Ka,si,ni,Ye,ri,Ya,oi,ii,xs,w,Qe,di,Qa,li,ci,Za,pi,fi,Ze,mi,es,ui,gi,et,hi,ts,_i,vi,tt,bi,as,$i,wi,at,ys,ae,st,Ei,ss,Di,Ts,T,nt,xi,ns,yi,Ti,rs,Ii,Bi,rt,Si,he,ot,Ni,os,ji,Ri,H,it,Ci,is,ki,Oi,ds,Ai,Is,se,dt,Li,ls,Pi,Bs,V,lt,Vi,cs,Fi,Mi,_e,ct,qi,ps,Ui,Ss;return Ee=new kl({}),De=new b({props:{name:"class datasets.DatasetBuilder",anchor:"datasets.DatasetBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"namespace",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L177"}}),xe=new b({props:{name:"as_dataset",anchor:"datasets.DatasetBuilder.as_dataset",parameters:[{name:"split",val:": typing.Optional[datasets.splits.Split] = None"},{name:"run_post_process",val:" = True"},{name:"ignore_verifications",val:" = False"},{name:"in_memory",val:" = False"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L741",parametersDescription:[{anchor:"datasets.DatasetBuilder.as_dataset.split",description:"<strong>split</strong> (<code>datasets.Split</code>) &#x2014; Which subset of the data to return.",name:"split"},{anchor:"datasets.DatasetBuilder.as_dataset.run_post_process",description:`<strong>run_post_process</strong> (bool, default=True) &#x2014; Whether to run post-processing dataset transforms and/or add
indexes.`,name:"run_post_process"},{anchor:"datasets.DatasetBuilder.as_dataset.ignore_verifications",description:`<strong>ignore_verifications</strong> (bool, default=False) &#x2014; Whether to ignore the verifications of the
downloaded/processed dataset information (checksums/size/splits/&#x2026;).`,name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.as_dataset.in_memory",description:"<strong>in_memory</strong> (bool, default=False) &#x2014; Whether to copy the data in-memory.",name:"in_memory"}],returnDescription:`
<p>datasets.Dataset</p>
`}}),ye=new b({props:{name:"download_and_prepare",anchor:"datasets.DatasetBuilder.download_and_prepare",parameters:[{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"download_mode",val:": typing.Optional[datasets.utils.download_manager.DownloadMode] = None"},{name:"ignore_verifications",val:": bool = False"},{name:"try_from_hf_gcs",val:": bool = True"},{name:"dl_manager",val:": typing.Optional[datasets.utils.download_manager.DownloadManager] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"**download_and_prepare_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L484",parametersDescription:[{anchor:"datasets.DatasetBuilder.download_and_prepare.download_config",description:'<strong>download_config</strong> (<a href="/docs/datasets/master/en/package_reference/builder_classes#datasets.DownloadConfig">DownloadConfig</a>, optional) &#x2014; specific download configuration parameters.',name:"download_config"},{anchor:"datasets.DatasetBuilder.download_and_prepare.download_mode",description:'<strong>download_mode</strong> (<a href="/docs/datasets/master/en/package_reference/builder_classes#datasets.DownloadMode">DownloadMode</a>, optional) &#x2014; select the download/generate mode - Default to <code>REUSE_DATASET_IF_EXISTS</code>',name:"download_mode"},{anchor:"datasets.DatasetBuilder.download_and_prepare.ignore_verifications",description:"<strong>ignore_verifications</strong> (<code>bool</code>) &#x2014; Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/&#x2026;)",name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.download_and_prepare.save_infos",description:"<strong>save_infos</strong> (<code>bool</code>) &#x2014; Save the dataset information (checksums/size/splits/&#x2026;)",name:"save_infos"},{anchor:"datasets.DatasetBuilder.download_and_prepare.try_from_hf_gcs",description:"<strong>try_from_hf_gcs</strong> (<code>bool</code>) &#x2014; If True, it will try to download the already prepared dataset from the Hf google cloud storage",name:"try_from_hf_gcs"},{anchor:"datasets.DatasetBuilder.download_and_prepare.dl_manager",description:'<strong>dl_manager</strong> (<a href="/docs/datasets/master/en/package_reference/builder_classes#datasets.DownloadManager">DownloadManager</a>, optional) &#x2014; specific Download Manger to use',name:"dl_manager"},{anchor:"datasets.DatasetBuilder.download_and_prepare.base_path",description:`<strong>base_path</strong> (<code>str</code>, optional) &#x2014; base path for relative paths that are used to download files. This can be a remote url.
If not specified, the value of the <em>base_path</em> attribute (<em>self.base_path</em>) will be used instead.`,name:"base_path"},{anchor:"datasets.DatasetBuilder.download_and_prepare.use_auth_token",description:`<strong>use_auth_token</strong> (<code>Union[str, bool]</code>, optional) &#x2014; Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.
If True, will get token from ~/.huggingface.`,name:"use_auth_token"}]}}),Te=new b({props:{name:"get_all_exported_dataset_infos",anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos",parameters:[],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L317"}}),Ie=new b({props:{name:"get_exported_dataset_info",anchor:"datasets.DatasetBuilder.get_exported_dataset_info",parameters:[],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L325"}}),Be=new b({props:{name:"get_imported_module_dir",anchor:"datasets.DatasetBuilder.get_imported_module_dir",parameters:[],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L479"}}),Se=new b({props:{name:"class datasets.GeneratorBasedBuilder",anchor:"datasets.GeneratorBasedBuilder",parameters:[{name:"*args",val:""},{name:"writer_batch_size",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L1009"}}),Ne=new b({props:{name:"class datasets.BeamBasedBuilder",anchor:"datasets.BeamBasedBuilder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L1169"}}),je=new b({props:{name:"class datasets.ArrowBasedBuilder",anchor:"datasets.ArrowBasedBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"namespace",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L1107"}}),Re=new b({props:{name:"class datasets.BuilderConfig",anchor:"datasets.BuilderConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = '0.0.0'"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L81",parametersDescription:[{anchor:"datasets.BuilderConfig.name",description:"<strong>name</strong> (<code>str</code>, default <code>&quot;default&quot;</code>) &#x2014;",name:"name"},{anchor:"datasets.BuilderConfig.version",description:'<strong>version</strong> (<a href="/docs/datasets/master/en/package_reference/builder_classes#datasets.Version">Version</a> or <code>str</code>, optional) &#x2014;',name:"version"},{anchor:"datasets.BuilderConfig.data_dir",description:"<strong>data_dir</strong> (<code>str</code>, optional) &#x2014;",name:"data_dir"},{anchor:"datasets.BuilderConfig.data_files",description:"<strong>data_files</strong> (<code>str</code> or <code>Sequence</code> or <code>Mapping</code>, optional) &#x2014; Path(s) to source data file(s).",name:"data_files"},{anchor:"datasets.BuilderConfig.description",description:"<strong>description</strong> (<code>str</code>, optional) &#x2014;",name:"description"}]}}),Oe=new b({props:{name:"create_config_id",anchor:"datasets.BuilderConfig.create_config_id",parameters:[{name:"config_kwargs",val:": dict"},{name:"custom_features",val:": typing.Optional[datasets.features.features.Features] = None"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L120"}}),Ae=new b({props:{name:"class datasets.DownloadManager",anchor:"datasets.DownloadManager",parameters:[{name:"dataset_name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"record_checksums",val:": bool = True"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L141"}}),Le=new b({props:{name:"download",anchor:"datasets.DownloadManager.download",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L259",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Pe=new b({props:{name:"download_and_extract",anchor:"datasets.DownloadManager.download_and_extract",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L367",returnDescription:`
<p><code>str</code>, extracted paths of given URL(s).</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Ve=new Ht({props:{code:"extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))",highlighted:'<span class="hljs-attr">extracted_paths</span> = dl_manager.extract(dl_manager.download(url_or_urls))'}}),Fe=new b({props:{name:"download_custom",anchor:"datasets.DownloadManager.download_custom",parameters:[{name:"url_or_urls",val:""},{name:"custom_download",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L218",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),qe=new b({props:{name:"extract",anchor:"datasets.DownloadManager.extract",parameters:[{name:"path_or_paths",val:""},{name:"num_proc",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L337",returnDescription:`
<p><code>str</code>, The extracted paths matching the given input
path_or_paths.</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Ue=new b({props:{name:"iter_archive",anchor:"datasets.DownloadManager.iter_archive",parameters:[{name:"path_or_buf",val:": typing.Union[str, _io.BufferedReader]"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L310",parametersDescription:[{anchor:"datasets.DownloadManager.iter_archive.path_or_buf",description:"<strong>path_or_buf</strong> (<code>str</code> or <code>io.BufferedReader</code>) &#x2014; Archive path or archive binary file object.",name:"path_or_buf"}]}}),He=new b({props:{name:"iter_files",anchor:"datasets.DownloadManager.iter_files",parameters:[{name:"paths",val:": typing.Union[str, typing.List[str]]"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L326",parametersDescription:[{anchor:"datasets.DownloadManager.iter_files.paths",description:"<strong>paths</strong> (<code>str</code> or <code>list</code> of <code>str</code>) &#x2014; Root paths.",name:"paths"}]}}),Ge=new b({props:{name:"ship_files_with_pipeline",anchor:"datasets.DownloadManager.ship_files_with_pipeline",parameters:[{name:"downloaded_path_or_paths",val:""},{name:"pipeline",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L183"}}),We=new b({props:{name:"class datasets.DownloadMode",anchor:"datasets.DownloadMode",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L44"}}),Je=new b({props:{name:"class datasets.SplitGenerator",anchor:"datasets.SplitGenerator",parameters:[{name:"name",val:": str"},{name:"gen_kwargs",val:": typing.Dict = <factory>"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/splits.py#L549",parametersDescription:[{anchor:"datasets.SplitGenerator.name",description:`<strong>name</strong> (str) &#x2014; Name of the Split for which the generator will
create the examples.
**gen_kwargs &#x2014; Keyword arguments to forward to the <code>DatasetBuilder._generate_examples</code> method
of the builder.`,name:"name"}]}}),Ke=new b({props:{name:"class datasets.Split",anchor:"datasets.Split",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/splits.py#L387"}}),Qe=new b({props:{name:"class datasets.NamedSplit",anchor:"datasets.NamedSplit",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/splits.py#L303"}}),Ze=new Ht({props:{code:`Each descriptor can be composed with other using addition or slice. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[0:25]) + datasets.Split.TEST

The resulting split will correspond to 25% of the train split merged with
100% of the test split.`,highlighted:`Each descriptor can be composed <span class="hljs-keyword">with</span> other using addition <span class="hljs-keyword">or</span> <span class="hljs-built_in">slice</span>. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">0</span>:<span class="hljs-number">25</span>]) + datasets.Split.TEST

The resulting split will correspond to <span class="hljs-number">25</span>% of the train split merged <span class="hljs-keyword">with</span>
<span class="hljs-number">100</span>% of the test split.`}}),et=new Ht({props:{code:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[75:])
)  # Error
split = datasets.Split.TEST + datasets.Split.ALL  # Error`,highlighted:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">75</span>:])
)  <span class="hljs-comment"># Error</span>
split = datasets.Split.TEST + datasets.Split.ALL  <span class="hljs-comment"># Error</span>`}}),tt=new Ht({props:{code:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
datasets.Split.TEST.subsplit(datasets.percent[:50])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:50])`,highlighted:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
datasets.Split.TEST.subsplit(datasets.percent[:<span class="hljs-number">50</span>])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),at=new Ht({props:{code:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:25]).subsplit(datasets.percent[:25])
split = (train.subsplit(datasets.percent[:25]) + test).subsplit(datasets.percent[:50])`,highlighted:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]).subsplit(datasets.percent[:<span class="hljs-number">25</span>])
split = (train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) + test).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),st=new b({props:{name:"class datasets.NamedSplitAll",anchor:"datasets.NamedSplitAll",parameters:[],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/splits.py#L372"}}),nt=new b({props:{name:"class datasets.ReadInstruction",anchor:"datasets.ReadInstruction",parameters:[{name:"split_name",val:""},{name:"rounding",val:" = None"},{name:"from_",val:" = None"},{name:"to",val:" = None"},{name:"unit",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_reader.py#L456"}}),rt=new Ht({props:{code:`# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec('test[:33%]'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction('test', to=33, unit='%'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%'))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]+train[1:-1]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%]+train[1:-1]'))
ds = datasets.load_dataset('mnist', split=(
datasets.ReadInstruction('test', to=33, unit='%') +
datasets.ReadInstruction('train', from_=1, to=-1, unit='abs')))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%](pct1_dropremainder)')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%](pct1_dropremainder)'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%', rounding="pct1_dropremainder"))

# 10-fold validation:
tests = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', from_=k, to=k+10, unit='%')
for k in range(0, 100, 10)])
trains = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')
for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(<span class="hljs-string">&#x27;test[:33%]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=(
datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) +
datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">1</span>, to=-<span class="hljs-number">1</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>)))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&quot;pct1_dropremainder&quot;</span>))

<span class="hljs-comment"># 10-fold validation:</span>
tests = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
trains = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}),ot=new b({props:{name:"from_spec",anchor:"datasets.ReadInstruction.from_spec",parameters:[{name:"spec",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_reader.py#L536",parametersDescription:[{anchor:"datasets.ReadInstruction.from_spec.spec",description:`<strong>spec</strong> (str) &#x2014; split(s) + optional slice(s) to read + optional rounding
if percents are used as the slicing unit. A slice can be specified,
using absolute numbers (int) or percentages (int). E.g.
<code>test</code>: test split.
<code>test + validation</code>: test split + validation split.
<code>test[10:]</code>: test split, minus its first 10 records.
<code>test[:10%]</code>: first 10% records of test split.
<code>test[:20%](pct1_dropremainder)</code>: first 10% records, rounded with
the <code>pct1_dropremainder</code> rounding.
<code>test[:-5%]+train[40%:60%]</code>: first 95% of test + middle 20% of
train.`,name:"spec"}],returnDescription:`
<p>ReadInstruction instance.</p>
`}}),it=new b({props:{name:"to_absolute",anchor:"datasets.ReadInstruction.to_absolute",parameters:[{name:"name2len",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_reader.py#L604",returnDescription:`
<p>list of _AbsoluteInstruction instances (corresponds to the + in spec).</p>
`}}),dt=new b({props:{name:"class datasets.DownloadConfig",anchor:"datasets.DownloadConfig",parameters:[{name:"cache_dir",val:": typing.Union[pathlib.Path, str, NoneType] = None"},{name:"force_download",val:": bool = False"},{name:"resume_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"proxies",val:": typing.Optional[typing.Dict] = None"},{name:"user_agent",val:": typing.Optional[str] = None"},{name:"extract_compressed_file",val:": bool = False"},{name:"force_extract",val:": bool = False"},{name:"delete_extracted",val:": bool = False"},{name:"use_etag",val:": bool = True"},{name:"num_proc",val:": typing.Optional[int] = None"},{name:"max_retries",val:": int = 1"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"ignore_url_params",val:": bool = False"},{name:"download_desc",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/file_utils.py#L153",parametersDescription:[{anchor:"datasets.DownloadConfig.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>Path</code>, optional) &#x2014; Specify a cache directory to save the file to (overwrite the
default cache dir).`,name:"cache_dir"},{anchor:"datasets.DownloadConfig.force_download",description:`<strong>force_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, re-dowload the file even if it&#x2019;s already cached in
the cache dir.`,name:"force_download"},{anchor:"datasets.DownloadConfig.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, resume the download if incompletly recieved file is
found.`,name:"resume_download"},{anchor:"datasets.DownloadConfig.proxies",description:"<strong>proxies</strong> (<code>dict</code>, optional) &#x2014;",name:"proxies"},{anchor:"datasets.DownloadConfig.user_agent",description:`<strong>user_agent</strong> (<code>str</code>, optional) &#x2014; Optional string or dict that will be appended to the user-agent on remote
requests.`,name:"user_agent"},{anchor:"datasets.DownloadConfig.extract_compressed_file",description:`<strong>extract_compressed_file</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True and the path point to a zip or tar file,
extract the compressed file in a folder along the archive.`,name:"extract_compressed_file"},{anchor:"datasets.DownloadConfig.force_extract",description:`<strong>force_extract</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True when extract_compressed_file is True and the archive
was already extracted, re-extract the archive and override the folder where it was extracted.`,name:"force_extract"},{anchor:"datasets.DownloadConfig.delete_extracted",description:"<strong>delete_extracted</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to delete (or keep) the extracted files.",name:"delete_extracted"},{anchor:"datasets.DownloadConfig.use_etag",description:"<strong>use_etag</strong> (<code>bool</code>, default <code>True</code>) &#x2014; Whether to use the ETag HTTP response header to validate the cached files.",name:"use_etag"},{anchor:"datasets.DownloadConfig.num_proc",description:"<strong>num_proc</strong> (<code>int</code>, optional) &#x2014; The number of processes to launch to download the files in parallel.",name:"num_proc"},{anchor:"datasets.DownloadConfig.max_retries",description:"<strong>max_retries</strong> (<code>int</code>, default <code>1</code>) &#x2014; The number of times to retry an HTTP request if it fails.",name:"max_retries"},{anchor:"datasets.DownloadConfig.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, optional) &#x2014; Optional string or boolean to use as Bearer token
for remote files on the Datasets Hub. If True, will get token from ~/.huggingface.`,name:"use_auth_token"},{anchor:"datasets.DownloadConfig.ignore_url_params",description:`<strong>ignore_url_params</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to strip all query parameters and #fragments from
the download URL before using it for caching the file.`,name:"ignore_url_params"},{anchor:"datasets.DownloadConfig.download_desc",description:"<strong>download_desc</strong> (<code>str</code>, optional) &#x2014; A description to be displayed alongside with the progress bar while downloading the files.",name:"download_desc"}]}}),lt=new b({props:{name:"class datasets.Version",anchor:"datasets.Version",parameters:[{name:"version_str",val:": str"},{name:"description",val:": typing.Optional[str] = None"},{name:"major",val:": typing.Union[str, int, NoneType] = None"},{name:"minor",val:": typing.Union[str, int, NoneType] = None"},{name:"patch",val:": typing.Union[str, int, NoneType] = None"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/version.py#L30",parametersDescription:[{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.major",description:"<strong>major</strong> (<code>str</code>) &#x2014;",name:"major"},{anchor:"datasets.Version.minor",description:"<strong>minor</strong> (<code>str</code>) &#x2014;",name:"minor"},{anchor:"datasets.Version.patch",description:"<strong>patch</strong> (<code>str</code>) &#x2014;",name:"patch"}]}}),ct=new b({props:{name:"match",anchor:"datasets.Version.match",parameters:[{name:"other_version",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/version.py#L92"}}),{c(){F=s("meta"),ft=d(),N=s("h1"),k=s("a"),Gt=s("span"),p(Ee.$$.fragment),tn=d(),Wt=s("span"),an=o("Builder classes"),ms=d(),M=s("p"),sn=o("\u{1F917} Datasets relies on two main classes during the dataset building process: "),mt=s("a"),nn=o("datasets.DatasetBuilder"),rn=o(" and "),ut=s("a"),on=o("datasets.BuilderConfig"),dn=o("."),us=d(),$=s("div"),p(De.$$.fragment),ln=d(),Xt=s("p"),cn=o("Abstract base class for all datasets."),pn=d(),gt=s("p"),zt=s("em"),fn=o("DatasetBuilder"),mn=o(" has 3 key methods:"),un=d(),W=s("ul"),ht=s("li"),Jt=s("code"),gn=o("datasets.DatasetBuilder.info"),hn=o(`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),_n=d(),_t=s("li"),vt=s("a"),vn=o("datasets.DatasetBuilder.download_and_prepare()"),bn=o(`: Downloads the source data
and writes it to disk.`),$n=d(),ne=s("li"),bt=s("a"),wn=o("datasets.DatasetBuilder.as_dataset()"),En=o(": Generates a "),Kt=s("em"),Dn=o("Dataset"),xn=o("."),yn=d(),O=s("p"),Yt=s("strong"),Tn=o("Configuration"),In=o(": Some "),Qt=s("em"),Bn=o("DatasetBuilder"),Sn=o(`s expose multiple variants of the
dataset by defining a `),Zt=s("em"),Nn=o("datasets.BuilderConfig"),jn=o(` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),ea=s("code"),Rn=o("datasets.DatasetBuilder.builder_configs()"),Cn=d(),re=s("div"),p(xe.$$.fragment),kn=d(),ta=s("p"),On=o("Return a Dataset for the specified split."),An=d(),oe=s("div"),p(ye.$$.fragment),Ln=d(),aa=s("p"),Pn=o("Downloads and prepares dataset for reading."),Vn=d(),ie=s("div"),p(Te.$$.fragment),Fn=d(),sa=s("p"),Mn=o("Empty dict if doesn\u2019t exist"),qn=d(),de=s("div"),p(Ie.$$.fragment),Un=d(),na=s("p"),Hn=o("Empty DatasetInfo if doesn\u2019t exist"),Gn=d(),le=s("div"),p(Be.$$.fragment),Wn=d(),ra=s("p"),Xn=o("Return the path of the module of this class or subclass."),gs=d(),A=s("div"),p(Se.$$.fragment),zn=d(),oa=s("p"),Jn=o("Base class for datasets with data generation based on dict generators."),Kn=d(),q=s("p"),ia=s("code"),Yn=o("GeneratorBasedBuilder"),Qn=o(` is a convenience class that abstracts away much
of the data writing and reading of `),da=s("code"),Zn=o("DatasetBuilder"),er=o(`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),la=s("code"),tr=o("_split_generators"),ar=o("). See the method docstrings for details."),hs=d(),X=s("div"),p(Ne.$$.fragment),sr=d(),ca=s("p"),nr=o("Beam based Builder."),_s=d(),z=s("div"),p(je.$$.fragment),rr=d(),pa=s("p"),or=o("Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),vs=d(),S=s("div"),p(Re.$$.fragment),ir=d(),Ce=s("p"),dr=o("Base class for "),$t=s("a"),lr=o("DatasetBuilder"),cr=o(" data configuration."),pr=d(),ke=s("p"),fr=o(`DatasetBuilder subclasses with data configuration options should subclass
`),wt=s("a"),mr=o("BuilderConfig"),ur=o(" and add their own properties."),gr=d(),U=s("div"),p(Oe.$$.fragment),hr=d(),fa=s("p"),_r=o(`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),vr=d(),J=s("ul"),ma=s("li"),br=o("the config kwargs that can be used to overwrite attributes"),$r=d(),ua=s("li"),wr=o("the custom features used to write the dataset"),Er=d(),ga=s("li"),Dr=o(`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),bs=d(),E=s("div"),p(Ae.$$.fragment),xr=d(),ce=s("div"),p(Le.$$.fragment),yr=d(),ha=s("p"),Tr=o("Download given url(s)."),Ir=d(),j=s("div"),p(Pe.$$.fragment),Br=d(),_a=s("p"),Sr=o("Download and extract given url_or_urls."),Nr=d(),va=s("p"),jr=o("Is roughly equivalent to:"),Rr=d(),p(Ve.$$.fragment),Cr=d(),pe=s("div"),p(Fe.$$.fragment),kr=d(),Me=s("p"),Or=o("Download given urls(s) by calling "),ba=s("code"),Ar=o("custom_download"),Lr=o("."),Pr=d(),fe=s("div"),p(qe.$$.fragment),Vr=d(),$a=s("p"),Fr=o("Extract given path(s)."),Mr=d(),me=s("div"),p(Ue.$$.fragment),qr=d(),wa=s("p"),Ur=o("Iterate over files within an archive."),Hr=d(),ue=s("div"),p(He.$$.fragment),Gr=d(),Ea=s("p"),Wr=o("Iterate over file paths."),Xr=d(),ge=s("div"),p(Ge.$$.fragment),zr=d(),Da=s("p"),Jr=o("Ship the files using Beam FileSystems to the pipeline temp dir."),$s=d(),B=s("div"),p(We.$$.fragment),Kr=d(),Et=s("p"),xa=s("code"),Yr=o("Enum"),Qr=o(" for how to treat pre-existing downloads and data."),Zr=d(),Xe=s("p"),eo=o("The default mode is "),ya=s("code"),to=o("REUSE_DATASET_IF_EXISTS"),ao=o(`, which will reuse both
raw downloads and the prepared dataset if they exist.`),so=d(),Ta=s("p"),no=o("The generations modes:"),ro=d(),ze=s("table"),Ia=s("thead"),K=s("tr"),ws=s("th"),oo=d(),Ba=s("th"),io=o("Downloads"),lo=d(),Sa=s("th"),co=o("Dataset"),po=d(),Y=s("tbody"),Q=s("tr"),Dt=s("td"),Na=s("code"),fo=o("REUSE_DATASET_IF_EXISTS"),mo=o(" (default)"),uo=d(),ja=s("td"),go=o("Reuse"),ho=d(),Ra=s("td"),_o=o("Reuse"),vo=d(),Z=s("tr"),Ca=s("td"),ka=s("code"),bo=o("REUSE_CACHE_IF_EXISTS"),$o=d(),Oa=s("td"),wo=o("Reuse"),Eo=d(),Aa=s("td"),Do=o("Fresh"),xo=d(),ee=s("tr"),La=s("td"),Pa=s("code"),yo=o("FORCE_REDOWNLOAD"),To=d(),Va=s("td"),Io=o("Fresh"),Bo=d(),Fa=s("td"),So=o("Fresh"),Es=d(),L=s("div"),p(Je.$$.fragment),No=d(),Ma=s("p"),jo=o("Defines the split information for the generator."),Ro=d(),te=s("p"),Co=o(`This should be used as returned value of
`),qa=s("code"),ko=o("GeneratorBasedBuilder._split_generators()"),Oo=o(`
See `),Ua=s("code"),Ao=o("GeneratorBasedBuilder._split_generators()"),Lo=o(`for more info and example
of usage.`),Ds=d(),y=s("div"),p(Ke.$$.fragment),Po=d(),xt=s("p"),Ha=s("code"),Vo=o("Enum"),Fo=o(" for dataset splits."),Mo=d(),Ga=s("p"),qo=o(`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),Uo=d(),P=s("ul"),yt=s("li"),Wa=s("code"),Ho=o("TRAIN"),Go=o(": the training data."),Wo=d(),Tt=s("li"),Xa=s("code"),Xo=o("VALIDATION"),zo=o(`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Jo=d(),It=s("li"),za=s("code"),Ko=o("TEST"),Yo=o(`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Qo=d(),Bt=s("li"),Ja=s("code"),Zo=o("ALL"),ei=o(": the union of all defined dataset splits."),ti=d(),St=s("p"),ai=o("Note: All splits, including compositions inherit from "),Ka=s("code"),si=o("datasets.SplitBase"),ni=d(),Ye=s("p"),ri=o("See the :doc:"),Ya=s("code"),oi=o("guide on splits </loading>"),ii=o(" for more information."),xs=d(),w=s("div"),p(Qe.$$.fragment),di=d(),Qa=s("p"),li=o("Descriptor corresponding to a named split (train, test, \u2026)."),ci=d(),Za=s("p"),pi=o("Example:"),fi=d(),p(Ze.$$.fragment),mi=d(),es=s("p"),ui=o(`Warning:
A split cannot be added twice, so the following will fail:`),gi=d(),p(et.$$.fragment),hi=d(),ts=s("p"),_i=o(`Warning:
The slices can be applied only one time. So the following are valid:`),vi=d(),p(tt.$$.fragment),bi=d(),as=s("p"),$i=o("But not:"),wi=d(),p(at.$$.fragment),ys=d(),ae=s("div"),p(st.$$.fragment),Ei=d(),ss=s("p"),Di=o("Split corresponding to the union of all defined dataset splits."),Ts=d(),T=s("div"),p(nt.$$.fragment),xi=d(),ns=s("p"),yi=o("Reading instruction for a dataset."),Ti=d(),rs=s("p"),Ii=o("Examples:"),Bi=d(),p(rt.$$.fragment),Si=d(),he=s("div"),p(ot.$$.fragment),Ni=d(),os=s("p"),ji=o("Creates a ReadInstruction instance out of a string spec."),Ri=d(),H=s("div"),p(it.$$.fragment),Ci=d(),is=s("p"),ki=o("Translate instruction into a list of absolute instructions."),Oi=d(),ds=s("p"),Ai=o("Those absolute instructions are then to be added together."),Is=d(),se=s("div"),p(dt.$$.fragment),Li=d(),ls=s("p"),Pi=o("Configuration for our cached path manager."),Bs=d(),V=s("div"),p(lt.$$.fragment),Vi=d(),cs=s("p"),Fi=o("Dataset version MAJOR.MINOR.PATCH."),Mi=d(),_e=s("div"),p(ct.$$.fragment),qi=d(),ps=s("p"),Ui=o("Returns True if other_version matches."),this.h()},l(a){const _=Rl('[data-svelte="svelte-1phssyn"]',document.head);F=n(_,"META",{name:!0,content:!0}),_.forEach(t),ft=l(a),N=n(a,"H1",{class:!0});var Ns=r(N);k=n(Ns,"A",{id:!0,class:!0,href:!0});var td=r(k);Gt=n(td,"SPAN",{});var ad=r(Gt);f(Ee.$$.fragment,ad),ad.forEach(t),td.forEach(t),tn=l(Ns),Wt=n(Ns,"SPAN",{});var sd=r(Wt);an=i(sd,"Builder classes"),sd.forEach(t),Ns.forEach(t),ms=l(a),M=n(a,"P",{});var Nt=r(M);sn=i(Nt,"\u{1F917} Datasets relies on two main classes during the dataset building process: "),mt=n(Nt,"A",{href:!0});var nd=r(mt);nn=i(nd,"datasets.DatasetBuilder"),nd.forEach(t),rn=i(Nt," and "),ut=n(Nt,"A",{href:!0});var rd=r(ut);on=i(rd,"datasets.BuilderConfig"),rd.forEach(t),dn=i(Nt,"."),Nt.forEach(t),us=l(a),$=n(a,"DIV",{class:!0});var D=r($);f(De.$$.fragment,D),ln=l(D),Xt=n(D,"P",{});var od=r(Xt);cn=i(od,"Abstract base class for all datasets."),od.forEach(t),pn=l(D),gt=n(D,"P",{});var Hi=r(gt);zt=n(Hi,"EM",{});var id=r(zt);fn=i(id,"DatasetBuilder"),id.forEach(t),mn=i(Hi," has 3 key methods:"),Hi.forEach(t),un=l(D),W=n(D,"UL",{});var jt=r(W);ht=n(jt,"LI",{});var Gi=r(ht);Jt=n(Gi,"CODE",{});var dd=r(Jt);gn=i(dd,"datasets.DatasetBuilder.info"),dd.forEach(t),hn=i(Gi,`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),Gi.forEach(t),_n=l(jt),_t=n(jt,"LI",{});var Wi=r(_t);vt=n(Wi,"A",{href:!0});var ld=r(vt);vn=i(ld,"datasets.DatasetBuilder.download_and_prepare()"),ld.forEach(t),bn=i(Wi,`: Downloads the source data
and writes it to disk.`),Wi.forEach(t),$n=l(jt),ne=n(jt,"LI",{});var fs=r(ne);bt=n(fs,"A",{href:!0});var cd=r(bt);wn=i(cd,"datasets.DatasetBuilder.as_dataset()"),cd.forEach(t),En=i(fs,": Generates a "),Kt=n(fs,"EM",{});var pd=r(Kt);Dn=i(pd,"Dataset"),pd.forEach(t),xn=i(fs,"."),fs.forEach(t),jt.forEach(t),yn=l(D),O=n(D,"P",{});var ve=r(O);Yt=n(ve,"STRONG",{});var fd=r(Yt);Tn=i(fd,"Configuration"),fd.forEach(t),In=i(ve,": Some "),Qt=n(ve,"EM",{});var md=r(Qt);Bn=i(md,"DatasetBuilder"),md.forEach(t),Sn=i(ve,`s expose multiple variants of the
dataset by defining a `),Zt=n(ve,"EM",{});var ud=r(Zt);Nn=i(ud,"datasets.BuilderConfig"),ud.forEach(t),jn=i(ve,` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),ea=n(ve,"CODE",{});var gd=r(ea);Rn=i(gd,"datasets.DatasetBuilder.builder_configs()"),gd.forEach(t),ve.forEach(t),Cn=l(D),re=n(D,"DIV",{class:!0});var js=r(re);f(xe.$$.fragment,js),kn=l(js),ta=n(js,"P",{});var hd=r(ta);On=i(hd,"Return a Dataset for the specified split."),hd.forEach(t),js.forEach(t),An=l(D),oe=n(D,"DIV",{class:!0});var Rs=r(oe);f(ye.$$.fragment,Rs),Ln=l(Rs),aa=n(Rs,"P",{});var _d=r(aa);Pn=i(_d,"Downloads and prepares dataset for reading."),_d.forEach(t),Rs.forEach(t),Vn=l(D),ie=n(D,"DIV",{class:!0});var Cs=r(ie);f(Te.$$.fragment,Cs),Fn=l(Cs),sa=n(Cs,"P",{});var vd=r(sa);Mn=i(vd,"Empty dict if doesn\u2019t exist"),vd.forEach(t),Cs.forEach(t),qn=l(D),de=n(D,"DIV",{class:!0});var ks=r(de);f(Ie.$$.fragment,ks),Un=l(ks),na=n(ks,"P",{});var bd=r(na);Hn=i(bd,"Empty DatasetInfo if doesn\u2019t exist"),bd.forEach(t),ks.forEach(t),Gn=l(D),le=n(D,"DIV",{class:!0});var Os=r(le);f(Be.$$.fragment,Os),Wn=l(Os),ra=n(Os,"P",{});var $d=r(ra);Xn=i($d,"Return the path of the module of this class or subclass."),$d.forEach(t),Os.forEach(t),D.forEach(t),gs=l(a),A=n(a,"DIV",{class:!0});var Rt=r(A);f(Se.$$.fragment,Rt),zn=l(Rt),oa=n(Rt,"P",{});var wd=r(oa);Jn=i(wd,"Base class for datasets with data generation based on dict generators."),wd.forEach(t),Kn=l(Rt),q=n(Rt,"P",{});var pt=r(q);ia=n(pt,"CODE",{});var Ed=r(ia);Yn=i(Ed,"GeneratorBasedBuilder"),Ed.forEach(t),Qn=i(pt,` is a convenience class that abstracts away much
of the data writing and reading of `),da=n(pt,"CODE",{});var Dd=r(da);Zn=i(Dd,"DatasetBuilder"),Dd.forEach(t),er=i(pt,`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),la=n(pt,"CODE",{});var xd=r(la);tr=i(xd,"_split_generators"),xd.forEach(t),ar=i(pt,"). See the method docstrings for details."),pt.forEach(t),Rt.forEach(t),hs=l(a),X=n(a,"DIV",{class:!0});var As=r(X);f(Ne.$$.fragment,As),sr=l(As),ca=n(As,"P",{});var yd=r(ca);nr=i(yd,"Beam based Builder."),yd.forEach(t),As.forEach(t),_s=l(a),z=n(a,"DIV",{class:!0});var Ls=r(z);f(je.$$.fragment,Ls),rr=l(Ls),pa=n(Ls,"P",{});var Td=r(pa);or=i(Td,"Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),Td.forEach(t),Ls.forEach(t),vs=l(a),S=n(a,"DIV",{class:!0});var be=r(S);f(Re.$$.fragment,be),ir=l(be),Ce=n(be,"P",{});var Ps=r(Ce);dr=i(Ps,"Base class for "),$t=n(Ps,"A",{href:!0});var Id=r($t);lr=i(Id,"DatasetBuilder"),Id.forEach(t),cr=i(Ps," data configuration."),Ps.forEach(t),pr=l(be),ke=n(be,"P",{});var Vs=r(ke);fr=i(Vs,`DatasetBuilder subclasses with data configuration options should subclass
`),wt=n(Vs,"A",{href:!0});var Bd=r(wt);mr=i(Bd,"BuilderConfig"),Bd.forEach(t),ur=i(Vs," and add their own properties."),Vs.forEach(t),gr=l(be),U=n(be,"DIV",{class:!0});var Ct=r(U);f(Oe.$$.fragment,Ct),hr=l(Ct),fa=n(Ct,"P",{});var Sd=r(fa);_r=i(Sd,`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),Sd.forEach(t),vr=l(Ct),J=n(Ct,"UL",{});var kt=r(J);ma=n(kt,"LI",{});var Nd=r(ma);br=i(Nd,"the config kwargs that can be used to overwrite attributes"),Nd.forEach(t),$r=l(kt),ua=n(kt,"LI",{});var jd=r(ua);wr=i(jd,"the custom features used to write the dataset"),jd.forEach(t),Er=l(kt),ga=n(kt,"LI",{});var Rd=r(ga);Dr=i(Rd,`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),Rd.forEach(t),kt.forEach(t),Ct.forEach(t),be.forEach(t),bs=l(a),E=n(a,"DIV",{class:!0});var I=r(E);f(Ae.$$.fragment,I),xr=l(I),ce=n(I,"DIV",{class:!0});var Fs=r(ce);f(Le.$$.fragment,Fs),yr=l(Fs),ha=n(Fs,"P",{});var Cd=r(ha);Tr=i(Cd,"Download given url(s)."),Cd.forEach(t),Fs.forEach(t),Ir=l(I),j=n(I,"DIV",{class:!0});var $e=r(j);f(Pe.$$.fragment,$e),Br=l($e),_a=n($e,"P",{});var kd=r(_a);Sr=i(kd,"Download and extract given url_or_urls."),kd.forEach(t),Nr=l($e),va=n($e,"P",{});var Od=r(va);jr=i(Od,"Is roughly equivalent to:"),Od.forEach(t),Rr=l($e),f(Ve.$$.fragment,$e),$e.forEach(t),Cr=l(I),pe=n(I,"DIV",{class:!0});var Ms=r(pe);f(Fe.$$.fragment,Ms),kr=l(Ms),Me=n(Ms,"P",{});var qs=r(Me);Or=i(qs,"Download given urls(s) by calling "),ba=n(qs,"CODE",{});var Ad=r(ba);Ar=i(Ad,"custom_download"),Ad.forEach(t),Lr=i(qs,"."),qs.forEach(t),Ms.forEach(t),Pr=l(I),fe=n(I,"DIV",{class:!0});var Us=r(fe);f(qe.$$.fragment,Us),Vr=l(Us),$a=n(Us,"P",{});var Ld=r($a);Fr=i(Ld,"Extract given path(s)."),Ld.forEach(t),Us.forEach(t),Mr=l(I),me=n(I,"DIV",{class:!0});var Hs=r(me);f(Ue.$$.fragment,Hs),qr=l(Hs),wa=n(Hs,"P",{});var Pd=r(wa);Ur=i(Pd,"Iterate over files within an archive."),Pd.forEach(t),Hs.forEach(t),Hr=l(I),ue=n(I,"DIV",{class:!0});var Gs=r(ue);f(He.$$.fragment,Gs),Gr=l(Gs),Ea=n(Gs,"P",{});var Vd=r(Ea);Wr=i(Vd,"Iterate over file paths."),Vd.forEach(t),Gs.forEach(t),Xr=l(I),ge=n(I,"DIV",{class:!0});var Ws=r(ge);f(Ge.$$.fragment,Ws),zr=l(Ws),Da=n(Ws,"P",{});var Fd=r(Da);Jr=i(Fd,"Ship the files using Beam FileSystems to the pipeline temp dir."),Fd.forEach(t),Ws.forEach(t),I.forEach(t),$s=l(a),B=n(a,"DIV",{class:!0});var G=r(B);f(We.$$.fragment,G),Kr=l(G),Et=n(G,"P",{});var Xi=r(Et);xa=n(Xi,"CODE",{});var Md=r(xa);Yr=i(Md,"Enum"),Md.forEach(t),Qr=i(Xi," for how to treat pre-existing downloads and data."),Xi.forEach(t),Zr=l(G),Xe=n(G,"P",{});var Xs=r(Xe);eo=i(Xs,"The default mode is "),ya=n(Xs,"CODE",{});var qd=r(ya);to=i(qd,"REUSE_DATASET_IF_EXISTS"),qd.forEach(t),ao=i(Xs,`, which will reuse both
raw downloads and the prepared dataset if they exist.`),Xs.forEach(t),so=l(G),Ta=n(G,"P",{});var Ud=r(Ta);no=i(Ud,"The generations modes:"),Ud.forEach(t),ro=l(G),ze=n(G,"TABLE",{});var zs=r(ze);Ia=n(zs,"THEAD",{});var Hd=r(Ia);K=n(Hd,"TR",{});var Ot=r(K);ws=n(Ot,"TH",{}),r(ws).forEach(t),oo=l(Ot),Ba=n(Ot,"TH",{});var Gd=r(Ba);io=i(Gd,"Downloads"),Gd.forEach(t),lo=l(Ot),Sa=n(Ot,"TH",{});var Wd=r(Sa);co=i(Wd,"Dataset"),Wd.forEach(t),Ot.forEach(t),Hd.forEach(t),po=l(zs),Y=n(zs,"TBODY",{});var At=r(Y);Q=n(At,"TR",{});var Lt=r(Q);Dt=n(Lt,"TD",{});var zi=r(Dt);Na=n(zi,"CODE",{});var Xd=r(Na);fo=i(Xd,"REUSE_DATASET_IF_EXISTS"),Xd.forEach(t),mo=i(zi," (default)"),zi.forEach(t),uo=l(Lt),ja=n(Lt,"TD",{});var zd=r(ja);go=i(zd,"Reuse"),zd.forEach(t),ho=l(Lt),Ra=n(Lt,"TD",{});var Jd=r(Ra);_o=i(Jd,"Reuse"),Jd.forEach(t),Lt.forEach(t),vo=l(At),Z=n(At,"TR",{});var Pt=r(Z);Ca=n(Pt,"TD",{});var Kd=r(Ca);ka=n(Kd,"CODE",{});var Yd=r(ka);bo=i(Yd,"REUSE_CACHE_IF_EXISTS"),Yd.forEach(t),Kd.forEach(t),$o=l(Pt),Oa=n(Pt,"TD",{});var Qd=r(Oa);wo=i(Qd,"Reuse"),Qd.forEach(t),Eo=l(Pt),Aa=n(Pt,"TD",{});var Zd=r(Aa);Do=i(Zd,"Fresh"),Zd.forEach(t),Pt.forEach(t),xo=l(At),ee=n(At,"TR",{});var Vt=r(ee);La=n(Vt,"TD",{});var el=r(La);Pa=n(el,"CODE",{});var tl=r(Pa);yo=i(tl,"FORCE_REDOWNLOAD"),tl.forEach(t),el.forEach(t),To=l(Vt),Va=n(Vt,"TD",{});var al=r(Va);Io=i(al,"Fresh"),al.forEach(t),Bo=l(Vt),Fa=n(Vt,"TD",{});var sl=r(Fa);So=i(sl,"Fresh"),sl.forEach(t),Vt.forEach(t),At.forEach(t),zs.forEach(t),G.forEach(t),Es=l(a),L=n(a,"DIV",{class:!0});var Ft=r(L);f(Je.$$.fragment,Ft),No=l(Ft),Ma=n(Ft,"P",{});var nl=r(Ma);jo=i(nl,"Defines the split information for the generator."),nl.forEach(t),Ro=l(Ft),te=n(Ft,"P",{});var Mt=r(te);Co=i(Mt,`This should be used as returned value of
`),qa=n(Mt,"CODE",{});var rl=r(qa);ko=i(rl,"GeneratorBasedBuilder._split_generators()"),rl.forEach(t),Oo=i(Mt,`
See `),Ua=n(Mt,"CODE",{});var ol=r(Ua);Ao=i(ol,"GeneratorBasedBuilder._split_generators()"),ol.forEach(t),Lo=i(Mt,`for more info and example
of usage.`),Mt.forEach(t),Ft.forEach(t),Ds=l(a),y=n(a,"DIV",{class:!0});var R=r(y);f(Ke.$$.fragment,R),Po=l(R),xt=n(R,"P",{});var Ji=r(xt);Ha=n(Ji,"CODE",{});var il=r(Ha);Vo=i(il,"Enum"),il.forEach(t),Fo=i(Ji," for dataset splits."),Ji.forEach(t),Mo=l(R),Ga=n(R,"P",{});var dl=r(Ga);qo=i(dl,`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),dl.forEach(t),Uo=l(R),P=n(R,"UL",{});var we=r(P);yt=n(we,"LI",{});var Ki=r(yt);Wa=n(Ki,"CODE",{});var ll=r(Wa);Ho=i(ll,"TRAIN"),ll.forEach(t),Go=i(Ki,": the training data."),Ki.forEach(t),Wo=l(we),Tt=n(we,"LI",{});var Yi=r(Tt);Xa=n(Yi,"CODE",{});var cl=r(Xa);Xo=i(cl,"VALIDATION"),cl.forEach(t),zo=i(Yi,`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Yi.forEach(t),Jo=l(we),It=n(we,"LI",{});var Qi=r(It);za=n(Qi,"CODE",{});var pl=r(za);Ko=i(pl,"TEST"),pl.forEach(t),Yo=i(Qi,`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Qi.forEach(t),Qo=l(we),Bt=n(we,"LI",{});var Zi=r(Bt);Ja=n(Zi,"CODE",{});var fl=r(Ja);Zo=i(fl,"ALL"),fl.forEach(t),ei=i(Zi,": the union of all defined dataset splits."),Zi.forEach(t),we.forEach(t),ti=l(R),St=n(R,"P",{});var ed=r(St);ai=i(ed,"Note: All splits, including compositions inherit from "),Ka=n(ed,"CODE",{});var ml=r(Ka);si=i(ml,"datasets.SplitBase"),ml.forEach(t),ed.forEach(t),ni=l(R),Ye=n(R,"P",{});var Js=r(Ye);ri=i(Js,"See the :doc:"),Ya=n(Js,"CODE",{});var ul=r(Ya);oi=i(ul,"guide on splits </loading>"),ul.forEach(t),ii=i(Js," for more information."),Js.forEach(t),R.forEach(t),xs=l(a),w=n(a,"DIV",{class:!0});var x=r(w);f(Qe.$$.fragment,x),di=l(x),Qa=n(x,"P",{});var gl=r(Qa);li=i(gl,"Descriptor corresponding to a named split (train, test, \u2026)."),gl.forEach(t),ci=l(x),Za=n(x,"P",{});var hl=r(Za);pi=i(hl,"Example:"),hl.forEach(t),fi=l(x),f(Ze.$$.fragment,x),mi=l(x),es=n(x,"P",{});var _l=r(es);ui=i(_l,`Warning:
A split cannot be added twice, so the following will fail:`),_l.forEach(t),gi=l(x),f(et.$$.fragment,x),hi=l(x),ts=n(x,"P",{});var vl=r(ts);_i=i(vl,`Warning:
The slices can be applied only one time. So the following are valid:`),vl.forEach(t),vi=l(x),f(tt.$$.fragment,x),bi=l(x),as=n(x,"P",{});var bl=r(as);$i=i(bl,"But not:"),bl.forEach(t),wi=l(x),f(at.$$.fragment,x),x.forEach(t),ys=l(a),ae=n(a,"DIV",{class:!0});var Ks=r(ae);f(st.$$.fragment,Ks),Ei=l(Ks),ss=n(Ks,"P",{});var $l=r(ss);Di=i($l,"Split corresponding to the union of all defined dataset splits."),$l.forEach(t),Ks.forEach(t),Ts=l(a),T=n(a,"DIV",{class:!0});var C=r(T);f(nt.$$.fragment,C),xi=l(C),ns=n(C,"P",{});var wl=r(ns);yi=i(wl,"Reading instruction for a dataset."),wl.forEach(t),Ti=l(C),rs=n(C,"P",{});var El=r(rs);Ii=i(El,"Examples:"),El.forEach(t),Bi=l(C),f(rt.$$.fragment,C),Si=l(C),he=n(C,"DIV",{class:!0});var Ys=r(he);f(ot.$$.fragment,Ys),Ni=l(Ys),os=n(Ys,"P",{});var Dl=r(os);ji=i(Dl,"Creates a ReadInstruction instance out of a string spec."),Dl.forEach(t),Ys.forEach(t),Ri=l(C),H=n(C,"DIV",{class:!0});var qt=r(H);f(it.$$.fragment,qt),Ci=l(qt),is=n(qt,"P",{});var xl=r(is);ki=i(xl,"Translate instruction into a list of absolute instructions."),xl.forEach(t),Oi=l(qt),ds=n(qt,"P",{});var yl=r(ds);Ai=i(yl,"Those absolute instructions are then to be added together."),yl.forEach(t),qt.forEach(t),C.forEach(t),Is=l(a),se=n(a,"DIV",{class:!0});var Qs=r(se);f(dt.$$.fragment,Qs),Li=l(Qs),ls=n(Qs,"P",{});var Tl=r(ls);Pi=i(Tl,"Configuration for our cached path manager."),Tl.forEach(t),Qs.forEach(t),Bs=l(a),V=n(a,"DIV",{class:!0});var Ut=r(V);f(lt.$$.fragment,Ut),Vi=l(Ut),cs=n(Ut,"P",{});var Il=r(cs);Fi=i(Il,"Dataset version MAJOR.MINOR.PATCH."),Il.forEach(t),Mi=l(Ut),_e=n(Ut,"DIV",{class:!0});var Zs=r(_e);f(ct.$$.fragment,Zs),qi=l(Zs),ps=n(Zs,"P",{});var Bl=r(ps);Ui=i(Bl,"Returns True if other_version matches."),Bl.forEach(t),Zs.forEach(t),Ut.forEach(t),this.h()},h(){c(F,"name","hf:doc:metadata"),c(F,"content",JSON.stringify(Al)),c(k,"id","datasets.DatasetBuilder"),c(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(k,"href","#datasets.DatasetBuilder"),c(N,"class","relative group"),c(mt,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.DatasetBuilder"),c(ut,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.BuilderConfig"),c(vt,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.DatasetBuilder.download_and_prepare"),c(bt,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.DatasetBuilder.as_dataset"),c(re,"class","docstring"),c(oe,"class","docstring"),c(ie,"class","docstring"),c(de,"class","docstring"),c(le,"class","docstring"),c($,"class","docstring"),c(A,"class","docstring"),c(X,"class","docstring"),c(z,"class","docstring"),c($t,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.DatasetBuilder"),c(wt,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.BuilderConfig"),c(U,"class","docstring"),c(S,"class","docstring"),c(ce,"class","docstring"),c(j,"class","docstring"),c(pe,"class","docstring"),c(fe,"class","docstring"),c(me,"class","docstring"),c(ue,"class","docstring"),c(ge,"class","docstring"),c(E,"class","docstring"),c(B,"class","docstring"),c(L,"class","docstring"),c(y,"class","docstring"),c(w,"class","docstring"),c(ae,"class","docstring"),c(he,"class","docstring"),c(H,"class","docstring"),c(T,"class","docstring"),c(se,"class","docstring"),c(_e,"class","docstring"),c(V,"class","docstring")},m(a,_){e(document.head,F),v(a,ft,_),v(a,N,_),e(N,k),e(k,Gt),m(Ee,Gt,null),e(N,tn),e(N,Wt),e(Wt,an),v(a,ms,_),v(a,M,_),e(M,sn),e(M,mt),e(mt,nn),e(M,rn),e(M,ut),e(ut,on),e(M,dn),v(a,us,_),v(a,$,_),m(De,$,null),e($,ln),e($,Xt),e(Xt,cn),e($,pn),e($,gt),e(gt,zt),e(zt,fn),e(gt,mn),e($,un),e($,W),e(W,ht),e(ht,Jt),e(Jt,gn),e(ht,hn),e(W,_n),e(W,_t),e(_t,vt),e(vt,vn),e(_t,bn),e(W,$n),e(W,ne),e(ne,bt),e(bt,wn),e(ne,En),e(ne,Kt),e(Kt,Dn),e(ne,xn),e($,yn),e($,O),e(O,Yt),e(Yt,Tn),e(O,In),e(O,Qt),e(Qt,Bn),e(O,Sn),e(O,Zt),e(Zt,Nn),e(O,jn),e(O,ea),e(ea,Rn),e($,Cn),e($,re),m(xe,re,null),e(re,kn),e(re,ta),e(ta,On),e($,An),e($,oe),m(ye,oe,null),e(oe,Ln),e(oe,aa),e(aa,Pn),e($,Vn),e($,ie),m(Te,ie,null),e(ie,Fn),e(ie,sa),e(sa,Mn),e($,qn),e($,de),m(Ie,de,null),e(de,Un),e(de,na),e(na,Hn),e($,Gn),e($,le),m(Be,le,null),e(le,Wn),e(le,ra),e(ra,Xn),v(a,gs,_),v(a,A,_),m(Se,A,null),e(A,zn),e(A,oa),e(oa,Jn),e(A,Kn),e(A,q),e(q,ia),e(ia,Yn),e(q,Qn),e(q,da),e(da,Zn),e(q,er),e(q,la),e(la,tr),e(q,ar),v(a,hs,_),v(a,X,_),m(Ne,X,null),e(X,sr),e(X,ca),e(ca,nr),v(a,_s,_),v(a,z,_),m(je,z,null),e(z,rr),e(z,pa),e(pa,or),v(a,vs,_),v(a,S,_),m(Re,S,null),e(S,ir),e(S,Ce),e(Ce,dr),e(Ce,$t),e($t,lr),e(Ce,cr),e(S,pr),e(S,ke),e(ke,fr),e(ke,wt),e(wt,mr),e(ke,ur),e(S,gr),e(S,U),m(Oe,U,null),e(U,hr),e(U,fa),e(fa,_r),e(U,vr),e(U,J),e(J,ma),e(ma,br),e(J,$r),e(J,ua),e(ua,wr),e(J,Er),e(J,ga),e(ga,Dr),v(a,bs,_),v(a,E,_),m(Ae,E,null),e(E,xr),e(E,ce),m(Le,ce,null),e(ce,yr),e(ce,ha),e(ha,Tr),e(E,Ir),e(E,j),m(Pe,j,null),e(j,Br),e(j,_a),e(_a,Sr),e(j,Nr),e(j,va),e(va,jr),e(j,Rr),m(Ve,j,null),e(E,Cr),e(E,pe),m(Fe,pe,null),e(pe,kr),e(pe,Me),e(Me,Or),e(Me,ba),e(ba,Ar),e(Me,Lr),e(E,Pr),e(E,fe),m(qe,fe,null),e(fe,Vr),e(fe,$a),e($a,Fr),e(E,Mr),e(E,me),m(Ue,me,null),e(me,qr),e(me,wa),e(wa,Ur),e(E,Hr),e(E,ue),m(He,ue,null),e(ue,Gr),e(ue,Ea),e(Ea,Wr),e(E,Xr),e(E,ge),m(Ge,ge,null),e(ge,zr),e(ge,Da),e(Da,Jr),v(a,$s,_),v(a,B,_),m(We,B,null),e(B,Kr),e(B,Et),e(Et,xa),e(xa,Yr),e(Et,Qr),e(B,Zr),e(B,Xe),e(Xe,eo),e(Xe,ya),e(ya,to),e(Xe,ao),e(B,so),e(B,Ta),e(Ta,no),e(B,ro),e(B,ze),e(ze,Ia),e(Ia,K),e(K,ws),e(K,oo),e(K,Ba),e(Ba,io),e(K,lo),e(K,Sa),e(Sa,co),e(ze,po),e(ze,Y),e(Y,Q),e(Q,Dt),e(Dt,Na),e(Na,fo),e(Dt,mo),e(Q,uo),e(Q,ja),e(ja,go),e(Q,ho),e(Q,Ra),e(Ra,_o),e(Y,vo),e(Y,Z),e(Z,Ca),e(Ca,ka),e(ka,bo),e(Z,$o),e(Z,Oa),e(Oa,wo),e(Z,Eo),e(Z,Aa),e(Aa,Do),e(Y,xo),e(Y,ee),e(ee,La),e(La,Pa),e(Pa,yo),e(ee,To),e(ee,Va),e(Va,Io),e(ee,Bo),e(ee,Fa),e(Fa,So),v(a,Es,_),v(a,L,_),m(Je,L,null),e(L,No),e(L,Ma),e(Ma,jo),e(L,Ro),e(L,te),e(te,Co),e(te,qa),e(qa,ko),e(te,Oo),e(te,Ua),e(Ua,Ao),e(te,Lo),v(a,Ds,_),v(a,y,_),m(Ke,y,null),e(y,Po),e(y,xt),e(xt,Ha),e(Ha,Vo),e(xt,Fo),e(y,Mo),e(y,Ga),e(Ga,qo),e(y,Uo),e(y,P),e(P,yt),e(yt,Wa),e(Wa,Ho),e(yt,Go),e(P,Wo),e(P,Tt),e(Tt,Xa),e(Xa,Xo),e(Tt,zo),e(P,Jo),e(P,It),e(It,za),e(za,Ko),e(It,Yo),e(P,Qo),e(P,Bt),e(Bt,Ja),e(Ja,Zo),e(Bt,ei),e(y,ti),e(y,St),e(St,ai),e(St,Ka),e(Ka,si),e(y,ni),e(y,Ye),e(Ye,ri),e(Ye,Ya),e(Ya,oi),e(Ye,ii),v(a,xs,_),v(a,w,_),m(Qe,w,null),e(w,di),e(w,Qa),e(Qa,li),e(w,ci),e(w,Za),e(Za,pi),e(w,fi),m(Ze,w,null),e(w,mi),e(w,es),e(es,ui),e(w,gi),m(et,w,null),e(w,hi),e(w,ts),e(ts,_i),e(w,vi),m(tt,w,null),e(w,bi),e(w,as),e(as,$i),e(w,wi),m(at,w,null),v(a,ys,_),v(a,ae,_),m(st,ae,null),e(ae,Ei),e(ae,ss),e(ss,Di),v(a,Ts,_),v(a,T,_),m(nt,T,null),e(T,xi),e(T,ns),e(ns,yi),e(T,Ti),e(T,rs),e(rs,Ii),e(T,Bi),m(rt,T,null),e(T,Si),e(T,he),m(ot,he,null),e(he,Ni),e(he,os),e(os,ji),e(T,Ri),e(T,H),m(it,H,null),e(H,Ci),e(H,is),e(is,ki),e(H,Oi),e(H,ds),e(ds,Ai),v(a,Is,_),v(a,se,_),m(dt,se,null),e(se,Li),e(se,ls),e(ls,Pi),v(a,Bs,_),v(a,V,_),m(lt,V,null),e(V,Vi),e(V,cs),e(cs,Fi),e(V,Mi),e(V,_e),m(ct,_e,null),e(_e,qi),e(_e,ps),e(ps,Ui),Ss=!0},p:Cl,i(a){Ss||(u(Ee.$$.fragment,a),u(De.$$.fragment,a),u(xe.$$.fragment,a),u(ye.$$.fragment,a),u(Te.$$.fragment,a),u(Ie.$$.fragment,a),u(Be.$$.fragment,a),u(Se.$$.fragment,a),u(Ne.$$.fragment,a),u(je.$$.fragment,a),u(Re.$$.fragment,a),u(Oe.$$.fragment,a),u(Ae.$$.fragment,a),u(Le.$$.fragment,a),u(Pe.$$.fragment,a),u(Ve.$$.fragment,a),u(Fe.$$.fragment,a),u(qe.$$.fragment,a),u(Ue.$$.fragment,a),u(He.$$.fragment,a),u(Ge.$$.fragment,a),u(We.$$.fragment,a),u(Je.$$.fragment,a),u(Ke.$$.fragment,a),u(Qe.$$.fragment,a),u(Ze.$$.fragment,a),u(et.$$.fragment,a),u(tt.$$.fragment,a),u(at.$$.fragment,a),u(st.$$.fragment,a),u(nt.$$.fragment,a),u(rt.$$.fragment,a),u(ot.$$.fragment,a),u(it.$$.fragment,a),u(dt.$$.fragment,a),u(lt.$$.fragment,a),u(ct.$$.fragment,a),Ss=!0)},o(a){g(Ee.$$.fragment,a),g(De.$$.fragment,a),g(xe.$$.fragment,a),g(ye.$$.fragment,a),g(Te.$$.fragment,a),g(Ie.$$.fragment,a),g(Be.$$.fragment,a),g(Se.$$.fragment,a),g(Ne.$$.fragment,a),g(je.$$.fragment,a),g(Re.$$.fragment,a),g(Oe.$$.fragment,a),g(Ae.$$.fragment,a),g(Le.$$.fragment,a),g(Pe.$$.fragment,a),g(Ve.$$.fragment,a),g(Fe.$$.fragment,a),g(qe.$$.fragment,a),g(Ue.$$.fragment,a),g(He.$$.fragment,a),g(Ge.$$.fragment,a),g(We.$$.fragment,a),g(Je.$$.fragment,a),g(Ke.$$.fragment,a),g(Qe.$$.fragment,a),g(Ze.$$.fragment,a),g(et.$$.fragment,a),g(tt.$$.fragment,a),g(at.$$.fragment,a),g(st.$$.fragment,a),g(nt.$$.fragment,a),g(rt.$$.fragment,a),g(ot.$$.fragment,a),g(it.$$.fragment,a),g(dt.$$.fragment,a),g(lt.$$.fragment,a),g(ct.$$.fragment,a),Ss=!1},d(a){t(F),a&&t(ft),a&&t(N),h(Ee),a&&t(ms),a&&t(M),a&&t(us),a&&t($),h(De),h(xe),h(ye),h(Te),h(Ie),h(Be),a&&t(gs),a&&t(A),h(Se),a&&t(hs),a&&t(X),h(Ne),a&&t(_s),a&&t(z),h(je),a&&t(vs),a&&t(S),h(Re),h(Oe),a&&t(bs),a&&t(E),h(Ae),h(Le),h(Pe),h(Ve),h(Fe),h(qe),h(Ue),h(He),h(Ge),a&&t($s),a&&t(B),h(We),a&&t(Es),a&&t(L),h(Je),a&&t(Ds),a&&t(y),h(Ke),a&&t(xs),a&&t(w),h(Qe),h(Ze),h(et),h(tt),h(at),a&&t(ys),a&&t(ae),h(st),a&&t(Ts),a&&t(T),h(nt),h(rt),h(ot),h(it),a&&t(Is),a&&t(se),h(dt),a&&t(Bs),a&&t(V),h(lt),h(ct)}}}const Al={local:"datasets.DatasetBuilder",title:"Builder classes"};function Ll(en,F,ft){let{fw:N}=F;return en.$$set=k=>{"fw"in k&&ft(0,N=k.fw)},[N]}class ql extends Sl{constructor(F){super();Nl(this,F,Ll,Ol,jl,{fw:0})}}export{ql as default,Al as metadata};
