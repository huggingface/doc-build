import{S as so,i as to,s as eo,e as a,k as d,w as m,t as i,M as ao,c as l,d as t,m as c,a as o,x as h,h as p,b as f,G as e,g as n,y as g,q as u,o as _,B as v,v as lo}from"../chunks/vendor-hf-doc-builder.js";import{T as oo}from"../chunks/Tip-hf-doc-builder.js";import{I as Ps}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as j}from"../chunks/CodeBlock-hf-doc-builder.js";function ro(Ft){let y,G,$,k,F,S,V,D;return{c(){y=a("p"),G=i("Remember to include your "),$=a("code"),k=i("aws_access_key_id"),F=i(" and "),S=a("code"),V=i("aws_secret_access_key"),D=i(" whenever you are interacting with a private S3 bucket.")},l(L){y=l(L,"P",{});var w=o(y);G=p(w,"Remember to include your "),$=l(w,"CODE",{});var E=o($);k=p(E,"aws_access_key_id"),E.forEach(t),F=p(w," and "),S=l(w,"CODE",{});var Bs=o(S);V=p(Bs,"aws_secret_access_key"),Bs.forEach(t),D=p(w," whenever you are interacting with a private S3 bucket."),w.forEach(t)},m(L,w){n(L,y,w),e(y,G),e(y,$),e($,k),e(y,F),e(y,S),e(S,V),e(y,D)},d(L){L&&t(y)}}}function no(Ft){let y,G,$,k,F,S,V,D,L,w,E,Bs,Gs,Te,Xe,Dt,O,Ws,Z,Ms,ze,Ce,Ys,Pe,Be,b,ss,Js,Ge,Oe,Us,ts,Ie,Ne,es,Ks,qe,He,Qs,as,Re,We,ls,Vs,Me,Ye,Zs,os,Je,Ue,rs,st,Ke,Qe,tt,ns,Ve,Ze,is,et,sa,ta,at,ps,ea,Lt,I,aa,lt,la,oa,Tt,T,N,ot,ds,ra,rt,na,Xt,X,q,nt,cs,ia,it,pa,zt,Os,pt,da,Ct,fs,Pt,ms,hs,ca,dt,fa,ma,Bt,gs,Gt,A,ha,ct,ga,ua,ft,_a,va,Ot,us,It,z,H,mt,_s,ya,ht,$a,Nt,R,Sa,Is,wa,ba,qt,vs,Ht,W,Rt,M,ja,gt,ka,Ea,Wt,ys,Mt,C,Y,ut,$s,Aa,_t,xa,Yt,J,Fa,Ns,Da,La,Jt,Ss,Ut,U,Ta,vt,Xa,za,Kt,ws,Qt,P,K,yt,bs,Ca,$t,Pa,Vt,qs,St,Ba,Zt,js,se,ks,wt,Ga,te,Es,ee,As,bt,Oa,ae,xs,le,B,Q,jt,Fs,Ia,kt,Na,oe,Hs,Et,qa,re,Ds,ne,Ls,At,Ha,ie,Ts,pe,Xs,xt,Ra,de,zs,ce;return S=new Ps({}),ds=new Ps({}),cs=new Ps({}),fs=new j({props:{code:"pip install datasets[s3]",highlighted:'&gt;&gt;&gt; pip <span class="hljs-keyword">install </span>datasets[<span class="hljs-built_in">s3</span>]'}}),gs=new j({props:{code:`import datasets
s3 = datasets.filesystems.S3FileSystem(anon=True)  
s3.ls('public-datasets/imdb/train')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> datasets
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = datasets.filesystems.S3FileSystem(anon=<span class="hljs-literal">True</span>)  
<span class="hljs-meta">&gt;&gt;&gt; </span>s3.ls(<span class="hljs-string">&#x27;public-datasets/imdb/train&#x27;</span>)
[<span class="hljs-string">&#x27;dataset_info.json.json&#x27;</span>,<span class="hljs-string">&#x27;dataset.arrow&#x27;</span>,<span class="hljs-string">&#x27;state.json&#x27;</span>]`}}),us=new j({props:{code:`import datasets
s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)  
s3.ls('my-private-datasets/imdb/train')  `,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> datasets
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)  
<span class="hljs-meta">&gt;&gt;&gt; </span>s3.ls(<span class="hljs-string">&#x27;my-private-datasets/imdb/train&#x27;</span>)  
[<span class="hljs-string">&#x27;dataset_info.json.json&#x27;</span>,<span class="hljs-string">&#x27;dataset.arrow&#x27;</span>,<span class="hljs-string">&#x27;state.json&#x27;</span>]`}}),_s=new Ps({}),vs=new j({props:{code:`from datasets.filesystems import S3FileSystem

s3 = S3FileSystem(anon=True)  

encoded_dataset.save_to_disk('s3://my-private-datasets/imdb/train', fs=s3)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets.filesystems <span class="hljs-keyword">import</span> S3FileSystem

<span class="hljs-comment"># create S3FileSystem instance</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = S3FileSystem(anon=<span class="hljs-literal">True</span>)  

<span class="hljs-comment"># saves encoded_dataset to your s3 bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&#x27;s3://my-private-datasets/imdb/train&#x27;</span>, fs=s3)`}}),W=new oo({props:{$$slots:{default:[ro]},$$scope:{ctx:Ft}}}),ys=new j({props:{code:`import botocore
from datasets.filesystems import S3FileSystem

s3_session = botocore.session.Session(profile='my_profile_name')

s3 = S3FileSystem(session=s3_session)  

encoded_dataset.save_to_disk('s3://my-private-datasets/imdb/train',fs=s3)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> botocore
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets.filesystems <span class="hljs-keyword">import</span> S3FileSystem

<span class="hljs-comment"># creates a botocore session with the provided AWS profile</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3_session = botocore.session.Session(profile=<span class="hljs-string">&#x27;my_profile_name&#x27;</span>)

<span class="hljs-comment"># create S3FileSystem instance with s3_session</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = S3FileSystem(session=s3_session)  

<span class="hljs-comment"># saves encoded_dataset to your s3 bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&#x27;s3://my-private-datasets/imdb/train&#x27;</span>,fs=s3)`}}),$s=new Ps({}),Ss=new j({props:{code:`from datasets import load_from_disk
from datasets.filesystems import S3FileSystem

s3 = S3FileSystem(anon=True)  

dataset = load_from_disk('s3://a-public-datasets/imdb/train',fs=s3)  

print(len(dataset))
# 25000`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_from_disk
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets.filesystems <span class="hljs-keyword">import</span> S3FileSystem

<span class="hljs-comment"># create S3FileSystem without credentials</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = S3FileSystem(anon=<span class="hljs-literal">True</span>)  

<span class="hljs-comment"># load encoded_dataset to from s3 bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_from_disk(<span class="hljs-string">&#x27;s3://a-public-datasets/imdb/train&#x27;</span>,fs=s3)  

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 25000</span>`}}),ws=new j({props:{code:`import botocore
from datasets.filesystems import S3FileSystem

s3_session = botocore.session.Session(profile='my_profile_name')

s3 = S3FileSystem(session=s3_session)

dataset = load_from_disk('s3://my-private-datasets/imdb/train',fs=s3)  

print(len(dataset))
# 25000`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> botocore
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets.filesystems <span class="hljs-keyword">import</span> S3FileSystem

<span class="hljs-comment"># create S3FileSystem instance with aws_access_key_id and aws_secret_access_key</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3_session = botocore.session.Session(profile=<span class="hljs-string">&#x27;my_profile_name&#x27;</span>)

<span class="hljs-comment"># create S3FileSystem instance with s3_session</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>s3 = S3FileSystem(session=s3_session)

<span class="hljs-comment"># load encoded_dataset to from s3 bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_from_disk(<span class="hljs-string">&#x27;s3://my-private-datasets/imdb/train&#x27;</span>,fs=s3)  

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 25000</span>`}}),bs=new Ps({}),js=new j({props:{code:`conda install -c conda-forge gcsfs
pip install gcsfs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">conda install -c conda-forge gcsfs</span>
# or install with pip
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">pip install gcsfs</span>`}}),Es=new j({props:{code:`import gcsfs

gcs = gcsfs.GCSFileSystem(project='my-google-project')

encoded_dataset.save_to_disk('gcs://my-private-datasets/imdb/train', fs=gcs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> gcsfs

<span class="hljs-comment"># create GCSFileSystem instance using default gcloud credentials with project</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>gcs = gcsfs.GCSFileSystem(project=<span class="hljs-string">&#x27;my-google-project&#x27;</span>)

<span class="hljs-comment"># saves encoded_dataset to your gcs bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&#x27;gcs://my-private-datasets/imdb/train&#x27;</span>, fs=gcs)`}}),xs=new j({props:{code:`import gcsfs
from datasets import load_from_disk

gcs = gcsfs.GCSFileSystem(project='my-google-project')

dataset = load_from_disk('gcs://my-private-datasets/imdb/train', fs=gcs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> gcsfs
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_from_disk

<span class="hljs-comment"># create GCSFileSystem instance using default gcloud credentials with project</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>gcs = gcsfs.GCSFileSystem(project=<span class="hljs-string">&#x27;my-google-project&#x27;</span>)

<span class="hljs-comment"># loads encoded_dataset from your gcs bucket</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_from_disk(<span class="hljs-string">&#x27;gcs://my-private-datasets/imdb/train&#x27;</span>, fs=gcs)`}}),Fs=new Ps({}),Ds=new j({props:{code:`conda install -c conda-forge adlfs
pip install adlfs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">conda install -c conda-forge adlfs</span>
# or install with pip
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">pip install adlfs</span>`}}),Ts=new j({props:{code:`import adlfs

abfs = adlfs.AzureBlobFileSystem(account_name="XXXX", account_key="XXXX")

encoded_dataset.save_to_disk('abfs://my-private-datasets/imdb/train', fs=abfs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> adlfs

<span class="hljs-comment"># create AzureBlobFileSystem instance with account_name and account_key</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>abfs = adlfs.AzureBlobFileSystem(account_name=<span class="hljs-string">&quot;XXXX&quot;</span>, account_key=<span class="hljs-string">&quot;XXXX&quot;</span>)

<span class="hljs-comment"># saves encoded_dataset to your azure container</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&#x27;abfs://my-private-datasets/imdb/train&#x27;</span>, fs=abfs)`}}),zs=new j({props:{code:`import adlfs
from datasets import load_from_disk

abfs = adlfs.AzureBlobFileSystem(account_name="XXXX", account_key="XXXX")

dataset = load_from_disk('abfs://my-private-datasets/imdb/train', fs=abfs)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> adlfs
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_from_disk

<span class="hljs-comment"># create AzureBlobFileSystem instance with account_name and account_key</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>abfs = adlfs.AzureBlobFileSystem(account_name=<span class="hljs-string">&quot;XXXX&quot;</span>, account_key=<span class="hljs-string">&quot;XXXX&quot;</span>)

<span class="hljs-comment"># loads encoded_dataset from your azure container</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_from_disk(<span class="hljs-string">&#x27;abfs://my-private-datasets/imdb/train&#x27;</span>, fs=abfs)`}}),{c(){y=a("meta"),G=d(),$=a("h1"),k=a("a"),F=a("span"),m(S.$$.fragment),V=d(),D=a("span"),L=i("Cloud storage"),w=d(),E=a("p"),Bs=i("\u{1F917} Datasets supports access to cloud storage providers through a S3 filesystem implementation: "),Gs=a("a"),Te=i("filesystems.S3FileSystem"),Xe=i(". You can save and load datasets from your Amazon S3 bucket in a Pythonic way. Take a look at the following table for other supported cloud storage providers:"),Dt=d(),O=a("table"),Ws=a("thead"),Z=a("tr"),Ms=a("th"),ze=i("Storage provider"),Ce=d(),Ys=a("th"),Pe=i("Filesystem implementation"),Be=d(),b=a("tbody"),ss=a("tr"),Js=a("td"),Ge=i("Amazon S3"),Oe=d(),Us=a("td"),ts=a("a"),Ie=i("s3fs"),Ne=d(),es=a("tr"),Ks=a("td"),qe=i("Google Cloud Storage"),He=d(),Qs=a("td"),as=a("a"),Re=i("gcsfs"),We=d(),ls=a("tr"),Vs=a("td"),Me=i("Azure Blob/DataLake"),Ye=d(),Zs=a("td"),os=a("a"),Je=i("adlfs"),Ue=d(),rs=a("tr"),st=a("td"),Ke=i("Dropbox"),Qe=d(),tt=a("td"),ns=a("a"),Ve=i("dropboxdrivefs"),Ze=d(),is=a("tr"),et=a("td"),sa=i("Google Drive"),ta=d(),at=a("td"),ps=a("a"),ea=i("gdrivefs"),Lt=d(),I=a("p"),aa=i("This guide will show you how to save and load datasets with "),lt=a("strong"),la=i("s3fs"),oa=i(" to a S3 bucket, but other filesystem implementations can be used similarly. An example is shown also for Google Cloud Storage and Azure Blob Storage."),Tt=d(),T=a("h2"),N=a("a"),ot=a("span"),m(ds.$$.fragment),ra=d(),rt=a("span"),na=i("Amazon S3"),Xt=d(),X=a("h3"),q=a("a"),nt=a("span"),m(cs.$$.fragment),ia=d(),it=a("span"),pa=i("Listing datasets"),zt=d(),Os=a("ol"),pt=a("li"),da=i("Install the S3 dependency with \u{1F917} Datasets:"),Ct=d(),m(fs.$$.fragment),Pt=d(),ms=a("ol"),hs=a("li"),ca=i("List files from a public S3 bucket with "),dt=a("code"),fa=i("s3.ls"),ma=i(":"),Bt=d(),m(gs.$$.fragment),Gt=d(),A=a("p"),ha=i("Access a private S3 bucket by entering your "),ct=a("code"),ga=i("aws_access_key_id"),ua=i(" and "),ft=a("code"),_a=i("aws_secret_access_key"),va=i(":"),Ot=d(),m(us.$$.fragment),It=d(),z=a("h3"),H=a("a"),mt=a("span"),m(_s.$$.fragment),ya=d(),ht=a("span"),$a=i("Saving datasets"),Nt=d(),R=a("p"),Sa=i("After you have processed your dataset, you can save it to S3 with "),Is=a("a"),wa=i("Dataset.save_to_disk()"),ba=i(":"),qt=d(),m(vs.$$.fragment),Ht=d(),m(W.$$.fragment),Rt=d(),M=a("p"),ja=i("Save your dataset with "),gt=a("code"),ka=i("botocore.session.Session"),Ea=i(" and a custom AWS profile:"),Wt=d(),m(ys.$$.fragment),Mt=d(),C=a("h3"),Y=a("a"),ut=a("span"),m($s.$$.fragment),Aa=d(),_t=a("span"),xa=i("Loading datasets"),Yt=d(),J=a("p"),Fa=i("When you are ready to use your dataset again, reload it with "),Ns=a("a"),Da=i("Dataset.load_from_disk()"),La=i(":"),Jt=d(),m(Ss.$$.fragment),Ut=d(),U=a("p"),Ta=i("Load with "),vt=a("code"),Xa=i("botocore.session.Session"),za=i(" and custom AWS profile:"),Kt=d(),m(ws.$$.fragment),Qt=d(),P=a("h2"),K=a("a"),yt=a("span"),m(bs.$$.fragment),Ca=d(),$t=a("span"),Pa=i("Google Cloud Storage"),Vt=d(),qs=a("ol"),St=a("li"),Ba=i("Install the Google Cloud Storage implementation:"),Zt=d(),m(js.$$.fragment),se=d(),ks=a("ol"),wt=a("li"),Ga=i("Save your dataset:"),te=d(),m(Es.$$.fragment),ee=d(),As=a("ol"),bt=a("li"),Oa=i("Load your dataset:"),ae=d(),m(xs.$$.fragment),le=d(),B=a("h2"),Q=a("a"),jt=a("span"),m(Fs.$$.fragment),Ia=d(),kt=a("span"),Na=i("Azure Blob Storage"),oe=d(),Hs=a("ol"),Et=a("li"),qa=i("Install the Azure Blob Storage implementation:"),re=d(),m(Ds.$$.fragment),ne=d(),Ls=a("ol"),At=a("li"),Ha=i("Save your dataset:"),ie=d(),m(Ts.$$.fragment),pe=d(),Xs=a("ol"),xt=a("li"),Ra=i("Load your dataset:"),de=d(),m(zs.$$.fragment),this.h()},l(s){const r=ao('[data-svelte="svelte-1phssyn"]',document.head);y=l(r,"META",{name:!0,content:!0}),r.forEach(t),G=c(s),$=l(s,"H1",{class:!0});var Cs=o($);k=l(Cs,"A",{id:!0,class:!0,href:!0});var Wa=o(k);F=l(Wa,"SPAN",{});var Ma=o(F);h(S.$$.fragment,Ma),Ma.forEach(t),Wa.forEach(t),V=c(Cs),D=l(Cs,"SPAN",{});var Ya=o(D);L=p(Ya,"Cloud storage"),Ya.forEach(t),Cs.forEach(t),w=c(s),E=l(s,"P",{});var fe=o(E);Bs=p(fe,"\u{1F917} Datasets supports access to cloud storage providers through a S3 filesystem implementation: "),Gs=l(fe,"A",{href:!0});var Ja=o(Gs);Te=p(Ja,"filesystems.S3FileSystem"),Ja.forEach(t),Xe=p(fe,". You can save and load datasets from your Amazon S3 bucket in a Pythonic way. Take a look at the following table for other supported cloud storage providers:"),fe.forEach(t),Dt=c(s),O=l(s,"TABLE",{});var me=o(O);Ws=l(me,"THEAD",{});var Ua=o(Ws);Z=l(Ua,"TR",{});var he=o(Z);Ms=l(he,"TH",{});var Ka=o(Ms);ze=p(Ka,"Storage provider"),Ka.forEach(t),Ce=c(he),Ys=l(he,"TH",{});var Qa=o(Ys);Pe=p(Qa,"Filesystem implementation"),Qa.forEach(t),he.forEach(t),Ua.forEach(t),Be=c(me),b=l(me,"TBODY",{});var x=o(b);ss=l(x,"TR",{});var ge=o(ss);Js=l(ge,"TD",{});var Va=o(Js);Ge=p(Va,"Amazon S3"),Va.forEach(t),Oe=c(ge),Us=l(ge,"TD",{});var Za=o(Us);ts=l(Za,"A",{href:!0,rel:!0});var sl=o(ts);Ie=p(sl,"s3fs"),sl.forEach(t),Za.forEach(t),ge.forEach(t),Ne=c(x),es=l(x,"TR",{});var ue=o(es);Ks=l(ue,"TD",{});var tl=o(Ks);qe=p(tl,"Google Cloud Storage"),tl.forEach(t),He=c(ue),Qs=l(ue,"TD",{});var el=o(Qs);as=l(el,"A",{href:!0,rel:!0});var al=o(as);Re=p(al,"gcsfs"),al.forEach(t),el.forEach(t),ue.forEach(t),We=c(x),ls=l(x,"TR",{});var _e=o(ls);Vs=l(_e,"TD",{});var ll=o(Vs);Me=p(ll,"Azure Blob/DataLake"),ll.forEach(t),Ye=c(_e),Zs=l(_e,"TD",{});var ol=o(Zs);os=l(ol,"A",{href:!0,rel:!0});var rl=o(os);Je=p(rl,"adlfs"),rl.forEach(t),ol.forEach(t),_e.forEach(t),Ue=c(x),rs=l(x,"TR",{});var ve=o(rs);st=l(ve,"TD",{});var nl=o(st);Ke=p(nl,"Dropbox"),nl.forEach(t),Qe=c(ve),tt=l(ve,"TD",{});var il=o(tt);ns=l(il,"A",{href:!0,rel:!0});var pl=o(ns);Ve=p(pl,"dropboxdrivefs"),pl.forEach(t),il.forEach(t),ve.forEach(t),Ze=c(x),is=l(x,"TR",{});var ye=o(is);et=l(ye,"TD",{});var dl=o(et);sa=p(dl,"Google Drive"),dl.forEach(t),ta=c(ye),at=l(ye,"TD",{});var cl=o(at);ps=l(cl,"A",{href:!0,rel:!0});var fl=o(ps);ea=p(fl,"gdrivefs"),fl.forEach(t),cl.forEach(t),ye.forEach(t),x.forEach(t),me.forEach(t),Lt=c(s),I=l(s,"P",{});var $e=o(I);aa=p($e,"This guide will show you how to save and load datasets with "),lt=l($e,"STRONG",{});var ml=o(lt);la=p(ml,"s3fs"),ml.forEach(t),oa=p($e," to a S3 bucket, but other filesystem implementations can be used similarly. An example is shown also for Google Cloud Storage and Azure Blob Storage."),$e.forEach(t),Tt=c(s),T=l(s,"H2",{class:!0});var Se=o(T);N=l(Se,"A",{id:!0,class:!0,href:!0});var hl=o(N);ot=l(hl,"SPAN",{});var gl=o(ot);h(ds.$$.fragment,gl),gl.forEach(t),hl.forEach(t),ra=c(Se),rt=l(Se,"SPAN",{});var ul=o(rt);na=p(ul,"Amazon S3"),ul.forEach(t),Se.forEach(t),Xt=c(s),X=l(s,"H3",{class:!0});var we=o(X);q=l(we,"A",{id:!0,class:!0,href:!0});var _l=o(q);nt=l(_l,"SPAN",{});var vl=o(nt);h(cs.$$.fragment,vl),vl.forEach(t),_l.forEach(t),ia=c(we),it=l(we,"SPAN",{});var yl=o(it);pa=p(yl,"Listing datasets"),yl.forEach(t),we.forEach(t),zt=c(s),Os=l(s,"OL",{});var $l=o(Os);pt=l($l,"LI",{});var Sl=o(pt);da=p(Sl,"Install the S3 dependency with \u{1F917} Datasets:"),Sl.forEach(t),$l.forEach(t),Ct=c(s),h(fs.$$.fragment,s),Pt=c(s),ms=l(s,"OL",{start:!0});var wl=o(ms);hs=l(wl,"LI",{});var be=o(hs);ca=p(be,"List files from a public S3 bucket with "),dt=l(be,"CODE",{});var bl=o(dt);fa=p(bl,"s3.ls"),bl.forEach(t),ma=p(be,":"),be.forEach(t),wl.forEach(t),Bt=c(s),h(gs.$$.fragment,s),Gt=c(s),A=l(s,"P",{});var Rs=o(A);ha=p(Rs,"Access a private S3 bucket by entering your "),ct=l(Rs,"CODE",{});var jl=o(ct);ga=p(jl,"aws_access_key_id"),jl.forEach(t),ua=p(Rs," and "),ft=l(Rs,"CODE",{});var kl=o(ft);_a=p(kl,"aws_secret_access_key"),kl.forEach(t),va=p(Rs,":"),Rs.forEach(t),Ot=c(s),h(us.$$.fragment,s),It=c(s),z=l(s,"H3",{class:!0});var je=o(z);H=l(je,"A",{id:!0,class:!0,href:!0});var El=o(H);mt=l(El,"SPAN",{});var Al=o(mt);h(_s.$$.fragment,Al),Al.forEach(t),El.forEach(t),ya=c(je),ht=l(je,"SPAN",{});var xl=o(ht);$a=p(xl,"Saving datasets"),xl.forEach(t),je.forEach(t),Nt=c(s),R=l(s,"P",{});var ke=o(R);Sa=p(ke,"After you have processed your dataset, you can save it to S3 with "),Is=l(ke,"A",{href:!0});var Fl=o(Is);wa=p(Fl,"Dataset.save_to_disk()"),Fl.forEach(t),ba=p(ke,":"),ke.forEach(t),qt=c(s),h(vs.$$.fragment,s),Ht=c(s),h(W.$$.fragment,s),Rt=c(s),M=l(s,"P",{});var Ee=o(M);ja=p(Ee,"Save your dataset with "),gt=l(Ee,"CODE",{});var Dl=o(gt);ka=p(Dl,"botocore.session.Session"),Dl.forEach(t),Ea=p(Ee," and a custom AWS profile:"),Ee.forEach(t),Wt=c(s),h(ys.$$.fragment,s),Mt=c(s),C=l(s,"H3",{class:!0});var Ae=o(C);Y=l(Ae,"A",{id:!0,class:!0,href:!0});var Ll=o(Y);ut=l(Ll,"SPAN",{});var Tl=o(ut);h($s.$$.fragment,Tl),Tl.forEach(t),Ll.forEach(t),Aa=c(Ae),_t=l(Ae,"SPAN",{});var Xl=o(_t);xa=p(Xl,"Loading datasets"),Xl.forEach(t),Ae.forEach(t),Yt=c(s),J=l(s,"P",{});var xe=o(J);Fa=p(xe,"When you are ready to use your dataset again, reload it with "),Ns=l(xe,"A",{href:!0});var zl=o(Ns);Da=p(zl,"Dataset.load_from_disk()"),zl.forEach(t),La=p(xe,":"),xe.forEach(t),Jt=c(s),h(Ss.$$.fragment,s),Ut=c(s),U=l(s,"P",{});var Fe=o(U);Ta=p(Fe,"Load with "),vt=l(Fe,"CODE",{});var Cl=o(vt);Xa=p(Cl,"botocore.session.Session"),Cl.forEach(t),za=p(Fe," and custom AWS profile:"),Fe.forEach(t),Kt=c(s),h(ws.$$.fragment,s),Qt=c(s),P=l(s,"H2",{class:!0});var De=o(P);K=l(De,"A",{id:!0,class:!0,href:!0});var Pl=o(K);yt=l(Pl,"SPAN",{});var Bl=o(yt);h(bs.$$.fragment,Bl),Bl.forEach(t),Pl.forEach(t),Ca=c(De),$t=l(De,"SPAN",{});var Gl=o($t);Pa=p(Gl,"Google Cloud Storage"),Gl.forEach(t),De.forEach(t),Vt=c(s),qs=l(s,"OL",{});var Ol=o(qs);St=l(Ol,"LI",{});var Il=o(St);Ba=p(Il,"Install the Google Cloud Storage implementation:"),Il.forEach(t),Ol.forEach(t),Zt=c(s),h(js.$$.fragment,s),se=c(s),ks=l(s,"OL",{start:!0});var Nl=o(ks);wt=l(Nl,"LI",{});var ql=o(wt);Ga=p(ql,"Save your dataset:"),ql.forEach(t),Nl.forEach(t),te=c(s),h(Es.$$.fragment,s),ee=c(s),As=l(s,"OL",{start:!0});var Hl=o(As);bt=l(Hl,"LI",{});var Rl=o(bt);Oa=p(Rl,"Load your dataset:"),Rl.forEach(t),Hl.forEach(t),ae=c(s),h(xs.$$.fragment,s),le=c(s),B=l(s,"H2",{class:!0});var Le=o(B);Q=l(Le,"A",{id:!0,class:!0,href:!0});var Wl=o(Q);jt=l(Wl,"SPAN",{});var Ml=o(jt);h(Fs.$$.fragment,Ml),Ml.forEach(t),Wl.forEach(t),Ia=c(Le),kt=l(Le,"SPAN",{});var Yl=o(kt);Na=p(Yl,"Azure Blob Storage"),Yl.forEach(t),Le.forEach(t),oe=c(s),Hs=l(s,"OL",{});var Jl=o(Hs);Et=l(Jl,"LI",{});var Ul=o(Et);qa=p(Ul,"Install the Azure Blob Storage implementation:"),Ul.forEach(t),Jl.forEach(t),re=c(s),h(Ds.$$.fragment,s),ne=c(s),Ls=l(s,"OL",{start:!0});var Kl=o(Ls);At=l(Kl,"LI",{});var Ql=o(At);Ha=p(Ql,"Save your dataset:"),Ql.forEach(t),Kl.forEach(t),ie=c(s),h(Ts.$$.fragment,s),pe=c(s),Xs=l(s,"OL",{start:!0});var Vl=o(Xs);xt=l(Vl,"LI",{});var Zl=o(xt);Ra=p(Zl,"Load your dataset:"),Zl.forEach(t),Vl.forEach(t),de=c(s),h(zs.$$.fragment,s),this.h()},h(){f(y,"name","hf:doc:metadata"),f(y,"content",JSON.stringify(io)),f(k,"id","cloud-storage"),f(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(k,"href","#cloud-storage"),f($,"class","relative group"),f(Gs,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.filesystems.S3FileSystem"),f(ts,"href","https://s3fs.readthedocs.io/en/latest/"),f(ts,"rel","nofollow"),f(as,"href","https://gcsfs.readthedocs.io/en/latest/"),f(as,"rel","nofollow"),f(os,"href","https://github.com/fsspec/adlfs"),f(os,"rel","nofollow"),f(ns,"href","https://github.com/MarineChap/dropboxdrivefs"),f(ns,"rel","nofollow"),f(ps,"href","https://github.com/intake/gdrivefs"),f(ps,"rel","nofollow"),f(N,"id","amazon-s3"),f(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(N,"href","#amazon-s3"),f(T,"class","relative group"),f(q,"id","listing-datasets"),f(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(q,"href","#listing-datasets"),f(X,"class","relative group"),f(ms,"start","2"),f(H,"id","saving-datasets"),f(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(H,"href","#saving-datasets"),f(z,"class","relative group"),f(Is,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.Dataset.save_to_disk"),f(Y,"id","loading-datasets"),f(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Y,"href","#loading-datasets"),f(C,"class","relative group"),f(Ns,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.Dataset.load_from_disk"),f(K,"id","google-cloud-storage"),f(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(K,"href","#google-cloud-storage"),f(P,"class","relative group"),f(ks,"start","2"),f(As,"start","3"),f(Q,"id","azure-blob-storage"),f(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(Q,"href","#azure-blob-storage"),f(B,"class","relative group"),f(Ls,"start","2"),f(Xs,"start","3")},m(s,r){e(document.head,y),n(s,G,r),n(s,$,r),e($,k),e(k,F),g(S,F,null),e($,V),e($,D),e(D,L),n(s,w,r),n(s,E,r),e(E,Bs),e(E,Gs),e(Gs,Te),e(E,Xe),n(s,Dt,r),n(s,O,r),e(O,Ws),e(Ws,Z),e(Z,Ms),e(Ms,ze),e(Z,Ce),e(Z,Ys),e(Ys,Pe),e(O,Be),e(O,b),e(b,ss),e(ss,Js),e(Js,Ge),e(ss,Oe),e(ss,Us),e(Us,ts),e(ts,Ie),e(b,Ne),e(b,es),e(es,Ks),e(Ks,qe),e(es,He),e(es,Qs),e(Qs,as),e(as,Re),e(b,We),e(b,ls),e(ls,Vs),e(Vs,Me),e(ls,Ye),e(ls,Zs),e(Zs,os),e(os,Je),e(b,Ue),e(b,rs),e(rs,st),e(st,Ke),e(rs,Qe),e(rs,tt),e(tt,ns),e(ns,Ve),e(b,Ze),e(b,is),e(is,et),e(et,sa),e(is,ta),e(is,at),e(at,ps),e(ps,ea),n(s,Lt,r),n(s,I,r),e(I,aa),e(I,lt),e(lt,la),e(I,oa),n(s,Tt,r),n(s,T,r),e(T,N),e(N,ot),g(ds,ot,null),e(T,ra),e(T,rt),e(rt,na),n(s,Xt,r),n(s,X,r),e(X,q),e(q,nt),g(cs,nt,null),e(X,ia),e(X,it),e(it,pa),n(s,zt,r),n(s,Os,r),e(Os,pt),e(pt,da),n(s,Ct,r),g(fs,s,r),n(s,Pt,r),n(s,ms,r),e(ms,hs),e(hs,ca),e(hs,dt),e(dt,fa),e(hs,ma),n(s,Bt,r),g(gs,s,r),n(s,Gt,r),n(s,A,r),e(A,ha),e(A,ct),e(ct,ga),e(A,ua),e(A,ft),e(ft,_a),e(A,va),n(s,Ot,r),g(us,s,r),n(s,It,r),n(s,z,r),e(z,H),e(H,mt),g(_s,mt,null),e(z,ya),e(z,ht),e(ht,$a),n(s,Nt,r),n(s,R,r),e(R,Sa),e(R,Is),e(Is,wa),e(R,ba),n(s,qt,r),g(vs,s,r),n(s,Ht,r),g(W,s,r),n(s,Rt,r),n(s,M,r),e(M,ja),e(M,gt),e(gt,ka),e(M,Ea),n(s,Wt,r),g(ys,s,r),n(s,Mt,r),n(s,C,r),e(C,Y),e(Y,ut),g($s,ut,null),e(C,Aa),e(C,_t),e(_t,xa),n(s,Yt,r),n(s,J,r),e(J,Fa),e(J,Ns),e(Ns,Da),e(J,La),n(s,Jt,r),g(Ss,s,r),n(s,Ut,r),n(s,U,r),e(U,Ta),e(U,vt),e(vt,Xa),e(U,za),n(s,Kt,r),g(ws,s,r),n(s,Qt,r),n(s,P,r),e(P,K),e(K,yt),g(bs,yt,null),e(P,Ca),e(P,$t),e($t,Pa),n(s,Vt,r),n(s,qs,r),e(qs,St),e(St,Ba),n(s,Zt,r),g(js,s,r),n(s,se,r),n(s,ks,r),e(ks,wt),e(wt,Ga),n(s,te,r),g(Es,s,r),n(s,ee,r),n(s,As,r),e(As,bt),e(bt,Oa),n(s,ae,r),g(xs,s,r),n(s,le,r),n(s,B,r),e(B,Q),e(Q,jt),g(Fs,jt,null),e(B,Ia),e(B,kt),e(kt,Na),n(s,oe,r),n(s,Hs,r),e(Hs,Et),e(Et,qa),n(s,re,r),g(Ds,s,r),n(s,ne,r),n(s,Ls,r),e(Ls,At),e(At,Ha),n(s,ie,r),g(Ts,s,r),n(s,pe,r),n(s,Xs,r),e(Xs,xt),e(xt,Ra),n(s,de,r),g(zs,s,r),ce=!0},p(s,[r]){const Cs={};r&2&&(Cs.$$scope={dirty:r,ctx:s}),W.$set(Cs)},i(s){ce||(u(S.$$.fragment,s),u(ds.$$.fragment,s),u(cs.$$.fragment,s),u(fs.$$.fragment,s),u(gs.$$.fragment,s),u(us.$$.fragment,s),u(_s.$$.fragment,s),u(vs.$$.fragment,s),u(W.$$.fragment,s),u(ys.$$.fragment,s),u($s.$$.fragment,s),u(Ss.$$.fragment,s),u(ws.$$.fragment,s),u(bs.$$.fragment,s),u(js.$$.fragment,s),u(Es.$$.fragment,s),u(xs.$$.fragment,s),u(Fs.$$.fragment,s),u(Ds.$$.fragment,s),u(Ts.$$.fragment,s),u(zs.$$.fragment,s),ce=!0)},o(s){_(S.$$.fragment,s),_(ds.$$.fragment,s),_(cs.$$.fragment,s),_(fs.$$.fragment,s),_(gs.$$.fragment,s),_(us.$$.fragment,s),_(_s.$$.fragment,s),_(vs.$$.fragment,s),_(W.$$.fragment,s),_(ys.$$.fragment,s),_($s.$$.fragment,s),_(Ss.$$.fragment,s),_(ws.$$.fragment,s),_(bs.$$.fragment,s),_(js.$$.fragment,s),_(Es.$$.fragment,s),_(xs.$$.fragment,s),_(Fs.$$.fragment,s),_(Ds.$$.fragment,s),_(Ts.$$.fragment,s),_(zs.$$.fragment,s),ce=!1},d(s){t(y),s&&t(G),s&&t($),v(S),s&&t(w),s&&t(E),s&&t(Dt),s&&t(O),s&&t(Lt),s&&t(I),s&&t(Tt),s&&t(T),v(ds),s&&t(Xt),s&&t(X),v(cs),s&&t(zt),s&&t(Os),s&&t(Ct),v(fs,s),s&&t(Pt),s&&t(ms),s&&t(Bt),v(gs,s),s&&t(Gt),s&&t(A),s&&t(Ot),v(us,s),s&&t(It),s&&t(z),v(_s),s&&t(Nt),s&&t(R),s&&t(qt),v(vs,s),s&&t(Ht),v(W,s),s&&t(Rt),s&&t(M),s&&t(Wt),v(ys,s),s&&t(Mt),s&&t(C),v($s),s&&t(Yt),s&&t(J),s&&t(Jt),v(Ss,s),s&&t(Ut),s&&t(U),s&&t(Kt),v(ws,s),s&&t(Qt),s&&t(P),v(bs),s&&t(Vt),s&&t(qs),s&&t(Zt),v(js,s),s&&t(se),s&&t(ks),s&&t(te),v(Es,s),s&&t(ee),s&&t(As),s&&t(ae),v(xs,s),s&&t(le),s&&t(B),v(Fs),s&&t(oe),s&&t(Hs),s&&t(re),v(Ds,s),s&&t(ne),s&&t(Ls),s&&t(ie),v(Ts,s),s&&t(pe),s&&t(Xs),s&&t(de),v(zs,s)}}}const io={local:"cloud-storage",sections:[{local:"amazon-s3",sections:[{local:"listing-datasets",title:"Listing datasets"},{local:"saving-datasets",title:"Saving datasets"},{local:"loading-datasets",title:"Loading datasets"}],title:"Amazon S3"},{local:"google-cloud-storage",title:"Google Cloud Storage"},{local:"azure-blob-storage",title:"Azure Blob Storage"}],title:"Cloud storage"};function po(Ft){return lo(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class go extends so{constructor(y){super();to(this,y,po,no,eo,{})}}export{go as default,io as metadata};
