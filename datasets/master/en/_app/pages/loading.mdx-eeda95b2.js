import{S as Vf,i as Mf,s as zf,e as l,k as f,t as i,c as o,a as n,m as c,h as p,d as t,b as h,g as r,F as a,Q as to,q as y,l as qu,n as Gr,o as b,B as x,p as Qr,w as k,y as E,j as Hu,G as Fu,$ as Tu,x as q,a0 as Lu,T as Ru,Y as Du,Z as Iu,M as Vu}from"../chunks/vendor-aa873a46.js";import{T as Ee}from"../chunks/Tip-f7f252ab.js";import{I as C}from"../chunks/IconCopyLink-d0ca3106.js";import{a as Nu,C as D}from"../chunks/CodeBlock-1f14baf3.js";import{b as Cu,I as Mu,a as zu}from"../chunks/IconTensorflow-b9816778.js";function Pu(P,d,m){const u=P.slice();return u[8]=d[m],u[10]=m,u}function Au(P){let d,m,u;var w=P[8].icon;function $(v){return{props:{classNames:"mr-1.5"}}}return w&&(d=new w($())),{c(){d&&k(d.$$.fragment),m=qu()},l(v){d&&q(d.$$.fragment,v),m=qu()},m(v,_){d&&E(d,v,_),r(v,m,_),u=!0},p(v,_){if(w!==(w=v[8].icon)){if(d){Gr();const j=d;b(j.$$.fragment,1,0,()=>{x(j,1)}),Qr()}w?(d=new w($()),k(d.$$.fragment),y(d.$$.fragment,1),E(d,m.parentNode,m)):d=null}},i(v){u||(d&&y(d.$$.fragment,v),u=!0)},o(v){d&&b(d.$$.fragment,v),u=!1},d(v){v&&t(m),d&&x(d,v)}}}function Su(P){let d,m,u,w=P[8].name+"",$,v,_,j,g,A,S,T=P[8].icon&&Au(P);function Y(){return P[6](P[8])}return{c(){d=l("button"),T&&T.c(),m=f(),u=l("p"),$=i(w),_=f(),this.h()},l(N){d=o(N,"BUTTON",{class:!0});var I=n(d);T&&T.l(I),m=c(I),u=o(I,"P",{class:!0});var U=n(u);$=p(U,w),U.forEach(t),_=c(I),I.forEach(t),this.h()},h(){h(u,"class",v="!m-0 "+P[8].classNames),h(d,"class",j="flex justify-center py-1.5 px-2.5 focus:outline-none rounded-"+(P[10]?"r":"l")+" "+(P[8].group!==P[1]&&"text-gray-500 filter grayscale"))},m(N,I){r(N,d,I),T&&T.m(d,null),a(d,m),a(d,u),a(u,$),a(d,_),g=!0,A||(S=to(d,"click",Y),A=!0)},p(N,I){P=N,P[8].icon?T?(T.p(P,I),I&1&&y(T,1)):(T=Au(P),T.c(),y(T,1),T.m(d,m)):T&&(Gr(),b(T,1,1,()=>{T=null}),Qr()),(!g||I&1)&&w!==(w=P[8].name+"")&&Hu($,w),(!g||I&1&&v!==(v="!m-0 "+P[8].classNames))&&h(u,"class",v),(!g||I&3&&j!==(j="flex justify-center py-1.5 px-2.5 focus:outline-none rounded-"+(P[10]?"r":"l")+" "+(P[8].group!==P[1]&&"text-gray-500 filter grayscale")))&&h(d,"class",j)},i(N){g||(y(T),g=!0)},o(N){b(T),g=!1},d(N){N&&t(d),T&&T.d(),A=!1,S()}}}function Bu(P){let d,m,u,w=P[3].filter(P[5]),$=[];for(let _=0;_<w.length;_+=1)$[_]=Su(Pu(P,w,_));const v=_=>b($[_],1,1,()=>{$[_]=null});return{c(){d=l("div"),m=l("div");for(let _=0;_<$.length;_+=1)$[_].c();this.h()},l(_){d=o(_,"DIV",{});var j=n(d);m=o(j,"DIV",{class:!0});var g=n(m);for(let A=0;A<$.length;A+=1)$[A].l(g);g.forEach(t),j.forEach(t),this.h()},h(){h(m,"class","bg-white leading-none border border-gray-100 rounded-lg inline-flex p-0.5 text-sm mb-4 select-none")},m(_,j){r(_,d,j),a(d,m);for(let g=0;g<$.length;g+=1)$[g].m(m,null);u=!0},p(_,[j]){if(j&27){w=_[3].filter(_[5]);let g;for(g=0;g<w.length;g+=1){const A=Pu(_,w,g);$[g]?($[g].p(A,j),y($[g],1)):($[g]=Su(A),$[g].c(),y($[g],1),$[g].m(m,null))}for(Gr(),g=w.length;g<$.length;g+=1)v(g);Qr()}},i(_){if(!u){for(let j=0;j<w.length;j+=1)y($[j]);u=!0}},o(_){$=$.filter(Boolean);for(let j=0;j<$.length;j+=1)b($[j]);u=!1},d(_){_&&t(d),Fu($,_)}}}function Uu(P,d,m){let u,{ids:w}=d;const $=w.join("-"),v=Cu($);Tu(P,v,S=>m(1,u=S));const _=[{id:"pt",classNames:"",icon:Mu,name:"Pytorch",group:"group1"},{id:"tf",classNames:"",icon:zu,name:"TensorFlow",group:"group2"},{id:"stringapi",classNames:"text-blue-600",name:"String API",group:"group1"},{id:"readinstruction",classNames:"text-blue-600",name:"ReadInstruction",group:"group2"}];function j(S){Lu(v,u=S,u)}const g=S=>w.includes(S.id),A=S=>j(S.group);return P.$$set=S=>{"ids"in S&&m(0,w=S.ids)},[w,u,v,_,j,g,A]}class Ou extends Vf{constructor(d){super();Mf(this,d,Uu,Bu,zf,{ids:0})}}function Ju(P){let d,m,u,w,$,v,_=P[1].highlighted+"",j;return m=new Nu({props:{classNames:"transition duration-200 ease-in-out "+(P[2]&&"opacity-0"),title:"Copy code excerpt to clipboard",value:P[1].code}}),$=new Ou({props:{ids:P[4]}}),{c(){d=l("div"),k(m.$$.fragment),u=f(),w=l("pre"),k($.$$.fragment),v=new Du,this.h()},l(g){d=o(g,"DIV",{class:!0});var A=n(d);q(m.$$.fragment,A),A.forEach(t),u=c(g),w=o(g,"PRE",{});var S=n(w);q($.$$.fragment,S),v=Iu(S),S.forEach(t),this.h()},h(){h(d,"class","absolute top-2.5 right-4"),v.a=null},m(g,A){r(g,d,A),E(m,d,null),r(g,u,A),r(g,w,A),E($,w,null),v.m(_,w),j=!0},p(g,A){const S={};A&4&&(S.classNames="transition duration-200 ease-in-out "+(g[2]&&"opacity-0")),A&2&&(S.value=g[1].code),m.$set(S),(!j||A&2)&&_!==(_=g[1].highlighted+"")&&v.p(_)},i(g){j||(y(m.$$.fragment,g),y($.$$.fragment,g),j=!0)},o(g){b(m.$$.fragment,g),b($.$$.fragment,g),j=!1},d(g){g&&t(d),x(m),g&&t(u),g&&t(w),x($)}}}function Yu(P){let d,m,u,w,$,v,_=P[0].highlighted+"",j;return m=new Nu({props:{classNames:"transition duration-200 ease-in-out "+(P[2]&&"opacity-0"),title:"Copy code excerpt to clipboard",value:P[0].code}}),$=new Ou({props:{ids:P[4]}}),{c(){d=l("div"),k(m.$$.fragment),u=f(),w=l("pre"),k($.$$.fragment),v=new Du,this.h()},l(g){d=o(g,"DIV",{class:!0});var A=n(d);q(m.$$.fragment,A),A.forEach(t),u=c(g),w=o(g,"PRE",{});var S=n(w);q($.$$.fragment,S),v=Iu(S),S.forEach(t),this.h()},h(){h(d,"class","absolute top-2.5 right-4"),v.a=null},m(g,A){r(g,d,A),E(m,d,null),r(g,u,A),r(g,w,A),E($,w,null),v.m(_,w),j=!0},p(g,A){const S={};A&4&&(S.classNames="transition duration-200 ease-in-out "+(g[2]&&"opacity-0")),A&1&&(S.value=g[0].code),m.$set(S),(!j||A&1)&&_!==(_=g[0].highlighted+"")&&v.p(_)},i(g){j||(y(m.$$.fragment,g),y($.$$.fragment,g),j=!0)},o(g){b(m.$$.fragment,g),b($.$$.fragment,g),j=!1},d(g){g&&t(d),x(m),g&&t(u),g&&t(w),x($)}}}function Wu(P){let d,m,u,w,$,v;const _=[Yu,Ju],j=[];function g(A,S){return A[3]==="group1"?0:1}return m=g(P),u=j[m]=_[m](P),{c(){d=l("div"),u.c(),this.h()},l(A){d=o(A,"DIV",{class:!0});var S=n(d);u.l(S),S.forEach(t),this.h()},h(){h(d,"class","code-block relative")},m(A,S){r(A,d,S),j[m].m(d,null),w=!0,$||(v=[to(d,"mouseover",P[6]),to(d,"focus",P[6]),to(d,"mouseout",P[7]),to(d,"focus",P[7])],$=!0)},p(A,[S]){let T=m;m=g(A),m===T?j[m].p(A,S):(Gr(),b(j[T],1,1,()=>{j[T]=null}),Qr(),u=j[m],u?u.p(A,S):(u=j[m]=_[m](A),u.c()),y(u,1),u.m(d,null))},i(A){w||(y(u),w=!0)},o(A){b(u),w=!1},d(A){A&&t(d),j[m].d(),$=!1,Ru(v)}}}function Gu(P,d,m){let u,{group1:w}=d,{group2:$}=d;const v=[w.id,$.id],_=v.join("-"),j=Cu(_);Tu(P,j,T=>m(3,u=T));let g=!0;function A(){m(2,g=!1)}function S(){m(2,g=!0)}return P.$$set=T=>{"group1"in T&&m(0,w=T.group1),"group2"in T&&m(1,$=T.group2)},[w,$,g,u,v,j,A,S]}class so extends Vf{constructor(d){super();Mf(this,d,Gu,Wu,zf,{group1:0,group2:1})}}function Qu(P){let d,m,u,w,$;return{c(){d=l("p"),m=i("Refer to the "),u=l("a"),w=i("upload_dataset_repo"),$=i(" guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),this.h()},l(v){d=o(v,"P",{});var _=n(d);m=p(_,"Refer to the "),u=o(_,"A",{href:!0});var j=n(u);w=p(j,"upload_dataset_repo"),j.forEach(t),$=p(_," guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),_.forEach(t),this.h()},h(){h(u,"href","#upload_dataset_repo")},m(v,_){r(v,d,_),a(d,m),a(d,u),a(u,w),a(d,$)},d(v){v&&t(d)}}}function Ku(P){let d,m,u,w,$;return{c(){d=l("p"),m=i("If you don\u2019t specify which data files to use, "),u=l("code"),w=i("load_dataset"),$=i(" will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data.")},l(v){d=o(v,"P",{});var _=n(d);m=p(_,"If you don\u2019t specify which data files to use, "),u=o(_,"CODE",{});var j=n(u);w=p(j,"load_dataset"),j.forEach(t),$=p(_," will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data."),_.forEach(t)},m(v,_){r(v,d,_),a(d,m),a(d,u),a(u,w),a(d,$)},d(v){v&&t(d)}}}function Xu(P){let d,m,u,w,$,v,_,j,g,A,S,T,Y,N,I,U,O;return{c(){d=l("p"),m=i("An object data type in "),u=l("a"),w=i("pandas.Series"),$=i(" doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),v=l("a"),_=i("datasets.Features"),j=i(" using the "),g=l("code"),A=i("from_dict"),S=i(" or "),T=l("code"),Y=i("from_pandas"),N=i(" methods. See the "),I=l("a"),U=i("troubleshoot"),O=i(" for more details on how to explicitly specify your own features."),this.h()},l(J){d=o(J,"P",{});var H=n(d);m=p(H,"An object data type in "),u=o(H,"A",{href:!0,rel:!0});var ba=n(u);w=p(ba,"pandas.Series"),ba.forEach(t),$=p(H," doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),v=o(H,"A",{href:!0});var ks=n(v);_=p(ks,"datasets.Features"),ks.forEach(t),j=p(H," using the "),g=o(H,"CODE",{});var ja=n(g);A=p(ja,"from_dict"),ja.forEach(t),S=p(H," or "),T=o(H,"CODE",{});var xa=n(T);Y=p(xa,"from_pandas"),xa.forEach(t),N=p(H," methods. See the "),I=o(H,"A",{href:!0});var Es=n(I);U=p(Es,"troubleshoot"),Es.forEach(t),O=p(H," for more details on how to explicitly specify your own features."),H.forEach(t),this.h()},h(){h(u,"href","https://pandas.pydata.org/docs/reference/api/pandas.Series.html"),h(u,"rel","nofollow"),h(v,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.Features"),h(I,"href","#troubleshoot")},m(J,H){r(J,d,H),a(d,m),a(d,u),a(u,w),a(d,$),a(d,v),a(v,_),a(d,j),a(d,g),a(g,A),a(d,S),a(d,T),a(T,Y),a(d,N),a(d,I),a(I,U),a(d,O)},d(J){J&&t(d)}}}function Zu(P){let d,m,u,w,$;return{c(){d=l("p"),m=i("Using "),u=l("code"),w=i("pct1_dropremainder"),$=i(" rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100.")},l(v){d=o(v,"P",{});var _=n(d);m=p(_,"Using "),u=o(_,"CODE",{});var j=n(u);w=p(j,"pct1_dropremainder"),j.forEach(t),$=p(_," rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100."),_.forEach(t)},m(v,_){r(v,d,_),a(d,m),a(d,u),a(u,w),a(d,$)},d(v){v&&t(d)}}}function sm(P){let d,m,u,w,$;return{c(){d=l("p"),m=i("See the "),u=l("a"),w=i("metric_script"),$=i(" guide for more details on how to write your own metric loading script."),this.h()},l(v){d=o(v,"P",{});var _=n(d);m=p(_,"See the "),u=o(_,"A",{href:!0});var j=n(u);w=p(j,"metric_script"),j.forEach(t),$=p(_," guide for more details on how to write your own metric loading script."),_.forEach(t),this.h()},h(){h(u,"href","#metric_script")},m(v,_){r(v,d,_),a(d,m),a(d,u),a(u,w),a(d,$)},d(v){v&&t(d)}}}function tm(P){let d,m,u,w,$;return{c(){d=l("p"),m=i("Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),u=l("a"),w=i("datasets.Metric.compute()"),$=i(" gathers all the predictions and references from the nodes, and computes the final metric."),this.h()},l(v){d=o(v,"P",{});var _=n(d);m=p(_,"Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),u=o(_,"A",{href:!0});var j=n(u);w=p(j,"datasets.Metric.compute()"),j.forEach(t),$=p(_," gathers all the predictions and references from the nodes, and computes the final metric."),_.forEach(t),this.h()},h(){h(u,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.Metric.compute")},m(v,_){r(v,d,_),a(d,m),a(d,u),a(u,w),a(d,$)},d(v){v&&t(d)}}}function am(P){let d,m,u,w,$,v,_,j,g,A,S,T,Y,N,I,U,O,J,H,ba,ks,ja,xa,Es,Kr,Xr,qe,Zr,si,Pe,ti,ao,ka,ai,eo,Ea,lo,os,qs,Ae,ct,ei,Se,li,oo,Ps,oi,Te,ni,ri,no,W,ii,qa,pi,di,ht,fi,ci,ro,ut,io,Pa,hi,po,As,ui,De,mi,gi,fo,mt,co,Ss,ho,F,_i,Ie,vi,$i,Ne,wi,yi,Ce,bi,ji,Oe,xi,ki,He,Ei,qi,uo,gt,mo,Ts,go,G,Pi,Fe,Ai,Si,_t,Ti,Di,_o,vt,vo,Ds,Ii,Le,Ni,Ci,$o,$t,wo,ns,Is,Re,wt,Oi,Ve,Hi,yo,L,Fi,Me,Li,Ri,ze,Vi,Mi,Be,zi,Bi,Ue,Ui,Ji,Aa,Yi,Wi,bo,rs,Ns,Je,yt,Gi,Ye,Qi,jo,Sa,Ki,xo,bt,ko,Ta,Xi,Eo,jt,qo,Da,Zi,Po,xt,Ao,Ia,sp,So,kt,To,Na,tp,Do,Et,Io,is,Cs,We,qt,ap,Ge,ep,No,Os,lp,Ca,op,np,Co,Pt,Oo,Oa,rp,Ho,At,Fo,Hs,ip,Qe,pp,dp,Lo,St,Ro,Ha,fp,Vo,Tt,Mo,Fa,cp,zo,ps,Fs,Ke,Dt,hp,Xe,up,Bo,La,mp,Uo,It,Jo,Ra,gp,Yo,Nt,Wo,ds,Ls,Ze,Ct,_p,sl,vp,Go,Va,$p,Qo,Ot,Ko,Ma,wp,Xo,Ht,Zo,fs,Rs,tl,Ft,yp,al,bp,sn,za,jp,tn,Ba,xp,an,Lt,en,R,kp,el,Ep,qp,ll,Pp,Ap,Ua,Sp,Tp,ol,Dp,Ip,ln,Rt,on,Ja,Np,nn,Vt,rn,V,Cp,nl,Op,Hp,rl,Fp,Lp,il,Rp,Vp,pn,cs,Vs,pl,Mt,Mp,dl,zp,dn,Ms,Bp,Ya,Up,Jp,fn,hs,zs,fl,zt,Yp,cl,Wp,cn,Bs,Gp,Wa,Qp,Kp,hn,Bt,un,us,Us,hl,Ut,Xp,ul,Zp,mn,Js,sd,Ga,td,ad,gn,Jt,_n,Ys,vn,ms,Ws,ml,Yt,ed,gl,ld,$n,Qa,od,wn,Q,nd,_l,rd,id,vl,pd,dd,yn,gs,Gs,$l,Wt,fd,wl,cd,bn,K,hd,Ka,ud,md,Xa,gd,_d,jn,X,vd,yl,$d,wd,bl,yd,bd,xn,Gt,kn,Qs,jd,jl,xd,kd,En,Qt,qn,Za,Ed,Pn,Kt,An,se,qd,Sn,Xt,Tn,te,Pd,Dn,Zt,In,_s,Ks,xl,sa,Ad,kl,Sd,Nn,ae,Td,Cn,ta,On,Xs,Dd,El,Id,Nd,Hn,aa,Fn,Zs,Ln,ee,Rn,vs,st,ql,ea,Cd,Pl,Od,Vn,le,Hd,Mn,$s,tt,Al,la,Fd,Sl,Ld,zn,M,Rd,oe,Vd,Md,Tl,zd,Bd,Dl,Ud,Jd,Bn,at,Yd,oa,Wd,Gd,Un,na,Jn,ws,et,Il,ra,Qd,Nl,Kd,Yn,Z,Xd,ne,Zd,sf,ia,tf,af,Wn,ss,ef,re,lf,of,ie,nf,rf,Gn,pa,Qn,ts,pf,Cl,df,ff,pe,cf,hf,Kn,da,Xn,de,uf,Zn,fa,sr,ys,lt,Ol,ca,mf,Hl,gf,tr,fe,_f,ar,ha,er,ot,lr,bs,nt,Fl,ua,vf,Ll,$f,or,ce,wf,nr,ma,rr,js,rt,Rl,ga,yf,Vl,bf,ir,he,jf,pr,ue,xf,dr,as,Ml,_a,kf,zl,Ef,qf,Pf,Bl,xs,Af,Ul,Sf,Tf,Jl,Df,If,Nf,Yl,va,Cf,me,Of,Hf,fr,$a,cr,it,hr,pt,Ff,Wl,Lf,Rf,ur,wa,mr;return v=new C({}),ct=new C({}),ut=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset('lhoestq/demo1')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;lhoestq/demo1&#x27;</span>)`}}),mt=new D({props:{code:`dataset = load_dataset(
  "lhoestq/custom_squad",
  revision="main"  # tag name, or branch name, or commit hash
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(
<span class="hljs-meta">&gt;&gt;&gt; </span>  <span class="hljs-string">&quot;lhoestq/custom_squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>  revision=<span class="hljs-string">&quot;main&quot;</span>  <span class="hljs-comment"># tag name, or branch name, or commit hash</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),Ss=new Ee({props:{$$slots:{default:[Qu]},$$scope:{ctx:P}}}),gt=new D({props:{code:`data_files = {"train": "train.csv", "test": "test.csv"}
dataset = load_dataset("namespace/your_dataset_name", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: <span class="hljs-string">&quot;train.csv&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>: <span class="hljs-string">&quot;test.csv&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;namespace/your_dataset_name&quot;</span>, data_files=data_files)`}}),Ts=new Ee({props:{warning:"&lcub;true}",$$slots:{default:[Ku]},$$scope:{ctx:P}}}),vt=new D({props:{code:`from datasets import load_dataset
c4_subset = load_dataset('allenai/c4', data_files='en/c4-train.0000*-of-01024.json.gz')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_subset = load_dataset(<span class="hljs-string">&#x27;allenai/c4&#x27;</span>, data_files=<span class="hljs-string">&#x27;en/c4-train.0000*-of-01024.json.gz&#x27;</span>)`}}),$t=new D({props:{code:`data_files = {"validation": "en/c4-validation.*.json.gz"}
c4_validation = load_dataset("allenai/c4", data_files=data_files, split="validation")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;validation&quot;</span>: <span class="hljs-string">&quot;en/c4-validation.*.json.gz&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_validation = load_dataset(<span class="hljs-string">&quot;allenai/c4&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;validation&quot;</span>)`}}),wt=new C({}),yt=new C({}),bt=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset('csv', data_files='my_file.csv')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.csv&#x27;</span>)`}}),jt=new D({props:{code:"dataset = load_dataset('csv', data_files=['my_file_1.csv', 'my_file_2.csv', 'my_file_3.csv'])",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=[<span class="hljs-string">&#x27;my_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_2.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_3.csv&#x27;</span>])'}}),xt=new D({props:{code:"dataset = load_dataset('csv', data_files={'train': ['my_train_file_1.csv', 'my_train_file_2.csv'] 'test': 'my_test_file.csv'})",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_train_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_train_file_2.csv&#x27;</span>] <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.csv&#x27;</span>})'}}),kt=new D({props:{code:`base_url = "https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/"
dataset = load_dataset('csv', data_files={'train': base_url + 'train.csv', 'test': base_url + 'test.csv'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train.csv&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: base_url + <span class="hljs-string">&#x27;test.csv&#x27;</span>})`}}),Et=new D({props:{code:`url = "https://domain.org/train_data.zip"
data_files = {"train": url}
dataset = load_dataset("csv", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://domain.org/train_data.zip&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: url}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=data_files)`}}),qt=new C({}),Pt=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset('json', data_files='my_file.json')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>)`}}),At=new D({props:{code:`{"a": 1, "b": 2.0, "c": "foo", "d": false}
{"a": 4, "b": -5.5, "c": null, "d": true}`,highlighted:`{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false}
{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}`}}),St=new D({props:{code:`{"version": "0.1.0",
    "data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},
            {"a": 4, "b": -5.5, "c": null, "d": true}]
}

>>> from datasets import load_dataset
>>> dataset = load_dataset('json', data_files='my_file.json', field='data')`,highlighted:`{<span class="hljs-string">&quot;version&quot;</span>: <span class="hljs-string">&quot;0.1.0&quot;</span>,
    <span class="hljs-string">&quot;data&quot;</span>: [{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false},
            {<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}]
}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>, field=<span class="hljs-string">&#x27;data&#x27;</span>)`}}),Tt=new D({props:{code:`base_url = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
dataset = load_dataset('json', data_files={'train': base_url + 'train-v1.1.json', 'validation': base_url + 'dev-v1.1.json'}, field="data")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://rajpurkar.github.io/SQuAD-explorer/dataset/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train-v1.1.json&#x27;</span>, <span class="hljs-string">&#x27;validation&#x27;</span>: base_url + <span class="hljs-string">&#x27;dev-v1.1.json&#x27;</span>}, field=<span class="hljs-string">&quot;data&quot;</span>)`}}),Dt=new C({}),It=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset('text', data_files={'train': ['my_text_1.txt', 'my_text_2.txt'], 'test': 'my_test_file.txt'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_text_1.txt&#x27;</span>, <span class="hljs-string">&#x27;my_text_2.txt&#x27;</span>], <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.txt&#x27;</span>})`}}),Nt=new D({props:{code:"dataset = load_dataset('text', data_files='https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files=<span class="hljs-string">&#x27;https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt&#x27;</span>)'}}),Ct=new C({}),Ot=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset("parquet", data_files={'train': 'train.parquet', 'test': 'test.parquet'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;train.parquet&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;test.parquet&#x27;</span>})`}}),Ht=new D({props:{code:`base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"
data_files = {"train": base_url + "wikipedia-train.parquet"}
wiki = load_dataset("parquet", data_files=data_files, split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: base_url + <span class="hljs-string">&quot;wikipedia-train.parquet&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>wiki = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Ft=new C({}),Lt=new D({props:{code:`   data/dog/xxx.png
   data/dog/xxy.png
   data/dog/xxz.png

   data/cat/123.png
   data/cat/nsdf3.png
   data/cat/asd932_.png`,highlighted:`   data<span class="hljs-regexp">/dog/</span>xxx.png
   data<span class="hljs-regexp">/dog/</span>xxy.png
   data<span class="hljs-regexp">/dog/</span>xxz.png

   data<span class="hljs-regexp">/cat/</span><span class="hljs-number">123</span>.png
   data<span class="hljs-regexp">/cat/</span>nsdf3.png
   data<span class="hljs-regexp">/cat/</span>asd932_.png`}}),Rt=new D({props:{code:`from datasets import load_dataset
dataset = load_dataset("imagefolder", data_dir="/path/to/data")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;imagefolder&quot;</span>, data_dir=<span class="hljs-string">&quot;/path/to/data&quot;</span>)`}}),Vt=new D({props:{code:'dataset = load_dataset("imagefolder", data_files="https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip", split="train")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;imagefolder&quot;</span>, data_files=<span class="hljs-string">&quot;https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)'}}),Mt=new C({}),zt=new C({}),Bt=new D({props:{code:`from datasets import Dataset
my_dict = {"a": [1, 2, 3]}
dataset = Dataset.from_dict(my_dict)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>my_dict = {<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_dict(my_dict)`}}),Ut=new C({}),Jt=new D({props:{code:`from datasets import Dataset
import pandas as pd
df = pd.DataFrame({"a": [1, 2, 3]})
dataset = Dataset.from_pandas(df)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-meta">&gt;&gt;&gt; </span>df = pd.DataFrame({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_pandas(df)`}}),Ys=new Ee({props:{warning:"&lcub;true}",$$slots:{default:[Xu]},$$scope:{ctx:P}}}),Yt=new C({}),Wt=new C({}),Gt=new so({props:{group1:{id:"stringapi",code:"train_test_ds = datasets.load_dataset('bookcorpus', split='train+test')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train+test&#x27;</span>)'},group2:{id:"readinstruction",code:`ri = datasets.ReadInstruction('train') + datasets.ReadInstruction('test')
train_test_ds = datasets.load_dataset('bookcorpus', split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}}),Qt=new so({props:{group1:{id:"stringapi",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split='train[10:20]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[10:20]&#x27;</span>)'},group2:{id:"readinstruction",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', from_=10, to=20, unit='abs'))",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">10</span>, to=<span class="hljs-number">20</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>))'}}}),Kt=new so({props:{group1:{id:"stringapi",code:"train_10pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]&#x27;</span>)'},group2:{id:"readinstruction",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', to=10, unit='%'))",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))'}}}),Xt=new so({props:{group1:{id:"stringapi",code:"train_10_80pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]+train[-80%:]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]+train[-80%:]&#x27;</span>)'},group2:{id:"readinstruction",code:`ri = (datasets.ReadInstruction('train', to=10, unit='%') + datasets.ReadInstruction('train', from_=-80, unit='%'))
train_10_80pct_ds = datasets.load_dataset('bookcorpus', split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = (datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=-<span class="hljs-number">80</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}}),Zt=new so({props:{group1:{id:"stringapi",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', split=[f'train[{k}%:{k+10}%]' for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', split=[f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[<span class="hljs-subst">{k}</span>%:<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[:<span class="hljs-subst">{k}</span>%]+train[<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%:]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`},group2:{id:"readinstruction",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', [datasets.ReadInstruction('train', from_=k, to=k+10, unit='%') for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', [(datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')) for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [(datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}}),sa=new C({}),ta=new D({props:{code:`# Assuming *train* split contains 999 records.
# 19 records, from 500 (included) to 519 (excluded).
>>> train_50_52_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%]')
# 20 records, from 519 (included) to 539 (excluded).
>>> train_52_54_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%]')`,highlighted:`<span class="hljs-comment"># Assuming *train* split contains 999 records.</span>
<span class="hljs-comment"># 19 records, from 500 (included) to 519 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%]&#x27;</span>)
<span class="hljs-comment"># 20 records, from 519 (included) to 539 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%]&#x27;</span>)`}}),aa=new D({props:{code:`# 18 records, from 450 (included) to 468 (excluded).
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction( 'train', from_=50, to=52, unit='%', rounding='pct1_dropremainder'))
# 18 records, from 468 (included) to 486 (excluded).
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train',from_=52, to=54, unit='%', rounding='pct1_dropremainder'))
# Or equivalently:
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%](pct1_dropremainder)')
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%](pct1_dropremainder)')`,highlighted:`<span class="hljs-comment"># 18 records, from 450 (included) to 468 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction( <span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">50</span>, to=<span class="hljs-number">52</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># 18 records, from 468 (included) to 486 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>,from_=<span class="hljs-number">52</span>, to=<span class="hljs-number">54</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># Or equivalently:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%](pct1_dropremainder)&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%](pct1_dropremainder)&#x27;</span>)`}}),Zs=new Ee({props:{warning:"&lcub;true}",$$slots:{default:[Zu]},$$scope:{ctx:P}}}),ea=new C({}),la=new C({}),na=new D({props:{code:'dataset = load_dataset("matinf", "summarization")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;matinf&quot;</span>, <span class="hljs-string">&quot;summarization&quot;</span>)
Downloading <span class="hljs-keyword">and</span> preparing dataset matinf/summarization (download: Unknown size, generated: <span class="hljs-number">246.89</span> MiB, post-processed: Unknown size, total: <span class="hljs-number">246.89</span> MiB) to /root/.cache/huggingface/datasets/matinf/summarization/<span class="hljs-number">1.0</span><span class="hljs-number">.0</span>/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...
AssertionError: The dataset matinf <span class="hljs-keyword">with</span> config summarization requires manual data. 
Please follow the manual download instructions: To use MATINF you have to download it manually. Please fill this google form (https://forms.gle/nkH4LVE4iNQeDzsc9). You will receive a download link <span class="hljs-keyword">and</span> a password once you complete the form. Please extract <span class="hljs-built_in">all</span> files <span class="hljs-keyword">in</span> one folder <span class="hljs-keyword">and</span> load the dataset <span class="hljs-keyword">with</span>: *datasets.load_dataset(<span class="hljs-string">&#x27;matinf&#x27;</span>, data_dir=<span class="hljs-string">&#x27;path/to/folder/folder_name&#x27;</span>)*. 
Manual data can be loaded <span class="hljs-keyword">with</span> \`datasets.load_dataset(matinf, data_dir=<span class="hljs-string">&#x27;&lt;path/to/manual/data&gt;&#x27;</span>) `}}),ra=new C({}),pa=new D({props:{code:`class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]
emotion_features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>class_names = [<span class="hljs-string">&quot;sadness&quot;</span>, <span class="hljs-string">&quot;joy&quot;</span>, <span class="hljs-string">&quot;love&quot;</span>, <span class="hljs-string">&quot;anger&quot;</span>, <span class="hljs-string">&quot;fear&quot;</span>, <span class="hljs-string">&quot;surprise&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>emotion_features = Features({<span class="hljs-string">&#x27;text&#x27;</span>: Value(<span class="hljs-string">&#x27;string&#x27;</span>), <span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(names=class_names)})`}}),da=new D({props:{code:"dataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'], features=emotion_features)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=file_dict, delimiter=<span class="hljs-string">&#x27;;&#x27;</span>, column_names=[<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>], features=emotion_features)'}}),fa=new D({props:{code:"dataset['train'].features",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&#x27;train&#x27;</span>].features
{<span class="hljs-string">&#x27;text&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">6</span>, names=[<span class="hljs-string">&#x27;sadness&#x27;</span>, <span class="hljs-string">&#x27;joy&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;anger&#x27;</span>, <span class="hljs-string">&#x27;fear&#x27;</span>, <span class="hljs-string">&#x27;surprise&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),ca=new C({}),ha=new D({props:{code:`from datasets import load_metric
metric = load_metric('PATH/TO/MY/METRIC/SCRIPT')

# Example of typical usage
for batch in dataset:
    inputs, references = batch
    predictions = model(inputs)
    metric.add_batch(predictions=predictions, references=references)
score = metric.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;PATH/TO/MY/METRIC/SCRIPT&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Example of typical usage</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataset:
<span class="hljs-meta">... </span>    inputs, references = batch
<span class="hljs-meta">... </span>    predictions = model(inputs)
<span class="hljs-meta">... </span>    metric.add_batch(predictions=predictions, references=references)
<span class="hljs-meta">&gt;&gt;&gt; </span>score = metric.compute()`}}),ot=new Ee({props:{$$slots:{default:[sm]},$$scope:{ctx:P}}}),ua=new C({}),ma=new D({props:{code:`from datasets import load_metric
metric = load_metric('bleurt', name='bleurt-base-128')
metric = load_metric('bleurt', name='bleurt-base-512')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-128&#x27;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-512&#x27;</span>)</span>`}}),ga=new C({}),$a=new D({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=rank)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=rank)`}}),it=new Ee({props:{$$slots:{default:[tm]},$$scope:{ctx:P}}}),wa=new D({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=process_id, experiment_id="My_experiment_10")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=process_id, experiment_id=<span class="hljs-string">&quot;My_experiment_10&quot;</span>)`}}),{c(){d=l("meta"),m=f(),u=l("h1"),w=l("a"),$=l("span"),k(v.$$.fragment),_=f(),j=l("span"),g=i("Load"),A=f(),S=l("p"),T=i("You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),Y=f(),N=l("p"),I=i("This guide will show you how to load a dataset from:"),U=f(),O=l("ul"),J=l("li"),H=i("The Hub without a dataset loading script"),ba=f(),ks=l("li"),ja=i("Local files"),xa=f(),Es=l("li"),Kr=i("In-memory data"),Xr=f(),qe=l("li"),Zr=i("Offline"),si=f(),Pe=l("li"),ti=i("A specific slice of a split"),ao=f(),ka=l("p"),ai=i("You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),eo=f(),Ea=l("a"),lo=f(),os=l("h2"),qs=l("a"),Ae=l("span"),k(ct.$$.fragment),ei=f(),Se=l("span"),li=i("Hugging Face Hub"),oo=f(),Ps=l("p"),oi=i("In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),Te=l("strong"),ni=i("without"),ri=i(" a loading script!"),no=f(),W=l("p"),ii=i("First, create a dataset repository and upload your data files. Then you can use "),qa=l("a"),pi=i("datasets.load_dataset()"),di=i(" like you learned in the tutorial. For example, load the files from this "),ht=l("a"),fi=i("demo repository"),ci=i(" by providing the repository namespace and dataset name:"),ro=f(),k(ut.$$.fragment),io=f(),Pa=l("p"),hi=i("This dataset repository contains CSV files, and this code loads all the data from the CSV files."),po=f(),As=l("p"),ui=i("Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),De=l("code"),mi=i("revision"),gi=i(" flag to specify which dataset version you want to load:"),fo=f(),k(mt.$$.fragment),co=f(),k(Ss.$$.fragment),ho=f(),F=l("p"),_i=i("If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),Ie=l("code"),vi=i("train"),$i=i(" split. Use the "),Ne=l("code"),wi=i("data_files"),yi=i(" parameter to map data files to splits like "),Ce=l("code"),bi=i("train"),ji=i(", "),Oe=l("code"),xi=i("validation"),ki=i(" and "),He=l("code"),Ei=i("test"),qi=i(":"),uo=f(),k(gt.$$.fragment),mo=f(),k(Ts.$$.fragment),go=f(),G=l("p"),Pi=i("You can also load a specific subset of the files with the "),Fe=l("code"),Ai=i("data_files"),Si=i(" parameter. The example below loads files from the "),_t=l("a"),Ti=i("C4 dataset"),Di=i(":"),_o=f(),k(vt.$$.fragment),vo=f(),Ds=l("p"),Ii=i("Specify a custom split with the "),Le=l("code"),Ni=i("split"),Ci=i(" parameter:"),$o=f(),k($t.$$.fragment),wo=f(),ns=l("h2"),Is=l("a"),Re=l("span"),k(wt.$$.fragment),Oi=f(),Ve=l("span"),Hi=i("Local and remote files"),yo=f(),L=l("p"),Fi=i("\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Me=l("code"),Li=i("csv"),Ri=i(", "),ze=l("code"),Vi=i("json"),Mi=i(", "),Be=l("code"),zi=i("txt"),Bi=i(" or "),Ue=l("code"),Ui=i("parquet"),Ji=i(" file. The "),Aa=l("a"),Yi=i("datasets.load_dataset()"),Wi=i(" method is able to load each of these file types."),bo=f(),rs=l("h3"),Ns=l("a"),Je=l("span"),k(yt.$$.fragment),Gi=f(),Ye=l("span"),Qi=i("CSV"),jo=f(),Sa=l("p"),Ki=i("\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),xo=f(),k(bt.$$.fragment),ko=f(),Ta=l("p"),Xi=i("If you have more than one CSV file:"),Eo=f(),k(jt.$$.fragment),qo=f(),Da=l("p"),Zi=i("You can also map the training and test splits to specific CSV files:"),Po=f(),k(xt.$$.fragment),Ao=f(),Ia=l("p"),sp=i("To load remote CSV files via HTTP, you can pass the URLs:"),So=f(),k(kt.$$.fragment),To=f(),Na=l("p"),tp=i("To load zipped CSV files:"),Do=f(),k(Et.$$.fragment),Io=f(),is=l("h3"),Cs=l("a"),We=l("span"),k(qt.$$.fragment),ap=f(),Ge=l("span"),ep=i("JSON"),No=f(),Os=l("p"),lp=i("JSON files are loaded directly with "),Ca=l("a"),op=i("datasets.load_dataset()"),np=i(" as shown below:"),Co=f(),k(Pt.$$.fragment),Oo=f(),Oa=l("p"),rp=i("JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),Ho=f(),k(At.$$.fragment),Fo=f(),Hs=l("p"),ip=i("Another JSON format you may encounter is a nested field, in which case you will need to specify the "),Qe=l("code"),pp=i("field"),dp=i(" argument as shown in the following:"),Lo=f(),k(St.$$.fragment),Ro=f(),Ha=l("p"),fp=i("To load remote JSON files via HTTP, you can pass the URLs:"),Vo=f(),k(Tt.$$.fragment),Mo=f(),Fa=l("p"),cp=i("While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),zo=f(),ps=l("h3"),Fs=l("a"),Ke=l("span"),k(Dt.$$.fragment),hp=f(),Xe=l("span"),up=i("Text files"),Bo=f(),La=l("p"),mp=i("Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),Uo=f(),k(It.$$.fragment),Jo=f(),Ra=l("p"),gp=i("To load remote TXT files via HTTP, you can pass the URLs:"),Yo=f(),k(Nt.$$.fragment),Wo=f(),ds=l("h3"),Ls=l("a"),Ze=l("span"),k(Ct.$$.fragment),_p=f(),sl=l("span"),vp=i("Parquet"),Go=f(),Va=l("p"),$p=i("Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),Qo=f(),k(Ot.$$.fragment),Ko=f(),Ma=l("p"),wp=i("To load remote parquet files via HTTP, you can pass the URLs:"),Xo=f(),k(Ht.$$.fragment),Zo=f(),fs=l("h3"),Rs=l("a"),tl=l("span"),k(Ft.$$.fragment),yp=f(),al=l("span"),bp=i("Image folders"),sn=f(),za=l("p"),jp=i("\u{1F917} Datasets can also load generic image folders."),tn=f(),Ba=l("p"),xp=i("The folder structure should look like this:"),an=f(),k(Lt.$$.fragment),en=f(),R=l("p"),kp=i("To load an "),el=l("code"),Ep=i("imagefolder"),qp=i(" dataset, simply pass the root path of the image folder to the "),ll=l("code"),Pp=i("data_dir"),Ap=i(" kwarg of "),Ua=l("a"),Sp=i("datasets.load_dataset()"),Tp=i(", which is a shorthand syntax for "),ol=l("code"),Dp=i("data_files=os.path.join(data_dir, **)"),Ip=i("."),ln=f(),k(Rt.$$.fragment),on=f(),Ja=l("p"),Np=i("To load remote image folders via HTTP, you can pass the URLs:"),nn=f(),k(Vt.$$.fragment),rn=f(),V=l("p"),Cp=i("The resulting dataset will include an "),nl=l("code"),Op=i("image"),Hp=i(" feature, which is a "),rl=l("code"),Fp=i("PIL.Image"),Lp=i(" loaded from the image file, and the corresponding "),il=l("code"),Rp=i("label"),Vp=i(" inferred from the directory structure."),pn=f(),cs=l("h2"),Vs=l("a"),pl=l("span"),k(Mt.$$.fragment),Mp=f(),dl=l("span"),zp=i("In-memory data"),dn=f(),Ms=l("p"),Bp=i("\u{1F917} Datasets will also allow you to create a "),Ya=l("a"),Up=i("datasets.Dataset"),Jp=i(" directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),fn=f(),hs=l("h3"),zs=l("a"),fl=l("span"),k(zt.$$.fragment),Yp=f(),cl=l("span"),Wp=i("Python dictionary"),cn=f(),Bs=l("p"),Gp=i("Load Python dictionaries with "),Wa=l("a"),Qp=i("datasets.Dataset.from_dict()"),Kp=i(":"),hn=f(),k(Bt.$$.fragment),un=f(),us=l("h3"),Us=l("a"),hl=l("span"),k(Ut.$$.fragment),Xp=f(),ul=l("span"),Zp=i("Pandas DataFrame"),mn=f(),Js=l("p"),sd=i("Load Pandas DataFrames with "),Ga=l("a"),td=i("datasets.Dataset.from_pandas()"),ad=i(":"),gn=f(),k(Jt.$$.fragment),_n=f(),k(Ys.$$.fragment),vn=f(),ms=l("h2"),Ws=l("a"),ml=l("span"),k(Yt.$$.fragment),ed=f(),gl=l("span"),ld=i("Offline"),$n=f(),Qa=l("p"),od=i("Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),wn=f(),Q=l("p"),nd=i("If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),_l=l("code"),rd=i("HF_DATASETS_OFFLINE"),id=i(" to "),vl=l("code"),pd=i("1"),dd=i(" to enable full offline mode."),yn=f(),gs=l("h2"),Gs=l("a"),$l=l("span"),k(Wt.$$.fragment),fd=f(),wl=l("span"),cd=i("Slice splits"),bn=f(),K=l("p"),hd=i("For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),Ka=l("a"),ud=i("datasets.ReadInstruction"),md=i(". Strings are more compact and readable for simple cases, while "),Xa=l("a"),gd=i("datasets.ReadInstruction"),_d=i(" is easier to use with variable slicing parameters."),jn=f(),X=l("p"),vd=i("Concatenate the "),yl=l("code"),$d=i("train"),wd=i(" and "),bl=l("code"),yd=i("test"),bd=i(" split by:"),xn=f(),k(Gt.$$.fragment),kn=f(),Qs=l("p"),jd=i("Select specific rows of the "),jl=l("code"),xd=i("train"),kd=i(" split:"),En=f(),k(Qt.$$.fragment),qn=f(),Za=l("p"),Ed=i("Or select a percentage of the split with:"),Pn=f(),k(Kt.$$.fragment),An=f(),se=l("p"),qd=i("You can even select a combination of percentages from each split:"),Sn=f(),k(Xt.$$.fragment),Tn=f(),te=l("p"),Pd=i("Finally, create cross-validated dataset splits by:"),Dn=f(),k(Zt.$$.fragment),In=f(),_s=l("h3"),Ks=l("a"),xl=l("span"),k(sa.$$.fragment),Ad=f(),kl=l("span"),Sd=i("Percent slicing and rounding"),Nn=f(),ae=l("p"),Td=i("For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),Cn=f(),k(ta.$$.fragment),On=f(),Xs=l("p"),Dd=i("If you want equal sized splits, use "),El=l("code"),Id=i("pct1_dropremainder"),Nd=i(" rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),Hn=f(),k(aa.$$.fragment),Fn=f(),k(Zs.$$.fragment),Ln=f(),ee=l("a"),Rn=f(),vs=l("h2"),st=l("a"),ql=l("span"),k(ea.$$.fragment),Cd=f(),Pl=l("span"),Od=i("Troubleshooting"),Vn=f(),le=l("p"),Hd=i("Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),Mn=f(),$s=l("h3"),tt=l("a"),Al=l("span"),k(la.$$.fragment),Fd=f(),Sl=l("span"),Ld=i("Manual download"),zn=f(),M=l("p"),Rd=i("Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),oe=l("a"),Vd=i("datasets.load_dataset()"),Md=i(" to throw an "),Tl=l("code"),zd=i("AssertionError"),Bd=i(". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),Dl=l("code"),Ud=i("data_dir"),Jd=i(" argument to specify the path to the files you just downloaded."),Bn=f(),at=l("p"),Yd=i("For example, if you try to download a configuration from the "),oa=l("a"),Wd=i("MATINF"),Gd=i(" dataset:"),Un=f(),k(na.$$.fragment),Jn=f(),ws=l("h3"),et=l("a"),Il=l("span"),k(ra.$$.fragment),Qd=f(),Nl=l("span"),Kd=i("Specify features"),Yn=f(),Z=l("p"),Xd=i("When you create a dataset from local files, the "),ne=l("a"),Zd=i("datasets.Features"),sf=i(" are automatically inferred by "),ia=l("a"),tf=i("Apache Arrow"),af=i(". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),Wn=f(),ss=l("p"),ef=i("The following example shows how you can add custom labels with "),re=l("a"),lf=i("datasets.ClassLabel"),of=i(". First, define your own labels using the "),ie=l("a"),nf=i("datasets.Features"),rf=i(" class:"),Gn=f(),k(pa.$$.fragment),Qn=f(),ts=l("p"),pf=i("Next, specify the "),Cl=l("code"),df=i("features"),ff=i(" argument in "),pe=l("a"),cf=i("datasets.load_dataset()"),hf=i(" with the features you just created:"),Kn=f(),k(da.$$.fragment),Xn=f(),de=l("p"),uf=i("Now when you look at your dataset features, you can see it uses the custom labels you defined:"),Zn=f(),k(fa.$$.fragment),sr=f(),ys=l("h2"),lt=l("a"),Ol=l("span"),k(ca.$$.fragment),mf=f(),Hl=l("span"),gf=i("Metrics"),tr=f(),fe=l("p"),_f=i("When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),ar=f(),k(ha.$$.fragment),er=f(),k(ot.$$.fragment),lr=f(),bs=l("h3"),nt=l("a"),Fl=l("span"),k(ua.$$.fragment),vf=f(),Ll=l("span"),$f=i("Load configurations"),or=f(),ce=l("p"),wf=i("It is possible for a metric to have different configurations. The configurations are stored in the [\u2018datasets.Metric.config_name\u2019] attribute. When you load a metric, provide the configuration name as shown in the following:"),nr=f(),k(ma.$$.fragment),rr=f(),js=l("h3"),rt=l("a"),Rl=l("span"),k(ga.$$.fragment),yf=f(),Vl=l("span"),bf=i("Distributed setup"),ir=f(),he=l("p"),jf=i("When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),pr=f(),ue=l("p"),xf=i("For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),dr=f(),as=l("ol"),Ml=l("li"),_a=l("p"),kf=i("Define the total number of processes with the "),zl=l("code"),Ef=i("num_process"),qf=i(" argument."),Pf=f(),Bl=l("li"),xs=l("p"),Af=i("Set the process "),Ul=l("code"),Sf=i("rank"),Tf=i(" as an integer between zero and "),Jl=l("code"),Df=i("num_process - 1"),If=i("."),Nf=f(),Yl=l("li"),va=l("p"),Cf=i("Load your metric with "),me=l("a"),Of=i("datasets.load_metric()"),Hf=i(" with these arguments:"),fr=f(),k($a.$$.fragment),cr=f(),k(it.$$.fragment),hr=f(),pt=l("p"),Ff=i("In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),Wl=l("code"),Lf=i("experiment_id"),Rf=i(" to distinguish the separate evaluations:"),ur=f(),k(wa.$$.fragment),this.h()},l(s){const e=Vu('[data-svelte="svelte-1phssyn"]',document.head);d=o(e,"META",{name:!0,content:!0}),e.forEach(t),m=c(s),u=o(s,"H1",{class:!0});var ya=n(u);w=o(ya,"A",{id:!0,class:!0,href:!0});var Gl=n(w);$=o(Gl,"SPAN",{});var Ql=n($);q(v.$$.fragment,Ql),Ql.forEach(t),Gl.forEach(t),_=c(ya),j=o(ya,"SPAN",{});var Kl=n(j);g=p(Kl,"Load"),Kl.forEach(t),ya.forEach(t),A=c(s),S=o(s,"P",{});var Xl=n(S);T=p(Xl,"You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),Xl.forEach(t),Y=c(s),N=o(s,"P",{});var Zl=n(N);I=p(Zl,"This guide will show you how to load a dataset from:"),Zl.forEach(t),U=c(s),O=o(s,"UL",{});var es=n(O);J=o(es,"LI",{});var Bf=n(J);H=p(Bf,"The Hub without a dataset loading script"),Bf.forEach(t),ba=c(es),ks=o(es,"LI",{});var Uf=n(ks);ja=p(Uf,"Local files"),Uf.forEach(t),xa=c(es),Es=o(es,"LI",{});var Jf=n(Es);Kr=p(Jf,"In-memory data"),Jf.forEach(t),Xr=c(es),qe=o(es,"LI",{});var Yf=n(qe);Zr=p(Yf,"Offline"),Yf.forEach(t),si=c(es),Pe=o(es,"LI",{});var Wf=n(Pe);ti=p(Wf,"A specific slice of a split"),Wf.forEach(t),es.forEach(t),ao=c(s),ka=o(s,"P",{});var Gf=n(ka);ai=p(Gf,"You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),Gf.forEach(t),eo=c(s),Ea=o(s,"A",{id:!0}),n(Ea).forEach(t),lo=c(s),os=o(s,"H2",{class:!0});var gr=n(os);qs=o(gr,"A",{id:!0,class:!0,href:!0});var Qf=n(qs);Ae=o(Qf,"SPAN",{});var Kf=n(Ae);q(ct.$$.fragment,Kf),Kf.forEach(t),Qf.forEach(t),ei=c(gr),Se=o(gr,"SPAN",{});var Xf=n(Se);li=p(Xf,"Hugging Face Hub"),Xf.forEach(t),gr.forEach(t),oo=c(s),Ps=o(s,"P",{});var _r=n(Ps);oi=p(_r,"In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),Te=o(_r,"STRONG",{});var Zf=n(Te);ni=p(Zf,"without"),Zf.forEach(t),ri=p(_r," a loading script!"),_r.forEach(t),no=c(s),W=o(s,"P",{});var ge=n(W);ii=p(ge,"First, create a dataset repository and upload your data files. Then you can use "),qa=o(ge,"A",{href:!0});var sc=n(qa);pi=p(sc,"datasets.load_dataset()"),sc.forEach(t),di=p(ge," like you learned in the tutorial. For example, load the files from this "),ht=o(ge,"A",{href:!0,rel:!0});var tc=n(ht);fi=p(tc,"demo repository"),tc.forEach(t),ci=p(ge," by providing the repository namespace and dataset name:"),ge.forEach(t),ro=c(s),q(ut.$$.fragment,s),io=c(s),Pa=o(s,"P",{});var ac=n(Pa);hi=p(ac,"This dataset repository contains CSV files, and this code loads all the data from the CSV files."),ac.forEach(t),po=c(s),As=o(s,"P",{});var vr=n(As);ui=p(vr,"Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),De=o(vr,"CODE",{});var ec=n(De);mi=p(ec,"revision"),ec.forEach(t),gi=p(vr," flag to specify which dataset version you want to load:"),vr.forEach(t),fo=c(s),q(mt.$$.fragment,s),co=c(s),q(Ss.$$.fragment,s),ho=c(s),F=o(s,"P",{});var z=n(F);_i=p(z,"If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),Ie=o(z,"CODE",{});var lc=n(Ie);vi=p(lc,"train"),lc.forEach(t),$i=p(z," split. Use the "),Ne=o(z,"CODE",{});var oc=n(Ne);wi=p(oc,"data_files"),oc.forEach(t),yi=p(z," parameter to map data files to splits like "),Ce=o(z,"CODE",{});var nc=n(Ce);bi=p(nc,"train"),nc.forEach(t),ji=p(z,", "),Oe=o(z,"CODE",{});var rc=n(Oe);xi=p(rc,"validation"),rc.forEach(t),ki=p(z," and "),He=o(z,"CODE",{});var ic=n(He);Ei=p(ic,"test"),ic.forEach(t),qi=p(z,":"),z.forEach(t),uo=c(s),q(gt.$$.fragment,s),mo=c(s),q(Ts.$$.fragment,s),go=c(s),G=o(s,"P",{});var _e=n(G);Pi=p(_e,"You can also load a specific subset of the files with the "),Fe=o(_e,"CODE",{});var pc=n(Fe);Ai=p(pc,"data_files"),pc.forEach(t),Si=p(_e," parameter. The example below loads files from the "),_t=o(_e,"A",{href:!0,rel:!0});var dc=n(_t);Ti=p(dc,"C4 dataset"),dc.forEach(t),Di=p(_e,":"),_e.forEach(t),_o=c(s),q(vt.$$.fragment,s),vo=c(s),Ds=o(s,"P",{});var $r=n(Ds);Ii=p($r,"Specify a custom split with the "),Le=o($r,"CODE",{});var fc=n(Le);Ni=p(fc,"split"),fc.forEach(t),Ci=p($r," parameter:"),$r.forEach(t),$o=c(s),q($t.$$.fragment,s),wo=c(s),ns=o(s,"H2",{class:!0});var wr=n(ns);Is=o(wr,"A",{id:!0,class:!0,href:!0});var cc=n(Is);Re=o(cc,"SPAN",{});var hc=n(Re);q(wt.$$.fragment,hc),hc.forEach(t),cc.forEach(t),Oi=c(wr),Ve=o(wr,"SPAN",{});var uc=n(Ve);Hi=p(uc,"Local and remote files"),uc.forEach(t),wr.forEach(t),yo=c(s),L=o(s,"P",{});var B=n(L);Fi=p(B,"\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Me=o(B,"CODE",{});var mc=n(Me);Li=p(mc,"csv"),mc.forEach(t),Ri=p(B,", "),ze=o(B,"CODE",{});var gc=n(ze);Vi=p(gc,"json"),gc.forEach(t),Mi=p(B,", "),Be=o(B,"CODE",{});var _c=n(Be);zi=p(_c,"txt"),_c.forEach(t),Bi=p(B," or "),Ue=o(B,"CODE",{});var vc=n(Ue);Ui=p(vc,"parquet"),vc.forEach(t),Ji=p(B," file. The "),Aa=o(B,"A",{href:!0});var $c=n(Aa);Yi=p($c,"datasets.load_dataset()"),$c.forEach(t),Wi=p(B," method is able to load each of these file types."),B.forEach(t),bo=c(s),rs=o(s,"H3",{class:!0});var yr=n(rs);Ns=o(yr,"A",{id:!0,class:!0,href:!0});var wc=n(Ns);Je=o(wc,"SPAN",{});var yc=n(Je);q(yt.$$.fragment,yc),yc.forEach(t),wc.forEach(t),Gi=c(yr),Ye=o(yr,"SPAN",{});var bc=n(Ye);Qi=p(bc,"CSV"),bc.forEach(t),yr.forEach(t),jo=c(s),Sa=o(s,"P",{});var jc=n(Sa);Ki=p(jc,"\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),jc.forEach(t),xo=c(s),q(bt.$$.fragment,s),ko=c(s),Ta=o(s,"P",{});var xc=n(Ta);Xi=p(xc,"If you have more than one CSV file:"),xc.forEach(t),Eo=c(s),q(jt.$$.fragment,s),qo=c(s),Da=o(s,"P",{});var kc=n(Da);Zi=p(kc,"You can also map the training and test splits to specific CSV files:"),kc.forEach(t),Po=c(s),q(xt.$$.fragment,s),Ao=c(s),Ia=o(s,"P",{});var Ec=n(Ia);sp=p(Ec,"To load remote CSV files via HTTP, you can pass the URLs:"),Ec.forEach(t),So=c(s),q(kt.$$.fragment,s),To=c(s),Na=o(s,"P",{});var qc=n(Na);tp=p(qc,"To load zipped CSV files:"),qc.forEach(t),Do=c(s),q(Et.$$.fragment,s),Io=c(s),is=o(s,"H3",{class:!0});var br=n(is);Cs=o(br,"A",{id:!0,class:!0,href:!0});var Pc=n(Cs);We=o(Pc,"SPAN",{});var Ac=n(We);q(qt.$$.fragment,Ac),Ac.forEach(t),Pc.forEach(t),ap=c(br),Ge=o(br,"SPAN",{});var Sc=n(Ge);ep=p(Sc,"JSON"),Sc.forEach(t),br.forEach(t),No=c(s),Os=o(s,"P",{});var jr=n(Os);lp=p(jr,"JSON files are loaded directly with "),Ca=o(jr,"A",{href:!0});var Tc=n(Ca);op=p(Tc,"datasets.load_dataset()"),Tc.forEach(t),np=p(jr," as shown below:"),jr.forEach(t),Co=c(s),q(Pt.$$.fragment,s),Oo=c(s),Oa=o(s,"P",{});var Dc=n(Oa);rp=p(Dc,"JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),Dc.forEach(t),Ho=c(s),q(At.$$.fragment,s),Fo=c(s),Hs=o(s,"P",{});var xr=n(Hs);ip=p(xr,"Another JSON format you may encounter is a nested field, in which case you will need to specify the "),Qe=o(xr,"CODE",{});var Ic=n(Qe);pp=p(Ic,"field"),Ic.forEach(t),dp=p(xr," argument as shown in the following:"),xr.forEach(t),Lo=c(s),q(St.$$.fragment,s),Ro=c(s),Ha=o(s,"P",{});var Nc=n(Ha);fp=p(Nc,"To load remote JSON files via HTTP, you can pass the URLs:"),Nc.forEach(t),Vo=c(s),q(Tt.$$.fragment,s),Mo=c(s),Fa=o(s,"P",{});var Cc=n(Fa);cp=p(Cc,"While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),Cc.forEach(t),zo=c(s),ps=o(s,"H3",{class:!0});var kr=n(ps);Fs=o(kr,"A",{id:!0,class:!0,href:!0});var Oc=n(Fs);Ke=o(Oc,"SPAN",{});var Hc=n(Ke);q(Dt.$$.fragment,Hc),Hc.forEach(t),Oc.forEach(t),hp=c(kr),Xe=o(kr,"SPAN",{});var Fc=n(Xe);up=p(Fc,"Text files"),Fc.forEach(t),kr.forEach(t),Bo=c(s),La=o(s,"P",{});var Lc=n(La);mp=p(Lc,"Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),Lc.forEach(t),Uo=c(s),q(It.$$.fragment,s),Jo=c(s),Ra=o(s,"P",{});var Rc=n(Ra);gp=p(Rc,"To load remote TXT files via HTTP, you can pass the URLs:"),Rc.forEach(t),Yo=c(s),q(Nt.$$.fragment,s),Wo=c(s),ds=o(s,"H3",{class:!0});var Er=n(ds);Ls=o(Er,"A",{id:!0,class:!0,href:!0});var Vc=n(Ls);Ze=o(Vc,"SPAN",{});var Mc=n(Ze);q(Ct.$$.fragment,Mc),Mc.forEach(t),Vc.forEach(t),_p=c(Er),sl=o(Er,"SPAN",{});var zc=n(sl);vp=p(zc,"Parquet"),zc.forEach(t),Er.forEach(t),Go=c(s),Va=o(s,"P",{});var Bc=n(Va);$p=p(Bc,"Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),Bc.forEach(t),Qo=c(s),q(Ot.$$.fragment,s),Ko=c(s),Ma=o(s,"P",{});var Uc=n(Ma);wp=p(Uc,"To load remote parquet files via HTTP, you can pass the URLs:"),Uc.forEach(t),Xo=c(s),q(Ht.$$.fragment,s),Zo=c(s),fs=o(s,"H3",{class:!0});var qr=n(fs);Rs=o(qr,"A",{id:!0,class:!0,href:!0});var Jc=n(Rs);tl=o(Jc,"SPAN",{});var Yc=n(tl);q(Ft.$$.fragment,Yc),Yc.forEach(t),Jc.forEach(t),yp=c(qr),al=o(qr,"SPAN",{});var Wc=n(al);bp=p(Wc,"Image folders"),Wc.forEach(t),qr.forEach(t),sn=c(s),za=o(s,"P",{});var Gc=n(za);jp=p(Gc,"\u{1F917} Datasets can also load generic image folders."),Gc.forEach(t),tn=c(s),Ba=o(s,"P",{});var Qc=n(Ba);xp=p(Qc,"The folder structure should look like this:"),Qc.forEach(t),an=c(s),q(Lt.$$.fragment,s),en=c(s),R=o(s,"P",{});var ls=n(R);kp=p(ls,"To load an "),el=o(ls,"CODE",{});var Kc=n(el);Ep=p(Kc,"imagefolder"),Kc.forEach(t),qp=p(ls," dataset, simply pass the root path of the image folder to the "),ll=o(ls,"CODE",{});var Xc=n(ll);Pp=p(Xc,"data_dir"),Xc.forEach(t),Ap=p(ls," kwarg of "),Ua=o(ls,"A",{href:!0});var Zc=n(Ua);Sp=p(Zc,"datasets.load_dataset()"),Zc.forEach(t),Tp=p(ls,", which is a shorthand syntax for "),ol=o(ls,"CODE",{});var sh=n(ol);Dp=p(sh,"data_files=os.path.join(data_dir, **)"),sh.forEach(t),Ip=p(ls,"."),ls.forEach(t),ln=c(s),q(Rt.$$.fragment,s),on=c(s),Ja=o(s,"P",{});var th=n(Ja);Np=p(th,"To load remote image folders via HTTP, you can pass the URLs:"),th.forEach(t),nn=c(s),q(Vt.$$.fragment,s),rn=c(s),V=o(s,"P",{});var dt=n(V);Cp=p(dt,"The resulting dataset will include an "),nl=o(dt,"CODE",{});var ah=n(nl);Op=p(ah,"image"),ah.forEach(t),Hp=p(dt," feature, which is a "),rl=o(dt,"CODE",{});var eh=n(rl);Fp=p(eh,"PIL.Image"),eh.forEach(t),Lp=p(dt," loaded from the image file, and the corresponding "),il=o(dt,"CODE",{});var lh=n(il);Rp=p(lh,"label"),lh.forEach(t),Vp=p(dt," inferred from the directory structure."),dt.forEach(t),pn=c(s),cs=o(s,"H2",{class:!0});var Pr=n(cs);Vs=o(Pr,"A",{id:!0,class:!0,href:!0});var oh=n(Vs);pl=o(oh,"SPAN",{});var nh=n(pl);q(Mt.$$.fragment,nh),nh.forEach(t),oh.forEach(t),Mp=c(Pr),dl=o(Pr,"SPAN",{});var rh=n(dl);zp=p(rh,"In-memory data"),rh.forEach(t),Pr.forEach(t),dn=c(s),Ms=o(s,"P",{});var Ar=n(Ms);Bp=p(Ar,"\u{1F917} Datasets will also allow you to create a "),Ya=o(Ar,"A",{href:!0});var ih=n(Ya);Up=p(ih,"datasets.Dataset"),ih.forEach(t),Jp=p(Ar," directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),Ar.forEach(t),fn=c(s),hs=o(s,"H3",{class:!0});var Sr=n(hs);zs=o(Sr,"A",{id:!0,class:!0,href:!0});var ph=n(zs);fl=o(ph,"SPAN",{});var dh=n(fl);q(zt.$$.fragment,dh),dh.forEach(t),ph.forEach(t),Yp=c(Sr),cl=o(Sr,"SPAN",{});var fh=n(cl);Wp=p(fh,"Python dictionary"),fh.forEach(t),Sr.forEach(t),cn=c(s),Bs=o(s,"P",{});var Tr=n(Bs);Gp=p(Tr,"Load Python dictionaries with "),Wa=o(Tr,"A",{href:!0});var ch=n(Wa);Qp=p(ch,"datasets.Dataset.from_dict()"),ch.forEach(t),Kp=p(Tr,":"),Tr.forEach(t),hn=c(s),q(Bt.$$.fragment,s),un=c(s),us=o(s,"H3",{class:!0});var Dr=n(us);Us=o(Dr,"A",{id:!0,class:!0,href:!0});var hh=n(Us);hl=o(hh,"SPAN",{});var uh=n(hl);q(Ut.$$.fragment,uh),uh.forEach(t),hh.forEach(t),Xp=c(Dr),ul=o(Dr,"SPAN",{});var mh=n(ul);Zp=p(mh,"Pandas DataFrame"),mh.forEach(t),Dr.forEach(t),mn=c(s),Js=o(s,"P",{});var Ir=n(Js);sd=p(Ir,"Load Pandas DataFrames with "),Ga=o(Ir,"A",{href:!0});var gh=n(Ga);td=p(gh,"datasets.Dataset.from_pandas()"),gh.forEach(t),ad=p(Ir,":"),Ir.forEach(t),gn=c(s),q(Jt.$$.fragment,s),_n=c(s),q(Ys.$$.fragment,s),vn=c(s),ms=o(s,"H2",{class:!0});var Nr=n(ms);Ws=o(Nr,"A",{id:!0,class:!0,href:!0});var _h=n(Ws);ml=o(_h,"SPAN",{});var vh=n(ml);q(Yt.$$.fragment,vh),vh.forEach(t),_h.forEach(t),ed=c(Nr),gl=o(Nr,"SPAN",{});var $h=n(gl);ld=p($h,"Offline"),$h.forEach(t),Nr.forEach(t),$n=c(s),Qa=o(s,"P",{});var wh=n(Qa);od=p(wh,"Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),wh.forEach(t),wn=c(s),Q=o(s,"P",{});var ve=n(Q);nd=p(ve,"If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),_l=o(ve,"CODE",{});var yh=n(_l);rd=p(yh,"HF_DATASETS_OFFLINE"),yh.forEach(t),id=p(ve," to "),vl=o(ve,"CODE",{});var bh=n(vl);pd=p(bh,"1"),bh.forEach(t),dd=p(ve," to enable full offline mode."),ve.forEach(t),yn=c(s),gs=o(s,"H2",{class:!0});var Cr=n(gs);Gs=o(Cr,"A",{id:!0,class:!0,href:!0});var jh=n(Gs);$l=o(jh,"SPAN",{});var xh=n($l);q(Wt.$$.fragment,xh),xh.forEach(t),jh.forEach(t),fd=c(Cr),wl=o(Cr,"SPAN",{});var kh=n(wl);cd=p(kh,"Slice splits"),kh.forEach(t),Cr.forEach(t),bn=c(s),K=o(s,"P",{});var $e=n(K);hd=p($e,"For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),Ka=o($e,"A",{href:!0});var Eh=n(Ka);ud=p(Eh,"datasets.ReadInstruction"),Eh.forEach(t),md=p($e,". Strings are more compact and readable for simple cases, while "),Xa=o($e,"A",{href:!0});var qh=n(Xa);gd=p(qh,"datasets.ReadInstruction"),qh.forEach(t),_d=p($e," is easier to use with variable slicing parameters."),$e.forEach(t),jn=c(s),X=o(s,"P",{});var we=n(X);vd=p(we,"Concatenate the "),yl=o(we,"CODE",{});var Ph=n(yl);$d=p(Ph,"train"),Ph.forEach(t),wd=p(we," and "),bl=o(we,"CODE",{});var Ah=n(bl);yd=p(Ah,"test"),Ah.forEach(t),bd=p(we," split by:"),we.forEach(t),xn=c(s),q(Gt.$$.fragment,s),kn=c(s),Qs=o(s,"P",{});var Or=n(Qs);jd=p(Or,"Select specific rows of the "),jl=o(Or,"CODE",{});var Sh=n(jl);xd=p(Sh,"train"),Sh.forEach(t),kd=p(Or," split:"),Or.forEach(t),En=c(s),q(Qt.$$.fragment,s),qn=c(s),Za=o(s,"P",{});var Th=n(Za);Ed=p(Th,"Or select a percentage of the split with:"),Th.forEach(t),Pn=c(s),q(Kt.$$.fragment,s),An=c(s),se=o(s,"P",{});var Dh=n(se);qd=p(Dh,"You can even select a combination of percentages from each split:"),Dh.forEach(t),Sn=c(s),q(Xt.$$.fragment,s),Tn=c(s),te=o(s,"P",{});var Ih=n(te);Pd=p(Ih,"Finally, create cross-validated dataset splits by:"),Ih.forEach(t),Dn=c(s),q(Zt.$$.fragment,s),In=c(s),_s=o(s,"H3",{class:!0});var Hr=n(_s);Ks=o(Hr,"A",{id:!0,class:!0,href:!0});var Nh=n(Ks);xl=o(Nh,"SPAN",{});var Ch=n(xl);q(sa.$$.fragment,Ch),Ch.forEach(t),Nh.forEach(t),Ad=c(Hr),kl=o(Hr,"SPAN",{});var Oh=n(kl);Sd=p(Oh,"Percent slicing and rounding"),Oh.forEach(t),Hr.forEach(t),Nn=c(s),ae=o(s,"P",{});var Hh=n(ae);Td=p(Hh,"For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),Hh.forEach(t),Cn=c(s),q(ta.$$.fragment,s),On=c(s),Xs=o(s,"P",{});var Fr=n(Xs);Dd=p(Fr,"If you want equal sized splits, use "),El=o(Fr,"CODE",{});var Fh=n(El);Id=p(Fh,"pct1_dropremainder"),Fh.forEach(t),Nd=p(Fr," rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),Fr.forEach(t),Hn=c(s),q(aa.$$.fragment,s),Fn=c(s),q(Zs.$$.fragment,s),Ln=c(s),ee=o(s,"A",{id:!0}),n(ee).forEach(t),Rn=c(s),vs=o(s,"H2",{class:!0});var Lr=n(vs);st=o(Lr,"A",{id:!0,class:!0,href:!0});var Lh=n(st);ql=o(Lh,"SPAN",{});var Rh=n(ql);q(ea.$$.fragment,Rh),Rh.forEach(t),Lh.forEach(t),Cd=c(Lr),Pl=o(Lr,"SPAN",{});var Vh=n(Pl);Od=p(Vh,"Troubleshooting"),Vh.forEach(t),Lr.forEach(t),Vn=c(s),le=o(s,"P",{});var Mh=n(le);Hd=p(Mh,"Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),Mh.forEach(t),Mn=c(s),$s=o(s,"H3",{class:!0});var Rr=n($s);tt=o(Rr,"A",{id:!0,class:!0,href:!0});var zh=n(tt);Al=o(zh,"SPAN",{});var Bh=n(Al);q(la.$$.fragment,Bh),Bh.forEach(t),zh.forEach(t),Fd=c(Rr),Sl=o(Rr,"SPAN",{});var Uh=n(Sl);Ld=p(Uh,"Manual download"),Uh.forEach(t),Rr.forEach(t),zn=c(s),M=o(s,"P",{});var ft=n(M);Rd=p(ft,"Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),oe=o(ft,"A",{href:!0});var Jh=n(oe);Vd=p(Jh,"datasets.load_dataset()"),Jh.forEach(t),Md=p(ft," to throw an "),Tl=o(ft,"CODE",{});var Yh=n(Tl);zd=p(Yh,"AssertionError"),Yh.forEach(t),Bd=p(ft,". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),Dl=o(ft,"CODE",{});var Wh=n(Dl);Ud=p(Wh,"data_dir"),Wh.forEach(t),Jd=p(ft," argument to specify the path to the files you just downloaded."),ft.forEach(t),Bn=c(s),at=o(s,"P",{});var Vr=n(at);Yd=p(Vr,"For example, if you try to download a configuration from the "),oa=o(Vr,"A",{href:!0,rel:!0});var Gh=n(oa);Wd=p(Gh,"MATINF"),Gh.forEach(t),Gd=p(Vr," dataset:"),Vr.forEach(t),Un=c(s),q(na.$$.fragment,s),Jn=c(s),ws=o(s,"H3",{class:!0});var Mr=n(ws);et=o(Mr,"A",{id:!0,class:!0,href:!0});var Qh=n(et);Il=o(Qh,"SPAN",{});var Kh=n(Il);q(ra.$$.fragment,Kh),Kh.forEach(t),Qh.forEach(t),Qd=c(Mr),Nl=o(Mr,"SPAN",{});var Xh=n(Nl);Kd=p(Xh,"Specify features"),Xh.forEach(t),Mr.forEach(t),Yn=c(s),Z=o(s,"P",{});var ye=n(Z);Xd=p(ye,"When you create a dataset from local files, the "),ne=o(ye,"A",{href:!0});var Zh=n(ne);Zd=p(Zh,"datasets.Features"),Zh.forEach(t),sf=p(ye," are automatically inferred by "),ia=o(ye,"A",{href:!0,rel:!0});var su=n(ia);tf=p(su,"Apache Arrow"),su.forEach(t),af=p(ye,". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),ye.forEach(t),Wn=c(s),ss=o(s,"P",{});var be=n(ss);ef=p(be,"The following example shows how you can add custom labels with "),re=o(be,"A",{href:!0});var tu=n(re);lf=p(tu,"datasets.ClassLabel"),tu.forEach(t),of=p(be,". First, define your own labels using the "),ie=o(be,"A",{href:!0});var au=n(ie);nf=p(au,"datasets.Features"),au.forEach(t),rf=p(be," class:"),be.forEach(t),Gn=c(s),q(pa.$$.fragment,s),Qn=c(s),ts=o(s,"P",{});var je=n(ts);pf=p(je,"Next, specify the "),Cl=o(je,"CODE",{});var eu=n(Cl);df=p(eu,"features"),eu.forEach(t),ff=p(je," argument in "),pe=o(je,"A",{href:!0});var lu=n(pe);cf=p(lu,"datasets.load_dataset()"),lu.forEach(t),hf=p(je," with the features you just created:"),je.forEach(t),Kn=c(s),q(da.$$.fragment,s),Xn=c(s),de=o(s,"P",{});var ou=n(de);uf=p(ou,"Now when you look at your dataset features, you can see it uses the custom labels you defined:"),ou.forEach(t),Zn=c(s),q(fa.$$.fragment,s),sr=c(s),ys=o(s,"H2",{class:!0});var zr=n(ys);lt=o(zr,"A",{id:!0,class:!0,href:!0});var nu=n(lt);Ol=o(nu,"SPAN",{});var ru=n(Ol);q(ca.$$.fragment,ru),ru.forEach(t),nu.forEach(t),mf=c(zr),Hl=o(zr,"SPAN",{});var iu=n(Hl);gf=p(iu,"Metrics"),iu.forEach(t),zr.forEach(t),tr=c(s),fe=o(s,"P",{});var pu=n(fe);_f=p(pu,"When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),pu.forEach(t),ar=c(s),q(ha.$$.fragment,s),er=c(s),q(ot.$$.fragment,s),lr=c(s),bs=o(s,"H3",{class:!0});var Br=n(bs);nt=o(Br,"A",{id:!0,class:!0,href:!0});var du=n(nt);Fl=o(du,"SPAN",{});var fu=n(Fl);q(ua.$$.fragment,fu),fu.forEach(t),du.forEach(t),vf=c(Br),Ll=o(Br,"SPAN",{});var cu=n(Ll);$f=p(cu,"Load configurations"),cu.forEach(t),Br.forEach(t),or=c(s),ce=o(s,"P",{});var hu=n(ce);wf=p(hu,"It is possible for a metric to have different configurations. The configurations are stored in the [\u2018datasets.Metric.config_name\u2019] attribute. When you load a metric, provide the configuration name as shown in the following:"),hu.forEach(t),nr=c(s),q(ma.$$.fragment,s),rr=c(s),js=o(s,"H3",{class:!0});var Ur=n(js);rt=o(Ur,"A",{id:!0,class:!0,href:!0});var uu=n(rt);Rl=o(uu,"SPAN",{});var mu=n(Rl);q(ga.$$.fragment,mu),mu.forEach(t),uu.forEach(t),yf=c(Ur),Vl=o(Ur,"SPAN",{});var gu=n(Vl);bf=p(gu,"Distributed setup"),gu.forEach(t),Ur.forEach(t),ir=c(s),he=o(s,"P",{});var _u=n(he);jf=p(_u,"When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),_u.forEach(t),pr=c(s),ue=o(s,"P",{});var vu=n(ue);xf=p(vu,"For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),vu.forEach(t),dr=c(s),as=o(s,"OL",{});var xe=n(as);Ml=o(xe,"LI",{});var $u=n(Ml);_a=o($u,"P",{});var Jr=n(_a);kf=p(Jr,"Define the total number of processes with the "),zl=o(Jr,"CODE",{});var wu=n(zl);Ef=p(wu,"num_process"),wu.forEach(t),qf=p(Jr," argument."),Jr.forEach(t),$u.forEach(t),Pf=c(xe),Bl=o(xe,"LI",{});var yu=n(Bl);xs=o(yu,"P",{});var ke=n(xs);Af=p(ke,"Set the process "),Ul=o(ke,"CODE",{});var bu=n(Ul);Sf=p(bu,"rank"),bu.forEach(t),Tf=p(ke," as an integer between zero and "),Jl=o(ke,"CODE",{});var ju=n(Jl);Df=p(ju,"num_process - 1"),ju.forEach(t),If=p(ke,"."),ke.forEach(t),yu.forEach(t),Nf=c(xe),Yl=o(xe,"LI",{});var xu=n(Yl);va=o(xu,"P",{});var Yr=n(va);Cf=p(Yr,"Load your metric with "),me=o(Yr,"A",{href:!0});var ku=n(me);Of=p(ku,"datasets.load_metric()"),ku.forEach(t),Hf=p(Yr," with these arguments:"),Yr.forEach(t),xu.forEach(t),xe.forEach(t),fr=c(s),q($a.$$.fragment,s),cr=c(s),q(it.$$.fragment,s),hr=c(s),pt=o(s,"P",{});var Wr=n(pt);Ff=p(Wr,"In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),Wl=o(Wr,"CODE",{});var Eu=n(Wl);Lf=p(Eu,"experiment_id"),Eu.forEach(t),Rf=p(Wr," to distinguish the separate evaluations:"),Wr.forEach(t),ur=c(s),q(wa.$$.fragment,s),this.h()},h(){h(d,"name","hf:doc:metadata"),h(d,"content",JSON.stringify(em)),h(w,"id","load"),h(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(w,"href","#load"),h(u,"class","relative group"),h(Ea,"id","load-from-the-hub"),h(qs,"id","hugging-face-hub"),h(qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(qs,"href","#hugging-face-hub"),h(os,"class","relative group"),h(qa,"href","/docs/datasets/master/en/package_reference/loading_methods#datasets.load_dataset"),h(ht,"href","https://huggingface.co/datasets/lhoestq/demo1"),h(ht,"rel","nofollow"),h(_t,"href","https://huggingface.co/datasets/allenai/c4"),h(_t,"rel","nofollow"),h(Is,"id","local-and-remote-files"),h(Is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Is,"href","#local-and-remote-files"),h(ns,"class","relative group"),h(Aa,"href","/docs/datasets/master/en/package_reference/loading_methods#datasets.load_dataset"),h(Ns,"id","csv"),h(Ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ns,"href","#csv"),h(rs,"class","relative group"),h(Cs,"id","json"),h(Cs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Cs,"href","#json"),h(is,"class","relative group"),h(Ca,"href","/docs/datasets/master/en/package_reference/loading_methods#datasets.load_dataset"),h(Fs,"id","text-files"),h(Fs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Fs,"href","#text-files"),h(ps,"class","relative group"),h(Ls,"id","parquet"),h(Ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ls,"href","#parquet"),h(ds,"class","relative group"),h(Rs,"id","image-folders"),h(Rs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Rs,"href","#image-folders"),h(fs,"class","relative group"),h(Ua,"href","/docs/datasets/master/en/package_reference/loading_methods#datasets.load_dataset"),h(Vs,"id","inmemory-data"),h(Vs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Vs,"href","#inmemory-data"),h(cs,"class","relative group"),h(Ya,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.Dataset"),h(zs,"id","python-dictionary"),h(zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(zs,"href","#python-dictionary"),h(hs,"class","relative group"),h(Wa,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.Dataset.from_dict"),h(Us,"id","pandas-dataframe"),h(Us,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Us,"href","#pandas-dataframe"),h(us,"class","relative group"),h(Ga,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.Dataset.from_pandas"),h(Ws,"id","offline"),h(Ws,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ws,"href","#offline"),h(ms,"class","relative group"),h(Gs,"id","slice-splits"),h(Gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Gs,"href","#slice-splits"),h(gs,"class","relative group"),h(Ka,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.ReadInstruction"),h(Xa,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.ReadInstruction"),h(Ks,"id","percent-slicing-and-rounding"),h(Ks,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ks,"href","#percent-slicing-and-rounding"),h(_s,"class","relative group"),h(ee,"id","troubleshoot"),h(st,"id","troubleshooting"),h(st,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(st,"href","#troubleshooting"),h(vs,"class","relative group"),h(tt,"id","manual-download"),h(tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(tt,"href","#manual-download"),h($s,"class","relative group"),h(oe,"href","/docs/datasets/master/en/package_reference/loading_methods#datasets.load_dataset"),h(oa,"href","https://huggingface.co/datasets/matinf"),h(oa,"rel","nofollow"),h(et,"id","specify-features"),h(et,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(et,"href","#specify-features"),h(ws,"class","relative group"),h(ne,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.Features"),h(ia,"href","https://arrow.apache.org/docs/"),h(ia,"rel","nofollow"),h(re,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.ClassLabel"),h(ie,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.Features"),h(pe,"href","/docs/datasets/master/en/package_reference/loading_methods#datasets.load_dataset"),h(lt,"id","metrics"),h(lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(lt,"href","#metrics"),h(ys,"class","relative group"),h(nt,"id","load-configurations"),h(nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(nt,"href","#load-configurations"),h(bs,"class","relative group"),h(rt,"id","distributed-setup"),h(rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(rt,"href","#distributed-setup"),h(js,"class","relative group"),h(me,"href","/docs/datasets/master/en/package_reference/loading_methods#datasets.load_metric")},m(s,e){a(document.head,d),r(s,m,e),r(s,u,e),a(u,w),a(w,$),E(v,$,null),a(u,_),a(u,j),a(j,g),r(s,A,e),r(s,S,e),a(S,T),r(s,Y,e),r(s,N,e),a(N,I),r(s,U,e),r(s,O,e),a(O,J),a(J,H),a(O,ba),a(O,ks),a(ks,ja),a(O,xa),a(O,Es),a(Es,Kr),a(O,Xr),a(O,qe),a(qe,Zr),a(O,si),a(O,Pe),a(Pe,ti),r(s,ao,e),r(s,ka,e),a(ka,ai),r(s,eo,e),r(s,Ea,e),r(s,lo,e),r(s,os,e),a(os,qs),a(qs,Ae),E(ct,Ae,null),a(os,ei),a(os,Se),a(Se,li),r(s,oo,e),r(s,Ps,e),a(Ps,oi),a(Ps,Te),a(Te,ni),a(Ps,ri),r(s,no,e),r(s,W,e),a(W,ii),a(W,qa),a(qa,pi),a(W,di),a(W,ht),a(ht,fi),a(W,ci),r(s,ro,e),E(ut,s,e),r(s,io,e),r(s,Pa,e),a(Pa,hi),r(s,po,e),r(s,As,e),a(As,ui),a(As,De),a(De,mi),a(As,gi),r(s,fo,e),E(mt,s,e),r(s,co,e),E(Ss,s,e),r(s,ho,e),r(s,F,e),a(F,_i),a(F,Ie),a(Ie,vi),a(F,$i),a(F,Ne),a(Ne,wi),a(F,yi),a(F,Ce),a(Ce,bi),a(F,ji),a(F,Oe),a(Oe,xi),a(F,ki),a(F,He),a(He,Ei),a(F,qi),r(s,uo,e),E(gt,s,e),r(s,mo,e),E(Ts,s,e),r(s,go,e),r(s,G,e),a(G,Pi),a(G,Fe),a(Fe,Ai),a(G,Si),a(G,_t),a(_t,Ti),a(G,Di),r(s,_o,e),E(vt,s,e),r(s,vo,e),r(s,Ds,e),a(Ds,Ii),a(Ds,Le),a(Le,Ni),a(Ds,Ci),r(s,$o,e),E($t,s,e),r(s,wo,e),r(s,ns,e),a(ns,Is),a(Is,Re),E(wt,Re,null),a(ns,Oi),a(ns,Ve),a(Ve,Hi),r(s,yo,e),r(s,L,e),a(L,Fi),a(L,Me),a(Me,Li),a(L,Ri),a(L,ze),a(ze,Vi),a(L,Mi),a(L,Be),a(Be,zi),a(L,Bi),a(L,Ue),a(Ue,Ui),a(L,Ji),a(L,Aa),a(Aa,Yi),a(L,Wi),r(s,bo,e),r(s,rs,e),a(rs,Ns),a(Ns,Je),E(yt,Je,null),a(rs,Gi),a(rs,Ye),a(Ye,Qi),r(s,jo,e),r(s,Sa,e),a(Sa,Ki),r(s,xo,e),E(bt,s,e),r(s,ko,e),r(s,Ta,e),a(Ta,Xi),r(s,Eo,e),E(jt,s,e),r(s,qo,e),r(s,Da,e),a(Da,Zi),r(s,Po,e),E(xt,s,e),r(s,Ao,e),r(s,Ia,e),a(Ia,sp),r(s,So,e),E(kt,s,e),r(s,To,e),r(s,Na,e),a(Na,tp),r(s,Do,e),E(Et,s,e),r(s,Io,e),r(s,is,e),a(is,Cs),a(Cs,We),E(qt,We,null),a(is,ap),a(is,Ge),a(Ge,ep),r(s,No,e),r(s,Os,e),a(Os,lp),a(Os,Ca),a(Ca,op),a(Os,np),r(s,Co,e),E(Pt,s,e),r(s,Oo,e),r(s,Oa,e),a(Oa,rp),r(s,Ho,e),E(At,s,e),r(s,Fo,e),r(s,Hs,e),a(Hs,ip),a(Hs,Qe),a(Qe,pp),a(Hs,dp),r(s,Lo,e),E(St,s,e),r(s,Ro,e),r(s,Ha,e),a(Ha,fp),r(s,Vo,e),E(Tt,s,e),r(s,Mo,e),r(s,Fa,e),a(Fa,cp),r(s,zo,e),r(s,ps,e),a(ps,Fs),a(Fs,Ke),E(Dt,Ke,null),a(ps,hp),a(ps,Xe),a(Xe,up),r(s,Bo,e),r(s,La,e),a(La,mp),r(s,Uo,e),E(It,s,e),r(s,Jo,e),r(s,Ra,e),a(Ra,gp),r(s,Yo,e),E(Nt,s,e),r(s,Wo,e),r(s,ds,e),a(ds,Ls),a(Ls,Ze),E(Ct,Ze,null),a(ds,_p),a(ds,sl),a(sl,vp),r(s,Go,e),r(s,Va,e),a(Va,$p),r(s,Qo,e),E(Ot,s,e),r(s,Ko,e),r(s,Ma,e),a(Ma,wp),r(s,Xo,e),E(Ht,s,e),r(s,Zo,e),r(s,fs,e),a(fs,Rs),a(Rs,tl),E(Ft,tl,null),a(fs,yp),a(fs,al),a(al,bp),r(s,sn,e),r(s,za,e),a(za,jp),r(s,tn,e),r(s,Ba,e),a(Ba,xp),r(s,an,e),E(Lt,s,e),r(s,en,e),r(s,R,e),a(R,kp),a(R,el),a(el,Ep),a(R,qp),a(R,ll),a(ll,Pp),a(R,Ap),a(R,Ua),a(Ua,Sp),a(R,Tp),a(R,ol),a(ol,Dp),a(R,Ip),r(s,ln,e),E(Rt,s,e),r(s,on,e),r(s,Ja,e),a(Ja,Np),r(s,nn,e),E(Vt,s,e),r(s,rn,e),r(s,V,e),a(V,Cp),a(V,nl),a(nl,Op),a(V,Hp),a(V,rl),a(rl,Fp),a(V,Lp),a(V,il),a(il,Rp),a(V,Vp),r(s,pn,e),r(s,cs,e),a(cs,Vs),a(Vs,pl),E(Mt,pl,null),a(cs,Mp),a(cs,dl),a(dl,zp),r(s,dn,e),r(s,Ms,e),a(Ms,Bp),a(Ms,Ya),a(Ya,Up),a(Ms,Jp),r(s,fn,e),r(s,hs,e),a(hs,zs),a(zs,fl),E(zt,fl,null),a(hs,Yp),a(hs,cl),a(cl,Wp),r(s,cn,e),r(s,Bs,e),a(Bs,Gp),a(Bs,Wa),a(Wa,Qp),a(Bs,Kp),r(s,hn,e),E(Bt,s,e),r(s,un,e),r(s,us,e),a(us,Us),a(Us,hl),E(Ut,hl,null),a(us,Xp),a(us,ul),a(ul,Zp),r(s,mn,e),r(s,Js,e),a(Js,sd),a(Js,Ga),a(Ga,td),a(Js,ad),r(s,gn,e),E(Jt,s,e),r(s,_n,e),E(Ys,s,e),r(s,vn,e),r(s,ms,e),a(ms,Ws),a(Ws,ml),E(Yt,ml,null),a(ms,ed),a(ms,gl),a(gl,ld),r(s,$n,e),r(s,Qa,e),a(Qa,od),r(s,wn,e),r(s,Q,e),a(Q,nd),a(Q,_l),a(_l,rd),a(Q,id),a(Q,vl),a(vl,pd),a(Q,dd),r(s,yn,e),r(s,gs,e),a(gs,Gs),a(Gs,$l),E(Wt,$l,null),a(gs,fd),a(gs,wl),a(wl,cd),r(s,bn,e),r(s,K,e),a(K,hd),a(K,Ka),a(Ka,ud),a(K,md),a(K,Xa),a(Xa,gd),a(K,_d),r(s,jn,e),r(s,X,e),a(X,vd),a(X,yl),a(yl,$d),a(X,wd),a(X,bl),a(bl,yd),a(X,bd),r(s,xn,e),E(Gt,s,e),r(s,kn,e),r(s,Qs,e),a(Qs,jd),a(Qs,jl),a(jl,xd),a(Qs,kd),r(s,En,e),E(Qt,s,e),r(s,qn,e),r(s,Za,e),a(Za,Ed),r(s,Pn,e),E(Kt,s,e),r(s,An,e),r(s,se,e),a(se,qd),r(s,Sn,e),E(Xt,s,e),r(s,Tn,e),r(s,te,e),a(te,Pd),r(s,Dn,e),E(Zt,s,e),r(s,In,e),r(s,_s,e),a(_s,Ks),a(Ks,xl),E(sa,xl,null),a(_s,Ad),a(_s,kl),a(kl,Sd),r(s,Nn,e),r(s,ae,e),a(ae,Td),r(s,Cn,e),E(ta,s,e),r(s,On,e),r(s,Xs,e),a(Xs,Dd),a(Xs,El),a(El,Id),a(Xs,Nd),r(s,Hn,e),E(aa,s,e),r(s,Fn,e),E(Zs,s,e),r(s,Ln,e),r(s,ee,e),r(s,Rn,e),r(s,vs,e),a(vs,st),a(st,ql),E(ea,ql,null),a(vs,Cd),a(vs,Pl),a(Pl,Od),r(s,Vn,e),r(s,le,e),a(le,Hd),r(s,Mn,e),r(s,$s,e),a($s,tt),a(tt,Al),E(la,Al,null),a($s,Fd),a($s,Sl),a(Sl,Ld),r(s,zn,e),r(s,M,e),a(M,Rd),a(M,oe),a(oe,Vd),a(M,Md),a(M,Tl),a(Tl,zd),a(M,Bd),a(M,Dl),a(Dl,Ud),a(M,Jd),r(s,Bn,e),r(s,at,e),a(at,Yd),a(at,oa),a(oa,Wd),a(at,Gd),r(s,Un,e),E(na,s,e),r(s,Jn,e),r(s,ws,e),a(ws,et),a(et,Il),E(ra,Il,null),a(ws,Qd),a(ws,Nl),a(Nl,Kd),r(s,Yn,e),r(s,Z,e),a(Z,Xd),a(Z,ne),a(ne,Zd),a(Z,sf),a(Z,ia),a(ia,tf),a(Z,af),r(s,Wn,e),r(s,ss,e),a(ss,ef),a(ss,re),a(re,lf),a(ss,of),a(ss,ie),a(ie,nf),a(ss,rf),r(s,Gn,e),E(pa,s,e),r(s,Qn,e),r(s,ts,e),a(ts,pf),a(ts,Cl),a(Cl,df),a(ts,ff),a(ts,pe),a(pe,cf),a(ts,hf),r(s,Kn,e),E(da,s,e),r(s,Xn,e),r(s,de,e),a(de,uf),r(s,Zn,e),E(fa,s,e),r(s,sr,e),r(s,ys,e),a(ys,lt),a(lt,Ol),E(ca,Ol,null),a(ys,mf),a(ys,Hl),a(Hl,gf),r(s,tr,e),r(s,fe,e),a(fe,_f),r(s,ar,e),E(ha,s,e),r(s,er,e),E(ot,s,e),r(s,lr,e),r(s,bs,e),a(bs,nt),a(nt,Fl),E(ua,Fl,null),a(bs,vf),a(bs,Ll),a(Ll,$f),r(s,or,e),r(s,ce,e),a(ce,wf),r(s,nr,e),E(ma,s,e),r(s,rr,e),r(s,js,e),a(js,rt),a(rt,Rl),E(ga,Rl,null),a(js,yf),a(js,Vl),a(Vl,bf),r(s,ir,e),r(s,he,e),a(he,jf),r(s,pr,e),r(s,ue,e),a(ue,xf),r(s,dr,e),r(s,as,e),a(as,Ml),a(Ml,_a),a(_a,kf),a(_a,zl),a(zl,Ef),a(_a,qf),a(as,Pf),a(as,Bl),a(Bl,xs),a(xs,Af),a(xs,Ul),a(Ul,Sf),a(xs,Tf),a(xs,Jl),a(Jl,Df),a(xs,If),a(as,Nf),a(as,Yl),a(Yl,va),a(va,Cf),a(va,me),a(me,Of),a(va,Hf),r(s,fr,e),E($a,s,e),r(s,cr,e),E(it,s,e),r(s,hr,e),r(s,pt,e),a(pt,Ff),a(pt,Wl),a(Wl,Lf),a(pt,Rf),r(s,ur,e),E(wa,s,e),mr=!0},p(s,[e]){const ya={};e&2&&(ya.$$scope={dirty:e,ctx:s}),Ss.$set(ya);const Gl={};e&2&&(Gl.$$scope={dirty:e,ctx:s}),Ts.$set(Gl);const Ql={};e&2&&(Ql.$$scope={dirty:e,ctx:s}),Ys.$set(Ql);const Kl={};e&2&&(Kl.$$scope={dirty:e,ctx:s}),Zs.$set(Kl);const Xl={};e&2&&(Xl.$$scope={dirty:e,ctx:s}),ot.$set(Xl);const Zl={};e&2&&(Zl.$$scope={dirty:e,ctx:s}),it.$set(Zl)},i(s){mr||(y(v.$$.fragment,s),y(ct.$$.fragment,s),y(ut.$$.fragment,s),y(mt.$$.fragment,s),y(Ss.$$.fragment,s),y(gt.$$.fragment,s),y(Ts.$$.fragment,s),y(vt.$$.fragment,s),y($t.$$.fragment,s),y(wt.$$.fragment,s),y(yt.$$.fragment,s),y(bt.$$.fragment,s),y(jt.$$.fragment,s),y(xt.$$.fragment,s),y(kt.$$.fragment,s),y(Et.$$.fragment,s),y(qt.$$.fragment,s),y(Pt.$$.fragment,s),y(At.$$.fragment,s),y(St.$$.fragment,s),y(Tt.$$.fragment,s),y(Dt.$$.fragment,s),y(It.$$.fragment,s),y(Nt.$$.fragment,s),y(Ct.$$.fragment,s),y(Ot.$$.fragment,s),y(Ht.$$.fragment,s),y(Ft.$$.fragment,s),y(Lt.$$.fragment,s),y(Rt.$$.fragment,s),y(Vt.$$.fragment,s),y(Mt.$$.fragment,s),y(zt.$$.fragment,s),y(Bt.$$.fragment,s),y(Ut.$$.fragment,s),y(Jt.$$.fragment,s),y(Ys.$$.fragment,s),y(Yt.$$.fragment,s),y(Wt.$$.fragment,s),y(Gt.$$.fragment,s),y(Qt.$$.fragment,s),y(Kt.$$.fragment,s),y(Xt.$$.fragment,s),y(Zt.$$.fragment,s),y(sa.$$.fragment,s),y(ta.$$.fragment,s),y(aa.$$.fragment,s),y(Zs.$$.fragment,s),y(ea.$$.fragment,s),y(la.$$.fragment,s),y(na.$$.fragment,s),y(ra.$$.fragment,s),y(pa.$$.fragment,s),y(da.$$.fragment,s),y(fa.$$.fragment,s),y(ca.$$.fragment,s),y(ha.$$.fragment,s),y(ot.$$.fragment,s),y(ua.$$.fragment,s),y(ma.$$.fragment,s),y(ga.$$.fragment,s),y($a.$$.fragment,s),y(it.$$.fragment,s),y(wa.$$.fragment,s),mr=!0)},o(s){b(v.$$.fragment,s),b(ct.$$.fragment,s),b(ut.$$.fragment,s),b(mt.$$.fragment,s),b(Ss.$$.fragment,s),b(gt.$$.fragment,s),b(Ts.$$.fragment,s),b(vt.$$.fragment,s),b($t.$$.fragment,s),b(wt.$$.fragment,s),b(yt.$$.fragment,s),b(bt.$$.fragment,s),b(jt.$$.fragment,s),b(xt.$$.fragment,s),b(kt.$$.fragment,s),b(Et.$$.fragment,s),b(qt.$$.fragment,s),b(Pt.$$.fragment,s),b(At.$$.fragment,s),b(St.$$.fragment,s),b(Tt.$$.fragment,s),b(Dt.$$.fragment,s),b(It.$$.fragment,s),b(Nt.$$.fragment,s),b(Ct.$$.fragment,s),b(Ot.$$.fragment,s),b(Ht.$$.fragment,s),b(Ft.$$.fragment,s),b(Lt.$$.fragment,s),b(Rt.$$.fragment,s),b(Vt.$$.fragment,s),b(Mt.$$.fragment,s),b(zt.$$.fragment,s),b(Bt.$$.fragment,s),b(Ut.$$.fragment,s),b(Jt.$$.fragment,s),b(Ys.$$.fragment,s),b(Yt.$$.fragment,s),b(Wt.$$.fragment,s),b(Gt.$$.fragment,s),b(Qt.$$.fragment,s),b(Kt.$$.fragment,s),b(Xt.$$.fragment,s),b(Zt.$$.fragment,s),b(sa.$$.fragment,s),b(ta.$$.fragment,s),b(aa.$$.fragment,s),b(Zs.$$.fragment,s),b(ea.$$.fragment,s),b(la.$$.fragment,s),b(na.$$.fragment,s),b(ra.$$.fragment,s),b(pa.$$.fragment,s),b(da.$$.fragment,s),b(fa.$$.fragment,s),b(ca.$$.fragment,s),b(ha.$$.fragment,s),b(ot.$$.fragment,s),b(ua.$$.fragment,s),b(ma.$$.fragment,s),b(ga.$$.fragment,s),b($a.$$.fragment,s),b(it.$$.fragment,s),b(wa.$$.fragment,s),mr=!1},d(s){t(d),s&&t(m),s&&t(u),x(v),s&&t(A),s&&t(S),s&&t(Y),s&&t(N),s&&t(U),s&&t(O),s&&t(ao),s&&t(ka),s&&t(eo),s&&t(Ea),s&&t(lo),s&&t(os),x(ct),s&&t(oo),s&&t(Ps),s&&t(no),s&&t(W),s&&t(ro),x(ut,s),s&&t(io),s&&t(Pa),s&&t(po),s&&t(As),s&&t(fo),x(mt,s),s&&t(co),x(Ss,s),s&&t(ho),s&&t(F),s&&t(uo),x(gt,s),s&&t(mo),x(Ts,s),s&&t(go),s&&t(G),s&&t(_o),x(vt,s),s&&t(vo),s&&t(Ds),s&&t($o),x($t,s),s&&t(wo),s&&t(ns),x(wt),s&&t(yo),s&&t(L),s&&t(bo),s&&t(rs),x(yt),s&&t(jo),s&&t(Sa),s&&t(xo),x(bt,s),s&&t(ko),s&&t(Ta),s&&t(Eo),x(jt,s),s&&t(qo),s&&t(Da),s&&t(Po),x(xt,s),s&&t(Ao),s&&t(Ia),s&&t(So),x(kt,s),s&&t(To),s&&t(Na),s&&t(Do),x(Et,s),s&&t(Io),s&&t(is),x(qt),s&&t(No),s&&t(Os),s&&t(Co),x(Pt,s),s&&t(Oo),s&&t(Oa),s&&t(Ho),x(At,s),s&&t(Fo),s&&t(Hs),s&&t(Lo),x(St,s),s&&t(Ro),s&&t(Ha),s&&t(Vo),x(Tt,s),s&&t(Mo),s&&t(Fa),s&&t(zo),s&&t(ps),x(Dt),s&&t(Bo),s&&t(La),s&&t(Uo),x(It,s),s&&t(Jo),s&&t(Ra),s&&t(Yo),x(Nt,s),s&&t(Wo),s&&t(ds),x(Ct),s&&t(Go),s&&t(Va),s&&t(Qo),x(Ot,s),s&&t(Ko),s&&t(Ma),s&&t(Xo),x(Ht,s),s&&t(Zo),s&&t(fs),x(Ft),s&&t(sn),s&&t(za),s&&t(tn),s&&t(Ba),s&&t(an),x(Lt,s),s&&t(en),s&&t(R),s&&t(ln),x(Rt,s),s&&t(on),s&&t(Ja),s&&t(nn),x(Vt,s),s&&t(rn),s&&t(V),s&&t(pn),s&&t(cs),x(Mt),s&&t(dn),s&&t(Ms),s&&t(fn),s&&t(hs),x(zt),s&&t(cn),s&&t(Bs),s&&t(hn),x(Bt,s),s&&t(un),s&&t(us),x(Ut),s&&t(mn),s&&t(Js),s&&t(gn),x(Jt,s),s&&t(_n),x(Ys,s),s&&t(vn),s&&t(ms),x(Yt),s&&t($n),s&&t(Qa),s&&t(wn),s&&t(Q),s&&t(yn),s&&t(gs),x(Wt),s&&t(bn),s&&t(K),s&&t(jn),s&&t(X),s&&t(xn),x(Gt,s),s&&t(kn),s&&t(Qs),s&&t(En),x(Qt,s),s&&t(qn),s&&t(Za),s&&t(Pn),x(Kt,s),s&&t(An),s&&t(se),s&&t(Sn),x(Xt,s),s&&t(Tn),s&&t(te),s&&t(Dn),x(Zt,s),s&&t(In),s&&t(_s),x(sa),s&&t(Nn),s&&t(ae),s&&t(Cn),x(ta,s),s&&t(On),s&&t(Xs),s&&t(Hn),x(aa,s),s&&t(Fn),x(Zs,s),s&&t(Ln),s&&t(ee),s&&t(Rn),s&&t(vs),x(ea),s&&t(Vn),s&&t(le),s&&t(Mn),s&&t($s),x(la),s&&t(zn),s&&t(M),s&&t(Bn),s&&t(at),s&&t(Un),x(na,s),s&&t(Jn),s&&t(ws),x(ra),s&&t(Yn),s&&t(Z),s&&t(Wn),s&&t(ss),s&&t(Gn),x(pa,s),s&&t(Qn),s&&t(ts),s&&t(Kn),x(da,s),s&&t(Xn),s&&t(de),s&&t(Zn),x(fa,s),s&&t(sr),s&&t(ys),x(ca),s&&t(tr),s&&t(fe),s&&t(ar),x(ha,s),s&&t(er),x(ot,s),s&&t(lr),s&&t(bs),x(ua),s&&t(or),s&&t(ce),s&&t(nr),x(ma,s),s&&t(rr),s&&t(js),x(ga),s&&t(ir),s&&t(he),s&&t(pr),s&&t(ue),s&&t(dr),s&&t(as),s&&t(fr),x($a,s),s&&t(cr),x(it,s),s&&t(hr),s&&t(pt),s&&t(ur),x(wa,s)}}}const em={local:"load",sections:[{local:"hugging-face-hub",title:"Hugging Face Hub"},{local:"local-and-remote-files",sections:[{local:"csv",title:"CSV"},{local:"json",title:"JSON"},{local:"text-files",title:"Text files"},{local:"parquet",title:"Parquet"},{local:"image-folders",title:"Image folders"}],title:"Local and remote files"},{local:"inmemory-data",sections:[{local:"python-dictionary",title:"Python dictionary"},{local:"pandas-dataframe",title:"Pandas DataFrame"}],title:"In-memory data"},{local:"offline",title:"Offline"},{local:"slice-splits",sections:[{local:"percent-slicing-and-rounding",title:"Percent slicing and rounding"}],title:"Slice splits"},{local:"troubleshooting",sections:[{local:"manual-download",title:"Manual download"},{local:"specify-features",title:"Specify features"}],title:"Troubleshooting"},{local:"metrics",sections:[{local:"load-configurations",title:"Load configurations"},{local:"distributed-setup",title:"Distributed setup"}],title:"Metrics"}],title:"Load"};function lm(P,d,m){let{fw:u}=d;return P.$$set=w=>{"fw"in w&&m(0,u=w.fw)},[u]}class dm extends Vf{constructor(d){super();Mf(this,d,lm,am,zf,{fw:0})}}export{dm as default,em as metadata};
