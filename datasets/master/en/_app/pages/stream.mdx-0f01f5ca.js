import{S as Bn,i as Gn,s as Wn,e as t,k as d,w as b,t as n,M as Kn,c as l,d as s,m as f,a as o,x as v,h as r,b as h,N as Un,F as a,g as i,y as j,q as w,o as $,B as k}from"../chunks/vendor-e67aec41.js";import{T as Cl}from"../chunks/Tip-76459d1c.js";import{I as ve}from"../chunks/IconCopyLink-ffd7f84e.js";import{C as R}from"../chunks/CodeBlock-ccf09204.js";import{C as Qn}from"../chunks/CodeBlockFw-a711fc3f.js";function Xn(Y){let c,_,m,g,y,u,T,I;return{c(){c=t("p"),_=n("An "),m=t("a"),g=n("datasets.IterableDataset"),y=n(" is useful for iterative jobs like training a model. You shouldn\u2019t use a "),u=t("a"),T=n("datasets.IterableDataset"),I=n(" for jobs that require random access to examples because you have to iterate all over it using a for loop. Getting the last example in an iterable dataset would require you to iterate over all the previous examples."),this.h()},l(q){c=l(q,"P",{});var x=o(c);_=r(x,"An "),m=l(x,"A",{href:!0});var A=o(m);g=r(A,"datasets.IterableDataset"),A.forEach(s),y=r(x," is useful for iterative jobs like training a model. You shouldn\u2019t use a "),u=l(x,"A",{href:!0});var M=o(u);T=r(M,"datasets.IterableDataset"),M.forEach(s),I=r(x," for jobs that require random access to examples because you have to iterate all over it using a for loop. Getting the last example in an iterable dataset would require you to iterate over all the previous examples."),x.forEach(s),this.h()},h(){h(m,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.IterableDataset"),h(u,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.IterableDataset")},m(q,x){i(q,c,x),a(c,_),a(c,m),a(m,g),a(c,y),a(c,u),a(u,T),a(c,I)},d(q){q&&s(c)}}}function Zn(Y){let c,_,m,g;return{c(){c=t("p"),_=t("a"),m=n("datasets.IterableDataset.shuffle()"),g=n(" will also shuffle the order of the shards if the dataset is sharded into multiple sets."),this.h()},l(y){c=l(y,"P",{});var u=o(c);_=l(u,"A",{href:!0});var T=o(_);m=r(T,"datasets.IterableDataset.shuffle()"),T.forEach(s),g=r(u," will also shuffle the order of the shards if the dataset is sharded into multiple sets."),u.forEach(s),this.h()},h(){h(_,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.IterableDataset.shuffle")},m(y,u){i(y,c,u),a(c,_),a(_,m),a(c,g)},d(y){y&&s(c)}}}function er(Y){let c,_,m,g,y,u,T,I,q,x,A,M,X;return{c(){c=t("p"),_=t("code"),m=n("take"),g=n(" and "),y=t("code"),u=n("skip"),T=n(" prevent future calls to "),I=t("code"),q=n("shuffle"),x=n(" because they lock in the order of the shards. You should "),A=t("code"),M=n("shuffle"),X=n(" your dataset before splitting it.")},l(S){c=l(S,"P",{});var E=o(c);_=l(E,"CODE",{});var Re=o(_);m=r(Re,"take"),Re.forEach(s),g=r(E," and "),y=l(E,"CODE",{});var Ye=o(y);u=r(Ye,"skip"),Ye.forEach(s),T=r(E," prevent future calls to "),I=l(E,"CODE",{});var Z=o(I);q=r(Z,"shuffle"),Z.forEach(s),x=r(E," because they lock in the order of the shards. You should "),A=l(E,"CODE",{});var He=o(A);M=r(He,"shuffle"),He.forEach(s),X=r(E," your dataset before splitting it."),E.forEach(s)},m(S,E){i(S,c,E),a(c,_),a(_,m),a(c,g),a(c,y),a(y,u),a(c,T),a(c,I),a(I,q),a(c,x),a(c,A),a(A,M),a(c,X)},d(S){S&&s(c)}}}function ar(Y){let c,_,m,g,y,u,T,I,q,x,A,M,X,S,E,Re,Ye,Z,He,Ka,H,Ve,Ll,Qs,Je,Ml,Qa,P,Xs,je,Zs,et,ja,at,st,Ue,tt,lt,Xa,we,Za,N,nt,Be,rt,ot,Ge,pt,it,es,ee,as,V,ae,wa,$e,ht,$a,dt,ss,z,ft,We,ct,mt,Ke,ut,_t,Qe,gt,bt,ts,C,vt,ka,jt,wt,ya,$t,kt,Xe,yt,xt,ls,ke,ns,se,rs,J,te,xa,ye,Et,Ea,Dt,os,le,It,Da,At,Tt,ps,ne,St,Ia,qt,Pt,is,xe,hs,U,re,Aa,Ee,zt,Ta,Ct,ds,Ze,Lt,fs,ea,oe,aa,Mt,Nt,Sa,Ot,Ft,cs,De,ms,sa,pe,ta,Rt,Yt,qa,Ht,Vt,us,Ie,_s,ie,gs,la,bs,B,he,Pa,Ae,Jt,za,Ut,vs,G,na,Bt,Gt,ra,Wt,Kt,js,Te,ws,de,Qt,Ca,Xt,Zt,$s,Se,ks,O,el,La,al,sl,Ma,tl,ll,ys,W,fe,Na,qe,nl,Oa,rl,xs,ce,ol,oa,pl,il,Es,Pe,Ds,K,me,Fa,ze,hl,Ra,dl,Is,D,fl,pa,cl,ml,ia,ul,_l,ha,gl,bl,da,vl,jl,fa,wl,$l,As,ca,kl,Ts,F,yl,ma,xl,El,Ya,Dl,Il,Ss,Ce,qs,Q,ue,Ha,Le,Al,Va,Tl,Ps,Me,ua,Sl,ql,zs,Ne,Cs,_a,Pl,Ls,Oe,Ms;return u=new ve({}),we=new R({props:{codee:`from datasets import load_dataset
dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
print(next(iter(dataset))),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset)))
{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, which he shared with John Blanchard during his first visit to Malawi. Chief Napoleon conveyed the desperate need for a program to intervene and care for the orphans and vulnerable children (OVC) in Malawi, and John committed to help...</span>`}}),ee=new Cl({props:{$$slots:{default:[Xn]},$$scope:{ctx:Y}}}),$e=new ve({}),ke=new R({props:{codee:`from datasets import load_dataset
dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
shuffled_dataset = dataset.shuffle(buffer_size=10_000, seed=42),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>shuffled_dataset = dataset.shuffle(buffer_size=<span class="hljs-number">10_000</span>, seed=<span class="hljs-number">42</span>)`}}),se=new Cl({props:{$$slots:{default:[Zn]},$$scope:{ctx:Y}}}),ye=new ve({}),xe=new R({props:{codee:`for epoch in range(epochs):
    shuffled_dataset.set_epoch(epoch)
    for example in shuffled_dataset:
        ...,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):
<span class="hljs-meta">... </span>    shuffled_dataset.set_epoch(epoch)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> shuffled_dataset:
<span class="hljs-meta">... </span>        ...`}}),Ee=new ve({}),De=new R({props:{codee:`dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
dataset_head = dataset.take(2)
list(dataset_head),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset_head = dataset.take(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(dataset_head)
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was...&#x27;</span>}, <span class="hljs-string">&#x27;{id&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Lily James cannot fight the music...&#x27;</span>}]`}}),Ie=new R({props:{codee:"train_dataset = shuffled_dataset.skip(1000),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_dataset = shuffled_dataset.skip(<span class="hljs-number">1000</span>)'}}),ie=new Cl({props:{warning:"&lcub;true}",$$slots:{default:[er]},$$scope:{ctx:Y}}}),Ae=new ve({}),Te=new R({props:{codee:`from datasets import interleave_datasets
from itertools import islice
en_dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
fr_dataset = load_dataset('oscar', "unshuffled_deduplicated_fr", split='train', streaming=True)

multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
print(list(islice(multilingual_dataset, 2))),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> interleave_datasets
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> itertools <span class="hljs-keyword">import</span> islice
<span class="hljs-meta">&gt;&gt;&gt; </span>en_dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>fr_dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_fr&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(islice(multilingual_dataset, <span class="hljs-number">2</span>)))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision...}, {&#x27;</span>text<span class="hljs-string">&#x27;: &quot;M\xE9dia de d\xE9bat d&#x27;</span><span class="hljs-built_in">id</span>\xE9es, de culture et de litt\xE9rature....}]`}}),Se=new R({props:{codee:`multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[0.8, 0.2], seed=42)
print(list(islice(multilingual_dataset_with_oversampling, 2))),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[<span class="hljs-number">0.8</span>, <span class="hljs-number">0.2</span>], seed=<span class="hljs-number">42</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(islice(multilingual_dataset_with_oversampling, <span class="hljs-number">2</span>)))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision...}, {&#x27;</span>text<span class="hljs-string">&#x27;: &#x27;</span>Lily James cannot fight the music...}]`}}),qe=new ve({}),Pe=new R({props:{codee:`from datasets import load_dataset
dataset = load_dataset('m4', 'en', streaming=True, split='train')
dataset = dataset.remove_columns('timestamp'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;m4&#x27;</span>, <span class="hljs-string">&#x27;en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.remove_columns(<span class="hljs-string">&#x27;timestamp&#x27;</span>)`}}),ze=new ve({}),Ce=new R({props:{codee:`from datasets import load_dataset
from transformers import AutoTokenizer
dataset = load_dataset("mc4", "en", streaming=True, split="train")
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
def encode(examples):
    return tokenizer(examples['text'], truncation=True, padding='max_length')
dataset = dataset.map(encode, batched=True)
next(iter(dataset)),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;mc4&quot;</span>, <span class="hljs-string">&quot;en&quot;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;distilbert-base-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&#x27;text&#x27;</span>], truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(encode, batched=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset))
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: <span class="hljs-number">101</span>, <span class="hljs-number">8466</span>, <span class="hljs-number">1018</span>, <span class="hljs-number">1010</span>, <span class="hljs-number">4029</span>, <span class="hljs-number">2475</span>, <span class="hljs-number">2062</span>, <span class="hljs-number">18558</span>, <span class="hljs-number">3100</span>, <span class="hljs-number">2061</span>, ...,<span class="hljs-number">1106</span>, <span class="hljs-number">3739</span>, <span class="hljs-number">102</span>],
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Le=new ve({}),Ne=new R({props:{codee:`buffer_size, seed = 10_000, 42
dataset = dataset.shuffle(buffer_size, seed),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>buffer_size, seed = <span class="hljs-number">10_000</span>, <span class="hljs-number">42</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.shuffle(buffer_size, seed)`}}),Oe=new Qn({props:{group1:{id:"pt",code:`import torch
from torch.utils.data import DataLoader
from transformers import AutoModelForMaskedLM, DataCollatorForLanguageModeling
from tqdm import tqdm
dataset = dataset.with_format("torch")
dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))
device = 'cuda' if torch.cuda.is_available() else 'cpu' 
model = AutoModelForMaskedLM.from_pretrained("distilbert-base-uncased")
model.train().to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)
for epoch in range(3):
    dataset.set_epoch(epoch)
    for i, batch in enumerate(tqdm(dataloader, total=5)):
        if i == 5:
            break
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs[0]
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        if i % 10 == 0:
            print(f"loss: {loss}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForMaskedLM, DataCollatorForLanguageModeling
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.with_format(<span class="hljs-string">&quot;torch&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span> 
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.train().to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = torch.optim.AdamW(params=model.parameters(), lr=<span class="hljs-number">1e-5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):
<span class="hljs-meta">... </span>    dataset.set_epoch(epoch)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tqdm(dataloader, total=<span class="hljs-number">5</span>)):
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">5</span>:
<span class="hljs-meta">... </span>            <span class="hljs-keyword">break</span>
<span class="hljs-meta">... </span>        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
<span class="hljs-meta">... </span>        outputs = model(**batch)
<span class="hljs-meta">... </span>        loss = outputs[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>        loss.backward()
<span class="hljs-meta">... </span>        optimizer.step()
<span class="hljs-meta">... </span>        optimizer.zero_grad()
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
<span class="hljs-meta">... </span>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;loss: <span class="hljs-subst">{loss}</span>&quot;</span>)`},group2:{id:"tf",code:"# WIP",highlighted:'<span class="hljs-comment"># WIP</span>'}}}),{c(){c=t("meta"),_=d(),m=t("h1"),g=t("a"),y=t("span"),b(u.$$.fragment),T=d(),I=t("span"),q=n("Stream"),x=d(),A=t("p"),M=n("Dataset streaming lets you get started with a dataset without waiting for the entire dataset to download. The data is downloaded progressively as you iterate over the dataset. This is especially helpful when:"),X=d(),S=t("ul"),E=t("li"),Re=n("You don\u2019t want to wait for an extremely large dataset to download."),Ye=d(),Z=t("li"),He=n("The dataset size exceeds the amount of disk space on your computer."),Ka=d(),H=t("div"),Ve=t("img"),Qs=d(),Je=t("img"),Qa=d(),P=t("p"),Xs=n("For example, the English split of the "),je=t("a"),Zs=n("OSCAR"),et=n(" dataset is 1.2 terabytes, but you can use it instantly with streaming. Stream a dataset by setting "),ja=t("code"),at=n("streaming=True"),st=n(" in "),Ue=t("a"),tt=n("datasets.load_dataset()"),lt=n(" as shown below:"),Xa=d(),b(we.$$.fragment),Za=d(),N=t("p"),nt=n("Loading a dataset in streaming mode creates a new dataset type instance (instead of the classic "),Be=t("a"),rt=n("datasets.Dataset"),ot=n(" object), known as an "),Ge=t("a"),pt=n("datasets.IterableDataset"),it=n(". This special type of dataset has its own set of processing methods shown below."),es=d(),b(ee.$$.fragment),as=d(),V=t("h2"),ae=t("a"),wa=t("span"),b($e.$$.fragment),ht=d(),$a=t("span"),dt=n("Shuffle"),ss=d(),z=t("p"),ft=n("Like a regular "),We=t("a"),ct=n("datasets.Dataset"),mt=n(" object, you can also shuffle a "),Ke=t("a"),ut=n("datasets.IterableDataset"),_t=n(" with "),Qe=t("a"),gt=n("datasets.IterableDataset.shuffle()"),bt=n("."),ts=d(),C=t("p"),vt=n("The "),ka=t("code"),jt=n("buffer_size"),wt=n(" argument controls the size of the buffer to randomly sample examples from. Let\u2019s say your dataset has one million examples, and you set the "),ya=t("code"),$t=n("buffer_size"),kt=n(" to ten thousand. "),Xe=t("a"),yt=n("datasets.IterableDataset.shuffle()"),xt=n(" will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are replaced with new examples."),ls=d(),b(ke.$$.fragment),ns=d(),b(se.$$.fragment),rs=d(),J=t("h2"),te=t("a"),xa=t("span"),b(ye.$$.fragment),Et=d(),Ea=t("span"),Dt=n("Reshuffle"),os=d(),le=t("p"),It=n("Sometimes you may want to reshuffle the dataset after each epoch. This will require you to set a different seed for each epoch. Use "),Da=t("code"),At=n("datasets.IterableDataset.set_epoch()"),Tt=n("in between epochs to tell the dataset what epoch you\u2019re on."),ps=d(),ne=t("p"),St=n("Your seed effectively becomes: "),Ia=t("code"),qt=n("initial seed + current epoch"),Pt=n("."),is=d(),b(xe.$$.fragment),hs=d(),U=t("h2"),re=t("a"),Aa=t("span"),b(Ee.$$.fragment),zt=d(),Ta=t("span"),Ct=n("Split dataset"),ds=d(),Ze=t("p"),Lt=n("You can split your dataset one of two ways:"),fs=d(),ea=t("ul"),oe=t("li"),aa=t("a"),Mt=n("datasets.IterableDataset.take()"),Nt=n(" returns the first "),Sa=t("code"),Ot=n("n"),Ft=n(" examples in a dataset:"),cs=d(),b(De.$$.fragment),ms=d(),sa=t("ul"),pe=t("li"),ta=t("a"),Rt=n("datasets.IterableDataset.skip()"),Yt=n(" omits the first "),qa=t("code"),Ht=n("n"),Vt=n(" examples in a dataset and returns the remaining examples:"),us=d(),b(Ie.$$.fragment),_s=d(),b(ie.$$.fragment),gs=d(),la=t("a"),bs=d(),B=t("h2"),he=t("a"),Pa=t("span"),b(Ae.$$.fragment),Jt=d(),za=t("span"),Ut=n("Interleave"),vs=d(),G=t("p"),na=t("a"),Bt=n("datasets.interleave_datasets()"),Gt=n(" can combine an "),ra=t("a"),Wt=n("datasets.IterableDataset"),Kt=n(" with other datasets. The combined dataset returns alternating examples from each of the original datasets."),js=d(),b(Te.$$.fragment),ws=d(),de=t("p"),Qt=n("Define sampling probabilities from each of the original datasets for more control over how each of them are sampled and combined. Set the "),Ca=t("code"),Xt=n("probabilities"),Zt=n(" argument with your desired sampling probabilities:"),$s=d(),b(Se.$$.fragment),ks=d(),O=t("p"),el=n("Around 80% of the final dataset is made of the "),La=t("code"),al=n("en_dataset"),sl=n(", and 20% of the "),Ma=t("code"),tl=n("fr_dataset"),ll=n("."),ys=d(),W=t("h2"),fe=t("a"),Na=t("span"),b(qe.$$.fragment),nl=d(),Oa=t("span"),rl=n("Remove"),xs=d(),ce=t("p"),ol=n("Remove columns on-the-fly with "),oa=t("a"),pl=n("datasets.IterableDataset.remove_columns()"),il=n(". Specify the name of the column to remove:"),Es=d(),b(Pe.$$.fragment),Ds=d(),K=t("h2"),me=t("a"),Fa=t("span"),b(ze.$$.fragment),hl=d(),Ra=t("span"),dl=n("Map"),Is=d(),D=t("p"),fl=n("Similar to the "),pa=t("a"),cl=n("datasets.Dataset.map()"),ml=n(" function for a regular "),ia=t("a"),ul=n("datasets.Dataset"),_l=n(", \u{1F917}  Datasets features "),ha=t("a"),gl=n("datasets.IterableDataset.map()"),bl=n(" for processing "),da=t("a"),vl=n("datasets.IterableDataset"),jl=n(`\\s.
`),fa=t("a"),wl=n("datasets.IterableDataset.map()"),$l=n(" applies processing on-the-fly when examples are streamed."),As=d(),ca=t("p"),kl=n("It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),Ts=d(),F=t("p"),yl=n("The following example demonstrates how to tokenize a "),ma=t("a"),xl=n("datasets.IterableDataset"),El=n(". The function needs to accept and output a "),Ya=t("code"),Dl=n("dict"),Il=n(":"),Ss=d(),b(Ce.$$.fragment),qs=d(),Q=t("h2"),ue=t("a"),Ha=t("span"),b(Le.$$.fragment),Al=d(),Va=t("span"),Tl=n("Stream in a training loop"),Ps=d(),Me=t("p"),ua=t("a"),Sl=n("datasets.IterableDataset"),ql=n(" can be integrated into a training loop. First, shuffle the dataset:"),zs=d(),b(Ne.$$.fragment),Cs=d(),_a=t("p"),Pl=n("Lastly, create a simple training loop and start training:"),Ls=d(),b(Oe.$$.fragment),this.h()},l(e){const p=Kn('[data-svelte="svelte-1phssyn"]',document.head);c=l(p,"META",{name:!0,content:!0}),p.forEach(s),_=f(e),m=l(e,"H1",{class:!0});var Fe=o(m);g=l(Fe,"A",{id:!0,class:!0,href:!0});var Ja=o(g);y=l(Ja,"SPAN",{});var Ua=o(y);v(u.$$.fragment,Ua),Ua.forEach(s),Ja.forEach(s),T=f(Fe),I=l(Fe,"SPAN",{});var Nl=o(I);q=r(Nl,"Stream"),Nl.forEach(s),Fe.forEach(s),x=f(e),A=l(e,"P",{});var Ol=o(A);M=r(Ol,"Dataset streaming lets you get started with a dataset without waiting for the entire dataset to download. The data is downloaded progressively as you iterate over the dataset. This is especially helpful when:"),Ol.forEach(s),X=f(e),S=l(e,"UL",{});var Ns=o(S);E=l(Ns,"LI",{});var Fl=o(E);Re=r(Fl,"You don\u2019t want to wait for an extremely large dataset to download."),Fl.forEach(s),Ye=f(Ns),Z=l(Ns,"LI",{});var Rl=o(Z);He=r(Rl,"The dataset size exceeds the amount of disk space on your computer."),Rl.forEach(s),Ns.forEach(s),Ka=f(e),H=l(e,"DIV",{class:!0});var Os=o(H);Ve=l(Os,"IMG",{class:!0,src:!0}),Qs=f(Os),Je=l(Os,"IMG",{class:!0,src:!0}),Os.forEach(s),Qa=f(e),P=l(e,"P",{});var _e=o(P);Xs=r(_e,"For example, the English split of the "),je=l(_e,"A",{href:!0,rel:!0});var Yl=o(je);Zs=r(Yl,"OSCAR"),Yl.forEach(s),et=r(_e," dataset is 1.2 terabytes, but you can use it instantly with streaming. Stream a dataset by setting "),ja=l(_e,"CODE",{});var Hl=o(ja);at=r(Hl,"streaming=True"),Hl.forEach(s),st=r(_e," in "),Ue=l(_e,"A",{href:!0});var Vl=o(Ue);tt=r(Vl,"datasets.load_dataset()"),Vl.forEach(s),lt=r(_e," as shown below:"),_e.forEach(s),Xa=f(e),v(we.$$.fragment,e),Za=f(e),N=l(e,"P",{});var ga=o(N);nt=r(ga,"Loading a dataset in streaming mode creates a new dataset type instance (instead of the classic "),Be=l(ga,"A",{href:!0});var Jl=o(Be);rt=r(Jl,"datasets.Dataset"),Jl.forEach(s),ot=r(ga," object), known as an "),Ge=l(ga,"A",{href:!0});var Ul=o(Ge);pt=r(Ul,"datasets.IterableDataset"),Ul.forEach(s),it=r(ga,". This special type of dataset has its own set of processing methods shown below."),ga.forEach(s),es=f(e),v(ee.$$.fragment,e),as=f(e),V=l(e,"H2",{class:!0});var Fs=o(V);ae=l(Fs,"A",{id:!0,class:!0,href:!0});var Bl=o(ae);wa=l(Bl,"SPAN",{});var Gl=o(wa);v($e.$$.fragment,Gl),Gl.forEach(s),Bl.forEach(s),ht=f(Fs),$a=l(Fs,"SPAN",{});var Wl=o($a);dt=r(Wl,"Shuffle"),Wl.forEach(s),Fs.forEach(s),ss=f(e),z=l(e,"P",{});var ge=o(z);ft=r(ge,"Like a regular "),We=l(ge,"A",{href:!0});var Kl=o(We);ct=r(Kl,"datasets.Dataset"),Kl.forEach(s),mt=r(ge," object, you can also shuffle a "),Ke=l(ge,"A",{href:!0});var Ql=o(Ke);ut=r(Ql,"datasets.IterableDataset"),Ql.forEach(s),_t=r(ge," with "),Qe=l(ge,"A",{href:!0});var Xl=o(Qe);gt=r(Xl,"datasets.IterableDataset.shuffle()"),Xl.forEach(s),bt=r(ge,"."),ge.forEach(s),ts=f(e),C=l(e,"P",{});var be=o(C);vt=r(be,"The "),ka=l(be,"CODE",{});var Zl=o(ka);jt=r(Zl,"buffer_size"),Zl.forEach(s),wt=r(be," argument controls the size of the buffer to randomly sample examples from. Let\u2019s say your dataset has one million examples, and you set the "),ya=l(be,"CODE",{});var en=o(ya);$t=r(en,"buffer_size"),en.forEach(s),kt=r(be," to ten thousand. "),Xe=l(be,"A",{href:!0});var an=o(Xe);yt=r(an,"datasets.IterableDataset.shuffle()"),an.forEach(s),xt=r(be," will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are replaced with new examples."),be.forEach(s),ls=f(e),v(ke.$$.fragment,e),ns=f(e),v(se.$$.fragment,e),rs=f(e),J=l(e,"H2",{class:!0});var Rs=o(J);te=l(Rs,"A",{id:!0,class:!0,href:!0});var sn=o(te);xa=l(sn,"SPAN",{});var tn=o(xa);v(ye.$$.fragment,tn),tn.forEach(s),sn.forEach(s),Et=f(Rs),Ea=l(Rs,"SPAN",{});var ln=o(Ea);Dt=r(ln,"Reshuffle"),ln.forEach(s),Rs.forEach(s),os=f(e),le=l(e,"P",{});var Ys=o(le);It=r(Ys,"Sometimes you may want to reshuffle the dataset after each epoch. This will require you to set a different seed for each epoch. Use "),Da=l(Ys,"CODE",{});var nn=o(Da);At=r(nn,"datasets.IterableDataset.set_epoch()"),nn.forEach(s),Tt=r(Ys,"in between epochs to tell the dataset what epoch you\u2019re on."),Ys.forEach(s),ps=f(e),ne=l(e,"P",{});var Hs=o(ne);St=r(Hs,"Your seed effectively becomes: "),Ia=l(Hs,"CODE",{});var rn=o(Ia);qt=r(rn,"initial seed + current epoch"),rn.forEach(s),Pt=r(Hs,"."),Hs.forEach(s),is=f(e),v(xe.$$.fragment,e),hs=f(e),U=l(e,"H2",{class:!0});var Vs=o(U);re=l(Vs,"A",{id:!0,class:!0,href:!0});var on=o(re);Aa=l(on,"SPAN",{});var pn=o(Aa);v(Ee.$$.fragment,pn),pn.forEach(s),on.forEach(s),zt=f(Vs),Ta=l(Vs,"SPAN",{});var hn=o(Ta);Ct=r(hn,"Split dataset"),hn.forEach(s),Vs.forEach(s),ds=f(e),Ze=l(e,"P",{});var dn=o(Ze);Lt=r(dn,"You can split your dataset one of two ways:"),dn.forEach(s),fs=f(e),ea=l(e,"UL",{});var fn=o(ea);oe=l(fn,"LI",{});var Ba=o(oe);aa=l(Ba,"A",{href:!0});var cn=o(aa);Mt=r(cn,"datasets.IterableDataset.take()"),cn.forEach(s),Nt=r(Ba," returns the first "),Sa=l(Ba,"CODE",{});var mn=o(Sa);Ot=r(mn,"n"),mn.forEach(s),Ft=r(Ba," examples in a dataset:"),Ba.forEach(s),fn.forEach(s),cs=f(e),v(De.$$.fragment,e),ms=f(e),sa=l(e,"UL",{});var un=o(sa);pe=l(un,"LI",{});var Ga=o(pe);ta=l(Ga,"A",{href:!0});var _n=o(ta);Rt=r(_n,"datasets.IterableDataset.skip()"),_n.forEach(s),Yt=r(Ga," omits the first "),qa=l(Ga,"CODE",{});var gn=o(qa);Ht=r(gn,"n"),gn.forEach(s),Vt=r(Ga," examples in a dataset and returns the remaining examples:"),Ga.forEach(s),un.forEach(s),us=f(e),v(Ie.$$.fragment,e),_s=f(e),v(ie.$$.fragment,e),gs=f(e),la=l(e,"A",{id:!0}),o(la).forEach(s),bs=f(e),B=l(e,"H2",{class:!0});var Js=o(B);he=l(Js,"A",{id:!0,class:!0,href:!0});var bn=o(he);Pa=l(bn,"SPAN",{});var vn=o(Pa);v(Ae.$$.fragment,vn),vn.forEach(s),bn.forEach(s),Jt=f(Js),za=l(Js,"SPAN",{});var jn=o(za);Ut=r(jn,"Interleave"),jn.forEach(s),Js.forEach(s),vs=f(e),G=l(e,"P",{});var Wa=o(G);na=l(Wa,"A",{href:!0});var wn=o(na);Bt=r(wn,"datasets.interleave_datasets()"),wn.forEach(s),Gt=r(Wa," can combine an "),ra=l(Wa,"A",{href:!0});var $n=o(ra);Wt=r($n,"datasets.IterableDataset"),$n.forEach(s),Kt=r(Wa," with other datasets. The combined dataset returns alternating examples from each of the original datasets."),Wa.forEach(s),js=f(e),v(Te.$$.fragment,e),ws=f(e),de=l(e,"P",{});var Us=o(de);Qt=r(Us,"Define sampling probabilities from each of the original datasets for more control over how each of them are sampled and combined. Set the "),Ca=l(Us,"CODE",{});var kn=o(Ca);Xt=r(kn,"probabilities"),kn.forEach(s),Zt=r(Us," argument with your desired sampling probabilities:"),Us.forEach(s),$s=f(e),v(Se.$$.fragment,e),ks=f(e),O=l(e,"P",{});var ba=o(O);el=r(ba,"Around 80% of the final dataset is made of the "),La=l(ba,"CODE",{});var yn=o(La);al=r(yn,"en_dataset"),yn.forEach(s),sl=r(ba,", and 20% of the "),Ma=l(ba,"CODE",{});var xn=o(Ma);tl=r(xn,"fr_dataset"),xn.forEach(s),ll=r(ba,"."),ba.forEach(s),ys=f(e),W=l(e,"H2",{class:!0});var Bs=o(W);fe=l(Bs,"A",{id:!0,class:!0,href:!0});var En=o(fe);Na=l(En,"SPAN",{});var Dn=o(Na);v(qe.$$.fragment,Dn),Dn.forEach(s),En.forEach(s),nl=f(Bs),Oa=l(Bs,"SPAN",{});var In=o(Oa);rl=r(In,"Remove"),In.forEach(s),Bs.forEach(s),xs=f(e),ce=l(e,"P",{});var Gs=o(ce);ol=r(Gs,"Remove columns on-the-fly with "),oa=l(Gs,"A",{href:!0});var An=o(oa);pl=r(An,"datasets.IterableDataset.remove_columns()"),An.forEach(s),il=r(Gs,". Specify the name of the column to remove:"),Gs.forEach(s),Es=f(e),v(Pe.$$.fragment,e),Ds=f(e),K=l(e,"H2",{class:!0});var Ws=o(K);me=l(Ws,"A",{id:!0,class:!0,href:!0});var Tn=o(me);Fa=l(Tn,"SPAN",{});var Sn=o(Fa);v(ze.$$.fragment,Sn),Sn.forEach(s),Tn.forEach(s),hl=f(Ws),Ra=l(Ws,"SPAN",{});var qn=o(Ra);dl=r(qn,"Map"),qn.forEach(s),Ws.forEach(s),Is=f(e),D=l(e,"P",{});var L=o(D);fl=r(L,"Similar to the "),pa=l(L,"A",{href:!0});var Pn=o(pa);cl=r(Pn,"datasets.Dataset.map()"),Pn.forEach(s),ml=r(L," function for a regular "),ia=l(L,"A",{href:!0});var zn=o(ia);ul=r(zn,"datasets.Dataset"),zn.forEach(s),_l=r(L,", \u{1F917}  Datasets features "),ha=l(L,"A",{href:!0});var Cn=o(ha);gl=r(Cn,"datasets.IterableDataset.map()"),Cn.forEach(s),bl=r(L," for processing "),da=l(L,"A",{href:!0});var Ln=o(da);vl=r(Ln,"datasets.IterableDataset"),Ln.forEach(s),jl=r(L,`\\s.
`),fa=l(L,"A",{href:!0});var Mn=o(fa);wl=r(Mn,"datasets.IterableDataset.map()"),Mn.forEach(s),$l=r(L," applies processing on-the-fly when examples are streamed."),L.forEach(s),As=f(e),ca=l(e,"P",{});var Nn=o(ca);kl=r(Nn,"It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),Nn.forEach(s),Ts=f(e),F=l(e,"P",{});var va=o(F);yl=r(va,"The following example demonstrates how to tokenize a "),ma=l(va,"A",{href:!0});var On=o(ma);xl=r(On,"datasets.IterableDataset"),On.forEach(s),El=r(va,". The function needs to accept and output a "),Ya=l(va,"CODE",{});var Fn=o(Ya);Dl=r(Fn,"dict"),Fn.forEach(s),Il=r(va,":"),va.forEach(s),Ss=f(e),v(Ce.$$.fragment,e),qs=f(e),Q=l(e,"H2",{class:!0});var Ks=o(Q);ue=l(Ks,"A",{id:!0,class:!0,href:!0});var Rn=o(ue);Ha=l(Rn,"SPAN",{});var Yn=o(Ha);v(Le.$$.fragment,Yn),Yn.forEach(s),Rn.forEach(s),Al=f(Ks),Va=l(Ks,"SPAN",{});var Hn=o(Va);Tl=r(Hn,"Stream in a training loop"),Hn.forEach(s),Ks.forEach(s),Ps=f(e),Me=l(e,"P",{});var zl=o(Me);ua=l(zl,"A",{href:!0});var Vn=o(ua);Sl=r(Vn,"datasets.IterableDataset"),Vn.forEach(s),ql=r(zl," can be integrated into a training loop. First, shuffle the dataset:"),zl.forEach(s),zs=f(e),v(Ne.$$.fragment,e),Cs=f(e),_a=l(e,"P",{});var Jn=o(_a);Pl=r(Jn,"Lastly, create a simple training loop and start training:"),Jn.forEach(s),Ls=f(e),v(Oe.$$.fragment,e),this.h()},h(){h(c,"name","hf:doc:metadata"),h(c,"content",JSON.stringify(sr)),h(g,"id","stream"),h(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(g,"href","#stream"),h(m,"class","relative group"),h(Ve,"class","block dark:hidden"),Un(Ve.src,Ll="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming.gif")||h(Ve,"src",Ll),h(Je,"class","hidden dark:block"),Un(Je.src,Ml="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming-dark.gif")||h(Je,"src",Ml),h(H,"class","flex justify-center"),h(je,"href","https://huggingface.co/datasets/oscar"),h(je,"rel","nofollow"),h(Ue,"href","/docs/datasets/master/en/package_reference/loading_methods#datasets.load_dataset"),h(Be,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.Dataset"),h(Ge,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.IterableDataset"),h(ae,"id","shuffle"),h(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ae,"href","#shuffle"),h(V,"class","relative group"),h(We,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.Dataset"),h(Ke,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.IterableDataset"),h(Qe,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.IterableDataset.shuffle"),h(Xe,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.IterableDataset.shuffle"),h(te,"id","reshuffle"),h(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(te,"href","#reshuffle"),h(J,"class","relative group"),h(re,"id","split-dataset"),h(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(re,"href","#split-dataset"),h(U,"class","relative group"),h(aa,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.IterableDataset.take"),h(ta,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.IterableDataset.skip"),h(la,"id","interleave_datasets"),h(he,"id","interleave"),h(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(he,"href","#interleave"),h(B,"class","relative group"),h(na,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.interleave_datasets"),h(ra,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.IterableDataset"),h(fe,"id","remove"),h(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(fe,"href","#remove"),h(W,"class","relative group"),h(oa,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.IterableDataset.remove_columns"),h(me,"id","map"),h(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(me,"href","#map"),h(K,"class","relative group"),h(pa,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.Dataset.map"),h(ia,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.Dataset"),h(ha,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.IterableDataset.map"),h(da,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.IterableDataset"),h(fa,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.IterableDataset.map"),h(ma,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.IterableDataset"),h(ue,"id","stream-in-a-training-loop"),h(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ue,"href","#stream-in-a-training-loop"),h(Q,"class","relative group"),h(ua,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.IterableDataset")},m(e,p){a(document.head,c),i(e,_,p),i(e,m,p),a(m,g),a(g,y),j(u,y,null),a(m,T),a(m,I),a(I,q),i(e,x,p),i(e,A,p),a(A,M),i(e,X,p),i(e,S,p),a(S,E),a(E,Re),a(S,Ye),a(S,Z),a(Z,He),i(e,Ka,p),i(e,H,p),a(H,Ve),a(H,Qs),a(H,Je),i(e,Qa,p),i(e,P,p),a(P,Xs),a(P,je),a(je,Zs),a(P,et),a(P,ja),a(ja,at),a(P,st),a(P,Ue),a(Ue,tt),a(P,lt),i(e,Xa,p),j(we,e,p),i(e,Za,p),i(e,N,p),a(N,nt),a(N,Be),a(Be,rt),a(N,ot),a(N,Ge),a(Ge,pt),a(N,it),i(e,es,p),j(ee,e,p),i(e,as,p),i(e,V,p),a(V,ae),a(ae,wa),j($e,wa,null),a(V,ht),a(V,$a),a($a,dt),i(e,ss,p),i(e,z,p),a(z,ft),a(z,We),a(We,ct),a(z,mt),a(z,Ke),a(Ke,ut),a(z,_t),a(z,Qe),a(Qe,gt),a(z,bt),i(e,ts,p),i(e,C,p),a(C,vt),a(C,ka),a(ka,jt),a(C,wt),a(C,ya),a(ya,$t),a(C,kt),a(C,Xe),a(Xe,yt),a(C,xt),i(e,ls,p),j(ke,e,p),i(e,ns,p),j(se,e,p),i(e,rs,p),i(e,J,p),a(J,te),a(te,xa),j(ye,xa,null),a(J,Et),a(J,Ea),a(Ea,Dt),i(e,os,p),i(e,le,p),a(le,It),a(le,Da),a(Da,At),a(le,Tt),i(e,ps,p),i(e,ne,p),a(ne,St),a(ne,Ia),a(Ia,qt),a(ne,Pt),i(e,is,p),j(xe,e,p),i(e,hs,p),i(e,U,p),a(U,re),a(re,Aa),j(Ee,Aa,null),a(U,zt),a(U,Ta),a(Ta,Ct),i(e,ds,p),i(e,Ze,p),a(Ze,Lt),i(e,fs,p),i(e,ea,p),a(ea,oe),a(oe,aa),a(aa,Mt),a(oe,Nt),a(oe,Sa),a(Sa,Ot),a(oe,Ft),i(e,cs,p),j(De,e,p),i(e,ms,p),i(e,sa,p),a(sa,pe),a(pe,ta),a(ta,Rt),a(pe,Yt),a(pe,qa),a(qa,Ht),a(pe,Vt),i(e,us,p),j(Ie,e,p),i(e,_s,p),j(ie,e,p),i(e,gs,p),i(e,la,p),i(e,bs,p),i(e,B,p),a(B,he),a(he,Pa),j(Ae,Pa,null),a(B,Jt),a(B,za),a(za,Ut),i(e,vs,p),i(e,G,p),a(G,na),a(na,Bt),a(G,Gt),a(G,ra),a(ra,Wt),a(G,Kt),i(e,js,p),j(Te,e,p),i(e,ws,p),i(e,de,p),a(de,Qt),a(de,Ca),a(Ca,Xt),a(de,Zt),i(e,$s,p),j(Se,e,p),i(e,ks,p),i(e,O,p),a(O,el),a(O,La),a(La,al),a(O,sl),a(O,Ma),a(Ma,tl),a(O,ll),i(e,ys,p),i(e,W,p),a(W,fe),a(fe,Na),j(qe,Na,null),a(W,nl),a(W,Oa),a(Oa,rl),i(e,xs,p),i(e,ce,p),a(ce,ol),a(ce,oa),a(oa,pl),a(ce,il),i(e,Es,p),j(Pe,e,p),i(e,Ds,p),i(e,K,p),a(K,me),a(me,Fa),j(ze,Fa,null),a(K,hl),a(K,Ra),a(Ra,dl),i(e,Is,p),i(e,D,p),a(D,fl),a(D,pa),a(pa,cl),a(D,ml),a(D,ia),a(ia,ul),a(D,_l),a(D,ha),a(ha,gl),a(D,bl),a(D,da),a(da,vl),a(D,jl),a(D,fa),a(fa,wl),a(D,$l),i(e,As,p),i(e,ca,p),a(ca,kl),i(e,Ts,p),i(e,F,p),a(F,yl),a(F,ma),a(ma,xl),a(F,El),a(F,Ya),a(Ya,Dl),a(F,Il),i(e,Ss,p),j(Ce,e,p),i(e,qs,p),i(e,Q,p),a(Q,ue),a(ue,Ha),j(Le,Ha,null),a(Q,Al),a(Q,Va),a(Va,Tl),i(e,Ps,p),i(e,Me,p),a(Me,ua),a(ua,Sl),a(Me,ql),i(e,zs,p),j(Ne,e,p),i(e,Cs,p),i(e,_a,p),a(_a,Pl),i(e,Ls,p),j(Oe,e,p),Ms=!0},p(e,[p]){const Fe={};p&2&&(Fe.$$scope={dirty:p,ctx:e}),ee.$set(Fe);const Ja={};p&2&&(Ja.$$scope={dirty:p,ctx:e}),se.$set(Ja);const Ua={};p&2&&(Ua.$$scope={dirty:p,ctx:e}),ie.$set(Ua)},i(e){Ms||(w(u.$$.fragment,e),w(we.$$.fragment,e),w(ee.$$.fragment,e),w($e.$$.fragment,e),w(ke.$$.fragment,e),w(se.$$.fragment,e),w(ye.$$.fragment,e),w(xe.$$.fragment,e),w(Ee.$$.fragment,e),w(De.$$.fragment,e),w(Ie.$$.fragment,e),w(ie.$$.fragment,e),w(Ae.$$.fragment,e),w(Te.$$.fragment,e),w(Se.$$.fragment,e),w(qe.$$.fragment,e),w(Pe.$$.fragment,e),w(ze.$$.fragment,e),w(Ce.$$.fragment,e),w(Le.$$.fragment,e),w(Ne.$$.fragment,e),w(Oe.$$.fragment,e),Ms=!0)},o(e){$(u.$$.fragment,e),$(we.$$.fragment,e),$(ee.$$.fragment,e),$($e.$$.fragment,e),$(ke.$$.fragment,e),$(se.$$.fragment,e),$(ye.$$.fragment,e),$(xe.$$.fragment,e),$(Ee.$$.fragment,e),$(De.$$.fragment,e),$(Ie.$$.fragment,e),$(ie.$$.fragment,e),$(Ae.$$.fragment,e),$(Te.$$.fragment,e),$(Se.$$.fragment,e),$(qe.$$.fragment,e),$(Pe.$$.fragment,e),$(ze.$$.fragment,e),$(Ce.$$.fragment,e),$(Le.$$.fragment,e),$(Ne.$$.fragment,e),$(Oe.$$.fragment,e),Ms=!1},d(e){s(c),e&&s(_),e&&s(m),k(u),e&&s(x),e&&s(A),e&&s(X),e&&s(S),e&&s(Ka),e&&s(H),e&&s(Qa),e&&s(P),e&&s(Xa),k(we,e),e&&s(Za),e&&s(N),e&&s(es),k(ee,e),e&&s(as),e&&s(V),k($e),e&&s(ss),e&&s(z),e&&s(ts),e&&s(C),e&&s(ls),k(ke,e),e&&s(ns),k(se,e),e&&s(rs),e&&s(J),k(ye),e&&s(os),e&&s(le),e&&s(ps),e&&s(ne),e&&s(is),k(xe,e),e&&s(hs),e&&s(U),k(Ee),e&&s(ds),e&&s(Ze),e&&s(fs),e&&s(ea),e&&s(cs),k(De,e),e&&s(ms),e&&s(sa),e&&s(us),k(Ie,e),e&&s(_s),k(ie,e),e&&s(gs),e&&s(la),e&&s(bs),e&&s(B),k(Ae),e&&s(vs),e&&s(G),e&&s(js),k(Te,e),e&&s(ws),e&&s(de),e&&s($s),k(Se,e),e&&s(ks),e&&s(O),e&&s(ys),e&&s(W),k(qe),e&&s(xs),e&&s(ce),e&&s(Es),k(Pe,e),e&&s(Ds),e&&s(K),k(ze),e&&s(Is),e&&s(D),e&&s(As),e&&s(ca),e&&s(Ts),e&&s(F),e&&s(Ss),k(Ce,e),e&&s(qs),e&&s(Q),k(Le),e&&s(Ps),e&&s(Me),e&&s(zs),k(Ne,e),e&&s(Cs),e&&s(_a),e&&s(Ls),k(Oe,e)}}}const sr={local:"stream",sections:[{local:"shuffle",title:"Shuffle"},{local:"reshuffle",title:"Reshuffle"},{local:"split-dataset",title:"Split dataset"},{local:"interleave",title:"Interleave"},{local:"remove",title:"Remove"},{local:"map",title:"Map"},{local:"stream-in-a-training-loop",title:"Stream in a training loop"}],title:"Stream"};function tr(Y,c,_){let{fw:m}=c;return Y.$$set=g=>{"fw"in g&&_(0,m=g.fw)},[m]}class ir extends Bn{constructor(c){super();Gn(this,c,tr,ar,Wn,{fw:0})}}export{ir as default,sr as metadata};
