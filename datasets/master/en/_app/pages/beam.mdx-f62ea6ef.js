import{S as Ve,i as Xe,s as Ze,e as r,k as _,w as G,t as l,M as ea,c as o,d as a,m as d,a as n,x as M,h as i,b as p,F as t,g as f,y as R,q as F,o as L,B as K}from"../chunks/vendor-e67aec41.js";import{T as aa}from"../chunks/Tip-76459d1c.js";import{I as ta}from"../chunks/IconCopyLink-ffd7f84e.js";import{C as fe}from"../chunks/CodeBlock-e2bcf023.js";function sa(U){let c,E;return{c(){c=r("p"),E=l("When you run your pipeline, you can adjust the parameters to change the runner (Flink or Spark), output location (S3 bucket or HDFS), and the number of workers.")},l(u){c=o(u,"P",{});var m=n(c);E=i(m,"When you run your pipeline, you can adjust the parameters to change the runner (Flink or Spark), output location (S3 bucket or HDFS), and the number of workers."),m.forEach(a)},m(u,m){f(u,c,m),t(c,E)},d(u){u&&a(c)}}}function ra(U){let c,E,u,m,J,y,ue,W,ce,X,h,me,v,he,_e,k,de,$e,A,Ee,be,j,ge,we,Z,$,ye,T,ve,ke,C,Ae,je,H,Te,Ce,ee,P,Y,Se,ae,S,te,q,z,qe,se,B,re,x,Q,Be,oe,N,le,D,V,xe,ne,I,ie,g,pe;return y=new ta({}),S=new fe({props:{code:`DATASET_NAME=your_dataset_name  # ex: wikipedia
CONFIG_NAME=your_config_name    # ex: 20200501.en`,highlighted:`<span class="hljs-attr">DATASET_NAME</span>=your_dataset_name  <span class="hljs-comment"># ex: wikipedia</span>
<span class="hljs-attr">CONFIG_NAME</span>=your_config_name    <span class="hljs-comment"># ex: 20200501.en</span>`}}),B=new fe({props:{code:`PROJECT=your_project
BUCKET=your_bucket
REGION=your_region`,highlighted:`<span class="hljs-attribute">PROJECT</span><span class="hljs-operator">=</span>your_project
<span class="hljs-attribute">BUCKET</span><span class="hljs-operator">=</span>your_bucket
<span class="hljs-attribute">REGION</span><span class="hljs-operator">=</span>your_region`}}),N=new fe({props:{code:`echo "datasets" > /tmp/beam_requirements.txt
echo "apache_beam" >> /tmp/beam_requirements.txt`,highlighted:`echo <span class="hljs-string">&quot;datasets&quot;</span> &gt; <span class="hljs-regexp">/tmp/</span>beam_requirements.txt
echo <span class="hljs-string">&quot;apache_beam&quot;</span> &gt;&gt; <span class="hljs-regexp">/tmp/</span>beam_requirements.txt`}}),I=new fe({props:{code:`datasets-cli run_beam datasets/$DATASET_NAME \\
--name $CONFIG_NAME \\
--save_infos \\
--cache_dir gs://$BUCKET/cache/datasets \\
--beam_pipeline_options=\\
"runner=DataflowRunner,project=$PROJECT,job_name=$DATASET_NAME-gen,"\\
"staging_location=gs://$BUCKET/binaries,temp_location=gs://$BUCKET/temp,"\\
"region=$REGION,requirements_file=/tmp/beam_requirements.txt"`,highlighted:`datasets-cli run_beam datasets/<span class="hljs-variable">$DATASET_NAME</span> \\
--name <span class="hljs-variable">$CONFIG_NAME</span> \\
--save_infos \\
--cache_dir gs://<span class="hljs-variable">$BUCKET</span>/cache/datasets \\
--beam_pipeline_options=\\
<span class="hljs-string">&quot;runner=DataflowRunner,project=<span class="hljs-variable">$PROJECT</span>,job_name=<span class="hljs-variable">$DATASET_NAME</span>-gen,&quot;</span>\\
<span class="hljs-string">&quot;staging_location=gs://<span class="hljs-variable">$BUCKET</span>/binaries,temp_location=gs://<span class="hljs-variable">$BUCKET</span>/temp,&quot;</span>\\
<span class="hljs-string">&quot;region=<span class="hljs-variable">$REGION</span>,requirements_file=/tmp/beam_requirements.txt&quot;</span>`}}),g=new aa({props:{$$slots:{default:[sa]},$$scope:{ctx:U}}}),{c(){c=r("meta"),E=_(),u=r("h1"),m=r("a"),J=r("span"),G(y.$$.fragment),ue=_(),W=r("span"),ce=l("Beam Datasets"),X=_(),h=r("p"),me=l("Some datasets are too large to be processed on a single machine. Instead, you can process them with "),v=r("a"),he=l("Apache Beam"),_e=l(", a library for parallel data processing. The processing pipeline is executed on a distributed processing backend such as "),k=r("a"),de=l("Apache Flink"),$e=l(", "),A=r("a"),Ee=l("Apache Spark"),be=l(", or "),j=r("a"),ge=l("Google Cloud Dataflow"),we=l("."),Z=_(),$=r("p"),ye=l("We have already created Beam pipelines for some of the larger datasets like "),T=r("a"),ve=l("wikipedia"),ke=l(", and "),C=r("a"),Ae=l("wiki40b"),je=l(". You can load these normally with "),H=r("code"),Te=l("datasets.Datasets.load_dataset"),Ce=l(". But if you want to run your own Beam pipeline with Dataflow, here is how:"),ee=_(),P=r("ol"),Y=r("li"),Se=l("Specify the dataset and configuration you want to process:"),ae=_(),G(S.$$.fragment),te=_(),q=r("ol"),z=r("li"),qe=l("Input your Google Cloud Platform information:"),se=_(),G(B.$$.fragment),re=_(),x=r("ol"),Q=r("li"),Be=l("Specify your Python requirements:"),oe=_(),G(N.$$.fragment),le=_(),D=r("ol"),V=r("li"),xe=l("Run the pipeline:"),ne=_(),G(I.$$.fragment),ie=_(),G(g.$$.fragment),this.h()},l(e){const s=ea('[data-svelte="svelte-1phssyn"]',document.head);c=o(s,"META",{name:!0,content:!0}),s.forEach(a),E=d(e),u=o(e,"H1",{class:!0});var O=n(u);m=o(O,"A",{id:!0,class:!0,href:!0});var Ne=n(m);J=o(Ne,"SPAN",{});var De=n(J);M(y.$$.fragment,De),De.forEach(a),Ne.forEach(a),ue=d(O),W=o(O,"SPAN",{});var Ie=n(W);ce=i(Ie,"Beam Datasets"),Ie.forEach(a),O.forEach(a),X=d(e),h=o(e,"P",{});var b=n(h);me=i(b,"Some datasets are too large to be processed on a single machine. Instead, you can process them with "),v=o(b,"A",{href:!0,rel:!0});var Oe=n(v);he=i(Oe,"Apache Beam"),Oe.forEach(a),_e=i(b,", a library for parallel data processing. The processing pipeline is executed on a distributed processing backend such as "),k=o(b,"A",{href:!0,rel:!0});var Pe=n(k);de=i(Pe,"Apache Flink"),Pe.forEach(a),$e=i(b,", "),A=o(b,"A",{href:!0,rel:!0});var Ge=n(A);Ee=i(Ge,"Apache Spark"),Ge.forEach(a),be=i(b,", or "),j=o(b,"A",{href:!0,rel:!0});var Me=n(j);ge=i(Me,"Google Cloud Dataflow"),Me.forEach(a),we=i(b,"."),b.forEach(a),Z=d(e),$=o(e,"P",{});var w=n($);ye=i(w,"We have already created Beam pipelines for some of the larger datasets like "),T=o(w,"A",{href:!0,rel:!0});var Re=n(T);ve=i(Re,"wikipedia"),Re.forEach(a),ke=i(w,", and "),C=o(w,"A",{href:!0,rel:!0});var Fe=n(C);Ae=i(Fe,"wiki40b"),Fe.forEach(a),je=i(w,". You can load these normally with "),H=o(w,"CODE",{});var Le=n(H);Te=i(Le,"datasets.Datasets.load_dataset"),Le.forEach(a),Ce=i(w,". But if you want to run your own Beam pipeline with Dataflow, here is how:"),w.forEach(a),ee=d(e),P=o(e,"OL",{});var Ke=n(P);Y=o(Ke,"LI",{});var Ue=n(Y);Se=i(Ue,"Specify the dataset and configuration you want to process:"),Ue.forEach(a),Ke.forEach(a),ae=d(e),M(S.$$.fragment,e),te=d(e),q=o(e,"OL",{start:!0});var Je=n(q);z=o(Je,"LI",{});var We=n(z);qe=i(We,"Input your Google Cloud Platform information:"),We.forEach(a),Je.forEach(a),se=d(e),M(B.$$.fragment,e),re=d(e),x=o(e,"OL",{start:!0});var He=n(x);Q=o(He,"LI",{});var Ye=n(Q);Be=i(Ye,"Specify your Python requirements:"),Ye.forEach(a),He.forEach(a),oe=d(e),M(N.$$.fragment,e),le=d(e),D=o(e,"OL",{start:!0});var ze=n(D);V=o(ze,"LI",{});var Qe=n(V);xe=i(Qe,"Run the pipeline:"),Qe.forEach(a),ze.forEach(a),ne=d(e),M(I.$$.fragment,e),ie=d(e),M(g.$$.fragment,e),this.h()},h(){p(c,"name","hf:doc:metadata"),p(c,"content",JSON.stringify(oa)),p(m,"id","beam-datasets"),p(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(m,"href","#beam-datasets"),p(u,"class","relative group"),p(v,"href","https://beam.apache.org/"),p(v,"rel","nofollow"),p(k,"href","https://flink.apache.org/"),p(k,"rel","nofollow"),p(A,"href","https://spark.apache.org/"),p(A,"rel","nofollow"),p(j,"href","https://cloud.google.com/dataflow"),p(j,"rel","nofollow"),p(T,"href","https://huggingface.co/datasets/wikipedia"),p(T,"rel","nofollow"),p(C,"href","https://huggingface.co/datasets/wiki40b"),p(C,"rel","nofollow"),p(q,"start","2"),p(x,"start","3"),p(D,"start","4")},m(e,s){t(document.head,c),f(e,E,s),f(e,u,s),t(u,m),t(m,J),R(y,J,null),t(u,ue),t(u,W),t(W,ce),f(e,X,s),f(e,h,s),t(h,me),t(h,v),t(v,he),t(h,_e),t(h,k),t(k,de),t(h,$e),t(h,A),t(A,Ee),t(h,be),t(h,j),t(j,ge),t(h,we),f(e,Z,s),f(e,$,s),t($,ye),t($,T),t(T,ve),t($,ke),t($,C),t(C,Ae),t($,je),t($,H),t(H,Te),t($,Ce),f(e,ee,s),f(e,P,s),t(P,Y),t(Y,Se),f(e,ae,s),R(S,e,s),f(e,te,s),f(e,q,s),t(q,z),t(z,qe),f(e,se,s),R(B,e,s),f(e,re,s),f(e,x,s),t(x,Q),t(Q,Be),f(e,oe,s),R(N,e,s),f(e,le,s),f(e,D,s),t(D,V),t(V,xe),f(e,ne,s),R(I,e,s),f(e,ie,s),R(g,e,s),pe=!0},p(e,[s]){const O={};s&2&&(O.$$scope={dirty:s,ctx:e}),g.$set(O)},i(e){pe||(F(y.$$.fragment,e),F(S.$$.fragment,e),F(B.$$.fragment,e),F(N.$$.fragment,e),F(I.$$.fragment,e),F(g.$$.fragment,e),pe=!0)},o(e){L(y.$$.fragment,e),L(S.$$.fragment,e),L(B.$$.fragment,e),L(N.$$.fragment,e),L(I.$$.fragment,e),L(g.$$.fragment,e),pe=!1},d(e){a(c),e&&a(E),e&&a(u),K(y),e&&a(X),e&&a(h),e&&a(Z),e&&a($),e&&a(ee),e&&a(P),e&&a(ae),K(S,e),e&&a(te),e&&a(q),e&&a(se),K(B,e),e&&a(re),e&&a(x),e&&a(oe),K(N,e),e&&a(le),e&&a(D),e&&a(ne),K(I,e),e&&a(ie),K(g,e)}}}const oa={local:"beam-datasets",title:"Beam Datasets"};function la(U,c,E){let{fw:u}=c;return U.$$set=m=>{"fw"in m&&E(0,u=m.fw)},[u]}class ua extends Ve{constructor(c){super();Xe(this,c,la,ra,Ze,{fw:0})}}export{ua as default,oa as metadata};
