import{S as hi,i as _i,s as vi,e as s,k as l,w as h,t as o,M as bi,c as r,d as t,m as i,a as n,x as _,h as d,b as E,F as e,g,y as v,q as b,o as $,B as w,v as $i,L as zt}from"../../chunks/vendor-8138ceec.js";import{D as y}from"../../chunks/Docstring-6fa3bd37.js";import{C as Kt}from"../../chunks/CodeBlock-fc89709f.js";import{I as wi}from"../../chunks/IconCopyLink-2dd3a6ac.js";import{E as Jt}from"../../chunks/ExampleCodeBlock-25dbadc2.js";function Ei(A){let f,T,x,c,D;return c=new Kt({props:{code:"extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))",highlighted:'<span class="hljs-attr">extracted_paths</span> = dl_manager.extract(dl_manager.download(url_or_urls))'}}),{c(){f=s("p"),T=o("Is roughly equivalent to:"),x=l(),h(c.$$.fragment)},l(p){f=r(p,"P",{});var u=n(f);T=d(u,"Is roughly equivalent to:"),u.forEach(t),x=i(p),_(c.$$.fragment,p)},m(p,u){g(p,f,u),e(f,T),g(p,x,u),v(c,p,u),D=!0},p:zt,i(p){D||(b(c.$$.fragment,p),D=!0)},o(p){$(c.$$.fragment,p),D=!1},d(p){p&&t(f),p&&t(x),w(c,p)}}}function xi(A){let f,T,x,c,D;return c=new Kt({props:{code:`Each descriptor can be composed with other using addition or slice. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[0:25]) + datasets.Split.TEST

The resulting split will correspond to 25% of the train split merged with
100% of the test split.`,highlighted:`Each descriptor can be composed <span class="hljs-keyword">with</span> other using addition <span class="hljs-keyword">or</span> <span class="hljs-built_in">slice</span>. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">0</span>:<span class="hljs-number">25</span>]) + datasets.Split.TEST

The resulting split will correspond to <span class="hljs-number">25</span>% of the train split merged <span class="hljs-keyword">with</span>
<span class="hljs-number">100</span>% of the test split.`}}),{c(){f=s("p"),T=o("Example:"),x=l(),h(c.$$.fragment)},l(p){f=r(p,"P",{});var u=n(f);T=d(u,"Example:"),u.forEach(t),x=i(p),_(c.$$.fragment,p)},m(p,u){g(p,f,u),e(f,T),g(p,x,u),v(c,p,u),D=!0},p:zt,i(p){D||(b(c.$$.fragment,p),D=!0)},o(p){$(c.$$.fragment,p),D=!1},d(p){p&&t(f),p&&t(x),w(c,p)}}}function Di(A){let f,T,x,c,D;return c=new Kt({props:{code:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[75:])
)  # Error
split = datasets.Split.TEST + datasets.Split.ALL  # Error`,highlighted:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">75</span>:])
)  <span class="hljs-comment"># Error</span>
split = datasets.Split.TEST + datasets.Split.ALL  <span class="hljs-comment"># Error</span>`}}),{c(){f=s("p"),T=o("A split cannot be added twice, so the following will fail:"),x=l(),h(c.$$.fragment)},l(p){f=r(p,"P",{});var u=n(f);T=d(u,"A split cannot be added twice, so the following will fail:"),u.forEach(t),x=i(p),_(c.$$.fragment,p)},m(p,u){g(p,f,u),e(f,T),g(p,x,u),v(c,p,u),D=!0},p:zt,i(p){D||(b(c.$$.fragment,p),D=!0)},o(p){$(c.$$.fragment,p),D=!1},d(p){p&&t(f),p&&t(x),w(c,p)}}}function yi(A){let f,T,x,c,D;return c=new Kt({props:{code:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
datasets.Split.TEST.subsplit(datasets.percent[:50])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:50])`,highlighted:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
datasets.Split.TEST.subsplit(datasets.percent[:<span class="hljs-number">50</span>])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),{c(){f=s("p"),T=o("The slices can be applied only one time. So the following are valid:"),x=l(),h(c.$$.fragment)},l(p){f=r(p,"P",{});var u=n(f);T=d(u,"The slices can be applied only one time. So the following are valid:"),u.forEach(t),x=i(p),_(c.$$.fragment,p)},m(p,u){g(p,f,u),e(f,T),g(p,x,u),v(c,p,u),D=!0},p:zt,i(p){D||(b(c.$$.fragment,p),D=!0)},o(p){$(c.$$.fragment,p),D=!1},d(p){p&&t(f),p&&t(x),w(c,p)}}}function Ti(A){let f,T,x,c,D;return c=new Kt({props:{code:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:25]).subsplit(datasets.percent[:25])
split = (train.subsplit(datasets.percent[:25]) + test).subsplit(datasets.percent[:50])`,highlighted:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]).subsplit(datasets.percent[:<span class="hljs-number">25</span>])
split = (train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) + test).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),{c(){f=s("p"),T=o("But not:"),x=l(),h(c.$$.fragment)},l(p){f=r(p,"P",{});var u=n(f);T=d(u,"But not:"),u.forEach(t),x=i(p),_(c.$$.fragment,p)},m(p,u){g(p,f,u),e(f,T),g(p,x,u),v(c,p,u),D=!0},p:zt,i(p){D||(b(c.$$.fragment,p),D=!0)},o(p){$(c.$$.fragment,p),D=!1},d(p){p&&t(f),p&&t(x),w(c,p)}}}function Ii(A){let f,T,x,c,D;return c=new Kt({props:{code:`# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec('test[:33%]'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction('test', to=33, unit='%'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%'))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]+train[1:-1]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%]+train[1:-1]'))
ds = datasets.load_dataset('mnist', split=(
datasets.ReadInstruction('test', to=33, unit='%') +
datasets.ReadInstruction('train', from_=1, to=-1, unit='abs')))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%](pct1_dropremainder)')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%](pct1_dropremainder)'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%', rounding="pct1_dropremainder"))

# 10-fold validation:
tests = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', from_=k, to=k+10, unit='%')
for k in range(0, 100, 10)])
trains = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')
for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(<span class="hljs-string">&#x27;test[:33%]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=(
datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) +
datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">1</span>, to=-<span class="hljs-number">1</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>)))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&quot;pct1_dropremainder&quot;</span>))

<span class="hljs-comment"># 10-fold validation:</span>
tests = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
trains = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}),{c(){f=s("p"),T=o("Examples:"),x=l(),h(c.$$.fragment)},l(p){f=r(p,"P",{});var u=n(f);T=d(u,"Examples:"),u.forEach(t),x=i(p),_(c.$$.fragment,p)},m(p,u){g(p,f,u),e(f,T),g(p,x,u),v(c,p,u),D=!0},p:zt,i(p){D||(b(c.$$.fragment,p),D=!0)},o(p){$(c.$$.fragment,p),D=!1},d(p){p&&t(f),p&&t(x),w(c,p)}}}function Bi(A){let f,T,x,c,D,p,u,Yt,rr,vs,H,nr,_t,or,dr,vt,lr,ir,bs,I,Re,pr,Qt,cr,fr,bt,Zt,mr,ur,gr,Y,$t,ea,hr,_r,vr,wt,Et,br,$r,wr,ce,xt,Er,xr,ta,Dr,yr,Tr,L,aa,Ir,Br,sa,Sr,Nr,ra,jr,kr,na,Rr,Cr,Or,fe,Ce,Ar,oa,Pr,Lr,me,Oe,Vr,da,Fr,Mr,ue,Ae,Ur,la,qr,Hr,ge,Pe,Gr,ia,Wr,Xr,he,Le,Jr,pa,zr,$s,F,Ve,Kr,ca,Yr,Qr,G,fa,Zr,en,ma,tn,an,ua,sn,rn,ws,Q,Fe,nn,ga,on,Es,Z,Me,dn,ha,ln,xs,P,Ue,pn,qe,cn,Dt,fn,mn,un,He,gn,yt,hn,_n,vn,W,Ge,bn,_a,$n,wn,ee,va,En,xn,ba,Dn,yn,$a,Tn,Ds,B,We,In,_e,Xe,Bn,wa,Sn,Nn,X,Je,jn,Ea,kn,Rn,ve,Cn,be,ze,On,Ke,An,xa,Pn,Ln,Vn,$e,Ye,Fn,Da,Mn,Un,we,Qe,qn,ya,Hn,Gn,Ee,Ze,Wn,Ta,Xn,Jn,xe,et,zn,Ia,Kn,ys,C,tt,Yn,Tt,Ba,Qn,Zn,eo,at,to,Sa,ao,so,ro,Na,no,oo,st,ja,te,Ts,lo,ka,io,po,Ra,co,fo,ae,se,It,Ca,mo,uo,go,Oa,ho,_o,Aa,vo,bo,re,Pa,La,$o,wo,Va,Eo,xo,Fa,Do,yo,ne,Ma,Ua,To,Io,qa,Bo,So,Ha,No,Is,M,rt,jo,Ga,ko,Ro,oe,Co,Wa,Oo,Ao,Xa,Po,Lo,Bs,j,nt,Vo,Bt,Ja,Fo,Mo,Uo,za,qo,Ho,U,St,Ka,Go,Wo,Xo,Nt,Ya,Jo,zo,Ko,jt,Qa,Yo,Qo,Zo,kt,Za,ed,td,ad,Rt,sd,es,rd,nd,ot,od,ts,dd,ld,Ss,S,dt,id,as,pd,cd,De,fd,ss,md,ud,ye,gd,rs,hd,_d,Te,vd,Ie,Ns,de,lt,bd,ns,$d,js,O,it,wd,os,Ed,xd,Be,Dd,Se,pt,yd,ds,Td,Id,J,ct,Bd,ls,Sd,Nd,is,jd,ks,le,ft,kd,ps,Rd,Rs,q,mt,Cd,cs,Od,Ad,Ne,ut,Pd,fs,Ld,Cs;return p=new wi({}),Re=new y({props:{name:"class datasets.DatasetBuilder",anchor:"datasets.DatasetBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"repo_id",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/builder.py#L177"}}),Ce=new y({props:{name:"as_dataset",anchor:"datasets.DatasetBuilder.as_dataset",parameters:[{name:"split",val:": typing.Optional[datasets.splits.Split] = None"},{name:"run_post_process",val:" = True"},{name:"ignore_verifications",val:" = False"},{name:"in_memory",val:" = False"}],parametersDescription:[{anchor:"datasets.DatasetBuilder.as_dataset.split",description:"<strong>split</strong> (<code>datasets.Split</code>) &#x2014; Which subset of the data to return.",name:"split"},{anchor:"datasets.DatasetBuilder.as_dataset.run_post_process",description:`<strong>run_post_process</strong> (bool, default=True) &#x2014; Whether to run post-processing dataset transforms and/or add
indexes.`,name:"run_post_process"},{anchor:"datasets.DatasetBuilder.as_dataset.ignore_verifications",description:`<strong>ignore_verifications</strong> (bool, default=False) &#x2014; Whether to ignore the verifications of the
downloaded/processed dataset information (checksums/size/splits/&#x2026;).`,name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.as_dataset.in_memory",description:"<strong>in_memory</strong> (bool, default=False) &#x2014; Whether to copy the data in-memory.",name:"in_memory"}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/builder.py#L752",returnDescription:`
<p>datasets.Dataset</p>
`}}),Oe=new y({props:{name:"download_and_prepare",anchor:"datasets.DatasetBuilder.download_and_prepare",parameters:[{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"download_mode",val:": typing.Optional[datasets.utils.download_manager.DownloadMode] = None"},{name:"ignore_verifications",val:": bool = False"},{name:"try_from_hf_gcs",val:": bool = True"},{name:"dl_manager",val:": typing.Optional[datasets.utils.download_manager.DownloadManager] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"**download_and_prepare_kwargs",val:""}],parametersDescription:[{anchor:"datasets.DatasetBuilder.download_and_prepare.download_config",description:'<strong>download_config</strong> (<a href="/docs/datasets/v2.2.1/en/package_reference/builder_classes#datasets.DownloadConfig">DownloadConfig</a>, optional) &#x2014; specific download configuration parameters.',name:"download_config"},{anchor:"datasets.DatasetBuilder.download_and_prepare.download_mode",description:'<strong>download_mode</strong> (<a href="/docs/datasets/v2.2.1/en/package_reference/builder_classes#datasets.DownloadMode">DownloadMode</a>, optional) &#x2014; select the download/generate mode - Default to <code>REUSE_DATASET_IF_EXISTS</code>',name:"download_mode"},{anchor:"datasets.DatasetBuilder.download_and_prepare.ignore_verifications",description:"<strong>ignore_verifications</strong> (<code>bool</code>) &#x2014; Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/&#x2026;)",name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.download_and_prepare.try_from_hf_gcs",description:"<strong>try_from_hf_gcs</strong> (<code>bool</code>) &#x2014; If True, it will try to download the already prepared dataset from the Hf google cloud storage",name:"try_from_hf_gcs"},{anchor:"datasets.DatasetBuilder.download_and_prepare.dl_manager",description:'<strong>dl_manager</strong> (<a href="/docs/datasets/v2.2.1/en/package_reference/builder_classes#datasets.DownloadManager">DownloadManager</a>, optional) &#x2014; specific Download Manger to use',name:"dl_manager"},{anchor:"datasets.DatasetBuilder.download_and_prepare.base_path",description:`<strong>base_path</strong> (<code>str</code>, optional) &#x2014; base path for relative paths that are used to download files. This can be a remote url.
If not specified, the value of the <em>base_path</em> attribute (<em>self.base_path</em>) will be used instead.`,name:"base_path"},{anchor:"datasets.DatasetBuilder.download_and_prepare.use_auth_token",description:`<strong>use_auth_token</strong> (<code>Union[str, bool]</code>, optional) &#x2014; Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.
If True, will get token from ~/.huggingface.`,name:"use_auth_token"}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/builder.py#L492"}}),Ae=new y({props:{name:"get_all_exported_dataset_infos",anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos",parameters:[],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/builder.py#L324"}}),Pe=new y({props:{name:"get_exported_dataset_info",anchor:"datasets.DatasetBuilder.get_exported_dataset_info",parameters:[],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/builder.py#L332"}}),Le=new y({props:{name:"get_imported_module_dir",anchor:"datasets.DatasetBuilder.get_imported_module_dir",parameters:[],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/builder.py#L487"}}),Ve=new y({props:{name:"class datasets.GeneratorBasedBuilder",anchor:"datasets.GeneratorBasedBuilder",parameters:[{name:"*args",val:""},{name:"writer_batch_size",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/builder.py#L1025"}}),Fe=new y({props:{name:"class datasets.BeamBasedBuilder",anchor:"datasets.BeamBasedBuilder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/builder.py#L1185"}}),Me=new y({props:{name:"class datasets.ArrowBasedBuilder",anchor:"datasets.ArrowBasedBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"repo_id",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/builder.py#L1123"}}),Ue=new y({props:{name:"class datasets.BuilderConfig",anchor:"datasets.BuilderConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = '0.0.0'"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"datasets.BuilderConfig.name",description:"<strong>name</strong> (<code>str</code>, default <code>&quot;default&quot;</code>) &#x2014;",name:"name"},{anchor:"datasets.BuilderConfig.version",description:'<strong>version</strong> (<a href="/docs/datasets/v2.2.1/en/package_reference/builder_classes#datasets.Version">Version</a> or <code>str</code>, optional) &#x2014;',name:"version"},{anchor:"datasets.BuilderConfig.data_dir",description:"<strong>data_dir</strong> (<code>str</code>, optional) &#x2014;",name:"data_dir"},{anchor:"datasets.BuilderConfig.data_files",description:"<strong>data_files</strong> (<code>str</code> or <code>Sequence</code> or <code>Mapping</code>, optional) &#x2014; Path(s) to source data file(s).",name:"data_files"},{anchor:"datasets.BuilderConfig.description",description:"<strong>description</strong> (<code>str</code>, optional) &#x2014;",name:"description"}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/builder.py#L81"}}),Ge=new y({props:{name:"create_config_id",anchor:"datasets.BuilderConfig.create_config_id",parameters:[{name:"config_kwargs",val:": dict"},{name:"custom_features",val:": typing.Optional[datasets.features.features.Features] = None"}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/builder.py#L120"}}),We=new y({props:{name:"class datasets.DownloadManager",anchor:"datasets.DownloadManager",parameters:[{name:"dataset_name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"record_checksums",val:" = True"}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/utils/download_manager.py#L141"}}),Xe=new y({props:{name:"download",anchor:"datasets.DownloadManager.download",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/utils/download_manager.py#L259",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Je=new y({props:{name:"download_and_extract",anchor:"datasets.DownloadManager.download_and_extract",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/utils/download_manager.py#L367",returnDescription:`
<p><code>str</code>, extracted paths of given URL(s).</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),ve=new Jt({props:{anchor:"datasets.DownloadManager.download_and_extract.example",$$slots:{default:[Ei]},$$scope:{ctx:A}}}),ze=new y({props:{name:"download_custom",anchor:"datasets.DownloadManager.download_custom",parameters:[{name:"url_or_urls",val:""},{name:"custom_download",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/utils/download_manager.py#L218",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Ye=new y({props:{name:"extract",anchor:"datasets.DownloadManager.extract",parameters:[{name:"path_or_paths",val:""},{name:"num_proc",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/utils/download_manager.py#L337",returnDescription:`
<p><code>str</code>, The extracted paths matching the given input
path_or_paths.</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Qe=new y({props:{name:"iter_archive",anchor:"datasets.DownloadManager.iter_archive",parameters:[{name:"path_or_buf",val:": typing.Union[str, _io.BufferedReader]"}],parametersDescription:[{anchor:"datasets.DownloadManager.iter_archive.path_or_buf",description:"<strong>path_or_buf</strong> (<code>str</code> or <code>io.BufferedReader</code>) &#x2014; Archive path or archive binary file object.",name:"path_or_buf"}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/utils/download_manager.py#L310"}}),Ze=new y({props:{name:"iter_files",anchor:"datasets.DownloadManager.iter_files",parameters:[{name:"paths",val:": typing.Union[str, typing.List[str]]"}],parametersDescription:[{anchor:"datasets.DownloadManager.iter_files.paths",description:"<strong>paths</strong> (<code>str</code> or <code>list</code> of <code>str</code>) &#x2014; Root paths.",name:"paths"}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/utils/download_manager.py#L326"}}),et=new y({props:{name:"ship_files_with_pipeline",anchor:"datasets.DownloadManager.ship_files_with_pipeline",parameters:[{name:"downloaded_path_or_paths",val:""},{name:"pipeline",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/utils/download_manager.py#L183"}}),tt=new y({props:{name:"class datasets.DownloadMode",anchor:"datasets.DownloadMode",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/utils/download_manager.py#L44"}}),rt=new y({props:{name:"class datasets.SplitGenerator",anchor:"datasets.SplitGenerator",parameters:[{name:"name",val:": str"},{name:"gen_kwargs",val:": typing.Dict = <factory>"}],parametersDescription:[{anchor:"datasets.SplitGenerator.name",description:`<strong>name</strong> (str) &#x2014; Name of the Split for which the generator will
create the examples.
**gen_kwargs &#x2014; Keyword arguments to forward to the <code>DatasetBuilder._generate_examples</code> method
of the builder.`,name:"name"}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/splits.py#L549"}}),nt=new y({props:{name:"class datasets.Split",anchor:"datasets.Split",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/splits.py#L387"}}),dt=new y({props:{name:"class datasets.NamedSplit",anchor:"datasets.NamedSplit",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/splits.py#L303"}}),De=new Jt({props:{anchor:"datasets.NamedSplit.example",$$slots:{default:[xi]},$$scope:{ctx:A}}}),ye=new Jt({props:{anchor:"datasets.NamedSplit.example-2",$$slots:{default:[Di]},$$scope:{ctx:A}}}),Te=new Jt({props:{anchor:"datasets.NamedSplit.example-3",$$slots:{default:[yi]},$$scope:{ctx:A}}}),Ie=new Jt({props:{anchor:"datasets.NamedSplit.example-4",$$slots:{default:[Ti]},$$scope:{ctx:A}}}),lt=new y({props:{name:"class datasets.NamedSplitAll",anchor:"datasets.NamedSplitAll",parameters:[],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/splits.py#L372"}}),it=new y({props:{name:"class datasets.ReadInstruction",anchor:"datasets.ReadInstruction",parameters:[{name:"split_name",val:""},{name:"rounding",val:" = None"},{name:"from_",val:" = None"},{name:"to",val:" = None"},{name:"unit",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/arrow_reader.py#L456"}}),Be=new Jt({props:{anchor:"datasets.ReadInstruction.example",$$slots:{default:[Ii]},$$scope:{ctx:A}}}),pt=new y({props:{name:"from_spec",anchor:"datasets.ReadInstruction.from_spec",parameters:[{name:"spec",val:""}],parametersDescription:[{anchor:"datasets.ReadInstruction.from_spec.spec",description:`<strong>spec</strong> (str) &#x2014; split(s) + optional slice(s) to read + optional rounding
if percents are used as the slicing unit. A slice can be specified,
using absolute numbers (int) or percentages (int). E.g.
<code>test</code>: test split.
<code>test + validation</code>: test split + validation split.
<code>test[10:]</code>: test split, minus its first 10 records.
<code>test[:10%]</code>: first 10% records of test split.
<code>test[:20%](pct1_dropremainder)</code>: first 10% records, rounded with
the <code>pct1_dropremainder</code> rounding.
<code>test[:-5%]+train[40%:60%]</code>: first 95% of test + middle 20% of
train.`,name:"spec"}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/arrow_reader.py#L536",returnDescription:`
<p>ReadInstruction instance.</p>
`}}),ct=new y({props:{name:"to_absolute",anchor:"datasets.ReadInstruction.to_absolute",parameters:[{name:"name2len",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/arrow_reader.py#L604",returnDescription:`
<p>list of _AbsoluteInstruction instances (corresponds to the + in spec).</p>
`}}),ft=new y({props:{name:"class datasets.DownloadConfig",anchor:"datasets.DownloadConfig",parameters:[{name:"cache_dir",val:": typing.Union[pathlib.Path, str, NoneType] = None"},{name:"force_download",val:": bool = False"},{name:"resume_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"proxies",val:": typing.Optional[typing.Dict] = None"},{name:"user_agent",val:": typing.Optional[str] = None"},{name:"extract_compressed_file",val:": bool = False"},{name:"force_extract",val:": bool = False"},{name:"delete_extracted",val:": bool = False"},{name:"use_etag",val:": bool = True"},{name:"num_proc",val:": typing.Optional[int] = None"},{name:"max_retries",val:": int = 1"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"ignore_url_params",val:": bool = False"},{name:"download_desc",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"datasets.DownloadConfig.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>Path</code>, optional) &#x2014; Specify a cache directory to save the file to (overwrite the
default cache dir).`,name:"cache_dir"},{anchor:"datasets.DownloadConfig.force_download",description:`<strong>force_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, re-dowload the file even if it&#x2019;s already cached in
the cache dir.`,name:"force_download"},{anchor:"datasets.DownloadConfig.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, resume the download if incompletly recieved file is
found.`,name:"resume_download"},{anchor:"datasets.DownloadConfig.proxies",description:"<strong>proxies</strong> (<code>dict</code>, optional) &#x2014;",name:"proxies"},{anchor:"datasets.DownloadConfig.user_agent",description:`<strong>user_agent</strong> (<code>str</code>, optional) &#x2014; Optional string or dict that will be appended to the user-agent on remote
requests.`,name:"user_agent"},{anchor:"datasets.DownloadConfig.extract_compressed_file",description:`<strong>extract_compressed_file</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True and the path point to a zip or tar file,
extract the compressed file in a folder along the archive.`,name:"extract_compressed_file"},{anchor:"datasets.DownloadConfig.force_extract",description:`<strong>force_extract</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True when extract_compressed_file is True and the archive
was already extracted, re-extract the archive and override the folder where it was extracted.`,name:"force_extract"},{anchor:"datasets.DownloadConfig.delete_extracted",description:"<strong>delete_extracted</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to delete (or keep) the extracted files.",name:"delete_extracted"},{anchor:"datasets.DownloadConfig.use_etag",description:"<strong>use_etag</strong> (<code>bool</code>, default <code>True</code>) &#x2014; Whether to use the ETag HTTP response header to validate the cached files.",name:"use_etag"},{anchor:"datasets.DownloadConfig.num_proc",description:"<strong>num_proc</strong> (<code>int</code>, optional) &#x2014; The number of processes to launch to download the files in parallel.",name:"num_proc"},{anchor:"datasets.DownloadConfig.max_retries",description:"<strong>max_retries</strong> (<code>int</code>, default <code>1</code>) &#x2014; The number of times to retry an HTTP request if it fails.",name:"max_retries"},{anchor:"datasets.DownloadConfig.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, optional) &#x2014; Optional string or boolean to use as Bearer token
for remote files on the Datasets Hub. If True, will get token from ~/.huggingface.`,name:"use_auth_token"},{anchor:"datasets.DownloadConfig.ignore_url_params",description:`<strong>ignore_url_params</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to strip all query parameters and #fragments from
the download URL before using it for caching the file.`,name:"ignore_url_params"},{anchor:"datasets.DownloadConfig.download_desc",description:"<strong>download_desc</strong> (<code>str</code>, optional) &#x2014; A description to be displayed alongside with the progress bar while downloading the files.",name:"download_desc"}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/utils/file_utils.py#L153"}}),mt=new y({props:{name:"class datasets.Version",anchor:"datasets.Version",parameters:[{name:"version_str",val:": str"},{name:"description",val:": typing.Optional[str] = None"},{name:"major",val:": typing.Union[str, int, NoneType] = None"},{name:"minor",val:": typing.Union[str, int, NoneType] = None"},{name:"patch",val:": typing.Union[str, int, NoneType] = None"}],parametersDescription:[{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.major",description:"<strong>major</strong> (<code>str</code>) &#x2014;",name:"major"},{anchor:"datasets.Version.minor",description:"<strong>minor</strong> (<code>str</code>) &#x2014;",name:"minor"},{anchor:"datasets.Version.patch",description:"<strong>patch</strong> (<code>str</code>) &#x2014;",name:"patch"}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/utils/version.py#L30"}}),ut=new y({props:{name:"match",anchor:"datasets.Version.match",parameters:[{name:"other_version",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.1/src/datasets/utils/version.py#L92"}}),{c(){f=s("meta"),T=l(),x=s("h1"),c=s("a"),D=s("span"),h(p.$$.fragment),u=l(),Yt=s("span"),rr=o("Builder classes"),vs=l(),H=s("p"),nr=o("\u{1F917} Datasets relies on two main classes during the dataset building process: "),_t=s("a"),or=o("DatasetBuilder"),dr=o(" and "),vt=s("a"),lr=o("BuilderConfig"),ir=o("."),bs=l(),I=s("div"),h(Re.$$.fragment),pr=l(),Qt=s("p"),cr=o("Abstract base class for all datasets."),fr=l(),bt=s("p"),Zt=s("em"),mr=o("DatasetBuilder"),ur=o(" has 3 key methods:"),gr=l(),Y=s("ul"),$t=s("li"),ea=s("code"),hr=o("datasets.DatasetBuilder.info"),_r=o(`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),vr=l(),wt=s("li"),Et=s("a"),br=o("datasets.DatasetBuilder.download_and_prepare()"),$r=o(`: Downloads the source data
and writes it to disk.`),wr=l(),ce=s("li"),xt=s("a"),Er=o("datasets.DatasetBuilder.as_dataset()"),xr=o(": Generates a "),ta=s("em"),Dr=o("Dataset"),yr=o("."),Tr=l(),L=s("p"),aa=s("strong"),Ir=o("Configuration"),Br=o(": Some "),sa=s("em"),Sr=o("DatasetBuilder"),Nr=o(`s expose multiple variants of the
dataset by defining a `),ra=s("em"),jr=o("datasets.BuilderConfig"),kr=o(` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),na=s("code"),Rr=o("datasets.DatasetBuilder.builder_configs()"),Cr=o("."),Or=l(),fe=s("div"),h(Ce.$$.fragment),Ar=l(),oa=s("p"),Pr=o("Return a Dataset for the specified split."),Lr=l(),me=s("div"),h(Oe.$$.fragment),Vr=l(),da=s("p"),Fr=o("Downloads and prepares dataset for reading."),Mr=l(),ue=s("div"),h(Ae.$$.fragment),Ur=l(),la=s("p"),qr=o("Empty dict if doesn\u2019t exist"),Hr=l(),ge=s("div"),h(Pe.$$.fragment),Gr=l(),ia=s("p"),Wr=o("Empty DatasetInfo if doesn\u2019t exist"),Xr=l(),he=s("div"),h(Le.$$.fragment),Jr=l(),pa=s("p"),zr=o("Return the path of the module of this class or subclass."),$s=l(),F=s("div"),h(Ve.$$.fragment),Kr=l(),ca=s("p"),Yr=o("Base class for datasets with data generation based on dict generators."),Qr=l(),G=s("p"),fa=s("code"),Zr=o("GeneratorBasedBuilder"),en=o(` is a convenience class that abstracts away much
of the data writing and reading of `),ma=s("code"),tn=o("DatasetBuilder"),an=o(`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),ua=s("code"),sn=o("_split_generators"),rn=o("). See the method docstrings for details."),ws=l(),Q=s("div"),h(Fe.$$.fragment),nn=l(),ga=s("p"),on=o("Beam based Builder."),Es=l(),Z=s("div"),h(Me.$$.fragment),dn=l(),ha=s("p"),ln=o("Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),xs=l(),P=s("div"),h(Ue.$$.fragment),pn=l(),qe=s("p"),cn=o("Base class for "),Dt=s("a"),fn=o("DatasetBuilder"),mn=o(" data configuration."),un=l(),He=s("p"),gn=o(`DatasetBuilder subclasses with data configuration options should subclass
`),yt=s("a"),hn=o("BuilderConfig"),_n=o(" and add their own properties."),vn=l(),W=s("div"),h(Ge.$$.fragment),bn=l(),_a=s("p"),$n=o(`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),wn=l(),ee=s("ul"),va=s("li"),En=o("the config kwargs that can be used to overwrite attributes"),xn=l(),ba=s("li"),Dn=o("the custom features used to write the dataset"),yn=l(),$a=s("li"),Tn=o(`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),Ds=l(),B=s("div"),h(We.$$.fragment),In=l(),_e=s("div"),h(Xe.$$.fragment),Bn=l(),wa=s("p"),Sn=o("Download given url(s)."),Nn=l(),X=s("div"),h(Je.$$.fragment),jn=l(),Ea=s("p"),kn=o("Download and extract given url_or_urls."),Rn=l(),h(ve.$$.fragment),Cn=l(),be=s("div"),h(ze.$$.fragment),On=l(),Ke=s("p"),An=o("Download given urls(s) by calling "),xa=s("code"),Pn=o("custom_download"),Ln=o("."),Vn=l(),$e=s("div"),h(Ye.$$.fragment),Fn=l(),Da=s("p"),Mn=o("Extract given path(s)."),Un=l(),we=s("div"),h(Qe.$$.fragment),qn=l(),ya=s("p"),Hn=o("Iterate over files within an archive."),Gn=l(),Ee=s("div"),h(Ze.$$.fragment),Wn=l(),Ta=s("p"),Xn=o("Iterate over file paths."),Jn=l(),xe=s("div"),h(et.$$.fragment),zn=l(),Ia=s("p"),Kn=o("Ship the files using Beam FileSystems to the pipeline temp dir."),ys=l(),C=s("div"),h(tt.$$.fragment),Yn=l(),Tt=s("p"),Ba=s("code"),Qn=o("Enum"),Zn=o(" for how to treat pre-existing downloads and data."),eo=l(),at=s("p"),to=o("The default mode is "),Sa=s("code"),ao=o("REUSE_DATASET_IF_EXISTS"),so=o(`, which will reuse both
raw downloads and the prepared dataset if they exist.`),ro=l(),Na=s("p"),no=o("The generations modes:"),oo=l(),st=s("table"),ja=s("thead"),te=s("tr"),Ts=s("th"),lo=l(),ka=s("th"),io=o("Downloads"),po=l(),Ra=s("th"),co=o("Dataset"),fo=l(),ae=s("tbody"),se=s("tr"),It=s("td"),Ca=s("code"),mo=o("REUSE_DATASET_IF_EXISTS"),uo=o(" (default)"),go=l(),Oa=s("td"),ho=o("Reuse"),_o=l(),Aa=s("td"),vo=o("Reuse"),bo=l(),re=s("tr"),Pa=s("td"),La=s("code"),$o=o("REUSE_CACHE_IF_EXISTS"),wo=l(),Va=s("td"),Eo=o("Reuse"),xo=l(),Fa=s("td"),Do=o("Fresh"),yo=l(),ne=s("tr"),Ma=s("td"),Ua=s("code"),To=o("FORCE_REDOWNLOAD"),Io=l(),qa=s("td"),Bo=o("Fresh"),So=l(),Ha=s("td"),No=o("Fresh"),Is=l(),M=s("div"),h(rt.$$.fragment),jo=l(),Ga=s("p"),ko=o("Defines the split information for the generator."),Ro=l(),oe=s("p"),Co=o(`This should be used as returned value of
`),Wa=s("code"),Oo=o("GeneratorBasedBuilder._split_generators()"),Ao=o(`.
See `),Xa=s("code"),Po=o("GeneratorBasedBuilder._split_generators()"),Lo=o(` for more info and example
of usage.`),Bs=l(),j=s("div"),h(nt.$$.fragment),Vo=l(),Bt=s("p"),Ja=s("code"),Fo=o("Enum"),Mo=o(" for dataset splits."),Uo=l(),za=s("p"),qo=o(`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),Ho=l(),U=s("ul"),St=s("li"),Ka=s("code"),Go=o("TRAIN"),Wo=o(": the training data."),Xo=l(),Nt=s("li"),Ya=s("code"),Jo=o("VALIDATION"),zo=o(`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Ko=l(),jt=s("li"),Qa=s("code"),Yo=o("TEST"),Qo=o(`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Zo=l(),kt=s("li"),Za=s("code"),ed=o("ALL"),td=o(": the union of all defined dataset splits."),ad=l(),Rt=s("p"),sd=o("Note: All splits, including compositions inherit from "),es=s("code"),rd=o("datasets.SplitBase"),nd=l(),ot=s("p"),od=o("See the :doc:"),ts=s("code"),dd=o("guide on splits </loading>"),ld=o(" for more information."),Ss=l(),S=s("div"),h(dt.$$.fragment),id=l(),as=s("p"),pd=o("Descriptor corresponding to a named split (train, test, \u2026)."),cd=l(),h(De.$$.fragment),fd=l(),ss=s("p"),md=o("Warning:"),ud=l(),h(ye.$$.fragment),gd=l(),rs=s("p"),hd=o("Warning:"),_d=l(),h(Te.$$.fragment),vd=l(),h(Ie.$$.fragment),Ns=l(),de=s("div"),h(lt.$$.fragment),bd=l(),ns=s("p"),$d=o("Split corresponding to the union of all defined dataset splits."),js=l(),O=s("div"),h(it.$$.fragment),wd=l(),os=s("p"),Ed=o("Reading instruction for a dataset."),xd=l(),h(Be.$$.fragment),Dd=l(),Se=s("div"),h(pt.$$.fragment),yd=l(),ds=s("p"),Td=o("Creates a ReadInstruction instance out of a string spec."),Id=l(),J=s("div"),h(ct.$$.fragment),Bd=l(),ls=s("p"),Sd=o("Translate instruction into a list of absolute instructions."),Nd=l(),is=s("p"),jd=o("Those absolute instructions are then to be added together."),ks=l(),le=s("div"),h(ft.$$.fragment),kd=l(),ps=s("p"),Rd=o("Configuration for our cached path manager."),Rs=l(),q=s("div"),h(mt.$$.fragment),Cd=l(),cs=s("p"),Od=o("Dataset version MAJOR.MINOR.PATCH."),Ad=l(),Ne=s("div"),h(ut.$$.fragment),Pd=l(),fs=s("p"),Ld=o("Returns True if other_version matches."),this.h()},l(a){const m=bi('[data-svelte="svelte-1phssyn"]',document.head);f=r(m,"META",{name:!0,content:!0}),m.forEach(t),T=i(a),x=r(a,"H1",{class:!0});var gt=n(x);c=r(gt,"A",{id:!0,class:!0,href:!0});var ms=n(c);D=r(ms,"SPAN",{});var us=n(D);_(p.$$.fragment,us),us.forEach(t),ms.forEach(t),u=i(gt),Yt=r(gt,"SPAN",{});var gs=n(Yt);rr=d(gs,"Builder classes"),gs.forEach(t),gt.forEach(t),vs=i(a),H=r(a,"P",{});var ie=n(H);nr=d(ie,"\u{1F917} Datasets relies on two main classes during the dataset building process: "),_t=r(ie,"A",{href:!0});var hs=n(_t);or=d(hs,"DatasetBuilder"),hs.forEach(t),dr=d(ie," and "),vt=r(ie,"A",{href:!0});var Kd=n(vt);lr=d(Kd,"BuilderConfig"),Kd.forEach(t),ir=d(ie,"."),ie.forEach(t),bs=i(a),I=r(a,"DIV",{class:!0});var N=n(I);_(Re.$$.fragment,N),pr=i(N),Qt=r(N,"P",{});var Yd=n(Qt);cr=d(Yd,"Abstract base class for all datasets."),Yd.forEach(t),fr=i(N),bt=r(N,"P",{});var Vd=n(bt);Zt=r(Vd,"EM",{});var Qd=n(Zt);mr=d(Qd,"DatasetBuilder"),Qd.forEach(t),ur=d(Vd," has 3 key methods:"),Vd.forEach(t),gr=i(N),Y=r(N,"UL",{});var Ct=n(Y);$t=r(Ct,"LI",{});var Fd=n($t);ea=r(Fd,"CODE",{});var Zd=n(ea);hr=d(Zd,"datasets.DatasetBuilder.info"),Zd.forEach(t),_r=d(Fd,`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),Fd.forEach(t),vr=i(Ct),wt=r(Ct,"LI",{});var Md=n(wt);Et=r(Md,"A",{href:!0});var el=n(Et);br=d(el,"datasets.DatasetBuilder.download_and_prepare()"),el.forEach(t),$r=d(Md,`: Downloads the source data
and writes it to disk.`),Md.forEach(t),wr=i(Ct),ce=r(Ct,"LI",{});var _s=n(ce);xt=r(_s,"A",{href:!0});var tl=n(xt);Er=d(tl,"datasets.DatasetBuilder.as_dataset()"),tl.forEach(t),xr=d(_s,": Generates a "),ta=r(_s,"EM",{});var al=n(ta);Dr=d(al,"Dataset"),al.forEach(t),yr=d(_s,"."),_s.forEach(t),Ct.forEach(t),Tr=i(N),L=r(N,"P",{});var pe=n(L);aa=r(pe,"STRONG",{});var sl=n(aa);Ir=d(sl,"Configuration"),sl.forEach(t),Br=d(pe,": Some "),sa=r(pe,"EM",{});var rl=n(sa);Sr=d(rl,"DatasetBuilder"),rl.forEach(t),Nr=d(pe,`s expose multiple variants of the
dataset by defining a `),ra=r(pe,"EM",{});var nl=n(ra);jr=d(nl,"datasets.BuilderConfig"),nl.forEach(t),kr=d(pe,` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),na=r(pe,"CODE",{});var ol=n(na);Rr=d(ol,"datasets.DatasetBuilder.builder_configs()"),ol.forEach(t),Cr=d(pe,"."),pe.forEach(t),Or=i(N),fe=r(N,"DIV",{class:!0});var Os=n(fe);_(Ce.$$.fragment,Os),Ar=i(Os),oa=r(Os,"P",{});var dl=n(oa);Pr=d(dl,"Return a Dataset for the specified split."),dl.forEach(t),Os.forEach(t),Lr=i(N),me=r(N,"DIV",{class:!0});var As=n(me);_(Oe.$$.fragment,As),Vr=i(As),da=r(As,"P",{});var ll=n(da);Fr=d(ll,"Downloads and prepares dataset for reading."),ll.forEach(t),As.forEach(t),Mr=i(N),ue=r(N,"DIV",{class:!0});var Ps=n(ue);_(Ae.$$.fragment,Ps),Ur=i(Ps),la=r(Ps,"P",{});var il=n(la);qr=d(il,"Empty dict if doesn\u2019t exist"),il.forEach(t),Ps.forEach(t),Hr=i(N),ge=r(N,"DIV",{class:!0});var Ls=n(ge);_(Pe.$$.fragment,Ls),Gr=i(Ls),ia=r(Ls,"P",{});var pl=n(ia);Wr=d(pl,"Empty DatasetInfo if doesn\u2019t exist"),pl.forEach(t),Ls.forEach(t),Xr=i(N),he=r(N,"DIV",{class:!0});var Vs=n(he);_(Le.$$.fragment,Vs),Jr=i(Vs),pa=r(Vs,"P",{});var cl=n(pa);zr=d(cl,"Return the path of the module of this class or subclass."),cl.forEach(t),Vs.forEach(t),N.forEach(t),$s=i(a),F=r(a,"DIV",{class:!0});var Ot=n(F);_(Ve.$$.fragment,Ot),Kr=i(Ot),ca=r(Ot,"P",{});var fl=n(ca);Yr=d(fl,"Base class for datasets with data generation based on dict generators."),fl.forEach(t),Qr=i(Ot),G=r(Ot,"P",{});var ht=n(G);fa=r(ht,"CODE",{});var ml=n(fa);Zr=d(ml,"GeneratorBasedBuilder"),ml.forEach(t),en=d(ht,` is a convenience class that abstracts away much
of the data writing and reading of `),ma=r(ht,"CODE",{});var ul=n(ma);tn=d(ul,"DatasetBuilder"),ul.forEach(t),an=d(ht,`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),ua=r(ht,"CODE",{});var gl=n(ua);sn=d(gl,"_split_generators"),gl.forEach(t),rn=d(ht,"). See the method docstrings for details."),ht.forEach(t),Ot.forEach(t),ws=i(a),Q=r(a,"DIV",{class:!0});var Fs=n(Q);_(Fe.$$.fragment,Fs),nn=i(Fs),ga=r(Fs,"P",{});var hl=n(ga);on=d(hl,"Beam based Builder."),hl.forEach(t),Fs.forEach(t),Es=i(a),Z=r(a,"DIV",{class:!0});var Ms=n(Z);_(Me.$$.fragment,Ms),dn=i(Ms),ha=r(Ms,"P",{});var _l=n(ha);ln=d(_l,"Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),_l.forEach(t),Ms.forEach(t),xs=i(a),P=r(a,"DIV",{class:!0});var je=n(P);_(Ue.$$.fragment,je),pn=i(je),qe=r(je,"P",{});var Us=n(qe);cn=d(Us,"Base class for "),Dt=r(Us,"A",{href:!0});var vl=n(Dt);fn=d(vl,"DatasetBuilder"),vl.forEach(t),mn=d(Us," data configuration."),Us.forEach(t),un=i(je),He=r(je,"P",{});var qs=n(He);gn=d(qs,`DatasetBuilder subclasses with data configuration options should subclass
`),yt=r(qs,"A",{href:!0});var bl=n(yt);hn=d(bl,"BuilderConfig"),bl.forEach(t),_n=d(qs," and add their own properties."),qs.forEach(t),vn=i(je),W=r(je,"DIV",{class:!0});var At=n(W);_(Ge.$$.fragment,At),bn=i(At),_a=r(At,"P",{});var $l=n(_a);$n=d($l,`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),$l.forEach(t),wn=i(At),ee=r(At,"UL",{});var Pt=n(ee);va=r(Pt,"LI",{});var wl=n(va);En=d(wl,"the config kwargs that can be used to overwrite attributes"),wl.forEach(t),xn=i(Pt),ba=r(Pt,"LI",{});var El=n(ba);Dn=d(El,"the custom features used to write the dataset"),El.forEach(t),yn=i(Pt),$a=r(Pt,"LI",{});var xl=n($a);Tn=d(xl,`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),xl.forEach(t),Pt.forEach(t),At.forEach(t),je.forEach(t),Ds=i(a),B=r(a,"DIV",{class:!0});var k=n(B);_(We.$$.fragment,k),In=i(k),_e=r(k,"DIV",{class:!0});var Hs=n(_e);_(Xe.$$.fragment,Hs),Bn=i(Hs),wa=r(Hs,"P",{});var Dl=n(wa);Sn=d(Dl,"Download given url(s)."),Dl.forEach(t),Hs.forEach(t),Nn=i(k),X=r(k,"DIV",{class:!0});var Lt=n(X);_(Je.$$.fragment,Lt),jn=i(Lt),Ea=r(Lt,"P",{});var yl=n(Ea);kn=d(yl,"Download and extract given url_or_urls."),yl.forEach(t),Rn=i(Lt),_(ve.$$.fragment,Lt),Lt.forEach(t),Cn=i(k),be=r(k,"DIV",{class:!0});var Gs=n(be);_(ze.$$.fragment,Gs),On=i(Gs),Ke=r(Gs,"P",{});var Ws=n(Ke);An=d(Ws,"Download given urls(s) by calling "),xa=r(Ws,"CODE",{});var Tl=n(xa);Pn=d(Tl,"custom_download"),Tl.forEach(t),Ln=d(Ws,"."),Ws.forEach(t),Gs.forEach(t),Vn=i(k),$e=r(k,"DIV",{class:!0});var Xs=n($e);_(Ye.$$.fragment,Xs),Fn=i(Xs),Da=r(Xs,"P",{});var Il=n(Da);Mn=d(Il,"Extract given path(s)."),Il.forEach(t),Xs.forEach(t),Un=i(k),we=r(k,"DIV",{class:!0});var Js=n(we);_(Qe.$$.fragment,Js),qn=i(Js),ya=r(Js,"P",{});var Bl=n(ya);Hn=d(Bl,"Iterate over files within an archive."),Bl.forEach(t),Js.forEach(t),Gn=i(k),Ee=r(k,"DIV",{class:!0});var zs=n(Ee);_(Ze.$$.fragment,zs),Wn=i(zs),Ta=r(zs,"P",{});var Sl=n(Ta);Xn=d(Sl,"Iterate over file paths."),Sl.forEach(t),zs.forEach(t),Jn=i(k),xe=r(k,"DIV",{class:!0});var Ks=n(xe);_(et.$$.fragment,Ks),zn=i(Ks),Ia=r(Ks,"P",{});var Nl=n(Ia);Kn=d(Nl,"Ship the files using Beam FileSystems to the pipeline temp dir."),Nl.forEach(t),Ks.forEach(t),k.forEach(t),ys=i(a),C=r(a,"DIV",{class:!0});var z=n(C);_(tt.$$.fragment,z),Yn=i(z),Tt=r(z,"P",{});var Ud=n(Tt);Ba=r(Ud,"CODE",{});var jl=n(Ba);Qn=d(jl,"Enum"),jl.forEach(t),Zn=d(Ud," for how to treat pre-existing downloads and data."),Ud.forEach(t),eo=i(z),at=r(z,"P",{});var Ys=n(at);to=d(Ys,"The default mode is "),Sa=r(Ys,"CODE",{});var kl=n(Sa);ao=d(kl,"REUSE_DATASET_IF_EXISTS"),kl.forEach(t),so=d(Ys,`, which will reuse both
raw downloads and the prepared dataset if they exist.`),Ys.forEach(t),ro=i(z),Na=r(z,"P",{});var Rl=n(Na);no=d(Rl,"The generations modes:"),Rl.forEach(t),oo=i(z),st=r(z,"TABLE",{});var Qs=n(st);ja=r(Qs,"THEAD",{});var Cl=n(ja);te=r(Cl,"TR",{});var Vt=n(te);Ts=r(Vt,"TH",{}),n(Ts).forEach(t),lo=i(Vt),ka=r(Vt,"TH",{});var Ol=n(ka);io=d(Ol,"Downloads"),Ol.forEach(t),po=i(Vt),Ra=r(Vt,"TH",{});var Al=n(Ra);co=d(Al,"Dataset"),Al.forEach(t),Vt.forEach(t),Cl.forEach(t),fo=i(Qs),ae=r(Qs,"TBODY",{});var Ft=n(ae);se=r(Ft,"TR",{});var Mt=n(se);It=r(Mt,"TD",{});var qd=n(It);Ca=r(qd,"CODE",{});var Pl=n(Ca);mo=d(Pl,"REUSE_DATASET_IF_EXISTS"),Pl.forEach(t),uo=d(qd," (default)"),qd.forEach(t),go=i(Mt),Oa=r(Mt,"TD",{});var Ll=n(Oa);ho=d(Ll,"Reuse"),Ll.forEach(t),_o=i(Mt),Aa=r(Mt,"TD",{});var Vl=n(Aa);vo=d(Vl,"Reuse"),Vl.forEach(t),Mt.forEach(t),bo=i(Ft),re=r(Ft,"TR",{});var Ut=n(re);Pa=r(Ut,"TD",{});var Fl=n(Pa);La=r(Fl,"CODE",{});var Ml=n(La);$o=d(Ml,"REUSE_CACHE_IF_EXISTS"),Ml.forEach(t),Fl.forEach(t),wo=i(Ut),Va=r(Ut,"TD",{});var Ul=n(Va);Eo=d(Ul,"Reuse"),Ul.forEach(t),xo=i(Ut),Fa=r(Ut,"TD",{});var ql=n(Fa);Do=d(ql,"Fresh"),ql.forEach(t),Ut.forEach(t),yo=i(Ft),ne=r(Ft,"TR",{});var qt=n(ne);Ma=r(qt,"TD",{});var Hl=n(Ma);Ua=r(Hl,"CODE",{});var Gl=n(Ua);To=d(Gl,"FORCE_REDOWNLOAD"),Gl.forEach(t),Hl.forEach(t),Io=i(qt),qa=r(qt,"TD",{});var Wl=n(qa);Bo=d(Wl,"Fresh"),Wl.forEach(t),So=i(qt),Ha=r(qt,"TD",{});var Xl=n(Ha);No=d(Xl,"Fresh"),Xl.forEach(t),qt.forEach(t),Ft.forEach(t),Qs.forEach(t),z.forEach(t),Is=i(a),M=r(a,"DIV",{class:!0});var Ht=n(M);_(rt.$$.fragment,Ht),jo=i(Ht),Ga=r(Ht,"P",{});var Jl=n(Ga);ko=d(Jl,"Defines the split information for the generator."),Jl.forEach(t),Ro=i(Ht),oe=r(Ht,"P",{});var Gt=n(oe);Co=d(Gt,`This should be used as returned value of
`),Wa=r(Gt,"CODE",{});var zl=n(Wa);Oo=d(zl,"GeneratorBasedBuilder._split_generators()"),zl.forEach(t),Ao=d(Gt,`.
See `),Xa=r(Gt,"CODE",{});var Kl=n(Xa);Po=d(Kl,"GeneratorBasedBuilder._split_generators()"),Kl.forEach(t),Lo=d(Gt,` for more info and example
of usage.`),Gt.forEach(t),Ht.forEach(t),Bs=i(a),j=r(a,"DIV",{class:!0});var V=n(j);_(nt.$$.fragment,V),Vo=i(V),Bt=r(V,"P",{});var Hd=n(Bt);Ja=r(Hd,"CODE",{});var Yl=n(Ja);Fo=d(Yl,"Enum"),Yl.forEach(t),Mo=d(Hd," for dataset splits."),Hd.forEach(t),Uo=i(V),za=r(V,"P",{});var Ql=n(za);qo=d(Ql,`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),Ql.forEach(t),Ho=i(V),U=r(V,"UL",{});var ke=n(U);St=r(ke,"LI",{});var Gd=n(St);Ka=r(Gd,"CODE",{});var Zl=n(Ka);Go=d(Zl,"TRAIN"),Zl.forEach(t),Wo=d(Gd,": the training data."),Gd.forEach(t),Xo=i(ke),Nt=r(ke,"LI",{});var Wd=n(Nt);Ya=r(Wd,"CODE",{});var ei=n(Ya);Jo=d(ei,"VALIDATION"),ei.forEach(t),zo=d(Wd,`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Wd.forEach(t),Ko=i(ke),jt=r(ke,"LI",{});var Xd=n(jt);Qa=r(Xd,"CODE",{});var ti=n(Qa);Yo=d(ti,"TEST"),ti.forEach(t),Qo=d(Xd,`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Xd.forEach(t),Zo=i(ke),kt=r(ke,"LI",{});var Jd=n(kt);Za=r(Jd,"CODE",{});var ai=n(Za);ed=d(ai,"ALL"),ai.forEach(t),td=d(Jd,": the union of all defined dataset splits."),Jd.forEach(t),ke.forEach(t),ad=i(V),Rt=r(V,"P",{});var zd=n(Rt);sd=d(zd,"Note: All splits, including compositions inherit from "),es=r(zd,"CODE",{});var si=n(es);rd=d(si,"datasets.SplitBase"),si.forEach(t),zd.forEach(t),nd=i(V),ot=r(V,"P",{});var Zs=n(ot);od=d(Zs,"See the :doc:"),ts=r(Zs,"CODE",{});var ri=n(ts);dd=d(ri,"guide on splits </loading>"),ri.forEach(t),ld=d(Zs," for more information."),Zs.forEach(t),V.forEach(t),Ss=i(a),S=r(a,"DIV",{class:!0});var R=n(S);_(dt.$$.fragment,R),id=i(R),as=r(R,"P",{});var ni=n(as);pd=d(ni,"Descriptor corresponding to a named split (train, test, \u2026)."),ni.forEach(t),cd=i(R),_(De.$$.fragment,R),fd=i(R),ss=r(R,"P",{});var oi=n(ss);md=d(oi,"Warning:"),oi.forEach(t),ud=i(R),_(ye.$$.fragment,R),gd=i(R),rs=r(R,"P",{});var di=n(rs);hd=d(di,"Warning:"),di.forEach(t),_d=i(R),_(Te.$$.fragment,R),vd=i(R),_(Ie.$$.fragment,R),R.forEach(t),Ns=i(a),de=r(a,"DIV",{class:!0});var er=n(de);_(lt.$$.fragment,er),bd=i(er),ns=r(er,"P",{});var li=n(ns);$d=d(li,"Split corresponding to the union of all defined dataset splits."),li.forEach(t),er.forEach(t),js=i(a),O=r(a,"DIV",{class:!0});var K=n(O);_(it.$$.fragment,K),wd=i(K),os=r(K,"P",{});var ii=n(os);Ed=d(ii,"Reading instruction for a dataset."),ii.forEach(t),xd=i(K),_(Be.$$.fragment,K),Dd=i(K),Se=r(K,"DIV",{class:!0});var tr=n(Se);_(pt.$$.fragment,tr),yd=i(tr),ds=r(tr,"P",{});var pi=n(ds);Td=d(pi,"Creates a ReadInstruction instance out of a string spec."),pi.forEach(t),tr.forEach(t),Id=i(K),J=r(K,"DIV",{class:!0});var Wt=n(J);_(ct.$$.fragment,Wt),Bd=i(Wt),ls=r(Wt,"P",{});var ci=n(ls);Sd=d(ci,"Translate instruction into a list of absolute instructions."),ci.forEach(t),Nd=i(Wt),is=r(Wt,"P",{});var fi=n(is);jd=d(fi,"Those absolute instructions are then to be added together."),fi.forEach(t),Wt.forEach(t),K.forEach(t),ks=i(a),le=r(a,"DIV",{class:!0});var ar=n(le);_(ft.$$.fragment,ar),kd=i(ar),ps=r(ar,"P",{});var mi=n(ps);Rd=d(mi,"Configuration for our cached path manager."),mi.forEach(t),ar.forEach(t),Rs=i(a),q=r(a,"DIV",{class:!0});var Xt=n(q);_(mt.$$.fragment,Xt),Cd=i(Xt),cs=r(Xt,"P",{});var ui=n(cs);Od=d(ui,"Dataset version MAJOR.MINOR.PATCH."),ui.forEach(t),Ad=i(Xt),Ne=r(Xt,"DIV",{class:!0});var sr=n(Ne);_(ut.$$.fragment,sr),Pd=i(sr),fs=r(sr,"P",{});var gi=n(fs);Ld=d(gi,"Returns True if other_version matches."),gi.forEach(t),sr.forEach(t),Xt.forEach(t),this.h()},h(){E(f,"name","hf:doc:metadata"),E(f,"content",JSON.stringify(Si)),E(c,"id","datasets.DatasetBuilder"),E(c,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),E(c,"href","#datasets.DatasetBuilder"),E(x,"class","relative group"),E(_t,"href","/docs/datasets/v2.2.1/en/package_reference/builder_classes#datasets.DatasetBuilder"),E(vt,"href","/docs/datasets/v2.2.1/en/package_reference/builder_classes#datasets.BuilderConfig"),E(Et,"href","/docs/datasets/v2.2.1/en/package_reference/builder_classes#datasets.DatasetBuilder.download_and_prepare"),E(xt,"href","/docs/datasets/v2.2.1/en/package_reference/builder_classes#datasets.DatasetBuilder.as_dataset"),E(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(Dt,"href","/docs/datasets/v2.2.1/en/package_reference/builder_classes#datasets.DatasetBuilder"),E(yt,"href","/docs/datasets/v2.2.1/en/package_reference/builder_classes#datasets.BuilderConfig"),E(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(_e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E($e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(we,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(Ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(Se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(Ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),E(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(a,m){e(document.head,f),g(a,T,m),g(a,x,m),e(x,c),e(c,D),v(p,D,null),e(x,u),e(x,Yt),e(Yt,rr),g(a,vs,m),g(a,H,m),e(H,nr),e(H,_t),e(_t,or),e(H,dr),e(H,vt),e(vt,lr),e(H,ir),g(a,bs,m),g(a,I,m),v(Re,I,null),e(I,pr),e(I,Qt),e(Qt,cr),e(I,fr),e(I,bt),e(bt,Zt),e(Zt,mr),e(bt,ur),e(I,gr),e(I,Y),e(Y,$t),e($t,ea),e(ea,hr),e($t,_r),e(Y,vr),e(Y,wt),e(wt,Et),e(Et,br),e(wt,$r),e(Y,wr),e(Y,ce),e(ce,xt),e(xt,Er),e(ce,xr),e(ce,ta),e(ta,Dr),e(ce,yr),e(I,Tr),e(I,L),e(L,aa),e(aa,Ir),e(L,Br),e(L,sa),e(sa,Sr),e(L,Nr),e(L,ra),e(ra,jr),e(L,kr),e(L,na),e(na,Rr),e(L,Cr),e(I,Or),e(I,fe),v(Ce,fe,null),e(fe,Ar),e(fe,oa),e(oa,Pr),e(I,Lr),e(I,me),v(Oe,me,null),e(me,Vr),e(me,da),e(da,Fr),e(I,Mr),e(I,ue),v(Ae,ue,null),e(ue,Ur),e(ue,la),e(la,qr),e(I,Hr),e(I,ge),v(Pe,ge,null),e(ge,Gr),e(ge,ia),e(ia,Wr),e(I,Xr),e(I,he),v(Le,he,null),e(he,Jr),e(he,pa),e(pa,zr),g(a,$s,m),g(a,F,m),v(Ve,F,null),e(F,Kr),e(F,ca),e(ca,Yr),e(F,Qr),e(F,G),e(G,fa),e(fa,Zr),e(G,en),e(G,ma),e(ma,tn),e(G,an),e(G,ua),e(ua,sn),e(G,rn),g(a,ws,m),g(a,Q,m),v(Fe,Q,null),e(Q,nn),e(Q,ga),e(ga,on),g(a,Es,m),g(a,Z,m),v(Me,Z,null),e(Z,dn),e(Z,ha),e(ha,ln),g(a,xs,m),g(a,P,m),v(Ue,P,null),e(P,pn),e(P,qe),e(qe,cn),e(qe,Dt),e(Dt,fn),e(qe,mn),e(P,un),e(P,He),e(He,gn),e(He,yt),e(yt,hn),e(He,_n),e(P,vn),e(P,W),v(Ge,W,null),e(W,bn),e(W,_a),e(_a,$n),e(W,wn),e(W,ee),e(ee,va),e(va,En),e(ee,xn),e(ee,ba),e(ba,Dn),e(ee,yn),e(ee,$a),e($a,Tn),g(a,Ds,m),g(a,B,m),v(We,B,null),e(B,In),e(B,_e),v(Xe,_e,null),e(_e,Bn),e(_e,wa),e(wa,Sn),e(B,Nn),e(B,X),v(Je,X,null),e(X,jn),e(X,Ea),e(Ea,kn),e(X,Rn),v(ve,X,null),e(B,Cn),e(B,be),v(ze,be,null),e(be,On),e(be,Ke),e(Ke,An),e(Ke,xa),e(xa,Pn),e(Ke,Ln),e(B,Vn),e(B,$e),v(Ye,$e,null),e($e,Fn),e($e,Da),e(Da,Mn),e(B,Un),e(B,we),v(Qe,we,null),e(we,qn),e(we,ya),e(ya,Hn),e(B,Gn),e(B,Ee),v(Ze,Ee,null),e(Ee,Wn),e(Ee,Ta),e(Ta,Xn),e(B,Jn),e(B,xe),v(et,xe,null),e(xe,zn),e(xe,Ia),e(Ia,Kn),g(a,ys,m),g(a,C,m),v(tt,C,null),e(C,Yn),e(C,Tt),e(Tt,Ba),e(Ba,Qn),e(Tt,Zn),e(C,eo),e(C,at),e(at,to),e(at,Sa),e(Sa,ao),e(at,so),e(C,ro),e(C,Na),e(Na,no),e(C,oo),e(C,st),e(st,ja),e(ja,te),e(te,Ts),e(te,lo),e(te,ka),e(ka,io),e(te,po),e(te,Ra),e(Ra,co),e(st,fo),e(st,ae),e(ae,se),e(se,It),e(It,Ca),e(Ca,mo),e(It,uo),e(se,go),e(se,Oa),e(Oa,ho),e(se,_o),e(se,Aa),e(Aa,vo),e(ae,bo),e(ae,re),e(re,Pa),e(Pa,La),e(La,$o),e(re,wo),e(re,Va),e(Va,Eo),e(re,xo),e(re,Fa),e(Fa,Do),e(ae,yo),e(ae,ne),e(ne,Ma),e(Ma,Ua),e(Ua,To),e(ne,Io),e(ne,qa),e(qa,Bo),e(ne,So),e(ne,Ha),e(Ha,No),g(a,Is,m),g(a,M,m),v(rt,M,null),e(M,jo),e(M,Ga),e(Ga,ko),e(M,Ro),e(M,oe),e(oe,Co),e(oe,Wa),e(Wa,Oo),e(oe,Ao),e(oe,Xa),e(Xa,Po),e(oe,Lo),g(a,Bs,m),g(a,j,m),v(nt,j,null),e(j,Vo),e(j,Bt),e(Bt,Ja),e(Ja,Fo),e(Bt,Mo),e(j,Uo),e(j,za),e(za,qo),e(j,Ho),e(j,U),e(U,St),e(St,Ka),e(Ka,Go),e(St,Wo),e(U,Xo),e(U,Nt),e(Nt,Ya),e(Ya,Jo),e(Nt,zo),e(U,Ko),e(U,jt),e(jt,Qa),e(Qa,Yo),e(jt,Qo),e(U,Zo),e(U,kt),e(kt,Za),e(Za,ed),e(kt,td),e(j,ad),e(j,Rt),e(Rt,sd),e(Rt,es),e(es,rd),e(j,nd),e(j,ot),e(ot,od),e(ot,ts),e(ts,dd),e(ot,ld),g(a,Ss,m),g(a,S,m),v(dt,S,null),e(S,id),e(S,as),e(as,pd),e(S,cd),v(De,S,null),e(S,fd),e(S,ss),e(ss,md),e(S,ud),v(ye,S,null),e(S,gd),e(S,rs),e(rs,hd),e(S,_d),v(Te,S,null),e(S,vd),v(Ie,S,null),g(a,Ns,m),g(a,de,m),v(lt,de,null),e(de,bd),e(de,ns),e(ns,$d),g(a,js,m),g(a,O,m),v(it,O,null),e(O,wd),e(O,os),e(os,Ed),e(O,xd),v(Be,O,null),e(O,Dd),e(O,Se),v(pt,Se,null),e(Se,yd),e(Se,ds),e(ds,Td),e(O,Id),e(O,J),v(ct,J,null),e(J,Bd),e(J,ls),e(ls,Sd),e(J,Nd),e(J,is),e(is,jd),g(a,ks,m),g(a,le,m),v(ft,le,null),e(le,kd),e(le,ps),e(ps,Rd),g(a,Rs,m),g(a,q,m),v(mt,q,null),e(q,Cd),e(q,cs),e(cs,Od),e(q,Ad),e(q,Ne),v(ut,Ne,null),e(Ne,Pd),e(Ne,fs),e(fs,Ld),Cs=!0},p(a,[m]){const gt={};m&2&&(gt.$$scope={dirty:m,ctx:a}),ve.$set(gt);const ms={};m&2&&(ms.$$scope={dirty:m,ctx:a}),De.$set(ms);const us={};m&2&&(us.$$scope={dirty:m,ctx:a}),ye.$set(us);const gs={};m&2&&(gs.$$scope={dirty:m,ctx:a}),Te.$set(gs);const ie={};m&2&&(ie.$$scope={dirty:m,ctx:a}),Ie.$set(ie);const hs={};m&2&&(hs.$$scope={dirty:m,ctx:a}),Be.$set(hs)},i(a){Cs||(b(p.$$.fragment,a),b(Re.$$.fragment,a),b(Ce.$$.fragment,a),b(Oe.$$.fragment,a),b(Ae.$$.fragment,a),b(Pe.$$.fragment,a),b(Le.$$.fragment,a),b(Ve.$$.fragment,a),b(Fe.$$.fragment,a),b(Me.$$.fragment,a),b(Ue.$$.fragment,a),b(Ge.$$.fragment,a),b(We.$$.fragment,a),b(Xe.$$.fragment,a),b(Je.$$.fragment,a),b(ve.$$.fragment,a),b(ze.$$.fragment,a),b(Ye.$$.fragment,a),b(Qe.$$.fragment,a),b(Ze.$$.fragment,a),b(et.$$.fragment,a),b(tt.$$.fragment,a),b(rt.$$.fragment,a),b(nt.$$.fragment,a),b(dt.$$.fragment,a),b(De.$$.fragment,a),b(ye.$$.fragment,a),b(Te.$$.fragment,a),b(Ie.$$.fragment,a),b(lt.$$.fragment,a),b(it.$$.fragment,a),b(Be.$$.fragment,a),b(pt.$$.fragment,a),b(ct.$$.fragment,a),b(ft.$$.fragment,a),b(mt.$$.fragment,a),b(ut.$$.fragment,a),Cs=!0)},o(a){$(p.$$.fragment,a),$(Re.$$.fragment,a),$(Ce.$$.fragment,a),$(Oe.$$.fragment,a),$(Ae.$$.fragment,a),$(Pe.$$.fragment,a),$(Le.$$.fragment,a),$(Ve.$$.fragment,a),$(Fe.$$.fragment,a),$(Me.$$.fragment,a),$(Ue.$$.fragment,a),$(Ge.$$.fragment,a),$(We.$$.fragment,a),$(Xe.$$.fragment,a),$(Je.$$.fragment,a),$(ve.$$.fragment,a),$(ze.$$.fragment,a),$(Ye.$$.fragment,a),$(Qe.$$.fragment,a),$(Ze.$$.fragment,a),$(et.$$.fragment,a),$(tt.$$.fragment,a),$(rt.$$.fragment,a),$(nt.$$.fragment,a),$(dt.$$.fragment,a),$(De.$$.fragment,a),$(ye.$$.fragment,a),$(Te.$$.fragment,a),$(Ie.$$.fragment,a),$(lt.$$.fragment,a),$(it.$$.fragment,a),$(Be.$$.fragment,a),$(pt.$$.fragment,a),$(ct.$$.fragment,a),$(ft.$$.fragment,a),$(mt.$$.fragment,a),$(ut.$$.fragment,a),Cs=!1},d(a){t(f),a&&t(T),a&&t(x),w(p),a&&t(vs),a&&t(H),a&&t(bs),a&&t(I),w(Re),w(Ce),w(Oe),w(Ae),w(Pe),w(Le),a&&t($s),a&&t(F),w(Ve),a&&t(ws),a&&t(Q),w(Fe),a&&t(Es),a&&t(Z),w(Me),a&&t(xs),a&&t(P),w(Ue),w(Ge),a&&t(Ds),a&&t(B),w(We),w(Xe),w(Je),w(ve),w(ze),w(Ye),w(Qe),w(Ze),w(et),a&&t(ys),a&&t(C),w(tt),a&&t(Is),a&&t(M),w(rt),a&&t(Bs),a&&t(j),w(nt),a&&t(Ss),a&&t(S),w(dt),w(De),w(ye),w(Te),w(Ie),a&&t(Ns),a&&t(de),w(lt),a&&t(js),a&&t(O),w(it),w(Be),w(pt),w(ct),a&&t(ks),a&&t(le),w(ft),a&&t(Rs),a&&t(q),w(mt),w(ut)}}}const Si={local:"datasets.DatasetBuilder",title:"Builder classes"};function Ni(A){return $i(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ai extends hi{constructor(f){super();_i(this,f,Ni,Bi,vi,{})}}export{Ai as default,Si as metadata};
