import{S as Pe,i as Be,s as Le,e as o,k as g,w as y,t as r,M as Ne,c,d as e,m as $,a as i,x,h as p,b as j,F as a,g as d,y as v,q as E,o as T,B as A,L as Ks}from"../chunks/vendor-aa873a46.js";import{T as Re}from"../chunks/Tip-f7f252ab.js";import{I as Xs}from"../chunks/IconCopyLink-d0ca3106.js";import{C as gs}from"../chunks/CodeBlock-1f14baf3.js";import{F as le,M as Vs}from"../chunks/Markdown-28d14b5b.js";import"../chunks/IconTensorflow-b9816778.js";function Oe(C){let t,l,n,h,b;return{c(){t=o("p"),l=r("For more detailed information on loading and processing a dataset, take a look at "),n=o("a"),h=r("Chapter 3"),b=r(" of the Hugging Face course! It covers additional important topics like dynamic padding, and fine-tuning with the Trainer API."),this.h()},l(f){t=c(f,"P",{});var _=i(t);l=p(_,"For more detailed information on loading and processing a dataset, take a look at "),n=c(_,"A",{href:!0,rel:!0});var G=i(n);h=p(G,"Chapter 3"),G.forEach(e),b=p(_," of the Hugging Face course! It covers additional important topics like dynamic padding, and fine-tuning with the Trainer API."),_.forEach(e),this.h()},h(){j(n,"href","https://huggingface.co/course/chapter3/1?fw=pt"),j(n,"rel","nofollow")},m(f,_){d(f,t,_),a(t,l),a(t,n),a(n,h),a(t,b)},d(f){f&&e(t)}}}function Ie(C){let t,l;return t=new gs({props:{code:`from transformers import AutoModelForSequenceClassification, AutoTokenizer
model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased')
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification, AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
Some weights of the model checkpoint at bert-base-cased were <span class="hljs-keyword">not</span> used when initializing BertForSequenceClassification: [<span class="hljs-string">&#x27;cls.predictions.bias&#x27;</span>, <span class="hljs-string">&#x27;cls.predictions.transform.dense.weight&#x27;</span>, <span class="hljs-string">&#x27;cls.predictions.transform.dense.bias&#x27;</span>, <span class="hljs-string">&#x27;cls.predictions.decoder.weight&#x27;</span>, <span class="hljs-string">&#x27;cls.seq_relationship.weight&#x27;</span>, <span class="hljs-string">&#x27;cls.seq_relationship.bias&#x27;</span>, <span class="hljs-string">&#x27;cls.predictions.transform.LayerNorm.weight&#x27;</span>, <span class="hljs-string">&#x27;cls.predictions.transform.LayerNorm.bias&#x27;</span>]
- This IS expected <span class="hljs-keyword">if</span> you are initializing BertForSequenceClassification <span class="hljs-keyword">from</span> the checkpoint of a model trained on another task <span class="hljs-keyword">or</span> <span class="hljs-keyword">with</span> another architecture (e.g. initializing a BertForSequenceClassification model <span class="hljs-keyword">from</span> a BertForPretraining model).
- This IS NOT expected <span class="hljs-keyword">if</span> you are initializing BertForSequenceClassification <span class="hljs-keyword">from</span> the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model <span class="hljs-keyword">from</span> a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were <span class="hljs-keyword">not</span> initialized <span class="hljs-keyword">from</span> the model checkpoint at bert-base-cased <span class="hljs-keyword">and</span> are newly initialized: [<span class="hljs-string">&#x27;classifier.weight&#x27;</span>, <span class="hljs-string">&#x27;classifier.bias&#x27;</span>]
You should probably TRAIN this model on a down-stream task to be able to use it <span class="hljs-keyword">for</span> predictions <span class="hljs-keyword">and</span> inference.
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)`}}),{c(){y(t.$$.fragment)},l(n){x(t.$$.fragment,n)},m(n,h){v(t,n,h),l=!0},p:Ks,i(n){l||(E(t.$$.fragment,n),l=!0)},o(n){T(t.$$.fragment,n),l=!1},d(n){A(t,n)}}}function Me(C){let t,l;return t=new Vs({props:{$$slots:{default:[Ie]},$$scope:{ctx:C}}}),{c(){y(t.$$.fragment)},l(n){x(t.$$.fragment,n)},m(n,h){v(t,n,h),l=!0},p(n,h){const b={};h&2&&(b.$$scope={dirty:h,ctx:n}),t.$set(b)},i(n){l||(E(t.$$.fragment,n),l=!0)},o(n){T(t.$$.fragment,n),l=!1},d(n){A(t,n)}}}function He(C){let t,l;return t=new gs({props:{code:`from transformers import TFAutoModelForSequenceClassification, AutoTokenizer
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification, AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
Some weights of the model checkpoint at bert-base-cased were <span class="hljs-keyword">not</span> used when initializing TFBertForSequenceClassification: [<span class="hljs-string">&#x27;nsp___cls&#x27;</span>, <span class="hljs-string">&#x27;mlm___cls&#x27;</span>]
- This IS expected <span class="hljs-keyword">if</span> you are initializing TFBertForSequenceClassification <span class="hljs-keyword">from</span> the checkpoint of a model trained on another task <span class="hljs-keyword">or</span> <span class="hljs-keyword">with</span> another architecture (e.g. initializing a BertForSequenceClassification model <span class="hljs-keyword">from</span> a BertForPretraining model).
- This IS NOT expected <span class="hljs-keyword">if</span> you are initializing TFBertForSequenceClassification <span class="hljs-keyword">from</span> the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model <span class="hljs-keyword">from</span> a BertForSequenceClassification model).
Some weights of TFBertForSequenceClassification were <span class="hljs-keyword">not</span> initialized <span class="hljs-keyword">from</span> the model checkpoint at bert-base-cased <span class="hljs-keyword">and</span> are newly initialized: [<span class="hljs-string">&#x27;dropout_37&#x27;</span>, <span class="hljs-string">&#x27;classifier&#x27;</span>]
You should probably TRAIN this model on a down-stream task to be able to use it <span class="hljs-keyword">for</span> predictions <span class="hljs-keyword">and</span> inference.
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)`}}),{c(){y(t.$$.fragment)},l(n){x(t.$$.fragment,n)},m(n,h){v(t,n,h),l=!0},p:Ks,i(n){l||(E(t.$$.fragment,n),l=!0)},o(n){T(t.$$.fragment,n),l=!1},d(n){A(t,n)}}}function Ge(C){let t,l;return t=new Vs({props:{$$slots:{default:[He]},$$scope:{ctx:C}}}),{c(){y(t.$$.fragment)},l(n){x(t.$$.fragment,n)},m(n,h){v(t,n,h),l=!0},p(n,h){const b={};h&2&&(b.$$scope={dirty:h,ctx:n}),t.$set(b)},i(n){l||(E(t.$$.fragment,n),l=!0)},o(n){T(t.$$.fragment,n),l=!1},d(n){A(t,n)}}}function We(C){let t,l,n,h,b,f,_,G,ms,B,W,us,Q,N,as,F,L,ds,X,k,fs,U,Y,ps,O,js,os,R,cs,H,I,q,S,bs,es,K,hs,M,ns,z,ts;return N=new gs({props:{code:"dataset = dataset.map(lambda examples: {'labels': examples['label']}, batched=True)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> examples: {<span class="hljs-string">&#x27;labels&#x27;</span>: examples[<span class="hljs-string">&#x27;label&#x27;</span>]}, batched=<span class="hljs-literal">True</span>)'}}),z=new gs({props:{code:`import torch
dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])
dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)
next(iter(dataloader))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_format(<span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;torch&#x27;</span>, columns=[<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = torch.utils.data.DataLoader(dataset, batch_size=<span class="hljs-number">32</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataloader))
{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                        ...,
                        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]),
<span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([[  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                  [  <span class="hljs-number">101</span>, <span class="hljs-number">10684</span>,  <span class="hljs-number">2599</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                  [  <span class="hljs-number">101</span>,  <span class="hljs-number">1220</span>,  <span class="hljs-number">1125</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                  ...,
                  [  <span class="hljs-number">101</span>, <span class="hljs-number">16944</span>,  <span class="hljs-number">1107</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                  [  <span class="hljs-number">101</span>,  <span class="hljs-number">1109</span>, <span class="hljs-number">11896</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                  [  <span class="hljs-number">101</span>,  <span class="hljs-number">1109</span>,  <span class="hljs-number">4173</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]]),
<span class="hljs-string">&#x27;label&#x27;</span>: tensor([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]),
<span class="hljs-string">&#x27;token_type_ids&#x27;</span>: tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     ...,
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])}`}}),{c(){t=o("ol"),l=o("li"),n=r("Rename the "),h=o("code"),b=r("label"),f=r(" column to "),_=o("code"),G=r("labels"),ms=r(", the expected input name in "),B=o("a"),W=r("BertForSequenceClassification"),us=r(":"),Q=g(),y(N.$$.fragment),as=g(),F=o("ol"),L=o("li"),ds=r("Retrieve the actual tensors from the Dataset object instead of using the current Python objects."),X=g(),k=o("li"),fs=r("Filter the dataset to only return the model inputs: "),U=o("code"),Y=r("input_ids"),ps=r(", "),O=o("code"),js=r("token_type_ids"),os=r(", and "),R=o("code"),cs=r("attention_mask"),H=r("."),I=g(),q=o("p"),S=o("a"),bs=r("datasets.Dataset.set_format()"),es=r(" completes the last two steps on-the-fly. After you set the format, wrap the dataset in "),K=o("code"),hs=r("torch.utils.data.DataLoader"),M=r(":"),ns=g(),y(z.$$.fragment),this.h()},l(m){t=c(m,"OL",{});var w=i(t);l=c(w,"LI",{});var D=i(l);n=p(D,"Rename the "),h=c(D,"CODE",{});var ks=i(h);b=p(ks,"label"),ks.forEach(e),f=p(D," column to "),_=c(D,"CODE",{});var $s=i(_);G=p($s,"labels"),$s.forEach(e),ms=p(D,", the expected input name in "),B=c(D,"A",{href:!0,rel:!0});var ls=i(B);W=p(ls,"BertForSequenceClassification"),ls.forEach(e),us=p(D,":"),D.forEach(e),w.forEach(e),Q=$(m),x(N.$$.fragment,m),as=$(m),F=c(m,"OL",{start:!0});var V=i(F);L=c(V,"LI",{});var Z=i(L);ds=p(Z,"Retrieve the actual tensors from the Dataset object instead of using the current Python objects."),Z.forEach(e),X=$(V),k=c(V,"LI",{});var P=i(k);fs=p(P,"Filter the dataset to only return the model inputs: "),U=c(P,"CODE",{});var rs=i(U);Y=p(rs,"input_ids"),rs.forEach(e),ps=p(P,", "),O=c(P,"CODE",{});var ys=i(O);js=p(ys,"token_type_ids"),ys.forEach(e),os=p(P,", and "),R=c(P,"CODE",{});var xs=i(R);cs=p(xs,"attention_mask"),xs.forEach(e),H=p(P,"."),P.forEach(e),V.forEach(e),I=$(m),q=c(m,"P",{});var J=i(q);S=c(J,"A",{href:!0});var ss=i(S);bs=p(ss,"datasets.Dataset.set_format()"),ss.forEach(e),es=p(J," completes the last two steps on-the-fly. After you set the format, wrap the dataset in "),K=c(J,"CODE",{});var _s=i(K);hs=p(_s,"torch.utils.data.DataLoader"),_s.forEach(e),M=p(J,":"),J.forEach(e),ns=$(m),x(z.$$.fragment,m),this.h()},h(){j(B,"href","https://huggingface.co/transformers/model_doc/bert#transformers.BertForSequenceClassification.forward"),j(B,"rel","nofollow"),j(F,"start","2"),j(S,"href","/docs/datasets/v2.0.0/en/package_reference/main_classes#datasets.Dataset.set_format")},m(m,w){d(m,t,w),a(t,l),a(l,n),a(l,h),a(h,b),a(l,f),a(l,_),a(_,G),a(l,ms),a(l,B),a(B,W),a(l,us),d(m,Q,w),v(N,m,w),d(m,as,w),d(m,F,w),a(F,L),a(L,ds),a(F,X),a(F,k),a(k,fs),a(k,U),a(U,Y),a(k,ps),a(k,O),a(O,js),a(k,os),a(k,R),a(R,cs),a(k,H),d(m,I,w),d(m,q,w),a(q,S),a(S,bs),a(q,es),a(q,K),a(K,hs),a(q,M),d(m,ns,w),v(z,m,w),ts=!0},p:Ks,i(m){ts||(E(N.$$.fragment,m),E(z.$$.fragment,m),ts=!0)},o(m){T(N.$$.fragment,m),T(z.$$.fragment,m),ts=!1},d(m){m&&e(t),m&&e(Q),A(N,m),m&&e(as),m&&e(F),m&&e(I),m&&e(q),m&&e(ns),A(z,m)}}}function Qe(C){let t,l;return t=new Vs({props:{$$slots:{default:[We]},$$scope:{ctx:C}}}),{c(){y(t.$$.fragment)},l(n){x(t.$$.fragment,n)},m(n,h){v(t,n,h),l=!0},p(n,h){const b={};h&2&&(b.$$scope={dirty:h,ctx:n}),t.$set(b)},i(n){l||(E(t.$$.fragment,n),l=!0)},o(n){T(t.$$.fragment,n),l=!1},d(n){A(t,n)}}}function Ue(C){let t,l,n,h,b,f,_,G,ms,B,W,us,Q,N,as,F,L,ds,X,k,fs,U,Y,ps,O,js,os,R,cs,H,I,q,S,bs,es,K,hs,M,ns,z,ts;return N=new gs({props:{code:"dataset = dataset.map(lambda examples: {'labels': examples['label']}, batched=True)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> examples: {<span class="hljs-string">&#x27;labels&#x27;</span>: examples[<span class="hljs-string">&#x27;label&#x27;</span>]}, batched=<span class="hljs-literal">True</span>)'}}),z=new gs({props:{code:`import tensorflow as tf
dataset.set_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])
features = {x: dataset[x].to_tensor(default_value=0, shape=[None, tokenizer.model_max_length]) for x in ['input_ids', 'token_type_ids', 'attention_mask']}
tfdataset = tf.data.Dataset.from_tensor_slices((features, dataset["labels"])).batch(32)
next(iter(tfdataset))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_format(<span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;tensorflow&#x27;</span>, columns=[<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>features = {x: dataset[x].to_tensor(default_value=<span class="hljs-number">0</span>, shape=[<span class="hljs-literal">None</span>, tokenizer.model_max_length]) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>tfdataset = tf.data.Dataset.from_tensor_slices((features, dataset[<span class="hljs-string">&quot;labels&quot;</span>])).batch(<span class="hljs-number">32</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(tfdataset))
({<span class="hljs-string">&#x27;input_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">32</span>, <span class="hljs-number">512</span>), dtype=int32, numpy=
array([[  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
   [  <span class="hljs-number">101</span>, <span class="hljs-number">10684</span>,  <span class="hljs-number">2599</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
   [  <span class="hljs-number">101</span>,  <span class="hljs-number">1220</span>,  <span class="hljs-number">1125</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
   ...,
   [  <span class="hljs-number">101</span>,  <span class="hljs-number">1109</span>,  <span class="hljs-number">2026</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
   [  <span class="hljs-number">101</span>, <span class="hljs-number">22263</span>,  <span class="hljs-number">1107</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
   [  <span class="hljs-number">101</span>,   <span class="hljs-number">142</span>,  <span class="hljs-number">1813</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]], dtype=int32)&gt;, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">32</span>, <span class="hljs-number">512</span>), dtype=int32, numpy=
array([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   ...,
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], dtype=int32)&gt;, <span class="hljs-string">&#x27;attention_mask&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">32</span>, <span class="hljs-number">512</span>), dtype=int32, numpy=
array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   ...,
   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], dtype=int32)&gt;}, &lt;tf.Tensor: shape=(<span class="hljs-number">32</span>,), dtype=int64, numpy=
array([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,
   <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>])&gt;)`}}),{c(){t=o("ol"),l=o("li"),n=r("Rename the "),h=o("code"),b=r("label"),f=r(" column to "),_=o("code"),G=r("labels"),ms=r(", the expected input name in "),B=o("a"),W=r("TFBertForSequenceClassification"),us=r(":"),Q=g(),y(N.$$.fragment),as=g(),F=o("ol"),L=o("li"),ds=r("Retrieve the actual tensors from the Dataset object instead of using the current Python objects."),X=g(),k=o("li"),fs=r("Filter the dataset to only return the model inputs: "),U=o("code"),Y=r("input_ids"),ps=r(", "),O=o("code"),js=r("token_type_ids"),os=r(", and "),R=o("code"),cs=r("attention_mask"),H=r("."),I=g(),q=o("p"),S=o("a"),bs=r("datasets.Dataset.set_format()"),es=r(" completes the last two steps on-the-fly. After you set the format, wrap the dataset in "),K=o("code"),hs=r("tf.data.Dataset"),M=r(":"),ns=g(),y(z.$$.fragment),this.h()},l(m){t=c(m,"OL",{});var w=i(t);l=c(w,"LI",{});var D=i(l);n=p(D,"Rename the "),h=c(D,"CODE",{});var ks=i(h);b=p(ks,"label"),ks.forEach(e),f=p(D," column to "),_=c(D,"CODE",{});var $s=i(_);G=p($s,"labels"),$s.forEach(e),ms=p(D,", the expected input name in "),B=c(D,"A",{href:!0,rel:!0});var ls=i(B);W=p(ls,"TFBertForSequenceClassification"),ls.forEach(e),us=p(D,":"),D.forEach(e),w.forEach(e),Q=$(m),x(N.$$.fragment,m),as=$(m),F=c(m,"OL",{start:!0});var V=i(F);L=c(V,"LI",{});var Z=i(L);ds=p(Z,"Retrieve the actual tensors from the Dataset object instead of using the current Python objects."),Z.forEach(e),X=$(V),k=c(V,"LI",{});var P=i(k);fs=p(P,"Filter the dataset to only return the model inputs: "),U=c(P,"CODE",{});var rs=i(U);Y=p(rs,"input_ids"),rs.forEach(e),ps=p(P,", "),O=c(P,"CODE",{});var ys=i(O);js=p(ys,"token_type_ids"),ys.forEach(e),os=p(P,", and "),R=c(P,"CODE",{});var xs=i(R);cs=p(xs,"attention_mask"),xs.forEach(e),H=p(P,"."),P.forEach(e),V.forEach(e),I=$(m),q=c(m,"P",{});var J=i(q);S=c(J,"A",{href:!0});var ss=i(S);bs=p(ss,"datasets.Dataset.set_format()"),ss.forEach(e),es=p(J," completes the last two steps on-the-fly. After you set the format, wrap the dataset in "),K=c(J,"CODE",{});var _s=i(K);hs=p(_s,"tf.data.Dataset"),_s.forEach(e),M=p(J,":"),J.forEach(e),ns=$(m),x(z.$$.fragment,m),this.h()},h(){j(B,"href","https://huggingface.co/transformers/model_doc/bert#tfbertforsequenceclassification"),j(B,"rel","nofollow"),j(F,"start","2"),j(S,"href","/docs/datasets/v2.0.0/en/package_reference/main_classes#datasets.Dataset.set_format")},m(m,w){d(m,t,w),a(t,l),a(l,n),a(l,h),a(h,b),a(l,f),a(l,_),a(_,G),a(l,ms),a(l,B),a(B,W),a(l,us),d(m,Q,w),v(N,m,w),d(m,as,w),d(m,F,w),a(F,L),a(L,ds),a(F,X),a(F,k),a(k,fs),a(k,U),a(U,Y),a(k,ps),a(k,O),a(O,js),a(k,os),a(k,R),a(R,cs),a(k,H),d(m,I,w),d(m,q,w),a(q,S),a(S,bs),a(q,es),a(q,K),a(K,hs),a(q,M),d(m,ns,w),v(z,m,w),ts=!0},p:Ks,i(m){ts||(E(N.$$.fragment,m),E(z.$$.fragment,m),ts=!0)},o(m){T(N.$$.fragment,m),T(z.$$.fragment,m),ts=!1},d(m){m&&e(t),m&&e(Q),A(N,m),m&&e(as),m&&e(F),m&&e(I),m&&e(q),m&&e(ns),A(z,m)}}}function Ye(C){let t,l;return t=new Vs({props:{$$slots:{default:[Ue]},$$scope:{ctx:C}}}),{c(){y(t.$$.fragment)},l(n){x(t.$$.fragment,n)},m(n,h){v(t,n,h),l=!0},p(n,h){const b={};h&2&&(b.$$scope={dirty:h,ctx:n}),t.$set(b)},i(n){l||(E(t.$$.fragment,n),l=!0)},o(n){T(t.$$.fragment,n),l=!1},d(n){A(t,n)}}}function Je(C){let t,l,n,h,b;return h=new gs({props:{code:`from tqdm import tqdm
device = 'cuda' if torch.cuda.is_available() else 'cpu' 
model.train().to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)
for epoch in range(3):
    for i, batch in enumerate(tqdm(dataloader)):
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs[0]
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        if i % 10 == 0:
            print(f"loss: {loss}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span> 
<span class="hljs-meta">&gt;&gt;&gt; </span>model.train().to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = torch.optim.AdamW(params=model.parameters(), lr=<span class="hljs-number">1e-5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tqdm(dataloader)):
<span class="hljs-meta">... </span>        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
<span class="hljs-meta">... </span>        outputs = model(**batch)
<span class="hljs-meta">... </span>        loss = outputs[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>        loss.backward()
<span class="hljs-meta">... </span>        optimizer.step()
<span class="hljs-meta">... </span>        optimizer.zero_grad()
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
<span class="hljs-meta">... </span>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;loss: <span class="hljs-subst">{loss}</span>&quot;</span>)`}}),{c(){t=o("p"),l=r("Lastly, create a simple training loop and start training:"),n=g(),y(h.$$.fragment)},l(f){t=c(f,"P",{});var _=i(t);l=p(_,"Lastly, create a simple training loop and start training:"),_.forEach(e),n=$(f),x(h.$$.fragment,f)},m(f,_){d(f,t,_),a(t,l),d(f,n,_),v(h,f,_),b=!0},p:Ks,i(f){b||(E(h.$$.fragment,f),b=!0)},o(f){T(h.$$.fragment,f),b=!1},d(f){f&&e(t),f&&e(n),A(h,f)}}}function Xe(C){let t,l;return t=new Vs({props:{$$slots:{default:[Je]},$$scope:{ctx:C}}}),{c(){y(t.$$.fragment)},l(n){x(t.$$.fragment,n)},m(n,h){v(t,n,h),l=!0},p(n,h){const b={};h&2&&(b.$$scope={dirty:h,ctx:n}),t.$set(b)},i(n){l||(E(t.$$.fragment,n),l=!0)},o(n){T(t.$$.fragment,n),l=!1},d(n){A(t,n)}}}function Ke(C){let t,l,n,h,b;return h=new gs({props:{code:`loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits=True)
opt = tf.keras.optimizers.Adam(learning_rate=3e-5)
model.compile(optimizer=opt, loss=loss_fn, metrics=["accuracy"])
model.fit(tfdataset, epochs=3)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>opt = tf.keras.optimizers.Adam(learning_rate=<span class="hljs-number">3e-5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=opt, loss=loss_fn, metrics=[<span class="hljs-string">&quot;accuracy&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(tfdataset, epochs=<span class="hljs-number">3</span>)`}}),{c(){t=o("p"),l=r("Lastly, compile the model and start training:"),n=g(),y(h.$$.fragment)},l(f){t=c(f,"P",{});var _=i(t);l=p(_,"Lastly, compile the model and start training:"),_.forEach(e),n=$(f),x(h.$$.fragment,f)},m(f,_){d(f,t,_),a(t,l),d(f,n,_),v(h,f,_),b=!0},p:Ks,i(f){b||(E(h.$$.fragment,f),b=!0)},o(f){T(h.$$.fragment,f),b=!1},d(f){f&&e(t),f&&e(n),A(h,f)}}}function Ve(C){let t,l;return t=new Vs({props:{$$slots:{default:[Ke]},$$scope:{ctx:C}}}),{c(){y(t.$$.fragment)},l(n){x(t.$$.fragment,n)},m(n,h){v(t,n,h),l=!0},p(n,h){const b={};h&2&&(b.$$scope={dirty:h,ctx:n}),t.$set(b)},i(n){l||(E(t.$$.fragment,n),l=!0)},o(n){T(t.$$.fragment,n),l=!1},d(n){A(t,n)}}}function Ze(C){let t,l,n,h,b,f,_,G,ms,B,W,us,Q,N,as,F,L,ds,X,k,fs,U,Y,ps,O,js,os,R,cs,H,I,q,S,bs,es,K,hs,M,ns,z,ts,m,w,D,ks,$s,ls,V,Z,P,rs,ys,xs,J,ss,_s,vs,Cs,Zs,Ls,Da,sa,Pa,ua,qs,Ba,Hs,La,Na,da,Ns,fa,is,Ra,aa,Oa,Ia,ea,Ma,Ha,na,Ga,Wa,ja,Es,Fs,ta,Rs,Qa,la,Ua,ba,Gs,Ya,ga,zs,$a,Ts,Ss,ra,Os,Ja,pa,Xa,_a,Ds,wa,As,Ps,oa,Is,Ka,ca,Va,ka,Ws,Za,ya,ws,se,Qs,ae,ee,Us,ne,te,xa;return f=new Xs({}),Y=new Re({props:{$$slots:{default:[Oe]},$$scope:{ctx:C}}}),R=new gs({props:{code:"pip install datasets",highlighted:'pip <span class="hljs-keyword">install</span> datasets'}}),S=new Xs({}),ls=new gs({props:{code:`from datasets import load_dataset
dataset = load_dataset('glue', 'mrpc', split='train')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)`}}),ss=new le({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ge],pytorch:[Me]},$$scope:{ctx:C}}}),Ls=new Xs({}),Ns=new gs({props:{code:`def encode(examples):
    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length')

dataset = dataset.map(encode, batched=True)
dataset[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&#x27;sentence1&#x27;</span>], examples[<span class="hljs-string">&#x27;sentence2&#x27;</span>], truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(encode, batched=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
<span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-number">0</span>,
<span class="hljs-string">&#x27;input_ids&#x27;</span>: array([  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">2292</span>, <span class="hljs-number">1119</span>,  <span class="hljs-number">1270</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>, <span class="hljs-number">11336</span>,  <span class="hljs-number">6732</span>, <span class="hljs-number">3384</span>,  <span class="hljs-number">1106</span>,  <span class="hljs-number">1140</span>,  <span class="hljs-number">1112</span>,  <span class="hljs-number">1178</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>, <span class="hljs-number">117</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>]),
<span class="hljs-string">&#x27;token_type_ids&#x27;</span>: array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]),
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])}`}}),Rs=new Xs({}),zs=new le({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ye],pytorch:[Qe]},$$scope:{ctx:C}}}),Os=new Xs({}),Ds=new le({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ve],pytorch:[Xe]},$$scope:{ctx:C}}}),Is=new Xs({}),{c(){t=o("meta"),l=g(),n=o("h1"),h=o("a"),b=o("span"),y(f.$$.fragment),_=g(),G=o("span"),ms=r("Quick Start"),B=g(),W=o("p"),us=r("The quick start is intended for developers who are ready to dive in to the code, and see an end-to-end example of how they can integrate \u{1F917} Datasets into their model training workflow. For beginners who are looking for a gentler introduction, we recommend you begin with the "),Q=o("a"),N=r("tutorials"),as=r("."),F=g(),L=o("p"),ds=r("In the quick start, you will walkthrough all the steps to fine-tune "),X=o("a"),k=r("BERT"),fs=r(" on a paraphrase classification task. Depending on the specific dataset you use, these steps may vary, but the general steps of how to load a dataset and process it are the same."),U=g(),y(Y.$$.fragment),ps=g(),O=o("p"),js=r("Get started by installing \u{1F917} Datasets:"),os=g(),y(R.$$.fragment),cs=g(),H=o("h2"),I=o("a"),q=o("span"),y(S.$$.fragment),bs=g(),es=o("span"),K=r("Load the dataset and model"),hs=g(),M=o("p"),ns=r("Begin by loading the "),z=o("a"),ts=r("Microsoft Research Paraphrase Corpus (MRPC)"),m=r(" training dataset from the "),w=o("a"),D=r("General Language Understanding Evaluation (GLUE) benchmark"),ks=r(". MRPC is a corpus of human annotated sentence pairs used to train a model to determine whether sentence pairs are semantically equivalent."),$s=g(),y(ls.$$.fragment),V=g(),Z=o("p"),P=r("Next, import the pre-trained BERT model and its tokenizer from the "),rs=o("a"),ys=r("\u{1F917} Transformers"),xs=r(" library:"),J=g(),y(ss.$$.fragment),_s=g(),vs=o("h2"),Cs=o("a"),Zs=o("span"),y(Ls.$$.fragment),Da=g(),sa=o("span"),Pa=r("Tokenize the dataset"),ua=g(),qs=o("p"),Ba=r("The next step is to tokenize the text in order to build sequences of integers the model can understand. Encode the entire dataset with "),Hs=o("a"),La=r("datasets.Dataset.map()"),Na=r(", and truncate and pad the inputs to the maximum length of the model. This ensures the appropriate tensor batches are built."),da=g(),y(Ns.$$.fragment),fa=g(),is=o("p"),Ra=r("Notice how there are three new columns in the dataset: "),aa=o("code"),Oa=r("input_ids"),Ia=r(", "),ea=o("code"),Ma=r("token_type_ids"),Ha=r(", and "),na=o("code"),Ga=r("attention_mask"),Wa=r(". These columns are the inputs to the model."),ja=g(),Es=o("h2"),Fs=o("a"),ta=o("span"),y(Rs.$$.fragment),Qa=g(),la=o("span"),Ua=r("Format the dataset"),ba=g(),Gs=o("p"),Ya=r("Depending on whether you are using PyTorch, TensorFlow, or JAX, you will need to format the dataset accordingly. There are three changes you need to make to the dataset:"),ga=g(),y(zs.$$.fragment),$a=g(),Ts=o("h2"),Ss=o("a"),ra=o("span"),y(Os.$$.fragment),Ja=g(),pa=o("span"),Xa=r("Train the model"),_a=g(),y(Ds.$$.fragment),wa=g(),As=o("h2"),Ps=o("a"),oa=o("span"),y(Is.$$.fragment),Ka=g(),ca=o("span"),Va=r("What's next?"),ka=g(),Ws=o("p"),Za=r("This completes the basic steps of loading a dataset to train a model. You loaded and processed the MRPC dataset to fine-tune BERT to determine whether sentence pairs have the same meaning."),ya=g(),ws=o("p"),se=r("For your next steps, take a look at our "),Qs=o("a"),ae=r("How-to guides"),ee=r(" and learn how to achieve a specific task (e.g. load a dataset offline, add a dataset to the Hub, change the name of a column). Or if you want to deepen your knowledge of \u{1F917} Datasets core concepts, read our "),Us=o("a"),ne=r("Conceptual Guides"),te=r("."),this.h()},l(s){const u=Ne('[data-svelte="svelte-1phssyn"]',document.head);t=c(u,"META",{name:!0,content:!0}),u.forEach(e),l=$(s),n=c(s,"H1",{class:!0});var Ms=i(n);h=c(Ms,"A",{id:!0,class:!0,href:!0});var ha=i(h);b=c(ha,"SPAN",{});var ia=i(b);x(f.$$.fragment,ia),ia.forEach(e),ha.forEach(e),_=$(Ms),G=c(Ms,"SPAN",{});var ma=i(G);ms=p(ma,"Quick Start"),ma.forEach(e),Ms.forEach(e),B=$(s),W=c(s,"P",{});var va=i(W);us=p(va,"The quick start is intended for developers who are ready to dive in to the code, and see an end-to-end example of how they can integrate \u{1F917} Datasets into their model training workflow. For beginners who are looking for a gentler introduction, we recommend you begin with the "),Q=c(va,"A",{href:!0});var re=i(Q);N=p(re,"tutorials"),re.forEach(e),as=p(va,"."),va.forEach(e),F=$(s),L=c(s,"P",{});var Ea=i(L);ds=p(Ea,"In the quick start, you will walkthrough all the steps to fine-tune "),X=c(Ea,"A",{href:!0,rel:!0});var pe=i(X);k=p(pe,"BERT"),pe.forEach(e),fs=p(Ea," on a paraphrase classification task. Depending on the specific dataset you use, these steps may vary, but the general steps of how to load a dataset and process it are the same."),Ea.forEach(e),U=$(s),x(Y.$$.fragment,s),ps=$(s),O=c(s,"P",{});var oe=i(O);js=p(oe,"Get started by installing \u{1F917} Datasets:"),oe.forEach(e),os=$(s),x(R.$$.fragment,s),cs=$(s),H=c(s,"H2",{class:!0});var Ta=i(H);I=c(Ta,"A",{id:!0,class:!0,href:!0});var ce=i(I);q=c(ce,"SPAN",{});var he=i(q);x(S.$$.fragment,he),he.forEach(e),ce.forEach(e),bs=$(Ta),es=c(Ta,"SPAN",{});var ie=i(es);K=p(ie,"Load the dataset and model"),ie.forEach(e),Ta.forEach(e),hs=$(s),M=c(s,"P",{});var Ys=i(M);ns=p(Ys,"Begin by loading the "),z=c(Ys,"A",{href:!0,rel:!0});var me=i(z);ts=p(me,"Microsoft Research Paraphrase Corpus (MRPC)"),me.forEach(e),m=p(Ys," training dataset from the "),w=c(Ys,"A",{href:!0,rel:!0});var ue=i(w);D=p(ue,"General Language Understanding Evaluation (GLUE) benchmark"),ue.forEach(e),ks=p(Ys,". MRPC is a corpus of human annotated sentence pairs used to train a model to determine whether sentence pairs are semantically equivalent."),Ys.forEach(e),$s=$(s),x(ls.$$.fragment,s),V=$(s),Z=c(s,"P",{});var Aa=i(Z);P=p(Aa,"Next, import the pre-trained BERT model and its tokenizer from the "),rs=c(Aa,"A",{href:!0,rel:!0});var de=i(rs);ys=p(de,"\u{1F917} Transformers"),de.forEach(e),xs=p(Aa," library:"),Aa.forEach(e),J=$(s),x(ss.$$.fragment,s),_s=$(s),vs=c(s,"H2",{class:!0});var Ca=i(vs);Cs=c(Ca,"A",{id:!0,class:!0,href:!0});var fe=i(Cs);Zs=c(fe,"SPAN",{});var je=i(Zs);x(Ls.$$.fragment,je),je.forEach(e),fe.forEach(e),Da=$(Ca),sa=c(Ca,"SPAN",{});var be=i(sa);Pa=p(be,"Tokenize the dataset"),be.forEach(e),Ca.forEach(e),ua=$(s),qs=c(s,"P",{});var qa=i(qs);Ba=p(qa,"The next step is to tokenize the text in order to build sequences of integers the model can understand. Encode the entire dataset with "),Hs=c(qa,"A",{href:!0});var ge=i(Hs);La=p(ge,"datasets.Dataset.map()"),ge.forEach(e),Na=p(qa,", and truncate and pad the inputs to the maximum length of the model. This ensures the appropriate tensor batches are built."),qa.forEach(e),da=$(s),x(Ns.$$.fragment,s),fa=$(s),is=c(s,"P",{});var Bs=i(is);Ra=p(Bs,"Notice how there are three new columns in the dataset: "),aa=c(Bs,"CODE",{});var $e=i(aa);Oa=p($e,"input_ids"),$e.forEach(e),Ia=p(Bs,", "),ea=c(Bs,"CODE",{});var _e=i(ea);Ma=p(_e,"token_type_ids"),_e.forEach(e),Ha=p(Bs,", and "),na=c(Bs,"CODE",{});var we=i(na);Ga=p(we,"attention_mask"),we.forEach(e),Wa=p(Bs,". These columns are the inputs to the model."),Bs.forEach(e),ja=$(s),Es=c(s,"H2",{class:!0});var Fa=i(Es);Fs=c(Fa,"A",{id:!0,class:!0,href:!0});var ke=i(Fs);ta=c(ke,"SPAN",{});var ye=i(ta);x(Rs.$$.fragment,ye),ye.forEach(e),ke.forEach(e),Qa=$(Fa),la=c(Fa,"SPAN",{});var xe=i(la);Ua=p(xe,"Format the dataset"),xe.forEach(e),Fa.forEach(e),ba=$(s),Gs=c(s,"P",{});var ve=i(Gs);Ya=p(ve,"Depending on whether you are using PyTorch, TensorFlow, or JAX, you will need to format the dataset accordingly. There are three changes you need to make to the dataset:"),ve.forEach(e),ga=$(s),x(zs.$$.fragment,s),$a=$(s),Ts=c(s,"H2",{class:!0});var za=i(Ts);Ss=c(za,"A",{id:!0,class:!0,href:!0});var Ee=i(Ss);ra=c(Ee,"SPAN",{});var Te=i(ra);x(Os.$$.fragment,Te),Te.forEach(e),Ee.forEach(e),Ja=$(za),pa=c(za,"SPAN",{});var Ae=i(pa);Xa=p(Ae,"Train the model"),Ae.forEach(e),za.forEach(e),_a=$(s),x(Ds.$$.fragment,s),wa=$(s),As=c(s,"H2",{class:!0});var Sa=i(As);Ps=c(Sa,"A",{id:!0,class:!0,href:!0});var Ce=i(Ps);oa=c(Ce,"SPAN",{});var qe=i(oa);x(Is.$$.fragment,qe),qe.forEach(e),Ce.forEach(e),Ka=$(Sa),ca=c(Sa,"SPAN",{});var Fe=i(ca);Va=p(Fe,"What's next?"),Fe.forEach(e),Sa.forEach(e),ka=$(s),Ws=c(s,"P",{});var ze=i(Ws);Za=p(ze,"This completes the basic steps of loading a dataset to train a model. You loaded and processed the MRPC dataset to fine-tune BERT to determine whether sentence pairs have the same meaning."),ze.forEach(e),ya=$(s),ws=c(s,"P",{});var Js=i(ws);se=p(Js,"For your next steps, take a look at our "),Qs=c(Js,"A",{href:!0});var Se=i(Qs);ae=p(Se,"How-to guides"),Se.forEach(e),ee=p(Js," and learn how to achieve a specific task (e.g. load a dataset offline, add a dataset to the Hub, change the name of a column). Or if you want to deepen your knowledge of \u{1F917} Datasets core concepts, read our "),Us=c(Js,"A",{href:!0});var De=i(Us);ne=p(De,"Conceptual Guides"),De.forEach(e),te=p(Js,"."),Js.forEach(e),this.h()},h(){j(t,"name","hf:doc:metadata"),j(t,"content",JSON.stringify(sn)),j(h,"id","quick-start"),j(h,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(h,"href","#quick-start"),j(n,"class","relative group"),j(Q,"href","./tutorial"),j(X,"href","https://huggingface.co/bert-base-cased"),j(X,"rel","nofollow"),j(I,"id","load-the-dataset-and-model"),j(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(I,"href","#load-the-dataset-and-model"),j(H,"class","relative group"),j(z,"href","https://huggingface.co/datasets/viewer/?dataset=glue&config=mrpc"),j(z,"rel","nofollow"),j(w,"href","https://huggingface.co/datasets/glue"),j(w,"rel","nofollow"),j(rs,"href","https://huggingface.co/transformers/"),j(rs,"rel","nofollow"),j(Cs,"id","tokenize-the-dataset"),j(Cs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(Cs,"href","#tokenize-the-dataset"),j(vs,"class","relative group"),j(Hs,"href","/docs/datasets/v2.0.0/en/package_reference/main_classes#datasets.Dataset.map"),j(Fs,"id","format-the-dataset"),j(Fs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(Fs,"href","#format-the-dataset"),j(Es,"class","relative group"),j(Ss,"id","train-the-model"),j(Ss,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(Ss,"href","#train-the-model"),j(Ts,"class","relative group"),j(Ps,"id","whats-next"),j(Ps,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(Ps,"href","#whats-next"),j(As,"class","relative group"),j(Qs,"href","./how_to"),j(Us,"href","./about_arrow")},m(s,u){a(document.head,t),d(s,l,u),d(s,n,u),a(n,h),a(h,b),v(f,b,null),a(n,_),a(n,G),a(G,ms),d(s,B,u),d(s,W,u),a(W,us),a(W,Q),a(Q,N),a(W,as),d(s,F,u),d(s,L,u),a(L,ds),a(L,X),a(X,k),a(L,fs),d(s,U,u),v(Y,s,u),d(s,ps,u),d(s,O,u),a(O,js),d(s,os,u),v(R,s,u),d(s,cs,u),d(s,H,u),a(H,I),a(I,q),v(S,q,null),a(H,bs),a(H,es),a(es,K),d(s,hs,u),d(s,M,u),a(M,ns),a(M,z),a(z,ts),a(M,m),a(M,w),a(w,D),a(M,ks),d(s,$s,u),v(ls,s,u),d(s,V,u),d(s,Z,u),a(Z,P),a(Z,rs),a(rs,ys),a(Z,xs),d(s,J,u),v(ss,s,u),d(s,_s,u),d(s,vs,u),a(vs,Cs),a(Cs,Zs),v(Ls,Zs,null),a(vs,Da),a(vs,sa),a(sa,Pa),d(s,ua,u),d(s,qs,u),a(qs,Ba),a(qs,Hs),a(Hs,La),a(qs,Na),d(s,da,u),v(Ns,s,u),d(s,fa,u),d(s,is,u),a(is,Ra),a(is,aa),a(aa,Oa),a(is,Ia),a(is,ea),a(ea,Ma),a(is,Ha),a(is,na),a(na,Ga),a(is,Wa),d(s,ja,u),d(s,Es,u),a(Es,Fs),a(Fs,ta),v(Rs,ta,null),a(Es,Qa),a(Es,la),a(la,Ua),d(s,ba,u),d(s,Gs,u),a(Gs,Ya),d(s,ga,u),v(zs,s,u),d(s,$a,u),d(s,Ts,u),a(Ts,Ss),a(Ss,ra),v(Os,ra,null),a(Ts,Ja),a(Ts,pa),a(pa,Xa),d(s,_a,u),v(Ds,s,u),d(s,wa,u),d(s,As,u),a(As,Ps),a(Ps,oa),v(Is,oa,null),a(As,Ka),a(As,ca),a(ca,Va),d(s,ka,u),d(s,Ws,u),a(Ws,Za),d(s,ya,u),d(s,ws,u),a(ws,se),a(ws,Qs),a(Qs,ae),a(ws,ee),a(ws,Us),a(Us,ne),a(ws,te),xa=!0},p(s,[u]){const Ms={};u&2&&(Ms.$$scope={dirty:u,ctx:s}),Y.$set(Ms);const ha={};u&2&&(ha.$$scope={dirty:u,ctx:s}),ss.$set(ha);const ia={};u&2&&(ia.$$scope={dirty:u,ctx:s}),zs.$set(ia);const ma={};u&2&&(ma.$$scope={dirty:u,ctx:s}),Ds.$set(ma)},i(s){xa||(E(f.$$.fragment,s),E(Y.$$.fragment,s),E(R.$$.fragment,s),E(S.$$.fragment,s),E(ls.$$.fragment,s),E(ss.$$.fragment,s),E(Ls.$$.fragment,s),E(Ns.$$.fragment,s),E(Rs.$$.fragment,s),E(zs.$$.fragment,s),E(Os.$$.fragment,s),E(Ds.$$.fragment,s),E(Is.$$.fragment,s),xa=!0)},o(s){T(f.$$.fragment,s),T(Y.$$.fragment,s),T(R.$$.fragment,s),T(S.$$.fragment,s),T(ls.$$.fragment,s),T(ss.$$.fragment,s),T(Ls.$$.fragment,s),T(Ns.$$.fragment,s),T(Rs.$$.fragment,s),T(zs.$$.fragment,s),T(Os.$$.fragment,s),T(Ds.$$.fragment,s),T(Is.$$.fragment,s),xa=!1},d(s){e(t),s&&e(l),s&&e(n),A(f),s&&e(B),s&&e(W),s&&e(F),s&&e(L),s&&e(U),A(Y,s),s&&e(ps),s&&e(O),s&&e(os),A(R,s),s&&e(cs),s&&e(H),A(S),s&&e(hs),s&&e(M),s&&e($s),A(ls,s),s&&e(V),s&&e(Z),s&&e(J),A(ss,s),s&&e(_s),s&&e(vs),A(Ls),s&&e(ua),s&&e(qs),s&&e(da),A(Ns,s),s&&e(fa),s&&e(is),s&&e(ja),s&&e(Es),A(Rs),s&&e(ba),s&&e(Gs),s&&e(ga),A(zs,s),s&&e($a),s&&e(Ts),A(Os),s&&e(_a),A(Ds,s),s&&e(wa),s&&e(As),A(Is),s&&e(ka),s&&e(Ws),s&&e(ya),s&&e(ws)}}}const sn={local:"quick-start",sections:[{local:"load-the-dataset-and-model",title:"Load the dataset and model"},{local:"tokenize-the-dataset",title:"Tokenize the dataset"},{local:"format-the-dataset",title:"Format the dataset"},{local:"train-the-model",title:"Train the model"},{local:"whats-next",title:"What's next?"}],title:"Quick Start"};function an(C,t,l){let{fw:n}=t;return C.$$set=h=>{"fw"in h&&l(0,n=h.fw)},[n]}class on extends Pe{constructor(t){super();Be(this,t,an,Ze,Le,{fw:0})}}export{on as default,sn as metadata};
