import{S as Wg,i as Bg,s as Qg,e as l,k as d,w as u,t as r,M as Gg,c as o,d as a,m as f,a as n,x as m,h as i,b as c,G as t,g as p,y as g,q as _,o as v,B as $,v as Kg}from"../chunks/vendor-hf-doc-builder.js";import{T as Ia}from"../chunks/Tip-hf-doc-builder.js";import{I as A}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as E}from"../chunks/CodeBlock-hf-doc-builder.js";import{C as an}from"../chunks/CodeBlockFw-hf-doc-builder.js";import"../chunks/IconTensorflow-hf-doc-builder.js";function Xg(D){let h,k,y,b,q;return{c(){h=l("p"),k=r("Refer to the "),y=l("a"),b=r("Upload a dataset to the Hub"),q=r(" tutorial for more details on how to create a dataset repository on the Hub, and how to upload your data files."),this.h()},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"Refer to the "),y=o(j,"A",{href:!0});var P=n(y);b=i(P,"Upload a dataset to the Hub"),P.forEach(a),q=i(j," tutorial for more details on how to create a dataset repository on the Hub, and how to upload your data files."),j.forEach(a),this.h()},h(){c(y,"href","./upload_dataset")},m(w,j){p(w,h,j),t(h,k),t(h,y),t(y,b),t(h,q)},d(w){w&&a(h)}}}function Zg(D){let h,k,y,b,q;return{c(){h=l("p"),k=r("If you don\u2019t specify which data files to use, "),y=l("a"),b=r("load_dataset()"),q=r(" will return all the data files. This can take a long time if you load a large dataset like C4, which is approximately 13TB of data."),this.h()},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"If you don\u2019t specify which data files to use, "),y=o(j,"A",{href:!0});var P=n(y);b=i(P,"load_dataset()"),P.forEach(a),q=i(j," will return all the data files. This can take a long time if you load a large dataset like C4, which is approximately 13TB of data."),j.forEach(a),this.h()},h(){c(y,"href","/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset")},m(w,j){p(w,h,j),t(h,k),t(h,y),t(y,b),t(h,q)},d(w){w&&a(h)}}}function s_(D){let h,k,y,b,q,w,j,P;return{c(){h=l("p"),k=r("You can specify "),y=l("a"),b=r("con"),q=r(" as a "),w=l("a"),j=r("URI string"),P=r(" for the \u{1F917} Datasets caching to work across sessions."),this.h()},l(I){h=o(I,"P",{});var T=n(h);k=i(T,"You can specify "),y=o(T,"A",{href:!0});var F=n(y);b=i(F,"con"),F.forEach(a),q=i(T," as a "),w=o(T,"A",{href:!0,rel:!0});var R=n(w);j=i(R,"URI string"),R.forEach(a),P=i(T," for the \u{1F917} Datasets caching to work across sessions."),T.forEach(a),this.h()},h(){c(y,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.from_sql.con"),c(w,"href","https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls"),c(w,"rel","nofollow")},m(I,T){p(I,h,T),t(h,k),t(h,y),t(y,b),t(h,q),t(h,w),t(w,j),t(h,P)},d(I){I&&a(h)}}}function a_(D){let h,k,y,b,q,w,j,P,I,T,F,R,Ss,Q,G,Ds,S,J,Oa,La,W,Ha,Fa,M,Ra,Ma;return{c(){h=l("p"),k=r("An object data type in "),y=l("a"),b=r("pandas.Series"),q=r(" doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length "),w=l("code"),j=r("0"),P=r(" or the Series only contains "),I=l("code"),T=r("None/NaN"),F=r(" objects, the type is set to "),R=l("code"),Ss=r("null"),Q=r(". Avoid potential errors by constructing an explicit schema with "),G=l("a"),Ds=r("Features"),S=r(" using the "),J=l("code"),Oa=r("from_dict"),La=r(" or "),W=l("code"),Ha=r("from_pandas"),Fa=r(" methods. See the "),M=l("a"),Ra=r("troubleshoot"),Ma=r(" section for more details on how to explicitly specify your own features."),this.h()},l(B){h=o(B,"P",{});var x=n(h);k=i(x,"An object data type in "),y=o(x,"A",{href:!0,rel:!0});var pe=n(y);b=i(pe,"pandas.Series"),pe.forEach(a),q=i(x," doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length "),w=o(x,"CODE",{});var Ts=n(w);j=i(Ts,"0"),Ts.forEach(a),P=i(x," or the Series only contains "),I=o(x,"CODE",{});var de=n(I);T=i(de,"None/NaN"),de.forEach(a),F=i(x," objects, the type is set to "),R=o(x,"CODE",{});var fe=n(R);Ss=i(fe,"null"),fe.forEach(a),Q=i(x,". Avoid potential errors by constructing an explicit schema with "),G=o(x,"A",{href:!0});var Ns=n(G);Ds=i(Ns,"Features"),Ns.forEach(a),S=i(x," using the "),J=o(x,"CODE",{});var ce=n(J);Oa=i(ce,"from_dict"),ce.forEach(a),La=i(x," or "),W=o(x,"CODE",{});var za=n(W);Ha=i(za,"from_pandas"),za.forEach(a),Fa=i(x," methods. See the "),M=o(x,"A",{href:!0});var O=n(M);Ra=i(O,"troubleshoot"),O.forEach(a),Ma=i(x," section for more details on how to explicitly specify your own features."),x.forEach(a),this.h()},h(){c(y,"href","https://pandas.pydata.org/docs/reference/api/pandas.Series.html"),c(y,"rel","nofollow"),c(G,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Features"),c(M,"href","./loading#specify-features")},m(B,x){p(B,h,x),t(h,k),t(h,y),t(y,b),t(h,q),t(h,w),t(w,j),t(h,P),t(h,I),t(I,T),t(h,F),t(h,R),t(R,Ss),t(h,Q),t(h,G),t(G,Ds),t(h,S),t(h,J),t(J,Oa),t(h,La),t(h,W),t(W,Ha),t(h,Fa),t(h,M),t(M,Ra),t(h,Ma)},d(B){B&&a(h)}}}function t_(D){let h,k,y,b;return{c(){h=l("p"),k=l("code"),y=r("pct1_dropremainder"),b=r(" rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100.")},l(q){h=o(q,"P",{});var w=n(h);k=o(w,"CODE",{});var j=n(k);y=i(j,"pct1_dropremainder"),j.forEach(a),b=i(w," rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100."),w.forEach(a)},m(q,w){p(q,h,w),t(h,k),t(k,y),t(h,b)},d(q){q&&a(h)}}}function e_(D){let h,k,y,b,q;return{c(){h=l("p"),k=r("Metrics is deprecated in \u{1F917} Datasets. To learn more about how to use metrics, take a look at the library \u{1F917} "),y=l("a"),b=r("Evaluate"),q=r("! In addition to metrics, you can find more tools for evaluating models and datasets."),this.h()},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"Metrics is deprecated in \u{1F917} Datasets. To learn more about how to use metrics, take a look at the library \u{1F917} "),y=o(j,"A",{href:!0,rel:!0});var P=n(y);b=i(P,"Evaluate"),P.forEach(a),q=i(j,"! In addition to metrics, you can find more tools for evaluating models and datasets."),j.forEach(a),this.h()},h(){c(y,"href","https://huggingface.co/docs/evaluate/index"),c(y,"rel","nofollow")},m(w,j){p(w,h,j),t(h,k),t(h,y),t(y,b),t(h,q)},d(w){w&&a(h)}}}function l_(D){let h,k,y,b,q;return{c(){h=l("p"),k=r("See the "),y=l("a"),b=r("Metrics"),q=r(" guide for more details on how to write your own metric loading script."),this.h()},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"See the "),y=o(j,"A",{href:!0});var P=n(y);b=i(P,"Metrics"),P.forEach(a),q=i(j," guide for more details on how to write your own metric loading script."),j.forEach(a),this.h()},h(){c(y,"href","./how_to_metrics#custom-metric-loading-script")},m(w,j){p(w,h,j),t(h,k),t(h,y),t(y,b),t(h,q)},d(w){w&&a(h)}}}function o_(D){let h,k,y,b,q;return{c(){h=l("p"),k=r("Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),y=l("a"),b=r("Metric.compute()"),q=r(" gathers all the predictions and references from the nodes, and computes the final metric."),this.h()},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),y=o(j,"A",{href:!0});var P=n(y);b=i(P,"Metric.compute()"),P.forEach(a),q=i(j," gathers all the predictions and references from the nodes, and computes the final metric."),j.forEach(a),this.h()},h(){c(y,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Metric.compute")},m(w,j){p(w,h,j),t(h,k),t(h,y),t(y,b),t(h,q)},d(w){w&&a(h)}}}function n_(D){let h,k,y,b,q,w,j,P,I,T,F,R,Ss,Q,G,Ds,S,J,Oa,La,W,Ha,Fa,M,Ra,Ma,B,x,pe,Ts,de,fe,Ns,ce,za,O,bp,Va,qp,kp,Ua,Ep,Pp,Ya,xp,Ap,tn,he,en,ns,Cs,_l,Ja,Sp,vl,Dp,ln,K,Tp,ue,Np,Cp,me,Ip,Op,on,Is,Lp,Wa,Hp,Fp,nn,Ba,rn,Os,Rp,$l,Mp,zp,pn,Qa,dn,Ls,fn,N,Vp,yl,Up,Yp,wl,Jp,Wp,jl,Bp,Qp,bl,Gp,Kp,ql,Xp,Zp,cn,Ga,hn,Hs,un,X,sd,kl,ad,td,El,ed,ld,mn,Ka,gn,Fs,od,Pl,nd,rd,_n,Xa,vn,rs,Rs,xl,Za,id,Al,pd,$n,Ms,dd,ge,fd,cd,yn,zs,Sl,hd,ud,Dl,md,wn,st,jn,is,Vs,Tl,at,gd,Nl,_d,bn,_e,vd,qn,tt,kn,Us,$d,ve,yd,wd,En,et,Pn,ps,Ys,Cl,lt,jd,Il,bd,xn,C,qd,Ol,kd,Ed,Ll,Pd,xd,Hl,Ad,Sd,Fl,Dd,Td,$e,Nd,Cd,An,ds,Js,Rl,ot,Id,Ml,Od,Sn,ye,Ld,Dn,nt,Tn,we,Hd,Nn,rt,Cn,je,Fd,In,it,On,be,Rd,Ln,pt,Hn,qe,Md,Fn,dt,Rn,fs,Ws,zl,ft,zd,Vl,Vd,Mn,Bs,Ud,ke,Yd,Jd,zn,ct,Vn,Ee,Wd,Un,ht,Yn,Qs,Bd,Ul,Qd,Gd,Jn,ut,Wn,Pe,Kd,Bn,mt,Qn,xe,Xd,Gn,cs,Gs,Yl,gt,Zd,Jl,sf,Kn,Ae,af,Xn,Se,tf,Zn,_t,sr,De,ef,ar,vt,tr,hs,Ks,Wl,$t,lf,Bl,of,er,Xs,nf,Te,rf,pf,lr,Ne,df,or,yt,nr,Ce,ff,rr,wt,ir,Zs,pr,us,sa,Ql,jt,cf,Gl,hf,dr,Ie,uf,fr,aa,mf,Kl,gf,_f,cr,bt,hr,ms,ta,Xl,qt,vf,Zl,$f,ur,ea,yf,Oe,wf,jf,mr,gs,la,so,kt,bf,ao,qf,gr,oa,kf,Le,Ef,Pf,_r,Et,vr,_s,na,to,Pt,xf,eo,Af,$r,ra,Sf,lo,Df,Tf,yr,xt,wr,vs,ia,oo,At,Nf,no,Cf,jr,pa,If,He,Of,Lf,br,St,qr,Fe,Hf,kr,da,Ff,ro,Rf,Mf,Er,Dt,Pr,$s,fa,io,Tt,zf,po,Vf,xr,ca,Uf,Re,Yf,Jf,Ar,Nt,Sr,ha,Dr,ys,ua,fo,Ct,Wf,co,Bf,Tr,Me,Qf,Nr,Z,Gf,ho,Kf,Xf,uo,Zf,sc,Cr,ws,ma,mo,It,ac,go,tc,Ir,ss,ec,ze,lc,oc,Ve,nc,rc,Or,as,ic,_o,pc,dc,vo,fc,cc,Lr,Ot,Hr,ga,hc,$o,uc,mc,Fr,Lt,Rr,Ue,gc,Mr,Ht,zr,Ye,_c,Vr,Ft,Ur,Je,vc,Yr,Rt,Jr,js,_a,yo,Mt,$c,wo,yc,Wr,We,wc,Br,zt,Qr,va,jc,jo,bc,qc,Gr,Vt,Kr,$a,Xr,Be,Zr,bs,ya,bo,Ut,kc,qo,Ec,si,Qe,Pc,ai,qs,wa,ko,Yt,xc,Eo,Ac,ti,z,Sc,Ge,Dc,Tc,Po,Nc,Cc,xo,Ic,Oc,ei,ja,Lc,Jt,Hc,Fc,li,Wt,oi,H,Rc,Ao,Mc,zc,So,Vc,Uc,Do,Yc,Jc,Ke,Wc,Bc,ni,ks,ba,To,Bt,Qc,No,Gc,ri,V,Kc,Xe,Xc,Zc,Qt,sh,ah,Ze,th,eh,ii,qa,lh,sl,oh,nh,pi,Gt,di,ts,rh,Co,ih,ph,al,dh,fh,fi,Kt,ci,tl,ch,hi,Xt,ui,Es,ka,Io,Zt,hh,Oo,uh,mi,Ea,gi,el,mh,_i,se,vi,Pa,$i,Ps,xa,Lo,ae,gh,Ho,_h,yi,es,vh,Fo,$h,yh,ll,wh,jh,wi,te,ji,xs,Aa,Ro,ee,bh,Mo,qh,bi,ol,kh,qi,nl,Eh,ki,ls,zo,le,Ph,Vo,xh,Ah,Sh,Uo,As,Dh,Yo,Th,Nh,Jo,Ch,Ih,Oh,Wo,oe,Lh,rl,Hh,Fh,Ei,ne,Pi,Sa,xi,Da,Rh,Bo,Mh,zh,Ai,re,Si;return w=new A({}),Ja=new A({}),Ba=new E({props:{code:`from datasets import load_dataset
dataset = load_dataset("lhoestq/demo1")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;lhoestq/demo1&quot;</span>)`}}),Qa=new E({props:{code:`dataset = load_dataset(
  "lhoestq/custom_squad",
  revision="main"  # tag name, or branch name, or commit hash
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(
<span class="hljs-meta">... </span>  <span class="hljs-string">&quot;lhoestq/custom_squad&quot;</span>,
<span class="hljs-meta">... </span>  revision=<span class="hljs-string">&quot;main&quot;</span>  <span class="hljs-comment"># tag name, or branch name, or commit hash</span>
<span class="hljs-meta">... </span>)`}}),Ls=new Ia({props:{$$slots:{default:[Xg]},$$scope:{ctx:D}}}),Ga=new E({props:{code:`data_files = {"train": "train.csv", "test": "test.csv"}
dataset = load_dataset("namespace/your_dataset_name", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: <span class="hljs-string">&quot;train.csv&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>: <span class="hljs-string">&quot;test.csv&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;namespace/your_dataset_name&quot;</span>, data_files=data_files)`}}),Hs=new Ia({props:{warning:!0,$$slots:{default:[Zg]},$$scope:{ctx:D}}}),Ka=new E({props:{code:`from datasets import load_dataset

c4_subset = load_dataset("allenai/c4", data_files="en/c4-train.0000*-of-01024.json.gz")

c4_subset = load_dataset("allenai/c4", data_dir="en")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-comment"># load files that match the grep pattern</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_subset = load_dataset(<span class="hljs-string">&quot;allenai/c4&quot;</span>, data_files=<span class="hljs-string">&quot;en/c4-train.0000*-of-01024.json.gz&quot;</span>)

<span class="hljs-comment"># load dataset from the en directory on the Hub</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_subset = load_dataset(<span class="hljs-string">&quot;allenai/c4&quot;</span>, data_dir=<span class="hljs-string">&quot;en&quot;</span>)`}}),Xa=new E({props:{code:`data_files = {"validation": "en/c4-validation.*.json.gz"}
c4_validation = load_dataset("allenai/c4", data_files=data_files, split="validation")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;validation&quot;</span>: <span class="hljs-string">&quot;en/c4-validation.*.json.gz&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_validation = load_dataset(<span class="hljs-string">&quot;allenai/c4&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;validation&quot;</span>)`}}),Za=new A({}),st=new E({props:{code:`dataset = load_dataset("path/to/local/loading_script/loading_script.py", split="train")
dataset = load_dataset("path/to/local/loading_script", split="train")  # equivalent because the file has the same name as the directory`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;path/to/local/loading_script/loading_script.py&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;path/to/local/loading_script&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)  <span class="hljs-comment"># equivalent because the file has the same name as the directory</span>`}}),at=new A({}),tt=new E({props:{code:"git clone https://huggingface.co/datasets/eli5",highlighted:'git <span class="hljs-built_in">clone</span> https://huggingface.co/datasets/eli5'}}),et=new E({props:{code:`from datasets import load_dataset
eli5 = load_dataset("path/to/local/eli5")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>eli5 = load_dataset(<span class="hljs-string">&quot;path/to/local/eli5&quot;</span>)`}}),lt=new A({}),ot=new A({}),nt=new E({props:{code:`from datasets import load_dataset
dataset = load_dataset("csv", data_files="my_file.csv")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=<span class="hljs-string">&quot;my_file.csv&quot;</span>)`}}),rt=new E({props:{code:'dataset = load_dataset("csv", data_files=["my_file_1.csv", "my_file_2.csv", "my_file_3.csv"])',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=[<span class="hljs-string">&quot;my_file_1.csv&quot;</span>, <span class="hljs-string">&quot;my_file_2.csv&quot;</span>, <span class="hljs-string">&quot;my_file_3.csv&quot;</span>])'}}),it=new E({props:{code:'dataset = load_dataset("csv", data_files={"train": ["my_train_file_1.csv", "my_train_file_2.csv"], "test": "my_test_file.csv"})',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files={<span class="hljs-string">&quot;train&quot;</span>: [<span class="hljs-string">&quot;my_train_file_1.csv&quot;</span>, <span class="hljs-string">&quot;my_train_file_2.csv&quot;</span>], <span class="hljs-string">&quot;test&quot;</span>: <span class="hljs-string">&quot;my_test_file.csv&quot;</span>})'}}),pt=new E({props:{code:`base_url = "https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/"
dataset = load_dataset('csv', data_files={'train': base_url + 'train.csv', 'test': base_url + 'test.csv'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train.csv&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: base_url + <span class="hljs-string">&#x27;test.csv&#x27;</span>})`}}),dt=new E({props:{code:`url = "https://domain.org/train_data.zip"
data_files = {"train": url}
dataset = load_dataset("csv", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://domain.org/train_data.zip&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: url}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=data_files)`}}),ft=new A({}),ct=new E({props:{code:`from datasets import load_dataset
dataset = load_dataset("json", data_files="my_file.json")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=<span class="hljs-string">&quot;my_file.json&quot;</span>)`}}),ht=new E({props:{code:`{"a": 1, "b": 2.0, "c": "foo", "d": false}
{"a": 4, "b": -5.5, "c": null, "d": true}`,highlighted:`<span class="hljs-punctuation">{</span><span class="hljs-attr">&quot;a&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;b&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">2.0</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;c&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;foo&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;d&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-keyword">false</span><span class="hljs-punctuation">}</span>
<span class="hljs-punctuation">{</span><span class="hljs-attr">&quot;a&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">4</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;b&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">-5.5</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;c&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-keyword">null</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;d&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-keyword">true</span><span class="hljs-punctuation">}</span>`}}),ut=new E({props:{code:`
from datasets import load_dataset
dataset = load_dataset("json", data_files="my_file.json", field="data")`,highlighted:`{<span class="hljs-string">&quot;version&quot;</span>: <span class="hljs-string">&quot;0.1.0&quot;</span>,
 <span class="hljs-string">&quot;data&quot;</span>: [{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false},
          {<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}]
}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files=<span class="hljs-string">&quot;my_file.json&quot;</span>, field=<span class="hljs-string">&quot;data&quot;</span>)`}}),mt=new E({props:{code:`base_url = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
dataset = load_dataset("json", data_files={"train": base_url + "train-v1.1.json", "validation": base_url + "dev-v1.1.json"}, field="data")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://rajpurkar.github.io/SQuAD-explorer/dataset/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;json&quot;</span>, data_files={<span class="hljs-string">&quot;train&quot;</span>: base_url + <span class="hljs-string">&quot;train-v1.1.json&quot;</span>, <span class="hljs-string">&quot;validation&quot;</span>: base_url + <span class="hljs-string">&quot;dev-v1.1.json&quot;</span>}, field=<span class="hljs-string">&quot;data&quot;</span>)`}}),gt=new A({}),_t=new E({props:{code:`from datasets import load_dataset
dataset = load_dataset("parquet", data_files={'train': 'train.parquet', 'test': 'test.parquet'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;train.parquet&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;test.parquet&#x27;</span>})`}}),vt=new E({props:{code:`base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"
data_files = {"train": base_url + "wikipedia-train.parquet"}
wiki = load_dataset("parquet", data_files=data_files, split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: base_url + <span class="hljs-string">&quot;wikipedia-train.parquet&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>wiki = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),$t=new A({}),yt=new E({props:{code:`from datasets import Dataset
dataset = Dataset.from_sql("data_table", "sqlite:///sqlite_file.db")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_sql(<span class="hljs-string">&quot;data_table&quot;</span>, <span class="hljs-string">&quot;sqlite:///sqlite_file.db&quot;</span>)`}}),wt=new E({props:{code:`from sqlite3 import connect
con = connect(":memory")
# db writes ...
from datasets import Dataset
dataset = Dataset.from_sql("SELECT text FROM table WHERE length(text) > 100 LIMIT 10", con)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> sqlite3 <span class="hljs-keyword">import</span> connect
<span class="hljs-meta">&gt;&gt;&gt; </span>con = connect(<span class="hljs-string">&quot;:memory&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># db writes ...</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_sql(<span class="hljs-string">&quot;SELECT text FROM table WHERE length(text) &gt; 100 LIMIT 10&quot;</span>, con)`}}),Zs=new Ia({props:{$$slots:{default:[s_]},$$scope:{ctx:D}}}),jt=new A({}),bt=new E({props:{code:`from datasets import load_dataset

oscar_afrikaans = load_dataset("oscar-corpus/OSCAR-2201", "af", num_proc=8)
imagenet = load_dataset("imagenet-1k", num_proc=8)
ml_librispeech_spanish = load_dataset("facebook/multilingual_librispeech", "spanish", num_proc=8)`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

oscar_afrikaans = load_dataset(<span class="hljs-string">&quot;oscar-corpus/OSCAR-2201&quot;</span>, <span class="hljs-string">&quot;af&quot;</span>, num_proc=<span class="hljs-number">8</span>)
imagenet = load_dataset(<span class="hljs-string">&quot;imagenet-1k&quot;</span>, num_proc=<span class="hljs-number">8</span>)
ml_librispeech_spanish = load_dataset(<span class="hljs-string">&quot;facebook/multilingual_librispeech&quot;</span>, <span class="hljs-string">&quot;spanish&quot;</span>, num_proc=<span class="hljs-number">8</span>)`}}),qt=new A({}),kt=new A({}),Et=new E({props:{code:`from datasets import Dataset
my_dict = {"a": [1, 2, 3]}
dataset = Dataset.from_dict(my_dict)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>my_dict = {<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_dict(my_dict)`}}),Pt=new A({}),xt=new E({props:{code:`from datasets import Dataset
my_list = [{"a": 1}, {"a": 2}, {"a": 3}]
dataset = Dataset.from_list(my_list)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>my_list = [{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>}, {<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">2</span>}, {<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">3</span>}]
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_list(my_list)`}}),At=new A({}),St=new E({props:{code:`from datasets import Dataset
def my_gen():
    for i in range(1, 4):
        yield {"a": i}
dataset = Dataset.from_generator(my_gen)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">my_gen</span>():
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>):
<span class="hljs-meta">... </span>        <span class="hljs-keyword">yield</span> {<span class="hljs-string">&quot;a&quot;</span>: i}
...
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_generator(my_gen)`}}),Dt=new E({props:{code:`def gen(shards):
    for shard in shards:
        with open(shard) as f:
            for line in f:
                yield {"line": line}
shards = [f"data{i}.txt" for i in range(32)]
ds = Dataset.from_generator(gen, gen_kwargs={"shards": shards})
ds = ds.shuffle(seed=42, buffer_size=10_000)  # shuffles the shards order + uses a shuffle buffer
from torch.utils.data import DataLoader
dataloader = DataLoader(ds.with_format("torch"), num_workers=4)  # give each worker a subset of 32/4=8 shards`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">gen</span>(<span class="hljs-params">shards</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> shard <span class="hljs-keyword">in</span> shards:
<span class="hljs-meta">... </span>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(shard) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>            <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f:
<span class="hljs-meta">... </span>                <span class="hljs-keyword">yield</span> {<span class="hljs-string">&quot;line&quot;</span>: line}
...
<span class="hljs-meta">&gt;&gt;&gt; </span>shards = [<span class="hljs-string">f&quot;data<span class="hljs-subst">{i}</span>.txt&quot;</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">32</span>)]
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = Dataset.from_generator(gen, gen_kwargs={<span class="hljs-string">&quot;shards&quot;</span>: shards})
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.shuffle(seed=<span class="hljs-number">42</span>, buffer_size=<span class="hljs-number">10_000</span>)  <span class="hljs-comment"># shuffles the shards order + uses a shuffle buffer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = DataLoader(ds.with_format(<span class="hljs-string">&quot;torch&quot;</span>), num_workers=<span class="hljs-number">4</span>)  <span class="hljs-comment"># give each worker a subset of 32/4=8 shards</span>`}}),Tt=new A({}),Nt=new E({props:{code:`from datasets import Dataset
import pandas as pd
df = pd.DataFrame({"a": [1, 2, 3]})
dataset = Dataset.from_pandas(df)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-meta">&gt;&gt;&gt; </span>df = pd.DataFrame({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_pandas(df)`}}),ha=new Ia({props:{warning:!0,$$slots:{default:[a_]},$$scope:{ctx:D}}}),Ct=new A({}),It=new A({}),Ot=new an({props:{group1:{id:"stringapi",code:'train_test_ds = datasets.load_dataset("bookcorpus", split="train+test")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=<span class="hljs-string">&quot;train+test&quot;</span>)'},group2:{id:"readinstruction",code:`ri = datasets.ReadInstruction("train") + datasets.ReadInstruction("test")
train_test_ds = datasets.load_dataset("bookcorpus", split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = datasets.ReadInstruction(<span class="hljs-string">&quot;train&quot;</span>) + datasets.ReadInstruction(<span class="hljs-string">&quot;test&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=ri)`}}}),Lt=new an({props:{group1:{id:"stringapi",code:'train_10_20_ds = datasets.load_dataset("bookcorpus", split="train[10:20]")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=<span class="hljs-string">&quot;train[10:20]&quot;</span>)'},group2:{id:"readinstruction",code:`train_10_20_ds = datasets.load_dataset("bookcorpu"', split=datasets.ReadInstruction("train", from_=10, to=20, unit="abs"))`,highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpu&quot;</span><span class="hljs-string">&#x27;, split=datasets.ReadInstruction(&quot;train&quot;, from_=10, to=20, unit=&quot;abs&quot;))</span>'}}}),Ht=new an({props:{group1:{id:"stringapi",code:'train_10pct_ds = datasets.load_dataset("bookcorpus", split="train[:10%]")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10pct_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=<span class="hljs-string">&quot;train[:10%]&quot;</span>)'},group2:{id:"readinstruction",code:'train_10_20_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train", to=10, unit="%"))',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&quot;train&quot;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&quot;%&quot;</span>))'}}}),Ft=new an({props:{group1:{id:"stringapi",code:'train_10_80pct_ds = datasets.load_dataset("bookcorpus", split="train[:10%]+train[-80%:]")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=<span class="hljs-string">&quot;train[:10%]+train[-80%:]&quot;</span>)'},group2:{id:"readinstruction",code:`ri = (datasets.ReadInstruction("train", to=10, unit="%") + datasets.ReadInstruction("train", from_=-80, unit="%"))
train_10_80pct_ds = datasets.load_dataset("bookcorpus", split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = (datasets.ReadInstruction(<span class="hljs-string">&quot;train&quot;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&quot;%&quot;</span>) + datasets.ReadInstruction(<span class="hljs-string">&quot;train&quot;</span>, from_=-<span class="hljs-number">80</span>, unit=<span class="hljs-string">&quot;%&quot;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=ri)`}}}),Rt=new an({props:{group1:{id:"stringapi",code:`val_ds = datasets.load_dataset("bookcorpus", split=[f"train[{k}%:{k+10}%]" for k in range(0, 100, 10)])
train_ds = datasets.load_dataset("bookcorpus", split=[f"train[:{k}%]+train[{k+10}%:]" for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>val_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=[<span class="hljs-string">f&quot;train[<span class="hljs-subst">{k}</span>%:<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%]&quot;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>train_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=[<span class="hljs-string">f&quot;train[:<span class="hljs-subst">{k}</span>%]+train[<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%:]&quot;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`},group2:{id:"readinstruction",code:`val_ds = datasets.load_dataset("bookcorpus", [datasets.ReadInstruction("train", from_=k, to=k+10, unit="%") for k in range(0, 100, 10)])
train_ds = datasets.load_dataset("bookcorpus", [(datasets.ReadInstruction("train", to=k, unit="%") + datasets.ReadInstruction("train", from_=k+10, unit="%")) for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>val_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, [datasets.ReadInstruction(<span class="hljs-string">&quot;train&quot;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&quot;%&quot;</span>) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>train_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, [(datasets.ReadInstruction(<span class="hljs-string">&quot;train&quot;</span>, to=k, unit=<span class="hljs-string">&quot;%&quot;</span>) + datasets.ReadInstruction(<span class="hljs-string">&quot;train&quot;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&quot;%&quot;</span>)) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}}),Mt=new A({}),zt=new E({props:{code:`train_50_52_ds = datasets.load_dataset("bookcorpus", split="train[50%:52%]")
train_52_54_ds = datasets.load_dataset("bookcorpus", split="train[52%:54%]")`,highlighted:`<span class="hljs-comment"># 19 records, from 500 (included) to 519 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=<span class="hljs-string">&quot;train[50%:52%]&quot;</span>)
<span class="hljs-comment"># 20 records, from 519 (included) to 539 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=<span class="hljs-string">&quot;train[52%:54%]&quot;</span>)`}}),Vt=new E({props:{code:`train_50_52pct1_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train", from_=50, to=52, unit="%", rounding="pct1_dropremainder"))
train_52_54pct1_ds = datasets.load_dataset("bookcorpus", split=datasets.ReadInstruction("train",from_=52, to=54, unit="%", rounding="pct1_dropremainder"))
train_50_52pct1_ds = datasets.load_dataset("bookcorpus", split="train[50%:52%](pct1_dropremainder)")
train_52_54pct1_ds = datasets.load_dataset("bookcorpus", split="train[52%:54%](pct1_dropremainder)")`,highlighted:`<span class="hljs-comment"># 18 records, from 450 (included) to 468 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&quot;train&quot;</span>, from_=<span class="hljs-number">50</span>, to=<span class="hljs-number">52</span>, unit=<span class="hljs-string">&quot;%&quot;</span>, rounding=<span class="hljs-string">&quot;pct1_dropremainder&quot;</span>))
<span class="hljs-comment"># 18 records, from 468 (included) to 486 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&quot;train&quot;</span>,from_=<span class="hljs-number">52</span>, to=<span class="hljs-number">54</span>, unit=<span class="hljs-string">&quot;%&quot;</span>, rounding=<span class="hljs-string">&quot;pct1_dropremainder&quot;</span>))
<span class="hljs-comment"># Or equivalently:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=<span class="hljs-string">&quot;train[50%:52%](pct1_dropremainder)&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&quot;bookcorpus&quot;</span>, split=<span class="hljs-string">&quot;train[52%:54%](pct1_dropremainder)&quot;</span>)`}}),$a=new Ia({props:{warning:!0,$$slots:{default:[t_]},$$scope:{ctx:D}}}),Ut=new A({}),Yt=new A({}),Wt=new E({props:{code:'dataset = load_dataset("matinf", "summarization")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;matinf&quot;</span>, <span class="hljs-string">&quot;summarization&quot;</span>)
Downloading <span class="hljs-keyword">and</span> preparing dataset matinf/summarization (download: Unknown size, generated: <span class="hljs-number">246.89</span> MiB, post-processed: Unknown size, total: <span class="hljs-number">246.89</span> MiB) to /root/.cache/huggingface/datasets/matinf/summarization/<span class="hljs-number">1.0</span><span class="hljs-number">.0</span>/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...
AssertionError: The dataset matinf <span class="hljs-keyword">with</span> config summarization requires manual data. 
Please follow the manual download instructions: To use MATINF you have to download it manually. Please fill this google form (https://forms.gle/nkH4LVE4iNQeDzsc9). You will receive a download link <span class="hljs-keyword">and</span> a password once you complete the form. Please extract <span class="hljs-built_in">all</span> files <span class="hljs-keyword">in</span> one folder <span class="hljs-keyword">and</span> load the dataset <span class="hljs-keyword">with</span>: *datasets.load_dataset(<span class="hljs-string">&#x27;matinf&#x27;</span>, data_dir=<span class="hljs-string">&#x27;path/to/folder/folder_name&#x27;</span>)*. 
Manual data can be loaded <span class="hljs-keyword">with</span> \`datasets.load_dataset(matinf, data_dir=<span class="hljs-string">&#x27;&lt;path/to/manual/data&gt;&#x27;</span>) `}}),Bt=new A({}),Gt=new E({props:{code:`class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]
emotion_features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>class_names = [<span class="hljs-string">&quot;sadness&quot;</span>, <span class="hljs-string">&quot;joy&quot;</span>, <span class="hljs-string">&quot;love&quot;</span>, <span class="hljs-string">&quot;anger&quot;</span>, <span class="hljs-string">&quot;fear&quot;</span>, <span class="hljs-string">&quot;surprise&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>emotion_features = Features({<span class="hljs-string">&#x27;text&#x27;</span>: Value(<span class="hljs-string">&#x27;string&#x27;</span>), <span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(names=class_names)})`}}),Kt=new E({props:{code:"dataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'], features=emotion_features)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=file_dict, delimiter=<span class="hljs-string">&#x27;;&#x27;</span>, column_names=[<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>], features=emotion_features)'}}),Xt=new E({props:{code:"dataset['train'].features",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&#x27;train&#x27;</span>].features
{<span class="hljs-string">&#x27;text&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">6</span>, names=[<span class="hljs-string">&#x27;sadness&#x27;</span>, <span class="hljs-string">&#x27;joy&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;anger&#x27;</span>, <span class="hljs-string">&#x27;fear&#x27;</span>, <span class="hljs-string">&#x27;surprise&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),Zt=new A({}),Ea=new Ia({props:{warning:!0,$$slots:{default:[e_]},$$scope:{ctx:D}}}),se=new E({props:{code:`from datasets import load_metric
metric = load_metric('PATH/TO/MY/METRIC/SCRIPT')

# Example of typical usage
for batch in dataset:
    inputs, references = batch
    predictions = model(inputs)
    metric.add_batch(predictions=predictions, references=references)
score = metric.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;PATH/TO/MY/METRIC/SCRIPT&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Example of typical usage</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataset:
<span class="hljs-meta">... </span>    inputs, references = batch
<span class="hljs-meta">... </span>    predictions = model(inputs)
<span class="hljs-meta">... </span>    metric.add_batch(predictions=predictions, references=references)
<span class="hljs-meta">&gt;&gt;&gt; </span>score = metric.compute()`}}),Pa=new Ia({props:{$$slots:{default:[l_]},$$scope:{ctx:D}}}),ae=new A({}),te=new E({props:{code:`from datasets import load_metric
metric = load_metric('bleurt', name='bleurt-base-128')
metric = load_metric('bleurt', name='bleurt-base-512')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-128&#x27;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-512&#x27;</span>)</span>`}}),ee=new A({}),ne=new E({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=rank)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=rank)`}}),Sa=new Ia({props:{$$slots:{default:[o_]},$$scope:{ctx:D}}}),re=new E({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=process_id, experiment_id="My_experiment_10")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=process_id, experiment_id=<span class="hljs-string">&quot;My_experiment_10&quot;</span>)`}}),{c(){h=l("meta"),k=d(),y=l("h1"),b=l("a"),q=l("span"),u(w.$$.fragment),j=d(),P=l("span"),I=r("Load"),T=d(),F=l("p"),R=r("Your data can be stored in various places; they can be on your local machine\u2019s disk, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever a dataset is stored, \u{1F917} Datasets can help you load it."),Ss=d(),Q=l("p"),G=r("This guide will show you how to load a dataset from:"),Ds=d(),S=l("ul"),J=l("li"),Oa=r("The Hub without a dataset loading script"),La=d(),W=l("li"),Ha=r("Local loading script"),Fa=d(),M=l("li"),Ra=r("Local files"),Ma=d(),B=l("li"),x=r("In-memory data"),pe=d(),Ts=l("li"),de=r("Offline"),fe=d(),Ns=l("li"),ce=r("A specific slice of a split"),za=d(),O=l("p"),bp=r("For more details specific to loading other dataset modalities, take a look at the "),Va=l("a"),qp=r("load audio dataset guide"),kp=r(", the "),Ua=l("a"),Ep=r("load image dataset guide"),Pp=r(", or the "),Ya=l("a"),xp=r("load text dataset guide"),Ap=r("."),tn=d(),he=l("a"),en=d(),ns=l("h2"),Cs=l("a"),_l=l("span"),u(Ja.$$.fragment),Sp=d(),vl=l("span"),Dp=r("Hugging Face Hub"),ln=d(),K=l("p"),Tp=r("Datasets are loaded from a dataset loading script that downloads and generates the dataset. However, you can also load a dataset from any dataset repository on the Hub without a loading script! Begin by "),ue=l("a"),Np=r("creating a dataset repository"),Cp=r(" and upload your data files. Now you can use the "),me=l("a"),Ip=r("load_dataset()"),Op=r(" function to load the dataset."),on=d(),Is=l("p"),Lp=r("For example, try loading the files from this "),Wa=l("a"),Hp=r("demo repository"),Fp=r(" by providing the repository namespace and dataset name. This dataset repository contains CSV files, and the code below loads the dataset from the CSV files:"),nn=d(),u(Ba.$$.fragment),rn=d(),Os=l("p"),Rp=r("Some datasets may have more than one version based on Git tags, branches, or commits. Use the "),$l=l("code"),Mp=r("revision"),zp=r(" parameter to specify the dataset version you want to load:"),pn=d(),u(Qa.$$.fragment),dn=d(),u(Ls.$$.fragment),fn=d(),N=l("p"),Vp=r("A dataset without a loading script by default loads all the data into the "),yl=l("code"),Up=r("train"),Yp=r(" split. Use the "),wl=l("code"),Jp=r("data_files"),Wp=r(" parameter to map data files to splits like "),jl=l("code"),Bp=r("train"),Qp=r(", "),bl=l("code"),Gp=r("validation"),Kp=r(" and "),ql=l("code"),Xp=r("test"),Zp=r(":"),cn=d(),u(Ga.$$.fragment),hn=d(),u(Hs.$$.fragment),un=d(),X=l("p"),sd=r("You can also load a specific subset of the files with the "),kl=l("code"),ad=r("data_files"),td=r(" or "),El=l("code"),ed=r("data_dir"),ld=r(" parameter. These parameters can accept a relative path which resolves to the base path corresponding to where the dataset is loaded from."),mn=d(),u(Ka.$$.fragment),gn=d(),Fs=l("p"),od=r("The "),Pl=l("code"),nd=r("split"),rd=r(" parameter can also map a data file to a specific split:"),_n=d(),u(Xa.$$.fragment),vn=d(),rs=l("h2"),Rs=l("a"),xl=l("span"),u(Za.$$.fragment),id=d(),Al=l("span"),pd=r("Local loading script"),$n=d(),Ms=l("p"),dd=r("You may have a \u{1F917} Datasets loading script locally on your computer. In this case, load the dataset by passing one of the following paths to "),ge=l("a"),fd=r("load_dataset()"),cd=r(":"),yn=d(),zs=l("ul"),Sl=l("li"),hd=r("The local path to the loading script file."),ud=d(),Dl=l("li"),md=r("The local path to the directory containing the loading script file (only if the script file has the same name as the directory)."),wn=d(),u(st.$$.fragment),jn=d(),is=l("h3"),Vs=l("a"),Tl=l("span"),u(at.$$.fragment),gd=d(),Nl=l("span"),_d=r("Edit loading script"),bn=d(),_e=l("p"),vd=r("You can also edit a loading script from the Hub to add your own modifications. Download the dataset repository locally so any data files referenced by a relative path in the loading script can be loaded:"),qn=d(),u(tt.$$.fragment),kn=d(),Us=l("p"),$d=r("Make your edits to the loading script and then load it by passing its local path to "),ve=l("a"),yd=r("load_dataset()"),wd=r(":"),En=d(),u(et.$$.fragment),Pn=d(),ps=l("h2"),Ys=l("a"),Cl=l("span"),u(lt.$$.fragment),jd=d(),Il=l("span"),bd=r("Local and remote files"),xn=d(),C=l("p"),qd=r("Datasets can be loaded from local files stored on your computer and from remote files. The datasets are most likely stored as a "),Ol=l("code"),kd=r("csv"),Ed=r(", "),Ll=l("code"),Pd=r("json"),xd=r(", "),Hl=l("code"),Ad=r("txt"),Sd=r(" or "),Fl=l("code"),Dd=r("parquet"),Td=r(" file. The "),$e=l("a"),Nd=r("load_dataset()"),Cd=r(" function can load each of these file types."),An=d(),ds=l("h3"),Js=l("a"),Rl=l("span"),u(ot.$$.fragment),Id=d(),Ml=l("span"),Od=r("CSV"),Sn=d(),ye=l("p"),Ld=r("\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),Dn=d(),u(nt.$$.fragment),Tn=d(),we=l("p"),Hd=r("If you have more than one CSV file:"),Nn=d(),u(rt.$$.fragment),Cn=d(),je=l("p"),Fd=r("You can also map the training and test splits to specific CSV files:"),In=d(),u(it.$$.fragment),On=d(),be=l("p"),Rd=r("To load remote CSV files via HTTP, pass the URLs instead:"),Ln=d(),u(pt.$$.fragment),Hn=d(),qe=l("p"),Md=r("To load zipped CSV files:"),Fn=d(),u(dt.$$.fragment),Rn=d(),fs=l("h3"),Ws=l("a"),zl=l("span"),u(ft.$$.fragment),zd=d(),Vl=l("span"),Vd=r("JSON"),Mn=d(),Bs=l("p"),Ud=r("JSON files are loaded directly with "),ke=l("a"),Yd=r("load_dataset()"),Jd=r(" as shown below:"),zn=d(),u(ct.$$.fragment),Vn=d(),Ee=l("p"),Wd=r("JSON files have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),Un=d(),u(ht.$$.fragment),Yn=d(),Qs=l("p"),Bd=r("Another JSON format you may encounter is a nested field, in which case you\u2019ll need to specify the "),Ul=l("code"),Qd=r("field"),Gd=r(" argument as shown in the following:"),Jn=d(),u(ut.$$.fragment),Wn=d(),Pe=l("p"),Kd=r("To load remote JSON files via HTTP, pass the URLs instead:"),Bn=d(),u(mt.$$.fragment),Qn=d(),xe=l("p"),Xd=r("While these are the most common JSON formats, you\u2019ll see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats and will fallback accordingly on the Python JSON loading methods to handle them."),Gn=d(),cs=l("h3"),Gs=l("a"),Yl=l("span"),u(gt.$$.fragment),Zd=d(),Jl=l("span"),sf=r("Parquet"),Kn=d(),Ae=l("p"),af=r("Parquet files are stored in a columnar format, unlike row-based files like a CSV. Large datasets may be stored in a Parquet file because it is more efficient and faster at returning your query."),Xn=d(),Se=l("p"),tf=r("To load a Parquet file:"),Zn=d(),u(_t.$$.fragment),sr=d(),De=l("p"),ef=r("To load remote Parquet files via HTTP, pass the URLs instead:"),ar=d(),u(vt.$$.fragment),tr=d(),hs=l("h3"),Ks=l("a"),Wl=l("span"),u($t.$$.fragment),lf=d(),Bl=l("span"),of=r("SQL"),er=d(),Xs=l("p"),nf=r("Read database contents with "),Te=l("a"),rf=r("Dataset.from_sql()"),pf=r(". Both table names and queries are supported."),lr=d(),Ne=l("p"),df=r("For example, a table from a SQLite file can be loaded with:"),or=d(),u(yt.$$.fragment),nr=d(),Ce=l("p"),ff=r("Use a query for a more precise read:"),rr=d(),u(wt.$$.fragment),ir=d(),u(Zs.$$.fragment),pr=d(),us=l("h2"),sa=l("a"),Ql=l("span"),u(jt.$$.fragment),cf=d(),Gl=l("span"),hf=r("Multiprocessing"),dr=d(),Ie=l("p"),uf=r("When a dataset is made of several files (that we call \u201Cshards\u201D), it is possible to significantly speed up the dataset downloading and preparation step."),fr=d(),aa=l("p"),mf=r("You can choose how many processes you\u2019d like to use to prepare a dataset in parallel using "),Kl=l("code"),gf=r("num_proc"),_f=r(`.
In this case, each process is given a subset of shards to prepare:`),cr=d(),u(bt.$$.fragment),hr=d(),ms=l("h2"),ta=l("a"),Xl=l("span"),u(qt.$$.fragment),vf=d(),Zl=l("span"),$f=r("In-memory data"),ur=d(),ea=l("p"),yf=r("\u{1F917} Datasets will also allow you to create a "),Oe=l("a"),wf=r("Dataset"),jf=r(" directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),mr=d(),gs=l("h3"),la=l("a"),so=l("span"),u(kt.$$.fragment),bf=d(),ao=l("span"),qf=r("Python dictionary"),gr=d(),oa=l("p"),kf=r("Load Python dictionaries with "),Le=l("a"),Ef=r("from_dict()"),Pf=r(":"),_r=d(),u(Et.$$.fragment),vr=d(),_s=l("h3"),na=l("a"),to=l("span"),u(Pt.$$.fragment),xf=d(),eo=l("span"),Af=r("Python list of dictionaries"),$r=d(),ra=l("p"),Sf=r("Load a list of Python dictionaries with "),lo=l("code"),Df=r("from_list()"),Tf=r(":"),yr=d(),u(xt.$$.fragment),wr=d(),vs=l("h3"),ia=l("a"),oo=l("span"),u(At.$$.fragment),Nf=d(),no=l("span"),Cf=r("Python generator"),jr=d(),pa=l("p"),If=r("Create a dataset from a Python generator with "),He=l("a"),Of=r("from_generator()"),Lf=r(":"),br=d(),u(St.$$.fragment),qr=d(),Fe=l("p"),Hf=r("This approach supports loading data larger than available memory."),kr=d(),da=l("p"),Ff=r("You can also define a sharded dataset by passing lists to "),ro=l("code"),Rf=r("gen_kwargs"),Mf=r(":"),Er=d(),u(Dt.$$.fragment),Pr=d(),$s=l("h3"),fa=l("a"),io=l("span"),u(Tt.$$.fragment),zf=d(),po=l("span"),Vf=r("Pandas DataFrame"),xr=d(),ca=l("p"),Uf=r("Load Pandas DataFrames with "),Re=l("a"),Yf=r("from_pandas()"),Jf=r(":"),Ar=d(),u(Nt.$$.fragment),Sr=d(),u(ha.$$.fragment),Dr=d(),ys=l("h2"),ua=l("a"),fo=l("span"),u(Ct.$$.fragment),Wf=d(),co=l("span"),Bf=r("Offline"),Tr=d(),Me=l("p"),Qf=r("Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),Nr=d(),Z=l("p"),Gf=r("If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),ho=l("code"),Kf=r("HF_DATASETS_OFFLINE"),Xf=r(" to "),uo=l("code"),Zf=r("1"),sc=r(" to enable full offline mode."),Cr=d(),ws=l("h2"),ma=l("a"),mo=l("span"),u(It.$$.fragment),ac=d(),go=l("span"),tc=r("Slice splits"),Ir=d(),ss=l("p"),ec=r("You can also choose only to load specific slices of a split. There are two options for slicing a split: using strings or the "),ze=l("a"),lc=r("ReadInstruction"),oc=r(" API. Strings are more compact and readable for simple cases, while "),Ve=l("a"),nc=r("ReadInstruction"),rc=r(" is easier to use with variable slicing parameters."),Or=d(),as=l("p"),ic=r("Concatenate a "),_o=l("code"),pc=r("train"),dc=r(" and "),vo=l("code"),fc=r("test"),cc=r(" split by:"),Lr=d(),u(Ot.$$.fragment),Hr=d(),ga=l("p"),hc=r("Select specific rows of the "),$o=l("code"),uc=r("train"),mc=r(" split:"),Fr=d(),u(Lt.$$.fragment),Rr=d(),Ue=l("p"),gc=r("Or select a percentage of a split with:"),Mr=d(),u(Ht.$$.fragment),zr=d(),Ye=l("p"),_c=r("Select a combination of percentages from each split:"),Vr=d(),u(Ft.$$.fragment),Ur=d(),Je=l("p"),vc=r("Finally, you can even create cross-validated splits. The example below creates 10-fold cross-validated splits. Each validation dataset is a 10% chunk, and the training dataset makes up the remaining complementary 90% chunk:"),Yr=d(),u(Rt.$$.fragment),Jr=d(),js=l("h3"),_a=l("a"),yo=l("span"),u(Mt.$$.fragment),$c=d(),wo=l("span"),yc=r("Percent slicing and rounding"),Wr=d(),We=l("p"),wc=r("The default behavior is to round the boundaries to the nearest integer for datasets where the requested slice boundaries do not divide evenly by 100. As shown below, some slices may contain more examples than others. For instance, if the following train split includes 999 records, then:"),Br=d(),u(zt.$$.fragment),Qr=d(),va=l("p"),jc=r("If you want equal sized splits, use "),jo=l("code"),bc=r("pct1_dropremainder"),qc=r(" rounding instead. This treats the specified percentage boundaries as multiples of 1%."),Gr=d(),u(Vt.$$.fragment),Kr=d(),u($a.$$.fragment),Xr=d(),Be=l("a"),Zr=d(),bs=l("h2"),ya=l("a"),bo=l("span"),u(Ut.$$.fragment),kc=d(),qo=l("span"),Ec=r("Troubleshooting"),si=d(),Qe=l("p"),Pc=r("Sometimes, you may get unexpected results when you load a dataset. Two of the most common issues you may encounter are manually downloading a dataset and specifying features of a dataset."),ai=d(),qs=l("h3"),wa=l("a"),ko=l("span"),u(Yt.$$.fragment),xc=d(),Eo=l("span"),Ac=r("Manual download"),ti=d(),z=l("p"),Sc=r("Certain datasets require you to manually download the dataset files due to licensing incompatibility or if the files are hidden behind a login page. This causes "),Ge=l("a"),Dc=r("load_dataset()"),Tc=r(" to throw an "),Po=l("code"),Nc=r("AssertionError"),Cc=r(". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you\u2019ve downloaded the files, use the "),xo=l("code"),Ic=r("data_dir"),Oc=r(" argument to specify the path to the files you just downloaded."),ei=d(),ja=l("p"),Lc=r("For example, if you try to download a configuration from the "),Jt=l("a"),Hc=r("MATINF"),Fc=r(" dataset:"),li=d(),u(Wt.$$.fragment),oi=d(),H=l("p"),Rc=r("If you\u2019ve already downloaded a dataset from the "),Ao=l("em"),Mc=r("Hub with a loading script"),zc=r(" to your computer, then you need to pass an absolute path to the "),So=l("code"),Vc=r("data_dir"),Uc=r(" or "),Do=l("code"),Yc=r("data_files"),Jc=r(" parameter to load that dataset. Otherwise, if you pass a relative path, "),Ke=l("a"),Wc=r("load_dataset()"),Bc=r(" will load the directory from the repository on the Hub instead of the local directory."),ni=d(),ks=l("h3"),ba=l("a"),To=l("span"),u(Bt.$$.fragment),Qc=d(),No=l("span"),Gc=r("Specify features"),ri=d(),V=l("p"),Kc=r("When you create a dataset from local files, the "),Xe=l("a"),Xc=r("Features"),Zc=r(" are automatically inferred by "),Qt=l("a"),sh=r("Apache Arrow"),ah=r(". However, the dataset\u2019s features may not always align with your expectations, or you may want to define the features yourself. The following example shows how you can add custom labels with the "),Ze=l("a"),th=r("ClassLabel"),eh=r(" feature."),ii=d(),qa=l("p"),lh=r("Start by defining your own labels with the "),sl=l("a"),oh=r("Features"),nh=r(" class:"),pi=d(),u(Gt.$$.fragment),di=d(),ts=l("p"),rh=r("Next, specify the "),Co=l("code"),ih=r("features"),ph=r(" parameter in "),al=l("a"),dh=r("load_dataset()"),fh=r(" with the features you just created:"),fi=d(),u(Kt.$$.fragment),ci=d(),tl=l("p"),ch=r("Now when you look at your dataset features, you can see it uses the custom labels you defined:"),hi=d(),u(Xt.$$.fragment),ui=d(),Es=l("h2"),ka=l("a"),Io=l("span"),u(Zt.$$.fragment),hh=d(),Oo=l("span"),uh=r("Metrics"),mi=d(),u(Ea.$$.fragment),gi=d(),el=l("p"),mh=r("When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),_i=d(),u(se.$$.fragment),vi=d(),u(Pa.$$.fragment),$i=d(),Ps=l("h3"),xa=l("a"),Lo=l("span"),u(ae.$$.fragment),gh=d(),Ho=l("span"),_h=r("Load configurations"),yi=d(),es=l("p"),vh=r("It is possible for a metric to have different configurations. The configurations are stored in the "),Fo=l("code"),$h=r("config_name"),yh=r(" parameter in "),ll=l("a"),wh=r("MetricInfo"),jh=r(" attribute. When you load a metric, provide the configuration name as shown in the following:"),wi=d(),u(te.$$.fragment),ji=d(),xs=l("h3"),Aa=l("a"),Ro=l("span"),u(ee.$$.fragment),bh=d(),Mo=l("span"),qh=r("Distributed setup"),bi=d(),ol=l("p"),kh=r("When working in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),qi=d(),nl=l("p"),Eh=r("For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),ki=d(),ls=l("ol"),zo=l("li"),le=l("p"),Ph=r("Define the total number of processes with the "),Vo=l("code"),xh=r("num_process"),Ah=r(" argument."),Sh=d(),Uo=l("li"),As=l("p"),Dh=r("Set the process "),Yo=l("code"),Th=r("rank"),Nh=r(" as an integer between zero and "),Jo=l("code"),Ch=r("num_process - 1"),Ih=r("."),Oh=d(),Wo=l("li"),oe=l("p"),Lh=r("Load your metric with "),rl=l("a"),Hh=r("load_metric()"),Fh=r(" with these arguments:"),Ei=d(),u(ne.$$.fragment),Pi=d(),u(Sa.$$.fragment),xi=d(),Da=l("p"),Rh=r("In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),Bo=l("code"),Mh=r("experiment_id"),zh=r(" to distinguish the separate evaluations:"),Ai=d(),u(re.$$.fragment),this.h()},l(s){const e=Gg('[data-svelte="svelte-1phssyn"]',document.head);h=o(e,"META",{name:!0,content:!0}),e.forEach(a),k=f(s),y=o(s,"H1",{class:!0});var ie=n(y);b=o(ie,"A",{id:!0,class:!0,href:!0});var Qo=n(b);q=o(Qo,"SPAN",{});var Go=n(q);m(w.$$.fragment,Go),Go.forEach(a),Qo.forEach(a),j=f(ie),P=o(ie,"SPAN",{});var Ko=n(P);I=i(Ko,"Load"),Ko.forEach(a),ie.forEach(a),T=f(s),F=o(s,"P",{});var Xo=n(F);R=i(Xo,"Your data can be stored in various places; they can be on your local machine\u2019s disk, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever a dataset is stored, \u{1F917} Datasets can help you load it."),Xo.forEach(a),Ss=f(s),Q=o(s,"P",{});var Zo=n(Q);G=i(Zo,"This guide will show you how to load a dataset from:"),Zo.forEach(a),Ds=f(s),S=o(s,"UL",{});var L=n(S);J=o(L,"LI",{});var sn=n(J);Oa=i(sn,"The Hub without a dataset loading script"),sn.forEach(a),La=f(L),W=o(L,"LI",{});var Vh=n(W);Ha=i(Vh,"Local loading script"),Vh.forEach(a),Fa=f(L),M=o(L,"LI",{});var Uh=n(M);Ra=i(Uh,"Local files"),Uh.forEach(a),Ma=f(L),B=o(L,"LI",{});var Yh=n(B);x=i(Yh,"In-memory data"),Yh.forEach(a),pe=f(L),Ts=o(L,"LI",{});var Jh=n(Ts);de=i(Jh,"Offline"),Jh.forEach(a),fe=f(L),Ns=o(L,"LI",{});var Wh=n(Ns);ce=i(Wh,"A specific slice of a split"),Wh.forEach(a),L.forEach(a),za=f(s),O=o(s,"P",{});var Ta=n(O);bp=i(Ta,"For more details specific to loading other dataset modalities, take a look at the "),Va=o(Ta,"A",{class:!0,href:!0});var Bh=n(Va);qp=i(Bh,"load audio dataset guide"),Bh.forEach(a),kp=i(Ta,", the "),Ua=o(Ta,"A",{class:!0,href:!0});var Qh=n(Ua);Ep=i(Qh,"load image dataset guide"),Qh.forEach(a),Pp=i(Ta,", or the "),Ya=o(Ta,"A",{class:!0,href:!0});var Gh=n(Ya);xp=i(Gh,"load text dataset guide"),Gh.forEach(a),Ap=i(Ta,"."),Ta.forEach(a),tn=f(s),he=o(s,"A",{id:!0}),n(he).forEach(a),en=f(s),ns=o(s,"H2",{class:!0});var Di=n(ns);Cs=o(Di,"A",{id:!0,class:!0,href:!0});var Kh=n(Cs);_l=o(Kh,"SPAN",{});var Xh=n(_l);m(Ja.$$.fragment,Xh),Xh.forEach(a),Kh.forEach(a),Sp=f(Di),vl=o(Di,"SPAN",{});var Zh=n(vl);Dp=i(Zh,"Hugging Face Hub"),Zh.forEach(a),Di.forEach(a),ln=f(s),K=o(s,"P",{});var il=n(K);Tp=i(il,"Datasets are loaded from a dataset loading script that downloads and generates the dataset. However, you can also load a dataset from any dataset repository on the Hub without a loading script! Begin by "),ue=o(il,"A",{href:!0});var su=n(ue);Np=i(su,"creating a dataset repository"),su.forEach(a),Cp=i(il," and upload your data files. Now you can use the "),me=o(il,"A",{href:!0});var au=n(me);Ip=i(au,"load_dataset()"),au.forEach(a),Op=i(il," function to load the dataset."),il.forEach(a),on=f(s),Is=o(s,"P",{});var Ti=n(Is);Lp=i(Ti,"For example, try loading the files from this "),Wa=o(Ti,"A",{href:!0,rel:!0});var tu=n(Wa);Hp=i(tu,"demo repository"),tu.forEach(a),Fp=i(Ti," by providing the repository namespace and dataset name. This dataset repository contains CSV files, and the code below loads the dataset from the CSV files:"),Ti.forEach(a),nn=f(s),m(Ba.$$.fragment,s),rn=f(s),Os=o(s,"P",{});var Ni=n(Os);Rp=i(Ni,"Some datasets may have more than one version based on Git tags, branches, or commits. Use the "),$l=o(Ni,"CODE",{});var eu=n($l);Mp=i(eu,"revision"),eu.forEach(a),zp=i(Ni," parameter to specify the dataset version you want to load:"),Ni.forEach(a),pn=f(s),m(Qa.$$.fragment,s),dn=f(s),m(Ls.$$.fragment,s),fn=f(s),N=o(s,"P",{});var U=n(N);Vp=i(U,"A dataset without a loading script by default loads all the data into the "),yl=o(U,"CODE",{});var lu=n(yl);Up=i(lu,"train"),lu.forEach(a),Yp=i(U," split. Use the "),wl=o(U,"CODE",{});var ou=n(wl);Jp=i(ou,"data_files"),ou.forEach(a),Wp=i(U," parameter to map data files to splits like "),jl=o(U,"CODE",{});var nu=n(jl);Bp=i(nu,"train"),nu.forEach(a),Qp=i(U,", "),bl=o(U,"CODE",{});var ru=n(bl);Gp=i(ru,"validation"),ru.forEach(a),Kp=i(U," and "),ql=o(U,"CODE",{});var iu=n(ql);Xp=i(iu,"test"),iu.forEach(a),Zp=i(U,":"),U.forEach(a),cn=f(s),m(Ga.$$.fragment,s),hn=f(s),m(Hs.$$.fragment,s),un=f(s),X=o(s,"P",{});var pl=n(X);sd=i(pl,"You can also load a specific subset of the files with the "),kl=o(pl,"CODE",{});var pu=n(kl);ad=i(pu,"data_files"),pu.forEach(a),td=i(pl," or "),El=o(pl,"CODE",{});var du=n(El);ed=i(du,"data_dir"),du.forEach(a),ld=i(pl," parameter. These parameters can accept a relative path which resolves to the base path corresponding to where the dataset is loaded from."),pl.forEach(a),mn=f(s),m(Ka.$$.fragment,s),gn=f(s),Fs=o(s,"P",{});var Ci=n(Fs);od=i(Ci,"The "),Pl=o(Ci,"CODE",{});var fu=n(Pl);nd=i(fu,"split"),fu.forEach(a),rd=i(Ci," parameter can also map a data file to a specific split:"),Ci.forEach(a),_n=f(s),m(Xa.$$.fragment,s),vn=f(s),rs=o(s,"H2",{class:!0});var Ii=n(rs);Rs=o(Ii,"A",{id:!0,class:!0,href:!0});var cu=n(Rs);xl=o(cu,"SPAN",{});var hu=n(xl);m(Za.$$.fragment,hu),hu.forEach(a),cu.forEach(a),id=f(Ii),Al=o(Ii,"SPAN",{});var uu=n(Al);pd=i(uu,"Local loading script"),uu.forEach(a),Ii.forEach(a),$n=f(s),Ms=o(s,"P",{});var Oi=n(Ms);dd=i(Oi,"You may have a \u{1F917} Datasets loading script locally on your computer. In this case, load the dataset by passing one of the following paths to "),ge=o(Oi,"A",{href:!0});var mu=n(ge);fd=i(mu,"load_dataset()"),mu.forEach(a),cd=i(Oi,":"),Oi.forEach(a),yn=f(s),zs=o(s,"UL",{});var Li=n(zs);Sl=o(Li,"LI",{});var gu=n(Sl);hd=i(gu,"The local path to the loading script file."),gu.forEach(a),ud=f(Li),Dl=o(Li,"LI",{});var _u=n(Dl);md=i(_u,"The local path to the directory containing the loading script file (only if the script file has the same name as the directory)."),_u.forEach(a),Li.forEach(a),wn=f(s),m(st.$$.fragment,s),jn=f(s),is=o(s,"H3",{class:!0});var Hi=n(is);Vs=o(Hi,"A",{id:!0,class:!0,href:!0});var vu=n(Vs);Tl=o(vu,"SPAN",{});var $u=n(Tl);m(at.$$.fragment,$u),$u.forEach(a),vu.forEach(a),gd=f(Hi),Nl=o(Hi,"SPAN",{});var yu=n(Nl);_d=i(yu,"Edit loading script"),yu.forEach(a),Hi.forEach(a),bn=f(s),_e=o(s,"P",{});var wu=n(_e);vd=i(wu,"You can also edit a loading script from the Hub to add your own modifications. Download the dataset repository locally so any data files referenced by a relative path in the loading script can be loaded:"),wu.forEach(a),qn=f(s),m(tt.$$.fragment,s),kn=f(s),Us=o(s,"P",{});var Fi=n(Us);$d=i(Fi,"Make your edits to the loading script and then load it by passing its local path to "),ve=o(Fi,"A",{href:!0});var ju=n(ve);yd=i(ju,"load_dataset()"),ju.forEach(a),wd=i(Fi,":"),Fi.forEach(a),En=f(s),m(et.$$.fragment,s),Pn=f(s),ps=o(s,"H2",{class:!0});var Ri=n(ps);Ys=o(Ri,"A",{id:!0,class:!0,href:!0});var bu=n(Ys);Cl=o(bu,"SPAN",{});var qu=n(Cl);m(lt.$$.fragment,qu),qu.forEach(a),bu.forEach(a),jd=f(Ri),Il=o(Ri,"SPAN",{});var ku=n(Il);bd=i(ku,"Local and remote files"),ku.forEach(a),Ri.forEach(a),xn=f(s),C=o(s,"P",{});var Y=n(C);qd=i(Y,"Datasets can be loaded from local files stored on your computer and from remote files. The datasets are most likely stored as a "),Ol=o(Y,"CODE",{});var Eu=n(Ol);kd=i(Eu,"csv"),Eu.forEach(a),Ed=i(Y,", "),Ll=o(Y,"CODE",{});var Pu=n(Ll);Pd=i(Pu,"json"),Pu.forEach(a),xd=i(Y,", "),Hl=o(Y,"CODE",{});var xu=n(Hl);Ad=i(xu,"txt"),xu.forEach(a),Sd=i(Y," or "),Fl=o(Y,"CODE",{});var Au=n(Fl);Dd=i(Au,"parquet"),Au.forEach(a),Td=i(Y," file. The "),$e=o(Y,"A",{href:!0});var Su=n($e);Nd=i(Su,"load_dataset()"),Su.forEach(a),Cd=i(Y," function can load each of these file types."),Y.forEach(a),An=f(s),ds=o(s,"H3",{class:!0});var Mi=n(ds);Js=o(Mi,"A",{id:!0,class:!0,href:!0});var Du=n(Js);Rl=o(Du,"SPAN",{});var Tu=n(Rl);m(ot.$$.fragment,Tu),Tu.forEach(a),Du.forEach(a),Id=f(Mi),Ml=o(Mi,"SPAN",{});var Nu=n(Ml);Od=i(Nu,"CSV"),Nu.forEach(a),Mi.forEach(a),Sn=f(s),ye=o(s,"P",{});var Cu=n(ye);Ld=i(Cu,"\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),Cu.forEach(a),Dn=f(s),m(nt.$$.fragment,s),Tn=f(s),we=o(s,"P",{});var Iu=n(we);Hd=i(Iu,"If you have more than one CSV file:"),Iu.forEach(a),Nn=f(s),m(rt.$$.fragment,s),Cn=f(s),je=o(s,"P",{});var Ou=n(je);Fd=i(Ou,"You can also map the training and test splits to specific CSV files:"),Ou.forEach(a),In=f(s),m(it.$$.fragment,s),On=f(s),be=o(s,"P",{});var Lu=n(be);Rd=i(Lu,"To load remote CSV files via HTTP, pass the URLs instead:"),Lu.forEach(a),Ln=f(s),m(pt.$$.fragment,s),Hn=f(s),qe=o(s,"P",{});var Hu=n(qe);Md=i(Hu,"To load zipped CSV files:"),Hu.forEach(a),Fn=f(s),m(dt.$$.fragment,s),Rn=f(s),fs=o(s,"H3",{class:!0});var zi=n(fs);Ws=o(zi,"A",{id:!0,class:!0,href:!0});var Fu=n(Ws);zl=o(Fu,"SPAN",{});var Ru=n(zl);m(ft.$$.fragment,Ru),Ru.forEach(a),Fu.forEach(a),zd=f(zi),Vl=o(zi,"SPAN",{});var Mu=n(Vl);Vd=i(Mu,"JSON"),Mu.forEach(a),zi.forEach(a),Mn=f(s),Bs=o(s,"P",{});var Vi=n(Bs);Ud=i(Vi,"JSON files are loaded directly with "),ke=o(Vi,"A",{href:!0});var zu=n(ke);Yd=i(zu,"load_dataset()"),zu.forEach(a),Jd=i(Vi," as shown below:"),Vi.forEach(a),zn=f(s),m(ct.$$.fragment,s),Vn=f(s),Ee=o(s,"P",{});var Vu=n(Ee);Wd=i(Vu,"JSON files have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),Vu.forEach(a),Un=f(s),m(ht.$$.fragment,s),Yn=f(s),Qs=o(s,"P",{});var Ui=n(Qs);Bd=i(Ui,"Another JSON format you may encounter is a nested field, in which case you\u2019ll need to specify the "),Ul=o(Ui,"CODE",{});var Uu=n(Ul);Qd=i(Uu,"field"),Uu.forEach(a),Gd=i(Ui," argument as shown in the following:"),Ui.forEach(a),Jn=f(s),m(ut.$$.fragment,s),Wn=f(s),Pe=o(s,"P",{});var Yu=n(Pe);Kd=i(Yu,"To load remote JSON files via HTTP, pass the URLs instead:"),Yu.forEach(a),Bn=f(s),m(mt.$$.fragment,s),Qn=f(s),xe=o(s,"P",{});var Ju=n(xe);Xd=i(Ju,"While these are the most common JSON formats, you\u2019ll see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats and will fallback accordingly on the Python JSON loading methods to handle them."),Ju.forEach(a),Gn=f(s),cs=o(s,"H3",{class:!0});var Yi=n(cs);Gs=o(Yi,"A",{id:!0,class:!0,href:!0});var Wu=n(Gs);Yl=o(Wu,"SPAN",{});var Bu=n(Yl);m(gt.$$.fragment,Bu),Bu.forEach(a),Wu.forEach(a),Zd=f(Yi),Jl=o(Yi,"SPAN",{});var Qu=n(Jl);sf=i(Qu,"Parquet"),Qu.forEach(a),Yi.forEach(a),Kn=f(s),Ae=o(s,"P",{});var Gu=n(Ae);af=i(Gu,"Parquet files are stored in a columnar format, unlike row-based files like a CSV. Large datasets may be stored in a Parquet file because it is more efficient and faster at returning your query."),Gu.forEach(a),Xn=f(s),Se=o(s,"P",{});var Ku=n(Se);tf=i(Ku,"To load a Parquet file:"),Ku.forEach(a),Zn=f(s),m(_t.$$.fragment,s),sr=f(s),De=o(s,"P",{});var Xu=n(De);ef=i(Xu,"To load remote Parquet files via HTTP, pass the URLs instead:"),Xu.forEach(a),ar=f(s),m(vt.$$.fragment,s),tr=f(s),hs=o(s,"H3",{class:!0});var Ji=n(hs);Ks=o(Ji,"A",{id:!0,class:!0,href:!0});var Zu=n(Ks);Wl=o(Zu,"SPAN",{});var sm=n(Wl);m($t.$$.fragment,sm),sm.forEach(a),Zu.forEach(a),lf=f(Ji),Bl=o(Ji,"SPAN",{});var am=n(Bl);of=i(am,"SQL"),am.forEach(a),Ji.forEach(a),er=f(s),Xs=o(s,"P",{});var Wi=n(Xs);nf=i(Wi,"Read database contents with "),Te=o(Wi,"A",{href:!0});var tm=n(Te);rf=i(tm,"Dataset.from_sql()"),tm.forEach(a),pf=i(Wi,". Both table names and queries are supported."),Wi.forEach(a),lr=f(s),Ne=o(s,"P",{});var em=n(Ne);df=i(em,"For example, a table from a SQLite file can be loaded with:"),em.forEach(a),or=f(s),m(yt.$$.fragment,s),nr=f(s),Ce=o(s,"P",{});var lm=n(Ce);ff=i(lm,"Use a query for a more precise read:"),lm.forEach(a),rr=f(s),m(wt.$$.fragment,s),ir=f(s),m(Zs.$$.fragment,s),pr=f(s),us=o(s,"H2",{class:!0});var Bi=n(us);sa=o(Bi,"A",{id:!0,class:!0,href:!0});var om=n(sa);Ql=o(om,"SPAN",{});var nm=n(Ql);m(jt.$$.fragment,nm),nm.forEach(a),om.forEach(a),cf=f(Bi),Gl=o(Bi,"SPAN",{});var rm=n(Gl);hf=i(rm,"Multiprocessing"),rm.forEach(a),Bi.forEach(a),dr=f(s),Ie=o(s,"P",{});var im=n(Ie);uf=i(im,"When a dataset is made of several files (that we call \u201Cshards\u201D), it is possible to significantly speed up the dataset downloading and preparation step."),im.forEach(a),fr=f(s),aa=o(s,"P",{});var Qi=n(aa);mf=i(Qi,"You can choose how many processes you\u2019d like to use to prepare a dataset in parallel using "),Kl=o(Qi,"CODE",{});var pm=n(Kl);gf=i(pm,"num_proc"),pm.forEach(a),_f=i(Qi,`.
In this case, each process is given a subset of shards to prepare:`),Qi.forEach(a),cr=f(s),m(bt.$$.fragment,s),hr=f(s),ms=o(s,"H2",{class:!0});var Gi=n(ms);ta=o(Gi,"A",{id:!0,class:!0,href:!0});var dm=n(ta);Xl=o(dm,"SPAN",{});var fm=n(Xl);m(qt.$$.fragment,fm),fm.forEach(a),dm.forEach(a),vf=f(Gi),Zl=o(Gi,"SPAN",{});var cm=n(Zl);$f=i(cm,"In-memory data"),cm.forEach(a),Gi.forEach(a),ur=f(s),ea=o(s,"P",{});var Ki=n(ea);yf=i(Ki,"\u{1F917} Datasets will also allow you to create a "),Oe=o(Ki,"A",{href:!0});var hm=n(Oe);wf=i(hm,"Dataset"),hm.forEach(a),jf=i(Ki," directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),Ki.forEach(a),mr=f(s),gs=o(s,"H3",{class:!0});var Xi=n(gs);la=o(Xi,"A",{id:!0,class:!0,href:!0});var um=n(la);so=o(um,"SPAN",{});var mm=n(so);m(kt.$$.fragment,mm),mm.forEach(a),um.forEach(a),bf=f(Xi),ao=o(Xi,"SPAN",{});var gm=n(ao);qf=i(gm,"Python dictionary"),gm.forEach(a),Xi.forEach(a),gr=f(s),oa=o(s,"P",{});var Zi=n(oa);kf=i(Zi,"Load Python dictionaries with "),Le=o(Zi,"A",{href:!0});var _m=n(Le);Ef=i(_m,"from_dict()"),_m.forEach(a),Pf=i(Zi,":"),Zi.forEach(a),_r=f(s),m(Et.$$.fragment,s),vr=f(s),_s=o(s,"H3",{class:!0});var sp=n(_s);na=o(sp,"A",{id:!0,class:!0,href:!0});var vm=n(na);to=o(vm,"SPAN",{});var $m=n(to);m(Pt.$$.fragment,$m),$m.forEach(a),vm.forEach(a),xf=f(sp),eo=o(sp,"SPAN",{});var ym=n(eo);Af=i(ym,"Python list of dictionaries"),ym.forEach(a),sp.forEach(a),$r=f(s),ra=o(s,"P",{});var ap=n(ra);Sf=i(ap,"Load a list of Python dictionaries with "),lo=o(ap,"CODE",{});var wm=n(lo);Df=i(wm,"from_list()"),wm.forEach(a),Tf=i(ap,":"),ap.forEach(a),yr=f(s),m(xt.$$.fragment,s),wr=f(s),vs=o(s,"H3",{class:!0});var tp=n(vs);ia=o(tp,"A",{id:!0,class:!0,href:!0});var jm=n(ia);oo=o(jm,"SPAN",{});var bm=n(oo);m(At.$$.fragment,bm),bm.forEach(a),jm.forEach(a),Nf=f(tp),no=o(tp,"SPAN",{});var qm=n(no);Cf=i(qm,"Python generator"),qm.forEach(a),tp.forEach(a),jr=f(s),pa=o(s,"P",{});var ep=n(pa);If=i(ep,"Create a dataset from a Python generator with "),He=o(ep,"A",{href:!0});var km=n(He);Of=i(km,"from_generator()"),km.forEach(a),Lf=i(ep,":"),ep.forEach(a),br=f(s),m(St.$$.fragment,s),qr=f(s),Fe=o(s,"P",{});var Em=n(Fe);Hf=i(Em,"This approach supports loading data larger than available memory."),Em.forEach(a),kr=f(s),da=o(s,"P",{});var lp=n(da);Ff=i(lp,"You can also define a sharded dataset by passing lists to "),ro=o(lp,"CODE",{});var Pm=n(ro);Rf=i(Pm,"gen_kwargs"),Pm.forEach(a),Mf=i(lp,":"),lp.forEach(a),Er=f(s),m(Dt.$$.fragment,s),Pr=f(s),$s=o(s,"H3",{class:!0});var op=n($s);fa=o(op,"A",{id:!0,class:!0,href:!0});var xm=n(fa);io=o(xm,"SPAN",{});var Am=n(io);m(Tt.$$.fragment,Am),Am.forEach(a),xm.forEach(a),zf=f(op),po=o(op,"SPAN",{});var Sm=n(po);Vf=i(Sm,"Pandas DataFrame"),Sm.forEach(a),op.forEach(a),xr=f(s),ca=o(s,"P",{});var np=n(ca);Uf=i(np,"Load Pandas DataFrames with "),Re=o(np,"A",{href:!0});var Dm=n(Re);Yf=i(Dm,"from_pandas()"),Dm.forEach(a),Jf=i(np,":"),np.forEach(a),Ar=f(s),m(Nt.$$.fragment,s),Sr=f(s),m(ha.$$.fragment,s),Dr=f(s),ys=o(s,"H2",{class:!0});var rp=n(ys);ua=o(rp,"A",{id:!0,class:!0,href:!0});var Tm=n(ua);fo=o(Tm,"SPAN",{});var Nm=n(fo);m(Ct.$$.fragment,Nm),Nm.forEach(a),Tm.forEach(a),Wf=f(rp),co=o(rp,"SPAN",{});var Cm=n(co);Bf=i(Cm,"Offline"),Cm.forEach(a),rp.forEach(a),Tr=f(s),Me=o(s,"P",{});var Im=n(Me);Qf=i(Im,"Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),Im.forEach(a),Nr=f(s),Z=o(s,"P",{});var dl=n(Z);Gf=i(dl,"If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),ho=o(dl,"CODE",{});var Om=n(ho);Kf=i(Om,"HF_DATASETS_OFFLINE"),Om.forEach(a),Xf=i(dl," to "),uo=o(dl,"CODE",{});var Lm=n(uo);Zf=i(Lm,"1"),Lm.forEach(a),sc=i(dl," to enable full offline mode."),dl.forEach(a),Cr=f(s),ws=o(s,"H2",{class:!0});var ip=n(ws);ma=o(ip,"A",{id:!0,class:!0,href:!0});var Hm=n(ma);mo=o(Hm,"SPAN",{});var Fm=n(mo);m(It.$$.fragment,Fm),Fm.forEach(a),Hm.forEach(a),ac=f(ip),go=o(ip,"SPAN",{});var Rm=n(go);tc=i(Rm,"Slice splits"),Rm.forEach(a),ip.forEach(a),Ir=f(s),ss=o(s,"P",{});var fl=n(ss);ec=i(fl,"You can also choose only to load specific slices of a split. There are two options for slicing a split: using strings or the "),ze=o(fl,"A",{href:!0});var Mm=n(ze);lc=i(Mm,"ReadInstruction"),Mm.forEach(a),oc=i(fl," API. Strings are more compact and readable for simple cases, while "),Ve=o(fl,"A",{href:!0});var zm=n(Ve);nc=i(zm,"ReadInstruction"),zm.forEach(a),rc=i(fl," is easier to use with variable slicing parameters."),fl.forEach(a),Or=f(s),as=o(s,"P",{});var cl=n(as);ic=i(cl,"Concatenate a "),_o=o(cl,"CODE",{});var Vm=n(_o);pc=i(Vm,"train"),Vm.forEach(a),dc=i(cl," and "),vo=o(cl,"CODE",{});var Um=n(vo);fc=i(Um,"test"),Um.forEach(a),cc=i(cl," split by:"),cl.forEach(a),Lr=f(s),m(Ot.$$.fragment,s),Hr=f(s),ga=o(s,"P",{});var pp=n(ga);hc=i(pp,"Select specific rows of the "),$o=o(pp,"CODE",{});var Ym=n($o);uc=i(Ym,"train"),Ym.forEach(a),mc=i(pp," split:"),pp.forEach(a),Fr=f(s),m(Lt.$$.fragment,s),Rr=f(s),Ue=o(s,"P",{});var Jm=n(Ue);gc=i(Jm,"Or select a percentage of a split with:"),Jm.forEach(a),Mr=f(s),m(Ht.$$.fragment,s),zr=f(s),Ye=o(s,"P",{});var Wm=n(Ye);_c=i(Wm,"Select a combination of percentages from each split:"),Wm.forEach(a),Vr=f(s),m(Ft.$$.fragment,s),Ur=f(s),Je=o(s,"P",{});var Bm=n(Je);vc=i(Bm,"Finally, you can even create cross-validated splits. The example below creates 10-fold cross-validated splits. Each validation dataset is a 10% chunk, and the training dataset makes up the remaining complementary 90% chunk:"),Bm.forEach(a),Yr=f(s),m(Rt.$$.fragment,s),Jr=f(s),js=o(s,"H3",{class:!0});var dp=n(js);_a=o(dp,"A",{id:!0,class:!0,href:!0});var Qm=n(_a);yo=o(Qm,"SPAN",{});var Gm=n(yo);m(Mt.$$.fragment,Gm),Gm.forEach(a),Qm.forEach(a),$c=f(dp),wo=o(dp,"SPAN",{});var Km=n(wo);yc=i(Km,"Percent slicing and rounding"),Km.forEach(a),dp.forEach(a),Wr=f(s),We=o(s,"P",{});var Xm=n(We);wc=i(Xm,"The default behavior is to round the boundaries to the nearest integer for datasets where the requested slice boundaries do not divide evenly by 100. As shown below, some slices may contain more examples than others. For instance, if the following train split includes 999 records, then:"),Xm.forEach(a),Br=f(s),m(zt.$$.fragment,s),Qr=f(s),va=o(s,"P",{});var fp=n(va);jc=i(fp,"If you want equal sized splits, use "),jo=o(fp,"CODE",{});var Zm=n(jo);bc=i(Zm,"pct1_dropremainder"),Zm.forEach(a),qc=i(fp," rounding instead. This treats the specified percentage boundaries as multiples of 1%."),fp.forEach(a),Gr=f(s),m(Vt.$$.fragment,s),Kr=f(s),m($a.$$.fragment,s),Xr=f(s),Be=o(s,"A",{id:!0}),n(Be).forEach(a),Zr=f(s),bs=o(s,"H2",{class:!0});var cp=n(bs);ya=o(cp,"A",{id:!0,class:!0,href:!0});var sg=n(ya);bo=o(sg,"SPAN",{});var ag=n(bo);m(Ut.$$.fragment,ag),ag.forEach(a),sg.forEach(a),kc=f(cp),qo=o(cp,"SPAN",{});var tg=n(qo);Ec=i(tg,"Troubleshooting"),tg.forEach(a),cp.forEach(a),si=f(s),Qe=o(s,"P",{});var eg=n(Qe);Pc=i(eg,"Sometimes, you may get unexpected results when you load a dataset. Two of the most common issues you may encounter are manually downloading a dataset and specifying features of a dataset."),eg.forEach(a),ai=f(s),qs=o(s,"H3",{class:!0});var hp=n(qs);wa=o(hp,"A",{id:!0,class:!0,href:!0});var lg=n(wa);ko=o(lg,"SPAN",{});var og=n(ko);m(Yt.$$.fragment,og),og.forEach(a),lg.forEach(a),xc=f(hp),Eo=o(hp,"SPAN",{});var ng=n(Eo);Ac=i(ng,"Manual download"),ng.forEach(a),hp.forEach(a),ti=f(s),z=o(s,"P",{});var Na=n(z);Sc=i(Na,"Certain datasets require you to manually download the dataset files due to licensing incompatibility or if the files are hidden behind a login page. This causes "),Ge=o(Na,"A",{href:!0});var rg=n(Ge);Dc=i(rg,"load_dataset()"),rg.forEach(a),Tc=i(Na," to throw an "),Po=o(Na,"CODE",{});var ig=n(Po);Nc=i(ig,"AssertionError"),ig.forEach(a),Cc=i(Na,". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you\u2019ve downloaded the files, use the "),xo=o(Na,"CODE",{});var pg=n(xo);Ic=i(pg,"data_dir"),pg.forEach(a),Oc=i(Na," argument to specify the path to the files you just downloaded."),Na.forEach(a),ei=f(s),ja=o(s,"P",{});var up=n(ja);Lc=i(up,"For example, if you try to download a configuration from the "),Jt=o(up,"A",{href:!0,rel:!0});var dg=n(Jt);Hc=i(dg,"MATINF"),dg.forEach(a),Fc=i(up," dataset:"),up.forEach(a),li=f(s),m(Wt.$$.fragment,s),oi=f(s),H=o(s,"P",{});var os=n(H);Rc=i(os,"If you\u2019ve already downloaded a dataset from the "),Ao=o(os,"EM",{});var fg=n(Ao);Mc=i(fg,"Hub with a loading script"),fg.forEach(a),zc=i(os," to your computer, then you need to pass an absolute path to the "),So=o(os,"CODE",{});var cg=n(So);Vc=i(cg,"data_dir"),cg.forEach(a),Uc=i(os," or "),Do=o(os,"CODE",{});var hg=n(Do);Yc=i(hg,"data_files"),hg.forEach(a),Jc=i(os," parameter to load that dataset. Otherwise, if you pass a relative path, "),Ke=o(os,"A",{href:!0});var ug=n(Ke);Wc=i(ug,"load_dataset()"),ug.forEach(a),Bc=i(os," will load the directory from the repository on the Hub instead of the local directory."),os.forEach(a),ni=f(s),ks=o(s,"H3",{class:!0});var mp=n(ks);ba=o(mp,"A",{id:!0,class:!0,href:!0});var mg=n(ba);To=o(mg,"SPAN",{});var gg=n(To);m(Bt.$$.fragment,gg),gg.forEach(a),mg.forEach(a),Qc=f(mp),No=o(mp,"SPAN",{});var _g=n(No);Gc=i(_g,"Specify features"),_g.forEach(a),mp.forEach(a),ri=f(s),V=o(s,"P",{});var Ca=n(V);Kc=i(Ca,"When you create a dataset from local files, the "),Xe=o(Ca,"A",{href:!0});var vg=n(Xe);Xc=i(vg,"Features"),vg.forEach(a),Zc=i(Ca," are automatically inferred by "),Qt=o(Ca,"A",{href:!0,rel:!0});var $g=n(Qt);sh=i($g,"Apache Arrow"),$g.forEach(a),ah=i(Ca,". However, the dataset\u2019s features may not always align with your expectations, or you may want to define the features yourself. The following example shows how you can add custom labels with the "),Ze=o(Ca,"A",{href:!0});var yg=n(Ze);th=i(yg,"ClassLabel"),yg.forEach(a),eh=i(Ca," feature."),Ca.forEach(a),ii=f(s),qa=o(s,"P",{});var gp=n(qa);lh=i(gp,"Start by defining your own labels with the "),sl=o(gp,"A",{href:!0});var wg=n(sl);oh=i(wg,"Features"),wg.forEach(a),nh=i(gp," class:"),gp.forEach(a),pi=f(s),m(Gt.$$.fragment,s),di=f(s),ts=o(s,"P",{});var hl=n(ts);rh=i(hl,"Next, specify the "),Co=o(hl,"CODE",{});var jg=n(Co);ih=i(jg,"features"),jg.forEach(a),ph=i(hl," parameter in "),al=o(hl,"A",{href:!0});var bg=n(al);dh=i(bg,"load_dataset()"),bg.forEach(a),fh=i(hl," with the features you just created:"),hl.forEach(a),fi=f(s),m(Kt.$$.fragment,s),ci=f(s),tl=o(s,"P",{});var qg=n(tl);ch=i(qg,"Now when you look at your dataset features, you can see it uses the custom labels you defined:"),qg.forEach(a),hi=f(s),m(Xt.$$.fragment,s),ui=f(s),Es=o(s,"H2",{class:!0});var _p=n(Es);ka=o(_p,"A",{id:!0,class:!0,href:!0});var kg=n(ka);Io=o(kg,"SPAN",{});var Eg=n(Io);m(Zt.$$.fragment,Eg),Eg.forEach(a),kg.forEach(a),hh=f(_p),Oo=o(_p,"SPAN",{});var Pg=n(Oo);uh=i(Pg,"Metrics"),Pg.forEach(a),_p.forEach(a),mi=f(s),m(Ea.$$.fragment,s),gi=f(s),el=o(s,"P",{});var xg=n(el);mh=i(xg,"When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),xg.forEach(a),_i=f(s),m(se.$$.fragment,s),vi=f(s),m(Pa.$$.fragment,s),$i=f(s),Ps=o(s,"H3",{class:!0});var vp=n(Ps);xa=o(vp,"A",{id:!0,class:!0,href:!0});var Ag=n(xa);Lo=o(Ag,"SPAN",{});var Sg=n(Lo);m(ae.$$.fragment,Sg),Sg.forEach(a),Ag.forEach(a),gh=f(vp),Ho=o(vp,"SPAN",{});var Dg=n(Ho);_h=i(Dg,"Load configurations"),Dg.forEach(a),vp.forEach(a),yi=f(s),es=o(s,"P",{});var ul=n(es);vh=i(ul,"It is possible for a metric to have different configurations. The configurations are stored in the "),Fo=o(ul,"CODE",{});var Tg=n(Fo);$h=i(Tg,"config_name"),Tg.forEach(a),yh=i(ul," parameter in "),ll=o(ul,"A",{href:!0});var Ng=n(ll);wh=i(Ng,"MetricInfo"),Ng.forEach(a),jh=i(ul," attribute. When you load a metric, provide the configuration name as shown in the following:"),ul.forEach(a),wi=f(s),m(te.$$.fragment,s),ji=f(s),xs=o(s,"H3",{class:!0});var $p=n(xs);Aa=o($p,"A",{id:!0,class:!0,href:!0});var Cg=n(Aa);Ro=o(Cg,"SPAN",{});var Ig=n(Ro);m(ee.$$.fragment,Ig),Ig.forEach(a),Cg.forEach(a),bh=f($p),Mo=o($p,"SPAN",{});var Og=n(Mo);qh=i(Og,"Distributed setup"),Og.forEach(a),$p.forEach(a),bi=f(s),ol=o(s,"P",{});var Lg=n(ol);kh=i(Lg,"When working in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),Lg.forEach(a),qi=f(s),nl=o(s,"P",{});var Hg=n(nl);Eh=i(Hg,"For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),Hg.forEach(a),ki=f(s),ls=o(s,"OL",{});var ml=n(ls);zo=o(ml,"LI",{});var Fg=n(zo);le=o(Fg,"P",{});var yp=n(le);Ph=i(yp,"Define the total number of processes with the "),Vo=o(yp,"CODE",{});var Rg=n(Vo);xh=i(Rg,"num_process"),Rg.forEach(a),Ah=i(yp," argument."),yp.forEach(a),Fg.forEach(a),Sh=f(ml),Uo=o(ml,"LI",{});var Mg=n(Uo);As=o(Mg,"P",{});var gl=n(As);Dh=i(gl,"Set the process "),Yo=o(gl,"CODE",{});var zg=n(Yo);Th=i(zg,"rank"),zg.forEach(a),Nh=i(gl," as an integer between zero and "),Jo=o(gl,"CODE",{});var Vg=n(Jo);Ch=i(Vg,"num_process - 1"),Vg.forEach(a),Ih=i(gl,"."),gl.forEach(a),Mg.forEach(a),Oh=f(ml),Wo=o(ml,"LI",{});var Ug=n(Wo);oe=o(Ug,"P",{});var wp=n(oe);Lh=i(wp,"Load your metric with "),rl=o(wp,"A",{href:!0});var Yg=n(rl);Hh=i(Yg,"load_metric()"),Yg.forEach(a),Fh=i(wp," with these arguments:"),wp.forEach(a),Ug.forEach(a),ml.forEach(a),Ei=f(s),m(ne.$$.fragment,s),Pi=f(s),m(Sa.$$.fragment,s),xi=f(s),Da=o(s,"P",{});var jp=n(Da);Rh=i(jp,"In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),Bo=o(jp,"CODE",{});var Jg=n(Bo);Mh=i(Jg,"experiment_id"),Jg.forEach(a),zh=i(jp," to distinguish the separate evaluations:"),jp.forEach(a),Ai=f(s),m(re.$$.fragment,s),this.h()},h(){c(h,"name","hf:doc:metadata"),c(h,"content",JSON.stringify(r_)),c(b,"id","load"),c(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(b,"href","#load"),c(y,"class","relative group"),c(Va,"class","underline decoration-pink-400 decoration-2 font-semibold"),c(Va,"href","./audio_load"),c(Ua,"class","underline decoration-yellow-400 decoration-2 font-semibold"),c(Ua,"href","./image_load"),c(Ya,"class","underline decoration-green-400 decoration-2 font-semibold"),c(Ya,"href","./nlp_load"),c(he,"id","load-from-the-hub"),c(Cs,"id","hugging-face-hub"),c(Cs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Cs,"href","#hugging-face-hub"),c(ns,"class","relative group"),c(ue,"href","share#create-the-repository"),c(me,"href","/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset"),c(Wa,"href","https://huggingface.co/datasets/lhoestq/demo1"),c(Wa,"rel","nofollow"),c(Rs,"id","local-loading-script"),c(Rs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Rs,"href","#local-loading-script"),c(rs,"class","relative group"),c(ge,"href","/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset"),c(Vs,"id","edit-loading-script"),c(Vs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Vs,"href","#edit-loading-script"),c(is,"class","relative group"),c(ve,"href","/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset"),c(Ys,"id","local-and-remote-files"),c(Ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ys,"href","#local-and-remote-files"),c(ps,"class","relative group"),c($e,"href","/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset"),c(Js,"id","csv"),c(Js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Js,"href","#csv"),c(ds,"class","relative group"),c(Ws,"id","json"),c(Ws,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ws,"href","#json"),c(fs,"class","relative group"),c(ke,"href","/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset"),c(Gs,"id","parquet"),c(Gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Gs,"href","#parquet"),c(cs,"class","relative group"),c(Ks,"id","sql"),c(Ks,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ks,"href","#sql"),c(hs,"class","relative group"),c(Te,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.from_sql"),c(sa,"id","multiprocessing"),c(sa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(sa,"href","#multiprocessing"),c(us,"class","relative group"),c(ta,"id","inmemory-data"),c(ta,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ta,"href","#inmemory-data"),c(ms,"class","relative group"),c(Oe,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset"),c(la,"id","python-dictionary"),c(la,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(la,"href","#python-dictionary"),c(gs,"class","relative group"),c(Le,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.from_dict"),c(na,"id","python-list-of-dictionaries"),c(na,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(na,"href","#python-list-of-dictionaries"),c(_s,"class","relative group"),c(ia,"id","python-generator"),c(ia,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ia,"href","#python-generator"),c(vs,"class","relative group"),c(He,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.from_generator"),c(fa,"id","pandas-dataframe"),c(fa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fa,"href","#pandas-dataframe"),c($s,"class","relative group"),c(Re,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.from_pandas"),c(ua,"id","offline"),c(ua,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ua,"href","#offline"),c(ys,"class","relative group"),c(ma,"id","slice-splits"),c(ma,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ma,"href","#slice-splits"),c(ws,"class","relative group"),c(ze,"href","/docs/datasets/main/en/package_reference/builder_classes#datasets.ReadInstruction"),c(Ve,"href","/docs/datasets/main/en/package_reference/builder_classes#datasets.ReadInstruction"),c(_a,"id","percent-slicing-and-rounding"),c(_a,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_a,"href","#percent-slicing-and-rounding"),c(js,"class","relative group"),c(Be,"id","troubleshoot"),c(ya,"id","troubleshooting"),c(ya,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ya,"href","#troubleshooting"),c(bs,"class","relative group"),c(wa,"id","manual-download"),c(wa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(wa,"href","#manual-download"),c(qs,"class","relative group"),c(Ge,"href","/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset"),c(Jt,"href","https://huggingface.co/datasets/matinf"),c(Jt,"rel","nofollow"),c(Ke,"href","/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset"),c(ba,"id","specify-features"),c(ba,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ba,"href","#specify-features"),c(ks,"class","relative group"),c(Xe,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Features"),c(Qt,"href","https://arrow.apache.org/docs/"),c(Qt,"rel","nofollow"),c(Ze,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.ClassLabel"),c(sl,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Features"),c(al,"href","/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset"),c(ka,"id","metrics"),c(ka,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ka,"href","#metrics"),c(Es,"class","relative group"),c(xa,"id","load-configurations"),c(xa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(xa,"href","#load-configurations"),c(Ps,"class","relative group"),c(ll,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.MetricInfo"),c(Aa,"id","distributed-setup"),c(Aa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Aa,"href","#distributed-setup"),c(xs,"class","relative group"),c(rl,"href","/docs/datasets/main/en/package_reference/loading_methods#datasets.load_metric")},m(s,e){t(document.head,h),p(s,k,e),p(s,y,e),t(y,b),t(b,q),g(w,q,null),t(y,j),t(y,P),t(P,I),p(s,T,e),p(s,F,e),t(F,R),p(s,Ss,e),p(s,Q,e),t(Q,G),p(s,Ds,e),p(s,S,e),t(S,J),t(J,Oa),t(S,La),t(S,W),t(W,Ha),t(S,Fa),t(S,M),t(M,Ra),t(S,Ma),t(S,B),t(B,x),t(S,pe),t(S,Ts),t(Ts,de),t(S,fe),t(S,Ns),t(Ns,ce),p(s,za,e),p(s,O,e),t(O,bp),t(O,Va),t(Va,qp),t(O,kp),t(O,Ua),t(Ua,Ep),t(O,Pp),t(O,Ya),t(Ya,xp),t(O,Ap),p(s,tn,e),p(s,he,e),p(s,en,e),p(s,ns,e),t(ns,Cs),t(Cs,_l),g(Ja,_l,null),t(ns,Sp),t(ns,vl),t(vl,Dp),p(s,ln,e),p(s,K,e),t(K,Tp),t(K,ue),t(ue,Np),t(K,Cp),t(K,me),t(me,Ip),t(K,Op),p(s,on,e),p(s,Is,e),t(Is,Lp),t(Is,Wa),t(Wa,Hp),t(Is,Fp),p(s,nn,e),g(Ba,s,e),p(s,rn,e),p(s,Os,e),t(Os,Rp),t(Os,$l),t($l,Mp),t(Os,zp),p(s,pn,e),g(Qa,s,e),p(s,dn,e),g(Ls,s,e),p(s,fn,e),p(s,N,e),t(N,Vp),t(N,yl),t(yl,Up),t(N,Yp),t(N,wl),t(wl,Jp),t(N,Wp),t(N,jl),t(jl,Bp),t(N,Qp),t(N,bl),t(bl,Gp),t(N,Kp),t(N,ql),t(ql,Xp),t(N,Zp),p(s,cn,e),g(Ga,s,e),p(s,hn,e),g(Hs,s,e),p(s,un,e),p(s,X,e),t(X,sd),t(X,kl),t(kl,ad),t(X,td),t(X,El),t(El,ed),t(X,ld),p(s,mn,e),g(Ka,s,e),p(s,gn,e),p(s,Fs,e),t(Fs,od),t(Fs,Pl),t(Pl,nd),t(Fs,rd),p(s,_n,e),g(Xa,s,e),p(s,vn,e),p(s,rs,e),t(rs,Rs),t(Rs,xl),g(Za,xl,null),t(rs,id),t(rs,Al),t(Al,pd),p(s,$n,e),p(s,Ms,e),t(Ms,dd),t(Ms,ge),t(ge,fd),t(Ms,cd),p(s,yn,e),p(s,zs,e),t(zs,Sl),t(Sl,hd),t(zs,ud),t(zs,Dl),t(Dl,md),p(s,wn,e),g(st,s,e),p(s,jn,e),p(s,is,e),t(is,Vs),t(Vs,Tl),g(at,Tl,null),t(is,gd),t(is,Nl),t(Nl,_d),p(s,bn,e),p(s,_e,e),t(_e,vd),p(s,qn,e),g(tt,s,e),p(s,kn,e),p(s,Us,e),t(Us,$d),t(Us,ve),t(ve,yd),t(Us,wd),p(s,En,e),g(et,s,e),p(s,Pn,e),p(s,ps,e),t(ps,Ys),t(Ys,Cl),g(lt,Cl,null),t(ps,jd),t(ps,Il),t(Il,bd),p(s,xn,e),p(s,C,e),t(C,qd),t(C,Ol),t(Ol,kd),t(C,Ed),t(C,Ll),t(Ll,Pd),t(C,xd),t(C,Hl),t(Hl,Ad),t(C,Sd),t(C,Fl),t(Fl,Dd),t(C,Td),t(C,$e),t($e,Nd),t(C,Cd),p(s,An,e),p(s,ds,e),t(ds,Js),t(Js,Rl),g(ot,Rl,null),t(ds,Id),t(ds,Ml),t(Ml,Od),p(s,Sn,e),p(s,ye,e),t(ye,Ld),p(s,Dn,e),g(nt,s,e),p(s,Tn,e),p(s,we,e),t(we,Hd),p(s,Nn,e),g(rt,s,e),p(s,Cn,e),p(s,je,e),t(je,Fd),p(s,In,e),g(it,s,e),p(s,On,e),p(s,be,e),t(be,Rd),p(s,Ln,e),g(pt,s,e),p(s,Hn,e),p(s,qe,e),t(qe,Md),p(s,Fn,e),g(dt,s,e),p(s,Rn,e),p(s,fs,e),t(fs,Ws),t(Ws,zl),g(ft,zl,null),t(fs,zd),t(fs,Vl),t(Vl,Vd),p(s,Mn,e),p(s,Bs,e),t(Bs,Ud),t(Bs,ke),t(ke,Yd),t(Bs,Jd),p(s,zn,e),g(ct,s,e),p(s,Vn,e),p(s,Ee,e),t(Ee,Wd),p(s,Un,e),g(ht,s,e),p(s,Yn,e),p(s,Qs,e),t(Qs,Bd),t(Qs,Ul),t(Ul,Qd),t(Qs,Gd),p(s,Jn,e),g(ut,s,e),p(s,Wn,e),p(s,Pe,e),t(Pe,Kd),p(s,Bn,e),g(mt,s,e),p(s,Qn,e),p(s,xe,e),t(xe,Xd),p(s,Gn,e),p(s,cs,e),t(cs,Gs),t(Gs,Yl),g(gt,Yl,null),t(cs,Zd),t(cs,Jl),t(Jl,sf),p(s,Kn,e),p(s,Ae,e),t(Ae,af),p(s,Xn,e),p(s,Se,e),t(Se,tf),p(s,Zn,e),g(_t,s,e),p(s,sr,e),p(s,De,e),t(De,ef),p(s,ar,e),g(vt,s,e),p(s,tr,e),p(s,hs,e),t(hs,Ks),t(Ks,Wl),g($t,Wl,null),t(hs,lf),t(hs,Bl),t(Bl,of),p(s,er,e),p(s,Xs,e),t(Xs,nf),t(Xs,Te),t(Te,rf),t(Xs,pf),p(s,lr,e),p(s,Ne,e),t(Ne,df),p(s,or,e),g(yt,s,e),p(s,nr,e),p(s,Ce,e),t(Ce,ff),p(s,rr,e),g(wt,s,e),p(s,ir,e),g(Zs,s,e),p(s,pr,e),p(s,us,e),t(us,sa),t(sa,Ql),g(jt,Ql,null),t(us,cf),t(us,Gl),t(Gl,hf),p(s,dr,e),p(s,Ie,e),t(Ie,uf),p(s,fr,e),p(s,aa,e),t(aa,mf),t(aa,Kl),t(Kl,gf),t(aa,_f),p(s,cr,e),g(bt,s,e),p(s,hr,e),p(s,ms,e),t(ms,ta),t(ta,Xl),g(qt,Xl,null),t(ms,vf),t(ms,Zl),t(Zl,$f),p(s,ur,e),p(s,ea,e),t(ea,yf),t(ea,Oe),t(Oe,wf),t(ea,jf),p(s,mr,e),p(s,gs,e),t(gs,la),t(la,so),g(kt,so,null),t(gs,bf),t(gs,ao),t(ao,qf),p(s,gr,e),p(s,oa,e),t(oa,kf),t(oa,Le),t(Le,Ef),t(oa,Pf),p(s,_r,e),g(Et,s,e),p(s,vr,e),p(s,_s,e),t(_s,na),t(na,to),g(Pt,to,null),t(_s,xf),t(_s,eo),t(eo,Af),p(s,$r,e),p(s,ra,e),t(ra,Sf),t(ra,lo),t(lo,Df),t(ra,Tf),p(s,yr,e),g(xt,s,e),p(s,wr,e),p(s,vs,e),t(vs,ia),t(ia,oo),g(At,oo,null),t(vs,Nf),t(vs,no),t(no,Cf),p(s,jr,e),p(s,pa,e),t(pa,If),t(pa,He),t(He,Of),t(pa,Lf),p(s,br,e),g(St,s,e),p(s,qr,e),p(s,Fe,e),t(Fe,Hf),p(s,kr,e),p(s,da,e),t(da,Ff),t(da,ro),t(ro,Rf),t(da,Mf),p(s,Er,e),g(Dt,s,e),p(s,Pr,e),p(s,$s,e),t($s,fa),t(fa,io),g(Tt,io,null),t($s,zf),t($s,po),t(po,Vf),p(s,xr,e),p(s,ca,e),t(ca,Uf),t(ca,Re),t(Re,Yf),t(ca,Jf),p(s,Ar,e),g(Nt,s,e),p(s,Sr,e),g(ha,s,e),p(s,Dr,e),p(s,ys,e),t(ys,ua),t(ua,fo),g(Ct,fo,null),t(ys,Wf),t(ys,co),t(co,Bf),p(s,Tr,e),p(s,Me,e),t(Me,Qf),p(s,Nr,e),p(s,Z,e),t(Z,Gf),t(Z,ho),t(ho,Kf),t(Z,Xf),t(Z,uo),t(uo,Zf),t(Z,sc),p(s,Cr,e),p(s,ws,e),t(ws,ma),t(ma,mo),g(It,mo,null),t(ws,ac),t(ws,go),t(go,tc),p(s,Ir,e),p(s,ss,e),t(ss,ec),t(ss,ze),t(ze,lc),t(ss,oc),t(ss,Ve),t(Ve,nc),t(ss,rc),p(s,Or,e),p(s,as,e),t(as,ic),t(as,_o),t(_o,pc),t(as,dc),t(as,vo),t(vo,fc),t(as,cc),p(s,Lr,e),g(Ot,s,e),p(s,Hr,e),p(s,ga,e),t(ga,hc),t(ga,$o),t($o,uc),t(ga,mc),p(s,Fr,e),g(Lt,s,e),p(s,Rr,e),p(s,Ue,e),t(Ue,gc),p(s,Mr,e),g(Ht,s,e),p(s,zr,e),p(s,Ye,e),t(Ye,_c),p(s,Vr,e),g(Ft,s,e),p(s,Ur,e),p(s,Je,e),t(Je,vc),p(s,Yr,e),g(Rt,s,e),p(s,Jr,e),p(s,js,e),t(js,_a),t(_a,yo),g(Mt,yo,null),t(js,$c),t(js,wo),t(wo,yc),p(s,Wr,e),p(s,We,e),t(We,wc),p(s,Br,e),g(zt,s,e),p(s,Qr,e),p(s,va,e),t(va,jc),t(va,jo),t(jo,bc),t(va,qc),p(s,Gr,e),g(Vt,s,e),p(s,Kr,e),g($a,s,e),p(s,Xr,e),p(s,Be,e),p(s,Zr,e),p(s,bs,e),t(bs,ya),t(ya,bo),g(Ut,bo,null),t(bs,kc),t(bs,qo),t(qo,Ec),p(s,si,e),p(s,Qe,e),t(Qe,Pc),p(s,ai,e),p(s,qs,e),t(qs,wa),t(wa,ko),g(Yt,ko,null),t(qs,xc),t(qs,Eo),t(Eo,Ac),p(s,ti,e),p(s,z,e),t(z,Sc),t(z,Ge),t(Ge,Dc),t(z,Tc),t(z,Po),t(Po,Nc),t(z,Cc),t(z,xo),t(xo,Ic),t(z,Oc),p(s,ei,e),p(s,ja,e),t(ja,Lc),t(ja,Jt),t(Jt,Hc),t(ja,Fc),p(s,li,e),g(Wt,s,e),p(s,oi,e),p(s,H,e),t(H,Rc),t(H,Ao),t(Ao,Mc),t(H,zc),t(H,So),t(So,Vc),t(H,Uc),t(H,Do),t(Do,Yc),t(H,Jc),t(H,Ke),t(Ke,Wc),t(H,Bc),p(s,ni,e),p(s,ks,e),t(ks,ba),t(ba,To),g(Bt,To,null),t(ks,Qc),t(ks,No),t(No,Gc),p(s,ri,e),p(s,V,e),t(V,Kc),t(V,Xe),t(Xe,Xc),t(V,Zc),t(V,Qt),t(Qt,sh),t(V,ah),t(V,Ze),t(Ze,th),t(V,eh),p(s,ii,e),p(s,qa,e),t(qa,lh),t(qa,sl),t(sl,oh),t(qa,nh),p(s,pi,e),g(Gt,s,e),p(s,di,e),p(s,ts,e),t(ts,rh),t(ts,Co),t(Co,ih),t(ts,ph),t(ts,al),t(al,dh),t(ts,fh),p(s,fi,e),g(Kt,s,e),p(s,ci,e),p(s,tl,e),t(tl,ch),p(s,hi,e),g(Xt,s,e),p(s,ui,e),p(s,Es,e),t(Es,ka),t(ka,Io),g(Zt,Io,null),t(Es,hh),t(Es,Oo),t(Oo,uh),p(s,mi,e),g(Ea,s,e),p(s,gi,e),p(s,el,e),t(el,mh),p(s,_i,e),g(se,s,e),p(s,vi,e),g(Pa,s,e),p(s,$i,e),p(s,Ps,e),t(Ps,xa),t(xa,Lo),g(ae,Lo,null),t(Ps,gh),t(Ps,Ho),t(Ho,_h),p(s,yi,e),p(s,es,e),t(es,vh),t(es,Fo),t(Fo,$h),t(es,yh),t(es,ll),t(ll,wh),t(es,jh),p(s,wi,e),g(te,s,e),p(s,ji,e),p(s,xs,e),t(xs,Aa),t(Aa,Ro),g(ee,Ro,null),t(xs,bh),t(xs,Mo),t(Mo,qh),p(s,bi,e),p(s,ol,e),t(ol,kh),p(s,qi,e),p(s,nl,e),t(nl,Eh),p(s,ki,e),p(s,ls,e),t(ls,zo),t(zo,le),t(le,Ph),t(le,Vo),t(Vo,xh),t(le,Ah),t(ls,Sh),t(ls,Uo),t(Uo,As),t(As,Dh),t(As,Yo),t(Yo,Th),t(As,Nh),t(As,Jo),t(Jo,Ch),t(As,Ih),t(ls,Oh),t(ls,Wo),t(Wo,oe),t(oe,Lh),t(oe,rl),t(rl,Hh),t(oe,Fh),p(s,Ei,e),g(ne,s,e),p(s,Pi,e),g(Sa,s,e),p(s,xi,e),p(s,Da,e),t(Da,Rh),t(Da,Bo),t(Bo,Mh),t(Da,zh),p(s,Ai,e),g(re,s,e),Si=!0},p(s,[e]){const ie={};e&2&&(ie.$$scope={dirty:e,ctx:s}),Ls.$set(ie);const Qo={};e&2&&(Qo.$$scope={dirty:e,ctx:s}),Hs.$set(Qo);const Go={};e&2&&(Go.$$scope={dirty:e,ctx:s}),Zs.$set(Go);const Ko={};e&2&&(Ko.$$scope={dirty:e,ctx:s}),ha.$set(Ko);const Xo={};e&2&&(Xo.$$scope={dirty:e,ctx:s}),$a.$set(Xo);const Zo={};e&2&&(Zo.$$scope={dirty:e,ctx:s}),Ea.$set(Zo);const L={};e&2&&(L.$$scope={dirty:e,ctx:s}),Pa.$set(L);const sn={};e&2&&(sn.$$scope={dirty:e,ctx:s}),Sa.$set(sn)},i(s){Si||(_(w.$$.fragment,s),_(Ja.$$.fragment,s),_(Ba.$$.fragment,s),_(Qa.$$.fragment,s),_(Ls.$$.fragment,s),_(Ga.$$.fragment,s),_(Hs.$$.fragment,s),_(Ka.$$.fragment,s),_(Xa.$$.fragment,s),_(Za.$$.fragment,s),_(st.$$.fragment,s),_(at.$$.fragment,s),_(tt.$$.fragment,s),_(et.$$.fragment,s),_(lt.$$.fragment,s),_(ot.$$.fragment,s),_(nt.$$.fragment,s),_(rt.$$.fragment,s),_(it.$$.fragment,s),_(pt.$$.fragment,s),_(dt.$$.fragment,s),_(ft.$$.fragment,s),_(ct.$$.fragment,s),_(ht.$$.fragment,s),_(ut.$$.fragment,s),_(mt.$$.fragment,s),_(gt.$$.fragment,s),_(_t.$$.fragment,s),_(vt.$$.fragment,s),_($t.$$.fragment,s),_(yt.$$.fragment,s),_(wt.$$.fragment,s),_(Zs.$$.fragment,s),_(jt.$$.fragment,s),_(bt.$$.fragment,s),_(qt.$$.fragment,s),_(kt.$$.fragment,s),_(Et.$$.fragment,s),_(Pt.$$.fragment,s),_(xt.$$.fragment,s),_(At.$$.fragment,s),_(St.$$.fragment,s),_(Dt.$$.fragment,s),_(Tt.$$.fragment,s),_(Nt.$$.fragment,s),_(ha.$$.fragment,s),_(Ct.$$.fragment,s),_(It.$$.fragment,s),_(Ot.$$.fragment,s),_(Lt.$$.fragment,s),_(Ht.$$.fragment,s),_(Ft.$$.fragment,s),_(Rt.$$.fragment,s),_(Mt.$$.fragment,s),_(zt.$$.fragment,s),_(Vt.$$.fragment,s),_($a.$$.fragment,s),_(Ut.$$.fragment,s),_(Yt.$$.fragment,s),_(Wt.$$.fragment,s),_(Bt.$$.fragment,s),_(Gt.$$.fragment,s),_(Kt.$$.fragment,s),_(Xt.$$.fragment,s),_(Zt.$$.fragment,s),_(Ea.$$.fragment,s),_(se.$$.fragment,s),_(Pa.$$.fragment,s),_(ae.$$.fragment,s),_(te.$$.fragment,s),_(ee.$$.fragment,s),_(ne.$$.fragment,s),_(Sa.$$.fragment,s),_(re.$$.fragment,s),Si=!0)},o(s){v(w.$$.fragment,s),v(Ja.$$.fragment,s),v(Ba.$$.fragment,s),v(Qa.$$.fragment,s),v(Ls.$$.fragment,s),v(Ga.$$.fragment,s),v(Hs.$$.fragment,s),v(Ka.$$.fragment,s),v(Xa.$$.fragment,s),v(Za.$$.fragment,s),v(st.$$.fragment,s),v(at.$$.fragment,s),v(tt.$$.fragment,s),v(et.$$.fragment,s),v(lt.$$.fragment,s),v(ot.$$.fragment,s),v(nt.$$.fragment,s),v(rt.$$.fragment,s),v(it.$$.fragment,s),v(pt.$$.fragment,s),v(dt.$$.fragment,s),v(ft.$$.fragment,s),v(ct.$$.fragment,s),v(ht.$$.fragment,s),v(ut.$$.fragment,s),v(mt.$$.fragment,s),v(gt.$$.fragment,s),v(_t.$$.fragment,s),v(vt.$$.fragment,s),v($t.$$.fragment,s),v(yt.$$.fragment,s),v(wt.$$.fragment,s),v(Zs.$$.fragment,s),v(jt.$$.fragment,s),v(bt.$$.fragment,s),v(qt.$$.fragment,s),v(kt.$$.fragment,s),v(Et.$$.fragment,s),v(Pt.$$.fragment,s),v(xt.$$.fragment,s),v(At.$$.fragment,s),v(St.$$.fragment,s),v(Dt.$$.fragment,s),v(Tt.$$.fragment,s),v(Nt.$$.fragment,s),v(ha.$$.fragment,s),v(Ct.$$.fragment,s),v(It.$$.fragment,s),v(Ot.$$.fragment,s),v(Lt.$$.fragment,s),v(Ht.$$.fragment,s),v(Ft.$$.fragment,s),v(Rt.$$.fragment,s),v(Mt.$$.fragment,s),v(zt.$$.fragment,s),v(Vt.$$.fragment,s),v($a.$$.fragment,s),v(Ut.$$.fragment,s),v(Yt.$$.fragment,s),v(Wt.$$.fragment,s),v(Bt.$$.fragment,s),v(Gt.$$.fragment,s),v(Kt.$$.fragment,s),v(Xt.$$.fragment,s),v(Zt.$$.fragment,s),v(Ea.$$.fragment,s),v(se.$$.fragment,s),v(Pa.$$.fragment,s),v(ae.$$.fragment,s),v(te.$$.fragment,s),v(ee.$$.fragment,s),v(ne.$$.fragment,s),v(Sa.$$.fragment,s),v(re.$$.fragment,s),Si=!1},d(s){a(h),s&&a(k),s&&a(y),$(w),s&&a(T),s&&a(F),s&&a(Ss),s&&a(Q),s&&a(Ds),s&&a(S),s&&a(za),s&&a(O),s&&a(tn),s&&a(he),s&&a(en),s&&a(ns),$(Ja),s&&a(ln),s&&a(K),s&&a(on),s&&a(Is),s&&a(nn),$(Ba,s),s&&a(rn),s&&a(Os),s&&a(pn),$(Qa,s),s&&a(dn),$(Ls,s),s&&a(fn),s&&a(N),s&&a(cn),$(Ga,s),s&&a(hn),$(Hs,s),s&&a(un),s&&a(X),s&&a(mn),$(Ka,s),s&&a(gn),s&&a(Fs),s&&a(_n),$(Xa,s),s&&a(vn),s&&a(rs),$(Za),s&&a($n),s&&a(Ms),s&&a(yn),s&&a(zs),s&&a(wn),$(st,s),s&&a(jn),s&&a(is),$(at),s&&a(bn),s&&a(_e),s&&a(qn),$(tt,s),s&&a(kn),s&&a(Us),s&&a(En),$(et,s),s&&a(Pn),s&&a(ps),$(lt),s&&a(xn),s&&a(C),s&&a(An),s&&a(ds),$(ot),s&&a(Sn),s&&a(ye),s&&a(Dn),$(nt,s),s&&a(Tn),s&&a(we),s&&a(Nn),$(rt,s),s&&a(Cn),s&&a(je),s&&a(In),$(it,s),s&&a(On),s&&a(be),s&&a(Ln),$(pt,s),s&&a(Hn),s&&a(qe),s&&a(Fn),$(dt,s),s&&a(Rn),s&&a(fs),$(ft),s&&a(Mn),s&&a(Bs),s&&a(zn),$(ct,s),s&&a(Vn),s&&a(Ee),s&&a(Un),$(ht,s),s&&a(Yn),s&&a(Qs),s&&a(Jn),$(ut,s),s&&a(Wn),s&&a(Pe),s&&a(Bn),$(mt,s),s&&a(Qn),s&&a(xe),s&&a(Gn),s&&a(cs),$(gt),s&&a(Kn),s&&a(Ae),s&&a(Xn),s&&a(Se),s&&a(Zn),$(_t,s),s&&a(sr),s&&a(De),s&&a(ar),$(vt,s),s&&a(tr),s&&a(hs),$($t),s&&a(er),s&&a(Xs),s&&a(lr),s&&a(Ne),s&&a(or),$(yt,s),s&&a(nr),s&&a(Ce),s&&a(rr),$(wt,s),s&&a(ir),$(Zs,s),s&&a(pr),s&&a(us),$(jt),s&&a(dr),s&&a(Ie),s&&a(fr),s&&a(aa),s&&a(cr),$(bt,s),s&&a(hr),s&&a(ms),$(qt),s&&a(ur),s&&a(ea),s&&a(mr),s&&a(gs),$(kt),s&&a(gr),s&&a(oa),s&&a(_r),$(Et,s),s&&a(vr),s&&a(_s),$(Pt),s&&a($r),s&&a(ra),s&&a(yr),$(xt,s),s&&a(wr),s&&a(vs),$(At),s&&a(jr),s&&a(pa),s&&a(br),$(St,s),s&&a(qr),s&&a(Fe),s&&a(kr),s&&a(da),s&&a(Er),$(Dt,s),s&&a(Pr),s&&a($s),$(Tt),s&&a(xr),s&&a(ca),s&&a(Ar),$(Nt,s),s&&a(Sr),$(ha,s),s&&a(Dr),s&&a(ys),$(Ct),s&&a(Tr),s&&a(Me),s&&a(Nr),s&&a(Z),s&&a(Cr),s&&a(ws),$(It),s&&a(Ir),s&&a(ss),s&&a(Or),s&&a(as),s&&a(Lr),$(Ot,s),s&&a(Hr),s&&a(ga),s&&a(Fr),$(Lt,s),s&&a(Rr),s&&a(Ue),s&&a(Mr),$(Ht,s),s&&a(zr),s&&a(Ye),s&&a(Vr),$(Ft,s),s&&a(Ur),s&&a(Je),s&&a(Yr),$(Rt,s),s&&a(Jr),s&&a(js),$(Mt),s&&a(Wr),s&&a(We),s&&a(Br),$(zt,s),s&&a(Qr),s&&a(va),s&&a(Gr),$(Vt,s),s&&a(Kr),$($a,s),s&&a(Xr),s&&a(Be),s&&a(Zr),s&&a(bs),$(Ut),s&&a(si),s&&a(Qe),s&&a(ai),s&&a(qs),$(Yt),s&&a(ti),s&&a(z),s&&a(ei),s&&a(ja),s&&a(li),$(Wt,s),s&&a(oi),s&&a(H),s&&a(ni),s&&a(ks),$(Bt),s&&a(ri),s&&a(V),s&&a(ii),s&&a(qa),s&&a(pi),$(Gt,s),s&&a(di),s&&a(ts),s&&a(fi),$(Kt,s),s&&a(ci),s&&a(tl),s&&a(hi),$(Xt,s),s&&a(ui),s&&a(Es),$(Zt),s&&a(mi),$(Ea,s),s&&a(gi),s&&a(el),s&&a(_i),$(se,s),s&&a(vi),$(Pa,s),s&&a($i),s&&a(Ps),$(ae),s&&a(yi),s&&a(es),s&&a(wi),$(te,s),s&&a(ji),s&&a(xs),$(ee),s&&a(bi),s&&a(ol),s&&a(qi),s&&a(nl),s&&a(ki),s&&a(ls),s&&a(Ei),$(ne,s),s&&a(Pi),$(Sa,s),s&&a(xi),s&&a(Da),s&&a(Ai),$(re,s)}}}const r_={local:"load",sections:[{local:"hugging-face-hub",title:"Hugging Face Hub"},{local:"local-loading-script",sections:[{local:"edit-loading-script",title:"Edit loading script"}],title:"Local loading script"},{local:"local-and-remote-files",sections:[{local:"csv",title:"CSV"},{local:"json",title:"JSON"},{local:"parquet",title:"Parquet"},{local:"sql",title:"SQL"}],title:"Local and remote files"},{local:"multiprocessing",title:"Multiprocessing"},{local:"inmemory-data",sections:[{local:"python-dictionary",title:"Python dictionary"},{local:"python-list-of-dictionaries",title:"Python list of dictionaries"},{local:"python-generator",title:"Python generator"},{local:"pandas-dataframe",title:"Pandas DataFrame"}],title:"In-memory data"},{local:"offline",title:"Offline"},{local:"slice-splits",sections:[{local:"percent-slicing-and-rounding",title:"Percent slicing and rounding"}],title:"Slice splits"},{local:"troubleshooting",sections:[{local:"manual-download",title:"Manual download"},{local:"specify-features",title:"Specify features"}],title:"Troubleshooting"},{local:"metrics",sections:[{local:"load-configurations",title:"Load configurations"},{local:"distributed-setup",title:"Distributed setup"}],title:"Metrics"}],title:"Load"};function i_(D){return Kg(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class m_ extends Wg{constructor(h){super();Bg(this,h,i_,n_,Qg,{})}}export{m_ as default,r_ as metadata};
