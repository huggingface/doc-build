import{S as Ji,i as Wi,s as Gi,e as n,k as c,w as _,t,M as Ki,c as r,d as a,m as f,a as o,x as v,h as l,b as h,N as Bi,G as s,g as i,y as j,q as b,o as $,B as x,v as Qi,L as Xi}from"../chunks/vendor-hf-doc-builder.js";import{T as $t}from"../chunks/Tip-hf-doc-builder.js";import{I as N}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as I}from"../chunks/CodeBlock-hf-doc-builder.js";import{F as Zi,M as eh}from"../chunks/Markdown-hf-doc-builder.js";import"../chunks/IconTensorflow-hf-doc-builder.js";function sh(q){let d,w,m,y,k,g,E,u;return{c(){d=n("p"),w=t("An "),m=n("a"),y=t("IterableDataset"),k=t(" is useful for iterative jobs like training a model. You shouldn\u2019t use a "),g=n("a"),E=t("IterableDataset"),u=t(" for jobs that require random access to examples because you have to iterate all over it using a for loop. Getting the last example in an iterable dataset would require you to iterate over all the previous examples."),this.h()},l(D){d=r(D,"P",{});var A=o(d);w=l(A,"An "),m=r(A,"A",{href:!0});var T=o(m);y=l(T,"IterableDataset"),T.forEach(a),k=l(A," is useful for iterative jobs like training a model. You shouldn\u2019t use a "),g=r(A,"A",{href:!0});var M=o(g);E=l(M,"IterableDataset"),M.forEach(a),u=l(A," for jobs that require random access to examples because you have to iterate all over it using a for loop. Getting the last example in an iterable dataset would require you to iterate over all the previous examples."),A.forEach(a),this.h()},h(){h(m,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset"),h(g,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset")},m(D,A){i(D,d,A),s(d,w),s(d,m),s(m,y),s(d,k),s(d,g),s(g,E),s(d,u)},d(D){D&&a(d)}}}function ah(q){let d,w,m,y;return{c(){d=n("p"),w=n("a"),m=t("IterableDataset.shuffle()"),y=t(" will also shuffle the order of the shards if the dataset is sharded into multiple sets."),this.h()},l(k){d=r(k,"P",{});var g=o(d);w=r(g,"A",{href:!0});var E=o(w);m=l(E,"IterableDataset.shuffle()"),E.forEach(a),y=l(g," will also shuffle the order of the shards if the dataset is sharded into multiple sets."),g.forEach(a),this.h()},h(){h(w,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset.shuffle")},m(k,g){i(k,d,g),s(d,w),s(w,m),s(d,y)},d(k){k&&a(d)}}}function th(q){let d,w,m,y,k,g,E,u,D,A,T,M,fe;return{c(){d=n("p"),w=n("code"),m=t("take"),y=t(" and "),k=n("code"),g=t("skip"),E=t(" prevent future calls to "),u=n("code"),D=t("shuffle"),A=t(" because they lock in the order of the shards. You should "),T=n("code"),M=t("shuffle"),fe=t(" your dataset before splitting it.")},l(C){d=r(C,"P",{});var P=o(d);w=r(P,"CODE",{});var Is=o(w);m=l(Is,"take"),Is.forEach(a),y=l(P," and "),k=r(P,"CODE",{});var Ts=o(k);g=l(Ts,"skip"),Ts.forEach(a),E=l(P," prevent future calls to "),u=r(P,"CODE",{});var de=o(u);D=l(de,"shuffle"),de.forEach(a),A=l(P," because they lock in the order of the shards. You should "),T=r(P,"CODE",{});var Ps=o(T);M=l(Ps,"shuffle"),Ps.forEach(a),fe=l(P," your dataset before splitting it."),P.forEach(a)},m(C,P){i(C,d,P),s(d,w),s(w,m),s(d,y),s(d,k),s(k,g),s(d,E),s(d,u),s(u,D),s(d,A),s(d,T),s(T,M),s(d,fe)},d(C){C&&a(d)}}}function lh(q){let d,w,m,y,k,g,E,u;return{c(){d=n("p"),w=t("Casting only works if the original feature type and new feature type are compatible. For example, you can cast a column with the feature type "),m=n("code"),y=t("Value('int32')"),k=t(" to "),g=n("code"),E=t("Value('bool')"),u=t(" if the original column only contains ones and zeros.")},l(D){d=r(D,"P",{});var A=o(d);w=l(A,"Casting only works if the original feature type and new feature type are compatible. For example, you can cast a column with the feature type "),m=r(A,"CODE",{});var T=o(m);y=l(T,"Value('int32')"),T.forEach(a),k=l(A," to "),g=r(A,"CODE",{});var M=o(g);E=l(M,"Value('bool')"),M.forEach(a),u=l(A," if the original column only contains ones and zeros."),A.forEach(a)},m(D,A){i(D,d,A),s(d,w),s(d,m),s(m,y),s(d,k),s(d,g),s(g,E),s(d,u)},d(D){D&&a(d)}}}function nh(q){let d,w,m,y,k;return{c(){d=n("p"),w=t("See other examples of batch processing in the "),m=n("a"),y=t("batched map processing"),k=t(" documentation. They work the same for iterable datasets."),this.h()},l(g){d=r(g,"P",{});var E=o(d);w=l(E,"See other examples of batch processing in the "),m=r(E,"A",{href:!0});var u=o(m);y=l(u,"batched map processing"),u.forEach(a),k=l(E," documentation. They work the same for iterable datasets."),E.forEach(a),this.h()},h(){h(m,"href","./process#batch-processing")},m(g,E){i(g,d,E),s(d,w),s(d,m),s(m,y),s(d,k)},d(g){g&&a(d)}}}function rh(q){let d,w,m,y,k,g,E;return d=new I({props:{code:`seed, buffer_size = 42, 10_000
dataset = dataset.shuffle(seed, buffer_size=buffer_size)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>seed, buffer_size = <span class="hljs-number">42</span>, <span class="hljs-number">10_000</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.shuffle(seed, buffer_size=buffer_size)`}}),g=new I({props:{code:`import torch
from torch.utils.data import DataLoader
from transformers import AutoModelForMaskedLM, DataCollatorForLanguageModeling
from tqdm import tqdm
dataset = dataset.with_format("torch")
dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))
device = 'cuda' if torch.cuda.is_available() else 'cpu' 
model = AutoModelForMaskedLM.from_pretrained("distilbert-base-uncased")
model.train().to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)
for epoch in range(3):
    dataset.set_epoch(epoch)
    for i, batch in enumerate(tqdm(dataloader, total=5)):
        if i == 5:
            break
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs[0]
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        if i % 10 == 0:
            print(f"loss: {loss}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForMaskedLM, DataCollatorForLanguageModeling
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.with_format(<span class="hljs-string">&quot;torch&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = DataLoader(dataset, collate_fn=DataCollatorForLanguageModeling(tokenizer))
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span> 
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.train().to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = torch.optim.AdamW(params=model.parameters(), lr=<span class="hljs-number">1e-5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):
<span class="hljs-meta">... </span>    dataset.set_epoch(epoch)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tqdm(dataloader, total=<span class="hljs-number">5</span>)):
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">5</span>:
<span class="hljs-meta">... </span>            <span class="hljs-keyword">break</span>
<span class="hljs-meta">... </span>        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
<span class="hljs-meta">... </span>        outputs = model(**batch)
<span class="hljs-meta">... </span>        loss = outputs[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>        loss.backward()
<span class="hljs-meta">... </span>        optimizer.step()
<span class="hljs-meta">... </span>        optimizer.zero_grad()
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
<span class="hljs-meta">... </span>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;loss: <span class="hljs-subst">{loss}</span>&quot;</span>)`}}),{c(){_(d.$$.fragment),w=c(),m=n("p"),y=t("Lastly, create a simple training loop and start training:"),k=c(),_(g.$$.fragment)},l(u){v(d.$$.fragment,u),w=f(u),m=r(u,"P",{});var D=o(m);y=l(D,"Lastly, create a simple training loop and start training:"),D.forEach(a),k=f(u),v(g.$$.fragment,u)},m(u,D){j(d,u,D),i(u,w,D),i(u,m,D),s(m,y),i(u,k,D),j(g,u,D),E=!0},p:Xi,i(u){E||(b(d.$$.fragment,u),b(g.$$.fragment,u),E=!0)},o(u){$(d.$$.fragment,u),$(g.$$.fragment,u),E=!1},d(u){x(d,u),u&&a(w),u&&a(m),u&&a(k),x(g,u)}}}function oh(q){let d,w;return d=new eh({props:{$$slots:{default:[rh]},$$scope:{ctx:q}}}),{c(){_(d.$$.fragment)},l(m){v(d.$$.fragment,m)},m(m,y){j(d,m,y),w=!0},p(m,y){const k={};y&2&&(k.$$scope={dirty:y,ctx:m}),d.$set(k)},i(m){w||(b(d.$$.fragment,m),w=!0)},o(m){$(d.$$.fragment,m),w=!1},d(m){x(d,m)}}}function ph(q){let d,w,m,y,k,g,E,u,D,A,T,M,fe,C,P,Is,Ts,de,Ps,xt,G,qs,ep,rn,Ss,sp,wt,O,on,We,pn,hn,ja,cn,fn,Cs,dn,mn,yt,Ge,kt,H,un,zs,gn,_n,Ns,vn,jn,Et,me,Dt,K,ue,ba,Ke,bn,$a,$n,At,V,xn,Ms,wn,yn,Ls,kn,En,Os,Dn,An,It,F,In,xa,Tn,Pn,wa,qn,Sn,Vs,Cn,zn,Tt,Qe,Pt,ge,qt,Q,_e,ya,Xe,Nn,ka,Mn,St,ve,Ln,Ea,On,Vn,Ct,je,Fn,Da,Rn,Yn,zt,Ze,Nt,X,be,Aa,es,Hn,Ia,Un,Mt,Fs,Bn,Lt,Rs,$e,Ys,Jn,Wn,Ta,Gn,Kn,Ot,ss,Vt,Hs,xe,Us,Qn,Xn,Pa,Zn,er,Ft,as,Rt,we,Yt,Bs,Ht,Z,ye,qa,ts,sr,Sa,ar,Ut,ee,Js,tr,lr,Ws,nr,rr,Bt,ls,Jt,ke,or,Ca,pr,ir,Wt,ns,Gt,U,hr,za,cr,fr,Na,dr,mr,Kt,z,ur,Ma,gr,_r,La,vr,jr,Oa,br,$r,Va,xr,wr,Qt,se,Ee,Fa,rs,yr,Ra,kr,Xt,Gs,Er,Zt,ae,De,Ya,os,Dr,Ha,Ar,el,Ae,Ir,Ks,Tr,Pr,sl,Ie,qr,Qs,Sr,Cr,al,ps,tl,te,Te,Ua,is,zr,Ba,Nr,ll,Pe,Mr,Xs,Lr,Or,nl,hs,rl,le,qe,Ja,cs,Vr,Wa,Fr,ol,L,Zs,Rr,Yr,Ga,Hr,Ur,Ka,Br,Jr,Qa,Wr,Gr,pl,fs,il,Se,hl,Ce,Kr,ea,Qr,Xr,cl,ds,fl,ne,ze,Xa,ms,Zr,Za,eo,dl,S,so,sa,ao,to,aa,lo,no,ta,ro,oo,la,po,io,na,ho,co,ml,ra,fo,ul,B,mo,oa,uo,go,et,_o,vo,gl,us,_l,Ne,jo,pa,bo,$o,vl,gs,jl,Me,xo,ia,wo,yo,bl,J,ko,st,Eo,Do,ha,Ao,Io,$l,_s,xl,re,Le,at,vs,To,tt,Po,wl,Y,ca,qo,So,lt,Co,zo,nt,No,Mo,yl,oe,Oe,rt,js,Lo,ot,Oo,kl,bs,El,Ve,Dl,pe,Fe,pt,$s,Vo,it,Fo,Al,Re,Ro,fa,Yo,Ho,Il,xs,Tl,ie,da,Uo,Bo,ht,Jo,Wo,Pl,ws,ql,he,Ye,ct,ys,Go,ft,Ko,Sl,ks,ma,Qo,Xo,Cl,He,zl;return g=new N({}),Ge=new I({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
print(next(iter(dataset)))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset)))
{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, which he shared with John Blanchard during his first visit to Malawi. Chief Napoleon conveyed the desperate need for a program to intervene and care for the orphans and vulnerable children (OVC) in Malawi, and John committed to help...</span>`}}),me=new $t({props:{$$slots:{default:[sh]},$$scope:{ctx:q}}}),Ke=new N({}),Qe=new I({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
shuffled_dataset = dataset.shuffle(seed=42, buffer_size=10_000)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>shuffled_dataset = dataset.shuffle(seed=<span class="hljs-number">42</span>, buffer_size=<span class="hljs-number">10_000</span>)`}}),ge=new $t({props:{$$slots:{default:[ah]},$$scope:{ctx:q}}}),Xe=new N({}),Ze=new I({props:{code:`for epoch in range(epochs):
    shuffled_dataset.set_epoch(epoch)
    for example in shuffled_dataset:
        ...`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):
<span class="hljs-meta">... </span>    shuffled_dataset.set_epoch(epoch)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> shuffled_dataset:
<span class="hljs-meta">... </span>        ...`}}),es=new N({}),ss=new I({props:{code:`dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
dataset_head = dataset.take(2)
list(dataset_head)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset_head = dataset.take(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(dataset_head)
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was...&#x27;</span>}, {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Lily James cannot fight the music...&#x27;</span>}]`}}),as=new I({props:{code:"train_dataset = shuffled_dataset.skip(1000)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_dataset = shuffled_dataset.skip(<span class="hljs-number">1000</span>)'}}),we=new $t({props:{warning:!0,$$slots:{default:[th]},$$scope:{ctx:q}}}),ts=new N({}),ls=new I({props:{code:`from datasets import interleave_datasets
en_dataset = load_dataset('oscar', "unshuffled_deduplicated_en", split='train', streaming=True)
fr_dataset = load_dataset('oscar', "unshuffled_deduplicated_fr", split='train', streaming=True)

multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
list(multilingual_dataset.take(2))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> interleave_datasets
<span class="hljs-meta">&gt;&gt;&gt; </span>en_dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_en&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>fr_dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&quot;unshuffled_deduplicated_fr&quot;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>multilingual_dataset = interleave_datasets([en_dataset, fr_dataset])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(multilingual_dataset.take(<span class="hljs-number">2</span>))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision...&#x27;</span>}, {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&quot;M\xE9dia de d\xE9bat d&#x27;id\xE9es, de culture et de litt\xE9rature...&quot;</span>}]`}}),ns=new I({props:{code:`multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[0.8, 0.2], seed=42)
list(multilingual_dataset_with_oversampling.take(2))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>multilingual_dataset_with_oversampling = interleave_datasets([en_dataset, fr_dataset], probabilities=[<span class="hljs-number">0.8</span>, <span class="hljs-number">0.2</span>], seed=<span class="hljs-number">42</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(multilingual_dataset_with_oversampling.take(<span class="hljs-number">2</span>))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision...&#x27;</span>}, {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Lily James cannot fight the music...&#x27;</span>}]`}}),rs=new N({}),os=new N({}),ps=new I({props:{code:`from datasets import load_dataset
dataset = load_dataset('mc4', 'en', streaming=True, split='train')
dataset = dataset.rename_column("text", "content")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;mc4&#x27;</span>, <span class="hljs-string">&#x27;en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.rename_column(<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>)`}}),is=new N({}),hs=new I({props:{code:`from datasets import load_dataset
dataset = load_dataset('mc4', 'en', streaming=True, split='train')
dataset = dataset.remove_columns('timestamp')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;mc4&#x27;</span>, <span class="hljs-string">&#x27;en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.remove_columns(<span class="hljs-string">&#x27;timestamp&#x27;</span>)`}}),cs=new N({}),fs=new I({props:{code:`from datasets import load_dataset
dataset = load_dataset('glue', 'mrpc', split='train')
dataset.features

from datasets import ClassLabel, Value
new_features = dataset.features.copy()
new_features["label"] = ClassLabel(names=['negative', 'positive'])
new_features["idx"] = Value('int64')
dataset = dataset.cast(new_features)
dataset.features`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;sentence2&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">2</span>, names=[<span class="hljs-string">&#x27;not_equivalent&#x27;</span>, <span class="hljs-string">&#x27;equivalent&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;idx&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int32&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> ClassLabel, Value
<span class="hljs-meta">&gt;&gt;&gt; </span>new_features = dataset.features.copy()
<span class="hljs-meta">&gt;&gt;&gt; </span>new_features[<span class="hljs-string">&quot;label&quot;</span>] = ClassLabel(names=[<span class="hljs-string">&#x27;negative&#x27;</span>, <span class="hljs-string">&#x27;positive&#x27;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>new_features[<span class="hljs-string">&quot;idx&quot;</span>] = Value(<span class="hljs-string">&#x27;int64&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast(new_features)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;sentence2&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">2</span>, names=[<span class="hljs-string">&#x27;negative&#x27;</span>, <span class="hljs-string">&#x27;positive&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;idx&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;int64&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),Se=new $t({props:{$$slots:{default:[lh]},$$scope:{ctx:q}}}),ds=new I({props:{code:`dataset.features

dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
dataset.features`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;audio&#x27;</span>: Audio(sampling_rate=<span class="hljs-number">44100</span>, mono=<span class="hljs-literal">True</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.features
{<span class="hljs-string">&#x27;audio&#x27;</span>: Audio(sampling_rate=<span class="hljs-number">16000</span>, mono=<span class="hljs-literal">True</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),ms=new N({}),us=new I({props:{code:`def add_prefix(example):
    example['text'] = 'My text: ' + example['text']
    return example`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">add_prefix</span>(<span class="hljs-params">example</span>):
<span class="hljs-meta">... </span>    example[<span class="hljs-string">&#x27;text&#x27;</span>] = <span class="hljs-string">&#x27;My text: &#x27;</span> + example[<span class="hljs-string">&#x27;text&#x27;</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> example`}}),gs=new I({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train')
updated_dataset = dataset.map(add_prefix)
list(updated_dataset.take(3))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&#x27;unshuffled_deduplicated_en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(add_prefix)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(updated_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Mtendere Village was inspired by...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Lily James cannot fight the music...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: &quot;I\\&#x27;d love to help kickstart...&#x27;</span>}]`}}),_s=new I({props:{code:`updated_dataset = dataset.map(add_prefix, remove_columns=["id"])
list(updated_dataset.take(3))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>updated_dataset = dataset.<span class="hljs-built_in">map</span>(add_prefix, remove_columns=[<span class="hljs-string">&quot;id&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(updated_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Mtendere Village was inspired by...&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: Lily James cannot fight the music...&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My text: &quot;I\\&#x27;d love to help kickstart...&#x27;</span>}]`}}),vs=new N({}),js=new N({}),bs=new I({props:{code:`from datasets import load_dataset
from transformers import AutoTokenizer
dataset = load_dataset("mc4", "en", streaming=True, split="train")
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
def encode(examples):
    return tokenizer(examples['text'], truncation=True, padding='max_length')
dataset = dataset.map(encode, batched=True, remove_columns=["text", "timestamp", "url"])
next(iter(dataset))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;mc4&quot;</span>, <span class="hljs-string">&quot;en&quot;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;distilbert-base-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&#x27;text&#x27;</span>], truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(encode, batched=<span class="hljs-literal">True</span>, remove_columns=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;timestamp&quot;</span>, <span class="hljs-string">&quot;url&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataset))
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: <span class="hljs-number">101</span>, <span class="hljs-number">8466</span>, <span class="hljs-number">1018</span>, <span class="hljs-number">1010</span>, <span class="hljs-number">4029</span>, <span class="hljs-number">2475</span>, <span class="hljs-number">2062</span>, <span class="hljs-number">18558</span>, <span class="hljs-number">3100</span>, <span class="hljs-number">2061</span>, ...,<span class="hljs-number">1106</span>, <span class="hljs-number">3739</span>, <span class="hljs-number">102</span>],
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Ve=new $t({props:{$$slots:{default:[nh]},$$scope:{ctx:q}}}),$s=new N({}),xs=new I({props:{code:`from datasets import load_dataset
dataset = load_dataset('oscar', 'unshuffled_deduplicated_en', streaming=True, split='train')
start_with_ar = dataset.filter(lambda example: example['text'].startswith('Ar'))
next(iter(start_with_ar))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;oscar&#x27;</span>, <span class="hljs-string">&#x27;unshuffled_deduplicated_en&#x27;</span>, streaming=<span class="hljs-literal">True</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_with_ar = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> example: example[<span class="hljs-string">&#x27;text&#x27;</span>].startswith(<span class="hljs-string">&#x27;Ar&#x27;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(start_with_ar))
{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Are you looking for Number the Stars (Essential Modern Classics)?...&#x27;</span>}`}}),ws=new I({props:{code:`even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)
list(even_dataset.take(3))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>even_dataset = dataset.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> example, idx: idx % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>, with_indices=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(even_dataset.take(<span class="hljs-number">3</span>))
[{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Mtendere Village was inspired by the vision of Chief Napoleon Dzombe, ...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;&quot;I\\&#x27;d love to help kickstart continued development! And 0 EUR/month...&#x27;</span>},
 {<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Are you looking for Number the Stars (Essential Modern Classics)? Normally, ...&#x27;</span>}]`}}),ys=new N({}),He=new Zi({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[oh]},$$scope:{ctx:q}}}),{c(){d=n("meta"),w=c(),m=n("h1"),y=n("a"),k=n("span"),_(g.$$.fragment),E=c(),u=n("span"),D=t("Stream"),A=c(),T=n("p"),M=t("Dataset streaming lets you get started with a dataset without waiting for the entire dataset to download. The data is downloaded progressively as you iterate over the dataset. This is especially helpful when:"),fe=c(),C=n("ul"),P=n("li"),Is=t("You don\u2019t want to wait for an extremely large dataset to download."),Ts=c(),de=n("li"),Ps=t("The dataset size exceeds the amount of disk space on your computer."),xt=c(),G=n("div"),qs=n("img"),rn=c(),Ss=n("img"),wt=c(),O=n("p"),on=t("For example, the English split of the "),We=n("a"),pn=t("OSCAR"),hn=t(" dataset is 1.2 terabytes, but you can use it instantly with streaming. Stream a dataset by setting "),ja=n("code"),cn=t("streaming=True"),fn=t(" in "),Cs=n("a"),dn=t("load_dataset()"),mn=t(" as shown below:"),yt=c(),_(Ge.$$.fragment),kt=c(),H=n("p"),un=t("Loading a dataset in streaming mode creates a new dataset type instance (instead of the classic "),zs=n("a"),gn=t("Dataset"),_n=t(" object), known as an "),Ns=n("a"),vn=t("IterableDataset"),jn=t(". This special type of dataset has its own set of processing methods shown below."),Et=c(),_(me.$$.fragment),Dt=c(),K=n("h2"),ue=n("a"),ba=n("span"),_(Ke.$$.fragment),bn=c(),$a=n("span"),$n=t("Shuffle"),At=c(),V=n("p"),xn=t("Like a regular "),Ms=n("a"),wn=t("Dataset"),yn=t(" object, you can also shuffle a "),Ls=n("a"),kn=t("IterableDataset"),En=t(" with "),Os=n("a"),Dn=t("IterableDataset.shuffle()"),An=t("."),It=c(),F=n("p"),In=t("The "),xa=n("code"),Tn=t("buffer_size"),Pn=t(" argument controls the size of the buffer to randomly sample examples from. Let\u2019s say your dataset has one million examples, and you set the "),wa=n("code"),qn=t("buffer_size"),Sn=t(" to ten thousand. "),Vs=n("a"),Cn=t("IterableDataset.shuffle()"),zn=t(" will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are replaced with new examples. By default, the buffer size is 1,000."),Tt=c(),_(Qe.$$.fragment),Pt=c(),_(ge.$$.fragment),qt=c(),Q=n("h2"),_e=n("a"),ya=n("span"),_(Xe.$$.fragment),Nn=c(),ka=n("span"),Mn=t("Reshuffle"),St=c(),ve=n("p"),Ln=t("Sometimes you may want to reshuffle the dataset after each epoch. This will require you to set a different seed for each epoch. Use "),Ea=n("code"),On=t("IterableDataset.set_epoch()"),Vn=t(" in between epochs to tell the dataset what epoch you\u2019re on."),Ct=c(),je=n("p"),Fn=t("Your seed effectively becomes: "),Da=n("code"),Rn=t("initial seed + current epoch"),Yn=t("."),zt=c(),_(Ze.$$.fragment),Nt=c(),X=n("h2"),be=n("a"),Aa=n("span"),_(es.$$.fragment),Hn=c(),Ia=n("span"),Un=t("Split dataset"),Mt=c(),Fs=n("p"),Bn=t("You can split your dataset one of two ways:"),Lt=c(),Rs=n("ul"),$e=n("li"),Ys=n("a"),Jn=t("IterableDataset.take()"),Wn=t(" returns the first "),Ta=n("code"),Gn=t("n"),Kn=t(" examples in a dataset:"),Ot=c(),_(ss.$$.fragment),Vt=c(),Hs=n("ul"),xe=n("li"),Us=n("a"),Qn=t("IterableDataset.skip()"),Xn=t(" omits the first "),Pa=n("code"),Zn=t("n"),er=t(" examples in a dataset and returns the remaining examples:"),Ft=c(),_(as.$$.fragment),Rt=c(),_(we.$$.fragment),Yt=c(),Bs=n("a"),Ht=c(),Z=n("h2"),ye=n("a"),qa=n("span"),_(ts.$$.fragment),sr=c(),Sa=n("span"),ar=t("Interleave"),Ut=c(),ee=n("p"),Js=n("a"),tr=t("interleave_datasets()"),lr=t(" can combine an "),Ws=n("a"),nr=t("IterableDataset"),rr=t(" with other datasets. The combined dataset returns alternating examples from each of the original datasets."),Bt=c(),_(ls.$$.fragment),Jt=c(),ke=n("p"),or=t("Define sampling probabilities from each of the original datasets for more control over how each of them are sampled and combined. Set the "),Ca=n("code"),pr=t("probabilities"),ir=t(" argument with your desired sampling probabilities:"),Wt=c(),_(ns.$$.fragment),Gt=c(),U=n("p"),hr=t("Around 80% of the final dataset is made of the "),za=n("code"),cr=t("en_dataset"),fr=t(", and 20% of the "),Na=n("code"),dr=t("fr_dataset"),mr=t("."),Kt=c(),z=n("p"),ur=t("You can also specify the "),Ma=n("code"),gr=t("stopping_strategy"),_r=t(". The default strategy, "),La=n("code"),vr=t("first_exhausted"),jr=t(`, is a subsampling strategy, i.e the dataset construction is stopped as soon one of the dataset runs out of samples.
You can specify `),Oa=n("code"),br=t("stopping_strategy=all_exhausted"),$r=t(` to execute an oversampling strategy. In this case, the dataset construction is stopped as soon as every samples in every dataset has been added at least once. In practice, it means that if a dataset is exhausted, it will return to the beginning of this dataset until the stop criterion has been reached.
Note that if no sampling probabilities are specified, the new dataset will have `),Va=n("code"),xr=t("max_length_datasets*nb_dataset samples"),wr=t("."),Qt=c(),se=n("h2"),Ee=n("a"),Fa=n("span"),_(rs.$$.fragment),yr=c(),Ra=n("span"),kr=t("Rename, remove, and cast"),Xt=c(),Gs=n("p"),Er=t("The following methods allow you to modify the columns of a dataset. These methods are useful for renaming or removing columns and changing columns to a new set of features."),Zt=c(),ae=n("h3"),De=n("a"),Ya=n("span"),_(os.$$.fragment),Dr=c(),Ha=n("span"),Ar=t("Rename"),el=c(),Ae=n("p"),Ir=t("Use "),Ks=n("a"),Tr=t("IterableDataset.rename_column()"),Pr=t(" when you need to rename a column in your dataset. Features associated with the original column are actually moved under the new column name, instead of just replacing the original column in-place."),sl=c(),Ie=n("p"),qr=t("Provide "),Qs=n("a"),Sr=t("IterableDataset.rename_column()"),Cr=t(" with the name of the original column, and the new column name:"),al=c(),_(ps.$$.fragment),tl=c(),te=n("h3"),Te=n("a"),Ua=n("span"),_(is.$$.fragment),zr=c(),Ba=n("span"),Nr=t("Remove"),ll=c(),Pe=n("p"),Mr=t("When you need to remove one or more columns, give "),Xs=n("a"),Lr=t("IterableDataset.remove_columns()"),Or=t(" the name of the column to remove. Remove more than one column by providing a list of column names:"),nl=c(),_(hs.$$.fragment),rl=c(),le=n("h3"),qe=n("a"),Ja=n("span"),_(cs.$$.fragment),Vr=c(),Wa=n("span"),Fr=t("Cast"),ol=c(),L=n("p"),Zs=n("a"),Rr=t("IterableDataset.cast()"),Yr=t(" changes the feature type of one or more columns. This method takes your new "),Ga=n("code"),Hr=t("Features"),Ur=t(" as its argument. The following sample code shows how to change the feature types of "),Ka=n("code"),Br=t("ClassLabel"),Jr=t(" and "),Qa=n("code"),Wr=t("Value"),Gr=t(":"),pl=c(),_(fs.$$.fragment),il=c(),_(Se.$$.fragment),hl=c(),Ce=n("p"),Kr=t("Use "),ea=n("a"),Qr=t("IterableDataset.cast_column()"),Xr=t(" to change the feature type of just one column. Pass the column name and its new feature type as arguments:"),cl=c(),_(ds.$$.fragment),fl=c(),ne=n("h2"),ze=n("a"),Xa=n("span"),_(ms.$$.fragment),Zr=c(),Za=n("span"),eo=t("Map"),dl=c(),S=n("p"),so=t("Similar to the "),sa=n("a"),ao=t("Dataset.map()"),to=t(" function for a regular "),aa=n("a"),lo=t("Dataset"),no=t(", \u{1F917}  Datasets features "),ta=n("a"),ro=t("IterableDataset.map()"),oo=t(" for processing an "),la=n("a"),po=t("IterableDataset"),io=t(`.
`),na=n("a"),ho=t("IterableDataset.map()"),co=t(" applies processing on-the-fly when examples are streamed."),ml=c(),ra=n("p"),fo=t("It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),ul=c(),B=n("p"),mo=t("The following example demonstrates how to tokenize a "),oa=n("a"),uo=t("IterableDataset"),go=t(". The function needs to accept and output a "),et=n("code"),_o=t("dict"),vo=t(":"),gl=c(),_(us.$$.fragment),_l=c(),Ne=n("p"),jo=t("Next, apply this function to the dataset with "),pa=n("a"),bo=t("IterableDataset.map()"),$o=t(":"),vl=c(),_(gs.$$.fragment),jl=c(),Me=n("p"),xo=t("Let\u2019s take a look at another example, except this time, you will remove a column with "),ia=n("a"),wo=t("IterableDataset.map()"),yo=t(". When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed."),bl=c(),J=n("p"),ko=t("Specify the column to remove with the "),st=n("code"),Eo=t("remove_columns"),Do=t(" argument in "),ha=n("a"),Ao=t("IterableDataset.map()"),Io=t(":"),$l=c(),_(_s.$$.fragment),xl=c(),re=n("h3"),Le=n("a"),at=n("span"),_(vs.$$.fragment),To=c(),tt=n("span"),Po=t("Batch processing"),wl=c(),Y=n("p"),ca=n("a"),qo=t("IterableDataset.map()"),So=t(" also supports working with batches of examples. Operate on batches by setting "),lt=n("code"),Co=t("batched=True"),zo=t(". The default batch size is 1000, but you can adjust it with the "),nt=n("code"),No=t("batch_size"),Mo=t(" argument. This opens the door to many interesting applications such as tokenization, splitting long sentences into shorter chunks, and data augmentation."),yl=c(),oe=n("h4"),Oe=n("a"),rt=n("span"),_(js.$$.fragment),Lo=c(),ot=n("span"),Oo=t("Tokenization"),kl=c(),_(bs.$$.fragment),El=c(),_(Ve.$$.fragment),Dl=c(),pe=n("h3"),Fe=n("a"),pt=n("span"),_($s.$$.fragment),Vo=c(),it=n("span"),Fo=t("Filter"),Al=c(),Re=n("p"),Ro=t("You can filter rows in the dataset based on a predicate function using "),fa=n("a"),Yo=t("Dataset.filter()"),Ho=t(". It returns rows that match a specified condition:"),Il=c(),_(xs.$$.fragment),Tl=c(),ie=n("p"),da=n("a"),Uo=t("Dataset.filter()"),Bo=t(" can also filter by indices if you set "),ht=n("code"),Jo=t("with_indices=True"),Wo=t(":"),Pl=c(),_(ws.$$.fragment),ql=c(),he=n("h2"),Ye=n("a"),ct=n("span"),_(ys.$$.fragment),Go=c(),ft=n("span"),Ko=t("Stream in a training loop"),Sl=c(),ks=n("p"),ma=n("a"),Qo=t("IterableDataset"),Xo=t(" can be integrated into a training loop. First, shuffle the dataset:"),Cl=c(),_(He.$$.fragment),this.h()},l(e){const p=Ki('[data-svelte="svelte-1phssyn"]',document.head);d=r(p,"META",{name:!0,content:!0}),p.forEach(a),w=f(e),m=r(e,"H1",{class:!0});var Es=o(m);y=r(Es,"A",{id:!0,class:!0,href:!0});var dt=o(y);k=r(dt,"SPAN",{});var mt=o(k);v(g.$$.fragment,mt),mt.forEach(a),dt.forEach(a),E=f(Es),u=r(Es,"SPAN",{});var ut=o(u);D=l(ut,"Stream"),ut.forEach(a),Es.forEach(a),A=f(e),T=r(e,"P",{});var gt=o(T);M=l(gt,"Dataset streaming lets you get started with a dataset without waiting for the entire dataset to download. The data is downloaded progressively as you iterate over the dataset. This is especially helpful when:"),gt.forEach(a),fe=f(e),C=r(e,"UL",{});var Ds=o(C);P=r(Ds,"LI",{});var ap=o(P);Is=l(ap,"You don\u2019t want to wait for an extremely large dataset to download."),ap.forEach(a),Ts=f(Ds),de=r(Ds,"LI",{});var tp=o(de);Ps=l(tp,"The dataset size exceeds the amount of disk space on your computer."),tp.forEach(a),Ds.forEach(a),xt=f(e),G=r(e,"DIV",{class:!0});var Nl=o(G);qs=r(Nl,"IMG",{class:!0,src:!0}),rn=f(Nl),Ss=r(Nl,"IMG",{class:!0,src:!0}),Nl.forEach(a),wt=f(e),O=r(e,"P",{});var Ue=o(O);on=l(Ue,"For example, the English split of the "),We=r(Ue,"A",{href:!0,rel:!0});var lp=o(We);pn=l(lp,"OSCAR"),lp.forEach(a),hn=l(Ue," dataset is 1.2 terabytes, but you can use it instantly with streaming. Stream a dataset by setting "),ja=r(Ue,"CODE",{});var np=o(ja);cn=l(np,"streaming=True"),np.forEach(a),fn=l(Ue," in "),Cs=r(Ue,"A",{href:!0});var rp=o(Cs);dn=l(rp,"load_dataset()"),rp.forEach(a),mn=l(Ue," as shown below:"),Ue.forEach(a),yt=f(e),v(Ge.$$.fragment,e),kt=f(e),H=r(e,"P",{});var ua=o(H);un=l(ua,"Loading a dataset in streaming mode creates a new dataset type instance (instead of the classic "),zs=r(ua,"A",{href:!0});var op=o(zs);gn=l(op,"Dataset"),op.forEach(a),_n=l(ua," object), known as an "),Ns=r(ua,"A",{href:!0});var pp=o(Ns);vn=l(pp,"IterableDataset"),pp.forEach(a),jn=l(ua,". This special type of dataset has its own set of processing methods shown below."),ua.forEach(a),Et=f(e),v(me.$$.fragment,e),Dt=f(e),K=r(e,"H2",{class:!0});var Ml=o(K);ue=r(Ml,"A",{id:!0,class:!0,href:!0});var ip=o(ue);ba=r(ip,"SPAN",{});var hp=o(ba);v(Ke.$$.fragment,hp),hp.forEach(a),ip.forEach(a),bn=f(Ml),$a=r(Ml,"SPAN",{});var cp=o($a);$n=l(cp,"Shuffle"),cp.forEach(a),Ml.forEach(a),At=f(e),V=r(e,"P",{});var Be=o(V);xn=l(Be,"Like a regular "),Ms=r(Be,"A",{href:!0});var fp=o(Ms);wn=l(fp,"Dataset"),fp.forEach(a),yn=l(Be," object, you can also shuffle a "),Ls=r(Be,"A",{href:!0});var dp=o(Ls);kn=l(dp,"IterableDataset"),dp.forEach(a),En=l(Be," with "),Os=r(Be,"A",{href:!0});var mp=o(Os);Dn=l(mp,"IterableDataset.shuffle()"),mp.forEach(a),An=l(Be,"."),Be.forEach(a),It=f(e),F=r(e,"P",{});var Je=o(F);In=l(Je,"The "),xa=r(Je,"CODE",{});var up=o(xa);Tn=l(up,"buffer_size"),up.forEach(a),Pn=l(Je," argument controls the size of the buffer to randomly sample examples from. Let\u2019s say your dataset has one million examples, and you set the "),wa=r(Je,"CODE",{});var gp=o(wa);qn=l(gp,"buffer_size"),gp.forEach(a),Sn=l(Je," to ten thousand. "),Vs=r(Je,"A",{href:!0});var _p=o(Vs);Cn=l(_p,"IterableDataset.shuffle()"),_p.forEach(a),zn=l(Je," will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are replaced with new examples. By default, the buffer size is 1,000."),Je.forEach(a),Tt=f(e),v(Qe.$$.fragment,e),Pt=f(e),v(ge.$$.fragment,e),qt=f(e),Q=r(e,"H2",{class:!0});var Ll=o(Q);_e=r(Ll,"A",{id:!0,class:!0,href:!0});var vp=o(_e);ya=r(vp,"SPAN",{});var jp=o(ya);v(Xe.$$.fragment,jp),jp.forEach(a),vp.forEach(a),Nn=f(Ll),ka=r(Ll,"SPAN",{});var bp=o(ka);Mn=l(bp,"Reshuffle"),bp.forEach(a),Ll.forEach(a),St=f(e),ve=r(e,"P",{});var Ol=o(ve);Ln=l(Ol,"Sometimes you may want to reshuffle the dataset after each epoch. This will require you to set a different seed for each epoch. Use "),Ea=r(Ol,"CODE",{});var $p=o(Ea);On=l($p,"IterableDataset.set_epoch()"),$p.forEach(a),Vn=l(Ol," in between epochs to tell the dataset what epoch you\u2019re on."),Ol.forEach(a),Ct=f(e),je=r(e,"P",{});var Vl=o(je);Fn=l(Vl,"Your seed effectively becomes: "),Da=r(Vl,"CODE",{});var xp=o(Da);Rn=l(xp,"initial seed + current epoch"),xp.forEach(a),Yn=l(Vl,"."),Vl.forEach(a),zt=f(e),v(Ze.$$.fragment,e),Nt=f(e),X=r(e,"H2",{class:!0});var Fl=o(X);be=r(Fl,"A",{id:!0,class:!0,href:!0});var wp=o(be);Aa=r(wp,"SPAN",{});var yp=o(Aa);v(es.$$.fragment,yp),yp.forEach(a),wp.forEach(a),Hn=f(Fl),Ia=r(Fl,"SPAN",{});var kp=o(Ia);Un=l(kp,"Split dataset"),kp.forEach(a),Fl.forEach(a),Mt=f(e),Fs=r(e,"P",{});var Ep=o(Fs);Bn=l(Ep,"You can split your dataset one of two ways:"),Ep.forEach(a),Lt=f(e),Rs=r(e,"UL",{});var Dp=o(Rs);$e=r(Dp,"LI",{});var _t=o($e);Ys=r(_t,"A",{href:!0});var Ap=o(Ys);Jn=l(Ap,"IterableDataset.take()"),Ap.forEach(a),Wn=l(_t," returns the first "),Ta=r(_t,"CODE",{});var Ip=o(Ta);Gn=l(Ip,"n"),Ip.forEach(a),Kn=l(_t," examples in a dataset:"),_t.forEach(a),Dp.forEach(a),Ot=f(e),v(ss.$$.fragment,e),Vt=f(e),Hs=r(e,"UL",{});var Tp=o(Hs);xe=r(Tp,"LI",{});var vt=o(xe);Us=r(vt,"A",{href:!0});var Pp=o(Us);Qn=l(Pp,"IterableDataset.skip()"),Pp.forEach(a),Xn=l(vt," omits the first "),Pa=r(vt,"CODE",{});var qp=o(Pa);Zn=l(qp,"n"),qp.forEach(a),er=l(vt," examples in a dataset and returns the remaining examples:"),vt.forEach(a),Tp.forEach(a),Ft=f(e),v(as.$$.fragment,e),Rt=f(e),v(we.$$.fragment,e),Yt=f(e),Bs=r(e,"A",{id:!0}),o(Bs).forEach(a),Ht=f(e),Z=r(e,"H2",{class:!0});var Rl=o(Z);ye=r(Rl,"A",{id:!0,class:!0,href:!0});var Sp=o(ye);qa=r(Sp,"SPAN",{});var Cp=o(qa);v(ts.$$.fragment,Cp),Cp.forEach(a),Sp.forEach(a),sr=f(Rl),Sa=r(Rl,"SPAN",{});var zp=o(Sa);ar=l(zp,"Interleave"),zp.forEach(a),Rl.forEach(a),Ut=f(e),ee=r(e,"P",{});var jt=o(ee);Js=r(jt,"A",{href:!0});var Np=o(Js);tr=l(Np,"interleave_datasets()"),Np.forEach(a),lr=l(jt," can combine an "),Ws=r(jt,"A",{href:!0});var Mp=o(Ws);nr=l(Mp,"IterableDataset"),Mp.forEach(a),rr=l(jt," with other datasets. The combined dataset returns alternating examples from each of the original datasets."),jt.forEach(a),Bt=f(e),v(ls.$$.fragment,e),Jt=f(e),ke=r(e,"P",{});var Yl=o(ke);or=l(Yl,"Define sampling probabilities from each of the original datasets for more control over how each of them are sampled and combined. Set the "),Ca=r(Yl,"CODE",{});var Lp=o(Ca);pr=l(Lp,"probabilities"),Lp.forEach(a),ir=l(Yl," argument with your desired sampling probabilities:"),Yl.forEach(a),Wt=f(e),v(ns.$$.fragment,e),Gt=f(e),U=r(e,"P",{});var ga=o(U);hr=l(ga,"Around 80% of the final dataset is made of the "),za=r(ga,"CODE",{});var Op=o(za);cr=l(Op,"en_dataset"),Op.forEach(a),fr=l(ga,", and 20% of the "),Na=r(ga,"CODE",{});var Vp=o(Na);dr=l(Vp,"fr_dataset"),Vp.forEach(a),mr=l(ga,"."),ga.forEach(a),Kt=f(e),z=r(e,"P",{});var W=o(z);ur=l(W,"You can also specify the "),Ma=r(W,"CODE",{});var Fp=o(Ma);gr=l(Fp,"stopping_strategy"),Fp.forEach(a),_r=l(W,". The default strategy, "),La=r(W,"CODE",{});var Rp=o(La);vr=l(Rp,"first_exhausted"),Rp.forEach(a),jr=l(W,`, is a subsampling strategy, i.e the dataset construction is stopped as soon one of the dataset runs out of samples.
You can specify `),Oa=r(W,"CODE",{});var Yp=o(Oa);br=l(Yp,"stopping_strategy=all_exhausted"),Yp.forEach(a),$r=l(W,` to execute an oversampling strategy. In this case, the dataset construction is stopped as soon as every samples in every dataset has been added at least once. In practice, it means that if a dataset is exhausted, it will return to the beginning of this dataset until the stop criterion has been reached.
Note that if no sampling probabilities are specified, the new dataset will have `),Va=r(W,"CODE",{});var Hp=o(Va);xr=l(Hp,"max_length_datasets*nb_dataset samples"),Hp.forEach(a),wr=l(W,"."),W.forEach(a),Qt=f(e),se=r(e,"H2",{class:!0});var Hl=o(se);Ee=r(Hl,"A",{id:!0,class:!0,href:!0});var Up=o(Ee);Fa=r(Up,"SPAN",{});var Bp=o(Fa);v(rs.$$.fragment,Bp),Bp.forEach(a),Up.forEach(a),yr=f(Hl),Ra=r(Hl,"SPAN",{});var Jp=o(Ra);kr=l(Jp,"Rename, remove, and cast"),Jp.forEach(a),Hl.forEach(a),Xt=f(e),Gs=r(e,"P",{});var Wp=o(Gs);Er=l(Wp,"The following methods allow you to modify the columns of a dataset. These methods are useful for renaming or removing columns and changing columns to a new set of features."),Wp.forEach(a),Zt=f(e),ae=r(e,"H3",{class:!0});var Ul=o(ae);De=r(Ul,"A",{id:!0,class:!0,href:!0});var Gp=o(De);Ya=r(Gp,"SPAN",{});var Kp=o(Ya);v(os.$$.fragment,Kp),Kp.forEach(a),Gp.forEach(a),Dr=f(Ul),Ha=r(Ul,"SPAN",{});var Qp=o(Ha);Ar=l(Qp,"Rename"),Qp.forEach(a),Ul.forEach(a),el=f(e),Ae=r(e,"P",{});var Bl=o(Ae);Ir=l(Bl,"Use "),Ks=r(Bl,"A",{href:!0});var Xp=o(Ks);Tr=l(Xp,"IterableDataset.rename_column()"),Xp.forEach(a),Pr=l(Bl," when you need to rename a column in your dataset. Features associated with the original column are actually moved under the new column name, instead of just replacing the original column in-place."),Bl.forEach(a),sl=f(e),Ie=r(e,"P",{});var Jl=o(Ie);qr=l(Jl,"Provide "),Qs=r(Jl,"A",{href:!0});var Zp=o(Qs);Sr=l(Zp,"IterableDataset.rename_column()"),Zp.forEach(a),Cr=l(Jl," with the name of the original column, and the new column name:"),Jl.forEach(a),al=f(e),v(ps.$$.fragment,e),tl=f(e),te=r(e,"H3",{class:!0});var Wl=o(te);Te=r(Wl,"A",{id:!0,class:!0,href:!0});var ei=o(Te);Ua=r(ei,"SPAN",{});var si=o(Ua);v(is.$$.fragment,si),si.forEach(a),ei.forEach(a),zr=f(Wl),Ba=r(Wl,"SPAN",{});var ai=o(Ba);Nr=l(ai,"Remove"),ai.forEach(a),Wl.forEach(a),ll=f(e),Pe=r(e,"P",{});var Gl=o(Pe);Mr=l(Gl,"When you need to remove one or more columns, give "),Xs=r(Gl,"A",{href:!0});var ti=o(Xs);Lr=l(ti,"IterableDataset.remove_columns()"),ti.forEach(a),Or=l(Gl," the name of the column to remove. Remove more than one column by providing a list of column names:"),Gl.forEach(a),nl=f(e),v(hs.$$.fragment,e),rl=f(e),le=r(e,"H3",{class:!0});var Kl=o(le);qe=r(Kl,"A",{id:!0,class:!0,href:!0});var li=o(qe);Ja=r(li,"SPAN",{});var ni=o(Ja);v(cs.$$.fragment,ni),ni.forEach(a),li.forEach(a),Vr=f(Kl),Wa=r(Kl,"SPAN",{});var ri=o(Wa);Fr=l(ri,"Cast"),ri.forEach(a),Kl.forEach(a),ol=f(e),L=r(e,"P",{});var ce=o(L);Zs=r(ce,"A",{href:!0});var oi=o(Zs);Rr=l(oi,"IterableDataset.cast()"),oi.forEach(a),Yr=l(ce," changes the feature type of one or more columns. This method takes your new "),Ga=r(ce,"CODE",{});var pi=o(Ga);Hr=l(pi,"Features"),pi.forEach(a),Ur=l(ce," as its argument. The following sample code shows how to change the feature types of "),Ka=r(ce,"CODE",{});var ii=o(Ka);Br=l(ii,"ClassLabel"),ii.forEach(a),Jr=l(ce," and "),Qa=r(ce,"CODE",{});var hi=o(Qa);Wr=l(hi,"Value"),hi.forEach(a),Gr=l(ce,":"),ce.forEach(a),pl=f(e),v(fs.$$.fragment,e),il=f(e),v(Se.$$.fragment,e),hl=f(e),Ce=r(e,"P",{});var Ql=o(Ce);Kr=l(Ql,"Use "),ea=r(Ql,"A",{href:!0});var ci=o(ea);Qr=l(ci,"IterableDataset.cast_column()"),ci.forEach(a),Xr=l(Ql," to change the feature type of just one column. Pass the column name and its new feature type as arguments:"),Ql.forEach(a),cl=f(e),v(ds.$$.fragment,e),fl=f(e),ne=r(e,"H2",{class:!0});var Xl=o(ne);ze=r(Xl,"A",{id:!0,class:!0,href:!0});var fi=o(ze);Xa=r(fi,"SPAN",{});var di=o(Xa);v(ms.$$.fragment,di),di.forEach(a),fi.forEach(a),Zr=f(Xl),Za=r(Xl,"SPAN",{});var mi=o(Za);eo=l(mi,"Map"),mi.forEach(a),Xl.forEach(a),dl=f(e),S=r(e,"P",{});var R=o(S);so=l(R,"Similar to the "),sa=r(R,"A",{href:!0});var ui=o(sa);ao=l(ui,"Dataset.map()"),ui.forEach(a),to=l(R," function for a regular "),aa=r(R,"A",{href:!0});var gi=o(aa);lo=l(gi,"Dataset"),gi.forEach(a),no=l(R,", \u{1F917}  Datasets features "),ta=r(R,"A",{href:!0});var _i=o(ta);ro=l(_i,"IterableDataset.map()"),_i.forEach(a),oo=l(R," for processing an "),la=r(R,"A",{href:!0});var vi=o(la);po=l(vi,"IterableDataset"),vi.forEach(a),io=l(R,`.
`),na=r(R,"A",{href:!0});var ji=o(na);ho=l(ji,"IterableDataset.map()"),ji.forEach(a),co=l(R," applies processing on-the-fly when examples are streamed."),R.forEach(a),ml=f(e),ra=r(e,"P",{});var bi=o(ra);fo=l(bi,"It allows you to apply a processing function to each example in a dataset, independently or in batches. This function can even create new rows and columns."),bi.forEach(a),ul=f(e),B=r(e,"P",{});var _a=o(B);mo=l(_a,"The following example demonstrates how to tokenize a "),oa=r(_a,"A",{href:!0});var $i=o(oa);uo=l($i,"IterableDataset"),$i.forEach(a),go=l(_a,". The function needs to accept and output a "),et=r(_a,"CODE",{});var xi=o(et);_o=l(xi,"dict"),xi.forEach(a),vo=l(_a,":"),_a.forEach(a),gl=f(e),v(us.$$.fragment,e),_l=f(e),Ne=r(e,"P",{});var Zl=o(Ne);jo=l(Zl,"Next, apply this function to the dataset with "),pa=r(Zl,"A",{href:!0});var wi=o(pa);bo=l(wi,"IterableDataset.map()"),wi.forEach(a),$o=l(Zl,":"),Zl.forEach(a),vl=f(e),v(gs.$$.fragment,e),jl=f(e),Me=r(e,"P",{});var en=o(Me);xo=l(en,"Let\u2019s take a look at another example, except this time, you will remove a column with "),ia=r(en,"A",{href:!0});var yi=o(ia);wo=l(yi,"IterableDataset.map()"),yi.forEach(a),yo=l(en,". When you remove a column, it is only removed after the example has been provided to the mapped function. This allows the mapped function to use the content of the columns before they are removed."),en.forEach(a),bl=f(e),J=r(e,"P",{});var va=o(J);ko=l(va,"Specify the column to remove with the "),st=r(va,"CODE",{});var ki=o(st);Eo=l(ki,"remove_columns"),ki.forEach(a),Do=l(va," argument in "),ha=r(va,"A",{href:!0});var Ei=o(ha);Ao=l(Ei,"IterableDataset.map()"),Ei.forEach(a),Io=l(va,":"),va.forEach(a),$l=f(e),v(_s.$$.fragment,e),xl=f(e),re=r(e,"H3",{class:!0});var sn=o(re);Le=r(sn,"A",{id:!0,class:!0,href:!0});var Di=o(Le);at=r(Di,"SPAN",{});var Ai=o(at);v(vs.$$.fragment,Ai),Ai.forEach(a),Di.forEach(a),To=f(sn),tt=r(sn,"SPAN",{});var Ii=o(tt);Po=l(Ii,"Batch processing"),Ii.forEach(a),sn.forEach(a),wl=f(e),Y=r(e,"P",{});var As=o(Y);ca=r(As,"A",{href:!0});var Ti=o(ca);qo=l(Ti,"IterableDataset.map()"),Ti.forEach(a),So=l(As," also supports working with batches of examples. Operate on batches by setting "),lt=r(As,"CODE",{});var Pi=o(lt);Co=l(Pi,"batched=True"),Pi.forEach(a),zo=l(As,". The default batch size is 1000, but you can adjust it with the "),nt=r(As,"CODE",{});var qi=o(nt);No=l(qi,"batch_size"),qi.forEach(a),Mo=l(As," argument. This opens the door to many interesting applications such as tokenization, splitting long sentences into shorter chunks, and data augmentation."),As.forEach(a),yl=f(e),oe=r(e,"H4",{class:!0});var an=o(oe);Oe=r(an,"A",{id:!0,class:!0,href:!0});var Si=o(Oe);rt=r(Si,"SPAN",{});var Ci=o(rt);v(js.$$.fragment,Ci),Ci.forEach(a),Si.forEach(a),Lo=f(an),ot=r(an,"SPAN",{});var zi=o(ot);Oo=l(zi,"Tokenization"),zi.forEach(a),an.forEach(a),kl=f(e),v(bs.$$.fragment,e),El=f(e),v(Ve.$$.fragment,e),Dl=f(e),pe=r(e,"H3",{class:!0});var tn=o(pe);Fe=r(tn,"A",{id:!0,class:!0,href:!0});var Ni=o(Fe);pt=r(Ni,"SPAN",{});var Mi=o(pt);v($s.$$.fragment,Mi),Mi.forEach(a),Ni.forEach(a),Vo=f(tn),it=r(tn,"SPAN",{});var Li=o(it);Fo=l(Li,"Filter"),Li.forEach(a),tn.forEach(a),Al=f(e),Re=r(e,"P",{});var ln=o(Re);Ro=l(ln,"You can filter rows in the dataset based on a predicate function using "),fa=r(ln,"A",{href:!0});var Oi=o(fa);Yo=l(Oi,"Dataset.filter()"),Oi.forEach(a),Ho=l(ln,". It returns rows that match a specified condition:"),ln.forEach(a),Il=f(e),v(xs.$$.fragment,e),Tl=f(e),ie=r(e,"P",{});var bt=o(ie);da=r(bt,"A",{href:!0});var Vi=o(da);Uo=l(Vi,"Dataset.filter()"),Vi.forEach(a),Bo=l(bt," can also filter by indices if you set "),ht=r(bt,"CODE",{});var Fi=o(ht);Jo=l(Fi,"with_indices=True"),Fi.forEach(a),Wo=l(bt,":"),bt.forEach(a),Pl=f(e),v(ws.$$.fragment,e),ql=f(e),he=r(e,"H2",{class:!0});var nn=o(he);Ye=r(nn,"A",{id:!0,class:!0,href:!0});var Ri=o(Ye);ct=r(Ri,"SPAN",{});var Yi=o(ct);v(ys.$$.fragment,Yi),Yi.forEach(a),Ri.forEach(a),Go=f(nn),ft=r(nn,"SPAN",{});var Hi=o(ft);Ko=l(Hi,"Stream in a training loop"),Hi.forEach(a),nn.forEach(a),Sl=f(e),ks=r(e,"P",{});var Zo=o(ks);ma=r(Zo,"A",{href:!0});var Ui=o(ma);Qo=l(Ui,"IterableDataset"),Ui.forEach(a),Xo=l(Zo," can be integrated into a training loop. First, shuffle the dataset:"),Zo.forEach(a),Cl=f(e),v(He.$$.fragment,e),this.h()},h(){h(d,"name","hf:doc:metadata"),h(d,"content",JSON.stringify(ih)),h(y,"id","stream"),h(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(y,"href","#stream"),h(m,"class","relative group"),h(qs,"class","block dark:hidden"),Bi(qs.src,ep="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming.gif")||h(qs,"src",ep),h(Ss,"class","hidden dark:block"),Bi(Ss.src,sp="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/streaming-dark.gif")||h(Ss,"src",sp),h(G,"class","flex justify-center"),h(We,"href","https://huggingface.co/datasets/oscar"),h(We,"rel","nofollow"),h(Cs,"href","/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset"),h(zs,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset"),h(Ns,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset"),h(ue,"id","shuffle"),h(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ue,"href","#shuffle"),h(K,"class","relative group"),h(Ms,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset"),h(Ls,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset"),h(Os,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset.shuffle"),h(Vs,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset.shuffle"),h(_e,"id","reshuffle"),h(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(_e,"href","#reshuffle"),h(Q,"class","relative group"),h(be,"id","split-dataset"),h(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(be,"href","#split-dataset"),h(X,"class","relative group"),h(Ys,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset.take"),h(Us,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset.skip"),h(Bs,"id","interleave_datasets"),h(ye,"id","interleave"),h(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ye,"href","#interleave"),h(Z,"class","relative group"),h(Js,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.interleave_datasets"),h(Ws,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset"),h(Ee,"id","rename-remove-and-cast"),h(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ee,"href","#rename-remove-and-cast"),h(se,"class","relative group"),h(De,"id","rename"),h(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(De,"href","#rename"),h(ae,"class","relative group"),h(Ks,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset.rename_column"),h(Qs,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset.rename_column"),h(Te,"id","remove"),h(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Te,"href","#remove"),h(te,"class","relative group"),h(Xs,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset.remove_columns"),h(qe,"id","cast"),h(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(qe,"href","#cast"),h(le,"class","relative group"),h(Zs,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset.cast"),h(ea,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset.cast_column"),h(ze,"id","map"),h(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ze,"href","#map"),h(ne,"class","relative group"),h(sa,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map"),h(aa,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset"),h(ta,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset.map"),h(la,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset"),h(na,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset.map"),h(oa,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset"),h(pa,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset.map"),h(ia,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset.map"),h(ha,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Le,"id","batch-processing"),h(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Le,"href","#batch-processing"),h(re,"class","relative group"),h(ca,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset.map"),h(Oe,"id","tokenization"),h(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Oe,"href","#tokenization"),h(oe,"class","relative group"),h(Fe,"id","filter"),h(Fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Fe,"href","#filter"),h(pe,"class","relative group"),h(fa,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.filter"),h(da,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.filter"),h(Ye,"id","stream-in-a-training-loop"),h(Ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ye,"href","#stream-in-a-training-loop"),h(he,"class","relative group"),h(ma,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset")},m(e,p){s(document.head,d),i(e,w,p),i(e,m,p),s(m,y),s(y,k),j(g,k,null),s(m,E),s(m,u),s(u,D),i(e,A,p),i(e,T,p),s(T,M),i(e,fe,p),i(e,C,p),s(C,P),s(P,Is),s(C,Ts),s(C,de),s(de,Ps),i(e,xt,p),i(e,G,p),s(G,qs),s(G,rn),s(G,Ss),i(e,wt,p),i(e,O,p),s(O,on),s(O,We),s(We,pn),s(O,hn),s(O,ja),s(ja,cn),s(O,fn),s(O,Cs),s(Cs,dn),s(O,mn),i(e,yt,p),j(Ge,e,p),i(e,kt,p),i(e,H,p),s(H,un),s(H,zs),s(zs,gn),s(H,_n),s(H,Ns),s(Ns,vn),s(H,jn),i(e,Et,p),j(me,e,p),i(e,Dt,p),i(e,K,p),s(K,ue),s(ue,ba),j(Ke,ba,null),s(K,bn),s(K,$a),s($a,$n),i(e,At,p),i(e,V,p),s(V,xn),s(V,Ms),s(Ms,wn),s(V,yn),s(V,Ls),s(Ls,kn),s(V,En),s(V,Os),s(Os,Dn),s(V,An),i(e,It,p),i(e,F,p),s(F,In),s(F,xa),s(xa,Tn),s(F,Pn),s(F,wa),s(wa,qn),s(F,Sn),s(F,Vs),s(Vs,Cn),s(F,zn),i(e,Tt,p),j(Qe,e,p),i(e,Pt,p),j(ge,e,p),i(e,qt,p),i(e,Q,p),s(Q,_e),s(_e,ya),j(Xe,ya,null),s(Q,Nn),s(Q,ka),s(ka,Mn),i(e,St,p),i(e,ve,p),s(ve,Ln),s(ve,Ea),s(Ea,On),s(ve,Vn),i(e,Ct,p),i(e,je,p),s(je,Fn),s(je,Da),s(Da,Rn),s(je,Yn),i(e,zt,p),j(Ze,e,p),i(e,Nt,p),i(e,X,p),s(X,be),s(be,Aa),j(es,Aa,null),s(X,Hn),s(X,Ia),s(Ia,Un),i(e,Mt,p),i(e,Fs,p),s(Fs,Bn),i(e,Lt,p),i(e,Rs,p),s(Rs,$e),s($e,Ys),s(Ys,Jn),s($e,Wn),s($e,Ta),s(Ta,Gn),s($e,Kn),i(e,Ot,p),j(ss,e,p),i(e,Vt,p),i(e,Hs,p),s(Hs,xe),s(xe,Us),s(Us,Qn),s(xe,Xn),s(xe,Pa),s(Pa,Zn),s(xe,er),i(e,Ft,p),j(as,e,p),i(e,Rt,p),j(we,e,p),i(e,Yt,p),i(e,Bs,p),i(e,Ht,p),i(e,Z,p),s(Z,ye),s(ye,qa),j(ts,qa,null),s(Z,sr),s(Z,Sa),s(Sa,ar),i(e,Ut,p),i(e,ee,p),s(ee,Js),s(Js,tr),s(ee,lr),s(ee,Ws),s(Ws,nr),s(ee,rr),i(e,Bt,p),j(ls,e,p),i(e,Jt,p),i(e,ke,p),s(ke,or),s(ke,Ca),s(Ca,pr),s(ke,ir),i(e,Wt,p),j(ns,e,p),i(e,Gt,p),i(e,U,p),s(U,hr),s(U,za),s(za,cr),s(U,fr),s(U,Na),s(Na,dr),s(U,mr),i(e,Kt,p),i(e,z,p),s(z,ur),s(z,Ma),s(Ma,gr),s(z,_r),s(z,La),s(La,vr),s(z,jr),s(z,Oa),s(Oa,br),s(z,$r),s(z,Va),s(Va,xr),s(z,wr),i(e,Qt,p),i(e,se,p),s(se,Ee),s(Ee,Fa),j(rs,Fa,null),s(se,yr),s(se,Ra),s(Ra,kr),i(e,Xt,p),i(e,Gs,p),s(Gs,Er),i(e,Zt,p),i(e,ae,p),s(ae,De),s(De,Ya),j(os,Ya,null),s(ae,Dr),s(ae,Ha),s(Ha,Ar),i(e,el,p),i(e,Ae,p),s(Ae,Ir),s(Ae,Ks),s(Ks,Tr),s(Ae,Pr),i(e,sl,p),i(e,Ie,p),s(Ie,qr),s(Ie,Qs),s(Qs,Sr),s(Ie,Cr),i(e,al,p),j(ps,e,p),i(e,tl,p),i(e,te,p),s(te,Te),s(Te,Ua),j(is,Ua,null),s(te,zr),s(te,Ba),s(Ba,Nr),i(e,ll,p),i(e,Pe,p),s(Pe,Mr),s(Pe,Xs),s(Xs,Lr),s(Pe,Or),i(e,nl,p),j(hs,e,p),i(e,rl,p),i(e,le,p),s(le,qe),s(qe,Ja),j(cs,Ja,null),s(le,Vr),s(le,Wa),s(Wa,Fr),i(e,ol,p),i(e,L,p),s(L,Zs),s(Zs,Rr),s(L,Yr),s(L,Ga),s(Ga,Hr),s(L,Ur),s(L,Ka),s(Ka,Br),s(L,Jr),s(L,Qa),s(Qa,Wr),s(L,Gr),i(e,pl,p),j(fs,e,p),i(e,il,p),j(Se,e,p),i(e,hl,p),i(e,Ce,p),s(Ce,Kr),s(Ce,ea),s(ea,Qr),s(Ce,Xr),i(e,cl,p),j(ds,e,p),i(e,fl,p),i(e,ne,p),s(ne,ze),s(ze,Xa),j(ms,Xa,null),s(ne,Zr),s(ne,Za),s(Za,eo),i(e,dl,p),i(e,S,p),s(S,so),s(S,sa),s(sa,ao),s(S,to),s(S,aa),s(aa,lo),s(S,no),s(S,ta),s(ta,ro),s(S,oo),s(S,la),s(la,po),s(S,io),s(S,na),s(na,ho),s(S,co),i(e,ml,p),i(e,ra,p),s(ra,fo),i(e,ul,p),i(e,B,p),s(B,mo),s(B,oa),s(oa,uo),s(B,go),s(B,et),s(et,_o),s(B,vo),i(e,gl,p),j(us,e,p),i(e,_l,p),i(e,Ne,p),s(Ne,jo),s(Ne,pa),s(pa,bo),s(Ne,$o),i(e,vl,p),j(gs,e,p),i(e,jl,p),i(e,Me,p),s(Me,xo),s(Me,ia),s(ia,wo),s(Me,yo),i(e,bl,p),i(e,J,p),s(J,ko),s(J,st),s(st,Eo),s(J,Do),s(J,ha),s(ha,Ao),s(J,Io),i(e,$l,p),j(_s,e,p),i(e,xl,p),i(e,re,p),s(re,Le),s(Le,at),j(vs,at,null),s(re,To),s(re,tt),s(tt,Po),i(e,wl,p),i(e,Y,p),s(Y,ca),s(ca,qo),s(Y,So),s(Y,lt),s(lt,Co),s(Y,zo),s(Y,nt),s(nt,No),s(Y,Mo),i(e,yl,p),i(e,oe,p),s(oe,Oe),s(Oe,rt),j(js,rt,null),s(oe,Lo),s(oe,ot),s(ot,Oo),i(e,kl,p),j(bs,e,p),i(e,El,p),j(Ve,e,p),i(e,Dl,p),i(e,pe,p),s(pe,Fe),s(Fe,pt),j($s,pt,null),s(pe,Vo),s(pe,it),s(it,Fo),i(e,Al,p),i(e,Re,p),s(Re,Ro),s(Re,fa),s(fa,Yo),s(Re,Ho),i(e,Il,p),j(xs,e,p),i(e,Tl,p),i(e,ie,p),s(ie,da),s(da,Uo),s(ie,Bo),s(ie,ht),s(ht,Jo),s(ie,Wo),i(e,Pl,p),j(ws,e,p),i(e,ql,p),i(e,he,p),s(he,Ye),s(Ye,ct),j(ys,ct,null),s(he,Go),s(he,ft),s(ft,Ko),i(e,Sl,p),i(e,ks,p),s(ks,ma),s(ma,Qo),s(ks,Xo),i(e,Cl,p),j(He,e,p),zl=!0},p(e,[p]){const Es={};p&2&&(Es.$$scope={dirty:p,ctx:e}),me.$set(Es);const dt={};p&2&&(dt.$$scope={dirty:p,ctx:e}),ge.$set(dt);const mt={};p&2&&(mt.$$scope={dirty:p,ctx:e}),we.$set(mt);const ut={};p&2&&(ut.$$scope={dirty:p,ctx:e}),Se.$set(ut);const gt={};p&2&&(gt.$$scope={dirty:p,ctx:e}),Ve.$set(gt);const Ds={};p&2&&(Ds.$$scope={dirty:p,ctx:e}),He.$set(Ds)},i(e){zl||(b(g.$$.fragment,e),b(Ge.$$.fragment,e),b(me.$$.fragment,e),b(Ke.$$.fragment,e),b(Qe.$$.fragment,e),b(ge.$$.fragment,e),b(Xe.$$.fragment,e),b(Ze.$$.fragment,e),b(es.$$.fragment,e),b(ss.$$.fragment,e),b(as.$$.fragment,e),b(we.$$.fragment,e),b(ts.$$.fragment,e),b(ls.$$.fragment,e),b(ns.$$.fragment,e),b(rs.$$.fragment,e),b(os.$$.fragment,e),b(ps.$$.fragment,e),b(is.$$.fragment,e),b(hs.$$.fragment,e),b(cs.$$.fragment,e),b(fs.$$.fragment,e),b(Se.$$.fragment,e),b(ds.$$.fragment,e),b(ms.$$.fragment,e),b(us.$$.fragment,e),b(gs.$$.fragment,e),b(_s.$$.fragment,e),b(vs.$$.fragment,e),b(js.$$.fragment,e),b(bs.$$.fragment,e),b(Ve.$$.fragment,e),b($s.$$.fragment,e),b(xs.$$.fragment,e),b(ws.$$.fragment,e),b(ys.$$.fragment,e),b(He.$$.fragment,e),zl=!0)},o(e){$(g.$$.fragment,e),$(Ge.$$.fragment,e),$(me.$$.fragment,e),$(Ke.$$.fragment,e),$(Qe.$$.fragment,e),$(ge.$$.fragment,e),$(Xe.$$.fragment,e),$(Ze.$$.fragment,e),$(es.$$.fragment,e),$(ss.$$.fragment,e),$(as.$$.fragment,e),$(we.$$.fragment,e),$(ts.$$.fragment,e),$(ls.$$.fragment,e),$(ns.$$.fragment,e),$(rs.$$.fragment,e),$(os.$$.fragment,e),$(ps.$$.fragment,e),$(is.$$.fragment,e),$(hs.$$.fragment,e),$(cs.$$.fragment,e),$(fs.$$.fragment,e),$(Se.$$.fragment,e),$(ds.$$.fragment,e),$(ms.$$.fragment,e),$(us.$$.fragment,e),$(gs.$$.fragment,e),$(_s.$$.fragment,e),$(vs.$$.fragment,e),$(js.$$.fragment,e),$(bs.$$.fragment,e),$(Ve.$$.fragment,e),$($s.$$.fragment,e),$(xs.$$.fragment,e),$(ws.$$.fragment,e),$(ys.$$.fragment,e),$(He.$$.fragment,e),zl=!1},d(e){a(d),e&&a(w),e&&a(m),x(g),e&&a(A),e&&a(T),e&&a(fe),e&&a(C),e&&a(xt),e&&a(G),e&&a(wt),e&&a(O),e&&a(yt),x(Ge,e),e&&a(kt),e&&a(H),e&&a(Et),x(me,e),e&&a(Dt),e&&a(K),x(Ke),e&&a(At),e&&a(V),e&&a(It),e&&a(F),e&&a(Tt),x(Qe,e),e&&a(Pt),x(ge,e),e&&a(qt),e&&a(Q),x(Xe),e&&a(St),e&&a(ve),e&&a(Ct),e&&a(je),e&&a(zt),x(Ze,e),e&&a(Nt),e&&a(X),x(es),e&&a(Mt),e&&a(Fs),e&&a(Lt),e&&a(Rs),e&&a(Ot),x(ss,e),e&&a(Vt),e&&a(Hs),e&&a(Ft),x(as,e),e&&a(Rt),x(we,e),e&&a(Yt),e&&a(Bs),e&&a(Ht),e&&a(Z),x(ts),e&&a(Ut),e&&a(ee),e&&a(Bt),x(ls,e),e&&a(Jt),e&&a(ke),e&&a(Wt),x(ns,e),e&&a(Gt),e&&a(U),e&&a(Kt),e&&a(z),e&&a(Qt),e&&a(se),x(rs),e&&a(Xt),e&&a(Gs),e&&a(Zt),e&&a(ae),x(os),e&&a(el),e&&a(Ae),e&&a(sl),e&&a(Ie),e&&a(al),x(ps,e),e&&a(tl),e&&a(te),x(is),e&&a(ll),e&&a(Pe),e&&a(nl),x(hs,e),e&&a(rl),e&&a(le),x(cs),e&&a(ol),e&&a(L),e&&a(pl),x(fs,e),e&&a(il),x(Se,e),e&&a(hl),e&&a(Ce),e&&a(cl),x(ds,e),e&&a(fl),e&&a(ne),x(ms),e&&a(dl),e&&a(S),e&&a(ml),e&&a(ra),e&&a(ul),e&&a(B),e&&a(gl),x(us,e),e&&a(_l),e&&a(Ne),e&&a(vl),x(gs,e),e&&a(jl),e&&a(Me),e&&a(bl),e&&a(J),e&&a($l),x(_s,e),e&&a(xl),e&&a(re),x(vs),e&&a(wl),e&&a(Y),e&&a(yl),e&&a(oe),x(js),e&&a(kl),x(bs,e),e&&a(El),x(Ve,e),e&&a(Dl),e&&a(pe),x($s),e&&a(Al),e&&a(Re),e&&a(Il),x(xs,e),e&&a(Tl),e&&a(ie),e&&a(Pl),x(ws,e),e&&a(ql),e&&a(he),x(ys),e&&a(Sl),e&&a(ks),e&&a(Cl),x(He,e)}}}const ih={local:"stream",sections:[{local:"shuffle",title:"Shuffle"},{local:"reshuffle",title:"Reshuffle"},{local:"split-dataset",title:"Split dataset"},{local:"interleave",title:"Interleave"},{local:"rename-remove-and-cast",sections:[{local:"rename",title:"Rename"},{local:"remove",title:"Remove"},{local:"cast",title:"Cast"}],title:"Rename, remove, and cast"},{local:"map",sections:[{local:"batch-processing",sections:[{local:"tokenization",title:"Tokenization"}],title:"Batch processing"},{local:"filter",title:"Filter"}],title:"Map"},{local:"stream-in-a-training-loop",title:"Stream in a training loop"}],title:"Stream"};function hh(q){return Qi(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class _h extends Ji{constructor(d){super();Wi(this,d,hh,ph,Gi,{})}}export{_h as default,ih as metadata};
