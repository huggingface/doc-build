import{S as wu,i as ju,s as bu,e as l,k as d,w as u,t as r,M as xu,c as o,d as a,m as f,a as n,x as m,h as i,b as c,F as t,g as p,y as g,q as _,o as v,B as $}from"../chunks/vendor-e67aec41.js";import{T as Ee}from"../chunks/Tip-76459d1c.js";import{I as P}from"../chunks/IconCopyLink-ffd7f84e.js";import{C as x}from"../chunks/CodeBlock-e2bcf023.js";import{C as so}from"../chunks/CodeBlockFw-1e02e2ba.js";function ku(C){let h,k,y,b,E;return{c(){h=l("p"),k=r("Refer to the "),y=l("a"),b=r("upload_dataset_repo"),E=r(" guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),this.h()},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"Refer to the "),y=o(j,"A",{href:!0});var q=n(y);b=i(q,"upload_dataset_repo"),q.forEach(a),E=i(j," guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),j.forEach(a),this.h()},h(){c(y,"href","#upload_dataset_repo")},m(w,j){p(w,h,j),t(h,k),t(h,y),t(y,b),t(h,E)},d(w){w&&a(h)}}}function Eu(C){let h,k,y,b,E;return{c(){h=l("p"),k=r("If you don\u2019t specify which data files to use, "),y=l("code"),b=r("load_dataset"),E=r(" will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data.")},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"If you don\u2019t specify which data files to use, "),y=o(j,"CODE",{});var q=n(y);b=i(q,"load_dataset"),q.forEach(a),E=i(j," will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data."),j.forEach(a)},m(w,j){p(w,h,j),t(h,k),t(h,y),t(y,b),t(h,E)},d(w){w&&a(h)}}}function qu(C){let h,k,y,b,E,w,j,q,ss,js,R,as,bs,z,M,xs,A;return{c(){h=l("p"),k=r("An object data type in "),y=l("a"),b=r("pandas.Series"),E=r(" doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),w=l("a"),j=r("datasets.Features"),q=r(" using the "),ss=l("code"),js=r("from_dict"),R=r(" or "),as=l("code"),bs=r("from_pandas"),z=r(" methods. See the "),M=l("a"),xs=r("troubleshoot"),A=r(" for more details on how to explicitly specify your own features."),this.h()},l(L){h=o(L,"P",{});var S=n(h);k=i(S,"An object data type in "),y=o(S,"A",{href:!0,rel:!0});var jt=n(y);b=i(jt,"pandas.Series"),jt.forEach(a),E=i(S," doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),w=o(S,"A",{href:!0});var ks=n(w);j=i(ks,"datasets.Features"),ks.forEach(a),q=i(S," using the "),ss=o(S,"CODE",{});var bt=n(ss);js=i(bt,"from_dict"),bt.forEach(a),R=i(S," or "),as=o(S,"CODE",{});var xt=n(as);bs=i(xt,"from_pandas"),xt.forEach(a),z=i(S," methods. See the "),M=o(S,"A",{href:!0});var Es=n(M);xs=i(Es,"troubleshoot"),Es.forEach(a),A=i(S," for more details on how to explicitly specify your own features."),S.forEach(a),this.h()},h(){c(y,"href","https://pandas.pydata.org/docs/reference/api/pandas.Series.html"),c(y,"rel","nofollow"),c(w,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Features"),c(M,"href","#troubleshoot")},m(L,S){p(L,h,S),t(h,k),t(h,y),t(y,b),t(h,E),t(h,w),t(w,j),t(h,q),t(h,ss),t(ss,js),t(h,R),t(h,as),t(as,bs),t(h,z),t(h,M),t(M,xs),t(h,A)},d(L){L&&a(h)}}}function Pu(C){let h,k,y,b,E;return{c(){h=l("p"),k=r("Using "),y=l("code"),b=r("pct1_dropremainder"),E=r(" rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100.")},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"Using "),y=o(j,"CODE",{});var q=n(y);b=i(q,"pct1_dropremainder"),q.forEach(a),E=i(j," rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100."),j.forEach(a)},m(w,j){p(w,h,j),t(h,k),t(h,y),t(y,b),t(h,E)},d(w){w&&a(h)}}}function Au(C){let h,k,y,b,E;return{c(){h=l("p"),k=r("See the "),y=l("a"),b=r("metric_script"),E=r(" guide for more details on how to write your own metric loading script."),this.h()},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"See the "),y=o(j,"A",{href:!0});var q=n(y);b=i(q,"metric_script"),q.forEach(a),E=i(j," guide for more details on how to write your own metric loading script."),j.forEach(a),this.h()},h(){c(y,"href","#metric_script")},m(w,j){p(w,h,j),t(h,k),t(h,y),t(y,b),t(h,E)},d(w){w&&a(h)}}}function Su(C){let h,k,y,b,E;return{c(){h=l("p"),k=r("Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),y=l("a"),b=r("datasets.Metric.compute()"),E=r(" gathers all the predictions and references from the nodes, and computes the final metric."),this.h()},l(w){h=o(w,"P",{});var j=n(h);k=i(j,"Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),y=o(j,"A",{href:!0});var q=n(y);b=i(q,"datasets.Metric.compute()"),q.forEach(a),E=i(j," gathers all the predictions and references from the nodes, and computes the final metric."),j.forEach(a),this.h()},h(){c(y,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Metric.compute")},m(w,j){p(w,h,j),t(h,k),t(h,y),t(y,b),t(h,E)},d(w){w&&a(h)}}}function Tu(C){let h,k,y,b,E,w,j,q,ss,js,R,as,bs,z,M,xs,A,L,S,jt,ks,bt,xt,Es,Wr,Gr,qe,Qr,Xr,Pe,Kr,ao,kt,Zr,to,Et,eo,ts,qs,Ae,ca,si,Se,ai,lo,Ps,ti,Te,ei,li,oo,V,oi,qt,ni,ri,ha,ii,pi,no,ua,ro,Pt,di,io,As,fi,De,ci,hi,po,ma,fo,Ss,co,T,ui,Ce,mi,gi,Ie,_i,vi,Ne,$i,yi,Oe,wi,ji,He,bi,xi,ho,ga,uo,Ts,mo,B,ki,Fe,Ei,qi,_a,Pi,Ai,go,va,_o,Ds,Si,Le,Ti,Di,vo,$a,$o,es,Cs,Re,ya,Ci,ze,Ii,yo,D,Ni,Me,Oi,Hi,Ve,Fi,Li,Be,Ri,zi,Ue,Mi,Vi,At,Bi,Ui,wo,ls,Is,Je,wa,Ji,Ye,Yi,jo,St,Wi,bo,ja,xo,Tt,Gi,ko,ba,Eo,Dt,Qi,qo,xa,Po,Ct,Xi,Ao,ka,So,It,Ki,To,Ea,Do,os,Ns,We,qa,Zi,Ge,sp,Co,Os,ap,Nt,tp,ep,Io,Pa,No,Ot,lp,Oo,Aa,Ho,Hs,op,Qe,np,rp,Fo,Sa,Lo,Ht,ip,Ro,Ta,zo,Ft,pp,Mo,ns,Fs,Xe,Da,dp,Ke,fp,Vo,Lt,cp,Bo,Ca,Uo,Rt,hp,Jo,Ia,Yo,rs,Ls,Ze,Na,up,sl,mp,Wo,zt,gp,Go,Oa,Qo,Mt,_p,Xo,Ha,Ko,is,Rs,al,Fa,vp,tl,$p,Zo,Vt,yp,sn,Bt,wp,an,La,tn,I,jp,el,bp,xp,ll,kp,Ep,Ut,qp,Pp,ol,Ap,Sp,en,Ra,ln,Jt,Tp,on,za,nn,N,Dp,nl,Cp,Ip,rl,Np,Op,il,Hp,Fp,rn,ps,zs,pl,Ma,Lp,dl,Rp,pn,Ms,zp,Yt,Mp,Vp,dn,ds,Vs,fl,Va,Bp,cl,Up,fn,Bs,Jp,Wt,Yp,Wp,cn,Ba,hn,fs,Us,hl,Ua,Gp,ul,Qp,un,Js,Xp,Gt,Kp,Zp,mn,Ja,gn,Ys,_n,cs,Ws,ml,Ya,sd,gl,ad,vn,Qt,td,$n,U,ed,_l,ld,od,vl,nd,rd,yn,hs,Gs,$l,Wa,id,yl,pd,wn,J,dd,Xt,fd,cd,Kt,hd,ud,jn,Y,md,wl,gd,_d,jl,vd,$d,bn,Ga,xn,Qs,yd,bl,wd,jd,kn,Qa,En,Zt,bd,qn,Xa,Pn,se,xd,An,Ka,Sn,ae,kd,Tn,Za,Dn,us,Xs,xl,st,Ed,kl,qd,Cn,te,Pd,In,at,Nn,Ks,Ad,El,Sd,Td,On,tt,Hn,Zs,Fn,ee,Ln,ms,sa,ql,et,Dd,Pl,Cd,Rn,le,Id,zn,gs,aa,Al,lt,Nd,Sl,Od,Mn,O,Hd,oe,Fd,Ld,Tl,Rd,zd,Dl,Md,Vd,Vn,ta,Bd,ot,Ud,Jd,Bn,nt,Un,_s,ea,Cl,rt,Yd,Il,Wd,Jn,W,Gd,ne,Qd,Xd,it,Kd,Zd,Yn,G,sf,re,af,tf,ie,ef,lf,Wn,pt,Gn,Q,of,Nl,nf,rf,pe,pf,df,Qn,dt,Xn,de,ff,Kn,ft,Zn,vs,la,Ol,ct,cf,Hl,hf,sr,fe,uf,ar,ht,tr,oa,er,$s,na,Fl,ut,mf,Ll,gf,lr,ce,_f,or,mt,nr,ys,ra,Rl,gt,vf,zl,$f,rr,he,yf,ir,ue,wf,pr,X,Ml,_t,jf,Vl,bf,xf,kf,Bl,ws,Ef,Ul,qf,Pf,Jl,Af,Sf,Tf,Yl,vt,Df,me,Cf,If,dr,$t,fr,ia,cr,pa,Nf,Wl,Of,Hf,hr,yt,ur;return w=new P({}),ca=new P({}),ua=new x({props:{code:`from datasets import load_dataset
dataset = load_dataset('lhoestq/demo1')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;lhoestq/demo1&#x27;</span>)`}}),ma=new x({props:{code:`dataset = load_dataset(
  "lhoestq/custom_squad",
  revision="main"  # tag name, or branch name, or commit hash
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(
<span class="hljs-meta">&gt;&gt;&gt; </span>  <span class="hljs-string">&quot;lhoestq/custom_squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>  revision=<span class="hljs-string">&quot;main&quot;</span>  <span class="hljs-comment"># tag name, or branch name, or commit hash</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),Ss=new Ee({props:{$$slots:{default:[ku]},$$scope:{ctx:C}}}),ga=new x({props:{code:`data_files = {"train": "train.csv", "test": "test.csv"}
dataset = load_dataset("namespace/your_dataset_name", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: <span class="hljs-string">&quot;train.csv&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>: <span class="hljs-string">&quot;test.csv&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;namespace/your_dataset_name&quot;</span>, data_files=data_files)`}}),Ts=new Ee({props:{warning:"&lcub;true}",$$slots:{default:[Eu]},$$scope:{ctx:C}}}),va=new x({props:{code:`from datasets import load_dataset
c4_subset = load_dataset('allenai/c4', data_files='en/c4-train.0000*-of-01024.json.gz')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_subset = load_dataset(<span class="hljs-string">&#x27;allenai/c4&#x27;</span>, data_files=<span class="hljs-string">&#x27;en/c4-train.0000*-of-01024.json.gz&#x27;</span>)`}}),$a=new x({props:{code:`data_files = {"validation": "en/c4-validation.*.json.gz"}
c4_validation = load_dataset("allenai/c4", data_files=data_files, split="validation")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;validation&quot;</span>: <span class="hljs-string">&quot;en/c4-validation.*.json.gz&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_validation = load_dataset(<span class="hljs-string">&quot;allenai/c4&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;validation&quot;</span>)`}}),ya=new P({}),wa=new P({}),ja=new x({props:{code:`from datasets import load_dataset
dataset = load_dataset('csv', data_files='my_file.csv')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.csv&#x27;</span>)`}}),ba=new x({props:{code:"dataset = load_dataset('csv', data_files=['my_file_1.csv', 'my_file_2.csv', 'my_file_3.csv'])",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=[<span class="hljs-string">&#x27;my_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_2.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_3.csv&#x27;</span>])'}}),xa=new x({props:{code:"dataset = load_dataset('csv', data_files={'train': ['my_train_file_1.csv', 'my_train_file_2.csv'] 'test': 'my_test_file.csv'})",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_train_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_train_file_2.csv&#x27;</span>] <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.csv&#x27;</span>})'}}),ka=new x({props:{code:`base_url = "https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/"
dataset = load_dataset('csv', data_files={'train': base_url + 'train.csv', 'test': base_url + 'test.csv'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train.csv&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: base_url + <span class="hljs-string">&#x27;test.csv&#x27;</span>})`}}),Ea=new x({props:{code:`url = "https://domain.org/train_data.zip"
data_files = {"train": url}
dataset = load_dataset("csv", data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://domain.org/train_data.zip&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: url}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=data_files)`}}),qa=new P({}),Pa=new x({props:{code:`from datasets import load_dataset
dataset = load_dataset('json', data_files='my_file.json')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>)`}}),Aa=new x({props:{code:`{"a": 1, "b": 2.0, "c": "foo", "d": false}
{"a": 4, "b": -5.5, "c": null, "d": true}`,highlighted:`{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false}
{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}`}}),Sa=new x({props:{code:`{"version": "0.1.0",
    "data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},
            {"a": 4, "b": -5.5, "c": null, "d": true}]
}

>>> from datasets import load_dataset
>>> dataset = load_dataset('json', data_files='my_file.json', field='data')`,highlighted:`{<span class="hljs-string">&quot;version&quot;</span>: <span class="hljs-string">&quot;0.1.0&quot;</span>,
    <span class="hljs-string">&quot;data&quot;</span>: [{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false},
            {<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}]
}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>, field=<span class="hljs-string">&#x27;data&#x27;</span>)`}}),Ta=new x({props:{code:`base_url = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
dataset = load_dataset('json', data_files={'train': base_url + 'train-v1.1.json', 'validation': base_url + 'dev-v1.1.json'}, field="data")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://rajpurkar.github.io/SQuAD-explorer/dataset/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train-v1.1.json&#x27;</span>, <span class="hljs-string">&#x27;validation&#x27;</span>: base_url + <span class="hljs-string">&#x27;dev-v1.1.json&#x27;</span>}, field=<span class="hljs-string">&quot;data&quot;</span>)`}}),Da=new P({}),Ca=new x({props:{code:`from datasets import load_dataset
dataset = load_dataset('text', data_files={'train': ['my_text_1.txt', 'my_text_2.txt'], 'test': 'my_test_file.txt'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_text_1.txt&#x27;</span>, <span class="hljs-string">&#x27;my_text_2.txt&#x27;</span>], <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.txt&#x27;</span>})`}}),Ia=new x({props:{code:"dataset = load_dataset('text', data_files='https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files=<span class="hljs-string">&#x27;https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt&#x27;</span>)'}}),Na=new P({}),Oa=new x({props:{code:`from datasets import load_dataset
dataset = load_dataset("parquet", data_files={'train': 'train.parquet', 'test': 'test.parquet'})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;train.parquet&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;test.parquet&#x27;</span>})`}}),Ha=new x({props:{code:`base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"
data_files = {"train": base_url + "wikipedia-train.parquet"}
wiki = load_dataset("parquet", data_files=data_files, split="train")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: base_url + <span class="hljs-string">&quot;wikipedia-train.parquet&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>wiki = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Fa=new P({}),La=new x({props:{code:`   data/dog/xxx.png
   data/dog/xxy.png
   data/dog/xxz.png

   data/cat/123.png
   data/cat/nsdf3.png
   data/cat/asd932_.png`,highlighted:`   data<span class="hljs-regexp">/dog/</span>xxx.png
   data<span class="hljs-regexp">/dog/</span>xxy.png
   data<span class="hljs-regexp">/dog/</span>xxz.png

   data<span class="hljs-regexp">/cat/</span><span class="hljs-number">123</span>.png
   data<span class="hljs-regexp">/cat/</span>nsdf3.png
   data<span class="hljs-regexp">/cat/</span>asd932_.png`}}),Ra=new x({props:{code:`from datasets import load_dataset
dataset = load_dataset("imagefolder", data_dir="/path/to/data")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;imagefolder&quot;</span>, data_dir=<span class="hljs-string">&quot;/path/to/data&quot;</span>)`}}),za=new x({props:{code:'dataset = load_dataset("imagefolder", data_files="https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip", split="train")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;imagefolder&quot;</span>, data_files=<span class="hljs-string">&quot;https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)'}}),Ma=new P({}),Va=new P({}),Ba=new x({props:{code:`from datasets import Dataset
my_dict = {"a": [1, 2, 3]}
dataset = Dataset.from_dict(my_dict)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>my_dict = {<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_dict(my_dict)`}}),Ua=new P({}),Ja=new x({props:{code:`from datasets import Dataset
import pandas as pd
df = pd.DataFrame({"a": [1, 2, 3]})
dataset = Dataset.from_pandas(df)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-meta">&gt;&gt;&gt; </span>df = pd.DataFrame({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_pandas(df)`}}),Ys=new Ee({props:{warning:"&lcub;true}",$$slots:{default:[qu]},$$scope:{ctx:C}}}),Ya=new P({}),Wa=new P({}),Ga=new so({props:{group1:{id:"stringapi",code:"train_test_ds = datasets.load_dataset('bookcorpus', split='train+test')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train+test&#x27;</span>)'},group2:{id:"readinstruction",code:`ri = datasets.ReadInstruction('train') + datasets.ReadInstruction('test')
train_test_ds = datasets.load_dataset('bookcorpus', split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}}),Qa=new so({props:{group1:{id:"stringapi",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split='train[10:20]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[10:20]&#x27;</span>)'},group2:{id:"readinstruction",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', from_=10, to=20, unit='abs'))",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">10</span>, to=<span class="hljs-number">20</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>))'}}}),Xa=new so({props:{group1:{id:"stringapi",code:"train_10pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]&#x27;</span>)'},group2:{id:"readinstruction",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', to=10, unit='%'))",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))'}}}),Ka=new so({props:{group1:{id:"stringapi",code:"train_10_80pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]+train[-80%:]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]+train[-80%:]&#x27;</span>)'},group2:{id:"readinstruction",code:`ri = (datasets.ReadInstruction('train', to=10, unit='%') + datasets.ReadInstruction('train', from_=-80, unit='%'))
train_10_80pct_ds = datasets.load_dataset('bookcorpus', split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = (datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=-<span class="hljs-number">80</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}}),Za=new so({props:{group1:{id:"stringapi",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', split=[f'train[{k}%:{k+10}%]' for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', split=[f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[<span class="hljs-subst">{k}</span>%:<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[:<span class="hljs-subst">{k}</span>%]+train[<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%:]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`},group2:{id:"readinstruction",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', [datasets.ReadInstruction('train', from_=k, to=k+10, unit='%') for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', [(datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')) for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [(datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}}),st=new P({}),at=new x({props:{code:`# Assuming *train* split contains 999 records.
# 19 records, from 500 (included) to 519 (excluded).
>>> train_50_52_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%]')
# 20 records, from 519 (included) to 539 (excluded).
>>> train_52_54_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%]')`,highlighted:`<span class="hljs-comment"># Assuming *train* split contains 999 records.</span>
<span class="hljs-comment"># 19 records, from 500 (included) to 519 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%]&#x27;</span>)
<span class="hljs-comment"># 20 records, from 519 (included) to 539 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%]&#x27;</span>)`}}),tt=new x({props:{code:`# 18 records, from 450 (included) to 468 (excluded).
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction( 'train', from_=50, to=52, unit='%', rounding='pct1_dropremainder'))
# 18 records, from 468 (included) to 486 (excluded).
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train',from_=52, to=54, unit='%', rounding='pct1_dropremainder'))
# Or equivalently:
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%](pct1_dropremainder)')
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%](pct1_dropremainder)')`,highlighted:`<span class="hljs-comment"># 18 records, from 450 (included) to 468 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction( <span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">50</span>, to=<span class="hljs-number">52</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># 18 records, from 468 (included) to 486 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>,from_=<span class="hljs-number">52</span>, to=<span class="hljs-number">54</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># Or equivalently:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%](pct1_dropremainder)&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%](pct1_dropremainder)&#x27;</span>)`}}),Zs=new Ee({props:{warning:"&lcub;true}",$$slots:{default:[Pu]},$$scope:{ctx:C}}}),et=new P({}),lt=new P({}),nt=new x({props:{code:'dataset = load_dataset("matinf", "summarization")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;matinf&quot;</span>, <span class="hljs-string">&quot;summarization&quot;</span>)
Downloading <span class="hljs-keyword">and</span> preparing dataset matinf/summarization (download: Unknown size, generated: <span class="hljs-number">246.89</span> MiB, post-processed: Unknown size, total: <span class="hljs-number">246.89</span> MiB) to /root/.cache/huggingface/datasets/matinf/summarization/<span class="hljs-number">1.0</span><span class="hljs-number">.0</span>/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...
AssertionError: The dataset matinf <span class="hljs-keyword">with</span> config summarization requires manual data. 
Please follow the manual download instructions: To use MATINF you have to download it manually. Please fill this google form (https://forms.gle/nkH4LVE4iNQeDzsc9). You will receive a download link <span class="hljs-keyword">and</span> a password once you complete the form. Please extract <span class="hljs-built_in">all</span> files <span class="hljs-keyword">in</span> one folder <span class="hljs-keyword">and</span> load the dataset <span class="hljs-keyword">with</span>: *datasets.load_dataset(<span class="hljs-string">&#x27;matinf&#x27;</span>, data_dir=<span class="hljs-string">&#x27;path/to/folder/folder_name&#x27;</span>)*. 
Manual data can be loaded <span class="hljs-keyword">with</span> \`datasets.load_dataset(matinf, data_dir=<span class="hljs-string">&#x27;&lt;path/to/manual/data&gt;&#x27;</span>) `}}),rt=new P({}),pt=new x({props:{code:`class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]
emotion_features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)})`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>class_names = [<span class="hljs-string">&quot;sadness&quot;</span>, <span class="hljs-string">&quot;joy&quot;</span>, <span class="hljs-string">&quot;love&quot;</span>, <span class="hljs-string">&quot;anger&quot;</span>, <span class="hljs-string">&quot;fear&quot;</span>, <span class="hljs-string">&quot;surprise&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>emotion_features = Features({<span class="hljs-string">&#x27;text&#x27;</span>: Value(<span class="hljs-string">&#x27;string&#x27;</span>), <span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(names=class_names)})`}}),dt=new x({props:{code:"dataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'], features=emotion_features)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=file_dict, delimiter=<span class="hljs-string">&#x27;;&#x27;</span>, column_names=[<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>], features=emotion_features)'}}),ft=new x({props:{code:"dataset['train'].features",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&#x27;train&#x27;</span>].features
{<span class="hljs-string">&#x27;text&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">6</span>, names=[<span class="hljs-string">&#x27;sadness&#x27;</span>, <span class="hljs-string">&#x27;joy&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;anger&#x27;</span>, <span class="hljs-string">&#x27;fear&#x27;</span>, <span class="hljs-string">&#x27;surprise&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),ct=new P({}),ht=new x({props:{code:`from datasets import load_metric
metric = load_metric('PATH/TO/MY/METRIC/SCRIPT')

# Example of typical usage
for batch in dataset:
    inputs, references = batch
    predictions = model(inputs)
    metric.add_batch(predictions=predictions, references=references)
score = metric.compute()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;PATH/TO/MY/METRIC/SCRIPT&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Example of typical usage</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataset:
<span class="hljs-meta">... </span>    inputs, references = batch
<span class="hljs-meta">... </span>    predictions = model(inputs)
<span class="hljs-meta">... </span>    metric.add_batch(predictions=predictions, references=references)
<span class="hljs-meta">&gt;&gt;&gt; </span>score = metric.compute()`}}),oa=new Ee({props:{$$slots:{default:[Au]},$$scope:{ctx:C}}}),ut=new P({}),mt=new x({props:{code:`from datasets import load_metric
metric = load_metric('bleurt', name='bleurt-base-128')
metric = load_metric('bleurt', name='bleurt-base-512')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-128&#x27;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-512&#x27;</span>)</span>`}}),gt=new P({}),$t=new x({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=rank)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=rank)`}}),ia=new Ee({props:{$$slots:{default:[Su]},$$scope:{ctx:C}}}),yt=new x({props:{code:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=process_id, experiment_id="My_experiment_10")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=process_id, experiment_id=<span class="hljs-string">&quot;My_experiment_10&quot;</span>)`}}),{c(){h=l("meta"),k=d(),y=l("h1"),b=l("a"),E=l("span"),u(w.$$.fragment),j=d(),q=l("span"),ss=r("Load"),js=d(),R=l("p"),as=r("You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),bs=d(),z=l("p"),M=r("This guide will show you how to load a dataset from:"),xs=d(),A=l("ul"),L=l("li"),S=r("The Hub without a dataset loading script"),jt=d(),ks=l("li"),bt=r("Local files"),xt=d(),Es=l("li"),Wr=r("In-memory data"),Gr=d(),qe=l("li"),Qr=r("Offline"),Xr=d(),Pe=l("li"),Kr=r("A specific slice of a split"),ao=d(),kt=l("p"),Zr=r("You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),to=d(),Et=l("a"),eo=d(),ts=l("h2"),qs=l("a"),Ae=l("span"),u(ca.$$.fragment),si=d(),Se=l("span"),ai=r("Hugging Face Hub"),lo=d(),Ps=l("p"),ti=r("In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),Te=l("strong"),ei=r("without"),li=r(" a loading script!"),oo=d(),V=l("p"),oi=r("First, create a dataset repository and upload your data files. Then you can use "),qt=l("a"),ni=r("datasets.load_dataset()"),ri=r(" like you learned in the tutorial. For example, load the files from this "),ha=l("a"),ii=r("demo repository"),pi=r(" by providing the repository namespace and dataset name:"),no=d(),u(ua.$$.fragment),ro=d(),Pt=l("p"),di=r("This dataset repository contains CSV files, and this code loads all the data from the CSV files."),io=d(),As=l("p"),fi=r("Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),De=l("code"),ci=r("revision"),hi=r(" flag to specify which dataset version you want to load:"),po=d(),u(ma.$$.fragment),fo=d(),u(Ss.$$.fragment),co=d(),T=l("p"),ui=r("If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),Ce=l("code"),mi=r("train"),gi=r(" split. Use the "),Ie=l("code"),_i=r("data_files"),vi=r(" parameter to map data files to splits like "),Ne=l("code"),$i=r("train"),yi=r(", "),Oe=l("code"),wi=r("validation"),ji=r(" and "),He=l("code"),bi=r("test"),xi=r(":"),ho=d(),u(ga.$$.fragment),uo=d(),u(Ts.$$.fragment),mo=d(),B=l("p"),ki=r("You can also load a specific subset of the files with the "),Fe=l("code"),Ei=r("data_files"),qi=r(" parameter. The example below loads files from the "),_a=l("a"),Pi=r("C4 dataset"),Ai=r(":"),go=d(),u(va.$$.fragment),_o=d(),Ds=l("p"),Si=r("Specify a custom split with the "),Le=l("code"),Ti=r("split"),Di=r(" parameter:"),vo=d(),u($a.$$.fragment),$o=d(),es=l("h2"),Cs=l("a"),Re=l("span"),u(ya.$$.fragment),Ci=d(),ze=l("span"),Ii=r("Local and remote files"),yo=d(),D=l("p"),Ni=r("\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Me=l("code"),Oi=r("csv"),Hi=r(", "),Ve=l("code"),Fi=r("json"),Li=r(", "),Be=l("code"),Ri=r("txt"),zi=r(" or "),Ue=l("code"),Mi=r("parquet"),Vi=r(" file. The "),At=l("a"),Bi=r("datasets.load_dataset()"),Ui=r(" method is able to load each of these file types."),wo=d(),ls=l("h3"),Is=l("a"),Je=l("span"),u(wa.$$.fragment),Ji=d(),Ye=l("span"),Yi=r("CSV"),jo=d(),St=l("p"),Wi=r("\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),bo=d(),u(ja.$$.fragment),xo=d(),Tt=l("p"),Gi=r("If you have more than one CSV file:"),ko=d(),u(ba.$$.fragment),Eo=d(),Dt=l("p"),Qi=r("You can also map the training and test splits to specific CSV files:"),qo=d(),u(xa.$$.fragment),Po=d(),Ct=l("p"),Xi=r("To load remote CSV files via HTTP, you can pass the URLs:"),Ao=d(),u(ka.$$.fragment),So=d(),It=l("p"),Ki=r("To load zipped CSV files:"),To=d(),u(Ea.$$.fragment),Do=d(),os=l("h3"),Ns=l("a"),We=l("span"),u(qa.$$.fragment),Zi=d(),Ge=l("span"),sp=r("JSON"),Co=d(),Os=l("p"),ap=r("JSON files are loaded directly with "),Nt=l("a"),tp=r("datasets.load_dataset()"),ep=r(" as shown below:"),Io=d(),u(Pa.$$.fragment),No=d(),Ot=l("p"),lp=r("JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),Oo=d(),u(Aa.$$.fragment),Ho=d(),Hs=l("p"),op=r("Another JSON format you may encounter is a nested field, in which case you will need to specify the "),Qe=l("code"),np=r("field"),rp=r(" argument as shown in the following:"),Fo=d(),u(Sa.$$.fragment),Lo=d(),Ht=l("p"),ip=r("To load remote JSON files via HTTP, you can pass the URLs:"),Ro=d(),u(Ta.$$.fragment),zo=d(),Ft=l("p"),pp=r("While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),Mo=d(),ns=l("h3"),Fs=l("a"),Xe=l("span"),u(Da.$$.fragment),dp=d(),Ke=l("span"),fp=r("Text files"),Vo=d(),Lt=l("p"),cp=r("Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),Bo=d(),u(Ca.$$.fragment),Uo=d(),Rt=l("p"),hp=r("To load remote TXT files via HTTP, you can pass the URLs:"),Jo=d(),u(Ia.$$.fragment),Yo=d(),rs=l("h3"),Ls=l("a"),Ze=l("span"),u(Na.$$.fragment),up=d(),sl=l("span"),mp=r("Parquet"),Wo=d(),zt=l("p"),gp=r("Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),Go=d(),u(Oa.$$.fragment),Qo=d(),Mt=l("p"),_p=r("To load remote parquet files via HTTP, you can pass the URLs:"),Xo=d(),u(Ha.$$.fragment),Ko=d(),is=l("h3"),Rs=l("a"),al=l("span"),u(Fa.$$.fragment),vp=d(),tl=l("span"),$p=r("Image folders"),Zo=d(),Vt=l("p"),yp=r("\u{1F917} Datasets can also load generic image folders."),sn=d(),Bt=l("p"),wp=r("The folder structure should look like this:"),an=d(),u(La.$$.fragment),tn=d(),I=l("p"),jp=r("To load an "),el=l("code"),bp=r("imagefolder"),xp=r(" dataset, simply pass the root path of the image folder to the "),ll=l("code"),kp=r("data_dir"),Ep=r(" kwarg of "),Ut=l("a"),qp=r("datasets.load_dataset()"),Pp=r(", which is a shorthand syntax for "),ol=l("code"),Ap=r("data_files=os.path.join(data_dir, **)"),Sp=r("."),en=d(),u(Ra.$$.fragment),ln=d(),Jt=l("p"),Tp=r("To load remote image folders via HTTP, you can pass the URLs:"),on=d(),u(za.$$.fragment),nn=d(),N=l("p"),Dp=r("The resulting dataset will include an "),nl=l("code"),Cp=r("image"),Ip=r(" feature, which is a "),rl=l("code"),Np=r("PIL.Image"),Op=r(" loaded from the image file, and the corresponding "),il=l("code"),Hp=r("label"),Fp=r(" inferred from the directory structure."),rn=d(),ps=l("h2"),zs=l("a"),pl=l("span"),u(Ma.$$.fragment),Lp=d(),dl=l("span"),Rp=r("In-memory data"),pn=d(),Ms=l("p"),zp=r("\u{1F917} Datasets will also allow you to create a "),Yt=l("a"),Mp=r("datasets.Dataset"),Vp=r(" directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),dn=d(),ds=l("h3"),Vs=l("a"),fl=l("span"),u(Va.$$.fragment),Bp=d(),cl=l("span"),Up=r("Python dictionary"),fn=d(),Bs=l("p"),Jp=r("Load Python dictionaries with "),Wt=l("a"),Yp=r("datasets.Dataset.from_dict()"),Wp=r(":"),cn=d(),u(Ba.$$.fragment),hn=d(),fs=l("h3"),Us=l("a"),hl=l("span"),u(Ua.$$.fragment),Gp=d(),ul=l("span"),Qp=r("Pandas DataFrame"),un=d(),Js=l("p"),Xp=r("Load Pandas DataFrames with "),Gt=l("a"),Kp=r("datasets.Dataset.from_pandas()"),Zp=r(":"),mn=d(),u(Ja.$$.fragment),gn=d(),u(Ys.$$.fragment),_n=d(),cs=l("h2"),Ws=l("a"),ml=l("span"),u(Ya.$$.fragment),sd=d(),gl=l("span"),ad=r("Offline"),vn=d(),Qt=l("p"),td=r("Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),$n=d(),U=l("p"),ed=r("If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),_l=l("code"),ld=r("HF_DATASETS_OFFLINE"),od=r(" to "),vl=l("code"),nd=r("1"),rd=r(" to enable full offline mode."),yn=d(),hs=l("h2"),Gs=l("a"),$l=l("span"),u(Wa.$$.fragment),id=d(),yl=l("span"),pd=r("Slice splits"),wn=d(),J=l("p"),dd=r("For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),Xt=l("a"),fd=r("datasets.ReadInstruction"),cd=r(". Strings are more compact and readable for simple cases, while "),Kt=l("a"),hd=r("datasets.ReadInstruction"),ud=r(" is easier to use with variable slicing parameters."),jn=d(),Y=l("p"),md=r("Concatenate the "),wl=l("code"),gd=r("train"),_d=r(" and "),jl=l("code"),vd=r("test"),$d=r(" split by:"),bn=d(),u(Ga.$$.fragment),xn=d(),Qs=l("p"),yd=r("Select specific rows of the "),bl=l("code"),wd=r("train"),jd=r(" split:"),kn=d(),u(Qa.$$.fragment),En=d(),Zt=l("p"),bd=r("Or select a percentage of the split with:"),qn=d(),u(Xa.$$.fragment),Pn=d(),se=l("p"),xd=r("You can even select a combination of percentages from each split:"),An=d(),u(Ka.$$.fragment),Sn=d(),ae=l("p"),kd=r("Finally, create cross-validated dataset splits by:"),Tn=d(),u(Za.$$.fragment),Dn=d(),us=l("h3"),Xs=l("a"),xl=l("span"),u(st.$$.fragment),Ed=d(),kl=l("span"),qd=r("Percent slicing and rounding"),Cn=d(),te=l("p"),Pd=r("For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),In=d(),u(at.$$.fragment),Nn=d(),Ks=l("p"),Ad=r("If you want equal sized splits, use "),El=l("code"),Sd=r("pct1_dropremainder"),Td=r(" rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),On=d(),u(tt.$$.fragment),Hn=d(),u(Zs.$$.fragment),Fn=d(),ee=l("a"),Ln=d(),ms=l("h2"),sa=l("a"),ql=l("span"),u(et.$$.fragment),Dd=d(),Pl=l("span"),Cd=r("Troubleshooting"),Rn=d(),le=l("p"),Id=r("Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),zn=d(),gs=l("h3"),aa=l("a"),Al=l("span"),u(lt.$$.fragment),Nd=d(),Sl=l("span"),Od=r("Manual download"),Mn=d(),O=l("p"),Hd=r("Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),oe=l("a"),Fd=r("datasets.load_dataset()"),Ld=r(" to throw an "),Tl=l("code"),Rd=r("AssertionError"),zd=r(". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),Dl=l("code"),Md=r("data_dir"),Vd=r(" argument to specify the path to the files you just downloaded."),Vn=d(),ta=l("p"),Bd=r("For example, if you try to download a configuration from the "),ot=l("a"),Ud=r("MATINF"),Jd=r(" dataset:"),Bn=d(),u(nt.$$.fragment),Un=d(),_s=l("h3"),ea=l("a"),Cl=l("span"),u(rt.$$.fragment),Yd=d(),Il=l("span"),Wd=r("Specify features"),Jn=d(),W=l("p"),Gd=r("When you create a dataset from local files, the "),ne=l("a"),Qd=r("datasets.Features"),Xd=r(" are automatically inferred by "),it=l("a"),Kd=r("Apache Arrow"),Zd=r(". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),Yn=d(),G=l("p"),sf=r("The following example shows how you can add custom labels with "),re=l("a"),af=r("datasets.ClassLabel"),tf=r(". First, define your own labels using the "),ie=l("a"),ef=r("datasets.Features"),lf=r(" class:"),Wn=d(),u(pt.$$.fragment),Gn=d(),Q=l("p"),of=r("Next, specify the "),Nl=l("code"),nf=r("features"),rf=r(" argument in "),pe=l("a"),pf=r("datasets.load_dataset()"),df=r(" with the features you just created:"),Qn=d(),u(dt.$$.fragment),Xn=d(),de=l("p"),ff=r("Now when you look at your dataset features, you can see it uses the custom labels you defined:"),Kn=d(),u(ft.$$.fragment),Zn=d(),vs=l("h2"),la=l("a"),Ol=l("span"),u(ct.$$.fragment),cf=d(),Hl=l("span"),hf=r("Metrics"),sr=d(),fe=l("p"),uf=r("When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),ar=d(),u(ht.$$.fragment),tr=d(),u(oa.$$.fragment),er=d(),$s=l("h3"),na=l("a"),Fl=l("span"),u(ut.$$.fragment),mf=d(),Ll=l("span"),gf=r("Load configurations"),lr=d(),ce=l("p"),_f=r("It is possible for a metric to have different configurations. The configurations are stored in the [\u2018datasets.Metric.config_name\u2019] attribute. When you load a metric, provide the configuration name as shown in the following:"),or=d(),u(mt.$$.fragment),nr=d(),ys=l("h3"),ra=l("a"),Rl=l("span"),u(gt.$$.fragment),vf=d(),zl=l("span"),$f=r("Distributed setup"),rr=d(),he=l("p"),yf=r("When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),ir=d(),ue=l("p"),wf=r("For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),pr=d(),X=l("ol"),Ml=l("li"),_t=l("p"),jf=r("Define the total number of processes with the "),Vl=l("code"),bf=r("num_process"),xf=r(" argument."),kf=d(),Bl=l("li"),ws=l("p"),Ef=r("Set the process "),Ul=l("code"),qf=r("rank"),Pf=r(" as an integer between zero and "),Jl=l("code"),Af=r("num_process - 1"),Sf=r("."),Tf=d(),Yl=l("li"),vt=l("p"),Df=r("Load your metric with "),me=l("a"),Cf=r("datasets.load_metric()"),If=r(" with these arguments:"),dr=d(),u($t.$$.fragment),fr=d(),u(ia.$$.fragment),cr=d(),pa=l("p"),Nf=r("In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),Wl=l("code"),Of=r("experiment_id"),Hf=r(" to distinguish the separate evaluations:"),hr=d(),u(yt.$$.fragment),this.h()},l(s){const e=xu('[data-svelte="svelte-1phssyn"]',document.head);h=o(e,"META",{name:!0,content:!0}),e.forEach(a),k=f(s),y=o(s,"H1",{class:!0});var wt=n(y);b=o(wt,"A",{id:!0,class:!0,href:!0});var Gl=n(b);E=o(Gl,"SPAN",{});var Ql=n(E);m(w.$$.fragment,Ql),Ql.forEach(a),Gl.forEach(a),j=f(wt),q=o(wt,"SPAN",{});var Xl=n(q);ss=i(Xl,"Load"),Xl.forEach(a),wt.forEach(a),js=f(s),R=o(s,"P",{});var Kl=n(R);as=i(Kl,"You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),Kl.forEach(a),bs=f(s),z=o(s,"P",{});var Zl=n(z);M=i(Zl,"This guide will show you how to load a dataset from:"),Zl.forEach(a),xs=f(s),A=o(s,"UL",{});var K=n(A);L=o(K,"LI",{});var Ff=n(L);S=i(Ff,"The Hub without a dataset loading script"),Ff.forEach(a),jt=f(K),ks=o(K,"LI",{});var Lf=n(ks);bt=i(Lf,"Local files"),Lf.forEach(a),xt=f(K),Es=o(K,"LI",{});var Rf=n(Es);Wr=i(Rf,"In-memory data"),Rf.forEach(a),Gr=f(K),qe=o(K,"LI",{});var zf=n(qe);Qr=i(zf,"Offline"),zf.forEach(a),Xr=f(K),Pe=o(K,"LI",{});var Mf=n(Pe);Kr=i(Mf,"A specific slice of a split"),Mf.forEach(a),K.forEach(a),ao=f(s),kt=o(s,"P",{});var Vf=n(kt);Zr=i(Vf,"You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),Vf.forEach(a),to=f(s),Et=o(s,"A",{id:!0}),n(Et).forEach(a),eo=f(s),ts=o(s,"H2",{class:!0});var mr=n(ts);qs=o(mr,"A",{id:!0,class:!0,href:!0});var Bf=n(qs);Ae=o(Bf,"SPAN",{});var Uf=n(Ae);m(ca.$$.fragment,Uf),Uf.forEach(a),Bf.forEach(a),si=f(mr),Se=o(mr,"SPAN",{});var Jf=n(Se);ai=i(Jf,"Hugging Face Hub"),Jf.forEach(a),mr.forEach(a),lo=f(s),Ps=o(s,"P",{});var gr=n(Ps);ti=i(gr,"In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),Te=o(gr,"STRONG",{});var Yf=n(Te);ei=i(Yf,"without"),Yf.forEach(a),li=i(gr," a loading script!"),gr.forEach(a),oo=f(s),V=o(s,"P",{});var ge=n(V);oi=i(ge,"First, create a dataset repository and upload your data files. Then you can use "),qt=o(ge,"A",{href:!0});var Wf=n(qt);ni=i(Wf,"datasets.load_dataset()"),Wf.forEach(a),ri=i(ge," like you learned in the tutorial. For example, load the files from this "),ha=o(ge,"A",{href:!0,rel:!0});var Gf=n(ha);ii=i(Gf,"demo repository"),Gf.forEach(a),pi=i(ge," by providing the repository namespace and dataset name:"),ge.forEach(a),no=f(s),m(ua.$$.fragment,s),ro=f(s),Pt=o(s,"P",{});var Qf=n(Pt);di=i(Qf,"This dataset repository contains CSV files, and this code loads all the data from the CSV files."),Qf.forEach(a),io=f(s),As=o(s,"P",{});var _r=n(As);fi=i(_r,"Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),De=o(_r,"CODE",{});var Xf=n(De);ci=i(Xf,"revision"),Xf.forEach(a),hi=i(_r," flag to specify which dataset version you want to load:"),_r.forEach(a),po=f(s),m(ma.$$.fragment,s),fo=f(s),m(Ss.$$.fragment,s),co=f(s),T=o(s,"P",{});var H=n(T);ui=i(H,"If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),Ce=o(H,"CODE",{});var Kf=n(Ce);mi=i(Kf,"train"),Kf.forEach(a),gi=i(H," split. Use the "),Ie=o(H,"CODE",{});var Zf=n(Ie);_i=i(Zf,"data_files"),Zf.forEach(a),vi=i(H," parameter to map data files to splits like "),Ne=o(H,"CODE",{});var sc=n(Ne);$i=i(sc,"train"),sc.forEach(a),yi=i(H,", "),Oe=o(H,"CODE",{});var ac=n(Oe);wi=i(ac,"validation"),ac.forEach(a),ji=i(H," and "),He=o(H,"CODE",{});var tc=n(He);bi=i(tc,"test"),tc.forEach(a),xi=i(H,":"),H.forEach(a),ho=f(s),m(ga.$$.fragment,s),uo=f(s),m(Ts.$$.fragment,s),mo=f(s),B=o(s,"P",{});var _e=n(B);ki=i(_e,"You can also load a specific subset of the files with the "),Fe=o(_e,"CODE",{});var ec=n(Fe);Ei=i(ec,"data_files"),ec.forEach(a),qi=i(_e," parameter. The example below loads files from the "),_a=o(_e,"A",{href:!0,rel:!0});var lc=n(_a);Pi=i(lc,"C4 dataset"),lc.forEach(a),Ai=i(_e,":"),_e.forEach(a),go=f(s),m(va.$$.fragment,s),_o=f(s),Ds=o(s,"P",{});var vr=n(Ds);Si=i(vr,"Specify a custom split with the "),Le=o(vr,"CODE",{});var oc=n(Le);Ti=i(oc,"split"),oc.forEach(a),Di=i(vr," parameter:"),vr.forEach(a),vo=f(s),m($a.$$.fragment,s),$o=f(s),es=o(s,"H2",{class:!0});var $r=n(es);Cs=o($r,"A",{id:!0,class:!0,href:!0});var nc=n(Cs);Re=o(nc,"SPAN",{});var rc=n(Re);m(ya.$$.fragment,rc),rc.forEach(a),nc.forEach(a),Ci=f($r),ze=o($r,"SPAN",{});var ic=n(ze);Ii=i(ic,"Local and remote files"),ic.forEach(a),$r.forEach(a),yo=f(s),D=o(s,"P",{});var F=n(D);Ni=i(F,"\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Me=o(F,"CODE",{});var pc=n(Me);Oi=i(pc,"csv"),pc.forEach(a),Hi=i(F,", "),Ve=o(F,"CODE",{});var dc=n(Ve);Fi=i(dc,"json"),dc.forEach(a),Li=i(F,", "),Be=o(F,"CODE",{});var fc=n(Be);Ri=i(fc,"txt"),fc.forEach(a),zi=i(F," or "),Ue=o(F,"CODE",{});var cc=n(Ue);Mi=i(cc,"parquet"),cc.forEach(a),Vi=i(F," file. The "),At=o(F,"A",{href:!0});var hc=n(At);Bi=i(hc,"datasets.load_dataset()"),hc.forEach(a),Ui=i(F," method is able to load each of these file types."),F.forEach(a),wo=f(s),ls=o(s,"H3",{class:!0});var yr=n(ls);Is=o(yr,"A",{id:!0,class:!0,href:!0});var uc=n(Is);Je=o(uc,"SPAN",{});var mc=n(Je);m(wa.$$.fragment,mc),mc.forEach(a),uc.forEach(a),Ji=f(yr),Ye=o(yr,"SPAN",{});var gc=n(Ye);Yi=i(gc,"CSV"),gc.forEach(a),yr.forEach(a),jo=f(s),St=o(s,"P",{});var _c=n(St);Wi=i(_c,"\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),_c.forEach(a),bo=f(s),m(ja.$$.fragment,s),xo=f(s),Tt=o(s,"P",{});var vc=n(Tt);Gi=i(vc,"If you have more than one CSV file:"),vc.forEach(a),ko=f(s),m(ba.$$.fragment,s),Eo=f(s),Dt=o(s,"P",{});var $c=n(Dt);Qi=i($c,"You can also map the training and test splits to specific CSV files:"),$c.forEach(a),qo=f(s),m(xa.$$.fragment,s),Po=f(s),Ct=o(s,"P",{});var yc=n(Ct);Xi=i(yc,"To load remote CSV files via HTTP, you can pass the URLs:"),yc.forEach(a),Ao=f(s),m(ka.$$.fragment,s),So=f(s),It=o(s,"P",{});var wc=n(It);Ki=i(wc,"To load zipped CSV files:"),wc.forEach(a),To=f(s),m(Ea.$$.fragment,s),Do=f(s),os=o(s,"H3",{class:!0});var wr=n(os);Ns=o(wr,"A",{id:!0,class:!0,href:!0});var jc=n(Ns);We=o(jc,"SPAN",{});var bc=n(We);m(qa.$$.fragment,bc),bc.forEach(a),jc.forEach(a),Zi=f(wr),Ge=o(wr,"SPAN",{});var xc=n(Ge);sp=i(xc,"JSON"),xc.forEach(a),wr.forEach(a),Co=f(s),Os=o(s,"P",{});var jr=n(Os);ap=i(jr,"JSON files are loaded directly with "),Nt=o(jr,"A",{href:!0});var kc=n(Nt);tp=i(kc,"datasets.load_dataset()"),kc.forEach(a),ep=i(jr," as shown below:"),jr.forEach(a),Io=f(s),m(Pa.$$.fragment,s),No=f(s),Ot=o(s,"P",{});var Ec=n(Ot);lp=i(Ec,"JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),Ec.forEach(a),Oo=f(s),m(Aa.$$.fragment,s),Ho=f(s),Hs=o(s,"P",{});var br=n(Hs);op=i(br,"Another JSON format you may encounter is a nested field, in which case you will need to specify the "),Qe=o(br,"CODE",{});var qc=n(Qe);np=i(qc,"field"),qc.forEach(a),rp=i(br," argument as shown in the following:"),br.forEach(a),Fo=f(s),m(Sa.$$.fragment,s),Lo=f(s),Ht=o(s,"P",{});var Pc=n(Ht);ip=i(Pc,"To load remote JSON files via HTTP, you can pass the URLs:"),Pc.forEach(a),Ro=f(s),m(Ta.$$.fragment,s),zo=f(s),Ft=o(s,"P",{});var Ac=n(Ft);pp=i(Ac,"While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),Ac.forEach(a),Mo=f(s),ns=o(s,"H3",{class:!0});var xr=n(ns);Fs=o(xr,"A",{id:!0,class:!0,href:!0});var Sc=n(Fs);Xe=o(Sc,"SPAN",{});var Tc=n(Xe);m(Da.$$.fragment,Tc),Tc.forEach(a),Sc.forEach(a),dp=f(xr),Ke=o(xr,"SPAN",{});var Dc=n(Ke);fp=i(Dc,"Text files"),Dc.forEach(a),xr.forEach(a),Vo=f(s),Lt=o(s,"P",{});var Cc=n(Lt);cp=i(Cc,"Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),Cc.forEach(a),Bo=f(s),m(Ca.$$.fragment,s),Uo=f(s),Rt=o(s,"P",{});var Ic=n(Rt);hp=i(Ic,"To load remote TXT files via HTTP, you can pass the URLs:"),Ic.forEach(a),Jo=f(s),m(Ia.$$.fragment,s),Yo=f(s),rs=o(s,"H3",{class:!0});var kr=n(rs);Ls=o(kr,"A",{id:!0,class:!0,href:!0});var Nc=n(Ls);Ze=o(Nc,"SPAN",{});var Oc=n(Ze);m(Na.$$.fragment,Oc),Oc.forEach(a),Nc.forEach(a),up=f(kr),sl=o(kr,"SPAN",{});var Hc=n(sl);mp=i(Hc,"Parquet"),Hc.forEach(a),kr.forEach(a),Wo=f(s),zt=o(s,"P",{});var Fc=n(zt);gp=i(Fc,"Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),Fc.forEach(a),Go=f(s),m(Oa.$$.fragment,s),Qo=f(s),Mt=o(s,"P",{});var Lc=n(Mt);_p=i(Lc,"To load remote parquet files via HTTP, you can pass the URLs:"),Lc.forEach(a),Xo=f(s),m(Ha.$$.fragment,s),Ko=f(s),is=o(s,"H3",{class:!0});var Er=n(is);Rs=o(Er,"A",{id:!0,class:!0,href:!0});var Rc=n(Rs);al=o(Rc,"SPAN",{});var zc=n(al);m(Fa.$$.fragment,zc),zc.forEach(a),Rc.forEach(a),vp=f(Er),tl=o(Er,"SPAN",{});var Mc=n(tl);$p=i(Mc,"Image folders"),Mc.forEach(a),Er.forEach(a),Zo=f(s),Vt=o(s,"P",{});var Vc=n(Vt);yp=i(Vc,"\u{1F917} Datasets can also load generic image folders."),Vc.forEach(a),sn=f(s),Bt=o(s,"P",{});var Bc=n(Bt);wp=i(Bc,"The folder structure should look like this:"),Bc.forEach(a),an=f(s),m(La.$$.fragment,s),tn=f(s),I=o(s,"P",{});var Z=n(I);jp=i(Z,"To load an "),el=o(Z,"CODE",{});var Uc=n(el);bp=i(Uc,"imagefolder"),Uc.forEach(a),xp=i(Z," dataset, simply pass the root path of the image folder to the "),ll=o(Z,"CODE",{});var Jc=n(ll);kp=i(Jc,"data_dir"),Jc.forEach(a),Ep=i(Z," kwarg of "),Ut=o(Z,"A",{href:!0});var Yc=n(Ut);qp=i(Yc,"datasets.load_dataset()"),Yc.forEach(a),Pp=i(Z,", which is a shorthand syntax for "),ol=o(Z,"CODE",{});var Wc=n(ol);Ap=i(Wc,"data_files=os.path.join(data_dir, **)"),Wc.forEach(a),Sp=i(Z,"."),Z.forEach(a),en=f(s),m(Ra.$$.fragment,s),ln=f(s),Jt=o(s,"P",{});var Gc=n(Jt);Tp=i(Gc,"To load remote image folders via HTTP, you can pass the URLs:"),Gc.forEach(a),on=f(s),m(za.$$.fragment,s),nn=f(s),N=o(s,"P",{});var da=n(N);Dp=i(da,"The resulting dataset will include an "),nl=o(da,"CODE",{});var Qc=n(nl);Cp=i(Qc,"image"),Qc.forEach(a),Ip=i(da," feature, which is a "),rl=o(da,"CODE",{});var Xc=n(rl);Np=i(Xc,"PIL.Image"),Xc.forEach(a),Op=i(da," loaded from the image file, and the corresponding "),il=o(da,"CODE",{});var Kc=n(il);Hp=i(Kc,"label"),Kc.forEach(a),Fp=i(da," inferred from the directory structure."),da.forEach(a),rn=f(s),ps=o(s,"H2",{class:!0});var qr=n(ps);zs=o(qr,"A",{id:!0,class:!0,href:!0});var Zc=n(zs);pl=o(Zc,"SPAN",{});var sh=n(pl);m(Ma.$$.fragment,sh),sh.forEach(a),Zc.forEach(a),Lp=f(qr),dl=o(qr,"SPAN",{});var ah=n(dl);Rp=i(ah,"In-memory data"),ah.forEach(a),qr.forEach(a),pn=f(s),Ms=o(s,"P",{});var Pr=n(Ms);zp=i(Pr,"\u{1F917} Datasets will also allow you to create a "),Yt=o(Pr,"A",{href:!0});var th=n(Yt);Mp=i(th,"datasets.Dataset"),th.forEach(a),Vp=i(Pr," directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),Pr.forEach(a),dn=f(s),ds=o(s,"H3",{class:!0});var Ar=n(ds);Vs=o(Ar,"A",{id:!0,class:!0,href:!0});var eh=n(Vs);fl=o(eh,"SPAN",{});var lh=n(fl);m(Va.$$.fragment,lh),lh.forEach(a),eh.forEach(a),Bp=f(Ar),cl=o(Ar,"SPAN",{});var oh=n(cl);Up=i(oh,"Python dictionary"),oh.forEach(a),Ar.forEach(a),fn=f(s),Bs=o(s,"P",{});var Sr=n(Bs);Jp=i(Sr,"Load Python dictionaries with "),Wt=o(Sr,"A",{href:!0});var nh=n(Wt);Yp=i(nh,"datasets.Dataset.from_dict()"),nh.forEach(a),Wp=i(Sr,":"),Sr.forEach(a),cn=f(s),m(Ba.$$.fragment,s),hn=f(s),fs=o(s,"H3",{class:!0});var Tr=n(fs);Us=o(Tr,"A",{id:!0,class:!0,href:!0});var rh=n(Us);hl=o(rh,"SPAN",{});var ih=n(hl);m(Ua.$$.fragment,ih),ih.forEach(a),rh.forEach(a),Gp=f(Tr),ul=o(Tr,"SPAN",{});var ph=n(ul);Qp=i(ph,"Pandas DataFrame"),ph.forEach(a),Tr.forEach(a),un=f(s),Js=o(s,"P",{});var Dr=n(Js);Xp=i(Dr,"Load Pandas DataFrames with "),Gt=o(Dr,"A",{href:!0});var dh=n(Gt);Kp=i(dh,"datasets.Dataset.from_pandas()"),dh.forEach(a),Zp=i(Dr,":"),Dr.forEach(a),mn=f(s),m(Ja.$$.fragment,s),gn=f(s),m(Ys.$$.fragment,s),_n=f(s),cs=o(s,"H2",{class:!0});var Cr=n(cs);Ws=o(Cr,"A",{id:!0,class:!0,href:!0});var fh=n(Ws);ml=o(fh,"SPAN",{});var ch=n(ml);m(Ya.$$.fragment,ch),ch.forEach(a),fh.forEach(a),sd=f(Cr),gl=o(Cr,"SPAN",{});var hh=n(gl);ad=i(hh,"Offline"),hh.forEach(a),Cr.forEach(a),vn=f(s),Qt=o(s,"P",{});var uh=n(Qt);td=i(uh,"Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),uh.forEach(a),$n=f(s),U=o(s,"P",{});var ve=n(U);ed=i(ve,"If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),_l=o(ve,"CODE",{});var mh=n(_l);ld=i(mh,"HF_DATASETS_OFFLINE"),mh.forEach(a),od=i(ve," to "),vl=o(ve,"CODE",{});var gh=n(vl);nd=i(gh,"1"),gh.forEach(a),rd=i(ve," to enable full offline mode."),ve.forEach(a),yn=f(s),hs=o(s,"H2",{class:!0});var Ir=n(hs);Gs=o(Ir,"A",{id:!0,class:!0,href:!0});var _h=n(Gs);$l=o(_h,"SPAN",{});var vh=n($l);m(Wa.$$.fragment,vh),vh.forEach(a),_h.forEach(a),id=f(Ir),yl=o(Ir,"SPAN",{});var $h=n(yl);pd=i($h,"Slice splits"),$h.forEach(a),Ir.forEach(a),wn=f(s),J=o(s,"P",{});var $e=n(J);dd=i($e,"For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),Xt=o($e,"A",{href:!0});var yh=n(Xt);fd=i(yh,"datasets.ReadInstruction"),yh.forEach(a),cd=i($e,". Strings are more compact and readable for simple cases, while "),Kt=o($e,"A",{href:!0});var wh=n(Kt);hd=i(wh,"datasets.ReadInstruction"),wh.forEach(a),ud=i($e," is easier to use with variable slicing parameters."),$e.forEach(a),jn=f(s),Y=o(s,"P",{});var ye=n(Y);md=i(ye,"Concatenate the "),wl=o(ye,"CODE",{});var jh=n(wl);gd=i(jh,"train"),jh.forEach(a),_d=i(ye," and "),jl=o(ye,"CODE",{});var bh=n(jl);vd=i(bh,"test"),bh.forEach(a),$d=i(ye," split by:"),ye.forEach(a),bn=f(s),m(Ga.$$.fragment,s),xn=f(s),Qs=o(s,"P",{});var Nr=n(Qs);yd=i(Nr,"Select specific rows of the "),bl=o(Nr,"CODE",{});var xh=n(bl);wd=i(xh,"train"),xh.forEach(a),jd=i(Nr," split:"),Nr.forEach(a),kn=f(s),m(Qa.$$.fragment,s),En=f(s),Zt=o(s,"P",{});var kh=n(Zt);bd=i(kh,"Or select a percentage of the split with:"),kh.forEach(a),qn=f(s),m(Xa.$$.fragment,s),Pn=f(s),se=o(s,"P",{});var Eh=n(se);xd=i(Eh,"You can even select a combination of percentages from each split:"),Eh.forEach(a),An=f(s),m(Ka.$$.fragment,s),Sn=f(s),ae=o(s,"P",{});var qh=n(ae);kd=i(qh,"Finally, create cross-validated dataset splits by:"),qh.forEach(a),Tn=f(s),m(Za.$$.fragment,s),Dn=f(s),us=o(s,"H3",{class:!0});var Or=n(us);Xs=o(Or,"A",{id:!0,class:!0,href:!0});var Ph=n(Xs);xl=o(Ph,"SPAN",{});var Ah=n(xl);m(st.$$.fragment,Ah),Ah.forEach(a),Ph.forEach(a),Ed=f(Or),kl=o(Or,"SPAN",{});var Sh=n(kl);qd=i(Sh,"Percent slicing and rounding"),Sh.forEach(a),Or.forEach(a),Cn=f(s),te=o(s,"P",{});var Th=n(te);Pd=i(Th,"For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),Th.forEach(a),In=f(s),m(at.$$.fragment,s),Nn=f(s),Ks=o(s,"P",{});var Hr=n(Ks);Ad=i(Hr,"If you want equal sized splits, use "),El=o(Hr,"CODE",{});var Dh=n(El);Sd=i(Dh,"pct1_dropremainder"),Dh.forEach(a),Td=i(Hr," rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),Hr.forEach(a),On=f(s),m(tt.$$.fragment,s),Hn=f(s),m(Zs.$$.fragment,s),Fn=f(s),ee=o(s,"A",{id:!0}),n(ee).forEach(a),Ln=f(s),ms=o(s,"H2",{class:!0});var Fr=n(ms);sa=o(Fr,"A",{id:!0,class:!0,href:!0});var Ch=n(sa);ql=o(Ch,"SPAN",{});var Ih=n(ql);m(et.$$.fragment,Ih),Ih.forEach(a),Ch.forEach(a),Dd=f(Fr),Pl=o(Fr,"SPAN",{});var Nh=n(Pl);Cd=i(Nh,"Troubleshooting"),Nh.forEach(a),Fr.forEach(a),Rn=f(s),le=o(s,"P",{});var Oh=n(le);Id=i(Oh,"Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),Oh.forEach(a),zn=f(s),gs=o(s,"H3",{class:!0});var Lr=n(gs);aa=o(Lr,"A",{id:!0,class:!0,href:!0});var Hh=n(aa);Al=o(Hh,"SPAN",{});var Fh=n(Al);m(lt.$$.fragment,Fh),Fh.forEach(a),Hh.forEach(a),Nd=f(Lr),Sl=o(Lr,"SPAN",{});var Lh=n(Sl);Od=i(Lh,"Manual download"),Lh.forEach(a),Lr.forEach(a),Mn=f(s),O=o(s,"P",{});var fa=n(O);Hd=i(fa,"Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),oe=o(fa,"A",{href:!0});var Rh=n(oe);Fd=i(Rh,"datasets.load_dataset()"),Rh.forEach(a),Ld=i(fa," to throw an "),Tl=o(fa,"CODE",{});var zh=n(Tl);Rd=i(zh,"AssertionError"),zh.forEach(a),zd=i(fa,". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),Dl=o(fa,"CODE",{});var Mh=n(Dl);Md=i(Mh,"data_dir"),Mh.forEach(a),Vd=i(fa," argument to specify the path to the files you just downloaded."),fa.forEach(a),Vn=f(s),ta=o(s,"P",{});var Rr=n(ta);Bd=i(Rr,"For example, if you try to download a configuration from the "),ot=o(Rr,"A",{href:!0,rel:!0});var Vh=n(ot);Ud=i(Vh,"MATINF"),Vh.forEach(a),Jd=i(Rr," dataset:"),Rr.forEach(a),Bn=f(s),m(nt.$$.fragment,s),Un=f(s),_s=o(s,"H3",{class:!0});var zr=n(_s);ea=o(zr,"A",{id:!0,class:!0,href:!0});var Bh=n(ea);Cl=o(Bh,"SPAN",{});var Uh=n(Cl);m(rt.$$.fragment,Uh),Uh.forEach(a),Bh.forEach(a),Yd=f(zr),Il=o(zr,"SPAN",{});var Jh=n(Il);Wd=i(Jh,"Specify features"),Jh.forEach(a),zr.forEach(a),Jn=f(s),W=o(s,"P",{});var we=n(W);Gd=i(we,"When you create a dataset from local files, the "),ne=o(we,"A",{href:!0});var Yh=n(ne);Qd=i(Yh,"datasets.Features"),Yh.forEach(a),Xd=i(we," are automatically inferred by "),it=o(we,"A",{href:!0,rel:!0});var Wh=n(it);Kd=i(Wh,"Apache Arrow"),Wh.forEach(a),Zd=i(we,". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),we.forEach(a),Yn=f(s),G=o(s,"P",{});var je=n(G);sf=i(je,"The following example shows how you can add custom labels with "),re=o(je,"A",{href:!0});var Gh=n(re);af=i(Gh,"datasets.ClassLabel"),Gh.forEach(a),tf=i(je,". First, define your own labels using the "),ie=o(je,"A",{href:!0});var Qh=n(ie);ef=i(Qh,"datasets.Features"),Qh.forEach(a),lf=i(je," class:"),je.forEach(a),Wn=f(s),m(pt.$$.fragment,s),Gn=f(s),Q=o(s,"P",{});var be=n(Q);of=i(be,"Next, specify the "),Nl=o(be,"CODE",{});var Xh=n(Nl);nf=i(Xh,"features"),Xh.forEach(a),rf=i(be," argument in "),pe=o(be,"A",{href:!0});var Kh=n(pe);pf=i(Kh,"datasets.load_dataset()"),Kh.forEach(a),df=i(be," with the features you just created:"),be.forEach(a),Qn=f(s),m(dt.$$.fragment,s),Xn=f(s),de=o(s,"P",{});var Zh=n(de);ff=i(Zh,"Now when you look at your dataset features, you can see it uses the custom labels you defined:"),Zh.forEach(a),Kn=f(s),m(ft.$$.fragment,s),Zn=f(s),vs=o(s,"H2",{class:!0});var Mr=n(vs);la=o(Mr,"A",{id:!0,class:!0,href:!0});var su=n(la);Ol=o(su,"SPAN",{});var au=n(Ol);m(ct.$$.fragment,au),au.forEach(a),su.forEach(a),cf=f(Mr),Hl=o(Mr,"SPAN",{});var tu=n(Hl);hf=i(tu,"Metrics"),tu.forEach(a),Mr.forEach(a),sr=f(s),fe=o(s,"P",{});var eu=n(fe);uf=i(eu,"When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),eu.forEach(a),ar=f(s),m(ht.$$.fragment,s),tr=f(s),m(oa.$$.fragment,s),er=f(s),$s=o(s,"H3",{class:!0});var Vr=n($s);na=o(Vr,"A",{id:!0,class:!0,href:!0});var lu=n(na);Fl=o(lu,"SPAN",{});var ou=n(Fl);m(ut.$$.fragment,ou),ou.forEach(a),lu.forEach(a),mf=f(Vr),Ll=o(Vr,"SPAN",{});var nu=n(Ll);gf=i(nu,"Load configurations"),nu.forEach(a),Vr.forEach(a),lr=f(s),ce=o(s,"P",{});var ru=n(ce);_f=i(ru,"It is possible for a metric to have different configurations. The configurations are stored in the [\u2018datasets.Metric.config_name\u2019] attribute. When you load a metric, provide the configuration name as shown in the following:"),ru.forEach(a),or=f(s),m(mt.$$.fragment,s),nr=f(s),ys=o(s,"H3",{class:!0});var Br=n(ys);ra=o(Br,"A",{id:!0,class:!0,href:!0});var iu=n(ra);Rl=o(iu,"SPAN",{});var pu=n(Rl);m(gt.$$.fragment,pu),pu.forEach(a),iu.forEach(a),vf=f(Br),zl=o(Br,"SPAN",{});var du=n(zl);$f=i(du,"Distributed setup"),du.forEach(a),Br.forEach(a),rr=f(s),he=o(s,"P",{});var fu=n(he);yf=i(fu,"When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),fu.forEach(a),ir=f(s),ue=o(s,"P",{});var cu=n(ue);wf=i(cu,"For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),cu.forEach(a),pr=f(s),X=o(s,"OL",{});var xe=n(X);Ml=o(xe,"LI",{});var hu=n(Ml);_t=o(hu,"P",{});var Ur=n(_t);jf=i(Ur,"Define the total number of processes with the "),Vl=o(Ur,"CODE",{});var uu=n(Vl);bf=i(uu,"num_process"),uu.forEach(a),xf=i(Ur," argument."),Ur.forEach(a),hu.forEach(a),kf=f(xe),Bl=o(xe,"LI",{});var mu=n(Bl);ws=o(mu,"P",{});var ke=n(ws);Ef=i(ke,"Set the process "),Ul=o(ke,"CODE",{});var gu=n(Ul);qf=i(gu,"rank"),gu.forEach(a),Pf=i(ke," as an integer between zero and "),Jl=o(ke,"CODE",{});var _u=n(Jl);Af=i(_u,"num_process - 1"),_u.forEach(a),Sf=i(ke,"."),ke.forEach(a),mu.forEach(a),Tf=f(xe),Yl=o(xe,"LI",{});var vu=n(Yl);vt=o(vu,"P",{});var Jr=n(vt);Df=i(Jr,"Load your metric with "),me=o(Jr,"A",{href:!0});var $u=n(me);Cf=i($u,"datasets.load_metric()"),$u.forEach(a),If=i(Jr," with these arguments:"),Jr.forEach(a),vu.forEach(a),xe.forEach(a),dr=f(s),m($t.$$.fragment,s),fr=f(s),m(ia.$$.fragment,s),cr=f(s),pa=o(s,"P",{});var Yr=n(pa);Nf=i(Yr,"In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),Wl=o(Yr,"CODE",{});var yu=n(Wl);Of=i(yu,"experiment_id"),yu.forEach(a),Hf=i(Yr," to distinguish the separate evaluations:"),Yr.forEach(a),hr=f(s),m(yt.$$.fragment,s),this.h()},h(){c(h,"name","hf:doc:metadata"),c(h,"content",JSON.stringify(Du)),c(b,"id","load"),c(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(b,"href","#load"),c(y,"class","relative group"),c(Et,"id","load-from-the-hub"),c(qs,"id","hugging-face-hub"),c(qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qs,"href","#hugging-face-hub"),c(ts,"class","relative group"),c(qt,"href","/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset"),c(ha,"href","https://huggingface.co/datasets/lhoestq/demo1"),c(ha,"rel","nofollow"),c(_a,"href","https://huggingface.co/datasets/allenai/c4"),c(_a,"rel","nofollow"),c(Cs,"id","local-and-remote-files"),c(Cs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Cs,"href","#local-and-remote-files"),c(es,"class","relative group"),c(At,"href","/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset"),c(Is,"id","csv"),c(Is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Is,"href","#csv"),c(ls,"class","relative group"),c(Ns,"id","json"),c(Ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ns,"href","#json"),c(os,"class","relative group"),c(Nt,"href","/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset"),c(Fs,"id","text-files"),c(Fs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fs,"href","#text-files"),c(ns,"class","relative group"),c(Ls,"id","parquet"),c(Ls,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ls,"href","#parquet"),c(rs,"class","relative group"),c(Rs,"id","image-folders"),c(Rs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Rs,"href","#image-folders"),c(is,"class","relative group"),c(Ut,"href","/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset"),c(zs,"id","inmemory-data"),c(zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zs,"href","#inmemory-data"),c(ps,"class","relative group"),c(Yt,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset"),c(Vs,"id","python-dictionary"),c(Vs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Vs,"href","#python-dictionary"),c(ds,"class","relative group"),c(Wt,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.from_dict"),c(Us,"id","pandas-dataframe"),c(Us,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Us,"href","#pandas-dataframe"),c(fs,"class","relative group"),c(Gt,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.from_pandas"),c(Ws,"id","offline"),c(Ws,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ws,"href","#offline"),c(cs,"class","relative group"),c(Gs,"id","slice-splits"),c(Gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Gs,"href","#slice-splits"),c(hs,"class","relative group"),c(Xt,"href","/docs/datasets/main/en/package_reference/builder_classes#datasets.ReadInstruction"),c(Kt,"href","/docs/datasets/main/en/package_reference/builder_classes#datasets.ReadInstruction"),c(Xs,"id","percent-slicing-and-rounding"),c(Xs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Xs,"href","#percent-slicing-and-rounding"),c(us,"class","relative group"),c(ee,"id","troubleshoot"),c(sa,"id","troubleshooting"),c(sa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(sa,"href","#troubleshooting"),c(ms,"class","relative group"),c(aa,"id","manual-download"),c(aa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(aa,"href","#manual-download"),c(gs,"class","relative group"),c(oe,"href","/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset"),c(ot,"href","https://huggingface.co/datasets/matinf"),c(ot,"rel","nofollow"),c(ea,"id","specify-features"),c(ea,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ea,"href","#specify-features"),c(_s,"class","relative group"),c(ne,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Features"),c(it,"href","https://arrow.apache.org/docs/"),c(it,"rel","nofollow"),c(re,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.ClassLabel"),c(ie,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Features"),c(pe,"href","/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset"),c(la,"id","metrics"),c(la,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(la,"href","#metrics"),c(vs,"class","relative group"),c(na,"id","load-configurations"),c(na,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(na,"href","#load-configurations"),c($s,"class","relative group"),c(ra,"id","distributed-setup"),c(ra,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ra,"href","#distributed-setup"),c(ys,"class","relative group"),c(me,"href","/docs/datasets/main/en/package_reference/loading_methods#datasets.load_metric")},m(s,e){t(document.head,h),p(s,k,e),p(s,y,e),t(y,b),t(b,E),g(w,E,null),t(y,j),t(y,q),t(q,ss),p(s,js,e),p(s,R,e),t(R,as),p(s,bs,e),p(s,z,e),t(z,M),p(s,xs,e),p(s,A,e),t(A,L),t(L,S),t(A,jt),t(A,ks),t(ks,bt),t(A,xt),t(A,Es),t(Es,Wr),t(A,Gr),t(A,qe),t(qe,Qr),t(A,Xr),t(A,Pe),t(Pe,Kr),p(s,ao,e),p(s,kt,e),t(kt,Zr),p(s,to,e),p(s,Et,e),p(s,eo,e),p(s,ts,e),t(ts,qs),t(qs,Ae),g(ca,Ae,null),t(ts,si),t(ts,Se),t(Se,ai),p(s,lo,e),p(s,Ps,e),t(Ps,ti),t(Ps,Te),t(Te,ei),t(Ps,li),p(s,oo,e),p(s,V,e),t(V,oi),t(V,qt),t(qt,ni),t(V,ri),t(V,ha),t(ha,ii),t(V,pi),p(s,no,e),g(ua,s,e),p(s,ro,e),p(s,Pt,e),t(Pt,di),p(s,io,e),p(s,As,e),t(As,fi),t(As,De),t(De,ci),t(As,hi),p(s,po,e),g(ma,s,e),p(s,fo,e),g(Ss,s,e),p(s,co,e),p(s,T,e),t(T,ui),t(T,Ce),t(Ce,mi),t(T,gi),t(T,Ie),t(Ie,_i),t(T,vi),t(T,Ne),t(Ne,$i),t(T,yi),t(T,Oe),t(Oe,wi),t(T,ji),t(T,He),t(He,bi),t(T,xi),p(s,ho,e),g(ga,s,e),p(s,uo,e),g(Ts,s,e),p(s,mo,e),p(s,B,e),t(B,ki),t(B,Fe),t(Fe,Ei),t(B,qi),t(B,_a),t(_a,Pi),t(B,Ai),p(s,go,e),g(va,s,e),p(s,_o,e),p(s,Ds,e),t(Ds,Si),t(Ds,Le),t(Le,Ti),t(Ds,Di),p(s,vo,e),g($a,s,e),p(s,$o,e),p(s,es,e),t(es,Cs),t(Cs,Re),g(ya,Re,null),t(es,Ci),t(es,ze),t(ze,Ii),p(s,yo,e),p(s,D,e),t(D,Ni),t(D,Me),t(Me,Oi),t(D,Hi),t(D,Ve),t(Ve,Fi),t(D,Li),t(D,Be),t(Be,Ri),t(D,zi),t(D,Ue),t(Ue,Mi),t(D,Vi),t(D,At),t(At,Bi),t(D,Ui),p(s,wo,e),p(s,ls,e),t(ls,Is),t(Is,Je),g(wa,Je,null),t(ls,Ji),t(ls,Ye),t(Ye,Yi),p(s,jo,e),p(s,St,e),t(St,Wi),p(s,bo,e),g(ja,s,e),p(s,xo,e),p(s,Tt,e),t(Tt,Gi),p(s,ko,e),g(ba,s,e),p(s,Eo,e),p(s,Dt,e),t(Dt,Qi),p(s,qo,e),g(xa,s,e),p(s,Po,e),p(s,Ct,e),t(Ct,Xi),p(s,Ao,e),g(ka,s,e),p(s,So,e),p(s,It,e),t(It,Ki),p(s,To,e),g(Ea,s,e),p(s,Do,e),p(s,os,e),t(os,Ns),t(Ns,We),g(qa,We,null),t(os,Zi),t(os,Ge),t(Ge,sp),p(s,Co,e),p(s,Os,e),t(Os,ap),t(Os,Nt),t(Nt,tp),t(Os,ep),p(s,Io,e),g(Pa,s,e),p(s,No,e),p(s,Ot,e),t(Ot,lp),p(s,Oo,e),g(Aa,s,e),p(s,Ho,e),p(s,Hs,e),t(Hs,op),t(Hs,Qe),t(Qe,np),t(Hs,rp),p(s,Fo,e),g(Sa,s,e),p(s,Lo,e),p(s,Ht,e),t(Ht,ip),p(s,Ro,e),g(Ta,s,e),p(s,zo,e),p(s,Ft,e),t(Ft,pp),p(s,Mo,e),p(s,ns,e),t(ns,Fs),t(Fs,Xe),g(Da,Xe,null),t(ns,dp),t(ns,Ke),t(Ke,fp),p(s,Vo,e),p(s,Lt,e),t(Lt,cp),p(s,Bo,e),g(Ca,s,e),p(s,Uo,e),p(s,Rt,e),t(Rt,hp),p(s,Jo,e),g(Ia,s,e),p(s,Yo,e),p(s,rs,e),t(rs,Ls),t(Ls,Ze),g(Na,Ze,null),t(rs,up),t(rs,sl),t(sl,mp),p(s,Wo,e),p(s,zt,e),t(zt,gp),p(s,Go,e),g(Oa,s,e),p(s,Qo,e),p(s,Mt,e),t(Mt,_p),p(s,Xo,e),g(Ha,s,e),p(s,Ko,e),p(s,is,e),t(is,Rs),t(Rs,al),g(Fa,al,null),t(is,vp),t(is,tl),t(tl,$p),p(s,Zo,e),p(s,Vt,e),t(Vt,yp),p(s,sn,e),p(s,Bt,e),t(Bt,wp),p(s,an,e),g(La,s,e),p(s,tn,e),p(s,I,e),t(I,jp),t(I,el),t(el,bp),t(I,xp),t(I,ll),t(ll,kp),t(I,Ep),t(I,Ut),t(Ut,qp),t(I,Pp),t(I,ol),t(ol,Ap),t(I,Sp),p(s,en,e),g(Ra,s,e),p(s,ln,e),p(s,Jt,e),t(Jt,Tp),p(s,on,e),g(za,s,e),p(s,nn,e),p(s,N,e),t(N,Dp),t(N,nl),t(nl,Cp),t(N,Ip),t(N,rl),t(rl,Np),t(N,Op),t(N,il),t(il,Hp),t(N,Fp),p(s,rn,e),p(s,ps,e),t(ps,zs),t(zs,pl),g(Ma,pl,null),t(ps,Lp),t(ps,dl),t(dl,Rp),p(s,pn,e),p(s,Ms,e),t(Ms,zp),t(Ms,Yt),t(Yt,Mp),t(Ms,Vp),p(s,dn,e),p(s,ds,e),t(ds,Vs),t(Vs,fl),g(Va,fl,null),t(ds,Bp),t(ds,cl),t(cl,Up),p(s,fn,e),p(s,Bs,e),t(Bs,Jp),t(Bs,Wt),t(Wt,Yp),t(Bs,Wp),p(s,cn,e),g(Ba,s,e),p(s,hn,e),p(s,fs,e),t(fs,Us),t(Us,hl),g(Ua,hl,null),t(fs,Gp),t(fs,ul),t(ul,Qp),p(s,un,e),p(s,Js,e),t(Js,Xp),t(Js,Gt),t(Gt,Kp),t(Js,Zp),p(s,mn,e),g(Ja,s,e),p(s,gn,e),g(Ys,s,e),p(s,_n,e),p(s,cs,e),t(cs,Ws),t(Ws,ml),g(Ya,ml,null),t(cs,sd),t(cs,gl),t(gl,ad),p(s,vn,e),p(s,Qt,e),t(Qt,td),p(s,$n,e),p(s,U,e),t(U,ed),t(U,_l),t(_l,ld),t(U,od),t(U,vl),t(vl,nd),t(U,rd),p(s,yn,e),p(s,hs,e),t(hs,Gs),t(Gs,$l),g(Wa,$l,null),t(hs,id),t(hs,yl),t(yl,pd),p(s,wn,e),p(s,J,e),t(J,dd),t(J,Xt),t(Xt,fd),t(J,cd),t(J,Kt),t(Kt,hd),t(J,ud),p(s,jn,e),p(s,Y,e),t(Y,md),t(Y,wl),t(wl,gd),t(Y,_d),t(Y,jl),t(jl,vd),t(Y,$d),p(s,bn,e),g(Ga,s,e),p(s,xn,e),p(s,Qs,e),t(Qs,yd),t(Qs,bl),t(bl,wd),t(Qs,jd),p(s,kn,e),g(Qa,s,e),p(s,En,e),p(s,Zt,e),t(Zt,bd),p(s,qn,e),g(Xa,s,e),p(s,Pn,e),p(s,se,e),t(se,xd),p(s,An,e),g(Ka,s,e),p(s,Sn,e),p(s,ae,e),t(ae,kd),p(s,Tn,e),g(Za,s,e),p(s,Dn,e),p(s,us,e),t(us,Xs),t(Xs,xl),g(st,xl,null),t(us,Ed),t(us,kl),t(kl,qd),p(s,Cn,e),p(s,te,e),t(te,Pd),p(s,In,e),g(at,s,e),p(s,Nn,e),p(s,Ks,e),t(Ks,Ad),t(Ks,El),t(El,Sd),t(Ks,Td),p(s,On,e),g(tt,s,e),p(s,Hn,e),g(Zs,s,e),p(s,Fn,e),p(s,ee,e),p(s,Ln,e),p(s,ms,e),t(ms,sa),t(sa,ql),g(et,ql,null),t(ms,Dd),t(ms,Pl),t(Pl,Cd),p(s,Rn,e),p(s,le,e),t(le,Id),p(s,zn,e),p(s,gs,e),t(gs,aa),t(aa,Al),g(lt,Al,null),t(gs,Nd),t(gs,Sl),t(Sl,Od),p(s,Mn,e),p(s,O,e),t(O,Hd),t(O,oe),t(oe,Fd),t(O,Ld),t(O,Tl),t(Tl,Rd),t(O,zd),t(O,Dl),t(Dl,Md),t(O,Vd),p(s,Vn,e),p(s,ta,e),t(ta,Bd),t(ta,ot),t(ot,Ud),t(ta,Jd),p(s,Bn,e),g(nt,s,e),p(s,Un,e),p(s,_s,e),t(_s,ea),t(ea,Cl),g(rt,Cl,null),t(_s,Yd),t(_s,Il),t(Il,Wd),p(s,Jn,e),p(s,W,e),t(W,Gd),t(W,ne),t(ne,Qd),t(W,Xd),t(W,it),t(it,Kd),t(W,Zd),p(s,Yn,e),p(s,G,e),t(G,sf),t(G,re),t(re,af),t(G,tf),t(G,ie),t(ie,ef),t(G,lf),p(s,Wn,e),g(pt,s,e),p(s,Gn,e),p(s,Q,e),t(Q,of),t(Q,Nl),t(Nl,nf),t(Q,rf),t(Q,pe),t(pe,pf),t(Q,df),p(s,Qn,e),g(dt,s,e),p(s,Xn,e),p(s,de,e),t(de,ff),p(s,Kn,e),g(ft,s,e),p(s,Zn,e),p(s,vs,e),t(vs,la),t(la,Ol),g(ct,Ol,null),t(vs,cf),t(vs,Hl),t(Hl,hf),p(s,sr,e),p(s,fe,e),t(fe,uf),p(s,ar,e),g(ht,s,e),p(s,tr,e),g(oa,s,e),p(s,er,e),p(s,$s,e),t($s,na),t(na,Fl),g(ut,Fl,null),t($s,mf),t($s,Ll),t(Ll,gf),p(s,lr,e),p(s,ce,e),t(ce,_f),p(s,or,e),g(mt,s,e),p(s,nr,e),p(s,ys,e),t(ys,ra),t(ra,Rl),g(gt,Rl,null),t(ys,vf),t(ys,zl),t(zl,$f),p(s,rr,e),p(s,he,e),t(he,yf),p(s,ir,e),p(s,ue,e),t(ue,wf),p(s,pr,e),p(s,X,e),t(X,Ml),t(Ml,_t),t(_t,jf),t(_t,Vl),t(Vl,bf),t(_t,xf),t(X,kf),t(X,Bl),t(Bl,ws),t(ws,Ef),t(ws,Ul),t(Ul,qf),t(ws,Pf),t(ws,Jl),t(Jl,Af),t(ws,Sf),t(X,Tf),t(X,Yl),t(Yl,vt),t(vt,Df),t(vt,me),t(me,Cf),t(vt,If),p(s,dr,e),g($t,s,e),p(s,fr,e),g(ia,s,e),p(s,cr,e),p(s,pa,e),t(pa,Nf),t(pa,Wl),t(Wl,Of),t(pa,Hf),p(s,hr,e),g(yt,s,e),ur=!0},p(s,[e]){const wt={};e&2&&(wt.$$scope={dirty:e,ctx:s}),Ss.$set(wt);const Gl={};e&2&&(Gl.$$scope={dirty:e,ctx:s}),Ts.$set(Gl);const Ql={};e&2&&(Ql.$$scope={dirty:e,ctx:s}),Ys.$set(Ql);const Xl={};e&2&&(Xl.$$scope={dirty:e,ctx:s}),Zs.$set(Xl);const Kl={};e&2&&(Kl.$$scope={dirty:e,ctx:s}),oa.$set(Kl);const Zl={};e&2&&(Zl.$$scope={dirty:e,ctx:s}),ia.$set(Zl)},i(s){ur||(_(w.$$.fragment,s),_(ca.$$.fragment,s),_(ua.$$.fragment,s),_(ma.$$.fragment,s),_(Ss.$$.fragment,s),_(ga.$$.fragment,s),_(Ts.$$.fragment,s),_(va.$$.fragment,s),_($a.$$.fragment,s),_(ya.$$.fragment,s),_(wa.$$.fragment,s),_(ja.$$.fragment,s),_(ba.$$.fragment,s),_(xa.$$.fragment,s),_(ka.$$.fragment,s),_(Ea.$$.fragment,s),_(qa.$$.fragment,s),_(Pa.$$.fragment,s),_(Aa.$$.fragment,s),_(Sa.$$.fragment,s),_(Ta.$$.fragment,s),_(Da.$$.fragment,s),_(Ca.$$.fragment,s),_(Ia.$$.fragment,s),_(Na.$$.fragment,s),_(Oa.$$.fragment,s),_(Ha.$$.fragment,s),_(Fa.$$.fragment,s),_(La.$$.fragment,s),_(Ra.$$.fragment,s),_(za.$$.fragment,s),_(Ma.$$.fragment,s),_(Va.$$.fragment,s),_(Ba.$$.fragment,s),_(Ua.$$.fragment,s),_(Ja.$$.fragment,s),_(Ys.$$.fragment,s),_(Ya.$$.fragment,s),_(Wa.$$.fragment,s),_(Ga.$$.fragment,s),_(Qa.$$.fragment,s),_(Xa.$$.fragment,s),_(Ka.$$.fragment,s),_(Za.$$.fragment,s),_(st.$$.fragment,s),_(at.$$.fragment,s),_(tt.$$.fragment,s),_(Zs.$$.fragment,s),_(et.$$.fragment,s),_(lt.$$.fragment,s),_(nt.$$.fragment,s),_(rt.$$.fragment,s),_(pt.$$.fragment,s),_(dt.$$.fragment,s),_(ft.$$.fragment,s),_(ct.$$.fragment,s),_(ht.$$.fragment,s),_(oa.$$.fragment,s),_(ut.$$.fragment,s),_(mt.$$.fragment,s),_(gt.$$.fragment,s),_($t.$$.fragment,s),_(ia.$$.fragment,s),_(yt.$$.fragment,s),ur=!0)},o(s){v(w.$$.fragment,s),v(ca.$$.fragment,s),v(ua.$$.fragment,s),v(ma.$$.fragment,s),v(Ss.$$.fragment,s),v(ga.$$.fragment,s),v(Ts.$$.fragment,s),v(va.$$.fragment,s),v($a.$$.fragment,s),v(ya.$$.fragment,s),v(wa.$$.fragment,s),v(ja.$$.fragment,s),v(ba.$$.fragment,s),v(xa.$$.fragment,s),v(ka.$$.fragment,s),v(Ea.$$.fragment,s),v(qa.$$.fragment,s),v(Pa.$$.fragment,s),v(Aa.$$.fragment,s),v(Sa.$$.fragment,s),v(Ta.$$.fragment,s),v(Da.$$.fragment,s),v(Ca.$$.fragment,s),v(Ia.$$.fragment,s),v(Na.$$.fragment,s),v(Oa.$$.fragment,s),v(Ha.$$.fragment,s),v(Fa.$$.fragment,s),v(La.$$.fragment,s),v(Ra.$$.fragment,s),v(za.$$.fragment,s),v(Ma.$$.fragment,s),v(Va.$$.fragment,s),v(Ba.$$.fragment,s),v(Ua.$$.fragment,s),v(Ja.$$.fragment,s),v(Ys.$$.fragment,s),v(Ya.$$.fragment,s),v(Wa.$$.fragment,s),v(Ga.$$.fragment,s),v(Qa.$$.fragment,s),v(Xa.$$.fragment,s),v(Ka.$$.fragment,s),v(Za.$$.fragment,s),v(st.$$.fragment,s),v(at.$$.fragment,s),v(tt.$$.fragment,s),v(Zs.$$.fragment,s),v(et.$$.fragment,s),v(lt.$$.fragment,s),v(nt.$$.fragment,s),v(rt.$$.fragment,s),v(pt.$$.fragment,s),v(dt.$$.fragment,s),v(ft.$$.fragment,s),v(ct.$$.fragment,s),v(ht.$$.fragment,s),v(oa.$$.fragment,s),v(ut.$$.fragment,s),v(mt.$$.fragment,s),v(gt.$$.fragment,s),v($t.$$.fragment,s),v(ia.$$.fragment,s),v(yt.$$.fragment,s),ur=!1},d(s){a(h),s&&a(k),s&&a(y),$(w),s&&a(js),s&&a(R),s&&a(bs),s&&a(z),s&&a(xs),s&&a(A),s&&a(ao),s&&a(kt),s&&a(to),s&&a(Et),s&&a(eo),s&&a(ts),$(ca),s&&a(lo),s&&a(Ps),s&&a(oo),s&&a(V),s&&a(no),$(ua,s),s&&a(ro),s&&a(Pt),s&&a(io),s&&a(As),s&&a(po),$(ma,s),s&&a(fo),$(Ss,s),s&&a(co),s&&a(T),s&&a(ho),$(ga,s),s&&a(uo),$(Ts,s),s&&a(mo),s&&a(B),s&&a(go),$(va,s),s&&a(_o),s&&a(Ds),s&&a(vo),$($a,s),s&&a($o),s&&a(es),$(ya),s&&a(yo),s&&a(D),s&&a(wo),s&&a(ls),$(wa),s&&a(jo),s&&a(St),s&&a(bo),$(ja,s),s&&a(xo),s&&a(Tt),s&&a(ko),$(ba,s),s&&a(Eo),s&&a(Dt),s&&a(qo),$(xa,s),s&&a(Po),s&&a(Ct),s&&a(Ao),$(ka,s),s&&a(So),s&&a(It),s&&a(To),$(Ea,s),s&&a(Do),s&&a(os),$(qa),s&&a(Co),s&&a(Os),s&&a(Io),$(Pa,s),s&&a(No),s&&a(Ot),s&&a(Oo),$(Aa,s),s&&a(Ho),s&&a(Hs),s&&a(Fo),$(Sa,s),s&&a(Lo),s&&a(Ht),s&&a(Ro),$(Ta,s),s&&a(zo),s&&a(Ft),s&&a(Mo),s&&a(ns),$(Da),s&&a(Vo),s&&a(Lt),s&&a(Bo),$(Ca,s),s&&a(Uo),s&&a(Rt),s&&a(Jo),$(Ia,s),s&&a(Yo),s&&a(rs),$(Na),s&&a(Wo),s&&a(zt),s&&a(Go),$(Oa,s),s&&a(Qo),s&&a(Mt),s&&a(Xo),$(Ha,s),s&&a(Ko),s&&a(is),$(Fa),s&&a(Zo),s&&a(Vt),s&&a(sn),s&&a(Bt),s&&a(an),$(La,s),s&&a(tn),s&&a(I),s&&a(en),$(Ra,s),s&&a(ln),s&&a(Jt),s&&a(on),$(za,s),s&&a(nn),s&&a(N),s&&a(rn),s&&a(ps),$(Ma),s&&a(pn),s&&a(Ms),s&&a(dn),s&&a(ds),$(Va),s&&a(fn),s&&a(Bs),s&&a(cn),$(Ba,s),s&&a(hn),s&&a(fs),$(Ua),s&&a(un),s&&a(Js),s&&a(mn),$(Ja,s),s&&a(gn),$(Ys,s),s&&a(_n),s&&a(cs),$(Ya),s&&a(vn),s&&a(Qt),s&&a($n),s&&a(U),s&&a(yn),s&&a(hs),$(Wa),s&&a(wn),s&&a(J),s&&a(jn),s&&a(Y),s&&a(bn),$(Ga,s),s&&a(xn),s&&a(Qs),s&&a(kn),$(Qa,s),s&&a(En),s&&a(Zt),s&&a(qn),$(Xa,s),s&&a(Pn),s&&a(se),s&&a(An),$(Ka,s),s&&a(Sn),s&&a(ae),s&&a(Tn),$(Za,s),s&&a(Dn),s&&a(us),$(st),s&&a(Cn),s&&a(te),s&&a(In),$(at,s),s&&a(Nn),s&&a(Ks),s&&a(On),$(tt,s),s&&a(Hn),$(Zs,s),s&&a(Fn),s&&a(ee),s&&a(Ln),s&&a(ms),$(et),s&&a(Rn),s&&a(le),s&&a(zn),s&&a(gs),$(lt),s&&a(Mn),s&&a(O),s&&a(Vn),s&&a(ta),s&&a(Bn),$(nt,s),s&&a(Un),s&&a(_s),$(rt),s&&a(Jn),s&&a(W),s&&a(Yn),s&&a(G),s&&a(Wn),$(pt,s),s&&a(Gn),s&&a(Q),s&&a(Qn),$(dt,s),s&&a(Xn),s&&a(de),s&&a(Kn),$(ft,s),s&&a(Zn),s&&a(vs),$(ct),s&&a(sr),s&&a(fe),s&&a(ar),$(ht,s),s&&a(tr),$(oa,s),s&&a(er),s&&a($s),$(ut),s&&a(lr),s&&a(ce),s&&a(or),$(mt,s),s&&a(nr),s&&a(ys),$(gt),s&&a(rr),s&&a(he),s&&a(ir),s&&a(ue),s&&a(pr),s&&a(X),s&&a(dr),$($t,s),s&&a(fr),$(ia,s),s&&a(cr),s&&a(pa),s&&a(hr),$(yt,s)}}}const Du={local:"load",sections:[{local:"hugging-face-hub",title:"Hugging Face Hub"},{local:"local-and-remote-files",sections:[{local:"csv",title:"CSV"},{local:"json",title:"JSON"},{local:"text-files",title:"Text files"},{local:"parquet",title:"Parquet"},{local:"image-folders",title:"Image folders"}],title:"Local and remote files"},{local:"inmemory-data",sections:[{local:"python-dictionary",title:"Python dictionary"},{local:"pandas-dataframe",title:"Pandas DataFrame"}],title:"In-memory data"},{local:"offline",title:"Offline"},{local:"slice-splits",sections:[{local:"percent-slicing-and-rounding",title:"Percent slicing and rounding"}],title:"Slice splits"},{local:"troubleshooting",sections:[{local:"manual-download",title:"Manual download"},{local:"specify-features",title:"Specify features"}],title:"Troubleshooting"},{local:"metrics",sections:[{local:"load-configurations",title:"Load configurations"},{local:"distributed-setup",title:"Distributed setup"}],title:"Metrics"}],title:"Load"};function Cu(C,h,k){let{fw:y}=h;return C.$$set=b=>{"fw"in b&&k(0,y=b.fw)},[y]}class Lu extends wu{constructor(h){super();ju(this,h,Cu,Tu,bu,{fw:0})}}export{Lu as default,Du as metadata};
