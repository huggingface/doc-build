import{S as Om,i as Mm,s as Vm,e as r,k as i,w as $,t as c,M as Fm,c as n,d as a,m as p,a as o,x as b,h as m,b as D,G as e,g as y,y as v,q as w,o as x,B as E,v as Um,L as I}from"../../chunks/vendor-hf-doc-builder.js";import{D as j}from"../../chunks/Docstring-hf-doc-builder.js";import{C as B}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as mn}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as S}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function zm(k){let d,h,f,l,u;return l=new B({props:{code:`from datasets import load_dataset_builder
builder = load_dataset_builder('rotten_tomatoes')
ds = builder.download_and_prepare()
ds = builder.as_dataset(split='train')
ds`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.download_and_prepare()
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.as_dataset(split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds
Dataset({
    features: [<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
    num_rows: <span class="hljs-number">8530</span>
})`}}),{c(){d=r("p"),h=c("Example:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"Example:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function Gm(k){let d,h;return d=new B({props:{code:`from datasets import load_dataset_builder
builder = load_dataset_builder("rotten_tomatoes")
ds = builder.download_and_prepare()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&quot;rotten_tomatoes&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.download_and_prepare()`}}),{c(){$(d.$$.fragment)},l(f){b(d.$$.fragment,f)},m(f,l){v(d,f,l),h=!0},p:I,i(f){h||(w(d.$$.fragment,f),h=!0)},o(f){x(d.$$.fragment,f),h=!1},d(f){E(d,f)}}}function Hm(k){let d,h;return d=new B({props:{code:`from datasets import load_dataset_builder
builder = load_dataset_builder("rotten_tomatoes")
ds = builder.download_and_prepare("./output_dir", file_format="parquet")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&quot;rotten_tomatoes&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.download_and_prepare(<span class="hljs-string">&quot;./output_dir&quot;</span>, file_format=<span class="hljs-string">&quot;parquet&quot;</span>)`}}),{c(){$(d.$$.fragment)},l(f){b(d.$$.fragment,f)},m(f,l){v(d,f,l),h=!0},p:I,i(f){h||(w(d.$$.fragment,f),h=!0)},o(f){x(d.$$.fragment,f),h=!1},d(f){E(d,f)}}}function Wm(k){let d,h;return d=new B({props:{code:`from datasets import load_dataset_builder
storage_options = {"key": aws_access_key_id, "secret": aws_secret_access_key}
builder = load_dataset_builder("rotten_tomatoes")
ds = builder.download_and_prepare("s3://my-bucket/my_rotten_tomatoes", storage_options=storage_options, file_format="parquet")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options = {<span class="hljs-string">&quot;key&quot;</span>: aws_access_key_id, <span class="hljs-string">&quot;secret&quot;</span>: aws_secret_access_key}
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&quot;rotten_tomatoes&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.download_and_prepare(<span class="hljs-string">&quot;s3://my-bucket/my_rotten_tomatoes&quot;</span>, storage_options=storage_options, file_format=<span class="hljs-string">&quot;parquet&quot;</span>)`}}),{c(){$(d.$$.fragment)},l(f){b(d.$$.fragment,f)},m(f,l){v(d,f,l),h=!0},p:I,i(f){h||(w(d.$$.fragment,f),h=!0)},o(f){x(d.$$.fragment,f),h=!1},d(f){E(d,f)}}}function Xm(k){let d,h,f,l,u;return l=new B({props:{code:`from datasets import load_dataset_builder
ds_builder = load_dataset_builder('rotten_tomatoes')
ds_builder.get_all_exported_dataset_infos()
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder.get_all_exported_dataset_infos()
{<span class="hljs-string">&#x27;default&#x27;</span>: DatasetInfo(description=<span class="hljs-string">&quot;Movie Review Dataset.
a dataset of containing 5,331 positive and 5,331 negative processed
s from Rotten Tomatoes movie reviews. This data was first used in Bo
 Lillian Lee, \`\`Seeing stars: Exploiting class relationships for
t categorization with respect to rating scales.&#x27;&#x27;, Proceedings of the
5.
ion=&#x27;@InProceedings{Pang+Lee:05a,
 =       {Bo Pang and Lillian Lee},
=        {Seeing stars: Exploiting class relationships for sentiment
          categorization with respect to rating scales},
tle =    {Proceedings of the ACL},
         2005

age=&#x27;http://www.cs.cornell.edu/people/pabo/movie-review-data/&#x27;, license=&#x27;&#x27;, features={&#x27;text&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;label&#x27;: ClassLabel(num_classes=2, names=[&#x27;neg&#x27;, &#x27;pos&#x27;], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input=&#x27;&#x27;, output=&#x27;&#x27;), task_templates=[TextClassification(task=&#x27;text-classification&#x27;, text_column=&#x27;text&#x27;, label_column=&#x27;label&#x27;)], builder_name=&#x27;rotten_tomatoes_movie_review&#x27;, config_name=&#x27;default&#x27;, version=1.0.0, splits={&#x27;train&#x27;: SplitInfo(name=&#x27;train&#x27;, num_bytes=1074810, num_examples=8530, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;validation&#x27;: SplitInfo(name=&#x27;validation&#x27;, num_bytes=134679, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;test&#x27;: SplitInfo(name=&#x27;test&#x27;, num_bytes=135972, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;)}, download_checksums={&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;: {&#x27;num_bytes&#x27;: 487770, &#x27;checksum&#x27;: &#x27;a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9&#x27;}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)}</span>`}}),{c(){d=r("p"),h=c("Example:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"Example:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function Km(k){let d,h,f,l,u;return l=new B({props:{code:`from datasets import load_dataset_builder
ds_builder = load_dataset_builder('rotten_tomatoes')
ds_builder.get_exported_dataset_info()
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder.get_exported_dataset_info()
DatasetInfo(description=<span class="hljs-string">&quot;Movie Review Dataset.
a dataset of containing 5,331 positive and 5,331 negative processed
s from Rotten Tomatoes movie reviews. This data was first used in Bo
 Lillian Lee, \`\`Seeing stars: Exploiting class relationships for
t categorization with respect to rating scales.&#x27;&#x27;, Proceedings of the
5.
ion=&#x27;@InProceedings{Pang+Lee:05a,
 =       {Bo Pang and Lillian Lee},
=        {Seeing stars: Exploiting class relationships for sentiment
          categorization with respect to rating scales},
tle =    {Proceedings of the ACL},
         2005

age=&#x27;http://www.cs.cornell.edu/people/pabo/movie-review-data/&#x27;, license=&#x27;&#x27;, features={&#x27;text&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;label&#x27;: ClassLabel(num_classes=2, names=[&#x27;neg&#x27;, &#x27;pos&#x27;], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input=&#x27;&#x27;, output=&#x27;&#x27;), task_templates=[TextClassification(task=&#x27;text-classification&#x27;, text_column=&#x27;text&#x27;, label_column=&#x27;label&#x27;)], builder_name=&#x27;rotten_tomatoes_movie_review&#x27;, config_name=&#x27;default&#x27;, version=1.0.0, splits={&#x27;train&#x27;: SplitInfo(name=&#x27;train&#x27;, num_bytes=1074810, num_examples=8530, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;validation&#x27;: SplitInfo(name=&#x27;validation&#x27;, num_bytes=134679, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;test&#x27;: SplitInfo(name=&#x27;test&#x27;, num_bytes=135972, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;)}, download_checksums={&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;: {&#x27;num_bytes&#x27;: 487770, &#x27;checksum&#x27;: &#x27;a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9&#x27;}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)</span>`}}),{c(){d=r("p"),h=c("Example:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"Example:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function Jm(k){let d,h,f,l,u;return l=new B({props:{code:"downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)'}}),{c(){d=r("p"),h=c("Example:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"Example:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function Ym(k){let d,h,f,l,u;return l=new B({props:{code:"extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))",highlighted:'<span class="hljs-attr">extracted_paths</span> = dl_manager.extract(dl_manager.download(url_or_urls))'}}),{c(){d=r("p"),h=c("Is roughly equivalent to:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"Is roughly equivalent to:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function Qm(k){let d,h,f,l,u;return l=new B({props:{code:"downloaded_files = dl_manager.download_custom('s3://my-bucket/data.zip', custom_download_for_my_private_bucket)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download_custom(<span class="hljs-string">&#x27;s3://my-bucket/data.zip&#x27;</span>, custom_download_for_my_private_bucket)'}}),{c(){d=r("p"),h=c("Example:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"Example:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function Zm(k){let d,h,f,l,u;return l=new B({props:{code:`downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
extracted_files = dl_manager.extract(downloaded_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>extracted_files = dl_manager.extract(downloaded_files)`}}),{c(){d=r("p"),h=c("Example:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"Example:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function eg(k){let d,h,f,l,u;return l=new B({props:{code:`archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
files = dl_manager.iter_archive(archive)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>archive = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_archive(archive)`}}),{c(){d=r("p"),h=c("Example:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"Example:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function tg(k){let d,h,f,l,u;return l=new B({props:{code:`files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')
files = dl_manager.iter_files(files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.download_and_extract(<span class="hljs-string">&#x27;https://huggingface.co/datasets/beans/resolve/main/data/train.zip&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_files(files)`}}),{c(){d=r("p"),h=c("Example:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"Example:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function ag(k){let d,h,f,l,u;return l=new B({props:{code:"downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)'}}),{c(){d=r("p"),h=c("Example:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"Example:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function sg(k){let d,h,f,l,u;return l=new B({props:{code:"extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))",highlighted:'<span class="hljs-attr">extracted_paths</span> = dl_manager.extract(dl_manager.download(url_or_urls))'}}),{c(){d=r("p"),h=c("Is roughly equivalent to:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"Is roughly equivalent to:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function rg(k){let d,h,f,l,u;return l=new B({props:{code:`downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
extracted_files = dl_manager.extract(downloaded_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>extracted_files = dl_manager.extract(downloaded_files)`}}),{c(){d=r("p"),h=c("Example:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"Example:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function ng(k){let d,h,f,l,u;return l=new B({props:{code:`archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
files = dl_manager.iter_archive(archive)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>archive = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_archive(archive)`}}),{c(){d=r("p"),h=c("Example:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"Example:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function og(k){let d,h,f,l,u;return l=new B({props:{code:`files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')
files = dl_manager.iter_files(files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.download_and_extract(<span class="hljs-string">&#x27;https://huggingface.co/datasets/beans/resolve/main/data/train.zip&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_files(files)`}}),{c(){d=r("p"),h=c("Example:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"Example:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function lg(k){let d,h,f,l,u;return l=new B({props:{code:`datasets.SplitGenerator(
    name=datasets.Split.TRAIN,
    gen_kwargs={"split_key": "train", "files": dl_manager.download_and_extract(url)},
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TRAIN,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and_extract(url)},
<span class="hljs-meta">... </span>)`}}),{c(){d=r("p"),h=c("Example:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"Example:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function dg(k){let d,h,f,l,u;return l=new B({props:{code:`datasets.SplitGenerator(
    name=datasets.Split.TRAIN,
    gen_kwargs={"split_key": "train", "files": dl_manager.download_and extract(url)},
),
datasets.SplitGenerator(
    name=datasets.Split.VALIDATION,
    gen_kwargs={"split_key": "validation", "files": dl_manager.download_and extract(url)},
),
datasets.SplitGenerator(
    name=datasets.Split.TEST,
    gen_kwargs={"split_key": "test", "files": dl_manager.download_and extract(url)},
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TRAIN,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and extract(url)},
<span class="hljs-meta">... </span>),
<span class="hljs-meta">... </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.VALIDATION,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;validation&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and extract(url)},
<span class="hljs-meta">... </span>),
<span class="hljs-meta">... </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TEST,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;test&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and extract(url)},
<span class="hljs-meta">... </span>)`}}),{c(){d=r("p"),h=c("Example:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"Example:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function ig(k){let d,h,f,l,u;return l=new B({props:{code:`Each descriptor can be composed with other using addition or slice. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[0:25]) + datasets.Split.TEST

The resulting split will correspond to 25% of the train split merged with
100% of the test split.`,highlighted:`Each descriptor can be composed <span class="hljs-keyword">with</span> other using addition <span class="hljs-keyword">or</span> <span class="hljs-built_in">slice</span>. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">0</span>:<span class="hljs-number">25</span>]) + datasets.Split.TEST

The resulting split will correspond to <span class="hljs-number">25</span>% of the train split merged <span class="hljs-keyword">with</span>
<span class="hljs-number">100</span>% of the test split.`}}),{c(){d=r("p"),h=c("Example:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"Example:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function pg(k){let d,h,f,l,u;return l=new B({props:{code:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[75:])
)  # Error
split = datasets.Split.TEST + datasets.Split.ALL  # Error`,highlighted:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">75</span>:])
)  <span class="hljs-comment"># Error</span>
split = datasets.Split.TEST + datasets.Split.ALL  <span class="hljs-comment"># Error</span>`}}),{c(){d=r("p"),h=c("A split cannot be added twice, so the following will fail:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"A split cannot be added twice, so the following will fail:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function cg(k){let d,h,f,l,u;return l=new B({props:{code:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
datasets.Split.TEST.subsplit(datasets.percent[:50])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:50])`,highlighted:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
datasets.Split.TEST.subsplit(datasets.percent[:<span class="hljs-number">50</span>])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),{c(){d=r("p"),h=c("The slices can be applied only one time. So the following are valid:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"The slices can be applied only one time. So the following are valid:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function mg(k){let d,h,f,l,u;return l=new B({props:{code:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:25]).subsplit(datasets.percent[:25])
split = (train.subsplit(datasets.percent[:25]) + test).subsplit(datasets.percent[:50])`,highlighted:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]).subsplit(datasets.percent[:<span class="hljs-number">25</span>])
split = (train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) + test).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),{c(){d=r("p"),h=c("But not:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"But not:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function gg(k){let d,h,f,l,u;return l=new B({props:{code:`# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec('test[:33%]'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction('test', to=33, unit='%'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%'))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]+train[1:-1]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%]+train[1:-1]'))
ds = datasets.load_dataset('mnist', split=(
datasets.ReadInstruction('test', to=33, unit='%') +
datasets.ReadInstruction('train', from_=1, to=-1, unit='abs')))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%](pct1_dropremainder)')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%](pct1_dropremainder)'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%', rounding="pct1_dropremainder"))

# 10-fold validation:
tests = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', from_=k, to=k+10, unit='%')
for k in range(0, 100, 10)])
trains = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')
for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(<span class="hljs-string">&#x27;test[:33%]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=(
datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) +
datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">1</span>, to=-<span class="hljs-number">1</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>)))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&quot;pct1_dropremainder&quot;</span>))

<span class="hljs-comment"># 10-fold validation:</span>
tests = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
trains = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}),{c(){d=r("p"),h=c("Examples:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"Examples:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function fg(k){let d,h,f,l,u;return l=new B({props:{code:'VERSION = datasets.Version("1.0.0")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>VERSION = datasets.Version(<span class="hljs-string">&quot;1.0.0&quot;</span>)'}}),{c(){d=r("p"),h=c("Example:"),f=i(),$(l.$$.fragment)},l(t){d=n(t,"P",{});var g=o(d);h=m(g,"Example:"),g.forEach(a),f=p(t),b(l.$$.fragment,t)},m(t,g){y(t,d,g),e(d,h),y(t,f,g),v(l,t,g),u=!0},p:I,i(t){u||(w(l.$$.fragment,t),u=!0)},o(t){x(l.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(l,t)}}}function ug(k){let d,h,f,l,u,t,g,is,Qn,gn,ve,Ve,ps,Dt,Zn,cs,eo,fn,ee,to,ya,ao,so,Da,ro,no,un,T,kt,oo,ms,lo,io,ka,gs,po,co,mo,we,jt,go,fs,fo,uo,ho,Tt,_o,us,$o,bo,vo,xe,wo,hs,xo,Eo,_s,yo,Do,ko,K,$s,jo,To,bs,So,Io,vs,Bo,No,ws,Po,Ro,Co,te,St,qo,xs,Ao,Lo,Fe,Oo,P,It,Mo,Es,Vo,Fo,ys,Uo,zo,ja,Go,Ds,Ho,Wo,Ue,Xo,ks,Ko,Jo,ze,Yo,js,Qo,Zo,Ge,el,ae,Bt,tl,Ts,al,sl,He,rl,se,Nt,nl,Ss,ol,ll,We,dl,Xe,Pt,il,Is,pl,hn,Q,Rt,cl,Bs,ml,gl,re,Ns,fl,ul,Ps,hl,_l,Rs,$l,bl,_n,Ee,Ct,vl,Cs,wl,$n,ye,qt,xl,qs,El,bn,G,At,yl,Lt,Dl,Ta,kl,jl,Tl,Ot,Sl,Sa,Il,Bl,Nl,ne,Mt,Pl,As,Rl,Cl,De,Ls,ql,Al,Os,Ll,Ol,Ms,Ml,vn,ke,Ke,Vs,Vt,Vl,Fs,Fl,wn,R,Ft,Ul,J,Ut,zl,Us,Gl,Hl,je,Wl,zs,Xl,Kl,Gs,Jl,Yl,Ql,Je,Zl,oe,zt,ed,Hs,td,ad,Ye,sd,le,Gt,rd,Ht,nd,Ws,od,ld,dd,Qe,id,de,Wt,pd,Xs,cd,md,Ze,gd,ie,Xt,fd,Ks,ud,hd,et,_d,pe,Kt,$d,Js,bd,vd,tt,wd,at,Jt,xd,Ys,Ed,xn,q,Yt,yd,H,Dd,Qs,kd,jd,Zs,Td,Sd,er,Id,Bd,tr,Nd,Pd,Rd,ce,Qt,Cd,ar,qd,Ad,st,Ld,me,Zt,Od,sr,Md,Vd,rt,Fd,ge,ea,Ud,rr,zd,Gd,nt,Hd,fe,ta,Wd,nr,Xd,Kd,ot,Jd,ue,aa,Yd,or,Qd,Zd,lt,En,Te,sa,ei,lr,ti,yn,V,ra,ai,Ia,dr,si,ri,ni,na,oi,ir,li,di,ii,pr,pi,ci,oa,cr,Se,Dn,mi,mr,gi,fi,gr,ui,hi,Ie,Be,Ba,fr,_i,$i,bi,ur,vi,wi,hr,xi,Ei,Ne,_r,$r,yi,Di,br,ki,ji,vr,Ti,Si,Pe,wr,xr,Ii,Bi,Er,Ni,Pi,yr,Ri,kn,Re,dt,Dr,la,Ci,kr,qi,jn,W,da,Ai,jr,Li,Oi,Ce,Mi,Tr,Vi,Fi,Sr,Ui,zi,Gi,it,Tn,A,ia,Hi,Na,Ir,Wi,Xi,Ki,Br,Ji,Yi,Z,Pa,Nr,Qi,Zi,ep,Ra,Pr,tp,ap,sp,Ca,Rr,rp,np,op,qa,Cr,lp,dp,ip,Aa,pp,qr,cp,mp,pa,gp,Ar,fp,up,hp,pt,Sn,C,ca,_p,Lr,$p,bp,ct,vp,Or,wp,xp,mt,Ep,Mr,yp,Dp,gt,kp,ft,In,qe,ma,jp,Vr,Tp,Bn,F,ga,Sp,Fr,Ip,Bp,ut,Np,ht,fa,Pp,Ur,Rp,Cp,he,ua,qp,zr,Ap,Lp,Gr,Op,Nn,Ae,_t,Hr,ha,Mp,Wr,Vp,Pn,X,_a,Fp,Xr,Up,zp,$t,Gp,bt,$a,Hp,Kr,Wp,Rn;return t=new mn({}),Dt=new mn({}),kt=new j({props:{name:"class datasets.DatasetBuilder",anchor:"datasets.DatasetBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"config_name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"repo_id",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"name",val:" = 'deprecated'"},{name:"**config_kwargs",val:""}],parametersDescription:[{anchor:"datasets.DatasetBuilder.cache_dir",description:"<strong>cache_dir</strong> (<em>str</em>, <em>optional</em>) &#x2014; Directory to cache data. Defaults to <code>&quot;~/.cache/huggingface/datasets&quot;</code>.",name:"cache_dir"},{anchor:"datasets.DatasetBuilder.config_name",description:`<strong>config_name</strong> (<em>str</em>, <em>optional</em>) &#x2014; Name of the dataset configuration.
It affects the data generated on disk: different configurations will have their own subdirectories and
versions.
If not provided, the default configuration is used (if it exists).</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						<p class="font-medium">Added in 2.3.0</p>
						
<p>Parameter <em>name</em> was renamed to <em>config_name</em>.</p>

					</div>`,name:"config_name"},{anchor:"datasets.DatasetBuilder.hash",description:`<strong>hash</strong> (<em>str</em>, <em>optional</em>) &#x2014; Hash specific to the dataset code. Used to update the caching directory when the
dataset loading script code is updated (to avoid reusing old data).
The typical caching directory (defined in <code>self._relative_data_dir</code>) is: <code>name/version/hash/</code>.`,name:"hash"},{anchor:"datasets.DatasetBuilder.base_path",description:`<strong>base_path</strong> (<em>str</em>, <em>optional</em>) &#x2014; Base path for relative paths that are used to download files.
This can be a remote URL.`,name:"base_path"},{anchor:"datasets.DatasetBuilder.features",description:`<strong>features</strong> ([<em>Features</em>], <em>optional</em>) &#x2014; Features types to use with this dataset.
It can be used to change the Features types of a dataset, for example.`,name:"features"},{anchor:"datasets.DatasetBuilder.use_auth_token",description:`<strong>use_auth_token</strong> (<em>str</em> or <em>bool</em>, <em>optional</em>) &#x2014; String or boolean to use as Bearer token for remote files on the
Datasets Hub. If <em>True</em>, will get token from <code>&quot;~/.huggingface&quot;</code>.`,name:"use_auth_token"},{anchor:"datasets.DatasetBuilder.repo_id",description:`<strong>repo_id</strong> (<em>str</em>, <em>optional</em>) &#x2014; ID of the dataset repository.
Used to distinguish builders with the same name but not coming from the same namespace, for example &#x201C;squad&#x201D;
and &#x201C;lhoestq/squad&#x201D; repo IDs. In the latter, the builder name would be &#x201C;lhoestq___squad&#x201D;.`,name:"repo_id"},{anchor:"datasets.DatasetBuilder.data_files",description:`<strong>data_files</strong> (<em>str</em> or <em>Sequence</em> or <em>Mapping</em>, <em>optional</em>) &#x2014; Path(s) to source data file(s).
For builders like &#x201C;csv&#x201D; or &#x201C;json&#x201D; that need the user to specify data files. They can be either
local or remote files. For convenience, you can use a DataFilesDict.`,name:"data_files"},{anchor:"datasets.DatasetBuilder.data_dir",description:`<strong>data_dir</strong> (<em>str</em>, <em>optional</em>) &#x2014; Path to directory containing source data file(s).
Use only if <em>data_files</em> is not passed, in which case it is equivalent to passing
<code>os.path.join(data_dir, &quot;**&quot;)</code> as <em>data_files</em>.
For builders that require manual download, it must be the path to the local directory containing the
manually downloaded data.`,name:"data_dir"},{anchor:"datasets.DatasetBuilder.name",description:`<strong>name</strong> (<em>str</em>) &#x2014; Configuration name for the dataset.</p>
<div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400">
						<p class="font-medium">Deprecated in 2.3.0</p>
						
<p>Use <em>config_name</em> instead.</p>

					</div>`,name:"name"},{anchor:"datasets.DatasetBuilder.*config_kwargs",description:`*<strong>*config_kwargs</strong> (additional keyword arguments) &#x2014; Keyword arguments to be passed to the corresponding builder
configuration class, set on the class attribute [<em>DatasetBuilder.BUILDER_CONFIG_CLASS</em>]. The builder
configuration class is [<em>BuilderConfig</em>] or a subclass of it.`,name:"*config_kwargs"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L187"}}),St=new j({props:{name:"as_dataset",anchor:"datasets.DatasetBuilder.as_dataset",parameters:[{name:"split",val:": typing.Optional[datasets.splits.Split] = None"},{name:"run_post_process",val:" = True"},{name:"ignore_verifications",val:" = False"},{name:"in_memory",val:" = False"}],parametersDescription:[{anchor:"datasets.DatasetBuilder.as_dataset.split",description:"<strong>split</strong> (<code>datasets.Split</code>) &#x2014; Which subset of the data to return.",name:"split"},{anchor:"datasets.DatasetBuilder.as_dataset.run_post_process",description:`<strong>run_post_process</strong> (bool, default=True) &#x2014; Whether to run post-processing dataset transforms and/or add
indexes.`,name:"run_post_process"},{anchor:"datasets.DatasetBuilder.as_dataset.ignore_verifications",description:`<strong>ignore_verifications</strong> (bool, default=False) &#x2014; Whether to ignore the verifications of the
downloaded/processed dataset information (checksums/size/splits/&#x2026;).`,name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.as_dataset.in_memory",description:"<strong>in_memory</strong> (bool, default=False) &#x2014; Whether to copy the data in-memory.",name:"in_memory"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L967",returnDescription:`
<p>datasets.Dataset</p>
`}}),Fe=new S({props:{anchor:"datasets.DatasetBuilder.as_dataset.example",$$slots:{default:[zm]},$$scope:{ctx:k}}}),It=new j({props:{name:"download_and_prepare",anchor:"datasets.DatasetBuilder.download_and_prepare",parameters:[{name:"output_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.download.download_config.DownloadConfig] = None"},{name:"download_mode",val:": typing.Optional[datasets.download.download_manager.DownloadMode] = None"},{name:"ignore_verifications",val:": bool = False"},{name:"try_from_hf_gcs",val:": bool = True"},{name:"dl_manager",val:": typing.Optional[datasets.download.download_manager.DownloadManager] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"file_format",val:": str = 'arrow'"},{name:"max_shard_size",val:": typing.Union[int, str, NoneType] = None"},{name:"storage_options",val:": typing.Optional[dict] = None"},{name:"**download_and_prepare_kwargs",val:""}],parametersDescription:[{anchor:"datasets.DatasetBuilder.download_and_prepare.output_dir",description:`<strong>output_dir</strong> (<code>str</code>, optional) &#x2014; output directory for the dataset.
Default to this builder&#x2019;s <code>cache_dir</code>, which is inside ~/.cache/huggingface/datasets by default.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						<p class="font-medium">Added in 2.5.0</p>
						
					</div>`,name:"output_dir"},{anchor:"datasets.DatasetBuilder.download_and_prepare.download_config",description:'<strong>download_config</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.DownloadConfig">DownloadConfig</a>, optional) &#x2014; specific download configuration parameters.',name:"download_config"},{anchor:"datasets.DatasetBuilder.download_and_prepare.download_mode",description:'<strong>download_mode</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.DownloadMode">DownloadMode</a>, optional) &#x2014; select the download/generate mode - Default to <code>REUSE_DATASET_IF_EXISTS</code>',name:"download_mode"},{anchor:"datasets.DatasetBuilder.download_and_prepare.ignore_verifications",description:"<strong>ignore_verifications</strong> (<code>bool</code>) &#x2014; Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/&#x2026;)",name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.download_and_prepare.try_from_hf_gcs",description:"<strong>try_from_hf_gcs</strong> (<code>bool</code>) &#x2014; If True, it will try to download the already prepared dataset from the Hf google cloud storage",name:"try_from_hf_gcs"},{anchor:"datasets.DatasetBuilder.download_and_prepare.dl_manager",description:'<strong>dl_manager</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.DownloadManager">DownloadManager</a>, optional) &#x2014; specific Download Manger to use',name:"dl_manager"},{anchor:"datasets.DatasetBuilder.download_and_prepare.base_path",description:`<strong>base_path</strong> (<code>str</code>, optional) &#x2014; base path for relative paths that are used to download files. This can be a remote url.
If not specified, the value of the <em>base_path</em> attribute (<em>self.base_path</em>) will be used instead.`,name:"base_path"},{anchor:"datasets.DatasetBuilder.download_and_prepare.use_auth_token",description:`<strong>use_auth_token</strong> (<code>Union[str, bool]</code>, optional) &#x2014; Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.
If True, will get token from ~/.huggingface.`,name:"use_auth_token"},{anchor:"datasets.DatasetBuilder.download_and_prepare.file_format",description:`<strong>file_format</strong> (<code>str</code>, optional) &#x2014; format of the data files in which the dataset will be written.
Supported formats: &#x201C;arrow&#x201D;, &#x201C;parquet&#x201D;. Default to &#x201C;arrow&#x201D; format.
If the format is &#x201C;parquet&#x201D;, then image and audio data are embedded into the Parquet files instead of pointing to local files.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						<p class="font-medium">Added in 2.5.0</p>
						
					</div>`,name:"file_format"},{anchor:"datasets.DatasetBuilder.download_and_prepare.max_shard_size",description:`<strong>max_shard_size</strong> (<code>Union[str, int]</code>, optional) &#x2014; Maximum number of bytes written per shard.
Only available for the &#x201C;parquet&#x201D; format with a default of &#x201C;500MB&#x201D;. The size is based on uncompressed data size,
so in practice your shard files may be smaller than <em>max_shard_size</em> thanks to Parquet compression.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						<p class="font-medium">Added in 2.5.0</p>
						
					</div>`,name:"max_shard_size"},{anchor:"datasets.DatasetBuilder.download_and_prepare.storage_options",description:`<strong>storage_options</strong> (<code>dict</code>, <em>optional</em>) &#x2014; Key/value pairs to be passed on to the caching file-system backend, if any.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						<p class="font-medium">Added in 2.5.0</p>
						
					</div>`,name:"storage_options"},{anchor:"datasets.DatasetBuilder.download_and_prepare.*download_and_prepare_kwargs",description:"*<strong>*download_and_prepare_kwargs</strong> (additional keyword arguments) &#x2014; Keyword arguments.",name:"*download_and_prepare_kwargs"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L596"}}),Ue=new S({props:{anchor:"datasets.DatasetBuilder.download_and_prepare.example",$$slots:{default:[Gm]},$$scope:{ctx:k}}}),ze=new S({props:{anchor:"datasets.DatasetBuilder.download_and_prepare.example-2",$$slots:{default:[Hm]},$$scope:{ctx:k}}}),Ge=new S({props:{anchor:"datasets.DatasetBuilder.download_and_prepare.example-3",$$slots:{default:[Wm]},$$scope:{ctx:k}}}),Bt=new j({props:{name:"get_all_exported_dataset_infos",anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos",parameters:[],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L385"}}),He=new S({props:{anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos.example",$$slots:{default:[Xm]},$$scope:{ctx:k}}}),Nt=new j({props:{name:"get_exported_dataset_info",anchor:"datasets.DatasetBuilder.get_exported_dataset_info",parameters:[],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L400"}}),We=new S({props:{anchor:"datasets.DatasetBuilder.get_exported_dataset_info.example",$$slots:{default:[Km]},$$scope:{ctx:k}}}),Pt=new j({props:{name:"get_imported_module_dir",anchor:"datasets.DatasetBuilder.get_imported_module_dir",parameters:[],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L583"}}),Rt=new j({props:{name:"class datasets.GeneratorBasedBuilder",anchor:"datasets.GeneratorBasedBuilder",parameters:[{name:"*args",val:""},{name:"writer_batch_size",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L1273"}}),Ct=new j({props:{name:"class datasets.BeamBasedBuilder",anchor:"datasets.BeamBasedBuilder",parameters:[{name:"*args",val:""},{name:"beam_runner",val:" = None"},{name:"beam_options",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L1554"}}),qt=new j({props:{name:"class datasets.ArrowBasedBuilder",anchor:"datasets.ArrowBasedBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"config_name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"repo_id",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"name",val:" = 'deprecated'"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L1431"}}),At=new j({props:{name:"class datasets.BuilderConfig",anchor:"datasets.BuilderConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = 0.0.0"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"datasets.BuilderConfig.name",description:"<strong>name</strong> (<code>str</code>, default <code>&quot;default&quot;</code>) &#x2014;",name:"name"},{anchor:"datasets.BuilderConfig.version",description:'<strong>version</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.Version">Version</a> or <code>str</code>, optional) &#x2014;',name:"version"},{anchor:"datasets.BuilderConfig.data_dir",description:"<strong>data_dir</strong> (<code>str</code>, optional) &#x2014;",name:"data_dir"},{anchor:"datasets.BuilderConfig.data_files",description:"<strong>data_files</strong> (<code>str</code> or <code>Sequence</code> or <code>Mapping</code>, optional) &#x2014; Path(s) to source data file(s).",name:"data_files"},{anchor:"datasets.BuilderConfig.description",description:"<strong>description</strong> (<code>str</code>, optional) &#x2014;",name:"description"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L92"}}),Mt=new j({props:{name:"create_config_id",anchor:"datasets.BuilderConfig.create_config_id",parameters:[{name:"config_kwargs",val:": dict"},{name:"custom_features",val:": typing.Optional[datasets.features.features.Features] = None"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L130"}}),Vt=new mn({}),Ft=new j({props:{name:"class datasets.DownloadManager",anchor:"datasets.DownloadManager",parameters:[{name:"dataset_name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.download.download_config.DownloadConfig] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"record_checksums",val:" = True"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/download/download_manager.py#L148"}}),Ut=new j({props:{name:"download",anchor:"datasets.DownloadManager.download",parameters:[{name:"url_or_urls",val:""}],parametersDescription:[{anchor:"datasets.DownloadManager.download.url_or_urls",description:"<strong>url_or_urls</strong> (<code>str</code> or <code>list</code> or <code>dict</code>) &#x2014; URL or list/dict of URLs to download. Each URL is a <code>str</code>.",name:"url_or_urls"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/download/download_manager.py#L280",returnDescription:`
<p>The downloaded paths matching the given input <code>url_or_urls</code>.</p>
`,returnType:`
<p><code>str</code> or <code>list</code> or <code>dict</code></p>
`}}),Je=new S({props:{anchor:"datasets.DownloadManager.download.example",$$slots:{default:[Jm]},$$scope:{ctx:k}}}),zt=new j({props:{name:"download_and_extract",anchor:"datasets.DownloadManager.download_and_extract",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/download/download_manager.py#L417",returnDescription:`
<p><code>str</code>, extracted paths of given URL(s).</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Ye=new S({props:{anchor:"datasets.DownloadManager.download_and_extract.example",$$slots:{default:[Ym]},$$scope:{ctx:k}}}),Gt=new j({props:{name:"download_custom",anchor:"datasets.DownloadManager.download_custom",parameters:[{name:"url_or_urls",val:""},{name:"custom_download",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/download/download_manager.py#L233",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Qe=new S({props:{anchor:"datasets.DownloadManager.download_custom.example",$$slots:{default:[Qm]},$$scope:{ctx:k}}}),Wt=new j({props:{name:"extract",anchor:"datasets.DownloadManager.extract",parameters:[{name:"path_or_paths",val:""},{name:"num_proc",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/download/download_manager.py#L380",returnDescription:`
<p><code>str</code>, The extracted paths matching the given input
path_or_paths.</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Ze=new S({props:{anchor:"datasets.DownloadManager.extract.example",$$slots:{default:[Zm]},$$scope:{ctx:k}}}),Xt=new j({props:{name:"iter_archive",anchor:"datasets.DownloadManager.iter_archive",parameters:[{name:"path_or_buf",val:": typing.Union[str, _io.BufferedReader]"}],parametersDescription:[{anchor:"datasets.DownloadManager.iter_archive.path_or_buf",description:"<strong>path_or_buf</strong> (<code>str</code> or <code>io.BufferedReader</code>) &#x2014; Archive path or archive binary file object.",name:"path_or_buf"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/download/download_manager.py#L339",returnType:"\n<p><code>tuple</code>[<code>str`, `io.BufferedReader</code>]</p>\n",isYield:!0}}),et=new S({props:{anchor:"datasets.DownloadManager.iter_archive.example",$$slots:{default:[eg]},$$scope:{ctx:k}}}),Kt=new j({props:{name:"iter_files",anchor:"datasets.DownloadManager.iter_files",parameters:[{name:"paths",val:": typing.Union[str, typing.List[str]]"}],parametersDescription:[{anchor:"datasets.DownloadManager.iter_files.paths",description:"<strong>paths</strong> (<code>str</code> or <code>list</code> of <code>str</code>) &#x2014; Root paths.",name:"paths"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/download/download_manager.py#L362",returnType:`
<p>str</p>
`,isYield:!0}}),tt=new S({props:{anchor:"datasets.DownloadManager.iter_files.example",$$slots:{default:[tg]},$$scope:{ctx:k}}}),Jt=new j({props:{name:"ship_files_with_pipeline",anchor:"datasets.DownloadManager.ship_files_with_pipeline",parameters:[{name:"downloaded_path_or_paths",val:""},{name:"pipeline",val:""}],parametersDescription:[{anchor:"datasets.DownloadManager.ship_files_with_pipeline.downloaded_path_or_paths",description:`<strong>downloaded_path_or_paths</strong> (<code>str</code> or <code>list[str]</code> or <code>dict[str, str]</code>) &#x2014; Nested structure containing the
downloaded path(s).`,name:"downloaded_path_or_paths"},{anchor:"datasets.DownloadManager.ship_files_with_pipeline.pipeline",description:"<strong>pipeline</strong> (<code>utils.beam_utils.BeamPipeline</code>) &#x2014; Apache Beam Pipeline.",name:"pipeline"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/download/download_manager.py#L190",returnDescription:`
<p><code>str</code> or <code>list[str]</code> or <code>dict[str, str]</code></p>
`}}),Yt=new j({props:{name:"class datasets.StreamingDownloadManager",anchor:"datasets.StreamingDownloadManager",parameters:[{name:"dataset_name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.download.download_config.DownloadConfig] = None"},{name:"base_path",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/download/streaming_download_manager.py#L853"}}),Qt=new j({props:{name:"download",anchor:"datasets.StreamingDownloadManager.download",parameters:[{name:"url_or_urls",val:""}],parametersDescription:[{anchor:"datasets.StreamingDownloadManager.download.url_or_urls",description:"<strong>url_or_urls</strong> (<code>str</code> or <code>list</code> or <code>dict</code>) &#x2014; URL or URLs to download and extract. Each url is a <code>str</code>.",name:"url_or_urls"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/download/streaming_download_manager.py#L879",returnDescription:`
<p>Downloaded paths matching the given input url_or_urls.</p>
`,returnType:`
<p><code>str</code></p>
`}}),st=new S({props:{anchor:"datasets.StreamingDownloadManager.download.example",$$slots:{default:[ag]},$$scope:{ctx:k}}}),Zt=new j({props:{name:"download_and_extract",anchor:"datasets.StreamingDownloadManager.download_and_extract",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/download/streaming_download_manager.py#L941",returnDescription:`
<p><code>str</code>, extracted paths of given URL(s).</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),rt=new S({props:{anchor:"datasets.StreamingDownloadManager.download_and_extract.example",$$slots:{default:[sg]},$$scope:{ctx:k}}}),ea=new j({props:{name:"extract",anchor:"datasets.StreamingDownloadManager.extract",parameters:[{name:"path_or_paths",val:""}],parametersDescription:[{anchor:"datasets.StreamingDownloadManager.extract.path_or_paths",description:"<strong>path_or_paths</strong> (<code>str</code> or <code>list</code> or <code>dict</code>) &#x2014; Path or paths of file to extract. Each path is a <code>str</code>.",name:"path_or_paths"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/download/streaming_download_manager.py#L904",returnDescription:`
<p>Extracted paths matching the given input path_or_paths.</p>
`,returnType:`
<p><code>str</code></p>
`}}),nt=new S({props:{anchor:"datasets.StreamingDownloadManager.extract.example",$$slots:{default:[rg]},$$scope:{ctx:k}}}),ta=new j({props:{name:"iter_archive",anchor:"datasets.StreamingDownloadManager.iter_archive",parameters:[{name:"urlpath_or_buf",val:": typing.Union[str, _io.BufferedReader]"}],parametersDescription:[{anchor:"datasets.StreamingDownloadManager.iter_archive.urlpath_or_buf",description:"<strong>urlpath_or_buf</strong> (<code>str</code> or <code>io.BufferedReader</code>) &#x2014; Archive path or archive binary file object.",name:"urlpath_or_buf"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/download/streaming_download_manager.py#L959",returnType:"\n<p><code>tuple</code>[<code>str`, `io.BufferedReader</code>]</p>\n",isYield:!0}}),ot=new S({props:{anchor:"datasets.StreamingDownloadManager.iter_archive.example",$$slots:{default:[ng]},$$scope:{ctx:k}}}),aa=new j({props:{name:"iter_files",anchor:"datasets.StreamingDownloadManager.iter_files",parameters:[{name:"urlpaths",val:": typing.Union[str, typing.List[str]]"}],parametersDescription:[{anchor:"datasets.StreamingDownloadManager.iter_files.urlpaths",description:"<strong>urlpaths</strong> (<code>str</code> or <code>list</code> of <code>str</code>) &#x2014; Root paths.",name:"urlpaths"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/download/streaming_download_manager.py#L982",returnType:`
<p>str</p>
`,isYield:!0}}),lt=new S({props:{anchor:"datasets.StreamingDownloadManager.iter_files.example",$$slots:{default:[og]},$$scope:{ctx:k}}}),sa=new j({props:{name:"class datasets.DownloadConfig",anchor:"datasets.DownloadConfig",parameters:[{name:"cache_dir",val:": typing.Union[str, pathlib.Path, NoneType] = None"},{name:"force_download",val:": bool = False"},{name:"resume_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"proxies",val:": typing.Optional[typing.Dict] = None"},{name:"user_agent",val:": typing.Optional[str] = None"},{name:"extract_compressed_file",val:": bool = False"},{name:"force_extract",val:": bool = False"},{name:"delete_extracted",val:": bool = False"},{name:"use_etag",val:": bool = True"},{name:"num_proc",val:": typing.Optional[int] = None"},{name:"max_retries",val:": int = 1"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"ignore_url_params",val:": bool = False"},{name:"download_desc",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"datasets.DownloadConfig.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>Path</code>, optional) &#x2014; Specify a cache directory to save the file to (overwrite the
default cache dir).`,name:"cache_dir"},{anchor:"datasets.DownloadConfig.force_download",description:`<strong>force_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, re-dowload the file even if it&#x2019;s already cached in
the cache dir.`,name:"force_download"},{anchor:"datasets.DownloadConfig.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, resume the download if incompletly recieved file is
found.`,name:"resume_download"},{anchor:"datasets.DownloadConfig.proxies",description:"<strong>proxies</strong> (<code>dict</code>, optional) &#x2014;",name:"proxies"},{anchor:"datasets.DownloadConfig.user_agent",description:`<strong>user_agent</strong> (<code>str</code>, optional) &#x2014; Optional string or dict that will be appended to the user-agent on remote
requests.`,name:"user_agent"},{anchor:"datasets.DownloadConfig.extract_compressed_file",description:`<strong>extract_compressed_file</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True and the path point to a zip or tar file,
extract the compressed file in a folder along the archive.`,name:"extract_compressed_file"},{anchor:"datasets.DownloadConfig.force_extract",description:`<strong>force_extract</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True when extract_compressed_file is True and the archive
was already extracted, re-extract the archive and override the folder where it was extracted.`,name:"force_extract"},{anchor:"datasets.DownloadConfig.delete_extracted",description:"<strong>delete_extracted</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to delete (or keep) the extracted files.",name:"delete_extracted"},{anchor:"datasets.DownloadConfig.use_etag",description:"<strong>use_etag</strong> (<code>bool</code>, default <code>True</code>) &#x2014; Whether to use the ETag HTTP response header to validate the cached files.",name:"use_etag"},{anchor:"datasets.DownloadConfig.num_proc",description:"<strong>num_proc</strong> (<code>int</code>, optional) &#x2014; The number of processes to launch to download the files in parallel.",name:"num_proc"},{anchor:"datasets.DownloadConfig.max_retries",description:"<strong>max_retries</strong> (<code>int</code>, default <code>1</code>) &#x2014; The number of times to retry an HTTP request if it fails.",name:"max_retries"},{anchor:"datasets.DownloadConfig.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, optional) &#x2014; Optional string or boolean to use as Bearer token
for remote files on the Datasets Hub. If True, will get token from ~/.huggingface.`,name:"use_auth_token"},{anchor:"datasets.DownloadConfig.ignore_url_params",description:`<strong>ignore_url_params</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to strip all query parameters and #fragments from
the download URL before using it for caching the file.`,name:"ignore_url_params"},{anchor:"datasets.DownloadConfig.download_desc",description:"<strong>download_desc</strong> (<code>str</code>, optional) &#x2014; A description to be displayed alongside with the progress bar while downloading the files.",name:"download_desc"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/download/download_config.py#L8"}}),ra=new j({props:{name:"class datasets.DownloadMode",anchor:"datasets.DownloadMode",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/download/download_manager.py#L39"}}),la=new mn({}),da=new j({props:{name:"class datasets.SplitGenerator",anchor:"datasets.SplitGenerator",parameters:[{name:"name",val:": str"},{name:"gen_kwargs",val:": typing.Dict = <factory>"}],parametersDescription:[{anchor:"datasets.SplitGenerator.name",description:`<strong>name</strong> (str) &#x2014; Name of the Split for which the generator will
create the examples.
**gen_kwargs &#x2014; Keyword arguments to forward to the <code>DatasetBuilder._generate_examples</code> method
of the builder.`,name:"name"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/splits.py#L594"}}),it=new S({props:{anchor:"datasets.SplitGenerator.example",$$slots:{default:[lg]},$$scope:{ctx:k}}}),ia=new j({props:{name:"class datasets.Split",anchor:"datasets.Split",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/splits.py#L399"}}),pt=new S({props:{anchor:"datasets.Split.example",$$slots:{default:[dg]},$$scope:{ctx:k}}}),ca=new j({props:{name:"class datasets.NamedSplit",anchor:"datasets.NamedSplit",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/splits.py#L312"}}),ct=new S({props:{anchor:"datasets.NamedSplit.example",$$slots:{default:[ig]},$$scope:{ctx:k}}}),mt=new S({props:{anchor:"datasets.NamedSplit.example-2",$$slots:{default:[pg]},$$scope:{ctx:k}}}),gt=new S({props:{anchor:"datasets.NamedSplit.example-3",$$slots:{default:[cg]},$$scope:{ctx:k}}}),ft=new S({props:{anchor:"datasets.NamedSplit.example-4",$$slots:{default:[mg]},$$scope:{ctx:k}}}),ma=new j({props:{name:"class datasets.NamedSplitAll",anchor:"datasets.NamedSplitAll",parameters:[],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/splits.py#L384"}}),ga=new j({props:{name:"class datasets.ReadInstruction",anchor:"datasets.ReadInstruction",parameters:[{name:"split_name",val:""},{name:"rounding",val:" = None"},{name:"from_",val:" = None"},{name:"to",val:" = None"},{name:"unit",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/arrow_reader.py#L457"}}),ut=new S({props:{anchor:"datasets.ReadInstruction.example",$$slots:{default:[gg]},$$scope:{ctx:k}}}),fa=new j({props:{name:"from_spec",anchor:"datasets.ReadInstruction.from_spec",parameters:[{name:"spec",val:""}],parametersDescription:[{anchor:"datasets.ReadInstruction.from_spec.spec",description:`<strong>spec</strong> (str) &#x2014; split(s) + optional slice(s) to read + optional rounding
if percents are used as the slicing unit. A slice can be specified,
using absolute numbers (int) or percentages (int). E.g.
<code>test</code>: test split.
<code>test + validation</code>: test split + validation split.
<code>test[10:]</code>: test split, minus its first 10 records.
<code>test[:10%]</code>: first 10% records of test split.
<code>test[:20%](pct1_dropremainder)</code>: first 10% records, rounded with
the <code>pct1_dropremainder</code> rounding.
<code>test[:-5%]+train[40%:60%]</code>: first 95% of test + middle 20% of
train.`,name:"spec"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/arrow_reader.py#L537",returnDescription:`
<p>ReadInstruction instance.</p>
`}}),ua=new j({props:{name:"to_absolute",anchor:"datasets.ReadInstruction.to_absolute",parameters:[{name:"name2len",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/arrow_reader.py#L605",returnDescription:`
<p>list of _AbsoluteInstruction instances (corresponds to the + in spec).</p>
`}}),ha=new mn({}),_a=new j({props:{name:"class datasets.Version",anchor:"datasets.Version",parameters:[{name:"version_str",val:": str"},{name:"description",val:": typing.Optional[str] = None"},{name:"major",val:": typing.Union[str, int, NoneType] = None"},{name:"minor",val:": typing.Union[str, int, NoneType] = None"},{name:"patch",val:": typing.Union[str, int, NoneType] = None"}],parametersDescription:[{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.major",description:"<strong>major</strong> (<code>str</code>) &#x2014;",name:"major"},{anchor:"datasets.Version.minor",description:"<strong>minor</strong> (<code>str</code>) &#x2014;",name:"minor"},{anchor:"datasets.Version.patch",description:"<strong>patch</strong> (<code>str</code>) &#x2014;",name:"patch"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/utils/version.py#L30"}}),$t=new S({props:{anchor:"datasets.Version.example",$$slots:{default:[fg]},$$scope:{ctx:k}}}),$a=new j({props:{name:"match",anchor:"datasets.Version.match",parameters:[{name:"other_version",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/utils/version.py#L101"}}),{c(){d=r("meta"),h=i(),f=r("h1"),l=r("a"),u=r("span"),$(t.$$.fragment),g=i(),is=r("span"),Qn=c("Builder classes"),gn=i(),ve=r("h2"),Ve=r("a"),ps=r("span"),$(Dt.$$.fragment),Zn=i(),cs=r("span"),eo=c("Builders"),fn=i(),ee=r("p"),to=c("\u{1F917} Datasets relies on two main classes during the dataset building process: "),ya=r("a"),ao=c("DatasetBuilder"),so=c(" and "),Da=r("a"),ro=c("BuilderConfig"),no=c("."),un=i(),T=r("div"),$(kt.$$.fragment),oo=i(),ms=r("p"),lo=c("Abstract base class for all datasets."),io=i(),ka=r("p"),gs=r("em"),po=c("DatasetBuilder"),co=c(" has 3 key methods:"),mo=i(),we=r("ul"),jt=r("li"),go=c("["),fs=r("em"),fo=c("DatasetBuilder.info"),uo=c(`]: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),ho=i(),Tt=r("li"),_o=c("["),us=r("em"),$o=c("DatasetBuilder.download_and_prepare"),bo=c(`]: Downloads the source data
and writes it to disk.`),vo=i(),xe=r("li"),wo=c("["),hs=r("em"),xo=c("DatasetBuilder.as_dataset"),Eo=c("]: Generates a ["),_s=r("em"),yo=c("Dataset"),Do=c("]."),ko=i(),K=r("p"),$s=r("strong"),jo=c("Configuration"),To=c(": Some "),bs=r("em"),So=c("DatasetBuilder"),Io=c(`s expose multiple variants of the
dataset by defining a [`),vs=r("em"),Bo=c("BuilderConfig"),No=c(`] subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in [`),ws=r("em"),Po=c("DatasetBuilder.builder_configs"),Ro=c("]."),Co=i(),te=r("div"),$(St.$$.fragment),qo=i(),xs=r("p"),Ao=c("Return a Dataset for the specified split."),Lo=i(),$(Fe.$$.fragment),Oo=i(),P=r("div"),$(It.$$.fragment),Mo=i(),Es=r("p"),Vo=c("Downloads and prepares dataset for reading."),Fo=i(),ys=r("p"),Uo=c("Example:"),zo=i(),ja=r("p"),Go=c("Downdload and prepare the dataset as Arrow files that can be loaded as a Dataset using "),Ds=r("em"),Ho=c("builder.as_dataset()"),Wo=i(),$(Ue.$$.fragment),Xo=i(),ks=r("p"),Ko=c("Downdload and prepare the dataset as sharded Parquet files locally"),Jo=i(),$(ze.$$.fragment),Yo=i(),js=r("p"),Qo=c("Downdload and prepare the dataset as sharded Parquet files in a cloud storage"),Zo=i(),$(Ge.$$.fragment),el=i(),ae=r("div"),$(Bt.$$.fragment),tl=i(),Ts=r("p"),al=c("Empty dict if doesn\u2019t exist"),sl=i(),$(He.$$.fragment),rl=i(),se=r("div"),$(Nt.$$.fragment),nl=i(),Ss=r("p"),ol=c("Empty DatasetInfo if doesn\u2019t exist"),ll=i(),$(We.$$.fragment),dl=i(),Xe=r("div"),$(Pt.$$.fragment),il=i(),Is=r("p"),pl=c("Return the path of the module of this class or subclass."),hn=i(),Q=r("div"),$(Rt.$$.fragment),cl=i(),Bs=r("p"),ml=c("Base class for datasets with data generation based on dict generators."),gl=i(),re=r("p"),Ns=r("code"),fl=c("GeneratorBasedBuilder"),ul=c(` is a convenience class that abstracts away much
of the data writing and reading of `),Ps=r("code"),hl=c("DatasetBuilder"),_l=c(`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),Rs=r("code"),$l=c("_split_generators"),bl=c("). See the method docstrings for details."),_n=i(),Ee=r("div"),$(Ct.$$.fragment),vl=i(),Cs=r("p"),wl=c("Beam based Builder."),$n=i(),ye=r("div"),$(qt.$$.fragment),xl=i(),qs=r("p"),El=c("Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),bn=i(),G=r("div"),$(At.$$.fragment),yl=i(),Lt=r("p"),Dl=c("Base class for "),Ta=r("a"),kl=c("DatasetBuilder"),jl=c(" data configuration."),Tl=i(),Ot=r("p"),Sl=c(`DatasetBuilder subclasses with data configuration options should subclass
`),Sa=r("a"),Il=c("BuilderConfig"),Bl=c(" and add their own properties."),Nl=i(),ne=r("div"),$(Mt.$$.fragment),Pl=i(),As=r("p"),Rl=c(`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),Cl=i(),De=r("ul"),Ls=r("li"),ql=c("the config kwargs that can be used to overwrite attributes"),Al=i(),Os=r("li"),Ll=c("the custom features used to write the dataset"),Ol=i(),Ms=r("li"),Ml=c(`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),vn=i(),ke=r("h2"),Ke=r("a"),Vs=r("span"),$(Vt.$$.fragment),Vl=i(),Fs=r("span"),Fl=c("Download"),wn=i(),R=r("div"),$(Ft.$$.fragment),Ul=i(),J=r("div"),$(Ut.$$.fragment),zl=i(),Us=r("p"),Gl=c("Download given URL(s)."),Hl=i(),je=r("p"),Wl=c("By default, if there is more than one URL to download, multiprocessing is used with maximum "),zs=r("code"),Xl=c("num_proc = 16"),Kl=c(`.
Pass customized `),Gs=r("code"),Jl=c("download_config.num_proc"),Yl=c(" to change this behavior."),Ql=i(),$(Je.$$.fragment),Zl=i(),oe=r("div"),$(zt.$$.fragment),ed=i(),Hs=r("p"),td=c("Download and extract given url_or_urls."),ad=i(),$(Ye.$$.fragment),sd=i(),le=r("div"),$(Gt.$$.fragment),rd=i(),Ht=r("p"),nd=c("Download given urls(s) by calling "),Ws=r("code"),od=c("custom_download"),ld=c("."),dd=i(),$(Qe.$$.fragment),id=i(),de=r("div"),$(Wt.$$.fragment),pd=i(),Xs=r("p"),cd=c("Extract given path(s)."),md=i(),$(Ze.$$.fragment),gd=i(),ie=r("div"),$(Xt.$$.fragment),fd=i(),Ks=r("p"),ud=c("Iterate over files within an archive."),hd=i(),$(et.$$.fragment),_d=i(),pe=r("div"),$(Kt.$$.fragment),$d=i(),Js=r("p"),bd=c("Iterate over file paths."),vd=i(),$(tt.$$.fragment),wd=i(),at=r("div"),$(Jt.$$.fragment),xd=i(),Ys=r("p"),Ed=c("Ship the files using Beam FileSystems to the pipeline temp dir."),xn=i(),q=r("div"),$(Yt.$$.fragment),yd=i(),H=r("p"),Dd=c(`Download manager that uses the \u201D::\u201D separator to navigate through (possibly remote) compressed archives.
Contrary to the regular DownloadManager, the `),Qs=r("code"),kd=c("download"),jd=c(" and "),Zs=r("code"),Td=c("extract"),Sd=c(` methods don\u2019t actually download nor extract
data, but they rather return the path or url that could be opened using the `),er=r("code"),Id=c("xopen"),Bd=c(` function which extends the
builtin `),tr=r("code"),Nd=c("open"),Pd=c(" function to stream data from remote files."),Rd=i(),ce=r("div"),$(Qt.$$.fragment),Cd=i(),ar=r("p"),qd=c("Download given url(s)."),Ad=i(),$(st.$$.fragment),Ld=i(),me=r("div"),$(Zt.$$.fragment),Od=i(),sr=r("p"),Md=c("Download and extract given url_or_urls."),Vd=i(),$(rt.$$.fragment),Fd=i(),ge=r("div"),$(ea.$$.fragment),Ud=i(),rr=r("p"),zd=c("Extract given path(s)."),Gd=i(),$(nt.$$.fragment),Hd=i(),fe=r("div"),$(ta.$$.fragment),Wd=i(),nr=r("p"),Xd=c("Iterate over files within an archive."),Kd=i(),$(ot.$$.fragment),Jd=i(),ue=r("div"),$(aa.$$.fragment),Yd=i(),or=r("p"),Qd=c("Iterate over files."),Zd=i(),$(lt.$$.fragment),En=i(),Te=r("div"),$(sa.$$.fragment),ei=i(),lr=r("p"),ti=c("Configuration for our cached path manager."),yn=i(),V=r("div"),$(ra.$$.fragment),ai=i(),Ia=r("p"),dr=r("code"),si=c("Enum"),ri=c(" for how to treat pre-existing downloads and data."),ni=i(),na=r("p"),oi=c("The default mode is "),ir=r("code"),li=c("REUSE_DATASET_IF_EXISTS"),di=c(`, which will reuse both
raw downloads and the prepared dataset if they exist.`),ii=i(),pr=r("p"),pi=c("The generations modes:"),ci=i(),oa=r("table"),cr=r("thead"),Se=r("tr"),Dn=r("th"),mi=i(),mr=r("th"),gi=c("Downloads"),fi=i(),gr=r("th"),ui=c("Dataset"),hi=i(),Ie=r("tbody"),Be=r("tr"),Ba=r("td"),fr=r("code"),_i=c("REUSE_DATASET_IF_EXISTS"),$i=c(" (default)"),bi=i(),ur=r("td"),vi=c("Reuse"),wi=i(),hr=r("td"),xi=c("Reuse"),Ei=i(),Ne=r("tr"),_r=r("td"),$r=r("code"),yi=c("REUSE_CACHE_IF_EXISTS"),Di=i(),br=r("td"),ki=c("Reuse"),ji=i(),vr=r("td"),Ti=c("Fresh"),Si=i(),Pe=r("tr"),wr=r("td"),xr=r("code"),Ii=c("FORCE_REDOWNLOAD"),Bi=i(),Er=r("td"),Ni=c("Fresh"),Pi=i(),yr=r("td"),Ri=c("Fresh"),kn=i(),Re=r("h2"),dt=r("a"),Dr=r("span"),$(la.$$.fragment),Ci=i(),kr=r("span"),qi=c("Splits"),jn=i(),W=r("div"),$(da.$$.fragment),Ai=i(),jr=r("p"),Li=c("Defines the split information for the generator."),Oi=i(),Ce=r("p"),Mi=c(`This should be used as returned value of
`),Tr=r("code"),Vi=c("GeneratorBasedBuilder._split_generators()"),Fi=c(`.
See `),Sr=r("code"),Ui=c("GeneratorBasedBuilder._split_generators()"),zi=c(` for more info and example
of usage.`),Gi=i(),$(it.$$.fragment),Tn=i(),A=r("div"),$(ia.$$.fragment),Hi=i(),Na=r("p"),Ir=r("code"),Wi=c("Enum"),Xi=c(" for dataset splits."),Ki=i(),Br=r("p"),Ji=c(`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),Yi=i(),Z=r("ul"),Pa=r("li"),Nr=r("code"),Qi=c("TRAIN"),Zi=c(": the training data."),ep=i(),Ra=r("li"),Pr=r("code"),tp=c("VALIDATION"),ap=c(`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),sp=i(),Ca=r("li"),Rr=r("code"),rp=c("TEST"),np=c(`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),op=i(),qa=r("li"),Cr=r("code"),lp=c("ALL"),dp=c(": the union of all defined dataset splits."),ip=i(),Aa=r("p"),pp=c("Note: All splits, including compositions inherit from "),qr=r("code"),cp=c("datasets.SplitBase"),mp=i(),pa=r("p"),gp=c("See the :doc:"),Ar=r("code"),fp=c("guide on splits </loading>"),up=c(" for more information."),hp=i(),$(pt.$$.fragment),Sn=i(),C=r("div"),$(ca.$$.fragment),_p=i(),Lr=r("p"),$p=c("Descriptor corresponding to a named split (train, test, \u2026)."),bp=i(),$(ct.$$.fragment),vp=i(),Or=r("p"),wp=c("Warning:"),xp=i(),$(mt.$$.fragment),Ep=i(),Mr=r("p"),yp=c("Warning:"),Dp=i(),$(gt.$$.fragment),kp=i(),$(ft.$$.fragment),In=i(),qe=r("div"),$(ma.$$.fragment),jp=i(),Vr=r("p"),Tp=c("Split corresponding to the union of all defined dataset splits."),Bn=i(),F=r("div"),$(ga.$$.fragment),Sp=i(),Fr=r("p"),Ip=c("Reading instruction for a dataset."),Bp=i(),$(ut.$$.fragment),Np=i(),ht=r("div"),$(fa.$$.fragment),Pp=i(),Ur=r("p"),Rp=c("Creates a ReadInstruction instance out of a string spec."),Cp=i(),he=r("div"),$(ua.$$.fragment),qp=i(),zr=r("p"),Ap=c("Translate instruction into a list of absolute instructions."),Lp=i(),Gr=r("p"),Op=c("Those absolute instructions are then to be added together."),Nn=i(),Ae=r("h2"),_t=r("a"),Hr=r("span"),$(ha.$$.fragment),Mp=i(),Wr=r("span"),Vp=c("Version"),Pn=i(),X=r("div"),$(_a.$$.fragment),Fp=i(),Xr=r("p"),Up=c("Dataset version MAJOR.MINOR.PATCH."),zp=i(),$($t.$$.fragment),Gp=i(),bt=r("div"),$($a.$$.fragment),Hp=i(),Kr=r("p"),Wp=c("Returns True if other_version matches."),this.h()},l(s){const _=Fm('[data-svelte="svelte-1phssyn"]',document.head);d=n(_,"META",{name:!0,content:!0}),_.forEach(a),h=p(s),f=n(s,"H1",{class:!0});var ba=o(f);l=n(ba,"A",{id:!0,class:!0,href:!0});var Jr=o(l);u=n(Jr,"SPAN",{});var Yr=o(u);b(t.$$.fragment,Yr),Yr.forEach(a),Jr.forEach(a),g=p(ba),is=n(ba,"SPAN",{});var Qr=o(is);Qn=m(Qr,"Builder classes"),Qr.forEach(a),ba.forEach(a),gn=p(s),ve=n(s,"H2",{class:!0});var va=o(ve);Ve=n(va,"A",{id:!0,class:!0,href:!0});var Zr=o(Ve);ps=n(Zr,"SPAN",{});var en=o(ps);b(Dt.$$.fragment,en),en.forEach(a),Zr.forEach(a),Zn=p(va),cs=n(va,"SPAN",{});var tn=o(cs);eo=m(tn,"Builders"),tn.forEach(a),va.forEach(a),fn=p(s),ee=n(s,"P",{});var Le=o(ee);to=m(Le,"\u{1F917} Datasets relies on two main classes during the dataset building process: "),ya=n(Le,"A",{href:!0});var an=o(ya);ao=m(an,"DatasetBuilder"),an.forEach(a),so=m(Le," and "),Da=n(Le,"A",{href:!0});var sn=o(Da);ro=m(sn,"BuilderConfig"),sn.forEach(a),no=m(Le,"."),Le.forEach(a),un=p(s),T=n(s,"DIV",{class:!0});var N=o(T);b(kt.$$.fragment,N),oo=p(N),ms=n(N,"P",{});var rn=o(ms);lo=m(rn,"Abstract base class for all datasets."),rn.forEach(a),io=p(N),ka=n(N,"P",{});var La=o(ka);gs=n(La,"EM",{});var nn=o(gs);po=m(nn,"DatasetBuilder"),nn.forEach(a),co=m(La," has 3 key methods:"),La.forEach(a),mo=p(N),we=n(N,"UL",{});var Oe=o(we);jt=n(Oe,"LI",{});var wa=o(jt);go=m(wa,"["),fs=n(wa,"EM",{});var on=o(fs);fo=m(on,"DatasetBuilder.info"),on.forEach(a),uo=m(wa,`]: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),wa.forEach(a),ho=p(Oe),Tt=n(Oe,"LI",{});var xa=o(Tt);_o=m(xa,"["),us=n(xa,"EM",{});var ln=o(us);$o=m(ln,"DatasetBuilder.download_and_prepare"),ln.forEach(a),bo=m(xa,`]: Downloads the source data
and writes it to disk.`),xa.forEach(a),vo=p(Oe),xe=n(Oe,"LI",{});var Me=o(xe);wo=m(Me,"["),hs=n(Me,"EM",{});var dn=o(hs);xo=m(dn,"DatasetBuilder.as_dataset"),dn.forEach(a),Eo=m(Me,"]: Generates a ["),_s=n(Me,"EM",{});var pn=o(_s);yo=m(pn,"Dataset"),pn.forEach(a),Do=m(Me,"]."),Me.forEach(a),Oe.forEach(a),ko=p(N),K=n(N,"P",{});var Y=o(K);$s=n(Y,"STRONG",{});var cn=o($s);jo=m(cn,"Configuration"),cn.forEach(a),To=m(Y,": Some "),bs=n(Y,"EM",{});var sc=o(bs);So=m(sc,"DatasetBuilder"),sc.forEach(a),Io=m(Y,`s expose multiple variants of the
dataset by defining a [`),vs=n(Y,"EM",{});var rc=o(vs);Bo=m(rc,"BuilderConfig"),rc.forEach(a),No=m(Y,`] subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in [`),ws=n(Y,"EM",{});var nc=o(ws);Po=m(nc,"DatasetBuilder.builder_configs"),nc.forEach(a),Ro=m(Y,"]."),Y.forEach(a),Co=p(N),te=n(N,"DIV",{class:!0});var Oa=o(te);b(St.$$.fragment,Oa),qo=p(Oa),xs=n(Oa,"P",{});var oc=o(xs);Ao=m(oc,"Return a Dataset for the specified split."),oc.forEach(a),Lo=p(Oa),b(Fe.$$.fragment,Oa),Oa.forEach(a),Oo=p(N),P=n(N,"DIV",{class:!0});var L=o(P);b(It.$$.fragment,L),Mo=p(L),Es=n(L,"P",{});var lc=o(Es);Vo=m(lc,"Downloads and prepares dataset for reading."),lc.forEach(a),Fo=p(L),ys=n(L,"P",{});var dc=o(ys);Uo=m(dc,"Example:"),dc.forEach(a),zo=p(L),ja=n(L,"P",{});var Xp=o(ja);Go=m(Xp,"Downdload and prepare the dataset as Arrow files that can be loaded as a Dataset using "),Ds=n(Xp,"EM",{});var ic=o(Ds);Ho=m(ic,"builder.as_dataset()"),ic.forEach(a),Xp.forEach(a),Wo=p(L),b(Ue.$$.fragment,L),Xo=p(L),ks=n(L,"P",{});var pc=o(ks);Ko=m(pc,"Downdload and prepare the dataset as sharded Parquet files locally"),pc.forEach(a),Jo=p(L),b(ze.$$.fragment,L),Yo=p(L),js=n(L,"P",{});var cc=o(js);Qo=m(cc,"Downdload and prepare the dataset as sharded Parquet files in a cloud storage"),cc.forEach(a),Zo=p(L),b(Ge.$$.fragment,L),L.forEach(a),el=p(N),ae=n(N,"DIV",{class:!0});var Ma=o(ae);b(Bt.$$.fragment,Ma),tl=p(Ma),Ts=n(Ma,"P",{});var mc=o(Ts);al=m(mc,"Empty dict if doesn\u2019t exist"),mc.forEach(a),sl=p(Ma),b(He.$$.fragment,Ma),Ma.forEach(a),rl=p(N),se=n(N,"DIV",{class:!0});var Va=o(se);b(Nt.$$.fragment,Va),nl=p(Va),Ss=n(Va,"P",{});var gc=o(Ss);ol=m(gc,"Empty DatasetInfo if doesn\u2019t exist"),gc.forEach(a),ll=p(Va),b(We.$$.fragment,Va),Va.forEach(a),dl=p(N),Xe=n(N,"DIV",{class:!0});var Cn=o(Xe);b(Pt.$$.fragment,Cn),il=p(Cn),Is=n(Cn,"P",{});var fc=o(Is);pl=m(fc,"Return the path of the module of this class or subclass."),fc.forEach(a),Cn.forEach(a),N.forEach(a),hn=p(s),Q=n(s,"DIV",{class:!0});var Fa=o(Q);b(Rt.$$.fragment,Fa),cl=p(Fa),Bs=n(Fa,"P",{});var uc=o(Bs);ml=m(uc,"Base class for datasets with data generation based on dict generators."),uc.forEach(a),gl=p(Fa),re=n(Fa,"P",{});var Ea=o(re);Ns=n(Ea,"CODE",{});var hc=o(Ns);fl=m(hc,"GeneratorBasedBuilder"),hc.forEach(a),ul=m(Ea,` is a convenience class that abstracts away much
of the data writing and reading of `),Ps=n(Ea,"CODE",{});var _c=o(Ps);hl=m(_c,"DatasetBuilder"),_c.forEach(a),_l=m(Ea,`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),Rs=n(Ea,"CODE",{});var $c=o(Rs);$l=m($c,"_split_generators"),$c.forEach(a),bl=m(Ea,"). See the method docstrings for details."),Ea.forEach(a),Fa.forEach(a),_n=p(s),Ee=n(s,"DIV",{class:!0});var qn=o(Ee);b(Ct.$$.fragment,qn),vl=p(qn),Cs=n(qn,"P",{});var bc=o(Cs);wl=m(bc,"Beam based Builder."),bc.forEach(a),qn.forEach(a),$n=p(s),ye=n(s,"DIV",{class:!0});var An=o(ye);b(qt.$$.fragment,An),xl=p(An),qs=n(An,"P",{});var vc=o(qs);El=m(vc,"Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),vc.forEach(a),An.forEach(a),bn=p(s),G=n(s,"DIV",{class:!0});var vt=o(G);b(At.$$.fragment,vt),yl=p(vt),Lt=n(vt,"P",{});var Ln=o(Lt);Dl=m(Ln,"Base class for "),Ta=n(Ln,"A",{href:!0});var wc=o(Ta);kl=m(wc,"DatasetBuilder"),wc.forEach(a),jl=m(Ln," data configuration."),Ln.forEach(a),Tl=p(vt),Ot=n(vt,"P",{});var On=o(Ot);Sl=m(On,`DatasetBuilder subclasses with data configuration options should subclass
`),Sa=n(On,"A",{href:!0});var xc=o(Sa);Il=m(xc,"BuilderConfig"),xc.forEach(a),Bl=m(On," and add their own properties."),On.forEach(a),Nl=p(vt),ne=n(vt,"DIV",{class:!0});var Ua=o(ne);b(Mt.$$.fragment,Ua),Pl=p(Ua),As=n(Ua,"P",{});var Ec=o(As);Rl=m(Ec,`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),Ec.forEach(a),Cl=p(Ua),De=n(Ua,"UL",{});var za=o(De);Ls=n(za,"LI",{});var yc=o(Ls);ql=m(yc,"the config kwargs that can be used to overwrite attributes"),yc.forEach(a),Al=p(za),Os=n(za,"LI",{});var Dc=o(Os);Ll=m(Dc,"the custom features used to write the dataset"),Dc.forEach(a),Ol=p(za),Ms=n(za,"LI",{});var kc=o(Ms);Ml=m(kc,`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),kc.forEach(a),za.forEach(a),Ua.forEach(a),vt.forEach(a),vn=p(s),ke=n(s,"H2",{class:!0});var Mn=o(ke);Ke=n(Mn,"A",{id:!0,class:!0,href:!0});var jc=o(Ke);Vs=n(jc,"SPAN",{});var Tc=o(Vs);b(Vt.$$.fragment,Tc),Tc.forEach(a),jc.forEach(a),Vl=p(Mn),Fs=n(Mn,"SPAN",{});var Sc=o(Fs);Fl=m(Sc,"Download"),Sc.forEach(a),Mn.forEach(a),wn=p(s),R=n(s,"DIV",{class:!0});var O=o(R);b(Ft.$$.fragment,O),Ul=p(O),J=n(O,"DIV",{class:!0});var wt=o(J);b(Ut.$$.fragment,wt),zl=p(wt),Us=n(wt,"P",{});var Ic=o(Us);Gl=m(Ic,"Download given URL(s)."),Ic.forEach(a),Hl=p(wt),je=n(wt,"P",{});var Ga=o(je);Wl=m(Ga,"By default, if there is more than one URL to download, multiprocessing is used with maximum "),zs=n(Ga,"CODE",{});var Bc=o(zs);Xl=m(Bc,"num_proc = 16"),Bc.forEach(a),Kl=m(Ga,`.
Pass customized `),Gs=n(Ga,"CODE",{});var Nc=o(Gs);Jl=m(Nc,"download_config.num_proc"),Nc.forEach(a),Yl=m(Ga," to change this behavior."),Ga.forEach(a),Ql=p(wt),b(Je.$$.fragment,wt),wt.forEach(a),Zl=p(O),oe=n(O,"DIV",{class:!0});var Ha=o(oe);b(zt.$$.fragment,Ha),ed=p(Ha),Hs=n(Ha,"P",{});var Pc=o(Hs);td=m(Pc,"Download and extract given url_or_urls."),Pc.forEach(a),ad=p(Ha),b(Ye.$$.fragment,Ha),Ha.forEach(a),sd=p(O),le=n(O,"DIV",{class:!0});var Wa=o(le);b(Gt.$$.fragment,Wa),rd=p(Wa),Ht=n(Wa,"P",{});var Vn=o(Ht);nd=m(Vn,"Download given urls(s) by calling "),Ws=n(Vn,"CODE",{});var Rc=o(Ws);od=m(Rc,"custom_download"),Rc.forEach(a),ld=m(Vn,"."),Vn.forEach(a),dd=p(Wa),b(Qe.$$.fragment,Wa),Wa.forEach(a),id=p(O),de=n(O,"DIV",{class:!0});var Xa=o(de);b(Wt.$$.fragment,Xa),pd=p(Xa),Xs=n(Xa,"P",{});var Cc=o(Xs);cd=m(Cc,"Extract given path(s)."),Cc.forEach(a),md=p(Xa),b(Ze.$$.fragment,Xa),Xa.forEach(a),gd=p(O),ie=n(O,"DIV",{class:!0});var Ka=o(ie);b(Xt.$$.fragment,Ka),fd=p(Ka),Ks=n(Ka,"P",{});var qc=o(Ks);ud=m(qc,"Iterate over files within an archive."),qc.forEach(a),hd=p(Ka),b(et.$$.fragment,Ka),Ka.forEach(a),_d=p(O),pe=n(O,"DIV",{class:!0});var Ja=o(pe);b(Kt.$$.fragment,Ja),$d=p(Ja),Js=n(Ja,"P",{});var Ac=o(Js);bd=m(Ac,"Iterate over file paths."),Ac.forEach(a),vd=p(Ja),b(tt.$$.fragment,Ja),Ja.forEach(a),wd=p(O),at=n(O,"DIV",{class:!0});var Fn=o(at);b(Jt.$$.fragment,Fn),xd=p(Fn),Ys=n(Fn,"P",{});var Lc=o(Ys);Ed=m(Lc,"Ship the files using Beam FileSystems to the pipeline temp dir."),Lc.forEach(a),Fn.forEach(a),O.forEach(a),xn=p(s),q=n(s,"DIV",{class:!0});var U=o(q);b(Yt.$$.fragment,U),yd=p(U),H=n(U,"P",{});var _e=o(H);Dd=m(_e,`Download manager that uses the \u201D::\u201D separator to navigate through (possibly remote) compressed archives.
Contrary to the regular DownloadManager, the `),Qs=n(_e,"CODE",{});var Oc=o(Qs);kd=m(Oc,"download"),Oc.forEach(a),jd=m(_e," and "),Zs=n(_e,"CODE",{});var Mc=o(Zs);Td=m(Mc,"extract"),Mc.forEach(a),Sd=m(_e,` methods don\u2019t actually download nor extract
data, but they rather return the path or url that could be opened using the `),er=n(_e,"CODE",{});var Vc=o(er);Id=m(Vc,"xopen"),Vc.forEach(a),Bd=m(_e,` function which extends the
builtin `),tr=n(_e,"CODE",{});var Fc=o(tr);Nd=m(Fc,"open"),Fc.forEach(a),Pd=m(_e," function to stream data from remote files."),_e.forEach(a),Rd=p(U),ce=n(U,"DIV",{class:!0});var Ya=o(ce);b(Qt.$$.fragment,Ya),Cd=p(Ya),ar=n(Ya,"P",{});var Uc=o(ar);qd=m(Uc,"Download given url(s)."),Uc.forEach(a),Ad=p(Ya),b(st.$$.fragment,Ya),Ya.forEach(a),Ld=p(U),me=n(U,"DIV",{class:!0});var Qa=o(me);b(Zt.$$.fragment,Qa),Od=p(Qa),sr=n(Qa,"P",{});var zc=o(sr);Md=m(zc,"Download and extract given url_or_urls."),zc.forEach(a),Vd=p(Qa),b(rt.$$.fragment,Qa),Qa.forEach(a),Fd=p(U),ge=n(U,"DIV",{class:!0});var Za=o(ge);b(ea.$$.fragment,Za),Ud=p(Za),rr=n(Za,"P",{});var Gc=o(rr);zd=m(Gc,"Extract given path(s)."),Gc.forEach(a),Gd=p(Za),b(nt.$$.fragment,Za),Za.forEach(a),Hd=p(U),fe=n(U,"DIV",{class:!0});var es=o(fe);b(ta.$$.fragment,es),Wd=p(es),nr=n(es,"P",{});var Hc=o(nr);Xd=m(Hc,"Iterate over files within an archive."),Hc.forEach(a),Kd=p(es),b(ot.$$.fragment,es),es.forEach(a),Jd=p(U),ue=n(U,"DIV",{class:!0});var ts=o(ue);b(aa.$$.fragment,ts),Yd=p(ts),or=n(ts,"P",{});var Wc=o(or);Qd=m(Wc,"Iterate over files."),Wc.forEach(a),Zd=p(ts),b(lt.$$.fragment,ts),ts.forEach(a),U.forEach(a),En=p(s),Te=n(s,"DIV",{class:!0});var Un=o(Te);b(sa.$$.fragment,Un),ei=p(Un),lr=n(Un,"P",{});var Xc=o(lr);ti=m(Xc,"Configuration for our cached path manager."),Xc.forEach(a),Un.forEach(a),yn=p(s),V=n(s,"DIV",{class:!0});var $e=o(V);b(ra.$$.fragment,$e),ai=p($e),Ia=n($e,"P",{});var Kp=o(Ia);dr=n(Kp,"CODE",{});var Kc=o(dr);si=m(Kc,"Enum"),Kc.forEach(a),ri=m(Kp," for how to treat pre-existing downloads and data."),Kp.forEach(a),ni=p($e),na=n($e,"P",{});var zn=o(na);oi=m(zn,"The default mode is "),ir=n(zn,"CODE",{});var Jc=o(ir);li=m(Jc,"REUSE_DATASET_IF_EXISTS"),Jc.forEach(a),di=m(zn,`, which will reuse both
raw downloads and the prepared dataset if they exist.`),zn.forEach(a),ii=p($e),pr=n($e,"P",{});var Yc=o(pr);pi=m(Yc,"The generations modes:"),Yc.forEach(a),ci=p($e),oa=n($e,"TABLE",{});var Gn=o(oa);cr=n(Gn,"THEAD",{});var Qc=o(cr);Se=n(Qc,"TR",{});var as=o(Se);Dn=n(as,"TH",{}),o(Dn).forEach(a),mi=p(as),mr=n(as,"TH",{});var Zc=o(mr);gi=m(Zc,"Downloads"),Zc.forEach(a),fi=p(as),gr=n(as,"TH",{});var em=o(gr);ui=m(em,"Dataset"),em.forEach(a),as.forEach(a),Qc.forEach(a),hi=p(Gn),Ie=n(Gn,"TBODY",{});var ss=o(Ie);Be=n(ss,"TR",{});var rs=o(Be);Ba=n(rs,"TD",{});var Jp=o(Ba);fr=n(Jp,"CODE",{});var tm=o(fr);_i=m(tm,"REUSE_DATASET_IF_EXISTS"),tm.forEach(a),$i=m(Jp," (default)"),Jp.forEach(a),bi=p(rs),ur=n(rs,"TD",{});var am=o(ur);vi=m(am,"Reuse"),am.forEach(a),wi=p(rs),hr=n(rs,"TD",{});var sm=o(hr);xi=m(sm,"Reuse"),sm.forEach(a),rs.forEach(a),Ei=p(ss),Ne=n(ss,"TR",{});var ns=o(Ne);_r=n(ns,"TD",{});var rm=o(_r);$r=n(rm,"CODE",{});var nm=o($r);yi=m(nm,"REUSE_CACHE_IF_EXISTS"),nm.forEach(a),rm.forEach(a),Di=p(ns),br=n(ns,"TD",{});var om=o(br);ki=m(om,"Reuse"),om.forEach(a),ji=p(ns),vr=n(ns,"TD",{});var lm=o(vr);Ti=m(lm,"Fresh"),lm.forEach(a),ns.forEach(a),Si=p(ss),Pe=n(ss,"TR",{});var os=o(Pe);wr=n(os,"TD",{});var dm=o(wr);xr=n(dm,"CODE",{});var im=o(xr);Ii=m(im,"FORCE_REDOWNLOAD"),im.forEach(a),dm.forEach(a),Bi=p(os),Er=n(os,"TD",{});var pm=o(Er);Ni=m(pm,"Fresh"),pm.forEach(a),Pi=p(os),yr=n(os,"TD",{});var cm=o(yr);Ri=m(cm,"Fresh"),cm.forEach(a),os.forEach(a),ss.forEach(a),Gn.forEach(a),$e.forEach(a),kn=p(s),Re=n(s,"H2",{class:!0});var Hn=o(Re);dt=n(Hn,"A",{id:!0,class:!0,href:!0});var mm=o(dt);Dr=n(mm,"SPAN",{});var gm=o(Dr);b(la.$$.fragment,gm),gm.forEach(a),mm.forEach(a),Ci=p(Hn),kr=n(Hn,"SPAN",{});var fm=o(kr);qi=m(fm,"Splits"),fm.forEach(a),Hn.forEach(a),jn=p(s),W=n(s,"DIV",{class:!0});var xt=o(W);b(da.$$.fragment,xt),Ai=p(xt),jr=n(xt,"P",{});var um=o(jr);Li=m(um,"Defines the split information for the generator."),um.forEach(a),Oi=p(xt),Ce=n(xt,"P",{});var ls=o(Ce);Mi=m(ls,`This should be used as returned value of
`),Tr=n(ls,"CODE",{});var hm=o(Tr);Vi=m(hm,"GeneratorBasedBuilder._split_generators()"),hm.forEach(a),Fi=m(ls,`.
See `),Sr=n(ls,"CODE",{});var _m=o(Sr);Ui=m(_m,"GeneratorBasedBuilder._split_generators()"),_m.forEach(a),zi=m(ls,` for more info and example
of usage.`),ls.forEach(a),Gi=p(xt),b(it.$$.fragment,xt),xt.forEach(a),Tn=p(s),A=n(s,"DIV",{class:!0});var z=o(A);b(ia.$$.fragment,z),Hi=p(z),Na=n(z,"P",{});var Yp=o(Na);Ir=n(Yp,"CODE",{});var $m=o(Ir);Wi=m($m,"Enum"),$m.forEach(a),Xi=m(Yp," for dataset splits."),Yp.forEach(a),Ki=p(z),Br=n(z,"P",{});var bm=o(Br);Ji=m(bm,`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),bm.forEach(a),Yi=p(z),Z=n(z,"UL",{});var Et=o(Z);Pa=n(Et,"LI",{});var Qp=o(Pa);Nr=n(Qp,"CODE",{});var vm=o(Nr);Qi=m(vm,"TRAIN"),vm.forEach(a),Zi=m(Qp,": the training data."),Qp.forEach(a),ep=p(Et),Ra=n(Et,"LI",{});var Zp=o(Ra);Pr=n(Zp,"CODE",{});var wm=o(Pr);tp=m(wm,"VALIDATION"),wm.forEach(a),ap=m(Zp,`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Zp.forEach(a),sp=p(Et),Ca=n(Et,"LI",{});var ec=o(Ca);Rr=n(ec,"CODE",{});var xm=o(Rr);rp=m(xm,"TEST"),xm.forEach(a),np=m(ec,`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),ec.forEach(a),op=p(Et),qa=n(Et,"LI",{});var tc=o(qa);Cr=n(tc,"CODE",{});var Em=o(Cr);lp=m(Em,"ALL"),Em.forEach(a),dp=m(tc,": the union of all defined dataset splits."),tc.forEach(a),Et.forEach(a),ip=p(z),Aa=n(z,"P",{});var ac=o(Aa);pp=m(ac,"Note: All splits, including compositions inherit from "),qr=n(ac,"CODE",{});var ym=o(qr);cp=m(ym,"datasets.SplitBase"),ym.forEach(a),ac.forEach(a),mp=p(z),pa=n(z,"P",{});var Wn=o(pa);gp=m(Wn,"See the :doc:"),Ar=n(Wn,"CODE",{});var Dm=o(Ar);fp=m(Dm,"guide on splits </loading>"),Dm.forEach(a),up=m(Wn," for more information."),Wn.forEach(a),hp=p(z),b(pt.$$.fragment,z),z.forEach(a),Sn=p(s),C=n(s,"DIV",{class:!0});var M=o(C);b(ca.$$.fragment,M),_p=p(M),Lr=n(M,"P",{});var km=o(Lr);$p=m(km,"Descriptor corresponding to a named split (train, test, \u2026)."),km.forEach(a),bp=p(M),b(ct.$$.fragment,M),vp=p(M),Or=n(M,"P",{});var jm=o(Or);wp=m(jm,"Warning:"),jm.forEach(a),xp=p(M),b(mt.$$.fragment,M),Ep=p(M),Mr=n(M,"P",{});var Tm=o(Mr);yp=m(Tm,"Warning:"),Tm.forEach(a),Dp=p(M),b(gt.$$.fragment,M),kp=p(M),b(ft.$$.fragment,M),M.forEach(a),In=p(s),qe=n(s,"DIV",{class:!0});var Xn=o(qe);b(ma.$$.fragment,Xn),jp=p(Xn),Vr=n(Xn,"P",{});var Sm=o(Vr);Tp=m(Sm,"Split corresponding to the union of all defined dataset splits."),Sm.forEach(a),Xn.forEach(a),Bn=p(s),F=n(s,"DIV",{class:!0});var be=o(F);b(ga.$$.fragment,be),Sp=p(be),Fr=n(be,"P",{});var Im=o(Fr);Ip=m(Im,"Reading instruction for a dataset."),Im.forEach(a),Bp=p(be),b(ut.$$.fragment,be),Np=p(be),ht=n(be,"DIV",{class:!0});var Kn=o(ht);b(fa.$$.fragment,Kn),Pp=p(Kn),Ur=n(Kn,"P",{});var Bm=o(Ur);Rp=m(Bm,"Creates a ReadInstruction instance out of a string spec."),Bm.forEach(a),Kn.forEach(a),Cp=p(be),he=n(be,"DIV",{class:!0});var ds=o(he);b(ua.$$.fragment,ds),qp=p(ds),zr=n(ds,"P",{});var Nm=o(zr);Ap=m(Nm,"Translate instruction into a list of absolute instructions."),Nm.forEach(a),Lp=p(ds),Gr=n(ds,"P",{});var Pm=o(Gr);Op=m(Pm,"Those absolute instructions are then to be added together."),Pm.forEach(a),ds.forEach(a),be.forEach(a),Nn=p(s),Ae=n(s,"H2",{class:!0});var Jn=o(Ae);_t=n(Jn,"A",{id:!0,class:!0,href:!0});var Rm=o(_t);Hr=n(Rm,"SPAN",{});var Cm=o(Hr);b(ha.$$.fragment,Cm),Cm.forEach(a),Rm.forEach(a),Mp=p(Jn),Wr=n(Jn,"SPAN",{});var qm=o(Wr);Vp=m(qm,"Version"),qm.forEach(a),Jn.forEach(a),Pn=p(s),X=n(s,"DIV",{class:!0});var yt=o(X);b(_a.$$.fragment,yt),Fp=p(yt),Xr=n(yt,"P",{});var Am=o(Xr);Up=m(Am,"Dataset version MAJOR.MINOR.PATCH."),Am.forEach(a),zp=p(yt),b($t.$$.fragment,yt),Gp=p(yt),bt=n(yt,"DIV",{class:!0});var Yn=o(bt);b($a.$$.fragment,Yn),Hp=p(Yn),Kr=n(Yn,"P",{});var Lm=o(Kr);Wp=m(Lm,"Returns True if other_version matches."),Lm.forEach(a),Yn.forEach(a),yt.forEach(a),this.h()},h(){D(d,"name","hf:doc:metadata"),D(d,"content",JSON.stringify(hg)),D(l,"id","builder-classes"),D(l,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(l,"href","#builder-classes"),D(f,"class","relative group"),D(Ve,"id","datasets.DatasetBuilder"),D(Ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(Ve,"href","#datasets.DatasetBuilder"),D(ve,"class","relative group"),D(ya,"href","/docs/datasets/main/en/package_reference/builder_classes#datasets.DatasetBuilder"),D(Da,"href","/docs/datasets/main/en/package_reference/builder_classes#datasets.BuilderConfig"),D(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(Xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(Ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(Ta,"href","/docs/datasets/main/en/package_reference/builder_classes#datasets.DatasetBuilder"),D(Sa,"href","/docs/datasets/main/en/package_reference/builder_classes#datasets.BuilderConfig"),D(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(Ke,"id","datasets.DownloadManager"),D(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(Ke,"href","#datasets.DownloadManager"),D(ke,"class","relative group"),D(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(at,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(dt,"id","datasets.SplitGenerator"),D(dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(dt,"href","#datasets.SplitGenerator"),D(Re,"class","relative group"),D(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(ht,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(_t,"id","datasets.Version"),D(_t,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),D(_t,"href","#datasets.Version"),D(Ae,"class","relative group"),D(bt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),D(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(s,_){e(document.head,d),y(s,h,_),y(s,f,_),e(f,l),e(l,u),v(t,u,null),e(f,g),e(f,is),e(is,Qn),y(s,gn,_),y(s,ve,_),e(ve,Ve),e(Ve,ps),v(Dt,ps,null),e(ve,Zn),e(ve,cs),e(cs,eo),y(s,fn,_),y(s,ee,_),e(ee,to),e(ee,ya),e(ya,ao),e(ee,so),e(ee,Da),e(Da,ro),e(ee,no),y(s,un,_),y(s,T,_),v(kt,T,null),e(T,oo),e(T,ms),e(ms,lo),e(T,io),e(T,ka),e(ka,gs),e(gs,po),e(ka,co),e(T,mo),e(T,we),e(we,jt),e(jt,go),e(jt,fs),e(fs,fo),e(jt,uo),e(we,ho),e(we,Tt),e(Tt,_o),e(Tt,us),e(us,$o),e(Tt,bo),e(we,vo),e(we,xe),e(xe,wo),e(xe,hs),e(hs,xo),e(xe,Eo),e(xe,_s),e(_s,yo),e(xe,Do),e(T,ko),e(T,K),e(K,$s),e($s,jo),e(K,To),e(K,bs),e(bs,So),e(K,Io),e(K,vs),e(vs,Bo),e(K,No),e(K,ws),e(ws,Po),e(K,Ro),e(T,Co),e(T,te),v(St,te,null),e(te,qo),e(te,xs),e(xs,Ao),e(te,Lo),v(Fe,te,null),e(T,Oo),e(T,P),v(It,P,null),e(P,Mo),e(P,Es),e(Es,Vo),e(P,Fo),e(P,ys),e(ys,Uo),e(P,zo),e(P,ja),e(ja,Go),e(ja,Ds),e(Ds,Ho),e(P,Wo),v(Ue,P,null),e(P,Xo),e(P,ks),e(ks,Ko),e(P,Jo),v(ze,P,null),e(P,Yo),e(P,js),e(js,Qo),e(P,Zo),v(Ge,P,null),e(T,el),e(T,ae),v(Bt,ae,null),e(ae,tl),e(ae,Ts),e(Ts,al),e(ae,sl),v(He,ae,null),e(T,rl),e(T,se),v(Nt,se,null),e(se,nl),e(se,Ss),e(Ss,ol),e(se,ll),v(We,se,null),e(T,dl),e(T,Xe),v(Pt,Xe,null),e(Xe,il),e(Xe,Is),e(Is,pl),y(s,hn,_),y(s,Q,_),v(Rt,Q,null),e(Q,cl),e(Q,Bs),e(Bs,ml),e(Q,gl),e(Q,re),e(re,Ns),e(Ns,fl),e(re,ul),e(re,Ps),e(Ps,hl),e(re,_l),e(re,Rs),e(Rs,$l),e(re,bl),y(s,_n,_),y(s,Ee,_),v(Ct,Ee,null),e(Ee,vl),e(Ee,Cs),e(Cs,wl),y(s,$n,_),y(s,ye,_),v(qt,ye,null),e(ye,xl),e(ye,qs),e(qs,El),y(s,bn,_),y(s,G,_),v(At,G,null),e(G,yl),e(G,Lt),e(Lt,Dl),e(Lt,Ta),e(Ta,kl),e(Lt,jl),e(G,Tl),e(G,Ot),e(Ot,Sl),e(Ot,Sa),e(Sa,Il),e(Ot,Bl),e(G,Nl),e(G,ne),v(Mt,ne,null),e(ne,Pl),e(ne,As),e(As,Rl),e(ne,Cl),e(ne,De),e(De,Ls),e(Ls,ql),e(De,Al),e(De,Os),e(Os,Ll),e(De,Ol),e(De,Ms),e(Ms,Ml),y(s,vn,_),y(s,ke,_),e(ke,Ke),e(Ke,Vs),v(Vt,Vs,null),e(ke,Vl),e(ke,Fs),e(Fs,Fl),y(s,wn,_),y(s,R,_),v(Ft,R,null),e(R,Ul),e(R,J),v(Ut,J,null),e(J,zl),e(J,Us),e(Us,Gl),e(J,Hl),e(J,je),e(je,Wl),e(je,zs),e(zs,Xl),e(je,Kl),e(je,Gs),e(Gs,Jl),e(je,Yl),e(J,Ql),v(Je,J,null),e(R,Zl),e(R,oe),v(zt,oe,null),e(oe,ed),e(oe,Hs),e(Hs,td),e(oe,ad),v(Ye,oe,null),e(R,sd),e(R,le),v(Gt,le,null),e(le,rd),e(le,Ht),e(Ht,nd),e(Ht,Ws),e(Ws,od),e(Ht,ld),e(le,dd),v(Qe,le,null),e(R,id),e(R,de),v(Wt,de,null),e(de,pd),e(de,Xs),e(Xs,cd),e(de,md),v(Ze,de,null),e(R,gd),e(R,ie),v(Xt,ie,null),e(ie,fd),e(ie,Ks),e(Ks,ud),e(ie,hd),v(et,ie,null),e(R,_d),e(R,pe),v(Kt,pe,null),e(pe,$d),e(pe,Js),e(Js,bd),e(pe,vd),v(tt,pe,null),e(R,wd),e(R,at),v(Jt,at,null),e(at,xd),e(at,Ys),e(Ys,Ed),y(s,xn,_),y(s,q,_),v(Yt,q,null),e(q,yd),e(q,H),e(H,Dd),e(H,Qs),e(Qs,kd),e(H,jd),e(H,Zs),e(Zs,Td),e(H,Sd),e(H,er),e(er,Id),e(H,Bd),e(H,tr),e(tr,Nd),e(H,Pd),e(q,Rd),e(q,ce),v(Qt,ce,null),e(ce,Cd),e(ce,ar),e(ar,qd),e(ce,Ad),v(st,ce,null),e(q,Ld),e(q,me),v(Zt,me,null),e(me,Od),e(me,sr),e(sr,Md),e(me,Vd),v(rt,me,null),e(q,Fd),e(q,ge),v(ea,ge,null),e(ge,Ud),e(ge,rr),e(rr,zd),e(ge,Gd),v(nt,ge,null),e(q,Hd),e(q,fe),v(ta,fe,null),e(fe,Wd),e(fe,nr),e(nr,Xd),e(fe,Kd),v(ot,fe,null),e(q,Jd),e(q,ue),v(aa,ue,null),e(ue,Yd),e(ue,or),e(or,Qd),e(ue,Zd),v(lt,ue,null),y(s,En,_),y(s,Te,_),v(sa,Te,null),e(Te,ei),e(Te,lr),e(lr,ti),y(s,yn,_),y(s,V,_),v(ra,V,null),e(V,ai),e(V,Ia),e(Ia,dr),e(dr,si),e(Ia,ri),e(V,ni),e(V,na),e(na,oi),e(na,ir),e(ir,li),e(na,di),e(V,ii),e(V,pr),e(pr,pi),e(V,ci),e(V,oa),e(oa,cr),e(cr,Se),e(Se,Dn),e(Se,mi),e(Se,mr),e(mr,gi),e(Se,fi),e(Se,gr),e(gr,ui),e(oa,hi),e(oa,Ie),e(Ie,Be),e(Be,Ba),e(Ba,fr),e(fr,_i),e(Ba,$i),e(Be,bi),e(Be,ur),e(ur,vi),e(Be,wi),e(Be,hr),e(hr,xi),e(Ie,Ei),e(Ie,Ne),e(Ne,_r),e(_r,$r),e($r,yi),e(Ne,Di),e(Ne,br),e(br,ki),e(Ne,ji),e(Ne,vr),e(vr,Ti),e(Ie,Si),e(Ie,Pe),e(Pe,wr),e(wr,xr),e(xr,Ii),e(Pe,Bi),e(Pe,Er),e(Er,Ni),e(Pe,Pi),e(Pe,yr),e(yr,Ri),y(s,kn,_),y(s,Re,_),e(Re,dt),e(dt,Dr),v(la,Dr,null),e(Re,Ci),e(Re,kr),e(kr,qi),y(s,jn,_),y(s,W,_),v(da,W,null),e(W,Ai),e(W,jr),e(jr,Li),e(W,Oi),e(W,Ce),e(Ce,Mi),e(Ce,Tr),e(Tr,Vi),e(Ce,Fi),e(Ce,Sr),e(Sr,Ui),e(Ce,zi),e(W,Gi),v(it,W,null),y(s,Tn,_),y(s,A,_),v(ia,A,null),e(A,Hi),e(A,Na),e(Na,Ir),e(Ir,Wi),e(Na,Xi),e(A,Ki),e(A,Br),e(Br,Ji),e(A,Yi),e(A,Z),e(Z,Pa),e(Pa,Nr),e(Nr,Qi),e(Pa,Zi),e(Z,ep),e(Z,Ra),e(Ra,Pr),e(Pr,tp),e(Ra,ap),e(Z,sp),e(Z,Ca),e(Ca,Rr),e(Rr,rp),e(Ca,np),e(Z,op),e(Z,qa),e(qa,Cr),e(Cr,lp),e(qa,dp),e(A,ip),e(A,Aa),e(Aa,pp),e(Aa,qr),e(qr,cp),e(A,mp),e(A,pa),e(pa,gp),e(pa,Ar),e(Ar,fp),e(pa,up),e(A,hp),v(pt,A,null),y(s,Sn,_),y(s,C,_),v(ca,C,null),e(C,_p),e(C,Lr),e(Lr,$p),e(C,bp),v(ct,C,null),e(C,vp),e(C,Or),e(Or,wp),e(C,xp),v(mt,C,null),e(C,Ep),e(C,Mr),e(Mr,yp),e(C,Dp),v(gt,C,null),e(C,kp),v(ft,C,null),y(s,In,_),y(s,qe,_),v(ma,qe,null),e(qe,jp),e(qe,Vr),e(Vr,Tp),y(s,Bn,_),y(s,F,_),v(ga,F,null),e(F,Sp),e(F,Fr),e(Fr,Ip),e(F,Bp),v(ut,F,null),e(F,Np),e(F,ht),v(fa,ht,null),e(ht,Pp),e(ht,Ur),e(Ur,Rp),e(F,Cp),e(F,he),v(ua,he,null),e(he,qp),e(he,zr),e(zr,Ap),e(he,Lp),e(he,Gr),e(Gr,Op),y(s,Nn,_),y(s,Ae,_),e(Ae,_t),e(_t,Hr),v(ha,Hr,null),e(Ae,Mp),e(Ae,Wr),e(Wr,Vp),y(s,Pn,_),y(s,X,_),v(_a,X,null),e(X,Fp),e(X,Xr),e(Xr,Up),e(X,zp),v($t,X,null),e(X,Gp),e(X,bt),v($a,bt,null),e(bt,Hp),e(bt,Kr),e(Kr,Wp),Rn=!0},p(s,[_]){const ba={};_&2&&(ba.$$scope={dirty:_,ctx:s}),Fe.$set(ba);const Jr={};_&2&&(Jr.$$scope={dirty:_,ctx:s}),Ue.$set(Jr);const Yr={};_&2&&(Yr.$$scope={dirty:_,ctx:s}),ze.$set(Yr);const Qr={};_&2&&(Qr.$$scope={dirty:_,ctx:s}),Ge.$set(Qr);const va={};_&2&&(va.$$scope={dirty:_,ctx:s}),He.$set(va);const Zr={};_&2&&(Zr.$$scope={dirty:_,ctx:s}),We.$set(Zr);const en={};_&2&&(en.$$scope={dirty:_,ctx:s}),Je.$set(en);const tn={};_&2&&(tn.$$scope={dirty:_,ctx:s}),Ye.$set(tn);const Le={};_&2&&(Le.$$scope={dirty:_,ctx:s}),Qe.$set(Le);const an={};_&2&&(an.$$scope={dirty:_,ctx:s}),Ze.$set(an);const sn={};_&2&&(sn.$$scope={dirty:_,ctx:s}),et.$set(sn);const N={};_&2&&(N.$$scope={dirty:_,ctx:s}),tt.$set(N);const rn={};_&2&&(rn.$$scope={dirty:_,ctx:s}),st.$set(rn);const La={};_&2&&(La.$$scope={dirty:_,ctx:s}),rt.$set(La);const nn={};_&2&&(nn.$$scope={dirty:_,ctx:s}),nt.$set(nn);const Oe={};_&2&&(Oe.$$scope={dirty:_,ctx:s}),ot.$set(Oe);const wa={};_&2&&(wa.$$scope={dirty:_,ctx:s}),lt.$set(wa);const on={};_&2&&(on.$$scope={dirty:_,ctx:s}),it.$set(on);const xa={};_&2&&(xa.$$scope={dirty:_,ctx:s}),pt.$set(xa);const ln={};_&2&&(ln.$$scope={dirty:_,ctx:s}),ct.$set(ln);const Me={};_&2&&(Me.$$scope={dirty:_,ctx:s}),mt.$set(Me);const dn={};_&2&&(dn.$$scope={dirty:_,ctx:s}),gt.$set(dn);const pn={};_&2&&(pn.$$scope={dirty:_,ctx:s}),ft.$set(pn);const Y={};_&2&&(Y.$$scope={dirty:_,ctx:s}),ut.$set(Y);const cn={};_&2&&(cn.$$scope={dirty:_,ctx:s}),$t.$set(cn)},i(s){Rn||(w(t.$$.fragment,s),w(Dt.$$.fragment,s),w(kt.$$.fragment,s),w(St.$$.fragment,s),w(Fe.$$.fragment,s),w(It.$$.fragment,s),w(Ue.$$.fragment,s),w(ze.$$.fragment,s),w(Ge.$$.fragment,s),w(Bt.$$.fragment,s),w(He.$$.fragment,s),w(Nt.$$.fragment,s),w(We.$$.fragment,s),w(Pt.$$.fragment,s),w(Rt.$$.fragment,s),w(Ct.$$.fragment,s),w(qt.$$.fragment,s),w(At.$$.fragment,s),w(Mt.$$.fragment,s),w(Vt.$$.fragment,s),w(Ft.$$.fragment,s),w(Ut.$$.fragment,s),w(Je.$$.fragment,s),w(zt.$$.fragment,s),w(Ye.$$.fragment,s),w(Gt.$$.fragment,s),w(Qe.$$.fragment,s),w(Wt.$$.fragment,s),w(Ze.$$.fragment,s),w(Xt.$$.fragment,s),w(et.$$.fragment,s),w(Kt.$$.fragment,s),w(tt.$$.fragment,s),w(Jt.$$.fragment,s),w(Yt.$$.fragment,s),w(Qt.$$.fragment,s),w(st.$$.fragment,s),w(Zt.$$.fragment,s),w(rt.$$.fragment,s),w(ea.$$.fragment,s),w(nt.$$.fragment,s),w(ta.$$.fragment,s),w(ot.$$.fragment,s),w(aa.$$.fragment,s),w(lt.$$.fragment,s),w(sa.$$.fragment,s),w(ra.$$.fragment,s),w(la.$$.fragment,s),w(da.$$.fragment,s),w(it.$$.fragment,s),w(ia.$$.fragment,s),w(pt.$$.fragment,s),w(ca.$$.fragment,s),w(ct.$$.fragment,s),w(mt.$$.fragment,s),w(gt.$$.fragment,s),w(ft.$$.fragment,s),w(ma.$$.fragment,s),w(ga.$$.fragment,s),w(ut.$$.fragment,s),w(fa.$$.fragment,s),w(ua.$$.fragment,s),w(ha.$$.fragment,s),w(_a.$$.fragment,s),w($t.$$.fragment,s),w($a.$$.fragment,s),Rn=!0)},o(s){x(t.$$.fragment,s),x(Dt.$$.fragment,s),x(kt.$$.fragment,s),x(St.$$.fragment,s),x(Fe.$$.fragment,s),x(It.$$.fragment,s),x(Ue.$$.fragment,s),x(ze.$$.fragment,s),x(Ge.$$.fragment,s),x(Bt.$$.fragment,s),x(He.$$.fragment,s),x(Nt.$$.fragment,s),x(We.$$.fragment,s),x(Pt.$$.fragment,s),x(Rt.$$.fragment,s),x(Ct.$$.fragment,s),x(qt.$$.fragment,s),x(At.$$.fragment,s),x(Mt.$$.fragment,s),x(Vt.$$.fragment,s),x(Ft.$$.fragment,s),x(Ut.$$.fragment,s),x(Je.$$.fragment,s),x(zt.$$.fragment,s),x(Ye.$$.fragment,s),x(Gt.$$.fragment,s),x(Qe.$$.fragment,s),x(Wt.$$.fragment,s),x(Ze.$$.fragment,s),x(Xt.$$.fragment,s),x(et.$$.fragment,s),x(Kt.$$.fragment,s),x(tt.$$.fragment,s),x(Jt.$$.fragment,s),x(Yt.$$.fragment,s),x(Qt.$$.fragment,s),x(st.$$.fragment,s),x(Zt.$$.fragment,s),x(rt.$$.fragment,s),x(ea.$$.fragment,s),x(nt.$$.fragment,s),x(ta.$$.fragment,s),x(ot.$$.fragment,s),x(aa.$$.fragment,s),x(lt.$$.fragment,s),x(sa.$$.fragment,s),x(ra.$$.fragment,s),x(la.$$.fragment,s),x(da.$$.fragment,s),x(it.$$.fragment,s),x(ia.$$.fragment,s),x(pt.$$.fragment,s),x(ca.$$.fragment,s),x(ct.$$.fragment,s),x(mt.$$.fragment,s),x(gt.$$.fragment,s),x(ft.$$.fragment,s),x(ma.$$.fragment,s),x(ga.$$.fragment,s),x(ut.$$.fragment,s),x(fa.$$.fragment,s),x(ua.$$.fragment,s),x(ha.$$.fragment,s),x(_a.$$.fragment,s),x($t.$$.fragment,s),x($a.$$.fragment,s),Rn=!1},d(s){a(d),s&&a(h),s&&a(f),E(t),s&&a(gn),s&&a(ve),E(Dt),s&&a(fn),s&&a(ee),s&&a(un),s&&a(T),E(kt),E(St),E(Fe),E(It),E(Ue),E(ze),E(Ge),E(Bt),E(He),E(Nt),E(We),E(Pt),s&&a(hn),s&&a(Q),E(Rt),s&&a(_n),s&&a(Ee),E(Ct),s&&a($n),s&&a(ye),E(qt),s&&a(bn),s&&a(G),E(At),E(Mt),s&&a(vn),s&&a(ke),E(Vt),s&&a(wn),s&&a(R),E(Ft),E(Ut),E(Je),E(zt),E(Ye),E(Gt),E(Qe),E(Wt),E(Ze),E(Xt),E(et),E(Kt),E(tt),E(Jt),s&&a(xn),s&&a(q),E(Yt),E(Qt),E(st),E(Zt),E(rt),E(ea),E(nt),E(ta),E(ot),E(aa),E(lt),s&&a(En),s&&a(Te),E(sa),s&&a(yn),s&&a(V),E(ra),s&&a(kn),s&&a(Re),E(la),s&&a(jn),s&&a(W),E(da),E(it),s&&a(Tn),s&&a(A),E(ia),E(pt),s&&a(Sn),s&&a(C),E(ca),E(ct),E(mt),E(gt),E(ft),s&&a(In),s&&a(qe),E(ma),s&&a(Bn),s&&a(F),E(ga),E(ut),E(fa),E(ua),s&&a(Nn),s&&a(Ae),E(ha),s&&a(Pn),s&&a(X),E(_a),E($t),E($a)}}}const hg={local:"builder-classes",sections:[{local:"datasets.DatasetBuilder",title:"Builders"},{local:"datasets.DownloadManager",title:"Download"},{local:"datasets.SplitGenerator",title:"Splits"},{local:"datasets.Version",title:"Version"}],title:"Builder classes"};function _g(k){return Um(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Eg extends Om{constructor(d){super();Mm(this,d,_g,ug,Vm,{})}}export{Eg as default,hg as metadata};
