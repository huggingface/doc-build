import{S as oc,i as lc,s as dc,e as r,k as p,w as $,t as m,M as ic,c as o,d as a,m as c,a as l,x as b,h as g,b as j,F as e,g as D,y as v,q as x,o as w,B as E,v as pc,L as B}from"../../chunks/vendor-8138ceec.js";import{D as k}from"../../chunks/Docstring-6fa3bd37.js";import{C as N}from"../../chunks/CodeBlock-fc89709f.js";import{I as cc}from"../../chunks/IconCopyLink-2dd3a6ac.js";import{E as S}from"../../chunks/ExampleCodeBlock-25dbadc2.js";function mc(y){let d,_,f,n,u;return n=new N({props:{code:`from datasets import load_dataset_builder
builder = load_dataset_builder('rotten_tomatoes')
ds = builder.download_and_prepare()
ds = builder.as_dataset(split='train')
ds`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.download_and_prepare()
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.as_dataset(split=<span class="hljs-string">&#x27;train&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds
Dataset({
    features: [<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
    num_rows: <span class="hljs-number">8530</span>
})`}}),{c(){d=r("p"),_=m("Example:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function gc(y){let d,_,f,n,u;return n=new N({props:{code:`from datasets import load_dataset_builder
builder = load_dataset_builder('rotten_tomatoes')
ds = builder.download_and_prepare()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = builder.download_and_prepare()`}}),{c(){d=r("p"),_=m("Example:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function fc(y){let d,_,f,n,u;return n=new N({props:{code:`from datasets import load_dataset_builder
ds_builder = load_dataset_builder('rotten_tomatoes')
ds_builder.get_all_exported_dataset_infos()
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder.get_all_exported_dataset_infos()
{<span class="hljs-string">&#x27;default&#x27;</span>: DatasetInfo(description=<span class="hljs-string">&quot;Movie Review Dataset.
a dataset of containing 5,331 positive and 5,331 negative processed
s from Rotten Tomatoes movie reviews. This data was first used in Bo
 Lillian Lee, \`\`Seeing stars: Exploiting class relationships for
t categorization with respect to rating scales.&#x27;&#x27;, Proceedings of the
5.
ion=&#x27;@InProceedings{Pang+Lee:05a,
 =       {Bo Pang and Lillian Lee},
=        {Seeing stars: Exploiting class relationships for sentiment
          categorization with respect to rating scales},
tle =    {Proceedings of the ACL},
         2005

age=&#x27;http://www.cs.cornell.edu/people/pabo/movie-review-data/&#x27;, license=&#x27;&#x27;, features={&#x27;text&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;label&#x27;: ClassLabel(num_classes=2, names=[&#x27;neg&#x27;, &#x27;pos&#x27;], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input=&#x27;&#x27;, output=&#x27;&#x27;), task_templates=[TextClassification(task=&#x27;text-classification&#x27;, text_column=&#x27;text&#x27;, label_column=&#x27;label&#x27;)], builder_name=&#x27;rotten_tomatoes_movie_review&#x27;, config_name=&#x27;default&#x27;, version=1.0.0, splits={&#x27;train&#x27;: SplitInfo(name=&#x27;train&#x27;, num_bytes=1074810, num_examples=8530, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;validation&#x27;: SplitInfo(name=&#x27;validation&#x27;, num_bytes=134679, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;test&#x27;: SplitInfo(name=&#x27;test&#x27;, num_bytes=135972, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;)}, download_checksums={&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;: {&#x27;num_bytes&#x27;: 487770, &#x27;checksum&#x27;: &#x27;a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9&#x27;}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)}</span>`}}),{c(){d=r("p"),_=m("Example:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function uc(y){let d,_,f,n,u;return n=new N({props:{code:`from datasets import load_dataset_builder
ds_builder = load_dataset_builder('rotten_tomatoes')
ds_builder.get_exported_dataset_info()
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder.get_exported_dataset_info()
DatasetInfo(description=<span class="hljs-string">&quot;Movie Review Dataset.
a dataset of containing 5,331 positive and 5,331 negative processed
s from Rotten Tomatoes movie reviews. This data was first used in Bo
 Lillian Lee, \`\`Seeing stars: Exploiting class relationships for
t categorization with respect to rating scales.&#x27;&#x27;, Proceedings of the
5.
ion=&#x27;@InProceedings{Pang+Lee:05a,
 =       {Bo Pang and Lillian Lee},
=        {Seeing stars: Exploiting class relationships for sentiment
          categorization with respect to rating scales},
tle =    {Proceedings of the ACL},
         2005

age=&#x27;http://www.cs.cornell.edu/people/pabo/movie-review-data/&#x27;, license=&#x27;&#x27;, features={&#x27;text&#x27;: Value(dtype=&#x27;string&#x27;, id=None), &#x27;label&#x27;: ClassLabel(num_classes=2, names=[&#x27;neg&#x27;, &#x27;pos&#x27;], id=None)}, post_processed=None, supervised_keys=SupervisedKeysData(input=&#x27;&#x27;, output=&#x27;&#x27;), task_templates=[TextClassification(task=&#x27;text-classification&#x27;, text_column=&#x27;text&#x27;, label_column=&#x27;label&#x27;)], builder_name=&#x27;rotten_tomatoes_movie_review&#x27;, config_name=&#x27;default&#x27;, version=1.0.0, splits={&#x27;train&#x27;: SplitInfo(name=&#x27;train&#x27;, num_bytes=1074810, num_examples=8530, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;validation&#x27;: SplitInfo(name=&#x27;validation&#x27;, num_bytes=134679, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;), &#x27;test&#x27;: SplitInfo(name=&#x27;test&#x27;, num_bytes=135972, num_examples=1066, dataset_name=&#x27;rotten_tomatoes_movie_review&#x27;)}, download_checksums={&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;: {&#x27;num_bytes&#x27;: 487770, &#x27;checksum&#x27;: &#x27;a05befe52aafda71d458d188a1c54506a998b1308613ba76bbda2e5029409ce9&#x27;}}, download_size=487770, post_processing_size=None, dataset_size=1345461, size_in_bytes=1833231)</span>`}}),{c(){d=r("p"),_=m("Example:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function _c(y){let d,_,f,n,u;return n=new N({props:{code:"downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)'}}),{c(){d=r("p"),_=m("Example:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function hc(y){let d,_,f,n,u;return n=new N({props:{code:"extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))",highlighted:'<span class="hljs-attr">extracted_paths</span> = dl_manager.extract(dl_manager.download(url_or_urls))'}}),{c(){d=r("p"),_=m("Is roughly equivalent to:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Is roughly equivalent to:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function $c(y){let d,_,f,n,u;return n=new N({props:{code:"downloaded_files = dl_manager.download_custom('s3://my-bucket/data.zip', custom_download_for_my_private_bucket)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download_custom(<span class="hljs-string">&#x27;s3://my-bucket/data.zip&#x27;</span>, custom_download_for_my_private_bucket)'}}),{c(){d=r("p"),_=m("Example:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function bc(y){let d,_,f,n,u;return n=new N({props:{code:`downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
extracted_files = dl_manager.extract(downloaded_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>extracted_files = dl_manager.extract(downloaded_files)`}}),{c(){d=r("p"),_=m("Example:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function vc(y){let d,_,f,n,u;return n=new N({props:{code:`archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
files = dl_manager.iter_archive(archive)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>archive = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_archive(archive)`}}),{c(){d=r("p"),_=m("Example:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function xc(y){let d,_,f,n,u;return n=new N({props:{code:`files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')
files = dl_manager.iter_files(files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.download_and_extract(<span class="hljs-string">&#x27;https://huggingface.co/datasets/beans/resolve/main/data/train.zip&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_files(files)`}}),{c(){d=r("p"),_=m("Example:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function wc(y){let d,_,f,n,u;return n=new N({props:{code:"downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)'}}),{c(){d=r("p"),_=m("Example:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function Ec(y){let d,_,f,n,u;return n=new N({props:{code:"extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))",highlighted:'<span class="hljs-attr">extracted_paths</span> = dl_manager.extract(dl_manager.download(url_or_urls))'}}),{c(){d=r("p"),_=m("Is roughly equivalent to:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Is roughly equivalent to:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function Dc(y){let d,_,f,n,u;return n=new N({props:{code:`downloaded_files = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
extracted_files = dl_manager.extract(downloaded_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>downloaded_files = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>extracted_files = dl_manager.extract(downloaded_files)`}}),{c(){d=r("p"),_=m("Example:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function yc(y){let d,_,f,n,u;return n=new N({props:{code:`archive = dl_manager.download('https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz')
files = dl_manager.iter_archive(archive)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>archive = dl_manager.download(<span class="hljs-string">&#x27;https://storage.googleapis.com/seldon-datasets/sentence_polarity_v1/rt-polaritydata.tar.gz&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_archive(archive)`}}),{c(){d=r("p"),_=m("Example:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function jc(y){let d,_,f,n,u;return n=new N({props:{code:`files = dl_manager.download_and_extract('https://huggingface.co/datasets/beans/resolve/main/data/train.zip')
files = dl_manager.iter_files(files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.download_and_extract(<span class="hljs-string">&#x27;https://huggingface.co/datasets/beans/resolve/main/data/train.zip&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>files = dl_manager.iter_files(files)`}}),{c(){d=r("p"),_=m("Example:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function kc(y){let d,_,f,n,u;return n=new N({props:{code:`datasets.SplitGenerator(
    name=datasets.Split.TRAIN,
    gen_kwargs={"split_key": "train", "files": dl_manager.download_and_extract(url)},
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TRAIN,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and_extract(url)},
<span class="hljs-meta">... </span>)`}}),{c(){d=r("p"),_=m("Example:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function Tc(y){let d,_,f,n,u;return n=new N({props:{code:`datasets.SplitGenerator(
    name=datasets.Split.TRAIN,
    gen_kwargs={"split_key": "train", "files": dl_manager.download_and extract(url)},
),
datasets.SplitGenerator(
    name=datasets.Split.VALIDATION,
    gen_kwargs={"split_key": "validation", "files": dl_manager.download_and extract(url)},
),
datasets.SplitGenerator(
    name=datasets.Split.TEST,
    gen_kwargs={"split_key": "test", "files": dl_manager.download_and extract(url)},
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TRAIN,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and extract(url)},
<span class="hljs-meta">... </span>),
<span class="hljs-meta">... </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.VALIDATION,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;validation&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and extract(url)},
<span class="hljs-meta">... </span>),
<span class="hljs-meta">... </span>datasets.SplitGenerator(
<span class="hljs-meta">... </span>    name=datasets.Split.TEST,
<span class="hljs-meta">... </span>    gen_kwargs={<span class="hljs-string">&quot;split_key&quot;</span>: <span class="hljs-string">&quot;test&quot;</span>, <span class="hljs-string">&quot;files&quot;</span>: dl_manager.download_and extract(url)},
<span class="hljs-meta">... </span>)`}}),{c(){d=r("p"),_=m("Example:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function Ic(y){let d,_,f,n,u;return n=new N({props:{code:`Each descriptor can be composed with other using addition or slice. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[0:25]) + datasets.Split.TEST

The resulting split will correspond to 25% of the train split merged with
100% of the test split.`,highlighted:`Each descriptor can be composed <span class="hljs-keyword">with</span> other using addition <span class="hljs-keyword">or</span> <span class="hljs-built_in">slice</span>. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">0</span>:<span class="hljs-number">25</span>]) + datasets.Split.TEST

The resulting split will correspond to <span class="hljs-number">25</span>% of the train split merged <span class="hljs-keyword">with</span>
<span class="hljs-number">100</span>% of the test split.`}}),{c(){d=r("p"),_=m("Example:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function Sc(y){let d,_,f,n,u;return n=new N({props:{code:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[75:])
)  # Error
split = datasets.Split.TEST + datasets.Split.ALL  # Error`,highlighted:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">75</span>:])
)  <span class="hljs-comment"># Error</span>
split = datasets.Split.TEST + datasets.Split.ALL  <span class="hljs-comment"># Error</span>`}}),{c(){d=r("p"),_=m("A split cannot be added twice, so the following will fail:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"A split cannot be added twice, so the following will fail:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function Bc(y){let d,_,f,n,u;return n=new N({props:{code:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
datasets.Split.TEST.subsplit(datasets.percent[:50])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:50])`,highlighted:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
datasets.Split.TEST.subsplit(datasets.percent[:<span class="hljs-number">50</span>])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),{c(){d=r("p"),_=m("The slices can be applied only one time. So the following are valid:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"The slices can be applied only one time. So the following are valid:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function Nc(y){let d,_,f,n,u;return n=new N({props:{code:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:25]).subsplit(datasets.percent[:25])
split = (train.subsplit(datasets.percent[:25]) + test).subsplit(datasets.percent[:50])`,highlighted:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]).subsplit(datasets.percent[:<span class="hljs-number">25</span>])
split = (train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) + test).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),{c(){d=r("p"),_=m("But not:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"But not:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function Rc(y){let d,_,f,n,u;return n=new N({props:{code:`# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec('test[:33%]'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction('test', to=33, unit='%'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%'))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]+train[1:-1]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%]+train[1:-1]'))
ds = datasets.load_dataset('mnist', split=(
datasets.ReadInstruction('test', to=33, unit='%') +
datasets.ReadInstruction('train', from_=1, to=-1, unit='abs')))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%](pct1_dropremainder)')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%](pct1_dropremainder)'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%', rounding="pct1_dropremainder"))

# 10-fold validation:
tests = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', from_=k, to=k+10, unit='%')
for k in range(0, 100, 10)])
trains = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')
for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(<span class="hljs-string">&#x27;test[:33%]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=(
datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) +
datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">1</span>, to=-<span class="hljs-number">1</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>)))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&quot;pct1_dropremainder&quot;</span>))

<span class="hljs-comment"># 10-fold validation:</span>
tests = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
trains = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}),{c(){d=r("p"),_=m("Examples:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Examples:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function Pc(y){let d,_,f,n,u;return n=new N({props:{code:'VERSION = datasets.Version("1.0.0")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>VERSION = datasets.Version(<span class="hljs-string">&quot;1.0.0&quot;</span>)'}}),{c(){d=r("p"),_=m("Example:"),f=p(),$(n.$$.fragment)},l(t){d=o(t,"P",{});var i=l(d);_=g(i,"Example:"),i.forEach(a),f=c(t),b(n.$$.fragment,t)},m(t,i){D(t,d,i),e(d,_),D(t,f,i),v(n,t,i),u=!0},p:B,i(t){u||(x(n.$$.fragment,t),u=!0)},o(t){w(n.$$.fragment,t),u=!1},d(t){t&&a(d),t&&a(f),E(n,t)}}}function Cc(y){let d,_,f,n,u,t,i,Wa,cr,Rn,Y,mr,aa,gr,fr,sa,ur,_r,Pn,T,mt,hr,Xa,$r,br,na,Ja,vr,xr,wr,be,ra,Ka,Er,Dr,yr,oa,la,jr,kr,Tr,Re,da,Ir,Sr,Ya,Br,Nr,Rr,W,Qa,Pr,Cr,Za,Lr,Or,es,Ar,Vr,ts,qr,Mr,Fr,Q,gt,zr,as,Ur,Gr,Pe,Hr,Z,ft,Wr,ss,Xr,Jr,Ce,Kr,ee,ut,Yr,ns,Qr,Zr,Le,eo,te,_t,to,rs,ao,so,Oe,no,Ae,ht,ro,os,oo,Cn,J,$t,lo,ls,io,po,ae,ds,co,mo,is,go,fo,ps,uo,_o,Ln,ve,bt,ho,cs,$o,On,xe,vt,bo,ms,vo,An,z,xt,xo,wt,wo,ia,Eo,Do,yo,Et,jo,pa,ko,To,Io,se,Dt,So,gs,Bo,No,we,fs,Ro,Po,us,Co,Lo,_s,Oo,Vn,R,yt,Ao,ne,jt,Vo,hs,qo,Mo,Ve,Fo,re,kt,zo,$s,Uo,Go,qe,Ho,oe,Tt,Wo,It,Xo,bs,Jo,Ko,Yo,Me,Qo,le,St,Zo,vs,el,tl,Fe,al,de,Bt,sl,xs,nl,rl,ze,ol,ie,Nt,ll,ws,dl,il,Ue,pl,Ge,Rt,cl,Es,ml,qn,C,Pt,gl,U,fl,Ds,ul,_l,ys,hl,$l,js,bl,vl,ks,xl,wl,El,pe,Ct,Dl,Ts,yl,jl,He,kl,ce,Lt,Tl,Is,Il,Sl,We,Bl,me,Ot,Nl,Ss,Rl,Pl,Xe,Cl,ge,At,Ll,Bs,Ol,Al,Je,Vl,fe,Vt,ql,Ns,Ml,Fl,Ke,Mn,V,qt,zl,ca,Rs,Ul,Gl,Hl,Mt,Wl,Ps,Xl,Jl,Kl,Cs,Yl,Ql,Ft,Ls,Ee,Fn,Zl,Os,ed,td,As,ad,sd,De,ye,ma,Vs,nd,rd,od,qs,ld,dd,Ms,id,pd,je,Fs,zs,cd,md,Us,gd,fd,Gs,ud,_d,ke,Hs,Ws,hd,$d,Xs,bd,vd,Js,xd,zn,G,zt,wd,Ks,Ed,Dd,Te,yd,Ys,jd,kd,Qs,Td,Id,Sd,Ye,Un,L,Ut,Bd,ga,Zs,Nd,Rd,Pd,en,Cd,Ld,K,fa,tn,Od,Ad,Vd,ua,an,qd,Md,Fd,_a,sn,zd,Ud,Gd,ha,nn,Hd,Wd,Xd,$a,Jd,rn,Kd,Yd,Gt,Qd,on,Zd,ei,ti,Qe,Gn,P,Ht,ai,ln,si,ni,Ze,ri,dn,oi,li,et,di,pn,ii,pi,tt,ci,at,Hn,Ie,Wt,mi,cn,gi,Wn,q,Xt,fi,mn,ui,_i,st,hi,nt,Jt,$i,gn,bi,vi,ue,Kt,xi,fn,wi,Ei,un,Di,Xn,Se,Yt,yi,_n,ji,Jn,H,Qt,ki,hn,Ti,Ii,rt,Si,ot,Zt,Bi,$n,Ni,Kn;return t=new cc({}),mt=new k({props:{name:"class datasets.DatasetBuilder",anchor:"datasets.DatasetBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"repo_id",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/builder.py#L177"}}),gt=new k({props:{name:"as_dataset",anchor:"datasets.DatasetBuilder.as_dataset",parameters:[{name:"split",val:": typing.Optional[datasets.splits.Split] = None"},{name:"run_post_process",val:" = True"},{name:"ignore_verifications",val:" = False"},{name:"in_memory",val:" = False"}],parametersDescription:[{anchor:"datasets.DatasetBuilder.as_dataset.split",description:"<strong>split</strong> (<code>datasets.Split</code>) &#x2014; Which subset of the data to return.",name:"split"},{anchor:"datasets.DatasetBuilder.as_dataset.run_post_process",description:`<strong>run_post_process</strong> (bool, default=True) &#x2014; Whether to run post-processing dataset transforms and/or add
indexes.`,name:"run_post_process"},{anchor:"datasets.DatasetBuilder.as_dataset.ignore_verifications",description:`<strong>ignore_verifications</strong> (bool, default=False) &#x2014; Whether to ignore the verifications of the
downloaded/processed dataset information (checksums/size/splits/&#x2026;).`,name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.as_dataset.in_memory",description:"<strong>in_memory</strong> (bool, default=False) &#x2014; Whether to copy the data in-memory.",name:"in_memory"}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/builder.py#L779",returnDescription:`
<p>datasets.Dataset</p>
`}}),Pe=new S({props:{anchor:"datasets.DatasetBuilder.as_dataset.example",$$slots:{default:[mc]},$$scope:{ctx:y}}}),ft=new k({props:{name:"download_and_prepare",anchor:"datasets.DatasetBuilder.download_and_prepare",parameters:[{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"download_mode",val:": typing.Optional[datasets.utils.download_manager.DownloadMode] = None"},{name:"ignore_verifications",val:": bool = False"},{name:"try_from_hf_gcs",val:": bool = True"},{name:"dl_manager",val:": typing.Optional[datasets.utils.download_manager.DownloadManager] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"**download_and_prepare_kwargs",val:""}],parametersDescription:[{anchor:"datasets.DatasetBuilder.download_and_prepare.download_config",description:'<strong>download_config</strong> (<a href="/docs/datasets/v2.2.2/en/package_reference/builder_classes#datasets.DownloadConfig">DownloadConfig</a>, optional) &#x2014; specific download configuration parameters.',name:"download_config"},{anchor:"datasets.DatasetBuilder.download_and_prepare.download_mode",description:'<strong>download_mode</strong> (<a href="/docs/datasets/v2.2.2/en/package_reference/builder_classes#datasets.DownloadMode">DownloadMode</a>, optional) &#x2014; select the download/generate mode - Default to <code>REUSE_DATASET_IF_EXISTS</code>',name:"download_mode"},{anchor:"datasets.DatasetBuilder.download_and_prepare.ignore_verifications",description:"<strong>ignore_verifications</strong> (<code>bool</code>) &#x2014; Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/&#x2026;)",name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.download_and_prepare.try_from_hf_gcs",description:"<strong>try_from_hf_gcs</strong> (<code>bool</code>) &#x2014; If True, it will try to download the already prepared dataset from the Hf google cloud storage",name:"try_from_hf_gcs"},{anchor:"datasets.DatasetBuilder.download_and_prepare.dl_manager",description:'<strong>dl_manager</strong> (<a href="/docs/datasets/v2.2.2/en/package_reference/builder_classes#datasets.DownloadManager">DownloadManager</a>, optional) &#x2014; specific Download Manger to use',name:"dl_manager"},{anchor:"datasets.DatasetBuilder.download_and_prepare.base_path",description:`<strong>base_path</strong> (<code>str</code>, optional) &#x2014; base path for relative paths that are used to download files. This can be a remote url.
If not specified, the value of the <em>base_path</em> attribute (<em>self.base_path</em>) will be used instead.`,name:"base_path"},{anchor:"datasets.DatasetBuilder.download_and_prepare.use_auth_token",description:`<strong>use_auth_token</strong> (<code>Union[str, bool]</code>, optional) &#x2014; Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.
If True, will get token from ~/.huggingface.`,name:"use_auth_token"}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/builder.py#L512"}}),Ce=new S({props:{anchor:"datasets.DatasetBuilder.download_and_prepare.example",$$slots:{default:[gc]},$$scope:{ctx:y}}}),ut=new k({props:{name:"get_all_exported_dataset_infos",anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos",parameters:[],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/builder.py#L324"}}),Le=new S({props:{anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos.example",$$slots:{default:[fc]},$$scope:{ctx:y}}}),_t=new k({props:{name:"get_exported_dataset_info",anchor:"datasets.DatasetBuilder.get_exported_dataset_info",parameters:[],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/builder.py#L342"}}),Oe=new S({props:{anchor:"datasets.DatasetBuilder.get_exported_dataset_info.example",$$slots:{default:[uc]},$$scope:{ctx:y}}}),ht=new k({props:{name:"get_imported_module_dir",anchor:"datasets.DatasetBuilder.get_imported_module_dir",parameters:[],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/builder.py#L507"}}),$t=new k({props:{name:"class datasets.GeneratorBasedBuilder",anchor:"datasets.GeneratorBasedBuilder",parameters:[{name:"*args",val:""},{name:"writer_batch_size",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/builder.py#L1066"}}),bt=new k({props:{name:"class datasets.BeamBasedBuilder",anchor:"datasets.BeamBasedBuilder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/builder.py#L1226"}}),vt=new k({props:{name:"class datasets.ArrowBasedBuilder",anchor:"datasets.ArrowBasedBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"repo_id",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/builder.py#L1164"}}),xt=new k({props:{name:"class datasets.BuilderConfig",anchor:"datasets.BuilderConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = '0.0.0'"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"datasets.BuilderConfig.name",description:"<strong>name</strong> (<code>str</code>, default <code>&quot;default&quot;</code>) &#x2014;",name:"name"},{anchor:"datasets.BuilderConfig.version",description:'<strong>version</strong> (<a href="/docs/datasets/v2.2.2/en/package_reference/builder_classes#datasets.Version">Version</a> or <code>str</code>, optional) &#x2014;',name:"version"},{anchor:"datasets.BuilderConfig.data_dir",description:"<strong>data_dir</strong> (<code>str</code>, optional) &#x2014;",name:"data_dir"},{anchor:"datasets.BuilderConfig.data_files",description:"<strong>data_files</strong> (<code>str</code> or <code>Sequence</code> or <code>Mapping</code>, optional) &#x2014; Path(s) to source data file(s).",name:"data_files"},{anchor:"datasets.BuilderConfig.description",description:"<strong>description</strong> (<code>str</code>, optional) &#x2014;",name:"description"}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/builder.py#L81"}}),Dt=new k({props:{name:"create_config_id",anchor:"datasets.BuilderConfig.create_config_id",parameters:[{name:"config_kwargs",val:": dict"},{name:"custom_features",val:": typing.Optional[datasets.features.features.Features] = None"}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/builder.py#L120"}}),yt=new k({props:{name:"class datasets.DownloadManager",anchor:"datasets.DownloadManager",parameters:[{name:"dataset_name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"record_checksums",val:" = True"}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/utils/download_manager.py#L141"}}),jt=new k({props:{name:"download",anchor:"datasets.DownloadManager.download",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/utils/download_manager.py#L265",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Ve=new S({props:{anchor:"datasets.DownloadManager.download.example",$$slots:{default:[_c]},$$scope:{ctx:y}}}),kt=new k({props:{name:"download_and_extract",anchor:"datasets.DownloadManager.download_and_extract",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/utils/download_manager.py#L400",returnDescription:`
<p><code>str</code>, extracted paths of given URL(s).</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),qe=new S({props:{anchor:"datasets.DownloadManager.download_and_extract.example",$$slots:{default:[hc]},$$scope:{ctx:y}}}),Tt=new k({props:{name:"download_custom",anchor:"datasets.DownloadManager.download_custom",parameters:[{name:"url_or_urls",val:""},{name:"custom_download",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/utils/download_manager.py#L218",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Me=new S({props:{anchor:"datasets.DownloadManager.download_custom.example",$$slots:{default:[$c]},$$scope:{ctx:y}}}),St=new k({props:{name:"extract",anchor:"datasets.DownloadManager.extract",parameters:[{name:"path_or_paths",val:""},{name:"num_proc",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/utils/download_manager.py#L363",returnDescription:`
<p><code>str</code>, The extracted paths matching the given input
path_or_paths.</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Fe=new S({props:{anchor:"datasets.DownloadManager.extract.example",$$slots:{default:[bc]},$$scope:{ctx:y}}}),Bt=new k({props:{name:"iter_archive",anchor:"datasets.DownloadManager.iter_archive",parameters:[{name:"path_or_buf",val:": typing.Union[str, _io.BufferedReader]"}],parametersDescription:[{anchor:"datasets.DownloadManager.iter_archive.path_or_buf",description:"<strong>path_or_buf</strong> (<code>str</code> or <code>io.BufferedReader</code>) &#x2014; Archive path or archive binary file object.",name:"path_or_buf"}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/utils/download_manager.py#L322"}}),ze=new S({props:{anchor:"datasets.DownloadManager.iter_archive.example",$$slots:{default:[vc]},$$scope:{ctx:y}}}),Nt=new k({props:{name:"iter_files",anchor:"datasets.DownloadManager.iter_files",parameters:[{name:"paths",val:": typing.Union[str, typing.List[str]]"}],parametersDescription:[{anchor:"datasets.DownloadManager.iter_files.paths",description:"<strong>paths</strong> (<code>str</code> or <code>list</code> of <code>str</code>) &#x2014; Root paths.",name:"paths"}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/utils/download_manager.py#L345"}}),Ue=new S({props:{anchor:"datasets.DownloadManager.iter_files.example",$$slots:{default:[xc]},$$scope:{ctx:y}}}),Rt=new k({props:{name:"ship_files_with_pipeline",anchor:"datasets.DownloadManager.ship_files_with_pipeline",parameters:[{name:"downloaded_path_or_paths",val:""},{name:"pipeline",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/utils/download_manager.py#L183"}}),Pt=new k({props:{name:"class datasets.StreamingDownloadManager",anchor:"datasets.StreamingDownloadManager",parameters:[{name:"dataset_name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"base_path",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/utils/streaming_download_manager.py#L766"}}),Ct=new k({props:{name:"download",anchor:"datasets.StreamingDownloadManager.download",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/utils/streaming_download_manager.py#L792",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),He=new S({props:{anchor:"datasets.StreamingDownloadManager.download.example",$$slots:{default:[wc]},$$scope:{ctx:y}}}),Lt=new k({props:{name:"download_and_extract",anchor:"datasets.StreamingDownloadManager.download_and_extract",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/utils/streaming_download_manager.py#L860",returnDescription:`
<p><code>str</code>, extracted paths of given URL(s).</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),We=new S({props:{anchor:"datasets.StreamingDownloadManager.download_and_extract.example",$$slots:{default:[Ec]},$$scope:{ctx:y}}}),Ot=new k({props:{name:"extract",anchor:"datasets.StreamingDownloadManager.extract",parameters:[{name:"path_or_paths",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/utils/streaming_download_manager.py#L819",returnDescription:`
<p><code>str</code>, The extracted paths matching the given input
path_or_paths.</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Xe=new S({props:{anchor:"datasets.StreamingDownloadManager.extract.example",$$slots:{default:[Dc]},$$scope:{ctx:y}}}),At=new k({props:{name:"iter_archive",anchor:"datasets.StreamingDownloadManager.iter_archive",parameters:[{name:"urlpath_or_buf",val:": typing.Union[str, _io.BufferedReader]"}],parametersDescription:[{anchor:"datasets.StreamingDownloadManager.iter_archive.urlpath_or_buf",description:"<strong>urlpath_or_buf</strong> (<code>str</code> or <code>io.BufferedReader</code>) &#x2014; Archive path or archive binary file object.",name:"urlpath_or_buf"}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/utils/streaming_download_manager.py#L878"}}),Je=new S({props:{anchor:"datasets.StreamingDownloadManager.iter_archive.example",$$slots:{default:[yc]},$$scope:{ctx:y}}}),Vt=new k({props:{name:"iter_files",anchor:"datasets.StreamingDownloadManager.iter_files",parameters:[{name:"urlpaths",val:": typing.Union[str, typing.List[str]]"}],parametersDescription:[{anchor:"datasets.StreamingDownloadManager.iter_files.urlpaths",description:"<strong>urlpaths</strong> (<code>str</code> or <code>list</code> of <code>str</code>) &#x2014; Root paths.",name:"urlpaths"}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/utils/streaming_download_manager.py#L901"}}),Ke=new S({props:{anchor:"datasets.StreamingDownloadManager.iter_files.example",$$slots:{default:[jc]},$$scope:{ctx:y}}}),qt=new k({props:{name:"class datasets.DownloadMode",anchor:"datasets.DownloadMode",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/utils/download_manager.py#L44"}}),zt=new k({props:{name:"class datasets.SplitGenerator",anchor:"datasets.SplitGenerator",parameters:[{name:"name",val:": str"},{name:"gen_kwargs",val:": typing.Dict = <factory>"}],parametersDescription:[{anchor:"datasets.SplitGenerator.name",description:`<strong>name</strong> (str) &#x2014; Name of the Split for which the generator will
create the examples.
**gen_kwargs &#x2014; Keyword arguments to forward to the <code>DatasetBuilder._generate_examples</code> method
of the builder.`,name:"name"}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/splits.py#L566"}}),Ye=new S({props:{anchor:"datasets.SplitGenerator.example",$$slots:{default:[kc]},$$scope:{ctx:y}}}),Ut=new k({props:{name:"class datasets.Split",anchor:"datasets.Split",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/splits.py#L387"}}),Qe=new S({props:{anchor:"datasets.Split.example",$$slots:{default:[Tc]},$$scope:{ctx:y}}}),Ht=new k({props:{name:"class datasets.NamedSplit",anchor:"datasets.NamedSplit",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/splits.py#L303"}}),Ze=new S({props:{anchor:"datasets.NamedSplit.example",$$slots:{default:[Ic]},$$scope:{ctx:y}}}),et=new S({props:{anchor:"datasets.NamedSplit.example-2",$$slots:{default:[Sc]},$$scope:{ctx:y}}}),tt=new S({props:{anchor:"datasets.NamedSplit.example-3",$$slots:{default:[Bc]},$$scope:{ctx:y}}}),at=new S({props:{anchor:"datasets.NamedSplit.example-4",$$slots:{default:[Nc]},$$scope:{ctx:y}}}),Wt=new k({props:{name:"class datasets.NamedSplitAll",anchor:"datasets.NamedSplitAll",parameters:[],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/splits.py#L372"}}),Xt=new k({props:{name:"class datasets.ReadInstruction",anchor:"datasets.ReadInstruction",parameters:[{name:"split_name",val:""},{name:"rounding",val:" = None"},{name:"from_",val:" = None"},{name:"to",val:" = None"},{name:"unit",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/arrow_reader.py#L456"}}),st=new S({props:{anchor:"datasets.ReadInstruction.example",$$slots:{default:[Rc]},$$scope:{ctx:y}}}),Jt=new k({props:{name:"from_spec",anchor:"datasets.ReadInstruction.from_spec",parameters:[{name:"spec",val:""}],parametersDescription:[{anchor:"datasets.ReadInstruction.from_spec.spec",description:`<strong>spec</strong> (str) &#x2014; split(s) + optional slice(s) to read + optional rounding
if percents are used as the slicing unit. A slice can be specified,
using absolute numbers (int) or percentages (int). E.g.
<code>test</code>: test split.
<code>test + validation</code>: test split + validation split.
<code>test[10:]</code>: test split, minus its first 10 records.
<code>test[:10%]</code>: first 10% records of test split.
<code>test[:20%](pct1_dropremainder)</code>: first 10% records, rounded with
the <code>pct1_dropremainder</code> rounding.
<code>test[:-5%]+train[40%:60%]</code>: first 95% of test + middle 20% of
train.`,name:"spec"}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/arrow_reader.py#L536",returnDescription:`
<p>ReadInstruction instance.</p>
`}}),Kt=new k({props:{name:"to_absolute",anchor:"datasets.ReadInstruction.to_absolute",parameters:[{name:"name2len",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/arrow_reader.py#L604",returnDescription:`
<p>list of _AbsoluteInstruction instances (corresponds to the + in spec).</p>
`}}),Yt=new k({props:{name:"class datasets.DownloadConfig",anchor:"datasets.DownloadConfig",parameters:[{name:"cache_dir",val:": typing.Union[pathlib.Path, str, NoneType] = None"},{name:"force_download",val:": bool = False"},{name:"resume_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"proxies",val:": typing.Optional[typing.Dict] = None"},{name:"user_agent",val:": typing.Optional[str] = None"},{name:"extract_compressed_file",val:": bool = False"},{name:"force_extract",val:": bool = False"},{name:"delete_extracted",val:": bool = False"},{name:"use_etag",val:": bool = True"},{name:"num_proc",val:": typing.Optional[int] = None"},{name:"max_retries",val:": int = 1"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"ignore_url_params",val:": bool = False"},{name:"download_desc",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"datasets.DownloadConfig.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>Path</code>, optional) &#x2014; Specify a cache directory to save the file to (overwrite the
default cache dir).`,name:"cache_dir"},{anchor:"datasets.DownloadConfig.force_download",description:`<strong>force_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, re-dowload the file even if it&#x2019;s already cached in
the cache dir.`,name:"force_download"},{anchor:"datasets.DownloadConfig.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, resume the download if incompletly recieved file is
found.`,name:"resume_download"},{anchor:"datasets.DownloadConfig.proxies",description:"<strong>proxies</strong> (<code>dict</code>, optional) &#x2014;",name:"proxies"},{anchor:"datasets.DownloadConfig.user_agent",description:`<strong>user_agent</strong> (<code>str</code>, optional) &#x2014; Optional string or dict that will be appended to the user-agent on remote
requests.`,name:"user_agent"},{anchor:"datasets.DownloadConfig.extract_compressed_file",description:`<strong>extract_compressed_file</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True and the path point to a zip or tar file,
extract the compressed file in a folder along the archive.`,name:"extract_compressed_file"},{anchor:"datasets.DownloadConfig.force_extract",description:`<strong>force_extract</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True when extract_compressed_file is True and the archive
was already extracted, re-extract the archive and override the folder where it was extracted.`,name:"force_extract"},{anchor:"datasets.DownloadConfig.delete_extracted",description:"<strong>delete_extracted</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to delete (or keep) the extracted files.",name:"delete_extracted"},{anchor:"datasets.DownloadConfig.use_etag",description:"<strong>use_etag</strong> (<code>bool</code>, default <code>True</code>) &#x2014; Whether to use the ETag HTTP response header to validate the cached files.",name:"use_etag"},{anchor:"datasets.DownloadConfig.num_proc",description:"<strong>num_proc</strong> (<code>int</code>, optional) &#x2014; The number of processes to launch to download the files in parallel.",name:"num_proc"},{anchor:"datasets.DownloadConfig.max_retries",description:"<strong>max_retries</strong> (<code>int</code>, default <code>1</code>) &#x2014; The number of times to retry an HTTP request if it fails.",name:"max_retries"},{anchor:"datasets.DownloadConfig.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, optional) &#x2014; Optional string or boolean to use as Bearer token
for remote files on the Datasets Hub. If True, will get token from ~/.huggingface.`,name:"use_auth_token"},{anchor:"datasets.DownloadConfig.ignore_url_params",description:`<strong>ignore_url_params</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to strip all query parameters and #fragments from
the download URL before using it for caching the file.`,name:"ignore_url_params"},{anchor:"datasets.DownloadConfig.download_desc",description:"<strong>download_desc</strong> (<code>str</code>, optional) &#x2014; A description to be displayed alongside with the progress bar while downloading the files.",name:"download_desc"}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/utils/file_utils.py#L153"}}),Qt=new k({props:{name:"class datasets.Version",anchor:"datasets.Version",parameters:[{name:"version_str",val:": str"},{name:"description",val:": typing.Optional[str] = None"},{name:"major",val:": typing.Union[str, int, NoneType] = None"},{name:"minor",val:": typing.Union[str, int, NoneType] = None"},{name:"patch",val:": typing.Union[str, int, NoneType] = None"}],parametersDescription:[{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.major",description:"<strong>major</strong> (<code>str</code>) &#x2014;",name:"major"},{anchor:"datasets.Version.minor",description:"<strong>minor</strong> (<code>str</code>) &#x2014;",name:"minor"},{anchor:"datasets.Version.patch",description:"<strong>patch</strong> (<code>str</code>) &#x2014;",name:"patch"}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/utils/version.py#L30"}}),rt=new S({props:{anchor:"datasets.Version.example",$$slots:{default:[Pc]},$$scope:{ctx:y}}}),Zt=new k({props:{name:"match",anchor:"datasets.Version.match",parameters:[{name:"other_version",val:""}],source:"https://github.com/huggingface/datasets/blob/2.2.2/src/datasets/utils/version.py#L98"}}),{c(){d=r("meta"),_=p(),f=r("h1"),n=r("a"),u=r("span"),$(t.$$.fragment),i=p(),Wa=r("span"),cr=m("Builder classes"),Rn=p(),Y=r("p"),mr=m("\u{1F917} Datasets relies on two main classes during the dataset building process: "),aa=r("a"),gr=m("DatasetBuilder"),fr=m(" and "),sa=r("a"),ur=m("BuilderConfig"),_r=m("."),Pn=p(),T=r("div"),$(mt.$$.fragment),hr=p(),Xa=r("p"),$r=m("Abstract base class for all datasets."),br=p(),na=r("p"),Ja=r("em"),vr=m("DatasetBuilder"),xr=m(" has 3 key methods:"),wr=p(),be=r("ul"),ra=r("li"),Ka=r("code"),Er=m("datasets.DatasetBuilder.info"),Dr=m(`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),yr=p(),oa=r("li"),la=r("a"),jr=m("datasets.DatasetBuilder.download_and_prepare()"),kr=m(`: Downloads the source data
and writes it to disk.`),Tr=p(),Re=r("li"),da=r("a"),Ir=m("datasets.DatasetBuilder.as_dataset()"),Sr=m(": Generates a "),Ya=r("em"),Br=m("Dataset"),Nr=m("."),Rr=p(),W=r("p"),Qa=r("strong"),Pr=m("Configuration"),Cr=m(": Some "),Za=r("em"),Lr=m("DatasetBuilder"),Or=m(`s expose multiple variants of the
dataset by defining a `),es=r("em"),Ar=m("datasets.BuilderConfig"),Vr=m(` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),ts=r("code"),qr=m("datasets.DatasetBuilder.builder_configs()"),Mr=m("."),Fr=p(),Q=r("div"),$(gt.$$.fragment),zr=p(),as=r("p"),Ur=m("Return a Dataset for the specified split."),Gr=p(),$(Pe.$$.fragment),Hr=p(),Z=r("div"),$(ft.$$.fragment),Wr=p(),ss=r("p"),Xr=m("Downloads and prepares dataset for reading."),Jr=p(),$(Ce.$$.fragment),Kr=p(),ee=r("div"),$(ut.$$.fragment),Yr=p(),ns=r("p"),Qr=m("Empty dict if doesn\u2019t exist"),Zr=p(),$(Le.$$.fragment),eo=p(),te=r("div"),$(_t.$$.fragment),to=p(),rs=r("p"),ao=m("Empty DatasetInfo if doesn\u2019t exist"),so=p(),$(Oe.$$.fragment),no=p(),Ae=r("div"),$(ht.$$.fragment),ro=p(),os=r("p"),oo=m("Return the path of the module of this class or subclass."),Cn=p(),J=r("div"),$($t.$$.fragment),lo=p(),ls=r("p"),io=m("Base class for datasets with data generation based on dict generators."),po=p(),ae=r("p"),ds=r("code"),co=m("GeneratorBasedBuilder"),mo=m(` is a convenience class that abstracts away much
of the data writing and reading of `),is=r("code"),go=m("DatasetBuilder"),fo=m(`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),ps=r("code"),uo=m("_split_generators"),_o=m("). See the method docstrings for details."),Ln=p(),ve=r("div"),$(bt.$$.fragment),ho=p(),cs=r("p"),$o=m("Beam based Builder."),On=p(),xe=r("div"),$(vt.$$.fragment),bo=p(),ms=r("p"),vo=m("Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),An=p(),z=r("div"),$(xt.$$.fragment),xo=p(),wt=r("p"),wo=m("Base class for "),ia=r("a"),Eo=m("DatasetBuilder"),Do=m(" data configuration."),yo=p(),Et=r("p"),jo=m(`DatasetBuilder subclasses with data configuration options should subclass
`),pa=r("a"),ko=m("BuilderConfig"),To=m(" and add their own properties."),Io=p(),se=r("div"),$(Dt.$$.fragment),So=p(),gs=r("p"),Bo=m(`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),No=p(),we=r("ul"),fs=r("li"),Ro=m("the config kwargs that can be used to overwrite attributes"),Po=p(),us=r("li"),Co=m("the custom features used to write the dataset"),Lo=p(),_s=r("li"),Oo=m(`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),Vn=p(),R=r("div"),$(yt.$$.fragment),Ao=p(),ne=r("div"),$(jt.$$.fragment),Vo=p(),hs=r("p"),qo=m("Download given url(s)."),Mo=p(),$(Ve.$$.fragment),Fo=p(),re=r("div"),$(kt.$$.fragment),zo=p(),$s=r("p"),Uo=m("Download and extract given url_or_urls."),Go=p(),$(qe.$$.fragment),Ho=p(),oe=r("div"),$(Tt.$$.fragment),Wo=p(),It=r("p"),Xo=m("Download given urls(s) by calling "),bs=r("code"),Jo=m("custom_download"),Ko=m("."),Yo=p(),$(Me.$$.fragment),Qo=p(),le=r("div"),$(St.$$.fragment),Zo=p(),vs=r("p"),el=m("Extract given path(s)."),tl=p(),$(Fe.$$.fragment),al=p(),de=r("div"),$(Bt.$$.fragment),sl=p(),xs=r("p"),nl=m("Iterate over files within an archive."),rl=p(),$(ze.$$.fragment),ol=p(),ie=r("div"),$(Nt.$$.fragment),ll=p(),ws=r("p"),dl=m("Iterate over file paths."),il=p(),$(Ue.$$.fragment),pl=p(),Ge=r("div"),$(Rt.$$.fragment),cl=p(),Es=r("p"),ml=m("Ship the files using Beam FileSystems to the pipeline temp dir."),qn=p(),C=r("div"),$(Pt.$$.fragment),gl=p(),U=r("p"),fl=m(`Download manager that uses the \u201D::\u201D separator to navigate through (possibly remote) compressed archives.
Contrary to the regular DownloadManager, the `),Ds=r("code"),ul=m("download"),_l=m(" and "),ys=r("code"),hl=m("extract"),$l=m(` methods don\u2019t actually download nor extract
data, but they rather return the path or url that could be opened using the `),js=r("code"),bl=m("xopen"),vl=m(` function which extends the
builtin `),ks=r("code"),xl=m("open"),wl=m(" function to stream data from remote files."),El=p(),pe=r("div"),$(Ct.$$.fragment),Dl=p(),Ts=r("p"),yl=m("Download given url(s)."),jl=p(),$(He.$$.fragment),kl=p(),ce=r("div"),$(Lt.$$.fragment),Tl=p(),Is=r("p"),Il=m("Download and extract given url_or_urls."),Sl=p(),$(We.$$.fragment),Bl=p(),me=r("div"),$(Ot.$$.fragment),Nl=p(),Ss=r("p"),Rl=m("Extract given path(s)."),Pl=p(),$(Xe.$$.fragment),Cl=p(),ge=r("div"),$(At.$$.fragment),Ll=p(),Bs=r("p"),Ol=m("Iterate over files within an archive."),Al=p(),$(Je.$$.fragment),Vl=p(),fe=r("div"),$(Vt.$$.fragment),ql=p(),Ns=r("p"),Ml=m("Iterate over files."),Fl=p(),$(Ke.$$.fragment),Mn=p(),V=r("div"),$(qt.$$.fragment),zl=p(),ca=r("p"),Rs=r("code"),Ul=m("Enum"),Gl=m(" for how to treat pre-existing downloads and data."),Hl=p(),Mt=r("p"),Wl=m("The default mode is "),Ps=r("code"),Xl=m("REUSE_DATASET_IF_EXISTS"),Jl=m(`, which will reuse both
raw downloads and the prepared dataset if they exist.`),Kl=p(),Cs=r("p"),Yl=m("The generations modes:"),Ql=p(),Ft=r("table"),Ls=r("thead"),Ee=r("tr"),Fn=r("th"),Zl=p(),Os=r("th"),ed=m("Downloads"),td=p(),As=r("th"),ad=m("Dataset"),sd=p(),De=r("tbody"),ye=r("tr"),ma=r("td"),Vs=r("code"),nd=m("REUSE_DATASET_IF_EXISTS"),rd=m(" (default)"),od=p(),qs=r("td"),ld=m("Reuse"),dd=p(),Ms=r("td"),id=m("Reuse"),pd=p(),je=r("tr"),Fs=r("td"),zs=r("code"),cd=m("REUSE_CACHE_IF_EXISTS"),md=p(),Us=r("td"),gd=m("Reuse"),fd=p(),Gs=r("td"),ud=m("Fresh"),_d=p(),ke=r("tr"),Hs=r("td"),Ws=r("code"),hd=m("FORCE_REDOWNLOAD"),$d=p(),Xs=r("td"),bd=m("Fresh"),vd=p(),Js=r("td"),xd=m("Fresh"),zn=p(),G=r("div"),$(zt.$$.fragment),wd=p(),Ks=r("p"),Ed=m("Defines the split information for the generator."),Dd=p(),Te=r("p"),yd=m(`This should be used as returned value of
`),Ys=r("code"),jd=m("GeneratorBasedBuilder._split_generators()"),kd=m(`.
See `),Qs=r("code"),Td=m("GeneratorBasedBuilder._split_generators()"),Id=m(` for more info and example
of usage.`),Sd=p(),$(Ye.$$.fragment),Un=p(),L=r("div"),$(Ut.$$.fragment),Bd=p(),ga=r("p"),Zs=r("code"),Nd=m("Enum"),Rd=m(" for dataset splits."),Pd=p(),en=r("p"),Cd=m(`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),Ld=p(),K=r("ul"),fa=r("li"),tn=r("code"),Od=m("TRAIN"),Ad=m(": the training data."),Vd=p(),ua=r("li"),an=r("code"),qd=m("VALIDATION"),Md=m(`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Fd=p(),_a=r("li"),sn=r("code"),zd=m("TEST"),Ud=m(`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Gd=p(),ha=r("li"),nn=r("code"),Hd=m("ALL"),Wd=m(": the union of all defined dataset splits."),Xd=p(),$a=r("p"),Jd=m("Note: All splits, including compositions inherit from "),rn=r("code"),Kd=m("datasets.SplitBase"),Yd=p(),Gt=r("p"),Qd=m("See the :doc:"),on=r("code"),Zd=m("guide on splits </loading>"),ei=m(" for more information."),ti=p(),$(Qe.$$.fragment),Gn=p(),P=r("div"),$(Ht.$$.fragment),ai=p(),ln=r("p"),si=m("Descriptor corresponding to a named split (train, test, \u2026)."),ni=p(),$(Ze.$$.fragment),ri=p(),dn=r("p"),oi=m("Warning:"),li=p(),$(et.$$.fragment),di=p(),pn=r("p"),ii=m("Warning:"),pi=p(),$(tt.$$.fragment),ci=p(),$(at.$$.fragment),Hn=p(),Ie=r("div"),$(Wt.$$.fragment),mi=p(),cn=r("p"),gi=m("Split corresponding to the union of all defined dataset splits."),Wn=p(),q=r("div"),$(Xt.$$.fragment),fi=p(),mn=r("p"),ui=m("Reading instruction for a dataset."),_i=p(),$(st.$$.fragment),hi=p(),nt=r("div"),$(Jt.$$.fragment),$i=p(),gn=r("p"),bi=m("Creates a ReadInstruction instance out of a string spec."),vi=p(),ue=r("div"),$(Kt.$$.fragment),xi=p(),fn=r("p"),wi=m("Translate instruction into a list of absolute instructions."),Ei=p(),un=r("p"),Di=m("Those absolute instructions are then to be added together."),Xn=p(),Se=r("div"),$(Yt.$$.fragment),yi=p(),_n=r("p"),ji=m("Configuration for our cached path manager."),Jn=p(),H=r("div"),$(Qt.$$.fragment),ki=p(),hn=r("p"),Ti=m("Dataset version MAJOR.MINOR.PATCH."),Ii=p(),$(rt.$$.fragment),Si=p(),ot=r("div"),$(Zt.$$.fragment),Bi=p(),$n=r("p"),Ni=m("Returns True if other_version matches."),this.h()},l(s){const h=ic('[data-svelte="svelte-1phssyn"]',document.head);d=o(h,"META",{name:!0,content:!0}),h.forEach(a),_=c(s),f=o(s,"H1",{class:!0});var ea=l(f);n=o(ea,"A",{id:!0,class:!0,href:!0});var bn=l(n);u=o(bn,"SPAN",{});var vn=l(u);b(t.$$.fragment,vn),vn.forEach(a),bn.forEach(a),i=c(ea),Wa=o(ea,"SPAN",{});var xn=l(Wa);cr=g(xn,"Builder classes"),xn.forEach(a),ea.forEach(a),Rn=c(s),Y=o(s,"P",{});var Be=l(Y);mr=g(Be,"\u{1F917} Datasets relies on two main classes during the dataset building process: "),aa=o(Be,"A",{href:!0});var wn=l(aa);gr=g(wn,"DatasetBuilder"),wn.forEach(a),fr=g(Be," and "),sa=o(Be,"A",{href:!0});var En=l(sa);ur=g(En,"BuilderConfig"),En.forEach(a),_r=g(Be,"."),Be.forEach(a),Pn=c(s),T=o(s,"DIV",{class:!0});var I=l(T);b(mt.$$.fragment,I),hr=c(I),Xa=o(I,"P",{});var Dn=l(Xa);$r=g(Dn,"Abstract base class for all datasets."),Dn.forEach(a),br=c(I),na=o(I,"P",{});var ba=l(na);Ja=o(ba,"EM",{});var yn=l(Ja);vr=g(yn,"DatasetBuilder"),yn.forEach(a),xr=g(ba," has 3 key methods:"),ba.forEach(a),wr=c(I),be=o(I,"UL",{});var Ne=l(be);ra=o(Ne,"LI",{});var va=l(ra);Ka=o(va,"CODE",{});var jn=l(Ka);Er=g(jn,"datasets.DatasetBuilder.info"),jn.forEach(a),Dr=g(va,`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),va.forEach(a),yr=c(Ne),oa=o(Ne,"LI",{});var xa=l(oa);la=o(xa,"A",{href:!0});var kn=l(la);jr=g(kn,"datasets.DatasetBuilder.download_and_prepare()"),kn.forEach(a),kr=g(xa,`: Downloads the source data
and writes it to disk.`),xa.forEach(a),Tr=c(Ne),Re=o(Ne,"LI",{});var lt=l(Re);da=o(lt,"A",{href:!0});var Tn=l(da);Ir=g(Tn,"datasets.DatasetBuilder.as_dataset()"),Tn.forEach(a),Sr=g(lt,": Generates a "),Ya=o(lt,"EM",{});var In=l(Ya);Br=g(In,"Dataset"),In.forEach(a),Nr=g(lt,"."),lt.forEach(a),Ne.forEach(a),Rr=c(I),W=o(I,"P",{});var X=l(W);Qa=o(X,"STRONG",{});var Sn=l(Qa);Pr=g(Sn,"Configuration"),Sn.forEach(a),Cr=g(X,": Some "),Za=o(X,"EM",{});var Bn=l(Za);Lr=g(Bn,"DatasetBuilder"),Bn.forEach(a),Or=g(X,`s expose multiple variants of the
dataset by defining a `),es=o(X,"EM",{});var Nn=l(es);Ar=g(Nn,"datasets.BuilderConfig"),Nn.forEach(a),Vr=g(X,` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),ts=o(X,"CODE",{});var Mi=l(ts);qr=g(Mi,"datasets.DatasetBuilder.builder_configs()"),Mi.forEach(a),Mr=g(X,"."),X.forEach(a),Fr=c(I),Q=o(I,"DIV",{class:!0});var wa=l(Q);b(gt.$$.fragment,wa),zr=c(wa),as=o(wa,"P",{});var Fi=l(as);Ur=g(Fi,"Return a Dataset for the specified split."),Fi.forEach(a),Gr=c(wa),b(Pe.$$.fragment,wa),wa.forEach(a),Hr=c(I),Z=o(I,"DIV",{class:!0});var Ea=l(Z);b(ft.$$.fragment,Ea),Wr=c(Ea),ss=o(Ea,"P",{});var zi=l(ss);Xr=g(zi,"Downloads and prepares dataset for reading."),zi.forEach(a),Jr=c(Ea),b(Ce.$$.fragment,Ea),Ea.forEach(a),Kr=c(I),ee=o(I,"DIV",{class:!0});var Da=l(ee);b(ut.$$.fragment,Da),Yr=c(Da),ns=o(Da,"P",{});var Ui=l(ns);Qr=g(Ui,"Empty dict if doesn\u2019t exist"),Ui.forEach(a),Zr=c(Da),b(Le.$$.fragment,Da),Da.forEach(a),eo=c(I),te=o(I,"DIV",{class:!0});var ya=l(te);b(_t.$$.fragment,ya),to=c(ya),rs=o(ya,"P",{});var Gi=l(rs);ao=g(Gi,"Empty DatasetInfo if doesn\u2019t exist"),Gi.forEach(a),so=c(ya),b(Oe.$$.fragment,ya),ya.forEach(a),no=c(I),Ae=o(I,"DIV",{class:!0});var Yn=l(Ae);b(ht.$$.fragment,Yn),ro=c(Yn),os=o(Yn,"P",{});var Hi=l(os);oo=g(Hi,"Return the path of the module of this class or subclass."),Hi.forEach(a),Yn.forEach(a),I.forEach(a),Cn=c(s),J=o(s,"DIV",{class:!0});var ja=l(J);b($t.$$.fragment,ja),lo=c(ja),ls=o(ja,"P",{});var Wi=l(ls);io=g(Wi,"Base class for datasets with data generation based on dict generators."),Wi.forEach(a),po=c(ja),ae=o(ja,"P",{});var ta=l(ae);ds=o(ta,"CODE",{});var Xi=l(ds);co=g(Xi,"GeneratorBasedBuilder"),Xi.forEach(a),mo=g(ta,` is a convenience class that abstracts away much
of the data writing and reading of `),is=o(ta,"CODE",{});var Ji=l(is);go=g(Ji,"DatasetBuilder"),Ji.forEach(a),fo=g(ta,`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),ps=o(ta,"CODE",{});var Ki=l(ps);uo=g(Ki,"_split_generators"),Ki.forEach(a),_o=g(ta,"). See the method docstrings for details."),ta.forEach(a),ja.forEach(a),Ln=c(s),ve=o(s,"DIV",{class:!0});var Qn=l(ve);b(bt.$$.fragment,Qn),ho=c(Qn),cs=o(Qn,"P",{});var Yi=l(cs);$o=g(Yi,"Beam based Builder."),Yi.forEach(a),Qn.forEach(a),On=c(s),xe=o(s,"DIV",{class:!0});var Zn=l(xe);b(vt.$$.fragment,Zn),bo=c(Zn),ms=o(Zn,"P",{});var Qi=l(ms);vo=g(Qi,"Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),Qi.forEach(a),Zn.forEach(a),An=c(s),z=o(s,"DIV",{class:!0});var dt=l(z);b(xt.$$.fragment,dt),xo=c(dt),wt=o(dt,"P",{});var er=l(wt);wo=g(er,"Base class for "),ia=o(er,"A",{href:!0});var Zi=l(ia);Eo=g(Zi,"DatasetBuilder"),Zi.forEach(a),Do=g(er," data configuration."),er.forEach(a),yo=c(dt),Et=o(dt,"P",{});var tr=l(Et);jo=g(tr,`DatasetBuilder subclasses with data configuration options should subclass
`),pa=o(tr,"A",{href:!0});var ep=l(pa);ko=g(ep,"BuilderConfig"),ep.forEach(a),To=g(tr," and add their own properties."),tr.forEach(a),Io=c(dt),se=o(dt,"DIV",{class:!0});var ka=l(se);b(Dt.$$.fragment,ka),So=c(ka),gs=o(ka,"P",{});var tp=l(gs);Bo=g(tp,`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),tp.forEach(a),No=c(ka),we=o(ka,"UL",{});var Ta=l(we);fs=o(Ta,"LI",{});var ap=l(fs);Ro=g(ap,"the config kwargs that can be used to overwrite attributes"),ap.forEach(a),Po=c(Ta),us=o(Ta,"LI",{});var sp=l(us);Co=g(sp,"the custom features used to write the dataset"),sp.forEach(a),Lo=c(Ta),_s=o(Ta,"LI",{});var np=l(_s);Oo=g(np,`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),np.forEach(a),Ta.forEach(a),ka.forEach(a),dt.forEach(a),Vn=c(s),R=o(s,"DIV",{class:!0});var O=l(R);b(yt.$$.fragment,O),Ao=c(O),ne=o(O,"DIV",{class:!0});var Ia=l(ne);b(jt.$$.fragment,Ia),Vo=c(Ia),hs=o(Ia,"P",{});var rp=l(hs);qo=g(rp,"Download given url(s)."),rp.forEach(a),Mo=c(Ia),b(Ve.$$.fragment,Ia),Ia.forEach(a),Fo=c(O),re=o(O,"DIV",{class:!0});var Sa=l(re);b(kt.$$.fragment,Sa),zo=c(Sa),$s=o(Sa,"P",{});var op=l($s);Uo=g(op,"Download and extract given url_or_urls."),op.forEach(a),Go=c(Sa),b(qe.$$.fragment,Sa),Sa.forEach(a),Ho=c(O),oe=o(O,"DIV",{class:!0});var Ba=l(oe);b(Tt.$$.fragment,Ba),Wo=c(Ba),It=o(Ba,"P",{});var ar=l(It);Xo=g(ar,"Download given urls(s) by calling "),bs=o(ar,"CODE",{});var lp=l(bs);Jo=g(lp,"custom_download"),lp.forEach(a),Ko=g(ar,"."),ar.forEach(a),Yo=c(Ba),b(Me.$$.fragment,Ba),Ba.forEach(a),Qo=c(O),le=o(O,"DIV",{class:!0});var Na=l(le);b(St.$$.fragment,Na),Zo=c(Na),vs=o(Na,"P",{});var dp=l(vs);el=g(dp,"Extract given path(s)."),dp.forEach(a),tl=c(Na),b(Fe.$$.fragment,Na),Na.forEach(a),al=c(O),de=o(O,"DIV",{class:!0});var Ra=l(de);b(Bt.$$.fragment,Ra),sl=c(Ra),xs=o(Ra,"P",{});var ip=l(xs);nl=g(ip,"Iterate over files within an archive."),ip.forEach(a),rl=c(Ra),b(ze.$$.fragment,Ra),Ra.forEach(a),ol=c(O),ie=o(O,"DIV",{class:!0});var Pa=l(ie);b(Nt.$$.fragment,Pa),ll=c(Pa),ws=o(Pa,"P",{});var pp=l(ws);dl=g(pp,"Iterate over file paths."),pp.forEach(a),il=c(Pa),b(Ue.$$.fragment,Pa),Pa.forEach(a),pl=c(O),Ge=o(O,"DIV",{class:!0});var sr=l(Ge);b(Rt.$$.fragment,sr),cl=c(sr),Es=o(sr,"P",{});var cp=l(Es);ml=g(cp,"Ship the files using Beam FileSystems to the pipeline temp dir."),cp.forEach(a),sr.forEach(a),O.forEach(a),qn=c(s),C=o(s,"DIV",{class:!0});var M=l(C);b(Pt.$$.fragment,M),gl=c(M),U=o(M,"P",{});var _e=l(U);fl=g(_e,`Download manager that uses the \u201D::\u201D separator to navigate through (possibly remote) compressed archives.
Contrary to the regular DownloadManager, the `),Ds=o(_e,"CODE",{});var mp=l(Ds);ul=g(mp,"download"),mp.forEach(a),_l=g(_e," and "),ys=o(_e,"CODE",{});var gp=l(ys);hl=g(gp,"extract"),gp.forEach(a),$l=g(_e,` methods don\u2019t actually download nor extract
data, but they rather return the path or url that could be opened using the `),js=o(_e,"CODE",{});var fp=l(js);bl=g(fp,"xopen"),fp.forEach(a),vl=g(_e,` function which extends the
builtin `),ks=o(_e,"CODE",{});var up=l(ks);xl=g(up,"open"),up.forEach(a),wl=g(_e," function to stream data from remote files."),_e.forEach(a),El=c(M),pe=o(M,"DIV",{class:!0});var Ca=l(pe);b(Ct.$$.fragment,Ca),Dl=c(Ca),Ts=o(Ca,"P",{});var _p=l(Ts);yl=g(_p,"Download given url(s)."),_p.forEach(a),jl=c(Ca),b(He.$$.fragment,Ca),Ca.forEach(a),kl=c(M),ce=o(M,"DIV",{class:!0});var La=l(ce);b(Lt.$$.fragment,La),Tl=c(La),Is=o(La,"P",{});var hp=l(Is);Il=g(hp,"Download and extract given url_or_urls."),hp.forEach(a),Sl=c(La),b(We.$$.fragment,La),La.forEach(a),Bl=c(M),me=o(M,"DIV",{class:!0});var Oa=l(me);b(Ot.$$.fragment,Oa),Nl=c(Oa),Ss=o(Oa,"P",{});var $p=l(Ss);Rl=g($p,"Extract given path(s)."),$p.forEach(a),Pl=c(Oa),b(Xe.$$.fragment,Oa),Oa.forEach(a),Cl=c(M),ge=o(M,"DIV",{class:!0});var Aa=l(ge);b(At.$$.fragment,Aa),Ll=c(Aa),Bs=o(Aa,"P",{});var bp=l(Bs);Ol=g(bp,"Iterate over files within an archive."),bp.forEach(a),Al=c(Aa),b(Je.$$.fragment,Aa),Aa.forEach(a),Vl=c(M),fe=o(M,"DIV",{class:!0});var Va=l(fe);b(Vt.$$.fragment,Va),ql=c(Va),Ns=o(Va,"P",{});var vp=l(Ns);Ml=g(vp,"Iterate over files."),vp.forEach(a),Fl=c(Va),b(Ke.$$.fragment,Va),Va.forEach(a),M.forEach(a),Mn=c(s),V=o(s,"DIV",{class:!0});var he=l(V);b(qt.$$.fragment,he),zl=c(he),ca=o(he,"P",{});var Ri=l(ca);Rs=o(Ri,"CODE",{});var xp=l(Rs);Ul=g(xp,"Enum"),xp.forEach(a),Gl=g(Ri," for how to treat pre-existing downloads and data."),Ri.forEach(a),Hl=c(he),Mt=o(he,"P",{});var nr=l(Mt);Wl=g(nr,"The default mode is "),Ps=o(nr,"CODE",{});var wp=l(Ps);Xl=g(wp,"REUSE_DATASET_IF_EXISTS"),wp.forEach(a),Jl=g(nr,`, which will reuse both
raw downloads and the prepared dataset if they exist.`),nr.forEach(a),Kl=c(he),Cs=o(he,"P",{});var Ep=l(Cs);Yl=g(Ep,"The generations modes:"),Ep.forEach(a),Ql=c(he),Ft=o(he,"TABLE",{});var rr=l(Ft);Ls=o(rr,"THEAD",{});var Dp=l(Ls);Ee=o(Dp,"TR",{});var qa=l(Ee);Fn=o(qa,"TH",{}),l(Fn).forEach(a),Zl=c(qa),Os=o(qa,"TH",{});var yp=l(Os);ed=g(yp,"Downloads"),yp.forEach(a),td=c(qa),As=o(qa,"TH",{});var jp=l(As);ad=g(jp,"Dataset"),jp.forEach(a),qa.forEach(a),Dp.forEach(a),sd=c(rr),De=o(rr,"TBODY",{});var Ma=l(De);ye=o(Ma,"TR",{});var Fa=l(ye);ma=o(Fa,"TD",{});var Pi=l(ma);Vs=o(Pi,"CODE",{});var kp=l(Vs);nd=g(kp,"REUSE_DATASET_IF_EXISTS"),kp.forEach(a),rd=g(Pi," (default)"),Pi.forEach(a),od=c(Fa),qs=o(Fa,"TD",{});var Tp=l(qs);ld=g(Tp,"Reuse"),Tp.forEach(a),dd=c(Fa),Ms=o(Fa,"TD",{});var Ip=l(Ms);id=g(Ip,"Reuse"),Ip.forEach(a),Fa.forEach(a),pd=c(Ma),je=o(Ma,"TR",{});var za=l(je);Fs=o(za,"TD",{});var Sp=l(Fs);zs=o(Sp,"CODE",{});var Bp=l(zs);cd=g(Bp,"REUSE_CACHE_IF_EXISTS"),Bp.forEach(a),Sp.forEach(a),md=c(za),Us=o(za,"TD",{});var Np=l(Us);gd=g(Np,"Reuse"),Np.forEach(a),fd=c(za),Gs=o(za,"TD",{});var Rp=l(Gs);ud=g(Rp,"Fresh"),Rp.forEach(a),za.forEach(a),_d=c(Ma),ke=o(Ma,"TR",{});var Ua=l(ke);Hs=o(Ua,"TD",{});var Pp=l(Hs);Ws=o(Pp,"CODE",{});var Cp=l(Ws);hd=g(Cp,"FORCE_REDOWNLOAD"),Cp.forEach(a),Pp.forEach(a),$d=c(Ua),Xs=o(Ua,"TD",{});var Lp=l(Xs);bd=g(Lp,"Fresh"),Lp.forEach(a),vd=c(Ua),Js=o(Ua,"TD",{});var Op=l(Js);xd=g(Op,"Fresh"),Op.forEach(a),Ua.forEach(a),Ma.forEach(a),rr.forEach(a),he.forEach(a),zn=c(s),G=o(s,"DIV",{class:!0});var it=l(G);b(zt.$$.fragment,it),wd=c(it),Ks=o(it,"P",{});var Ap=l(Ks);Ed=g(Ap,"Defines the split information for the generator."),Ap.forEach(a),Dd=c(it),Te=o(it,"P",{});var Ga=l(Te);yd=g(Ga,`This should be used as returned value of
`),Ys=o(Ga,"CODE",{});var Vp=l(Ys);jd=g(Vp,"GeneratorBasedBuilder._split_generators()"),Vp.forEach(a),kd=g(Ga,`.
See `),Qs=o(Ga,"CODE",{});var qp=l(Qs);Td=g(qp,"GeneratorBasedBuilder._split_generators()"),qp.forEach(a),Id=g(Ga,` for more info and example
of usage.`),Ga.forEach(a),Sd=c(it),b(Ye.$$.fragment,it),it.forEach(a),Un=c(s),L=o(s,"DIV",{class:!0});var F=l(L);b(Ut.$$.fragment,F),Bd=c(F),ga=o(F,"P",{});var Ci=l(ga);Zs=o(Ci,"CODE",{});var Mp=l(Zs);Nd=g(Mp,"Enum"),Mp.forEach(a),Rd=g(Ci," for dataset splits."),Ci.forEach(a),Pd=c(F),en=o(F,"P",{});var Fp=l(en);Cd=g(Fp,`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),Fp.forEach(a),Ld=c(F),K=o(F,"UL",{});var pt=l(K);fa=o(pt,"LI",{});var Li=l(fa);tn=o(Li,"CODE",{});var zp=l(tn);Od=g(zp,"TRAIN"),zp.forEach(a),Ad=g(Li,": the training data."),Li.forEach(a),Vd=c(pt),ua=o(pt,"LI",{});var Oi=l(ua);an=o(Oi,"CODE",{});var Up=l(an);qd=g(Up,"VALIDATION"),Up.forEach(a),Md=g(Oi,`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Oi.forEach(a),Fd=c(pt),_a=o(pt,"LI",{});var Ai=l(_a);sn=o(Ai,"CODE",{});var Gp=l(sn);zd=g(Gp,"TEST"),Gp.forEach(a),Ud=g(Ai,`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Ai.forEach(a),Gd=c(pt),ha=o(pt,"LI",{});var Vi=l(ha);nn=o(Vi,"CODE",{});var Hp=l(nn);Hd=g(Hp,"ALL"),Hp.forEach(a),Wd=g(Vi,": the union of all defined dataset splits."),Vi.forEach(a),pt.forEach(a),Xd=c(F),$a=o(F,"P",{});var qi=l($a);Jd=g(qi,"Note: All splits, including compositions inherit from "),rn=o(qi,"CODE",{});var Wp=l(rn);Kd=g(Wp,"datasets.SplitBase"),Wp.forEach(a),qi.forEach(a),Yd=c(F),Gt=o(F,"P",{});var or=l(Gt);Qd=g(or,"See the :doc:"),on=o(or,"CODE",{});var Xp=l(on);Zd=g(Xp,"guide on splits </loading>"),Xp.forEach(a),ei=g(or," for more information."),or.forEach(a),ti=c(F),b(Qe.$$.fragment,F),F.forEach(a),Gn=c(s),P=o(s,"DIV",{class:!0});var A=l(P);b(Ht.$$.fragment,A),ai=c(A),ln=o(A,"P",{});var Jp=l(ln);si=g(Jp,"Descriptor corresponding to a named split (train, test, \u2026)."),Jp.forEach(a),ni=c(A),b(Ze.$$.fragment,A),ri=c(A),dn=o(A,"P",{});var Kp=l(dn);oi=g(Kp,"Warning:"),Kp.forEach(a),li=c(A),b(et.$$.fragment,A),di=c(A),pn=o(A,"P",{});var Yp=l(pn);ii=g(Yp,"Warning:"),Yp.forEach(a),pi=c(A),b(tt.$$.fragment,A),ci=c(A),b(at.$$.fragment,A),A.forEach(a),Hn=c(s),Ie=o(s,"DIV",{class:!0});var lr=l(Ie);b(Wt.$$.fragment,lr),mi=c(lr),cn=o(lr,"P",{});var Qp=l(cn);gi=g(Qp,"Split corresponding to the union of all defined dataset splits."),Qp.forEach(a),lr.forEach(a),Wn=c(s),q=o(s,"DIV",{class:!0});var $e=l(q);b(Xt.$$.fragment,$e),fi=c($e),mn=o($e,"P",{});var Zp=l(mn);ui=g(Zp,"Reading instruction for a dataset."),Zp.forEach(a),_i=c($e),b(st.$$.fragment,$e),hi=c($e),nt=o($e,"DIV",{class:!0});var dr=l(nt);b(Jt.$$.fragment,dr),$i=c(dr),gn=o(dr,"P",{});var ec=l(gn);bi=g(ec,"Creates a ReadInstruction instance out of a string spec."),ec.forEach(a),dr.forEach(a),vi=c($e),ue=o($e,"DIV",{class:!0});var Ha=l(ue);b(Kt.$$.fragment,Ha),xi=c(Ha),fn=o(Ha,"P",{});var tc=l(fn);wi=g(tc,"Translate instruction into a list of absolute instructions."),tc.forEach(a),Ei=c(Ha),un=o(Ha,"P",{});var ac=l(un);Di=g(ac,"Those absolute instructions are then to be added together."),ac.forEach(a),Ha.forEach(a),$e.forEach(a),Xn=c(s),Se=o(s,"DIV",{class:!0});var ir=l(Se);b(Yt.$$.fragment,ir),yi=c(ir),_n=o(ir,"P",{});var sc=l(_n);ji=g(sc,"Configuration for our cached path manager."),sc.forEach(a),ir.forEach(a),Jn=c(s),H=o(s,"DIV",{class:!0});var ct=l(H);b(Qt.$$.fragment,ct),ki=c(ct),hn=o(ct,"P",{});var nc=l(hn);Ti=g(nc,"Dataset version MAJOR.MINOR.PATCH."),nc.forEach(a),Ii=c(ct),b(rt.$$.fragment,ct),Si=c(ct),ot=o(ct,"DIV",{class:!0});var pr=l(ot);b(Zt.$$.fragment,pr),Bi=c(pr),$n=o(pr,"P",{});var rc=l($n);Ni=g(rc,"Returns True if other_version matches."),rc.forEach(a),pr.forEach(a),ct.forEach(a),this.h()},h(){j(d,"name","hf:doc:metadata"),j(d,"content",JSON.stringify(Lc)),j(n,"id","datasets.DatasetBuilder"),j(n,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(n,"href","#datasets.DatasetBuilder"),j(f,"class","relative group"),j(aa,"href","/docs/datasets/v2.2.2/en/package_reference/builder_classes#datasets.DatasetBuilder"),j(sa,"href","/docs/datasets/v2.2.2/en/package_reference/builder_classes#datasets.BuilderConfig"),j(la,"href","/docs/datasets/v2.2.2/en/package_reference/builder_classes#datasets.DatasetBuilder.download_and_prepare"),j(da,"href","/docs/datasets/v2.2.2/en/package_reference/builder_classes#datasets.DatasetBuilder.as_dataset"),j(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(Ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(ia,"href","/docs/datasets/v2.2.2/en/package_reference/builder_classes#datasets.DatasetBuilder"),j(pa,"href","/docs/datasets/v2.2.2/en/package_reference/builder_classes#datasets.BuilderConfig"),j(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(Ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(nt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(Se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(ot,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(s,h){e(document.head,d),D(s,_,h),D(s,f,h),e(f,n),e(n,u),v(t,u,null),e(f,i),e(f,Wa),e(Wa,cr),D(s,Rn,h),D(s,Y,h),e(Y,mr),e(Y,aa),e(aa,gr),e(Y,fr),e(Y,sa),e(sa,ur),e(Y,_r),D(s,Pn,h),D(s,T,h),v(mt,T,null),e(T,hr),e(T,Xa),e(Xa,$r),e(T,br),e(T,na),e(na,Ja),e(Ja,vr),e(na,xr),e(T,wr),e(T,be),e(be,ra),e(ra,Ka),e(Ka,Er),e(ra,Dr),e(be,yr),e(be,oa),e(oa,la),e(la,jr),e(oa,kr),e(be,Tr),e(be,Re),e(Re,da),e(da,Ir),e(Re,Sr),e(Re,Ya),e(Ya,Br),e(Re,Nr),e(T,Rr),e(T,W),e(W,Qa),e(Qa,Pr),e(W,Cr),e(W,Za),e(Za,Lr),e(W,Or),e(W,es),e(es,Ar),e(W,Vr),e(W,ts),e(ts,qr),e(W,Mr),e(T,Fr),e(T,Q),v(gt,Q,null),e(Q,zr),e(Q,as),e(as,Ur),e(Q,Gr),v(Pe,Q,null),e(T,Hr),e(T,Z),v(ft,Z,null),e(Z,Wr),e(Z,ss),e(ss,Xr),e(Z,Jr),v(Ce,Z,null),e(T,Kr),e(T,ee),v(ut,ee,null),e(ee,Yr),e(ee,ns),e(ns,Qr),e(ee,Zr),v(Le,ee,null),e(T,eo),e(T,te),v(_t,te,null),e(te,to),e(te,rs),e(rs,ao),e(te,so),v(Oe,te,null),e(T,no),e(T,Ae),v(ht,Ae,null),e(Ae,ro),e(Ae,os),e(os,oo),D(s,Cn,h),D(s,J,h),v($t,J,null),e(J,lo),e(J,ls),e(ls,io),e(J,po),e(J,ae),e(ae,ds),e(ds,co),e(ae,mo),e(ae,is),e(is,go),e(ae,fo),e(ae,ps),e(ps,uo),e(ae,_o),D(s,Ln,h),D(s,ve,h),v(bt,ve,null),e(ve,ho),e(ve,cs),e(cs,$o),D(s,On,h),D(s,xe,h),v(vt,xe,null),e(xe,bo),e(xe,ms),e(ms,vo),D(s,An,h),D(s,z,h),v(xt,z,null),e(z,xo),e(z,wt),e(wt,wo),e(wt,ia),e(ia,Eo),e(wt,Do),e(z,yo),e(z,Et),e(Et,jo),e(Et,pa),e(pa,ko),e(Et,To),e(z,Io),e(z,se),v(Dt,se,null),e(se,So),e(se,gs),e(gs,Bo),e(se,No),e(se,we),e(we,fs),e(fs,Ro),e(we,Po),e(we,us),e(us,Co),e(we,Lo),e(we,_s),e(_s,Oo),D(s,Vn,h),D(s,R,h),v(yt,R,null),e(R,Ao),e(R,ne),v(jt,ne,null),e(ne,Vo),e(ne,hs),e(hs,qo),e(ne,Mo),v(Ve,ne,null),e(R,Fo),e(R,re),v(kt,re,null),e(re,zo),e(re,$s),e($s,Uo),e(re,Go),v(qe,re,null),e(R,Ho),e(R,oe),v(Tt,oe,null),e(oe,Wo),e(oe,It),e(It,Xo),e(It,bs),e(bs,Jo),e(It,Ko),e(oe,Yo),v(Me,oe,null),e(R,Qo),e(R,le),v(St,le,null),e(le,Zo),e(le,vs),e(vs,el),e(le,tl),v(Fe,le,null),e(R,al),e(R,de),v(Bt,de,null),e(de,sl),e(de,xs),e(xs,nl),e(de,rl),v(ze,de,null),e(R,ol),e(R,ie),v(Nt,ie,null),e(ie,ll),e(ie,ws),e(ws,dl),e(ie,il),v(Ue,ie,null),e(R,pl),e(R,Ge),v(Rt,Ge,null),e(Ge,cl),e(Ge,Es),e(Es,ml),D(s,qn,h),D(s,C,h),v(Pt,C,null),e(C,gl),e(C,U),e(U,fl),e(U,Ds),e(Ds,ul),e(U,_l),e(U,ys),e(ys,hl),e(U,$l),e(U,js),e(js,bl),e(U,vl),e(U,ks),e(ks,xl),e(U,wl),e(C,El),e(C,pe),v(Ct,pe,null),e(pe,Dl),e(pe,Ts),e(Ts,yl),e(pe,jl),v(He,pe,null),e(C,kl),e(C,ce),v(Lt,ce,null),e(ce,Tl),e(ce,Is),e(Is,Il),e(ce,Sl),v(We,ce,null),e(C,Bl),e(C,me),v(Ot,me,null),e(me,Nl),e(me,Ss),e(Ss,Rl),e(me,Pl),v(Xe,me,null),e(C,Cl),e(C,ge),v(At,ge,null),e(ge,Ll),e(ge,Bs),e(Bs,Ol),e(ge,Al),v(Je,ge,null),e(C,Vl),e(C,fe),v(Vt,fe,null),e(fe,ql),e(fe,Ns),e(Ns,Ml),e(fe,Fl),v(Ke,fe,null),D(s,Mn,h),D(s,V,h),v(qt,V,null),e(V,zl),e(V,ca),e(ca,Rs),e(Rs,Ul),e(ca,Gl),e(V,Hl),e(V,Mt),e(Mt,Wl),e(Mt,Ps),e(Ps,Xl),e(Mt,Jl),e(V,Kl),e(V,Cs),e(Cs,Yl),e(V,Ql),e(V,Ft),e(Ft,Ls),e(Ls,Ee),e(Ee,Fn),e(Ee,Zl),e(Ee,Os),e(Os,ed),e(Ee,td),e(Ee,As),e(As,ad),e(Ft,sd),e(Ft,De),e(De,ye),e(ye,ma),e(ma,Vs),e(Vs,nd),e(ma,rd),e(ye,od),e(ye,qs),e(qs,ld),e(ye,dd),e(ye,Ms),e(Ms,id),e(De,pd),e(De,je),e(je,Fs),e(Fs,zs),e(zs,cd),e(je,md),e(je,Us),e(Us,gd),e(je,fd),e(je,Gs),e(Gs,ud),e(De,_d),e(De,ke),e(ke,Hs),e(Hs,Ws),e(Ws,hd),e(ke,$d),e(ke,Xs),e(Xs,bd),e(ke,vd),e(ke,Js),e(Js,xd),D(s,zn,h),D(s,G,h),v(zt,G,null),e(G,wd),e(G,Ks),e(Ks,Ed),e(G,Dd),e(G,Te),e(Te,yd),e(Te,Ys),e(Ys,jd),e(Te,kd),e(Te,Qs),e(Qs,Td),e(Te,Id),e(G,Sd),v(Ye,G,null),D(s,Un,h),D(s,L,h),v(Ut,L,null),e(L,Bd),e(L,ga),e(ga,Zs),e(Zs,Nd),e(ga,Rd),e(L,Pd),e(L,en),e(en,Cd),e(L,Ld),e(L,K),e(K,fa),e(fa,tn),e(tn,Od),e(fa,Ad),e(K,Vd),e(K,ua),e(ua,an),e(an,qd),e(ua,Md),e(K,Fd),e(K,_a),e(_a,sn),e(sn,zd),e(_a,Ud),e(K,Gd),e(K,ha),e(ha,nn),e(nn,Hd),e(ha,Wd),e(L,Xd),e(L,$a),e($a,Jd),e($a,rn),e(rn,Kd),e(L,Yd),e(L,Gt),e(Gt,Qd),e(Gt,on),e(on,Zd),e(Gt,ei),e(L,ti),v(Qe,L,null),D(s,Gn,h),D(s,P,h),v(Ht,P,null),e(P,ai),e(P,ln),e(ln,si),e(P,ni),v(Ze,P,null),e(P,ri),e(P,dn),e(dn,oi),e(P,li),v(et,P,null),e(P,di),e(P,pn),e(pn,ii),e(P,pi),v(tt,P,null),e(P,ci),v(at,P,null),D(s,Hn,h),D(s,Ie,h),v(Wt,Ie,null),e(Ie,mi),e(Ie,cn),e(cn,gi),D(s,Wn,h),D(s,q,h),v(Xt,q,null),e(q,fi),e(q,mn),e(mn,ui),e(q,_i),v(st,q,null),e(q,hi),e(q,nt),v(Jt,nt,null),e(nt,$i),e(nt,gn),e(gn,bi),e(q,vi),e(q,ue),v(Kt,ue,null),e(ue,xi),e(ue,fn),e(fn,wi),e(ue,Ei),e(ue,un),e(un,Di),D(s,Xn,h),D(s,Se,h),v(Yt,Se,null),e(Se,yi),e(Se,_n),e(_n,ji),D(s,Jn,h),D(s,H,h),v(Qt,H,null),e(H,ki),e(H,hn),e(hn,Ti),e(H,Ii),v(rt,H,null),e(H,Si),e(H,ot),v(Zt,ot,null),e(ot,Bi),e(ot,$n),e($n,Ni),Kn=!0},p(s,[h]){const ea={};h&2&&(ea.$$scope={dirty:h,ctx:s}),Pe.$set(ea);const bn={};h&2&&(bn.$$scope={dirty:h,ctx:s}),Ce.$set(bn);const vn={};h&2&&(vn.$$scope={dirty:h,ctx:s}),Le.$set(vn);const xn={};h&2&&(xn.$$scope={dirty:h,ctx:s}),Oe.$set(xn);const Be={};h&2&&(Be.$$scope={dirty:h,ctx:s}),Ve.$set(Be);const wn={};h&2&&(wn.$$scope={dirty:h,ctx:s}),qe.$set(wn);const En={};h&2&&(En.$$scope={dirty:h,ctx:s}),Me.$set(En);const I={};h&2&&(I.$$scope={dirty:h,ctx:s}),Fe.$set(I);const Dn={};h&2&&(Dn.$$scope={dirty:h,ctx:s}),ze.$set(Dn);const ba={};h&2&&(ba.$$scope={dirty:h,ctx:s}),Ue.$set(ba);const yn={};h&2&&(yn.$$scope={dirty:h,ctx:s}),He.$set(yn);const Ne={};h&2&&(Ne.$$scope={dirty:h,ctx:s}),We.$set(Ne);const va={};h&2&&(va.$$scope={dirty:h,ctx:s}),Xe.$set(va);const jn={};h&2&&(jn.$$scope={dirty:h,ctx:s}),Je.$set(jn);const xa={};h&2&&(xa.$$scope={dirty:h,ctx:s}),Ke.$set(xa);const kn={};h&2&&(kn.$$scope={dirty:h,ctx:s}),Ye.$set(kn);const lt={};h&2&&(lt.$$scope={dirty:h,ctx:s}),Qe.$set(lt);const Tn={};h&2&&(Tn.$$scope={dirty:h,ctx:s}),Ze.$set(Tn);const In={};h&2&&(In.$$scope={dirty:h,ctx:s}),et.$set(In);const X={};h&2&&(X.$$scope={dirty:h,ctx:s}),tt.$set(X);const Sn={};h&2&&(Sn.$$scope={dirty:h,ctx:s}),at.$set(Sn);const Bn={};h&2&&(Bn.$$scope={dirty:h,ctx:s}),st.$set(Bn);const Nn={};h&2&&(Nn.$$scope={dirty:h,ctx:s}),rt.$set(Nn)},i(s){Kn||(x(t.$$.fragment,s),x(mt.$$.fragment,s),x(gt.$$.fragment,s),x(Pe.$$.fragment,s),x(ft.$$.fragment,s),x(Ce.$$.fragment,s),x(ut.$$.fragment,s),x(Le.$$.fragment,s),x(_t.$$.fragment,s),x(Oe.$$.fragment,s),x(ht.$$.fragment,s),x($t.$$.fragment,s),x(bt.$$.fragment,s),x(vt.$$.fragment,s),x(xt.$$.fragment,s),x(Dt.$$.fragment,s),x(yt.$$.fragment,s),x(jt.$$.fragment,s),x(Ve.$$.fragment,s),x(kt.$$.fragment,s),x(qe.$$.fragment,s),x(Tt.$$.fragment,s),x(Me.$$.fragment,s),x(St.$$.fragment,s),x(Fe.$$.fragment,s),x(Bt.$$.fragment,s),x(ze.$$.fragment,s),x(Nt.$$.fragment,s),x(Ue.$$.fragment,s),x(Rt.$$.fragment,s),x(Pt.$$.fragment,s),x(Ct.$$.fragment,s),x(He.$$.fragment,s),x(Lt.$$.fragment,s),x(We.$$.fragment,s),x(Ot.$$.fragment,s),x(Xe.$$.fragment,s),x(At.$$.fragment,s),x(Je.$$.fragment,s),x(Vt.$$.fragment,s),x(Ke.$$.fragment,s),x(qt.$$.fragment,s),x(zt.$$.fragment,s),x(Ye.$$.fragment,s),x(Ut.$$.fragment,s),x(Qe.$$.fragment,s),x(Ht.$$.fragment,s),x(Ze.$$.fragment,s),x(et.$$.fragment,s),x(tt.$$.fragment,s),x(at.$$.fragment,s),x(Wt.$$.fragment,s),x(Xt.$$.fragment,s),x(st.$$.fragment,s),x(Jt.$$.fragment,s),x(Kt.$$.fragment,s),x(Yt.$$.fragment,s),x(Qt.$$.fragment,s),x(rt.$$.fragment,s),x(Zt.$$.fragment,s),Kn=!0)},o(s){w(t.$$.fragment,s),w(mt.$$.fragment,s),w(gt.$$.fragment,s),w(Pe.$$.fragment,s),w(ft.$$.fragment,s),w(Ce.$$.fragment,s),w(ut.$$.fragment,s),w(Le.$$.fragment,s),w(_t.$$.fragment,s),w(Oe.$$.fragment,s),w(ht.$$.fragment,s),w($t.$$.fragment,s),w(bt.$$.fragment,s),w(vt.$$.fragment,s),w(xt.$$.fragment,s),w(Dt.$$.fragment,s),w(yt.$$.fragment,s),w(jt.$$.fragment,s),w(Ve.$$.fragment,s),w(kt.$$.fragment,s),w(qe.$$.fragment,s),w(Tt.$$.fragment,s),w(Me.$$.fragment,s),w(St.$$.fragment,s),w(Fe.$$.fragment,s),w(Bt.$$.fragment,s),w(ze.$$.fragment,s),w(Nt.$$.fragment,s),w(Ue.$$.fragment,s),w(Rt.$$.fragment,s),w(Pt.$$.fragment,s),w(Ct.$$.fragment,s),w(He.$$.fragment,s),w(Lt.$$.fragment,s),w(We.$$.fragment,s),w(Ot.$$.fragment,s),w(Xe.$$.fragment,s),w(At.$$.fragment,s),w(Je.$$.fragment,s),w(Vt.$$.fragment,s),w(Ke.$$.fragment,s),w(qt.$$.fragment,s),w(zt.$$.fragment,s),w(Ye.$$.fragment,s),w(Ut.$$.fragment,s),w(Qe.$$.fragment,s),w(Ht.$$.fragment,s),w(Ze.$$.fragment,s),w(et.$$.fragment,s),w(tt.$$.fragment,s),w(at.$$.fragment,s),w(Wt.$$.fragment,s),w(Xt.$$.fragment,s),w(st.$$.fragment,s),w(Jt.$$.fragment,s),w(Kt.$$.fragment,s),w(Yt.$$.fragment,s),w(Qt.$$.fragment,s),w(rt.$$.fragment,s),w(Zt.$$.fragment,s),Kn=!1},d(s){a(d),s&&a(_),s&&a(f),E(t),s&&a(Rn),s&&a(Y),s&&a(Pn),s&&a(T),E(mt),E(gt),E(Pe),E(ft),E(Ce),E(ut),E(Le),E(_t),E(Oe),E(ht),s&&a(Cn),s&&a(J),E($t),s&&a(Ln),s&&a(ve),E(bt),s&&a(On),s&&a(xe),E(vt),s&&a(An),s&&a(z),E(xt),E(Dt),s&&a(Vn),s&&a(R),E(yt),E(jt),E(Ve),E(kt),E(qe),E(Tt),E(Me),E(St),E(Fe),E(Bt),E(ze),E(Nt),E(Ue),E(Rt),s&&a(qn),s&&a(C),E(Pt),E(Ct),E(He),E(Lt),E(We),E(Ot),E(Xe),E(At),E(Je),E(Vt),E(Ke),s&&a(Mn),s&&a(V),E(qt),s&&a(zn),s&&a(G),E(zt),E(Ye),s&&a(Un),s&&a(L),E(Ut),E(Qe),s&&a(Gn),s&&a(P),E(Ht),E(Ze),E(et),E(tt),E(at),s&&a(Hn),s&&a(Ie),E(Wt),s&&a(Wn),s&&a(q),E(Xt),E(st),E(Jt),E(Kt),s&&a(Xn),s&&a(Se),E(Yt),s&&a(Jn),s&&a(H),E(Qt),E(rt),E(Zt)}}}const Lc={local:"datasets.DatasetBuilder",title:"Builder classes"};function Oc(y){return pc(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class zc extends oc{constructor(d){super();lc(this,d,Oc,Cc,dc,{})}}export{zc as default,Lc as metadata};
