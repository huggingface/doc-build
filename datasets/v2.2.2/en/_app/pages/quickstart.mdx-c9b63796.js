import{S as De,i as Be,s as Le,e as o,k as g,w as y,t as l,M as Ne,c,d as e,m as $,a as h,x,h as r,b as j,F as a,g as d,y as v,q as E,o as T,B as A,v as Re,L as Ks}from"../chunks/vendor-8138ceec.js";import{T as Oe}from"../chunks/Tip-12722dfc.js";import{I as Xs}from"../chunks/IconCopyLink-2dd3a6ac.js";import{C as gs}from"../chunks/CodeBlock-fc89709f.js";import{F as le,M as Vs}from"../chunks/Markdown-7202589c.js";import"../chunks/IconTensorflow-7f573d67.js";function Me(C){let t,p,n,m,b;return{c(){t=o("p"),p=l("For more detailed information on loading and processing a dataset, take a look at "),n=o("a"),m=l("Chapter 3"),b=l(" of the Hugging Face course! It covers additional important topics like dynamic padding, and fine-tuning with the Trainer API."),this.h()},l(f){t=c(f,"P",{});var _=h(t);p=r(_,"For more detailed information on loading and processing a dataset, take a look at "),n=c(_,"A",{href:!0,rel:!0});var G=h(n);m=r(G,"Chapter 3"),G.forEach(e),b=r(_," of the Hugging Face course! It covers additional important topics like dynamic padding, and fine-tuning with the Trainer API."),_.forEach(e),this.h()},h(){j(n,"href","https://huggingface.co/course/chapter3/1?fw=pt"),j(n,"rel","nofollow")},m(f,_){d(f,t,_),a(t,p),a(t,n),a(n,m),a(t,b)},d(f){f&&e(t)}}}function Ie(C){let t,p;return t=new gs({props:{code:`from transformers import AutoModelForSequenceClassification, AutoTokenizer
model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased')
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification, AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
Some weights of the model checkpoint at bert-base-cased were <span class="hljs-keyword">not</span> used when initializing BertForSequenceClassification: [<span class="hljs-string">&#x27;cls.predictions.bias&#x27;</span>, <span class="hljs-string">&#x27;cls.predictions.transform.dense.weight&#x27;</span>, <span class="hljs-string">&#x27;cls.predictions.transform.dense.bias&#x27;</span>, <span class="hljs-string">&#x27;cls.predictions.decoder.weight&#x27;</span>, <span class="hljs-string">&#x27;cls.seq_relationship.weight&#x27;</span>, <span class="hljs-string">&#x27;cls.seq_relationship.bias&#x27;</span>, <span class="hljs-string">&#x27;cls.predictions.transform.LayerNorm.weight&#x27;</span>, <span class="hljs-string">&#x27;cls.predictions.transform.LayerNorm.bias&#x27;</span>]
- This IS expected <span class="hljs-keyword">if</span> you are initializing BertForSequenceClassification <span class="hljs-keyword">from</span> the checkpoint of a model trained on another task <span class="hljs-keyword">or</span> <span class="hljs-keyword">with</span> another architecture (e.g. initializing a BertForSequenceClassification model <span class="hljs-keyword">from</span> a BertForPretraining model).
- This IS NOT expected <span class="hljs-keyword">if</span> you are initializing BertForSequenceClassification <span class="hljs-keyword">from</span> the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model <span class="hljs-keyword">from</span> a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were <span class="hljs-keyword">not</span> initialized <span class="hljs-keyword">from</span> the model checkpoint at bert-base-cased <span class="hljs-keyword">and</span> are newly initialized: [<span class="hljs-string">&#x27;classifier.weight&#x27;</span>, <span class="hljs-string">&#x27;classifier.bias&#x27;</span>]
You should probably TRAIN this model on a down-stream task to be able to use it <span class="hljs-keyword">for</span> predictions <span class="hljs-keyword">and</span> inference.
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)`}}),{c(){y(t.$$.fragment)},l(n){x(t.$$.fragment,n)},m(n,m){v(t,n,m),p=!0},p:Ks,i(n){p||(E(t.$$.fragment,n),p=!0)},o(n){T(t.$$.fragment,n),p=!1},d(n){A(t,n)}}}function He(C){let t,p;return t=new Vs({props:{$$slots:{default:[Ie]},$$scope:{ctx:C}}}),{c(){y(t.$$.fragment)},l(n){x(t.$$.fragment,n)},m(n,m){v(t,n,m),p=!0},p(n,m){const b={};m&2&&(b.$$scope={dirty:m,ctx:n}),t.$set(b)},i(n){p||(E(t.$$.fragment,n),p=!0)},o(n){T(t.$$.fragment,n),p=!1},d(n){A(t,n)}}}function Ge(C){let t,p;return t=new gs({props:{code:`from transformers import TFAutoModelForSequenceClassification, AutoTokenizer
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification, AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
Some weights of the model checkpoint at bert-base-cased were <span class="hljs-keyword">not</span> used when initializing TFBertForSequenceClassification: [<span class="hljs-string">&#x27;nsp___cls&#x27;</span>, <span class="hljs-string">&#x27;mlm___cls&#x27;</span>]
- This IS expected <span class="hljs-keyword">if</span> you are initializing TFBertForSequenceClassification <span class="hljs-keyword">from</span> the checkpoint of a model trained on another task <span class="hljs-keyword">or</span> <span class="hljs-keyword">with</span> another architecture (e.g. initializing a BertForSequenceClassification model <span class="hljs-keyword">from</span> a BertForPretraining model).
- This IS NOT expected <span class="hljs-keyword">if</span> you are initializing TFBertForSequenceClassification <span class="hljs-keyword">from</span> the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model <span class="hljs-keyword">from</span> a BertForSequenceClassification model).
Some weights of TFBertForSequenceClassification were <span class="hljs-keyword">not</span> initialized <span class="hljs-keyword">from</span> the model checkpoint at bert-base-cased <span class="hljs-keyword">and</span> are newly initialized: [<span class="hljs-string">&#x27;dropout_37&#x27;</span>, <span class="hljs-string">&#x27;classifier&#x27;</span>]
You should probably TRAIN this model on a down-stream task to be able to use it <span class="hljs-keyword">for</span> predictions <span class="hljs-keyword">and</span> inference.
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)`}}),{c(){y(t.$$.fragment)},l(n){x(t.$$.fragment,n)},m(n,m){v(t,n,m),p=!0},p:Ks,i(n){p||(E(t.$$.fragment,n),p=!0)},o(n){T(t.$$.fragment,n),p=!1},d(n){A(t,n)}}}function Ue(C){let t,p;return t=new Vs({props:{$$slots:{default:[Ge]},$$scope:{ctx:C}}}),{c(){y(t.$$.fragment)},l(n){x(t.$$.fragment,n)},m(n,m){v(t,n,m),p=!0},p(n,m){const b={};m&2&&(b.$$scope={dirty:m,ctx:n}),t.$set(b)},i(n){p||(E(t.$$.fragment,n),p=!0)},o(n){T(t.$$.fragment,n),p=!1},d(n){A(t,n)}}}function We(C){let t,p,n,m,b,f,_,G,ms,B,U,us,W,N,as,F,L,ds,X,k,fs,Q,Y,ps,O,js,os,R,cs,H,M,q,S,bs,es,K,hs,I,ns,z,ts;return N=new gs({props:{code:"dataset = dataset.map(lambda examples: {'labels': examples['label']}, batched=True)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> examples: {<span class="hljs-string">&#x27;labels&#x27;</span>: examples[<span class="hljs-string">&#x27;label&#x27;</span>]}, batched=<span class="hljs-literal">True</span>)'}}),z=new gs({props:{code:`import torch
dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])
dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)
next(iter(dataloader))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_format(<span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;torch&#x27;</span>, columns=[<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>dataloader = torch.utils.data.DataLoader(dataset, batch_size=<span class="hljs-number">32</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(dataloader))
{<span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                        ...,
                        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]),
<span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([[  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                  [  <span class="hljs-number">101</span>, <span class="hljs-number">10684</span>,  <span class="hljs-number">2599</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                  [  <span class="hljs-number">101</span>,  <span class="hljs-number">1220</span>,  <span class="hljs-number">1125</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                  ...,
                  [  <span class="hljs-number">101</span>, <span class="hljs-number">16944</span>,  <span class="hljs-number">1107</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                  [  <span class="hljs-number">101</span>,  <span class="hljs-number">1109</span>, <span class="hljs-number">11896</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
                  [  <span class="hljs-number">101</span>,  <span class="hljs-number">1109</span>,  <span class="hljs-number">4173</span>,  ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]]),
<span class="hljs-string">&#x27;label&#x27;</span>: tensor([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]),
<span class="hljs-string">&#x27;token_type_ids&#x27;</span>: tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     ...,
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                     [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>,  ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])}`}}),{c(){t=o("ol"),p=o("li"),n=l("Rename the "),m=o("code"),b=l("label"),f=l(" column to "),_=o("code"),G=l("labels"),ms=l(", the expected input name in "),B=o("a"),U=l("BertForSequenceClassification"),us=l(":"),W=g(),y(N.$$.fragment),as=g(),F=o("ol"),L=o("li"),ds=l("Retrieve the actual tensors from the Dataset object instead of using the current Python objects."),X=g(),k=o("li"),fs=l("Filter the dataset to only return the model inputs: "),Q=o("code"),Y=l("input_ids"),ps=l(", "),O=o("code"),js=l("token_type_ids"),os=l(", and "),R=o("code"),cs=l("attention_mask"),H=l("."),M=g(),q=o("p"),S=o("a"),bs=l("Dataset.set_format()"),es=l(" completes the last two steps on-the-fly. After you set the format, wrap the dataset in "),K=o("code"),hs=l("torch.utils.data.DataLoader"),I=l(":"),ns=g(),y(z.$$.fragment),this.h()},l(i){t=c(i,"OL",{});var w=h(t);p=c(w,"LI",{});var P=h(p);n=r(P,"Rename the "),m=c(P,"CODE",{});var ks=h(m);b=r(ks,"label"),ks.forEach(e),f=r(P," column to "),_=c(P,"CODE",{});var $s=h(_);G=r($s,"labels"),$s.forEach(e),ms=r(P,", the expected input name in "),B=c(P,"A",{href:!0,rel:!0});var ls=h(B);U=r(ls,"BertForSequenceClassification"),ls.forEach(e),us=r(P,":"),P.forEach(e),w.forEach(e),W=$(i),x(N.$$.fragment,i),as=$(i),F=c(i,"OL",{start:!0});var V=h(F);L=c(V,"LI",{});var Z=h(L);ds=r(Z,"Retrieve the actual tensors from the Dataset object instead of using the current Python objects."),Z.forEach(e),X=$(V),k=c(V,"LI",{});var D=h(k);fs=r(D,"Filter the dataset to only return the model inputs: "),Q=c(D,"CODE",{});var rs=h(Q);Y=r(rs,"input_ids"),rs.forEach(e),ps=r(D,", "),O=c(D,"CODE",{});var ys=h(O);js=r(ys,"token_type_ids"),ys.forEach(e),os=r(D,", and "),R=c(D,"CODE",{});var xs=h(R);cs=r(xs,"attention_mask"),xs.forEach(e),H=r(D,"."),D.forEach(e),V.forEach(e),M=$(i),q=c(i,"P",{});var J=h(q);S=c(J,"A",{href:!0});var ss=h(S);bs=r(ss,"Dataset.set_format()"),ss.forEach(e),es=r(J," completes the last two steps on-the-fly. After you set the format, wrap the dataset in "),K=c(J,"CODE",{});var _s=h(K);hs=r(_s,"torch.utils.data.DataLoader"),_s.forEach(e),I=r(J,":"),J.forEach(e),ns=$(i),x(z.$$.fragment,i),this.h()},h(){j(B,"href","https://huggingface.co/transformers/model_doc/bert#transformers.BertForSequenceClassification.forward"),j(B,"rel","nofollow"),j(F,"start","2"),j(S,"href","/docs/datasets/v2.2.2/en/package_reference/main_classes#datasets.Dataset.set_format")},m(i,w){d(i,t,w),a(t,p),a(p,n),a(p,m),a(m,b),a(p,f),a(p,_),a(_,G),a(p,ms),a(p,B),a(B,U),a(p,us),d(i,W,w),v(N,i,w),d(i,as,w),d(i,F,w),a(F,L),a(L,ds),a(F,X),a(F,k),a(k,fs),a(k,Q),a(Q,Y),a(k,ps),a(k,O),a(O,js),a(k,os),a(k,R),a(R,cs),a(k,H),d(i,M,w),d(i,q,w),a(q,S),a(S,bs),a(q,es),a(q,K),a(K,hs),a(q,I),d(i,ns,w),v(z,i,w),ts=!0},p:Ks,i(i){ts||(E(N.$$.fragment,i),E(z.$$.fragment,i),ts=!0)},o(i){T(N.$$.fragment,i),T(z.$$.fragment,i),ts=!1},d(i){i&&e(t),i&&e(W),A(N,i),i&&e(as),i&&e(F),i&&e(M),i&&e(q),i&&e(ns),A(z,i)}}}function Qe(C){let t,p;return t=new Vs({props:{$$slots:{default:[We]},$$scope:{ctx:C}}}),{c(){y(t.$$.fragment)},l(n){x(t.$$.fragment,n)},m(n,m){v(t,n,m),p=!0},p(n,m){const b={};m&2&&(b.$$scope={dirty:m,ctx:n}),t.$set(b)},i(n){p||(E(t.$$.fragment,n),p=!0)},o(n){T(t.$$.fragment,n),p=!1},d(n){A(t,n)}}}function Ye(C){let t,p,n,m,b,f,_,G,ms,B,U,us,W,N,as,F,L,ds,X,k,fs,Q,Y,ps,O,js,os,R,cs,H,M,q,S,bs,es,K,hs,I,ns,z,ts;return N=new gs({props:{code:"dataset = dataset.map(lambda examples: {'labels': examples['label']}, batched=True)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> examples: {<span class="hljs-string">&#x27;labels&#x27;</span>: examples[<span class="hljs-string">&#x27;label&#x27;</span>]}, batched=<span class="hljs-literal">True</span>)'}}),z=new gs({props:{code:`import tensorflow as tf
dataset.set_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])
features = {x: dataset[x].to_tensor(default_value=0, shape=[None, tokenizer.model_max_length]) for x in ['input_ids', 'token_type_ids', 'attention_mask']}
tfdataset = tf.data.Dataset.from_tensor_slices((features, dataset["labels"])).batch(32)
next(iter(tfdataset))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset.set_format(<span class="hljs-built_in">type</span>=<span class="hljs-string">&#x27;tensorflow&#x27;</span>, columns=[<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;labels&#x27;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>features = {x: dataset[x].to_tensor(default_value=<span class="hljs-number">0</span>, shape=[<span class="hljs-literal">None</span>, tokenizer.model_max_length]) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>, <span class="hljs-string">&#x27;attention_mask&#x27;</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>tfdataset = tf.data.Dataset.from_tensor_slices((features, dataset[<span class="hljs-string">&quot;labels&quot;</span>])).batch(<span class="hljs-number">32</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(tfdataset))
({<span class="hljs-string">&#x27;input_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">32</span>, <span class="hljs-number">512</span>), dtype=int32, numpy=
array([[  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
   [  <span class="hljs-number">101</span>, <span class="hljs-number">10684</span>,  <span class="hljs-number">2599</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
   [  <span class="hljs-number">101</span>,  <span class="hljs-number">1220</span>,  <span class="hljs-number">1125</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
   ...,
   [  <span class="hljs-number">101</span>,  <span class="hljs-number">1109</span>,  <span class="hljs-number">2026</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
   [  <span class="hljs-number">101</span>, <span class="hljs-number">22263</span>,  <span class="hljs-number">1107</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>],
   [  <span class="hljs-number">101</span>,   <span class="hljs-number">142</span>,  <span class="hljs-number">1813</span>, ...,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>,     <span class="hljs-number">0</span>]], dtype=int32)&gt;, <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">32</span>, <span class="hljs-number">512</span>), dtype=int32, numpy=
array([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   ...,
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], dtype=int32)&gt;, <span class="hljs-string">&#x27;attention_mask&#x27;</span>: &lt;tf.Tensor: shape=(<span class="hljs-number">32</span>, <span class="hljs-number">512</span>), dtype=int32, numpy=
array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   ...,
   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, ..., <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]], dtype=int32)&gt;}, &lt;tf.Tensor: shape=(<span class="hljs-number">32</span>,), dtype=int64, numpy=
array([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>,
   <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>])&gt;)`}}),{c(){t=o("ol"),p=o("li"),n=l("Rename the "),m=o("code"),b=l("label"),f=l(" column to "),_=o("code"),G=l("labels"),ms=l(", the expected input name in "),B=o("a"),U=l("TFBertForSequenceClassification"),us=l(":"),W=g(),y(N.$$.fragment),as=g(),F=o("ol"),L=o("li"),ds=l("Retrieve the actual tensors from the Dataset object instead of using the current Python objects."),X=g(),k=o("li"),fs=l("Filter the dataset to only return the model inputs: "),Q=o("code"),Y=l("input_ids"),ps=l(", "),O=o("code"),js=l("token_type_ids"),os=l(", and "),R=o("code"),cs=l("attention_mask"),H=l("."),M=g(),q=o("p"),S=o("a"),bs=l("Dataset.set_format()"),es=l(" completes the last two steps on-the-fly. After you set the format, wrap the dataset in "),K=o("code"),hs=l("tf.data.Dataset"),I=l(":"),ns=g(),y(z.$$.fragment),this.h()},l(i){t=c(i,"OL",{});var w=h(t);p=c(w,"LI",{});var P=h(p);n=r(P,"Rename the "),m=c(P,"CODE",{});var ks=h(m);b=r(ks,"label"),ks.forEach(e),f=r(P," column to "),_=c(P,"CODE",{});var $s=h(_);G=r($s,"labels"),$s.forEach(e),ms=r(P,", the expected input name in "),B=c(P,"A",{href:!0,rel:!0});var ls=h(B);U=r(ls,"TFBertForSequenceClassification"),ls.forEach(e),us=r(P,":"),P.forEach(e),w.forEach(e),W=$(i),x(N.$$.fragment,i),as=$(i),F=c(i,"OL",{start:!0});var V=h(F);L=c(V,"LI",{});var Z=h(L);ds=r(Z,"Retrieve the actual tensors from the Dataset object instead of using the current Python objects."),Z.forEach(e),X=$(V),k=c(V,"LI",{});var D=h(k);fs=r(D,"Filter the dataset to only return the model inputs: "),Q=c(D,"CODE",{});var rs=h(Q);Y=r(rs,"input_ids"),rs.forEach(e),ps=r(D,", "),O=c(D,"CODE",{});var ys=h(O);js=r(ys,"token_type_ids"),ys.forEach(e),os=r(D,", and "),R=c(D,"CODE",{});var xs=h(R);cs=r(xs,"attention_mask"),xs.forEach(e),H=r(D,"."),D.forEach(e),V.forEach(e),M=$(i),q=c(i,"P",{});var J=h(q);S=c(J,"A",{href:!0});var ss=h(S);bs=r(ss,"Dataset.set_format()"),ss.forEach(e),es=r(J," completes the last two steps on-the-fly. After you set the format, wrap the dataset in "),K=c(J,"CODE",{});var _s=h(K);hs=r(_s,"tf.data.Dataset"),_s.forEach(e),I=r(J,":"),J.forEach(e),ns=$(i),x(z.$$.fragment,i),this.h()},h(){j(B,"href","https://huggingface.co/transformers/model_doc/bert#tfbertforsequenceclassification"),j(B,"rel","nofollow"),j(F,"start","2"),j(S,"href","/docs/datasets/v2.2.2/en/package_reference/main_classes#datasets.Dataset.set_format")},m(i,w){d(i,t,w),a(t,p),a(p,n),a(p,m),a(m,b),a(p,f),a(p,_),a(_,G),a(p,ms),a(p,B),a(B,U),a(p,us),d(i,W,w),v(N,i,w),d(i,as,w),d(i,F,w),a(F,L),a(L,ds),a(F,X),a(F,k),a(k,fs),a(k,Q),a(Q,Y),a(k,ps),a(k,O),a(O,js),a(k,os),a(k,R),a(R,cs),a(k,H),d(i,M,w),d(i,q,w),a(q,S),a(S,bs),a(q,es),a(q,K),a(K,hs),a(q,I),d(i,ns,w),v(z,i,w),ts=!0},p:Ks,i(i){ts||(E(N.$$.fragment,i),E(z.$$.fragment,i),ts=!0)},o(i){T(N.$$.fragment,i),T(z.$$.fragment,i),ts=!1},d(i){i&&e(t),i&&e(W),A(N,i),i&&e(as),i&&e(F),i&&e(M),i&&e(q),i&&e(ns),A(z,i)}}}function Je(C){let t,p;return t=new Vs({props:{$$slots:{default:[Ye]},$$scope:{ctx:C}}}),{c(){y(t.$$.fragment)},l(n){x(t.$$.fragment,n)},m(n,m){v(t,n,m),p=!0},p(n,m){const b={};m&2&&(b.$$scope={dirty:m,ctx:n}),t.$set(b)},i(n){p||(E(t.$$.fragment,n),p=!0)},o(n){T(t.$$.fragment,n),p=!1},d(n){A(t,n)}}}function Xe(C){let t,p,n,m,b;return m=new gs({props:{code:`from tqdm import tqdm
device = 'cuda' if torch.cuda.is_available() else 'cpu' 
model.train().to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)
for epoch in range(3):
    for i, batch in enumerate(tqdm(dataloader)):
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs[0]
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        if i % 10 == 0:
            print(f"loss: {loss}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&#x27;cuda&#x27;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;cpu&#x27;</span> 
<span class="hljs-meta">&gt;&gt;&gt; </span>model.train().to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = torch.optim.AdamW(params=model.parameters(), lr=<span class="hljs-number">1e-5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(tqdm(dataloader)):
<span class="hljs-meta">... </span>        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
<span class="hljs-meta">... </span>        outputs = model(**batch)
<span class="hljs-meta">... </span>        loss = outputs[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>        loss.backward()
<span class="hljs-meta">... </span>        optimizer.step()
<span class="hljs-meta">... </span>        optimizer.zero_grad()
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:
<span class="hljs-meta">... </span>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;loss: <span class="hljs-subst">{loss}</span>&quot;</span>)`}}),{c(){t=o("p"),p=l("Lastly, create a simple training loop and start training:"),n=g(),y(m.$$.fragment)},l(f){t=c(f,"P",{});var _=h(t);p=r(_,"Lastly, create a simple training loop and start training:"),_.forEach(e),n=$(f),x(m.$$.fragment,f)},m(f,_){d(f,t,_),a(t,p),d(f,n,_),v(m,f,_),b=!0},p:Ks,i(f){b||(E(m.$$.fragment,f),b=!0)},o(f){T(m.$$.fragment,f),b=!1},d(f){f&&e(t),f&&e(n),A(m,f)}}}function Ke(C){let t,p;return t=new Vs({props:{$$slots:{default:[Xe]},$$scope:{ctx:C}}}),{c(){y(t.$$.fragment)},l(n){x(t.$$.fragment,n)},m(n,m){v(t,n,m),p=!0},p(n,m){const b={};m&2&&(b.$$scope={dirty:m,ctx:n}),t.$set(b)},i(n){p||(E(t.$$.fragment,n),p=!0)},o(n){T(t.$$.fragment,n),p=!1},d(n){A(t,n)}}}function Ve(C){let t,p,n,m,b;return m=new gs({props:{code:`loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits=True)
opt = tf.keras.optimizers.Adam(learning_rate=3e-5)
model.compile(optimizer=opt, loss=loss_fn, metrics=["accuracy"])
model.fit(tfdataset, epochs=3)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE, from_logits=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>opt = tf.keras.optimizers.Adam(learning_rate=<span class="hljs-number">3e-5</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=opt, loss=loss_fn, metrics=[<span class="hljs-string">&quot;accuracy&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(tfdataset, epochs=<span class="hljs-number">3</span>)`}}),{c(){t=o("p"),p=l("Lastly, compile the model and start training:"),n=g(),y(m.$$.fragment)},l(f){t=c(f,"P",{});var _=h(t);p=r(_,"Lastly, compile the model and start training:"),_.forEach(e),n=$(f),x(m.$$.fragment,f)},m(f,_){d(f,t,_),a(t,p),d(f,n,_),v(m,f,_),b=!0},p:Ks,i(f){b||(E(m.$$.fragment,f),b=!0)},o(f){T(m.$$.fragment,f),b=!1},d(f){f&&e(t),f&&e(n),A(m,f)}}}function Ze(C){let t,p;return t=new Vs({props:{$$slots:{default:[Ve]},$$scope:{ctx:C}}}),{c(){y(t.$$.fragment)},l(n){x(t.$$.fragment,n)},m(n,m){v(t,n,m),p=!0},p(n,m){const b={};m&2&&(b.$$scope={dirty:m,ctx:n}),t.$set(b)},i(n){p||(E(t.$$.fragment,n),p=!0)},o(n){T(t.$$.fragment,n),p=!1},d(n){A(t,n)}}}function sn(C){let t,p,n,m,b,f,_,G,ms,B,U,us,W,N,as,F,L,ds,X,k,fs,Q,Y,ps,O,js,os,R,cs,H,M,q,S,bs,es,K,hs,I,ns,z,ts,i,w,P,ks,$s,ls,V,Z,D,rs,ys,xs,J,ss,_s,vs,Cs,Zs,Ls,Pa,sa,Da,ua,qs,Ba,Hs,La,Na,da,Ns,fa,is,Ra,aa,Oa,Ma,ea,Ia,Ha,na,Ga,Ua,ja,Es,Fs,ta,Rs,Wa,la,Qa,ba,Gs,Ya,ga,zs,$a,Ts,Ss,ra,Os,Ja,pa,Xa,_a,Ps,wa,As,Ds,oa,Ms,Ka,ca,Va,ka,Us,Za,ya,ws,se,Ws,ae,ee,Qs,ne,te,xa;return f=new Xs({}),Y=new Oe({props:{$$slots:{default:[Me]},$$scope:{ctx:C}}}),R=new gs({props:{code:"pip install datasets",highlighted:'pip <span class="hljs-keyword">install</span> datasets'}}),S=new Xs({}),ls=new gs({props:{code:`from datasets import load_dataset
dataset = load_dataset('glue', 'mrpc', split='train')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)`}}),ss=new le({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ue],pytorch:[He]},$$scope:{ctx:C}}}),Ls=new Xs({}),Ns=new gs({props:{code:`def encode(examples):
    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length')

dataset = dataset.map(encode, batched=True)
dataset[0]`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&#x27;sentence1&#x27;</span>], examples[<span class="hljs-string">&#x27;sentence2&#x27;</span>], truncation=<span class="hljs-literal">True</span>, padding=<span class="hljs-string">&#x27;max_length&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(encode, batched=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;sentence1&#x27;</span>: <span class="hljs-string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;sentence2&#x27;</span>: <span class="hljs-string">&#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>,
<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
<span class="hljs-string">&#x27;idx&#x27;</span>: <span class="hljs-number">0</span>,
<span class="hljs-string">&#x27;input_ids&#x27;</span>: array([  <span class="hljs-number">101</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">2292</span>, <span class="hljs-number">1119</span>,  <span class="hljs-number">1270</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>,   <span class="hljs-number">117</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>, <span class="hljs-number">11336</span>,  <span class="hljs-number">6732</span>, <span class="hljs-number">3384</span>,  <span class="hljs-number">1106</span>,  <span class="hljs-number">1140</span>,  <span class="hljs-number">1112</span>,  <span class="hljs-number">1178</span>,   <span class="hljs-number">107</span>,  <span class="hljs-number">1103</span>,  <span class="hljs-number">7737</span>,   <span class="hljs-number">107</span>, <span class="hljs-number">117</span>,  <span class="hljs-number">7277</span>,  <span class="hljs-number">2180</span>,  <span class="hljs-number">5303</span>,  <span class="hljs-number">4806</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">1711</span>,  <span class="hljs-number">1104</span>,  <span class="hljs-number">9938</span>, <span class="hljs-number">4267</span>, <span class="hljs-number">12223</span>, <span class="hljs-number">21811</span>,  <span class="hljs-number">1117</span>,  <span class="hljs-number">2554</span>,   <span class="hljs-number">119</span>,   <span class="hljs-number">102</span>]),
<span class="hljs-string">&#x27;token_type_ids&#x27;</span>: array([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]),
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: array([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])}`}}),Rs=new Xs({}),zs=new le({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Je],pytorch:[Qe]},$$scope:{ctx:C}}}),Os=new Xs({}),Ps=new le({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ze],pytorch:[Ke]},$$scope:{ctx:C}}}),Ms=new Xs({}),{c(){t=o("meta"),p=g(),n=o("h1"),m=o("a"),b=o("span"),y(f.$$.fragment),_=g(),G=o("span"),ms=l("Quick Start"),B=g(),U=o("p"),us=l("The quick start is intended for developers who are ready to dive in to the code, and see an end-to-end example of how they can integrate \u{1F917} Datasets into their model training workflow. For beginners who are looking for a gentler introduction, we recommend you begin with the "),W=o("a"),N=l("tutorials"),as=l("."),F=g(),L=o("p"),ds=l("In the quick start, you will walkthrough all the steps to fine-tune "),X=o("a"),k=l("BERT"),fs=l(" on a paraphrase classification task. Depending on the specific dataset you use, these steps may vary, but the general steps of how to load a dataset and process it are the same."),Q=g(),y(Y.$$.fragment),ps=g(),O=o("p"),js=l("Get started by installing \u{1F917} Datasets:"),os=g(),y(R.$$.fragment),cs=g(),H=o("h2"),M=o("a"),q=o("span"),y(S.$$.fragment),bs=g(),es=o("span"),K=l("Load the dataset and model"),hs=g(),I=o("p"),ns=l("Begin by loading the "),z=o("a"),ts=l("Microsoft Research Paraphrase Corpus (MRPC)"),i=l(" training dataset from the "),w=o("a"),P=l("General Language Understanding Evaluation (GLUE) benchmark"),ks=l(". MRPC is a corpus of human annotated sentence pairs used to train a model to determine whether sentence pairs are semantically equivalent."),$s=g(),y(ls.$$.fragment),V=g(),Z=o("p"),D=l("Next, import the pre-trained BERT model and its tokenizer from the "),rs=o("a"),ys=l("\u{1F917} Transformers"),xs=l(" library:"),J=g(),y(ss.$$.fragment),_s=g(),vs=o("h2"),Cs=o("a"),Zs=o("span"),y(Ls.$$.fragment),Pa=g(),sa=o("span"),Da=l("Tokenize the dataset"),ua=g(),qs=o("p"),Ba=l("The next step is to tokenize the text in order to build sequences of integers the model can understand. Encode the entire dataset with "),Hs=o("a"),La=l("Dataset.map()"),Na=l(", and truncate and pad the inputs to the maximum length of the model. This ensures the appropriate tensor batches are built."),da=g(),y(Ns.$$.fragment),fa=g(),is=o("p"),Ra=l("Notice how there are three new columns in the dataset: "),aa=o("code"),Oa=l("input_ids"),Ma=l(", "),ea=o("code"),Ia=l("token_type_ids"),Ha=l(", and "),na=o("code"),Ga=l("attention_mask"),Ua=l(". These columns are the inputs to the model."),ja=g(),Es=o("h2"),Fs=o("a"),ta=o("span"),y(Rs.$$.fragment),Wa=g(),la=o("span"),Qa=l("Format the dataset"),ba=g(),Gs=o("p"),Ya=l("Depending on whether you are using PyTorch, TensorFlow, or JAX, you will need to format the dataset accordingly. There are three changes you need to make to the dataset:"),ga=g(),y(zs.$$.fragment),$a=g(),Ts=o("h2"),Ss=o("a"),ra=o("span"),y(Os.$$.fragment),Ja=g(),pa=o("span"),Xa=l("Train the model"),_a=g(),y(Ps.$$.fragment),wa=g(),As=o("h2"),Ds=o("a"),oa=o("span"),y(Ms.$$.fragment),Ka=g(),ca=o("span"),Va=l("What's next?"),ka=g(),Us=o("p"),Za=l("This completes the basic steps of loading a dataset to train a model. You loaded and processed the MRPC dataset to fine-tune BERT to determine whether sentence pairs have the same meaning."),ya=g(),ws=o("p"),se=l("For your next steps, take a look at our "),Ws=o("a"),ae=l("How-to guides"),ee=l(" and learn how to achieve a specific task (e.g. load a dataset offline, add a dataset to the Hub, change the name of a column). Or if you want to deepen your knowledge of \u{1F917} Datasets core concepts, read our "),Qs=o("a"),ne=l("Conceptual Guides"),te=l("."),this.h()},l(s){const u=Ne('[data-svelte="svelte-1phssyn"]',document.head);t=c(u,"META",{name:!0,content:!0}),u.forEach(e),p=$(s),n=c(s,"H1",{class:!0});var Is=h(n);m=c(Is,"A",{id:!0,class:!0,href:!0});var ha=h(m);b=c(ha,"SPAN",{});var ia=h(b);x(f.$$.fragment,ia),ia.forEach(e),ha.forEach(e),_=$(Is),G=c(Is,"SPAN",{});var ma=h(G);ms=r(ma,"Quick Start"),ma.forEach(e),Is.forEach(e),B=$(s),U=c(s,"P",{});var va=h(U);us=r(va,"The quick start is intended for developers who are ready to dive in to the code, and see an end-to-end example of how they can integrate \u{1F917} Datasets into their model training workflow. For beginners who are looking for a gentler introduction, we recommend you begin with the "),W=c(va,"A",{href:!0});var re=h(W);N=r(re,"tutorials"),re.forEach(e),as=r(va,"."),va.forEach(e),F=$(s),L=c(s,"P",{});var Ea=h(L);ds=r(Ea,"In the quick start, you will walkthrough all the steps to fine-tune "),X=c(Ea,"A",{href:!0,rel:!0});var pe=h(X);k=r(pe,"BERT"),pe.forEach(e),fs=r(Ea," on a paraphrase classification task. Depending on the specific dataset you use, these steps may vary, but the general steps of how to load a dataset and process it are the same."),Ea.forEach(e),Q=$(s),x(Y.$$.fragment,s),ps=$(s),O=c(s,"P",{});var oe=h(O);js=r(oe,"Get started by installing \u{1F917} Datasets:"),oe.forEach(e),os=$(s),x(R.$$.fragment,s),cs=$(s),H=c(s,"H2",{class:!0});var Ta=h(H);M=c(Ta,"A",{id:!0,class:!0,href:!0});var ce=h(M);q=c(ce,"SPAN",{});var he=h(q);x(S.$$.fragment,he),he.forEach(e),ce.forEach(e),bs=$(Ta),es=c(Ta,"SPAN",{});var ie=h(es);K=r(ie,"Load the dataset and model"),ie.forEach(e),Ta.forEach(e),hs=$(s),I=c(s,"P",{});var Ys=h(I);ns=r(Ys,"Begin by loading the "),z=c(Ys,"A",{href:!0,rel:!0});var me=h(z);ts=r(me,"Microsoft Research Paraphrase Corpus (MRPC)"),me.forEach(e),i=r(Ys," training dataset from the "),w=c(Ys,"A",{href:!0,rel:!0});var ue=h(w);P=r(ue,"General Language Understanding Evaluation (GLUE) benchmark"),ue.forEach(e),ks=r(Ys,". MRPC is a corpus of human annotated sentence pairs used to train a model to determine whether sentence pairs are semantically equivalent."),Ys.forEach(e),$s=$(s),x(ls.$$.fragment,s),V=$(s),Z=c(s,"P",{});var Aa=h(Z);D=r(Aa,"Next, import the pre-trained BERT model and its tokenizer from the "),rs=c(Aa,"A",{href:!0,rel:!0});var de=h(rs);ys=r(de,"\u{1F917} Transformers"),de.forEach(e),xs=r(Aa," library:"),Aa.forEach(e),J=$(s),x(ss.$$.fragment,s),_s=$(s),vs=c(s,"H2",{class:!0});var Ca=h(vs);Cs=c(Ca,"A",{id:!0,class:!0,href:!0});var fe=h(Cs);Zs=c(fe,"SPAN",{});var je=h(Zs);x(Ls.$$.fragment,je),je.forEach(e),fe.forEach(e),Pa=$(Ca),sa=c(Ca,"SPAN",{});var be=h(sa);Da=r(be,"Tokenize the dataset"),be.forEach(e),Ca.forEach(e),ua=$(s),qs=c(s,"P",{});var qa=h(qs);Ba=r(qa,"The next step is to tokenize the text in order to build sequences of integers the model can understand. Encode the entire dataset with "),Hs=c(qa,"A",{href:!0});var ge=h(Hs);La=r(ge,"Dataset.map()"),ge.forEach(e),Na=r(qa,", and truncate and pad the inputs to the maximum length of the model. This ensures the appropriate tensor batches are built."),qa.forEach(e),da=$(s),x(Ns.$$.fragment,s),fa=$(s),is=c(s,"P",{});var Bs=h(is);Ra=r(Bs,"Notice how there are three new columns in the dataset: "),aa=c(Bs,"CODE",{});var $e=h(aa);Oa=r($e,"input_ids"),$e.forEach(e),Ma=r(Bs,", "),ea=c(Bs,"CODE",{});var _e=h(ea);Ia=r(_e,"token_type_ids"),_e.forEach(e),Ha=r(Bs,", and "),na=c(Bs,"CODE",{});var we=h(na);Ga=r(we,"attention_mask"),we.forEach(e),Ua=r(Bs,". These columns are the inputs to the model."),Bs.forEach(e),ja=$(s),Es=c(s,"H2",{class:!0});var Fa=h(Es);Fs=c(Fa,"A",{id:!0,class:!0,href:!0});var ke=h(Fs);ta=c(ke,"SPAN",{});var ye=h(ta);x(Rs.$$.fragment,ye),ye.forEach(e),ke.forEach(e),Wa=$(Fa),la=c(Fa,"SPAN",{});var xe=h(la);Qa=r(xe,"Format the dataset"),xe.forEach(e),Fa.forEach(e),ba=$(s),Gs=c(s,"P",{});var ve=h(Gs);Ya=r(ve,"Depending on whether you are using PyTorch, TensorFlow, or JAX, you will need to format the dataset accordingly. There are three changes you need to make to the dataset:"),ve.forEach(e),ga=$(s),x(zs.$$.fragment,s),$a=$(s),Ts=c(s,"H2",{class:!0});var za=h(Ts);Ss=c(za,"A",{id:!0,class:!0,href:!0});var Ee=h(Ss);ra=c(Ee,"SPAN",{});var Te=h(ra);x(Os.$$.fragment,Te),Te.forEach(e),Ee.forEach(e),Ja=$(za),pa=c(za,"SPAN",{});var Ae=h(pa);Xa=r(Ae,"Train the model"),Ae.forEach(e),za.forEach(e),_a=$(s),x(Ps.$$.fragment,s),wa=$(s),As=c(s,"H2",{class:!0});var Sa=h(As);Ds=c(Sa,"A",{id:!0,class:!0,href:!0});var Ce=h(Ds);oa=c(Ce,"SPAN",{});var qe=h(oa);x(Ms.$$.fragment,qe),qe.forEach(e),Ce.forEach(e),Ka=$(Sa),ca=c(Sa,"SPAN",{});var Fe=h(ca);Va=r(Fe,"What's next?"),Fe.forEach(e),Sa.forEach(e),ka=$(s),Us=c(s,"P",{});var ze=h(Us);Za=r(ze,"This completes the basic steps of loading a dataset to train a model. You loaded and processed the MRPC dataset to fine-tune BERT to determine whether sentence pairs have the same meaning."),ze.forEach(e),ya=$(s),ws=c(s,"P",{});var Js=h(ws);se=r(Js,"For your next steps, take a look at our "),Ws=c(Js,"A",{href:!0});var Se=h(Ws);ae=r(Se,"How-to guides"),Se.forEach(e),ee=r(Js," and learn how to achieve a specific task (e.g. load a dataset offline, add a dataset to the Hub, change the name of a column). Or if you want to deepen your knowledge of \u{1F917} Datasets core concepts, read our "),Qs=c(Js,"A",{href:!0});var Pe=h(Qs);ne=r(Pe,"Conceptual Guides"),Pe.forEach(e),te=r(Js,"."),Js.forEach(e),this.h()},h(){j(t,"name","hf:doc:metadata"),j(t,"content",JSON.stringify(an)),j(m,"id","quick-start"),j(m,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(m,"href","#quick-start"),j(n,"class","relative group"),j(W,"href","./tutorial"),j(X,"href","https://huggingface.co/bert-base-cased"),j(X,"rel","nofollow"),j(M,"id","load-the-dataset-and-model"),j(M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(M,"href","#load-the-dataset-and-model"),j(H,"class","relative group"),j(z,"href","https://huggingface.co/datasets/glue/viewer/mrpc"),j(z,"rel","nofollow"),j(w,"href","https://huggingface.co/datasets/glue"),j(w,"rel","nofollow"),j(rs,"href","https://huggingface.co/transformers/"),j(rs,"rel","nofollow"),j(Cs,"id","tokenize-the-dataset"),j(Cs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(Cs,"href","#tokenize-the-dataset"),j(vs,"class","relative group"),j(Hs,"href","/docs/datasets/v2.2.2/en/package_reference/main_classes#datasets.Dataset.map"),j(Fs,"id","format-the-dataset"),j(Fs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(Fs,"href","#format-the-dataset"),j(Es,"class","relative group"),j(Ss,"id","train-the-model"),j(Ss,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(Ss,"href","#train-the-model"),j(Ts,"class","relative group"),j(Ds,"id","whats-next"),j(Ds,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),j(Ds,"href","#whats-next"),j(As,"class","relative group"),j(Ws,"href","./how_to"),j(Qs,"href","./about_arrow")},m(s,u){a(document.head,t),d(s,p,u),d(s,n,u),a(n,m),a(m,b),v(f,b,null),a(n,_),a(n,G),a(G,ms),d(s,B,u),d(s,U,u),a(U,us),a(U,W),a(W,N),a(U,as),d(s,F,u),d(s,L,u),a(L,ds),a(L,X),a(X,k),a(L,fs),d(s,Q,u),v(Y,s,u),d(s,ps,u),d(s,O,u),a(O,js),d(s,os,u),v(R,s,u),d(s,cs,u),d(s,H,u),a(H,M),a(M,q),v(S,q,null),a(H,bs),a(H,es),a(es,K),d(s,hs,u),d(s,I,u),a(I,ns),a(I,z),a(z,ts),a(I,i),a(I,w),a(w,P),a(I,ks),d(s,$s,u),v(ls,s,u),d(s,V,u),d(s,Z,u),a(Z,D),a(Z,rs),a(rs,ys),a(Z,xs),d(s,J,u),v(ss,s,u),d(s,_s,u),d(s,vs,u),a(vs,Cs),a(Cs,Zs),v(Ls,Zs,null),a(vs,Pa),a(vs,sa),a(sa,Da),d(s,ua,u),d(s,qs,u),a(qs,Ba),a(qs,Hs),a(Hs,La),a(qs,Na),d(s,da,u),v(Ns,s,u),d(s,fa,u),d(s,is,u),a(is,Ra),a(is,aa),a(aa,Oa),a(is,Ma),a(is,ea),a(ea,Ia),a(is,Ha),a(is,na),a(na,Ga),a(is,Ua),d(s,ja,u),d(s,Es,u),a(Es,Fs),a(Fs,ta),v(Rs,ta,null),a(Es,Wa),a(Es,la),a(la,Qa),d(s,ba,u),d(s,Gs,u),a(Gs,Ya),d(s,ga,u),v(zs,s,u),d(s,$a,u),d(s,Ts,u),a(Ts,Ss),a(Ss,ra),v(Os,ra,null),a(Ts,Ja),a(Ts,pa),a(pa,Xa),d(s,_a,u),v(Ps,s,u),d(s,wa,u),d(s,As,u),a(As,Ds),a(Ds,oa),v(Ms,oa,null),a(As,Ka),a(As,ca),a(ca,Va),d(s,ka,u),d(s,Us,u),a(Us,Za),d(s,ya,u),d(s,ws,u),a(ws,se),a(ws,Ws),a(Ws,ae),a(ws,ee),a(ws,Qs),a(Qs,ne),a(ws,te),xa=!0},p(s,[u]){const Is={};u&2&&(Is.$$scope={dirty:u,ctx:s}),Y.$set(Is);const ha={};u&2&&(ha.$$scope={dirty:u,ctx:s}),ss.$set(ha);const ia={};u&2&&(ia.$$scope={dirty:u,ctx:s}),zs.$set(ia);const ma={};u&2&&(ma.$$scope={dirty:u,ctx:s}),Ps.$set(ma)},i(s){xa||(E(f.$$.fragment,s),E(Y.$$.fragment,s),E(R.$$.fragment,s),E(S.$$.fragment,s),E(ls.$$.fragment,s),E(ss.$$.fragment,s),E(Ls.$$.fragment,s),E(Ns.$$.fragment,s),E(Rs.$$.fragment,s),E(zs.$$.fragment,s),E(Os.$$.fragment,s),E(Ps.$$.fragment,s),E(Ms.$$.fragment,s),xa=!0)},o(s){T(f.$$.fragment,s),T(Y.$$.fragment,s),T(R.$$.fragment,s),T(S.$$.fragment,s),T(ls.$$.fragment,s),T(ss.$$.fragment,s),T(Ls.$$.fragment,s),T(Ns.$$.fragment,s),T(Rs.$$.fragment,s),T(zs.$$.fragment,s),T(Os.$$.fragment,s),T(Ps.$$.fragment,s),T(Ms.$$.fragment,s),xa=!1},d(s){e(t),s&&e(p),s&&e(n),A(f),s&&e(B),s&&e(U),s&&e(F),s&&e(L),s&&e(Q),A(Y,s),s&&e(ps),s&&e(O),s&&e(os),A(R,s),s&&e(cs),s&&e(H),A(S),s&&e(hs),s&&e(I),s&&e($s),A(ls,s),s&&e(V),s&&e(Z),s&&e(J),A(ss,s),s&&e(_s),s&&e(vs),A(Ls),s&&e(ua),s&&e(qs),s&&e(da),A(Ns,s),s&&e(fa),s&&e(is),s&&e(ja),s&&e(Es),A(Rs),s&&e(ba),s&&e(Gs),s&&e(ga),A(zs,s),s&&e($a),s&&e(Ts),A(Os),s&&e(_a),A(Ps,s),s&&e(wa),s&&e(As),A(Ms),s&&e(ka),s&&e(Us),s&&e(ya),s&&e(ws)}}}const an={local:"quick-start",sections:[{local:"load-the-dataset-and-model",title:"Load the dataset and model"},{local:"tokenize-the-dataset",title:"Tokenize the dataset"},{local:"format-the-dataset",title:"Format the dataset"},{local:"train-the-model",title:"Train the model"},{local:"whats-next",title:"What's next?"}],title:"Quick Start"};function en(C){return Re(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class cn extends De{constructor(t){super();Be(this,t,en,sn,Le,{})}}export{cn as default,an as metadata};
