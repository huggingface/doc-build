import{S as Dn,i as kn,s as An,e as s,k as f,w as q,t as l,M as Tn,c as o,d as a,m as c,a as r,x as F,h as i,b as n,N as yn,G as e,g as h,y as G,q as U,o as W,B as V,v as $n}from"../chunks/vendor-hf-doc-builder.js";import{T as En}from"../chunks/Tip-hf-doc-builder.js";import{I as Be}from"../chunks/IconCopyLink-hf-doc-builder.js";function Bn(at){let u,m,p,_,g;return{c(){u=s("p"),m=l("Read the "),p=s("a"),_=l("Share"),g=l(" section to learn more about how to share a dataset. This section also provides a step-by-step guide on how to write your own dataset loading script!"),this.h()},l(v){u=o(v,"P",{});var b=r(u);m=i(b,"Read the "),p=o(b,"A",{href:!0});var w=r(p);_=i(w,"Share"),w.forEach(a),g=i(b," section to learn more about how to share a dataset. This section also provides a step-by-step guide on how to write your own dataset loading script!"),b.forEach(a),this.h()},h(){n(p,"href","./share")},m(v,b){h(v,u,b),e(u,m),e(u,p),e(p,_),e(u,g)},d(v){v&&a(u)}}}function Cn(at){let u,m,p,_,g,v,b,w,Ce,re;return{c(){u=s("p"),m=s("a"),p=l("DownloadManager.download_and_extract()"),_=l(" can download files from a wide range of sources. If the data files are hosted on a special access server, you should use "),g=s("code"),v=l("DownloadManger.download_custom"),b=l(". Refer to the reference of "),w=s("a"),Ce=l("DownloadManager"),re=l(" for more details."),this.h()},l(y){u=o(y,"P",{});var k=r(u);m=o(k,"A",{href:!0});var Y=r(m);p=i(Y,"DownloadManager.download_and_extract()"),Y.forEach(a),_=i(k," can download files from a wide range of sources. If the data files are hosted on a special access server, you should use "),g=o(k,"CODE",{});var st=r(g);v=i(st,"DownloadManger.download_custom"),st.forEach(a),b=i(k,". Refer to the reference of "),w=o(k,"A",{href:!0});var ot=r(w);Ce=i(ot,"DownloadManager"),ot.forEach(a),re=i(k," for more details."),k.forEach(a),this.h()},h(){n(m,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.DownloadManager.download_and_extract"),n(w,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.DownloadManager")},m(y,k){h(y,u,k),e(u,m),e(m,p),e(u,_),e(u,g),e(g,v),e(u,b),e(u,w),e(w,Ce),e(u,re)},d(y){y&&a(u)}}}function In(at){let u,m,p,_,g,v,b,w,Ce,re,y,k,Y,st,ot,Qa,z,le,Yt,Ie,no,rt,ho,zt,fo,Xa,lt,co,es,it,uo,ts,ie,Zt,po,vo,Jt,_o,as,de,mo,dt,go,wo,ss,ne,bo,nt,yo,Eo,os,he,rs,ht,Do,ls,ft,ko,is,Z,fe,jt,Pe,Ao,Kt,To,ds,L,$o,ct,Bo,Co,ut,Io,Po,ns,Se,Qt,bi,hs,J,ce,Xt,Le,So,ea,Lo,fs,C,pt,xo,Ho,vt,Oo,Ro,_t,No,Mo,cs,ue,ta,xe,aa,qo,Fo,sa,Go,Uo,A,He,oa,ra,Wo,Vo,la,Yo,zo,Oe,ia,da,Zo,Jo,na,jo,Ko,Re,ha,fa,Qo,Xo,ca,er,tr,Ne,ua,pa,ar,sr,va,or,rr,Me,_a,ma,lr,ir,ga,dr,us,x,nr,mt,hr,fr,gt,cr,ur,ps,pe,wa,j,pr,wt,vr,_r,ba,mr,gr,wr,ya,K,br,bt,yr,Er,yt,Dr,kr,vs,H,Ar,Et,Tr,$r,Dt,Br,Cr,_s,Q,ve,Ea,qe,Ir,Da,Pr,ms,X,kt,Sr,Lr,At,xr,Hr,gs,Fe,ka,yi,ws,_e,Or,Tt,Rr,Nr,bs,me,Aa,T,Ta,Mr,qr,$a,Fr,Gr,$t,Ur,Wr,Bt,Vr,Yr,zr,Ge,$,Ba,Zr,Jr,Ct,jr,Kr,It,Qr,Xr,Pt,el,tl,al,I,sl,St,ol,rl,Lt,ll,il,Ca,dl,nl,ys,ge,Es,Ue,We,E,Ia,hl,fl,Pa,cl,ul,Sa,pl,vl,La,_l,ml,xa,gl,wl,bl,P,yl,Ha,El,Dl,Oa,kl,Al,xt,Tl,$l,Ds,ee,we,Ra,Ve,Bl,Na,Cl,ks,be,Il,Ht,Pl,Sl,As,O,Ll,Ye,xl,Hl,ze,Ol,Rl,Ts,te,ye,Ma,Ze,Nl,qa,Ml,$s,R,ql,Ot,Fl,Gl,Rt,Ul,Wl,Bs,D,Fa,Vl,Yl,Ga,zl,Zl,Ua,Jl,jl,Je,Kl,Wa,Ql,Xl,ei,je,ti,Va,ai,si,Cs,B,oi,Ya,ri,li,Nt,ii,di,Ke,ni,hi,Is,ae,Ee,za,Qe,fi,Za,ci,Ps,De,ui,Xe,pi,vi,Ss,ke,_i,Ja,mi,gi,Ls;return v=new Be({}),Ie=new Be({}),he=new En({props:{$$slots:{default:[Bn]},$$scope:{ctx:at}}}),Pe=new Be({}),Le=new Be({}),qe=new Be({}),ge=new En({props:{$$slots:{default:[Cn]},$$scope:{ctx:at}}}),Ve=new Be({}),Ze=new Be({}),Qe=new Be({}),{c(){u=s("meta"),m=f(),p=s("h1"),_=s("a"),g=s("span"),q(v.$$.fragment),b=f(),w=s("span"),Ce=l("Build and load"),re=f(),y=s("p"),k=l("Nearly every deep learning workflow begins with loading a dataset, which makes it one of the most important steps. With \u{1F917} Datasets, there are more than 900 datasets available to help you get started with your NLP task. All you have to do is call: "),Y=s("a"),st=l("load_dataset()"),ot=l(" to take your first step. This function is a true workhorse in every sense because it builds and loads every dataset you use."),Qa=f(),z=s("h2"),le=s("a"),Yt=s("span"),q(Ie.$$.fragment),no=f(),rt=s("span"),ho=l("ELI5: "),zt=s("code"),fo=l("load_dataset"),Xa=f(),lt=s("p"),co=l("Let\u2019s begin with a basic Explain Like I\u2019m Five."),es=f(),it=s("p"),uo=l("A dataset is a directory that contains:"),ts=f(),ie=s("ul"),Zt=s("li"),po=l("Some data files in generic formats (JSON, CSV, Parquet, text, etc.)"),vo=f(),Jt=s("li"),_o=l("An optional dataset script if it requires some code to read the data files. This is used to load files of all formats and structures."),as=f(),de=s("p"),mo=l("The "),dt=s("a"),go=l("load_dataset()"),wo=l(` function fetches the requested dataset locally or from the Hugging Face Hub.
The Hub is a central repository where all the Hugging Face datasets and models are stored.`),ss=f(),ne=s("p"),bo=l("If the dataset only contains data files, then "),nt=s("a"),yo=l("load_dataset()"),Eo=l(` automatically infers how to load the data files from their extensions (json, csv, parquet, txt, etc.).
If the dataset has a dataset script, then it downloads and imports it from the Hugging Face Hub.
Code in the dataset script defines the dataset information (description, features, URL to the original files, etc.), and tells \u{1F917} Datasets how to generate and display examples from it.`),os=f(),q(he.$$.fragment),rs=f(),ht=s("p"),Do=l("The dataset script downloads the dataset files from the original URL, generates the dataset and caches it in an Arrow table on your drive. If you\u2019ve downloaded the dataset before, then \u{1F917} Datasets will reload it from the cache to save you the trouble of downloading it again."),ls=f(),ft=s("p"),ko=l("Now that you have a high-level understanding about how datasets are built, let\u2019s take a closer look at the nuts and bolts of how all this works."),is=f(),Z=s("h2"),fe=s("a"),jt=s("span"),q(Pe.$$.fragment),Ao=f(),Kt=s("span"),To=l("Building a dataset"),ds=f(),L=s("p"),$o=l("When you load a dataset for the first time, \u{1F917} Datasets takes the raw data file and builds it into a table of rows and typed columns. There are two main classes responsible for building a dataset: "),ct=s("a"),Bo=l("BuilderConfig"),Co=l(" and "),ut=s("a"),Io=l("DatasetBuilder"),Po=l("."),ns=f(),Se=s("div"),Qt=s("img"),hs=f(),J=s("h3"),ce=s("a"),Xt=s("span"),q(Le.$$.fragment),So=f(),ea=s("span"),Lo=l("BuilderConfig"),fs=f(),C=s("p"),pt=s("a"),xo=l("BuilderConfig"),Ho=l(" is the configuration class of "),vt=s("a"),Oo=l("DatasetBuilder"),Ro=l(". The "),_t=s("a"),No=l("BuilderConfig"),Mo=l(" contains the following basic attributes about a dataset:"),cs=f(),ue=s("table"),ta=s("thead"),xe=s("tr"),aa=s("th"),qo=l("Attribute"),Fo=f(),sa=s("th"),Go=l("Description"),Uo=f(),A=s("tbody"),He=s("tr"),oa=s("td"),ra=s("code"),Wo=l("name"),Vo=f(),la=s("td"),Yo=l("Short name of the dataset."),zo=f(),Oe=s("tr"),ia=s("td"),da=s("code"),Zo=l("version"),Jo=f(),na=s("td"),jo=l("Dataset version identifier."),Ko=f(),Re=s("tr"),ha=s("td"),fa=s("code"),Qo=l("data_dir"),Xo=f(),ca=s("td"),er=l("Stores the path to a local folder containing the data files."),tr=f(),Ne=s("tr"),ua=s("td"),pa=s("code"),ar=l("data_files"),sr=f(),va=s("td"),or=l("Stores paths to local data files."),rr=f(),Me=s("tr"),_a=s("td"),ma=s("code"),lr=l("description"),ir=f(),ga=s("td"),dr=l("Description of the dataset."),us=f(),x=s("p"),nr=l("If you want to add additional attributes to your dataset such as the class labels, you can subclass the base "),mt=s("a"),hr=l("BuilderConfig"),fr=l(" class. There are two ways to populate the attributes of a "),gt=s("a"),cr=l("BuilderConfig"),ur=l(" class or subclass:"),ps=f(),pe=s("ul"),wa=s("li"),j=s("p"),pr=l("Provide a list of predefined "),wt=s("a"),vr=l("BuilderConfig"),_r=l(" class (or subclass) instances in the datasets "),ba=s("code"),mr=l("DatasetBuilder.BUILDER_CONFIGS()"),gr=l(" attribute."),wr=f(),ya=s("li"),K=s("p"),br=l("When you call "),bt=s("a"),yr=l("load_dataset()"),Er=l(", any keyword arguments that are not specific to the method will be used to set the associated attributes of the "),yt=s("a"),Dr=l("BuilderConfig"),kr=l(" class. This will override the predefined attributes if a specific configuration was selected."),vs=f(),H=s("p"),Ar=l("You can also set the "),Et=s("a"),Tr=l("DatasetBuilder.BUILDER_CONFIG_CLASS"),$r=l(" to any custom subclass of "),Dt=s("a"),Br=l("BuilderConfig"),Cr=l("."),_s=f(),Q=s("h3"),ve=s("a"),Ea=s("span"),q(qe.$$.fragment),Ir=f(),Da=s("span"),Pr=l("DatasetBuilder"),ms=f(),X=s("p"),kt=s("a"),Sr=l("DatasetBuilder"),Lr=l(" accesses all the attributes inside "),At=s("a"),xr=l("BuilderConfig"),Hr=l(" to build the actual dataset."),gs=f(),Fe=s("div"),ka=s("img"),ws=f(),_e=s("p"),Or=l("There are three main methods in "),Tt=s("a"),Rr=l("DatasetBuilder"),Nr=l(":"),bs=f(),me=s("ol"),Aa=s("li"),T=s("p"),Ta=s("code"),Mr=l("DatasetBuilder._info()"),qr=l(" is in charge of defining the dataset attributes. When you call "),$a=s("code"),Fr=l("dataset.info"),Gr=l(", \u{1F917} Datasets returns the information stored here. Likewise, the "),$t=s("a"),Ur=l("Features"),Wr=l(" are also specified here. Remember, the "),Bt=s("a"),Vr=l("Features"),Yr=l(" are like the skeleton of the dataset. It provides the names and types of each column."),zr=f(),Ge=s("li"),$=s("p"),Ba=s("code"),Zr=l("DatasetBuilder._split_generator"),Jr=l(" downloads or retrieves the requested data files, organizes them into splits, and defines specific arguments for the generation process. This method has a "),Ct=s("a"),jr=l("DownloadManager"),Kr=l(" that downloads files or fetches them from your local filesystem. Within the "),It=s("a"),Qr=l("DownloadManager"),Xr=l(", there is a "),Pt=s("a"),el=l("DownloadManager.download_and_extract()"),tl=l(" method that accepts a dictionary of URLs to the original data files, and downloads the requested files. Accepted inputs include: a single URL or path, or a list/dictionary of URLs or paths. Any compressed file types like TAR, GZIP and ZIP archives will be automatically extracted."),al=f(),I=s("p"),sl=l("Once the files are downloaded, "),St=s("a"),ol=l("SplitGenerator"),rl=l(" organizes them into splits. The "),Lt=s("a"),ll=l("SplitGenerator"),il=l(" contains the name of the split, and any keyword arguments that are provided to the "),Ca=s("code"),dl=l("DatasetBuilder._generate_examples"),nl=l(" method. The keyword arguments can be specific to each split, and typically comprise at least the local path to the data files for each split."),ys=f(),q(ge.$$.fragment),Es=f(),Ue=s("ol"),We=s("li"),E=s("p"),Ia=s("code"),hl=l("DatasetBuilder._generate_examples"),fl=l(" reads and parses the data files for a split. Then it yields dataset examples according to the format specified in the "),Pa=s("code"),cl=l("features"),ul=l(" from "),Sa=s("code"),pl=l("DatasetBuilder._info()"),vl=l(". The input of "),La=s("code"),_l=l("DatasetBuilder._generate_examples"),ml=l(" is actually the "),xa=s("code"),gl=l("filepath"),wl=l(" provided in the keyword arguments of the last method."),bl=f(),P=s("p"),yl=l("The dataset is generated with a Python generator, which doesn\u2019t load all the data in memory. As a result, the generator can handle large datasets. However, before the generated samples are flushed to the dataset file on disk, they are stored in an "),Ha=s("code"),El=l("ArrowWriter"),Dl=l(" buffer. This means the generated samples are written by batch. If your dataset samples consumes a lot of memory (images or videos), then make sure to specify a low value for the "),Oa=s("code"),kl=l("DEFAULT_WRITER_BATCH_SIZE"),Al=l(" attribute in "),xt=s("a"),Tl=l("DatasetBuilder"),$l=l(". We recommend not exceeding a size of 200 MB."),Ds=f(),ee=s("h2"),we=s("a"),Ra=s("span"),q(Ve.$$.fragment),Bl=f(),Na=s("span"),Cl=l("Without loading scripts"),ks=f(),be=s("p"),Il=l("As a user, you want to be able to quickly use a dataset. Implementing a dataset loading script can sometimes get in the way, or it may be a barrier for some people without a developer background. \u{1F917} Datasets removes this barrier by making it possible to load any dataset from the Hub without a dataset loading script. All a user has to do is upload the data files (see "),Ht=s("a"),Pl=l("upload_dataset_repo"),Sl=l(" for a list of supported file formats) to a dataset repository on the Hub, and they will be able to load that dataset without having to create a loading script. This doesn\u2019t mean we are moving away from loading scripts because they still offer the most flexibility in controlling how a dataset is generated."),As=f(),O=s("p"),Ll=l("The loading script-free method uses the "),Ye=s("a"),xl=l("huggingface_hub"),Hl=l(" library to list the files in a dataset repository. You can also provide a path to a local directory instead of a repository name, in which case \u{1F917} Datasets will use "),ze=s("a"),Ol=l("glob"),Rl=l(" instead. Depending on the format of the data files available, one of the data file builders will create your dataset for you. If you have a CSV file, the CSV builder will be used and if you have a Parquet file, the Parquet builder will be used. The drawback of this approach is it\u2019s not possible to simultaneously load a CSV and JSON file. You will need to load the two file types separately, and then concatenate them."),Ts=f(),te=s("h2"),ye=s("a"),Ma=s("span"),q(Ze.$$.fragment),Nl=f(),qa=s("span"),Ml=l("Maintaining integrity"),$s=f(),R=s("p"),ql=l("To ensure a dataset is complete, "),Ot=s("a"),Fl=l("load_dataset()"),Gl=l(" will perform a series of tests on the downloaded files to make sure everything is there. This way, you don\u2019t encounter any surprises when your requested dataset doesn\u2019t get generated as expected. "),Rt=s("a"),Ul=l("load_dataset()"),Wl=l(" verifies:"),Bs=f(),D=s("ul"),Fa=s("li"),Vl=l("The list of downloaded files."),Yl=f(),Ga=s("li"),zl=l("The number of bytes of the downloaded files."),Zl=f(),Ua=s("li"),Jl=l("The SHA256 checksums of the downloaded files."),jl=f(),Je=s("li"),Kl=l("The number of splits in the generated "),Wa=s("code"),Ql=l("DatasetDict"),Xl=l("."),ei=f(),je=s("li"),ti=l("The number of samples in each split of the generated "),Va=s("code"),ai=l("DatasetDict"),si=l("."),Cs=f(),B=s("p"),oi=l(`If the dataset doesn\u2019t pass the verifications, it is likely that the original host of the dataset made some changes in the data files.
In this case, an error is raised to alert that the dataset has changed.
To ignore the error, one needs to specify `),Ya=s("code"),ri=l("ignore_verifications=True"),li=l(" in "),Nt=s("a"),ii=l("load_dataset()"),di=l(`.
Anytime you see a verification error, feel free to `),Ke=s("a"),ni=l("open an issue on GitHub"),hi=l(" so that we can update the integrity checks for this dataset."),Is=f(),ae=s("h2"),Ee=s("a"),za=s("span"),q(Qe.$$.fragment),fi=f(),Za=s("span"),ci=l("Security"),Ps=f(),De=s("p"),ui=l("The dataset repositories on the Hub are scanned for malware, see more information "),Xe=s("a"),pi=l("here"),vi=l("."),Ss=f(),ke=s("p"),_i=l(`Moreover the datasets that were constributed on our GitHub repository have all been reviewed by our maintainers.
The code of these datasets is considered `),Ja=s("strong"),mi=l("safe"),gi=l(`.
It concerns datasets that are not under a namespace, e.g. \u201Csquad\u201D or \u201Cglue\u201D, unlike the other datasets that are named \u201Cusername/dataset_name\u201D or \u201Corg/dataset_name\u201D.`),this.h()},l(t){const d=Tn('[data-svelte="svelte-1phssyn"]',document.head);u=o(d,"META",{name:!0,content:!0}),d.forEach(a),m=c(t),p=o(t,"H1",{class:!0});var et=r(p);_=o(et,"A",{id:!0,class:!0,href:!0});var ja=r(_);g=o(ja,"SPAN",{});var Ei=r(g);F(v.$$.fragment,Ei),Ei.forEach(a),ja.forEach(a),b=c(et),w=o(et,"SPAN",{});var Di=r(w);Ce=i(Di,"Build and load"),Di.forEach(a),et.forEach(a),re=c(t),y=o(t,"P",{});var xs=r(y);k=i(xs,"Nearly every deep learning workflow begins with loading a dataset, which makes it one of the most important steps. With \u{1F917} Datasets, there are more than 900 datasets available to help you get started with your NLP task. All you have to do is call: "),Y=o(xs,"A",{href:!0});var ki=r(Y);st=i(ki,"load_dataset()"),ki.forEach(a),ot=i(xs," to take your first step. This function is a true workhorse in every sense because it builds and loads every dataset you use."),xs.forEach(a),Qa=c(t),z=o(t,"H2",{class:!0});var Hs=r(z);le=o(Hs,"A",{id:!0,class:!0,href:!0});var Ai=r(le);Yt=o(Ai,"SPAN",{});var Ti=r(Yt);F(Ie.$$.fragment,Ti),Ti.forEach(a),Ai.forEach(a),no=c(Hs),rt=o(Hs,"SPAN",{});var wi=r(rt);ho=i(wi,"ELI5: "),zt=o(wi,"CODE",{});var $i=r(zt);fo=i($i,"load_dataset"),$i.forEach(a),wi.forEach(a),Hs.forEach(a),Xa=c(t),lt=o(t,"P",{});var Bi=r(lt);co=i(Bi,"Let\u2019s begin with a basic Explain Like I\u2019m Five."),Bi.forEach(a),es=c(t),it=o(t,"P",{});var Ci=r(it);uo=i(Ci,"A dataset is a directory that contains:"),Ci.forEach(a),ts=c(t),ie=o(t,"UL",{});var Os=r(ie);Zt=o(Os,"LI",{});var Ii=r(Zt);po=i(Ii,"Some data files in generic formats (JSON, CSV, Parquet, text, etc.)"),Ii.forEach(a),vo=c(Os),Jt=o(Os,"LI",{});var Pi=r(Jt);_o=i(Pi,"An optional dataset script if it requires some code to read the data files. This is used to load files of all formats and structures."),Pi.forEach(a),Os.forEach(a),as=c(t),de=o(t,"P",{});var Rs=r(de);mo=i(Rs,"The "),dt=o(Rs,"A",{href:!0});var Si=r(dt);go=i(Si,"load_dataset()"),Si.forEach(a),wo=i(Rs,` function fetches the requested dataset locally or from the Hugging Face Hub.
The Hub is a central repository where all the Hugging Face datasets and models are stored.`),Rs.forEach(a),ss=c(t),ne=o(t,"P",{});var Ns=r(ne);bo=i(Ns,"If the dataset only contains data files, then "),nt=o(Ns,"A",{href:!0});var Li=r(nt);yo=i(Li,"load_dataset()"),Li.forEach(a),Eo=i(Ns,` automatically infers how to load the data files from their extensions (json, csv, parquet, txt, etc.).
If the dataset has a dataset script, then it downloads and imports it from the Hugging Face Hub.
Code in the dataset script defines the dataset information (description, features, URL to the original files, etc.), and tells \u{1F917} Datasets how to generate and display examples from it.`),Ns.forEach(a),os=c(t),F(he.$$.fragment,t),rs=c(t),ht=o(t,"P",{});var xi=r(ht);Do=i(xi,"The dataset script downloads the dataset files from the original URL, generates the dataset and caches it in an Arrow table on your drive. If you\u2019ve downloaded the dataset before, then \u{1F917} Datasets will reload it from the cache to save you the trouble of downloading it again."),xi.forEach(a),ls=c(t),ft=o(t,"P",{});var Hi=r(ft);ko=i(Hi,"Now that you have a high-level understanding about how datasets are built, let\u2019s take a closer look at the nuts and bolts of how all this works."),Hi.forEach(a),is=c(t),Z=o(t,"H2",{class:!0});var Ms=r(Z);fe=o(Ms,"A",{id:!0,class:!0,href:!0});var Oi=r(fe);jt=o(Oi,"SPAN",{});var Ri=r(jt);F(Pe.$$.fragment,Ri),Ri.forEach(a),Oi.forEach(a),Ao=c(Ms),Kt=o(Ms,"SPAN",{});var Ni=r(Kt);To=i(Ni,"Building a dataset"),Ni.forEach(a),Ms.forEach(a),ds=c(t),L=o(t,"P",{});var Mt=r(L);$o=i(Mt,"When you load a dataset for the first time, \u{1F917} Datasets takes the raw data file and builds it into a table of rows and typed columns. There are two main classes responsible for building a dataset: "),ct=o(Mt,"A",{href:!0});var Mi=r(ct);Bo=i(Mi,"BuilderConfig"),Mi.forEach(a),Co=i(Mt," and "),ut=o(Mt,"A",{href:!0});var qi=r(ut);Io=i(qi,"DatasetBuilder"),qi.forEach(a),Po=i(Mt,"."),Mt.forEach(a),ns=c(t),Se=o(t,"DIV",{class:!0});var Fi=r(Se);Qt=o(Fi,"IMG",{src:!0}),Fi.forEach(a),hs=c(t),J=o(t,"H3",{class:!0});var qs=r(J);ce=o(qs,"A",{id:!0,class:!0,href:!0});var Gi=r(ce);Xt=o(Gi,"SPAN",{});var Ui=r(Xt);F(Le.$$.fragment,Ui),Ui.forEach(a),Gi.forEach(a),So=c(qs),ea=o(qs,"SPAN",{});var Wi=r(ea);Lo=i(Wi,"BuilderConfig"),Wi.forEach(a),qs.forEach(a),fs=c(t),C=o(t,"P",{});var tt=r(C);pt=o(tt,"A",{href:!0});var Vi=r(pt);xo=i(Vi,"BuilderConfig"),Vi.forEach(a),Ho=i(tt," is the configuration class of "),vt=o(tt,"A",{href:!0});var Yi=r(vt);Oo=i(Yi,"DatasetBuilder"),Yi.forEach(a),Ro=i(tt,". The "),_t=o(tt,"A",{href:!0});var zi=r(_t);No=i(zi,"BuilderConfig"),zi.forEach(a),Mo=i(tt," contains the following basic attributes about a dataset:"),tt.forEach(a),cs=c(t),ue=o(t,"TABLE",{});var Fs=r(ue);ta=o(Fs,"THEAD",{});var Zi=r(ta);xe=o(Zi,"TR",{});var Gs=r(xe);aa=o(Gs,"TH",{});var Ji=r(aa);qo=i(Ji,"Attribute"),Ji.forEach(a),Fo=c(Gs),sa=o(Gs,"TH",{});var ji=r(sa);Go=i(ji,"Description"),ji.forEach(a),Gs.forEach(a),Zi.forEach(a),Uo=c(Fs),A=o(Fs,"TBODY",{});var N=r(A);He=o(N,"TR",{});var Us=r(He);oa=o(Us,"TD",{});var Ki=r(oa);ra=o(Ki,"CODE",{});var Qi=r(ra);Wo=i(Qi,"name"),Qi.forEach(a),Ki.forEach(a),Vo=c(Us),la=o(Us,"TD",{});var Xi=r(la);Yo=i(Xi,"Short name of the dataset."),Xi.forEach(a),Us.forEach(a),zo=c(N),Oe=o(N,"TR",{});var Ws=r(Oe);ia=o(Ws,"TD",{});var ed=r(ia);da=o(ed,"CODE",{});var td=r(da);Zo=i(td,"version"),td.forEach(a),ed.forEach(a),Jo=c(Ws),na=o(Ws,"TD",{});var ad=r(na);jo=i(ad,"Dataset version identifier."),ad.forEach(a),Ws.forEach(a),Ko=c(N),Re=o(N,"TR",{});var Vs=r(Re);ha=o(Vs,"TD",{});var sd=r(ha);fa=o(sd,"CODE",{});var od=r(fa);Qo=i(od,"data_dir"),od.forEach(a),sd.forEach(a),Xo=c(Vs),ca=o(Vs,"TD",{});var rd=r(ca);er=i(rd,"Stores the path to a local folder containing the data files."),rd.forEach(a),Vs.forEach(a),tr=c(N),Ne=o(N,"TR",{});var Ys=r(Ne);ua=o(Ys,"TD",{});var ld=r(ua);pa=o(ld,"CODE",{});var id=r(pa);ar=i(id,"data_files"),id.forEach(a),ld.forEach(a),sr=c(Ys),va=o(Ys,"TD",{});var dd=r(va);or=i(dd,"Stores paths to local data files."),dd.forEach(a),Ys.forEach(a),rr=c(N),Me=o(N,"TR",{});var zs=r(Me);_a=o(zs,"TD",{});var nd=r(_a);ma=o(nd,"CODE",{});var hd=r(ma);lr=i(hd,"description"),hd.forEach(a),nd.forEach(a),ir=c(zs),ga=o(zs,"TD",{});var fd=r(ga);dr=i(fd,"Description of the dataset."),fd.forEach(a),zs.forEach(a),N.forEach(a),Fs.forEach(a),us=c(t),x=o(t,"P",{});var qt=r(x);nr=i(qt,"If you want to add additional attributes to your dataset such as the class labels, you can subclass the base "),mt=o(qt,"A",{href:!0});var cd=r(mt);hr=i(cd,"BuilderConfig"),cd.forEach(a),fr=i(qt," class. There are two ways to populate the attributes of a "),gt=o(qt,"A",{href:!0});var ud=r(gt);cr=i(ud,"BuilderConfig"),ud.forEach(a),ur=i(qt," class or subclass:"),qt.forEach(a),ps=c(t),pe=o(t,"UL",{});var Zs=r(pe);wa=o(Zs,"LI",{});var pd=r(wa);j=o(pd,"P",{});var Ft=r(j);pr=i(Ft,"Provide a list of predefined "),wt=o(Ft,"A",{href:!0});var vd=r(wt);vr=i(vd,"BuilderConfig"),vd.forEach(a),_r=i(Ft," class (or subclass) instances in the datasets "),ba=o(Ft,"CODE",{});var _d=r(ba);mr=i(_d,"DatasetBuilder.BUILDER_CONFIGS()"),_d.forEach(a),gr=i(Ft," attribute."),Ft.forEach(a),pd.forEach(a),wr=c(Zs),ya=o(Zs,"LI",{});var md=r(ya);K=o(md,"P",{});var Gt=r(K);br=i(Gt,"When you call "),bt=o(Gt,"A",{href:!0});var gd=r(bt);yr=i(gd,"load_dataset()"),gd.forEach(a),Er=i(Gt,", any keyword arguments that are not specific to the method will be used to set the associated attributes of the "),yt=o(Gt,"A",{href:!0});var wd=r(yt);Dr=i(wd,"BuilderConfig"),wd.forEach(a),kr=i(Gt," class. This will override the predefined attributes if a specific configuration was selected."),Gt.forEach(a),md.forEach(a),Zs.forEach(a),vs=c(t),H=o(t,"P",{});var Ut=r(H);Ar=i(Ut,"You can also set the "),Et=o(Ut,"A",{href:!0});var bd=r(Et);Tr=i(bd,"DatasetBuilder.BUILDER_CONFIG_CLASS"),bd.forEach(a),$r=i(Ut," to any custom subclass of "),Dt=o(Ut,"A",{href:!0});var yd=r(Dt);Br=i(yd,"BuilderConfig"),yd.forEach(a),Cr=i(Ut,"."),Ut.forEach(a),_s=c(t),Q=o(t,"H3",{class:!0});var Js=r(Q);ve=o(Js,"A",{id:!0,class:!0,href:!0});var Ed=r(ve);Ea=o(Ed,"SPAN",{});var Dd=r(Ea);F(qe.$$.fragment,Dd),Dd.forEach(a),Ed.forEach(a),Ir=c(Js),Da=o(Js,"SPAN",{});var kd=r(Da);Pr=i(kd,"DatasetBuilder"),kd.forEach(a),Js.forEach(a),ms=c(t),X=o(t,"P",{});var Ka=r(X);kt=o(Ka,"A",{href:!0});var Ad=r(kt);Sr=i(Ad,"DatasetBuilder"),Ad.forEach(a),Lr=i(Ka," accesses all the attributes inside "),At=o(Ka,"A",{href:!0});var Td=r(At);xr=i(Td,"BuilderConfig"),Td.forEach(a),Hr=i(Ka," to build the actual dataset."),Ka.forEach(a),gs=c(t),Fe=o(t,"DIV",{class:!0});var $d=r(Fe);ka=o($d,"IMG",{src:!0}),$d.forEach(a),ws=c(t),_e=o(t,"P",{});var js=r(_e);Or=i(js,"There are three main methods in "),Tt=o(js,"A",{href:!0});var Bd=r(Tt);Rr=i(Bd,"DatasetBuilder"),Bd.forEach(a),Nr=i(js,":"),js.forEach(a),bs=c(t),me=o(t,"OL",{});var Ks=r(me);Aa=o(Ks,"LI",{});var Cd=r(Aa);T=o(Cd,"P",{});var se=r(T);Ta=o(se,"CODE",{});var Id=r(Ta);Mr=i(Id,"DatasetBuilder._info()"),Id.forEach(a),qr=i(se," is in charge of defining the dataset attributes. When you call "),$a=o(se,"CODE",{});var Pd=r($a);Fr=i(Pd,"dataset.info"),Pd.forEach(a),Gr=i(se,", \u{1F917} Datasets returns the information stored here. Likewise, the "),$t=o(se,"A",{href:!0});var Sd=r($t);Ur=i(Sd,"Features"),Sd.forEach(a),Wr=i(se," are also specified here. Remember, the "),Bt=o(se,"A",{href:!0});var Ld=r(Bt);Vr=i(Ld,"Features"),Ld.forEach(a),Yr=i(se," are like the skeleton of the dataset. It provides the names and types of each column."),se.forEach(a),Cd.forEach(a),zr=c(Ks),Ge=o(Ks,"LI",{});var Qs=r(Ge);$=o(Qs,"P",{});var oe=r($);Ba=o(oe,"CODE",{});var xd=r(Ba);Zr=i(xd,"DatasetBuilder._split_generator"),xd.forEach(a),Jr=i(oe," downloads or retrieves the requested data files, organizes them into splits, and defines specific arguments for the generation process. This method has a "),Ct=o(oe,"A",{href:!0});var Hd=r(Ct);jr=i(Hd,"DownloadManager"),Hd.forEach(a),Kr=i(oe," that downloads files or fetches them from your local filesystem. Within the "),It=o(oe,"A",{href:!0});var Od=r(It);Qr=i(Od,"DownloadManager"),Od.forEach(a),Xr=i(oe,", there is a "),Pt=o(oe,"A",{href:!0});var Rd=r(Pt);el=i(Rd,"DownloadManager.download_and_extract()"),Rd.forEach(a),tl=i(oe," method that accepts a dictionary of URLs to the original data files, and downloads the requested files. Accepted inputs include: a single URL or path, or a list/dictionary of URLs or paths. Any compressed file types like TAR, GZIP and ZIP archives will be automatically extracted."),oe.forEach(a),al=c(Qs),I=o(Qs,"P",{});var Ae=r(I);sl=i(Ae,"Once the files are downloaded, "),St=o(Ae,"A",{href:!0});var Nd=r(St);ol=i(Nd,"SplitGenerator"),Nd.forEach(a),rl=i(Ae," organizes them into splits. The "),Lt=o(Ae,"A",{href:!0});var Md=r(Lt);ll=i(Md,"SplitGenerator"),Md.forEach(a),il=i(Ae," contains the name of the split, and any keyword arguments that are provided to the "),Ca=o(Ae,"CODE",{});var qd=r(Ca);dl=i(qd,"DatasetBuilder._generate_examples"),qd.forEach(a),nl=i(Ae," method. The keyword arguments can be specific to each split, and typically comprise at least the local path to the data files for each split."),Ae.forEach(a),Qs.forEach(a),Ks.forEach(a),ys=c(t),F(ge.$$.fragment,t),Es=c(t),Ue=o(t,"OL",{start:!0});var Fd=r(Ue);We=o(Fd,"LI",{});var Xs=r(We);E=o(Xs,"P",{});var S=r(E);Ia=o(S,"CODE",{});var Gd=r(Ia);hl=i(Gd,"DatasetBuilder._generate_examples"),Gd.forEach(a),fl=i(S," reads and parses the data files for a split. Then it yields dataset examples according to the format specified in the "),Pa=o(S,"CODE",{});var Ud=r(Pa);cl=i(Ud,"features"),Ud.forEach(a),ul=i(S," from "),Sa=o(S,"CODE",{});var Wd=r(Sa);pl=i(Wd,"DatasetBuilder._info()"),Wd.forEach(a),vl=i(S,". The input of "),La=o(S,"CODE",{});var Vd=r(La);_l=i(Vd,"DatasetBuilder._generate_examples"),Vd.forEach(a),ml=i(S," is actually the "),xa=o(S,"CODE",{});var Yd=r(xa);gl=i(Yd,"filepath"),Yd.forEach(a),wl=i(S," provided in the keyword arguments of the last method."),S.forEach(a),bl=c(Xs),P=o(Xs,"P",{});var Te=r(P);yl=i(Te,"The dataset is generated with a Python generator, which doesn\u2019t load all the data in memory. As a result, the generator can handle large datasets. However, before the generated samples are flushed to the dataset file on disk, they are stored in an "),Ha=o(Te,"CODE",{});var zd=r(Ha);El=i(zd,"ArrowWriter"),zd.forEach(a),Dl=i(Te," buffer. This means the generated samples are written by batch. If your dataset samples consumes a lot of memory (images or videos), then make sure to specify a low value for the "),Oa=o(Te,"CODE",{});var Zd=r(Oa);kl=i(Zd,"DEFAULT_WRITER_BATCH_SIZE"),Zd.forEach(a),Al=i(Te," attribute in "),xt=o(Te,"A",{href:!0});var Jd=r(xt);Tl=i(Jd,"DatasetBuilder"),Jd.forEach(a),$l=i(Te,". We recommend not exceeding a size of 200 MB."),Te.forEach(a),Xs.forEach(a),Fd.forEach(a),Ds=c(t),ee=o(t,"H2",{class:!0});var eo=r(ee);we=o(eo,"A",{id:!0,class:!0,href:!0});var jd=r(we);Ra=o(jd,"SPAN",{});var Kd=r(Ra);F(Ve.$$.fragment,Kd),Kd.forEach(a),jd.forEach(a),Bl=c(eo),Na=o(eo,"SPAN",{});var Qd=r(Na);Cl=i(Qd,"Without loading scripts"),Qd.forEach(a),eo.forEach(a),ks=c(t),be=o(t,"P",{});var to=r(be);Il=i(to,"As a user, you want to be able to quickly use a dataset. Implementing a dataset loading script can sometimes get in the way, or it may be a barrier for some people without a developer background. \u{1F917} Datasets removes this barrier by making it possible to load any dataset from the Hub without a dataset loading script. All a user has to do is upload the data files (see "),Ht=o(to,"A",{href:!0});var Xd=r(Ht);Pl=i(Xd,"upload_dataset_repo"),Xd.forEach(a),Sl=i(to," for a list of supported file formats) to a dataset repository on the Hub, and they will be able to load that dataset without having to create a loading script. This doesn\u2019t mean we are moving away from loading scripts because they still offer the most flexibility in controlling how a dataset is generated."),to.forEach(a),As=c(t),O=o(t,"P",{});var Wt=r(O);Ll=i(Wt,"The loading script-free method uses the "),Ye=o(Wt,"A",{href:!0,rel:!0});var en=r(Ye);xl=i(en,"huggingface_hub"),en.forEach(a),Hl=i(Wt," library to list the files in a dataset repository. You can also provide a path to a local directory instead of a repository name, in which case \u{1F917} Datasets will use "),ze=o(Wt,"A",{href:!0,rel:!0});var tn=r(ze);Ol=i(tn,"glob"),tn.forEach(a),Rl=i(Wt," instead. Depending on the format of the data files available, one of the data file builders will create your dataset for you. If you have a CSV file, the CSV builder will be used and if you have a Parquet file, the Parquet builder will be used. The drawback of this approach is it\u2019s not possible to simultaneously load a CSV and JSON file. You will need to load the two file types separately, and then concatenate them."),Wt.forEach(a),Ts=c(t),te=o(t,"H2",{class:!0});var ao=r(te);ye=o(ao,"A",{id:!0,class:!0,href:!0});var an=r(ye);Ma=o(an,"SPAN",{});var sn=r(Ma);F(Ze.$$.fragment,sn),sn.forEach(a),an.forEach(a),Nl=c(ao),qa=o(ao,"SPAN",{});var on=r(qa);Ml=i(on,"Maintaining integrity"),on.forEach(a),ao.forEach(a),$s=c(t),R=o(t,"P",{});var Vt=r(R);ql=i(Vt,"To ensure a dataset is complete, "),Ot=o(Vt,"A",{href:!0});var rn=r(Ot);Fl=i(rn,"load_dataset()"),rn.forEach(a),Gl=i(Vt," will perform a series of tests on the downloaded files to make sure everything is there. This way, you don\u2019t encounter any surprises when your requested dataset doesn\u2019t get generated as expected. "),Rt=o(Vt,"A",{href:!0});var ln=r(Rt);Ul=i(ln,"load_dataset()"),ln.forEach(a),Wl=i(Vt," verifies:"),Vt.forEach(a),Bs=c(t),D=o(t,"UL",{});var M=r(D);Fa=o(M,"LI",{});var dn=r(Fa);Vl=i(dn,"The list of downloaded files."),dn.forEach(a),Yl=c(M),Ga=o(M,"LI",{});var nn=r(Ga);zl=i(nn,"The number of bytes of the downloaded files."),nn.forEach(a),Zl=c(M),Ua=o(M,"LI",{});var hn=r(Ua);Jl=i(hn,"The SHA256 checksums of the downloaded files."),hn.forEach(a),jl=c(M),Je=o(M,"LI",{});var so=r(Je);Kl=i(so,"The number of splits in the generated "),Wa=o(so,"CODE",{});var fn=r(Wa);Ql=i(fn,"DatasetDict"),fn.forEach(a),Xl=i(so,"."),so.forEach(a),ei=c(M),je=o(M,"LI",{});var oo=r(je);ti=i(oo,"The number of samples in each split of the generated "),Va=o(oo,"CODE",{});var cn=r(Va);ai=i(cn,"DatasetDict"),cn.forEach(a),si=i(oo,"."),oo.forEach(a),M.forEach(a),Cs=c(t),B=o(t,"P",{});var $e=r(B);oi=i($e,`If the dataset doesn\u2019t pass the verifications, it is likely that the original host of the dataset made some changes in the data files.
In this case, an error is raised to alert that the dataset has changed.
To ignore the error, one needs to specify `),Ya=o($e,"CODE",{});var un=r(Ya);ri=i(un,"ignore_verifications=True"),un.forEach(a),li=i($e," in "),Nt=o($e,"A",{href:!0});var pn=r(Nt);ii=i(pn,"load_dataset()"),pn.forEach(a),di=i($e,`.
Anytime you see a verification error, feel free to `),Ke=o($e,"A",{href:!0,rel:!0});var vn=r(Ke);ni=i(vn,"open an issue on GitHub"),vn.forEach(a),hi=i($e," so that we can update the integrity checks for this dataset."),$e.forEach(a),Is=c(t),ae=o(t,"H2",{class:!0});var ro=r(ae);Ee=o(ro,"A",{id:!0,class:!0,href:!0});var _n=r(Ee);za=o(_n,"SPAN",{});var mn=r(za);F(Qe.$$.fragment,mn),mn.forEach(a),_n.forEach(a),fi=c(ro),Za=o(ro,"SPAN",{});var gn=r(Za);ci=i(gn,"Security"),gn.forEach(a),ro.forEach(a),Ps=c(t),De=o(t,"P",{});var lo=r(De);ui=i(lo,"The dataset repositories on the Hub are scanned for malware, see more information "),Xe=o(lo,"A",{href:!0,rel:!0});var wn=r(Xe);pi=i(wn,"here"),wn.forEach(a),vi=i(lo,"."),lo.forEach(a),Ss=c(t),ke=o(t,"P",{});var io=r(ke);_i=i(io,`Moreover the datasets that were constributed on our GitHub repository have all been reviewed by our maintainers.
The code of these datasets is considered `),Ja=o(io,"STRONG",{});var bn=r(Ja);mi=i(bn,"safe"),bn.forEach(a),gi=i(io,`.
It concerns datasets that are not under a namespace, e.g. \u201Csquad\u201D or \u201Cglue\u201D, unlike the other datasets that are named \u201Cusername/dataset_name\u201D or \u201Corg/dataset_name\u201D.`),io.forEach(a),this.h()},h(){n(u,"name","hf:doc:metadata"),n(u,"content",JSON.stringify(Pn)),n(_,"id","build-and-load"),n(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(_,"href","#build-and-load"),n(p,"class","relative group"),n(Y,"href","/docs/datasets/v2.5.0/en/package_reference/loading_methods#datasets.load_dataset"),n(le,"id","eli5-loaddataset"),n(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(le,"href","#eli5-loaddataset"),n(z,"class","relative group"),n(dt,"href","/docs/datasets/v2.5.0/en/package_reference/loading_methods#datasets.load_dataset"),n(nt,"href","/docs/datasets/v2.5.0/en/package_reference/loading_methods#datasets.load_dataset"),n(fe,"id","building-a-dataset"),n(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(fe,"href","#building-a-dataset"),n(Z,"class","relative group"),n(ct,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.BuilderConfig"),n(ut,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.DatasetBuilder"),yn(Qt.src,bi="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/builderconfig.png")||n(Qt,"src",bi),n(Se,"class","flex justify-center"),n(ce,"id","datasets-builderconfig"),n(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(ce,"href","#datasets-builderconfig"),n(J,"class","relative group"),n(pt,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.BuilderConfig"),n(vt,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.DatasetBuilder"),n(_t,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.BuilderConfig"),n(mt,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.BuilderConfig"),n(gt,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.BuilderConfig"),n(wt,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.BuilderConfig"),n(bt,"href","/docs/datasets/v2.5.0/en/package_reference/loading_methods#datasets.load_dataset"),n(yt,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.BuilderConfig"),n(Et,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.BuilderConfig"),n(Dt,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.BuilderConfig"),n(ve,"id","datasets-datasetbuilder"),n(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(ve,"href","#datasets-datasetbuilder"),n(Q,"class","relative group"),n(kt,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.DatasetBuilder"),n(At,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.BuilderConfig"),yn(ka.src,yi="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/datasets/datasetbuilder.png")||n(ka,"src",yi),n(Fe,"class","flex justify-center"),n(Tt,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.DatasetBuilder"),n($t,"href","/docs/datasets/v2.5.0/en/package_reference/main_classes#datasets.Features"),n(Bt,"href","/docs/datasets/v2.5.0/en/package_reference/main_classes#datasets.Features"),n(Ct,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.DownloadManager"),n(It,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.DownloadManager"),n(Pt,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.DownloadManager.download_and_extract"),n(St,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.SplitGenerator"),n(Lt,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.SplitGenerator"),n(xt,"href","/docs/datasets/v2.5.0/en/package_reference/builder_classes#datasets.DatasetBuilder"),n(Ue,"start","3"),n(we,"id","without-loading-scripts"),n(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(we,"href","#without-loading-scripts"),n(ee,"class","relative group"),n(Ht,"href","#upload_dataset_repo"),n(Ye,"href","https://github.com/huggingface/huggingface_hub"),n(Ye,"rel","nofollow"),n(ze,"href","https://docs.python.org/3/library/glob"),n(ze,"rel","nofollow"),n(ye,"id","maintaining-integrity"),n(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(ye,"href","#maintaining-integrity"),n(te,"class","relative group"),n(Ot,"href","/docs/datasets/v2.5.0/en/package_reference/loading_methods#datasets.load_dataset"),n(Rt,"href","/docs/datasets/v2.5.0/en/package_reference/loading_methods#datasets.load_dataset"),n(Nt,"href","/docs/datasets/v2.5.0/en/package_reference/loading_methods#datasets.load_dataset"),n(Ke,"href","https://github.com/huggingface/datasets/issues"),n(Ke,"rel","nofollow"),n(Ee,"id","security"),n(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(Ee,"href","#security"),n(ae,"class","relative group"),n(Xe,"href","https://huggingface.co/docs/hub/security#malware-scanning"),n(Xe,"rel","nofollow")},m(t,d){e(document.head,u),h(t,m,d),h(t,p,d),e(p,_),e(_,g),G(v,g,null),e(p,b),e(p,w),e(w,Ce),h(t,re,d),h(t,y,d),e(y,k),e(y,Y),e(Y,st),e(y,ot),h(t,Qa,d),h(t,z,d),e(z,le),e(le,Yt),G(Ie,Yt,null),e(z,no),e(z,rt),e(rt,ho),e(rt,zt),e(zt,fo),h(t,Xa,d),h(t,lt,d),e(lt,co),h(t,es,d),h(t,it,d),e(it,uo),h(t,ts,d),h(t,ie,d),e(ie,Zt),e(Zt,po),e(ie,vo),e(ie,Jt),e(Jt,_o),h(t,as,d),h(t,de,d),e(de,mo),e(de,dt),e(dt,go),e(de,wo),h(t,ss,d),h(t,ne,d),e(ne,bo),e(ne,nt),e(nt,yo),e(ne,Eo),h(t,os,d),G(he,t,d),h(t,rs,d),h(t,ht,d),e(ht,Do),h(t,ls,d),h(t,ft,d),e(ft,ko),h(t,is,d),h(t,Z,d),e(Z,fe),e(fe,jt),G(Pe,jt,null),e(Z,Ao),e(Z,Kt),e(Kt,To),h(t,ds,d),h(t,L,d),e(L,$o),e(L,ct),e(ct,Bo),e(L,Co),e(L,ut),e(ut,Io),e(L,Po),h(t,ns,d),h(t,Se,d),e(Se,Qt),h(t,hs,d),h(t,J,d),e(J,ce),e(ce,Xt),G(Le,Xt,null),e(J,So),e(J,ea),e(ea,Lo),h(t,fs,d),h(t,C,d),e(C,pt),e(pt,xo),e(C,Ho),e(C,vt),e(vt,Oo),e(C,Ro),e(C,_t),e(_t,No),e(C,Mo),h(t,cs,d),h(t,ue,d),e(ue,ta),e(ta,xe),e(xe,aa),e(aa,qo),e(xe,Fo),e(xe,sa),e(sa,Go),e(ue,Uo),e(ue,A),e(A,He),e(He,oa),e(oa,ra),e(ra,Wo),e(He,Vo),e(He,la),e(la,Yo),e(A,zo),e(A,Oe),e(Oe,ia),e(ia,da),e(da,Zo),e(Oe,Jo),e(Oe,na),e(na,jo),e(A,Ko),e(A,Re),e(Re,ha),e(ha,fa),e(fa,Qo),e(Re,Xo),e(Re,ca),e(ca,er),e(A,tr),e(A,Ne),e(Ne,ua),e(ua,pa),e(pa,ar),e(Ne,sr),e(Ne,va),e(va,or),e(A,rr),e(A,Me),e(Me,_a),e(_a,ma),e(ma,lr),e(Me,ir),e(Me,ga),e(ga,dr),h(t,us,d),h(t,x,d),e(x,nr),e(x,mt),e(mt,hr),e(x,fr),e(x,gt),e(gt,cr),e(x,ur),h(t,ps,d),h(t,pe,d),e(pe,wa),e(wa,j),e(j,pr),e(j,wt),e(wt,vr),e(j,_r),e(j,ba),e(ba,mr),e(j,gr),e(pe,wr),e(pe,ya),e(ya,K),e(K,br),e(K,bt),e(bt,yr),e(K,Er),e(K,yt),e(yt,Dr),e(K,kr),h(t,vs,d),h(t,H,d),e(H,Ar),e(H,Et),e(Et,Tr),e(H,$r),e(H,Dt),e(Dt,Br),e(H,Cr),h(t,_s,d),h(t,Q,d),e(Q,ve),e(ve,Ea),G(qe,Ea,null),e(Q,Ir),e(Q,Da),e(Da,Pr),h(t,ms,d),h(t,X,d),e(X,kt),e(kt,Sr),e(X,Lr),e(X,At),e(At,xr),e(X,Hr),h(t,gs,d),h(t,Fe,d),e(Fe,ka),h(t,ws,d),h(t,_e,d),e(_e,Or),e(_e,Tt),e(Tt,Rr),e(_e,Nr),h(t,bs,d),h(t,me,d),e(me,Aa),e(Aa,T),e(T,Ta),e(Ta,Mr),e(T,qr),e(T,$a),e($a,Fr),e(T,Gr),e(T,$t),e($t,Ur),e(T,Wr),e(T,Bt),e(Bt,Vr),e(T,Yr),e(me,zr),e(me,Ge),e(Ge,$),e($,Ba),e(Ba,Zr),e($,Jr),e($,Ct),e(Ct,jr),e($,Kr),e($,It),e(It,Qr),e($,Xr),e($,Pt),e(Pt,el),e($,tl),e(Ge,al),e(Ge,I),e(I,sl),e(I,St),e(St,ol),e(I,rl),e(I,Lt),e(Lt,ll),e(I,il),e(I,Ca),e(Ca,dl),e(I,nl),h(t,ys,d),G(ge,t,d),h(t,Es,d),h(t,Ue,d),e(Ue,We),e(We,E),e(E,Ia),e(Ia,hl),e(E,fl),e(E,Pa),e(Pa,cl),e(E,ul),e(E,Sa),e(Sa,pl),e(E,vl),e(E,La),e(La,_l),e(E,ml),e(E,xa),e(xa,gl),e(E,wl),e(We,bl),e(We,P),e(P,yl),e(P,Ha),e(Ha,El),e(P,Dl),e(P,Oa),e(Oa,kl),e(P,Al),e(P,xt),e(xt,Tl),e(P,$l),h(t,Ds,d),h(t,ee,d),e(ee,we),e(we,Ra),G(Ve,Ra,null),e(ee,Bl),e(ee,Na),e(Na,Cl),h(t,ks,d),h(t,be,d),e(be,Il),e(be,Ht),e(Ht,Pl),e(be,Sl),h(t,As,d),h(t,O,d),e(O,Ll),e(O,Ye),e(Ye,xl),e(O,Hl),e(O,ze),e(ze,Ol),e(O,Rl),h(t,Ts,d),h(t,te,d),e(te,ye),e(ye,Ma),G(Ze,Ma,null),e(te,Nl),e(te,qa),e(qa,Ml),h(t,$s,d),h(t,R,d),e(R,ql),e(R,Ot),e(Ot,Fl),e(R,Gl),e(R,Rt),e(Rt,Ul),e(R,Wl),h(t,Bs,d),h(t,D,d),e(D,Fa),e(Fa,Vl),e(D,Yl),e(D,Ga),e(Ga,zl),e(D,Zl),e(D,Ua),e(Ua,Jl),e(D,jl),e(D,Je),e(Je,Kl),e(Je,Wa),e(Wa,Ql),e(Je,Xl),e(D,ei),e(D,je),e(je,ti),e(je,Va),e(Va,ai),e(je,si),h(t,Cs,d),h(t,B,d),e(B,oi),e(B,Ya),e(Ya,ri),e(B,li),e(B,Nt),e(Nt,ii),e(B,di),e(B,Ke),e(Ke,ni),e(B,hi),h(t,Is,d),h(t,ae,d),e(ae,Ee),e(Ee,za),G(Qe,za,null),e(ae,fi),e(ae,Za),e(Za,ci),h(t,Ps,d),h(t,De,d),e(De,ui),e(De,Xe),e(Xe,pi),e(De,vi),h(t,Ss,d),h(t,ke,d),e(ke,_i),e(ke,Ja),e(Ja,mi),e(ke,gi),Ls=!0},p(t,[d]){const et={};d&2&&(et.$$scope={dirty:d,ctx:t}),he.$set(et);const ja={};d&2&&(ja.$$scope={dirty:d,ctx:t}),ge.$set(ja)},i(t){Ls||(U(v.$$.fragment,t),U(Ie.$$.fragment,t),U(he.$$.fragment,t),U(Pe.$$.fragment,t),U(Le.$$.fragment,t),U(qe.$$.fragment,t),U(ge.$$.fragment,t),U(Ve.$$.fragment,t),U(Ze.$$.fragment,t),U(Qe.$$.fragment,t),Ls=!0)},o(t){W(v.$$.fragment,t),W(Ie.$$.fragment,t),W(he.$$.fragment,t),W(Pe.$$.fragment,t),W(Le.$$.fragment,t),W(qe.$$.fragment,t),W(ge.$$.fragment,t),W(Ve.$$.fragment,t),W(Ze.$$.fragment,t),W(Qe.$$.fragment,t),Ls=!1},d(t){a(u),t&&a(m),t&&a(p),V(v),t&&a(re),t&&a(y),t&&a(Qa),t&&a(z),V(Ie),t&&a(Xa),t&&a(lt),t&&a(es),t&&a(it),t&&a(ts),t&&a(ie),t&&a(as),t&&a(de),t&&a(ss),t&&a(ne),t&&a(os),V(he,t),t&&a(rs),t&&a(ht),t&&a(ls),t&&a(ft),t&&a(is),t&&a(Z),V(Pe),t&&a(ds),t&&a(L),t&&a(ns),t&&a(Se),t&&a(hs),t&&a(J),V(Le),t&&a(fs),t&&a(C),t&&a(cs),t&&a(ue),t&&a(us),t&&a(x),t&&a(ps),t&&a(pe),t&&a(vs),t&&a(H),t&&a(_s),t&&a(Q),V(qe),t&&a(ms),t&&a(X),t&&a(gs),t&&a(Fe),t&&a(ws),t&&a(_e),t&&a(bs),t&&a(me),t&&a(ys),V(ge,t),t&&a(Es),t&&a(Ue),t&&a(Ds),t&&a(ee),V(Ve),t&&a(ks),t&&a(be),t&&a(As),t&&a(O),t&&a(Ts),t&&a(te),V(Ze),t&&a($s),t&&a(R),t&&a(Bs),t&&a(D),t&&a(Cs),t&&a(B),t&&a(Is),t&&a(ae),V(Qe),t&&a(Ps),t&&a(De),t&&a(Ss),t&&a(ke)}}}const Pn={local:"build-and-load",sections:[{local:"eli5-loaddataset",title:"ELI5: `load_dataset`"},{local:"building-a-dataset",sections:[{local:"datasets-builderconfig",title:"BuilderConfig"},{local:"datasets-datasetbuilder",title:"DatasetBuilder"}],title:"Building a dataset"},{local:"without-loading-scripts",title:"Without loading scripts"},{local:"maintaining-integrity",title:"Maintaining integrity"},{local:"security",title:"Security"}],title:"Build and load"};function Sn(at){return $n(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class On extends Dn{constructor(u){super();kn(this,u,Sn,In,An,{})}}export{On as default,Pn as metadata};
